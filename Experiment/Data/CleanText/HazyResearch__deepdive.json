{
    "rionda": "Zifei, this is an interesting idea if you would like to work on it.\nNote that the active pipeline should be taken into account when doing the above checks.\n. @zifeishan Is this still an issue?\n. Who can?\nOn Mon, Nov 10, 2014 at 6:59 PM, Feiran Wang notifications@github.com\nwrote:\n\nIt's still 1.6. (I don't have permission to modify it) @zifeishan\nhttps://github.com/zifeishan\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/8#issuecomment-62494933.\n. We need a documented process about generating the AMI and uploading it, so that any of us can do it.\n. @zhangce : Ce, where's your AMI? How do we fork it? Sorry for the stupid questions, I never worked with AMIs\n. This is part of a widespread issue: DD errors are impossible to parse for whoever is not familiar with the code.\n. @zifeishan : Anything we can do about this?\n. I don't know what message we get when this happens, so I trust you.\nSuggestion 1. is quite important for usability in my opinion.\nI don't agree about 2.\nFor 3, it depends, we should pay attention to the messages we get and think if a \"normal\" user would understand them.\n. This happens often more generally, i.e., that an exception is thrown but  DD keeps running without really doing anything. What shall we do about it?\n. @zifeishan : does your most recent pull request fix this?\n. I guess it may occur at any point where the JDBC connection is opened? Or if the database \"disappears\" from the network or something? @SenWu can you give us more info, please?\n. You can call \"psql ${DBNAME} -c 'your query'\". It's a workaround.\n\nI'm not sure we can/want to allow both a script and a sql query with the same config directory. Perhaps we can have \"before_sql\" and \"after_sql\" ? The order should be well defined though (e.g., the script before the sql or viceversa)\n. Not a priority. Marking as won't fix and closing.\n. Yes, this would be quite useful indeed.\n. I believe this is still an issue. It would be cool if we can figure it out.\n. Is this fixed?\n. As with most PL/Py bugs, we probably won't fix it.\n. Is this still an issue?\n. Can we have some more details? Is this still an issue?\n. Is this still relevant?\n. Is this still an issue?\n. Is this still an issue?\n. Merged. Closing.\n. is this still an issue? Someone told me that plpy is completely broken anyway, but I may not remember well.\n. I'm considering the plpy extractor support a \"Tier-2\" feature: if it works good, otherwise, well, no one is using it anyway. If we don't eat our own dogfood we cannot even keep it update and functional, and if we're not using it it means that it was either useless or at least not convenient to use.\n. Nah, let's keep everything as it is. If the plpy extractor rottens completely, we'll remove it at some point in the future.\n. Unless you have a quick fix for this, but it's up to you. I don't consider this too important.\n. Is this still an issue?\n. Is this still an issue?\n. Sure. It's a minor issue though.\n. Can we close this? @zifeishan \n. The idea is good, but I wonder whether it is possible to change it now. Perhaps we can create a relation with a different name and make dd_inference_result_variables_mapped_weights a view that just select everything in that table. I wonder how critical it is though.\n. @zifeishan : Looks good. I'm merging and will give you more feedback tomorrow.\n. @zifeishan This would be pretty awesome, yes.\n. No, definitively not, but it would be good to do it incrementally, and perhaps require it for new code.\n. 2 is the right approach here.\n. Fixed, I believe, let's close this.\n. I think this was fixed, can we please close it? @feiranwang @zhangce @SenWu .\n. Hi Zifei, I'll look at this tomorrow afternoon.\nOn Sat, Sep 27, 2014 at 1:54 PM, Zifei Shan notifications@github.com\nwrote:\n\n@rionda https://github.com/rionda @feiranwang\nhttps://github.com/feiranwang Would you finalize the review of this? I\nknow that this many changes are hard to review, but I want to aim at this\nweekend that all the code here is tested. Thanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/143#issuecomment-57065416.\n\n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n. Yes please. If it passes the tests, merge it to both develop and master, as\nper development protocol of hotfix branches.\nThank you for your work!\nOn Wed, Sep 24, 2014 at 2:07 AM, Zifei Shan notifications@github.com\nwrote:\n\nShip it!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/145#issuecomment-56628637.\n\n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n. Are we done? Please merge this ASAP if it's done, because I need this feature. Thank you!\n. By the way, this doesn't work with GreenPlum:\n10:01:47 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] DEBUG EXECUTING.... SELECT fast_seqassign('dd_weights_person_ner_bias_foreign', 0);\n10:01:48 [inferenceManager] ERROR ERROR: Cannot parallelize an UPDATE statement that updates the distribution columns\n  Where: SQL statement \"update dd_weights_person_ner_bias_foreign set id = updateid(0, gp_segment_id, ARRAY[21], ARRAY[1], ARRAY[1]);\"\n. Remember to merge both to develop and to master.\nThanks a lot.\nOn Wed, Sep 24, 2014 at 2:59 PM, Feiran Wang notifications@github.com\nwrote:\n\nMerged #145 https://github.com/HazyResearch/deepdive/pull/145.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/145#event-169782058.\n\n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n. Can't we just download/install it/set it up on the fly? This may require a\nseparate test run as it may take longish.\nLet me know.\nBest,\nMatteo\nOn Wed, Sep 24, 2014 at 6:59 PM, Feiran Wang notifications@github.com\nwrote:\n\nIt seems travis does not naturally support greenplum.\nhttp://docs.travis-ci.com/user/database-setup/\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/145#issuecomment-56752008.\n\n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n. @zifeishan : It seems the failure of the Travis build was a false positive, is that correct?\n. @zifeishan  Also please document this feature somewhere in the documents.\n. That's what I meant. Looks good to me. Remember to merge both in develop\nand master.\nOn Sat, Oct 11, 2014 at 12:44 AM, Zifei Shan notifications@github.com\nwrote:\n\n@rionda https://github.com/rionda What do you mean by false positive?\nIt seems like an error that Travis occasionally has...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/153#issuecomment-58741509.\n\n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n. @zifeishan Feel free to merge to develop by yourself, but I would wait a bit before merging it to master. Is there a special reason why this is high priority or an actual hotfix?\n. Yes, I agree. Please merge it to develop and let's see if it creates any\nissue for anyone. If not, we'll merge to master at some point.\nOn Sat, Oct 11, 2014 at 9:53 PM, Zifei Shan notifications@github.com\nwrote:\n\n@rionda https://github.com/rionda It was just that I was coding up the\ngenomics example and find this issue. Maybe it is a good idea to put it in\ndevelop for a while since nobody other than me actually uses this feature\nnow.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/153#issuecomment-58773561.\n\n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n. @feiranwang  @zhangce  Are we going to have this feature only on Linux or also on Mac? Please let me know, so we decide whether to merge or wait for the mac binary. Thanks =)\n. The links should end in \".html\". Check that the links are correct, and if\ntravis throws an error make sure you look at it, you may have missed a link.\nOn Thu, Oct 16, 2014 at 3:42 PM, gengyl08 notifications@github.com wrote:\n\n@rionda https://github.com/rionda @feiranwang\nhttps://github.com/feiranwang I think the errors come from changing the\npaths from .html to .md.\nThe previous .html links will result in 404 not found and the current .md\nis working.\nWhat do you guys think is the correct solution?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/158#issuecomment-59442450.\n\n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n. Feiran, you can push, this was fixed.\nOn Thu, Oct 16, 2014 at 3:33 PM, Feiran Wang notifications@github.com\nwrote:\n\n@gengyl08 https://github.com/gengyl08 I'll push my changes after travis\nis passed\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/158#issuecomment-59441612.\n\n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n. Yes, I think it was a false positive, not an actual error.\nOn Thu, Oct 16, 2014 at 4:47 PM, Feiran Wang notifications@github.com\nwrote:\n\nGot this error... maybe accidental issue\nThe command \"bash ./src/test/bash/linkcheck.sh\nhttp://deepdive.stanford.edu/\" exited with 1.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/158#issuecomment-59448130.\n\n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n. @zifeishan : I left some comments. It looks almost ready to me. Great job.\n. Can we have a travis job running MySQL? (or is it already on?)\n. Great.\n. It looks good to me, I'm going to merge it.\n. Merged. Closing the issue.\n. @zifeishan : The mac binary was updated (I assume for --quiet). What about the linux binary?\n. @zifeishan, I see you're still committing stuff for fixing the non-termination. This is great, thanks a lot. When you're done let me know.\n. @netj: Remember to send me an email when you open a pull request, otherwise I won't know.\n. (note the there are underscores around dd_variable_type, but the markdown parser of github deletes them)\n. Awesome, thanks @zifeishan !\n. Ah. Yes, I remembered there was something that failed. Ok, let's see if we can find a workaround.\n. Looking at travis it seems to have failed in MySQL as well?\n. That's fine for me. \n. We fixed this issue in another way, so we can close this issue.\n. @zifeishan Instead of catching and ignoring all exceptions, can we catch only the ones that are thrown in case of an error?\n. Already merged.\n. @SenWu, @zifeishan can someone review this for me, please? \n. @zifeishan The build weirdly failed in mysql, which @zhangce and I find quite strange. Can you please help us here?\n. @zifeishan @zhangce The error given by travis in MySQL is definitively reproduceable (it just happened again). Here's the text:\n21:19:59 [DataLoader(akka://deepdive)] INFO  mysql  deepdive_test  -u root  -P 3306  -h 127.0.0.1  --silent -N -e \"SELECT * FROM _dd_variables_has_spouse_view\" > out/test_spouse/dd_variables_has_spouse\n21:19:59 [Helpers$(akka://deepdive)] INFO  Executing command: \"/tmp/unload3355120762408015864.sh\" \n21:20:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:21:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:22:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:23:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:24:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:24:52 [DataLoader(akka://deepdive)] DEBUG Executing query via JDBC: DROP VIEW _dd_variables_has_spouse_view\n21:24:52 [DataLoader(akka://deepdive)] ERROR com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure\nThe last packet successfully received from the server was 292,937 milliseconds ago.  The last packet sent successfully to the server was 13 milliseconds ago.\n21:24:52 [profiler] DEBUG ending report_id=inference_grounding\n21:24:52 [taskManager] INFO  Completed task_id=inference_grounding with Failure(java.sql.SQLException: Already closed.)\n21:24:52 [taskManager] ERROR task=inference_grounding Failed: java.sql.SQLException: Already closed.\n21:24:52 [taskManager] ERROR Forcing shutdown\n21:24:52 [taskManager] ERROR Cancelling task=calibration\n21:24:52 [taskManager] ERROR Cancelling task=inferencetest/test_mysql.sh: line 60: 17791 Killed                  sbt \"test-only org.deepdive.test.integration.MysqlSpouseExample -- -oF\"\nIt happens when executing line 589 of SQLInferenceDataStore.java . \nIt looks almost like a connection timeout to me...any idea?\nYou can see the details of the Travis build here: https://travis-ci.org/HazyResearch/deepdive/jobs/43391021\n. I'll add it, let's see what happens.\n. It seems crazy though, the view definitively exists: we create it!\n. @zifeishan We need your MySQL expertise here. @zhangce found this: https://stackoverflow.com/questions/2983248/com-mysql-jdbc-exceptions-jdbc4-communicationsexception-communications-link-fai\nCan you please give it a look? I wonder whether it is some problem in the way we create the SQL connection with MySQL ?\n. @zifeishan well done, Zifei. Is there a way we can create the index only in the MySQL case? \n. We don't want to do it for psql and gp because this index is only used once, so we don't want to spend the time creating it. \nCan you please change the code so that it is only created for MySQL? Thanks\n. Yes sorry, I didn't know you commited the change.\n. @feiranwang : I'll merge but we will need documentation for the new option before the release. Thanks!\n. This looks good (and very useful), but please document the table name (and schema) in the apposite document. \n. Ok, @zifeishan , merged.\n. @zifeishan please don't just comment out code (especially without comments). Remove it if it's no longer needed.\n. @zifeishan , if you have time can you please look at this?\nThank you.\n. Zifei, this is an interesting idea if you would like to work on it.\nNote that the active pipeline should be taken into account when doing the above checks.\n. @zifeishan Is this still an issue?\n. Who can?\nOn Mon, Nov 10, 2014 at 6:59 PM, Feiran Wang notifications@github.com\nwrote:\n\nIt's still 1.6. (I don't have permission to modify it) @zifeishan\nhttps://github.com/zifeishan\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/8#issuecomment-62494933.\n. We need a documented process about generating the AMI and uploading it, so that any of us can do it.\n. @zhangce : Ce, where's your AMI? How do we fork it? Sorry for the stupid questions, I never worked with AMIs\n. This is part of a widespread issue: DD errors are impossible to parse for whoever is not familiar with the code.\n. @zifeishan : Anything we can do about this?\n. I don't know what message we get when this happens, so I trust you.\nSuggestion 1. is quite important for usability in my opinion.\nI don't agree about 2.\nFor 3, it depends, we should pay attention to the messages we get and think if a \"normal\" user would understand them.\n. This happens often more generally, i.e., that an exception is thrown but  DD keeps running without really doing anything. What shall we do about it?\n. @zifeishan : does your most recent pull request fix this?\n. I guess it may occur at any point where the JDBC connection is opened? Or if the database \"disappears\" from the network or something? @SenWu can you give us more info, please?\n. You can call \"psql ${DBNAME} -c 'your query'\". It's a workaround.\n\nI'm not sure we can/want to allow both a script and a sql query with the same config directory. Perhaps we can have \"before_sql\" and \"after_sql\" ? The order should be well defined though (e.g., the script before the sql or viceversa)\n. Not a priority. Marking as won't fix and closing.\n. Yes, this would be quite useful indeed.\n. I believe this is still an issue. It would be cool if we can figure it out.\n. Is this fixed?\n. As with most PL/Py bugs, we probably won't fix it.\n. Is this still an issue?\n. Can we have some more details? Is this still an issue?\n. Is this still relevant?\n. Is this still an issue?\n. Is this still an issue?\n. Merged. Closing.\n. is this still an issue? Someone told me that plpy is completely broken anyway, but I may not remember well.\n. I'm considering the plpy extractor support a \"Tier-2\" feature: if it works good, otherwise, well, no one is using it anyway. If we don't eat our own dogfood we cannot even keep it update and functional, and if we're not using it it means that it was either useless or at least not convenient to use.\n. Nah, let's keep everything as it is. If the plpy extractor rottens completely, we'll remove it at some point in the future.\n. Unless you have a quick fix for this, but it's up to you. I don't consider this too important.\n. Is this still an issue?\n. Is this still an issue?\n. Sure. It's a minor issue though.\n. Can we close this? @zifeishan \n. The idea is good, but I wonder whether it is possible to change it now. Perhaps we can create a relation with a different name and make dd_inference_result_variables_mapped_weights a view that just select everything in that table. I wonder how critical it is though.\n. @zifeishan : Looks good. I'm merging and will give you more feedback tomorrow.\n. @zifeishan This would be pretty awesome, yes.\n. No, definitively not, but it would be good to do it incrementally, and perhaps require it for new code.\n. 2 is the right approach here.\n. Fixed, I believe, let's close this.\n. I think this was fixed, can we please close it? @feiranwang @zhangce @SenWu .\n. Hi Zifei, I'll look at this tomorrow afternoon.\nOn Sat, Sep 27, 2014 at 1:54 PM, Zifei Shan notifications@github.com\nwrote:\n\n@rionda https://github.com/rionda @feiranwang\nhttps://github.com/feiranwang Would you finalize the review of this? I\nknow that this many changes are hard to review, but I want to aim at this\nweekend that all the code here is tested. Thanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/143#issuecomment-57065416.\n\n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n. Yes please. If it passes the tests, merge it to both develop and master, as\nper development protocol of hotfix branches.\nThank you for your work!\nOn Wed, Sep 24, 2014 at 2:07 AM, Zifei Shan notifications@github.com\nwrote:\n\nShip it!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/145#issuecomment-56628637.\n\n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n. Are we done? Please merge this ASAP if it's done, because I need this feature. Thank you!\n. By the way, this doesn't work with GreenPlum:\n10:01:47 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] DEBUG EXECUTING.... SELECT fast_seqassign('dd_weights_person_ner_bias_foreign', 0);\n10:01:48 [inferenceManager] ERROR ERROR: Cannot parallelize an UPDATE statement that updates the distribution columns\n  Where: SQL statement \"update dd_weights_person_ner_bias_foreign set id = updateid(0, gp_segment_id, ARRAY[21], ARRAY[1], ARRAY[1]);\"\n. Remember to merge both to develop and to master.\nThanks a lot.\nOn Wed, Sep 24, 2014 at 2:59 PM, Feiran Wang notifications@github.com\nwrote:\n\nMerged #145 https://github.com/HazyResearch/deepdive/pull/145.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/145#event-169782058.\n\n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n. Can't we just download/install it/set it up on the fly? This may require a\nseparate test run as it may take longish.\nLet me know.\nBest,\nMatteo\nOn Wed, Sep 24, 2014 at 6:59 PM, Feiran Wang notifications@github.com\nwrote:\n\nIt seems travis does not naturally support greenplum.\nhttp://docs.travis-ci.com/user/database-setup/\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/145#issuecomment-56752008.\n\n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n. @zifeishan : It seems the failure of the Travis build was a false positive, is that correct?\n. @zifeishan  Also please document this feature somewhere in the documents.\n. That's what I meant. Looks good to me. Remember to merge both in develop\nand master.\nOn Sat, Oct 11, 2014 at 12:44 AM, Zifei Shan notifications@github.com\nwrote:\n\n@rionda https://github.com/rionda What do you mean by false positive?\nIt seems like an error that Travis occasionally has...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/153#issuecomment-58741509.\n\n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n. @zifeishan Feel free to merge to develop by yourself, but I would wait a bit before merging it to master. Is there a special reason why this is high priority or an actual hotfix?\n. Yes, I agree. Please merge it to develop and let's see if it creates any\nissue for anyone. If not, we'll merge to master at some point.\nOn Sat, Oct 11, 2014 at 9:53 PM, Zifei Shan notifications@github.com\nwrote:\n\n@rionda https://github.com/rionda It was just that I was coding up the\ngenomics example and find this issue. Maybe it is a good idea to put it in\ndevelop for a while since nobody other than me actually uses this feature\nnow.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/153#issuecomment-58773561.\n\n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n. @feiranwang  @zhangce  Are we going to have this feature only on Linux or also on Mac? Please let me know, so we decide whether to merge or wait for the mac binary. Thanks =)\n. The links should end in \".html\". Check that the links are correct, and if\ntravis throws an error make sure you look at it, you may have missed a link.\nOn Thu, Oct 16, 2014 at 3:42 PM, gengyl08 notifications@github.com wrote:\n\n@rionda https://github.com/rionda @feiranwang\nhttps://github.com/feiranwang I think the errors come from changing the\npaths from .html to .md.\nThe previous .html links will result in 404 not found and the current .md\nis working.\nWhat do you guys think is the correct solution?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/158#issuecomment-59442450.\n\n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n. Feiran, you can push, this was fixed.\nOn Thu, Oct 16, 2014 at 3:33 PM, Feiran Wang notifications@github.com\nwrote:\n\n@gengyl08 https://github.com/gengyl08 I'll push my changes after travis\nis passed\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/158#issuecomment-59441612.\n\n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n. Yes, I think it was a false positive, not an actual error.\nOn Thu, Oct 16, 2014 at 4:47 PM, Feiran Wang notifications@github.com\nwrote:\n\nGot this error... maybe accidental issue\nThe command \"bash ./src/test/bash/linkcheck.sh\nhttp://deepdive.stanford.edu/\" exited with 1.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/158#issuecomment-59448130.\n\n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n. @zifeishan : I left some comments. It looks almost ready to me. Great job.\n. Can we have a travis job running MySQL? (or is it already on?)\n. Great.\n. It looks good to me, I'm going to merge it.\n. Merged. Closing the issue.\n. @zifeishan : The mac binary was updated (I assume for --quiet). What about the linux binary?\n. @zifeishan, I see you're still committing stuff for fixing the non-termination. This is great, thanks a lot. When you're done let me know.\n. @netj: Remember to send me an email when you open a pull request, otherwise I won't know.\n. (note the there are underscores around dd_variable_type, but the markdown parser of github deletes them)\n. Awesome, thanks @zifeishan !\n. Ah. Yes, I remembered there was something that failed. Ok, let's see if we can find a workaround.\n. Looking at travis it seems to have failed in MySQL as well?\n. That's fine for me. \n. We fixed this issue in another way, so we can close this issue.\n. @zifeishan Instead of catching and ignoring all exceptions, can we catch only the ones that are thrown in case of an error?\n. Already merged.\n. @SenWu, @zifeishan can someone review this for me, please? \n. @zifeishan The build weirdly failed in mysql, which @zhangce and I find quite strange. Can you please help us here?\n. @zifeishan @zhangce The error given by travis in MySQL is definitively reproduceable (it just happened again). Here's the text:\n21:19:59 [DataLoader(akka://deepdive)] INFO  mysql  deepdive_test  -u root  -P 3306  -h 127.0.0.1  --silent -N -e \"SELECT * FROM _dd_variables_has_spouse_view\" > out/test_spouse/dd_variables_has_spouse\n21:19:59 [Helpers$(akka://deepdive)] INFO  Executing command: \"/tmp/unload3355120762408015864.sh\" \n21:20:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:21:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:22:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:23:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:24:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:24:52 [DataLoader(akka://deepdive)] DEBUG Executing query via JDBC: DROP VIEW _dd_variables_has_spouse_view\n21:24:52 [DataLoader(akka://deepdive)] ERROR com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure\nThe last packet successfully received from the server was 292,937 milliseconds ago.  The last packet sent successfully to the server was 13 milliseconds ago.\n21:24:52 [profiler] DEBUG ending report_id=inference_grounding\n21:24:52 [taskManager] INFO  Completed task_id=inference_grounding with Failure(java.sql.SQLException: Already closed.)\n21:24:52 [taskManager] ERROR task=inference_grounding Failed: java.sql.SQLException: Already closed.\n21:24:52 [taskManager] ERROR Forcing shutdown\n21:24:52 [taskManager] ERROR Cancelling task=calibration\n21:24:52 [taskManager] ERROR Cancelling task=inferencetest/test_mysql.sh: line 60: 17791 Killed                  sbt \"test-only org.deepdive.test.integration.MysqlSpouseExample -- -oF\"\nIt happens when executing line 589 of SQLInferenceDataStore.java . \nIt looks almost like a connection timeout to me...any idea?\nYou can see the details of the Travis build here: https://travis-ci.org/HazyResearch/deepdive/jobs/43391021\n. I'll add it, let's see what happens.\n. It seems crazy though, the view definitively exists: we create it!\n. @zifeishan We need your MySQL expertise here. @zhangce found this: https://stackoverflow.com/questions/2983248/com-mysql-jdbc-exceptions-jdbc4-communicationsexception-communications-link-fai\nCan you please give it a look? I wonder whether it is some problem in the way we create the SQL connection with MySQL ?\n. @zifeishan well done, Zifei. Is there a way we can create the index only in the MySQL case? \n. We don't want to do it for psql and gp because this index is only used once, so we don't want to spend the time creating it. \nCan you please change the code so that it is only created for MySQL? Thanks\n. Yes sorry, I didn't know you commited the change.\n. @feiranwang : I'll merge but we will need documentation for the new option before the release. Thanks!\n. This looks good (and very useful), but please document the table name (and schema) in the apposite document. \n. Ok, @zifeishan , merged.\n. @zifeishan please don't just comment out code (especially without comments). Remove it if it's no longer needed.\n. @zifeishan , if you have time can you please look at this?\nThank you.\n. ",
    "zifeishan": "Last time I made a typo in \"calibration.holdout_query\" and the system just did not holdout anything... We need this sanity check!\n. Recently I found OCR might need this, e.g. ngram frequency is a continuous value. @zhangce \n. AMI is running properly now. This shouldn't be an issue. Better to check with @feiranwang \n. I think Ce @zhangce created the AMI?\nAlternatively, we can add into the AMI guide to let users install JDK7 manually.\n. I think in general the error message is informative of what's happening. Can anyone give some examples where error message doesn't make sense? \nI guess there are some improvements:\n1. sanity check on config, \n2. reduce debug output and make info-level output as default (?),\n3. try to rewrite some error message\n. People can now do it. Example: \n```\next_sentence_author {\n      style: \"sql_extractor\"\n      dependencies: [\"ext_sentences_nlp\"]\n      sql: \"\"\"\n        DROP TABLE IF EXISTS sentence_author CASCADE;\n    CREATE TABLE sentence_author (\n        id bigserial primary key,\n        sentence_id bigint,\n        publication_id bigint,\n        author_id bigint,\n        is_true boolean\n      ) ;\n\n    INSERT INTO sentence_author (sentence_id, publication_id, \n            author_id, is_true)\n    SELECT  sentences.id AS sentence_id, \n            document_id AS publication_id, \n            authors.author_id,\n            null::boolean is_true\n      FROM  sentences, authors;\n  \"\"\"\n}\n\n```\nbefore / after script is still acceptable in these extractors.\n. Not sure.. what exactly is the issue?\n. It fixes non-termination errors in case of: \n- parsing config file\n- invalid inference rule\nI am not sure where this no-database-connection error may occur. In fact, I realized that this fix does not guarantee fixing all non-termination errors, since we has multiple Akka actors (InferenceManager, ExtractionManager, etc), and simply catching errors in DeepDive class may not handle all cases. But it do fix a portion of them.\n. This is simply awesome!!! Love it!\nOn Apr 13, 2014, at 7:12 PM, Denny Britz notifications@github.com wrote:\n\nWe don't have an example application for that right now, but there is documentation at http://deepdive.stanford.edu/doc/calibration.html\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nZifei Shan\nM.S. student in Computer Science, Stanford University (2015)\nB.S. in Computer Science, Peking University (2013)\n. Ah, I was using an older version of application.conf.\nIt was solved by naming the variables as people1.smokes and people2.has_cancer. It is kind of ugly though.\n. However it would be nice if we support things like this, rather than playing hacks with variable names... This is not so elegant.\n. Works when the query does not return results.\n. Done in 811e351d27002d414ef9c90cab6edec487406cbc.\n. The issue seems to be not reproducible.\n. Same as after script.\nThis would be so useful, since most before/after scripts are actually running one/multiple SQL queries.\n. It's a pretty old issue, seems not critical but still somewhat useful.\nWalkarounds can be having scripts to execute it (datastore-specific, a bit wordy), or using a sql_extractor (a bit wordy, hard to manage pipeline).\nNot sure if we want to support in this version. Does anyone find it an urging need? We may have much easier \"mindbender\" syntax in the future.\n. Just a note: EXPLAIN can do at least SELECT / INSERT / UPDATE queries. Cannot do CREATE TABLE, etc.\n. Can we print both QUERY and QUERY PLAN, instead of query plan itself? @SenWu \n. Done in bbf936358c2192ad099b8f776f2fc92a4e0f45e4.\n. Done. In current systems if no factors are active, all steps after extraction will be skipped.\n. Is this done?\n. Already fixed.\n. We need to change (4) and document others.\n. - Documentation \n  - ID convention: force developers create \"id bigint\" column for variable tables, but not to use the column\n- Known issues in code\n  - default extractors (udf_extractor) still assigns \"id\" to output JSON.\n. Done.\n. @netj @SenWu @msushkov The HTML is updated. It would be great if you can look through it and see whether it makes sense!\n. @feiranwang mentioned that skip_learning and weight_table has been disabled in the newest grounding. Will remove them soon.\n. @feiranwang Updates on this? Can we have these configurations again?\n. Thanks! Have you tested them with the whole system?\nOn May 4, 2014, at 4:54 PM, Feiran Wang notifications@github.com wrote:\n\nskip_learning and weight table are supported in grounding.\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nZifei Shan\nM.S. student in Computer Science, Stanford University (2015)\nB.S. in Computer Science, Peking University (2013)\n. Good point. Actually -l 0 has already been set in SettingsParser. I will remove isfixed=true.\nI see skiplearning /weighttable code is commented out right now. I'll add them back.\n. Done in d65b5567c8d39914a65c90eed373a000e40f6e33\n. Leave to next release.\n. I think grounding pipeline used gpfdist (unload) rather than gpload.\nOn May 9, 2014, at 1:57 PM, Mikhail Sushkov notifications@github.com wrote:\n\nClarification: this code is not in the grounding pipeline yet.\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nZifei Shan\nM.S. student in Computer Science, Stanford University (2015)\nB.S. in Computer Science, Peking University (2013)\n. Are you sure the example works? OCR example cannot pass on my side.\nThe issue is that there are TWO variables in ONE table.\nAssigining ID will only give ONE id to each ROW, but there are two variables in this row.\nHow are we dealing with this? I guess one solution is to restrict that every table can only contain one variable,  update the example and docs...\n```\n02:35:40 [] ERROR SQL execution failed (Reason: ERROR: duplicate key value violates unique constraint \"label2_pkey\"\n  Detail: Key (id)=(620) already exists.):\nUPDATE label2 SET id = nextval('id_sequence')\n02:35:40 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] ERROR org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"label2_pkey\"\n  Detail: Key (id)=(620) already exists.\n02:35:40 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  [Error] Please check the SQL cmd!\n02:35:40 [inferenceManager] ERROR ERROR: duplicate key value violates unique constraint \"label2_pkey\"\n  Detail: Key (id)=(620) already exists.\norg.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"label2_pkey\"\n  Detail: Key (id)=(620) already exists.\n```\n. Awesome! Would you fix it?\n. - spouse_example contains 3 implementations with different extractor frameworks.\n- Try to implement all possible extractors in the according framework.\n- (Optional) Write an extractor walkthrough with different versions of spouse example\n. Done in 5a6d6518ba1ac64bb395ab052213a5b8d1b21e2d. Need to fix tests.\n. @SenWu Can you pull the changes and test the nlp_extractor? Maybe test it with spouse example. Thanks!\n. Tested and updated in 6f84113f79fb91eef93d3db720a5e2d3b7e7eedd\n. Also make a README for Smoke example that explains the application.\ne.g. Calibration plot is almost empty. What does it do? \n. Verified by Feiran.\n. Done in 20408ac00e56f9eae45478494c255fab58b32c62. Needs more testing for ID assignment.\n. @netj Thanks Jaeho! I totally agreed that they should be fixed :)\nWe are working on the example code now.. Will fix documentation later.\nIf you want you are welcome to fix them in ocr / smoke example!\n. @netj Jaeho:\nIt would be great if you can modify the examples on the website to catch this case.\nJust pull and update .md files in gh-pages branch. Send me a note if you're done; I will deploy them to the website after you finished.\nThanks!\n. nice catch! Fixed.\n. @netj  Sorry I also updated the spouse example in walkthrough quite a bit, since our syntax changed. Hope it doesn't affect your modification!\n. Is this documented right now?\n. @feiranwang Just found there was an issue for this. Sorry you ran into this.\n. It has been fixed in the recent push 32425afb2fa69fc98bebb03a387efd37bb9a2fb7.\nThe problem was because forgetting stripping a line end.\nI also refactored the TSV extractors to be coherent with default extractors, and integrated the updated features in the walkthrough.\n```\nselect count(*) from has_spouse;\ncount\n75422\n(1 row)\nselect count(*) from people_mentions;\ncount\n39266\n(1 row)\nselect count(*) from has_spouse_features;\ncount\n1160450\n(1 row)\n```\n. Done in 666b5fa0779f0a6885aee4231606a46338e0051d\n. Done in cb121a12f9f8b46966464ebf1937b16c628b0046.\n. Done in 8fbaae9d7a55b597f9bc05cd5beec799181b8476.\n. Developers should not need to specify sampler_cmd any more.\n. Is it in logback.xml? I did so, but not seem to work...\nOn May 3, 2014, at 10:15 AM, Feiran Wang notifications@github.com wrote:\n\nset log level to INFO, will do before release\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nZifei Shan\nM.S. student in Computer Science, Stanford University (2015)\nB.S. in Computer Science, Peking University (2013)\n. Thanks!!\n. Thanks! I like your beautiful format! :+1: \n. Wow, that's a lot of proofreading work! Thanks so much Jaeho!! \n. Done in new documentation 2beb3cf.\n. Done\n. @feiranwang Do you have time to write some tests for these?\n. Need enhancement for all units; need integration of examples. Leave to next release.\n. Done.\n. Fixed. Is it tested?\n. @feiranwang What do results you get look like?\n. I think this might be pretty urgent. @feiranwang Let's do the following:\n1. check with other examples whether results are correct. \n   If yes, maybe the current way we right spouse_example is wrong (either schema or feature extraction)\n   If no (all examples cannot get correct results), maybe some variable / weight id mapping in grounding is wrong.\n2. compare spouse_example between previous ones. Compare features of a certain sentence, labels of a certain relation, etc.\nLet's fix this!\n. Fixed in bbe0cf5. Thanks!\n. Current plot for spouse_example:\n\n. Works now!\n. This bug is actually a more general non-termination bug when executing SQL queries by a file. Fixed in https://github.com/HazyResearch/deepdive/pull/251.\n. Creating view for group by works! Rewriting plpy_extractor compilers. Syntax will change.\n. Done in 111c5c622576194da0affb139e01320f4da3054f. Documentation updated in c68eea8ae1e9d52a84f4c36c5191b8a7592ad8bf.\n. [?] Get rid of (\"ps aux\" #> \"grep deepdive\" #> Seq(\"awk\", \"{print $2}\") #> \"xargs -L 1 kill -9\").!!\nCan we just remove this? Too ugly..\n. Removed.\n. Will be done after this release.\n. Fixed in 5e39c0242e1b68b3b4ffa7c5ebc0f117c7bd94de.\n. Where do we unzip using script now?\n. It's only for travis testing system to add dependencies to finish tests.\n. Done in 8088a8fa16403bd3bfba15c86663fbe8826b2ae6 and bdad6b130a9c740f3d56216a8651953889948cce.\n. Why?\n. Leave to next release.\n. Hi Feiran,\nAmir's problem is fixed. It's not a bug of extractor.\nWhat is your problem? Where can I see your code?\nOn May 6, 2014, at 2:39 PM, Feiran Wang notifications@github.com wrote:\n\nAmir and I got the same error message\nWhen running spouse walkthrough (Amir)\nKeyError: 'sentence_id'\nWhen testing Logistic Regression (Feiran)\nKeyError: 'id'\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nZifei Shan\nM.S. student in Computer Science, Stanford University (2015)\nB.S. in Computer Science, Peking University (2013)\n. Can you show me your changed code in current syntax? Thanks!\nOn May 6, 2014, at 5:04 PM, Feiran Wang notifications@github.com wrote:\n\nI thought it was due to the extractor... what is Amir's problem?\nMy problem happens in deepdive/src/test/scala/integration/LogisticRegression.scala. I changed it to the current syntax.\nUncomment it and run the tests, I get keyError: 'id'...\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nZifei Shan\nM.S. student in Computer Science, Stanford University (2015)\nB.S. in Computer Science, Peking University (2013)\n. Developers must not use id in extractors now, which is coherent with our current requirements.\n. This is a great point. Thanks Jaeho!\nWe would like users to install the libraries and set environmental variables.\nIf you follow the installation guide (http://deepdive.stanford.edu/doc/installation.html), it will first ask you to install DimmWitted (http://deepdive.stanford.edu/doc/sampler.html), where specific instructions are included.\nI think it would be nice if we use Makefile to do this library installation, do sbt compile, run the tests (optionally?), and let user set environmental variables. Can you make a script like that and test it on several machines (Mac, Linux)?\nThanks,\nZifei\nOn May 7, 2014, at 5:12 PM, Jaeho Shin notifications@github.com wrote:\n\nI was going through the walkthrough again with the latest master/develop branch, and found ./run.sh hanging like below:\n15:51:03 [sampler] INFO  Executing: util/sampler-dw-mac gibbs -w /Users/netj/Projects/2014/deepdive/out/2014-05-07T155033/graph.weights -v /Users/netj/Projects/2014/deepdive/out/2014-05-07T155033/graph.variables -f /Users/netj/Projects/2014/deepdive/out/2014-05-07T155033/graph.factors -e /Users/netj/Projects/2014/deepdive/out/2014-05-07T155033/graph.edges -m /Users/netj/Projects/2014/deepdive/out/2014-05-07T155033/graph.meta.csv -o /Users/netj/Projects/2014/deepdive/out/2014-05-07T155033 -l 1000 -s 1 -i 1000 --alpha 0.1 --diminish 0.99\ndyld: Library not loaded: @executable_path/../lib/dw_mac/lib/protobuf/lib/libprotobuf.8.dylib\n  Referenced from: /Users/netj/Projects/2014/deepdive/util/sampler-dw-mac\n  Reason: image not found\nIt turns out I skipped the ./test.sh and the necessary shared libraries weren't correctly installed/extracted at the right places.\nI'm puzzled why such an important build/install step is not included in the sbt build config, but hanging around in a test script, commented as only for travis environment. I'd love to fix this but I have no clue how sbt works, even where the build is declared or the entry point is. Can't we use a simple Makefile that calls sbt compile instead if more tasks are handled by shell scripts?\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nZifei Shan\nM.S. student in Computer Science, Stanford University (2015)\nB.S. in Computer Science, Peking University (2013)\n. Yeah, I think there are some great design suggestions there. Let's talk more about it later.\nFor the other comment, I agree that using Makefile to install is orthogonal to using run.sh to launch the app. So it should be fine to just update the installation part for now.\nI will double-check the pull request and merge it later :) Thanks Jaeho!\n. @zhangce We've find some way to do this! Let's put it into next release.\n. @netj  Thanks!  Ce have found something called sbt-pack, which compiles and packs the Scala project into a single executable. Instead of running sbt, developers can run this executable (we can name it as deepdive). Not so many changes will be introduced (but indeed a few regarding env vars & current dir). Not sure which is better, we should discuss!\n. Done for now. I guess more improvements will come with mindbender...\n. I've updated the docs. Now you can do it! @netj \n(Feiran and Ce will go through another pass, so it may be better to either finish by tonight, or wait until tomorrow :)  )\n. Thanks for your awesome work @netj !!\n. Thanks!\n. @netj Yes it is here: https://github.com/HazyResearch/deepdive/blob/master/examples/spouse_example/data/non-spouses.tsv\nAny problem?\n. @netj Ah, good point! It's a problem of plpy_extractor: since the function is running inside the database, the file path must be able to be accessed by the database servers. On the testing machine I didn't figure out a way for database server (on rocky) to access /afs without permission issues... Any advice?\n. @feiranwang Thanks for catching! Will fix recently.\n. @feiranwang Not sure, still need to check... We are recently proposing to refactor json_extractor with the help of @netj , specifically to get the json <-> csv/tsv done in another language. Probably it is also a good time to keep an eye on this issue.\n. I am fixing it right now. It is originally designed to eliminate null values in extractor input, and our tests also enforce that. Do we have specific concerns for doing that?\n. Fixed in https://github.com/HazyResearch/deepdive/pull/163.\n. Other issues include:\n- \"run\" cannot be in multiple lines\n. - must trim all the comments before parsing. If developers have # def run (..) it will breaks.\n. We originally think that it will be much faster since it eliminate IOs from and to the database. It is indeed faster in some IO-bound extractors on small to medium databases. \nThe PLPY language itself seems to have problems operating on large GP clusters, but I am not sure. @zhangce ?\nNow it seems nobody is using plpy extractor so I did not do more bug-fixing on it. Should we deprecate plpy extractor? \n. Not sure I fully get it.. =) Do we want to keep the functionality, but move it from our main extractor documentation to some extra page?\n. Fixed in PR https://github.com/HazyResearch/deepdive/pull/251.\n. We are already doing this right now. The permissions are eventually set in Helpers.executeCmd.\n. @netj I just found that jekyll --watch can automatically detect changes. We can add it into the makefile (in host option).\n. So awesome!!!\n. @feiranwang  I use print >>sys.stderr, message and it works, like below:\n20:00:47 [extractorRunner-ext_sup_orderaware] INFO  SUPERVISION DATA NOT EXISTS: /dfs/hulk/0/zifei/ocr/supervision_escaped//JOURNAL_57898.seq\nAre you sure you have set your logback.xml properly? \nIf you are using deepdive command, try make build again in deepdive root folder, in develop branch.\n. I am also having this problem now. When is the earliest timepoint you have this problem? Thanks!\nOn Jul 1, 2014, at 12:21 AM, Feiran Wang notifications@github.com wrote:\n\nThe problem indeed exists.\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nZifei Shan\nM.S. student in Computer Science, Stanford University (2015)\n. Solved in 8cd72356c66a44ba9e7b76ed8dc68ca58fce2167.\n. @netj Thanks!\n. Not right now. I will fix this in develop branch at some point.\n. Fixed in d328604c947d4c3b1decbb46adc39466e4599402.\n. @feiranwang Can you post your detailed logback.xml? When I try the following it works.\n```\n\n\n\n\n\n${simplerPattern}\n\n\nlog/${bySecond}.txt\n${defaultPattern}\n\n\n\n\n\n\n\n```\nNote that \"root\" block needs \"debug\" level. I currently cannot find a way to separate STDOUT and FILE logging levels.\n. I have changed UDF output to INFO level in 8cd72356c66a44ba9e7b76ed8dc68ca58fce2167. Please pull develop branch.\n. @dennybritz Denny, do you have any ideas regarding this?\n. I submitted a pull request https://github.com/HazyResearch/deepdive/pull/162\nthat addresses this issue. I renamed it to dd_inference_result_weights_mapping and kept the original view for backward compatibility.\n. Fixed in #162 .\n. @rionda Please check this. Thanks!\n. @rionda @feiranwang @amirabs  Need some review for this! Thanks!\n. See branch zifei-scaladoc.\nSee http://web.stanford.edu/~zifei/deepdive/doc/api/#package for a sample of deployed scaladoc.\nIt is largely unfinished. I guess we won't include it in this release?\n. @feiranwang Please review this piece of code and let's iterate on it. Thanks!\n. @rionda @feiranwang Would you finalize the review of this? I know that this many changes are hard to review, but I want to aim at this weekend that all the code here is tested. Thanks!\n. Fixed by @feiranwang in f2f726080d20cc43ce70b1de177ee6808d08a8b6.\n. Ship it!\n. I discussed with Sen and it seems possible to install GP on Travis and test it. I (or maybe Sen) can look into that later, after mysql port is finished.\n. Solved in #153 \n. @rionda Matteo, would you review this hot fix?\n. @rionda What do you mean by false positive? It seems like an error that Travis occasionally has...\n. @rionda It was just that I was coding up the genomics example and find this issue. Maybe it is a good idea to put it in develop for a while since nobody other than me actually uses this feature now.\n. @larryxiao  @netj \nJust FYI, In one of my recent pull requests to develop (https://github.com/HazyResearch/deepdive/pull/153), I introduced Context.deepdiveHome in the code. It is assigned as the value of environment variable $DEEPDIVE_HOME if set, or current directory if not set. Scripts like util/tobinary.py are then located by Context.deepdiveHome/util/tobinary.py.\nFeel free to change it If you have a better way :) But just be aware of the current solution.\n. Thanks! =)  I am not sure if we want to introduce a environment variable like this, though... @netj what do you think?\n. @netj Sorry for the confusion, but it is NOT that all the paths in application.conf will be resolved as if the base path were in $DEEPDIVE_HOME, BUT only some internal functions (e.g. util/tobinary.py, util/sampler-dw-linux, which are not exposed to users) in the Scala code will be resolved using $DEEPDIVE_HOME. \nThe base path in application.conf is still current working dir, i.e. whatever developer runs deepdive. If deepdive is run on application root folder (say APP_HOME), then you can refer to udf/ext_people.py in in current difectory, as an example.\nMany of our current examples do set different environment variables $DEEPDIVE_HOME and $APP_HOME and refer to them in application.conf, and start the application in $DEEPDIVE_HOME. There are many alternative ways to do it though.\n. ndbloader integration done. Ready to merge after review.\n. Yes, travis is already testing both psql and mysql.\n. Passed tests on GP.\n. @rionda Yes it is for --quiet.\nThe linux library is already updated in a previous commit by Feiran: 7cca8be8a7956209f294425ed4c0ede171ca4a34\n. I realized that this non-termination fix does not guarantee fixing all non-termination errors, since we has multiple Akka actors (InferenceManager, ExtractionManager, etc), and simply catching errors in DeepDive class may not handle all cases.\nThis PR fixes non-termination when:\n- invalid configuration file (mandatory fields are not present)\n- invalid factor input_query\n- sampler throws error\n. @rionda I am done. It can be merged after passing Travis test. Thanks!\n. Just confirmed and mysql also supports ALTER TABLE DROP COLUMN IF EXISTS ... syntax. The thing it does not support was dropping indexes if exists.\nFixed in https://github.com/HazyResearch/deepdive/pull/176. Waiting for tests to finish.\n. Tested on GP and fails. There is actually a problem on GP that it does not support DROP COLUMN IF EXISTS syntax... Will try to find a workaround.\n. Yes. It passed the tests on my local MariaDB, though. Looks like it's a corner case query that some older systems do not implement. \nI'm adding a try-catch block with simple DROP COLUMN index. Does that sound OK?\n. It could be hard since the error messages and exception types are different across different DBMS.\nBTW, Feiran is working on a new implementation without changing the variable table, and it will be out today. With that we shouldn't have this issue any more. @feiranwang  \nSo we can probably discard this PR.\n. I tested locally and the problem was because a join query in inference takes too long. MySQL uses Blocked Nested Loop Join, so we have to create indexes to speed up its joins.\nI created an index on id column on the temporary variable type table and it seems to work fine. Let's wait to see the results.\n. @rionda Fixed now.\n. Yes, a good way to do it would be putting a functional block of code containing the index creation in a function, and overriding that function in MysqlInferenceDatastore.scala. However I am not sure why we don't want to create the index in Psql. IMHO, indexes basically make everything faster if we use them properly (as long as we don't modify the type table, which I think is true).\n. @rionda Do we still want to merge this fix? \n. Note that this is a PR to develop branch. This feature can be released later.\n@rionda @zhangce @feiranwang Would anyone review my code here? It's mostly changes to grounding. Thanks!\n. @rionda Docs are updated. Thanks!\n. @rionda Sorry. These code to extract coref may be added into the nlp extractor in the future. Added a bit documentation to the commented code.\n. Looks good to me. @rionda \n. @chrismre I think it is not fixed yet.  I will run a test on an example application to make sure if the problem actually exists, and send a report to Ce.\n. Looks good to me other than the comments. Have you tested on a GP machine? @feiranwang \n. @zhangce @feiranwang  The env var was no longer used. Thanks!\n. @chrismre The code itself works, but Sen proposed some changes to the configuration format that I haven't got time to add. \nI think we can first merge the current request, and I will update the configurations for parallel loading / grounding later. @SenWu do you think so?\n. @chrismre The current pull request is backward compatible. It just adds a configuration option extraction.parallel_loading. I agree that we should discuss before changing the format of application.conf, but the current pull request should be safe in terms of backward compatibility.\nSen was suggesting that we remove the inference.parallel_grounding and extraction.parallel_loading options, and I think this might need more discussion after merging this pull request.\nI am confused on what to do with the current pull request. Although it does not break the format, it does add a new configuration option. Should we avoid that?\n. @chrismre Sorry for the delay.\nFor 1, the optional configuration handeling, I will leave it as it is, as it's a part of a larger refactoring.\nFor 2, regarding the python in-database extractors, we had a discussion in https://github.com/HazyResearch/deepdive/issues/104. There are some known unfixed bugs in plpy_extractor, and right now nobody is actively using this extractor type, so we once postponed the maintenance of that extractor type. I still agree that plpy_extractor is I/O-optimized, while tsv_extractor is computationally optimized, and there are cases that using plpy_extractor is faster. I will remove the deprecation in the documentation, and fix the bugs later.\nFor 3, sorry that this goes into the pull request. I will remove it.\n. @mikecafarella I have done the changes mentioned in the last message. (For 2, I will not fix the plpy bugs for this pull request.) Thanks!\n. @SenWu Thanks! I am using US patents. They are similar to research papers.\nI saw that in the Genomics application, we are replacing the \"O\"s with lemmas. Is that a common improvement?\n. @netj Fixed the duplicated lines.\nThe tests are now wired up with the main test.sh from DeepDive.\n. @netj We have many places in our system that tries to create tables by appending strings to existing tables, e.g.:\nhttps://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/datastore/Dataloader.scala#L48\nhttps://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala#L893\nI think our system is implicitly supposing users to use not-too-long table names (<30 characters should be fine, we haven't tested its limit there). However in this bug, the system is stricter since it used  \"${outputRel}_unload_${funcName}\" and funcName is \"func_${task.extractor.name}\" so a 20-char table (e.g. relation_mention_features) will easily break it. After the fix it is coherent with other places.\nI agree that eventually the system should not pose any implicit restrictions to the length of table names, or we have to mention what the restrictions are.\n. @chrismre As I understand, this happens a long time ago, when we decided to do grounding by assigning an id column to each row in the variable table. Sampler would require different variables to have different ids, but it is not possible under the current grounding framework since each table that contain variables only have one id column.\n. I will fix the docs tonight.\n. Merged and deployed.\n. @zhangce Should we merge and deploy this?\n. @zhangce Do we want to link this banner to somewhere? Currently it links to nowhere.\n. @netj Could you review these changes? Thanks!\n. Merged and deployed.\n. It turns out I am using a version without XL integration.\n. Starting from release v0.6.0, the speed is acceptable.\n. Got it. Thanks!\n. I was running an old application, which does not run via deepdive run. Thanks!\n. @netj Should we merge this? I've addressed the comments by setting the number of splits to parallelism. Thanks!\n. Agreed that this is very useful in many applications.\nOn Sun, Sep 27, 2015 at 2:33 AM, zhangce notifications@github.com wrote:\n\n@thodrek https://github.com/thodrek 's new framework should provide an\nultimate cleaner solution here. For short term, it should also be an easy\nfix.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/416#issuecomment-143507063\n.\n\n\nZifei Shan\nM.S. in Computer Science, Stanford University (2015)\n. Hi @netj , I notice that the deepdive sql '[multiple SQL queries]' does not work under postgres-XL. It's probably necessary to manually split these queries into individual commands when compiling sql_extractor to bash scripts.\nHere is how we previously split SQL queries by semicolons: https://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/datastore/JdbcDataStore.scala#L87\n. @netj \nThis seems to be a bigger issue than sql_extractor for PGXL: I am trying out some other extractors but it had other problems with this line: https://github.com/HazyResearch/deepdive/blob/dataflow-compiler/runner/compute-driver/local/compute-execute#L123 since it's also calling multiple queries with deepdive sql. I guess a cleaner workaround would be splitting queries for  https://github.com/HazyResearch/deepdive/blob/dataflow-compiler/database/db-driver/postgresql-xl/db-execute?\nRegarding the PGXL issue, I could not find discussions for it online. A sample query that would fail on my environment:\n$ psql $DBNAME -c  \"create table tmptmp(id int);\ninsert into tmptmp values(1);\nselect * from tmptmp;\ndrop table tmptmp;\"\nERROR:  Unexpected response from the Datanodes for 'T' message, current request type 1\nERROR:  Unexpected response from the Datanodes for 'T' message, current request type 1\nERROR:  Unexpected response from the data nodes for 'D' message, current request type 0\nServer version: Postgres-XL 9.2.0.\n. @netj I just found an alternative and it works: instead of \ndeepdive sql  \"create table tmptmp(id int);\ninsert into tmptmp values(1);\nselect * from tmptmp;\ndrop table tmptmp;\"\n, if I run:\necho  \"create table tmptmp(id int);\ninsert into tmptmp values(1);\nselect * from tmptmp;\ndrop table tmptmp;\" | deepdive sql\nit will work for PGXL. I think if we change db-execute of PGXL, to pipe the SQL query into a deepdive sql command, it would work out nicely.\n. @netj Yes. Try psql -f commands.sql. It works with pgxl as well.\n. @netj Regarding 0.8 vs 0.7.2, will 0.7.2 include the grounding improvements that's currently in master branch? That is valuable since it doubles the grounding performance in our setting.\n. @alldefector  Great findings. I ran the pipeline end-to-end 10 times with #570 and confirmed that the problem went away.\n. NVM, it turned out that I forgot to initialize the submodules. (have to run git submodule update --init before installation)\n. Breaking spouse example grounding, discarding. In the future may want to make this dynamic / configurable.\n. Last time I made a typo in \"calibration.holdout_query\" and the system just did not holdout anything... We need this sanity check!\n. Recently I found OCR might need this, e.g. ngram frequency is a continuous value. @zhangce \n. AMI is running properly now. This shouldn't be an issue. Better to check with @feiranwang \n. I think Ce @zhangce created the AMI?\nAlternatively, we can add into the AMI guide to let users install JDK7 manually.\n. I think in general the error message is informative of what's happening. Can anyone give some examples where error message doesn't make sense? \nI guess there are some improvements:\n1. sanity check on config, \n2. reduce debug output and make info-level output as default (?),\n3. try to rewrite some error message\n. People can now do it. Example: \n```\next_sentence_author {\n      style: \"sql_extractor\"\n      dependencies: [\"ext_sentences_nlp\"]\n      sql: \"\"\"\n        DROP TABLE IF EXISTS sentence_author CASCADE;\n    CREATE TABLE sentence_author (\n        id bigserial primary key,\n        sentence_id bigint,\n        publication_id bigint,\n        author_id bigint,\n        is_true boolean\n      ) ;\n\n    INSERT INTO sentence_author (sentence_id, publication_id, \n            author_id, is_true)\n    SELECT  sentences.id AS sentence_id, \n            document_id AS publication_id, \n            authors.author_id,\n            null::boolean is_true\n      FROM  sentences, authors;\n  \"\"\"\n}\n\n```\nbefore / after script is still acceptable in these extractors.\n. Not sure.. what exactly is the issue?\n. It fixes non-termination errors in case of: \n- parsing config file\n- invalid inference rule\nI am not sure where this no-database-connection error may occur. In fact, I realized that this fix does not guarantee fixing all non-termination errors, since we has multiple Akka actors (InferenceManager, ExtractionManager, etc), and simply catching errors in DeepDive class may not handle all cases. But it do fix a portion of them.\n. This is simply awesome!!! Love it!\nOn Apr 13, 2014, at 7:12 PM, Denny Britz notifications@github.com wrote:\n\nWe don't have an example application for that right now, but there is documentation at http://deepdive.stanford.edu/doc/calibration.html\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nZifei Shan\nM.S. student in Computer Science, Stanford University (2015)\nB.S. in Computer Science, Peking University (2013)\n. Ah, I was using an older version of application.conf.\nIt was solved by naming the variables as people1.smokes and people2.has_cancer. It is kind of ugly though.\n. However it would be nice if we support things like this, rather than playing hacks with variable names... This is not so elegant.\n. Works when the query does not return results.\n. Done in 811e351d27002d414ef9c90cab6edec487406cbc.\n. The issue seems to be not reproducible.\n. Same as after script.\nThis would be so useful, since most before/after scripts are actually running one/multiple SQL queries.\n. It's a pretty old issue, seems not critical but still somewhat useful.\nWalkarounds can be having scripts to execute it (datastore-specific, a bit wordy), or using a sql_extractor (a bit wordy, hard to manage pipeline).\nNot sure if we want to support in this version. Does anyone find it an urging need? We may have much easier \"mindbender\" syntax in the future.\n. Just a note: EXPLAIN can do at least SELECT / INSERT / UPDATE queries. Cannot do CREATE TABLE, etc.\n. Can we print both QUERY and QUERY PLAN, instead of query plan itself? @SenWu \n. Done in bbf936358c2192ad099b8f776f2fc92a4e0f45e4.\n. Done. In current systems if no factors are active, all steps after extraction will be skipped.\n. Is this done?\n. Already fixed.\n. We need to change (4) and document others.\n. - Documentation \n  - ID convention: force developers create \"id bigint\" column for variable tables, but not to use the column\n- Known issues in code\n  - default extractors (udf_extractor) still assigns \"id\" to output JSON.\n. Done.\n. @netj @SenWu @msushkov The HTML is updated. It would be great if you can look through it and see whether it makes sense!\n. @feiranwang mentioned that skip_learning and weight_table has been disabled in the newest grounding. Will remove them soon.\n. @feiranwang Updates on this? Can we have these configurations again?\n. Thanks! Have you tested them with the whole system?\nOn May 4, 2014, at 4:54 PM, Feiran Wang notifications@github.com wrote:\n\nskip_learning and weight table are supported in grounding.\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nZifei Shan\nM.S. student in Computer Science, Stanford University (2015)\nB.S. in Computer Science, Peking University (2013)\n. Good point. Actually -l 0 has already been set in SettingsParser. I will remove isfixed=true.\nI see skiplearning /weighttable code is commented out right now. I'll add them back.\n. Done in d65b5567c8d39914a65c90eed373a000e40f6e33\n. Leave to next release.\n. I think grounding pipeline used gpfdist (unload) rather than gpload.\nOn May 9, 2014, at 1:57 PM, Mikhail Sushkov notifications@github.com wrote:\n\nClarification: this code is not in the grounding pipeline yet.\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nZifei Shan\nM.S. student in Computer Science, Stanford University (2015)\nB.S. in Computer Science, Peking University (2013)\n. Are you sure the example works? OCR example cannot pass on my side.\nThe issue is that there are TWO variables in ONE table.\nAssigining ID will only give ONE id to each ROW, but there are two variables in this row.\nHow are we dealing with this? I guess one solution is to restrict that every table can only contain one variable,  update the example and docs...\n```\n02:35:40 [] ERROR SQL execution failed (Reason: ERROR: duplicate key value violates unique constraint \"label2_pkey\"\n  Detail: Key (id)=(620) already exists.):\nUPDATE label2 SET id = nextval('id_sequence')\n02:35:40 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] ERROR org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"label2_pkey\"\n  Detail: Key (id)=(620) already exists.\n02:35:40 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  [Error] Please check the SQL cmd!\n02:35:40 [inferenceManager] ERROR ERROR: duplicate key value violates unique constraint \"label2_pkey\"\n  Detail: Key (id)=(620) already exists.\norg.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"label2_pkey\"\n  Detail: Key (id)=(620) already exists.\n```\n. Awesome! Would you fix it?\n. - spouse_example contains 3 implementations with different extractor frameworks.\n- Try to implement all possible extractors in the according framework.\n- (Optional) Write an extractor walkthrough with different versions of spouse example\n. Done in 5a6d6518ba1ac64bb395ab052213a5b8d1b21e2d. Need to fix tests.\n. @SenWu Can you pull the changes and test the nlp_extractor? Maybe test it with spouse example. Thanks!\n. Tested and updated in 6f84113f79fb91eef93d3db720a5e2d3b7e7eedd\n. Also make a README for Smoke example that explains the application.\ne.g. Calibration plot is almost empty. What does it do? \n. Verified by Feiran.\n. Done in 20408ac00e56f9eae45478494c255fab58b32c62. Needs more testing for ID assignment.\n. @netj Thanks Jaeho! I totally agreed that they should be fixed :)\nWe are working on the example code now.. Will fix documentation later.\nIf you want you are welcome to fix them in ocr / smoke example!\n. @netj Jaeho:\nIt would be great if you can modify the examples on the website to catch this case.\nJust pull and update .md files in gh-pages branch. Send me a note if you're done; I will deploy them to the website after you finished.\nThanks!\n. nice catch! Fixed.\n. @netj  Sorry I also updated the spouse example in walkthrough quite a bit, since our syntax changed. Hope it doesn't affect your modification!\n. Is this documented right now?\n. @feiranwang Just found there was an issue for this. Sorry you ran into this.\n. It has been fixed in the recent push 32425afb2fa69fc98bebb03a387efd37bb9a2fb7.\nThe problem was because forgetting stripping a line end.\nI also refactored the TSV extractors to be coherent with default extractors, and integrated the updated features in the walkthrough.\n```\nselect count(*) from has_spouse;\ncount\n75422\n(1 row)\nselect count(*) from people_mentions;\ncount\n39266\n(1 row)\nselect count(*) from has_spouse_features;\ncount\n1160450\n(1 row)\n```\n. Done in 666b5fa0779f0a6885aee4231606a46338e0051d\n. Done in cb121a12f9f8b46966464ebf1937b16c628b0046.\n. Done in 8fbaae9d7a55b597f9bc05cd5beec799181b8476.\n. Developers should not need to specify sampler_cmd any more.\n. Is it in logback.xml? I did so, but not seem to work...\nOn May 3, 2014, at 10:15 AM, Feiran Wang notifications@github.com wrote:\n\nset log level to INFO, will do before release\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nZifei Shan\nM.S. student in Computer Science, Stanford University (2015)\nB.S. in Computer Science, Peking University (2013)\n. Thanks!!\n. Thanks! I like your beautiful format! :+1: \n. Wow, that's a lot of proofreading work! Thanks so much Jaeho!! \n. Done in new documentation 2beb3cf.\n. Done\n. @feiranwang Do you have time to write some tests for these?\n. Need enhancement for all units; need integration of examples. Leave to next release.\n. Done.\n. Fixed. Is it tested?\n. @feiranwang What do results you get look like?\n. I think this might be pretty urgent. @feiranwang Let's do the following:\n1. check with other examples whether results are correct. \n   If yes, maybe the current way we right spouse_example is wrong (either schema or feature extraction)\n   If no (all examples cannot get correct results), maybe some variable / weight id mapping in grounding is wrong.\n2. compare spouse_example between previous ones. Compare features of a certain sentence, labels of a certain relation, etc.\nLet's fix this!\n. Fixed in bbe0cf5. Thanks!\n. Current plot for spouse_example:\n\n. Works now!\n. This bug is actually a more general non-termination bug when executing SQL queries by a file. Fixed in https://github.com/HazyResearch/deepdive/pull/251.\n. Creating view for group by works! Rewriting plpy_extractor compilers. Syntax will change.\n. Done in 111c5c622576194da0affb139e01320f4da3054f. Documentation updated in c68eea8ae1e9d52a84f4c36c5191b8a7592ad8bf.\n. [?] Get rid of (\"ps aux\" #> \"grep deepdive\" #> Seq(\"awk\", \"{print $2}\") #> \"xargs -L 1 kill -9\").!!\nCan we just remove this? Too ugly..\n. Removed.\n. Will be done after this release.\n. Fixed in 5e39c0242e1b68b3b4ffa7c5ebc0f117c7bd94de.\n. Where do we unzip using script now?\n. It's only for travis testing system to add dependencies to finish tests.\n. Done in 8088a8fa16403bd3bfba15c86663fbe8826b2ae6 and bdad6b130a9c740f3d56216a8651953889948cce.\n. Why?\n. Leave to next release.\n. Hi Feiran,\nAmir's problem is fixed. It's not a bug of extractor.\nWhat is your problem? Where can I see your code?\nOn May 6, 2014, at 2:39 PM, Feiran Wang notifications@github.com wrote:\n\nAmir and I got the same error message\nWhen running spouse walkthrough (Amir)\nKeyError: 'sentence_id'\nWhen testing Logistic Regression (Feiran)\nKeyError: 'id'\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nZifei Shan\nM.S. student in Computer Science, Stanford University (2015)\nB.S. in Computer Science, Peking University (2013)\n. Can you show me your changed code in current syntax? Thanks!\nOn May 6, 2014, at 5:04 PM, Feiran Wang notifications@github.com wrote:\n\nI thought it was due to the extractor... what is Amir's problem?\nMy problem happens in deepdive/src/test/scala/integration/LogisticRegression.scala. I changed it to the current syntax.\nUncomment it and run the tests, I get keyError: 'id'...\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nZifei Shan\nM.S. student in Computer Science, Stanford University (2015)\nB.S. in Computer Science, Peking University (2013)\n. Developers must not use id in extractors now, which is coherent with our current requirements.\n. This is a great point. Thanks Jaeho!\nWe would like users to install the libraries and set environmental variables.\nIf you follow the installation guide (http://deepdive.stanford.edu/doc/installation.html), it will first ask you to install DimmWitted (http://deepdive.stanford.edu/doc/sampler.html), where specific instructions are included.\nI think it would be nice if we use Makefile to do this library installation, do sbt compile, run the tests (optionally?), and let user set environmental variables. Can you make a script like that and test it on several machines (Mac, Linux)?\nThanks,\nZifei\nOn May 7, 2014, at 5:12 PM, Jaeho Shin notifications@github.com wrote:\n\nI was going through the walkthrough again with the latest master/develop branch, and found ./run.sh hanging like below:\n15:51:03 [sampler] INFO  Executing: util/sampler-dw-mac gibbs -w /Users/netj/Projects/2014/deepdive/out/2014-05-07T155033/graph.weights -v /Users/netj/Projects/2014/deepdive/out/2014-05-07T155033/graph.variables -f /Users/netj/Projects/2014/deepdive/out/2014-05-07T155033/graph.factors -e /Users/netj/Projects/2014/deepdive/out/2014-05-07T155033/graph.edges -m /Users/netj/Projects/2014/deepdive/out/2014-05-07T155033/graph.meta.csv -o /Users/netj/Projects/2014/deepdive/out/2014-05-07T155033 -l 1000 -s 1 -i 1000 --alpha 0.1 --diminish 0.99\ndyld: Library not loaded: @executable_path/../lib/dw_mac/lib/protobuf/lib/libprotobuf.8.dylib\n  Referenced from: /Users/netj/Projects/2014/deepdive/util/sampler-dw-mac\n  Reason: image not found\nIt turns out I skipped the ./test.sh and the necessary shared libraries weren't correctly installed/extracted at the right places.\nI'm puzzled why such an important build/install step is not included in the sbt build config, but hanging around in a test script, commented as only for travis environment. I'd love to fix this but I have no clue how sbt works, even where the build is declared or the entry point is. Can't we use a simple Makefile that calls sbt compile instead if more tasks are handled by shell scripts?\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nZifei Shan\nM.S. student in Computer Science, Stanford University (2015)\nB.S. in Computer Science, Peking University (2013)\n. Yeah, I think there are some great design suggestions there. Let's talk more about it later.\nFor the other comment, I agree that using Makefile to install is orthogonal to using run.sh to launch the app. So it should be fine to just update the installation part for now.\nI will double-check the pull request and merge it later :) Thanks Jaeho!\n. @zhangce We've find some way to do this! Let's put it into next release.\n. @netj  Thanks!  Ce have found something called sbt-pack, which compiles and packs the Scala project into a single executable. Instead of running sbt, developers can run this executable (we can name it as deepdive). Not so many changes will be introduced (but indeed a few regarding env vars & current dir). Not sure which is better, we should discuss!\n. Done for now. I guess more improvements will come with mindbender...\n. I've updated the docs. Now you can do it! @netj \n(Feiran and Ce will go through another pass, so it may be better to either finish by tonight, or wait until tomorrow :)  )\n. Thanks for your awesome work @netj !!\n. Thanks!\n. @netj Yes it is here: https://github.com/HazyResearch/deepdive/blob/master/examples/spouse_example/data/non-spouses.tsv\nAny problem?\n. @netj Ah, good point! It's a problem of plpy_extractor: since the function is running inside the database, the file path must be able to be accessed by the database servers. On the testing machine I didn't figure out a way for database server (on rocky) to access /afs without permission issues... Any advice?\n. @feiranwang Thanks for catching! Will fix recently.\n. @feiranwang Not sure, still need to check... We are recently proposing to refactor json_extractor with the help of @netj , specifically to get the json <-> csv/tsv done in another language. Probably it is also a good time to keep an eye on this issue.\n. I am fixing it right now. It is originally designed to eliminate null values in extractor input, and our tests also enforce that. Do we have specific concerns for doing that?\n. Fixed in https://github.com/HazyResearch/deepdive/pull/163.\n. Other issues include:\n- \"run\" cannot be in multiple lines\n. - must trim all the comments before parsing. If developers have # def run (..) it will breaks.\n. We originally think that it will be much faster since it eliminate IOs from and to the database. It is indeed faster in some IO-bound extractors on small to medium databases. \nThe PLPY language itself seems to have problems operating on large GP clusters, but I am not sure. @zhangce ?\nNow it seems nobody is using plpy extractor so I did not do more bug-fixing on it. Should we deprecate plpy extractor? \n. Not sure I fully get it.. =) Do we want to keep the functionality, but move it from our main extractor documentation to some extra page?\n. Fixed in PR https://github.com/HazyResearch/deepdive/pull/251.\n. We are already doing this right now. The permissions are eventually set in Helpers.executeCmd.\n. @netj I just found that jekyll --watch can automatically detect changes. We can add it into the makefile (in host option).\n. So awesome!!!\n. @feiranwang  I use print >>sys.stderr, message and it works, like below:\n20:00:47 [extractorRunner-ext_sup_orderaware] INFO  SUPERVISION DATA NOT EXISTS: /dfs/hulk/0/zifei/ocr/supervision_escaped//JOURNAL_57898.seq\nAre you sure you have set your logback.xml properly? \nIf you are using deepdive command, try make build again in deepdive root folder, in develop branch.\n. I am also having this problem now. When is the earliest timepoint you have this problem? Thanks!\nOn Jul 1, 2014, at 12:21 AM, Feiran Wang notifications@github.com wrote:\n\nThe problem indeed exists.\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nZifei Shan\nM.S. student in Computer Science, Stanford University (2015)\n. Solved in 8cd72356c66a44ba9e7b76ed8dc68ca58fce2167.\n. @netj Thanks!\n. Not right now. I will fix this in develop branch at some point.\n. Fixed in d328604c947d4c3b1decbb46adc39466e4599402.\n. @feiranwang Can you post your detailed logback.xml? When I try the following it works.\n```\n\n\n\n\n\n${simplerPattern}\n\n\nlog/${bySecond}.txt\n${defaultPattern}\n\n\n\n\n\n\n\n```\nNote that \"root\" block needs \"debug\" level. I currently cannot find a way to separate STDOUT and FILE logging levels.\n. I have changed UDF output to INFO level in 8cd72356c66a44ba9e7b76ed8dc68ca58fce2167. Please pull develop branch.\n. @dennybritz Denny, do you have any ideas regarding this?\n. I submitted a pull request https://github.com/HazyResearch/deepdive/pull/162\nthat addresses this issue. I renamed it to dd_inference_result_weights_mapping and kept the original view for backward compatibility.\n. Fixed in #162 .\n. @rionda Please check this. Thanks!\n. @rionda @feiranwang @amirabs  Need some review for this! Thanks!\n. See branch zifei-scaladoc.\nSee http://web.stanford.edu/~zifei/deepdive/doc/api/#package for a sample of deployed scaladoc.\nIt is largely unfinished. I guess we won't include it in this release?\n. @feiranwang Please review this piece of code and let's iterate on it. Thanks!\n. @rionda @feiranwang Would you finalize the review of this? I know that this many changes are hard to review, but I want to aim at this weekend that all the code here is tested. Thanks!\n. Fixed by @feiranwang in f2f726080d20cc43ce70b1de177ee6808d08a8b6.\n. Ship it!\n. I discussed with Sen and it seems possible to install GP on Travis and test it. I (or maybe Sen) can look into that later, after mysql port is finished.\n. Solved in #153 \n. @rionda Matteo, would you review this hot fix?\n. @rionda What do you mean by false positive? It seems like an error that Travis occasionally has...\n. @rionda It was just that I was coding up the genomics example and find this issue. Maybe it is a good idea to put it in develop for a while since nobody other than me actually uses this feature now.\n. @larryxiao  @netj \nJust FYI, In one of my recent pull requests to develop (https://github.com/HazyResearch/deepdive/pull/153), I introduced Context.deepdiveHome in the code. It is assigned as the value of environment variable $DEEPDIVE_HOME if set, or current directory if not set. Scripts like util/tobinary.py are then located by Context.deepdiveHome/util/tobinary.py.\nFeel free to change it If you have a better way :) But just be aware of the current solution.\n. Thanks! =)  I am not sure if we want to introduce a environment variable like this, though... @netj what do you think?\n. @netj Sorry for the confusion, but it is NOT that all the paths in application.conf will be resolved as if the base path were in $DEEPDIVE_HOME, BUT only some internal functions (e.g. util/tobinary.py, util/sampler-dw-linux, which are not exposed to users) in the Scala code will be resolved using $DEEPDIVE_HOME. \nThe base path in application.conf is still current working dir, i.e. whatever developer runs deepdive. If deepdive is run on application root folder (say APP_HOME), then you can refer to udf/ext_people.py in in current difectory, as an example.\nMany of our current examples do set different environment variables $DEEPDIVE_HOME and $APP_HOME and refer to them in application.conf, and start the application in $DEEPDIVE_HOME. There are many alternative ways to do it though.\n. ndbloader integration done. Ready to merge after review.\n. Yes, travis is already testing both psql and mysql.\n. Passed tests on GP.\n. @rionda Yes it is for --quiet.\nThe linux library is already updated in a previous commit by Feiran: 7cca8be8a7956209f294425ed4c0ede171ca4a34\n. I realized that this non-termination fix does not guarantee fixing all non-termination errors, since we has multiple Akka actors (InferenceManager, ExtractionManager, etc), and simply catching errors in DeepDive class may not handle all cases.\nThis PR fixes non-termination when:\n- invalid configuration file (mandatory fields are not present)\n- invalid factor input_query\n- sampler throws error\n. @rionda I am done. It can be merged after passing Travis test. Thanks!\n. Just confirmed and mysql also supports ALTER TABLE DROP COLUMN IF EXISTS ... syntax. The thing it does not support was dropping indexes if exists.\nFixed in https://github.com/HazyResearch/deepdive/pull/176. Waiting for tests to finish.\n. Tested on GP and fails. There is actually a problem on GP that it does not support DROP COLUMN IF EXISTS syntax... Will try to find a workaround.\n. Yes. It passed the tests on my local MariaDB, though. Looks like it's a corner case query that some older systems do not implement. \nI'm adding a try-catch block with simple DROP COLUMN index. Does that sound OK?\n. It could be hard since the error messages and exception types are different across different DBMS.\nBTW, Feiran is working on a new implementation without changing the variable table, and it will be out today. With that we shouldn't have this issue any more. @feiranwang  \nSo we can probably discard this PR.\n. I tested locally and the problem was because a join query in inference takes too long. MySQL uses Blocked Nested Loop Join, so we have to create indexes to speed up its joins.\nI created an index on id column on the temporary variable type table and it seems to work fine. Let's wait to see the results.\n. @rionda Fixed now.\n. Yes, a good way to do it would be putting a functional block of code containing the index creation in a function, and overriding that function in MysqlInferenceDatastore.scala. However I am not sure why we don't want to create the index in Psql. IMHO, indexes basically make everything faster if we use them properly (as long as we don't modify the type table, which I think is true).\n. @rionda Do we still want to merge this fix? \n. Note that this is a PR to develop branch. This feature can be released later.\n@rionda @zhangce @feiranwang Would anyone review my code here? It's mostly changes to grounding. Thanks!\n. @rionda Docs are updated. Thanks!\n. @rionda Sorry. These code to extract coref may be added into the nlp extractor in the future. Added a bit documentation to the commented code.\n. Looks good to me. @rionda \n. @chrismre I think it is not fixed yet.  I will run a test on an example application to make sure if the problem actually exists, and send a report to Ce.\n. Looks good to me other than the comments. Have you tested on a GP machine? @feiranwang \n. @zhangce @feiranwang  The env var was no longer used. Thanks!\n. @chrismre The code itself works, but Sen proposed some changes to the configuration format that I haven't got time to add. \nI think we can first merge the current request, and I will update the configurations for parallel loading / grounding later. @SenWu do you think so?\n. @chrismre The current pull request is backward compatible. It just adds a configuration option extraction.parallel_loading. I agree that we should discuss before changing the format of application.conf, but the current pull request should be safe in terms of backward compatibility.\nSen was suggesting that we remove the inference.parallel_grounding and extraction.parallel_loading options, and I think this might need more discussion after merging this pull request.\nI am confused on what to do with the current pull request. Although it does not break the format, it does add a new configuration option. Should we avoid that?\n. @chrismre Sorry for the delay.\nFor 1, the optional configuration handeling, I will leave it as it is, as it's a part of a larger refactoring.\nFor 2, regarding the python in-database extractors, we had a discussion in https://github.com/HazyResearch/deepdive/issues/104. There are some known unfixed bugs in plpy_extractor, and right now nobody is actively using this extractor type, so we once postponed the maintenance of that extractor type. I still agree that plpy_extractor is I/O-optimized, while tsv_extractor is computationally optimized, and there are cases that using plpy_extractor is faster. I will remove the deprecation in the documentation, and fix the bugs later.\nFor 3, sorry that this goes into the pull request. I will remove it.\n. @mikecafarella I have done the changes mentioned in the last message. (For 2, I will not fix the plpy bugs for this pull request.) Thanks!\n. @SenWu Thanks! I am using US patents. They are similar to research papers.\nI saw that in the Genomics application, we are replacing the \"O\"s with lemmas. Is that a common improvement?\n. @netj Fixed the duplicated lines.\nThe tests are now wired up with the main test.sh from DeepDive.\n. @netj We have many places in our system that tries to create tables by appending strings to existing tables, e.g.:\nhttps://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/datastore/Dataloader.scala#L48\nhttps://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala#L893\nI think our system is implicitly supposing users to use not-too-long table names (<30 characters should be fine, we haven't tested its limit there). However in this bug, the system is stricter since it used  \"${outputRel}_unload_${funcName}\" and funcName is \"func_${task.extractor.name}\" so a 20-char table (e.g. relation_mention_features) will easily break it. After the fix it is coherent with other places.\nI agree that eventually the system should not pose any implicit restrictions to the length of table names, or we have to mention what the restrictions are.\n. @chrismre As I understand, this happens a long time ago, when we decided to do grounding by assigning an id column to each row in the variable table. Sampler would require different variables to have different ids, but it is not possible under the current grounding framework since each table that contain variables only have one id column.\n. I will fix the docs tonight.\n. Merged and deployed.\n. @zhangce Should we merge and deploy this?\n. @zhangce Do we want to link this banner to somewhere? Currently it links to nowhere.\n. @netj Could you review these changes? Thanks!\n. Merged and deployed.\n. It turns out I am using a version without XL integration.\n. Starting from release v0.6.0, the speed is acceptable.\n. Got it. Thanks!\n. I was running an old application, which does not run via deepdive run. Thanks!\n. @netj Should we merge this? I've addressed the comments by setting the number of splits to parallelism. Thanks!\n. Agreed that this is very useful in many applications.\nOn Sun, Sep 27, 2015 at 2:33 AM, zhangce notifications@github.com wrote:\n\n@thodrek https://github.com/thodrek 's new framework should provide an\nultimate cleaner solution here. For short term, it should also be an easy\nfix.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/416#issuecomment-143507063\n.\n\n\nZifei Shan\nM.S. in Computer Science, Stanford University (2015)\n. Hi @netj , I notice that the deepdive sql '[multiple SQL queries]' does not work under postgres-XL. It's probably necessary to manually split these queries into individual commands when compiling sql_extractor to bash scripts.\nHere is how we previously split SQL queries by semicolons: https://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/datastore/JdbcDataStore.scala#L87\n. @netj \nThis seems to be a bigger issue than sql_extractor for PGXL: I am trying out some other extractors but it had other problems with this line: https://github.com/HazyResearch/deepdive/blob/dataflow-compiler/runner/compute-driver/local/compute-execute#L123 since it's also calling multiple queries with deepdive sql. I guess a cleaner workaround would be splitting queries for  https://github.com/HazyResearch/deepdive/blob/dataflow-compiler/database/db-driver/postgresql-xl/db-execute?\nRegarding the PGXL issue, I could not find discussions for it online. A sample query that would fail on my environment:\n$ psql $DBNAME -c  \"create table tmptmp(id int);\ninsert into tmptmp values(1);\nselect * from tmptmp;\ndrop table tmptmp;\"\nERROR:  Unexpected response from the Datanodes for 'T' message, current request type 1\nERROR:  Unexpected response from the Datanodes for 'T' message, current request type 1\nERROR:  Unexpected response from the data nodes for 'D' message, current request type 0\nServer version: Postgres-XL 9.2.0.\n. @netj I just found an alternative and it works: instead of \ndeepdive sql  \"create table tmptmp(id int);\ninsert into tmptmp values(1);\nselect * from tmptmp;\ndrop table tmptmp;\"\n, if I run:\necho  \"create table tmptmp(id int);\ninsert into tmptmp values(1);\nselect * from tmptmp;\ndrop table tmptmp;\" | deepdive sql\nit will work for PGXL. I think if we change db-execute of PGXL, to pipe the SQL query into a deepdive sql command, it would work out nicely.\n. @netj Yes. Try psql -f commands.sql. It works with pgxl as well.\n. @netj Regarding 0.8 vs 0.7.2, will 0.7.2 include the grounding improvements that's currently in master branch? That is valuable since it doubles the grounding performance in our setting.\n. @alldefector  Great findings. I ran the pipeline end-to-end 10 times with #570 and confirmed that the problem went away.\n. NVM, it turned out that I forgot to initialize the submodules. (have to run git submodule update --init before installation)\n. Breaking spouse example grounding, discarding. In the future may want to make this dynamic / configurable.\n. ",
    "netj": "Unless there's a concrete example that needs this demultiplexing, this sounds like unnecessary complexity.\n. We now have ddlib for text-based apps.\n. This is too dated and lacks detail for action. Let's close this and reopen or create a new issue if needed.\n. This is a great idea but lacks detail to be actionable. Let's close this and create a new issue with concrete ideas later.\n. This is too dated and lacks detail for action. Let's close this and reopen or create a new issue if needed.\n. By having a robust universal installation method, our AMIs will be deprecated.  See #302.\n. This is perhaps done by #251, which runs with flag psql -v ON_ERROR_STOP=1.\n. Duplicate of #1\n. This is done by #207 or more specifically: ff36cdc07b0a1573b01b24dc21d9eb822f0a8d45.\n. From a newbie's point of view, I thought the repetition of names in most example code for application.conf was rendering the documentation unnecessarily more complicated.  Unless there's a good reason to repeat the names, why don't we shorten them?\nFYI https://github.com/typesafehub/config/blob/master/HOCON.md#readme\nI can help with this one, as I already made some changes while reading.\n. My pleasure!  I'm nearly done, and this may be off-topic, but do you know why extractors pop up in the examples for inference rules document? See: https://github.com/HazyResearch/deepdive/blame/gh-pages/doc/inference_rules.md#L65\n. Probably fixed by #251 which adds a flag psql -v ON_ERROR_STOP=1 to stop on error.\n. Probably fixed by #251, which stops on SQL error.\n. I can surely create a clean Makefile to do that, but it would force everyone who had an app to modify their run.sh to call make instead of sbt.  I think we need a well designed solution for this \"user facing part\" rather than adding ad-hoc layers as temporary fixes, because these small things will pile up and make DeepDive look daunting to use.\nI wish DeepDive provided a clean \"facade\" command (perhaps named deepdive?) that the app users/developers could call from their app directory for performing the standard tasks, e.g., extraction, grounding, learning/inference, end-to-end, etc.  I really like the declarative style (application.conf) of defining apps, but the rest of the necessary glue code seems pretty brittle.  Here's my suggestion: DeepDive users would simply add a directory to their PATH environment, and call the deepdive command from their app dirs to work on them.  Outputs will stay within the app as well (instead of DeepDive source tree).  The typical iterative workflow will be user's modifying the application.conf file and/or some udf code, then running deepdive something to reflect changes to the knowledge base (analogous to Git: work; git status; work; git add ...; work; git commit; # repeat).  I'm sure this suggested way would be much easier for maintaining both apps and infra than the current way of duplicating the environment set up (env.sh) and start up code (run.sh) all over the apps.  By simplifying the interface, we will be able to freely introduce any changes to how DeepDive is invoked because such parts will no longer be part of the apps but will stay within the common infrastructure.  I'd be happy to do the necessary grunt work for this, but I'd like to first listen to your thoughts, and observe what are the typical tasks/steps.\nAs a separate issue, instead of using different names (sampler-dw-linux, sampler-dw-mac) for the sampler for each platform, I'd rather use the same name (sampler-dw) and let the build to take care of placing the correct executable on PATH.  This would free other parts of DeepDive (Scala, scripts, etc.) from having to worry about detecting the OS, and let them use the same command.\n. Okay, on a second thought, I should probably add a short Makefile and just change the installation instruction to use make instead of sbt compile.  The facade command design and run.sh/env.sh duplication can be continued on a separate issue, as well as unifying the sampler name.\n. Thanks @feiranwang \n. @zifeishan Do you mean you've already found a way?  I saw #109, and it seems you did with @zhangce, but wasn't clear.\nIn fact, I know a nice way to create an extensible facade command (which I've already used in many other projects including 3X), as well as a way to package everything (jars, shared libs, executables, etc.) into a self-extracting/installing single executable file using BuildKit.  But I was reluctant to start coding because of the huge change it could introduce, without discussing much with you first.\n. I see.  That sounds very useful and promising for now.  However, I would argue against tying the facade command to a single language, because not everything can be done easily/concisely in Scala.\nI think my extensible approach will play nice with such sbt-pack binary, so please go ahead and do it.  Meanwhile, we can discuss what the user friendly command-line interface should look like, and improve it further.\n. Thanks @zifeishan, I'll wait until tomorrow.\n. @zifeishan I was asking why the example was opening a file with an absolute path (/dfs/rulk/0/deepdive/shared/...) instead of one included in the source tree.\n. Oops.  I meant merge into develop.  Does anyone know how to change the PR's target branch?  I should probably recreate it..\n. Superseded by #236 \n. Consider using set -e.  It will halt executing the script whenever a command ends with a non-zero exit status, and return it.  You can't really add such if statements after every line. :)\n. Seems like it was a temporary TravisCI issue.\n. Another option to consider is augmenting the PATH environment variable and call the scripts by name, not their full paths.  We could either add a section to application.conf that lets user declare where the scripts are, or simply prepend the absolute path of the util directory from run.sh when running deepdive (e.g., PATH=\"$DEEPDIVE_HOME/util:$PATH\"  deepdive -c ...).  Former is a better solution while it involves more change, but for the meantime I think the latter is a fine workaround to get things working with minimal change.\nI personally think we should eventually get rid of run.sh and let user simply call deepdive under the application root dir, and most environment setup currently done in the template script should be handled by DeepDive internally, not by a script exposed at user's application.\n. @zifeishan So, you mean all the paths in application.conf will be resolved as if the base path were in $DEEPDIVE_HOME, right?  And that variable is going to default to the current working dir?  I'd rather set the default base path as the application.conf's path.  That way, whoever runs the application won't have to worry about where to start it from, but only care about where that app is.\nPerhaps, we might need two path variables to be truly useful: one for where the DeepDive's builtin stuffs are (DEEPDIVE_HOME), and another where the application-specific files are (DEEPDIVE_APP_ROOT or so).\n. I guess you're right.  I thought I tested it with postgres on my local machine, but it was still running against a GP server.  I'll change this to CSV.\n. Duplicate issue #404 has more details\n. Superseded by #237 \n. @feiranwang Can you fix the pdf? The right link seems to be http://deepdive.stanford.edu/doc/basics/inference_rule_functions.html with basics/ in between.\n. No, please fix the broken link to inference_rule_functions.html in the second page of that tutorial pdf.\n. This sounds very similar to #280.\n. Thanks for your patch.  However, we'll address this environment/relative-path-mess once and for all by introducing a nice facade command soon.  The extra level has been corrected now.\n. Intermittent hanging became more regular after refactoring the tests, and I believe 0bc74f20e9d5fac0a05801b21fbb7a3810ddfcc4 fixes this issue.\n. It should be easier to write a Docker-compatible or environment-neutral tutorial after removing many hard coded pathnames from our examples.\n. Hi @stephenjbarr, sorry our Docker support became low priority as we moved our focus on making the software work on common environments.  Our installer can now set up any Ubuntu or Debian-based cloud instances ready to run DeepDive with a single command.  I'm not familiar with how Postgres containers are set up or the specific way containers are supposed to communicate, but DeepDive is able to talk to any postgres server configured in db.url or $DEEPDIVE_DB_URL, so it shouldn't be too hard to have it talk to a Postgres container.  It'd be great if you or someone who uses Docker on a daily basis could contribute docs and fixes.\n. This will be dramatically simplified in the next release:\n- With from deepdive import *,\n- @tsv_extractor and @returns decorators and the argument default values abstracts away the TSV parsing in UDFs as well as\n- how they're called from DDlog.\n. @feiranwang Wasn't this also fixed somewhere in sampler?  Could you get that merged with the code for #258 as well as any other sampler related ones, then bring the final binary in?\n. The path issue here seems to be corrected when #245 was fixed.\n. The new deepdive command uses a fresh output directory for every run, basically isolating each run's tmp dir from each other. https://github.com/HazyResearch/deepdive/blob/270a74b8ba7b5e1c1bbb0b87af2e4983ae28ecff/shell/deepdive-run\n. @adamwgoldberg Could you advise what I should do about the Shippable failure?  It seems gem install jekyll is failing which has nothing to do with my commits.\n. Thanks for clarifying!  I'll go ahead and ask for someone to merge this PR.\n. It seems you should merge develop again.  Otherwise, looks good to me.  Nice cleanup!\n. Can you briefly summarize how this cardinality change impacts the grounding and whether there's any interaction with the sample?  Does it only lift the supported upperbound for multinomial?  Then can't we take the max length of the cardinality among the multinomial variables?\n. So there aren't any side effects such as the size of the grounded factor graph increasing because of this for existing apps?  1e10 practically sounds big enough, but I still think it'd be better to decide the necessary number of digits based on user's schema.\n. Looks good overall.  Adding tests for plpy certainly makes it easier to understand how ddext works, although I think we should avoid rewriting user's code and eventually redesign the plpy extractor interface to be more compatible with tsv extractors.\nBtw how are the new tests triggered from the test.sh?  Can you wire that up and handle the inline comments as well?\n. @zhangce Was this all taken care of?\n. @feiranwang I thought this was now possible through a sampler option, no?  Maybe I'm confusing with #271?\n. It seems sampler's sample_evidence branch has the code: https://github.com/HazyResearch/sampler/commit/b721e65060a50f9d71500c7d2ff118e8d90bfa01#diff-bcc8c161c87b19685947789ac42e9fe9R32 @feiranwang Could you bring the binary here, also documenting the sampler option?\n. Thanks for pointing out more dependency issues.  We'll get a fix out there that detects them earlier very soon.  gnuplot is currently used for rendering the calibration plot, and you simply don't get the images in the final output without it.\n. Did you figure out why the crash was happening?\n. The crash seemed to be a hardware issue (raiders1, right @zifeishan?).  The doc about the option could be expanded, but if the default is reasonable discussion about tuning seems unnecessary.  Will close this for now.\n. Taken care by #262.\n@tiangolo An updated guide for Ubuntu 14.04 sounds great!  Thanks!\n. Thanks for the fix!\n. ANALYZE doesn't work for mysql, see: 4af3a508ca08b0153560f1835d91af18b33b0135\n. I'll take care of this asap and backport to master.\n. I thought I put that in, but maybe not.  You can try that again, but a cursory look at 0253dd7 makes me even more confused: it seems analyzeTable() is returning string, but how is it getting executed?  At least the one I put in called execute().\nThis combinatorial explosion over the control flow is really a mess and I think we should simply strip the current mysql code and redo it when necessary, possibly in a way that's well isolated from the postgres/greenplum implementation.\n. Where are we using FeatureStats table?  Unless maintaining this table after every run saves majority of apps time for some set of well-defined frequent tasks, let's drop it from DD and move it to Braindump or Dashboard.\n. I see it's commented out by https://github.com/HazyResearch/deepdive/commit/d1f8449ef990a378cde0e6f763bebd8167f821db#diff-e754f8521322016721729d620efbe663.  @zifeishan Could you create a PR that removes the relevant code if you have no objection?\n. Fixes #269 \n. Greenplum (gpload) seems to be not on PATH.  As there are so many environment variables that can go wrong, it may be a good idea to perform some form of runtime dependency check from Main.  A very limited check is done at build/install time by #266, but we'll need different checks based on what database is used.  These could be added as basic sanity checks for application.conf, e.g., checking the udf codes, database connections, etc.\n. Looks okay, but what if the outputRel is longer than 50 chars?  Shouldn't we use a purely generated temporary table name?\n. Good point.  I created #276 as a TODO.\n. I like Ce's reason behind the decision. This was a well known limitation among us but the docs weren't updated timely. Let's fix the docs asap and clearly state why we restrict to one variable per relation.\nMore importantly, I think this is a good time for us to go over the docs thoroughly to find and fix more discrepancies. Also as @chrismre suggests, let's make our docs more testable by integrating all examples in the docs (as well as those in our source tree) into our tests. That'll help us keep everything in sync more easily. Let's come up with a more concrete plan in/after our meeting and button things up to carve out a new release! :)\n. How does this work/pass tests without the new sampler binaries?\n. Because of this issue, @feiranwang introduced the --learn_non_evidence option for the 0.6.0 release, turned off by default, to prevent users from seeing a sudden drop in their quality.  We should fix the quality issue in the correct approach and get rid of the option in the next release.\n. I believe the quality issue has been resolved by a bugfix in https://github.com/HazyResearch/sampler/commit/5347483558bfa7286f41050c5e536bb2e1945cc9\n. Nice to see the coverage from coveralls.io!  Other than the comments above, looks good.\n. Looks good, merging.\n. Could you port this to develop as well in another PR soon?  Because that's where we mostly want to monitor the coverage.\n. Spotted a few errors.  Please fix them.\n. @abresler Could you elaborate on how you installed Postgres?  You need to set PGUSER and PGPASSWORD environment variables correctly.  You're perhaps trying your password with the wrong PGUSER or don't have an account set up in Postgres yet in this case.\n. Did you set up anything besides running postgres -D /opt/homebrew/var/postgres?  Does psql -l show you a list of databases?\nIf standard stuff all fails, you can use initdb /var/tmp/dd.db to create a new home for your database and start postgres with postgres -D /var/tmp/dd.db where /var/tmp/dd.db can be any path you want to store the data.\n. Thanks!\n. @raphaelhoffmann @alldefector Adding a brief introduction page how to use PostgresXL with DeepDive would be awesome for our upcoming release.  Seems like doc/doc/advanced/postgres-xl.md is a natural place?  It probably doesn't have to be as detailed as Greenplum's.\n. This was pending on one last bug in the sampler's incremental mode with NUMA.  @zhangce and I just decided we can go ahead with the release and follow up with a patch soon.\nYes, the website pass can be done anytime.  Any particular parts you want revision?  The new branching policy we'll use makes it easier as the latest code as well as the website kept in master--currently any change made to website needs to be carefully merged to develop, which is quite annoying around release time.\n. It seems the bugfix for HazyResearch/sampler#10 may take a day or two, and the delayed release has been a blocker (mostly mentally) so I wanted to move on to the next things.  However, we can still wait for the patch and mark the release.  Either way is actually fine.\n. Sure.  There's already a rough sketch I made a while ago in the Milestones tab, but we can definitely revise this\u2013perhaps splitting into two? 0.7.0 for the cleanups with easy NLP and 0.8.0 for the rest.  More concrete issues can be created to guide individual tasks as I'm already doing for my cleanup.\n. I don't like quickly growing version numbers either, but at least I agree with the Semantic Versioning telling us to keep the last number for patching previous releases.  For example, when we patch the known bug in sampler a week later, I think it's simpler to give 0.6.1 to it instead of 0.6.0.1 or so, and use 0.7 for the next milestone.\n. Done by 935957dd4b1f25fb085aa8723057fd2e76cf6200\n. Looks great!  I read through it without executing any commands, so there may be some detail I missed, but anyone who's familiar with PGXL should be able to get DD up and running.  Maybe adding a link to the XL spouse_example may be also helpful?\n. @raphaelhoffmann Any updates?  Otherwise, I'd like to give a try.\n. I tested on a 14.04 ec2 instance, and the doc seems accurate after some minor edits.  Will merge.\n. After installing, I tried running make test and found a test isn't passing with Postgres-XL (PostgresInferenceRunnerSpec: Postgres inference data store, grounding the factor graph with Multinomial variables, should work with weight variables) with the following error:\norg.postgresql.util.PSQLException: ERROR: Postgres-XL does not currently support ORDER BY in subqueries\n[...]\n     at org.deepdive.datastore.JdbcDataStore$$anonfun$executeSqlQueries$2.apply(JdbcDataStore.scala:71)\n     at org.deepdive.datastore.JdbcDataStore$$anonfun$executeSqlQueries$2.apply(JdbcDataStore.scala:68)\n[...]\nAny ideas?\n. @feiranwang Feiran, could you comment on why we need those ORDER BY clauses for multinomial?  The original code had ORDER BY even for the _unsorted one with a comment about Greenplum.\n. If dealing with Greenplum was the sole purpose, this will be another good example why we should completely isolate each port at the cost of duplicating some SQL code.\n. @feiranwang Does the factors with \"MultinomialFactorFunction\" need to be grounded in a particular order?  I can hardly imagine that being the case, but you're the expert here.  These ORDER BYs actually originate from your commit for multinomial support and it seems @zhangce just tried to fix some issues on GP.\n. Thanks for your clarification, but what we need here is not adding the comments about current assumption. We're trying to eliminate the ORDER BY in the DeepDive grounding. The code you pointed seems to be dealing with compact factor after loading the input. So, is there any way we can do the ordering from the sampler side, since we're already doing lots of reordering and compaction there anyway?\n. We decided to put this in without multinomial support for the moment.  On grounding multinomial factors, we discussed two possible solutions:\n1. find a general solution for parallel/distributed databases to produce the correct order assumed by the sampler, e.g., using row_number(), or\n2. produce extra info that can be easily generated by parallel/distributed databases and have the sampler recover the assumed order of weight ids.\n. This is a great idea.  I totally agree we should move towards this direction to reduce the maintenance burden.  I'll include new instructions in this release if you could share the scripts you have.\n. Very nice!  I added a temporary redirect to the website for /install.  It should be updated to point to the master branch upon release, but we can test things with your branch for now.  I think it's better to use raw.github.com instead and add curl -fsSL.\nbash\ncurl -fsSL http://deepdive.stanford.edu/install | bash -s\n. I think ultimately we want to use the same installation scripts for Travis or Shippable tests as well.  Currently, variants of these build/runtime dependency check+installation commands exist all over the place (Makefile, lib/dw_extract.sh, lib/check-depends.sh, .travis.yml, Dockerfile, and now a new set of install scripts!).  Besides, DeepDive doesn't have a clear distinction between source and shippable runtime code.  Instead of distributing obscure binaries over git and running apps against the scala sources and scripts, I seriously think we should build a completely separate tarball that can be unpacked somewhere and just works as long as the external dependency requirements are met.  I'll start working on this cleanup right after the current release, and the git clone lines of your proposed installation script will become a simple tarball extraction.  However, we'll still need scripts that installs runtime dependencies for each platform and backend database pair, so we should put more attention to this end.  It would be nice if we can nail down the layout for the scripts in this release and eventually eliminate all variants we have.  If you don't mind, I'll fork your branch to have our Makefile and travis rely on it.\n. Increasing test coverage will happen in the next release.\n. Due to the lack of code coverage tools for our primary languages: bash and jq, code coverage and coveralls badge has been dropped.  I'd say we should do a more thorough code review of these less bloated code. Nobody would argue that one should write database queries in C just because their coverage can be easily measured while it's not so for SQL.  Anyway, having a simple profiling tool that shows which commands are actually unused or used how many times may be useful, though.  Let's keep our eyes open looking for such tool and track it in this issue.\n. @adamwgoldberg It seems your shippable-branch contains a working shippable.yml.  However, I'm questioning why we need Shippable in addition to Travis.  Please comment on why we need Shippable if there's a straightforward reason. (maybe in connection to Docker?)\nOne nice part I see is Shippable gives a good summary on failed tests/assertions in Scala code.  However, if it doesn't provide any real extra value, then I think we should get rid of it to reduce maintenance burden.\n. Now, #311 will clear the Shippable error on our side, but tests using COPY FROM keeps failing due to their limitation (Shippable/support#1503).\n. I see.  Perf testing sounds like a compelling reason to keep container-based tests.  But for now, yeah let's turn it off if you can.\n. Thanks.  Did you just remove repo hooks from GitHub, or something else from Shippable?  It wasn't very clear to me how to turn it off.\n. That explains.  Thanks!\n. I'm thinking we could keep the source (.tex?) for the file together if that's not too much work. \n. Looks great, merging.\n. Could you squash the submodule and binary update into a single commit, with a more descriptive message (what it fixes, why it had to be updated, etc.)?  Maybe even with the doc updates.  That way it's much clearer what those opaque changes are.\n. Thanks for squashing.  Should we turn the --sample_evidence flag on by default as suggested?  Is there a test that checks whether the flag works?\n. I'll update the installation docs in this PR.\n. On an Ubuntu instance, we can now test the scripts with:\nbash\nexport BRANCH=netj-installers\nbash <(curl -fsSL https://raw.github.com/HazyResearch/deepdive/$BRANCH/util/install.sh)\n. @raphaelhoffmann Glad you like it!  I think we can polish scripts for postgres, XL, etc. over time, e.g., accepting important configuration variables interactively.\n. Thanks for the comments!  I think for minor ones or ones with clear edits (6? and 7) you could modify the scripts on this branch directly from GitHub.  I found that feature pretty handy.\nTaking care of dependency (2-4) is definitely something to figure out.  I'm tempted to simply support an optional precondition function (can_install_*) for every install_* functions, which controls visibility, e.g., show deepdive_git_repo only if git is there.  Regarding 4, psql probably shouldn't count as a requirement for deepdive.\n1 and 5 are bash issues.  I'm relying on select, and that's the default behavior.  We could certainly put more effort polishing this part, but I wanted to keep the first version super simple with a bash builtin.  I have a nice set of bash snippets we can use later for creating slick interactive prompts.  5 is due to my set -u which prevents a lot of errors, but I guess we can turn that off by default for the rest of the installer scripts.  I'll quickly get this fixed.\n. Thanks for your edits!  The tests are failing sometimes.  That needs to be fixed soon, but we know this PR is independent from the code that's failing.  So I think we can go ahead and merge.\n. My limited understanding keeps me from being perfectly convinced that this approach works exactly the same, but I'll trust you and @feiranwang on that.\n. Please remember to update the submodule as well next time.  Thanks!\n. It seems the unit tests are now passing, but during the test_incremental, it still gives an error:\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: DROP SEQUENCE IF EXISTS dd_variable_sequence CASCADE                                                 \n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: CREATE SEQUENCE dd_variable_sequence MINVALUE -1 START 0\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback...  SELECT num_variables FROM dd_incremental_meta_data\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: UPDATE dd_delta_has_spouse SET id = NULL\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: UPDATE dd_delta_has_spouse AS t0 SET id = t1.id \n        FROM has_spouse t1\n        WHERE   t0.relation_id = t1.relation_id  AND (t0.label = t1.label \n        OR (t0.label is NULL AND t1.label is NULL))\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: DROP TABLE IF EXISTS dd_delta_has_spouse_inc CASCADE\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback... SELECT version() LIKE '%Postgres-XL%';\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: CREATE UNLOGGED TABLE dd_delta_has_spouse_inc AS  \n        SELECT id, relation_id, label\n        FROM dd_delta_has_spouse \n        WHERE id is NULL\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: ALTER SEQUENCE dd_variable_sequence RESTART 4685\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback... SELECT version() LIKE '%Greenplum%';\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback... SELECT version() LIKE '%Postgres-XL%';\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: SELECT copy_table_assign_ids_replace('public', 'dd_delta_has_spouse_inc', 'id', 4685)\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback... SELECT COUNT(*) FROM dd_delta_has_spouse_inc;\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: UPDATE dd_delta_has_spouse AS t0 SET id = t1.id \n        FROM dd_delta_has_spouse_inc t1\n        WHERE   t0.relation_id = t1.relation_id  AND (t0.label = t1.label \n        OR (t0.label is NULL AND t1.label is NULL))\n04:31:30 [PostgresDataStore(akka://deepdive)] ERROR org.postgresql.util.PSQLException: ERROR: could not plan this distributed update\n  Detail: correlated UPDATE or updating distribution column currently not supported in Postgres-XL.\n04:31:30 [profiler] DEBUG ending report_id=inference_grounding\n04:31:30 [taskManager] INFO  Completed task_id=inference_grounding with Failure(org.postgresql.util.PSQLException: ERROR: could not plan this distributed update\n  Detail: correlated UPDATE or updating distribution column currently not supported in Postgres-XL.)\n04:31:30 [taskManager] ERROR task=inference_grounding Failed: org.postgresql.util.PSQLException: ERROR: could not plan this distributed update\n  Detail: correlated UPDATE or updating distribution column currently not supported in Postgres-XL.\n04:31:30 [taskManager] ERROR Forcing shutdown\n. @feiranwang This seems like the UPDATE statement that deduplicates by copying the base ids to the delta, right?  Is there any other way than updating the dd_delta table, such as doing an OUTER JOIN, by first using the base variable and COALESCE-ing to the id in delta or so?\n. @feiranwang Sorry for not being clear.  I was thinking of an alternative approach creating a view to select the right id instead of overwriting a column.\n. @feiranwang Yes, the ids will be physically assigned to both delta and base, but I think what you do there is you choose the ids in the base table if the delta variable already existed, right?  So all I'm saying is we could devise a nondestructive way to choose the id from the base for delta rows that join with a base row, and just fallback to the id assigned for delta otherwise.  I know this isn't a simple change, but I'm guessing we could've done this to avoid these compatibility issues with distributed databases.\n. Closing as we're solving this problem from #321.\n. For the record, the latest DELETE fix fails on PGXL during incremental test with the following error:\n+ ./run.sh spouse_example.symmetry.ddl --incremental inference inc.f1+f2+symmetry.out\n[...]\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: CREATE UNLOGGED TABLE dd_graph_weights (id bigint, isfixed int, initvalue real, cardinality text,\n       description text) DISTRIBUTE BY REPLICATION\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: DROP SEQUENCE IF EXISTS dd_weight_sequence CASCADE\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: CREATE SEQUENCE dd_weight_sequence MINVALUE -1 START 0\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: DROP SEQUENCE IF EXISTS dd_factor_sequence CASCADE\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: CREATE SEQUENCE dd_factor_sequence MINVALUE -1 START 0\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback...  SELECT num_weights FROM dd_incremental_meta_data\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: ALTER SEQUENCE dd_weight_sequence RESTART 2221\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback...  SELECT num_factors FROM dd_incremental_meta_data\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: ALTER SEQUENCE dd_factor_sequence RESTART 40310\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: DROP TABLE IF EXISTS dd_graph_weights CASCADE\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback... SELECT version() LIKE '%Postgres-XL%';\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: CREATE UNLOGGED TABLE dd_graph_weights (id bigint, isfixed int, initvalue real, cardinality text,\n     description text)\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: DROP TABLE IF EXISTS dd_query_dd_new_has_spouse_0 CASCADE\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback... SELECT version() LIKE '%Postgres-XL%';\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: CREATE UNLOGGED TABLE dd_query_dd_new_has_spouse_0 AS SELECT R0.id AS \"dd_new_has_spouse.R0.id\" , R2.feature AS \"has_spouse_features.R2.feature\" FROM dd_new_has_spouse R0, dd_delta_has_spouse_candidates R1, has_spouse_features R2 WHERE R1.relation_id = R0.relation_id AND R2.relation_id = R0.relation_id UNION ALL SELECT R0.id AS \"dd_new_has_spouse.R0.id\" , R2.feature AS \"dd_delta_has_spouse_features.R2.feature\" FROM dd_new_has_spouse R0, dd_new_has_spouse_candidates R1, dd_delta_has_spouse_features R2 WHERE R1.relation_id = R0.relation_id AND R2.relation_id = R0.relation_id\n10:01:33 [PostgresDataStore(akka://deepdive)] ERROR org.postgresql.util.PSQLException: ERROR: Failed to COMMIT the transaction on one or more nodes\n10:01:33 [profiler] DEBUG ending report_id=inference_grounding\n10:01:33 [taskManager] INFO  Completed task_id=inference_grounding with Failure(org.postgresql.util.PSQLException: ERROR: Failed to COMMIT the transaction on one or more nodes)\n10:01:33 [taskManager] ERROR task=inference_grounding Failed: org.postgresql.util.PSQLException: ERROR: Failed to COMMIT the transaction on one or more nodes\n10:01:33 [taskManager] ERROR Forcing shutdown\n10:01:33 [taskManager] ERROR Cancelling task=calibration\n10:01:33 [taskManager] ERROR Cancelling task=inference\n/home/ubuntu/deepdive.xl/sbt/sbt: line 1:  6461 Killed                  java $SBT_OPTS -jar `dirname $0`/sbt-launch.jar \"$@\"\nmake: *** [test] Error 137\n. @SenWu @raphaelhoffmann Are you sure?  Do you see the incremental test ending with two PASS lines?  I can consistently reproduce the error.  I just started a new EC2 instance, installed PGXL with our installer, and ran make test to get the same error.\n. @raphaelhoffmann Yes, maybe my environment is somehow interfering?  I'd like to cross verify it's working.\n. @raphaelhoffmann Thanks for the info.  Yes, this may be relevant.  One quirk I always observed was the first createdb for deepdive_test was always failing, and I had to restart the test.  On the second run, it appeared to be running fine, but there such issue might have been lingering to cause the transaction error in incremental tests.  I'll trust your test results and merge this.\n. Please see my inline comments.  Otherwise, it looks pretty good except that most of the new parts aren't under the standard test suite.  It'd be nice to run the piggy_extractor example as another integration test if that's possible with a simple shell script similar to test/test_incremental.sh. I'm planning to rewrite all integration tests in Scala into simple shell scripts that directly test code under examples/, so let's defer this if you're tempted to copy/paste your example into src/test/scala/.\n. Adding w=1 to the diff will show just the script and modification to Makefile and travis.yml.\n. I'm reluctant to add it to developer's normal workflow.  Anyone who set up the editor appropriately will never create violations, so I think it's only going to slow down the innocent.  Gatekeeping bad code from Travis for PRs seems enough to me.  Speeding up Travis tests is another thing I'm planning to do in #325.\nI made sure one can easily check what's wrong with the make command and open the problematic files from an editor if they ever need to:\nbash\nvim $(make checkstyle)\nI found an interesting tool to deal with whitespaces from the Q&A.  I guess we should rewrite the checks with it sometime later.  The script I think should eventually be rewritten as another test suite with a proper test framework like Bats or so.\n. Thanks for reviewing. There's no formal button for approving PRs. You can just express approval like this and since we're both commiters either of us can press merge! :)\n. @feiranwang commented on wrong place?  I was confused when I looked at the PR :)  Will merge myself.\n. @alldefector Using alternative paths is now possible by aebb987bf39eb766a3831b02b6d612f0c3f5f5e7\n. Done in ca568ae1cc2e56b09a097a8f4f4009d57ee40a3f\n. Thanks for your input!\nFor the moment, there's no test we prepared for the binary installation, but this is an important point we missed, and we'll add an easy way to test very soon.\nThe tutorial pages have all been revised with the new, cleaner app structure and command-line interface.  Please take a fresh look at them. :)\n. Can I ask why it's in a file named segmentio.js?\n. I wished the two fixes were in separate PRs, but if you think they're better together, I'm okay with either way.  Not 100% sure about all the details of multinomial incremental grounding part, but the code looks relatively okay.  We should really refactor and duplicate the code for different databases soon to stop introducing more branches to the control flow.\n. LGTM\n. @zifeishan Could you confirm if this is still an issue with the latest and close otherwise?\n. PTAL\n. You're right.  v0.7.0 introduced big changes, but that's why the version number was bumped.  It is strongly recommended to reorganize your code according to the new standard app layout to use the latest version because a lot of other features will assume it.\nHowever, for the moment, I believe you can rewrite the command as follows, similar to how an example that's not fully migrated yet is doing:\nbash\ndeepdive env java org.deepdive.Main -c application.conf -o output_dir\nThe deepdive run command in fact supports the -c and -o options, although they aren't well documented yet.\n. I'll have to take a deeper look at the code soon, but could you add a test (.bats file) for this format=json part with the corner cases we manually checked?\n. That's a good question.  You can just run deepdive sql within a .bats file as long as it sources the env.sh file.  The test/env.sh is setting up the PATH correctly for the staged deepdive command.\n. Please see my comments.  I can help you rewrite the .py to construct a list of functions if you want.\n. It seems earlier versions of postgres also needs this workaround.  Travis tests are broken which has psql 9.1: https://travis-ci.org/HazyResearch/deepdive/builds/74359333#L2436-L2439\nI think we may check the postgres version to see if to_json() is supported and decide which approach to use, instead of keeping this as a greenplum specific extension.  The version check should ideally be done once from db-parse to set some env vars that controls db-query.\n. I'll take care of the version check once you finish greenplum's db-query.\n. I'm looking into distinguishing empty string vs null with csv.reader.  Please let me know if you have a good idea.  Also any comment on my new code is welcome!\n. No, psql produces a clearly different csv: empty strings are quoted (...,\"\",...), but NULLs aren't: (...,,...), as I commented in the csvtojson.py.\n. My tests probably got way too extreme, but I think it's worth spending some time once and for all getting this part perfectly reliable. Unfortunately our script cannot handle all the corners, but the limitations are at least clearly written in the tests, and hopefully they'll affect very few users who use old versions. \nPlease take a look and let's get this in unless you have concerns. \n. Thanks for taking a look.  You're right.  Tests fail on mysql spouse example quite often.  Not sure why though.  Can you put up an issue or look into it?\n. @chrismre Unfortunately, not yet. You're probably remembering a fix for the pipeline. #329 is supposed to fix all these nasty glitches, but other priorities have been slowing down its progress.  I'll see if I can quickly get some important pieces done first to prevent these issues.\n@kconor Sorry, the dependencies config isn't reliable!  For the moment, please specify all extractors in the pipelines config.\n. For the record, our current guess of the cause is having different extractor implementations for mysql.  However, for the moment, let's just retry the test once more when it fails to suppress false alarms from Travis because that's basically what we're currently doing anyway.  We should get to the bottom of this and unify the underlying code eventually.\n. Closing as MySQL tests were dropped since nobody's seriously using it.\n. Sounds like a great suggestion!  Admittedly, the current initdb command that literally initializes the whole database is not very friendly to such incremental changes.  It was the best abstraction at the moment given the opaque schema.sql and setup_database.sh-style scripts most apps were using.  Ideally, I think DeepDive should have more visibility into the database schema to provide finer grained control.\nSpeaking more concretely:\n- [x] We could replace schema.sql with a schema.json where the backend-specific CREATE TABLE SQLs are generated by DD.  This JSON format could stay in sync with DDlog's, which becomes another reason to use DDlog!\nExample schema.json.\n- [x] Creating/truncating database tables individually could be done much easily, e.g.:\nbash\n  deepdive initdb sentences ontology mentions\n- [x] Instead of having the flat and opaque input/init.sh script do everything, we could set up a granular convention for keeping the data/script to load into individual tables, e.g.:\nbash\n  deepdive load sentences\nloading input/sentences.{tsv,csv,json-seq} or running input/load-sentences.sh.  It'd be great to have this directly load columns produced by Bazaar Pipe as well.\n- [ ] Detecting changes to the schema becomes also significantly easier, and we could provide a handy way to evolve the schema, e.g., running autogenerated ALTER TABLEs with something like:\nbash\n  deepdive migrate gene_mentions\n@chrismre I think there may have been a similar sounding issue in the past, but I believe this is a new thing due to the deepdive command we introduced in the latest release.\n. Sorry for the hassle guys.  It turns out we didn't test carefully enough on vanilla OS X.  The installer script should now work as expected now.\n. Not yet, but paving the way for it!\n. LGTM\n. @feiranwang Didn't we solve this from ddlog compiler and deepdive-run at some point by providing a reasonable default parallelism and making this a non-issue?  Could you give a check and close?\n. @alldefector Yes, that's certainly where we're headed.  We're trying to preserve DDlog's high-level info in a JSON format to do various operations.  @feiranwang has already made some progress on the creating/altering parts in #376 and we'll try to expose more operations to give complete control to the user.\n. PATH set up is done by deepdive command.  Note that $DEEPDIVE_HOME is no longer the root of the source tree to make binary releases possible.  Instead stage.sh places all scripts under util/ in the directory that becomes $DEEPDIVE_HOME.\nI think you're not running the scala code through deepdive env which takes care of setting up the environment correctly.\n. Did you use the binary release option from the installer?\n. I see.  The updated changelog for 0.7.0 will hopefully prevent others from falling into the same issue!\n. @astrung Sorry you ran into this.  Could you tell us whether that failing update is happening after some debconf and add-apt-repository commands?  (Creating a gist of your entire output may be much more helpful.)  Have you tried changing your Ubuntu mirror to a more reliable one?  It seems the one you're using is not up-to-date.  I'm not sure how apt-get update could only fail from our installer script because we're not altering anything specially.\n@chrismre This runtime dependency installer is actually being tested on Travis upon every build.  However as you know, some unusual user configuration can always break things.  We could take a more aggressive approach to bundle exact versions of all our dependencies to make these errors much less probable.\n. @astrung Glad to hear it worked.  The installer makes sure you have all the dependencies installed upfront to save you from debugging runtime errors.  You can install just DeepDive without any system updates using the deepdive_from_release option.\n. Can you please rebase your commits instead of keep merging master?\n. Thanks for making these changes!  It seems ddlog_manual.md is duplicated in advanced/ as well as basics/.  Shouldn't we keep only one (perhaps in advanced/)?\nI initially thought we were going to keep that page as an evolving document on DDlog's wiki.  If you think it's good to keep a snapshot on DeepDive's website, I'm okay with that but let's put a link to the wiki from that page, clearly stating the latest version is on the wiki.\nAlso, can you make sure util/ddlog.jar is no longer there and .gitignore'd?  I think I removed it at one point, but it seems some merge introduced it again.\n. Thanks for you updates.  Could you fix the regressions and inconsistencies in the docs I mentioned?\n. Thanks! LGTM now\n. Please see my minor comments.  I want deepdive load to support more formats (e.g., Bazaar/Pipe's json columns) and implicit data locations for tables under input/, but this is a good start.\n. Looks good.  Seems like the python script is not necessary.\n. Looks like a nice workaround for an inefficiency in our current extractor runner.  Please see my inline comment.\n. @zifeishan I'm a bit reluctant since we'll address this in a new codebase, but I'm fine merging if you guys need this fix in the short term.\n. Doesn't deepdive initdb TABLE still drop and create the whole database before creating the given table, affecting others?  Here's what I think users expect from the initdb command:\n- When there are arguments, DD should drop/create/load just those specified tables.  Assuming it's a DDlog app, DD should drop/create the tables from the DDlog schema, then optionally load data to the new tables from an assumed path under input/ by some naming convention.  It's an error if it's not a DDlog app.\n- When no argument is given, DD should drop/create all known tables.  If it's a DDlog app, all tables defined in the schema should be created then loaded as if the names were all given manually.  If it's not DDlog, it should rely on schema.sql and input/init.sh to initialize the database.  For this last non-DDlog case, DD should perhaps do a dropdb to be backward compatible.\nIn any case, DD should first make sure the database is created.\n. Nice updates.  Please see my comments.\n. Looks good, merging.\n. Thanks for your quick fix.  Please see my comments to make it more robust to future changes.\n. It seems the Postgres tests are all skipped perhaps because of some parsing issue?\n. Any anticipated side effects if we drop DISTINCT?  I think there must have been a good reason or use case why we added it in the first place.\n. Closing since weight reuse has been overhauled.\n. Done in #437 and HazyResearch/sampler#13\n. I think this is no longer the case for 0.8 but can you confirm @raphaelhoffmann?  I'm not sure what was being copied.  What \"copy\" were you referring to? materializing the categorical variables or also Booleans? or something done inside the stored procedures?\n. Sequence assignment has been overhauled and this issue is probably no longer applicable.\n. Hi @vsoch, it's possible to do the tagging with RegexNER while running CoreNLP, but in typical DeepDive apps it's done as you first thought by writing extractors that scan every word/phrase.  That's partly because your tagging (or candidate mapping) extractor evolves a lot while most of the compute heavy NLP don't, so decoupling speeds up the development.  We're currently working on an easier way to construct efficient extractors that rely on dictionaries or set of patterns, but until we have something readily usable, I think it's fine to get started with a naive version and optimize as needed.  PaleoDD may be a good example to look at.\n. Thanks for pointing out! These are rough corners that were overlooked for json. @feiranwang could you take a look?\n. This becomes less of an issue with #565 and the @tsv/tsj_extractor decorators.  I agree ddlog function calls can be improved to alias the input columns per its declaration.  However, since we dropped support for json_extractor, which is the only one where the column names matter, I'm closing this issue.\n. Thanks for the fix!  Ideally, these line numbers should be auto generated based on some markers at some point.\n. @feiranwang Sorry I wasn't clear about my intention.  I was postponing the merge because it was modifying the docs that may conflict with other more urgent issues.  It looks good so please base your commits on top of #376.\n. Thanks for the fix!  Looks good other than that minor one.\n. Hi @astrung,\n- The first one isn't an issue because DeepDive streams the records from database to the Python UDFs, and as long as the UDF operates on each row independently (row-wise extractor), memory won't be a limit.  There is however an implementation artifact that duplicates the input data for UDF on filesystem (#428), so the disk storage can be a limit.   But we plan to lift this limitation soon by not spilling to disk.\n- I think you're asking for the pipeline.relearn_from configuration which allows you to test with what's already learned.\n. Hi @ukliu, sorry but run_deepdive_tests option has been added after 0.7.0 was released, so the release itself doesn't support testing installation, but you can run tests with the maintenance branch.  Try:\nbash\nRELEASE=v0.7.x  bash <(curl -fsSL deepdive.stanford.edu/install) run_deepdive_tests\n. @ukliu I see.  It seems your postgresql is not installed with plpython support.  We are trying to eliminate that dependency in the next release, but for now you can reinstall it with:\nbrew uninstall postgresql\nbrew install postgresql --with-python\nor use our installer's postgresql option.\n. Glad it helped!  Yes, that test is for a deprecated feature and intentionally skipped.\n. @ukliu I'm sorry for not responding earlier to this simple issue. You probably forgot to add the directory where deepdive is installed to the PATH environment (3rd step of Quick Installation).  Open a new terminal if you already modified ~/.bash_profile.\n. @ukliu My pleasure!\n. plpy extractor support is gone in 0.8 in favor of tsv extractor\n. @ThomasPalomares @juhanaka @ajratner can you create a PR if making ddlib play nice with Python3 is this simple?  Thanks!\n. Closed by #532 \n. @zhangce Yes, perhaps adding in the README a link to the right section of the tutorial or even to the opendata page is enough.  Once people download these datasets, they seem to get detached from the web page and expect things to be self-contained.\n. I partly agree this isn't crucial with the teaser.  In my case, I was trying to confirm the whole dataset's format was identical to the teaser, but it was too large to download.  I thought we could switch to a superior format because we're going to republish them anyway for fixing other bugs.\n. Is is_keyword_here.key_word_product declared as a deepdive.schema.variables?  The histogram centered in the middle tells that DeepDive was not able to learn anything usually due to an empty training set.\n. Closing this for lack of response. Please reopen with details if needed. Thanks!\n. Hi @vsoch, a new release is out (v0.7.1) which removed the plpythonu dependency for postgresql.  Can you see if that helps?  You can use the same installation instruction to get the latest release:\nbash\nbash <(curl -fsSL deepdive.stanford.edu/install)\n. Let me know if you want to look into any of your problem together.  I can Hangout/Skype later in the afternoon PT.  I'll send a direct email.\n. I remember using v0.7.1 resolved this issue (although we had to use more workaround for SSL).\n. There's no way yet.  I agree deepdive run should be idempotent.  I think the semantics of ddlog programs should be defined as starting from a state where all intensional relations are empty.  We could try to detect which relations are intensional vs. extensional, i.e. defined by ddlog vs. whose data come from external sources, or provide a syntax (or just annotation?) and truncate all intensional relations upon every run.\nIf there aren't use cases for running multiple functions to fill up one relation, e.g., table1 += f(...) :- ... .  table1 += g(...) :- ... ., then we could simply truncate (or drop/create) the output table for running functions for the moment and fully expand this EDB/IDB distinction later.  @feiranwang How does this sound?\n. Sounds great @raphaelhoffmann. Thanks for exploring. \nI think we should add a separate command or commands that create/enumerate/drop the schema or versions rather than overloading deepdive initdb.  We can discuss more on what operations are needed with versioning and at what granularity for completeness.\nNot being able to support MySQL is okay. We just need to provide a graceful way in the driver. \nI wish we could completely drop the use of JDBC and have Scala rely on deepdive-sql eval command as well.  This would allow us to concentrate our effort.  Not confirmed how big this change would be.  I'm good if there's a workaround for this, but ssl validation support for example also requires extra code for JDBC. \n. @raphaelhoffmann Great to hear it's working!\nYou can in fact already invent your own annotation and use ddlog export-schema's json output to figure out user's table.  However I agree we should decide on a fixed annotation recognized by the deepdive subcommand that begins a new schema, sets up the search path, and drops such tables to make it visible.  It seems you changed deepdive initdb's semantics and added createdb, but I think it's better to keep initdb and add a separate command.  Moreover, this SCHEMA stuff is postgresql-specific, so it should go under the driver.  I'll have to put a little more thought into this.\nTaking out JDBC seems to be more difficult than I first thought.  It's quite deeply rooted and many parts are entangled with it, especially tests are heavily tied to JDBC, and json_extractor, calibration plot, piggy_extractor are relying on fancy type introspection.  However, in almost all meaningful cases, we just need a count or boolean back, so I'd say it's desirable to get rid of JDBC.  Migrating some parts out of scala and dropping some features temporarily will make this easier.\n. Thanks for making these fixes.\nI think the dataset README is still confusing to just point to the general walkthrough.  How about pointing to the opendata's \"Data Format (NLP Markups)\" section and enumerating the list of columns there under the first bullet?  In fact, the opendata TSVs are slightly different from the walkthrough: they have provenance and word index columns, sentence id appears after document id, the dependency paths are represented differently using two columns.  I think these datasets are often consumed/prepared by people who have no idea/interest in what they're processing or how they are actually used in DD, so I think we need to be very precise in describing the exact schema.\n. Looks great!\nOn the first point, I meant there may be professional DBAs involved who prepares the database for the actual DD user, who don't necessarily want to follow the details of DD tutorial.  Anyway the new schema page looks great.\nI completely agree the tutorial should be updated at one point to include Bazaar and embrace its format.\n. Thanks for your fix!  deepdive.conf is in HOCON syntax, which doesn't require double quotes, but it's nice to have them for consistency within that example.\n. Another incident of confusion here: HazyResearch/dd-genomics#261.  Apparently, not having --sample-evidence on by default confuses even the most sophisticated users of DeepDive.  I'd say keeping this off by default is a terrible design, and we should rather communicate the important distinction between training/testing in a more explicit way, e.g., by providing extra views to inference results for all candidates while keeping the current views intact.\n. Thanks for fixing this bug.  Merging with a minor bash style fix.\n. Documentation update looks good.\nA few high-level comments:\n- Ideally, we should get rid of run.sh, and have deepdive run accept a phase (mode) option that gets passed down to ddlog compile.\n- Most of setup_database.sh lines should be moved to input/init.sh, and deepdive initdb should be used instead of createdb in 0-setup.sh.\n. @juhanaka awesome!  You'll need to see https://github.com/HazyResearch/ddlog to fix the compiler.\n. The new type checker for supervision columns seems to be the cause. I think using undeclared relations shouldn't be allowed eventually but we can get the type checker quickly fixed to skip undeclared relations for now. @seojiwon Can you make a fix?\n. @raphaelhoffmann Thanks for reporting.  I'll create a maintenance release tomorrow.\nBtw, do you think it's useful to have an automated binary build of master (and every maintenance branch, e.g., v0.7.x)?  It can free you from going through a tedious source build and we won't have to create a bunch of hotfix releases for these small bug fixes.\n. Unfortunately, you have to create two DeepDive applications for that, first running one for A, then taking extractions to run another for B.  However, we strongly recommend to stay within a single app to do \"joint inference\" by expressing the correlations between all variables in A and B within one factor graph.\n. Here's the script in an ancient branch that was going to replace the generated exec_parallel.sh scripts, avoiding materializing any split inputs to do udfs: https://github.com/HazyResearch/deepdive/blob/netj-inmemory-extraction/util/run_parallel.sh\nThe extraction part needs a complete rewrite IMO. Dependencies don't make sense, parallelizing is suboptimal, akka fills up the log with noise. Especially Splitting into fixed size chunks incurs unnecessary udf process startup costs, and spilling them requires extra storage.\nIf this is a blocker for you, I think you can workaround using the above script.  Make sure you use the c branch of mkmimo. \n. We now have a dedicated UDF doc that implies how to plug in these general extractors, then highlighting our Python support.  Maybe we can write another section about general udfs.  The new spouse example in fact already includes such non-Python but Java/Scala extractor (CoreNLP):  nlp_markup.sh with its DDlog counterpart.  So it's really a matter of writing about them, or maybe the page already is clear enough?\n. @vsoch The ids are supposed to be assigned before running your holdout query.  Not sure why it didn't.\n@feiranwang Any idea why the ids for the variables table can remain null when executing the holdout query?\n. Sorry, the current dependency handling is largely broken, and most power users so far preferred to manually manage their pipelines.  I completely agree this shouldn't be the default, and we'll get this fixed pretty soon as we get rid of Akka.  We're going to refactor the extractor runner part to easily support various compute environments other than local processes, so we'll definitely take dependencies more seriously.  Your help\u2013code or comments\u2013will be appreciated!\n. Could you describe ideally how you want to run just the inference with the learned weights, e.g., at the level of deepdive command? Maybe we need a separate deepdive learn and/or deepdive infer commands? But you'd want to run extraction for new variables right?\n. Done by #481 \n. Done in https://github.com/HazyResearch/deepdive/blob/3997a8d3b9fd78ff3fe8b3bbd86803889d411451/doc/ops-model.md#reusing-weights regarding #481 \n. LGTM\n. Looks good.  Now the grounding code looks even more scarier, but it'll hopefully get ironed it out as we migrate to the new data flow compiler architecture.\n. Does the sampler already support the Imply3 factor or did you forget to add the submodule changes?\n. LGTM\n. @SenWu could you reflect this change to the docathon-v0.8 branch?  I think we should primarily document the original mode, and keep as a small note of the difference for inc mode at the end.\nhttps://github.com/HazyResearch/deepdive/blob/docathon-v0.8/doc/factor_graph_schema.md\n. Done in 1831e39ceeea9041e9bc77f32b35809376b22051\n. Looks good other than that one inline comment.  We should do the same for Mac btw.\n. Awesome!\n. See the developer's guide linked from that page.\n. The link is buried in the middle of the text and not very visible.  We'll make it clearer.  Thanks for your feedback!\n- [x] add a build from source section to installation page\n- [x] document build dependencies in developer's guide\n. Done in e54092a76a3efb75889c704fdd817fb9fe3a9379 and f8e1353d8d7406adfdc2ce0fdcb592ef8772a412, and they'll appear in the next release.\n. You need to have libnuma-devel installed and use gcc >= 4.8.  These were recently added dependencies to the master branch as we started to build sampler from source.  You can also stick to the v0.7.x branch which don't need them.\n. Btw libnuma-dev gets installed on Ubuntu when you ask the installer for build dependencies.\nAnd now installer knows which yum packages to install on some RedHat-based distros.\n. Thanks for reporting.  This has been fixed in the master via 49c0bd478d22fc26ae4913c0add94ca3577311d7.  You could also merge or cherry-pick it in the jdbc_removal branch.\n. Thanks for adding this feature!  Can you do a make checkstyle and also rebase to the latest branch in addition to my inline comments?\n. @ThomasPalomares Please rebase onto 959282f11c6265c8f85ea9327eba9d0b85f46408 as the latest commit does not fully pass the test yet.  Sorry!\n. LGTM\n. FYI Here's the SVG rendered data flow for spouse example and data flow for chunking example.\nI forgot to mention that this PR requires no change to the user's app code, except maybe a few extra dependency information that were missing in the manually written deepdive.conf.  The run.sh that typically handled env setup and many other hacks should be moved to the right place such as input/.  @ThomasPalomares and I were able to get the genomics app (in DDlog) working with minimal changes.\n. @zifeishan Regarding multiple SQL statements, I'm reluctant to do a blind split on semicolons, as any string literal may contain it.  Do you know why PGXL is not supporting them or any workaround?  One safe way for DD to deal with multiple statements is to support an array of strings for the sql field for sql_extractors, although this will break backward compatibility (but who cares! : )  How does it sound?\n@raphaelhoffmann Thanks for your detailed feedback!\n1. The runtime dependency build hasn't been thoroughly tested, hence such bugs.  Specifically on graphviz, I feel the build can be simplified without pulling in X11.  Will come up with a fix.\n2. The submodule build has been modified to honor the developer's working tree, so whatever is on disk gets built, as opposed to forcing all submodules to be the exact version recorded in deepdive's commit or the git index.  So forgetting to run git submodule update --init before a build can now result in an unexpected result.  Previously, running build without a git add path/to/submodule wiped out changes to the submodule, upsetting people who were modifying them.  The previous behavior seems to be upsetting less number of people, so I'll revert that or maybe abort the build instead of silently making surprises under the hood.\n3. The terminal messup is due to the pv command showing progress bar, competing with the logs printed to the same terminal.  I tried to reduce that error but haven't figured out how to completely eliminate that.  As a workaround you can run reset whenever that happens by blinding typing it followed by an return/enter.  We need a tty expert here.  There should be a simple invisible escape sequence we can print to ensure the terminal is restore in a sane state.\n4. mkmimo had a busy waiting bug on OS X where the poll(2) syscall returns too fast with slow sinks.  I added a fix for it in f2fabb16cd5b6efab237bb7bf9ec29c97971d851, but kept it separate to keep this giant PR stable for review for a while.  Since we've got some eyes and hands over the code, I can bring in the latest changes, but that'll require another round of tests to be safe.\n. @zifeishan If this is a blocker, we can quickly apply the workaround to pgxl driver, which is actually super simple.  db-execute can call psql multiple times after splitting the query argument by semicolon and that'll handle everything.  Wanna take a stab at it?\n. @zifeishan Interesting.  However, that'll take away stdin for psql, which is problematic for db-load and other potential use cases. Is there a way to tell psql a sql file to run instead?\n. @alldefector Sorry the conversation went private through Slack.  Yes, that's exactly our plan, except using bash processes substitution to turn $sql into a readable file.  This is so much better than dangerously splitting the SQL queries in a sloppy way.  (btw you mean the block is run as a transaction, right?)\n@zifeishan Could you make the changes, confirm it working, and add the commit to this PR?\n. @feiranwang Can you try putting a .done suffix to the target? Maybe I forgot to add that case. Will also take a look into this. \n. @feiranwang It turns out to be a missing schema.json issue, so I converted schema.sql into app.ddlog as an easy way to generate the json file.  The compiled Makefile was missing some dependencies for the mentioned target, which gets generated from the relational schema.  I fixed things to show such errors more transparently, such as:\n$ deepdive plan process/grounding/variable/person_has_cancer/assign_id\nmake: *** No rule to make target `data/person_has_cancer.done', needed by `process/grounding/variable_id_partition.done'.  Stop.\nError in dependencies found for process/grounding/variable/person_has_cancer/assign_id\nIdeally, these should be caught in the deepdive compile phase.  It'd be nice if you can contribute a checker that prevents this kind of error.  Also, it seems we're missing a test for the smoke example.\n. Regarding the future of Scala code, I'm still deciding whether to drop it even earlier and keep the latest in something like 0.7.2, or to have it coexist with the compiler in 0.8.0 and drop it immediately after. In any case I'll make sure the latest including this fix is readily usable. \nI haven't put much thought if the grounding sql queries from the new compiler will perform well in pgxl. At least there's no update anymore. Curious to hear any success or failure story there. \n. You know I prefer completely hiding them. However do you think it's okay to always require a supervision rule that fills the labels from a separate data source? This may be in fact a good thing that discourages direct supervision with no practical difference. The only things that appear more complicated are the toy examples. If you agree I'm happy to revert the last commit that introduced this inconsistency. \n. @alldefector Your point makes a lot of sense that it'll be a bigger surprise later.  I'd love to drop it asap to move forward.  Anyone else has concern/objection here?\n@feiranwang Of course. There'll be a nice changelog that summarizes all the user facing changes, but in a subsequent PR or commit for preparing the release.  In fact, there are so many inconsistencies to docs introduced by this drop.  Many doc pages need rewrite before the release.\n. Compression certainly has overhead.  The question is whether it'll be a bottleneck.  I'm trying to ground a larger one by running spouse example on a larger corpus I synthesized.  But it revealed mkmimo's lower throughput, and running much slower than expected.\nMeanwhile, here're my notes from doing a quick overhead test with several choices: https://gist.github.com/netj/c6f15bb78ff3a52057cb\n. Before I forget, I'll drop some numbers I got a while ago for a large factor graph I synthesized by duplicating the corpus for the spouse example (~12GB uncompressed, 199k vars, 16k weights, 337M factors).\nLOADED VARIABLES: #199907\n         N_QUERY: #139603\n         N_EVID : #60304\nLOADED WEIGHTS: #16664\nLOADED FACTORS: #337742718\nFollowing are rough measurements on raiders6 with 111 processes, only accounting the dumping time and loading time.\nuncompressed\n\n11828322038 bytes (~12GiB)\n401.224535 secs\n\npbzip2\n\n197572897 bytes (~191MiB; 59.8x smaller)\n420.276131 secs (+19s; +4.7% increase)\n\nbzip2\n\n195875810 bytes (~189MiB; 60.4x smaller)\n464.805231 secs (+64s; +16% increase)\n\nSince the full grounding took significantly more time (materializing the factors, weights), I'd say compression overhead is negligible while it's savings on storage footprint and in turn I/O are quite dramatic.  The higher-than-usual compression rate (>>10x) is probably due to the regularity in the factor graph data representation.  I think we should turn this on by default unless there's a really good counter argument.\n. I thought we were dropping Scala, so this literal PR won't go into 0.8, but into 0.7.2 or so.\nReusing learned weights is one of the two missing features in the new compiler (the other being GP's parallel unloading/loading).  I'm planning to implement them pretty soon but other priorities are starting to invade my calendar so it may take more than a week.  However, I can make sure this kind of UPDATE does not show up in the compiled grounding code by doing a simple OUTER JOIN when dumping the weights.\n. @zifeishan Sure, I'll backport most changes relevant to Scala code to 0.7.x and release 0.7.2.  So let's get this in to master, then freeze the Scala codebase in master.\n. LGTM Thanks for this fix!\n. Sorry for the confusion.  That feature (#184) has been dropped a while ago due to a bug (#204) and the documentation/example hasn't been updated properly.  For the moment, please stick to using a single --reg_param option.  We'll update the doc and examples very soon.\n@zifeishan @zhangce @feiranwang Can you fix this easily or should we completely remove it from docs & examples?\n. It now takes multiple, but just uses the last.  See #204 for automatic selection of the parameter.  Closing this instead.\n. @chrismre @HazyResearch/genomics This is the fix for the throughput issue @ThomasPalomares explained in #453 \n. Nope, Travis also confirms everything's fine.  The test failed fast when prior to our fix, but since the fix it was INSERTing too much rows unnecessarily and made the test too costly so I tweaked it a bit.  Please merge if it LGTY\n. @raphaelhoffmann I'll bring the fix into master and v0.7.x soon.  For the moment, you can manually point your submodule to it.\n. DD0.8 currently does not support learned weight reusing yet, so no it doesn't have the issue now but should be careful.\n. Yes this is super important and I'm trying to come up with the right way to do this hopefully before this release. @raphaelhoffmann and I synced up on this. The fix to Scala code base won't be portable to 0.8. \n. Is this applicable to ddlog? Or when using deepdive.conf directly?\n. The internal ids are at least always bigint I believe but for other fields giving warnings sounds good. \n. See also #231 for past attempts of Docker support.  We will be looking into this at some point, but if you can contribute a Dockerfile that would be awesome!\n. @ajratner  ping?  Can we get this reviewed asap?\n. This is fixed in v0.7.x branch as 49c0bd478d22fc26ae4913c0add94ca3577311d7.  Duplicate of #443.\n. For functional dependencies, DeepDive can recognize columns of variable relations without @key annotations if it is present on any column.  However, It's not clear to me how exactly such dependencies should be grounded for the sampler.  Should they be turned into categorical variables, or attach oneIsTrue factors to them?  Can someone clarify a bit for me?\n. Right, unless we turn it into categorical, there's probably no difference in the blowup.  This sounds also doable at the DDlog level, by desugaring such cases.  I think you'll want to correlate such categorical variables with other boolean ones, but I believe that is currently not possible.  I think we'll need to add a few more ways to mix them as well to make this actually useful, implications, etc.  Please correct me if I'm wrong.  With my limited exposure to such use cases, the current Categorical/Multinomial support doesn't really typecheck with the rest in my head, so I wish someone could clarify everything with a full blown example.\n. Thanks for merging but it was failing at Travis.  Not sure why it's failing there, but in the future, let's only merge when everything's green.\n. @ajratner This now has all green lights.  You can merge this whenever you want and I'll merge master into docathon-v0.8 so everyone's on the latest code.\n. Updated ddlog submodule.  PTAL :)\n. You can simply point your app to the right postgresql://user:password@host:port/dbname in db.url if you're using 0.7.x. \n. Please take a look, comment, then merge once everything looks good to you. Standard code review :)\nYou can then perhaps rebase your branch. \n. It'll largely be up to the driver but that'd be nice feature to have. compute-status for example can show all running processes for the config, right? I'd recommend hiding private commands under a clear namespace such as compute-remote- and let others be end user friendly by default. \n. Rebased, fixing a few merge conflicts.\n. @raphaelhoffmann You mean feeding the deepdive-sql's  output directly to the format_converter/wc processes works fine but hangs with the run/process/**/dump/run.sh?  I couldn't easily reproduce the slow progress.\nThere apparently is an mkmimo issue on Mac.  Non-blocking mkmimo seems to create sudden high load to the CPU, then reboots every Mac, probably because of heat/temperature surge?  I'm investigating how to fix this, but I'm hoping a multithreaded implementation with blocking I/O could mitigate this.  For the moment I recommend keeping the DEEPDIVE_NUM_PROCESSES low like 3-4.\n. @alldefector mkmimo crashing Linux? I haven't experienced that myself yet. The potentially low throughput may also be due to a database issue. \n. @raphaelhoffmann Thanks for testing and reviewing it!  I'll press the button myself.\n. LGTM  Thanks a lot for making these edits!\n. Why was this closed?  I think the edits are great.  The Travis error is due to trailing spaces, which can be easily fixed.\n. I think this was fixed in docathon branch by putting NULL::BOOLEAN. I'm not entirely clear if this is ok right now (potentially putting duplicates) but this is how it should be written in my view. \n. Could you post the error line, for reference?\n. Can you run ddlog print app.ddlog and see if the brackets are correct there?\nThis may be a bug I introduced while reducing the number of generated parentheses in the SQL..\n. Have you tried a clean build of the new commit? The header dependency is not accurate and resulting in corrupted executable when built incrementally..\n. Thanks for such quick yet thorough proofreading!  It reads so much better now.  Just adding a bit more edits of mine and merging.\n. Looks like the long names truncated can cause some unintended consequences. We'll need to support ways to prevent this by hashing the full name to a fixed length etc. along with a way to annotate rules with a user defined name. \n. This seems related to #276\n. Sorry I was sidetracked the entire day after a quick fix attempt.  It's apparent that I left a bug in the sequence assignment for GP and never noticed it because db-driver currently lacks tests for many parts after getting a quick expansion.  Moreover, we don't have automatic tests for GP.  We should figure out how to install/run GP on Travis.\nSampler resorting was ridiculous so I already proposed a fix, exploiting the fact that we assign ids from a consecutive range beginning from zero: HazyResearch/sampler#14.  As long as the sequence assignment works the sampler should be working fine.\nWill fix this asap.  Sorry for alerting so many people for this silly bug!\n. @alldefector PTAL\n. I'm figuring out the best way to deal with these conflicting/lacking shared library issues.  Something wasn't completely buttoned up in the latest release as well as the result from the source builds as you can tell, and I'm currently working on that in the background.  Greenplum has it's own environment setup script that's recommended to be sourced, but that often breaks other system software.  For that matter, my recommendation is to use a set of wrappers over Greenplum executables to isolate the necessary environment changes, instead of contaminating your normal shell environment.\nRegarding GLIBC version symbol issue, I honestly don't know how to control it or if it's even controllable.  If you build on a recent distro, your binary ends up requiring some of them, which is very undesirable.  Any input on this end would be appreciated!\n. @ThomasPalomares can you expand the test to include the input that triggered the bug and expected output?\nBut wait.. I was looking at deepdive_udf_util.bats and it seems it's just a duplicate of deepdive_sql.bats without the relevant tests!  @ajratner I thought you added some tests somewhere.. or maybe I was hallucinated?\nIt should be fairly simple to tailor that bats file to test ddlib.util if we just take the NastyTSV and feed it to an identity Python UDF that uses @tsv_extractor and @returns, etc.  Moreover, this test can be one level up, i.e., directly under test/ since it's not database dependent, or we can even move it to ddlib/test/.\n. @ajratner Is the bug confusing NULLs in arrays as empty strings?  I gave up this case because I was relying on the csv package, but if we're parsing commas ourselves, I think the problem boiled down to a matter of distinguishing ...,,... from ...,\"\",....\n@ThomasPalomares I think the new test is too large involving too many other parts, essentially everything from deepdive: ddlog, compiler, runner, local compute driver etc.\n- [x] Can't we just feed the NastyTSV directly to the identity.py, then check whether it outputs the same thing?\nbash\n@test \"ddlib.util (@tsv_extractor and @returns) works against nasty input\" {\n    diff -u <(echo \"$NastyTSV\")  <(python \"$BATS_TEST_DIRNAME\"/identity.py <<<\"$NastyTSV\")\n}\nidentity.py may be even just inlined in the bats file with -c.\n- [x] The rest of the content of the .bats file is duplicate and should be removed.\n- [x] Also, the test is not dependent on any database, so let's move it out of test/postgresql/, say under ddlib/test/.  You can look at other subdir's env.sh.\n. @SenWu Let's try to get this online asap.  My notes and scripts for raiders kept in https://github.com/HazyResearch/greenplum-howto may be useful for Travis as well (gpdb.install.sh).\n. Thanks!\n. Thanks for the clear steps!  I'm planning to introduce a higher level operation that integrates this PG schema support with the rest of the process timestamps etc.  This will be super helpful for that direction.\n. @alldefector The docs are going to be updated by #506, but we can definitely add some checks to the example script.  Another big pitfall is not having wget installed on Mac, which leads to silently passing over a Bazaar/Parser setup error (the english SR parser download).  Will propose a fix there too.\n. @henryre Thanks for suggesting the edits!  To make it work completely from scratch, we need some revisions.  We may need to add/revise commands to keep things simple though.  Why don't we first make the docs complete as I pointed out, then simplify it in a later PR?\n. :) Glad we didn't merge this earlier prematurely as it provided strong motivation for improving other parts.\n. You can use deepdive redo instead if it has already been done.  I think the tutorial was assuming everything was done the first time.  Do you think we should update it as well?\n. Thanks for the suggestion, but it'd be better if you could create a pull request.  I won't fix this as tuple vs. list isn't very important in this context.\n. @igozali Sorry the code has been sitting here forever.  Although many parts can be improved, I think it's in a relatively good shape.  It's probably better to improve it down the road based on what we see from more instances of concrete compute drivers rather than hypothetical ones we imagine now.\n. @alldefector Nope.  Back then @zifeishan and I confirmed this was happening on a data path that involves just gpdb's psql.\n. This no longer seems to be the issue\n. I may have forgotten make became a runtime dependency. Will push a fix fairly soon. Thanks for reporting!\n. Thanks for your input @xiaoling.  This has been already added as a build dependency in the installers.  Maybe it's time to ask your admin for a:\nbash\nsudo bash <(curl -fsSL git.io/getdeepdive) _deepdive_build_deps\n. Hi @naddou14, not exactly sure what you mean by custom annotations, but you can think of most pieces DeepDive offer for data processing and statistical inference at the mention level as doing the \"custom annotations\" over the corpus.  We use different terms like \"mention level extraction\" or \"candidate mapping\" but they can be viewed as the same things.  The spouse example used in our tutorial is basically decorating the corpus with a custom \"mention of spouse relationship\" annotations.\n. If those are output as extra NER tags, you'll be able to access them from DeepDive.  Spouse example shows how to run CoreNLP through Bazaar/Parser and use the PERSON NER tag to map candidates. It sounds like you can just plug them into DeepDive?\n. There is a known rebooting issue with the latest OS X (10.11) that we're looking into. It's caused by a component called mkmimo which streams the data using nonblocking IO, and it seems latest OS X's kernel has issues with high rate of poll syscalls possibly with a combination of hardware (MBP). We don't have a clean solution yet but you can either use a Linux machine instead or tune the following env variables to keep a reasonable throughput while not crashing:\nexport THROTTLE_SLEEP_MSEC=10 # higher the safer but slower\nexport DEEPDIVE_NUM_PROCESSES=1 # lower the safer but slower\nI'll update here once we find a fix for this. \n. Yes, you'll have to remove that part from the installation for the moment.  I'll push an update soon that gets rid of it, and hopefully a new version of mkmimo that mitigates this issue by default.\n. @lanphan You can increase the THROTTLE_SLEEP_MSEC parameter to make it less likely to crash, but the throughput will become awful.  Since you're eagerly looking for a solution, let's try some workarounds we currently have.  These all involve replacing the mkmimo executable installed under util/ of your DeepDive installation.\nFirst, let's keep a backup:\nbash\n(set -eu; cd $(deepdive whereis installed util/); cp -pf mkmimo mkmimo.orig)\n1. If you clone the fix-for-mac-reboots branch and run make, you get a mkmimo executable for replacement.\nActually, you can just run the following command to patch your installation, assuming deepdive is on your $PATH:\nbash\n   (set -eu; git clone https://github.com/netj/mkmimo.git mkmimo-wip --branch fix-for-mac-reboots; cd mkmimo-wip; make; install -v mkmimo $(deepdive whereis installed util/mkmimo))\nWith this one, you can use a higher value for THROTTLE_SLEEP_USEC (note this is *_USEC in microseconds not milliseconds) without sacrificing much throughput in some cases, e.g., export THROTTLE_SLEEP_USEC=100 which is 0.1ms.  10 gives good throughput but crashes quite often.  You can try higher values like 1000, 10000, 20000, or even 100000 to be safe at the cost of some throughput.\n2. If it's still hard to find the right parameter that doesn't crash your Mac, or you just want something that functions, try this dumb version written in bash.  It's dumb and inefficient incurring a lot of disk I/O but should get you through the data flow without crashing your Mac.  You can download it and replace the util/mkmimo file, making sure you turn on the executable bit.\n   The following command does what I wrote above:\nbash\n   (set -eu; cd $(deepdive whereis installed util/); curl -fRLO https://github.com/netj/mkmimo/raw/bash-impl-poc/mkmimo.sh; chmod -v +x mkmimo.sh; install -v mkmimo.sh mkmimo)\nFinally, if you want to restore the backed up original, here's the oneliner:\nbash\n(set -eu; cd $(deepdive whereis installed util/); install -v mkmimo.orig mkmimo)\nHope this helps!\n. @lanphan Glad to hear that it works fine on Linux. There's no throttling done on Linux (those parameters default to zero) so the versions we tried on Mac won't have much difference. Actually it may have marginal improvement so no harm trying. The same instructions can be used.\nIf you're using Postgres, increasing the DEEPDIVE_NUM_PARALLEL_UNLOADS and DEEPDIVE_NUM_PARALLEL_LOADS from 1 to 3-4 may give you some more speedup. \n. @lanphan Yes those same flags work with different database drivers.\n. 1. You can preprocess the data in anyway if you have a .sh script under input/ as we do for input/articles.tsv.sh.\n2. The doc_id that appeared before the ERROR line wasn't necessarily the input at the moment since they all run in parallel.  With a quick grep '\\\\r' *.jsonl you can see many articles contain \\r.  To filter these, we actually had a gsub in input/articles.tsv.sh of spouse example that takes care of this.  Maybe we should add a comment to make this more apparent.\n3. DeepDive currently doesn't checkpoint within a single \"process,\" but it does once you finish and move on to the next one.  However, this would be a useful feature for development. We'll think about how this could be done without sacrificing efficiency too much.  For other types of compute resource drivers that are coming along, this partitioning and checkpointing will become the default so you can run and resume things idempotently.\nThe intended way to run the whole corpus of signalmedia-1m, is to put the directory under input/ so the file sits at input/signalmedia/signalmedia-1m.jsonl. That way, the input/articles.tsv.sh will pick up the .jsonl file and apply the necessary filters.  You may want to remove the grep commands to not drop any articles.  Please reopen if you find more issues.\n. Yes, I confirmed there seems to be issues with the existing \\r handling within jq.  I'll fix this asap and update here.  Meanwhile, you could just add a good old sed line, which is probably going to be safe and more complete than before keeping carriage-return-phobic PostgreSQL happy:\n``` bash\ncat \"$corpus\" |\ngrep -E 'wife|husband|married' |\nhead -100 |\njq -r '[.id, .content] | @tsv' |\ntake care of carriage returns\nsed 's/\\r//g'\n``\n. I don't think the errors above are from graphviz or so, but due to [dashbeing/bin/sh`](https://wiki.ubuntu.com/DashAsBinSh).  I'll look into this soon.\nThe pbzip2 errors on the other hand can be ignored as it's just complaining about empty files.  Often times empty parts are produced when there's a skew in the parallelization, esp. with smaller dataset.  If you used the mkmimo.sh, then it's not doing a good job spreading the work across processes, so you probably had greater chance of getting empty parts.\n. Possibly.  Please try again after installing the latest stable release.  mkmimo has been updated to a version that probably won't crash your Mac any more.\n. In most cases you can just overwrite by running the installer again. However it may be safer to put your old one aside and just repeat the installation. \n. I believe this is no longer an issue since the fix with multithreaded mkmimo.\n. Please use db.url greenplum://. Another reason for #494 \n. Not sure about the exact cause, but sometimes scala dependencies can go corrupt or the jar downloads sporadically fail.  Maybe you can retry after removing ~/.ivy2?  Soon we're going to strip the thick glue code and introduce a more direct way to use CoreNLP.  However, I don't see an obvious reason why this shouldn't work in Docker.\n. We will soon merge #566 (a thin wrapper for CoreNLP) and Docker compatibility changes, which will get rid of these issues.\n. Thanks a lot for the work!  I put a bit more tests around it (turning them into .bats) and merged after fixing another inconsistency in Python3: map(None, l1, l2, ...)\n. @alldefector I share the same high-level concerns about complexity with you, but the rest of your arguments don't seemed to be factual or convincing to me.\n1. First, the consecutive id assignment step itself isn't a major bottleneck.  Joining the assigned ids back is clearly an overhead, but it's what we pay for higher performance later.  Moreover, using a different method that computes unique ids for variables and weights may not necessarily be cheaper as @chrismre mentioned.\n2. Consecutive sequential id is crucial for the sampler to keep its sequential access pattern.  Otherwise, the sampler itself will eventually have to do similar graph rewriting from each fingerprint to a compact space at loading time. (unordered_map can't be used literally in the code.)  Without the id assignment, grounding is just deferring the problem to a later component, not solving the impedance mismatch at the right place and moment.\n3. The user-space to array-space mapping is the core of DeepDive's grounding.  You can't take that out and let user be responsible for it.  If users already have matrices and arrays, nicely mapped from their raw data, why bother using DeepDive and not Julia or a ton of other packages?  I see DD's core value as helping users construct such array-space objects for analytics without having to deal with the underlying data management plumbing.\nRegarding complexity (also to @chrismre): I don't think the current implementation is complicated at all; It's just linear to the complexity it has to handle.  For every V, F, W, D (scopes for categoricals) to be grounded, the compiler has a SQL template and a few pipeline before/after to map user's spec to a SQL query.  Then to deal with the difference Boolean vs. categorical/multinomial introduces to each of those, doubling the SQL templates from four to eight.  Of course there are hidden/intermediate data for the real work of id assignment, each of which also has a SQL template, making up to about 10 SQL templates.  So, if you think keeping all of these ~10 SQL templates in a single file is really what makes it difficult to reason about, we can easily split into smaller pieces by literally cut/pasting them into separate files.  Grounding was just put in one blob initially while others were sliced and diced as I wasn't sure what separation-of-concerns would be ideal for the future.  It seems we can at least separate it into three now: the Boolean/multinomial cases and the common steps for both.  (@zifeishan may be able to help make this happen sooner)\nOne interesting direction to explore is turning the id assignment (hence grounding) into two phases: first use fingerprint to deal with user-space to fingerprint mapping; then have the second phase deal with fingerprint to sequential id mapping--which sounds nearly identical to the current grounding but hopefully simpler as we can contain complexity in user's data/code in the first phase (many vars, factors, etc.).  The second phase may become simpler and easier to understand as it focus on the core problem: taking a simplified table from the first phase, mapping fingerprints in V, W to a sequential ids, then rewriting fingerprints in F to sequential ids with the maps while generating input for the sampler.\n. Looks much clearer!  Glad to see another example of less code making things more general.  I forgot what bug is still there?\n. @alldefector Yes I was testing things myself, and it took a while on my new Mac.  I don't think we can increase Travis timeout nor would it be a good thing to have to wait for an hour per test, so let's reduce the data size.\n. I think we can shrink things to 20% or less (just on Travis or by default) and lower the F score bar for chunking (getting ~0.65-0.7 for that size) since we're mainly interested in whether it works end to end.  I can modify the travis.yml/bats if you want.\nexport SUBSAMPLE_NUM_WORDS_TRAIN=1000 SUBSAMPLE_NUM_WORDS_TEST=200\nI'm also noticing that spouse example now takes significantly longer than before on my Mac.  postgres stuck at materializing the JOIN with dd_variable_* table.\n. I meant we can set the time zone in the psql that feeds input to the script such that the UTC offset we add is going to be always correct. The script doesn't have be general since pgtsv_to_json is really just a shim for working around a limitation in some db-drivers and not intended for general use outside of it. \n. I don't understand why anyone should ever want to use timestamp without time zone (it doesn't make sense as it's a reference to an absolute point in time and not subject to interpretation). http://stackoverflow.com/a/6158432\nIt's fine to use any time zone in the JSON output as long as we are consistent with the data source. Since pgtsv to json is used only once in our codebase https://github.com/HazyResearch/deepdive/search?utf8=\u2713&q=pgtsv_to_json, we can just put the following line before the COPY statement to ensure we get timestamps serialized in a certain tz\nSET timezone = 'UTC';\nNot sure why pgtsv to json needs any change from the first version. This is a problem in the data source not the data encoder. \nBTw JSON defines one and only one standard datetime format: ISO8601 which is also used by XML Schema, etc. so i agree we shouldn't produce some nonstandard format. It's no longer JSON if we try to sneak in some weird date time format. \n. Ok make sense now. Keeping a way to pass thru incomplete data may be something useful too anyway. \n. LGTM \n. Looks great!  Should we also get rid of the categories column in the weights view here, I remember you flagged as unnecessary in a previous commit?\n. Not sure if it helps but ddlib.util has gone through a similar pain https://github.com/HazyResearch/deepdive/pull/498\nGiven the complexity of the codec I think we should just ditch PG TSV and switch our wire format completely to JSON at least for non primitive values\n. LGTM.  PG TSV won't be dropped anytime soon.\n. Is the switch to csv for distinguishing NULLs in arrays? I remember python's csv reader is fundamentally flawed for distinguishing empty strings from nulls (,\"\", vs ,,) in arrays.\n. Just confirming it's a hard problem and pointing out that nulls inside arrays are encoded differently from values outside.. at least for TEXT. Not sure about csv. Csv is generally not friendly to standard UNIX tools and many scripts including mkmimo that rely on one-line-per-row.\n. I now remembered why we couldn't use CSV or at least Python's csv.reader.\nFirst, let's look at TSV for the values that give us headache:\npostgres=# COPY (SELECT 'NULL', NULL, E'\\\\N', '', ARRAY['NULL', NULL, E'\\\\N', '']::TEXT[]) TO STDOUT;\nNULL    \\N  \\\\N     {\"NULL\",NULL,\"\\\\\\\\N\",\"\"}\nAlthough the TABs are invisible, you can tell each value has a distinct encoding in TSV that can be relatively easily parsed by repeatedly splitting and unescaping.\nNow, let's add CSV to it and see how it looks:\npostgres=# COPY (SELECT 'NULL', NULL, E'\\\\N', '', ARRAY['NULL', NULL, E'\\\\N', '']::TEXT[]) TO STDOUT CSV;\nNULL,,\\N,\"\",\"{\"\"NULL\"\",NULL,\"\"\\\\N\"\",\"\"\"\"}\"\nSeems not so bad, so we may try to use Python's csv.reader, but we immediately face a challenge of distinguishing the second field for NULL ,, from empty string at the fourth ,\"\",. (This is a stupid limitation of Python's csv package.  I've looked hard but no alternatives than paying a huge performance overhead by just parsing csv ourselves in Python.)\nYou may think it's possible to use csv.reader if we change the NULL to use a different encoding, but it's impossible to avoid ambiguity no matter what you do without the ability to distinguish a quoted value from ones that are not quoted.  For example, you'll treat user's \\N as NULL if you mess with the NULL AS ... clause.\npostgres=# COPY (SELECT 'NULL', NULL, E'\\\\N', '', ARRAY['NULL', NULL, E'\\\\N', '']::TEXT[]) TO STDOUT CSV NULL AS E'\\\\N';\nNULL,\\N,\"\\N\",,\"{\"\"NULL\"\",NULL,\"\"\\\\N\"\",\"\"\"\"}\"\nSo we shall give up csv.reader and end up with a very slow pg2json piece in Python, or switch to another language.  (Good news is that at least the NULLs in arrays are consistently NULL.)\n\nMoreover, I don't understand how unicode and their escape sequences were magically handled by switching to Python3 and maybe also CSV, but if we're going to get this part absolutely right, I'd argue we have to explicitly handle all of Postgres' escape sequences similar to this: https://github.com/HazyResearch/deepdive/blob/fa1a48fa52044ecac2bfdb538a1e18b80e239f2c/database/tsv2tsj#L32-L41\nPersonally, I'm inclined to completely hiding Postgres' TSV format in the db-drivers and moving all UDFs and other pieces completely away from it.  JSON per line maybe okay but it still has a lot of overhead (repeated column names), so I was prototyping the TSJ format.  PG TSV is a complete mess and the root of all these unnecessary complexity, and I believe we should solve this problem once and for all in the db-driver.  I think the recent PRs fixing the json support is in the right direction, and we probably need to be more systematic.\n. @shahin Can you add some tests that check proper unicode handling?  It seems tweaking tsv2tsj to handle arrays can be a simple way of fixing pg?sv_to_json here.\n. @shahin \n\n\nWhy do we bundle bash with DeepDive? This seems like a very low-level dependency to include in our distribution. As a DeepDive user or new developer, I imagine I'd be very surprised to find out (eventually) that it's not using the same bash that I'm using on my host system. Seems like an explicit dependency on a given bash version would be clearer.\n\n\nBecause it's almost impossible to ask every user to install the exact bash version on their system, we decided to bundle the exact version our scripts need: 4.3.x.  Bundling GNU coreutils is along the same line.  Mac users wasted many hours/days if not weeks because of these since it's bash 3 and BSD.  Docker could achieve something similar, but it rather avoids the portability problem not solving it, and it's too heavy to just get these few megs of portable dependencies (on my Mac: bash is 7MB, coreutils is 9MB).\n\n\nAs long as we're bundling bash, should we be using it for all tests? An exception for a single .bats seems easily overlooked, and now our tests are targeting different platforms. You know more about bats than I do -- what's the best way to run them all on bash 4.3?\n\n\nSure.  The example I commented was just for illustration.  I agree it's better to run all tests with our own stuff, which is already the case for the rest, but bash used by bats was an unfortunate exception that wasted your time.  I think you can prefix the make test in .travis.yml with dist/stage/bin/deepdive env:\nbash\ndist/stage/bin/deepdive env  make test\n. @chrismre Python has even more version issues (py2, py3, pypy, ...) and we already solved the shell version issue long ago.  I don't think it's a good idea to revisit an already solved problem.\n. @shahin \n\n\nMac users wasted many hours/days if not weeks because of these since it's bash 3 and BSD.\n\nWas this before homebrew/macports/fink? All the Mac users I know install tons of packages with GNU/newer dependencies, but I don't think many of those actually bundle bash or coreutils. I wonder if we could revisit this later and simplify our build.\n\nThe audience of the tool has been less technically skilled (scientists, doctors) than typical software engineers around you, and they often don't have control over the compute resources they want to run DeepDive on.  I believe bundling even more, e.g., postgresql clients, would even lower the bar.  Many users are in fact struggling and mentioning they waste most time at installing dependencies we don't bundle, e.g., postgresql clients, which can be easily added with reasonable defaults.\nIf we start distributing releases via Homebrew, APT, yum, then yes we should install the runtime dependencies natively rather than bundling.  However, that requires more maintenance effort (otherwise, your package gets tossed out) and lower coverage (just Mac and Ubuntu/Debian?) than just bundling whatever we depend on.  The bundles build is completely separate from the rest, so you can just comment out lines from extern/bundle.conf to omit them.  I agree the initial build for developers takes too long, so it's a good idea to have developers install all runtime deps themselves, and have the bundled dependencies only built at the time we're creating binary releases.\n. > sorry for hijacking this thread:\n@shahin No worries.  This is a perfect place/timing to discuss the matter.\n\nDo we have other data about our user base that I could check out?\n\nUnfortunately, we don't have a systematically collected profile of user base (yet..).  But it's clear that there are many users who aren't proficient at operating/programming software, just wanting to extract databases from their dark data with minimal effort.\n\n\nMany users are in fact struggling and mentioning they waste most time at installing dependencies we don't bundle\nIsn't this exactly what dependency management tools are for?\n\n\nOur typical users can't install any software via APT, yum, ports, etc. on their compute resource.  They don't have root access, their IT management refuses to install anything, and they can't offer their own machines or cloud instances.  That's what I meant by lack of \"control.\"  No matter how much effort we put into leveraging all those nice infrastructure laid by package managers, a major number of users will need an alternative, more straightforward way.\n\nIn many ways a package manager reduces maintenance effort. Package managers take things like CPU architecture, security patches, bug fixes, API-stable updates, custom shared libraries, etc. into account when resolving dependencies. If we're saving time on maintenance today then it's because we're ignoring all these things, not because we've found a way to do them better than a package manager!\n\nI agree that's the right way, but behind all those rigorous things, someone had to put time and effort identifying and writing correct debian/control files, brew formulae, Portfiles, etc.  Maintaining them requires even more effort as the dependent packages are also constantly evolving and can break your own package in nontrivial ways.  We may consider publishing these packages once DeepDive gets stable enough, but I wouldn't consider package managers as the exclusive way of distributing releases.  I'd still argue for keeping standalone releases that simplifies many users' lives.\n\n\nand lower coverage\n\nThe abstraction we get from ordinary dependency management seems likely to increase coverage, not lower it. If we just specify dependency names then DeepDive can be ported to any platform that implements those names, including the BSDs, Cygwin, or whatever. If we bundle dependency code or binaries, then it'll run only on platforms that we take the time to bundle dependencies for.\n\nDeepDive can already be ported to any POSIX-compliant platform by just running make on its source tree.  As long as the bundled dependencies themselves are portable (most of them are also built from source), DD source tree can produce a standalone tarball exactly for that platform.  For the moment, we just publish two x86_64 binary releases for Mac and Linux (Ubuntu and RedHat).  But we can always target more platforms with a make, e.g., if BASH on Windows turns out to be working great.  I was saying this approach has greater coverage than optimistically targeting a few package managers.  In fact, having a portable source is a prerequisite to publishing packages in many venues anyway.\n\nWhat am I missing here?\n\nI guess you're missing that bundled dependencies are also built from source.  DeepDive installer also takes care of installing some runtime dependencies that weren't bundled via APT, Homebrew, yum, namely Java, Python, Perl that are pretty universally available.  In the long run, I totally agree we should turn all dependencies into proper package manager specs, instead of bundling any in the published package.  (Keeping standalone release is a separate issue)  Anyway, in general, hooking all the dependencies up to the rest of the OS packages may seem trivial, but the devil is always in the details, and it's often a moving target, out of our control.\n. @alldefector Sounds like a good idea.  Can the code be shared?  Or I'd like to just revert all the unicode commits to restore Travis status to green (master has been broken for a while due to this making hard for everything else).  The performance isn't a huge deal as this part should only activate for older PG and GP.  It won't be hard to find a neat way to plug the psycopg2 code into the existing db-driver (db-query).  Setting up a DeepDive-wide PYTHONPATH for bundling may require a bit of thought and work, which I can take care of.\n. @alldefector OK, then I can just restore things in this PR.\nI think we'll soon remove the dependency on psql and other standard PG interface by bundling it.  We don't need to put slow code in the critical data flow for aesthetic reasons. (bitten by that many times)  Btw TSJ is pretty much done on top of standard PG interface in the other branch.  In the future we could rewrite TSJ in a way that bypasses PG TSV, but I think it's going to look like PG UDFs in C/C++ rather than something in Python.\n. It's escaped as per Postgres' TSV or TEXT format.\nYour insert into gives me a different result, but it seems you're inserting a literal backslash, and that's what you're seeing from the psql prompt.\nI get the same if I do this:\nbash\n$ deepdive sql 'insert into tmp values ('\\''\\u00a0'\\'')'\nINSERT 0 1\n$ deepdive sql eval 'select * from tmp'  \n\\\\u00a0\nHowever, if you use CSV, you get the literal text:\n$ deepdive sql eval 'select * from tmp' format=csv\n\\u00a0\nSo, it's clearly literal backslash that's inserted by the SQL.\nNote that this is completely different if you use the E'...' in Postgres' SQL.\nbash\n$ deepdive sql 'insert into tmp values (E'\\''\\u00a0'\\'')'\nINSERT 0 1\n$ deepdive sql eval 'select * from tmp'  \n\u00a0\n$ deepdive sql eval 'select * from tmp'  format=csv\n\u00a0\n$ deepdive sql eval 'select * from tmp'  format=json\n{\"t\":\"\u00a0\"}\nUPDATE:\nFrom the psql prompt, you can confirm it's the actual NBSP inserted this time:\n```\n$ deepdive sql\ndb=# select * from tmp;\n t \n\n(1 row)\n``\n.deepdive sqlso far did not define its own format and simply tried to stick with standard PG TSV, CSV, or JSON-object-per-line formats.  I don't thinkdeepdive sqlintroduces any surprise when the user gets an escaped backslash for a backslash they INSERTed in this case.  Doing otherwise would be a huge surprise corrupting user's data.  It's rather the careless user's fault or PG's confusing SQL literal syntax to be blamed rather than thedeepdive sql` trying to stay consistent per standard PG TSV encoding.  If it were input TSV that contained a single backslash, there of course won't be extra backslash escapes, but the record was inserted via SQL in this case.\nI agree we need a very clear and simple SerDe.  Our assumption was that PG's TSV was simple but actually it's much worse than what it appears to be at the surface.  The complexity of TSV places a huge burden on everyone who consumes/produces it.  It cannot be solved by a single entity.  Every UDF or tool that wants to produce a correct TSV has to deal with all of its corner cases, which is hard and confusing, hence the current chaos.  I think we should consider completely ditching the PG TSV format and moving to a saner one that can be solved once by the infra eliminating the burden on every consumer/producer.\nI came to a conclusion that JSON is the simplest encoding that's universally available.  However, repeating the data schema by encoding each row as a JSON object is too wasteful and suboptimal.  Hence, I think we should use something like TSJ (tab-separated JSON) I've been prototyping which doesn't repeat the schema (or column names) but still keeps each column in JSON encoding to eliminate any burden of correct SerDe implementation.  That way we can guarantee the data in motion and rest can be read and written without any issues.  TSJ in particular simply requires the consumer to 1) split by TAB, then 2) parse each column in JSON, and the producer to 3) encode each column in JSON then 4) join by TAB to emit a line.  I'm pretty sure TSJ is the simplest one can get that can be programmed in 1-2 lines in most programming languages.  PG TSV requires at least a hundred lines of Python code, and we can never be sure if it's correct or not.  This has to be repeated for every programming language we may need to use for consuming the data.\n. > Sorry for the confusion. My intention was to insert a literal backslash. In the original example, the field is for a json string where the unicode char is stored literally. I guess PG COPY is escaping the field that we don't need it to.\nPG COPY has to escape backslashes for consistency because it'll escape any newline in the field with \\n for example.  Without it, you're getting corrupted data, even though such whitespace may not influence parsed JSON, it matters for other fields.\n\nThanks for the tip. But the problem is that this issue came up when the table is printed out as the input to a udf. I'm not sure if we can specify the csv format inside DeepDive & ddlog. Maybe in this case, I should do something like (borrowing the spouse app for example)?\n... handles *csv/json* lines.\nIs this supported?\n\nNope.  CSV is very awkward for parallelizing as you need to fully parse it just to find the record boundaries, so it's not supported and won't be.  The only option at the moment is having the UDF to fully handle all the PG TSV escape sequences, which is actually nontrivial and why we want a simpler data exchange format for everyone.  If it's a Python UDF, we fortunately have solved the problem in ddlib.util as a @tsv_extractor decorator.  However, for other environment, I'm sorry but you're on your own.\nIs this a problem hooking up Bazaar/Parser?\n. @alldefector Sorry for the lag.  I thought I posted a long answer the other day, but lost it somewhere.\nSerDe for semi-structured data in PG TSV is hard and JSON within TSV simply propagates the extra overhead for TSV escaping/unescaping everywhere, so yes, something like TSJ is desirable as the standard data exchange format.  The culprit is the awkwardness of PG COPY formats.  Everyone else can be happy once we put a thin wrapper around it for data load/unload and that's what's going to done by DeepDive.  TSJ can remain very simple in PG's relational schema as well: nothing special for primitive types, JSON encoding for any structured data instead of PG's ARRAY or so.  Therefore, we don't need DeepDive to monopolize data load/unload although it should be strongly recommended over a direct psql COPY FROM serialized data.  For any direct access, e.g., JDBC, psycopg, nothing special.\nOn your concern about speed: I'd say parsing JSON can't be slower than a real PG TSV parser for the equivalent data.  It surely will be slower than a crappy UDF that ignores all PG TSV escape sequences, corrupts data and crashes itself.  However a decent JSON library in any given language (often optimized by hand or native code) isn't going to be slower than a crude implementation of full PG TSV.  DeepDive's db-driver will try not to parse the full JSON but just work with TSV escapes to keep the overhead minimal (see: tsv2tsj and tsj2fmt).\n. Nothing to be done in DD for this.\nTake away: PG TSV encoding is misleading and difficult; let's move to something else, probably TSJ.\n. Nice.  Maybe we should use a more descriptive name? imbalance or bad_linear_semantics?\n. @clfarron4 Thanks for the fix!  I think a runtime dependency that DD bundles requires it (moreutils ts), which seems to be included in modern Perl distributions (at least since 5.16).  Out of curiosity could you share your cat /etc/redhat-release and perl -v?\n. LGTM\n. deepdive redo init/app weights will do the job.\ndeepdive run was mainly kept for backward compatibility, but I guess we can refresh it to be something like: deepdive mark todo init/app && deepdive do all\n. It should.. I believe the behavior dropping db hasn't been fixed yet.\n. @shahin Thanks for taking a look!\n- Sorry those numbers were buried in the commit messages.  Let me copy them here for everyone.  Below are three commits showing how Python implementation's overhead went from 25-30x down to 1.5-3x.\n- I'd say these measurements aren't DeepDive-specific as the db-drivers just called the TSJ pieces (db-query-tsj and tsv2tsj.pl) and the rest was just direct interaction with postgres and them.\n- We can throw away the Perl impl., but it's just kept here as it can be useful for converting existing PGTSV to TSJ format.  However, I doubt it'll be more useful to anyone by giving a name usable in Perl like use DeepDive::PGTSJ qw(tsv2tsj); ....  Command-line is already too simple:\ndeepdive env tsv2tsj <old.tsv >new.tsj\n- Nothing was measured space-wise as it's quite obvious.  TSJ had a few % increase over TSV on a dataset with many text arrays (per-token NLP markups in the sentences table).  But you can cook any number you want with a different dataset and schema.\n\n\nTSJ unloading throughput experimentation (c62a2113a6618c000144afb144ea796a25f98edb)\n\n\nI was hopeful for psycopg2 + ujson to deliver some throughput, but unfortunately there\u2019s even more overhead..\nobviously while unpacking the values from PG and serializing into JSON.\nWith simpler table (two text columns with large content) it\u2019s quite good.\nFor unloading the articles table in our spouse example repeated 100 times (~423MiB, 111k rows):\n1. PG TSV shows 204MiB/s or 55k/s\n2. psycopg2 + ujson in Python single threaded gives ~142MiB/s or 37k/s\n3. PG TSV to TSJ (w/ Perl regexes) parallel gives ~40MiB/s or 10k/s\n4. PG TSV to TSJ (w/ Perl regexes) single threaded gives ~16MiB/s or 4.2k/s\nHowever it got 25x slower (from 1.5x) with more complex table (9 columns: text, int, json arrays).\nFor unloading the sentences table in our spouse example repeated 100 times (~3GiB, 3.11M lines):\n1. PG TSV shows 320MiB/s or 314k/s\n2. PG TSV to TSJ (w/ Perl regexes) parallel gives ~56MiB/s or 55k/s\n3. PG TSV to TSJ (w/ Perl regexes) single threaded gives ~22MiB/s or 22k/s\n4. psycopg2 + ujson in Python single threaded gives ~13MiB/s or 13k/s\nRought notes on how to reproduce:\n```\nexport PATH=\"$PWD\"/dist/stage/bin:\"$PATH\"\nexport DEEPDIVE_DB_URL=postgresql:///deepdive_tsj_unload_throughput\n: prep input\ncd examples/spouse\ndeepdive db init\ndeepdive load articles  input/articles-1000.tsj.bz2\ndeepdive load sentences input/sentences-1000.tsj.bz2\ncd -\ndeepdive create view a100 as \"SELECT * FROM articles  $(perl -e 'print \" UNION ALL SELECT * FROM articles \" x 100')\"\ndeepdive create view s100 as \"SELECT * FROM sentences $(perl -e 'print \" UNION ALL SELECT * FROM sentences\" x 100')\"\n: 1. PG TSV\ndeepdive unload a100 a.tsv\n: 2. PG TSV to TSJ (w/ Perl regexes) parallel (8-way)\nDEEPDIVE_USE_TSV2TSJ_NPROCS=8  deepdive unload a100 a.tsj\n: 3. PG TSV to TSJ (w/ Perl regexes) single threaded\nDEEPDIVE_USE_TSV2TSJ_NPROCS=1  deepdive unload a100 a.tsj\n: 4. psycopg2 + ujson in Python single threaded\nDEEPDIVE_USE_TSV2TSJ=false     deepdive unload a100 a.tsj\n: repeat with s100 s.tsv and s.tsj\n```\n2. Improves db-query-tsj throughput (49230ee40009cff709aa76966cdd72b713daeb16)\nUsing ujson.loads for psycopg's json typecaster boosted throughput to\n25k/s from 13k/s.\nBypassing the json.loads bumped it to ~60k/s.\ncProfile + snakeviz shows 5% is taken by isinstance() so maybe it's\nworth generating loop body code ahead of time based on column types.\nDefaults to use Python psycopg2+ujson for TSJ as it shows better\nperformance than taking the tsv2tsj route.\n3. Rewrites db-query-tsj to codegen Python ahead of time (a8a4811c42fb2d39d2692a0511391725d5ac5a7f)\nSince every row has fixed number of columns within a single execution,\nuse the type info to generate code that uses direct accesses with\nindexes.  No need to pay the cost of iterating over collections\ndynamically.\nThis gives 1.5x increase in throughput: 60k/s to 95k/s\nand now with parallel unloads (e.g., deepdive unload sentences\ns-{0..7}.tsj it can go up to 200MiB/s on the latest MacBook Pro)\n. Alright, here're some numbers, in a notebook!  (I wish I was more fluent to produce nice charts as well, but that's for next time :)\n\nhttps://gist.github.com/netj/c5ef35e74488500da95701f90e471765\nHope you are convinced why I think JSON lines (or jsonl) doesn't make sense but TSJ does.  I still have more cooking to do to make TSJ appear better than TSV, but I hope you get the idea.\n. I think @thodrek convinced @feiranwang to add this.  Could you give a brief motivation for observation vars?\n. I think their main value is providing a set of constants for factors that don't participate in training, which cannot be done with evidence variables alone.  I'm okay with either way: keeping with the fix or purging as nobody seems to actually care that much.\n. See if grounding with the following produces correct result:\nexport DEEPDIVE_NUM_PROCESSES=1\nYou can probably bunzip2 the single domain binary and feed the sampler with the rest of inputs to see if flatten is the culprit or something else.\nI'm suspecting the load_domains may be terminating earlier..\n. Great finding of the bug!  I totally overlooked such concurrency issue.  I agree we should never use process substitution as a data sink.  Other dump processes should also be rewritten to use standard pipes.  I don't think there's any issue.\n. Process substitution is quite inevitable when there are more than one output channel.. so maybe we have to live with it in such cases.\nI was quickly experimenting with it and it seems my interactive bash actually waits on the delayed subprocess below:\nseq 100 |\ntee >(sleep 2; wc -l >foo) |\nsha1sum >bar\nNot sure why this works (even in nohup) but the grounding scripts with dw don't.  Maybe dw or pbzip2 is handling them specially..\nBut I guess we can just synchronize with files like below which is guaranteed to work.\nrm -f foo.ok\nseq 100 |\ntee >(sleep 1; wc -l >foo; :>foo.ok) |\nsha1sum >bar\nuntil [[ -e foo.ok ]]; do sleep 0.1; done\nI can take care of the rest, turning the last line into a separate command, like wait-on.\n. LGTM\n. @shahin Thanks for the fixes and conda packaging.  In addition to my inline suggestions, could you add a simple note on maintaining the conda package in doc/developer.md?\nMost benefits of conda you listed sounded like tautology to me, i.e., \"conda packaging is good for using conda,\" not solving any new problem, but the first reason is good enough for a complete switch: \"A. Installation of dependencies without sudo.\"\nAlso, please keep in mind that DD has to allow two different PYTHONPATHs (or environments in general): user/app's and deepdive's own.  So far, the distinction has not been clear throughout DD's code, often causing strange UDF issues or DD crashes.  Naively forcing all DD users to share a single PYTHONPATH and PATH with conda will make certain UDFs impossible to use with DD.  Greenplum is a good example of such nightmare as users are misguided to switch to their internal python for accessing the database.\n. Seems not an issue in the latest code, so closing. Moreover, we're bundling jq and bash now, which would reduce the likelihood of these issues (7f048e0dfad55a9ee79d7fcb514d206b013da4cd).\n. This seems to be a bug in Bazaar/Parser producing malformed values.  We'll soon release a more direct way to use CoreNLP which won't suffer from these nasty issues.  The parsed 1M articles using it is already available for download from #566.\n. I guess we never actually tried our build on Debian, only testing a built release on it.\nDeepDive's installer does a bunch of add-apt-repository for Ubuntu-specific APT repos, which may need to be dropped/adapted for Debian.  For now, I'd recommend manually installing the packages listed here: https://github.com/HazyResearch/deepdive/blob/master/util/install/install.Ubuntu.sh\nA patch to make the installer more portable would be also very appreciated. (maybe we could add different APT sources depending on $LSB)\n. Sorry but we do not plan to support Debian for build.  Please use Ubuntu if possible.  Patches to util/install/install.Ubuntu.sh for Debian are welcome though!\n. Please retry make build after:\ngit submodule update --init\nWe are trying to migrate our runtime dependencies to conda lately and some parts of our build have gotten a bit loose.\n. Your app is on a filesystem that doesn't support named pipes, e.g., a volume shared with host.  Please try again after moving it to a normal filesystem.\n. @alldefector This is awesome.  Will take a closer look at the code later, but only changing the id assignment sounds neat.  So, the id assignment keeps each shard's lower 48bits ids compact/sequential, right?\n1. I remember that materializing F (or at least the user's input_query) was beneficial even for just the first case, finding all distinct weights, so back then when I was switching default to views, I gave up turning everything into views except the ones that need id assignment.  However, this may have changed since the flattening (PR #589).  Maybe a simple heuristic-based optimizer might work that depends on the existence of join in F.\n2. I'm a strong believer of execution plans, so I think we can/should compile a series of commands for the scale-up/out execution as well.  This will be a bad idea if the algorithm is going to become completely stochastic or data driven, but explicitly writing down what the system will do has been very helpful for understanding and debugging than analyzing the logs and control flow to understand what the system was trying to do in the past.\n   - We can keep everything as views, define a few key operations for the on-demand partition grounding and construct a sampler driver in terms of those, e.g., dump or stream V/F/W/D for partition i.  The operations can be codegen by DD as we create the companion views.\n   - Scale-up driver will look like a script with a simple for-loop invoking the sampler for each partition.\n   - Scale-out could also look like a for-loop with a slightly expanded body, interspersed with extra ops, such as merging weights and broadcasting them.\n   - The low-level scheduling/callback can just rely on ssh remote execution as opposed to reinventing a custom protocol.  The sampler can simply expose a command-line entrypoint to each step/sub-step of the algorithm, checkpointing/resuming or mmap'ing inputs.  FG shard and weights could be streamed over ssh as well or just rely on a shared filesystem.  Easy to repeat/reproduce a range of epochs and debug.\nThis is how we were planning scale-up/out with C++ DW in mind prior to NS.  Some parts may not be feasible for Python (e.g., mmap), so long living processes/services might be a must for holding the loaded FG shard.  However, I'd still push the idea for having a layer of abstraction between the algorithm and the low-level bits (reusing as much as possible) and a functional architecture to make it easy to run/test each step of a distributed setup that can easily become complicated and awkward to debug. (Sorry if NS is already on this direction, I'm not fully following the developments there)\n. The bundling of runtime dependencies was left broken since the conda stuff.  Since conda doesn't seem to cover all open source we use for all platforms (osx), let's drop it.  Fixes to all build issues are coming soon.\n. We're bundling everything once again by 7f048e0dfad55a9ee79d7fcb514d206b013da4cd, so this should no longer happen.\n. @alldefector Looks like the credentials are not decrypted for non-HazyResearch github users.  Will fix travis.yml to skip the pushing instead of hanging\n. @xiaoling Can't think of a better solution than the random delays.  If numpy already handles parallelization, you could limit DEEPDIVE_NUM_PROCESSES=1.\n. This is mainly an issue with numpy/OpenBLAS that DeepDive cannot do much about.  Closing since a workaround is recorded here.  Let's add these stuffs to FAQ if people leave lots of reactions.\n. Sorry this is a long standing intermittent bug in sbt, required by our old CoreNLP wrapper, used by our spouse example.  Please retry after removing ~/.ivy2 and ~/.sbt.  These kind of strange bugs will be gone once we merge #566.\nDuplicate of #530\n. Duplicate of #504 \n. It looks like you are running tests under a locale that doesn't treat . as the decimal mark.\nPlease rerun them after:\nLC_NUMERIC=C\nWe'll fix the tests to be robust from user locale in the future.\nSee also: http://stackoverflow.com/questions/12845997/unexplicable-error-in-bash-printf-command-inside-a-script-returns-invalid-numb\n. Seems like you're using the example code on the master branch but the 0.8-STABLE version for DeepDive.  We'll release 0.9 soon that can run the example on master.  Meanwhile, I recommend running our Docker images: https://github.com/HazyResearch/deepdive/blob/master/doc/installation.md#launch-without-installing\n. @profressor Sorry but there was a glitch in the installer.  It is now fixed, but for extra certainty, you can use the following command:\nINSTALLER_BRANCH=master RELEASE=master  bash <(curl -fsSL git.io/getdeepdive) deepdive_docker_sandbox\n. @profressor Thanks for spotting another glitch.  The fix is just pushed now, and I made sure it works for me on a fresh Docker for Mac installation.  This Docker sandbox we're introducing in this release, honestly, hasn't been tested thoroughly or used widely yet, so your early feedback is very helpful.\n. You should run the spouse example within the sandbox.  The output you posted seems to be coming from the deepdive 0.8.x installed outside the sandbox.  The sandbox includes the spouse example code and data with new version of deepdive that no longer relies on bazaar/parser.  It should be very handy to try and apply to your own data.. Sorry for my lagging answers:\n1. There's no older release that includes the deepdive-corenlp interface that can run the example in the master branch.\n2. The code is ready in master.  Documentation needs a bit of update, which I'm trying to find time to work on.\n3. We will keep maintaining the installer with binary releases for 0.9 for folks who cannot use Docker, but Docker will be the recommended setup.\nI see how Docker adds another layer of complexity to your setup.  However, I believe it's the probably the best solution for both developers and users to create a uniform environment to build and depend on.  Here's my recommendation:\n* The sandbox is supposed to be used with docker-compose so you can worry less on other things such as linking with database containers.  If you want to directly operate at the Docker level, you must link a hazyresearch/postgres container as database when running the hazyresearch/deepdive image.\n* However, I don't see a compelling reason to avoid Docker Compose as it already does the linking for you and also mounts the host path sandbox/workdir/ to /ConfinedWater/workdir/ in the container and keeps the database on host sandbox/database/.\n  cd sandbox\n  docker-compose run --rm --entrypoint bash deepdive-notebooks\n* Please comment here if you find anything awkward with this setup.  We can certainly work out a better way.. Sorry for the frustration.  Bazaar/Parser, DeepDive 0.8 used to rely on had various build issues/bugs, so we're migrating to directly call CoreNLP in the upcoming 0.9 release.  If you need a fully parsed signalmedia 1m corpus, please consider using the already processed data linked from #566.  If you're trying to parse your own text data, please check out the sandbox docker image that holds the release candidate: https://github.com/HazyResearch/deepdive/blob/master/doc/installation.md#launch-without-installing. Hi Bruno,\nIf you're using the sandbox, postgres runs in a separate container (database service in the docker-compose.yml).  You'd have to either break into that container with docker exec -it ... bash after identifying it from docker ps, or add some config params you find from https://hub.docker.com/_/postgres/ to the .yml file.\nThere's no password for the user jovyan (from official jupyter notebook containers), but you can always pretend as root in any container by telling docker exec --user 0 ..... Please see the instructions in the latest https://github.com/LatticeData/deepdive/blob/alpha/doc/installation.md to use the latest code.. Sorry I meant https://github.com/HazyResearch/deepdive/blob/master/doc/installation.md. Unless there's a concrete example that needs this demultiplexing, this sounds like unnecessary complexity.\n. We now have ddlib for text-based apps.\n. This is too dated and lacks detail for action. Let's close this and reopen or create a new issue if needed.\n. This is a great idea but lacks detail to be actionable. Let's close this and create a new issue with concrete ideas later.\n. This is too dated and lacks detail for action. Let's close this and reopen or create a new issue if needed.\n. By having a robust universal installation method, our AMIs will be deprecated.  See #302.\n. This is perhaps done by #251, which runs with flag psql -v ON_ERROR_STOP=1.\n. Duplicate of #1\n. This is done by #207 or more specifically: ff36cdc07b0a1573b01b24dc21d9eb822f0a8d45.\n. From a newbie's point of view, I thought the repetition of names in most example code for application.conf was rendering the documentation unnecessarily more complicated.  Unless there's a good reason to repeat the names, why don't we shorten them?\nFYI https://github.com/typesafehub/config/blob/master/HOCON.md#readme\nI can help with this one, as I already made some changes while reading.\n. My pleasure!  I'm nearly done, and this may be off-topic, but do you know why extractors pop up in the examples for inference rules document? See: https://github.com/HazyResearch/deepdive/blame/gh-pages/doc/inference_rules.md#L65\n. Probably fixed by #251 which adds a flag psql -v ON_ERROR_STOP=1 to stop on error.\n. Probably fixed by #251, which stops on SQL error.\n. I can surely create a clean Makefile to do that, but it would force everyone who had an app to modify their run.sh to call make instead of sbt.  I think we need a well designed solution for this \"user facing part\" rather than adding ad-hoc layers as temporary fixes, because these small things will pile up and make DeepDive look daunting to use.\nI wish DeepDive provided a clean \"facade\" command (perhaps named deepdive?) that the app users/developers could call from their app directory for performing the standard tasks, e.g., extraction, grounding, learning/inference, end-to-end, etc.  I really like the declarative style (application.conf) of defining apps, but the rest of the necessary glue code seems pretty brittle.  Here's my suggestion: DeepDive users would simply add a directory to their PATH environment, and call the deepdive command from their app dirs to work on them.  Outputs will stay within the app as well (instead of DeepDive source tree).  The typical iterative workflow will be user's modifying the application.conf file and/or some udf code, then running deepdive something to reflect changes to the knowledge base (analogous to Git: work; git status; work; git add ...; work; git commit; # repeat).  I'm sure this suggested way would be much easier for maintaining both apps and infra than the current way of duplicating the environment set up (env.sh) and start up code (run.sh) all over the apps.  By simplifying the interface, we will be able to freely introduce any changes to how DeepDive is invoked because such parts will no longer be part of the apps but will stay within the common infrastructure.  I'd be happy to do the necessary grunt work for this, but I'd like to first listen to your thoughts, and observe what are the typical tasks/steps.\nAs a separate issue, instead of using different names (sampler-dw-linux, sampler-dw-mac) for the sampler for each platform, I'd rather use the same name (sampler-dw) and let the build to take care of placing the correct executable on PATH.  This would free other parts of DeepDive (Scala, scripts, etc.) from having to worry about detecting the OS, and let them use the same command.\n. Okay, on a second thought, I should probably add a short Makefile and just change the installation instruction to use make instead of sbt compile.  The facade command design and run.sh/env.sh duplication can be continued on a separate issue, as well as unifying the sampler name.\n. Thanks @feiranwang \n. @zifeishan Do you mean you've already found a way?  I saw #109, and it seems you did with @zhangce, but wasn't clear.\nIn fact, I know a nice way to create an extensible facade command (which I've already used in many other projects including 3X), as well as a way to package everything (jars, shared libs, executables, etc.) into a self-extracting/installing single executable file using BuildKit.  But I was reluctant to start coding because of the huge change it could introduce, without discussing much with you first.\n. I see.  That sounds very useful and promising for now.  However, I would argue against tying the facade command to a single language, because not everything can be done easily/concisely in Scala.\nI think my extensible approach will play nice with such sbt-pack binary, so please go ahead and do it.  Meanwhile, we can discuss what the user friendly command-line interface should look like, and improve it further.\n. Thanks @zifeishan, I'll wait until tomorrow.\n. @zifeishan I was asking why the example was opening a file with an absolute path (/dfs/rulk/0/deepdive/shared/...) instead of one included in the source tree.\n. Oops.  I meant merge into develop.  Does anyone know how to change the PR's target branch?  I should probably recreate it..\n. Superseded by #236 \n. Consider using set -e.  It will halt executing the script whenever a command ends with a non-zero exit status, and return it.  You can't really add such if statements after every line. :)\n. Seems like it was a temporary TravisCI issue.\n. Another option to consider is augmenting the PATH environment variable and call the scripts by name, not their full paths.  We could either add a section to application.conf that lets user declare where the scripts are, or simply prepend the absolute path of the util directory from run.sh when running deepdive (e.g., PATH=\"$DEEPDIVE_HOME/util:$PATH\"  deepdive -c ...).  Former is a better solution while it involves more change, but for the meantime I think the latter is a fine workaround to get things working with minimal change.\nI personally think we should eventually get rid of run.sh and let user simply call deepdive under the application root dir, and most environment setup currently done in the template script should be handled by DeepDive internally, not by a script exposed at user's application.\n. @zifeishan So, you mean all the paths in application.conf will be resolved as if the base path were in $DEEPDIVE_HOME, right?  And that variable is going to default to the current working dir?  I'd rather set the default base path as the application.conf's path.  That way, whoever runs the application won't have to worry about where to start it from, but only care about where that app is.\nPerhaps, we might need two path variables to be truly useful: one for where the DeepDive's builtin stuffs are (DEEPDIVE_HOME), and another where the application-specific files are (DEEPDIVE_APP_ROOT or so).\n. I guess you're right.  I thought I tested it with postgres on my local machine, but it was still running against a GP server.  I'll change this to CSV.\n. Duplicate issue #404 has more details\n. Superseded by #237 \n. @feiranwang Can you fix the pdf? The right link seems to be http://deepdive.stanford.edu/doc/basics/inference_rule_functions.html with basics/ in between.\n. No, please fix the broken link to inference_rule_functions.html in the second page of that tutorial pdf.\n. This sounds very similar to #280.\n. Thanks for your patch.  However, we'll address this environment/relative-path-mess once and for all by introducing a nice facade command soon.  The extra level has been corrected now.\n. Intermittent hanging became more regular after refactoring the tests, and I believe 0bc74f20e9d5fac0a05801b21fbb7a3810ddfcc4 fixes this issue.\n. It should be easier to write a Docker-compatible or environment-neutral tutorial after removing many hard coded pathnames from our examples.\n. Hi @stephenjbarr, sorry our Docker support became low priority as we moved our focus on making the software work on common environments.  Our installer can now set up any Ubuntu or Debian-based cloud instances ready to run DeepDive with a single command.  I'm not familiar with how Postgres containers are set up or the specific way containers are supposed to communicate, but DeepDive is able to talk to any postgres server configured in db.url or $DEEPDIVE_DB_URL, so it shouldn't be too hard to have it talk to a Postgres container.  It'd be great if you or someone who uses Docker on a daily basis could contribute docs and fixes.\n. This will be dramatically simplified in the next release:\n- With from deepdive import *,\n- @tsv_extractor and @returns decorators and the argument default values abstracts away the TSV parsing in UDFs as well as\n- how they're called from DDlog.\n. @feiranwang Wasn't this also fixed somewhere in sampler?  Could you get that merged with the code for #258 as well as any other sampler related ones, then bring the final binary in?\n. The path issue here seems to be corrected when #245 was fixed.\n. The new deepdive command uses a fresh output directory for every run, basically isolating each run's tmp dir from each other. https://github.com/HazyResearch/deepdive/blob/270a74b8ba7b5e1c1bbb0b87af2e4983ae28ecff/shell/deepdive-run\n. @adamwgoldberg Could you advise what I should do about the Shippable failure?  It seems gem install jekyll is failing which has nothing to do with my commits.\n. Thanks for clarifying!  I'll go ahead and ask for someone to merge this PR.\n. It seems you should merge develop again.  Otherwise, looks good to me.  Nice cleanup!\n. Can you briefly summarize how this cardinality change impacts the grounding and whether there's any interaction with the sample?  Does it only lift the supported upperbound for multinomial?  Then can't we take the max length of the cardinality among the multinomial variables?\n. So there aren't any side effects such as the size of the grounded factor graph increasing because of this for existing apps?  1e10 practically sounds big enough, but I still think it'd be better to decide the necessary number of digits based on user's schema.\n. Looks good overall.  Adding tests for plpy certainly makes it easier to understand how ddext works, although I think we should avoid rewriting user's code and eventually redesign the plpy extractor interface to be more compatible with tsv extractors.\nBtw how are the new tests triggered from the test.sh?  Can you wire that up and handle the inline comments as well?\n. @zhangce Was this all taken care of?\n. @feiranwang I thought this was now possible through a sampler option, no?  Maybe I'm confusing with #271?\n. It seems sampler's sample_evidence branch has the code: https://github.com/HazyResearch/sampler/commit/b721e65060a50f9d71500c7d2ff118e8d90bfa01#diff-bcc8c161c87b19685947789ac42e9fe9R32 @feiranwang Could you bring the binary here, also documenting the sampler option?\n. Thanks for pointing out more dependency issues.  We'll get a fix out there that detects them earlier very soon.  gnuplot is currently used for rendering the calibration plot, and you simply don't get the images in the final output without it.\n. Did you figure out why the crash was happening?\n. The crash seemed to be a hardware issue (raiders1, right @zifeishan?).  The doc about the option could be expanded, but if the default is reasonable discussion about tuning seems unnecessary.  Will close this for now.\n. Taken care by #262.\n@tiangolo An updated guide for Ubuntu 14.04 sounds great!  Thanks!\n. Thanks for the fix!\n. ANALYZE doesn't work for mysql, see: 4af3a508ca08b0153560f1835d91af18b33b0135\n. I'll take care of this asap and backport to master.\n. I thought I put that in, but maybe not.  You can try that again, but a cursory look at 0253dd7 makes me even more confused: it seems analyzeTable() is returning string, but how is it getting executed?  At least the one I put in called execute().\nThis combinatorial explosion over the control flow is really a mess and I think we should simply strip the current mysql code and redo it when necessary, possibly in a way that's well isolated from the postgres/greenplum implementation.\n. Where are we using FeatureStats table?  Unless maintaining this table after every run saves majority of apps time for some set of well-defined frequent tasks, let's drop it from DD and move it to Braindump or Dashboard.\n. I see it's commented out by https://github.com/HazyResearch/deepdive/commit/d1f8449ef990a378cde0e6f763bebd8167f821db#diff-e754f8521322016721729d620efbe663.  @zifeishan Could you create a PR that removes the relevant code if you have no objection?\n. Fixes #269 \n. Greenplum (gpload) seems to be not on PATH.  As there are so many environment variables that can go wrong, it may be a good idea to perform some form of runtime dependency check from Main.  A very limited check is done at build/install time by #266, but we'll need different checks based on what database is used.  These could be added as basic sanity checks for application.conf, e.g., checking the udf codes, database connections, etc.\n. Looks okay, but what if the outputRel is longer than 50 chars?  Shouldn't we use a purely generated temporary table name?\n. Good point.  I created #276 as a TODO.\n. I like Ce's reason behind the decision. This was a well known limitation among us but the docs weren't updated timely. Let's fix the docs asap and clearly state why we restrict to one variable per relation.\nMore importantly, I think this is a good time for us to go over the docs thoroughly to find and fix more discrepancies. Also as @chrismre suggests, let's make our docs more testable by integrating all examples in the docs (as well as those in our source tree) into our tests. That'll help us keep everything in sync more easily. Let's come up with a more concrete plan in/after our meeting and button things up to carve out a new release! :)\n. How does this work/pass tests without the new sampler binaries?\n. Because of this issue, @feiranwang introduced the --learn_non_evidence option for the 0.6.0 release, turned off by default, to prevent users from seeing a sudden drop in their quality.  We should fix the quality issue in the correct approach and get rid of the option in the next release.\n. I believe the quality issue has been resolved by a bugfix in https://github.com/HazyResearch/sampler/commit/5347483558bfa7286f41050c5e536bb2e1945cc9\n. Nice to see the coverage from coveralls.io!  Other than the comments above, looks good.\n. Looks good, merging.\n. Could you port this to develop as well in another PR soon?  Because that's where we mostly want to monitor the coverage.\n. Spotted a few errors.  Please fix them.\n. @abresler Could you elaborate on how you installed Postgres?  You need to set PGUSER and PGPASSWORD environment variables correctly.  You're perhaps trying your password with the wrong PGUSER or don't have an account set up in Postgres yet in this case.\n. Did you set up anything besides running postgres -D /opt/homebrew/var/postgres?  Does psql -l show you a list of databases?\nIf standard stuff all fails, you can use initdb /var/tmp/dd.db to create a new home for your database and start postgres with postgres -D /var/tmp/dd.db where /var/tmp/dd.db can be any path you want to store the data.\n. Thanks!\n. @raphaelhoffmann @alldefector Adding a brief introduction page how to use PostgresXL with DeepDive would be awesome for our upcoming release.  Seems like doc/doc/advanced/postgres-xl.md is a natural place?  It probably doesn't have to be as detailed as Greenplum's.\n. This was pending on one last bug in the sampler's incremental mode with NUMA.  @zhangce and I just decided we can go ahead with the release and follow up with a patch soon.\nYes, the website pass can be done anytime.  Any particular parts you want revision?  The new branching policy we'll use makes it easier as the latest code as well as the website kept in master--currently any change made to website needs to be carefully merged to develop, which is quite annoying around release time.\n. It seems the bugfix for HazyResearch/sampler#10 may take a day or two, and the delayed release has been a blocker (mostly mentally) so I wanted to move on to the next things.  However, we can still wait for the patch and mark the release.  Either way is actually fine.\n. Sure.  There's already a rough sketch I made a while ago in the Milestones tab, but we can definitely revise this\u2013perhaps splitting into two? 0.7.0 for the cleanups with easy NLP and 0.8.0 for the rest.  More concrete issues can be created to guide individual tasks as I'm already doing for my cleanup.\n. I don't like quickly growing version numbers either, but at least I agree with the Semantic Versioning telling us to keep the last number for patching previous releases.  For example, when we patch the known bug in sampler a week later, I think it's simpler to give 0.6.1 to it instead of 0.6.0.1 or so, and use 0.7 for the next milestone.\n. Done by 935957dd4b1f25fb085aa8723057fd2e76cf6200\n. Looks great!  I read through it without executing any commands, so there may be some detail I missed, but anyone who's familiar with PGXL should be able to get DD up and running.  Maybe adding a link to the XL spouse_example may be also helpful?\n. @raphaelhoffmann Any updates?  Otherwise, I'd like to give a try.\n. I tested on a 14.04 ec2 instance, and the doc seems accurate after some minor edits.  Will merge.\n. After installing, I tried running make test and found a test isn't passing with Postgres-XL (PostgresInferenceRunnerSpec: Postgres inference data store, grounding the factor graph with Multinomial variables, should work with weight variables) with the following error:\norg.postgresql.util.PSQLException: ERROR: Postgres-XL does not currently support ORDER BY in subqueries\n[...]\n     at org.deepdive.datastore.JdbcDataStore$$anonfun$executeSqlQueries$2.apply(JdbcDataStore.scala:71)\n     at org.deepdive.datastore.JdbcDataStore$$anonfun$executeSqlQueries$2.apply(JdbcDataStore.scala:68)\n[...]\nAny ideas?\n. @feiranwang Feiran, could you comment on why we need those ORDER BY clauses for multinomial?  The original code had ORDER BY even for the _unsorted one with a comment about Greenplum.\n. If dealing with Greenplum was the sole purpose, this will be another good example why we should completely isolate each port at the cost of duplicating some SQL code.\n. @feiranwang Does the factors with \"MultinomialFactorFunction\" need to be grounded in a particular order?  I can hardly imagine that being the case, but you're the expert here.  These ORDER BYs actually originate from your commit for multinomial support and it seems @zhangce just tried to fix some issues on GP.\n. Thanks for your clarification, but what we need here is not adding the comments about current assumption. We're trying to eliminate the ORDER BY in the DeepDive grounding. The code you pointed seems to be dealing with compact factor after loading the input. So, is there any way we can do the ordering from the sampler side, since we're already doing lots of reordering and compaction there anyway?\n. We decided to put this in without multinomial support for the moment.  On grounding multinomial factors, we discussed two possible solutions:\n1. find a general solution for parallel/distributed databases to produce the correct order assumed by the sampler, e.g., using row_number(), or\n2. produce extra info that can be easily generated by parallel/distributed databases and have the sampler recover the assumed order of weight ids.\n. This is a great idea.  I totally agree we should move towards this direction to reduce the maintenance burden.  I'll include new instructions in this release if you could share the scripts you have.\n. Very nice!  I added a temporary redirect to the website for /install.  It should be updated to point to the master branch upon release, but we can test things with your branch for now.  I think it's better to use raw.github.com instead and add curl -fsSL.\nbash\ncurl -fsSL http://deepdive.stanford.edu/install | bash -s\n. I think ultimately we want to use the same installation scripts for Travis or Shippable tests as well.  Currently, variants of these build/runtime dependency check+installation commands exist all over the place (Makefile, lib/dw_extract.sh, lib/check-depends.sh, .travis.yml, Dockerfile, and now a new set of install scripts!).  Besides, DeepDive doesn't have a clear distinction between source and shippable runtime code.  Instead of distributing obscure binaries over git and running apps against the scala sources and scripts, I seriously think we should build a completely separate tarball that can be unpacked somewhere and just works as long as the external dependency requirements are met.  I'll start working on this cleanup right after the current release, and the git clone lines of your proposed installation script will become a simple tarball extraction.  However, we'll still need scripts that installs runtime dependencies for each platform and backend database pair, so we should put more attention to this end.  It would be nice if we can nail down the layout for the scripts in this release and eventually eliminate all variants we have.  If you don't mind, I'll fork your branch to have our Makefile and travis rely on it.\n. Increasing test coverage will happen in the next release.\n. Due to the lack of code coverage tools for our primary languages: bash and jq, code coverage and coveralls badge has been dropped.  I'd say we should do a more thorough code review of these less bloated code. Nobody would argue that one should write database queries in C just because their coverage can be easily measured while it's not so for SQL.  Anyway, having a simple profiling tool that shows which commands are actually unused or used how many times may be useful, though.  Let's keep our eyes open looking for such tool and track it in this issue.\n. @adamwgoldberg It seems your shippable-branch contains a working shippable.yml.  However, I'm questioning why we need Shippable in addition to Travis.  Please comment on why we need Shippable if there's a straightforward reason. (maybe in connection to Docker?)\nOne nice part I see is Shippable gives a good summary on failed tests/assertions in Scala code.  However, if it doesn't provide any real extra value, then I think we should get rid of it to reduce maintenance burden.\n. Now, #311 will clear the Shippable error on our side, but tests using COPY FROM keeps failing due to their limitation (Shippable/support#1503).\n. I see.  Perf testing sounds like a compelling reason to keep container-based tests.  But for now, yeah let's turn it off if you can.\n. Thanks.  Did you just remove repo hooks from GitHub, or something else from Shippable?  It wasn't very clear to me how to turn it off.\n. That explains.  Thanks!\n. I'm thinking we could keep the source (.tex?) for the file together if that's not too much work. \n. Looks great, merging.\n. Could you squash the submodule and binary update into a single commit, with a more descriptive message (what it fixes, why it had to be updated, etc.)?  Maybe even with the doc updates.  That way it's much clearer what those opaque changes are.\n. Thanks for squashing.  Should we turn the --sample_evidence flag on by default as suggested?  Is there a test that checks whether the flag works?\n. I'll update the installation docs in this PR.\n. On an Ubuntu instance, we can now test the scripts with:\nbash\nexport BRANCH=netj-installers\nbash <(curl -fsSL https://raw.github.com/HazyResearch/deepdive/$BRANCH/util/install.sh)\n. @raphaelhoffmann Glad you like it!  I think we can polish scripts for postgres, XL, etc. over time, e.g., accepting important configuration variables interactively.\n. Thanks for the comments!  I think for minor ones or ones with clear edits (6? and 7) you could modify the scripts on this branch directly from GitHub.  I found that feature pretty handy.\nTaking care of dependency (2-4) is definitely something to figure out.  I'm tempted to simply support an optional precondition function (can_install_*) for every install_* functions, which controls visibility, e.g., show deepdive_git_repo only if git is there.  Regarding 4, psql probably shouldn't count as a requirement for deepdive.\n1 and 5 are bash issues.  I'm relying on select, and that's the default behavior.  We could certainly put more effort polishing this part, but I wanted to keep the first version super simple with a bash builtin.  I have a nice set of bash snippets we can use later for creating slick interactive prompts.  5 is due to my set -u which prevents a lot of errors, but I guess we can turn that off by default for the rest of the installer scripts.  I'll quickly get this fixed.\n. Thanks for your edits!  The tests are failing sometimes.  That needs to be fixed soon, but we know this PR is independent from the code that's failing.  So I think we can go ahead and merge.\n. My limited understanding keeps me from being perfectly convinced that this approach works exactly the same, but I'll trust you and @feiranwang on that.\n. Please remember to update the submodule as well next time.  Thanks!\n. It seems the unit tests are now passing, but during the test_incremental, it still gives an error:\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: DROP SEQUENCE IF EXISTS dd_variable_sequence CASCADE                                                 \n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: CREATE SEQUENCE dd_variable_sequence MINVALUE -1 START 0\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback...  SELECT num_variables FROM dd_incremental_meta_data\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: UPDATE dd_delta_has_spouse SET id = NULL\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: UPDATE dd_delta_has_spouse AS t0 SET id = t1.id \n        FROM has_spouse t1\n        WHERE   t0.relation_id = t1.relation_id  AND (t0.label = t1.label \n        OR (t0.label is NULL AND t1.label is NULL))\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: DROP TABLE IF EXISTS dd_delta_has_spouse_inc CASCADE\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback... SELECT version() LIKE '%Postgres-XL%';\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: CREATE UNLOGGED TABLE dd_delta_has_spouse_inc AS  \n        SELECT id, relation_id, label\n        FROM dd_delta_has_spouse \n        WHERE id is NULL\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: ALTER SEQUENCE dd_variable_sequence RESTART 4685\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback... SELECT version() LIKE '%Greenplum%';\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback... SELECT version() LIKE '%Postgres-XL%';\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: SELECT copy_table_assign_ids_replace('public', 'dd_delta_has_spouse_inc', 'id', 4685)\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback... SELECT COUNT(*) FROM dd_delta_has_spouse_inc;\n04:31:30 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: UPDATE dd_delta_has_spouse AS t0 SET id = t1.id \n        FROM dd_delta_has_spouse_inc t1\n        WHERE   t0.relation_id = t1.relation_id  AND (t0.label = t1.label \n        OR (t0.label is NULL AND t1.label is NULL))\n04:31:30 [PostgresDataStore(akka://deepdive)] ERROR org.postgresql.util.PSQLException: ERROR: could not plan this distributed update\n  Detail: correlated UPDATE or updating distribution column currently not supported in Postgres-XL.\n04:31:30 [profiler] DEBUG ending report_id=inference_grounding\n04:31:30 [taskManager] INFO  Completed task_id=inference_grounding with Failure(org.postgresql.util.PSQLException: ERROR: could not plan this distributed update\n  Detail: correlated UPDATE or updating distribution column currently not supported in Postgres-XL.)\n04:31:30 [taskManager] ERROR task=inference_grounding Failed: org.postgresql.util.PSQLException: ERROR: could not plan this distributed update\n  Detail: correlated UPDATE or updating distribution column currently not supported in Postgres-XL.\n04:31:30 [taskManager] ERROR Forcing shutdown\n. @feiranwang This seems like the UPDATE statement that deduplicates by copying the base ids to the delta, right?  Is there any other way than updating the dd_delta table, such as doing an OUTER JOIN, by first using the base variable and COALESCE-ing to the id in delta or so?\n. @feiranwang Sorry for not being clear.  I was thinking of an alternative approach creating a view to select the right id instead of overwriting a column.\n. @feiranwang Yes, the ids will be physically assigned to both delta and base, but I think what you do there is you choose the ids in the base table if the delta variable already existed, right?  So all I'm saying is we could devise a nondestructive way to choose the id from the base for delta rows that join with a base row, and just fallback to the id assigned for delta otherwise.  I know this isn't a simple change, but I'm guessing we could've done this to avoid these compatibility issues with distributed databases.\n. Closing as we're solving this problem from #321.\n. For the record, the latest DELETE fix fails on PGXL during incremental test with the following error:\n+ ./run.sh spouse_example.symmetry.ddl --incremental inference inc.f1+f2+symmetry.out\n[...]\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: CREATE UNLOGGED TABLE dd_graph_weights (id bigint, isfixed int, initvalue real, cardinality text,\n       description text) DISTRIBUTE BY REPLICATION\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: DROP SEQUENCE IF EXISTS dd_weight_sequence CASCADE\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: CREATE SEQUENCE dd_weight_sequence MINVALUE -1 START 0\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: DROP SEQUENCE IF EXISTS dd_factor_sequence CASCADE\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: CREATE SEQUENCE dd_factor_sequence MINVALUE -1 START 0\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback...  SELECT num_weights FROM dd_incremental_meta_data\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: ALTER SEQUENCE dd_weight_sequence RESTART 2221\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback...  SELECT num_factors FROM dd_incremental_meta_data\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: ALTER SEQUENCE dd_factor_sequence RESTART 40310\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: DROP TABLE IF EXISTS dd_graph_weights CASCADE\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback... SELECT version() LIKE '%Postgres-XL%';\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: CREATE UNLOGGED TABLE dd_graph_weights (id bigint, isfixed int, initvalue real, cardinality text,\n     description text)\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: DROP TABLE IF EXISTS dd_query_dd_new_has_spouse_0 CASCADE\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback... SELECT version() LIKE '%Postgres-XL%';\n10:01:33 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: CREATE UNLOGGED TABLE dd_query_dd_new_has_spouse_0 AS SELECT R0.id AS \"dd_new_has_spouse.R0.id\" , R2.feature AS \"has_spouse_features.R2.feature\" FROM dd_new_has_spouse R0, dd_delta_has_spouse_candidates R1, has_spouse_features R2 WHERE R1.relation_id = R0.relation_id AND R2.relation_id = R0.relation_id UNION ALL SELECT R0.id AS \"dd_new_has_spouse.R0.id\" , R2.feature AS \"dd_delta_has_spouse_features.R2.feature\" FROM dd_new_has_spouse R0, dd_new_has_spouse_candidates R1, dd_delta_has_spouse_features R2 WHERE R1.relation_id = R0.relation_id AND R2.relation_id = R0.relation_id\n10:01:33 [PostgresDataStore(akka://deepdive)] ERROR org.postgresql.util.PSQLException: ERROR: Failed to COMMIT the transaction on one or more nodes\n10:01:33 [profiler] DEBUG ending report_id=inference_grounding\n10:01:33 [taskManager] INFO  Completed task_id=inference_grounding with Failure(org.postgresql.util.PSQLException: ERROR: Failed to COMMIT the transaction on one or more nodes)\n10:01:33 [taskManager] ERROR task=inference_grounding Failed: org.postgresql.util.PSQLException: ERROR: Failed to COMMIT the transaction on one or more nodes\n10:01:33 [taskManager] ERROR Forcing shutdown\n10:01:33 [taskManager] ERROR Cancelling task=calibration\n10:01:33 [taskManager] ERROR Cancelling task=inference\n/home/ubuntu/deepdive.xl/sbt/sbt: line 1:  6461 Killed                  java $SBT_OPTS -jar `dirname $0`/sbt-launch.jar \"$@\"\nmake: *** [test] Error 137\n. @SenWu @raphaelhoffmann Are you sure?  Do you see the incremental test ending with two PASS lines?  I can consistently reproduce the error.  I just started a new EC2 instance, installed PGXL with our installer, and ran make test to get the same error.\n. @raphaelhoffmann Yes, maybe my environment is somehow interfering?  I'd like to cross verify it's working.\n. @raphaelhoffmann Thanks for the info.  Yes, this may be relevant.  One quirk I always observed was the first createdb for deepdive_test was always failing, and I had to restart the test.  On the second run, it appeared to be running fine, but there such issue might have been lingering to cause the transaction error in incremental tests.  I'll trust your test results and merge this.\n. Please see my inline comments.  Otherwise, it looks pretty good except that most of the new parts aren't under the standard test suite.  It'd be nice to run the piggy_extractor example as another integration test if that's possible with a simple shell script similar to test/test_incremental.sh. I'm planning to rewrite all integration tests in Scala into simple shell scripts that directly test code under examples/, so let's defer this if you're tempted to copy/paste your example into src/test/scala/.\n. Adding w=1 to the diff will show just the script and modification to Makefile and travis.yml.\n. I'm reluctant to add it to developer's normal workflow.  Anyone who set up the editor appropriately will never create violations, so I think it's only going to slow down the innocent.  Gatekeeping bad code from Travis for PRs seems enough to me.  Speeding up Travis tests is another thing I'm planning to do in #325.\nI made sure one can easily check what's wrong with the make command and open the problematic files from an editor if they ever need to:\nbash\nvim $(make checkstyle)\nI found an interesting tool to deal with whitespaces from the Q&A.  I guess we should rewrite the checks with it sometime later.  The script I think should eventually be rewritten as another test suite with a proper test framework like Bats or so.\n. Thanks for reviewing. There's no formal button for approving PRs. You can just express approval like this and since we're both commiters either of us can press merge! :)\n. @feiranwang commented on wrong place?  I was confused when I looked at the PR :)  Will merge myself.\n. @alldefector Using alternative paths is now possible by aebb987bf39eb766a3831b02b6d612f0c3f5f5e7\n. Done in ca568ae1cc2e56b09a097a8f4f4009d57ee40a3f\n. Thanks for your input!\nFor the moment, there's no test we prepared for the binary installation, but this is an important point we missed, and we'll add an easy way to test very soon.\nThe tutorial pages have all been revised with the new, cleaner app structure and command-line interface.  Please take a fresh look at them. :)\n. Can I ask why it's in a file named segmentio.js?\n. I wished the two fixes were in separate PRs, but if you think they're better together, I'm okay with either way.  Not 100% sure about all the details of multinomial incremental grounding part, but the code looks relatively okay.  We should really refactor and duplicate the code for different databases soon to stop introducing more branches to the control flow.\n. LGTM\n. @zifeishan Could you confirm if this is still an issue with the latest and close otherwise?\n. PTAL\n. You're right.  v0.7.0 introduced big changes, but that's why the version number was bumped.  It is strongly recommended to reorganize your code according to the new standard app layout to use the latest version because a lot of other features will assume it.\nHowever, for the moment, I believe you can rewrite the command as follows, similar to how an example that's not fully migrated yet is doing:\nbash\ndeepdive env java org.deepdive.Main -c application.conf -o output_dir\nThe deepdive run command in fact supports the -c and -o options, although they aren't well documented yet.\n. I'll have to take a deeper look at the code soon, but could you add a test (.bats file) for this format=json part with the corner cases we manually checked?\n. That's a good question.  You can just run deepdive sql within a .bats file as long as it sources the env.sh file.  The test/env.sh is setting up the PATH correctly for the staged deepdive command.\n. Please see my comments.  I can help you rewrite the .py to construct a list of functions if you want.\n. It seems earlier versions of postgres also needs this workaround.  Travis tests are broken which has psql 9.1: https://travis-ci.org/HazyResearch/deepdive/builds/74359333#L2436-L2439\nI think we may check the postgres version to see if to_json() is supported and decide which approach to use, instead of keeping this as a greenplum specific extension.  The version check should ideally be done once from db-parse to set some env vars that controls db-query.\n. I'll take care of the version check once you finish greenplum's db-query.\n. I'm looking into distinguishing empty string vs null with csv.reader.  Please let me know if you have a good idea.  Also any comment on my new code is welcome!\n. No, psql produces a clearly different csv: empty strings are quoted (...,\"\",...), but NULLs aren't: (...,,...), as I commented in the csvtojson.py.\n. My tests probably got way too extreme, but I think it's worth spending some time once and for all getting this part perfectly reliable. Unfortunately our script cannot handle all the corners, but the limitations are at least clearly written in the tests, and hopefully they'll affect very few users who use old versions. \nPlease take a look and let's get this in unless you have concerns. \n. Thanks for taking a look.  You're right.  Tests fail on mysql spouse example quite often.  Not sure why though.  Can you put up an issue or look into it?\n. @chrismre Unfortunately, not yet. You're probably remembering a fix for the pipeline. #329 is supposed to fix all these nasty glitches, but other priorities have been slowing down its progress.  I'll see if I can quickly get some important pieces done first to prevent these issues.\n@kconor Sorry, the dependencies config isn't reliable!  For the moment, please specify all extractors in the pipelines config.\n. For the record, our current guess of the cause is having different extractor implementations for mysql.  However, for the moment, let's just retry the test once more when it fails to suppress false alarms from Travis because that's basically what we're currently doing anyway.  We should get to the bottom of this and unify the underlying code eventually.\n. Closing as MySQL tests were dropped since nobody's seriously using it.\n. Sounds like a great suggestion!  Admittedly, the current initdb command that literally initializes the whole database is not very friendly to such incremental changes.  It was the best abstraction at the moment given the opaque schema.sql and setup_database.sh-style scripts most apps were using.  Ideally, I think DeepDive should have more visibility into the database schema to provide finer grained control.\nSpeaking more concretely:\n- [x] We could replace schema.sql with a schema.json where the backend-specific CREATE TABLE SQLs are generated by DD.  This JSON format could stay in sync with DDlog's, which becomes another reason to use DDlog!\nExample schema.json.\n- [x] Creating/truncating database tables individually could be done much easily, e.g.:\nbash\n  deepdive initdb sentences ontology mentions\n- [x] Instead of having the flat and opaque input/init.sh script do everything, we could set up a granular convention for keeping the data/script to load into individual tables, e.g.:\nbash\n  deepdive load sentences\nloading input/sentences.{tsv,csv,json-seq} or running input/load-sentences.sh.  It'd be great to have this directly load columns produced by Bazaar Pipe as well.\n- [ ] Detecting changes to the schema becomes also significantly easier, and we could provide a handy way to evolve the schema, e.g., running autogenerated ALTER TABLEs with something like:\nbash\n  deepdive migrate gene_mentions\n@chrismre I think there may have been a similar sounding issue in the past, but I believe this is a new thing due to the deepdive command we introduced in the latest release.\n. Sorry for the hassle guys.  It turns out we didn't test carefully enough on vanilla OS X.  The installer script should now work as expected now.\n. Not yet, but paving the way for it!\n. LGTM\n. @feiranwang Didn't we solve this from ddlog compiler and deepdive-run at some point by providing a reasonable default parallelism and making this a non-issue?  Could you give a check and close?\n. @alldefector Yes, that's certainly where we're headed.  We're trying to preserve DDlog's high-level info in a JSON format to do various operations.  @feiranwang has already made some progress on the creating/altering parts in #376 and we'll try to expose more operations to give complete control to the user.\n. PATH set up is done by deepdive command.  Note that $DEEPDIVE_HOME is no longer the root of the source tree to make binary releases possible.  Instead stage.sh places all scripts under util/ in the directory that becomes $DEEPDIVE_HOME.\nI think you're not running the scala code through deepdive env which takes care of setting up the environment correctly.\n. Did you use the binary release option from the installer?\n. I see.  The updated changelog for 0.7.0 will hopefully prevent others from falling into the same issue!\n. @astrung Sorry you ran into this.  Could you tell us whether that failing update is happening after some debconf and add-apt-repository commands?  (Creating a gist of your entire output may be much more helpful.)  Have you tried changing your Ubuntu mirror to a more reliable one?  It seems the one you're using is not up-to-date.  I'm not sure how apt-get update could only fail from our installer script because we're not altering anything specially.\n@chrismre This runtime dependency installer is actually being tested on Travis upon every build.  However as you know, some unusual user configuration can always break things.  We could take a more aggressive approach to bundle exact versions of all our dependencies to make these errors much less probable.\n. @astrung Glad to hear it worked.  The installer makes sure you have all the dependencies installed upfront to save you from debugging runtime errors.  You can install just DeepDive without any system updates using the deepdive_from_release option.\n. Can you please rebase your commits instead of keep merging master?\n. Thanks for making these changes!  It seems ddlog_manual.md is duplicated in advanced/ as well as basics/.  Shouldn't we keep only one (perhaps in advanced/)?\nI initially thought we were going to keep that page as an evolving document on DDlog's wiki.  If you think it's good to keep a snapshot on DeepDive's website, I'm okay with that but let's put a link to the wiki from that page, clearly stating the latest version is on the wiki.\nAlso, can you make sure util/ddlog.jar is no longer there and .gitignore'd?  I think I removed it at one point, but it seems some merge introduced it again.\n. Thanks for you updates.  Could you fix the regressions and inconsistencies in the docs I mentioned?\n. Thanks! LGTM now\n. Please see my minor comments.  I want deepdive load to support more formats (e.g., Bazaar/Pipe's json columns) and implicit data locations for tables under input/, but this is a good start.\n. Looks good.  Seems like the python script is not necessary.\n. Looks like a nice workaround for an inefficiency in our current extractor runner.  Please see my inline comment.\n. @zifeishan I'm a bit reluctant since we'll address this in a new codebase, but I'm fine merging if you guys need this fix in the short term.\n. Doesn't deepdive initdb TABLE still drop and create the whole database before creating the given table, affecting others?  Here's what I think users expect from the initdb command:\n- When there are arguments, DD should drop/create/load just those specified tables.  Assuming it's a DDlog app, DD should drop/create the tables from the DDlog schema, then optionally load data to the new tables from an assumed path under input/ by some naming convention.  It's an error if it's not a DDlog app.\n- When no argument is given, DD should drop/create all known tables.  If it's a DDlog app, all tables defined in the schema should be created then loaded as if the names were all given manually.  If it's not DDlog, it should rely on schema.sql and input/init.sh to initialize the database.  For this last non-DDlog case, DD should perhaps do a dropdb to be backward compatible.\nIn any case, DD should first make sure the database is created.\n. Nice updates.  Please see my comments.\n. Looks good, merging.\n. Thanks for your quick fix.  Please see my comments to make it more robust to future changes.\n. It seems the Postgres tests are all skipped perhaps because of some parsing issue?\n. Any anticipated side effects if we drop DISTINCT?  I think there must have been a good reason or use case why we added it in the first place.\n. Closing since weight reuse has been overhauled.\n. Done in #437 and HazyResearch/sampler#13\n. I think this is no longer the case for 0.8 but can you confirm @raphaelhoffmann?  I'm not sure what was being copied.  What \"copy\" were you referring to? materializing the categorical variables or also Booleans? or something done inside the stored procedures?\n. Sequence assignment has been overhauled and this issue is probably no longer applicable.\n. Hi @vsoch, it's possible to do the tagging with RegexNER while running CoreNLP, but in typical DeepDive apps it's done as you first thought by writing extractors that scan every word/phrase.  That's partly because your tagging (or candidate mapping) extractor evolves a lot while most of the compute heavy NLP don't, so decoupling speeds up the development.  We're currently working on an easier way to construct efficient extractors that rely on dictionaries or set of patterns, but until we have something readily usable, I think it's fine to get started with a naive version and optimize as needed.  PaleoDD may be a good example to look at.\n. Thanks for pointing out! These are rough corners that were overlooked for json. @feiranwang could you take a look?\n. This becomes less of an issue with #565 and the @tsv/tsj_extractor decorators.  I agree ddlog function calls can be improved to alias the input columns per its declaration.  However, since we dropped support for json_extractor, which is the only one where the column names matter, I'm closing this issue.\n. Thanks for the fix!  Ideally, these line numbers should be auto generated based on some markers at some point.\n. @feiranwang Sorry I wasn't clear about my intention.  I was postponing the merge because it was modifying the docs that may conflict with other more urgent issues.  It looks good so please base your commits on top of #376.\n. Thanks for the fix!  Looks good other than that minor one.\n. Hi @astrung,\n- The first one isn't an issue because DeepDive streams the records from database to the Python UDFs, and as long as the UDF operates on each row independently (row-wise extractor), memory won't be a limit.  There is however an implementation artifact that duplicates the input data for UDF on filesystem (#428), so the disk storage can be a limit.   But we plan to lift this limitation soon by not spilling to disk.\n- I think you're asking for the pipeline.relearn_from configuration which allows you to test with what's already learned.\n. Hi @ukliu, sorry but run_deepdive_tests option has been added after 0.7.0 was released, so the release itself doesn't support testing installation, but you can run tests with the maintenance branch.  Try:\nbash\nRELEASE=v0.7.x  bash <(curl -fsSL deepdive.stanford.edu/install) run_deepdive_tests\n. @ukliu I see.  It seems your postgresql is not installed with plpython support.  We are trying to eliminate that dependency in the next release, but for now you can reinstall it with:\nbrew uninstall postgresql\nbrew install postgresql --with-python\nor use our installer's postgresql option.\n. Glad it helped!  Yes, that test is for a deprecated feature and intentionally skipped.\n. @ukliu I'm sorry for not responding earlier to this simple issue. You probably forgot to add the directory where deepdive is installed to the PATH environment (3rd step of Quick Installation).  Open a new terminal if you already modified ~/.bash_profile.\n. @ukliu My pleasure!\n. plpy extractor support is gone in 0.8 in favor of tsv extractor\n. @ThomasPalomares @juhanaka @ajratner can you create a PR if making ddlib play nice with Python3 is this simple?  Thanks!\n. Closed by #532 \n. @zhangce Yes, perhaps adding in the README a link to the right section of the tutorial or even to the opendata page is enough.  Once people download these datasets, they seem to get detached from the web page and expect things to be self-contained.\n. I partly agree this isn't crucial with the teaser.  In my case, I was trying to confirm the whole dataset's format was identical to the teaser, but it was too large to download.  I thought we could switch to a superior format because we're going to republish them anyway for fixing other bugs.\n. Is is_keyword_here.key_word_product declared as a deepdive.schema.variables?  The histogram centered in the middle tells that DeepDive was not able to learn anything usually due to an empty training set.\n. Closing this for lack of response. Please reopen with details if needed. Thanks!\n. Hi @vsoch, a new release is out (v0.7.1) which removed the plpythonu dependency for postgresql.  Can you see if that helps?  You can use the same installation instruction to get the latest release:\nbash\nbash <(curl -fsSL deepdive.stanford.edu/install)\n. Let me know if you want to look into any of your problem together.  I can Hangout/Skype later in the afternoon PT.  I'll send a direct email.\n. I remember using v0.7.1 resolved this issue (although we had to use more workaround for SSL).\n. There's no way yet.  I agree deepdive run should be idempotent.  I think the semantics of ddlog programs should be defined as starting from a state where all intensional relations are empty.  We could try to detect which relations are intensional vs. extensional, i.e. defined by ddlog vs. whose data come from external sources, or provide a syntax (or just annotation?) and truncate all intensional relations upon every run.\nIf there aren't use cases for running multiple functions to fill up one relation, e.g., table1 += f(...) :- ... .  table1 += g(...) :- ... ., then we could simply truncate (or drop/create) the output table for running functions for the moment and fully expand this EDB/IDB distinction later.  @feiranwang How does this sound?\n. Sounds great @raphaelhoffmann. Thanks for exploring. \nI think we should add a separate command or commands that create/enumerate/drop the schema or versions rather than overloading deepdive initdb.  We can discuss more on what operations are needed with versioning and at what granularity for completeness.\nNot being able to support MySQL is okay. We just need to provide a graceful way in the driver. \nI wish we could completely drop the use of JDBC and have Scala rely on deepdive-sql eval command as well.  This would allow us to concentrate our effort.  Not confirmed how big this change would be.  I'm good if there's a workaround for this, but ssl validation support for example also requires extra code for JDBC. \n. @raphaelhoffmann Great to hear it's working!\nYou can in fact already invent your own annotation and use ddlog export-schema's json output to figure out user's table.  However I agree we should decide on a fixed annotation recognized by the deepdive subcommand that begins a new schema, sets up the search path, and drops such tables to make it visible.  It seems you changed deepdive initdb's semantics and added createdb, but I think it's better to keep initdb and add a separate command.  Moreover, this SCHEMA stuff is postgresql-specific, so it should go under the driver.  I'll have to put a little more thought into this.\nTaking out JDBC seems to be more difficult than I first thought.  It's quite deeply rooted and many parts are entangled with it, especially tests are heavily tied to JDBC, and json_extractor, calibration plot, piggy_extractor are relying on fancy type introspection.  However, in almost all meaningful cases, we just need a count or boolean back, so I'd say it's desirable to get rid of JDBC.  Migrating some parts out of scala and dropping some features temporarily will make this easier.\n. Thanks for making these fixes.\nI think the dataset README is still confusing to just point to the general walkthrough.  How about pointing to the opendata's \"Data Format (NLP Markups)\" section and enumerating the list of columns there under the first bullet?  In fact, the opendata TSVs are slightly different from the walkthrough: they have provenance and word index columns, sentence id appears after document id, the dependency paths are represented differently using two columns.  I think these datasets are often consumed/prepared by people who have no idea/interest in what they're processing or how they are actually used in DD, so I think we need to be very precise in describing the exact schema.\n. Looks great!\nOn the first point, I meant there may be professional DBAs involved who prepares the database for the actual DD user, who don't necessarily want to follow the details of DD tutorial.  Anyway the new schema page looks great.\nI completely agree the tutorial should be updated at one point to include Bazaar and embrace its format.\n. Thanks for your fix!  deepdive.conf is in HOCON syntax, which doesn't require double quotes, but it's nice to have them for consistency within that example.\n. Another incident of confusion here: HazyResearch/dd-genomics#261.  Apparently, not having --sample-evidence on by default confuses even the most sophisticated users of DeepDive.  I'd say keeping this off by default is a terrible design, and we should rather communicate the important distinction between training/testing in a more explicit way, e.g., by providing extra views to inference results for all candidates while keeping the current views intact.\n. Thanks for fixing this bug.  Merging with a minor bash style fix.\n. Documentation update looks good.\nA few high-level comments:\n- Ideally, we should get rid of run.sh, and have deepdive run accept a phase (mode) option that gets passed down to ddlog compile.\n- Most of setup_database.sh lines should be moved to input/init.sh, and deepdive initdb should be used instead of createdb in 0-setup.sh.\n. @juhanaka awesome!  You'll need to see https://github.com/HazyResearch/ddlog to fix the compiler.\n. The new type checker for supervision columns seems to be the cause. I think using undeclared relations shouldn't be allowed eventually but we can get the type checker quickly fixed to skip undeclared relations for now. @seojiwon Can you make a fix?\n. @raphaelhoffmann Thanks for reporting.  I'll create a maintenance release tomorrow.\nBtw, do you think it's useful to have an automated binary build of master (and every maintenance branch, e.g., v0.7.x)?  It can free you from going through a tedious source build and we won't have to create a bunch of hotfix releases for these small bug fixes.\n. Unfortunately, you have to create two DeepDive applications for that, first running one for A, then taking extractions to run another for B.  However, we strongly recommend to stay within a single app to do \"joint inference\" by expressing the correlations between all variables in A and B within one factor graph.\n. Here's the script in an ancient branch that was going to replace the generated exec_parallel.sh scripts, avoiding materializing any split inputs to do udfs: https://github.com/HazyResearch/deepdive/blob/netj-inmemory-extraction/util/run_parallel.sh\nThe extraction part needs a complete rewrite IMO. Dependencies don't make sense, parallelizing is suboptimal, akka fills up the log with noise. Especially Splitting into fixed size chunks incurs unnecessary udf process startup costs, and spilling them requires extra storage.\nIf this is a blocker for you, I think you can workaround using the above script.  Make sure you use the c branch of mkmimo. \n. We now have a dedicated UDF doc that implies how to plug in these general extractors, then highlighting our Python support.  Maybe we can write another section about general udfs.  The new spouse example in fact already includes such non-Python but Java/Scala extractor (CoreNLP):  nlp_markup.sh with its DDlog counterpart.  So it's really a matter of writing about them, or maybe the page already is clear enough?\n. @vsoch The ids are supposed to be assigned before running your holdout query.  Not sure why it didn't.\n@feiranwang Any idea why the ids for the variables table can remain null when executing the holdout query?\n. Sorry, the current dependency handling is largely broken, and most power users so far preferred to manually manage their pipelines.  I completely agree this shouldn't be the default, and we'll get this fixed pretty soon as we get rid of Akka.  We're going to refactor the extractor runner part to easily support various compute environments other than local processes, so we'll definitely take dependencies more seriously.  Your help\u2013code or comments\u2013will be appreciated!\n. Could you describe ideally how you want to run just the inference with the learned weights, e.g., at the level of deepdive command? Maybe we need a separate deepdive learn and/or deepdive infer commands? But you'd want to run extraction for new variables right?\n. Done by #481 \n. Done in https://github.com/HazyResearch/deepdive/blob/3997a8d3b9fd78ff3fe8b3bbd86803889d411451/doc/ops-model.md#reusing-weights regarding #481 \n. LGTM\n. Looks good.  Now the grounding code looks even more scarier, but it'll hopefully get ironed it out as we migrate to the new data flow compiler architecture.\n. Does the sampler already support the Imply3 factor or did you forget to add the submodule changes?\n. LGTM\n. @SenWu could you reflect this change to the docathon-v0.8 branch?  I think we should primarily document the original mode, and keep as a small note of the difference for inc mode at the end.\nhttps://github.com/HazyResearch/deepdive/blob/docathon-v0.8/doc/factor_graph_schema.md\n. Done in 1831e39ceeea9041e9bc77f32b35809376b22051\n. Looks good other than that one inline comment.  We should do the same for Mac btw.\n. Awesome!\n. See the developer's guide linked from that page.\n. The link is buried in the middle of the text and not very visible.  We'll make it clearer.  Thanks for your feedback!\n- [x] add a build from source section to installation page\n- [x] document build dependencies in developer's guide\n. Done in e54092a76a3efb75889c704fdd817fb9fe3a9379 and f8e1353d8d7406adfdc2ce0fdcb592ef8772a412, and they'll appear in the next release.\n. You need to have libnuma-devel installed and use gcc >= 4.8.  These were recently added dependencies to the master branch as we started to build sampler from source.  You can also stick to the v0.7.x branch which don't need them.\n. Btw libnuma-dev gets installed on Ubuntu when you ask the installer for build dependencies.\nAnd now installer knows which yum packages to install on some RedHat-based distros.\n. Thanks for reporting.  This has been fixed in the master via 49c0bd478d22fc26ae4913c0add94ca3577311d7.  You could also merge or cherry-pick it in the jdbc_removal branch.\n. Thanks for adding this feature!  Can you do a make checkstyle and also rebase to the latest branch in addition to my inline comments?\n. @ThomasPalomares Please rebase onto 959282f11c6265c8f85ea9327eba9d0b85f46408 as the latest commit does not fully pass the test yet.  Sorry!\n. LGTM\n. FYI Here's the SVG rendered data flow for spouse example and data flow for chunking example.\nI forgot to mention that this PR requires no change to the user's app code, except maybe a few extra dependency information that were missing in the manually written deepdive.conf.  The run.sh that typically handled env setup and many other hacks should be moved to the right place such as input/.  @ThomasPalomares and I were able to get the genomics app (in DDlog) working with minimal changes.\n. @zifeishan Regarding multiple SQL statements, I'm reluctant to do a blind split on semicolons, as any string literal may contain it.  Do you know why PGXL is not supporting them or any workaround?  One safe way for DD to deal with multiple statements is to support an array of strings for the sql field for sql_extractors, although this will break backward compatibility (but who cares! : )  How does it sound?\n@raphaelhoffmann Thanks for your detailed feedback!\n1. The runtime dependency build hasn't been thoroughly tested, hence such bugs.  Specifically on graphviz, I feel the build can be simplified without pulling in X11.  Will come up with a fix.\n2. The submodule build has been modified to honor the developer's working tree, so whatever is on disk gets built, as opposed to forcing all submodules to be the exact version recorded in deepdive's commit or the git index.  So forgetting to run git submodule update --init before a build can now result in an unexpected result.  Previously, running build without a git add path/to/submodule wiped out changes to the submodule, upsetting people who were modifying them.  The previous behavior seems to be upsetting less number of people, so I'll revert that or maybe abort the build instead of silently making surprises under the hood.\n3. The terminal messup is due to the pv command showing progress bar, competing with the logs printed to the same terminal.  I tried to reduce that error but haven't figured out how to completely eliminate that.  As a workaround you can run reset whenever that happens by blinding typing it followed by an return/enter.  We need a tty expert here.  There should be a simple invisible escape sequence we can print to ensure the terminal is restore in a sane state.\n4. mkmimo had a busy waiting bug on OS X where the poll(2) syscall returns too fast with slow sinks.  I added a fix for it in f2fabb16cd5b6efab237bb7bf9ec29c97971d851, but kept it separate to keep this giant PR stable for review for a while.  Since we've got some eyes and hands over the code, I can bring in the latest changes, but that'll require another round of tests to be safe.\n. @zifeishan If this is a blocker, we can quickly apply the workaround to pgxl driver, which is actually super simple.  db-execute can call psql multiple times after splitting the query argument by semicolon and that'll handle everything.  Wanna take a stab at it?\n. @zifeishan Interesting.  However, that'll take away stdin for psql, which is problematic for db-load and other potential use cases. Is there a way to tell psql a sql file to run instead?\n. @alldefector Sorry the conversation went private through Slack.  Yes, that's exactly our plan, except using bash processes substitution to turn $sql into a readable file.  This is so much better than dangerously splitting the SQL queries in a sloppy way.  (btw you mean the block is run as a transaction, right?)\n@zifeishan Could you make the changes, confirm it working, and add the commit to this PR?\n. @feiranwang Can you try putting a .done suffix to the target? Maybe I forgot to add that case. Will also take a look into this. \n. @feiranwang It turns out to be a missing schema.json issue, so I converted schema.sql into app.ddlog as an easy way to generate the json file.  The compiled Makefile was missing some dependencies for the mentioned target, which gets generated from the relational schema.  I fixed things to show such errors more transparently, such as:\n$ deepdive plan process/grounding/variable/person_has_cancer/assign_id\nmake: *** No rule to make target `data/person_has_cancer.done', needed by `process/grounding/variable_id_partition.done'.  Stop.\nError in dependencies found for process/grounding/variable/person_has_cancer/assign_id\nIdeally, these should be caught in the deepdive compile phase.  It'd be nice if you can contribute a checker that prevents this kind of error.  Also, it seems we're missing a test for the smoke example.\n. Regarding the future of Scala code, I'm still deciding whether to drop it even earlier and keep the latest in something like 0.7.2, or to have it coexist with the compiler in 0.8.0 and drop it immediately after. In any case I'll make sure the latest including this fix is readily usable. \nI haven't put much thought if the grounding sql queries from the new compiler will perform well in pgxl. At least there's no update anymore. Curious to hear any success or failure story there. \n. You know I prefer completely hiding them. However do you think it's okay to always require a supervision rule that fills the labels from a separate data source? This may be in fact a good thing that discourages direct supervision with no practical difference. The only things that appear more complicated are the toy examples. If you agree I'm happy to revert the last commit that introduced this inconsistency. \n. @alldefector Your point makes a lot of sense that it'll be a bigger surprise later.  I'd love to drop it asap to move forward.  Anyone else has concern/objection here?\n@feiranwang Of course. There'll be a nice changelog that summarizes all the user facing changes, but in a subsequent PR or commit for preparing the release.  In fact, there are so many inconsistencies to docs introduced by this drop.  Many doc pages need rewrite before the release.\n. Compression certainly has overhead.  The question is whether it'll be a bottleneck.  I'm trying to ground a larger one by running spouse example on a larger corpus I synthesized.  But it revealed mkmimo's lower throughput, and running much slower than expected.\nMeanwhile, here're my notes from doing a quick overhead test with several choices: https://gist.github.com/netj/c6f15bb78ff3a52057cb\n. Before I forget, I'll drop some numbers I got a while ago for a large factor graph I synthesized by duplicating the corpus for the spouse example (~12GB uncompressed, 199k vars, 16k weights, 337M factors).\nLOADED VARIABLES: #199907\n         N_QUERY: #139603\n         N_EVID : #60304\nLOADED WEIGHTS: #16664\nLOADED FACTORS: #337742718\nFollowing are rough measurements on raiders6 with 111 processes, only accounting the dumping time and loading time.\nuncompressed\n\n11828322038 bytes (~12GiB)\n401.224535 secs\n\npbzip2\n\n197572897 bytes (~191MiB; 59.8x smaller)\n420.276131 secs (+19s; +4.7% increase)\n\nbzip2\n\n195875810 bytes (~189MiB; 60.4x smaller)\n464.805231 secs (+64s; +16% increase)\n\nSince the full grounding took significantly more time (materializing the factors, weights), I'd say compression overhead is negligible while it's savings on storage footprint and in turn I/O are quite dramatic.  The higher-than-usual compression rate (>>10x) is probably due to the regularity in the factor graph data representation.  I think we should turn this on by default unless there's a really good counter argument.\n. I thought we were dropping Scala, so this literal PR won't go into 0.8, but into 0.7.2 or so.\nReusing learned weights is one of the two missing features in the new compiler (the other being GP's parallel unloading/loading).  I'm planning to implement them pretty soon but other priorities are starting to invade my calendar so it may take more than a week.  However, I can make sure this kind of UPDATE does not show up in the compiled grounding code by doing a simple OUTER JOIN when dumping the weights.\n. @zifeishan Sure, I'll backport most changes relevant to Scala code to 0.7.x and release 0.7.2.  So let's get this in to master, then freeze the Scala codebase in master.\n. LGTM Thanks for this fix!\n. Sorry for the confusion.  That feature (#184) has been dropped a while ago due to a bug (#204) and the documentation/example hasn't been updated properly.  For the moment, please stick to using a single --reg_param option.  We'll update the doc and examples very soon.\n@zifeishan @zhangce @feiranwang Can you fix this easily or should we completely remove it from docs & examples?\n. It now takes multiple, but just uses the last.  See #204 for automatic selection of the parameter.  Closing this instead.\n. @chrismre @HazyResearch/genomics This is the fix for the throughput issue @ThomasPalomares explained in #453 \n. Nope, Travis also confirms everything's fine.  The test failed fast when prior to our fix, but since the fix it was INSERTing too much rows unnecessarily and made the test too costly so I tweaked it a bit.  Please merge if it LGTY\n. @raphaelhoffmann I'll bring the fix into master and v0.7.x soon.  For the moment, you can manually point your submodule to it.\n. DD0.8 currently does not support learned weight reusing yet, so no it doesn't have the issue now but should be careful.\n. Yes this is super important and I'm trying to come up with the right way to do this hopefully before this release. @raphaelhoffmann and I synced up on this. The fix to Scala code base won't be portable to 0.8. \n. Is this applicable to ddlog? Or when using deepdive.conf directly?\n. The internal ids are at least always bigint I believe but for other fields giving warnings sounds good. \n. See also #231 for past attempts of Docker support.  We will be looking into this at some point, but if you can contribute a Dockerfile that would be awesome!\n. @ajratner  ping?  Can we get this reviewed asap?\n. This is fixed in v0.7.x branch as 49c0bd478d22fc26ae4913c0add94ca3577311d7.  Duplicate of #443.\n. For functional dependencies, DeepDive can recognize columns of variable relations without @key annotations if it is present on any column.  However, It's not clear to me how exactly such dependencies should be grounded for the sampler.  Should they be turned into categorical variables, or attach oneIsTrue factors to them?  Can someone clarify a bit for me?\n. Right, unless we turn it into categorical, there's probably no difference in the blowup.  This sounds also doable at the DDlog level, by desugaring such cases.  I think you'll want to correlate such categorical variables with other boolean ones, but I believe that is currently not possible.  I think we'll need to add a few more ways to mix them as well to make this actually useful, implications, etc.  Please correct me if I'm wrong.  With my limited exposure to such use cases, the current Categorical/Multinomial support doesn't really typecheck with the rest in my head, so I wish someone could clarify everything with a full blown example.\n. Thanks for merging but it was failing at Travis.  Not sure why it's failing there, but in the future, let's only merge when everything's green.\n. @ajratner This now has all green lights.  You can merge this whenever you want and I'll merge master into docathon-v0.8 so everyone's on the latest code.\n. Updated ddlog submodule.  PTAL :)\n. You can simply point your app to the right postgresql://user:password@host:port/dbname in db.url if you're using 0.7.x. \n. Please take a look, comment, then merge once everything looks good to you. Standard code review :)\nYou can then perhaps rebase your branch. \n. It'll largely be up to the driver but that'd be nice feature to have. compute-status for example can show all running processes for the config, right? I'd recommend hiding private commands under a clear namespace such as compute-remote- and let others be end user friendly by default. \n. Rebased, fixing a few merge conflicts.\n. @raphaelhoffmann You mean feeding the deepdive-sql's  output directly to the format_converter/wc processes works fine but hangs with the run/process/**/dump/run.sh?  I couldn't easily reproduce the slow progress.\nThere apparently is an mkmimo issue on Mac.  Non-blocking mkmimo seems to create sudden high load to the CPU, then reboots every Mac, probably because of heat/temperature surge?  I'm investigating how to fix this, but I'm hoping a multithreaded implementation with blocking I/O could mitigate this.  For the moment I recommend keeping the DEEPDIVE_NUM_PROCESSES low like 3-4.\n. @alldefector mkmimo crashing Linux? I haven't experienced that myself yet. The potentially low throughput may also be due to a database issue. \n. @raphaelhoffmann Thanks for testing and reviewing it!  I'll press the button myself.\n. LGTM  Thanks a lot for making these edits!\n. Why was this closed?  I think the edits are great.  The Travis error is due to trailing spaces, which can be easily fixed.\n. I think this was fixed in docathon branch by putting NULL::BOOLEAN. I'm not entirely clear if this is ok right now (potentially putting duplicates) but this is how it should be written in my view. \n. Could you post the error line, for reference?\n. Can you run ddlog print app.ddlog and see if the brackets are correct there?\nThis may be a bug I introduced while reducing the number of generated parentheses in the SQL..\n. Have you tried a clean build of the new commit? The header dependency is not accurate and resulting in corrupted executable when built incrementally..\n. Thanks for such quick yet thorough proofreading!  It reads so much better now.  Just adding a bit more edits of mine and merging.\n. Looks like the long names truncated can cause some unintended consequences. We'll need to support ways to prevent this by hashing the full name to a fixed length etc. along with a way to annotate rules with a user defined name. \n. This seems related to #276\n. Sorry I was sidetracked the entire day after a quick fix attempt.  It's apparent that I left a bug in the sequence assignment for GP and never noticed it because db-driver currently lacks tests for many parts after getting a quick expansion.  Moreover, we don't have automatic tests for GP.  We should figure out how to install/run GP on Travis.\nSampler resorting was ridiculous so I already proposed a fix, exploiting the fact that we assign ids from a consecutive range beginning from zero: HazyResearch/sampler#14.  As long as the sequence assignment works the sampler should be working fine.\nWill fix this asap.  Sorry for alerting so many people for this silly bug!\n. @alldefector PTAL\n. I'm figuring out the best way to deal with these conflicting/lacking shared library issues.  Something wasn't completely buttoned up in the latest release as well as the result from the source builds as you can tell, and I'm currently working on that in the background.  Greenplum has it's own environment setup script that's recommended to be sourced, but that often breaks other system software.  For that matter, my recommendation is to use a set of wrappers over Greenplum executables to isolate the necessary environment changes, instead of contaminating your normal shell environment.\nRegarding GLIBC version symbol issue, I honestly don't know how to control it or if it's even controllable.  If you build on a recent distro, your binary ends up requiring some of them, which is very undesirable.  Any input on this end would be appreciated!\n. @ThomasPalomares can you expand the test to include the input that triggered the bug and expected output?\nBut wait.. I was looking at deepdive_udf_util.bats and it seems it's just a duplicate of deepdive_sql.bats without the relevant tests!  @ajratner I thought you added some tests somewhere.. or maybe I was hallucinated?\nIt should be fairly simple to tailor that bats file to test ddlib.util if we just take the NastyTSV and feed it to an identity Python UDF that uses @tsv_extractor and @returns, etc.  Moreover, this test can be one level up, i.e., directly under test/ since it's not database dependent, or we can even move it to ddlib/test/.\n. @ajratner Is the bug confusing NULLs in arrays as empty strings?  I gave up this case because I was relying on the csv package, but if we're parsing commas ourselves, I think the problem boiled down to a matter of distinguishing ...,,... from ...,\"\",....\n@ThomasPalomares I think the new test is too large involving too many other parts, essentially everything from deepdive: ddlog, compiler, runner, local compute driver etc.\n- [x] Can't we just feed the NastyTSV directly to the identity.py, then check whether it outputs the same thing?\nbash\n@test \"ddlib.util (@tsv_extractor and @returns) works against nasty input\" {\n    diff -u <(echo \"$NastyTSV\")  <(python \"$BATS_TEST_DIRNAME\"/identity.py <<<\"$NastyTSV\")\n}\nidentity.py may be even just inlined in the bats file with -c.\n- [x] The rest of the content of the .bats file is duplicate and should be removed.\n- [x] Also, the test is not dependent on any database, so let's move it out of test/postgresql/, say under ddlib/test/.  You can look at other subdir's env.sh.\n. @SenWu Let's try to get this online asap.  My notes and scripts for raiders kept in https://github.com/HazyResearch/greenplum-howto may be useful for Travis as well (gpdb.install.sh).\n. Thanks!\n. Thanks for the clear steps!  I'm planning to introduce a higher level operation that integrates this PG schema support with the rest of the process timestamps etc.  This will be super helpful for that direction.\n. @alldefector The docs are going to be updated by #506, but we can definitely add some checks to the example script.  Another big pitfall is not having wget installed on Mac, which leads to silently passing over a Bazaar/Parser setup error (the english SR parser download).  Will propose a fix there too.\n. @henryre Thanks for suggesting the edits!  To make it work completely from scratch, we need some revisions.  We may need to add/revise commands to keep things simple though.  Why don't we first make the docs complete as I pointed out, then simplify it in a later PR?\n. :) Glad we didn't merge this earlier prematurely as it provided strong motivation for improving other parts.\n. You can use deepdive redo instead if it has already been done.  I think the tutorial was assuming everything was done the first time.  Do you think we should update it as well?\n. Thanks for the suggestion, but it'd be better if you could create a pull request.  I won't fix this as tuple vs. list isn't very important in this context.\n. @igozali Sorry the code has been sitting here forever.  Although many parts can be improved, I think it's in a relatively good shape.  It's probably better to improve it down the road based on what we see from more instances of concrete compute drivers rather than hypothetical ones we imagine now.\n. @alldefector Nope.  Back then @zifeishan and I confirmed this was happening on a data path that involves just gpdb's psql.\n. This no longer seems to be the issue\n. I may have forgotten make became a runtime dependency. Will push a fix fairly soon. Thanks for reporting!\n. Thanks for your input @xiaoling.  This has been already added as a build dependency in the installers.  Maybe it's time to ask your admin for a:\nbash\nsudo bash <(curl -fsSL git.io/getdeepdive) _deepdive_build_deps\n. Hi @naddou14, not exactly sure what you mean by custom annotations, but you can think of most pieces DeepDive offer for data processing and statistical inference at the mention level as doing the \"custom annotations\" over the corpus.  We use different terms like \"mention level extraction\" or \"candidate mapping\" but they can be viewed as the same things.  The spouse example used in our tutorial is basically decorating the corpus with a custom \"mention of spouse relationship\" annotations.\n. If those are output as extra NER tags, you'll be able to access them from DeepDive.  Spouse example shows how to run CoreNLP through Bazaar/Parser and use the PERSON NER tag to map candidates. It sounds like you can just plug them into DeepDive?\n. There is a known rebooting issue with the latest OS X (10.11) that we're looking into. It's caused by a component called mkmimo which streams the data using nonblocking IO, and it seems latest OS X's kernel has issues with high rate of poll syscalls possibly with a combination of hardware (MBP). We don't have a clean solution yet but you can either use a Linux machine instead or tune the following env variables to keep a reasonable throughput while not crashing:\nexport THROTTLE_SLEEP_MSEC=10 # higher the safer but slower\nexport DEEPDIVE_NUM_PROCESSES=1 # lower the safer but slower\nI'll update here once we find a fix for this. \n. Yes, you'll have to remove that part from the installation for the moment.  I'll push an update soon that gets rid of it, and hopefully a new version of mkmimo that mitigates this issue by default.\n. @lanphan You can increase the THROTTLE_SLEEP_MSEC parameter to make it less likely to crash, but the throughput will become awful.  Since you're eagerly looking for a solution, let's try some workarounds we currently have.  These all involve replacing the mkmimo executable installed under util/ of your DeepDive installation.\nFirst, let's keep a backup:\nbash\n(set -eu; cd $(deepdive whereis installed util/); cp -pf mkmimo mkmimo.orig)\n1. If you clone the fix-for-mac-reboots branch and run make, you get a mkmimo executable for replacement.\nActually, you can just run the following command to patch your installation, assuming deepdive is on your $PATH:\nbash\n   (set -eu; git clone https://github.com/netj/mkmimo.git mkmimo-wip --branch fix-for-mac-reboots; cd mkmimo-wip; make; install -v mkmimo $(deepdive whereis installed util/mkmimo))\nWith this one, you can use a higher value for THROTTLE_SLEEP_USEC (note this is *_USEC in microseconds not milliseconds) without sacrificing much throughput in some cases, e.g., export THROTTLE_SLEEP_USEC=100 which is 0.1ms.  10 gives good throughput but crashes quite often.  You can try higher values like 1000, 10000, 20000, or even 100000 to be safe at the cost of some throughput.\n2. If it's still hard to find the right parameter that doesn't crash your Mac, or you just want something that functions, try this dumb version written in bash.  It's dumb and inefficient incurring a lot of disk I/O but should get you through the data flow without crashing your Mac.  You can download it and replace the util/mkmimo file, making sure you turn on the executable bit.\n   The following command does what I wrote above:\nbash\n   (set -eu; cd $(deepdive whereis installed util/); curl -fRLO https://github.com/netj/mkmimo/raw/bash-impl-poc/mkmimo.sh; chmod -v +x mkmimo.sh; install -v mkmimo.sh mkmimo)\nFinally, if you want to restore the backed up original, here's the oneliner:\nbash\n(set -eu; cd $(deepdive whereis installed util/); install -v mkmimo.orig mkmimo)\nHope this helps!\n. @lanphan Glad to hear that it works fine on Linux. There's no throttling done on Linux (those parameters default to zero) so the versions we tried on Mac won't have much difference. Actually it may have marginal improvement so no harm trying. The same instructions can be used.\nIf you're using Postgres, increasing the DEEPDIVE_NUM_PARALLEL_UNLOADS and DEEPDIVE_NUM_PARALLEL_LOADS from 1 to 3-4 may give you some more speedup. \n. @lanphan Yes those same flags work with different database drivers.\n. 1. You can preprocess the data in anyway if you have a .sh script under input/ as we do for input/articles.tsv.sh.\n2. The doc_id that appeared before the ERROR line wasn't necessarily the input at the moment since they all run in parallel.  With a quick grep '\\\\r' *.jsonl you can see many articles contain \\r.  To filter these, we actually had a gsub in input/articles.tsv.sh of spouse example that takes care of this.  Maybe we should add a comment to make this more apparent.\n3. DeepDive currently doesn't checkpoint within a single \"process,\" but it does once you finish and move on to the next one.  However, this would be a useful feature for development. We'll think about how this could be done without sacrificing efficiency too much.  For other types of compute resource drivers that are coming along, this partitioning and checkpointing will become the default so you can run and resume things idempotently.\nThe intended way to run the whole corpus of signalmedia-1m, is to put the directory under input/ so the file sits at input/signalmedia/signalmedia-1m.jsonl. That way, the input/articles.tsv.sh will pick up the .jsonl file and apply the necessary filters.  You may want to remove the grep commands to not drop any articles.  Please reopen if you find more issues.\n. Yes, I confirmed there seems to be issues with the existing \\r handling within jq.  I'll fix this asap and update here.  Meanwhile, you could just add a good old sed line, which is probably going to be safe and more complete than before keeping carriage-return-phobic PostgreSQL happy:\n``` bash\ncat \"$corpus\" |\ngrep -E 'wife|husband|married' |\nhead -100 |\njq -r '[.id, .content] | @tsv' |\ntake care of carriage returns\nsed 's/\\r//g'\n``\n. I don't think the errors above are from graphviz or so, but due to [dashbeing/bin/sh`](https://wiki.ubuntu.com/DashAsBinSh).  I'll look into this soon.\nThe pbzip2 errors on the other hand can be ignored as it's just complaining about empty files.  Often times empty parts are produced when there's a skew in the parallelization, esp. with smaller dataset.  If you used the mkmimo.sh, then it's not doing a good job spreading the work across processes, so you probably had greater chance of getting empty parts.\n. Possibly.  Please try again after installing the latest stable release.  mkmimo has been updated to a version that probably won't crash your Mac any more.\n. In most cases you can just overwrite by running the installer again. However it may be safer to put your old one aside and just repeat the installation. \n. I believe this is no longer an issue since the fix with multithreaded mkmimo.\n. Please use db.url greenplum://. Another reason for #494 \n. Not sure about the exact cause, but sometimes scala dependencies can go corrupt or the jar downloads sporadically fail.  Maybe you can retry after removing ~/.ivy2?  Soon we're going to strip the thick glue code and introduce a more direct way to use CoreNLP.  However, I don't see an obvious reason why this shouldn't work in Docker.\n. We will soon merge #566 (a thin wrapper for CoreNLP) and Docker compatibility changes, which will get rid of these issues.\n. Thanks a lot for the work!  I put a bit more tests around it (turning them into .bats) and merged after fixing another inconsistency in Python3: map(None, l1, l2, ...)\n. @alldefector I share the same high-level concerns about complexity with you, but the rest of your arguments don't seemed to be factual or convincing to me.\n1. First, the consecutive id assignment step itself isn't a major bottleneck.  Joining the assigned ids back is clearly an overhead, but it's what we pay for higher performance later.  Moreover, using a different method that computes unique ids for variables and weights may not necessarily be cheaper as @chrismre mentioned.\n2. Consecutive sequential id is crucial for the sampler to keep its sequential access pattern.  Otherwise, the sampler itself will eventually have to do similar graph rewriting from each fingerprint to a compact space at loading time. (unordered_map can't be used literally in the code.)  Without the id assignment, grounding is just deferring the problem to a later component, not solving the impedance mismatch at the right place and moment.\n3. The user-space to array-space mapping is the core of DeepDive's grounding.  You can't take that out and let user be responsible for it.  If users already have matrices and arrays, nicely mapped from their raw data, why bother using DeepDive and not Julia or a ton of other packages?  I see DD's core value as helping users construct such array-space objects for analytics without having to deal with the underlying data management plumbing.\nRegarding complexity (also to @chrismre): I don't think the current implementation is complicated at all; It's just linear to the complexity it has to handle.  For every V, F, W, D (scopes for categoricals) to be grounded, the compiler has a SQL template and a few pipeline before/after to map user's spec to a SQL query.  Then to deal with the difference Boolean vs. categorical/multinomial introduces to each of those, doubling the SQL templates from four to eight.  Of course there are hidden/intermediate data for the real work of id assignment, each of which also has a SQL template, making up to about 10 SQL templates.  So, if you think keeping all of these ~10 SQL templates in a single file is really what makes it difficult to reason about, we can easily split into smaller pieces by literally cut/pasting them into separate files.  Grounding was just put in one blob initially while others were sliced and diced as I wasn't sure what separation-of-concerns would be ideal for the future.  It seems we can at least separate it into three now: the Boolean/multinomial cases and the common steps for both.  (@zifeishan may be able to help make this happen sooner)\nOne interesting direction to explore is turning the id assignment (hence grounding) into two phases: first use fingerprint to deal with user-space to fingerprint mapping; then have the second phase deal with fingerprint to sequential id mapping--which sounds nearly identical to the current grounding but hopefully simpler as we can contain complexity in user's data/code in the first phase (many vars, factors, etc.).  The second phase may become simpler and easier to understand as it focus on the core problem: taking a simplified table from the first phase, mapping fingerprints in V, W to a sequential ids, then rewriting fingerprints in F to sequential ids with the maps while generating input for the sampler.\n. Looks much clearer!  Glad to see another example of less code making things more general.  I forgot what bug is still there?\n. @alldefector Yes I was testing things myself, and it took a while on my new Mac.  I don't think we can increase Travis timeout nor would it be a good thing to have to wait for an hour per test, so let's reduce the data size.\n. I think we can shrink things to 20% or less (just on Travis or by default) and lower the F score bar for chunking (getting ~0.65-0.7 for that size) since we're mainly interested in whether it works end to end.  I can modify the travis.yml/bats if you want.\nexport SUBSAMPLE_NUM_WORDS_TRAIN=1000 SUBSAMPLE_NUM_WORDS_TEST=200\nI'm also noticing that spouse example now takes significantly longer than before on my Mac.  postgres stuck at materializing the JOIN with dd_variable_* table.\n. I meant we can set the time zone in the psql that feeds input to the script such that the UTC offset we add is going to be always correct. The script doesn't have be general since pgtsv_to_json is really just a shim for working around a limitation in some db-drivers and not intended for general use outside of it. \n. I don't understand why anyone should ever want to use timestamp without time zone (it doesn't make sense as it's a reference to an absolute point in time and not subject to interpretation). http://stackoverflow.com/a/6158432\nIt's fine to use any time zone in the JSON output as long as we are consistent with the data source. Since pgtsv to json is used only once in our codebase https://github.com/HazyResearch/deepdive/search?utf8=\u2713&q=pgtsv_to_json, we can just put the following line before the COPY statement to ensure we get timestamps serialized in a certain tz\nSET timezone = 'UTC';\nNot sure why pgtsv to json needs any change from the first version. This is a problem in the data source not the data encoder. \nBTw JSON defines one and only one standard datetime format: ISO8601 which is also used by XML Schema, etc. so i agree we shouldn't produce some nonstandard format. It's no longer JSON if we try to sneak in some weird date time format. \n. Ok make sense now. Keeping a way to pass thru incomplete data may be something useful too anyway. \n. LGTM \n. Looks great!  Should we also get rid of the categories column in the weights view here, I remember you flagged as unnecessary in a previous commit?\n. Not sure if it helps but ddlib.util has gone through a similar pain https://github.com/HazyResearch/deepdive/pull/498\nGiven the complexity of the codec I think we should just ditch PG TSV and switch our wire format completely to JSON at least for non primitive values\n. LGTM.  PG TSV won't be dropped anytime soon.\n. Is the switch to csv for distinguishing NULLs in arrays? I remember python's csv reader is fundamentally flawed for distinguishing empty strings from nulls (,\"\", vs ,,) in arrays.\n. Just confirming it's a hard problem and pointing out that nulls inside arrays are encoded differently from values outside.. at least for TEXT. Not sure about csv. Csv is generally not friendly to standard UNIX tools and many scripts including mkmimo that rely on one-line-per-row.\n. I now remembered why we couldn't use CSV or at least Python's csv.reader.\nFirst, let's look at TSV for the values that give us headache:\npostgres=# COPY (SELECT 'NULL', NULL, E'\\\\N', '', ARRAY['NULL', NULL, E'\\\\N', '']::TEXT[]) TO STDOUT;\nNULL    \\N  \\\\N     {\"NULL\",NULL,\"\\\\\\\\N\",\"\"}\nAlthough the TABs are invisible, you can tell each value has a distinct encoding in TSV that can be relatively easily parsed by repeatedly splitting and unescaping.\nNow, let's add CSV to it and see how it looks:\npostgres=# COPY (SELECT 'NULL', NULL, E'\\\\N', '', ARRAY['NULL', NULL, E'\\\\N', '']::TEXT[]) TO STDOUT CSV;\nNULL,,\\N,\"\",\"{\"\"NULL\"\",NULL,\"\"\\\\N\"\",\"\"\"\"}\"\nSeems not so bad, so we may try to use Python's csv.reader, but we immediately face a challenge of distinguishing the second field for NULL ,, from empty string at the fourth ,\"\",. (This is a stupid limitation of Python's csv package.  I've looked hard but no alternatives than paying a huge performance overhead by just parsing csv ourselves in Python.)\nYou may think it's possible to use csv.reader if we change the NULL to use a different encoding, but it's impossible to avoid ambiguity no matter what you do without the ability to distinguish a quoted value from ones that are not quoted.  For example, you'll treat user's \\N as NULL if you mess with the NULL AS ... clause.\npostgres=# COPY (SELECT 'NULL', NULL, E'\\\\N', '', ARRAY['NULL', NULL, E'\\\\N', '']::TEXT[]) TO STDOUT CSV NULL AS E'\\\\N';\nNULL,\\N,\"\\N\",,\"{\"\"NULL\"\",NULL,\"\"\\\\N\"\",\"\"\"\"}\"\nSo we shall give up csv.reader and end up with a very slow pg2json piece in Python, or switch to another language.  (Good news is that at least the NULLs in arrays are consistently NULL.)\n\nMoreover, I don't understand how unicode and their escape sequences were magically handled by switching to Python3 and maybe also CSV, but if we're going to get this part absolutely right, I'd argue we have to explicitly handle all of Postgres' escape sequences similar to this: https://github.com/HazyResearch/deepdive/blob/fa1a48fa52044ecac2bfdb538a1e18b80e239f2c/database/tsv2tsj#L32-L41\nPersonally, I'm inclined to completely hiding Postgres' TSV format in the db-drivers and moving all UDFs and other pieces completely away from it.  JSON per line maybe okay but it still has a lot of overhead (repeated column names), so I was prototyping the TSJ format.  PG TSV is a complete mess and the root of all these unnecessary complexity, and I believe we should solve this problem once and for all in the db-driver.  I think the recent PRs fixing the json support is in the right direction, and we probably need to be more systematic.\n. @shahin Can you add some tests that check proper unicode handling?  It seems tweaking tsv2tsj to handle arrays can be a simple way of fixing pg?sv_to_json here.\n. @shahin \n\n\nWhy do we bundle bash with DeepDive? This seems like a very low-level dependency to include in our distribution. As a DeepDive user or new developer, I imagine I'd be very surprised to find out (eventually) that it's not using the same bash that I'm using on my host system. Seems like an explicit dependency on a given bash version would be clearer.\n\n\nBecause it's almost impossible to ask every user to install the exact bash version on their system, we decided to bundle the exact version our scripts need: 4.3.x.  Bundling GNU coreutils is along the same line.  Mac users wasted many hours/days if not weeks because of these since it's bash 3 and BSD.  Docker could achieve something similar, but it rather avoids the portability problem not solving it, and it's too heavy to just get these few megs of portable dependencies (on my Mac: bash is 7MB, coreutils is 9MB).\n\n\nAs long as we're bundling bash, should we be using it for all tests? An exception for a single .bats seems easily overlooked, and now our tests are targeting different platforms. You know more about bats than I do -- what's the best way to run them all on bash 4.3?\n\n\nSure.  The example I commented was just for illustration.  I agree it's better to run all tests with our own stuff, which is already the case for the rest, but bash used by bats was an unfortunate exception that wasted your time.  I think you can prefix the make test in .travis.yml with dist/stage/bin/deepdive env:\nbash\ndist/stage/bin/deepdive env  make test\n. @chrismre Python has even more version issues (py2, py3, pypy, ...) and we already solved the shell version issue long ago.  I don't think it's a good idea to revisit an already solved problem.\n. @shahin \n\n\nMac users wasted many hours/days if not weeks because of these since it's bash 3 and BSD.\n\nWas this before homebrew/macports/fink? All the Mac users I know install tons of packages with GNU/newer dependencies, but I don't think many of those actually bundle bash or coreutils. I wonder if we could revisit this later and simplify our build.\n\nThe audience of the tool has been less technically skilled (scientists, doctors) than typical software engineers around you, and they often don't have control over the compute resources they want to run DeepDive on.  I believe bundling even more, e.g., postgresql clients, would even lower the bar.  Many users are in fact struggling and mentioning they waste most time at installing dependencies we don't bundle, e.g., postgresql clients, which can be easily added with reasonable defaults.\nIf we start distributing releases via Homebrew, APT, yum, then yes we should install the runtime dependencies natively rather than bundling.  However, that requires more maintenance effort (otherwise, your package gets tossed out) and lower coverage (just Mac and Ubuntu/Debian?) than just bundling whatever we depend on.  The bundles build is completely separate from the rest, so you can just comment out lines from extern/bundle.conf to omit them.  I agree the initial build for developers takes too long, so it's a good idea to have developers install all runtime deps themselves, and have the bundled dependencies only built at the time we're creating binary releases.\n. > sorry for hijacking this thread:\n@shahin No worries.  This is a perfect place/timing to discuss the matter.\n\nDo we have other data about our user base that I could check out?\n\nUnfortunately, we don't have a systematically collected profile of user base (yet..).  But it's clear that there are many users who aren't proficient at operating/programming software, just wanting to extract databases from their dark data with minimal effort.\n\n\nMany users are in fact struggling and mentioning they waste most time at installing dependencies we don't bundle\nIsn't this exactly what dependency management tools are for?\n\n\nOur typical users can't install any software via APT, yum, ports, etc. on their compute resource.  They don't have root access, their IT management refuses to install anything, and they can't offer their own machines or cloud instances.  That's what I meant by lack of \"control.\"  No matter how much effort we put into leveraging all those nice infrastructure laid by package managers, a major number of users will need an alternative, more straightforward way.\n\nIn many ways a package manager reduces maintenance effort. Package managers take things like CPU architecture, security patches, bug fixes, API-stable updates, custom shared libraries, etc. into account when resolving dependencies. If we're saving time on maintenance today then it's because we're ignoring all these things, not because we've found a way to do them better than a package manager!\n\nI agree that's the right way, but behind all those rigorous things, someone had to put time and effort identifying and writing correct debian/control files, brew formulae, Portfiles, etc.  Maintaining them requires even more effort as the dependent packages are also constantly evolving and can break your own package in nontrivial ways.  We may consider publishing these packages once DeepDive gets stable enough, but I wouldn't consider package managers as the exclusive way of distributing releases.  I'd still argue for keeping standalone releases that simplifies many users' lives.\n\n\nand lower coverage\n\nThe abstraction we get from ordinary dependency management seems likely to increase coverage, not lower it. If we just specify dependency names then DeepDive can be ported to any platform that implements those names, including the BSDs, Cygwin, or whatever. If we bundle dependency code or binaries, then it'll run only on platforms that we take the time to bundle dependencies for.\n\nDeepDive can already be ported to any POSIX-compliant platform by just running make on its source tree.  As long as the bundled dependencies themselves are portable (most of them are also built from source), DD source tree can produce a standalone tarball exactly for that platform.  For the moment, we just publish two x86_64 binary releases for Mac and Linux (Ubuntu and RedHat).  But we can always target more platforms with a make, e.g., if BASH on Windows turns out to be working great.  I was saying this approach has greater coverage than optimistically targeting a few package managers.  In fact, having a portable source is a prerequisite to publishing packages in many venues anyway.\n\nWhat am I missing here?\n\nI guess you're missing that bundled dependencies are also built from source.  DeepDive installer also takes care of installing some runtime dependencies that weren't bundled via APT, Homebrew, yum, namely Java, Python, Perl that are pretty universally available.  In the long run, I totally agree we should turn all dependencies into proper package manager specs, instead of bundling any in the published package.  (Keeping standalone release is a separate issue)  Anyway, in general, hooking all the dependencies up to the rest of the OS packages may seem trivial, but the devil is always in the details, and it's often a moving target, out of our control.\n. @alldefector Sounds like a good idea.  Can the code be shared?  Or I'd like to just revert all the unicode commits to restore Travis status to green (master has been broken for a while due to this making hard for everything else).  The performance isn't a huge deal as this part should only activate for older PG and GP.  It won't be hard to find a neat way to plug the psycopg2 code into the existing db-driver (db-query).  Setting up a DeepDive-wide PYTHONPATH for bundling may require a bit of thought and work, which I can take care of.\n. @alldefector OK, then I can just restore things in this PR.\nI think we'll soon remove the dependency on psql and other standard PG interface by bundling it.  We don't need to put slow code in the critical data flow for aesthetic reasons. (bitten by that many times)  Btw TSJ is pretty much done on top of standard PG interface in the other branch.  In the future we could rewrite TSJ in a way that bypasses PG TSV, but I think it's going to look like PG UDFs in C/C++ rather than something in Python.\n. It's escaped as per Postgres' TSV or TEXT format.\nYour insert into gives me a different result, but it seems you're inserting a literal backslash, and that's what you're seeing from the psql prompt.\nI get the same if I do this:\nbash\n$ deepdive sql 'insert into tmp values ('\\''\\u00a0'\\'')'\nINSERT 0 1\n$ deepdive sql eval 'select * from tmp'  \n\\\\u00a0\nHowever, if you use CSV, you get the literal text:\n$ deepdive sql eval 'select * from tmp' format=csv\n\\u00a0\nSo, it's clearly literal backslash that's inserted by the SQL.\nNote that this is completely different if you use the E'...' in Postgres' SQL.\nbash\n$ deepdive sql 'insert into tmp values (E'\\''\\u00a0'\\'')'\nINSERT 0 1\n$ deepdive sql eval 'select * from tmp'  \n\u00a0\n$ deepdive sql eval 'select * from tmp'  format=csv\n\u00a0\n$ deepdive sql eval 'select * from tmp'  format=json\n{\"t\":\"\u00a0\"}\nUPDATE:\nFrom the psql prompt, you can confirm it's the actual NBSP inserted this time:\n```\n$ deepdive sql\ndb=# select * from tmp;\n t \n\n(1 row)\n``\n.deepdive sqlso far did not define its own format and simply tried to stick with standard PG TSV, CSV, or JSON-object-per-line formats.  I don't thinkdeepdive sqlintroduces any surprise when the user gets an escaped backslash for a backslash they INSERTed in this case.  Doing otherwise would be a huge surprise corrupting user's data.  It's rather the careless user's fault or PG's confusing SQL literal syntax to be blamed rather than thedeepdive sql` trying to stay consistent per standard PG TSV encoding.  If it were input TSV that contained a single backslash, there of course won't be extra backslash escapes, but the record was inserted via SQL in this case.\nI agree we need a very clear and simple SerDe.  Our assumption was that PG's TSV was simple but actually it's much worse than what it appears to be at the surface.  The complexity of TSV places a huge burden on everyone who consumes/produces it.  It cannot be solved by a single entity.  Every UDF or tool that wants to produce a correct TSV has to deal with all of its corner cases, which is hard and confusing, hence the current chaos.  I think we should consider completely ditching the PG TSV format and moving to a saner one that can be solved once by the infra eliminating the burden on every consumer/producer.\nI came to a conclusion that JSON is the simplest encoding that's universally available.  However, repeating the data schema by encoding each row as a JSON object is too wasteful and suboptimal.  Hence, I think we should use something like TSJ (tab-separated JSON) I've been prototyping which doesn't repeat the schema (or column names) but still keeps each column in JSON encoding to eliminate any burden of correct SerDe implementation.  That way we can guarantee the data in motion and rest can be read and written without any issues.  TSJ in particular simply requires the consumer to 1) split by TAB, then 2) parse each column in JSON, and the producer to 3) encode each column in JSON then 4) join by TAB to emit a line.  I'm pretty sure TSJ is the simplest one can get that can be programmed in 1-2 lines in most programming languages.  PG TSV requires at least a hundred lines of Python code, and we can never be sure if it's correct or not.  This has to be repeated for every programming language we may need to use for consuming the data.\n. > Sorry for the confusion. My intention was to insert a literal backslash. In the original example, the field is for a json string where the unicode char is stored literally. I guess PG COPY is escaping the field that we don't need it to.\nPG COPY has to escape backslashes for consistency because it'll escape any newline in the field with \\n for example.  Without it, you're getting corrupted data, even though such whitespace may not influence parsed JSON, it matters for other fields.\n\nThanks for the tip. But the problem is that this issue came up when the table is printed out as the input to a udf. I'm not sure if we can specify the csv format inside DeepDive & ddlog. Maybe in this case, I should do something like (borrowing the spouse app for example)?\n... handles *csv/json* lines.\nIs this supported?\n\nNope.  CSV is very awkward for parallelizing as you need to fully parse it just to find the record boundaries, so it's not supported and won't be.  The only option at the moment is having the UDF to fully handle all the PG TSV escape sequences, which is actually nontrivial and why we want a simpler data exchange format for everyone.  If it's a Python UDF, we fortunately have solved the problem in ddlib.util as a @tsv_extractor decorator.  However, for other environment, I'm sorry but you're on your own.\nIs this a problem hooking up Bazaar/Parser?\n. @alldefector Sorry for the lag.  I thought I posted a long answer the other day, but lost it somewhere.\nSerDe for semi-structured data in PG TSV is hard and JSON within TSV simply propagates the extra overhead for TSV escaping/unescaping everywhere, so yes, something like TSJ is desirable as the standard data exchange format.  The culprit is the awkwardness of PG COPY formats.  Everyone else can be happy once we put a thin wrapper around it for data load/unload and that's what's going to done by DeepDive.  TSJ can remain very simple in PG's relational schema as well: nothing special for primitive types, JSON encoding for any structured data instead of PG's ARRAY or so.  Therefore, we don't need DeepDive to monopolize data load/unload although it should be strongly recommended over a direct psql COPY FROM serialized data.  For any direct access, e.g., JDBC, psycopg, nothing special.\nOn your concern about speed: I'd say parsing JSON can't be slower than a real PG TSV parser for the equivalent data.  It surely will be slower than a crappy UDF that ignores all PG TSV escape sequences, corrupts data and crashes itself.  However a decent JSON library in any given language (often optimized by hand or native code) isn't going to be slower than a crude implementation of full PG TSV.  DeepDive's db-driver will try not to parse the full JSON but just work with TSV escapes to keep the overhead minimal (see: tsv2tsj and tsj2fmt).\n. Nothing to be done in DD for this.\nTake away: PG TSV encoding is misleading and difficult; let's move to something else, probably TSJ.\n. Nice.  Maybe we should use a more descriptive name? imbalance or bad_linear_semantics?\n. @clfarron4 Thanks for the fix!  I think a runtime dependency that DD bundles requires it (moreutils ts), which seems to be included in modern Perl distributions (at least since 5.16).  Out of curiosity could you share your cat /etc/redhat-release and perl -v?\n. LGTM\n. deepdive redo init/app weights will do the job.\ndeepdive run was mainly kept for backward compatibility, but I guess we can refresh it to be something like: deepdive mark todo init/app && deepdive do all\n. It should.. I believe the behavior dropping db hasn't been fixed yet.\n. @shahin Thanks for taking a look!\n- Sorry those numbers were buried in the commit messages.  Let me copy them here for everyone.  Below are three commits showing how Python implementation's overhead went from 25-30x down to 1.5-3x.\n- I'd say these measurements aren't DeepDive-specific as the db-drivers just called the TSJ pieces (db-query-tsj and tsv2tsj.pl) and the rest was just direct interaction with postgres and them.\n- We can throw away the Perl impl., but it's just kept here as it can be useful for converting existing PGTSV to TSJ format.  However, I doubt it'll be more useful to anyone by giving a name usable in Perl like use DeepDive::PGTSJ qw(tsv2tsj); ....  Command-line is already too simple:\ndeepdive env tsv2tsj <old.tsv >new.tsj\n- Nothing was measured space-wise as it's quite obvious.  TSJ had a few % increase over TSV on a dataset with many text arrays (per-token NLP markups in the sentences table).  But you can cook any number you want with a different dataset and schema.\n\n\nTSJ unloading throughput experimentation (c62a2113a6618c000144afb144ea796a25f98edb)\n\n\nI was hopeful for psycopg2 + ujson to deliver some throughput, but unfortunately there\u2019s even more overhead..\nobviously while unpacking the values from PG and serializing into JSON.\nWith simpler table (two text columns with large content) it\u2019s quite good.\nFor unloading the articles table in our spouse example repeated 100 times (~423MiB, 111k rows):\n1. PG TSV shows 204MiB/s or 55k/s\n2. psycopg2 + ujson in Python single threaded gives ~142MiB/s or 37k/s\n3. PG TSV to TSJ (w/ Perl regexes) parallel gives ~40MiB/s or 10k/s\n4. PG TSV to TSJ (w/ Perl regexes) single threaded gives ~16MiB/s or 4.2k/s\nHowever it got 25x slower (from 1.5x) with more complex table (9 columns: text, int, json arrays).\nFor unloading the sentences table in our spouse example repeated 100 times (~3GiB, 3.11M lines):\n1. PG TSV shows 320MiB/s or 314k/s\n2. PG TSV to TSJ (w/ Perl regexes) parallel gives ~56MiB/s or 55k/s\n3. PG TSV to TSJ (w/ Perl regexes) single threaded gives ~22MiB/s or 22k/s\n4. psycopg2 + ujson in Python single threaded gives ~13MiB/s or 13k/s\nRought notes on how to reproduce:\n```\nexport PATH=\"$PWD\"/dist/stage/bin:\"$PATH\"\nexport DEEPDIVE_DB_URL=postgresql:///deepdive_tsj_unload_throughput\n: prep input\ncd examples/spouse\ndeepdive db init\ndeepdive load articles  input/articles-1000.tsj.bz2\ndeepdive load sentences input/sentences-1000.tsj.bz2\ncd -\ndeepdive create view a100 as \"SELECT * FROM articles  $(perl -e 'print \" UNION ALL SELECT * FROM articles \" x 100')\"\ndeepdive create view s100 as \"SELECT * FROM sentences $(perl -e 'print \" UNION ALL SELECT * FROM sentences\" x 100')\"\n: 1. PG TSV\ndeepdive unload a100 a.tsv\n: 2. PG TSV to TSJ (w/ Perl regexes) parallel (8-way)\nDEEPDIVE_USE_TSV2TSJ_NPROCS=8  deepdive unload a100 a.tsj\n: 3. PG TSV to TSJ (w/ Perl regexes) single threaded\nDEEPDIVE_USE_TSV2TSJ_NPROCS=1  deepdive unload a100 a.tsj\n: 4. psycopg2 + ujson in Python single threaded\nDEEPDIVE_USE_TSV2TSJ=false     deepdive unload a100 a.tsj\n: repeat with s100 s.tsv and s.tsj\n```\n2. Improves db-query-tsj throughput (49230ee40009cff709aa76966cdd72b713daeb16)\nUsing ujson.loads for psycopg's json typecaster boosted throughput to\n25k/s from 13k/s.\nBypassing the json.loads bumped it to ~60k/s.\ncProfile + snakeviz shows 5% is taken by isinstance() so maybe it's\nworth generating loop body code ahead of time based on column types.\nDefaults to use Python psycopg2+ujson for TSJ as it shows better\nperformance than taking the tsv2tsj route.\n3. Rewrites db-query-tsj to codegen Python ahead of time (a8a4811c42fb2d39d2692a0511391725d5ac5a7f)\nSince every row has fixed number of columns within a single execution,\nuse the type info to generate code that uses direct accesses with\nindexes.  No need to pay the cost of iterating over collections\ndynamically.\nThis gives 1.5x increase in throughput: 60k/s to 95k/s\nand now with parallel unloads (e.g., deepdive unload sentences\ns-{0..7}.tsj it can go up to 200MiB/s on the latest MacBook Pro)\n. Alright, here're some numbers, in a notebook!  (I wish I was more fluent to produce nice charts as well, but that's for next time :)\n\nhttps://gist.github.com/netj/c5ef35e74488500da95701f90e471765\nHope you are convinced why I think JSON lines (or jsonl) doesn't make sense but TSJ does.  I still have more cooking to do to make TSJ appear better than TSV, but I hope you get the idea.\n. I think @thodrek convinced @feiranwang to add this.  Could you give a brief motivation for observation vars?\n. I think their main value is providing a set of constants for factors that don't participate in training, which cannot be done with evidence variables alone.  I'm okay with either way: keeping with the fix or purging as nobody seems to actually care that much.\n. See if grounding with the following produces correct result:\nexport DEEPDIVE_NUM_PROCESSES=1\nYou can probably bunzip2 the single domain binary and feed the sampler with the rest of inputs to see if flatten is the culprit or something else.\nI'm suspecting the load_domains may be terminating earlier..\n. Great finding of the bug!  I totally overlooked such concurrency issue.  I agree we should never use process substitution as a data sink.  Other dump processes should also be rewritten to use standard pipes.  I don't think there's any issue.\n. Process substitution is quite inevitable when there are more than one output channel.. so maybe we have to live with it in such cases.\nI was quickly experimenting with it and it seems my interactive bash actually waits on the delayed subprocess below:\nseq 100 |\ntee >(sleep 2; wc -l >foo) |\nsha1sum >bar\nNot sure why this works (even in nohup) but the grounding scripts with dw don't.  Maybe dw or pbzip2 is handling them specially..\nBut I guess we can just synchronize with files like below which is guaranteed to work.\nrm -f foo.ok\nseq 100 |\ntee >(sleep 1; wc -l >foo; :>foo.ok) |\nsha1sum >bar\nuntil [[ -e foo.ok ]]; do sleep 0.1; done\nI can take care of the rest, turning the last line into a separate command, like wait-on.\n. LGTM\n. @shahin Thanks for the fixes and conda packaging.  In addition to my inline suggestions, could you add a simple note on maintaining the conda package in doc/developer.md?\nMost benefits of conda you listed sounded like tautology to me, i.e., \"conda packaging is good for using conda,\" not solving any new problem, but the first reason is good enough for a complete switch: \"A. Installation of dependencies without sudo.\"\nAlso, please keep in mind that DD has to allow two different PYTHONPATHs (or environments in general): user/app's and deepdive's own.  So far, the distinction has not been clear throughout DD's code, often causing strange UDF issues or DD crashes.  Naively forcing all DD users to share a single PYTHONPATH and PATH with conda will make certain UDFs impossible to use with DD.  Greenplum is a good example of such nightmare as users are misguided to switch to their internal python for accessing the database.\n. Seems not an issue in the latest code, so closing. Moreover, we're bundling jq and bash now, which would reduce the likelihood of these issues (7f048e0dfad55a9ee79d7fcb514d206b013da4cd).\n. This seems to be a bug in Bazaar/Parser producing malformed values.  We'll soon release a more direct way to use CoreNLP which won't suffer from these nasty issues.  The parsed 1M articles using it is already available for download from #566.\n. I guess we never actually tried our build on Debian, only testing a built release on it.\nDeepDive's installer does a bunch of add-apt-repository for Ubuntu-specific APT repos, which may need to be dropped/adapted for Debian.  For now, I'd recommend manually installing the packages listed here: https://github.com/HazyResearch/deepdive/blob/master/util/install/install.Ubuntu.sh\nA patch to make the installer more portable would be also very appreciated. (maybe we could add different APT sources depending on $LSB)\n. Sorry but we do not plan to support Debian for build.  Please use Ubuntu if possible.  Patches to util/install/install.Ubuntu.sh for Debian are welcome though!\n. Please retry make build after:\ngit submodule update --init\nWe are trying to migrate our runtime dependencies to conda lately and some parts of our build have gotten a bit loose.\n. Your app is on a filesystem that doesn't support named pipes, e.g., a volume shared with host.  Please try again after moving it to a normal filesystem.\n. @alldefector This is awesome.  Will take a closer look at the code later, but only changing the id assignment sounds neat.  So, the id assignment keeps each shard's lower 48bits ids compact/sequential, right?\n1. I remember that materializing F (or at least the user's input_query) was beneficial even for just the first case, finding all distinct weights, so back then when I was switching default to views, I gave up turning everything into views except the ones that need id assignment.  However, this may have changed since the flattening (PR #589).  Maybe a simple heuristic-based optimizer might work that depends on the existence of join in F.\n2. I'm a strong believer of execution plans, so I think we can/should compile a series of commands for the scale-up/out execution as well.  This will be a bad idea if the algorithm is going to become completely stochastic or data driven, but explicitly writing down what the system will do has been very helpful for understanding and debugging than analyzing the logs and control flow to understand what the system was trying to do in the past.\n   - We can keep everything as views, define a few key operations for the on-demand partition grounding and construct a sampler driver in terms of those, e.g., dump or stream V/F/W/D for partition i.  The operations can be codegen by DD as we create the companion views.\n   - Scale-up driver will look like a script with a simple for-loop invoking the sampler for each partition.\n   - Scale-out could also look like a for-loop with a slightly expanded body, interspersed with extra ops, such as merging weights and broadcasting them.\n   - The low-level scheduling/callback can just rely on ssh remote execution as opposed to reinventing a custom protocol.  The sampler can simply expose a command-line entrypoint to each step/sub-step of the algorithm, checkpointing/resuming or mmap'ing inputs.  FG shard and weights could be streamed over ssh as well or just rely on a shared filesystem.  Easy to repeat/reproduce a range of epochs and debug.\nThis is how we were planning scale-up/out with C++ DW in mind prior to NS.  Some parts may not be feasible for Python (e.g., mmap), so long living processes/services might be a must for holding the loaded FG shard.  However, I'd still push the idea for having a layer of abstraction between the algorithm and the low-level bits (reusing as much as possible) and a functional architecture to make it easy to run/test each step of a distributed setup that can easily become complicated and awkward to debug. (Sorry if NS is already on this direction, I'm not fully following the developments there)\n. The bundling of runtime dependencies was left broken since the conda stuff.  Since conda doesn't seem to cover all open source we use for all platforms (osx), let's drop it.  Fixes to all build issues are coming soon.\n. We're bundling everything once again by 7f048e0dfad55a9ee79d7fcb514d206b013da4cd, so this should no longer happen.\n. @alldefector Looks like the credentials are not decrypted for non-HazyResearch github users.  Will fix travis.yml to skip the pushing instead of hanging\n. @xiaoling Can't think of a better solution than the random delays.  If numpy already handles parallelization, you could limit DEEPDIVE_NUM_PROCESSES=1.\n. This is mainly an issue with numpy/OpenBLAS that DeepDive cannot do much about.  Closing since a workaround is recorded here.  Let's add these stuffs to FAQ if people leave lots of reactions.\n. Sorry this is a long standing intermittent bug in sbt, required by our old CoreNLP wrapper, used by our spouse example.  Please retry after removing ~/.ivy2 and ~/.sbt.  These kind of strange bugs will be gone once we merge #566.\nDuplicate of #530\n. Duplicate of #504 \n. It looks like you are running tests under a locale that doesn't treat . as the decimal mark.\nPlease rerun them after:\nLC_NUMERIC=C\nWe'll fix the tests to be robust from user locale in the future.\nSee also: http://stackoverflow.com/questions/12845997/unexplicable-error-in-bash-printf-command-inside-a-script-returns-invalid-numb\n. Seems like you're using the example code on the master branch but the 0.8-STABLE version for DeepDive.  We'll release 0.9 soon that can run the example on master.  Meanwhile, I recommend running our Docker images: https://github.com/HazyResearch/deepdive/blob/master/doc/installation.md#launch-without-installing\n. @profressor Sorry but there was a glitch in the installer.  It is now fixed, but for extra certainty, you can use the following command:\nINSTALLER_BRANCH=master RELEASE=master  bash <(curl -fsSL git.io/getdeepdive) deepdive_docker_sandbox\n. @profressor Thanks for spotting another glitch.  The fix is just pushed now, and I made sure it works for me on a fresh Docker for Mac installation.  This Docker sandbox we're introducing in this release, honestly, hasn't been tested thoroughly or used widely yet, so your early feedback is very helpful.\n. You should run the spouse example within the sandbox.  The output you posted seems to be coming from the deepdive 0.8.x installed outside the sandbox.  The sandbox includes the spouse example code and data with new version of deepdive that no longer relies on bazaar/parser.  It should be very handy to try and apply to your own data.. Sorry for my lagging answers:\n1. There's no older release that includes the deepdive-corenlp interface that can run the example in the master branch.\n2. The code is ready in master.  Documentation needs a bit of update, which I'm trying to find time to work on.\n3. We will keep maintaining the installer with binary releases for 0.9 for folks who cannot use Docker, but Docker will be the recommended setup.\nI see how Docker adds another layer of complexity to your setup.  However, I believe it's the probably the best solution for both developers and users to create a uniform environment to build and depend on.  Here's my recommendation:\n* The sandbox is supposed to be used with docker-compose so you can worry less on other things such as linking with database containers.  If you want to directly operate at the Docker level, you must link a hazyresearch/postgres container as database when running the hazyresearch/deepdive image.\n* However, I don't see a compelling reason to avoid Docker Compose as it already does the linking for you and also mounts the host path sandbox/workdir/ to /ConfinedWater/workdir/ in the container and keeps the database on host sandbox/database/.\n  cd sandbox\n  docker-compose run --rm --entrypoint bash deepdive-notebooks\n* Please comment here if you find anything awkward with this setup.  We can certainly work out a better way.. Sorry for the frustration.  Bazaar/Parser, DeepDive 0.8 used to rely on had various build issues/bugs, so we're migrating to directly call CoreNLP in the upcoming 0.9 release.  If you need a fully parsed signalmedia 1m corpus, please consider using the already processed data linked from #566.  If you're trying to parse your own text data, please check out the sandbox docker image that holds the release candidate: https://github.com/HazyResearch/deepdive/blob/master/doc/installation.md#launch-without-installing. Hi Bruno,\nIf you're using the sandbox, postgres runs in a separate container (database service in the docker-compose.yml).  You'd have to either break into that container with docker exec -it ... bash after identifying it from docker ps, or add some config params you find from https://hub.docker.com/_/postgres/ to the .yml file.\nThere's no password for the user jovyan (from official jupyter notebook containers), but you can always pretend as root in any container by telling docker exec --user 0 ..... Please see the instructions in the latest https://github.com/LatticeData/deepdive/blob/alpha/doc/installation.md to use the latest code.. Sorry I meant https://github.com/HazyResearch/deepdive/blob/master/doc/installation.md. ",
    "feiranwang": "Duplicate in #77. Added in feiran-multinomial branch\n. The most recent one is not configured by me, but I can check it. @zifeishan \n. It's still 1.6. (I don't have permission to modify it) @zifeishan \n. Chunking example\n. Oh you are right!\n. @dennybritz Thanks for the comments!\nCardinality of binary, just a minor thing. I see the cardinality of binary is set to 0 or sometimes 1. It should be 2, but if this field is of no use for binary, it's ok...\nThe csv serializer is for test. I will remove it later. I'll add unit tests for the binary format.\n. Thanks. Will look into it.\n. @dennybritz The test issue is due to issuing a query without closing the connection. It has been fixed :)\n. Default sampler args has been changed.\n. skip_learning and weight table are supported in grounding.\n. Seem to work, but I don't know how to reproduce Tai's bug.\nAlso, I think skip learning should set sampler args -l 0 rather than set isfixed = true... setting isfixed doesn't have correct semantics...\n. Yes, I added the code by commented out...\n. Two variables are not in the same table. We have label1, label2. The problem is id is declared as primary key. This can be solved by declaring id as bigint.\n```\n    CREATE TABLE label1(\n    id    BIGSERIAL PRIMARY KEY, \n    wid   INT, val BOOLEAN);\nCREATE TABLE label2(\nid    BIGSERIAL PRIMARY KEY, \nwid   INT, val BOOLEAN);\n\n```\n. Yes, I fixed it locally. I'll push it later on.\n. Done in b0d123aa87108a621218907ab9fb7b68524b6186\n. Done in b805a85df6625fbcdc1ef15a4bbcd9c8399f65fd\n. Done in cf5975e08aa21971f5efdc712beb0c8209013839\n. Done in edc4296bf9c0dd4ea3be2c18841eb32b717cdbbf\n. duplicate issue\n. needs full log to see what happened...\n. Fixed in f7abdd0b2da84f2122f191e9ca71021cecb55472\n. set log level to INFO, will do before release\n. Yes. I'll look into it.\n. Since id will not be allowed to use by users, the tables should not be distributed by id (this column may be empty). We can tell users not to distribute by id if they are using gp\n. Fixed in f7abdd0b2da84f2122f191e9ca71021cecb55472\n. I'm dealing with sampler issue right row...\nAfterwards, I can do \nAdd new application.conf syntax (like relearn, extrcactor type) to unit tests\nLogistic Regression test is commented out right now\nbut I don't know how to test extractor...\n. It's no longer supported... because we don't use this table any more...\n. Yes, tested.\n. I checked ocr, and it's pretty reasonable. I also tested on pharmgkb, here's what I got\n\n. For spouse example\n-l 300 -i 500 --alpha 0.1\n\n. My guess is there is a bug in spouse example. Maybe the variables are features are not correctly connected.\n. Will look into it.\n. I can run on madmax1\n09:41:50 [profiler] INFO  ext_people SUCCESS [38500 ms]\n09:41:50 [profiler] INFO  ext_has_spouse_candidates SUCCESS [13672 ms]\n09:41:50 [profiler] INFO  ext_has_spouse_features SUCCESS [68510 ms]\n09:41:50 [profiler] INFO  inference_grounding SUCCESS [5617 ms]\n09:41:50 [profiler] INFO  inference SUCCESS [10198 ms]\n09:41:50 [profiler] INFO  calibration plot written to /lfs/madmax/0/feiran/deepdive/out/2014-05-05T093932/calibration/has_spouse.is_true.png [0 ms]\n09:41:50 [profiler] INFO  calibration SUCCESS [926 ms]\nseems to me it's assigning id problem, i'll test again\n. Test using rokcy's database, it runs...\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  Writing grounding queries to file=\"/tmp/grounding3399495869192337498.sql\" \n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  Using Greenplum = true\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  Executing grounding query...\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  Executing: \"/tmp/assignId5585274005363230742.sh\" \n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  NOTICE:  drop cascades to function _fast_seqassign(character varying,bigint)\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  NOTICE:  drop cascades to function fast_seqassign(character varying,bigint)\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  NOTICE:  drop cascades to function updateid(bigint,integer,integer[],bigint[],bigint[])\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  NOTICE:  drop cascades to function clear_count_1(integer)\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  CREATE FUNCTION\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  NOTICE:  table \"tmp_gpsid_count\" does not exist, skipping\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  CONTEXT:  SQL statement \"drop table if exists tmp_gpsid_count cascade;\"\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  PL/pgSQL function \"fast_seqassign\" line 2 at execute statement\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  NOTICE:  table \"tmp_gpsid_count_noagg\" does not exist, skipping\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  CONTEXT:  SQL statement \"drop table if exists tmp_gpsid_count_noagg cascade;\"\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  PL/pgSQL function \"fast_seqassign\" line 3 at execute statement\n10:05:25 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  NOTICE:  EXECUTING _fast_seqassign()...\n10:05:26 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO   fast_seqassign \n10:05:26 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  ----------------\n10:05:26 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO   \n10:05:26 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  (1 row)\n. will push again\n. Fixed in d47f817a6e9f352528a2f70216ca22603c4a4498\n. also, sampler name should be\nsampler-dw-linux\nsampler-dw-mac\n. I think we can remove this...\nThe system fails to shut down in many cases...\n. test.sh under root directory\n. Ce told to open this issue... not sure the problem...\n. I thought it was due to the extractor... what is Amir's problem?\nMy problem happens in  deepdive/src/test/scala/integration/LogisticRegression.scala. I changed it to the current syntax.\nUncomment it and run the tests, I get keyError: 'id'...\n. It's already there, just commented out :)\n. I think Makefile has been added. Closing.\n. To my understanding, this is no longer needed since the variable ids are handled by deepdive not by the user.\n. Sounds good!\n. The weight regularization is done by shrinking the weights during each SGD epoch:\nweight = weight * 1/(1+0.01*current_stepsize) \nIt's equivalent to l1 regularization, which is determined by stepsize. There is no separate parameter for regularization. \n. @zifeishan This is this solved I think?\n. Thanks! I didn't change logback.xml, am I supposed to change something? \nSen and I have the same problem for both master and develop. (we were not using deepdive binary)\n. seems to be working... report later if any problem occurs\n. The problem indeed exists.\n. I observed this problem 15 days ago. Not sure when it first appeared...\n. please see my email for my answers.\n. Should have been solved in Sep.\n. Fixed. Added comments for future developers to pay attention to id problem.\n. Yes, also merged into develop. Thanks!\n. @zhangce It seems travis does not naturally support greenplum. http://docs.travis-ci.com/user/database-setup/\n. @rionda It's only for Linux, since Macs do not have numa architecture, sampler for mac will only keep the original factor graph data.\n. @gengyl08 I'll push my changes after travis is passed\n. Got this error... maybe accidental issue\nThe command \"bash ./src/test/bash/linkcheck.sh http://deepdive.stanford.edu/\" exited with 1.\n. Turned out there's no problem.\n. Looks good to me!\n. The link seems to be fixed already. It links to a crappy tutorial I wrote...\n. ok I see\n. Not yet... will test it. Thanks! @zifeishan \n. This doesn't have conflict with refactoring for now. We'll figure out how to deal with greenplum unloading when the streaming interface is ready to go.\n. Done in another pull request.\n. @zifeishan Do you have any idea on this?\n. The normal workflow for deepdive would be assigning id before assigning holdout. This error means your ids are probably not assigned properly. Can you paste the full log?\n.  We discussed this could be an akka issue. I wasn't able to reproduce this issue since we discussed, and looking through travis, this one didn't happen in last two months. \n@adamwgoldberg are you still having this issue?\n. @mikecafarella The pull request has been updated in response to the code review.\n. @mikecafarella Thanks! I'll look into that.\n. @mikecafarella The tmp files are now put into deepdive's folder. This folder is used to store other tmp files, and should be large enough.\n. There's a bug in imply function in sampler. I think Ce fixed it. Please check the compiled binary (release/dw_linux) under HazyResearch/sampler.\n. Sure. That's what I'm thinking. I'll prepare samplers that include all these fixes for release.\n. Hi @Pede23, thanks for reporting the issue. It seems we don't have this problem for all the machines we have tested. Maybe using a newer version of PostgreSQL (> 9) can solve this problem.\n. We use a fixed length encoding for multinomial cardinality. Previously it's 5 digits, which supports up to 1e5 cardinality. I extended to 10 digits. Multinomial may be refactored later, but this PR is just an extension for cardinality.\n. It's different from #271 as this is for inference results. This feature is added in sample_evidence branch, I'll add it to release.\n. which branch are you using?\n. The problem is not in the sampler. The log info has nfac : 0 and nedge: 0, which indicates the factor graph is not correct. Can you post the full log (by running make test)?\n. line 3041 sh: 1: bc: not found\nSorry we should have written this clearly: Deepdive makes use of bash calculator bc. I think the problem will be gone if you install bc.\n. I'm not sure why 20GB factor graph crashes the machine. I experimented with 60GB factor graph on raiders, and it works fine.\nThe doc is under http://deepdive.stanford.edu/doc/basics/sampler.html\n. I think this commit https://github.com/HazyResearch/deepdive/commit/0253dd711e403d577814efcab791d92c0ff79577 might be better. It handles psql/mysql issue .\nShould I put that in?\n. It's in SQLInferenceRunner. It has been commented out in develop branch. I agree we should drop it and move it to braindump or dashboard. @zifeishan \n. closing... cannot automatically merge\n. Looks good. Since master is always failing now... I'll merge it.\n. I'll look into it, and construct a test in deepdive.\n. @raphaelhoffmann I tried to run your example, and the sampler gave me the same inference result (0.5). I'm not sure why it has different results. I'll continue checking this.\nHowever, I do find a bug that results in wrong number of variables for each factor. I'll fix that soon. \n. @raphaelhoffmann Sorry I wasn't able to reproduce that on my mac. Maybe try cleaning up before compiling by running make clean?\n. @raphaelhoffmann I am using this Apple LLVM version 6.1.0 (clang-602.0.49) (based on LLVM 3.6.0svn). I'll check in the fix I mentioned above and see if that is related.\n. I'll check this.\n. I think I found the problem. I'll add test after fixing this issue. \n. @raphaelhoffmann I fixed this in hotfix_array branch. Now the results should make sense. The results should be\ngroup 1: P(v = 1) = (e^3)     / (1 + e^3)  = 0.95\ngroup 2: P(v = 1) = (e^3 + 1) / (3 + e^3)  = 0.91\ngroup 3: P(v = 1) = (e^3 + 3) / (7 + e^3)  = 0.85\ngroup 4: P(v = 1) = (e^3 + 7) / (15 + e^3) = 0.77\nwhere v is a variable in the group.\n. New tests will be merged soon.\n. @zhangce I'll fix this.\n. @zhangce Sure, I'll take care of it. Yes, we did this before. I found there's a bug in update function which will cause problem if we sample non-evidence variables. I'll fix it as well.\n. Fixed in https://github.com/HazyResearch/sampler/commit/ca3e8d45ef1c1c7d8badd16f82eb02847118d744.\n. Currently we have biased coin test for boolean and CRF test for multinomial under https://github.com/HazyResearch/sampler/tree/master/test. @zhangce what other tests do you think can be added?\n. Added partially observed variable test in https://github.com/HazyResearch/sampler/commit/9296be214f40acda711d84ff4045154deccc5f2d.\n. Sure, I'll check with Zifei.\n. @zhangce Here is the commit https://github.com/HazyResearch/sampler/commit/ca3e8d45ef1c1c7d8badd16f82eb02847118d744. The change is sampling all variables during learning.\n. @zhangce I think the code follows the above procedure. Case 2 is reflected in https://github.com/HazyResearch/sampler/blob/master/src/app/gibbs/single_thread_sampler.cpp#L50.\nIf v is not evidence, we flip it in I_e (line 50 - 66), and flip it in I (line 68-77), and update weight.\n. @zhangce They're all initialized to 0 for unknown weights.\nDo you think it makes sense in case 2, we flip the variables, but do not use them to update weights, since they do not give information about the weights? Maybe we can talk in more detail.\n. Partial results: the weights do not look correct. There are very large weights (~30) and very small weights (~-30).\n. @raphaelhoffmann Thanks! That's helpful. I also observed similar issues with the new sampler. I'll start from debugging logistic regression (without symmetry rule).\n. Using a smaller step size we can get similar distributions as before. @zhangce do you think this is still an issue? \n. @netj I'm not sure why we need those two tables. There are comments by Ce on https://github.com/HazyResearch/deepdive/blob/develop/src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala#L526. It seems to be used to fix some Greenplum issue according to the comment.\n. @netj I thought you were discussing the greenplum issue... Yes, MultinomialFactorFunction is grounded in a particular order for convenient array-like access to weights in the sampler. (Information about multinomial here http://deepdive.stanford.edu/doc/basics/schema.html, and https://github.com/HazyResearch/sampler/blob/master/src/dstruct/factor_graph/factor_graph.cpp#L51). I'll add comments for this.\n. Added comments in #314\n. @netj Briefly discussed with Raphael, this order by is used to make sure weight ids for multinomial factor are in a particular order. Raphael mentioned this may be achieved by changing the sequence assignment.\n. @netj Added source\n. I think it may be better to keep the default behavior as before and give users the option to change, for backward compatibility.\nThis arg was tested locally. I'll also add a test for this in sampler's tests.\n. Test for sample_evidence was added in sampler's develop branch. I utilized the existing test, and add --sample_evidence to it, and checked the results.\n. This feature is available in develop branch right row. You can search skip_learning in the doc folder. There are some documentation about how to use learned weights to make new predictions. More details will come out soon.\n. @netj Here we need to fill in id column (assign id) for variable table (dd_delta_has_spouse). Not sure I get your point... how to update a column without using UPDATE?\n. @netj I think we need those variable ids to be physically assigned in the variable table. Those variable ids are used in later parts, such as holdout, inference results, etc.\n. @netj I looked at the log again. It's actually id shouldn't be the first column (default distribution key). This is a documented requirement for variable tables in greenplum (http://deepdive.stanford.edu/doc/advanced/greenplum.html). This also holds for non-incremental version. I've updated develop_fix to reflect this. \n@raphaelhoffmann Sorry the problem that this PR is solving is actually not PXL's problem. I fixed the table schema to avoid updating a distribution key, so I think this PR doesn't need to go in. \n. For some temp tables, we use the default distribution key (the first column), and put a proper column as the first column. I think currently we don't write the distribution key explicitly due to two reasons:\n1. we are sharing sql queries between different datastores\n2. we don't know the exact schema of that relation, but we know which column should not be used as distribution key\n. Yes, Jaeho proposed in the next refactoring to replicate all the sql queries for each datastore, so we can optimize each sql query for each datastore individually. \n. Also updated distribution key in incremental mode\n. Looks good.\n. Sorry, there are some refactoring in assigning weight ids, so I put the commits together. It's better if they were separate.\n. @netj Updated. Thanks!\n. Looks good. Merging.\n. Is there an easy way to call deepdive sql inside .bats file?\n. Thanks for the detailed comments! Looking into them.\n. Updated.\n. Thanks for helping on this! I updating the rest.\n. Instead of writing postgresql version-specific code, can we just unify this by using our own script?\n. Thanks! Pushed my commit.\n. Good point. I think that's hard, as the csv is the same...\n. I see...\n. Looks good to me. The tests are comprehensive. \nNot related to this PR, mysql spouse tests fails from time to time. I didn't find obvious problems yet.\n. Sure.\n. FYI, ddlog tutorial is on http://deepdive.stanford.edu/doc/basics/ddlog.html.\n. deepdive load is added in #368. I'll also take care of making initdb to be able to create specific tables.\n. Looks good. Merging.\n. This is done in 0b387b388a2d182e0b4e26f4bff3ba7ee1f74864. Closing.\n. Sorry I think ddlog does not support views right now, it has a problem when accessing columns of a view (In Ce's example, he uses views as input to functions, but not accessing individual columns). For a temporary fix, you can define the schema, and then use it. I'll fix this soon.\nLet me know if you have any further issues. Maybe under ddlog repo and ping me directly :)\n. Sorry about that. Updated.\n. I agree we can maintain the wiki, and put a link on the website instead of putting up the document.\nI'll check the jar stuff.\n. Reworked the modifications so that everything is based on the latest master.\n. Looks good. Merging.\n. Looks good. Merging.\n. Thanks! Will update accordingly.\n. Updated.\n. Synced up with Zifei. The described approach applies to dumping a single table or local join of tables. In DD, an extractor input query could be an arbitrary SQL query, and the approach could not apply to such a general case. We could optimize for specific types of queries using the ad hoc approach, by getting hints from the users, which I think might be too specific.\n. Sure, we can add support for that.\n. Yes, basically our idea is to add parameters from the user to deepdive, and perform local join and dumping.\n. Thanks! Will update accordingly.\n. @netj Updated. Thanks!\n. @chrismre There was a bug introduced by an earlier commit, and it is fixed in #376. The incremental test is stable after merging #376.\n. I'll look into this. Btw, the recommended extractor type is tsv extractor, which is much faster than the json extractor. In tsv extractors, columns names are irrelevant, while the order does.\n. Hi Jaeho, I was trying to update the interface as well as the incremental ddlog program to reflect recent ddlog changes. Since #376 updates ddlog, I think it would be better if #376 is reviewed and I base this work on top of it.\n. @netj Sounds good. Thanks!\n. @zhangce The doc is updated in incremental_update branch. I'm updating the interface while running experiments... \n. @raphaelhoffmann I think what you need is ddlog truncating the output tables before running the extractors, as we usually do in deepdive.conf. In that way, every deepdive run will produce the same output instead of accumulating results from previous runs. \n. The default setting is that deepdive only output probabilities for non-evidence variables. If you also want to see the probabilities for evidence variables, you can turn on the --sample_evidence flag in the sampler. Please refer to http://deepdive.stanford.edu/doc/basics/configuration.html#sampler and http://deepdive.stanford.edu/doc/basics/sampler.html.\n. @raphaelhoffmann This can be achieved by turning on the sampler argument --sample_evidence, which will let the sampler output probabilities for evidence variables as well. The sampler argument could be added in deepdive.conf, and when the app.ddlog is compiled by deepdive run, this option will get incorporated.\n. @alldefector For ddlog application, according to this doc, there can also be a deepdive.conf, which contains extra configuration (sampler args, etc.). For example, deepdive.conf may look like \ndeepdive.sampler.sampler_args: \"-l 200 -s 1 -i 500 --alpha 0.1 --sample_evidence\"\n. Just to add a point for 1, the users don't need to create copies of training data, they just need to turn the sampler flag on. Then, the sampler will sample evidence variables during inference.\n. Updated and rebased so that the .gitignore change is not included.\n. Looking into this.\n. @netj Hi Jaeho, I checked this program works on my recent ddlog, and it seems this problem occurs after merging seojiwon's HazyResearch/ddlog#56.\n. @vsoch The problem is that variable relation has_related_concept is not declared in schema.variables.\n. That's awesome! :)\n. Yes, the sampler already has support for this. https://github.com/HazyResearch/sampler/blob/master/src/dstruct/factor_graph/factor.h#L151\n. Updated. PTAL.\n. @netj I tried this on several examples. deepdive run and  deepdive do all work great. I have a small problem with deepdive do. When I type deepdive do, it gives a list of targets. In smoke example there's a process/grounding/variable/person_has_cancer/assign_id. When I tried to run with that target, it gives an error\nprocess/grounding/variable/person_has_cancer/assign_id: Unknown target\nIs this expected?\n. @netj  I added a .done suffix to the target and it gives the same error\nprocess/grounding/variable/person_has_cancer/assign_id.done: Unknown target\n. I think it's ok to keep the current semantics, and it's good to be able to directly load data into variable relations.\n. It would be great to have a changelog that highlights the major differences from the users' perspective. Some major changes for me as a user: user operations (deepdive compile, etc.), extractor output behavior (replace instead of insert), deprecated features (json extractor, etc.).\n. I see. Makes sense!\n. Maybe we should test it on a larger factor graph (> 1GB) to see how it performs?\n. Seems there's a huge saving in space with negligible overhead! Merging.\n. Will fix this soon.\n. @alldefector Sorry I'm not quite following here. It seems this can be done using multinomial variables or boolean with one-is-true constraints. Could you give a concrete example that requires more support in the system?\n. Looks good. Merging.\n. @alldefector The sampler does not require weight sorted by id in the binary input (graph.weights). It will sort the weights by id after loading the factor graph. https://github.com/HazyResearch/sampler/blob/master/src/dstruct/factor_graph/factor_graph.cpp#L222\n. Duplicate in #77. Added in feiran-multinomial branch\n. The most recent one is not configured by me, but I can check it. @zifeishan \n. It's still 1.6. (I don't have permission to modify it) @zifeishan \n. Chunking example\n. Oh you are right!\n. @dennybritz Thanks for the comments!\nCardinality of binary, just a minor thing. I see the cardinality of binary is set to 0 or sometimes 1. It should be 2, but if this field is of no use for binary, it's ok...\nThe csv serializer is for test. I will remove it later. I'll add unit tests for the binary format.\n. Thanks. Will look into it.\n. @dennybritz The test issue is due to issuing a query without closing the connection. It has been fixed :)\n. Default sampler args has been changed.\n. skip_learning and weight table are supported in grounding.\n. Seem to work, but I don't know how to reproduce Tai's bug.\nAlso, I think skip learning should set sampler args -l 0 rather than set isfixed = true... setting isfixed doesn't have correct semantics...\n. Yes, I added the code by commented out...\n. Two variables are not in the same table. We have label1, label2. The problem is id is declared as primary key. This can be solved by declaring id as bigint.\n```\n    CREATE TABLE label1(\n    id    BIGSERIAL PRIMARY KEY, \n    wid   INT, val BOOLEAN);\nCREATE TABLE label2(\nid    BIGSERIAL PRIMARY KEY, \nwid   INT, val BOOLEAN);\n\n```\n. Yes, I fixed it locally. I'll push it later on.\n. Done in b0d123aa87108a621218907ab9fb7b68524b6186\n. Done in b805a85df6625fbcdc1ef15a4bbcd9c8399f65fd\n. Done in cf5975e08aa21971f5efdc712beb0c8209013839\n. Done in edc4296bf9c0dd4ea3be2c18841eb32b717cdbbf\n. duplicate issue\n. needs full log to see what happened...\n. Fixed in f7abdd0b2da84f2122f191e9ca71021cecb55472\n. set log level to INFO, will do before release\n. Yes. I'll look into it.\n. Since id will not be allowed to use by users, the tables should not be distributed by id (this column may be empty). We can tell users not to distribute by id if they are using gp\n. Fixed in f7abdd0b2da84f2122f191e9ca71021cecb55472\n. I'm dealing with sampler issue right row...\nAfterwards, I can do \nAdd new application.conf syntax (like relearn, extrcactor type) to unit tests\nLogistic Regression test is commented out right now\nbut I don't know how to test extractor...\n. It's no longer supported... because we don't use this table any more...\n. Yes, tested.\n. I checked ocr, and it's pretty reasonable. I also tested on pharmgkb, here's what I got\n\n. For spouse example\n-l 300 -i 500 --alpha 0.1\n\n. My guess is there is a bug in spouse example. Maybe the variables are features are not correctly connected.\n. Will look into it.\n. I can run on madmax1\n09:41:50 [profiler] INFO  ext_people SUCCESS [38500 ms]\n09:41:50 [profiler] INFO  ext_has_spouse_candidates SUCCESS [13672 ms]\n09:41:50 [profiler] INFO  ext_has_spouse_features SUCCESS [68510 ms]\n09:41:50 [profiler] INFO  inference_grounding SUCCESS [5617 ms]\n09:41:50 [profiler] INFO  inference SUCCESS [10198 ms]\n09:41:50 [profiler] INFO  calibration plot written to /lfs/madmax/0/feiran/deepdive/out/2014-05-05T093932/calibration/has_spouse.is_true.png [0 ms]\n09:41:50 [profiler] INFO  calibration SUCCESS [926 ms]\nseems to me it's assigning id problem, i'll test again\n. Test using rokcy's database, it runs...\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  Writing grounding queries to file=\"/tmp/grounding3399495869192337498.sql\" \n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  Using Greenplum = true\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  Executing grounding query...\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  Executing: \"/tmp/assignId5585274005363230742.sh\" \n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  NOTICE:  drop cascades to function _fast_seqassign(character varying,bigint)\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  NOTICE:  drop cascades to function fast_seqassign(character varying,bigint)\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  NOTICE:  drop cascades to function updateid(bigint,integer,integer[],bigint[],bigint[])\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  NOTICE:  drop cascades to function clear_count_1(integer)\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  CREATE FUNCTION\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  NOTICE:  table \"tmp_gpsid_count\" does not exist, skipping\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  CONTEXT:  SQL statement \"drop table if exists tmp_gpsid_count cascade;\"\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  PL/pgSQL function \"fast_seqassign\" line 2 at execute statement\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  NOTICE:  table \"tmp_gpsid_count_noagg\" does not exist, skipping\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  CONTEXT:  SQL statement \"drop table if exists tmp_gpsid_count_noagg cascade;\"\n10:05:24 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  PL/pgSQL function \"fast_seqassign\" line 3 at execute statement\n10:05:25 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  NOTICE:  EXECUTING _fast_seqassign()...\n10:05:26 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO   fast_seqassign \n10:05:26 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  ----------------\n10:05:26 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO   \n10:05:26 [PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)] INFO  (1 row)\n. will push again\n. Fixed in d47f817a6e9f352528a2f70216ca22603c4a4498\n. also, sampler name should be\nsampler-dw-linux\nsampler-dw-mac\n. I think we can remove this...\nThe system fails to shut down in many cases...\n. test.sh under root directory\n. Ce told to open this issue... not sure the problem...\n. I thought it was due to the extractor... what is Amir's problem?\nMy problem happens in  deepdive/src/test/scala/integration/LogisticRegression.scala. I changed it to the current syntax.\nUncomment it and run the tests, I get keyError: 'id'...\n. It's already there, just commented out :)\n. I think Makefile has been added. Closing.\n. To my understanding, this is no longer needed since the variable ids are handled by deepdive not by the user.\n. Sounds good!\n. The weight regularization is done by shrinking the weights during each SGD epoch:\nweight = weight * 1/(1+0.01*current_stepsize) \nIt's equivalent to l1 regularization, which is determined by stepsize. There is no separate parameter for regularization. \n. @zifeishan This is this solved I think?\n. Thanks! I didn't change logback.xml, am I supposed to change something? \nSen and I have the same problem for both master and develop. (we were not using deepdive binary)\n. seems to be working... report later if any problem occurs\n. The problem indeed exists.\n. I observed this problem 15 days ago. Not sure when it first appeared...\n. please see my email for my answers.\n. Should have been solved in Sep.\n. Fixed. Added comments for future developers to pay attention to id problem.\n. Yes, also merged into develop. Thanks!\n. @zhangce It seems travis does not naturally support greenplum. http://docs.travis-ci.com/user/database-setup/\n. @rionda It's only for Linux, since Macs do not have numa architecture, sampler for mac will only keep the original factor graph data.\n. @gengyl08 I'll push my changes after travis is passed\n. Got this error... maybe accidental issue\nThe command \"bash ./src/test/bash/linkcheck.sh http://deepdive.stanford.edu/\" exited with 1.\n. Turned out there's no problem.\n. Looks good to me!\n. The link seems to be fixed already. It links to a crappy tutorial I wrote...\n. ok I see\n. Not yet... will test it. Thanks! @zifeishan \n. This doesn't have conflict with refactoring for now. We'll figure out how to deal with greenplum unloading when the streaming interface is ready to go.\n. Done in another pull request.\n. @zifeishan Do you have any idea on this?\n. The normal workflow for deepdive would be assigning id before assigning holdout. This error means your ids are probably not assigned properly. Can you paste the full log?\n.  We discussed this could be an akka issue. I wasn't able to reproduce this issue since we discussed, and looking through travis, this one didn't happen in last two months. \n@adamwgoldberg are you still having this issue?\n. @mikecafarella The pull request has been updated in response to the code review.\n. @mikecafarella Thanks! I'll look into that.\n. @mikecafarella The tmp files are now put into deepdive's folder. This folder is used to store other tmp files, and should be large enough.\n. There's a bug in imply function in sampler. I think Ce fixed it. Please check the compiled binary (release/dw_linux) under HazyResearch/sampler.\n. Sure. That's what I'm thinking. I'll prepare samplers that include all these fixes for release.\n. Hi @Pede23, thanks for reporting the issue. It seems we don't have this problem for all the machines we have tested. Maybe using a newer version of PostgreSQL (> 9) can solve this problem.\n. We use a fixed length encoding for multinomial cardinality. Previously it's 5 digits, which supports up to 1e5 cardinality. I extended to 10 digits. Multinomial may be refactored later, but this PR is just an extension for cardinality.\n. It's different from #271 as this is for inference results. This feature is added in sample_evidence branch, I'll add it to release.\n. which branch are you using?\n. The problem is not in the sampler. The log info has nfac : 0 and nedge: 0, which indicates the factor graph is not correct. Can you post the full log (by running make test)?\n. line 3041 sh: 1: bc: not found\nSorry we should have written this clearly: Deepdive makes use of bash calculator bc. I think the problem will be gone if you install bc.\n. I'm not sure why 20GB factor graph crashes the machine. I experimented with 60GB factor graph on raiders, and it works fine.\nThe doc is under http://deepdive.stanford.edu/doc/basics/sampler.html\n. I think this commit https://github.com/HazyResearch/deepdive/commit/0253dd711e403d577814efcab791d92c0ff79577 might be better. It handles psql/mysql issue .\nShould I put that in?\n. It's in SQLInferenceRunner. It has been commented out in develop branch. I agree we should drop it and move it to braindump or dashboard. @zifeishan \n. closing... cannot automatically merge\n. Looks good. Since master is always failing now... I'll merge it.\n. I'll look into it, and construct a test in deepdive.\n. @raphaelhoffmann I tried to run your example, and the sampler gave me the same inference result (0.5). I'm not sure why it has different results. I'll continue checking this.\nHowever, I do find a bug that results in wrong number of variables for each factor. I'll fix that soon. \n. @raphaelhoffmann Sorry I wasn't able to reproduce that on my mac. Maybe try cleaning up before compiling by running make clean?\n. @raphaelhoffmann I am using this Apple LLVM version 6.1.0 (clang-602.0.49) (based on LLVM 3.6.0svn). I'll check in the fix I mentioned above and see if that is related.\n. I'll check this.\n. I think I found the problem. I'll add test after fixing this issue. \n. @raphaelhoffmann I fixed this in hotfix_array branch. Now the results should make sense. The results should be\ngroup 1: P(v = 1) = (e^3)     / (1 + e^3)  = 0.95\ngroup 2: P(v = 1) = (e^3 + 1) / (3 + e^3)  = 0.91\ngroup 3: P(v = 1) = (e^3 + 3) / (7 + e^3)  = 0.85\ngroup 4: P(v = 1) = (e^3 + 7) / (15 + e^3) = 0.77\nwhere v is a variable in the group.\n. New tests will be merged soon.\n. @zhangce I'll fix this.\n. @zhangce Sure, I'll take care of it. Yes, we did this before. I found there's a bug in update function which will cause problem if we sample non-evidence variables. I'll fix it as well.\n. Fixed in https://github.com/HazyResearch/sampler/commit/ca3e8d45ef1c1c7d8badd16f82eb02847118d744.\n. Currently we have biased coin test for boolean and CRF test for multinomial under https://github.com/HazyResearch/sampler/tree/master/test. @zhangce what other tests do you think can be added?\n. Added partially observed variable test in https://github.com/HazyResearch/sampler/commit/9296be214f40acda711d84ff4045154deccc5f2d.\n. Sure, I'll check with Zifei.\n. @zhangce Here is the commit https://github.com/HazyResearch/sampler/commit/ca3e8d45ef1c1c7d8badd16f82eb02847118d744. The change is sampling all variables during learning.\n. @zhangce I think the code follows the above procedure. Case 2 is reflected in https://github.com/HazyResearch/sampler/blob/master/src/app/gibbs/single_thread_sampler.cpp#L50.\nIf v is not evidence, we flip it in I_e (line 50 - 66), and flip it in I (line 68-77), and update weight.\n. @zhangce They're all initialized to 0 for unknown weights.\nDo you think it makes sense in case 2, we flip the variables, but do not use them to update weights, since they do not give information about the weights? Maybe we can talk in more detail.\n. Partial results: the weights do not look correct. There are very large weights (~30) and very small weights (~-30).\n. @raphaelhoffmann Thanks! That's helpful. I also observed similar issues with the new sampler. I'll start from debugging logistic regression (without symmetry rule).\n. Using a smaller step size we can get similar distributions as before. @zhangce do you think this is still an issue? \n. @netj I'm not sure why we need those two tables. There are comments by Ce on https://github.com/HazyResearch/deepdive/blob/develop/src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala#L526. It seems to be used to fix some Greenplum issue according to the comment.\n. @netj I thought you were discussing the greenplum issue... Yes, MultinomialFactorFunction is grounded in a particular order for convenient array-like access to weights in the sampler. (Information about multinomial here http://deepdive.stanford.edu/doc/basics/schema.html, and https://github.com/HazyResearch/sampler/blob/master/src/dstruct/factor_graph/factor_graph.cpp#L51). I'll add comments for this.\n. Added comments in #314\n. @netj Briefly discussed with Raphael, this order by is used to make sure weight ids for multinomial factor are in a particular order. Raphael mentioned this may be achieved by changing the sequence assignment.\n. @netj Added source\n. I think it may be better to keep the default behavior as before and give users the option to change, for backward compatibility.\nThis arg was tested locally. I'll also add a test for this in sampler's tests.\n. Test for sample_evidence was added in sampler's develop branch. I utilized the existing test, and add --sample_evidence to it, and checked the results.\n. This feature is available in develop branch right row. You can search skip_learning in the doc folder. There are some documentation about how to use learned weights to make new predictions. More details will come out soon.\n. @netj Here we need to fill in id column (assign id) for variable table (dd_delta_has_spouse). Not sure I get your point... how to update a column without using UPDATE?\n. @netj I think we need those variable ids to be physically assigned in the variable table. Those variable ids are used in later parts, such as holdout, inference results, etc.\n. @netj I looked at the log again. It's actually id shouldn't be the first column (default distribution key). This is a documented requirement for variable tables in greenplum (http://deepdive.stanford.edu/doc/advanced/greenplum.html). This also holds for non-incremental version. I've updated develop_fix to reflect this. \n@raphaelhoffmann Sorry the problem that this PR is solving is actually not PXL's problem. I fixed the table schema to avoid updating a distribution key, so I think this PR doesn't need to go in. \n. For some temp tables, we use the default distribution key (the first column), and put a proper column as the first column. I think currently we don't write the distribution key explicitly due to two reasons:\n1. we are sharing sql queries between different datastores\n2. we don't know the exact schema of that relation, but we know which column should not be used as distribution key\n. Yes, Jaeho proposed in the next refactoring to replicate all the sql queries for each datastore, so we can optimize each sql query for each datastore individually. \n. Also updated distribution key in incremental mode\n. Looks good.\n. Sorry, there are some refactoring in assigning weight ids, so I put the commits together. It's better if they were separate.\n. @netj Updated. Thanks!\n. Looks good. Merging.\n. Is there an easy way to call deepdive sql inside .bats file?\n. Thanks for the detailed comments! Looking into them.\n. Updated.\n. Thanks for helping on this! I updating the rest.\n. Instead of writing postgresql version-specific code, can we just unify this by using our own script?\n. Thanks! Pushed my commit.\n. Good point. I think that's hard, as the csv is the same...\n. I see...\n. Looks good to me. The tests are comprehensive. \nNot related to this PR, mysql spouse tests fails from time to time. I didn't find obvious problems yet.\n. Sure.\n. FYI, ddlog tutorial is on http://deepdive.stanford.edu/doc/basics/ddlog.html.\n. deepdive load is added in #368. I'll also take care of making initdb to be able to create specific tables.\n. Looks good. Merging.\n. This is done in 0b387b388a2d182e0b4e26f4bff3ba7ee1f74864. Closing.\n. Sorry I think ddlog does not support views right now, it has a problem when accessing columns of a view (In Ce's example, he uses views as input to functions, but not accessing individual columns). For a temporary fix, you can define the schema, and then use it. I'll fix this soon.\nLet me know if you have any further issues. Maybe under ddlog repo and ping me directly :)\n. Sorry about that. Updated.\n. I agree we can maintain the wiki, and put a link on the website instead of putting up the document.\nI'll check the jar stuff.\n. Reworked the modifications so that everything is based on the latest master.\n. Looks good. Merging.\n. Looks good. Merging.\n. Thanks! Will update accordingly.\n. Updated.\n. Synced up with Zifei. The described approach applies to dumping a single table or local join of tables. In DD, an extractor input query could be an arbitrary SQL query, and the approach could not apply to such a general case. We could optimize for specific types of queries using the ad hoc approach, by getting hints from the users, which I think might be too specific.\n. Sure, we can add support for that.\n. Yes, basically our idea is to add parameters from the user to deepdive, and perform local join and dumping.\n. Thanks! Will update accordingly.\n. @netj Updated. Thanks!\n. @chrismre There was a bug introduced by an earlier commit, and it is fixed in #376. The incremental test is stable after merging #376.\n. I'll look into this. Btw, the recommended extractor type is tsv extractor, which is much faster than the json extractor. In tsv extractors, columns names are irrelevant, while the order does.\n. Hi Jaeho, I was trying to update the interface as well as the incremental ddlog program to reflect recent ddlog changes. Since #376 updates ddlog, I think it would be better if #376 is reviewed and I base this work on top of it.\n. @netj Sounds good. Thanks!\n. @zhangce The doc is updated in incremental_update branch. I'm updating the interface while running experiments... \n. @raphaelhoffmann I think what you need is ddlog truncating the output tables before running the extractors, as we usually do in deepdive.conf. In that way, every deepdive run will produce the same output instead of accumulating results from previous runs. \n. The default setting is that deepdive only output probabilities for non-evidence variables. If you also want to see the probabilities for evidence variables, you can turn on the --sample_evidence flag in the sampler. Please refer to http://deepdive.stanford.edu/doc/basics/configuration.html#sampler and http://deepdive.stanford.edu/doc/basics/sampler.html.\n. @raphaelhoffmann This can be achieved by turning on the sampler argument --sample_evidence, which will let the sampler output probabilities for evidence variables as well. The sampler argument could be added in deepdive.conf, and when the app.ddlog is compiled by deepdive run, this option will get incorporated.\n. @alldefector For ddlog application, according to this doc, there can also be a deepdive.conf, which contains extra configuration (sampler args, etc.). For example, deepdive.conf may look like \ndeepdive.sampler.sampler_args: \"-l 200 -s 1 -i 500 --alpha 0.1 --sample_evidence\"\n. Just to add a point for 1, the users don't need to create copies of training data, they just need to turn the sampler flag on. Then, the sampler will sample evidence variables during inference.\n. Updated and rebased so that the .gitignore change is not included.\n. Looking into this.\n. @netj Hi Jaeho, I checked this program works on my recent ddlog, and it seems this problem occurs after merging seojiwon's HazyResearch/ddlog#56.\n. @vsoch The problem is that variable relation has_related_concept is not declared in schema.variables.\n. That's awesome! :)\n. Yes, the sampler already has support for this. https://github.com/HazyResearch/sampler/blob/master/src/dstruct/factor_graph/factor.h#L151\n. Updated. PTAL.\n. @netj I tried this on several examples. deepdive run and  deepdive do all work great. I have a small problem with deepdive do. When I type deepdive do, it gives a list of targets. In smoke example there's a process/grounding/variable/person_has_cancer/assign_id. When I tried to run with that target, it gives an error\nprocess/grounding/variable/person_has_cancer/assign_id: Unknown target\nIs this expected?\n. @netj  I added a .done suffix to the target and it gives the same error\nprocess/grounding/variable/person_has_cancer/assign_id.done: Unknown target\n. I think it's ok to keep the current semantics, and it's good to be able to directly load data into variable relations.\n. It would be great to have a changelog that highlights the major differences from the users' perspective. Some major changes for me as a user: user operations (deepdive compile, etc.), extractor output behavior (replace instead of insert), deprecated features (json extractor, etc.).\n. I see. Makes sense!\n. Maybe we should test it on a larger factor graph (> 1GB) to see how it performs?\n. Seems there's a huge saving in space with negligible overhead! Merging.\n. Will fix this soon.\n. @alldefector Sorry I'm not quite following here. It seems this can be done using multinomial variables or boolean with one-is-true constraints. Could you give a concrete example that requires more support in the system?\n. Looks good. Merging.\n. @alldefector The sampler does not require weight sorted by id in the binary input (graph.weights). It will sort the weights by id after loading the factor graph. https://github.com/HazyResearch/sampler/blob/master/src/dstruct/factor_graph/factor_graph.cpp#L222\n. ",
    "zhangce": "I forked Denny's AMI and created one... So any one else\nshould have the same permission if they fork my AMI,\npublish it, and I delete the old one.\nCe\nOn Wed, Nov 12, 2014 at 3:34 PM, Zifei Shan notifications@github.com\nwrote:\n\nI think Ce @zhangce https://github.com/zhangce created the AMI?\nAlternatively, we can add into the AMI guide to let users install JDK7\nmanually.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/8#issuecomment-62798485.\n. Follow this one (http://deepdive.stanford.edu/doc/advanced/ec2.html) you\ncan create your own AMI.\n\nCe\nOn Fri, Nov 21, 2014 at 8:13 AM, Matteo Riondato notifications@github.com\nwrote:\n\n@zhangce https://github.com/zhangce : Ce, where's your AMI? How do we\nfork it? Sorry for the stupid questions, I never worked with AMIs\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/8#issuecomment-63974717.\n. > > Suggest users to put columns like id / mention_id not at the first\n\ncolumn of table, or GP will auto-make it as distribution key if no distributed\nby is specified\n\n\nThe correct rule is to ask user add DISTRIBUTED BY clause in their CREATE\nTABLE command...\n\n\ntell users how to choose distribution key (to minimize skew and locale\nbetter)\n\n\nTell them there is a GP manual............\nCe\nOn Sat, May 3, 2014 at 2:44 PM, Zifei Shan notifications@github.com wrote:\n\nThere're several caveats to use our new system with GP. It's good if we\nmake a new documentation for these:\n1.\ndistribution key\n    - variable id column cannot be the distribution key (important!)\n      @feiranwang https://github.com/feiranwang\n      - any column that needs sequential assignment (e.g. \"mention_id\" in\n      spouse example) cannot be the distribution key\n      - Suggest users to put columns like id / mention_id not at the\n      first column of table, or GP will auto-make it as distribution key if no distributed\n      by is specified\n      - tell users how to choose distribution key (to minimize skew and\n      locale better)\n    2.\nHow to optimize queries in distributed database like GP\n   3.\nany other experience is welcomed..\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/HazyResearch/deepdive/issues/62\n.\n. @rionda Thanks! Patches for 2 are on the way :-)\n\nCe\n. @rionda @zifeishan @feiranwang \n\n\nBy the way, this doesn't work with GreenPlum:\n\n\nCan we add GP to Travis?\nCe\n. Does mysql needs DROP VIEW IF EXISTS?\nCe\nOn Mon, Dec 8, 2014 at 3:33 PM, Matteo Riondato notifications@github.com\nwrote:\n\n@zifeishan https://github.com/zifeishan @zhangce\nhttps://github.com/zhangce The error given by travis in MySQL is\ndefinitively reproduceable (it just happened again). Here's the text:\n21:19:59 [DataLoader(akka://deepdive)] INFO  mysql  deepdive_test  -u root  -P 3306  -h 127.0.0.1  --silent -N -e \"SELECT * FROM _dd_variables_has_spouse_view\" > out/test_spouse/dd_variables_has_spouse\n21:19:59 [Helpers$(akka://deepdive)] INFO  Executing command: \"/tmp/unload3355120762408015864.sh\"\n21:20:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:21:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:22:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:23:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:24:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:24:52 [DataLoader(akka://deepdive)] DEBUG Executing query via JDBC: DROP VIEW _dd_variables_has_spouse_view\n21:24:52 [DataLoader(akka://deepdive)] ERROR com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure\nThe last packet successfully received from the server was 292,937 milliseconds ago.  The last packet sent successfully to the server was 13 milliseconds ago.\n21:24:52 [profiler] DEBUG ending report_id=inference_grounding\n21:24:52 [taskManager] INFO  Completed task_id=inference_grounding with Failure(java.sql.SQLException: Already closed.)\n21:24:52 [taskManager] ERROR task=inference_grounding Failed: java.sql.SQLException: Already closed.\n21:24:52 [taskManager] ERROR Forcing shutdown\n21:24:52 [taskManager] ERROR Cancelling task=calibration\n21:24:52 [taskManager] ERROR Cancelling task=inferencetest/test_mysql.sh: line 60: 17791 Killed                  sbt \"test-only org.deepdive.test.integration.MysqlSpouseExample -- -oF\"\nIt happens when executing line 589 of SQLInferenceDataStore.java .\nIt looks almost like a connection timeout to me...any idea?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/180#issuecomment-66191374.\n. I just reproduced this bug, will fix it today\n\nCe\nOn Feb 2, 2015 3:05 AM, \"chrismre\" notifications@github.com wrote:\n\nWhat is going on with this? Is this fixed?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/204#issuecomment-72414060\n.\n. Sorry for the lack of update there.\n\nThe Wiki, BHL, PLOS are updated to the correct format. BMC and PMC \nwill be updated ETA before Wednesday (Usually couple days just to copy\na data set to the Web server).\nCe\n. @feiranwang can you construct a unit test and check into the sampler repo?\nI can fix the code as long as something fails.\nThanks!\nCe\nOn Apr 27, 2015 8:19 PM, \"chrismre\" notifications@github.com wrote:\n\n@feiranwang https://github.com/feiranwang @zhangce\nhttps://github.com/zhangce @netj https://github.com/netj Take a look\nplease, and let's add in some regression here... :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/268#issuecomment-96869888\n.\n. @feiranwang can you check in the fix we did before for this and deploy the\nbinary? (I think you have the latest version of the patch for this\n(~Feb~March))\n\nCe\nOn Apr 28, 2015 6:47 PM, \"chrismre\" notifications@github.com wrote:\n\n@zhangce https://github.com/zhangce take a look at this :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/269#issuecomment-97264253\n.\n. We need some graph where there are hidden variables that do not have\nevidence during learning.\n\nA(evid) -- B(not evid) connected by factor (A==B)\nYou have training data\n    1                  1\n    1                  1\n    1                  1\n    1                  0\nYou should expect the factor has a weight ~1.09 (log 3) if B is sampled.\nOtherwise, you should expect the factor has a very small negative weight if\nB is init'ed with 0.\nOr, you should expect the factor has a very large positive weight if B is\ninit'ed with 1.\nCe\nOn Thu, Apr 30, 2015 at 12:47 PM, Feiran Wang notifications@github.com\nwrote:\n\nCurrently we have biased coin test for boolean and CRF test for\nmultinomial under https://github.com/HazyResearch/sampler/tree/master/test.\n@zhangce https://github.com/zhangce what other tests do you think can\nbe added?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/269#issuecomment-97895910\n.\n. > >  As I understand, this happens a\n\nlong time ago, when we decided to do grounding by assigning an id column\nto each row in the variable table. Sampler would require different\nvariables to have different ids, but it is not possible under the current\ngrounding framework since each table that contain variables only have one\nid column.\n\n\nI think we did not decide to add this constraint (\"One variable columns in\na same table\") due to whether our \"grounding framework\" can support it or\nnot :) -- If this constraint does not make sense, we should not alter the\nsemantic\njust because it is easy to write grounding or the sampler...\nI think the reason we made this decision is that we thought adding this\nconstraints will make it clear that each tuple in the database corresponds\nto\none random variable. We thought this makes the semantic more clear to\nthe user; in the mean time, does not hurt our expressive power.\nCe\nOn Thu, Apr 30, 2015 at 11:38 PM, chrismre notifications@github.com wrote:\n\nSure, but how did the docs get so far out of sync?!?\nOn Thu, Apr 30, 2015 at 9:37 PM, Zifei Shan notifications@github.com\nwrote:\n\n@chrismre https://github.com/chrismre As I understand, this happens a\nlong time ago, when we decided to do grounding by assigning an id column\nto each row in the variable table. Sampler would require different\nvariables to have different ids, but it is not possible under the current\ngrounding framework since each table that contain variables only have one\nid column.\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/issues/277#issuecomment-98041731>\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/277#issuecomment-98042106\n.\n. @feiranwang What changes have been done on Apr 29?\n\n@raphaelhoffmann For the spouse example, does the old sampler (the one with lower variance) produces the \"same quality\" result as the \"new sampler\"?\nCe\n. @feiranwang I am confused by this code.\nThe learning works as follows. There are two set of possible worlds, one contains\nall possible worlds that are consistent with the evidence (call it I_e), another are\nfree to choose values on evidence values (call it I).\nThe goal of learning is to make sure the expectation of factors matches between I_e and I.\nTherefore, we need to draw samples from I_e and I.\n1. if v is evidence, you consider to flip it in I; you do not care I_e because it cannot change in I_e\n 2. if v is not evidence, you consider to flip it in I; then you consider to flip it in I_e\nNow, in 2, you only flip it in I but not I_e? Is this where the bug sits?\nCe\n. @feiranwang ah sorry, I understand it now.\nHow are the weight being initialized now? Random?\nCe\n. @netj draws a similar point in https://github.com/HazyResearch/sampler/commit/9296be214f40acda711d84ff4045154deccc5f2d#commitcomment-11008261 that we could easily use the \"format-converter in DeepDive\" to produce the binary given a text input. I think this should be pretty easy to do (maybe also with the Bats thing for testing that @netj used in that ticket?)\nCe\n. I think it looks good.\nCe\nOn Wed, May 27, 2015 at 10:56 PM, Zifei Shan notifications@github.com\nwrote:\n\n@zhangce https://github.com/zhangce Should we merge and deploy this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/307#issuecomment-106163968\n.\n. > > @zhangce https://github.com/zhangce Do we want to link this banner to\n\nsomewhere? Currently it links to nowhere.\n\n\nI think it is OK because it is a general description of DD.\nCe\nOn Thu, May 28, 2015 at 1:56 AM, Zifei Shan notifications@github.com\nwrote:\n\n@zhangce https://github.com/zhangce Do we want to link this banner to\nsomewhere? Currently it links to nowhere.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/307#issuecomment-106200742\n.\n. @netj Can you merge this? This is our current tracking with Google Analytics under the account contact.hazy@gmail.com\n\nThanks!\n. how many cpu cores are busy if you top?\nCan you let me know which machine or some connection string to DB?\nCe\nOn Sep 6, 2015 3:27 PM, \"Raphael Hoffmann\" notifications@github.com wrote:\n\nPerhaps it has to do with the volume /lfs/local/0 being 99% full. We may\nhave to ask people again to free up space.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138132793\n.\n. The finger print of tuples in GP is a little weird (it is too physical in\nthe sense that there are no guarantee of when it will not change at all)...\nneed to do more profiling after Raphael's current run. That function should\nbe faster than 2MB/s if the DB is working properly.\nOn Sep 6, 2015 9:04 PM, \"alldefector\" notifications@github.com wrote:\nOr if the ID fields are used as conceptual foreign keys in the DB, maybe we\ncould use a fingerprint of the tuple.\nOn Sun, Sep 6, 2015 at 8:43 PM Feng Niu niufeng14@gmail.com wrote:\n\nCorrect me if I'm wrong -- looks like all those post-hoc-assigned\nsequence\nIDs are always used as array indexes in the sampler and never for SQL\njoins. If that's true, what are the challenges to do without the sequence\nassignment steps?\nFeng\nOn Sun, Sep 6, 2015 at 5:43 PM Raphael Hoffmann \nnotifications@github.com\nwrote:\n\nSure! The log of the run is here:\n/lfs/local/1/raphaelh/dd/new/deepdive/log/2015-09-06T093453.txt\nDatabase is memex_mar2015_large and it's running inference only. I\nlaunched\nthis job at 9:30AM and it still hasn't reached the sampling phase.\nThanks!\nOn Sun, Sep 6, 2015 at 5:28 PM, SenWu notifications@github.com wrote:\n\nThanks! Could you share the log of your run to us?\n\u2014\nReply to this email directly or view it on GitHub\n<\n\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138140054\n\n.\n\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138144569\n.\n\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138178468\n.\n. @raphaelhoffmann I profile the query SELECT fast_seqassign('dd_query_rates_features', 0); which took 2 hours in your log. I think it is a DB problem instead of how we implement this function.\n\nThe key ratio to see whether the bottleneck is really the fast_seqassign function is to see the ratio of execution time between\n`create table tmp as select * from dd_query_rates_features;`\nand\n`SELECT fast_seqassign('dd_query_rates_features', 0);`\nIf the ratio is off by 1 significantly, that means the bottleneck is fast_seqassign, which I think is not the case here (I did not let the CREATE TABLE query finish--it is not making much progress after 1 hour)...\nI think for this Github issue, we need to restart DB, try to VACUUM etc.\n@alldefector The game of optimizing this query is try to eliminate the one pass of table writing caused by UPDATE (because postgresql does not have in-place update)... We have no global sync for contiguous IDs, so we might not be able to see much improvement if we do non-contiguous IDs... \n@chrismre mentioned yesterday that why we do not use generate_sequence (I forget why I did not use it yesterday... Now I remember :)) The problem is that GP's optimizer (at least in 2013) does not know that tuples with the same gp_segment_id is on the same segment, so I can not get a sequential scan plan with generate_sequence... I do agree that if we change that Python function to C++, we might see some improvement, but we need to profile it first (by reporting the ratio of runtime between the above two queries)\nCe \n. @alldefector Your recommendation makes perfect sense for one fast_seqassign call (there are two such calls that are expensive)\n1. There is one `fast_seqassign` call for variables loaded from the output of extractors. \n2. There is another `fast_seqassign` call for factors output by SQL grounding queries.\nFor 1, we could just have a really fast shell script to assign IDs in TSV before loading. (just like your global sequence ID service idea)\nFor 2, we don't really need ID for factors, so that part can probably be eliminated completely.\n. Did you VACUUM?\nCe\nOn Sep 7, 2015 7:45 PM, \"Raphael Hoffmann\" notifications@github.com wrote:\n\nWe restarted greenplum (thanks, Sen!), free'd up some space, and re-ran\nthis process. The log of the new run is here:\n/lfs/local/1/raphaelh/dd/new/deepdive/log/2015-09-06T093453.txt\nWe are still seeing the same runtimes (3h for sequence assignment of\nrates, 2 or 3h for sequence assignment of features). I didn't yet run the\nCREATE TABLE command Ce suggested, but I suspect that the runtime is indeed\nnormal.\nCe: I think the changes you propose sound great! Any chance you can make a\nfirst stab at it?\nThanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138415509\n.\n. @alldefector @raphaelhoffmann Sorry I am confused on this... The 2:18h number is on raiders4 that you just said the IO bandwidth is 4x lower than than raiders3?... Maybe we should do proper profiling before we jump into discussions of all those details? Let's fix the machine for you first, and then restart our discussion?\n. I think a larger bottleneck seems to be the third query not the sequential\nassign one?\nBut in the next version of the sampler, we will get rid of this query\nsoon...\n\nCe\nOn Sep 8, 2015 6:37 PM, \"Raphael Hoffmann\" notifications@github.com wrote:\n\n@zhangce https://github.com/zhangce. Sounds good, we can re-run the\nexperiment once raiders4 is working properly again.\nHere are a few more measured runtimes for another extractor on an AWS\nr3.8xlarge instance, using a RAID over the local SSD volumes for I/O. I/O\nthroughput is very good on that machine (600-700MB/sec).\n3:00h     running extractor\n1:40h     SELECT copy_table_assign_ids_replace('public', 'dd_query_f_isoutcall_features', 'id', 0)\n3:00h     COPY (SELECT DISTINCT t0.id AS factor_id, t1.id AS weight_id,  \"isoutcall.id\"\n           FROM dd_query_f_isoutcall_features t0, dd_weights_f_isoutcall_features t1\n           WHERE  t0.\"isoutcall.feature\" = t1.\"isoutcall.feature\") TO STDOUT;\n1:45h     CREATE UNLOGGED TABLE dd_isoutcall_vtype AS SELECT t0.id, CASE WHEN t2.variable_id IS NOT NULL AND is_outcall IS NOT NULL THEN 2\n                           WHEN t1.variable_id IS NOT NULL THEN 0\n                           WHEN is_outcall IS NOT NULL THEN 1\n                           ELSE 0\n                      END as dd_variable_type\n          FROM isoutcall t0 LEFT OUTER JOIN dd_graph_variables_holdout t1 ON t0.id=t1.variable_id\n          LEFT OUTER JOIN dd_graph_variables_observation t2 ON t0.id=t2.variable_id\n0:45h     Unloading factor graph to disk (tobinary)\n0:30h     Sampler loading factor graph until crash due to out of memory\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138749010\n.\n. @raphaelhoffmann Lets move this discussion off the public issue to avoid broadcast to everyone (It is getting too much detailed to most subscriber)?\n\nThe 100MB vs. 150MB throughput does not explain the 4x difference you observed? Should we Skype quickly? Sorry some numbers do not quite add up and I am still confused on what is the issue...\nThanks!\nCe\n. @raphaelhoffmann You are right, for the following query\nSELECT DISTINCT t0.id AS factor_id, t1.id AS weight_id, \"ismassageparlorad.id\"\nFROM dd_query_f_ismassageparlorad_features t0, dd_weights_f_ismassageparlorad_features t1\nWHERE t0.\"ismassageparlorad.feature\" = t1.\"ismassageparlorad.feature\"\nThe second relation (dd_weights_f_ismassageparlorad_features) is grounded by running DISTINCT\nover the first relation (dd_query_f_ismassageparlorad_features). Therefore, there is no way this query can generate duplicate result.\n@SenWu Can you document this in the code?\nCe\n. PR https://github.com/HazyResearch/deepdive/pull/397\n. @netj Is anyone doing the pass of documentation here? I can still see\ngrammatical errors in that page (I am happy to do one pass if no one is doing this)\n. > > @zhangce The doc is updated in incremental_update branch. I'm updating the interface while running experiments...\n@thodrek \n. @netj Thanks, I integrated your feedback. \n. @netj Can you help with this one? Sorry I am not that familiar with the current installer of DD. Thanks!\n. @netj The fixed run started, ETA tomorrow afternoon for all datasets. Sorry for the bug here.\n. @netj Maybe these three should happen in the walkthrough? When I \ndescribe the data format in the open data page, I pointed it to the \nwalkthrough... \n. @thodrek Sorry for the confusion, just to be clear, the open data doc is not the core part--you can focus on the main doc (I can deal with everything related to open data opendata/index.md)\n. I see, I will make it clear in the README where to look up. Thanks!\nCe\nOn Sep 26, 2015 8:15 AM, \"Jaeho Shin\" notifications@github.com wrote:\n\n@zhangce https://github.com/zhangce Yes, perhaps adding in the README a\nlink to the right section of the tutorial or even to the opendata page is\nenough. Once people download these datasets, they seem to get detached from\nthe web page and expect things to be self-contained.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/406#issuecomment-143462260\n.\n. @netj is this really necessary? The goal of the teaser data set is to make sure the user can download a small sample instead of the whole file... Can you let me know more details why the teaser data set is not enough such that the user need to download partial full data set with this trick?\n\nIf there is a need to download partial dataset larger than the teaser but smaller than the full, I can add a larger teaser in the middle.\n. > > because we're going to republish them anyway for fixing other bugs.\nOnly 2 out of 8 datasets need republishing. I will make the new ones\n.tar.gz. (I personally like zip better but that's not important :)) For the\nothers, I will leave them untouched for now if it does not impact user\nexperience.\nCe\nOn Sep 26, 2015 8:12 AM, \"Jaeho Shin\" notifications@github.com wrote:\n\nI partly agree this isn't crucial with the teaser. In my case, I was\ntrying to confirm the whole dataset's format was identical to the teaser,\nbut it was too large to download. I thought we could switch to a superior\nformat because we're going to republish them anyway for fixing other bugs.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/407#issuecomment-143462159\n.\n. > > A third question is, I learned from the website that @zhangce has developed a geodeepdive DEMO but could not find any website/software devoted to that. \n\n@ukliu In terms of your third question, you can find the code for PaleoDeepDive at https://github.com/HazyResearch/pdd . You can follow a similar structure for your\napplication. If you want more simple examples (and because you are from Shanan's team), \nJulia also has an application that she builds from scratch over the summer using DeepDive\nthat is potentially more easier to follow; but you probably need to ask Shanan for the program.\nIf you have any Geo-related, Paleo-related, or PDF-acquisition-related questions, we can\ndiscuss via email (and I will point you to the right people to ask).\nThanks!\nCe\n. > > Why not give out the ddlog version? it's easier to use right? and it's the\n\n\npreferred way...\n\n\nYes, ddlog is the preferred way to write applications now. The code in \nhttps://github.com/HazyResearch/pdd is already in ddlog.\n. @ukliu Sorry, just added you.\n. @chrismre The default plpythonu dependency is scheduled to be removed in the coming release (~this weekend?) If users need it, they can install it by themselves; but by default, DD will not require sudo very soon.\n. I think one possible view of this is that each DeepDive run should have its own schema inside a database instance (as in PG's terminology, it is just a group of table with names, the default schema is public)--every DD run creates a new schema inside a DB instance, but never deletes/alters the old schema (we can have some tools to help the user to clean old DD-created tables in those old schema, but that is users' choice) . In case that data in the old schema can be reused (e.g., sentences table), DeepDive manages that reuse (as part of the incremental process). The guarantee is that DeepDive is functional and immutable on the schemas created before the current run.\n. Thanks! I will make these changes.\n\n\nI think these datasets are often consumed/prepared by people who have no idea/interest in what they're processing or how they are actually used in DD\n\n\nI think if the user does not want to use DD to process this data, we do provide the CoNLL format, which is a standard format that user can use.\n\n\nIn fact, the opendata TSVs are slightly different from the walkthrough: they have provenance and word index columns, sentence id appears after document id, the dependency paths are represented differently using two columns. \n\n\nThis might not be a short-term thing, but I feel like we need to change the walkthrough to make it Bazaar-compliant OR make Bazaar walkthrough-compliant. Open data is not the place that we define the schema; Bazaar is the place that we define the schema for both open data and walkthrough to use.\n. @netj I did another pass.\n. @thodrek 's new framework should provide an ultimate cleaner solution here. For short term, it should also be an easy fix. \n. @feiranwang Thanks, Feiran! Yeah, I forget you added this before...\n. @feiranwang @raphaelhoffmann @alldefector \nFollowing is my opinion on this discussion... tl;dr I think it is the user's responsibility to create copies or add the flag to sample evidences (I actually don't think we should ever have a flag to sample evidences, but that is a separate issue).\nThere are couple ways to deal with supervision data:\n1. Assume it is users' responsibility to create copies if they need to create copies;\n2. Directly include it into the inference result without running inference;\n3. Automatically run inference over copies and include them into the inference result.\n1 is what we are doing.\n2 is obviously wrong.\n3 looks good at first glance because it saves you one query or flag. However, \nif 3 is not done correctly, it is very dangerous because it leads to very wrong experimental \ndesign by not decoupling training and testing. How to take advantage of supervised evidence \nbut still ensure the experiment result is not cheating for looking at training example is tricky \nand domain-specific, and I don't think we are ready to define the default here without confusing \nmore users (and give them wrong numbers).\n3 looks easy for the programs that mainly contain regressions. But what is 3's formal semantic in general\nfactor graph? When you have many general correlations between variables, how should these copies been connected with other variables? Should these copies be connected to copies of other variables? Should these copies be connected to the evidences of other variables? I think these decision should be made by the user by creating copies if they know what they are doing.\n. > > We may at least want to emphasize this setting in the tutorials and examples since some users may not be aware.\n@raphaelhoffmann I agree that we should have a tutorial to tell users about different ways of treating distant supervision rules.\nMy comment was only about the topic that whether sample_evidences should be a default setting or not, which I think we should not set it as default.\n. > > 1. Should we give the user an output table that contains all extractions regardless of whether they are supervised or predicted?\n\n\nI suppose everyone would agree that we say yes to question 2. \n\n\nWith @thodrek's principled framework on noisy distant supervision, I would say yes.\nIf we are talking about the current version of DeepDive, and the question is whether we provide that table by default, I would say no to this question. I feel like outputting supervised labels to the user by default as extractions might bring in more confusions , and cause questions about the training/testing splitting on any numbers they get and we reported. This is another line of confusion that we can currently avoid.\nI do agree that we should have a tutorial about this and teach the user how to do whatever they decide to do by themselves, but by default, I think DeepDive should not provide that output table that samples evidence variables unless the user explicitly forced the system to do so.\n. @raphaelhoffmann Where did you define the relation candidates_unnest in the program?\n. @raphaelhoffmann Can you give us the full program to reproduce this? The program you provided should throw this error because you do not give ddlog the schema of candidates_unnest...\n. @raphaelhoffmann Sorry for the confusion, I didn't know that in ddlog we can use a relation without declare its schema.\n. I forked Denny's AMI and created one... So any one else\nshould have the same permission if they fork my AMI,\npublish it, and I delete the old one.\nCe\nOn Wed, Nov 12, 2014 at 3:34 PM, Zifei Shan notifications@github.com\nwrote:\n\nI think Ce @zhangce https://github.com/zhangce created the AMI?\nAlternatively, we can add into the AMI guide to let users install JDK7\nmanually.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/8#issuecomment-62798485.\n. Follow this one (http://deepdive.stanford.edu/doc/advanced/ec2.html) you\ncan create your own AMI.\n\nCe\nOn Fri, Nov 21, 2014 at 8:13 AM, Matteo Riondato notifications@github.com\nwrote:\n\n@zhangce https://github.com/zhangce : Ce, where's your AMI? How do we\nfork it? Sorry for the stupid questions, I never worked with AMIs\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/8#issuecomment-63974717.\n. > > Suggest users to put columns like id / mention_id not at the first\n\ncolumn of table, or GP will auto-make it as distribution key if no distributed\nby is specified\n\n\nThe correct rule is to ask user add DISTRIBUTED BY clause in their CREATE\nTABLE command...\n\n\ntell users how to choose distribution key (to minimize skew and locale\nbetter)\n\n\nTell them there is a GP manual............\nCe\nOn Sat, May 3, 2014 at 2:44 PM, Zifei Shan notifications@github.com wrote:\n\nThere're several caveats to use our new system with GP. It's good if we\nmake a new documentation for these:\n1.\ndistribution key\n    - variable id column cannot be the distribution key (important!)\n      @feiranwang https://github.com/feiranwang\n      - any column that needs sequential assignment (e.g. \"mention_id\" in\n      spouse example) cannot be the distribution key\n      - Suggest users to put columns like id / mention_id not at the\n      first column of table, or GP will auto-make it as distribution key if no distributed\n      by is specified\n      - tell users how to choose distribution key (to minimize skew and\n      locale better)\n    2.\nHow to optimize queries in distributed database like GP\n   3.\nany other experience is welcomed..\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/HazyResearch/deepdive/issues/62\n.\n. @rionda Thanks! Patches for 2 are on the way :-)\n\nCe\n. @rionda @zifeishan @feiranwang \n\n\nBy the way, this doesn't work with GreenPlum:\n\n\nCan we add GP to Travis?\nCe\n. Does mysql needs DROP VIEW IF EXISTS?\nCe\nOn Mon, Dec 8, 2014 at 3:33 PM, Matteo Riondato notifications@github.com\nwrote:\n\n@zifeishan https://github.com/zifeishan @zhangce\nhttps://github.com/zhangce The error given by travis in MySQL is\ndefinitively reproduceable (it just happened again). Here's the text:\n21:19:59 [DataLoader(akka://deepdive)] INFO  mysql  deepdive_test  -u root  -P 3306  -h 127.0.0.1  --silent -N -e \"SELECT * FROM _dd_variables_has_spouse_view\" > out/test_spouse/dd_variables_has_spouse\n21:19:59 [Helpers$(akka://deepdive)] INFO  Executing command: \"/tmp/unload3355120762408015864.sh\"\n21:20:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:21:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:22:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:23:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:24:20 [taskManager] INFO  Memory usage: 95/1963MB (max: 1963MB)\n21:24:52 [DataLoader(akka://deepdive)] DEBUG Executing query via JDBC: DROP VIEW _dd_variables_has_spouse_view\n21:24:52 [DataLoader(akka://deepdive)] ERROR com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure\nThe last packet successfully received from the server was 292,937 milliseconds ago.  The last packet sent successfully to the server was 13 milliseconds ago.\n21:24:52 [profiler] DEBUG ending report_id=inference_grounding\n21:24:52 [taskManager] INFO  Completed task_id=inference_grounding with Failure(java.sql.SQLException: Already closed.)\n21:24:52 [taskManager] ERROR task=inference_grounding Failed: java.sql.SQLException: Already closed.\n21:24:52 [taskManager] ERROR Forcing shutdown\n21:24:52 [taskManager] ERROR Cancelling task=calibration\n21:24:52 [taskManager] ERROR Cancelling task=inferencetest/test_mysql.sh: line 60: 17791 Killed                  sbt \"test-only org.deepdive.test.integration.MysqlSpouseExample -- -oF\"\nIt happens when executing line 589 of SQLInferenceDataStore.java .\nIt looks almost like a connection timeout to me...any idea?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/180#issuecomment-66191374.\n. I just reproduced this bug, will fix it today\n\nCe\nOn Feb 2, 2015 3:05 AM, \"chrismre\" notifications@github.com wrote:\n\nWhat is going on with this? Is this fixed?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/204#issuecomment-72414060\n.\n. Sorry for the lack of update there.\n\nThe Wiki, BHL, PLOS are updated to the correct format. BMC and PMC \nwill be updated ETA before Wednesday (Usually couple days just to copy\na data set to the Web server).\nCe\n. @feiranwang can you construct a unit test and check into the sampler repo?\nI can fix the code as long as something fails.\nThanks!\nCe\nOn Apr 27, 2015 8:19 PM, \"chrismre\" notifications@github.com wrote:\n\n@feiranwang https://github.com/feiranwang @zhangce\nhttps://github.com/zhangce @netj https://github.com/netj Take a look\nplease, and let's add in some regression here... :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/268#issuecomment-96869888\n.\n. @feiranwang can you check in the fix we did before for this and deploy the\nbinary? (I think you have the latest version of the patch for this\n(~Feb~March))\n\nCe\nOn Apr 28, 2015 6:47 PM, \"chrismre\" notifications@github.com wrote:\n\n@zhangce https://github.com/zhangce take a look at this :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/269#issuecomment-97264253\n.\n. We need some graph where there are hidden variables that do not have\nevidence during learning.\n\nA(evid) -- B(not evid) connected by factor (A==B)\nYou have training data\n    1                  1\n    1                  1\n    1                  1\n    1                  0\nYou should expect the factor has a weight ~1.09 (log 3) if B is sampled.\nOtherwise, you should expect the factor has a very small negative weight if\nB is init'ed with 0.\nOr, you should expect the factor has a very large positive weight if B is\ninit'ed with 1.\nCe\nOn Thu, Apr 30, 2015 at 12:47 PM, Feiran Wang notifications@github.com\nwrote:\n\nCurrently we have biased coin test for boolean and CRF test for\nmultinomial under https://github.com/HazyResearch/sampler/tree/master/test.\n@zhangce https://github.com/zhangce what other tests do you think can\nbe added?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/269#issuecomment-97895910\n.\n. > >  As I understand, this happens a\n\nlong time ago, when we decided to do grounding by assigning an id column\nto each row in the variable table. Sampler would require different\nvariables to have different ids, but it is not possible under the current\ngrounding framework since each table that contain variables only have one\nid column.\n\n\nI think we did not decide to add this constraint (\"One variable columns in\na same table\") due to whether our \"grounding framework\" can support it or\nnot :) -- If this constraint does not make sense, we should not alter the\nsemantic\njust because it is easy to write grounding or the sampler...\nI think the reason we made this decision is that we thought adding this\nconstraints will make it clear that each tuple in the database corresponds\nto\none random variable. We thought this makes the semantic more clear to\nthe user; in the mean time, does not hurt our expressive power.\nCe\nOn Thu, Apr 30, 2015 at 11:38 PM, chrismre notifications@github.com wrote:\n\nSure, but how did the docs get so far out of sync?!?\nOn Thu, Apr 30, 2015 at 9:37 PM, Zifei Shan notifications@github.com\nwrote:\n\n@chrismre https://github.com/chrismre As I understand, this happens a\nlong time ago, when we decided to do grounding by assigning an id column\nto each row in the variable table. Sampler would require different\nvariables to have different ids, but it is not possible under the current\ngrounding framework since each table that contain variables only have one\nid column.\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/issues/277#issuecomment-98041731>\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/277#issuecomment-98042106\n.\n. @feiranwang What changes have been done on Apr 29?\n\n@raphaelhoffmann For the spouse example, does the old sampler (the one with lower variance) produces the \"same quality\" result as the \"new sampler\"?\nCe\n. @feiranwang I am confused by this code.\nThe learning works as follows. There are two set of possible worlds, one contains\nall possible worlds that are consistent with the evidence (call it I_e), another are\nfree to choose values on evidence values (call it I).\nThe goal of learning is to make sure the expectation of factors matches between I_e and I.\nTherefore, we need to draw samples from I_e and I.\n1. if v is evidence, you consider to flip it in I; you do not care I_e because it cannot change in I_e\n 2. if v is not evidence, you consider to flip it in I; then you consider to flip it in I_e\nNow, in 2, you only flip it in I but not I_e? Is this where the bug sits?\nCe\n. @feiranwang ah sorry, I understand it now.\nHow are the weight being initialized now? Random?\nCe\n. @netj draws a similar point in https://github.com/HazyResearch/sampler/commit/9296be214f40acda711d84ff4045154deccc5f2d#commitcomment-11008261 that we could easily use the \"format-converter in DeepDive\" to produce the binary given a text input. I think this should be pretty easy to do (maybe also with the Bats thing for testing that @netj used in that ticket?)\nCe\n. I think it looks good.\nCe\nOn Wed, May 27, 2015 at 10:56 PM, Zifei Shan notifications@github.com\nwrote:\n\n@zhangce https://github.com/zhangce Should we merge and deploy this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/307#issuecomment-106163968\n.\n. > > @zhangce https://github.com/zhangce Do we want to link this banner to\n\nsomewhere? Currently it links to nowhere.\n\n\nI think it is OK because it is a general description of DD.\nCe\nOn Thu, May 28, 2015 at 1:56 AM, Zifei Shan notifications@github.com\nwrote:\n\n@zhangce https://github.com/zhangce Do we want to link this banner to\nsomewhere? Currently it links to nowhere.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/307#issuecomment-106200742\n.\n. @netj Can you merge this? This is our current tracking with Google Analytics under the account contact.hazy@gmail.com\n\nThanks!\n. how many cpu cores are busy if you top?\nCan you let me know which machine or some connection string to DB?\nCe\nOn Sep 6, 2015 3:27 PM, \"Raphael Hoffmann\" notifications@github.com wrote:\n\nPerhaps it has to do with the volume /lfs/local/0 being 99% full. We may\nhave to ask people again to free up space.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138132793\n.\n. The finger print of tuples in GP is a little weird (it is too physical in\nthe sense that there are no guarantee of when it will not change at all)...\nneed to do more profiling after Raphael's current run. That function should\nbe faster than 2MB/s if the DB is working properly.\nOn Sep 6, 2015 9:04 PM, \"alldefector\" notifications@github.com wrote:\nOr if the ID fields are used as conceptual foreign keys in the DB, maybe we\ncould use a fingerprint of the tuple.\nOn Sun, Sep 6, 2015 at 8:43 PM Feng Niu niufeng14@gmail.com wrote:\n\nCorrect me if I'm wrong -- looks like all those post-hoc-assigned\nsequence\nIDs are always used as array indexes in the sampler and never for SQL\njoins. If that's true, what are the challenges to do without the sequence\nassignment steps?\nFeng\nOn Sun, Sep 6, 2015 at 5:43 PM Raphael Hoffmann \nnotifications@github.com\nwrote:\n\nSure! The log of the run is here:\n/lfs/local/1/raphaelh/dd/new/deepdive/log/2015-09-06T093453.txt\nDatabase is memex_mar2015_large and it's running inference only. I\nlaunched\nthis job at 9:30AM and it still hasn't reached the sampling phase.\nThanks!\nOn Sun, Sep 6, 2015 at 5:28 PM, SenWu notifications@github.com wrote:\n\nThanks! Could you share the log of your run to us?\n\u2014\nReply to this email directly or view it on GitHub\n<\n\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138140054\n\n.\n\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138144569\n.\n\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138178468\n.\n. @raphaelhoffmann I profile the query SELECT fast_seqassign('dd_query_rates_features', 0); which took 2 hours in your log. I think it is a DB problem instead of how we implement this function.\n\nThe key ratio to see whether the bottleneck is really the fast_seqassign function is to see the ratio of execution time between\n`create table tmp as select * from dd_query_rates_features;`\nand\n`SELECT fast_seqassign('dd_query_rates_features', 0);`\nIf the ratio is off by 1 significantly, that means the bottleneck is fast_seqassign, which I think is not the case here (I did not let the CREATE TABLE query finish--it is not making much progress after 1 hour)...\nI think for this Github issue, we need to restart DB, try to VACUUM etc.\n@alldefector The game of optimizing this query is try to eliminate the one pass of table writing caused by UPDATE (because postgresql does not have in-place update)... We have no global sync for contiguous IDs, so we might not be able to see much improvement if we do non-contiguous IDs... \n@chrismre mentioned yesterday that why we do not use generate_sequence (I forget why I did not use it yesterday... Now I remember :)) The problem is that GP's optimizer (at least in 2013) does not know that tuples with the same gp_segment_id is on the same segment, so I can not get a sequential scan plan with generate_sequence... I do agree that if we change that Python function to C++, we might see some improvement, but we need to profile it first (by reporting the ratio of runtime between the above two queries)\nCe \n. @alldefector Your recommendation makes perfect sense for one fast_seqassign call (there are two such calls that are expensive)\n1. There is one `fast_seqassign` call for variables loaded from the output of extractors. \n2. There is another `fast_seqassign` call for factors output by SQL grounding queries.\nFor 1, we could just have a really fast shell script to assign IDs in TSV before loading. (just like your global sequence ID service idea)\nFor 2, we don't really need ID for factors, so that part can probably be eliminated completely.\n. Did you VACUUM?\nCe\nOn Sep 7, 2015 7:45 PM, \"Raphael Hoffmann\" notifications@github.com wrote:\n\nWe restarted greenplum (thanks, Sen!), free'd up some space, and re-ran\nthis process. The log of the new run is here:\n/lfs/local/1/raphaelh/dd/new/deepdive/log/2015-09-06T093453.txt\nWe are still seeing the same runtimes (3h for sequence assignment of\nrates, 2 or 3h for sequence assignment of features). I didn't yet run the\nCREATE TABLE command Ce suggested, but I suspect that the runtime is indeed\nnormal.\nCe: I think the changes you propose sound great! Any chance you can make a\nfirst stab at it?\nThanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138415509\n.\n. @alldefector @raphaelhoffmann Sorry I am confused on this... The 2:18h number is on raiders4 that you just said the IO bandwidth is 4x lower than than raiders3?... Maybe we should do proper profiling before we jump into discussions of all those details? Let's fix the machine for you first, and then restart our discussion?\n. I think a larger bottleneck seems to be the third query not the sequential\nassign one?\nBut in the next version of the sampler, we will get rid of this query\nsoon...\n\nCe\nOn Sep 8, 2015 6:37 PM, \"Raphael Hoffmann\" notifications@github.com wrote:\n\n@zhangce https://github.com/zhangce. Sounds good, we can re-run the\nexperiment once raiders4 is working properly again.\nHere are a few more measured runtimes for another extractor on an AWS\nr3.8xlarge instance, using a RAID over the local SSD volumes for I/O. I/O\nthroughput is very good on that machine (600-700MB/sec).\n3:00h     running extractor\n1:40h     SELECT copy_table_assign_ids_replace('public', 'dd_query_f_isoutcall_features', 'id', 0)\n3:00h     COPY (SELECT DISTINCT t0.id AS factor_id, t1.id AS weight_id,  \"isoutcall.id\"\n           FROM dd_query_f_isoutcall_features t0, dd_weights_f_isoutcall_features t1\n           WHERE  t0.\"isoutcall.feature\" = t1.\"isoutcall.feature\") TO STDOUT;\n1:45h     CREATE UNLOGGED TABLE dd_isoutcall_vtype AS SELECT t0.id, CASE WHEN t2.variable_id IS NOT NULL AND is_outcall IS NOT NULL THEN 2\n                           WHEN t1.variable_id IS NOT NULL THEN 0\n                           WHEN is_outcall IS NOT NULL THEN 1\n                           ELSE 0\n                      END as dd_variable_type\n          FROM isoutcall t0 LEFT OUTER JOIN dd_graph_variables_holdout t1 ON t0.id=t1.variable_id\n          LEFT OUTER JOIN dd_graph_variables_observation t2 ON t0.id=t2.variable_id\n0:45h     Unloading factor graph to disk (tobinary)\n0:30h     Sampler loading factor graph until crash due to out of memory\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138749010\n.\n. @raphaelhoffmann Lets move this discussion off the public issue to avoid broadcast to everyone (It is getting too much detailed to most subscriber)?\n\nThe 100MB vs. 150MB throughput does not explain the 4x difference you observed? Should we Skype quickly? Sorry some numbers do not quite add up and I am still confused on what is the issue...\nThanks!\nCe\n. @raphaelhoffmann You are right, for the following query\nSELECT DISTINCT t0.id AS factor_id, t1.id AS weight_id, \"ismassageparlorad.id\"\nFROM dd_query_f_ismassageparlorad_features t0, dd_weights_f_ismassageparlorad_features t1\nWHERE t0.\"ismassageparlorad.feature\" = t1.\"ismassageparlorad.feature\"\nThe second relation (dd_weights_f_ismassageparlorad_features) is grounded by running DISTINCT\nover the first relation (dd_query_f_ismassageparlorad_features). Therefore, there is no way this query can generate duplicate result.\n@SenWu Can you document this in the code?\nCe\n. PR https://github.com/HazyResearch/deepdive/pull/397\n. @netj Is anyone doing the pass of documentation here? I can still see\ngrammatical errors in that page (I am happy to do one pass if no one is doing this)\n. > > @zhangce The doc is updated in incremental_update branch. I'm updating the interface while running experiments...\n@thodrek \n. @netj Thanks, I integrated your feedback. \n. @netj Can you help with this one? Sorry I am not that familiar with the current installer of DD. Thanks!\n. @netj The fixed run started, ETA tomorrow afternoon for all datasets. Sorry for the bug here.\n. @netj Maybe these three should happen in the walkthrough? When I \ndescribe the data format in the open data page, I pointed it to the \nwalkthrough... \n. @thodrek Sorry for the confusion, just to be clear, the open data doc is not the core part--you can focus on the main doc (I can deal with everything related to open data opendata/index.md)\n. I see, I will make it clear in the README where to look up. Thanks!\nCe\nOn Sep 26, 2015 8:15 AM, \"Jaeho Shin\" notifications@github.com wrote:\n\n@zhangce https://github.com/zhangce Yes, perhaps adding in the README a\nlink to the right section of the tutorial or even to the opendata page is\nenough. Once people download these datasets, they seem to get detached from\nthe web page and expect things to be self-contained.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/406#issuecomment-143462260\n.\n. @netj is this really necessary? The goal of the teaser data set is to make sure the user can download a small sample instead of the whole file... Can you let me know more details why the teaser data set is not enough such that the user need to download partial full data set with this trick?\n\nIf there is a need to download partial dataset larger than the teaser but smaller than the full, I can add a larger teaser in the middle.\n. > > because we're going to republish them anyway for fixing other bugs.\nOnly 2 out of 8 datasets need republishing. I will make the new ones\n.tar.gz. (I personally like zip better but that's not important :)) For the\nothers, I will leave them untouched for now if it does not impact user\nexperience.\nCe\nOn Sep 26, 2015 8:12 AM, \"Jaeho Shin\" notifications@github.com wrote:\n\nI partly agree this isn't crucial with the teaser. In my case, I was\ntrying to confirm the whole dataset's format was identical to the teaser,\nbut it was too large to download. I thought we could switch to a superior\nformat because we're going to republish them anyway for fixing other bugs.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/407#issuecomment-143462159\n.\n. > > A third question is, I learned from the website that @zhangce has developed a geodeepdive DEMO but could not find any website/software devoted to that. \n\n@ukliu In terms of your third question, you can find the code for PaleoDeepDive at https://github.com/HazyResearch/pdd . You can follow a similar structure for your\napplication. If you want more simple examples (and because you are from Shanan's team), \nJulia also has an application that she builds from scratch over the summer using DeepDive\nthat is potentially more easier to follow; but you probably need to ask Shanan for the program.\nIf you have any Geo-related, Paleo-related, or PDF-acquisition-related questions, we can\ndiscuss via email (and I will point you to the right people to ask).\nThanks!\nCe\n. > > Why not give out the ddlog version? it's easier to use right? and it's the\n\n\npreferred way...\n\n\nYes, ddlog is the preferred way to write applications now. The code in \nhttps://github.com/HazyResearch/pdd is already in ddlog.\n. @ukliu Sorry, just added you.\n. @chrismre The default plpythonu dependency is scheduled to be removed in the coming release (~this weekend?) If users need it, they can install it by themselves; but by default, DD will not require sudo very soon.\n. I think one possible view of this is that each DeepDive run should have its own schema inside a database instance (as in PG's terminology, it is just a group of table with names, the default schema is public)--every DD run creates a new schema inside a DB instance, but never deletes/alters the old schema (we can have some tools to help the user to clean old DD-created tables in those old schema, but that is users' choice) . In case that data in the old schema can be reused (e.g., sentences table), DeepDive manages that reuse (as part of the incremental process). The guarantee is that DeepDive is functional and immutable on the schemas created before the current run.\n. Thanks! I will make these changes.\n\n\nI think these datasets are often consumed/prepared by people who have no idea/interest in what they're processing or how they are actually used in DD\n\n\nI think if the user does not want to use DD to process this data, we do provide the CoNLL format, which is a standard format that user can use.\n\n\nIn fact, the opendata TSVs are slightly different from the walkthrough: they have provenance and word index columns, sentence id appears after document id, the dependency paths are represented differently using two columns. \n\n\nThis might not be a short-term thing, but I feel like we need to change the walkthrough to make it Bazaar-compliant OR make Bazaar walkthrough-compliant. Open data is not the place that we define the schema; Bazaar is the place that we define the schema for both open data and walkthrough to use.\n. @netj I did another pass.\n. @thodrek 's new framework should provide an ultimate cleaner solution here. For short term, it should also be an easy fix. \n. @feiranwang Thanks, Feiran! Yeah, I forget you added this before...\n. @feiranwang @raphaelhoffmann @alldefector \nFollowing is my opinion on this discussion... tl;dr I think it is the user's responsibility to create copies or add the flag to sample evidences (I actually don't think we should ever have a flag to sample evidences, but that is a separate issue).\nThere are couple ways to deal with supervision data:\n1. Assume it is users' responsibility to create copies if they need to create copies;\n2. Directly include it into the inference result without running inference;\n3. Automatically run inference over copies and include them into the inference result.\n1 is what we are doing.\n2 is obviously wrong.\n3 looks good at first glance because it saves you one query or flag. However, \nif 3 is not done correctly, it is very dangerous because it leads to very wrong experimental \ndesign by not decoupling training and testing. How to take advantage of supervised evidence \nbut still ensure the experiment result is not cheating for looking at training example is tricky \nand domain-specific, and I don't think we are ready to define the default here without confusing \nmore users (and give them wrong numbers).\n3 looks easy for the programs that mainly contain regressions. But what is 3's formal semantic in general\nfactor graph? When you have many general correlations between variables, how should these copies been connected with other variables? Should these copies be connected to copies of other variables? Should these copies be connected to the evidences of other variables? I think these decision should be made by the user by creating copies if they know what they are doing.\n. > > We may at least want to emphasize this setting in the tutorials and examples since some users may not be aware.\n@raphaelhoffmann I agree that we should have a tutorial to tell users about different ways of treating distant supervision rules.\nMy comment was only about the topic that whether sample_evidences should be a default setting or not, which I think we should not set it as default.\n. > > 1. Should we give the user an output table that contains all extractions regardless of whether they are supervised or predicted?\n\n\nI suppose everyone would agree that we say yes to question 2. \n\n\nWith @thodrek's principled framework on noisy distant supervision, I would say yes.\nIf we are talking about the current version of DeepDive, and the question is whether we provide that table by default, I would say no to this question. I feel like outputting supervised labels to the user by default as extractions might bring in more confusions , and cause questions about the training/testing splitting on any numbers they get and we reported. This is another line of confusion that we can currently avoid.\nI do agree that we should have a tutorial about this and teach the user how to do whatever they decide to do by themselves, but by default, I think DeepDive should not provide that output table that samples evidence variables unless the user explicitly forced the system to do so.\n. @raphaelhoffmann Where did you define the relation candidates_unnest in the program?\n. @raphaelhoffmann Can you give us the full program to reproduce this? The program you provided should throw this error because you do not give ddlog the schema of candidates_unnest...\n. @raphaelhoffmann Sorry for the confusion, I didn't know that in ddlog we can use a relation without declare its schema.\n. ",
    "dennybritz": "Done in https://github.com/HazyResearch/deepdive/commit/52bfab47fb9a567f86ebe68f6b6619b5e3a5b161\n. @feiranwang  Hi! I left so minor comments but it looks good to me on a high level! But let's definitely add unit tests for the new serializer(s). \nWhat do you need to fix with the cardinality of binary?\n. Thanks, I will look into this.\nOn Thu, Apr 3, 2014 at 2:13 PM, Feiran Wang notifications@github.comwrote:\n\nError in develop branch but not in master.\n14:05:38.050 [default-dispatcher-2][profiler][Profiler] DEBUG starting report_id=inference_grounding\n14:05:38.051 [default-dispatcher-3][PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)][PostgresInferenceDataStoreComponent$PostgresInferenceDataStore] INFO  Writing grounding queries to file=\"/var/folders/rz/0l6t9_w90hs_k6l6fq7nlsxm0000gn/T/grounding8297874664321351755.sql\"\n14:05:38.052 [default-dispatcher-6][taskManager][TaskManager] INFO  Added task_id=inference\n14:05:38.053 [default-dispatcher-6][taskManager][TaskManager] INFO  0/1 tasks eligible.\n14:05:38.053 [default-dispatcher-6][taskManager][TaskManager] INFO  Tasks not_eligible: Set(inference)\n14:05:38.054 [default-dispatcher-6][taskManager][TaskManager] INFO  Added task_id=calibration\n14:05:38.054 [default-dispatcher-6][taskManager][TaskManager] INFO  0/2 tasks eligible.\n14:05:38.055 [default-dispatcher-6][taskManager][TaskManager] INFO  Tasks not_eligible: Set(inference, calibration)\n14:05:38.056 [default-dispatcher-6][taskManager][TaskManager] INFO  Added task_id=report\n14:05:38.057 [default-dispatcher-6][taskManager][TaskManager] INFO  0/3 tasks eligible.\n14:05:38.058 [default-dispatcher-6][taskManager][TaskManager] INFO  Tasks not_eligible: Set(inference, report, calibration)\n14:05:38.058 [default-dispatcher-6][taskManager][TaskManager] INFO  Added task_id=shutdown\n14:05:38.059 [default-dispatcher-6][taskManager][TaskManager] INFO  0/4 tasks eligible.\n14:05:38.059 [default-dispatcher-6][taskManager][TaskManager] INFO  Tasks not_eligible: Set(shutdown, inference, report, calibration)\n14:05:38.076 [default-dispatcher-3][PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)][PostgresInferenceDataStoreComponent$PostgresInferenceDataStore] INFO  Executing grounding query...\n14:05:38.351 [][][StatementExecutor$$anon$1] ERROR SQL execution failed (Reason: ERROR: invalid input syntax for integer: \"\"\n  Position: 184):\nINSERT INTO dd_graph_weights(initial_value, is_fixed, description) SELECT DISTINCT 0.0 AS wValue, false AS wIsFixed, 'label1-' || (CASE WHEN \"features.feature_id\" IS NULL THEN '' ELSE \"features.feature_id\" END) || \"label1_val_cardinality\" AS wCmd FROM label1_query GROUP BY wValue, wIsFixed, wCmd\n14:05:38.370 [default-dispatcher-3][inferenceManager][OneForOneStrategy] ERROR ERROR: invalid input syntax for integer: \"\"\n  Position: 184\norg.postgresql.util.PSQLException: ERROR: invalid input syntax for integer: \"\"\n  Position: 184\n    at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2157) ~[postgresql-9.2-1003-jdbc4.jar:na]\n    at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1886) ~[postgresql-9.2-1003-jdbc4.jar:na]\n    at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255) ~[postgresql-9.2-1003-jdbc4.jar:na]\n    at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:555) ~[postgresql-9.2-1003-jdbc4.jar:na]\n    at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:417) ~[postgresql-9.2-1003-jdbc4.jar:na]\n    at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:410) ~[postgresql-9.2-1003-jdbc4.jar:na]\n    at org.apache.commons.dbcp.DelegatingPreparedStatement.execute(DelegatingPreparedStatement.java:172) ~[commons-dbcp-1.4.jar:1.4]\n    at org.apache.commons.dbcp.DelegatingPreparedStatement.execute(DelegatingPreparedStatement.java:172) ~[commons-dbcp-1.4.jar:1.4]\n    at scalikejdbc.StatementExecutor$$anonfun$execute$1.apply$mcZ$sp(StatementExecutor.scala:295) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.StatementExecutor$$anonfun$execute$1.apply(StatementExecutor.scala:295) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.StatementExecutor$$anonfun$execute$1.apply(StatementExecutor.scala:295) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.StatementExecutor$NakedExecutor.apply(StatementExecutor.scala:33) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.StatementExecutor$$anon$1.scalikejdbc$StatementExecutor$LoggingSQLAndTiming$$super$apply(StatementExecutor.scala:291) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.StatementExecutor$LoggingSQLAndTiming$class.apply(StatementExecutor.scala:238) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.StatementExecutor$$anon$1.scalikejdbc$StatementExecutor$LoggingSQLIfFailed$$super$apply(StatementExecutor.scala:291) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.StatementExecutor$LoggingSQLIfFailed$class.apply(StatementExecutor.scala:269) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.StatementExecutor$$anon$1.apply(StatementExecutor.scala:291) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.StatementExecutor.execute(StatementExecutor.scala:295) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.DBSession$$anonfun$executeWithFilters$1.apply(DBSession.scala:248) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.DBSession$$anonfun$executeWithFilters$1.apply(DBSession.scala:246) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.LoanPattern$.using(LoanPattern.scala:29) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.package$.using(package.scala:76) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.DBSession$class.executeWithFilters(DBSession.scala:245) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.ActiveSession.executeWithFilters(DBSession.scala:420) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.SQLExecution.apply(SQL.scala:441) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at org.deepdive.inference.SQLInferenceDataStore$$anonfun$4$$anonfun$apply$4.apply(SQLInferenceDataStore.scala:39) ~[classes/:na]\n    at org.deepdive.inference.SQLInferenceDataStore$$anonfun$4$$anonfun$apply$4.apply(SQLInferenceDataStore.scala:38) ~[classes/:na]\n    at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) ~[scala-library.jar:0.13.1]\n    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108) ~[scala-library.jar:0.13.1]\n    at org.deepdive.inference.SQLInferenceDataStore$$anonfun$4.apply(SQLInferenceDataStore.scala:38) ~[classes/:na]\n    at org.deepdive.inference.SQLInferenceDataStore$$anonfun$4.apply(SQLInferenceDataStore.scala:37) ~[classes/:na]\n    at scalikejdbc.DBConnection$$anonfun$autoCommit$1.apply(DB.scala:185) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.DBConnection$$anonfun$autoCommit$1.apply(DB.scala:184) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.LoanPattern$.using(LoanPattern.scala:29) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.package$.using(package.scala:76) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.DBConnection$class.autoCommit(DB.scala:184) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.DB.autoCommit(DB.scala:498) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.DB$$anonfun$autoCommit$2.apply(DB.scala:641) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.DB$$anonfun$autoCommit$2.apply(DB.scala:640) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.LoanPattern$.using(LoanPattern.scala:29) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.package$.using(package.scala:76) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.DB$.autoCommit(DB.scala:640) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at org.deepdive.inference.SQLInferenceDataStore$class.execute(SQLInferenceDataStore.scala:37) ~[classes/:na]\n    at org.deepdive.inference.PostgresInferenceDataStoreComponent$PostgresInferenceDataStore.execute(PostgresInferenceDataStore.scala:19) ~[classes/:na]\n    at org.deepdive.inference.SQLInferenceDataStore$class.groundFactorGraph(SQLInferenceDataStore.scala:536) ~[classes/:na]\n    at org.deepdive.inference.PostgresInferenceDataStoreComponent$PostgresInferenceDataStore.groundFactorGraph(PostgresInferenceDataStore.scala:19) ~[classes/:na]\n    at org.deepdive.inference.InferenceManager$$anonfun$receive$1.applyOrElse(InferenceManager.scala:59) ~[classes/:na]\n    at akka.actor.Actor$class.aroundReceive(Actor.scala:467) ~[akka-actor_2.10-2.3-M2.jar:2.3-M2]\n    at org.deepdive.inference.InferenceManager$PostgresInferenceManager.aroundReceive(InferenceManager.scala:116) ~[classes/:na]\n    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:491) [akka-actor_2.10-2.3-M2.jar:2.3-M2]\n    at akka.actor.ActorCell.invoke(ActorCell.scala:462) [akka-actor_2.10-2.3-M2.jar:2.3-M2]\n    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) [akka-actor_2.10-2.3-M2.jar:2.3-M2]\n    at akka.dispatch.Mailbox.run(Mailbox.scala:219) [akka-actor_2.10-2.3-M2.jar:2.3-M2]\n    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:385) [akka-actor_2.10-2.3-M2.jar:2.3-M2]\n    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library.jar:na]\n    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library.jar:na]\n    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library.jar:na]\n    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library.jar:na]\n14:05:38.372 [default-dispatcher-6][inferenceManager][InferenceManager$PostgresInferenceManager] INFO  Starting\n14:05:38.372 [default-dispatcher-3][factorGraphBuilder][FactorGraphBuilder$PostgresFactorGraphBuilder] INFO  Starting\n\nReply to this email directly or view it on GitHubhttps://github.com/HazyResearch/deepdive/issues/16\n.\n\n\nBest,\nDenny\n. @feiranwang  Fixed in 09059c58ee263d4e60168c88dde7267974841421\nThanks for reporting.\n. @feiranwang  Looks good, I'm working on the Scala sampler right now. Can you move the inference test classes into the test/ directory instead of the main code?\n. @feiranwang I added the new sampler on https://github.com/HazyResearch/deepdive/tree/feiranwang-develop\nHowever, some of the tests seem to be hanging with the new serializier. Can you check why?\n. Great, thanks!\n. We don't have an example application for that right now, but there is documentation at http://deepdive.stanford.edu/doc/calibration.html\n. Thanks for the report, fixed in 5bb6e7f3b12fc8bf0b167507632452436bdde02e\n. Done in https://github.com/HazyResearch/deepdive/commit/52bfab47fb9a567f86ebe68f6b6619b5e3a5b161\n. @feiranwang  Hi! I left so minor comments but it looks good to me on a high level! But let's definitely add unit tests for the new serializer(s). \nWhat do you need to fix with the cardinality of binary?\n. Thanks, I will look into this.\nOn Thu, Apr 3, 2014 at 2:13 PM, Feiran Wang notifications@github.comwrote:\n\nError in develop branch but not in master.\n14:05:38.050 [default-dispatcher-2][profiler][Profiler] DEBUG starting report_id=inference_grounding\n14:05:38.051 [default-dispatcher-3][PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)][PostgresInferenceDataStoreComponent$PostgresInferenceDataStore] INFO  Writing grounding queries to file=\"/var/folders/rz/0l6t9_w90hs_k6l6fq7nlsxm0000gn/T/grounding8297874664321351755.sql\"\n14:05:38.052 [default-dispatcher-6][taskManager][TaskManager] INFO  Added task_id=inference\n14:05:38.053 [default-dispatcher-6][taskManager][TaskManager] INFO  0/1 tasks eligible.\n14:05:38.053 [default-dispatcher-6][taskManager][TaskManager] INFO  Tasks not_eligible: Set(inference)\n14:05:38.054 [default-dispatcher-6][taskManager][TaskManager] INFO  Added task_id=calibration\n14:05:38.054 [default-dispatcher-6][taskManager][TaskManager] INFO  0/2 tasks eligible.\n14:05:38.055 [default-dispatcher-6][taskManager][TaskManager] INFO  Tasks not_eligible: Set(inference, calibration)\n14:05:38.056 [default-dispatcher-6][taskManager][TaskManager] INFO  Added task_id=report\n14:05:38.057 [default-dispatcher-6][taskManager][TaskManager] INFO  0/3 tasks eligible.\n14:05:38.058 [default-dispatcher-6][taskManager][TaskManager] INFO  Tasks not_eligible: Set(inference, report, calibration)\n14:05:38.058 [default-dispatcher-6][taskManager][TaskManager] INFO  Added task_id=shutdown\n14:05:38.059 [default-dispatcher-6][taskManager][TaskManager] INFO  0/4 tasks eligible.\n14:05:38.059 [default-dispatcher-6][taskManager][TaskManager] INFO  Tasks not_eligible: Set(shutdown, inference, report, calibration)\n14:05:38.076 [default-dispatcher-3][PostgresInferenceDataStoreComponent$PostgresInferenceDataStore(akka://deepdive)][PostgresInferenceDataStoreComponent$PostgresInferenceDataStore] INFO  Executing grounding query...\n14:05:38.351 [][][StatementExecutor$$anon$1] ERROR SQL execution failed (Reason: ERROR: invalid input syntax for integer: \"\"\n  Position: 184):\nINSERT INTO dd_graph_weights(initial_value, is_fixed, description) SELECT DISTINCT 0.0 AS wValue, false AS wIsFixed, 'label1-' || (CASE WHEN \"features.feature_id\" IS NULL THEN '' ELSE \"features.feature_id\" END) || \"label1_val_cardinality\" AS wCmd FROM label1_query GROUP BY wValue, wIsFixed, wCmd\n14:05:38.370 [default-dispatcher-3][inferenceManager][OneForOneStrategy] ERROR ERROR: invalid input syntax for integer: \"\"\n  Position: 184\norg.postgresql.util.PSQLException: ERROR: invalid input syntax for integer: \"\"\n  Position: 184\n    at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2157) ~[postgresql-9.2-1003-jdbc4.jar:na]\n    at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1886) ~[postgresql-9.2-1003-jdbc4.jar:na]\n    at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255) ~[postgresql-9.2-1003-jdbc4.jar:na]\n    at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:555) ~[postgresql-9.2-1003-jdbc4.jar:na]\n    at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:417) ~[postgresql-9.2-1003-jdbc4.jar:na]\n    at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:410) ~[postgresql-9.2-1003-jdbc4.jar:na]\n    at org.apache.commons.dbcp.DelegatingPreparedStatement.execute(DelegatingPreparedStatement.java:172) ~[commons-dbcp-1.4.jar:1.4]\n    at org.apache.commons.dbcp.DelegatingPreparedStatement.execute(DelegatingPreparedStatement.java:172) ~[commons-dbcp-1.4.jar:1.4]\n    at scalikejdbc.StatementExecutor$$anonfun$execute$1.apply$mcZ$sp(StatementExecutor.scala:295) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.StatementExecutor$$anonfun$execute$1.apply(StatementExecutor.scala:295) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.StatementExecutor$$anonfun$execute$1.apply(StatementExecutor.scala:295) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.StatementExecutor$NakedExecutor.apply(StatementExecutor.scala:33) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.StatementExecutor$$anon$1.scalikejdbc$StatementExecutor$LoggingSQLAndTiming$$super$apply(StatementExecutor.scala:291) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.StatementExecutor$LoggingSQLAndTiming$class.apply(StatementExecutor.scala:238) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.StatementExecutor$$anon$1.scalikejdbc$StatementExecutor$LoggingSQLIfFailed$$super$apply(StatementExecutor.scala:291) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.StatementExecutor$LoggingSQLIfFailed$class.apply(StatementExecutor.scala:269) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.StatementExecutor$$anon$1.apply(StatementExecutor.scala:291) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.StatementExecutor.execute(StatementExecutor.scala:295) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.DBSession$$anonfun$executeWithFilters$1.apply(DBSession.scala:248) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.DBSession$$anonfun$executeWithFilters$1.apply(DBSession.scala:246) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.LoanPattern$.using(LoanPattern.scala:29) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.package$.using(package.scala:76) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.DBSession$class.executeWithFilters(DBSession.scala:245) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.ActiveSession.executeWithFilters(DBSession.scala:420) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.SQLExecution.apply(SQL.scala:441) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at org.deepdive.inference.SQLInferenceDataStore$$anonfun$4$$anonfun$apply$4.apply(SQLInferenceDataStore.scala:39) ~[classes/:na]\n    at org.deepdive.inference.SQLInferenceDataStore$$anonfun$4$$anonfun$apply$4.apply(SQLInferenceDataStore.scala:38) ~[classes/:na]\n    at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) ~[scala-library.jar:0.13.1]\n    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108) ~[scala-library.jar:0.13.1]\n    at org.deepdive.inference.SQLInferenceDataStore$$anonfun$4.apply(SQLInferenceDataStore.scala:38) ~[classes/:na]\n    at org.deepdive.inference.SQLInferenceDataStore$$anonfun$4.apply(SQLInferenceDataStore.scala:37) ~[classes/:na]\n    at scalikejdbc.DBConnection$$anonfun$autoCommit$1.apply(DB.scala:185) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.DBConnection$$anonfun$autoCommit$1.apply(DB.scala:184) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.LoanPattern$.using(LoanPattern.scala:29) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.package$.using(package.scala:76) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.DBConnection$class.autoCommit(DB.scala:184) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.DB.autoCommit(DB.scala:498) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.DB$$anonfun$autoCommit$2.apply(DB.scala:641) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.DB$$anonfun$autoCommit$2.apply(DB.scala:640) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.LoanPattern$.using(LoanPattern.scala:29) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.package$.using(package.scala:76) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at scalikejdbc.DB$.autoCommit(DB.scala:640) ~[scalikejdbc_2.10-1.7.4.jar:1.7.4]\n    at org.deepdive.inference.SQLInferenceDataStore$class.execute(SQLInferenceDataStore.scala:37) ~[classes/:na]\n    at org.deepdive.inference.PostgresInferenceDataStoreComponent$PostgresInferenceDataStore.execute(PostgresInferenceDataStore.scala:19) ~[classes/:na]\n    at org.deepdive.inference.SQLInferenceDataStore$class.groundFactorGraph(SQLInferenceDataStore.scala:536) ~[classes/:na]\n    at org.deepdive.inference.PostgresInferenceDataStoreComponent$PostgresInferenceDataStore.groundFactorGraph(PostgresInferenceDataStore.scala:19) ~[classes/:na]\n    at org.deepdive.inference.InferenceManager$$anonfun$receive$1.applyOrElse(InferenceManager.scala:59) ~[classes/:na]\n    at akka.actor.Actor$class.aroundReceive(Actor.scala:467) ~[akka-actor_2.10-2.3-M2.jar:2.3-M2]\n    at org.deepdive.inference.InferenceManager$PostgresInferenceManager.aroundReceive(InferenceManager.scala:116) ~[classes/:na]\n    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:491) [akka-actor_2.10-2.3-M2.jar:2.3-M2]\n    at akka.actor.ActorCell.invoke(ActorCell.scala:462) [akka-actor_2.10-2.3-M2.jar:2.3-M2]\n    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) [akka-actor_2.10-2.3-M2.jar:2.3-M2]\n    at akka.dispatch.Mailbox.run(Mailbox.scala:219) [akka-actor_2.10-2.3-M2.jar:2.3-M2]\n    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:385) [akka-actor_2.10-2.3-M2.jar:2.3-M2]\n    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library.jar:na]\n    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library.jar:na]\n    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library.jar:na]\n    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library.jar:na]\n14:05:38.372 [default-dispatcher-6][inferenceManager][InferenceManager$PostgresInferenceManager] INFO  Starting\n14:05:38.372 [default-dispatcher-3][factorGraphBuilder][FactorGraphBuilder$PostgresFactorGraphBuilder] INFO  Starting\n\nReply to this email directly or view it on GitHubhttps://github.com/HazyResearch/deepdive/issues/16\n.\n\n\nBest,\nDenny\n. @feiranwang  Fixed in 09059c58ee263d4e60168c88dde7267974841421\nThanks for reporting.\n. @feiranwang  Looks good, I'm working on the Scala sampler right now. Can you move the inference test classes into the test/ directory instead of the main code?\n. @feiranwang I added the new sampler on https://github.com/HazyResearch/deepdive/tree/feiranwang-develop\nHowever, some of the tests seem to be hanging with the new serializier. Can you check why?\n. Great, thanks!\n. We don't have an example application for that right now, but there is documentation at http://deepdive.stanford.edu/doc/calibration.html\n. Thanks for the report, fixed in 5bb6e7f3b12fc8bf0b167507632452436bdde02e\n. ",
    "msushkov": "e.g. EXPLAIN SELECT * FROM t;\n. @feiranwang Apparently this code already exists for the grounding? So we can just call a function from the extractor code.\n. Clarification: this code is not in the grounding pipeline yet.\n. Updated syntax. Need to verify that the output is sensible.\n. fixing now\n. removed that code since the user should set the sampler settings during installation\n. fixing now\n. fixed\n. 18:30:32 [sampler] INFO  Executing: util/sampler-dw-mac gibbs -w /Users/msushkov/Dropbox/Stanford/deepdive/out/2014-05-05T183016/graph.weights -v /Users/msushkov/Dropbox/Stanford/deepdive/out/2014-05-05T183016/graph.variables -f /Users/msushkov/Dropbox/Stanford/deepdive/out/2014-05-05T183016/graph.factors -e /Users/msushkov/Dropbox/Stanford/deepdive/out/2014-05-05T183016/graph.edges -m /Users/msushkov/Dropbox/Stanford/deepdive/out/2014-05-05T183016/graph.meta.csv -o /Users/msushkov/Dropbox/Stanford/deepdive/out/2014-05-05T183016 -l 300 -s 1 -i 500 --alpha 0.1\ndyld: Library not loaded: /Users/feiran/workspace/release/dimmwitted/lib/protobuf-2.5.0/../protobuf/lib/libprotobuf.8.dylib\n  Referenced from: /Users/msushkov/Dropbox/Stanford/deepdive/util/sampler-dw-mac\n  Reason: image not found\n. e.g. EXPLAIN SELECT * FROM t;\n. @feiranwang Apparently this code already exists for the grounding? So we can just call a function from the extractor code.\n. Clarification: this code is not in the grounding pipeline yet.\n. Updated syntax. Need to verify that the output is sensible.\n. fixing now\n. removed that code since the user should set the sampler settings during installation\n. fixing now\n. fixed\n. 18:30:32 [sampler] INFO  Executing: util/sampler-dw-mac gibbs -w /Users/msushkov/Dropbox/Stanford/deepdive/out/2014-05-05T183016/graph.weights -v /Users/msushkov/Dropbox/Stanford/deepdive/out/2014-05-05T183016/graph.variables -f /Users/msushkov/Dropbox/Stanford/deepdive/out/2014-05-05T183016/graph.factors -e /Users/msushkov/Dropbox/Stanford/deepdive/out/2014-05-05T183016/graph.edges -m /Users/msushkov/Dropbox/Stanford/deepdive/out/2014-05-05T183016/graph.meta.csv -o /Users/msushkov/Dropbox/Stanford/deepdive/out/2014-05-05T183016 -l 300 -s 1 -i 500 --alpha 0.1\ndyld: Library not loaded: /Users/feiran/workspace/release/dimmwitted/lib/protobuf-2.5.0/../protobuf/lib/libprotobuf.8.dylib\n  Referenced from: /Users/msushkov/Dropbox/Stanford/deepdive/util/sampler-dw-mac\n  Reason: image not found\n. ",
    "tomMulholland": "I found the error finally. In the raw text tables, there were a few null entries - IDs with no associated text.\n. I found the error finally. In the raw text tables, there were a few null entries - IDs with no associated text.\n. ",
    "SenWu": "\n\nIt involves some code refactoring, unit tests and load function. \nI tested it under postgres, and Sen tested under Greenplum.\n\n\nFirst, are you sure this runs on GP?\n115        GPLOAD:\n116          INPUT:\n122        OUTPUT:\nWhy OUTPUT is in the same level of indentation with GPLOAD?\nYAML is indentation-sensitive. I am expecting this line gives\nczhang@raiders4:~$ gpload -f gpload.yaml\n2014-09-13 21:53:47|ERROR|unexpected key: \"output\"\nDId you guys really run the code you wrote on GP? Or am I \nmissing anything?\n- I fixed this bug and tested it using following code:\n  import scala.sys.process._\n    (\"cp /lfs/local/0/senwu/deepdive_loader/app/kbp-mintz/a.txt /lfs/local/0/senwu/develop/grounding/a.txt\").!!\n    import org.deepdive.datastore.DataLoader\n    val du = new DataLoader\n    du.load(s\"/lfs/local/0/senwu/develop/grounding/a.txt\", s\"ccc\", settings.dbSettings, true)\nThe load function insert the data into table ccc successfully.\n  Please review and double check the code.\n111        DATABASE: ${dbSettings.dbname}\nUSER: ${dbSettings.user}\nHOST: ${dbSettings.host}\nPORT: ${dbSettings.port}\nYou miss the case where dbSettings.dbname matches null. See line 68 in ExtractorRunner.scala\n- We checked the null dbname in the Deepdive.scala line 44-50\n  val dbSettings = settings.dbSettings\n  dbSettings.dbname match {\n    case \"\" =>\n      log.error(s\"parsing dbname failed\")\n      Context.shutdown()\n    case _ =>\n  }\n  Also, shouldn't the password also be here?\n- I have no idea. Feiran, can you answer this question?\n139      val sql = \"\"\"COPY \"\"\" + s\"${tablename} FROM STDIN\"\nYou are assuming the DB client and server are on the same machine. On line\n85 of the same file, YOURSELF uses a different one: \\COPY\n- I have no idea. Feiran, can you answer this question?\n128      val cmd = s\"gpload -f ${loadyaml.getAbsolutePath()}\"\nCan we assume gpload is directly callable? Shouldn't there be an command line\noption specify where is the binary? \n- I have no idea. Feiran, can you answer this question?\n. I don't the we can change the configuration file too frequently. We should remove parallel_grounding parameter and use only one parameter to control paralleLoading and parallelGrounding in dbsettings. @zifeishan \n. I will think about it. Thanks!\nCan I ask which materials(research paper or news article) are you using? I agree these features will have a lot of O if you are using research paper.\nI recommend you remove these features in your own applications. @zifeishan \n. Yes. Basically, you can ignore features with high coverage in examples. @zifeishan \n. @feiranwang will prepare a sampler (fixed all known bugs) for release.\n. @coveralls If I understand coveralls correctly, it will find the source code ./ here, while our source code is not here. We did this https://github.com/HazyResearch/deepdive/blob/sen-coverage/test/submit_coverage.sh to let coveralls access the source code. Do you a better way to do this, e.g., a parameter to set path of source code?\n. Right now, we just use the default distributed key setting (first column) in GP.\n. For the record, Raphael passed all tests on PGXL in develop_fix branch (1a3a2221aa42df258d53cdea70428130ea47ce55). No \"ERROR: Failed to COMMIT the transaction on one or more nodes\" this time.\n. Updated the first two tasks.\n. Updated. Please review.\n. Sure, let me learn something about it.\n. @vsoch Let me rephrase your question. Do you mean whether we can configure Git to accept the our particular signed server certificate for deepdive repo (https remote)?\n. @vsoch Thanks for your clarification. We will add ssl option in next update.\n. @vsoch Can you try our new update in ssl_support branch? In order to use ssl connections.You can specify ?ssl=true in the connection string, e.g., \njdbc:postgresql://localhost:5432/dbname?ssl=true\nAnd Deepdive will connect to database via ssl connection with the non-validating ssl factory.\n. @vsoch Happy to know it works.\n@chrismre Doing doc now.\n. Added doc in https://github.com/HazyResearch/deepdive/commit/4d39e685af090af776fbca5ccb0fa15a9cbcb602.\n. @raphaelhoffmann I think you can delete it.\n. @raphaelhoffmann  Thanks! Could you share the log of your run to us? \n. @raphaelhoffmann From the log file, there is nothing wrong... Please let me know if you have further information.\n@alldefector We use assigned variable IDs in SQL joins to ground edges, you can find it here: https://github.com/HazyResearch/deepdive/blob/1573cb703079b10cb2dcc93b3d67e09d079d628c/src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala#L718.\n. @alldefector If we assign non-contiguous IDs, then we need do remapping non-contiguous IDs to contiguous IDs in sampler.\nIs it possible to tweak the sampler so that we can feed it variables and factors without sequential IDs?\nYes, we can do this. However, we might have following things to take care:\n1. If we don't assign sequential IDs, then we need have unique keys to identify different variables. \n2. Using assigned IDs instead of fingerprints, we can reduce the size of factor graph as much as possible.\n. Thanks! Will update accordingly.\n. Updated.\n. Updated docs. Please check, thanks!.\n. @raphaelhoffmann You lost permission when you re-run DD. You can either re-login or use reauth command to get authenticated access.\n. @raphaelhoffmann It might be your DD installation. Did you install your DD/sbt in afs? If all your installations (sbt, DD) are in lfs, then everything will be fine.\n. @zhangce Yes, I found I add it in our incremental update which it is unnecessary. Sorry about that. Will add specification in the code.\n. Updated.\n. LGTM.\n. Upated. Please check.\n. @raphaelhoffmann We will always run ANALYZE TABLE_NAME after each extractor. For example, in tsv extractor, we have https://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/extraction/ExtractorRunner.scala#L509.\n. Only one comment for corner cases. Looks good other than this.\n. LGTM.\n. I will sync with Zifei and push this feature in another branch.\n. I am not sure it is stable. I only test it on small examples (around 100k docs). But this is really a good opportunity to test my code and put it in. I am talking to Feng about the case. :)\n. Yes, I have already synced with Feng and I will add partition support based on DD v0.8. And I briefly talked with Jaeho about it yesterday and we will come up a concrete plan for it on Wednesday.\n. @BrettMeehan is making progress now. Brett, could you give us some updates here? Thanks.\nBTW: for large factor graph, I suggest using only 1 numa node(the sampler argument is -c 1).\n. > > It involves some code refactoring, unit tests and load function. \n\n\nI tested it under postgres, and Sen tested under Greenplum.\n\n\nFirst, are you sure this runs on GP?\n115        GPLOAD:\n116          INPUT:\n122        OUTPUT:\nWhy OUTPUT is in the same level of indentation with GPLOAD?\nYAML is indentation-sensitive. I am expecting this line gives\nczhang@raiders4:~$ gpload -f gpload.yaml\n2014-09-13 21:53:47|ERROR|unexpected key: \"output\"\nDId you guys really run the code you wrote on GP? Or am I \nmissing anything?\n- I fixed this bug and tested it using following code:\n  import scala.sys.process._\n    (\"cp /lfs/local/0/senwu/deepdive_loader/app/kbp-mintz/a.txt /lfs/local/0/senwu/develop/grounding/a.txt\").!!\n    import org.deepdive.datastore.DataLoader\n    val du = new DataLoader\n    du.load(s\"/lfs/local/0/senwu/develop/grounding/a.txt\", s\"ccc\", settings.dbSettings, true)\nThe load function insert the data into table ccc successfully.\n  Please review and double check the code.\n111        DATABASE: ${dbSettings.dbname}\nUSER: ${dbSettings.user}\nHOST: ${dbSettings.host}\nPORT: ${dbSettings.port}\nYou miss the case where dbSettings.dbname matches null. See line 68 in ExtractorRunner.scala\n- We checked the null dbname in the Deepdive.scala line 44-50\n  val dbSettings = settings.dbSettings\n  dbSettings.dbname match {\n    case \"\" =>\n      log.error(s\"parsing dbname failed\")\n      Context.shutdown()\n    case _ =>\n  }\n  Also, shouldn't the password also be here?\n- I have no idea. Feiran, can you answer this question?\n139      val sql = \"\"\"COPY \"\"\" + s\"${tablename} FROM STDIN\"\nYou are assuming the DB client and server are on the same machine. On line\n85 of the same file, YOURSELF uses a different one: \\COPY\n- I have no idea. Feiran, can you answer this question?\n128      val cmd = s\"gpload -f ${loadyaml.getAbsolutePath()}\"\nCan we assume gpload is directly callable? Shouldn't there be an command line\noption specify where is the binary? \n- I have no idea. Feiran, can you answer this question?\n. I don't the we can change the configuration file too frequently. We should remove parallel_grounding parameter and use only one parameter to control paralleLoading and parallelGrounding in dbsettings. @zifeishan \n. I will think about it. Thanks!\nCan I ask which materials(research paper or news article) are you using? I agree these features will have a lot of O if you are using research paper.\nI recommend you remove these features in your own applications. @zifeishan \n. Yes. Basically, you can ignore features with high coverage in examples. @zifeishan \n. @feiranwang will prepare a sampler (fixed all known bugs) for release.\n. @coveralls If I understand coveralls correctly, it will find the source code ./ here, while our source code is not here. We did this https://github.com/HazyResearch/deepdive/blob/sen-coverage/test/submit_coverage.sh to let coveralls access the source code. Do you a better way to do this, e.g., a parameter to set path of source code?\n. Right now, we just use the default distributed key setting (first column) in GP.\n. For the record, Raphael passed all tests on PGXL in develop_fix branch (1a3a2221aa42df258d53cdea70428130ea47ce55). No \"ERROR: Failed to COMMIT the transaction on one or more nodes\" this time.\n. Updated the first two tasks.\n. Updated. Please review.\n. Sure, let me learn something about it.\n. @vsoch Let me rephrase your question. Do you mean whether we can configure Git to accept the our particular signed server certificate for deepdive repo (https remote)?\n. @vsoch Thanks for your clarification. We will add ssl option in next update.\n. @vsoch Can you try our new update in ssl_support branch? In order to use ssl connections.You can specify ?ssl=true in the connection string, e.g., \njdbc:postgresql://localhost:5432/dbname?ssl=true\nAnd Deepdive will connect to database via ssl connection with the non-validating ssl factory.\n. @vsoch Happy to know it works.\n@chrismre Doing doc now.\n. Added doc in https://github.com/HazyResearch/deepdive/commit/4d39e685af090af776fbca5ccb0fa15a9cbcb602.\n. @raphaelhoffmann I think you can delete it.\n. @raphaelhoffmann  Thanks! Could you share the log of your run to us? \n. @raphaelhoffmann From the log file, there is nothing wrong... Please let me know if you have further information.\n@alldefector We use assigned variable IDs in SQL joins to ground edges, you can find it here: https://github.com/HazyResearch/deepdive/blob/1573cb703079b10cb2dcc93b3d67e09d079d628c/src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala#L718.\n. @alldefector If we assign non-contiguous IDs, then we need do remapping non-contiguous IDs to contiguous IDs in sampler.\nIs it possible to tweak the sampler so that we can feed it variables and factors without sequential IDs?\nYes, we can do this. However, we might have following things to take care:\n1. If we don't assign sequential IDs, then we need have unique keys to identify different variables. \n2. Using assigned IDs instead of fingerprints, we can reduce the size of factor graph as much as possible.\n. Thanks! Will update accordingly.\n. Updated.\n. Updated docs. Please check, thanks!.\n. @raphaelhoffmann You lost permission when you re-run DD. You can either re-login or use reauth command to get authenticated access.\n. @raphaelhoffmann It might be your DD installation. Did you install your DD/sbt in afs? If all your installations (sbt, DD) are in lfs, then everything will be fine.\n. @zhangce Yes, I found I add it in our incremental update which it is unnecessary. Sorry about that. Will add specification in the code.\n. Updated.\n. LGTM.\n. Upated. Please check.\n. @raphaelhoffmann We will always run ANALYZE TABLE_NAME after each extractor. For example, in tsv extractor, we have https://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/extraction/ExtractorRunner.scala#L509.\n. Only one comment for corner cases. Looks good other than this.\n. LGTM.\n. I will sync with Zifei and push this feature in another branch.\n. I am not sure it is stable. I only test it on small examples (around 100k docs). But this is really a good opportunity to test my code and put it in. I am talking to Feng about the case. :)\n. Yes, I have already synced with Feng and I will add partition support based on DD v0.8. And I briefly talked with Jaeho about it yesterday and we will come up a concrete plan for it on Wednesday.\n. @BrettMeehan is making progress now. Brett, could you give us some updates here? Thanks.\nBTW: for large factor graph, I suggest using only 1 numa node(the sampler argument is -c 1).\n. ",
    "larryxiao": "sorry I didn't see guide on contributing code, I hope it's not rude to make pr =D\nAs for specifying log and output dir in application.conf.\nI think I can manage the output part, as I understand it, it's parsed and saved in Context.\nBut options for logging is specified in logback.xml, I'm not familar with configuring akka.event.slf4j.Slf4jLogger, can you give some guidance? Thank you!\n. Yes, both would work. And it's best when user can simply call deepdive.\nFor those scripts referenced from deepdive itself, not by the application, maybe should have them installed along with deepdive.\n. @zifeishan \nThat's neat!\nI think this can be closed?\n. sorry I didn't see guide on contributing code, I hope it's not rude to make pr =D\nAs for specifying log and output dir in application.conf.\nI think I can manage the output part, as I understand it, it's parsed and saved in Context.\nBut options for logging is specified in logback.xml, I'm not familar with configuring akka.event.slf4j.Slf4jLogger, can you give some guidance? Thank you!\n. Yes, both would work. And it's best when user can simply call deepdive.\nFor those scripts referenced from deepdive itself, not by the application, maybe should have them installed along with deepdive.\n. @zifeishan \nThat's neat!\nI think this can be closed?\n. ",
    "gengyl08": "@rionda @feiranwang  I think the errors come from changing the paths from .html to .md.\nThe previous .html links will result in 404 not found and the current .md is working.\nWhat do you guys think is the correct solution?\n. @rionda @feiranwang  I think the errors come from changing the paths from .html to .md.\nThe previous .html links will result in 404 not found and the current .md is working.\nWhat do you guys think is the correct solution?\n. ",
    "chrismre": "What is going on with this? Is this fixed?\n. Is this fixed?!?\n. @zhangce wtf. @ajratner \n. Hi Guys, I wrote a code review that I shared with you via Google docs... this file needs a refactor. Let's get that done!\n. Guys, why is this request still sitting here? Sen is this code ready to merge?\n. Guys I'd like to get this into develop so we can have a few days with it! Can you fix this properly and then do the pull? It looks like a little bit of extra work to do it the right way :)\n. @zifeishan, I agree with @SenWu. If this change breaks the current application.conf format, we should make sure to talk about it as a group...\n. @zifeishan If it's backward compatible, then no problem... move it in :)\n. @feiranwang @mikecafarella @zifeishan Is this going in?\n. @feiranwang @SenWu Does this interfere with your refactor or your coming revamp of loading? Is it easy for you to pull these changes? In particular, are we planning to get rid of the gpfdist stuff and replace it with the streaming interface?  \nIf this commit goes in, I would advocate that this go into the grounding refactor, not develop.\n@zifeishan There are some things we should consider cleaning up before this pull goes in:\n1.  Use optional instead of \"\" to represent nulls) as in https://github.com/HazyResearch/deepdive/pull/209/files#diff-0679872da5a85cac4903ab0bbf315847R25\n   I'm guessing this is probably a larger refactor of the way we parse the config file, which should be handled separately.\n2. Who decide to deprecate the python in-database extractors, which are much faster than this approach? These kind of design decisions should be discussed, and I don't recall even a single email. Also, I don't agree with the statement or the decision right now. \n3. What does this portion of the commit have to do with parallel loading? https://github.com/HazyResearch/deepdive/pull/209/files#diff-f3eb1e9f4b57dbda44815a559475e4d9R64 \n. I'm not sure I follow this discussion :). Is something in this family of features useful in any application?\n. Let's discuss moving the LSA/PCA and simple CRF-like back into a branch?\ndon't care where =)\nOn Sun, Aug 21, 2016 at 1:52 PM alldefector notifications@github.com\nwrote:\n\nBucketization or continuous features make total sense.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/213#issuecomment-241281376,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AHPtuByWVY6jau8Ael0TbKtLUjV17m9vks5qiLqlgaJpZM4Da0yo\n.\n. What is going on here? this looks like you created the pull request and do the merge... who did the code review on this?\n. Is this fixed?!\n. @mikecafarella Mike, can you do a code review here? This is phase one of the grounding refactor. There is some more going on to remove obvious code redundancy. We want to make sure that all new code has comments, is factored to be easy to understand, etc. This push is not intended to address  redundancy issues (multiple data stores).\n. @feiranwang Ah good, I see now you did catch all the language drops.\n@mikecafarella This should go out very soon. It corrects a major issue (CASCADE DROPS) and is generally cleaner code. Any chance we can get this out today? I'm even tempted to hotfix for 0.05 because the CASCADE DROP that Feiran removed are so awful....\n. @mikecafarella @feiranwang What's our status here? I'd like to pull this...\n. @netj has this been merged?\n. @zhangce Can we sync this up?\n. @netj @feiranwang We need to check for dependencies! This is a nasty, annoying bug!\n. The MYSQL port is broken, and it is likely to remain so for a while. I\nwould prefer to push this patch, since MySQL is neglected (and was never\nrecommended...)\n\nOn Mon, Apr 27, 2015 at 8:24 AM, alldefector notifications@github.com\nwrote:\n\nThanks, Jaeho. Looks like it's getting a bit complicated to backport...\nUnless develop are going to be merged within a couple days, could you or\nFeiran patch master with the 5fa7493\nhttps://github.com/HazyResearch/deepdive/commit/5fa7493e0c1a73c144ff79f75719856d72f7f8cc\nchange? It'd make grounding 50X faster for current users.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/263#issuecomment-96706286.\n. @feiranwang @zhangce @netj Take a look please, and let's add in some regression here... :)\n. @zhangce @netj Can you take a look at this too? Can we get some regressions?!?! :) We had some... they appear to be broken or not expressive enough... \n\nLet's just track this down!\n. Please make sure that we have regressions for each of the factor #s!\nOn Tue, Apr 28, 2015 at 2:11 PM, Feiran Wang notifications@github.com\nwrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann I fixed this in\nhotfix_array branch. Now the results should make sense. The results should\nbe\ngroup 1: P(v = 1) = (e^3)     / (1 + e^3)  = 0.95\ngroup 2: P(v = 1) = (e^3 + 1) / (3 + e^3)  = 0.91\ngroup 3: P(v = 1) = (e^3 + 3) / (7 + e^3)  = 0.85\ngroup 4: P(v = 1) = (e^3 + 7) / (15 + e^3) = 0.77\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/268#issuecomment-97208072\n.\n. Great! Please make sure a suite of new tests are setup for this :)\n. @zhangce take a look at this :)\n. @netj @feiranwang @zhangce Guys, this should have triggered a larger response... you should have a series of small regressions about learning parameters correctly (e.g., samples of 1000 coin flips, etc.) Where are these regression tests?\n. @zhangce @zifeishan @netj \n\nHow is this possible? When did this happen?\n. Sure, but how did the docs get so far out of sync?!?\nOn Thu, Apr 30, 2015 at 9:37 PM, Zifei Shan notifications@github.com\nwrote:\n\n@chrismre https://github.com/chrismre As I understand, this happens a\nlong time ago, when we decided to do grounding by assigning an id column\nto each row in the variable table. Sampler would require different\nvariables to have different ids, but it is not possible under the current\ngrounding framework since each table that contain variables only have one\nid column.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/277#issuecomment-98041731\n.\n. @zifeishan let's wait for @netj to weigh in on the fixes. I'd like it if all the tutorials were in the examples directory and part of the tests.\n\n@feiranwang This should also have some learning tutorial with simple results :)\n. So can we close this issue? Are the docs up to date? \n. I love this! Please add this to the plans for 0.06.1 release (the maintenance release after DDLog)\n. This is open in two issues. Closing this one.\n. Is this closed?\n. @zifeishan you can do a quick review and push to the website. If Mike and Jaeho want to make changes, they can do a branch :)\n. @netj @zhangce @feiranwang Can you take a look at this?\n. did we get any closure on this? I think brew install doesn't automatically createdb appropriately, perhaps we can add a note about this in the docs?\n. Makes sense! We are thinking of a few screencasts... we just have to wait\nfor summer for our day jobs to simmer down!\nOn Sun, May 24, 2015 at 11:10 AM, Alex Bresler notifications@github.com\nwrote:\n\nClean reinstalled and got it working sorry about that! Video or screen\ncast on setup could be super helpful though\n\nAlex Bresler\nabresler@asbcllc.com\n\u200bwww.asbcllc.com\u200b\n917-455-0239\u200b (cell)\u200b\nOn May 24, 2015 2:01 PM, \"chrismre\" notifications@github.com wrote:\n\ndid we get any closure on this? I think brew install doesn't\nautomatically\ncreatedb appropriately, perhaps we can add a note about this in the docs?\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/issues/294#issuecomment-105043291\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/294#issuecomment-105045505\n.\n. @zhangce @nejt We're bringing back the dark data terminology... people seem to like it.\n. I already make deployed it.\n. Any updates here? :)\n\nWe do need a pass on the text of the website too!\n. Ok, that's fine. Why isn't the patch just done? I'd prefer to release all\nat once...\nAlso, can I play with some ddlog examples :)\nOn Wed, Jun 17, 2015 at 10:06 AM, Jaeho Shin notifications@github.com\nwrote:\n\nThis was pending on one last bug in the sampler's incremental mode with\nNUMA. @zhangce https://github.com/zhangce and I just decided we can go\nahead with the release and follow up with a patch soon.\nYes, the website pass can be done anytime. Any particular parts you want\nrevision? The new branching policy we'll use makes it easier as the latest\ncode as well as the website kept in master--currently any change made to\nwebsite needs to be carefully merged to develop, which is quite annoying\naround release time.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/299#issuecomment-112878761\n.\n. No that's fine. Go ahead with the release, and let's get a list of what's\nnext!\n\nOn Wed, Jun 17, 2015 at 10:17 AM Jaeho Shin notifications@github.com\nwrote:\n\nIt seems the bugfix for HazyResearch/sampler#10\nhttps://github.com/HazyResearch/sampler/issues/10 may take a day or\ntwo, and the delayed release has been a blocker (mostly mentally) so I\nwanted to move on to the next things. However, we can still wait for the\npatch and mark the release. Either way is actually fine.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/299#issuecomment-112882097\n.\n. Sounds good. I would quibble that those should be 0.6.1 and 0.6.2 :)\n\nOn Wed, Jun 17, 2015 at 10:28 AM, Jaeho Shin notifications@github.com\nwrote:\n\nSure. There's already a rough sketch I made a while ago in the Milestones\ntab https://github.com/HazyResearch/deepdive/milestones, but we can\ndefinitely revise this\u2013perhaps splitting into two? 0.7.0 for the cleanups\nwith easy NLP and 0.8.0 for the rest. More concrete issues can be created\nto guide individual tasks as I'm already doing for my cleanup.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/299#issuecomment-112884527\n.\n. I would argue that the 0.x is meant to signify we are research software not\nready for release, so just prepend that. :)\n\nOn Wed, Jun 17, 2015 at 10:39 AM, Jaeho Shin notifications@github.com\nwrote:\n\nI don't like quickly growing version numbers either, but at least I agree\nwith the Semantic Versioning http://semver.org telling us to keep the\nlast number for patching previous releases. For example, when we patch the\nknown bug in sampler a week later, I think it's simpler to give 0.6.1 to it\ninstead of 0.6.0.1 or so, and use 0.7 for the next milestone.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/299#issuecomment-112887324\n.\n. This looks awesome! Has anyone run through the tutorial?\n. :(\n. Hi Mina,\n\nShould be! @feiranwang cc: @netj. We're pushing for a release right now, so it may be a small delay until they can respond. We'll get back to you soon! All the best, Chris \n. We should always set the partitioning key right?\nOn Sat, Jun 13, 2015 at 11:53 PM, Raphael Hoffmann <notifications@github.com\n\nwrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann Sorry the problem\nthat this PR is solving is actually not PXL's problem. I fixed the table\nschema to avoid updating a distribution key, so I think this PR doesn't\nneed to go in.\nSounds good. So, it seems both GP and XL use the first column as the\ndistribution key if no other column is explicitly set to be the\ndistribution key.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/320#issuecomment-111792568\n.\n. Hmmm... this seems like a pretty important decision to be left to default.\nMaybe we should revisit this later... Chris\n\nOn Sun, Jun 14, 2015 at 12:15 AM, Feiran Wang notifications@github.com\nwrote:\n\nFor some temp tables, we use the default distribution key (the first\ncolumn), and put a proper column as the first column. I think currently we\ndon't write the distribution key explicitly due to two reasons:\n1. we are sharing sql queries between different datastores\n2. we don't know the exact schema of that relation, but we know which\ncolumn should not be used as distribution key\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/320#issuecomment-111794425\n.\n. Jaeho is a wise man :)\n\nOn Sun, Jun 14, 2015 at 12:18 AM, Feiran Wang notifications@github.com\nwrote:\n\nYes, Jaeho proposed in the next refactoring to replicate all the sql\nqueries for each datastore, so we can optimize each sql query for each\ndatastore individually.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/320#issuecomment-111794500\n.\n. +1\n. This is awesome!\n. @feiranwang @netj after code review, let's mail back the user who needed this!!\n. @netj I thought this was fixed some time back... Also Kevin are you using ddlog? This takes care of building the dependencies for you... @feiranwang is that tutorial up?\n. This should have been fixed a long time ago... @netj @feiranwang ... :)\nLet's get it done!!!\n\nOn Mon, Aug 10, 2015 at 11:58 AM, Alex Ratner notifications@github.com\nwrote:\n\nThis is minor / low-priority, but it's convenient to be able to e.g. add a\nnew entity type or otherwise tweak the schema, and then run something like\ninitdb, without also dropping the sentences table say...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/351.\n. Any updates? This is an obvious pain point in the language that we should take care of soon! :)\n. Is this the end of akka?\n\nOn Wed, Aug 12, 2015 at 1:31 AM, Jaeho Shin notifications@github.com\nwrote:\n\n\nUses named arguments syntax in constructors.\nUses .copy() with named arguments when possible.\nSplits unit tests into individual .bats files, so each can be\n  run/excluded selectively.\n\n(Part of #329 https://github.com/HazyResearch/deepdive/issues/329)\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/HazyResearch/deepdive/pull/353\nCommit Summary\n- Collapses multiple .scala files for settings\n- Rewrites SettingsParser as a series of transformers\n- Splits scalatests.bats into .bats file per ScalaTest class\n- Rewrites SettingsParserSpec test to use named arguments\n- Reflects defaults for extractors and factors to config\n- Moves some settings checks to SettingsParser\n- Settings formatting cleanup\n- Organizes imports in settings\n- Fixes SettingsParser bugs\n- Fixes how scala tests were run through bats\n- Fixes SettingsParser to recognize MERGE mode\n- Converts to named arguments for Extractor settings\n- Converts to named arguments style for FactorDesc settings\n- Converts to named arguments style for DbSettings settings\n- Converts to named arguments style for CalibrationSettings\n- Cleans up FactorFunctionVariable constructor usage\n- minor cleanup of settings\nFile Changes\n- M Makefile\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-0 (5)\n- M src/main/resources/application.conf\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-1 (5)\n- M src/main/scala/org/deepdive/DeepDive.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-2 (32)\n- M src/main/scala/org/deepdive/settings/Settings.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-3 (245)\n- M src/main/scala/org/deepdive/settings/SettingsParser.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-4 (701)\n- D\n  src/main/scala/org/deepdive/settings/models/CalibrationSettings.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-5 (4)\n- D src/main/scala/org/deepdive/settings/models/Connection.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-6 (4)\n- D src/main/scala/org/deepdive/settings/models/DbSettings.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-7 (15)\n- D\n  src/main/scala/org/deepdive/settings/models/ExtractionSettings.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-8 (4)\n- D src/main/scala/org/deepdive/settings/models/Extractor.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-9 (21)\n- D src/main/scala/org/deepdive/settings/models/FactorDesc.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-10 (18)\n- D src/main/scala/org/deepdive/settings/models/FactorFunction.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-11 (77)\n- D\n  src/main/scala/org/deepdive/settings/models/InferenceSettings.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-12 (4)\n- D src/main/scala/org/deepdive/settings/models/InputQuery.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-13 (14)\n- D src/main/scala/org/deepdive/settings/models/LoaderConfig.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-14 (9)\n- D\n  src/main/scala/org/deepdive/settings/models/PipelineSettings.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-15 (12)\n- D src/main/scala/org/deepdive/settings/models/SamplerSettings.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-16 (4)\n- D src/main/scala/org/deepdive/settings/models/SchemaSettings.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-17 (28)\n- D src/main/scala/org/deepdive/settings/parsing/DataTypeParser.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-18 (10)\n- D\n  src/main/scala/org/deepdive/settings/parsing/FactorFunctionParser.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-19 (71)\n- D\n  src/main/scala/org/deepdive/settings/parsing/FactorWeightParser.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-20 (17)\n- D\n  src/main/scala/org/deepdive/settings/parsing/InputQueryParser.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-21 (18)\n- M src/test/scala/helpers/TestHelper.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-22 (26)\n- M src/test/scala/unit/datastore/DataLoaderSpec.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-23 (4)\n- M src/test/scala/unit/extraction/ExtractionManagerSpec.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-24 (54)\n- M src/test/scala/unit/extraction/ExtractorRunnerSpec.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-25\n  (413)\n- M src/test/scala/unit/helpers/HelpersSpec.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-26 (21)\n- M src/test/scala/unit/inference/InferenceManagerSpec.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-27 (7)\n- M src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-28\n  (299)\n- M src/test/scala/unit/settings/FactorFunctionParserSpec.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-29 (49)\n- M src/test/scala/unit/settings/SettingsParserSpec.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-30\n  (319)\n- M test/bats.mk\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-31 (23)\n- M test/enumerate-tests.sh\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-32 (1)\n- A test/greenplum/scalatests\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-33 (1)\n- A test/mysql/scalatests\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-34 (1)\n- D test/mysql/scalatests.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-35 (1)\n- D test/postgresql/scalatests.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-36 (78)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.DataLoaderSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-37 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.DataTypeParserSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-38 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.ExtractionManagerSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-39 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.ExtractorRunnerSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-40 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.FactorFunctionParserSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-41 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.FactorWeightParserSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-42 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.FileDataUtilsSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-43 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.HelpersSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-44 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.InferenceManagerSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-45 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.InputQueryParserSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-46 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.PostgresExtractionDataStoreSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-47 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.PostgresInferenceRunnerSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-48 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.ProcessExecutorSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-49 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.ProfilerSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-50 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.SamplerSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-51 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.SettingsParserSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-52 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.TaskManagerSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-53 (13)\n- M test/postgresql/update-scalatests.bats.sh\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-54 (31)\nPatch Links:\n- https://github.com/HazyResearch/deepdive/pull/353.patch\n- https://github.com/HazyResearch/deepdive/pull/353.diff\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/353.\n. I like the default and then override strategy... but Jaeho is the boss.\n\nOn Wed, Aug 12, 2015 at 3:33 PM, Alex Ratner notifications@github.com\nwrote:\n\nHow can you set the extractor-level parallelism (i.g. the xargs\nparallelism for running each single extractor) in DDLog?\nI know that you can (presumably, from v0.6.x) go into the compiled\ndeepdive.conf file and then add e.g. parallelism: ${PARALLELISM} to every\nextractor, but is there some better way to do this / can there be?\nThe question of whether we'd want to set this for each individual\nextractor (as is done pre-DDLog) or just have one global specification is\nworth asking I think:\n- The former offers more flexibility (for ex: what if a certain\n  extractor has large overhead for loading resources, but then is super-fast\n  to run, and so we might want to run it with lower parallelism)\n- The latter is much simpler\nI'd be very happy with just the latter; either way seems like this should\nbe incorporated somehow (what is the default now btw..?)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/356.\n. I like it!\n\nOn Wed, Aug 12, 2015, 9:47 PM Alex Ratner notifications@github.com wrote:\n\nIs this on the roadmap out of curiosity? Seems like this would be nice and\nalso fairly simple way to centralize the basic DD app in the ddlog file- or\nis there a way to do this already? Thanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/357.\n. @feiranwang @netj this is a ddlog issue?\n. here is fine :) I'm just interested in DDlog issues right now. Make sure you send notes to those involved.\n. @feiranwang @netj @zhangce I thought we had this all nicely fixed on ubuntu? :) \n. @raphaelhoffmann did you write the EL tutorial up? @zhangce @netj \n. Absolutely! This may require some conversation about how to do it properly... but in our human trafficking applications there are essentially no databases and distant supervision works well...\n\none way to think about it is to write some (low quality) simple rules as distant supervision. for example, we had lots of collisions about which location (is it concord CA? TX?) -- but we heuristically used phone number to grab those which were less ambiguous. \nSince I don't know the domain, I can't generate these rules... but this would be fun to meet about...\n@ajratner @netj would love to have that elastic search regular expression integration... then as they search for rules we could data program :)\n. Roughly, the game works like this:\n- You heuristically generate many different kinds of low quality examples. Concretely, you write a bunch of regular expressions and python code. They labels you generate will be noisy--this is OK. It's a little disturbing from a classical ML perspective but fine from a stat/optimization perspective. \n- Now, the game is to get some redundancy in the labels so that you can \"denoise the labels\" Also, if there is structure between the label sets, you can even denoise to an even greater extent. \n- Of course, you may choose to label some data or to fix some rules, but engineering-wise it's hopefully easier... we have some theory that gives situations in which noise from many crappy rules is enough to learn to essentially perfect accuracy. \nBut, it's mostly in latex, and we need more experience... hence my excitement :) \nWe're building tools to support this type of extreme distant supervision... @zhangce @ajratner @netj @thodrek  COULD BE FUN!! \n. I don't know... :) @netj \n. hmm... this should be handled by DD... Seems easy enough? @zhangce @feiranwang \n. Why not make sure the simple cases work? We don't have to handle the\ngeneral case to be useful, right?\nOn Sat, Aug 29, 2015 at 9:27 PM Feiran Wang notifications@github.com\nwrote:\n\nSynced up with Zifei. The described approach applies to dumping a single\ntable or local join of tables. In DD, an extractor input query could be an\narbitrary SQL query, and the approach could not apply to such a general\ncase. We could optimize for specific types of queries using the ad hoc\napproach, by getting hints from the users, which I think might be too\nspecific.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/372#issuecomment-136084126\n.\n. Maybe just an annotation?\n\nOn Sat, Aug 29, 2015 at 9:41 PM Feiran Wang notifications@github.com\nwrote:\n\nSure, we can add support for that.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/372#issuecomment-136084646\n.\n. @netj @SenWu @zhangce I don't see why this would be an issue... Can you guys look at this? We'll get back to you soon!!\n. @SenWu Can you look at this?\n. You are far too nice!\n\nWe should be able to fix this soon... some of our team is very seriously\nstudying ... in hawaii for the week... I haven't seen them at the\nconference, so I assume they are in their rooms coding :)\nChris\nOn Thu, Sep 3, 2015 at 9:13 AM, vsoch notifications@github.com wrote:\n\nFantastic, many thanks @SenWu https://github.com/SenWu. There isn't\nhuge rush because I was able to go through the entire walkthrough on my\nlocal machine, and am not held back in any way to start developing our\nmethods locally. It's really a beautiful piece of software - I of course\nexpect to have bugs and what not, but the workflow with the database,\nmaking extraction rules, and running pipelines is very intuitive! :L)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/375#issuecomment-137499209\n.\n. Adding @SenWu. We should doc this.\n. @feiranwang Is this waiting @netj approval? He should be assigned :)\n. @zhangce @SenWu How could this be? This performance bug should have been fixed months ago.\n. This makes me really nervous :)\n\nOn Sun, Sep 13, 2015 at 9:39 AM Jaeho Shin notifications@github.com wrote:\n\nAny anticipated side effects if we drop DISTINCT? I think there must have\nbeen a good reason or use case why we added it in the first place.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/382#issuecomment-139893472\n.\n. If DISTINCT is not needed, then there has to be some convention somewhere\nthat ensure DISTINCT is unnecessary. IF you want to remove it, explain what\nthat convention and make sure it's documented (and ideally checked!)\n\nOn Sun, Sep 13, 2015 at 10:39 AM alldefector notifications@github.com\nwrote:\n\nI wasn't able to find who first introduced DISTINCT with git blame because\nof some refactoring. Maybe the initial author just has to speak up about\nhis thought process... Or someone with better git skills could track it\ndown...\nOn Sun, Sep 13, 2015 at 10:24 AM chrismre notifications@github.com\nwrote:\n\nThis makes me really nervous :)\nOn Sun, Sep 13, 2015 at 9:39 AM Jaeho Shin notifications@github.com\nwrote:\n\nAny anticipated side effects if we drop DISTINCT? I think there must\nhave\nbeen a good reason or use case why we added it in the first place.\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/pull/382#issuecomment-139893472\n.\n\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/pull/382#issuecomment-139897221>\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/382#issuecomment-139898855\n.\n. @SenWu @zhangce We have some issues failing on the pr... can we figure out the issue here? How robust is the failing incremental test? \n\nI'm, for example, worried that it would fail on another architecture or with different random draws, etc. Can someone give me some better understanding of this test? (in meeting is fine)\n. sounds good. Thanks for the explanation!\nOn Sun, Sep 20, 2015 at 3:07 AM Feiran Wang notifications@github.com\nwrote:\n\n@chrismre https://github.com/chrismre There was a bug introduced by an\nearlier commit, and it is fixed in #376\nhttps://github.com/HazyResearch/deepdive/pull/376. The incremental test\nis stable after merging #376\nhttps://github.com/HazyResearch/deepdive/pull/376.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/382#issuecomment-141769747\n.\n. @netj and @ajratner  is thinking about how to make this process dramatically easier... I don't know what the current status is :) \n. adding more people I emailed :) @thodrek @feiranwang \n. I'm not sure how good those examples actually are... @netj when you get back online can you sync up?\n. This is a great point, and we do support both for that reason. Speed issues\nto one side, we'd prefer a nice human reasable format. However, I think\nwhat Feiran's saying is that the speed penalty for JSON can be\nunreasonable. We've had people stub their toes on building JSON extractors,\nonly to find out it doesn't scale later. You're more than welcome to use\nit!\n\nOn Fri, Sep 18, 2015 at 12:27 AM Alexis BRENON notifications@github.com\nwrote:\n\nEven if it's faster, I think that JSON extractor, based on column name and\nso, that gives some semantic to your extractor, is far better for\nmaintenance: in case you have to add or remove some attributes, you can\neasily check its existence with JSON, but not with TSV. Both are useful,\ndepending on your goal : speed or continued existence.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/389#issuecomment-141369558\n.\n. I'm not so sure... they should be much faster... can we make sure it\ndoesn't blow up in our faces?\n\nOn Tue, Sep 22, 2015 at 6:55 PM alldefector notifications@github.com\nwrote:\n\nOK, looks like it's specific to plpy extractors:\nhttps://github.com/HazyResearch/deepdive/blob/e8b9dc329102c53bb273455953e8ac67a62e579c/src/main/scala/org/deepdive/extraction/ExtractorRunner.scala#L553\nhttps://github.com/HazyResearch/deepdive/blob/master/util/ddext_input_sql_translator.py#L103\nIf we avoid using plpy extractors, looks like DDLog works on PGXL.\nTime to deprecate plpy extractors?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/403#issuecomment-142470880\n.\n. I just want to say--we should definitely be able to get you setup! There\nare (ridiculous in my view) restrictions on some of the data from\nGeoDeepDive... however, we can set you up with those tasks.\n\nYour background is exactly the type of person we want to enable. I'd\npropose a deal: we can help you get setup (we'd do this anyway :) ), but\nperhaps you can help us document how to make it easier to use and write the\nnext version.\nI'd also say if you can stick to open source information (CC data), we can\nput your data our there and get even more users to help you in your goal.\nThis is very exciting for us!\nAll the best, Chris\nOn Fri, Sep 25, 2015 at 9:58 AM ukliu notifications@github.com wrote:\n\nFirst of all, thanks @netj https://github.com/netj for helping me with\nthe deepdive installation and the tutorial walk-through. It took me a long\ntime to go through all the tutorial programs line by line and understand\nhow they work.\nI have one small question on the tutorial programs, on\nhttps://github.com/HazyResearch/deepdive/blob/master/examples/tutorial_example/step1-basic/deepdive.conf\nline 27,32: Does not \u201csentences\u201d need to be defined?\nI found \u201csentences_dump.csv\u201d and \u201csentences_dump_large.csv\u201d in the folder\n/spouse/input. There is no file names as \u2018sentences.tsv\u201d. I am wondering,\ndoes the tsv-extractor take those two cvs files as \u2018sentences\u2019?\nIn addition, I have some general questions with the deepdive.\nMy goal of learning deepdive is to use it to extract and analyze relations\nout of scientific pdfs (or database containing pdfs). I am from a\nbackground of geology, familiar with Python, with some experience of Apache\nSpark, limited (only know the basic operations such as select, join, etc)\nexperience with SQL, and no experience with Java. For me to master the\ndeepdive, I believe that I have to learn to write the programs similar to\nwhat are shown in the tutorial material. My first question is, are there\nsome toy problems similar to the tutorial walk-through used to train\nnewbies?\nAnother question is, I used the Mac terminal to run the tutorial programs,\nwhich might be perfect for program developers but not friendly enough to me\nand probably to future deepdive users. Is there a software with GUI that\ncan perform deepdive operations? Do you guys use Java to debug your\ndeepdive programs?\nA third question is, I learned from the website that @zhangce\nhttps://github.com/zhangce has developed a geodeepdive DEMO but could\nnot find any website/software devoted to that. Is it still under\ndevelopment or the software has been released privately? Can I use it to\nperform my task, e.g., extract elemental, mineral and geochronological\nrelations from geological pdf articles/ databases? How do you guys extract\nrelations from pdfs?\nThanks for your patience!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/409.\n. Why not give out the ddlog version? it's easier to use right? and it's the\npreferred way...\n\nOn Fri, Sep 25, 2015 at 10:08 AM zhangce notifications@github.com wrote:\n\nA third question is, I learned from the website that @zhangce\nhttps://github.com/zhangce has developed a geodeepdive DEMO but could\nnot find any website/software devoted to that.\n@ukliu https://github.com/ukliu In terms of your third question, you\ncan find the code for PaleoDeepDive at https://github.com/HazyResearch/pdd\n. You can follow a similar structure for your\napplication. If you want more simple examples (and because you are from\nShanan's team),\nJulia also has an application that she builds from scratch over the summer\nusing DeepDive\nthat is potentially more easier to follow; but you probably need to ask\nShanan for the program.\nIf you have any Geo-related, Paleo-related, or PDF-acquisition-related\nquestions, we can\ndiscuss via email (and I will point you to the right people to ask).\nThanks!\nCe\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/409#issuecomment-143288959\n.\n. I should have checked =)\n\nOn Fri, Sep 25, 2015 at 10:20 AM zhangce notifications@github.com wrote:\n\nWhy not give out the ddlog version? it's easier to use right? and it's the\npreferred way...\nYes, ddlog is the preferred way to write applications now. The code in\nhttps://github.com/HazyResearch/pdd is already in ddlog.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/409#issuecomment-143292768\n.\n. @netj can you setup a Skype with @vsoch, let's make sure this is resolved :)\n. I think the plpython dependency is gone (or will be soon). @netj can\nconfirm :)\n\nOn Sat, Sep 26, 2015 at 7:56 PM vsoch notifications@github.com wrote:\n\nOnce I know more about the database, it might be needed to setup a Skype\nto talk about the different options for setting this up to run in parallel.\nI have a few ideas, will likely try a few things, and very likely will have\nquestions!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/410#issuecomment-143514227\n.\n. There should be a mode to warn if you are going to drop data. We should\nNEVER drop extensional , user supplied data. NEVER.\n\nIntensional relations should only be dropped in a \"force\" mode or something\n(imo).\nChris\nOn Sat, Sep 26, 2015 at 8:29 AM Jaeho Shin notifications@github.com wrote:\n\nThere's no way yet. I agree deepdive run should be idempotent. I think\nthe semantics of ddlog programs should be defined as starting from a state\nwhere all intensional relations are empty. We could try to detect which\nrelations are intensional vs. extensional, i.e. defined by ddlog vs. whose\ndata come from external sources, or provide a syntax (or just annotation?)\nand truncate all intensional relations upon every run.\nIf there aren't use cases for running multiple functions to fill up one\nrelation, e.g., table1 += f(...) :- ... . table1 += g(...) :- ... ., then\nwe could simply truncate (or drop/create) the output table for running\nfunctions for the moment and fully expand this EDB/IDB distinction later.\n@feiranwang https://github.com/feiranwang How does this sound?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/412#issuecomment-143463916\n.\n. This is why we have incremental processing...\n\nOn Sat, Sep 26, 2015 at 9:00 AM Raphael Hoffmann notifications@github.com\nwrote:\n\nWe just need a simple way to re-run deepdive run, for example after you\nmade some changes to a udf.\nIn that case you need to reset the generated tables. However, you can't\nuse deepdive initdb because it will clear the entire database. It is also\ntedious and error-prone to use deepdive sql because you can easily forget\na table.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/412#issuecomment-143466518\n.\n. Let be clear here. WE CANNOT DELETE USER DATA. :) It cannot happen. So\nplease make sure the default is that this truncation does not happen.\n\nChris\nOn Sat, Sep 26, 2015 at 11:33 AM Raphael Hoffmann notifications@github.com\nwrote:\n\n@feiranwang https://github.com/feiranwang That's exactly right. I just\na need a simple solution for the short term. Is there a way to truncate\nin ddlog?\nI understand that this more imperative way doesn't fit well with the\ndeclarative of nature of datalog, but the same may be true for \"+=\"...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/412#issuecomment-143478431\n.\n. let me propose that we have a separate call to deepdive that drops the\ntables @netj. Dropping tables does not seem like a feature that should be\nin the language...\n\nOn Sat, Sep 26, 2015 at 11:36 AM Chris Re chris.re@gmail.com wrote:\n\nLet be clear here. WE CANNOT DELETE USER DATA. :) It cannot happen. So\nplease make sure the default is that this truncation does not happen.\nChris\nOn Sat, Sep 26, 2015 at 11:33 AM Raphael Hoffmann \nnotifications@github.com wrote:\n\n@feiranwang https://github.com/feiranwang That's exactly right. I just\na need a simple solution for the short term. Is there a way to truncate\nin ddlog?\nI understand that this more imperative way doesn't fit well with the\ndeclarative of nature of datalog, but the same may be true for \"+=\"...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/412#issuecomment-143478431\n.\n. Thanks for the kind words! @netj @zhangce ?\n. This should not be on by default.\n\n\nOn Sun, Sep 27, 2015 at 9:00 PM Feiran Wang notifications@github.com\nwrote:\n\n@alldefector https://github.com/alldefector For ddlog application,\naccording to this doc\nhttp://deepdive.stanford.edu/doc/advanced/deepdiveapp.html#structure,\nthere can also be a deepdive.conf, which contains extra configuration\n(sampler args, etc.). For example, deepdive.conf may look like\ndeepdive.sampler.sampler_args: \"-l 200 -s 1 -i 500 --alpha 0.1 --sample_evidence\"\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/416#issuecomment-143636119\n.\n. @netj @zhangce What's going on here?\n. Oh no! Can we get that fixed =)\n. @netj @zhangce we should fix this!\n. Wow.\n. Let's not get crazy.\n\nOn Wed, Dec 16, 2015 at 6:29 PM alldefector notifications@github.com\nwrote:\n\nSounds like 1.0 instead of 0.8 :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/445#issuecomment-165317353\n.\n. This is awesome :)\n\nOn Thu, Dec 17, 2015 at 7:25 AM Jaeho Shin notifications@github.com wrote:\n\nFYI Here's the SVG rendered data flow for spouse example\nhttps://gist.github.com/netj/f4abe6eff63b837c7046#file-dataflow-svg and data\nflow for chunking example\nhttps://gist.github.com/netj/a4e363b92ffeb2fa95b7#file-dataflow-svg.\nI forgot to mention that this PR requires no change to the user's app\ncode, except maybe a few extra dependency information that were missing\nin the manually written deepdive.conf. The run.sh that typically handled\nenv setup and many other hacks should be moved to the right place such as\ninput/. @ThomasPalomares https://github.com/ThomasPalomares and I were\nable to get the genomics app (in DDlog) working with minimal changes.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/445#issuecomment-165483060\n.\n. Great feedback!!\n\nOn Tue, Dec 29, 2015 at 12:42 PM Raphael Hoffmann notifications@github.com\nwrote:\n\nHi @netj https://github.com/netj, this new version of deepdive is\nreally great! Here are a few notes I took while experimenting.\n1.\nWhen building on OSX, I got error that X11/Xlib.h couldn't be found.\n   This error\n   was thrown by buildkit module for graphviz.\n/usr/include/tk.h:78:23: fatal error: X11/Xlib.h: No such file or directory\nFollowing these\n   http://stackoverflow.com/questions/11465258/xlib-h-not-found-when-building-graphviz-on-mac-os-x-10-8-mountain-lion\n   instructions, I fixed it by adding a symlink\nln -s /opt/X11/include/X11 /usr/local/include/X11\nAfterwards, it was not sufficient to rerun 'make'. I manually cd'ed to\n   deepdive/depends/bundled/graphviz/graphviz-2.38.0 and ran ./configure\n   && make && make install. Then I could run \"make\" in deepdive\n   2.\nddlog apps didn't work with the default submodule commit, but \u2013\n   following your\n   suggestions \u2013 everything worked fine after upgrading to ddlog commit\n   ee0b359c9f018c19df5eddbf3bff60bdf076e091.\nNow everything works fine, but I've encountered two different problems in\na small number of runs:\n1.\nSometimes, stdin is no longer showing on the screen after a successful deepdive\n   do\n   execution. That is, I can type commands and they execute, but the\n   letters I type are not\n   being shown on the screen. I'm using iterm2.\n   2.\nSometimes (rarely), a mkmimo process kept running at 100% cpu usage,\n   after deepdive do\n   terminated with an error (possibly related to missing input data).\nI'll try to investigate more carefully if I encounter these again.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/445#issuecomment-167874340\n.\n. AWESOME! :)\n\nOn Thu, Jan 14, 2016 at 7:58 PM Thomas Palomares notifications@github.com\nwrote:\n\n@chrismre https://github.com/chrismre: this fixes the issue about\ndistributed_by in temporary tables.\nWith that + a bug in mkimo that Jaeho noticed, the time consuming\nextractions are now ~50x faster\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/453#issuecomment-171866395\n.\n. cc: @HazyResearch/genomics Does this fix the issue from today's meeting?\n. Ugh! I think we need this feature of reusing weights... This has got to be in a quick fix for 0.8.1 or something... @raphaelhoffmann does your fix work on 0.8? Can you create a PR for it?\n. This should have really been in the code a year ago @feiranwang. @zhangce Can you fix this?\n. ancient systems... dastardly.\n\nOn Thu, Jan 28, 2016 at 5:15 PM alldefector notifications@github.com\nwrote:\n\nThat'd be great! FWIW, these ancient systems probably had such blocking\nfrom the get-go:\nhttp://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/Gibbs%20sampling.aspx\nhttps://alchemy.cs.washington.edu/tutorial/tutorial.pdf (page 4)\nhttp://i.stanford.edu/hazy/tuffy/doc/tuffy-manual.pdf (page 8)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/471#issuecomment-176509148\n.\n. Bump. Can someone explain what's going on here? @netj \n. Also, @netj and @feiranwang, it should be super easy to declare categorical that are linked (say for our old entity linking design, they need to be able to express \n\nLinksToOne(candidate, entity) as a categorical random variable (each candidate maps to one entity). \nThis should be super easy (@key is fine). @zhangce, I know you're traveling but your thoughts are welcome when you get back on line :)\n@ajratner I don't want to redo the full constraints--no one seem to use them, and they have lots of code complexity... maybe in the summer :)\n. This was a very old feature... where did this go @zhangce @netj @SenWu \nYou don't even need syntax for this, it's easy to detect... @feiranwang \n. Zifei is out of the country.  :)\nIs this code stable? I thought you built this months ago! Has it been code reviewed? It would be awesome to put in !\nMoving the comment to another issue.\n. Ha, OK :)\n. Sen, how are we doing here?\n. How did we lose the ability to bring connected components into memory?\nThis needs to be fixed soon... Let's talk about this in dev meeting.\n@SenWu Isn't your code ready? The internal version has been there for MONTHS (years?)\nOn Tue, Feb 23, 2016 at 11:43 PM alldefector notifications@github.com\nwrote:\n\nYes, the sampler uses 40GB of memory with -c 1 and works for now. But we\nhave at least 50X as many docs. So partitioning is critical.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/478#issuecomment-188128352\n.\n. Yikes.\n\nOn Sun, Feb 21, 2016 at 2:36 AM alldefector notifications@github.com\nwrote:\n\nIf we replace the fast_seqassign with the regular PG's sequence\nassignment (even without the ORDER BY id hack for dump_weights),\neverything works out (example finishes successfully): the id column\nvalues are unique now, though the order of the result from select * from\ndd_weightsmulti_inf_istrue_tag limit 3; is still nondeterministic --\nmaybe somehow the order doesn't matter for sampler loading after all??? But\nthat defies the code\nfor (long i = 0; i < s; i++) {\n    assert(this->weights[i].id == i);\n  }\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/495#issuecomment-186793075\n.\n. Ugh, @netj any ideas? Shouldn't there be a unit test exactly for this?\n. Adding @ajratner @thodrek as their current experiments may be hampered by this bug.\n. Wait, why does the sampler do a resort?! That seems like a huge waste of time, and it shouldn't be done with the partitioning code...\n\nCan someone do an overhaul of categoricals with a thorough code review. This code seems to generate a large number of bug reports, and there seem to be no unit tests that catch any of these bugs. \n. Excellent!\nOn Mon, Feb 22, 2016 at 11:00 AM Jaeho Shin notifications@github.com\nwrote:\n\nSorry I was sidetracked the entire day after a quick fix attempt. It's\napparent that I left a bug in the sequence assignment for GP and never\nnoticed it because db-driver currently lacks tests for many parts after\ngetting a quick expansion. Moreover, we don't have automatic tests for GP.\nWe should figure out how to install/run GP on Travis.\nSampler resorting was ridiculous so I already proposed a fix, exploiting\nthe fact that we assign ids from a consecutive range beginning from zero:\nHazyResearch/sampler#14 https://github.com/HazyResearch/sampler/pull/14.\nAs long as the sequence assignment works the sampler should be working fine.\nWill fix this asap. Sorry for alerting so many people for this silly bug!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/495#issuecomment-187321097\n.\n. @netj We've been talking about simple modules, I think this would help here too :).... and I didn't mention it to AJR :)\n. Any closure here? Can we close this issue? :)\n. @raphaelhoff Can you leave a few samples of what you'd like to do? In particular, you should be able to filter based on the categorical value.... and what do you mean by correlations between values (They are exclusive...)\n@netj @alldefector \n. - So this is a performance optimization? Is there something semantically wrong with having those weights? Is this in response to a particular performance issue (as it seems the system should just do the right thing here...) \n- If you only wanted to add manual features for equal pairs, then why not write a rule that filters on equality? This should generate weights only for those pairs (that's the meaning of the weight being a function of the supported pairs. We've had this syntax for a while... @netj has this changed? We can discuss on the white board.\n\nWhy worry about these impossible combinations instead of learning 0 weights from data--especially since your impossible combinations now have to be enumerated by hand (which seems to defeat the point of learning).\ncc: @mikecafarella \n. This really shouldn't be so hard to resolve. Introducing a probe on each variable and factor could be a killer for performance... However, since after an initial pass the indexes are completely resolved, one could imagine bloating the data structure slightly to cache the needed indexes/pointers.\nWhat makes me a little concerned is that...I'm not sure why this code is difficult or complex... it's a relatively straightforward bit of code. It's not clear to me that this change isn't to fix a symptom of the deeper problem of low quality code in the compiler.\n. Maybe our reliance on shell versions and scripts is too much... why not use\nsomething like python that seems to be more portable? ...\nOn Tue, Jun 28, 2016 at 9:54 AM Jaeho Shin notifications@github.com wrote:\n\n@shahin https://github.com/shahin\n1. Why do we bundle bash with DeepDive? This seems like a very\n   low-level dependency to include in our distribution. As a DeepDive user or\n   new developer, I imagine I'd be very surprised to find out (eventually)\n   that it's not using the same bash that I'm using on my host system. Seems\n   like an explicit dependency on a given bash version would be clearer.\nBecause it's almost impossible to ask every user to install the exact bash\nversion on their system, we decided to bundle the exact version our scripts\nneed: 4.3.x. Bundling GNU coreutils is along the same line. Mac users\nwasted many hours/days if not weeks because of these since it's bash 3 and\nBSD. Docker could achieve something similar, but it rather avoids the\nportability problem not solving it, and it's too heavy to just get these\nfew megs of portable dependencies (on my Mac: bash is 7MB, coreutils is\n9MB).\n1. As long as we're bundling bash, should we be using it for all\n   tests? An exception for a single .bats seems easily overlooked, and\n   now our tests are targeting different platforms. You know more about bats\n   than I do -- what's the best way to run them all on bash 4.3?\nSure. The example I commented was just for illustration. I agree it's\nbetter to run all tests with our own stuff, which is already the case for\nthe rest, but bash used by bats was an unfortunate exception that wasted\nyour time. I think you can prefix the make test in .travis.yml with dist/stage/bin/deepdive\nenv:\ndist/stage/bin/deepdive env  make test\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/551#issuecomment-229111366,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AHPtuDiERKFmFNw4GmxEsDxun6OQ6eEeks5qQVG5gaJpZM4I8ClL\n.\n. I think @alldefector is in charge of this change--and it should be pushed to master soon :) Adding folks in here who can answer the definitive state of the code. Thanks for the kind words!\n. Grounding could be a bottleneck. Is the code that generates a partition on the fly too fragile or something? As it seems like being able to generate the partitions on demand (or not) would be helpful. That could be a down the road feature, but I'm curious about this design choice (e.g., we could avoid ever touching disk).\n. You may want to check out http://hazyresearch.github.io/snorkel/, which is\nmore directly about weak supervision.\n\nDeepDive can do all this (and much more!). This flexibility means that not\nall representation decisions are obvious.\nChris\nOn Mon, Dec 19, 2016 at 8:24 PM \u5b59\u660e\u660e notifications@github.com wrote:\n\nWhen I want to define a unsupervised model by using \"p(x,y)=NULL: ....\",\nthe program report:\ncolumn \"label\" is of type boolean but expression is of type text\nIt seems that the error is about a bug in sql generation module.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/608#issuecomment-268151386,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AHPtuEFo-aPa3pwU-pGppjuv12ulh3Pfks5rJ1iIgaJpZM4LRaM2\n.\n. DeepDive can express essentially any factor graph. You'll need to write the rules that create the required factor graph. A default factor graph for this process is described in the Snorkel/data programming paper. \n\nHope that helps! Chris. What is going on with this? Is this fixed?\n. Is this fixed?!?\n. @zhangce wtf. @ajratner \n. Hi Guys, I wrote a code review that I shared with you via Google docs... this file needs a refactor. Let's get that done!\n. Guys, why is this request still sitting here? Sen is this code ready to merge?\n. Guys I'd like to get this into develop so we can have a few days with it! Can you fix this properly and then do the pull? It looks like a little bit of extra work to do it the right way :)\n. @zifeishan, I agree with @SenWu. If this change breaks the current application.conf format, we should make sure to talk about it as a group...\n. @zifeishan If it's backward compatible, then no problem... move it in :)\n. @feiranwang @mikecafarella @zifeishan Is this going in?\n. @feiranwang @SenWu Does this interfere with your refactor or your coming revamp of loading? Is it easy for you to pull these changes? In particular, are we planning to get rid of the gpfdist stuff and replace it with the streaming interface?  \nIf this commit goes in, I would advocate that this go into the grounding refactor, not develop.\n@zifeishan There are some things we should consider cleaning up before this pull goes in:\n1.  Use optional instead of \"\" to represent nulls) as in https://github.com/HazyResearch/deepdive/pull/209/files#diff-0679872da5a85cac4903ab0bbf315847R25\n   I'm guessing this is probably a larger refactor of the way we parse the config file, which should be handled separately.\n2. Who decide to deprecate the python in-database extractors, which are much faster than this approach? These kind of design decisions should be discussed, and I don't recall even a single email. Also, I don't agree with the statement or the decision right now. \n3. What does this portion of the commit have to do with parallel loading? https://github.com/HazyResearch/deepdive/pull/209/files#diff-f3eb1e9f4b57dbda44815a559475e4d9R64 \n. I'm not sure I follow this discussion :). Is something in this family of features useful in any application?\n. Let's discuss moving the LSA/PCA and simple CRF-like back into a branch?\ndon't care where =)\nOn Sun, Aug 21, 2016 at 1:52 PM alldefector notifications@github.com\nwrote:\n\nBucketization or continuous features make total sense.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/213#issuecomment-241281376,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AHPtuByWVY6jau8Ael0TbKtLUjV17m9vks5qiLqlgaJpZM4Da0yo\n.\n. What is going on here? this looks like you created the pull request and do the merge... who did the code review on this?\n. Is this fixed?!\n. @mikecafarella Mike, can you do a code review here? This is phase one of the grounding refactor. There is some more going on to remove obvious code redundancy. We want to make sure that all new code has comments, is factored to be easy to understand, etc. This push is not intended to address  redundancy issues (multiple data stores).\n. @feiranwang Ah good, I see now you did catch all the language drops.\n@mikecafarella This should go out very soon. It corrects a major issue (CASCADE DROPS) and is generally cleaner code. Any chance we can get this out today? I'm even tempted to hotfix for 0.05 because the CASCADE DROP that Feiran removed are so awful....\n. @mikecafarella @feiranwang What's our status here? I'd like to pull this...\n. @netj has this been merged?\n. @zhangce Can we sync this up?\n. @netj @feiranwang We need to check for dependencies! This is a nasty, annoying bug!\n. The MYSQL port is broken, and it is likely to remain so for a while. I\nwould prefer to push this patch, since MySQL is neglected (and was never\nrecommended...)\n\nOn Mon, Apr 27, 2015 at 8:24 AM, alldefector notifications@github.com\nwrote:\n\nThanks, Jaeho. Looks like it's getting a bit complicated to backport...\nUnless develop are going to be merged within a couple days, could you or\nFeiran patch master with the 5fa7493\nhttps://github.com/HazyResearch/deepdive/commit/5fa7493e0c1a73c144ff79f75719856d72f7f8cc\nchange? It'd make grounding 50X faster for current users.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/263#issuecomment-96706286.\n. @feiranwang @zhangce @netj Take a look please, and let's add in some regression here... :)\n. @zhangce @netj Can you take a look at this too? Can we get some regressions?!?! :) We had some... they appear to be broken or not expressive enough... \n\nLet's just track this down!\n. Please make sure that we have regressions for each of the factor #s!\nOn Tue, Apr 28, 2015 at 2:11 PM, Feiran Wang notifications@github.com\nwrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann I fixed this in\nhotfix_array branch. Now the results should make sense. The results should\nbe\ngroup 1: P(v = 1) = (e^3)     / (1 + e^3)  = 0.95\ngroup 2: P(v = 1) = (e^3 + 1) / (3 + e^3)  = 0.91\ngroup 3: P(v = 1) = (e^3 + 3) / (7 + e^3)  = 0.85\ngroup 4: P(v = 1) = (e^3 + 7) / (15 + e^3) = 0.77\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/268#issuecomment-97208072\n.\n. Great! Please make sure a suite of new tests are setup for this :)\n. @zhangce take a look at this :)\n. @netj @feiranwang @zhangce Guys, this should have triggered a larger response... you should have a series of small regressions about learning parameters correctly (e.g., samples of 1000 coin flips, etc.) Where are these regression tests?\n. @zhangce @zifeishan @netj \n\nHow is this possible? When did this happen?\n. Sure, but how did the docs get so far out of sync?!?\nOn Thu, Apr 30, 2015 at 9:37 PM, Zifei Shan notifications@github.com\nwrote:\n\n@chrismre https://github.com/chrismre As I understand, this happens a\nlong time ago, when we decided to do grounding by assigning an id column\nto each row in the variable table. Sampler would require different\nvariables to have different ids, but it is not possible under the current\ngrounding framework since each table that contain variables only have one\nid column.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/277#issuecomment-98041731\n.\n. @zifeishan let's wait for @netj to weigh in on the fixes. I'd like it if all the tutorials were in the examples directory and part of the tests.\n\n@feiranwang This should also have some learning tutorial with simple results :)\n. So can we close this issue? Are the docs up to date? \n. I love this! Please add this to the plans for 0.06.1 release (the maintenance release after DDLog)\n. This is open in two issues. Closing this one.\n. Is this closed?\n. @zifeishan you can do a quick review and push to the website. If Mike and Jaeho want to make changes, they can do a branch :)\n. @netj @zhangce @feiranwang Can you take a look at this?\n. did we get any closure on this? I think brew install doesn't automatically createdb appropriately, perhaps we can add a note about this in the docs?\n. Makes sense! We are thinking of a few screencasts... we just have to wait\nfor summer for our day jobs to simmer down!\nOn Sun, May 24, 2015 at 11:10 AM, Alex Bresler notifications@github.com\nwrote:\n\nClean reinstalled and got it working sorry about that! Video or screen\ncast on setup could be super helpful though\n\nAlex Bresler\nabresler@asbcllc.com\n\u200bwww.asbcllc.com\u200b\n917-455-0239\u200b (cell)\u200b\nOn May 24, 2015 2:01 PM, \"chrismre\" notifications@github.com wrote:\n\ndid we get any closure on this? I think brew install doesn't\nautomatically\ncreatedb appropriately, perhaps we can add a note about this in the docs?\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/issues/294#issuecomment-105043291\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/294#issuecomment-105045505\n.\n. @zhangce @nejt We're bringing back the dark data terminology... people seem to like it.\n. I already make deployed it.\n. Any updates here? :)\n\nWe do need a pass on the text of the website too!\n. Ok, that's fine. Why isn't the patch just done? I'd prefer to release all\nat once...\nAlso, can I play with some ddlog examples :)\nOn Wed, Jun 17, 2015 at 10:06 AM, Jaeho Shin notifications@github.com\nwrote:\n\nThis was pending on one last bug in the sampler's incremental mode with\nNUMA. @zhangce https://github.com/zhangce and I just decided we can go\nahead with the release and follow up with a patch soon.\nYes, the website pass can be done anytime. Any particular parts you want\nrevision? The new branching policy we'll use makes it easier as the latest\ncode as well as the website kept in master--currently any change made to\nwebsite needs to be carefully merged to develop, which is quite annoying\naround release time.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/299#issuecomment-112878761\n.\n. No that's fine. Go ahead with the release, and let's get a list of what's\nnext!\n\nOn Wed, Jun 17, 2015 at 10:17 AM Jaeho Shin notifications@github.com\nwrote:\n\nIt seems the bugfix for HazyResearch/sampler#10\nhttps://github.com/HazyResearch/sampler/issues/10 may take a day or\ntwo, and the delayed release has been a blocker (mostly mentally) so I\nwanted to move on to the next things. However, we can still wait for the\npatch and mark the release. Either way is actually fine.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/299#issuecomment-112882097\n.\n. Sounds good. I would quibble that those should be 0.6.1 and 0.6.2 :)\n\nOn Wed, Jun 17, 2015 at 10:28 AM, Jaeho Shin notifications@github.com\nwrote:\n\nSure. There's already a rough sketch I made a while ago in the Milestones\ntab https://github.com/HazyResearch/deepdive/milestones, but we can\ndefinitely revise this\u2013perhaps splitting into two? 0.7.0 for the cleanups\nwith easy NLP and 0.8.0 for the rest. More concrete issues can be created\nto guide individual tasks as I'm already doing for my cleanup.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/299#issuecomment-112884527\n.\n. I would argue that the 0.x is meant to signify we are research software not\nready for release, so just prepend that. :)\n\nOn Wed, Jun 17, 2015 at 10:39 AM, Jaeho Shin notifications@github.com\nwrote:\n\nI don't like quickly growing version numbers either, but at least I agree\nwith the Semantic Versioning http://semver.org telling us to keep the\nlast number for patching previous releases. For example, when we patch the\nknown bug in sampler a week later, I think it's simpler to give 0.6.1 to it\ninstead of 0.6.0.1 or so, and use 0.7 for the next milestone.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/299#issuecomment-112887324\n.\n. This looks awesome! Has anyone run through the tutorial?\n. :(\n. Hi Mina,\n\nShould be! @feiranwang cc: @netj. We're pushing for a release right now, so it may be a small delay until they can respond. We'll get back to you soon! All the best, Chris \n. We should always set the partitioning key right?\nOn Sat, Jun 13, 2015 at 11:53 PM, Raphael Hoffmann <notifications@github.com\n\nwrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann Sorry the problem\nthat this PR is solving is actually not PXL's problem. I fixed the table\nschema to avoid updating a distribution key, so I think this PR doesn't\nneed to go in.\nSounds good. So, it seems both GP and XL use the first column as the\ndistribution key if no other column is explicitly set to be the\ndistribution key.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/320#issuecomment-111792568\n.\n. Hmmm... this seems like a pretty important decision to be left to default.\nMaybe we should revisit this later... Chris\n\nOn Sun, Jun 14, 2015 at 12:15 AM, Feiran Wang notifications@github.com\nwrote:\n\nFor some temp tables, we use the default distribution key (the first\ncolumn), and put a proper column as the first column. I think currently we\ndon't write the distribution key explicitly due to two reasons:\n1. we are sharing sql queries between different datastores\n2. we don't know the exact schema of that relation, but we know which\ncolumn should not be used as distribution key\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/320#issuecomment-111794425\n.\n. Jaeho is a wise man :)\n\nOn Sun, Jun 14, 2015 at 12:18 AM, Feiran Wang notifications@github.com\nwrote:\n\nYes, Jaeho proposed in the next refactoring to replicate all the sql\nqueries for each datastore, so we can optimize each sql query for each\ndatastore individually.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/320#issuecomment-111794500\n.\n. +1\n. This is awesome!\n. @feiranwang @netj after code review, let's mail back the user who needed this!!\n. @netj I thought this was fixed some time back... Also Kevin are you using ddlog? This takes care of building the dependencies for you... @feiranwang is that tutorial up?\n. This should have been fixed a long time ago... @netj @feiranwang ... :)\nLet's get it done!!!\n\nOn Mon, Aug 10, 2015 at 11:58 AM, Alex Ratner notifications@github.com\nwrote:\n\nThis is minor / low-priority, but it's convenient to be able to e.g. add a\nnew entity type or otherwise tweak the schema, and then run something like\ninitdb, without also dropping the sentences table say...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/351.\n. Any updates? This is an obvious pain point in the language that we should take care of soon! :)\n. Is this the end of akka?\n\nOn Wed, Aug 12, 2015 at 1:31 AM, Jaeho Shin notifications@github.com\nwrote:\n\n\nUses named arguments syntax in constructors.\nUses .copy() with named arguments when possible.\nSplits unit tests into individual .bats files, so each can be\n  run/excluded selectively.\n\n(Part of #329 https://github.com/HazyResearch/deepdive/issues/329)\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/HazyResearch/deepdive/pull/353\nCommit Summary\n- Collapses multiple .scala files for settings\n- Rewrites SettingsParser as a series of transformers\n- Splits scalatests.bats into .bats file per ScalaTest class\n- Rewrites SettingsParserSpec test to use named arguments\n- Reflects defaults for extractors and factors to config\n- Moves some settings checks to SettingsParser\n- Settings formatting cleanup\n- Organizes imports in settings\n- Fixes SettingsParser bugs\n- Fixes how scala tests were run through bats\n- Fixes SettingsParser to recognize MERGE mode\n- Converts to named arguments for Extractor settings\n- Converts to named arguments style for FactorDesc settings\n- Converts to named arguments style for DbSettings settings\n- Converts to named arguments style for CalibrationSettings\n- Cleans up FactorFunctionVariable constructor usage\n- minor cleanup of settings\nFile Changes\n- M Makefile\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-0 (5)\n- M src/main/resources/application.conf\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-1 (5)\n- M src/main/scala/org/deepdive/DeepDive.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-2 (32)\n- M src/main/scala/org/deepdive/settings/Settings.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-3 (245)\n- M src/main/scala/org/deepdive/settings/SettingsParser.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-4 (701)\n- D\n  src/main/scala/org/deepdive/settings/models/CalibrationSettings.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-5 (4)\n- D src/main/scala/org/deepdive/settings/models/Connection.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-6 (4)\n- D src/main/scala/org/deepdive/settings/models/DbSettings.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-7 (15)\n- D\n  src/main/scala/org/deepdive/settings/models/ExtractionSettings.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-8 (4)\n- D src/main/scala/org/deepdive/settings/models/Extractor.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-9 (21)\n- D src/main/scala/org/deepdive/settings/models/FactorDesc.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-10 (18)\n- D src/main/scala/org/deepdive/settings/models/FactorFunction.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-11 (77)\n- D\n  src/main/scala/org/deepdive/settings/models/InferenceSettings.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-12 (4)\n- D src/main/scala/org/deepdive/settings/models/InputQuery.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-13 (14)\n- D src/main/scala/org/deepdive/settings/models/LoaderConfig.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-14 (9)\n- D\n  src/main/scala/org/deepdive/settings/models/PipelineSettings.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-15 (12)\n- D src/main/scala/org/deepdive/settings/models/SamplerSettings.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-16 (4)\n- D src/main/scala/org/deepdive/settings/models/SchemaSettings.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-17 (28)\n- D src/main/scala/org/deepdive/settings/parsing/DataTypeParser.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-18 (10)\n- D\n  src/main/scala/org/deepdive/settings/parsing/FactorFunctionParser.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-19 (71)\n- D\n  src/main/scala/org/deepdive/settings/parsing/FactorWeightParser.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-20 (17)\n- D\n  src/main/scala/org/deepdive/settings/parsing/InputQueryParser.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-21 (18)\n- M src/test/scala/helpers/TestHelper.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-22 (26)\n- M src/test/scala/unit/datastore/DataLoaderSpec.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-23 (4)\n- M src/test/scala/unit/extraction/ExtractionManagerSpec.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-24 (54)\n- M src/test/scala/unit/extraction/ExtractorRunnerSpec.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-25\n  (413)\n- M src/test/scala/unit/helpers/HelpersSpec.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-26 (21)\n- M src/test/scala/unit/inference/InferenceManagerSpec.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-27 (7)\n- M src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-28\n  (299)\n- M src/test/scala/unit/settings/FactorFunctionParserSpec.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-29 (49)\n- M src/test/scala/unit/settings/SettingsParserSpec.scala\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-30\n  (319)\n- M test/bats.mk\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-31 (23)\n- M test/enumerate-tests.sh\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-32 (1)\n- A test/greenplum/scalatests\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-33 (1)\n- A test/mysql/scalatests\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-34 (1)\n- D test/mysql/scalatests.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-35 (1)\n- D test/postgresql/scalatests.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-36 (78)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.DataLoaderSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-37 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.DataTypeParserSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-38 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.ExtractionManagerSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-39 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.ExtractorRunnerSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-40 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.FactorFunctionParserSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-41 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.FactorWeightParserSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-42 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.FileDataUtilsSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-43 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.HelpersSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-44 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.InferenceManagerSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-45 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.InputQueryParserSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-46 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.PostgresExtractionDataStoreSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-47 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.PostgresInferenceRunnerSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-48 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.ProcessExecutorSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-49 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.ProfilerSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-50 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.SamplerSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-51 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.SettingsParserSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-52 (13)\n- A\n  test/postgresql/scalatests/org.deepdive.test.unit.TaskManagerSpec.bats\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-53 (13)\n- M test/postgresql/update-scalatests.bats.sh\n  https://github.com/HazyResearch/deepdive/pull/353/files#diff-54 (31)\nPatch Links:\n- https://github.com/HazyResearch/deepdive/pull/353.patch\n- https://github.com/HazyResearch/deepdive/pull/353.diff\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/353.\n. I like the default and then override strategy... but Jaeho is the boss.\n\nOn Wed, Aug 12, 2015 at 3:33 PM, Alex Ratner notifications@github.com\nwrote:\n\nHow can you set the extractor-level parallelism (i.g. the xargs\nparallelism for running each single extractor) in DDLog?\nI know that you can (presumably, from v0.6.x) go into the compiled\ndeepdive.conf file and then add e.g. parallelism: ${PARALLELISM} to every\nextractor, but is there some better way to do this / can there be?\nThe question of whether we'd want to set this for each individual\nextractor (as is done pre-DDLog) or just have one global specification is\nworth asking I think:\n- The former offers more flexibility (for ex: what if a certain\n  extractor has large overhead for loading resources, but then is super-fast\n  to run, and so we might want to run it with lower parallelism)\n- The latter is much simpler\nI'd be very happy with just the latter; either way seems like this should\nbe incorporated somehow (what is the default now btw..?)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/356.\n. I like it!\n\nOn Wed, Aug 12, 2015, 9:47 PM Alex Ratner notifications@github.com wrote:\n\nIs this on the roadmap out of curiosity? Seems like this would be nice and\nalso fairly simple way to centralize the basic DD app in the ddlog file- or\nis there a way to do this already? Thanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/357.\n. @feiranwang @netj this is a ddlog issue?\n. here is fine :) I'm just interested in DDlog issues right now. Make sure you send notes to those involved.\n. @feiranwang @netj @zhangce I thought we had this all nicely fixed on ubuntu? :) \n. @raphaelhoffmann did you write the EL tutorial up? @zhangce @netj \n. Absolutely! This may require some conversation about how to do it properly... but in our human trafficking applications there are essentially no databases and distant supervision works well...\n\none way to think about it is to write some (low quality) simple rules as distant supervision. for example, we had lots of collisions about which location (is it concord CA? TX?) -- but we heuristically used phone number to grab those which were less ambiguous. \nSince I don't know the domain, I can't generate these rules... but this would be fun to meet about...\n@ajratner @netj would love to have that elastic search regular expression integration... then as they search for rules we could data program :)\n. Roughly, the game works like this:\n- You heuristically generate many different kinds of low quality examples. Concretely, you write a bunch of regular expressions and python code. They labels you generate will be noisy--this is OK. It's a little disturbing from a classical ML perspective but fine from a stat/optimization perspective. \n- Now, the game is to get some redundancy in the labels so that you can \"denoise the labels\" Also, if there is structure between the label sets, you can even denoise to an even greater extent. \n- Of course, you may choose to label some data or to fix some rules, but engineering-wise it's hopefully easier... we have some theory that gives situations in which noise from many crappy rules is enough to learn to essentially perfect accuracy. \nBut, it's mostly in latex, and we need more experience... hence my excitement :) \nWe're building tools to support this type of extreme distant supervision... @zhangce @ajratner @netj @thodrek  COULD BE FUN!! \n. I don't know... :) @netj \n. hmm... this should be handled by DD... Seems easy enough? @zhangce @feiranwang \n. Why not make sure the simple cases work? We don't have to handle the\ngeneral case to be useful, right?\nOn Sat, Aug 29, 2015 at 9:27 PM Feiran Wang notifications@github.com\nwrote:\n\nSynced up with Zifei. The described approach applies to dumping a single\ntable or local join of tables. In DD, an extractor input query could be an\narbitrary SQL query, and the approach could not apply to such a general\ncase. We could optimize for specific types of queries using the ad hoc\napproach, by getting hints from the users, which I think might be too\nspecific.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/372#issuecomment-136084126\n.\n. Maybe just an annotation?\n\nOn Sat, Aug 29, 2015 at 9:41 PM Feiran Wang notifications@github.com\nwrote:\n\nSure, we can add support for that.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/372#issuecomment-136084646\n.\n. @netj @SenWu @zhangce I don't see why this would be an issue... Can you guys look at this? We'll get back to you soon!!\n. @SenWu Can you look at this?\n. You are far too nice!\n\nWe should be able to fix this soon... some of our team is very seriously\nstudying ... in hawaii for the week... I haven't seen them at the\nconference, so I assume they are in their rooms coding :)\nChris\nOn Thu, Sep 3, 2015 at 9:13 AM, vsoch notifications@github.com wrote:\n\nFantastic, many thanks @SenWu https://github.com/SenWu. There isn't\nhuge rush because I was able to go through the entire walkthrough on my\nlocal machine, and am not held back in any way to start developing our\nmethods locally. It's really a beautiful piece of software - I of course\nexpect to have bugs and what not, but the workflow with the database,\nmaking extraction rules, and running pipelines is very intuitive! :L)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/375#issuecomment-137499209\n.\n. Adding @SenWu. We should doc this.\n. @feiranwang Is this waiting @netj approval? He should be assigned :)\n. @zhangce @SenWu How could this be? This performance bug should have been fixed months ago.\n. This makes me really nervous :)\n\nOn Sun, Sep 13, 2015 at 9:39 AM Jaeho Shin notifications@github.com wrote:\n\nAny anticipated side effects if we drop DISTINCT? I think there must have\nbeen a good reason or use case why we added it in the first place.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/382#issuecomment-139893472\n.\n. If DISTINCT is not needed, then there has to be some convention somewhere\nthat ensure DISTINCT is unnecessary. IF you want to remove it, explain what\nthat convention and make sure it's documented (and ideally checked!)\n\nOn Sun, Sep 13, 2015 at 10:39 AM alldefector notifications@github.com\nwrote:\n\nI wasn't able to find who first introduced DISTINCT with git blame because\nof some refactoring. Maybe the initial author just has to speak up about\nhis thought process... Or someone with better git skills could track it\ndown...\nOn Sun, Sep 13, 2015 at 10:24 AM chrismre notifications@github.com\nwrote:\n\nThis makes me really nervous :)\nOn Sun, Sep 13, 2015 at 9:39 AM Jaeho Shin notifications@github.com\nwrote:\n\nAny anticipated side effects if we drop DISTINCT? I think there must\nhave\nbeen a good reason or use case why we added it in the first place.\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/pull/382#issuecomment-139893472\n.\n\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/pull/382#issuecomment-139897221>\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/382#issuecomment-139898855\n.\n. @SenWu @zhangce We have some issues failing on the pr... can we figure out the issue here? How robust is the failing incremental test? \n\nI'm, for example, worried that it would fail on another architecture or with different random draws, etc. Can someone give me some better understanding of this test? (in meeting is fine)\n. sounds good. Thanks for the explanation!\nOn Sun, Sep 20, 2015 at 3:07 AM Feiran Wang notifications@github.com\nwrote:\n\n@chrismre https://github.com/chrismre There was a bug introduced by an\nearlier commit, and it is fixed in #376\nhttps://github.com/HazyResearch/deepdive/pull/376. The incremental test\nis stable after merging #376\nhttps://github.com/HazyResearch/deepdive/pull/376.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/382#issuecomment-141769747\n.\n. @netj and @ajratner  is thinking about how to make this process dramatically easier... I don't know what the current status is :) \n. adding more people I emailed :) @thodrek @feiranwang \n. I'm not sure how good those examples actually are... @netj when you get back online can you sync up?\n. This is a great point, and we do support both for that reason. Speed issues\nto one side, we'd prefer a nice human reasable format. However, I think\nwhat Feiran's saying is that the speed penalty for JSON can be\nunreasonable. We've had people stub their toes on building JSON extractors,\nonly to find out it doesn't scale later. You're more than welcome to use\nit!\n\nOn Fri, Sep 18, 2015 at 12:27 AM Alexis BRENON notifications@github.com\nwrote:\n\nEven if it's faster, I think that JSON extractor, based on column name and\nso, that gives some semantic to your extractor, is far better for\nmaintenance: in case you have to add or remove some attributes, you can\neasily check its existence with JSON, but not with TSV. Both are useful,\ndepending on your goal : speed or continued existence.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/389#issuecomment-141369558\n.\n. I'm not so sure... they should be much faster... can we make sure it\ndoesn't blow up in our faces?\n\nOn Tue, Sep 22, 2015 at 6:55 PM alldefector notifications@github.com\nwrote:\n\nOK, looks like it's specific to plpy extractors:\nhttps://github.com/HazyResearch/deepdive/blob/e8b9dc329102c53bb273455953e8ac67a62e579c/src/main/scala/org/deepdive/extraction/ExtractorRunner.scala#L553\nhttps://github.com/HazyResearch/deepdive/blob/master/util/ddext_input_sql_translator.py#L103\nIf we avoid using plpy extractors, looks like DDLog works on PGXL.\nTime to deprecate plpy extractors?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/403#issuecomment-142470880\n.\n. I just want to say--we should definitely be able to get you setup! There\nare (ridiculous in my view) restrictions on some of the data from\nGeoDeepDive... however, we can set you up with those tasks.\n\nYour background is exactly the type of person we want to enable. I'd\npropose a deal: we can help you get setup (we'd do this anyway :) ), but\nperhaps you can help us document how to make it easier to use and write the\nnext version.\nI'd also say if you can stick to open source information (CC data), we can\nput your data our there and get even more users to help you in your goal.\nThis is very exciting for us!\nAll the best, Chris\nOn Fri, Sep 25, 2015 at 9:58 AM ukliu notifications@github.com wrote:\n\nFirst of all, thanks @netj https://github.com/netj for helping me with\nthe deepdive installation and the tutorial walk-through. It took me a long\ntime to go through all the tutorial programs line by line and understand\nhow they work.\nI have one small question on the tutorial programs, on\nhttps://github.com/HazyResearch/deepdive/blob/master/examples/tutorial_example/step1-basic/deepdive.conf\nline 27,32: Does not \u201csentences\u201d need to be defined?\nI found \u201csentences_dump.csv\u201d and \u201csentences_dump_large.csv\u201d in the folder\n/spouse/input. There is no file names as \u2018sentences.tsv\u201d. I am wondering,\ndoes the tsv-extractor take those two cvs files as \u2018sentences\u2019?\nIn addition, I have some general questions with the deepdive.\nMy goal of learning deepdive is to use it to extract and analyze relations\nout of scientific pdfs (or database containing pdfs). I am from a\nbackground of geology, familiar with Python, with some experience of Apache\nSpark, limited (only know the basic operations such as select, join, etc)\nexperience with SQL, and no experience with Java. For me to master the\ndeepdive, I believe that I have to learn to write the programs similar to\nwhat are shown in the tutorial material. My first question is, are there\nsome toy problems similar to the tutorial walk-through used to train\nnewbies?\nAnother question is, I used the Mac terminal to run the tutorial programs,\nwhich might be perfect for program developers but not friendly enough to me\nand probably to future deepdive users. Is there a software with GUI that\ncan perform deepdive operations? Do you guys use Java to debug your\ndeepdive programs?\nA third question is, I learned from the website that @zhangce\nhttps://github.com/zhangce has developed a geodeepdive DEMO but could\nnot find any website/software devoted to that. Is it still under\ndevelopment or the software has been released privately? Can I use it to\nperform my task, e.g., extract elemental, mineral and geochronological\nrelations from geological pdf articles/ databases? How do you guys extract\nrelations from pdfs?\nThanks for your patience!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/409.\n. Why not give out the ddlog version? it's easier to use right? and it's the\npreferred way...\n\nOn Fri, Sep 25, 2015 at 10:08 AM zhangce notifications@github.com wrote:\n\nA third question is, I learned from the website that @zhangce\nhttps://github.com/zhangce has developed a geodeepdive DEMO but could\nnot find any website/software devoted to that.\n@ukliu https://github.com/ukliu In terms of your third question, you\ncan find the code for PaleoDeepDive at https://github.com/HazyResearch/pdd\n. You can follow a similar structure for your\napplication. If you want more simple examples (and because you are from\nShanan's team),\nJulia also has an application that she builds from scratch over the summer\nusing DeepDive\nthat is potentially more easier to follow; but you probably need to ask\nShanan for the program.\nIf you have any Geo-related, Paleo-related, or PDF-acquisition-related\nquestions, we can\ndiscuss via email (and I will point you to the right people to ask).\nThanks!\nCe\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/409#issuecomment-143288959\n.\n. I should have checked =)\n\nOn Fri, Sep 25, 2015 at 10:20 AM zhangce notifications@github.com wrote:\n\nWhy not give out the ddlog version? it's easier to use right? and it's the\npreferred way...\nYes, ddlog is the preferred way to write applications now. The code in\nhttps://github.com/HazyResearch/pdd is already in ddlog.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/409#issuecomment-143292768\n.\n. @netj can you setup a Skype with @vsoch, let's make sure this is resolved :)\n. I think the plpython dependency is gone (or will be soon). @netj can\nconfirm :)\n\nOn Sat, Sep 26, 2015 at 7:56 PM vsoch notifications@github.com wrote:\n\nOnce I know more about the database, it might be needed to setup a Skype\nto talk about the different options for setting this up to run in parallel.\nI have a few ideas, will likely try a few things, and very likely will have\nquestions!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/410#issuecomment-143514227\n.\n. There should be a mode to warn if you are going to drop data. We should\nNEVER drop extensional , user supplied data. NEVER.\n\nIntensional relations should only be dropped in a \"force\" mode or something\n(imo).\nChris\nOn Sat, Sep 26, 2015 at 8:29 AM Jaeho Shin notifications@github.com wrote:\n\nThere's no way yet. I agree deepdive run should be idempotent. I think\nthe semantics of ddlog programs should be defined as starting from a state\nwhere all intensional relations are empty. We could try to detect which\nrelations are intensional vs. extensional, i.e. defined by ddlog vs. whose\ndata come from external sources, or provide a syntax (or just annotation?)\nand truncate all intensional relations upon every run.\nIf there aren't use cases for running multiple functions to fill up one\nrelation, e.g., table1 += f(...) :- ... . table1 += g(...) :- ... ., then\nwe could simply truncate (or drop/create) the output table for running\nfunctions for the moment and fully expand this EDB/IDB distinction later.\n@feiranwang https://github.com/feiranwang How does this sound?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/412#issuecomment-143463916\n.\n. This is why we have incremental processing...\n\nOn Sat, Sep 26, 2015 at 9:00 AM Raphael Hoffmann notifications@github.com\nwrote:\n\nWe just need a simple way to re-run deepdive run, for example after you\nmade some changes to a udf.\nIn that case you need to reset the generated tables. However, you can't\nuse deepdive initdb because it will clear the entire database. It is also\ntedious and error-prone to use deepdive sql because you can easily forget\na table.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/412#issuecomment-143466518\n.\n. Let be clear here. WE CANNOT DELETE USER DATA. :) It cannot happen. So\nplease make sure the default is that this truncation does not happen.\n\nChris\nOn Sat, Sep 26, 2015 at 11:33 AM Raphael Hoffmann notifications@github.com\nwrote:\n\n@feiranwang https://github.com/feiranwang That's exactly right. I just\na need a simple solution for the short term. Is there a way to truncate\nin ddlog?\nI understand that this more imperative way doesn't fit well with the\ndeclarative of nature of datalog, but the same may be true for \"+=\"...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/412#issuecomment-143478431\n.\n. let me propose that we have a separate call to deepdive that drops the\ntables @netj. Dropping tables does not seem like a feature that should be\nin the language...\n\nOn Sat, Sep 26, 2015 at 11:36 AM Chris Re chris.re@gmail.com wrote:\n\nLet be clear here. WE CANNOT DELETE USER DATA. :) It cannot happen. So\nplease make sure the default is that this truncation does not happen.\nChris\nOn Sat, Sep 26, 2015 at 11:33 AM Raphael Hoffmann \nnotifications@github.com wrote:\n\n@feiranwang https://github.com/feiranwang That's exactly right. I just\na need a simple solution for the short term. Is there a way to truncate\nin ddlog?\nI understand that this more imperative way doesn't fit well with the\ndeclarative of nature of datalog, but the same may be true for \"+=\"...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/412#issuecomment-143478431\n.\n. Thanks for the kind words! @netj @zhangce ?\n. This should not be on by default.\n\n\nOn Sun, Sep 27, 2015 at 9:00 PM Feiran Wang notifications@github.com\nwrote:\n\n@alldefector https://github.com/alldefector For ddlog application,\naccording to this doc\nhttp://deepdive.stanford.edu/doc/advanced/deepdiveapp.html#structure,\nthere can also be a deepdive.conf, which contains extra configuration\n(sampler args, etc.). For example, deepdive.conf may look like\ndeepdive.sampler.sampler_args: \"-l 200 -s 1 -i 500 --alpha 0.1 --sample_evidence\"\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/416#issuecomment-143636119\n.\n. @netj @zhangce What's going on here?\n. Oh no! Can we get that fixed =)\n. @netj @zhangce we should fix this!\n. Wow.\n. Let's not get crazy.\n\nOn Wed, Dec 16, 2015 at 6:29 PM alldefector notifications@github.com\nwrote:\n\nSounds like 1.0 instead of 0.8 :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/445#issuecomment-165317353\n.\n. This is awesome :)\n\nOn Thu, Dec 17, 2015 at 7:25 AM Jaeho Shin notifications@github.com wrote:\n\nFYI Here's the SVG rendered data flow for spouse example\nhttps://gist.github.com/netj/f4abe6eff63b837c7046#file-dataflow-svg and data\nflow for chunking example\nhttps://gist.github.com/netj/a4e363b92ffeb2fa95b7#file-dataflow-svg.\nI forgot to mention that this PR requires no change to the user's app\ncode, except maybe a few extra dependency information that were missing\nin the manually written deepdive.conf. The run.sh that typically handled\nenv setup and many other hacks should be moved to the right place such as\ninput/. @ThomasPalomares https://github.com/ThomasPalomares and I were\nable to get the genomics app (in DDlog) working with minimal changes.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/445#issuecomment-165483060\n.\n. Great feedback!!\n\nOn Tue, Dec 29, 2015 at 12:42 PM Raphael Hoffmann notifications@github.com\nwrote:\n\nHi @netj https://github.com/netj, this new version of deepdive is\nreally great! Here are a few notes I took while experimenting.\n1.\nWhen building on OSX, I got error that X11/Xlib.h couldn't be found.\n   This error\n   was thrown by buildkit module for graphviz.\n/usr/include/tk.h:78:23: fatal error: X11/Xlib.h: No such file or directory\nFollowing these\n   http://stackoverflow.com/questions/11465258/xlib-h-not-found-when-building-graphviz-on-mac-os-x-10-8-mountain-lion\n   instructions, I fixed it by adding a symlink\nln -s /opt/X11/include/X11 /usr/local/include/X11\nAfterwards, it was not sufficient to rerun 'make'. I manually cd'ed to\n   deepdive/depends/bundled/graphviz/graphviz-2.38.0 and ran ./configure\n   && make && make install. Then I could run \"make\" in deepdive\n   2.\nddlog apps didn't work with the default submodule commit, but \u2013\n   following your\n   suggestions \u2013 everything worked fine after upgrading to ddlog commit\n   ee0b359c9f018c19df5eddbf3bff60bdf076e091.\nNow everything works fine, but I've encountered two different problems in\na small number of runs:\n1.\nSometimes, stdin is no longer showing on the screen after a successful deepdive\n   do\n   execution. That is, I can type commands and they execute, but the\n   letters I type are not\n   being shown on the screen. I'm using iterm2.\n   2.\nSometimes (rarely), a mkmimo process kept running at 100% cpu usage,\n   after deepdive do\n   terminated with an error (possibly related to missing input data).\nI'll try to investigate more carefully if I encounter these again.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/445#issuecomment-167874340\n.\n. AWESOME! :)\n\nOn Thu, Jan 14, 2016 at 7:58 PM Thomas Palomares notifications@github.com\nwrote:\n\n@chrismre https://github.com/chrismre: this fixes the issue about\ndistributed_by in temporary tables.\nWith that + a bug in mkimo that Jaeho noticed, the time consuming\nextractions are now ~50x faster\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/453#issuecomment-171866395\n.\n. cc: @HazyResearch/genomics Does this fix the issue from today's meeting?\n. Ugh! I think we need this feature of reusing weights... This has got to be in a quick fix for 0.8.1 or something... @raphaelhoffmann does your fix work on 0.8? Can you create a PR for it?\n. This should have really been in the code a year ago @feiranwang. @zhangce Can you fix this?\n. ancient systems... dastardly.\n\nOn Thu, Jan 28, 2016 at 5:15 PM alldefector notifications@github.com\nwrote:\n\nThat'd be great! FWIW, these ancient systems probably had such blocking\nfrom the get-go:\nhttp://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/Gibbs%20sampling.aspx\nhttps://alchemy.cs.washington.edu/tutorial/tutorial.pdf (page 4)\nhttp://i.stanford.edu/hazy/tuffy/doc/tuffy-manual.pdf (page 8)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/471#issuecomment-176509148\n.\n. Bump. Can someone explain what's going on here? @netj \n. Also, @netj and @feiranwang, it should be super easy to declare categorical that are linked (say for our old entity linking design, they need to be able to express \n\nLinksToOne(candidate, entity) as a categorical random variable (each candidate maps to one entity). \nThis should be super easy (@key is fine). @zhangce, I know you're traveling but your thoughts are welcome when you get back on line :)\n@ajratner I don't want to redo the full constraints--no one seem to use them, and they have lots of code complexity... maybe in the summer :)\n. This was a very old feature... where did this go @zhangce @netj @SenWu \nYou don't even need syntax for this, it's easy to detect... @feiranwang \n. Zifei is out of the country.  :)\nIs this code stable? I thought you built this months ago! Has it been code reviewed? It would be awesome to put in !\nMoving the comment to another issue.\n. Ha, OK :)\n. Sen, how are we doing here?\n. How did we lose the ability to bring connected components into memory?\nThis needs to be fixed soon... Let's talk about this in dev meeting.\n@SenWu Isn't your code ready? The internal version has been there for MONTHS (years?)\nOn Tue, Feb 23, 2016 at 11:43 PM alldefector notifications@github.com\nwrote:\n\nYes, the sampler uses 40GB of memory with -c 1 and works for now. But we\nhave at least 50X as many docs. So partitioning is critical.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/478#issuecomment-188128352\n.\n. Yikes.\n\nOn Sun, Feb 21, 2016 at 2:36 AM alldefector notifications@github.com\nwrote:\n\nIf we replace the fast_seqassign with the regular PG's sequence\nassignment (even without the ORDER BY id hack for dump_weights),\neverything works out (example finishes successfully): the id column\nvalues are unique now, though the order of the result from select * from\ndd_weightsmulti_inf_istrue_tag limit 3; is still nondeterministic --\nmaybe somehow the order doesn't matter for sampler loading after all??? But\nthat defies the code\nfor (long i = 0; i < s; i++) {\n    assert(this->weights[i].id == i);\n  }\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/495#issuecomment-186793075\n.\n. Ugh, @netj any ideas? Shouldn't there be a unit test exactly for this?\n. Adding @ajratner @thodrek as their current experiments may be hampered by this bug.\n. Wait, why does the sampler do a resort?! That seems like a huge waste of time, and it shouldn't be done with the partitioning code...\n\nCan someone do an overhaul of categoricals with a thorough code review. This code seems to generate a large number of bug reports, and there seem to be no unit tests that catch any of these bugs. \n. Excellent!\nOn Mon, Feb 22, 2016 at 11:00 AM Jaeho Shin notifications@github.com\nwrote:\n\nSorry I was sidetracked the entire day after a quick fix attempt. It's\napparent that I left a bug in the sequence assignment for GP and never\nnoticed it because db-driver currently lacks tests for many parts after\ngetting a quick expansion. Moreover, we don't have automatic tests for GP.\nWe should figure out how to install/run GP on Travis.\nSampler resorting was ridiculous so I already proposed a fix, exploiting\nthe fact that we assign ids from a consecutive range beginning from zero:\nHazyResearch/sampler#14 https://github.com/HazyResearch/sampler/pull/14.\nAs long as the sequence assignment works the sampler should be working fine.\nWill fix this asap. Sorry for alerting so many people for this silly bug!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/495#issuecomment-187321097\n.\n. @netj We've been talking about simple modules, I think this would help here too :).... and I didn't mention it to AJR :)\n. Any closure here? Can we close this issue? :)\n. @raphaelhoff Can you leave a few samples of what you'd like to do? In particular, you should be able to filter based on the categorical value.... and what do you mean by correlations between values (They are exclusive...)\n@netj @alldefector \n. - So this is a performance optimization? Is there something semantically wrong with having those weights? Is this in response to a particular performance issue (as it seems the system should just do the right thing here...) \n- If you only wanted to add manual features for equal pairs, then why not write a rule that filters on equality? This should generate weights only for those pairs (that's the meaning of the weight being a function of the supported pairs. We've had this syntax for a while... @netj has this changed? We can discuss on the white board.\n\nWhy worry about these impossible combinations instead of learning 0 weights from data--especially since your impossible combinations now have to be enumerated by hand (which seems to defeat the point of learning).\ncc: @mikecafarella \n. This really shouldn't be so hard to resolve. Introducing a probe on each variable and factor could be a killer for performance... However, since after an initial pass the indexes are completely resolved, one could imagine bloating the data structure slightly to cache the needed indexes/pointers.\nWhat makes me a little concerned is that...I'm not sure why this code is difficult or complex... it's a relatively straightforward bit of code. It's not clear to me that this change isn't to fix a symptom of the deeper problem of low quality code in the compiler.\n. Maybe our reliance on shell versions and scripts is too much... why not use\nsomething like python that seems to be more portable? ...\nOn Tue, Jun 28, 2016 at 9:54 AM Jaeho Shin notifications@github.com wrote:\n\n@shahin https://github.com/shahin\n1. Why do we bundle bash with DeepDive? This seems like a very\n   low-level dependency to include in our distribution. As a DeepDive user or\n   new developer, I imagine I'd be very surprised to find out (eventually)\n   that it's not using the same bash that I'm using on my host system. Seems\n   like an explicit dependency on a given bash version would be clearer.\nBecause it's almost impossible to ask every user to install the exact bash\nversion on their system, we decided to bundle the exact version our scripts\nneed: 4.3.x. Bundling GNU coreutils is along the same line. Mac users\nwasted many hours/days if not weeks because of these since it's bash 3 and\nBSD. Docker could achieve something similar, but it rather avoids the\nportability problem not solving it, and it's too heavy to just get these\nfew megs of portable dependencies (on my Mac: bash is 7MB, coreutils is\n9MB).\n1. As long as we're bundling bash, should we be using it for all\n   tests? An exception for a single .bats seems easily overlooked, and\n   now our tests are targeting different platforms. You know more about bats\n   than I do -- what's the best way to run them all on bash 4.3?\nSure. The example I commented was just for illustration. I agree it's\nbetter to run all tests with our own stuff, which is already the case for\nthe rest, but bash used by bats was an unfortunate exception that wasted\nyour time. I think you can prefix the make test in .travis.yml with dist/stage/bin/deepdive\nenv:\ndist/stage/bin/deepdive env  make test\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/551#issuecomment-229111366,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AHPtuDiERKFmFNw4GmxEsDxun6OQ6eEeks5qQVG5gaJpZM4I8ClL\n.\n. I think @alldefector is in charge of this change--and it should be pushed to master soon :) Adding folks in here who can answer the definitive state of the code. Thanks for the kind words!\n. Grounding could be a bottleneck. Is the code that generates a partition on the fly too fragile or something? As it seems like being able to generate the partitions on demand (or not) would be helpful. That could be a down the road feature, but I'm curious about this design choice (e.g., we could avoid ever touching disk).\n. You may want to check out http://hazyresearch.github.io/snorkel/, which is\nmore directly about weak supervision.\n\nDeepDive can do all this (and much more!). This flexibility means that not\nall representation decisions are obvious.\nChris\nOn Mon, Dec 19, 2016 at 8:24 PM \u5b59\u660e\u660e notifications@github.com wrote:\n\nWhen I want to define a unsupervised model by using \"p(x,y)=NULL: ....\",\nthe program report:\ncolumn \"label\" is of type boolean but expression is of type text\nIt seems that the error is about a bug in sql generation module.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/608#issuecomment-268151386,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AHPtuEFo-aPa3pwU-pGppjuv12ulh3Pfks5rJ1iIgaJpZM4LRaM2\n.\n. DeepDive can express essentially any factor graph. You'll need to write the rules that create the required factor graph. A default factor graph for this process is described in the Snorkel/data programming paper. \n\nHope that helps! Chris. ",
    "alldefector": "Now we have treedlib and other stuff...\n. Bucketization or continuous features make total sense.\n. YES\n. We have docker for build now (and conda packaging). Let's use https://github.com/HazyResearch/deepdive/issues/467 to track docker for distribution.\n. Out of scope...\n. We shall have ooc / so sampler soon\n. Thanks, Jaeho. Looks like it's getting a bit complicated to backport... Unless develop are going to be merged within a couple days, could you or Feiran patch master with the 5fa7493 change? It'd make grounding 50X faster for current users.\n. We don't have temp tables like the above anymore, though we still have tables like dd_categories_$usertable. 14 chars longer isn't a big concern (even though we could shorten it to dd_cat_). Either way, as long as the DB consistently panics and gives meaningful error messages, that should enough not to frustrate the user for now.\n. Just rebased on master again.\n. Added documentation and unit tests. Will need to think about logging.\n. @netj yes, I'd leave the integration tests to you.. thanks!\nI'm done with this prototype for now. Could you take another look at the new changes? I'm still using fifo with an eager log sucker thread and a 100K-line circular buffer in memory.\n. This is great! Maybe also add some git hooks to enforce from the client side? Travis takes too long..\nE.g.: http://stackoverflow.com/questions/591923/make-git-automatically-remove-trailing-whitespace-before-committing\n. I don't really understand the script, but it looks quite legit! How do I approve the PR?\n. Awesome change! Haven't looked at all the scripts. Would be nice if there is cmd option for the app directory and/or app conf file name.\n. Do we plan to keep the ddlog --> conf --> DD flow in the long term? The translation loses the nicely defined user schema in ddlog even if it does generate the .sql file.\n@netj any plan to pass all the structures (schema, rules) in ddlog to DD directly so that DD can more tightly manage the database (e.g., creating / loading / altering user tables directly instead of having the cumbersome .sql file)?\n. @astrung the \"data programming\" process that @chrismre is not embodied in https://github.com/HazyResearch/snorkel\n. Related to this, when loading data, the coordinator is also the bottleneck. We haven't found a good way to do parallel loading yet. PG 9.5 offers the ability to switch b/w LOGGED and UNLOGGED for a table, which could be used to speed up data loading for PG. Unfortunately, PGXL is based on PG 9.2 and so cannot benefit from that.\n. @raphaelhoffmann : what throughput do you get for loading with UNLOGGED? Did you find a work-around to turn off UNLOGGED after loading?\n. PGXL distributes by first usable (primitive types) column by default. If both base tables were distributed by the ID column, this join would probably make each data node send data to all other data nodes -- by the same token, each node would also process data from all other nodes. And the cross-node data pipes might not be very efficiently implemented.\nIdeally the DDL code generated by DD should specify the best dist key (by feature in this case).\n. Correct me if I'm wrong -- looks like all those post-hoc-assigned sequence\nIDs are always used as array indexes in the sampler and never for SQL\njoins. If that's true, what are the challenges to do without the sequence\nassignment steps?\nFeng\nOn Sun, Sep 6, 2015 at 5:43 PM Raphael Hoffmann notifications@github.com\nwrote:\n\nSure! The log of the run is here:\n/lfs/local/1/raphaelh/dd/new/deepdive/log/2015-09-06T093453.txt\nDatabase is memex_mar2015_large and it's running inference only. I launched\nthis job at 9:30AM and it still hasn't reached the sampling phase.\nThanks!\nOn Sun, Sep 6, 2015 at 5:28 PM, SenWu notifications@github.com wrote:\n\nThanks! Could you share the log of your run to us?\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138140054\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138144569\n.\n. Or if the ID fields are used as conceptual foreign keys in the DB, maybe we\ncould use a fingerprint of the tuple.\n\nOn Sun, Sep 6, 2015 at 8:43 PM Feng Niu niufeng14@gmail.com wrote:\n\nCorrect me if I'm wrong -- looks like all those post-hoc-assigned sequence\nIDs are always used as array indexes in the sampler and never for SQL\njoins. If that's true, what are the challenges to do without the sequence\nassignment steps?\nFeng\nOn Sun, Sep 6, 2015 at 5:43 PM Raphael Hoffmann notifications@github.com\nwrote:\n\nSure! The log of the run is here:\n/lfs/local/1/raphaelh/dd/new/deepdive/log/2015-09-06T093453.txt\nDatabase is memex_mar2015_large and it's running inference only. I\nlaunched\nthis job at 9:30AM and it still hasn't reached the sampling phase.\nThanks!\nOn Sun, Sep 6, 2015 at 5:28 PM, SenWu notifications@github.com wrote:\n\nThanks! Could you share the log of your run to us?\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138140054\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138144569\n.\n. Oh, I meant doing fingerprinting in our code over the tuple content as the\ntuple is generated. Basically a hash function of the tuple.\n\n\nOn Sun, Sep 6, 2015 at 9:08 PM zhangce notifications@github.com wrote:\n\nThe finger print of tuples in GP is a little weird (it is too physical in\nthe sense that there are no guarantee of when it will not change at all)...\nneed to do more profiling after Raphael's current run. That function should\nbe faster than 2MB/s if the DB is working properly.\nOn Sep 6, 2015 9:04 PM, \"alldefector\" notifications@github.com wrote:\n\nOr if the ID fields are used as conceptual foreign keys in the DB, maybe\nwe\ncould use a fingerprint of the tuple.\nOn Sun, Sep 6, 2015 at 8:43 PM Feng Niu niufeng14@gmail.com wrote:\n\nCorrect me if I'm wrong -- looks like all those post-hoc-assigned\nsequence\nIDs are always used as array indexes in the sampler and never for SQL\njoins. If that's true, what are the challenges to do without the\nsequence\nassignment steps?\nFeng\nOn Sun, Sep 6, 2015 at 5:43 PM Raphael Hoffmann \nnotifications@github.com\nwrote:\n\nSure! The log of the run is here:\n/lfs/local/1/raphaelh/dd/new/deepdive/log/2015-09-06T093453.txt\nDatabase is memex_mar2015_large and it's running inference only. I\nlaunched\nthis job at 9:30AM and it still hasn't reached the sampling phase.\nThanks!\nOn Sun, Sep 6, 2015 at 5:28 PM, SenWu notifications@github.com\nwrote:\n\nThanks! Could you share the log of your run to us?\n\u2014\nReply to this email directly or view it on GitHub\n<\n\n\n\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138140054\n\n\n\n.\n\n\u2014\nReply to this email directly or view it on GitHub\n<\n\n\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138144569\n\n\n.\n\n\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138178468\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138180360\n.\n. Thanks @SenWu. If we assign non-contiguous IDs or fingerprints to the\ntuples (say in Java or bash) right before each tuple is inserted into the\nDB, those joins should still work, right?\n\nIs it possible to tweak the sampler so that we can feed it variables and\nfactors without sequential IDs?\nOn Sun, Sep 6, 2015 at 9:12 PM SenWu notifications@github.com wrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann From the log file,\nthere is nothing wrong... Please let me know if you have further\ninformation.\n@alldefector https://github.com/alldefector We use assigned variable\nIDs in SQL joins to ground edges, you can find it here:\nhttps://github.com/HazyResearch/deepdive/blob/1573cb703079b10cb2dcc93b3d67e09d079d628c/src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala#L718\n.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138182930\n.\n. @zhangce Yeah, getting rid of table rewrite (UPDATE) is exactly what I had\nin mind. Is the sequentialness is absolutely necessary but generating it\ninside the DB is costly, can we design a way to generate the IDs from the\nextractor driver code? If all extraction is done in one process, we could\njust use a simple integer variable. If there are multiple extraction\nprocesses, maybe we could use some sort of global sequence ID service to\ncoordinate b/w the extraction processes. One example of such a service\nwould be a synchronized piece of shared memory (say /dev/shm) storing next\navailable ID. Each process can reserve IDs by chunks -- \"I'm taking the\nrange 1-1000, increment the global counter to 1001\". That way, there might\nbe gaps in the final list of IDs, but the amount of gaps is limited to\nchunk_size X num_processors.\n\nOn Mon, Sep 7, 2015 at 9:56 AM zhangce notifications@github.com wrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann I profile the query SELECT\nfast_seqassign('dd_query_rates_features', 0); which took 2 hours in your\nlog. I think it is a DB problem instead of how we implement this function.\nThe key ratio to see whether the bottleneck is really the fast_seqassign\nfunction is to see the ratio of execution time between\n`create table tmp as select * from dd_query_rates_features;`\nand\n`SELECT fast_seqassign('dd_query_rates_features', 0);`\nIf the ratio is off by 1 significantly, that means the bottleneck is\nfast_seqassign, which I think is not the case here (I did not let the\nCREATE TABLE query finish--it is not making much progress after 1 hour)...\nI think for this Github issue, we need to restart DB, try to VACUUM etc.\n@alldefector https://github.com/alldefector The game of optimizing this\nquery is try to eliminate the one pass of table writing caused by UPDATE\n(because postgresql does not have in-place update)... We have no global\nsync for contiguous IDs, so we might not be able to see much improvement if\nwe do non-contiguous IDs...\n@chrismre https://github.com/chrismre mentioned yesterday that why we\ndo not use generate_sequence (I forget why I did not use it yesterday...\nNow I remember :)) The problem is that GP's optimizer (at least in 2013)\ndoes not know that tuples with the same gp_segment_id is on the same\nsegment, so I can not get a sequential scan plan with generate_sequence...\nI do agree that if we change that Python function to C++, we might see some\nimprovement, but we need to profile it first (by reporting the ratio of\nruntime between the above two queries)\nCe\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138340266\n.\n. Awesome! Who is going to own those changes?\n\nOn Mon, Sep 7, 2015 at 2:40 PM zhangce notifications@github.com wrote:\n\n@alldefector https://github.com/alldefector Your recommendation makes\nperfect sense for one fast_seqassign call (there are two such calls that\nare expensive)\n1. There is one fast_seqassign call for variables loaded from the output of extractors.\n2. There is another fast_seqassign call for factors output by SQL grounding queries.\nFor 1, we could just have a really fast shell script to assign IDs in TSV\nbefore loading. (just like your global sequence ID service idea)\nFor 2, we don't really need ID for factors, so that part can probably be\neliminated completely.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138382390\n.\n. Looks like we may want to revisit the design of the internal tables. For\nexample,\nhttps://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/inference/InferenceNamespace.scala#L27-L28\n\nWe have these two prefixes: \"dd_query_${tableName}\"\nand \"dd_factors_${tableName}\".\n\"dd_query_\" is presumably for variables, but it's actually used for\nfactors. Variable tables are created by users (i.e., no prefix).\n\"dd_factors_\" is unused anywhere.\nI suppose at some point somebody planned to copy user-created tables (e.g.,\ngenes) to internal variable tables (e.g., dd_query_genes), and if that's\nthe case, we could piggyback ID assignment on the copy. Not sure if any\nprevious version of DD ever did such copying.\nIf there is a strong reason to not have internal variable tables, why don't\nwe just add an auto-increment (with shared sequence) ID column to each\nuser-created/defined variable table? (I'd ditch attempted support for\nnon-pg family DBs, namely yourSQL.)\nWould the shared sequence be a bottleneck for parallel DBs?\nOn Tue, Sep 8, 2015 at 2:23 PM Raphael Hoffmann notifications@github.com\nwrote:\n\nI ran VACUUM last night and then restarted the job. The time for sequence\nassignment on \"rates\" went down from 3h to 2:18h. I haven't checked the\ntime for sequence assignment on the feature table yet.\nSo, it's a small improvement, but still sequence assignment is a major\nbottleneck.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138706567\n.\n. Based on my experience with Java, NoClassDefFoundError might occur when a\nthread is cleaned up (some sort of race condition between the user code and\nJVM's thread manager I think). The DD log file is still printing every\nminute:\n\n21:05:53.772 [default-dispatcher-3][taskManager][TaskManager] INFO  Memory\nusage: 107/2101MB (max: 27305MB)\n21:06:53.772 [default-dispatcher-3][taskManager][TaskManager] INFO  Memory\nusage: 107/2101MB (max: 27305MB)\n21:07:53.772 [default-dispatcher-3][taskManager][TaskManager] INFO  Memory\nusage: 107/2101MB (max: 27305MB)\n21:08:53.772 [default-dispatcher-3][taskManager][TaskManager] INFO  Memory\nusage: 108/2101MB (max: 27305MB)\n...\nBut looks like it's just akka puking -- I don't see tobinary.py or\nformat_converter running.\nDoes the converter or the python script print out logs that are not\nredirected by Java? Those logs would be useful. Maybe generate the input\nfile and halt DD and then run the converter manually so we can see the logs.\nOn Sun, Sep 6, 2015 at 8:10 PM Raphael Hoffmann notifications@github.com\nwrote:\n\nSee log below. There are two issues here:\n1. I am getting a java.lang.NoClassDefFoundError when running\n   format_converter.\n2. The error is only reported on the console, but not in the error log\n   file. After that, the system goes into an idle state, where to_binary.py is\n   no longer running; however the DD process is, and there seems to be no\n   activity. Unfortunately, the relevant files eg. dd_variables_rates are\n   deleted when this error occurs making it time-consuming to re-run.\nI've reproduced this error twice.\n17:30:11 [PostgresInferenceRunnerComponent$PostgresInferenceRunner(akka://deepdive)] INFO  Converting grounding file format...\n17:30:11 [PostgresInferenceRunnerComponent$PostgresInferenceRunner(akka://deepdive)] DEBUG Executing: python /lfs/raiders4/1/raphaelh/dd/new/deepdive/util/tobinary.py /lfs/raide\nrs4/1/raphaelh/dd/new/deepdive/out /lfs/raiders4/1/raphaelh/dd/new/deepdive/util/format_converter /lfs/raiders4/1/raphaelh/dd/new/deepdive/out\n17:30:53 [taskManager] INFO  Memory usage: 721/2082MB (max: 27305MB)\n17:31:53 [taskManager] INFO  Memory usage: 721/2082MB (max: 27305MB)\nException: java.lang.NoClassDefFoundError thrown from the UncaughtExceptionHandler in thread \"Thread-15\"\nException: java.lang.NoClassDefFoundError thrown from the UncaughtExceptionHandler in thread \"Thread-14\"\n17:32:53 [taskManager] INFO  Memory usage: 721/2082MB (max: 27305MB)\n17:33:53 [taskManager] INFO  Memory usage: 721/2082MB (max: 27305MB)\n17:34:53 [taskManager] INFO  Memory usage: 722/2082MB (max: 27305MB)\n17:35:53 [taskManager] INFO  Memory usage: 722/2082MB (max: 27305MB)\n...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/381.\n. @zhangce is working on the next-gen petabyte-scale sampler that will do without the object boxing and converter!\n. I wasn't able to find who first introduced DISTINCT with git blame because\nof some refactoring. Maybe the initial author just has to speak up about\nhis thought process... Or someone with better git skills could track it\ndown...\n\nOn Sun, Sep 13, 2015 at 10:24 AM chrismre notifications@github.com wrote:\n\nThis makes me really nervous :)\nOn Sun, Sep 13, 2015 at 9:39 AM Jaeho Shin notifications@github.com\nwrote:\n\nAny anticipated side effects if we drop DISTINCT? I think there must have\nbeen a good reason or use case why we added it in the first place.\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/pull/382#issuecomment-139893472>\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/382#issuecomment-139897221\n.\n. The UPDATE query for assignment would effectively copy all tuples being updated. I believe that's what @raphaelhoffmann meant... Looks like var ID assignment was thrown out a couple of months ago, so this should have been addressed...\n. OK, looks like it's specific to plpy extractors:\nhttps://github.com/HazyResearch/deepdive/blob/e8b9dc329102c53bb273455953e8ac67a62e579c/src/main/scala/org/deepdive/extraction/ExtractorRunner.scala#L553\nhttps://github.com/HazyResearch/deepdive/blob/master/util/ddext_input_sql_translator.py#L103\n\nIf we avoid using plpy extractors, looks like DDLog works on PGXL.\nTime to deprecate plpy extractors?\n. Yes, it appears plpythonu does require super user: http://stackoverflow.com/questions/2848704/postgresql-running-python-stored-procedures-as-a-normal-user\n. Love Ce's idea! Schema-level separation between user data and internal data also came up when we were designing a live-ingestion-and-processing system (which is of course incremental).\nA stop gap might be some syntax like table1 = f(...) (as opposed to table1 += f(...)), which would clear the table before populating it.\n. @netj what do you think of @zhangce 's proposal above? We could specify the search_path (say dd_run_0003, public) at the connection level or in the DB connection string.\nEncapsulating all the internal tables away from user data will be a great move. Benefits are manyfold:\n- cleanness: easy to see what's input, what's output, and what's intermediate\n- lower risk of losing user data\n- more friendly for people developing DD in a scientific / iterative manner (preserve runs)\n- more friendly for people developing multiple extractors against the same input (think 200GB sentences table in GP and five people writing / running extractors against it)\n. @astrung that might be a symptom of https://github.com/HazyResearch/deepdive/issues/416\nNamely that 75422 - 70969 tuples were \"supervised\".\n. Closing this following #416\n. Hi @feiranwang , did you mean we can specify --sample_evidence in app.ddlog or the deepdive run command line? I wasn't able to find out how...\nIn light of people's confusion regarding this design choice, what do you guys think if we turn the flag on by default?\n. Looks like there are two issues:\n1. Do we run inference on supervised data points or not? How?\n2. Should we give the user an output table that contains all extractions regardless of whether they are supervised or predicted?\nI suppose everyone would agree that we say yes to question 2. And 2 is the source of lots of confusions; e.g., wondering why there is data loss, wondering why MindBender doesn't index all the data, and wondering why many DD programs make strange \"unsupervised copies\" of \"supervised\" tuples.\nA regular user of DD doesn't have a good understanding of \"evidence variables\" or how Gibbs sampling works. To them, the input to DD is some source data and some seed dictionaries or patterns; the output should be all extractions / predictions. It may well be the case that some of the \"predictions\" are (supervision) rule-based, they are still predictions nevertheless. With the upcoming \"data programming\" framework, I've heard that those rules would start to have weights and become not that black and white -- by then, I suppose it'll become natural to have a probability for \"supervised\" predictions as well.\n. @netj Looks like this has been resolved by the above PR in 0.8. Feel free to reopen if that's not the case.\n. Related: #424\nSupport for result pipelining would be great. Looks like a key challenge would be to cleanly model the \"result\" (see #416).\n. OK, UDFs are language-agnostic and decoupled from DD core (only connection is command line string in ddlog and data pipes at runtime). @netj we should probably emphasize these points in the next version of docs. Feel free to close either now or when doc is updated.\n. Sounds like 1.0 instead of 0.8 :)\n. @zifeishan @netj are you guys suggesting that we write $sql to a temp file and use -f to run it (so that the whole block isn't run as one transaction)? Sounds like it should work. We only need to update PGXL's db-execute driver, right?\n. Apparently -f does not run the file in one transaction unless you also specify -1 or --single-transaction: http://postgres-xc.sourceforge.net/docs/1_0/app-psql.html\nThat's actually what we need because PGXL would throw up (Zifei's error messages) if we try to run a bunch of mutating / DDL statements in one transaction.\n. With the change, you are still updating the initvalue field; and it's not a key column.\nReal problem seems to be that PGXL doesn't support correlated UPDATE involving multiple nodes:\nhttp://sourceforge.net/p/postgres-xc/mailman/message/26727433/\nThere is a newer version of PGXC (basis of PGXL) that may have solved that limitation:\nhttps://github.com/postgres-x2/postgres-x2\nWe could try it at some point.\nAnyway, I heard that the scala code will go away right after the 0.8 release?\n. My vote would be to drop scala in 0.8.0 because doing it in 0.8.1 may come as a bigger surprise since it's a minor release (or we'd have to unnecessarily wait till 0.9.0). Cannot wait to see Akka die!\n. @netj should this go to 0.8?\n. Got it. Thanks!\n. Great idea. @netj it may be as simple as adding something like abs(weight) > 0.001 to the grounding queries (likely also a join with the weight value table)?\n. Let's call these \"flexible-arity factors\". It should not be hard to add support for them. However, we need to see more of compelling use cases, and decide the degree of customizability we are going to provide:\n- boolean vars with a fixed indicator function (say AND over an array of bool vars) is easy in ddlog (though we still need to figure out the syntax since var IDs are not part of user schema now)\n- categorical vars are trickier because we'd need array of  pairs\n- we've thought about supporting user-defined factor functions (LLVM?); not clear if we should attack that directly or try to support (in ddlog and sampler) a more limited form of customization such as flexible-arity factors\n. Var IDs are now no longer exposed to the user schema space.\n. I'd prefer DD to auto compile by default since that's the most common use case.\n. The OneIsTrue factor type doesn't seem to be implemented as a constraint, so categorical variable seems like the way to go. For grounding, I'd do a GROUP BY @key to get an array of tuple IDs for each group; and each such array would be a categorical variable in the sampler.\n. Very good point! Yes, looks like we do have to keep the identities of the underlying boolean variables in those groups so that they can correlate with other variables. I don't know how categorical variables are currently implemented in the sampler. For things to work, the underlying boolean variables must be referenceable in the factors.\nConceptually it'd be cleaner to say all variables are boolean and it's possible enforce one-is-true constraints among a group of variables. As for implementation, I don't know if it's easier for the Variable class or the Factor class to handle such constraints...\n. Yes, it is the one-is-true constraint. However, the sampler doesn't seem to support this constraint currently. There is a factor type by the same name, but it's not a constraint.\nAlso, there is no front-end support in DeepDive / DDLog to take advantage of it once we do have such support in the sampler.\n. That'd be great! FWIW, these ancient systems probably had such blocking from the get-go:\nhttp://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/Gibbs%20sampling.aspx\nhttps://alchemy.cs.washington.edu/tutorial/tutorial.pdf (page 4)\nhttp://i.stanford.edu/hazy/tuffy/doc/tuffy-manual.pdf (page 8)\n. OK, we do have categorical vars now.\n. @SenWu Any update?\nWe have an app with 100K docs, 13M vars, 42M weights, 248M edges, and 242M factors. The machine has 2 NUMA nodes. DD was stuck at CREATE FG ON NODE ...1 for over an hour with the sampler using 70GB of RAM and the machine apparently thrashing...\n2016-02-23 17:58:50.972139 ################################################\n2016-02-23 17:58:50.972150 # nvar               : 13447656\n2016-02-23 17:58:50.972162 # nfac               : 242889469\n2016-02-23 17:58:50.972175 # nweight            : 42308790\n2016-02-23 17:58:50.972187 # nedge              : 248001424\n2016-02-23 17:58:50.972200 ################################################\n2016-02-23 18:02:27.024966 LOADED VARIABLES: #13447656\n2016-02-23 18:02:27.025074          N_QUERY: #11900206\n2016-02-23 18:02:27.025096          N_EVID : #1547450\n2016-02-23 18:02:47.727552 LOADED WEIGHTS: #42308790\n2016-02-23 18:05:38.553535 LOADED FACTORS: #242889469\n2016-02-23 18:09:43.023361 CREATE FG ON NODE ...1\n. Yes, the sampler uses 40GB of memory with -c 1 and works for now. But we have at least 50X as many docs. So partitioning is critical.\n. @raphaelhoffmann mentioned that he saw this problem on Linux as well...\n. OK, we tried adding ORDER BY id (and ORDER BY id, categories) but still get the same error.\nAnd this seems to be a bigger issue: \nFrom GP:\nselect * from dd_weightsmulti_inf_istrue_tag order by id, categories limit 20;\n dd_weight_column_0 | isfixed | initvalue | id  | categories \n--------------------+---------+-----------+-----+------------\n word=Misa          | f       |         0 | 169 | 0\n word=Leahy         | f       |         0 | 170 | 0\n word=Misa          | f       |         0 | 170 | 1\n word=entries       | f       |         0 | 171 | 0\n word=Leahy         | f       |         0 | 171 | 1\n word=Misa          | f       |         0 | 171 | 2\n word=sponsorship   | f       |         0 | 172 | 0\n word=entries       | f       |         0 | 172 | 1\n word=Leahy         | f       |         0 | 172 | 2\n word=Misa          | f       |         0 | 172 | 3\n word=A.            | f       |         0 | 173 | 0\n word=sponsorship   | f       |         0 | 173 | 1\n word=entries       | f       |         0 | 173 | 2\n word=Leahy         | f       |         0 | 173 | 3\n word=Misa          | f       |         0 | 173 | 4\n word=sold          | f       |         0 | 174 | 0\n word=A.            | f       |         0 | 174 | 1\n word=sponsorship   | f       |         0 | 174 | 2\n word=entries       | f       |         0 | 174 | 3\n word=Leahy         | f       |         0 | 174 | 4\n(20 rows)\nFrom PG:\n```\nselect * from dd_weightsmulti_inf_istrue_tag order by id, categories limit 20;\ndd_weight_column_0  | isfixed | initvalue | id  | categories \n---------------------+---------+-----------+-----+------------\n word=Cardiovascular | f       |         0 | 169 | 0\n word=Cardiovascular | f       |         0 | 170 | 1\n word=Cardiovascular | f       |         0 | 171 | 2\n word=Cardiovascular | f       |         0 | 172 | 3\n word=Cardiovascular | f       |         0 | 173 | 4\n word=Cardiovascular | f       |         0 | 174 | 5\n word=Cardiovascular | f       |         0 | 175 | 6\n word=Cardiovascular | f       |         0 | 176 | 7\n word=Cardiovascular | f       |         0 | 177 | 8\n word=Cardiovascular | f       |         0 | 178 | 9\n word=Cardiovascular | f       |         0 | 179 | 10\n word=Cardiovascular | f       |         0 | 180 | 11\n word=Cardiovascular | f       |         0 | 181 | 12\n word=Scotia         | f       |         0 | 182 | 0\n word=Scotia         | f       |         0 | 183 | 1\n word=Scotia         | f       |         0 | 184 | 2\n word=Scotia         | f       |         0 | 185 | 3\n word=Scotia         | f       |         0 | 186 | 4\n word=Scotia         | f       |         0 | 187 | 5\n word=Scotia         | f       |         0 | 188 | 6\n(20 rows)\n```\nIt looks like that something is wrong with the weight ID assignment in GP. Not sure if that's related to this console output:\n2016-02-21 01:15:28.272272 + deepdive db assign_sequential_id dd_weights_inf_istrue_tag id 169 13\n2016-02-21 01:15:28.318028 NOTICE:  language \"plpgsql\" already exists, skipping\n2016-02-21 01:15:28.318075 CREATE LANGUAGE\n2016-02-21 01:15:28.332492 ERROR:  language \"plpythonu\" already exists\n2016-02-21 01:15:28.461849 NOTICE:  table \"tmp_gpsid_count\" does not exist, skipping\n2016-02-21 01:15:28.461964 CONTEXT:  SQL statement \"drop table if exists tmp_gpsid_count cascade;\"\n2016-02-21 01:15:28.461980 PL/pgSQL function \"fast_seqassign\" line 2 at execute statement\n2016-02-21 01:15:28.461997 NOTICE:  table \"tmp_gpsid_count_noagg\" does not exist, skipping\n2016-02-21 01:15:28.462011 CONTEXT:  SQL statement \"drop table if exists tmp_gpsid_count_noagg cascade;\"\n2016-02-21 01:15:28.462023 PL/pgSQL function \"fast_seqassign\" line 3 at execute statement\n2016-02-21 01:15:29.023230 NOTICE:  EXECUTING _fast_seqassign()...\n2016-02-21 01:15:30.125127 ERROR:  column \"cname\" does not exist\n2016-02-21 01:15:30.125280 LINE 1: SELECT  ('update ' ||  $1  || ' set ' || cname || ' = update...\n2016-02-21 01:15:30.125355                                                  ^\n2016-02-21 01:15:30.125386 QUERY:  SELECT  ('update ' ||  $1  || ' set ' || cname || ' = updateid(' ||  $2  || ', gp_segment_id, ARRAY[' ||  $3  || '], ARRAY[' ||  $4  || '], ARRAY[' ||  $5  || ']);')::text is not null\n2016-02-21 01:15:30.125426 CONTEXT:  PL/pgSQL function \"_fast_seqassign\" line 12 at if\n2016-02-21 01:15:30.125477 SQL statement \"select * from _fast_seqassign('dd_weights_inf_istrue_tag', 169);\"\n2016-02-21 01:15:30.125551 PL/pgSQL function \"fast_seqassign\" line 8 at execute statement\n2016-02-21 01:15:30.204862 NOTICE:  sequence \"dd_seq_dd_weights_inf_istrue_tag_id\" does not exist, skipping\n. If we replace the fast_seqassign with the regular PG's sequence assignment (even without the ORDER BY id hack for dump_weights), everything works out (example finishes successfully): the id column values are unique now, though the order of the result from select * from dd_weightsmulti_inf_istrue_tag limit 3; is still nondeterministic -- maybe somehow the order doesn't matter for sampler loading after all??? But that defies the code\nfor (long i = 0; i < s; i++) {\n    assert(this->weights[i].id == i);\n  }\n. Looks like this is the bug: the db-assign_sequential_id script for GP didn't keep up with the inc parameter (step size between consecutive IDs) that was presumably introduced for categorical variables:\nhttps://github.com/HazyResearch/deepdive/blob/6786a806ec04b087db6b29fbdd1f5c469a6e6601/compiler/compile-config/compile-config-2.01-grounding#L385-L396\nhttps://github.com/HazyResearch/deepdive/blob/13e8788b9bf39e2d3aa83d0883a6391c9c9d6592/database/db-driver/postgresql/db-assign_sequential_id\nhttps://github.com/HazyResearch/deepdive/blob/13e8788b9bf39e2d3aa83d0883a6391c9c9d6592/database/db-driver/greenplum/db-assign_sequential_id\n. Two side notes:\n1. Right now categorical variables have a fixed size and grounding is via a cross join between the \"pseudo weight table\" and range(variable_size). How hard is it to support categorical vars whose size is dependent on the data (e.g., a mention can be linked to any number of candidate entities)? This is essentially https://github.com/HazyResearch/deepdive/issues/471 @SenWu @netj \n2. The title question is still a puzzle for me: Weight dumping is not sorted by id, but sampler expects so -- yet DD works fine even when GP returns non-deterministically ordered data. @feiranwang @zhangce\n. @feiranwang That explains it! I'm renaming this issue now...\n. We also need to add the inc parameter (as in PG's version this script).\n. LGTM!\n. You are probably running Java 7. CoreNLP requires Java 8.\n@netj We've encountered the same problem a couple of days ago. We should probably add some Java version validation code in the example code.\n. The \"do / redo / done\" workflow management is great, but I haven't got the hang of it...\nRelated issues include https://github.com/HazyResearch/deepdive/issues/563 and https://github.com/HazyResearch/deepdive/issues/502 and https://github.com/HazyResearch/deepdive/issues/554\n- Even for a simple program, the workflow graph can have many details already, with all steps implicitly derived from ddlog (including built in ones like init/app and those based on table and rule names). Can we simplify and explicify the workflow? For example, maybe we could use the higher granularity of AJR's \"labeled groups of operations\"?\n- The automagic dependency resolution is great, but for most programs in practice, the lexical order (sequences of rules or groups) probably suffices? If there are interleaved fancy subgraphs that allow for efficiency savings in incremental runs, they probably should have been broken down into smaller modules in the first place?\n- The automagic execution state (and mark done/todo) management is great, but it also requires the user to have a full understanding of the underlying workflow and keep a mental model in sync. The source code versioning issue here makes it even more complicated. Maybe let's consider a (separate) stateless execution model?\n. @netj: we are still living with this heisenbug... Does it look like it could be mkmimo?\n. The \"equal pair\" or skip-chain factors are certainly still supported: http://deepdive.stanford.edu/example-chunking#3-statistical-learning-and-inference\nI guess \"filter based on the categorical value\" is not supported yet? @netj \n. @raphaelhoff here is the link again: http://deepdive.stanford.edu/example-chunking#3-statistical-learning-and-inference\n\"equal pair\" means equal words, not equal labels. Equal labels would be learned by DD.\nWe can discuss quality improvement offline. For performance, it seems to be closely related to the dense / sparse FG representation @netj is working on for dynamic-scope categorical vars. As DD evolves, it's possible that we'll have more and more sophistication in graph representation; e.g., maybe some notion of lazy grounding.\n. @syadlowsky Those are great points. The syntax as is can be quite confusing and some cleanup is in order... and yes, better documentation. For now, you can see the distinctions in this section: http://deepdive.stanford.edu/example-spouse#2-distant-supervision-with-data-and-rules\nIn short:\n- = is for supervision (assigning TRUE / FALSE / NULL to a tuple), not data flow; e.g. is_spouse(p1, p2) = TRUE :- golden_marriage(p1, p2)\n- += populates a relation with a UDF; e.g., is_spouse_cands += extract_cands(sid, text) :- sentences(sid, text)\n- :- delimits the rule's head from body, as in the above examples. Besides the above complex heads, the rule can also be simply is_spouse_cands(p1, p2) :- all_person_pairs(p1, p2) which is just a (optionally materialized) view.\nAll of those cases are also mentioned in the above link.\n. @RominYue What version of DD were you running (deepdive version)? Could you try the same on the latest version (git pull; git submodule update --init; make build; make install) and see if the error persists?\n. LGTM\n. Sorry for being curt in the first post... Since the variable and factor arrays are critical for the sampler's performance, we have to do the ID sequentialization at some point. I like the two-phase idea, and think that it can probably indeed simplify things: the fingerprinting would simplify the SQL joins (no more ID mapping tables shadowing every base table), and the second-phase mapping might actually be easily doable by the sampler's loading phase (by keeping a few fingerprint->ID lookup tables in memory).\n. Tabling this for now that I've become the 2nd person who can work with the jq code...\n. buildkit moreutils_0.58 dependency is blocking travis...\n. The biased coin example? The deepdive.conf code stopped working because of the var table change...\nWill address the comments soon!\n. Travis seems to always fail because the chunking example times out (10min):\nNo output has been received in the last 10 minutes, this potentially indicates a stalled build or something wrong with the build itself.\n. @netj PTAL. Also, is it possible to increase the travis timeout limit? (Could also try reducing number of docs in chunking test.)\n. Yeah, those joins from reuse_weight stage of the spouse example never finish on my macOS either... I'm trying reducing the chunking numbers.\n. In the spouse test, the \"postgresql spouse example reuse weights\" step, there are only 3688 variables, but the big join in https://github.com/HazyResearch/deepdive/issues/541 could not finish even after 2HR... Maybe we should try to change the key from two columns to one.\n. @shahin Related: The missing 'T' issue is a bug in postgres's to_json that was fixed only in 9.5...\nhttp://postgresql.nabble.com/BUG-12578-row-to-json-and-to-json-add-T-in-timestamp-field-td5834396.html\n. LGTM\n. I assume you meant dd_graph_weights.categories? Removing now.\n. LGTM!\n. Let's just port this to python3.\n. see also https://github.com/HazyResearch/deepdive/pull/549\n. @netj bundling GP and PG clients are terrific ideas. They are causing lots of pain right now.\n. @netj: @shahin has a prototype export_to_json routine using psycopg2 that's only 1.5-3x as slow compared to COPY to TSV. Maybe let's just bundle psycopg2 and call it a day here?\n. @netj Great! @shahin can push the script in the next K days (perhaps in a separate PR).\nIf we could remove the dependency on \"psql\" (and others like \"createdb\"), that'd be fabulous! That would also make TSJ implementation much easier, I suppose.\n. We use TSV very extensively. I have seen folks patching the symptoms of issues like this in many places. This is a fundamental issue for data in motion (and at rest) in general. We need to nail down a robust data format and stick with it consistently everywhere. Just \"TSV\" is not specific enough as a standard.\n. So looks like PG COPY by default escapes chars with backslash, and deepdive sql internally wraps the query with COPY. When the original string has a single backslash, a user probably won't expect the output of deepdive sql to contain two backslashes.\n1. Maybe we want to override the default escaping behavior of COPY with things like ESCAPE WITH E'\\b' so that what the user sees with psql is also what she gets with deepdive sql?\n2. We may also want to change other DeepDive operators (e.g., load) similarly to be symmetric.\n3. The SerDe behavior needs to be well-known to users so that other systems know how to consume / produce data.\n. As far as universal support goes, JSON encoding does seem to be the best choice and TSJ seems like a great tradeoff between simplicity, usability, and performance. I think we can probably commit to TSJ as the standard human-readable data format if we can do it without adding much overhead in terms of both human efficiency and machine efficiency.\nSpecifically, we need to decide where and how to plug in the SerDe implementation. For example, if it's only in DeepDive, people won't be able to load and unload data in desired format by directly interacting with the DB. So either we let DeepDive monopolize data load/unload, or we need to implement/distribute the SerDe inside the DB server or alongside DB clients (namely wherever people call psql). Or maybe we can just make it clear that ad hoc load/unload is outside the TSJ sphere and extra care is required for ad hoc data to join the sphere.\nAnother concern is speed. For example, can we make TSJ loading at most 10% slower than TSV loading? Would json.loads calls in Python slow down a dummy UDF's throughput by 10X?\nJSON string quoting seems simple enough: http://stackoverflow.com/a/27516892\nGiven that, would we like to roll our own specialized faster parser/serializer in case the standard JSON routines are slow?\n. dup of #494... It always comes back...\n. This removes dropdb:\nhttps://github.com/HazyResearch/deepdive/pull/592/commits/90a371bb423c2b5c68bd0a6015c57da60e15c2a4\n. @netj The naming is optimized for the future, i.e., when we manage to implement the RATIO semantics, it's also the time we achieve balance :)\n. LGTM!\n. @netj PTAL\n. LGTM!\n. The only place where variable.is_observation is used seems to be only at the beginning of GibbsSamplerThread::sample_sgd_single_variable: if (variable.is_observation) return; Not sure if there is any code for \"outputting different values based on the value of those observed variables\"... (perhaps quasi-hard rules in ddlog?)\nRegardless, looks like this bug has been there since Dec 2015 (when netj revamped grounding with jq) -- and nobody has noticed! Assuming that is_evidence works for the intended use case, perhaps let's just purge is_observation?\n. OK, to minimize effort, I'm landing this for now. Next time we make any related changes / refactoring, we can do the merciless purge...\n. OK, that's indeed related. Here is what I tried:\nrm -f run/model/grounding/variable/link/domains.part*\nDEEPDIVE_NUM_PROCESSES=XXX deepdive redo process/grounding/variable/link/dump_domains process/grounding/combine_factorgraph process/model/learning\nI tried different values of XXX. If XXX is 1 or 20, var count from load_domains is correct. If XXX is 28 or above, var count from load_domains becomes smaller than expected.\nIf I just run\nDEEPDIVE_NUM_PROCESSES=30 deepdive redo process/grounding/combine_factorgraph process/model/learning\nthe var count is always correct.\nSo, something seems fishy with the concurrency control of deepdive compute execute or perhaps file flushing in sampler-dw text2bin domain /dev/stdin >(pbzip2 >domains.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2), or the symlink creation in combine_factorgraph... (In other words, load_domains tried to read the part files before they are ready.)\n. In https://github.com/HazyResearch/deepdive/blob/master/runner/compute-driver/local/compute-execute#L106 , maybe that detects only the completion of sampler-dw text2bin even if pbzip2 continues to run?\n. If I add sleep 2 at the end of process/grounding/variable/link/dump_domains/run.sh, the problem is fixed. I also saw that the mtime of some run/model/grounding/variable/link/domains* files is GREATER than when flatten factorgraph/domains was executed. Those kind of confirm that deepdive compute execute didn't wait for the dump processes to fully finish.\n. This post suggests that there is no obvious way to wait for the redirection subprocess (pbzip2):\nhttp://unix.stackexchange.com/questions/65192/how-to-wait-for-a-subprocess-used-for-i-o-redirection\nIf we replace\nsampler-dw text2bin domain /dev/stdin >(pbzip2 >domains.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2)\nwith\nsampler-dw text2bin domain /dev/stdin /dev/stdout | pbzip2 >domains.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2\nThe problem seems to go away (correct var count).\nLesson is that we should avoid using redirection subprocesses in compute execute commands.\n@netj any potential issue you see with this change?\n. Great. What's the best way to deal with the existing pipes, such as https://github.com/HazyResearch/deepdive/blob/master/compiler/compile-config/compile-config-2.01-grounding#L598-L604 ?\n. Just got bitten by this again in the sharding branch... Worked around (https://github.com/HazyResearch/deepdive/pull/592/commits/0a215b9f7c1b73614f2d57c4ca02552d86278149), but still don't have a good solution for cases where both redirection and tee are used.\n. LGTM!\n. @shahin could you merge or rebase master?\n. @raphaelhoff I suppose git submodule update --init fixed them all?\n. @vpsm Sorry about the errors. I was able to reproduce that in a clean repo. If you run mkdir .build first before make depends, it seems to work. Let me know if it doesn't. We'll fix the Makefile soon. (cc @netj )\n. I have never seen this error before and tests run fine. It sounds like a versioning issue with jq or bash in a bad build (like you said, \"stale code\").\n@raphaelhoff  Could you print the relevant lines from \"run.sh\"? Maybe also edit it to print out versions of jq and bash.\n. same problem happens when building from a clean repo.\n. @minafarid I cannot seem to repro this... Could you run deepdive version to check the build date of deepdive and if it's more than a couple of months old, run git pull; make build; make install to rebuild? Let me know if the problem persists even after that.\n. Yeah, we've seen this error a couple of times... Try running git submodule update --init after git pull.\n. If you ran deepdive do all, there should be a view called dd_inference_result_weights_mapping.\n. @minafarid deepdive run is deprecated... https://github.com/HazyResearch/deepdive/issues/563\n. Hi @vpsm , we've merged the categorical support into master a couple of months ago. The documentation on the DeepDive website is still stale though as we haven't made a new release yet...\nIn any case, the chunking example is actually categorical. In the current version of DD, here is how you declare a categorical variable:\nhttps://github.com/HazyResearch/deepdive/blob/master/examples/chunking/app.ddlog#L1-L5\nThe @key annotation marks the \"possible-world\" key, and fields without @key are categorical values (i.e., classes). Note that the @key columns are NOT a primary key for the chunk table in the DB sense.\nAll those inference rules are categorical:\nhttps://github.com/HazyResearch/deepdive/blob/master/examples/chunking/app.ddlog#L39-L88\nNote that we have got rid of the old awkward Multinomial(...) syntax. Right now, we only support the AND factor function over categorical vars (chunk(s, w1, tag1) ^ chunk(s, w2, tag2) but not say chunk(s, w1, tag1) => chunk(s, w2, tag2)), and don't support mixing categorical vars with boolean vars in the inference rule yet. However, we are making changes (e.g., https://github.com/HazyResearch/deepdive/pull/589) to move toward supporting them.\nIf you'd like to play with the chunking example on a laptop, you may want to run these first:\nexport SUBSAMPLE_NUM_WORDS_TRAIN=2000\nexport SUBSAMPLE_NUM_WORDS_TEST=2000\nThe bigram and word-embedding factors are particularly dense (7M factors for 4K vars) to demonstrate the ddlog language, and the above commands would scope down the input data to make sure DD doesn't take forever... (we'll make this example snappier by default at some point.)\n. @vpsm yes, exactly! In the first case, you can leave out both @keys -- if no field has @key, implicitly all fields form the key and it's a boolean var.\n. Hi @vpsm closing this thread for now. If you encounter other issues (compiling or otherwise), feel free to open a new one :)\n. Right, once we have sampler scale-out, the big joins and disk IO can be the bottleneck. It's actually easy to do per-partition on-demand grounding with the current code and never have the factors hit the disk. To make that happen, there are two things we need to change:\n1. Right now, for each inference rule, we materialize the factor table F that's essentially the rule's conjunctive query joined with the variable ID assignment tables. F is then used in two ways: 1) to generate distinct weight parameters for ID assignment; 2) to generate DW binary format factors for the sampler (joining F with weight ID assignment and category ID assignment). Having these two use cases was the reason that we wanted to materialize F. But in hindsight, it probably makes more sense to have F as a view for the first use case... For the second use case, a view can probably save a lot of IO as well... @netj thoughts?\n2. The current DD execution mode is a self-contained workflow. Almost by definition of \"on demand\", we need to turn the shell scripts driven learning / inference process into a cleaner model-control architecture where data exchange, scheduling, and callbacks can happen between the DB and sampler services.\n. @chrisdesa let me know if this change is in harmony with your partitioning algo.\n. Yes, 16 bits for shard ID, 48 bits for compact var ID.\nRe F materialization, the issue is that when you have a rule like @weight(type) Honest(p) :- PeopleLocation(p, city), CityType(city, type) where PeopleLocation is a huge table and CityType is a small table, to find distinct weights from the materialized table, we'd scan a huge table. But if F is a view, the DB might be smart enough to ignore the join and get them from the small table directly. Of course, there are more complicated weight params that would require the join -- in which case we'd evaluate the join twice and materialization may be more efficient. Maybe we could use EXPLAIN to decide?\nRe sampler interface, numbskull is actually currently using mmap for data loading:\nhttps://github.com/HazyResearch/numbskull/blob/master/numbskull/numbskull.py#L257\n. Tested on real-world work loads, and all work as expected (linearly reduced memory, linearly faster epoch speed, similar weights, probs, and quality numbers).\nPiggybacked a bunch of nasty bug fixes (destroying database, data unloading race condition due to text2bin >(pbzip2 ...)).\nChatted with @thodrek briefly yesterday, who's going to make it really fly on multiple hosts. @thodrek changing this line allows you to avoid materializing the factors: https://github.com/HazyResearch/deepdive/blob/30aeaafc41aaec856bb49e35cbe0fba319a65fef/compiler/compile-config/compile-config-2.01-grounding#L378\n. @thodrek Can I land this so you can build other fancy things on top?\n. @netj looks like dockerhub access is flaky:\n$ DOCKER_IMAGE_TO_PUSH=$DOCKER_IMAGE_TEST_PREFIX; if [[ $TRAVIS_TEST_RESULT = 0 ]]; then DOCKER_IMAGE_TO_PUSH+=PASS; else DOCKER_IMAGE_TO_PUSH+=FAIL; fi;\nafter_script.2\n0.01s$ if [[ $TRAVIS_PULL_REQUEST = false ]]; then tag=travis.$TRAVIS_BRANCH; else tag=travis-pr$TRAVIS_PULL_REQUEST; fi; tag=$(echo -n \"$tag\" | tr -c 'A-Za-z0-9_.-' _);\n$ docker login -e \"$encrypted_DOCKER_EMAIL\" -u \"$encrypted_DOCKER_USERNAME\" -p \"$encrypted_DOCKER_PASSWORD\";\nFlag --email has been deprecated, will be removed in 1.13.\nLogin with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: \nNo output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.\n. Awesome!\n. @rudaoshi the person_has_canser.id column indicates that it was created by an old version (say v0.8) of DeepDive, and that schema is no longer compatible with the examples in the latest git repo. We are about to release the next version, but until then, you could build from git by running make build; make install. Then running the examples should work. FYI, with the latest build, here is the schema:\nTable \"public.person_has_cancer\"\n    Column     |       Type       | Modifiers \n---------------+------------------+-----------\n person_id     | bigint           | \n dd_label      | boolean          | \n dd_truthiness | double precision |. @rudaoshi Here is an example rule with manually assigned weights (adapted from the \"census\" example):\n@weight(1.2)\nrich(id) :- adult(id, _, workclass, _, _, _, _, _, _, _, _, _, _, _, _, income_bracket).\nYou can change the rule body (the adult... part) to encode the \"common sense\" cases.\n. @rudaoshi note that there are two colons: NULL::boolean. If that doesn't work, try cast(null as boolean).. @rudaoshi Could you try building DD from git (see https://github.com/HazyResearch/deepdive/issues/607#issuecomment-268308601) and rerunning this program? If the compilation problem persists, could you post the error messages? Thanks!. @paidi that does seem like OOM... The log content suggests that you were probably running an old version of DeepDive (v0.8). In an upcoming release (coming soon), we'll introduce a factor graph partitioning feature, where a partitionable factor graph is processed piece by piece in memory. We haven't written up the documentation for this yet, but if you want to try it now using master (git clone and then make build; make install), the \"census\" example has turned on this feature:\nhttps://github.com/HazyResearch/deepdive/blob/master/examples/census/app.ddlog#L23\nhttps://github.com/HazyResearch/deepdive/blob/master/examples/census/deepdive.conf#L3\nIn words, you can enable partitioning by changing two places in your DD program\n1. In app.ddlog, add @partition to the appropriate column (e.g., doc_id) of the variable relation.\n2. In deepdive.conf, add sampler.partitions: $NUM_PARTITIONS, where $NUM_PARTITIONS is the number of partitions.\nInternally, DeepDive partitions the graph based on the hash value of the column marked with @partition.. @dBlairMorton yes, the build process does require internet access as it fetches code and dependencies. Building on an open system and then copying the binaries should work.. Hi @melporto \n1. Yes, there is the dw gibbs subcommand that transforms the text format into binary format. You can find example commands here: https://github.com/HazyResearch/sampler/blob/master/test/text2bin.bats\nIn DeepDive, you'd run it like deepdive env sampler-dw text2bin --help\n\n\nThe binary data format is documented here: https://github.com/HazyResearch/sampler/blob/master/doc/binary_format.md#variables-binary\n\n\nThe metadata file is a simple text line with 4 integers: https://github.com/HazyResearch/sampler/blob/master/doc/binary_format.md#metadata-text\nInternally, DeepDive generates the file here: https://github.com/HazyResearch/deepdive/blob/master/compiler/compile-config/compile-config-2.01-grounding#L697\nIf you'd like to manually edit the factor graph data, you could also generate the file manually.\nIf you are interested, you could also check out the intermediate data generated by DeepDive in the run/model/grounding/ and run/model/factorgraph directories. Note that only (compressed) binary files are there and the text format is never stored on disk.\n. Thanks for reporting this, @cognitronz . Yes, we've learned that this dummy file requirement is a bit cumbersome. We'll address it in the upcoming release (likely by removing this requirement).. Hi @EadrenKing , it could take 1 min or so to get started. If it took much longer than that, one possibility is that the available memory was low and the OS started paging. Perhaps check the OS stats to see if that's the case?. Now we have treedlib and other stuff...\n. Bucketization or continuous features make total sense.\n. YES\n. We have docker for build now (and conda packaging). Let's use https://github.com/HazyResearch/deepdive/issues/467 to track docker for distribution.\n. Out of scope...\n. We shall have ooc / so sampler soon\n. Thanks, Jaeho. Looks like it's getting a bit complicated to backport... Unless develop are going to be merged within a couple days, could you or Feiran patch master with the 5fa7493 change? It'd make grounding 50X faster for current users.\n. We don't have temp tables like the above anymore, though we still have tables like dd_categories_$usertable. 14 chars longer isn't a big concern (even though we could shorten it to dd_cat_). Either way, as long as the DB consistently panics and gives meaningful error messages, that should enough not to frustrate the user for now.\n. Just rebased on master again.\n. Added documentation and unit tests. Will need to think about logging.\n. @netj yes, I'd leave the integration tests to you.. thanks!\n\n\nI'm done with this prototype for now. Could you take another look at the new changes? I'm still using fifo with an eager log sucker thread and a 100K-line circular buffer in memory.\n. This is great! Maybe also add some git hooks to enforce from the client side? Travis takes too long..\nE.g.: http://stackoverflow.com/questions/591923/make-git-automatically-remove-trailing-whitespace-before-committing\n. I don't really understand the script, but it looks quite legit! How do I approve the PR?\n. Awesome change! Haven't looked at all the scripts. Would be nice if there is cmd option for the app directory and/or app conf file name.\n. Do we plan to keep the ddlog --> conf --> DD flow in the long term? The translation loses the nicely defined user schema in ddlog even if it does generate the .sql file.\n@netj any plan to pass all the structures (schema, rules) in ddlog to DD directly so that DD can more tightly manage the database (e.g., creating / loading / altering user tables directly instead of having the cumbersome .sql file)?\n. @astrung the \"data programming\" process that @chrismre is not embodied in https://github.com/HazyResearch/snorkel\n. Related to this, when loading data, the coordinator is also the bottleneck. We haven't found a good way to do parallel loading yet. PG 9.5 offers the ability to switch b/w LOGGED and UNLOGGED for a table, which could be used to speed up data loading for PG. Unfortunately, PGXL is based on PG 9.2 and so cannot benefit from that.\n. @raphaelhoffmann : what throughput do you get for loading with UNLOGGED? Did you find a work-around to turn off UNLOGGED after loading?\n. PGXL distributes by first usable (primitive types) column by default. If both base tables were distributed by the ID column, this join would probably make each data node send data to all other data nodes -- by the same token, each node would also process data from all other nodes. And the cross-node data pipes might not be very efficiently implemented.\nIdeally the DDL code generated by DD should specify the best dist key (by feature in this case).\n. Correct me if I'm wrong -- looks like all those post-hoc-assigned sequence\nIDs are always used as array indexes in the sampler and never for SQL\njoins. If that's true, what are the challenges to do without the sequence\nassignment steps?\nFeng\nOn Sun, Sep 6, 2015 at 5:43 PM Raphael Hoffmann notifications@github.com\nwrote:\n\nSure! The log of the run is here:\n/lfs/local/1/raphaelh/dd/new/deepdive/log/2015-09-06T093453.txt\nDatabase is memex_mar2015_large and it's running inference only. I launched\nthis job at 9:30AM and it still hasn't reached the sampling phase.\nThanks!\nOn Sun, Sep 6, 2015 at 5:28 PM, SenWu notifications@github.com wrote:\n\nThanks! Could you share the log of your run to us?\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138140054\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138144569\n.\n. Or if the ID fields are used as conceptual foreign keys in the DB, maybe we\ncould use a fingerprint of the tuple.\n\nOn Sun, Sep 6, 2015 at 8:43 PM Feng Niu niufeng14@gmail.com wrote:\n\nCorrect me if I'm wrong -- looks like all those post-hoc-assigned sequence\nIDs are always used as array indexes in the sampler and never for SQL\njoins. If that's true, what are the challenges to do without the sequence\nassignment steps?\nFeng\nOn Sun, Sep 6, 2015 at 5:43 PM Raphael Hoffmann notifications@github.com\nwrote:\n\nSure! The log of the run is here:\n/lfs/local/1/raphaelh/dd/new/deepdive/log/2015-09-06T093453.txt\nDatabase is memex_mar2015_large and it's running inference only. I\nlaunched\nthis job at 9:30AM and it still hasn't reached the sampling phase.\nThanks!\nOn Sun, Sep 6, 2015 at 5:28 PM, SenWu notifications@github.com wrote:\n\nThanks! Could you share the log of your run to us?\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138140054\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138144569\n.\n. Oh, I meant doing fingerprinting in our code over the tuple content as the\ntuple is generated. Basically a hash function of the tuple.\n\n\nOn Sun, Sep 6, 2015 at 9:08 PM zhangce notifications@github.com wrote:\n\nThe finger print of tuples in GP is a little weird (it is too physical in\nthe sense that there are no guarantee of when it will not change at all)...\nneed to do more profiling after Raphael's current run. That function should\nbe faster than 2MB/s if the DB is working properly.\nOn Sep 6, 2015 9:04 PM, \"alldefector\" notifications@github.com wrote:\n\nOr if the ID fields are used as conceptual foreign keys in the DB, maybe\nwe\ncould use a fingerprint of the tuple.\nOn Sun, Sep 6, 2015 at 8:43 PM Feng Niu niufeng14@gmail.com wrote:\n\nCorrect me if I'm wrong -- looks like all those post-hoc-assigned\nsequence\nIDs are always used as array indexes in the sampler and never for SQL\njoins. If that's true, what are the challenges to do without the\nsequence\nassignment steps?\nFeng\nOn Sun, Sep 6, 2015 at 5:43 PM Raphael Hoffmann \nnotifications@github.com\nwrote:\n\nSure! The log of the run is here:\n/lfs/local/1/raphaelh/dd/new/deepdive/log/2015-09-06T093453.txt\nDatabase is memex_mar2015_large and it's running inference only. I\nlaunched\nthis job at 9:30AM and it still hasn't reached the sampling phase.\nThanks!\nOn Sun, Sep 6, 2015 at 5:28 PM, SenWu notifications@github.com\nwrote:\n\nThanks! Could you share the log of your run to us?\n\u2014\nReply to this email directly or view it on GitHub\n<\n\n\n\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138140054\n\n\n\n.\n\n\u2014\nReply to this email directly or view it on GitHub\n<\n\n\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138144569\n\n\n.\n\n\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138178468\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138180360\n.\n. Thanks @SenWu. If we assign non-contiguous IDs or fingerprints to the\ntuples (say in Java or bash) right before each tuple is inserted into the\nDB, those joins should still work, right?\n\nIs it possible to tweak the sampler so that we can feed it variables and\nfactors without sequential IDs?\nOn Sun, Sep 6, 2015 at 9:12 PM SenWu notifications@github.com wrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann From the log file,\nthere is nothing wrong... Please let me know if you have further\ninformation.\n@alldefector https://github.com/alldefector We use assigned variable\nIDs in SQL joins to ground edges, you can find it here:\nhttps://github.com/HazyResearch/deepdive/blob/1573cb703079b10cb2dcc93b3d67e09d079d628c/src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala#L718\n.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138182930\n.\n. @zhangce Yeah, getting rid of table rewrite (UPDATE) is exactly what I had\nin mind. Is the sequentialness is absolutely necessary but generating it\ninside the DB is costly, can we design a way to generate the IDs from the\nextractor driver code? If all extraction is done in one process, we could\njust use a simple integer variable. If there are multiple extraction\nprocesses, maybe we could use some sort of global sequence ID service to\ncoordinate b/w the extraction processes. One example of such a service\nwould be a synchronized piece of shared memory (say /dev/shm) storing next\navailable ID. Each process can reserve IDs by chunks -- \"I'm taking the\nrange 1-1000, increment the global counter to 1001\". That way, there might\nbe gaps in the final list of IDs, but the amount of gaps is limited to\nchunk_size X num_processors.\n\nOn Mon, Sep 7, 2015 at 9:56 AM zhangce notifications@github.com wrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann I profile the query SELECT\nfast_seqassign('dd_query_rates_features', 0); which took 2 hours in your\nlog. I think it is a DB problem instead of how we implement this function.\nThe key ratio to see whether the bottleneck is really the fast_seqassign\nfunction is to see the ratio of execution time between\n`create table tmp as select * from dd_query_rates_features;`\nand\n`SELECT fast_seqassign('dd_query_rates_features', 0);`\nIf the ratio is off by 1 significantly, that means the bottleneck is\nfast_seqassign, which I think is not the case here (I did not let the\nCREATE TABLE query finish--it is not making much progress after 1 hour)...\nI think for this Github issue, we need to restart DB, try to VACUUM etc.\n@alldefector https://github.com/alldefector The game of optimizing this\nquery is try to eliminate the one pass of table writing caused by UPDATE\n(because postgresql does not have in-place update)... We have no global\nsync for contiguous IDs, so we might not be able to see much improvement if\nwe do non-contiguous IDs...\n@chrismre https://github.com/chrismre mentioned yesterday that why we\ndo not use generate_sequence (I forget why I did not use it yesterday...\nNow I remember :)) The problem is that GP's optimizer (at least in 2013)\ndoes not know that tuples with the same gp_segment_id is on the same\nsegment, so I can not get a sequential scan plan with generate_sequence...\nI do agree that if we change that Python function to C++, we might see some\nimprovement, but we need to profile it first (by reporting the ratio of\nruntime between the above two queries)\nCe\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138340266\n.\n. Awesome! Who is going to own those changes?\n\nOn Mon, Sep 7, 2015 at 2:40 PM zhangce notifications@github.com wrote:\n\n@alldefector https://github.com/alldefector Your recommendation makes\nperfect sense for one fast_seqassign call (there are two such calls that\nare expensive)\n1. There is one fast_seqassign call for variables loaded from the output of extractors.\n2. There is another fast_seqassign call for factors output by SQL grounding queries.\nFor 1, we could just have a really fast shell script to assign IDs in TSV\nbefore loading. (just like your global sequence ID service idea)\nFor 2, we don't really need ID for factors, so that part can probably be\neliminated completely.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138382390\n.\n. Looks like we may want to revisit the design of the internal tables. For\nexample,\nhttps://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/inference/InferenceNamespace.scala#L27-L28\n\nWe have these two prefixes: \"dd_query_${tableName}\"\nand \"dd_factors_${tableName}\".\n\"dd_query_\" is presumably for variables, but it's actually used for\nfactors. Variable tables are created by users (i.e., no prefix).\n\"dd_factors_\" is unused anywhere.\nI suppose at some point somebody planned to copy user-created tables (e.g.,\ngenes) to internal variable tables (e.g., dd_query_genes), and if that's\nthe case, we could piggyback ID assignment on the copy. Not sure if any\nprevious version of DD ever did such copying.\nIf there is a strong reason to not have internal variable tables, why don't\nwe just add an auto-increment (with shared sequence) ID column to each\nuser-created/defined variable table? (I'd ditch attempted support for\nnon-pg family DBs, namely yourSQL.)\nWould the shared sequence be a bottleneck for parallel DBs?\nOn Tue, Sep 8, 2015 at 2:23 PM Raphael Hoffmann notifications@github.com\nwrote:\n\nI ran VACUUM last night and then restarted the job. The time for sequence\nassignment on \"rates\" went down from 3h to 2:18h. I haven't checked the\ntime for sequence assignment on the feature table yet.\nSo, it's a small improvement, but still sequence assignment is a major\nbottleneck.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138706567\n.\n. Based on my experience with Java, NoClassDefFoundError might occur when a\nthread is cleaned up (some sort of race condition between the user code and\nJVM's thread manager I think). The DD log file is still printing every\nminute:\n\n21:05:53.772 [default-dispatcher-3][taskManager][TaskManager] INFO  Memory\nusage: 107/2101MB (max: 27305MB)\n21:06:53.772 [default-dispatcher-3][taskManager][TaskManager] INFO  Memory\nusage: 107/2101MB (max: 27305MB)\n21:07:53.772 [default-dispatcher-3][taskManager][TaskManager] INFO  Memory\nusage: 107/2101MB (max: 27305MB)\n21:08:53.772 [default-dispatcher-3][taskManager][TaskManager] INFO  Memory\nusage: 108/2101MB (max: 27305MB)\n...\nBut looks like it's just akka puking -- I don't see tobinary.py or\nformat_converter running.\nDoes the converter or the python script print out logs that are not\nredirected by Java? Those logs would be useful. Maybe generate the input\nfile and halt DD and then run the converter manually so we can see the logs.\nOn Sun, Sep 6, 2015 at 8:10 PM Raphael Hoffmann notifications@github.com\nwrote:\n\nSee log below. There are two issues here:\n1. I am getting a java.lang.NoClassDefFoundError when running\n   format_converter.\n2. The error is only reported on the console, but not in the error log\n   file. After that, the system goes into an idle state, where to_binary.py is\n   no longer running; however the DD process is, and there seems to be no\n   activity. Unfortunately, the relevant files eg. dd_variables_rates are\n   deleted when this error occurs making it time-consuming to re-run.\nI've reproduced this error twice.\n17:30:11 [PostgresInferenceRunnerComponent$PostgresInferenceRunner(akka://deepdive)] INFO  Converting grounding file format...\n17:30:11 [PostgresInferenceRunnerComponent$PostgresInferenceRunner(akka://deepdive)] DEBUG Executing: python /lfs/raiders4/1/raphaelh/dd/new/deepdive/util/tobinary.py /lfs/raide\nrs4/1/raphaelh/dd/new/deepdive/out /lfs/raiders4/1/raphaelh/dd/new/deepdive/util/format_converter /lfs/raiders4/1/raphaelh/dd/new/deepdive/out\n17:30:53 [taskManager] INFO  Memory usage: 721/2082MB (max: 27305MB)\n17:31:53 [taskManager] INFO  Memory usage: 721/2082MB (max: 27305MB)\nException: java.lang.NoClassDefFoundError thrown from the UncaughtExceptionHandler in thread \"Thread-15\"\nException: java.lang.NoClassDefFoundError thrown from the UncaughtExceptionHandler in thread \"Thread-14\"\n17:32:53 [taskManager] INFO  Memory usage: 721/2082MB (max: 27305MB)\n17:33:53 [taskManager] INFO  Memory usage: 721/2082MB (max: 27305MB)\n17:34:53 [taskManager] INFO  Memory usage: 722/2082MB (max: 27305MB)\n17:35:53 [taskManager] INFO  Memory usage: 722/2082MB (max: 27305MB)\n...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/381.\n. @zhangce is working on the next-gen petabyte-scale sampler that will do without the object boxing and converter!\n. I wasn't able to find who first introduced DISTINCT with git blame because\nof some refactoring. Maybe the initial author just has to speak up about\nhis thought process... Or someone with better git skills could track it\ndown...\n\nOn Sun, Sep 13, 2015 at 10:24 AM chrismre notifications@github.com wrote:\n\nThis makes me really nervous :)\nOn Sun, Sep 13, 2015 at 9:39 AM Jaeho Shin notifications@github.com\nwrote:\n\nAny anticipated side effects if we drop DISTINCT? I think there must have\nbeen a good reason or use case why we added it in the first place.\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/pull/382#issuecomment-139893472>\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/382#issuecomment-139897221\n.\n. The UPDATE query for assignment would effectively copy all tuples being updated. I believe that's what @raphaelhoffmann meant... Looks like var ID assignment was thrown out a couple of months ago, so this should have been addressed...\n. OK, looks like it's specific to plpy extractors:\nhttps://github.com/HazyResearch/deepdive/blob/e8b9dc329102c53bb273455953e8ac67a62e579c/src/main/scala/org/deepdive/extraction/ExtractorRunner.scala#L553\nhttps://github.com/HazyResearch/deepdive/blob/master/util/ddext_input_sql_translator.py#L103\n\nIf we avoid using plpy extractors, looks like DDLog works on PGXL.\nTime to deprecate plpy extractors?\n. Yes, it appears plpythonu does require super user: http://stackoverflow.com/questions/2848704/postgresql-running-python-stored-procedures-as-a-normal-user\n. Love Ce's idea! Schema-level separation between user data and internal data also came up when we were designing a live-ingestion-and-processing system (which is of course incremental).\nA stop gap might be some syntax like table1 = f(...) (as opposed to table1 += f(...)), which would clear the table before populating it.\n. @netj what do you think of @zhangce 's proposal above? We could specify the search_path (say dd_run_0003, public) at the connection level or in the DB connection string.\nEncapsulating all the internal tables away from user data will be a great move. Benefits are manyfold:\n- cleanness: easy to see what's input, what's output, and what's intermediate\n- lower risk of losing user data\n- more friendly for people developing DD in a scientific / iterative manner (preserve runs)\n- more friendly for people developing multiple extractors against the same input (think 200GB sentences table in GP and five people writing / running extractors against it)\n. @astrung that might be a symptom of https://github.com/HazyResearch/deepdive/issues/416\nNamely that 75422 - 70969 tuples were \"supervised\".\n. Closing this following #416\n. Hi @feiranwang , did you mean we can specify --sample_evidence in app.ddlog or the deepdive run command line? I wasn't able to find out how...\nIn light of people's confusion regarding this design choice, what do you guys think if we turn the flag on by default?\n. Looks like there are two issues:\n1. Do we run inference on supervised data points or not? How?\n2. Should we give the user an output table that contains all extractions regardless of whether they are supervised or predicted?\nI suppose everyone would agree that we say yes to question 2. And 2 is the source of lots of confusions; e.g., wondering why there is data loss, wondering why MindBender doesn't index all the data, and wondering why many DD programs make strange \"unsupervised copies\" of \"supervised\" tuples.\nA regular user of DD doesn't have a good understanding of \"evidence variables\" or how Gibbs sampling works. To them, the input to DD is some source data and some seed dictionaries or patterns; the output should be all extractions / predictions. It may well be the case that some of the \"predictions\" are (supervision) rule-based, they are still predictions nevertheless. With the upcoming \"data programming\" framework, I've heard that those rules would start to have weights and become not that black and white -- by then, I suppose it'll become natural to have a probability for \"supervised\" predictions as well.\n. @netj Looks like this has been resolved by the above PR in 0.8. Feel free to reopen if that's not the case.\n. Related: #424\nSupport for result pipelining would be great. Looks like a key challenge would be to cleanly model the \"result\" (see #416).\n. OK, UDFs are language-agnostic and decoupled from DD core (only connection is command line string in ddlog and data pipes at runtime). @netj we should probably emphasize these points in the next version of docs. Feel free to close either now or when doc is updated.\n. Sounds like 1.0 instead of 0.8 :)\n. @zifeishan @netj are you guys suggesting that we write $sql to a temp file and use -f to run it (so that the whole block isn't run as one transaction)? Sounds like it should work. We only need to update PGXL's db-execute driver, right?\n. Apparently -f does not run the file in one transaction unless you also specify -1 or --single-transaction: http://postgres-xc.sourceforge.net/docs/1_0/app-psql.html\nThat's actually what we need because PGXL would throw up (Zifei's error messages) if we try to run a bunch of mutating / DDL statements in one transaction.\n. With the change, you are still updating the initvalue field; and it's not a key column.\nReal problem seems to be that PGXL doesn't support correlated UPDATE involving multiple nodes:\nhttp://sourceforge.net/p/postgres-xc/mailman/message/26727433/\nThere is a newer version of PGXC (basis of PGXL) that may have solved that limitation:\nhttps://github.com/postgres-x2/postgres-x2\nWe could try it at some point.\nAnyway, I heard that the scala code will go away right after the 0.8 release?\n. My vote would be to drop scala in 0.8.0 because doing it in 0.8.1 may come as a bigger surprise since it's a minor release (or we'd have to unnecessarily wait till 0.9.0). Cannot wait to see Akka die!\n. @netj should this go to 0.8?\n. Got it. Thanks!\n. Great idea. @netj it may be as simple as adding something like abs(weight) > 0.001 to the grounding queries (likely also a join with the weight value table)?\n. Let's call these \"flexible-arity factors\". It should not be hard to add support for them. However, we need to see more of compelling use cases, and decide the degree of customizability we are going to provide:\n- boolean vars with a fixed indicator function (say AND over an array of bool vars) is easy in ddlog (though we still need to figure out the syntax since var IDs are not part of user schema now)\n- categorical vars are trickier because we'd need array of  pairs\n- we've thought about supporting user-defined factor functions (LLVM?); not clear if we should attack that directly or try to support (in ddlog and sampler) a more limited form of customization such as flexible-arity factors\n. Var IDs are now no longer exposed to the user schema space.\n. I'd prefer DD to auto compile by default since that's the most common use case.\n. The OneIsTrue factor type doesn't seem to be implemented as a constraint, so categorical variable seems like the way to go. For grounding, I'd do a GROUP BY @key to get an array of tuple IDs for each group; and each such array would be a categorical variable in the sampler.\n. Very good point! Yes, looks like we do have to keep the identities of the underlying boolean variables in those groups so that they can correlate with other variables. I don't know how categorical variables are currently implemented in the sampler. For things to work, the underlying boolean variables must be referenceable in the factors.\nConceptually it'd be cleaner to say all variables are boolean and it's possible enforce one-is-true constraints among a group of variables. As for implementation, I don't know if it's easier for the Variable class or the Factor class to handle such constraints...\n. Yes, it is the one-is-true constraint. However, the sampler doesn't seem to support this constraint currently. There is a factor type by the same name, but it's not a constraint.\nAlso, there is no front-end support in DeepDive / DDLog to take advantage of it once we do have such support in the sampler.\n. That'd be great! FWIW, these ancient systems probably had such blocking from the get-go:\nhttp://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/Gibbs%20sampling.aspx\nhttps://alchemy.cs.washington.edu/tutorial/tutorial.pdf (page 4)\nhttp://i.stanford.edu/hazy/tuffy/doc/tuffy-manual.pdf (page 8)\n. OK, we do have categorical vars now.\n. @SenWu Any update?\nWe have an app with 100K docs, 13M vars, 42M weights, 248M edges, and 242M factors. The machine has 2 NUMA nodes. DD was stuck at CREATE FG ON NODE ...1 for over an hour with the sampler using 70GB of RAM and the machine apparently thrashing...\n2016-02-23 17:58:50.972139 ################################################\n2016-02-23 17:58:50.972150 # nvar               : 13447656\n2016-02-23 17:58:50.972162 # nfac               : 242889469\n2016-02-23 17:58:50.972175 # nweight            : 42308790\n2016-02-23 17:58:50.972187 # nedge              : 248001424\n2016-02-23 17:58:50.972200 ################################################\n2016-02-23 18:02:27.024966 LOADED VARIABLES: #13447656\n2016-02-23 18:02:27.025074          N_QUERY: #11900206\n2016-02-23 18:02:27.025096          N_EVID : #1547450\n2016-02-23 18:02:47.727552 LOADED WEIGHTS: #42308790\n2016-02-23 18:05:38.553535 LOADED FACTORS: #242889469\n2016-02-23 18:09:43.023361 CREATE FG ON NODE ...1\n. Yes, the sampler uses 40GB of memory with -c 1 and works for now. But we have at least 50X as many docs. So partitioning is critical.\n. @raphaelhoffmann mentioned that he saw this problem on Linux as well...\n. OK, we tried adding ORDER BY id (and ORDER BY id, categories) but still get the same error.\nAnd this seems to be a bigger issue: \nFrom GP:\nselect * from dd_weightsmulti_inf_istrue_tag order by id, categories limit 20;\n dd_weight_column_0 | isfixed | initvalue | id  | categories \n--------------------+---------+-----------+-----+------------\n word=Misa          | f       |         0 | 169 | 0\n word=Leahy         | f       |         0 | 170 | 0\n word=Misa          | f       |         0 | 170 | 1\n word=entries       | f       |         0 | 171 | 0\n word=Leahy         | f       |         0 | 171 | 1\n word=Misa          | f       |         0 | 171 | 2\n word=sponsorship   | f       |         0 | 172 | 0\n word=entries       | f       |         0 | 172 | 1\n word=Leahy         | f       |         0 | 172 | 2\n word=Misa          | f       |         0 | 172 | 3\n word=A.            | f       |         0 | 173 | 0\n word=sponsorship   | f       |         0 | 173 | 1\n word=entries       | f       |         0 | 173 | 2\n word=Leahy         | f       |         0 | 173 | 3\n word=Misa          | f       |         0 | 173 | 4\n word=sold          | f       |         0 | 174 | 0\n word=A.            | f       |         0 | 174 | 1\n word=sponsorship   | f       |         0 | 174 | 2\n word=entries       | f       |         0 | 174 | 3\n word=Leahy         | f       |         0 | 174 | 4\n(20 rows)\nFrom PG:\n```\nselect * from dd_weightsmulti_inf_istrue_tag order by id, categories limit 20;\ndd_weight_column_0  | isfixed | initvalue | id  | categories \n---------------------+---------+-----------+-----+------------\n word=Cardiovascular | f       |         0 | 169 | 0\n word=Cardiovascular | f       |         0 | 170 | 1\n word=Cardiovascular | f       |         0 | 171 | 2\n word=Cardiovascular | f       |         0 | 172 | 3\n word=Cardiovascular | f       |         0 | 173 | 4\n word=Cardiovascular | f       |         0 | 174 | 5\n word=Cardiovascular | f       |         0 | 175 | 6\n word=Cardiovascular | f       |         0 | 176 | 7\n word=Cardiovascular | f       |         0 | 177 | 8\n word=Cardiovascular | f       |         0 | 178 | 9\n word=Cardiovascular | f       |         0 | 179 | 10\n word=Cardiovascular | f       |         0 | 180 | 11\n word=Cardiovascular | f       |         0 | 181 | 12\n word=Scotia         | f       |         0 | 182 | 0\n word=Scotia         | f       |         0 | 183 | 1\n word=Scotia         | f       |         0 | 184 | 2\n word=Scotia         | f       |         0 | 185 | 3\n word=Scotia         | f       |         0 | 186 | 4\n word=Scotia         | f       |         0 | 187 | 5\n word=Scotia         | f       |         0 | 188 | 6\n(20 rows)\n```\nIt looks like that something is wrong with the weight ID assignment in GP. Not sure if that's related to this console output:\n2016-02-21 01:15:28.272272 + deepdive db assign_sequential_id dd_weights_inf_istrue_tag id 169 13\n2016-02-21 01:15:28.318028 NOTICE:  language \"plpgsql\" already exists, skipping\n2016-02-21 01:15:28.318075 CREATE LANGUAGE\n2016-02-21 01:15:28.332492 ERROR:  language \"plpythonu\" already exists\n2016-02-21 01:15:28.461849 NOTICE:  table \"tmp_gpsid_count\" does not exist, skipping\n2016-02-21 01:15:28.461964 CONTEXT:  SQL statement \"drop table if exists tmp_gpsid_count cascade;\"\n2016-02-21 01:15:28.461980 PL/pgSQL function \"fast_seqassign\" line 2 at execute statement\n2016-02-21 01:15:28.461997 NOTICE:  table \"tmp_gpsid_count_noagg\" does not exist, skipping\n2016-02-21 01:15:28.462011 CONTEXT:  SQL statement \"drop table if exists tmp_gpsid_count_noagg cascade;\"\n2016-02-21 01:15:28.462023 PL/pgSQL function \"fast_seqassign\" line 3 at execute statement\n2016-02-21 01:15:29.023230 NOTICE:  EXECUTING _fast_seqassign()...\n2016-02-21 01:15:30.125127 ERROR:  column \"cname\" does not exist\n2016-02-21 01:15:30.125280 LINE 1: SELECT  ('update ' ||  $1  || ' set ' || cname || ' = update...\n2016-02-21 01:15:30.125355                                                  ^\n2016-02-21 01:15:30.125386 QUERY:  SELECT  ('update ' ||  $1  || ' set ' || cname || ' = updateid(' ||  $2  || ', gp_segment_id, ARRAY[' ||  $3  || '], ARRAY[' ||  $4  || '], ARRAY[' ||  $5  || ']);')::text is not null\n2016-02-21 01:15:30.125426 CONTEXT:  PL/pgSQL function \"_fast_seqassign\" line 12 at if\n2016-02-21 01:15:30.125477 SQL statement \"select * from _fast_seqassign('dd_weights_inf_istrue_tag', 169);\"\n2016-02-21 01:15:30.125551 PL/pgSQL function \"fast_seqassign\" line 8 at execute statement\n2016-02-21 01:15:30.204862 NOTICE:  sequence \"dd_seq_dd_weights_inf_istrue_tag_id\" does not exist, skipping\n. If we replace the fast_seqassign with the regular PG's sequence assignment (even without the ORDER BY id hack for dump_weights), everything works out (example finishes successfully): the id column values are unique now, though the order of the result from select * from dd_weightsmulti_inf_istrue_tag limit 3; is still nondeterministic -- maybe somehow the order doesn't matter for sampler loading after all??? But that defies the code\nfor (long i = 0; i < s; i++) {\n    assert(this->weights[i].id == i);\n  }\n. Looks like this is the bug: the db-assign_sequential_id script for GP didn't keep up with the inc parameter (step size between consecutive IDs) that was presumably introduced for categorical variables:\nhttps://github.com/HazyResearch/deepdive/blob/6786a806ec04b087db6b29fbdd1f5c469a6e6601/compiler/compile-config/compile-config-2.01-grounding#L385-L396\nhttps://github.com/HazyResearch/deepdive/blob/13e8788b9bf39e2d3aa83d0883a6391c9c9d6592/database/db-driver/postgresql/db-assign_sequential_id\nhttps://github.com/HazyResearch/deepdive/blob/13e8788b9bf39e2d3aa83d0883a6391c9c9d6592/database/db-driver/greenplum/db-assign_sequential_id\n. Two side notes:\n1. Right now categorical variables have a fixed size and grounding is via a cross join between the \"pseudo weight table\" and range(variable_size). How hard is it to support categorical vars whose size is dependent on the data (e.g., a mention can be linked to any number of candidate entities)? This is essentially https://github.com/HazyResearch/deepdive/issues/471 @SenWu @netj \n2. The title question is still a puzzle for me: Weight dumping is not sorted by id, but sampler expects so -- yet DD works fine even when GP returns non-deterministically ordered data. @feiranwang @zhangce\n. @feiranwang That explains it! I'm renaming this issue now...\n. We also need to add the inc parameter (as in PG's version this script).\n. LGTM!\n. You are probably running Java 7. CoreNLP requires Java 8.\n@netj We've encountered the same problem a couple of days ago. We should probably add some Java version validation code in the example code.\n. The \"do / redo / done\" workflow management is great, but I haven't got the hang of it...\nRelated issues include https://github.com/HazyResearch/deepdive/issues/563 and https://github.com/HazyResearch/deepdive/issues/502 and https://github.com/HazyResearch/deepdive/issues/554\n- Even for a simple program, the workflow graph can have many details already, with all steps implicitly derived from ddlog (including built in ones like init/app and those based on table and rule names). Can we simplify and explicify the workflow? For example, maybe we could use the higher granularity of AJR's \"labeled groups of operations\"?\n- The automagic dependency resolution is great, but for most programs in practice, the lexical order (sequences of rules or groups) probably suffices? If there are interleaved fancy subgraphs that allow for efficiency savings in incremental runs, they probably should have been broken down into smaller modules in the first place?\n- The automagic execution state (and mark done/todo) management is great, but it also requires the user to have a full understanding of the underlying workflow and keep a mental model in sync. The source code versioning issue here makes it even more complicated. Maybe let's consider a (separate) stateless execution model?\n. @netj: we are still living with this heisenbug... Does it look like it could be mkmimo?\n. The \"equal pair\" or skip-chain factors are certainly still supported: http://deepdive.stanford.edu/example-chunking#3-statistical-learning-and-inference\nI guess \"filter based on the categorical value\" is not supported yet? @netj \n. @raphaelhoff here is the link again: http://deepdive.stanford.edu/example-chunking#3-statistical-learning-and-inference\n\"equal pair\" means equal words, not equal labels. Equal labels would be learned by DD.\nWe can discuss quality improvement offline. For performance, it seems to be closely related to the dense / sparse FG representation @netj is working on for dynamic-scope categorical vars. As DD evolves, it's possible that we'll have more and more sophistication in graph representation; e.g., maybe some notion of lazy grounding.\n. @syadlowsky Those are great points. The syntax as is can be quite confusing and some cleanup is in order... and yes, better documentation. For now, you can see the distinctions in this section: http://deepdive.stanford.edu/example-spouse#2-distant-supervision-with-data-and-rules\nIn short:\n- = is for supervision (assigning TRUE / FALSE / NULL to a tuple), not data flow; e.g. is_spouse(p1, p2) = TRUE :- golden_marriage(p1, p2)\n- += populates a relation with a UDF; e.g., is_spouse_cands += extract_cands(sid, text) :- sentences(sid, text)\n- :- delimits the rule's head from body, as in the above examples. Besides the above complex heads, the rule can also be simply is_spouse_cands(p1, p2) :- all_person_pairs(p1, p2) which is just a (optionally materialized) view.\nAll of those cases are also mentioned in the above link.\n. @RominYue What version of DD were you running (deepdive version)? Could you try the same on the latest version (git pull; git submodule update --init; make build; make install) and see if the error persists?\n. LGTM\n. Sorry for being curt in the first post... Since the variable and factor arrays are critical for the sampler's performance, we have to do the ID sequentialization at some point. I like the two-phase idea, and think that it can probably indeed simplify things: the fingerprinting would simplify the SQL joins (no more ID mapping tables shadowing every base table), and the second-phase mapping might actually be easily doable by the sampler's loading phase (by keeping a few fingerprint->ID lookup tables in memory).\n. Tabling this for now that I've become the 2nd person who can work with the jq code...\n. buildkit moreutils_0.58 dependency is blocking travis...\n. The biased coin example? The deepdive.conf code stopped working because of the var table change...\nWill address the comments soon!\n. Travis seems to always fail because the chunking example times out (10min):\nNo output has been received in the last 10 minutes, this potentially indicates a stalled build or something wrong with the build itself.\n. @netj PTAL. Also, is it possible to increase the travis timeout limit? (Could also try reducing number of docs in chunking test.)\n. Yeah, those joins from reuse_weight stage of the spouse example never finish on my macOS either... I'm trying reducing the chunking numbers.\n. In the spouse test, the \"postgresql spouse example reuse weights\" step, there are only 3688 variables, but the big join in https://github.com/HazyResearch/deepdive/issues/541 could not finish even after 2HR... Maybe we should try to change the key from two columns to one.\n. @shahin Related: The missing 'T' issue is a bug in postgres's to_json that was fixed only in 9.5...\nhttp://postgresql.nabble.com/BUG-12578-row-to-json-and-to-json-add-T-in-timestamp-field-td5834396.html\n. LGTM\n. I assume you meant dd_graph_weights.categories? Removing now.\n. LGTM!\n. Let's just port this to python3.\n. see also https://github.com/HazyResearch/deepdive/pull/549\n. @netj bundling GP and PG clients are terrific ideas. They are causing lots of pain right now.\n. @netj: @shahin has a prototype export_to_json routine using psycopg2 that's only 1.5-3x as slow compared to COPY to TSV. Maybe let's just bundle psycopg2 and call it a day here?\n. @netj Great! @shahin can push the script in the next K days (perhaps in a separate PR).\nIf we could remove the dependency on \"psql\" (and others like \"createdb\"), that'd be fabulous! That would also make TSJ implementation much easier, I suppose.\n. We use TSV very extensively. I have seen folks patching the symptoms of issues like this in many places. This is a fundamental issue for data in motion (and at rest) in general. We need to nail down a robust data format and stick with it consistently everywhere. Just \"TSV\" is not specific enough as a standard.\n. So looks like PG COPY by default escapes chars with backslash, and deepdive sql internally wraps the query with COPY. When the original string has a single backslash, a user probably won't expect the output of deepdive sql to contain two backslashes.\n1. Maybe we want to override the default escaping behavior of COPY with things like ESCAPE WITH E'\\b' so that what the user sees with psql is also what she gets with deepdive sql?\n2. We may also want to change other DeepDive operators (e.g., load) similarly to be symmetric.\n3. The SerDe behavior needs to be well-known to users so that other systems know how to consume / produce data.\n. As far as universal support goes, JSON encoding does seem to be the best choice and TSJ seems like a great tradeoff between simplicity, usability, and performance. I think we can probably commit to TSJ as the standard human-readable data format if we can do it without adding much overhead in terms of both human efficiency and machine efficiency.\nSpecifically, we need to decide where and how to plug in the SerDe implementation. For example, if it's only in DeepDive, people won't be able to load and unload data in desired format by directly interacting with the DB. So either we let DeepDive monopolize data load/unload, or we need to implement/distribute the SerDe inside the DB server or alongside DB clients (namely wherever people call psql). Or maybe we can just make it clear that ad hoc load/unload is outside the TSJ sphere and extra care is required for ad hoc data to join the sphere.\nAnother concern is speed. For example, can we make TSJ loading at most 10% slower than TSV loading? Would json.loads calls in Python slow down a dummy UDF's throughput by 10X?\nJSON string quoting seems simple enough: http://stackoverflow.com/a/27516892\nGiven that, would we like to roll our own specialized faster parser/serializer in case the standard JSON routines are slow?\n. dup of #494... It always comes back...\n. This removes dropdb:\nhttps://github.com/HazyResearch/deepdive/pull/592/commits/90a371bb423c2b5c68bd0a6015c57da60e15c2a4\n. @netj The naming is optimized for the future, i.e., when we manage to implement the RATIO semantics, it's also the time we achieve balance :)\n. LGTM!\n. @netj PTAL\n. LGTM!\n. The only place where variable.is_observation is used seems to be only at the beginning of GibbsSamplerThread::sample_sgd_single_variable: if (variable.is_observation) return; Not sure if there is any code for \"outputting different values based on the value of those observed variables\"... (perhaps quasi-hard rules in ddlog?)\nRegardless, looks like this bug has been there since Dec 2015 (when netj revamped grounding with jq) -- and nobody has noticed! Assuming that is_evidence works for the intended use case, perhaps let's just purge is_observation?\n. OK, to minimize effort, I'm landing this for now. Next time we make any related changes / refactoring, we can do the merciless purge...\n. OK, that's indeed related. Here is what I tried:\nrm -f run/model/grounding/variable/link/domains.part*\nDEEPDIVE_NUM_PROCESSES=XXX deepdive redo process/grounding/variable/link/dump_domains process/grounding/combine_factorgraph process/model/learning\nI tried different values of XXX. If XXX is 1 or 20, var count from load_domains is correct. If XXX is 28 or above, var count from load_domains becomes smaller than expected.\nIf I just run\nDEEPDIVE_NUM_PROCESSES=30 deepdive redo process/grounding/combine_factorgraph process/model/learning\nthe var count is always correct.\nSo, something seems fishy with the concurrency control of deepdive compute execute or perhaps file flushing in sampler-dw text2bin domain /dev/stdin >(pbzip2 >domains.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2), or the symlink creation in combine_factorgraph... (In other words, load_domains tried to read the part files before they are ready.)\n. In https://github.com/HazyResearch/deepdive/blob/master/runner/compute-driver/local/compute-execute#L106 , maybe that detects only the completion of sampler-dw text2bin even if pbzip2 continues to run?\n. If I add sleep 2 at the end of process/grounding/variable/link/dump_domains/run.sh, the problem is fixed. I also saw that the mtime of some run/model/grounding/variable/link/domains* files is GREATER than when flatten factorgraph/domains was executed. Those kind of confirm that deepdive compute execute didn't wait for the dump processes to fully finish.\n. This post suggests that there is no obvious way to wait for the redirection subprocess (pbzip2):\nhttp://unix.stackexchange.com/questions/65192/how-to-wait-for-a-subprocess-used-for-i-o-redirection\nIf we replace\nsampler-dw text2bin domain /dev/stdin >(pbzip2 >domains.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2)\nwith\nsampler-dw text2bin domain /dev/stdin /dev/stdout | pbzip2 >domains.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2\nThe problem seems to go away (correct var count).\nLesson is that we should avoid using redirection subprocesses in compute execute commands.\n@netj any potential issue you see with this change?\n. Great. What's the best way to deal with the existing pipes, such as https://github.com/HazyResearch/deepdive/blob/master/compiler/compile-config/compile-config-2.01-grounding#L598-L604 ?\n. Just got bitten by this again in the sharding branch... Worked around (https://github.com/HazyResearch/deepdive/pull/592/commits/0a215b9f7c1b73614f2d57c4ca02552d86278149), but still don't have a good solution for cases where both redirection and tee are used.\n. LGTM!\n. @shahin could you merge or rebase master?\n. @raphaelhoff I suppose git submodule update --init fixed them all?\n. @vpsm Sorry about the errors. I was able to reproduce that in a clean repo. If you run mkdir .build first before make depends, it seems to work. Let me know if it doesn't. We'll fix the Makefile soon. (cc @netj )\n. I have never seen this error before and tests run fine. It sounds like a versioning issue with jq or bash in a bad build (like you said, \"stale code\").\n@raphaelhoff  Could you print the relevant lines from \"run.sh\"? Maybe also edit it to print out versions of jq and bash.\n. same problem happens when building from a clean repo.\n. @minafarid I cannot seem to repro this... Could you run deepdive version to check the build date of deepdive and if it's more than a couple of months old, run git pull; make build; make install to rebuild? Let me know if the problem persists even after that.\n. Yeah, we've seen this error a couple of times... Try running git submodule update --init after git pull.\n. If you ran deepdive do all, there should be a view called dd_inference_result_weights_mapping.\n. @minafarid deepdive run is deprecated... https://github.com/HazyResearch/deepdive/issues/563\n. Hi @vpsm , we've merged the categorical support into master a couple of months ago. The documentation on the DeepDive website is still stale though as we haven't made a new release yet...\nIn any case, the chunking example is actually categorical. In the current version of DD, here is how you declare a categorical variable:\nhttps://github.com/HazyResearch/deepdive/blob/master/examples/chunking/app.ddlog#L1-L5\nThe @key annotation marks the \"possible-world\" key, and fields without @key are categorical values (i.e., classes). Note that the @key columns are NOT a primary key for the chunk table in the DB sense.\nAll those inference rules are categorical:\nhttps://github.com/HazyResearch/deepdive/blob/master/examples/chunking/app.ddlog#L39-L88\nNote that we have got rid of the old awkward Multinomial(...) syntax. Right now, we only support the AND factor function over categorical vars (chunk(s, w1, tag1) ^ chunk(s, w2, tag2) but not say chunk(s, w1, tag1) => chunk(s, w2, tag2)), and don't support mixing categorical vars with boolean vars in the inference rule yet. However, we are making changes (e.g., https://github.com/HazyResearch/deepdive/pull/589) to move toward supporting them.\nIf you'd like to play with the chunking example on a laptop, you may want to run these first:\nexport SUBSAMPLE_NUM_WORDS_TRAIN=2000\nexport SUBSAMPLE_NUM_WORDS_TEST=2000\nThe bigram and word-embedding factors are particularly dense (7M factors for 4K vars) to demonstrate the ddlog language, and the above commands would scope down the input data to make sure DD doesn't take forever... (we'll make this example snappier by default at some point.)\n. @vpsm yes, exactly! In the first case, you can leave out both @keys -- if no field has @key, implicitly all fields form the key and it's a boolean var.\n. Hi @vpsm closing this thread for now. If you encounter other issues (compiling or otherwise), feel free to open a new one :)\n. Right, once we have sampler scale-out, the big joins and disk IO can be the bottleneck. It's actually easy to do per-partition on-demand grounding with the current code and never have the factors hit the disk. To make that happen, there are two things we need to change:\n1. Right now, for each inference rule, we materialize the factor table F that's essentially the rule's conjunctive query joined with the variable ID assignment tables. F is then used in two ways: 1) to generate distinct weight parameters for ID assignment; 2) to generate DW binary format factors for the sampler (joining F with weight ID assignment and category ID assignment). Having these two use cases was the reason that we wanted to materialize F. But in hindsight, it probably makes more sense to have F as a view for the first use case... For the second use case, a view can probably save a lot of IO as well... @netj thoughts?\n2. The current DD execution mode is a self-contained workflow. Almost by definition of \"on demand\", we need to turn the shell scripts driven learning / inference process into a cleaner model-control architecture where data exchange, scheduling, and callbacks can happen between the DB and sampler services.\n. @chrisdesa let me know if this change is in harmony with your partitioning algo.\n. Yes, 16 bits for shard ID, 48 bits for compact var ID.\nRe F materialization, the issue is that when you have a rule like @weight(type) Honest(p) :- PeopleLocation(p, city), CityType(city, type) where PeopleLocation is a huge table and CityType is a small table, to find distinct weights from the materialized table, we'd scan a huge table. But if F is a view, the DB might be smart enough to ignore the join and get them from the small table directly. Of course, there are more complicated weight params that would require the join -- in which case we'd evaluate the join twice and materialization may be more efficient. Maybe we could use EXPLAIN to decide?\nRe sampler interface, numbskull is actually currently using mmap for data loading:\nhttps://github.com/HazyResearch/numbskull/blob/master/numbskull/numbskull.py#L257\n. Tested on real-world work loads, and all work as expected (linearly reduced memory, linearly faster epoch speed, similar weights, probs, and quality numbers).\nPiggybacked a bunch of nasty bug fixes (destroying database, data unloading race condition due to text2bin >(pbzip2 ...)).\nChatted with @thodrek briefly yesterday, who's going to make it really fly on multiple hosts. @thodrek changing this line allows you to avoid materializing the factors: https://github.com/HazyResearch/deepdive/blob/30aeaafc41aaec856bb49e35cbe0fba319a65fef/compiler/compile-config/compile-config-2.01-grounding#L378\n. @thodrek Can I land this so you can build other fancy things on top?\n. @netj looks like dockerhub access is flaky:\n$ DOCKER_IMAGE_TO_PUSH=$DOCKER_IMAGE_TEST_PREFIX; if [[ $TRAVIS_TEST_RESULT = 0 ]]; then DOCKER_IMAGE_TO_PUSH+=PASS; else DOCKER_IMAGE_TO_PUSH+=FAIL; fi;\nafter_script.2\n0.01s$ if [[ $TRAVIS_PULL_REQUEST = false ]]; then tag=travis.$TRAVIS_BRANCH; else tag=travis-pr$TRAVIS_PULL_REQUEST; fi; tag=$(echo -n \"$tag\" | tr -c 'A-Za-z0-9_.-' _);\n$ docker login -e \"$encrypted_DOCKER_EMAIL\" -u \"$encrypted_DOCKER_USERNAME\" -p \"$encrypted_DOCKER_PASSWORD\";\nFlag --email has been deprecated, will be removed in 1.13.\nLogin with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: \nNo output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.\n. Awesome!\n. @rudaoshi the person_has_canser.id column indicates that it was created by an old version (say v0.8) of DeepDive, and that schema is no longer compatible with the examples in the latest git repo. We are about to release the next version, but until then, you could build from git by running make build; make install. Then running the examples should work. FYI, with the latest build, here is the schema:\nTable \"public.person_has_cancer\"\n    Column     |       Type       | Modifiers \n---------------+------------------+-----------\n person_id     | bigint           | \n dd_label      | boolean          | \n dd_truthiness | double precision |. @rudaoshi Here is an example rule with manually assigned weights (adapted from the \"census\" example):\n@weight(1.2)\nrich(id) :- adult(id, _, workclass, _, _, _, _, _, _, _, _, _, _, _, _, income_bracket).\nYou can change the rule body (the adult... part) to encode the \"common sense\" cases.\n. @rudaoshi note that there are two colons: NULL::boolean. If that doesn't work, try cast(null as boolean).. @rudaoshi Could you try building DD from git (see https://github.com/HazyResearch/deepdive/issues/607#issuecomment-268308601) and rerunning this program? If the compilation problem persists, could you post the error messages? Thanks!. @paidi that does seem like OOM... The log content suggests that you were probably running an old version of DeepDive (v0.8). In an upcoming release (coming soon), we'll introduce a factor graph partitioning feature, where a partitionable factor graph is processed piece by piece in memory. We haven't written up the documentation for this yet, but if you want to try it now using master (git clone and then make build; make install), the \"census\" example has turned on this feature:\nhttps://github.com/HazyResearch/deepdive/blob/master/examples/census/app.ddlog#L23\nhttps://github.com/HazyResearch/deepdive/blob/master/examples/census/deepdive.conf#L3\nIn words, you can enable partitioning by changing two places in your DD program\n1. In app.ddlog, add @partition to the appropriate column (e.g., doc_id) of the variable relation.\n2. In deepdive.conf, add sampler.partitions: $NUM_PARTITIONS, where $NUM_PARTITIONS is the number of partitions.\nInternally, DeepDive partitions the graph based on the hash value of the column marked with @partition.. @dBlairMorton yes, the build process does require internet access as it fetches code and dependencies. Building on an open system and then copying the binaries should work.. Hi @melporto \n1. Yes, there is the dw gibbs subcommand that transforms the text format into binary format. You can find example commands here: https://github.com/HazyResearch/sampler/blob/master/test/text2bin.bats\nIn DeepDive, you'd run it like deepdive env sampler-dw text2bin --help\n\n\nThe binary data format is documented here: https://github.com/HazyResearch/sampler/blob/master/doc/binary_format.md#variables-binary\n\n\nThe metadata file is a simple text line with 4 integers: https://github.com/HazyResearch/sampler/blob/master/doc/binary_format.md#metadata-text\nInternally, DeepDive generates the file here: https://github.com/HazyResearch/deepdive/blob/master/compiler/compile-config/compile-config-2.01-grounding#L697\nIf you'd like to manually edit the factor graph data, you could also generate the file manually.\nIf you are interested, you could also check out the intermediate data generated by DeepDive in the run/model/grounding/ and run/model/factorgraph directories. Note that only (compressed) binary files are there and the text format is never stored on disk.\n. Thanks for reporting this, @cognitronz . Yes, we've learned that this dummy file requirement is a bit cumbersome. We'll address it in the upcoming release (likely by removing this requirement).. Hi @EadrenKing , it could take 1 min or so to get started. If it took much longer than that, one possibility is that the available memory was low and the OS started paging. Perhaps check the OS stats to see if that's the case?. \n\n",
    "mikecafarella": "@zifeishan It looks like a lot has changed here, and that you are planning to make further changes.  I will do a full review when your new changes are prepared.\n. @chrismre I am reviewing now...\n. @adamwgoldberg It's worthwhile to have a new master pull on the merits.  Don't need to do it just for the flag.  I'll close this, and do a new request with a proper feature list, etc.\n. Feiran, I went over the PR.  Everything looks good except that File.createTmpFile() call.  Can you address this?\n. @zifeishan It looks like a lot has changed here, and that you are planning to make further changes.  I will do a full review when your new changes are prepared.\n. @chrismre I am reviewing now...\n. @adamwgoldberg It's worthwhile to have a new master pull on the merits.  Don't need to do it just for the flag.  I'll close this, and do a new request with a proper feature list, etc.\n. Feiran, I went over the PR.  Everything looks good except that File.createTmpFile() call.  Can you address this?\n. ",
    "samzhang111": "I figured it out. tutorial_example/step1-basic/run.sh exports DEEPDIVE_DIR with an extra ../. It is the right number when you run it from the examples directory, but too many if you copy it to app/spouse.\nThe tutorial made it seem like I should make $APP_HOME and $DEEPDIVE_HOME into global environment variables, so I was surprised that they were overwritten by relative paths.\n. Fixing #218 fixed this as well. Forgot to include the variable in schema.variables.\n. I figured it out. tutorial_example/step1-basic/run.sh exports DEEPDIVE_DIR with an extra ../. It is the right number when you run it from the examples directory, but too many if you copy it to app/spouse.\nThe tutorial made it seem like I should make $APP_HOME and $DEEPDIVE_HOME into global environment variables, so I was surprised that they were overwritten by relative paths.\n. Fixing #218 fixed this as well. Forgot to include the variable in schema.variables.\n. ",
    "adamwgoldberg": "@feiranwang @SenWu \n. I couldn't reproduce this after a couple of attempts, but that doesn't mean it's fixed necessarily. I recall the failure rate was pretty low (1 out of 10 or lower). I'm fine closing the issue if others aren't concerned\u2013if someone else sees it again we at least know the fix.\n. Approved in person by @mikecafarella :) \n. It looks like it failed on a fluke from a Scala library failing to download. The same commit a8b6f71 has a green build on Develop. \n@mikecafarella Is merging OK? \n. @mikecafarella Build is green after no changes. \n. We also might want to merge this into master after just in order to get a green build again. It never updated the icon even though we retested the green commit we discussed yesterday. \n. This is a known issue: #231\n. Thanks for looping me in.\nhttps://app.shippable.com/builds/54e5caa0b0802f100019d491 is an example of a working Shippable build. \nI made changes on the \"shippable-branch\" branch that I have not yet had a chance to merge, so the shippable.yml file on this branch is out of date. \nYou can safely ignore this as Shippable is not currently being used for performance testing, but longer-term this is fine as this is currently the expected behavior. \n. Right, the original reason was to have performance testing on known hardware. I think that's something we can table for now. I'll remove it now. \n. Turned it off. Opened #313 as well. Past pull requests may still show failures. \n. I disabled the project on shippable.com and removed the repo hooks. I\nsuppose you didn't have access to the shippable login. Whoops. :)\nOn Sun, May 31, 2015 at 12:19 AM Jaeho Shin notifications@github.com\nwrote:\n\nThanks. Did you just remove repo hooks from GitHub, or something else from\nShippable? It wasn't very clear to me how to turn it off.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/304#issuecomment-107136712\n.\n. @feiranwang @SenWu \n. I couldn't reproduce this after a couple of attempts, but that doesn't mean it's fixed necessarily. I recall the failure rate was pretty low (1 out of 10 or lower). I'm fine closing the issue if others aren't concerned\u2013if someone else sees it again we at least know the fix.\n. Approved in person by @mikecafarella :) \n. It looks like it failed on a fluke from a Scala library failing to download. The same commit a8b6f71 has a green build on Develop. \n\n@mikecafarella Is merging OK? \n. @mikecafarella Build is green after no changes. \n. We also might want to merge this into master after just in order to get a green build again. It never updated the icon even though we retested the green commit we discussed yesterday. \n. This is a known issue: #231\n. Thanks for looping me in.\nhttps://app.shippable.com/builds/54e5caa0b0802f100019d491 is an example of a working Shippable build. \nI made changes on the \"shippable-branch\" branch that I have not yet had a chance to merge, so the shippable.yml file on this branch is out of date. \nYou can safely ignore this as Shippable is not currently being used for performance testing, but longer-term this is fine as this is currently the expected behavior. \n. Right, the original reason was to have performance testing on known hardware. I think that's something we can table for now. I'll remove it now. \n. Turned it off. Opened #313 as well. Past pull requests may still show failures. \n. I disabled the project on shippable.com and removed the repo hooks. I\nsuppose you didn't have access to the shippable login. Whoops. :)\nOn Sun, May 31, 2015 at 12:19 AM Jaeho Shin notifications@github.com\nwrote:\n\nThanks. Did you just remove repo hooks from GitHub, or something else from\nShippable? It wasn't very clear to me how to turn it off.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/304#issuecomment-107136712\n.\n. \n",
    "stephenjbarr": "I am struggling getting Deep Dive working with docker. In particular, it is not clear how the Postgres container needs to be setup in order to work with Deep Drive. Would it be possible to supply the docker command to bring up the Postgres container and Docker container together. Or, even better, to show a Docker compose script showing how to launch this together.\n. Hi @netj I will try to spend some time on this next week. I am just getting started with DeepDive in general so I may have a bit of questions to ask along the way. I was able to instlal Deep Dive onto an EC2 image and that seemed to work okay, although the Markov Logic Network examples gave some $DBPORT error that I will need to track down.\nThanks! I'll be in touch.\n. I am struggling getting Deep Dive working with docker. In particular, it is not clear how the Postgres container needs to be setup in order to work with Deep Drive. Would it be possible to supply the docker command to bring up the Postgres container and Docker container together. Or, even better, to show a Docker compose script showing how to launch this together.\n. Hi @netj I will try to spend some time on this next week. I am just getting started with DeepDive in general so I may have a bit of questions to ask along the way. I was able to instlal Deep Dive onto an EC2 image and that seemed to work okay, although the Markov Logic Network examples gave some $DBPORT error that I will need to track down.\nThanks! I'll be in touch.\n. ",
    "raphaelhoffmann": "Fixed.\n. I noticed that this is only fixed for some cases. For example, if the schema in the plpy function doesn't match, or if the plpy UDF doesn't compile, the system will still report success and continue processing. Eg.\n12:56:10 [taskManager] INFO  Memory usage: 341/2476MB (max: 27159MB)\n12:56:20 [Helpers$(akka://deepdive)] INFO  ERROR:  function \"func_extract_isincalloutcall\" error fetching next item from iterator (plpython.c:4648)  (seg37 slice1 raiders4:60037 pid=32382) (cdbdisp.c:1499)\n12:56:20 [Helpers$(akka://deepdive)] INFO  DETAIL:\n12:56:20 [Helpers$(akka://deepdive)] INFO        TypeError: unsupported operand type(s) for &: 'str' and 'int'\n12:56:20 [Helpers$(akka://deepdive)] INFO        Traceback (most recent call last):\n12:56:20 [Helpers$(akka://deepdive)] INFO    PL/Python function \"func_extract_isincalloutcall\", line 189, in <module>\n12:56:20 [Helpers$(akka://deepdive)] INFO    PL/Python function \"func_extract_isincalloutcall\", line 242, in _compile\n12:56:20 [Helpers$(akka://deepdive)] INFO    PL/Python function \"func_extract_isincalloutcall\", line 505, in compile\n12:56:20 [Helpers$(akka://deepdive)] INFO    PL/Python function \"func_extract_isincalloutcall\", line 678, in parse\n12:56:20 [Helpers$(akka://deepdive)] INFO    PL/Python function \"func_extract_isincalloutcall\", line 313, in _parse_sub\n12:56:20 [Helpers$(akka://deepdive)] INFO    PL/Python function \"func_extract_isincalloutcall\", line 406, in _parse\n12:56:20 [Helpers$(akka://deepdive)] INFO  PL/Python function \"func_extract_isincalloutcall\"\n12:56:21 [Helpers$(akka://deepdive)] INFO  DROP VIEW\n12:56:21 [extractorRunner-extract_isincalloutcall] INFO  Finish executing UDF in database!\n12:56:21 [extractorRunner-extract_isincalloutcall] DEBUG Analyzing output relation.\n. Problem was environment pointed to a different version of deepdive. Everything works fine on master branch.\n. Duplicate of #252 \n. I can point to a running ec-2 instance with that issue for debugging.\n. @feiranwang I noticed that when you use the dw binary included with deepdive, it returns the correct result (0.5). However, when you compile the latest master version of the sampler repo and use that, it will return the wrong result (1.0). Here, I was using clang++ on MacOS for compilation.\n. @feiranwang Hmm, just ran make clean and recompiled \u2013\u00a0same problem. What compiler are you using? \nI ran\nsudo port install clang-3.7\nsudo port select --set clang mp-clang-3.7 \nCXX=/opt/local/bin/clang++ make\n. @feiranwang Thanks for the hotfix, this example seems to work now. However, some results are still surprising. \nChange the data as follows:\ncreate_candidates: {\n        style: sql_extractor\n        sql: \"\"\"DROP TABLE IF EXISTS candidates CASCADE;\n                CREATE TABLE candidates (id BIGINT, group_id INTEGER, target INTEGER, is_true BOOLEAN);\n                INSERT INTO candidates VALUES (NULL, 1, 0, NULL);\n                INSERT INTO candidates VALUES (NULL, 2, 0, NULL);\n                INSERT INTO candidates VALUES (NULL, 2, 1, NULL);\n                INSERT INTO candidates VALUES (NULL, 3, 0, NULL);\n                INSERT INTO candidates VALUES (NULL, 3, 1, NULL);\n                INSERT INTO candidates VALUES (NULL, 3, 2, NULL);\n                INSERT INTO candidates VALUES (NULL, 4, 0, NULL);\n                INSERT INTO candidates VALUES (NULL, 4, 1, NULL);\n                INSERT INTO candidates VALUES (NULL, 4, 2, NULL);\n                INSERT INTO candidates VALUES (NULL, 4, 3, NULL);\n               \"\"\"\n    }\nThen look at the output:\n```\nselect * from candidates_is_true_inference order by group_id, target;\nid | group_id | target | is_true | category | expectation\n----+----------+--------+---------+----------+-------------\n  0 |        1 |      0 |         |        1 |           1\n  1 |        2 |      0 |         |        1 |       0.489\n  2 |        2 |      1 |         |        1 |       0.956\n  3 |        3 |      0 |         |        1 |       0.498\n  4 |        3 |      1 |         |        1 |       0.912\n  5 |        3 |      2 |         |        1 |       0.918\n  6 |        4 |      0 |         |        1 |       0.499\n  7 |        4 |      1 |         |        1 |        0.88\n  8 |        4 |      2 |         |        1 |       0.884\n  9 |        4 |      3 |         |        1 |       0.875\n(10 rows)\n```\nThe expectation should be ~1 for all variables. \n. It works great now. Thanks, Feiran!\n. I did some measurements of variance on the simple spouse example (/deepdive/examples/spouse_example). In each case, I ran 20 iterations of the sampler using the default settings.\nI tried both the sampler before April 29th (old) and the sampler after April 29th (new). Zifei also suggested to try switching the symmetry factor (Equal(A,B) with learned weight) on and off. Below are variance of expectations, and variance of learned weights for all combinations:\nnew sampler, with symmetry\nAverage variance expectations  : 0.05173602954174087\nAverage variance weights       : 2.868037752352032\nold sampler, with symmetry\nAverage variance expectations  : 0.00468157241586358\nAverage variance weights       : 0.0030183971679320445\nnew sampler, without symmetry\nAverage variance expectations  : 0.08879336425759929\nAverage variance weights       : 0.21628128085863527\nold sampler, without symmetry\nAverage variance expectations  : 0.0011099367678029117\nAverage variance weights       : 0.0012100521803007904\nWe can conclude\n- variance goes up when we add the symmetry factor\n- variance goes up when we use the new sampler\n. @feiranwang Here are some numbers on the Chunking Example. This example has 3 factors: 1. logistic regression, 2. linear chain CRF, 3. skip-chain CRF. All weights are learned.\nDetails below, but the high-level message is that the results are largely consistent with the previous experiment.\n(The only surprise is that adding skip-chain CRF factors reduces variance in both cases !?)\nold sampler, 3 factors\nAverage variance expectations  : 3.478433970813495e-05\nAverage variance weights       : 0.0013675461712029696\nold sampler, 2 factors\nAverage variance expectations  : 3.2690989117771345e-05\nAverage variance weights       : 0.0015932013418684728\nold sampler, 1 factor\nAverage variance expectations  : 1.547835036452601e-05\nAverage variance weights       : 0.0007665820282586536\nnew sampler, 3 factors\nAverage variance expectations  : 9.510826048157778e-05\nAverage variance weights       : 0.003406234197422947\nnew sampler, 2 factors\nAverage variance expectations  : 0.00011861153511291191\nAverage variance weights       : 0.005766963065798057\nnew sampler, 1 factor\nAverage variance expectations  : 2.1697729640831944e-05\nAverage variance weights       : 0.001199654528888731\n. Some experimentation on the spouse example. Here I'm keeping the factor graph fixed and also the parameters to the sampler (-l 300 -s 1 -i 500 --alpha 0.1). The question I'm interested in is how the commit on April 29th (sampler repo) affects results.\nAfter commit ca3e8d45ef1c1c7d8badd16f82eb02847118d744\nINFERENCE CALIBRATION (QUERY BINS):\nPROB BIN 0.0~0.1  -->  # 65023\nPROB BIN 0.1~0.2  -->  # 11\nPROB BIN 0.2~0.3  -->  # 7\nPROB BIN 0.3~0.4  -->  # 8\nPROB BIN 0.4~0.5  -->  # 10\nPROB BIN 0.5~0.6  -->  # 8\nPROB BIN 0.6~0.7  -->  # 6\nPROB BIN 0.7~0.8  -->  # 10\nPROB BIN 0.8~0.9  -->  # 14\nPROB BIN 0.9~0.10  -->  # 5903\nAfter previous commit eaeac6838ff03c9bccd42d32ea3a0bed3436aab6\nINFERENCE CALIBRATION (QUERY BINS):\nPROB BIN 0.0~0.1  -->  # 12350\nPROB BIN 0.1~0.2  -->  # 2897\nPROB BIN 0.2~0.3  -->  # 2064\nPROB BIN 0.3~0.4  -->  # 2054\nPROB BIN 0.4~0.5  -->  # 1856\nPROB BIN 0.5~0.6  -->  # 2085\nPROB BIN 0.6~0.7  -->  # 2490\nPROB BIN 0.7~0.8  -->  # 3251\nPROB BIN 0.8~0.9  -->  # 5695\nPROB BIN 0.9~0.10  -->  # 36258\nWhy do these distributions look so different? Perhaps the parameters to the sampler have to be calibrated to account for sampling from more variables. Increasing the -i, -l, -s parameters by 10x on the newer version of the sampler has little effect. However, increasing the regularization parameter to -b 10 gives us\nAfter commit ca3... with -b 10\nINFERENCE CALIBRATION (QUERY BINS):\nPROB BIN 0.0~0.1  -->  # 29013\nPROB BIN 0.1~0.2  -->  # 3185\nPROB BIN 0.2~0.3  -->  # 2707\nPROB BIN 0.3~0.4  -->  # 2438\nPROB BIN 0.4~0.5  -->  # 2205\nPROB BIN 0.5~0.6  -->  # 2155\nPROB BIN 0.6~0.7  -->  # 2030\nPROB BIN 0.7~0.8  -->  # 2289\nPROB BIN 0.8~0.9  -->  # 2373\nPROB BIN 0.9~0.10  -->  # 22605\nIs it possible that the regularization parameter is not calibrated correctly in the latest version?\n. Hey Feiran,\nI just added a few more results on variance which might help with debugging:\nhttps://github.com/HazyResearch/deepdive/issues/282\nRaphael\nOn Wed, May 13, 2015 at 6:02 PM, Feiran Wang notifications@github.com\nwrote:\n\nPartial results: the weights do not look correct. There are very large\nweights (~30) and very small weights (~-30).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/283#issuecomment-101869921\n.\n. That sounds great, that solves the issue!\n\nLonger term, we may still want to switch to an even more readable representation. I think the text format is still pretty hard to parse for a human, especially when compared to the Alchemy format, but I think for now it's great.\nAnother minor point: I like using a testing framework that is less complex than C++, but we are now using 4 different testing frameworks (for C++, scala, python, bash). It seems we could simplify by sticking to python for this. I imagine a future version of deepdive which only uses C++ and python.\n. This looks great. Adds new UDF for fast fast sequence assignment similar to the existing UDF for greenplum.\n. Added pull request for PostgresXL documentation #301\n. Thanks, Jaeho! I fixed the writing in response to your feedback. Will walk through the tutorial on a fresh ec-2 instance and see if copy-paste gets me all the way through. Will let you know.\n. @netj It would be great if you could give it a try. Thanks!\nOn Fri, May 29, 2015 at 4:17 PM, Jaeho Shin notifications@github.com\nwrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann Any updates?\nOtherwise, I'd like to give a try.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/301#issuecomment-106956504\n.\n. Oh. I guess never tested multinomial variables. We'll have to rewrite that\nSQL query for XL to avoid ORDER BY in the subqueries. I'll create a fix.\n\nOn Sat, May 30, 2015 at 10:08 PM, Jaeho Shin notifications@github.com\nwrote:\n\nAfter installing, I tried running make test and found a test isn't\npassing with Postgres-XL (PostgresInferenceRunnerSpec: Postgres inference\ndata store, grounding the factor graph with Multinomial variables, should\nwork with weight variables) with the following error:\norg.postgresql.util.PSQLException: ERROR: Postgres-XL does not currently support ORDER BY in subqueries\n[...]\n     at org.deepdive.datastore.JdbcDataStore$$anonfun$executeSqlQueries$2.apply(JdbcDataStore.scala:71)\n     at org.deepdive.datastore.JdbcDataStore$$anonfun$executeSqlQueries$2.apply(JdbcDataStore.scala:68)\n[...]\nAny ideas?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/301#issuecomment-107128334\n.\n. @netj @zifeishan I removed two ORDER BY clauses that are used on temporary multinomial tables.\nAll tests are passing now on XL. However, I don't completely understand why we had the ORDER BY clauses there: Does one of you know? Are they OK to be removed? If not, there might be another way to make it work by changing the XL distribution strategy for these tables, e.g. setting it to replication.\n\nhttps://github.com/HazyResearch/deepdive/blob/raphael-pgxl/src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala#L601-L621\n. @zhangce @netj @feiranwang Thanks, Feiran!\nCe, can you help us here?\n. It should be possible to fix this in the database. Changing replication strategy doesn't help (XL still throwing error), but I think that we we might be able to fix this by changing our UDF for sequence assignment. It currently uses row_number(); we might be able to get the ORDER BY in here.\n. Thanks \u2013 that's great. I just checked some scripts into branch\nraphael-script.\nYou might want to adjust the scripts a bit.\nThe raw urls which you suggested work fine, eg.\ncurl -s\nhttps://raw.githubusercontent.com/hazyresearch/deepdive/raphael-script/util/script/dduser_dd.sh\n| bash\nOn Fri, May 29, 2015 at 5:09 PM, Jaeho Shin notifications@github.com\nwrote:\n\nThis is a great idea. I totally agree we should move towards this\ndirection to reduce the maintenance burden. I'll include new instructions\nin this release if you could share the scripts you have.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/302#issuecomment-106965346\n.\n. @netj I cleaned up the scripts. The new installer works great.\n\nGet an ec-2 instance with Ubuntu 14.04 and 100GB boot volume. Then run\ncurl -s https://raw.githubusercontent.com/hazyresearch/deepdive/raphael-script/util/script/entry.sh | bash\nCould you link http://deepdive.stanford.edu/install to above script?\n. Sounds great. I agree that the testing frameworks can also be based on the same scripts. For my use cases, I'm actually fine with building the sources from scratch for every install. Our code base (develop) is changing so frequently that a pre-built image often won't work for me. I think it would be great to make the script better to show helpful error messages, allow fully automated mode, check all dependencies, support OSX, etc.\n. @netj This new set of installer scripts is fantastic!! Way better than I imagined! Testing it now.\n. Sounds good. I did some testing on an ec-2 ubuntu 14.04 instance. A few comments:\n1. After one installation finishes, it simply asks \"# Select what to install (enter a number or q to quit)?\", but it doesn't show the options. Can we show the options again?\n2. There are some dependencies between the different menu options.\n3. For example, running deepdive_git_repo without anything else results in an error due to missing git.\n4. Also, running deepdive_git_repo throws an error, if psql has not been installed yet:\n   lib/check-depends.sh: line 7: type: psql: not found\n   make: ** [depends] Error 1\n5. Trying the pgxl installer I got the following error\n   /etc/profile: line 4: PS1: unbound variable\n6. For the pgxl installer, I need to be able to adjust the number of data nodes (and maybe target dir). The problem is that 16 nodes is only possible on machines with a very large amount of memory. Would it be possible to add some logic in there like I had in the install.sh in my branch? https://github.com/HazyResearch/deepdive/blob/raphael-script/util/script/install.sh\n7. Instead of setting up user/password for pg/pgxl, can we just do \n   echo 'local   all   all   trust' | sudo tee -a /etc/postgresql/9./main/pg_hba.conf\n. @netj Great. I think we're ready to merge. \n. @netj I'd like to merge but I see that travis is failing because the spouse example gets an accuracy of 88% (not 90%). Is this something that can be ignored here? Let me know if I can go ahead and merge.\n. Thanks! This new installer will be a game-changer for DeepDive!\n. > @raphaelhoffmann https://github.com/raphaelhoffmann Sorry the problem\n\nthat this PR is solving is actually not PXL's problem. I fixed the table\nschema to avoid updating a distribution key, so I think this PR doesn't\nneed to go in.\nSounds good. So, it seems both GP and XL use the first column as the\ndistribution key if no other column is explicitly set to be the\ndistribution key.\n. Really? I am pretty sure and I have one witness. You are on develop_fix, right?\nOn Jun 15, 2015, at 3:01 PM, Jaeho Shin notifications@github.com wrote:\n@SenWu @raphaelhoffmann Are you sure? Do you see the incremental test ending with two PASS lines? I can consistently reproduce the error. I just started a new EC2 instance, installed PGXL with our installer, and ran make test to get the same error.\n\u2014\nReply to this email directly or view it on GitHub.\n. Based on your error message, this might be relevant: alldefector figured\nout why XL nodes sometimes get stuck and how to recover from that. I'm just\n\ncopy/pasting his message here:\nA hard lesson I've learned about PGXL is that we should avoid running any\nDML / DDL using plpy.execute inside a UDF (the same is is probably true for\nplpgsql). I did that; sometimes it works, and sometimes it fails and PGXL\nenters a limbo state where you simply cannot drop a table even after\nrestarting all servers. The culprit turned out to be some orphan prepared\ntransactions on the data nodes locking certain tables to block anybody else\nfrom grabbing exclusive locks (hence DROP TABLE hangs). I had to use\ncommands like these to fix the DB:\nfor p in 3001 3002 3003; do psql -p $p spouses -c 'select * from\npg_prepared_xacts'; done\nfor p in 3001 3002 3003; do psql -p $p spouses -c \"rollback prepared\n'_\\$XC\\$3220'\"; done\nHere is a related trick for releasing monitoring spurious locks:\nhttp://stackoverflow.com/questions/1063043/how-to-release-possible-postgres-row-locks\nNext time PGXL misbehaves, try those to salvage it.\nOn Mon, Jun 15, 2015 at 3:17 PM, Jaeho Shin notifications@github.com\nwrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann Yes, maybe my\nenvironment is somehow interfering? I'd like to cross verify it's working.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/321#issuecomment-112226579\n.\n. I did some experiments which suggest PGXL unload performance is really\ngood! I'm consistently getting 200MB/sec, which is the max I/O performance\nfor the machine.\n\nHowever, I also did experience the slow unload performance Zifei has\nreported on a different machine (with the same specs). This must be a PGXL\nbug or configuration issue.\nWhat is really slow, however, is loading (25MB/sec) which is CPU-bound and\nnot parallel.\nalldefector: PGXL does support UNLOGGED and we've been using it.\nOn Sat, Aug 29, 2015 at 9:54 PM, Feiran Wang notifications@github.com\nwrote:\n\nYes, basically our idea is to add parameters from the user to deepdive,\nand perform local join and dumping.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/372#issuecomment-136084999\n.\n. Loading into UNLOGGED table: 100MB/sec\nLoading into LOGGED table: 21MB/sec\n\nPostgres 9.5 includes a new command\nALTER TABLE ... SET LOGGED;\nBut there does not seem to be an option to convert unlogged to logged in\n9.2/PGXL.\nOn Tue, Sep 1, 2015 at 6:25 PM, alldefector notifications@github.com\nwrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann : what throughput\ndo you get for loading with UNLOGGED? Did you find a work-around to turn\noff UNLOGGED after loading?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/372#issuecomment-136909441\n.\n. I created a little script that does parallel unload from PGXL, runs a\ncommand, then does parallel loading. It reads the port numbers of the\nrunning PGXL nodes from the coordinator.\n\nhttps://gist.github.com/raphaelhoffmann/916a97435801d438c919\nThis should be fast, but I am still planning to do a more careful\nperformance analysis.\nOne more thing: On Greenplum, I noticed one can also get a load time\nimprovement by running multiple \\COPY in parallel \u2013 however, on raiders4\nthere's a sweet spot which is somewhere between 2 and 9 parallel processes.\nAfter that, you see a decrease in performance, likely due to cache misses.\nOn Tue, Sep 1, 2015 at 6:42 PM, Raphael Hoffmann <raphael.hoffmann@gmail.com\n\nwrote:\nLoading into UNLOGGED table: 100MB/sec\nLoading into LOGGED table: 21MB/sec\nPostgres 9.5 includes a new command\nALTER TABLE ... SET LOGGED;\nBut there does not seem to be an option to convert unlogged to logged in\n9.2/PGXL.\nOn Tue, Sep 1, 2015 at 6:25 PM, alldefector notifications@github.com\nwrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann : what throughput\ndo you get for loading with UNLOGGED? Did you find a work-around to turn\noff UNLOGGED after loading?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/372#issuecomment-136909441\n.\n. dd_query_f_ismassageparlorad_features (38GB) is distributed by \"ismassageparlorad.id\".\ndd_weights_f_ismassageparlorad_features (18MB!) is distributed by REPLICATION.\n\n\nThis join should be trivial, no matter how the first table is distributed.\nWe also can't re-distribute the table for every join, because re-distribution is very expensive. I'm currently re-distributing the first table by feature, but the process is CPU-bound with only the coordinator showing activity (100% CPU).\nMaybe I'm using too many nodes (32).\n. I tried re-running the query after redistributing the first table.\nIt's running now on our Azure instance using PGXL at a rate of 3.8MB/sec (this doesn't include the time for redistribution which itself took maybe an hour or more).\nI also checked my logs to see how a corresponding query executed on Greenplum on raiders4. There, the rate is 1.8MB/sec (but we don't do redistribution). \nNot sure why this takes so long using either database system. The data is so small that it could easily fit in memory.  \nalldefector \u2013\u00a0when you get a chance can you log into the machine and see if there's an easy fix? Thanks.\n. Here are the query plans:\nGreenplum (after ANALYZE):\nGather Motion 48:1  (slice2; segments: 48)  (cost=106201580.45..115136401.10 rows=11701014 width=24)\n   ->  HashAggregate  (cost=106201580.45..115136401.10 rows=11701014 width=24)\n         Group By: t0.id, t1.id, t0.\"rates.id\"\n         ->  Hash Join  (cost=13119111.20..98670881.40 rows=11701014 width=24)\n               Hash Cond: t0.feature = t1.feature\n               ->  Seq Scan on dd_query_rates_features t0  (cost=0.00..7913286.40 rows=11701014 width=39)\n               ->  Hash  (cost=5525981.00..5525981.00 rows=10945052 width=37)\n                     ->  Broadcast Motion 48:48  (slice1; segments: 48)  (cost=0.00..5525981.00 rows=10945052 width=37)\n                           ->  Seq Scan on dd_weights_rates_features t1  (cost=0.00..162905.52 rows=228022 width=37)\nPGXL (after redistribution and ANALYZE):\nUnique  (cost=300927152.97..315138286.41 rows=1421113344 width=24)\n   ->  Remote Subquery Scan on all (data1,data10,data11,data12,data13,data14,data15,data16,data17,data18,data19,data2,data20,data21,data22,data23,data24,data25,data26,data27,data28,data29,data3,data30,data31,data32,data4,data5,data6,data7,data8,data9)  (cost=300927152.97..304479936.33 rows=1421113344 width=24)\n         ->  Unique  (cost=300927152.97..304479936.33 rows=1421113344 width=24)\n               ->  Sort  (cost=300927152.97..304479936.33 rows=1421113344 width=24)\n                     Sort Key: t0.id, t1.id, t0.\"ismassageparlorad.id\"\n                     ->  Hash Join  (cost=9161.86..55742915.54 rows=1421113344 width=24)\n                           Hash Cond: (t0.\"ismassageparlorad.feature\" = t1.\"ismassageparlorad.feature\")\n                           ->  Seq Scan on dd_query_f_ismassageparlorad_features_x t0  (cost=0.00..23758703.44 rows=1421113344 width=22)\n                           ->  Hash  (cost=6040.27..6040.27 rows=249727 width=70)\n                                 ->  Seq Scan on dd_weights_f_ismassageparlorad_features t1  (cost=0.00..6040.27 rows=249727 width=70)\nThe unique is unnecessary and it is dropped if you remove DISTINCT from the query; when executing, I ran without DISTINCT, in which case I got the following query plan:\nRemote Subquery Scan on all (data1,data10,data11,data12,data13,data14,data15,data16,data17,data18,data19,data2,data20,data21,data22,data23,data24,data25,data26,data27,data28,data29,data3,data30,data31,data32,data4,data5,data6,data7,data8,data9)  (cost=9161.86..55742915.54 rows=1421113344 width=24)\n   ->  Hash Join  (cost=9161.86..55742915.54 rows=1421113344 width=24)\n         Hash Cond: (t0.\"ismassageparlorad.feature\" = t1.\"ismassageparlorad.feature\")\n         ->  Seq Scan on dd_query_f_ismassageparlorad_features_x t0  (cost=0.00..23758703.44 rows=1421113344 width=22)\n         ->  Hash  (cost=6040.27..6040.27 rows=249727 width=70)\n               ->  Seq Scan on dd_weights_f_ismassageparlorad_features t1  (cost=0.00..6040.27 rows=249727 width=70)\n. @SenWu Is DISTINCT necessary for this query?\n. Perhaps it has to do with the volume /lfs/local/0 being 99% full. We may have to ask people again to free up space.\n. The second phase of sequence assignment just finished, and I want to let it continue (raiders4, DB is memex_mar2015_large). I'll let you know the next time time I'm running sequence assignment.\n. Sure! The log of the run is here:\n/lfs/local/1/raphaelh/dd/new/deepdive/log/2015-09-06T093453.txt\nDatabase is memex_mar2015_large and it's running inference only. I launched\nthis job at 9:30AM and it still hasn't reached the sampling phase.\nThanks!\nOn Sun, Sep 6, 2015 at 5:28 PM, SenWu notifications@github.com wrote:\n\nThanks! Could you share the log of your run to us?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138140054\n.\n. Ce: Job is no longer running. It may have to do with the disk being too full.\nOn Sep 6, 2015, at 9:08 PM, zhangce notifications@github.com wrote:\nThe finger print of tuples in GP is a little weird (it is too physical in\nthe sense that there are no guarantee of when it will not change at all)...\nneed to do more profiling after Raphael's current run. That function should\nbe faster than 2MB/s if the DB is working properly.\nOn Sep 6, 2015 9:04 PM, \"alldefector\" notifications@github.com wrote:\n\nOr if the ID fields are used as conceptual foreign keys in the DB, maybe we\ncould use a fingerprint of the tuple.\nOn Sun, Sep 6, 2015 at 8:43 PM Feng Niu niufeng14@gmail.com wrote:\n\nCorrect me if I'm wrong -- looks like all those post-hoc-assigned\nsequence\nIDs are always used as array indexes in the sampler and never for SQL\njoins. If that's true, what are the challenges to do without the sequence\nassignment steps?\nFeng\nOn Sun, Sep 6, 2015 at 5:43 PM Raphael Hoffmann \nnotifications@github.com\nwrote:\n\nSure! The log of the run is here:\n/lfs/local/1/raphaelh/dd/new/deepdive/log/2015-09-06T093453.txt\nDatabase is memex_mar2015_large and it's running inference only. I\nlaunched\nthis job at 9:30AM and it still hasn't reached the sampling phase.\nThanks!\nOn Sun, Sep 6, 2015 at 5:28 PM, SenWu notifications@github.com wrote:\n\nThanks! Could you share the log of your run to us?\n\u2014\nReply to this email directly or view it on GitHub\n<\n\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138140054\n\n.\n\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138144569\n.\n\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138178468\n.\n\u2014\nReply to this email directly or view it on GitHub.\n. We restarted greenplum (thanks, @SenWu !), free'd up some space, and re-ran this process. The log of the new run is here:\n\n\n/lfs/local/1/raphaelh/dd/new/deepdive/log/2015-09-06T093453.txt  \nWe are still seeing the same runtimes (3h for sequence assignment of rates, 2 or 3h for sequence assignment of features). I didn't yet run the CREATE TABLE command Ce suggested, but I suspect that the runtime is indeed normal.\n@zhangce: I think the changes you propose sound great! Any chance you can make a first stab at it? \nThanks!\n. I ran VACUUM last night and then restarted the job. The time for sequence assignment on \"rates\" went down from 3h to 2:18h. I haven't checked the time for sequence assignment on the feature table yet.\nSo, it's a small improvement, but still sequence assignment is a major bottleneck.\n. Assigning the IDs when writing the table the first time seems to be the right thing to do. But \u2013 I thought that the reason why we have stored procedures for sequence assignment is because we found that the sequence generators were too slow (?)\nIf the sequence generator turns out to be a bottleneck, it's likely because it's syncing after every row. In that case we could create a stored procedure that does the insert in batches...\n. @zhangce. Sounds good, we can re-run the experiment once raiders4 is working properly again.\nHere are a few more measured runtimes for another extractor on an AWS r3.8xlarge instance, using a RAID over the local SSD volumes for I/O. I/O throughput is very good on that machine (600-700MB/sec).\n```\n3:00h     running extractor\n1:40h     SELECT copy_table_assign_ids_replace('public', 'dd_query_f_isoutcall_features', 'id', 0)\n3:00h     COPY (SELECT DISTINCT t0.id AS factor_id, t1.id AS weight_id,  \"isoutcall.id\"\n           FROM dd_query_f_isoutcall_features t0, dd_weights_f_isoutcall_features t1\n           WHERE  t0.\"isoutcall.feature\" = t1.\"isoutcall.feature\") TO STDOUT;\n1:45h     CREATE UNLOGGED TABLE dd_isoutcall_vtype AS SELECT t0.id, CASE WHEN t2.variable_id IS NOT NULL AND is_outcall IS NOT NULL THEN 2\n                           WHEN t1.variable_id IS NOT NULL THEN 0\n                           WHEN is_outcall IS NOT NULL THEN 1\n                           ELSE 0\n                      END as dd_variable_type\n          FROM isoutcall t0 LEFT OUTER JOIN dd_graph_variables_holdout t1 ON t0.id=t1.variable_id\n          LEFT OUTER JOIN dd_graph_variables_observation t2 ON t0.id=t2.variable_id\n0:45h     Unloading factor graph to disk (tobinary)\n0:30h     Sampler loading factor graph until crash due to out of memory\n```\n. I see. @SenWu has also created a pull request that gets rid of the DISTINCT in the third query. That simplifies the query plan, getting rid of a sort, speeding this query up. \n. @zhangce I'm done with my processing work, so this might be a good time to investigate the issues on raiders4. Both seem to have the same configuration with two NAS volumes:\n/lfs/local/0  (~15TB)\n/lfs/local/1  (~32TB)\nI measured sequential write I/O (using dd if=/dev/zero of=/lfs/local/1/raphaelh/xxx bs=64k count=131072 conv=fsync) and found that the speed for /lfs/local/0 is about 100MB/sec and for /lfs/local/1 is about 150MB/sec. These numbers are the same for raiders3 and raiders4. Since /lfs/local/1 is much larger, and thus likely contains more disks, I expect a larger difference for random I/O.\nOn raiders3, greenplum uses /lfs/local/1 for storage, on raiders4, greenplum uses /lfs/local/0 for storage. That could explain some of the difference in performance. Also, the volume is still 96% full on raiders4 (used to be 99%) and there might be fragmentation. \nSo, I think the TODOs to fix raider4 are:\n1. move greenplum to /lfs/local/1.\n2. make sure no disk is more than 90% full.\n@SenWu Would it be possible to move greenplum over?\n. Since this thread discusses multiple topics, we decided to close this issue and open separate new issues. We will also continue our investigation of hardware issues offline.\n. > Does the converter or the python script print out logs that are not\n\nredirected by Java? Those logs would be useful. Maybe generate the input\nfile and halt DD and then run the converter manually so we can see the\nlogs.\n\nThat sounds good \u2013 I'll run this overnight.\n. OK. It's restarted. In both cases when I got this error, I also had to later start a new shell, since simply re-running DD threw the following error:\n./run.sh\njava.io.IOException: Permission denied\n        at java.io.UnixFileSystem.createFileExclusively(Native Method)\n        at java.io.File.createNewFile(File.java:1012)\n        at xsbt.boot.Locks$.apply0(Locks.scala:35)\n        at xsbt.boot.Locks$.apply(Locks.scala:28)\n        at xsbt.boot.Launch.locked(Launch.scala:178)\n        at xsbt.boot.Launch.app(Launch.scala:93)\n        at xsbt.boot.Launch.app(Launch.scala:91)\n        at xsbt.boot.Launch$.run(Launch.scala:51)\n        at xsbt.boot.Launch$$anonfun$explicit$1.apply(Launch.scala:45)\n        at xsbt.boot.Launch$.launch(Launch.scala:65)\n        at xsbt.boot.Launch$.apply(Launch.scala:16)\n        at xsbt.boot.Boot$.runImpl(Boot.scala:32)\n        at xsbt.boot.Boot$.main(Boot.scala:21)\n        at xsbt.boot.Boot.main(Boot.scala)\nError during sbt execution: java.io.IOException: Permission denied\n. @SenWu I see \u2013 but I don't understand why this is happening. What is\ncausing this?\nOn Sun, Sep 6, 2015 at 10:58 PM, SenWu notifications@github.com wrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann You lost permission\nwhen you re-run DD. You can either re-login or use reauth command to get\nauthenticated access.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/381#issuecomment-138196644\n.\n. Not sure what it's trying to create in /afs, but you're right: It appears\nto be some Stanford security policy. Thanks.\n\nOn Sun, Sep 6, 2015 at 11:14 PM, SenWu notifications@github.com wrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann It might be your DD\ninstallation. Did you install your DD/sbt in afs? If all your installations\n(sbt, DD) are in lfs, then everything will be fine.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/381#issuecomment-138199029\n.\n. This issue is resolved. Thanks for the helpful tip, @SenWu !\n\nAlthough I didn't have sbt installed locally, I was using the sbt launcher in the DeepDive directory sbt/sbt \"run ...\".  Without further arguments, it puts ivy packages and lock files into the user's home directory. The errors went away when explicitly instructing sbt to use different paths:\njava -Dsbt.global.base=/lfs/local/1/raphaelh/.sbt -Dsbt.ivy.home=/lfs/local/1/raphaelh/.ivy2 -Divy.home=/lfs/local/1/raphaelh/.ivy2 -jar sbt/sbt-launch.jar \"run -c $DIRNAME/deepdive.conf -o $TMP_DIR\"\nI think it would make sense to set these paths automatically in the DeepDive launch script, or perhaps at least warn/document somewhere, since many of us are running DeepDive on Stanford machines and will inevitably run into this problem when running on larger datasets. \n. Yes, some more documentation would be helpful here. Here's why I think that DISTINCT is not necessary. Our query is \nhttps://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala#L719\nClaim 1: For every tuple in the left table, there is at most one join tuple.\nThe join condition is a key for the right table. \nhttps://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala#L710\nhttps://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala#L688\nhttps://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala#L579\nClaim 2: Each join tuple is distinct.\nThe join selects factor_id which is a key for the left table.\nhttps://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala#L563\n. There's another problem with re-using a weight table: The same query is quite slow when the weight table is large (which is the case for all our HT extractors). However, after adding an index on the feature column, it is running quite well.\nI suggest we also update the documentation, recommending an index on the feature column when re-using a weight table.\n. We just need a simple way to re-run deepdive run, for example after you made some changes to a udf.\nIn that case you need to reset the generated tables. However, you can't use deepdive initdb because it will clear the entire database. It is also tedious and error-prone to use deepdive sql because you can easily forget a table. \n. @feiranwang That's exactly right. I just a need a simple solution for the short term. Is there a way to truncate in ddlog?\nI understand that this more imperative way doesn't fit well with the declarative nature of datalog, but the same may be true for \"+=\"...\n. Yes. Some command like deepdive clear which checks app.ddlog for all tables and then truncates each one would be handy. It might make sense to ask the user for confirmation.\n. To add to @alldefector's bullets: We now create one database per extractor,\nbecause it takes too long to run multiple extractors at once and we need to keep all\nintermediate output. Our problem now is loading the sentences table (264G, > 1h),\nwhich we wouldn't have with schema support.\n@netj @zhangce We did a bunch of experiments around @zhangce 's idea of using separate schemas,\nand have a preliminary integration of schema support in DeepDive. Support for schemas\ndoes require a few changes to the code base, so it would be great to discuss how we want\nto proceed.\n1. Specifying the search_path in the DB connection string only works with newer\n   versions of Postgresql, not Greenplum. It is possible, however, to run SQL on GP:\n   SET search_path TO dd_run_0003, public;. Also note: Most RDBMSs have schema\n   support, but not MySQL.\n2. The connection pool used by DeepDive (DBCP 1.x) does not support schemas.\n   Fortunately, it is easy to change the connection pool.\nIn build.sbt I added a dependency to\n\"org.apache.commons\" % \"commons-dbcp2\" % \"2.1.1\"\nand in deepdive.conf I added \ndeepdive.db.default {\n      poolFactoryName: \"commons-dbcp2\"\n      ...\n   }\nNo other change was necessary for this.\n3. We now need to create the schema when we run deepdive initdb. I think we\n   should not let this command drop and create the entire DB; it should\n   just create a new schema for the next run. In util/deepdive-initdb, I added\ndeepdive-sql \"CREATE SCHEMA $SCHEMA_NAME;\"\n4. Finally, we need to make sure that JDBC uses the schema when\n   creating a connection. Unfortunately, the default search_path on\n   the database (ALTER DATABASE ... SET search_path = ...)\n   is ignored by JDBC, and it doesn't look like scalikejdbc supports\n   schemas or SQL queries for connection initialization. As a workaround, \n   I am now setting the schema in JdbcDataStore:\ndef borrowConnection() : Connection = {\n    val c = ConnectionPool.borrow()\n    c.setSchema(SCHEMA_NAME)\n    c\n   }\nSo, this this might not be the most elegant solution, but works. Any thoughts on whether we\nwant something like this and on how to make the integration better?\n. I added a branch with all the necessary changes.\nhttps://github.com/HazyResearch/deepdive/compare/84898b1c9b9337a64057c521def5008b34d8913f...schema_support\nIf you build this and then, in your DD app, run\nexport DEEPDIVE_SCHEMA=my\ndeepdive initdb\nthen all generated tables will be in schema my. You can verify this in psql by running\nSET search_path = my,public;\n\\d\n. We are now using schemas for our DeepDive app. It works well, and saves us a lot of time.\nIf you have access to our repo, you can see how we are now launching the extractors.\nSeparate commands for the schemas would be nice, but are not absolutely necessary.\nSomething that would be very useful though is a DDLOG annotation that marks a table as a user table which DeepDive should assume exists and not try to drop and re-create. That way DeepDive can follow the search path to select the table from a different schema during execution.\nI absolutely agree about replacing JDBC with calls to \"deepdive sql\". Also, it doesn't make sense why we are using a connection pool, since we don't have many concurrent requests and it just adds complexity.\nIf JDBC was replaced with deepdive sql, no change to the scala code base would be necessary in order to support schemas, since \"deepdive sql\" observes the DB-level search_path.\n@netj How difficult do you think it would be to replace JDBC?\n. @zhangce @SenWu @feiranwang Can you show us a few lines of ddlog code that does this copying of supervised examples? Or should we already do it inside our UDFs?\n. Thanks! That works! @alldefector I think we can close this issue.\n. +1. Turning it on by default (and supporting switch --disable_evidence_sampling) would also allow us to avoid having to edit deepdive.conf.\n. @zhangce I agree that there are different things you might want to do with DeepDive, and in some cases you do not want to sample evidence variables.\nWhen building extractors with noisy distant supervision rules, however, you likely do want to sample evidence variables. We may at least want to emphasize this setting in the tutorials and examples since some users may not be aware.\n. @SenWu Thanks! It looks like we ran into an issue with PGXL, not DDLOG. \nAfter restarting PGXL, the time for this query went down from 38 min to 3 min, and the query plans were good again. (We kept the old logs and query plans both before/after analyze in case they are useful.) Due to this and other issues, we decided to move our processing back to Greenplum.\n. Hi @netj, this new version of deepdive is really great! Here are a few notes I took while experimenting.\n1. When building on OSX, I got error that X11/Xlib.h couldn't be found. This error\n   was thrown by buildkit module for graphviz.\n/usr/include/tk.h:78:23: fatal error: X11/Xlib.h: No such file or directory\nFollowing these instructions, I fixed it by adding a symlink\nln -s /opt/X11/include/X11 /usr/local/include/X11\nAfterwards, it was not sufficient to rerun 'make'. I manually cd'ed to deepdive/depends/bundled/graphviz/graphviz-2.38.0 and ran ./configure && make && make install. Then I could run \"make\" in deepdive\n2. ddlog apps didn't work with the default submodule commit, but \u2013 following your\n   suggestions \u2013 everything worked fine after upgrading to ddlog commit HazyResearch/ddlog@ee0b359c9f018c19df5eddbf3bff60bdf076e091.\nNow everything works fine, but I've encountered two different problems in a small number of runs:\n1. Sometimes, stdin is no longer showing on the screen after a successful deepdive do\n   execution. That is, I can type commands and they execute, but the letters I type are not\n   being shown on the screen. I'm using iterm2.\n2. Sometimes (rarely), a mkmimo process kept running at 100% cpu usage, after deepdive do \n   terminated with an error (possibly related to missing input data).\nI'll try to investigate more carefully if I encounter these again. \n. @feiranwang @netj Wow, thanks guys, for fixing this so quickly!\n. I noticed when using deepdive.conf, but I think the issue applies to ddlog as well.\n. @netj Thanks for looking at this. I'm on commit 802ec0d5284704c1b8bdd71c8a788758553d19ef and am trying to run the chunking example. Let me send you the logs. \n. @netf found the problem: there are some conditions missing in a query in the example, see here https://github.com/HazyResearch/deepdive/blob/docathon-v0.8/examples/chunking/app.ddlog#L38 \nI added [word_id1 = word_id2 + 1], word1 != NULL, but am still investigating why results are not the same as with old chunking example.\n. Problem is now fixed with https://github.com/HazyResearch/deepdive/pull/485\n. This works really great! Ready to merge.\n. Wow, thanks for fixing this so quickly!\n. Great! That worked!\n. Tested fix on the chunking example, but I'm still getting the same problem. The sequence assignment output in the log is now\n2016-02-21 10:27:33.852620 + deepdive db assign_sequential_id tag id 0\n2016-02-21 10:27:33.908521 NOTICE:  language \"plpgsql\" already exists, skipping\n2016-02-21 10:27:33.908568 CREATE LANGUAGE\n2016-02-21 10:27:34.194183 CREATE LANGUAGE\n2016-02-21 10:27:34.410781 NOTICE:  table \"tmp_gpsid_count\" does not exist, skipping\n2016-02-21 10:27:34.410909 CONTEXT:  SQL statement \"drop table if exists tmp_gpsid_count cascade;\"\n2016-02-21 10:27:34.410954 PL/pgSQL function \"fast_seqassign\" line 2 at execute statement\n2016-02-21 10:27:34.410984 NOTICE:  table \"tmp_gpsid_count_noagg\" does not exist, skipping\n2016-02-21 10:27:34.411008 CONTEXT:  SQL statement \"drop table if exists tmp_gpsid_count_noagg cascade;\"\n2016-02-21 10:27:34.411035 PL/pgSQL function \"fast_seqassign\" line 3 at execute statement\n2016-02-21 10:27:35.037462 NOTICE:  EXECUTING _fast_seqassign()...\n2016-02-21 10:27:35.703437  fast_seqassign\n2016-02-21 10:27:35.703535 ----------------\n2016-02-21 10:27:35.703555\n2016-02-21 10:27:35.703572 (1 row)\n2016-02-21 10:27:35.703588\n2016-02-21 10:27:35.704557 + deepdive db generate_series dd_categories_tag category 0 12\n2016-02-21 10:27:35.980159 CREATE VIEW\n2016-02-21 10:27:35.981578 mark_done process/grounding/variable/tag/assign_id\nSo, the fix removed the error message at this point, but we later still crash with:\n2016-02-21 10:57:45.739665 sampler-dw.bin: src/dstruct/factor_graph/factor_graph.cpp:375: void dd::FactorGraph::safety_check(): Assertion `this->weights[i].id == i' failed.\n2016-02-21 10:57:45.914532 process/model/learning/run.sh: line 22: 135592 Aborted                 (core dumped) sampler-dw gibbs -w <(flatten factorgraph/weights) -v <(flatten factorgraph/variables) -f <(flatten factorgraph/factors) -m factorgraph/meta -o weights -l 1000 -s 1 -i 1000 --alpha 0.01 --sample_evidence\nThe following query shows that there are still duplicate IDs in the database:\n```\nselect count(distinct id) from dd_weightsmulti_inf_istrue_tag;\n\n20676\n(1 row)\nselect count(*) from dd_weightsmulti_inf_istrue_tag;\n count\n\n268632\n(1 row)\n```\nIn the case of postgres, the count is 268632 in both cases.\n. Computing the skip-chain factors in a python UDF now.\n. We are using deepdive sql eval ... format=json to export data from greenplum to python. For integer and floating point columns, however, this export only worked for fields that did not contain NULL values. With this fix, NULL values get translated to python's None value.\n. Fixed.\n. I noticed that this is only fixed for some cases. For example, if the schema in the plpy function doesn't match, or if the plpy UDF doesn't compile, the system will still report success and continue processing. Eg.\n12:56:10 [taskManager] INFO  Memory usage: 341/2476MB (max: 27159MB)\n12:56:20 [Helpers$(akka://deepdive)] INFO  ERROR:  function \"func_extract_isincalloutcall\" error fetching next item from iterator (plpython.c:4648)  (seg37 slice1 raiders4:60037 pid=32382) (cdbdisp.c:1499)\n12:56:20 [Helpers$(akka://deepdive)] INFO  DETAIL:\n12:56:20 [Helpers$(akka://deepdive)] INFO        TypeError: unsupported operand type(s) for &: 'str' and 'int'\n12:56:20 [Helpers$(akka://deepdive)] INFO        Traceback (most recent call last):\n12:56:20 [Helpers$(akka://deepdive)] INFO    PL/Python function \"func_extract_isincalloutcall\", line 189, in <module>\n12:56:20 [Helpers$(akka://deepdive)] INFO    PL/Python function \"func_extract_isincalloutcall\", line 242, in _compile\n12:56:20 [Helpers$(akka://deepdive)] INFO    PL/Python function \"func_extract_isincalloutcall\", line 505, in compile\n12:56:20 [Helpers$(akka://deepdive)] INFO    PL/Python function \"func_extract_isincalloutcall\", line 678, in parse\n12:56:20 [Helpers$(akka://deepdive)] INFO    PL/Python function \"func_extract_isincalloutcall\", line 313, in _parse_sub\n12:56:20 [Helpers$(akka://deepdive)] INFO    PL/Python function \"func_extract_isincalloutcall\", line 406, in _parse\n12:56:20 [Helpers$(akka://deepdive)] INFO  PL/Python function \"func_extract_isincalloutcall\"\n12:56:21 [Helpers$(akka://deepdive)] INFO  DROP VIEW\n12:56:21 [extractorRunner-extract_isincalloutcall] INFO  Finish executing UDF in database!\n12:56:21 [extractorRunner-extract_isincalloutcall] DEBUG Analyzing output relation.\n. Problem was environment pointed to a different version of deepdive. Everything works fine on master branch.\n. Duplicate of #252 \n. I can point to a running ec-2 instance with that issue for debugging.\n. @feiranwang I noticed that when you use the dw binary included with deepdive, it returns the correct result (0.5). However, when you compile the latest master version of the sampler repo and use that, it will return the wrong result (1.0). Here, I was using clang++ on MacOS for compilation.\n. @feiranwang Hmm, just ran make clean and recompiled \u2013\u00a0same problem. What compiler are you using? \nI ran\nsudo port install clang-3.7\nsudo port select --set clang mp-clang-3.7 \nCXX=/opt/local/bin/clang++ make\n. @feiranwang Thanks for the hotfix, this example seems to work now. However, some results are still surprising. \nChange the data as follows:\ncreate_candidates: {\n        style: sql_extractor\n        sql: \"\"\"DROP TABLE IF EXISTS candidates CASCADE;\n                CREATE TABLE candidates (id BIGINT, group_id INTEGER, target INTEGER, is_true BOOLEAN);\n                INSERT INTO candidates VALUES (NULL, 1, 0, NULL);\n                INSERT INTO candidates VALUES (NULL, 2, 0, NULL);\n                INSERT INTO candidates VALUES (NULL, 2, 1, NULL);\n                INSERT INTO candidates VALUES (NULL, 3, 0, NULL);\n                INSERT INTO candidates VALUES (NULL, 3, 1, NULL);\n                INSERT INTO candidates VALUES (NULL, 3, 2, NULL);\n                INSERT INTO candidates VALUES (NULL, 4, 0, NULL);\n                INSERT INTO candidates VALUES (NULL, 4, 1, NULL);\n                INSERT INTO candidates VALUES (NULL, 4, 2, NULL);\n                INSERT INTO candidates VALUES (NULL, 4, 3, NULL);\n               \"\"\"\n    }\nThen look at the output:\n```\nselect * from candidates_is_true_inference order by group_id, target;\nid | group_id | target | is_true | category | expectation\n----+----------+--------+---------+----------+-------------\n  0 |        1 |      0 |         |        1 |           1\n  1 |        2 |      0 |         |        1 |       0.489\n  2 |        2 |      1 |         |        1 |       0.956\n  3 |        3 |      0 |         |        1 |       0.498\n  4 |        3 |      1 |         |        1 |       0.912\n  5 |        3 |      2 |         |        1 |       0.918\n  6 |        4 |      0 |         |        1 |       0.499\n  7 |        4 |      1 |         |        1 |        0.88\n  8 |        4 |      2 |         |        1 |       0.884\n  9 |        4 |      3 |         |        1 |       0.875\n(10 rows)\n```\nThe expectation should be ~1 for all variables. \n. It works great now. Thanks, Feiran!\n. I did some measurements of variance on the simple spouse example (/deepdive/examples/spouse_example). In each case, I ran 20 iterations of the sampler using the default settings.\nI tried both the sampler before April 29th (old) and the sampler after April 29th (new). Zifei also suggested to try switching the symmetry factor (Equal(A,B) with learned weight) on and off. Below are variance of expectations, and variance of learned weights for all combinations:\nnew sampler, with symmetry\nAverage variance expectations  : 0.05173602954174087\nAverage variance weights       : 2.868037752352032\nold sampler, with symmetry\nAverage variance expectations  : 0.00468157241586358\nAverage variance weights       : 0.0030183971679320445\nnew sampler, without symmetry\nAverage variance expectations  : 0.08879336425759929\nAverage variance weights       : 0.21628128085863527\nold sampler, without symmetry\nAverage variance expectations  : 0.0011099367678029117\nAverage variance weights       : 0.0012100521803007904\nWe can conclude\n- variance goes up when we add the symmetry factor\n- variance goes up when we use the new sampler\n. @feiranwang Here are some numbers on the Chunking Example. This example has 3 factors: 1. logistic regression, 2. linear chain CRF, 3. skip-chain CRF. All weights are learned.\nDetails below, but the high-level message is that the results are largely consistent with the previous experiment.\n(The only surprise is that adding skip-chain CRF factors reduces variance in both cases !?)\nold sampler, 3 factors\nAverage variance expectations  : 3.478433970813495e-05\nAverage variance weights       : 0.0013675461712029696\nold sampler, 2 factors\nAverage variance expectations  : 3.2690989117771345e-05\nAverage variance weights       : 0.0015932013418684728\nold sampler, 1 factor\nAverage variance expectations  : 1.547835036452601e-05\nAverage variance weights       : 0.0007665820282586536\nnew sampler, 3 factors\nAverage variance expectations  : 9.510826048157778e-05\nAverage variance weights       : 0.003406234197422947\nnew sampler, 2 factors\nAverage variance expectations  : 0.00011861153511291191\nAverage variance weights       : 0.005766963065798057\nnew sampler, 1 factor\nAverage variance expectations  : 2.1697729640831944e-05\nAverage variance weights       : 0.001199654528888731\n. Some experimentation on the spouse example. Here I'm keeping the factor graph fixed and also the parameters to the sampler (-l 300 -s 1 -i 500 --alpha 0.1). The question I'm interested in is how the commit on April 29th (sampler repo) affects results.\nAfter commit ca3e8d45ef1c1c7d8badd16f82eb02847118d744\nINFERENCE CALIBRATION (QUERY BINS):\nPROB BIN 0.0~0.1  -->  # 65023\nPROB BIN 0.1~0.2  -->  # 11\nPROB BIN 0.2~0.3  -->  # 7\nPROB BIN 0.3~0.4  -->  # 8\nPROB BIN 0.4~0.5  -->  # 10\nPROB BIN 0.5~0.6  -->  # 8\nPROB BIN 0.6~0.7  -->  # 6\nPROB BIN 0.7~0.8  -->  # 10\nPROB BIN 0.8~0.9  -->  # 14\nPROB BIN 0.9~0.10  -->  # 5903\nAfter previous commit eaeac6838ff03c9bccd42d32ea3a0bed3436aab6\nINFERENCE CALIBRATION (QUERY BINS):\nPROB BIN 0.0~0.1  -->  # 12350\nPROB BIN 0.1~0.2  -->  # 2897\nPROB BIN 0.2~0.3  -->  # 2064\nPROB BIN 0.3~0.4  -->  # 2054\nPROB BIN 0.4~0.5  -->  # 1856\nPROB BIN 0.5~0.6  -->  # 2085\nPROB BIN 0.6~0.7  -->  # 2490\nPROB BIN 0.7~0.8  -->  # 3251\nPROB BIN 0.8~0.9  -->  # 5695\nPROB BIN 0.9~0.10  -->  # 36258\nWhy do these distributions look so different? Perhaps the parameters to the sampler have to be calibrated to account for sampling from more variables. Increasing the -i, -l, -s parameters by 10x on the newer version of the sampler has little effect. However, increasing the regularization parameter to -b 10 gives us\nAfter commit ca3... with -b 10\nINFERENCE CALIBRATION (QUERY BINS):\nPROB BIN 0.0~0.1  -->  # 29013\nPROB BIN 0.1~0.2  -->  # 3185\nPROB BIN 0.2~0.3  -->  # 2707\nPROB BIN 0.3~0.4  -->  # 2438\nPROB BIN 0.4~0.5  -->  # 2205\nPROB BIN 0.5~0.6  -->  # 2155\nPROB BIN 0.6~0.7  -->  # 2030\nPROB BIN 0.7~0.8  -->  # 2289\nPROB BIN 0.8~0.9  -->  # 2373\nPROB BIN 0.9~0.10  -->  # 22605\nIs it possible that the regularization parameter is not calibrated correctly in the latest version?\n. Hey Feiran,\nI just added a few more results on variance which might help with debugging:\nhttps://github.com/HazyResearch/deepdive/issues/282\nRaphael\nOn Wed, May 13, 2015 at 6:02 PM, Feiran Wang notifications@github.com\nwrote:\n\nPartial results: the weights do not look correct. There are very large\nweights (~30) and very small weights (~-30).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/283#issuecomment-101869921\n.\n. That sounds great, that solves the issue!\n\nLonger term, we may still want to switch to an even more readable representation. I think the text format is still pretty hard to parse for a human, especially when compared to the Alchemy format, but I think for now it's great.\nAnother minor point: I like using a testing framework that is less complex than C++, but we are now using 4 different testing frameworks (for C++, scala, python, bash). It seems we could simplify by sticking to python for this. I imagine a future version of deepdive which only uses C++ and python.\n. This looks great. Adds new UDF for fast fast sequence assignment similar to the existing UDF for greenplum.\n. Added pull request for PostgresXL documentation #301\n. Thanks, Jaeho! I fixed the writing in response to your feedback. Will walk through the tutorial on a fresh ec-2 instance and see if copy-paste gets me all the way through. Will let you know.\n. @netj It would be great if you could give it a try. Thanks!\nOn Fri, May 29, 2015 at 4:17 PM, Jaeho Shin notifications@github.com\nwrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann Any updates?\nOtherwise, I'd like to give a try.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/301#issuecomment-106956504\n.\n. Oh. I guess never tested multinomial variables. We'll have to rewrite that\nSQL query for XL to avoid ORDER BY in the subqueries. I'll create a fix.\n\nOn Sat, May 30, 2015 at 10:08 PM, Jaeho Shin notifications@github.com\nwrote:\n\nAfter installing, I tried running make test and found a test isn't\npassing with Postgres-XL (PostgresInferenceRunnerSpec: Postgres inference\ndata store, grounding the factor graph with Multinomial variables, should\nwork with weight variables) with the following error:\norg.postgresql.util.PSQLException: ERROR: Postgres-XL does not currently support ORDER BY in subqueries\n[...]\n     at org.deepdive.datastore.JdbcDataStore$$anonfun$executeSqlQueries$2.apply(JdbcDataStore.scala:71)\n     at org.deepdive.datastore.JdbcDataStore$$anonfun$executeSqlQueries$2.apply(JdbcDataStore.scala:68)\n[...]\nAny ideas?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/301#issuecomment-107128334\n.\n. @netj @zifeishan I removed two ORDER BY clauses that are used on temporary multinomial tables.\nAll tests are passing now on XL. However, I don't completely understand why we had the ORDER BY clauses there: Does one of you know? Are they OK to be removed? If not, there might be another way to make it work by changing the XL distribution strategy for these tables, e.g. setting it to replication.\n\nhttps://github.com/HazyResearch/deepdive/blob/raphael-pgxl/src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala#L601-L621\n. @zhangce @netj @feiranwang Thanks, Feiran!\nCe, can you help us here?\n. It should be possible to fix this in the database. Changing replication strategy doesn't help (XL still throwing error), but I think that we we might be able to fix this by changing our UDF for sequence assignment. It currently uses row_number(); we might be able to get the ORDER BY in here.\n. Thanks \u2013 that's great. I just checked some scripts into branch\nraphael-script.\nYou might want to adjust the scripts a bit.\nThe raw urls which you suggested work fine, eg.\ncurl -s\nhttps://raw.githubusercontent.com/hazyresearch/deepdive/raphael-script/util/script/dduser_dd.sh\n| bash\nOn Fri, May 29, 2015 at 5:09 PM, Jaeho Shin notifications@github.com\nwrote:\n\nThis is a great idea. I totally agree we should move towards this\ndirection to reduce the maintenance burden. I'll include new instructions\nin this release if you could share the scripts you have.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/302#issuecomment-106965346\n.\n. @netj I cleaned up the scripts. The new installer works great.\n\nGet an ec-2 instance with Ubuntu 14.04 and 100GB boot volume. Then run\ncurl -s https://raw.githubusercontent.com/hazyresearch/deepdive/raphael-script/util/script/entry.sh | bash\nCould you link http://deepdive.stanford.edu/install to above script?\n. Sounds great. I agree that the testing frameworks can also be based on the same scripts. For my use cases, I'm actually fine with building the sources from scratch for every install. Our code base (develop) is changing so frequently that a pre-built image often won't work for me. I think it would be great to make the script better to show helpful error messages, allow fully automated mode, check all dependencies, support OSX, etc.\n. @netj This new set of installer scripts is fantastic!! Way better than I imagined! Testing it now.\n. Sounds good. I did some testing on an ec-2 ubuntu 14.04 instance. A few comments:\n1. After one installation finishes, it simply asks \"# Select what to install (enter a number or q to quit)?\", but it doesn't show the options. Can we show the options again?\n2. There are some dependencies between the different menu options.\n3. For example, running deepdive_git_repo without anything else results in an error due to missing git.\n4. Also, running deepdive_git_repo throws an error, if psql has not been installed yet:\n   lib/check-depends.sh: line 7: type: psql: not found\n   make: ** [depends] Error 1\n5. Trying the pgxl installer I got the following error\n   /etc/profile: line 4: PS1: unbound variable\n6. For the pgxl installer, I need to be able to adjust the number of data nodes (and maybe target dir). The problem is that 16 nodes is only possible on machines with a very large amount of memory. Would it be possible to add some logic in there like I had in the install.sh in my branch? https://github.com/HazyResearch/deepdive/blob/raphael-script/util/script/install.sh\n7. Instead of setting up user/password for pg/pgxl, can we just do \n   echo 'local   all   all   trust' | sudo tee -a /etc/postgresql/9./main/pg_hba.conf\n. @netj Great. I think we're ready to merge. \n. @netj I'd like to merge but I see that travis is failing because the spouse example gets an accuracy of 88% (not 90%). Is this something that can be ignored here? Let me know if I can go ahead and merge.\n. Thanks! This new installer will be a game-changer for DeepDive!\n. > @raphaelhoffmann https://github.com/raphaelhoffmann Sorry the problem\n\nthat this PR is solving is actually not PXL's problem. I fixed the table\nschema to avoid updating a distribution key, so I think this PR doesn't\nneed to go in.\nSounds good. So, it seems both GP and XL use the first column as the\ndistribution key if no other column is explicitly set to be the\ndistribution key.\n. Really? I am pretty sure and I have one witness. You are on develop_fix, right?\nOn Jun 15, 2015, at 3:01 PM, Jaeho Shin notifications@github.com wrote:\n@SenWu @raphaelhoffmann Are you sure? Do you see the incremental test ending with two PASS lines? I can consistently reproduce the error. I just started a new EC2 instance, installed PGXL with our installer, and ran make test to get the same error.\n\u2014\nReply to this email directly or view it on GitHub.\n. Based on your error message, this might be relevant: alldefector figured\nout why XL nodes sometimes get stuck and how to recover from that. I'm just\n\ncopy/pasting his message here:\nA hard lesson I've learned about PGXL is that we should avoid running any\nDML / DDL using plpy.execute inside a UDF (the same is is probably true for\nplpgsql). I did that; sometimes it works, and sometimes it fails and PGXL\nenters a limbo state where you simply cannot drop a table even after\nrestarting all servers. The culprit turned out to be some orphan prepared\ntransactions on the data nodes locking certain tables to block anybody else\nfrom grabbing exclusive locks (hence DROP TABLE hangs). I had to use\ncommands like these to fix the DB:\nfor p in 3001 3002 3003; do psql -p $p spouses -c 'select * from\npg_prepared_xacts'; done\nfor p in 3001 3002 3003; do psql -p $p spouses -c \"rollback prepared\n'_\\$XC\\$3220'\"; done\nHere is a related trick for releasing monitoring spurious locks:\nhttp://stackoverflow.com/questions/1063043/how-to-release-possible-postgres-row-locks\nNext time PGXL misbehaves, try those to salvage it.\nOn Mon, Jun 15, 2015 at 3:17 PM, Jaeho Shin notifications@github.com\nwrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann Yes, maybe my\nenvironment is somehow interfering? I'd like to cross verify it's working.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/321#issuecomment-112226579\n.\n. I did some experiments which suggest PGXL unload performance is really\ngood! I'm consistently getting 200MB/sec, which is the max I/O performance\nfor the machine.\n\nHowever, I also did experience the slow unload performance Zifei has\nreported on a different machine (with the same specs). This must be a PGXL\nbug or configuration issue.\nWhat is really slow, however, is loading (25MB/sec) which is CPU-bound and\nnot parallel.\nalldefector: PGXL does support UNLOGGED and we've been using it.\nOn Sat, Aug 29, 2015 at 9:54 PM, Feiran Wang notifications@github.com\nwrote:\n\nYes, basically our idea is to add parameters from the user to deepdive,\nand perform local join and dumping.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/372#issuecomment-136084999\n.\n. Loading into UNLOGGED table: 100MB/sec\nLoading into LOGGED table: 21MB/sec\n\nPostgres 9.5 includes a new command\nALTER TABLE ... SET LOGGED;\nBut there does not seem to be an option to convert unlogged to logged in\n9.2/PGXL.\nOn Tue, Sep 1, 2015 at 6:25 PM, alldefector notifications@github.com\nwrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann : what throughput\ndo you get for loading with UNLOGGED? Did you find a work-around to turn\noff UNLOGGED after loading?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/372#issuecomment-136909441\n.\n. I created a little script that does parallel unload from PGXL, runs a\ncommand, then does parallel loading. It reads the port numbers of the\nrunning PGXL nodes from the coordinator.\n\nhttps://gist.github.com/raphaelhoffmann/916a97435801d438c919\nThis should be fast, but I am still planning to do a more careful\nperformance analysis.\nOne more thing: On Greenplum, I noticed one can also get a load time\nimprovement by running multiple \\COPY in parallel \u2013 however, on raiders4\nthere's a sweet spot which is somewhere between 2 and 9 parallel processes.\nAfter that, you see a decrease in performance, likely due to cache misses.\nOn Tue, Sep 1, 2015 at 6:42 PM, Raphael Hoffmann <raphael.hoffmann@gmail.com\n\nwrote:\nLoading into UNLOGGED table: 100MB/sec\nLoading into LOGGED table: 21MB/sec\nPostgres 9.5 includes a new command\nALTER TABLE ... SET LOGGED;\nBut there does not seem to be an option to convert unlogged to logged in\n9.2/PGXL.\nOn Tue, Sep 1, 2015 at 6:25 PM, alldefector notifications@github.com\nwrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann : what throughput\ndo you get for loading with UNLOGGED? Did you find a work-around to turn\noff UNLOGGED after loading?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/372#issuecomment-136909441\n.\n. dd_query_f_ismassageparlorad_features (38GB) is distributed by \"ismassageparlorad.id\".\ndd_weights_f_ismassageparlorad_features (18MB!) is distributed by REPLICATION.\n\n\nThis join should be trivial, no matter how the first table is distributed.\nWe also can't re-distribute the table for every join, because re-distribution is very expensive. I'm currently re-distributing the first table by feature, but the process is CPU-bound with only the coordinator showing activity (100% CPU).\nMaybe I'm using too many nodes (32).\n. I tried re-running the query after redistributing the first table.\nIt's running now on our Azure instance using PGXL at a rate of 3.8MB/sec (this doesn't include the time for redistribution which itself took maybe an hour or more).\nI also checked my logs to see how a corresponding query executed on Greenplum on raiders4. There, the rate is 1.8MB/sec (but we don't do redistribution). \nNot sure why this takes so long using either database system. The data is so small that it could easily fit in memory.  \nalldefector \u2013\u00a0when you get a chance can you log into the machine and see if there's an easy fix? Thanks.\n. Here are the query plans:\nGreenplum (after ANALYZE):\nGather Motion 48:1  (slice2; segments: 48)  (cost=106201580.45..115136401.10 rows=11701014 width=24)\n   ->  HashAggregate  (cost=106201580.45..115136401.10 rows=11701014 width=24)\n         Group By: t0.id, t1.id, t0.\"rates.id\"\n         ->  Hash Join  (cost=13119111.20..98670881.40 rows=11701014 width=24)\n               Hash Cond: t0.feature = t1.feature\n               ->  Seq Scan on dd_query_rates_features t0  (cost=0.00..7913286.40 rows=11701014 width=39)\n               ->  Hash  (cost=5525981.00..5525981.00 rows=10945052 width=37)\n                     ->  Broadcast Motion 48:48  (slice1; segments: 48)  (cost=0.00..5525981.00 rows=10945052 width=37)\n                           ->  Seq Scan on dd_weights_rates_features t1  (cost=0.00..162905.52 rows=228022 width=37)\nPGXL (after redistribution and ANALYZE):\nUnique  (cost=300927152.97..315138286.41 rows=1421113344 width=24)\n   ->  Remote Subquery Scan on all (data1,data10,data11,data12,data13,data14,data15,data16,data17,data18,data19,data2,data20,data21,data22,data23,data24,data25,data26,data27,data28,data29,data3,data30,data31,data32,data4,data5,data6,data7,data8,data9)  (cost=300927152.97..304479936.33 rows=1421113344 width=24)\n         ->  Unique  (cost=300927152.97..304479936.33 rows=1421113344 width=24)\n               ->  Sort  (cost=300927152.97..304479936.33 rows=1421113344 width=24)\n                     Sort Key: t0.id, t1.id, t0.\"ismassageparlorad.id\"\n                     ->  Hash Join  (cost=9161.86..55742915.54 rows=1421113344 width=24)\n                           Hash Cond: (t0.\"ismassageparlorad.feature\" = t1.\"ismassageparlorad.feature\")\n                           ->  Seq Scan on dd_query_f_ismassageparlorad_features_x t0  (cost=0.00..23758703.44 rows=1421113344 width=22)\n                           ->  Hash  (cost=6040.27..6040.27 rows=249727 width=70)\n                                 ->  Seq Scan on dd_weights_f_ismassageparlorad_features t1  (cost=0.00..6040.27 rows=249727 width=70)\nThe unique is unnecessary and it is dropped if you remove DISTINCT from the query; when executing, I ran without DISTINCT, in which case I got the following query plan:\nRemote Subquery Scan on all (data1,data10,data11,data12,data13,data14,data15,data16,data17,data18,data19,data2,data20,data21,data22,data23,data24,data25,data26,data27,data28,data29,data3,data30,data31,data32,data4,data5,data6,data7,data8,data9)  (cost=9161.86..55742915.54 rows=1421113344 width=24)\n   ->  Hash Join  (cost=9161.86..55742915.54 rows=1421113344 width=24)\n         Hash Cond: (t0.\"ismassageparlorad.feature\" = t1.\"ismassageparlorad.feature\")\n         ->  Seq Scan on dd_query_f_ismassageparlorad_features_x t0  (cost=0.00..23758703.44 rows=1421113344 width=22)\n         ->  Hash  (cost=6040.27..6040.27 rows=249727 width=70)\n               ->  Seq Scan on dd_weights_f_ismassageparlorad_features t1  (cost=0.00..6040.27 rows=249727 width=70)\n. @SenWu Is DISTINCT necessary for this query?\n. Perhaps it has to do with the volume /lfs/local/0 being 99% full. We may have to ask people again to free up space.\n. The second phase of sequence assignment just finished, and I want to let it continue (raiders4, DB is memex_mar2015_large). I'll let you know the next time time I'm running sequence assignment.\n. Sure! The log of the run is here:\n/lfs/local/1/raphaelh/dd/new/deepdive/log/2015-09-06T093453.txt\nDatabase is memex_mar2015_large and it's running inference only. I launched\nthis job at 9:30AM and it still hasn't reached the sampling phase.\nThanks!\nOn Sun, Sep 6, 2015 at 5:28 PM, SenWu notifications@github.com wrote:\n\nThanks! Could you share the log of your run to us?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138140054\n.\n. Ce: Job is no longer running. It may have to do with the disk being too full.\nOn Sep 6, 2015, at 9:08 PM, zhangce notifications@github.com wrote:\nThe finger print of tuples in GP is a little weird (it is too physical in\nthe sense that there are no guarantee of when it will not change at all)...\nneed to do more profiling after Raphael's current run. That function should\nbe faster than 2MB/s if the DB is working properly.\nOn Sep 6, 2015 9:04 PM, \"alldefector\" notifications@github.com wrote:\n\nOr if the ID fields are used as conceptual foreign keys in the DB, maybe we\ncould use a fingerprint of the tuple.\nOn Sun, Sep 6, 2015 at 8:43 PM Feng Niu niufeng14@gmail.com wrote:\n\nCorrect me if I'm wrong -- looks like all those post-hoc-assigned\nsequence\nIDs are always used as array indexes in the sampler and never for SQL\njoins. If that's true, what are the challenges to do without the sequence\nassignment steps?\nFeng\nOn Sun, Sep 6, 2015 at 5:43 PM Raphael Hoffmann \nnotifications@github.com\nwrote:\n\nSure! The log of the run is here:\n/lfs/local/1/raphaelh/dd/new/deepdive/log/2015-09-06T093453.txt\nDatabase is memex_mar2015_large and it's running inference only. I\nlaunched\nthis job at 9:30AM and it still hasn't reached the sampling phase.\nThanks!\nOn Sun, Sep 6, 2015 at 5:28 PM, SenWu notifications@github.com wrote:\n\nThanks! Could you share the log of your run to us?\n\u2014\nReply to this email directly or view it on GitHub\n<\n\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138140054\n\n.\n\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138144569\n.\n\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/379#issuecomment-138178468\n.\n\u2014\nReply to this email directly or view it on GitHub.\n. We restarted greenplum (thanks, @SenWu !), free'd up some space, and re-ran this process. The log of the new run is here:\n\n\n/lfs/local/1/raphaelh/dd/new/deepdive/log/2015-09-06T093453.txt  \nWe are still seeing the same runtimes (3h for sequence assignment of rates, 2 or 3h for sequence assignment of features). I didn't yet run the CREATE TABLE command Ce suggested, but I suspect that the runtime is indeed normal.\n@zhangce: I think the changes you propose sound great! Any chance you can make a first stab at it? \nThanks!\n. I ran VACUUM last night and then restarted the job. The time for sequence assignment on \"rates\" went down from 3h to 2:18h. I haven't checked the time for sequence assignment on the feature table yet.\nSo, it's a small improvement, but still sequence assignment is a major bottleneck.\n. Assigning the IDs when writing the table the first time seems to be the right thing to do. But \u2013 I thought that the reason why we have stored procedures for sequence assignment is because we found that the sequence generators were too slow (?)\nIf the sequence generator turns out to be a bottleneck, it's likely because it's syncing after every row. In that case we could create a stored procedure that does the insert in batches...\n. @zhangce. Sounds good, we can re-run the experiment once raiders4 is working properly again.\nHere are a few more measured runtimes for another extractor on an AWS r3.8xlarge instance, using a RAID over the local SSD volumes for I/O. I/O throughput is very good on that machine (600-700MB/sec).\n```\n3:00h     running extractor\n1:40h     SELECT copy_table_assign_ids_replace('public', 'dd_query_f_isoutcall_features', 'id', 0)\n3:00h     COPY (SELECT DISTINCT t0.id AS factor_id, t1.id AS weight_id,  \"isoutcall.id\"\n           FROM dd_query_f_isoutcall_features t0, dd_weights_f_isoutcall_features t1\n           WHERE  t0.\"isoutcall.feature\" = t1.\"isoutcall.feature\") TO STDOUT;\n1:45h     CREATE UNLOGGED TABLE dd_isoutcall_vtype AS SELECT t0.id, CASE WHEN t2.variable_id IS NOT NULL AND is_outcall IS NOT NULL THEN 2\n                           WHEN t1.variable_id IS NOT NULL THEN 0\n                           WHEN is_outcall IS NOT NULL THEN 1\n                           ELSE 0\n                      END as dd_variable_type\n          FROM isoutcall t0 LEFT OUTER JOIN dd_graph_variables_holdout t1 ON t0.id=t1.variable_id\n          LEFT OUTER JOIN dd_graph_variables_observation t2 ON t0.id=t2.variable_id\n0:45h     Unloading factor graph to disk (tobinary)\n0:30h     Sampler loading factor graph until crash due to out of memory\n```\n. I see. @SenWu has also created a pull request that gets rid of the DISTINCT in the third query. That simplifies the query plan, getting rid of a sort, speeding this query up. \n. @zhangce I'm done with my processing work, so this might be a good time to investigate the issues on raiders4. Both seem to have the same configuration with two NAS volumes:\n/lfs/local/0  (~15TB)\n/lfs/local/1  (~32TB)\nI measured sequential write I/O (using dd if=/dev/zero of=/lfs/local/1/raphaelh/xxx bs=64k count=131072 conv=fsync) and found that the speed for /lfs/local/0 is about 100MB/sec and for /lfs/local/1 is about 150MB/sec. These numbers are the same for raiders3 and raiders4. Since /lfs/local/1 is much larger, and thus likely contains more disks, I expect a larger difference for random I/O.\nOn raiders3, greenplum uses /lfs/local/1 for storage, on raiders4, greenplum uses /lfs/local/0 for storage. That could explain some of the difference in performance. Also, the volume is still 96% full on raiders4 (used to be 99%) and there might be fragmentation. \nSo, I think the TODOs to fix raider4 are:\n1. move greenplum to /lfs/local/1.\n2. make sure no disk is more than 90% full.\n@SenWu Would it be possible to move greenplum over?\n. Since this thread discusses multiple topics, we decided to close this issue and open separate new issues. We will also continue our investigation of hardware issues offline.\n. > Does the converter or the python script print out logs that are not\n\nredirected by Java? Those logs would be useful. Maybe generate the input\nfile and halt DD and then run the converter manually so we can see the\nlogs.\n\nThat sounds good \u2013 I'll run this overnight.\n. OK. It's restarted. In both cases when I got this error, I also had to later start a new shell, since simply re-running DD threw the following error:\n./run.sh\njava.io.IOException: Permission denied\n        at java.io.UnixFileSystem.createFileExclusively(Native Method)\n        at java.io.File.createNewFile(File.java:1012)\n        at xsbt.boot.Locks$.apply0(Locks.scala:35)\n        at xsbt.boot.Locks$.apply(Locks.scala:28)\n        at xsbt.boot.Launch.locked(Launch.scala:178)\n        at xsbt.boot.Launch.app(Launch.scala:93)\n        at xsbt.boot.Launch.app(Launch.scala:91)\n        at xsbt.boot.Launch$.run(Launch.scala:51)\n        at xsbt.boot.Launch$$anonfun$explicit$1.apply(Launch.scala:45)\n        at xsbt.boot.Launch$.launch(Launch.scala:65)\n        at xsbt.boot.Launch$.apply(Launch.scala:16)\n        at xsbt.boot.Boot$.runImpl(Boot.scala:32)\n        at xsbt.boot.Boot$.main(Boot.scala:21)\n        at xsbt.boot.Boot.main(Boot.scala)\nError during sbt execution: java.io.IOException: Permission denied\n. @SenWu I see \u2013 but I don't understand why this is happening. What is\ncausing this?\nOn Sun, Sep 6, 2015 at 10:58 PM, SenWu notifications@github.com wrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann You lost permission\nwhen you re-run DD. You can either re-login or use reauth command to get\nauthenticated access.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/381#issuecomment-138196644\n.\n. Not sure what it's trying to create in /afs, but you're right: It appears\nto be some Stanford security policy. Thanks.\n\nOn Sun, Sep 6, 2015 at 11:14 PM, SenWu notifications@github.com wrote:\n\n@raphaelhoffmann https://github.com/raphaelhoffmann It might be your DD\ninstallation. Did you install your DD/sbt in afs? If all your installations\n(sbt, DD) are in lfs, then everything will be fine.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/381#issuecomment-138199029\n.\n. This issue is resolved. Thanks for the helpful tip, @SenWu !\n\nAlthough I didn't have sbt installed locally, I was using the sbt launcher in the DeepDive directory sbt/sbt \"run ...\".  Without further arguments, it puts ivy packages and lock files into the user's home directory. The errors went away when explicitly instructing sbt to use different paths:\njava -Dsbt.global.base=/lfs/local/1/raphaelh/.sbt -Dsbt.ivy.home=/lfs/local/1/raphaelh/.ivy2 -Divy.home=/lfs/local/1/raphaelh/.ivy2 -jar sbt/sbt-launch.jar \"run -c $DIRNAME/deepdive.conf -o $TMP_DIR\"\nI think it would make sense to set these paths automatically in the DeepDive launch script, or perhaps at least warn/document somewhere, since many of us are running DeepDive on Stanford machines and will inevitably run into this problem when running on larger datasets. \n. Yes, some more documentation would be helpful here. Here's why I think that DISTINCT is not necessary. Our query is \nhttps://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala#L719\nClaim 1: For every tuple in the left table, there is at most one join tuple.\nThe join condition is a key for the right table. \nhttps://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala#L710\nhttps://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala#L688\nhttps://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala#L579\nClaim 2: Each join tuple is distinct.\nThe join selects factor_id which is a key for the left table.\nhttps://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala#L563\n. There's another problem with re-using a weight table: The same query is quite slow when the weight table is large (which is the case for all our HT extractors). However, after adding an index on the feature column, it is running quite well.\nI suggest we also update the documentation, recommending an index on the feature column when re-using a weight table.\n. We just need a simple way to re-run deepdive run, for example after you made some changes to a udf.\nIn that case you need to reset the generated tables. However, you can't use deepdive initdb because it will clear the entire database. It is also tedious and error-prone to use deepdive sql because you can easily forget a table. \n. @feiranwang That's exactly right. I just a need a simple solution for the short term. Is there a way to truncate in ddlog?\nI understand that this more imperative way doesn't fit well with the declarative nature of datalog, but the same may be true for \"+=\"...\n. Yes. Some command like deepdive clear which checks app.ddlog for all tables and then truncates each one would be handy. It might make sense to ask the user for confirmation.\n. To add to @alldefector's bullets: We now create one database per extractor,\nbecause it takes too long to run multiple extractors at once and we need to keep all\nintermediate output. Our problem now is loading the sentences table (264G, > 1h),\nwhich we wouldn't have with schema support.\n@netj @zhangce We did a bunch of experiments around @zhangce 's idea of using separate schemas,\nand have a preliminary integration of schema support in DeepDive. Support for schemas\ndoes require a few changes to the code base, so it would be great to discuss how we want\nto proceed.\n1. Specifying the search_path in the DB connection string only works with newer\n   versions of Postgresql, not Greenplum. It is possible, however, to run SQL on GP:\n   SET search_path TO dd_run_0003, public;. Also note: Most RDBMSs have schema\n   support, but not MySQL.\n2. The connection pool used by DeepDive (DBCP 1.x) does not support schemas.\n   Fortunately, it is easy to change the connection pool.\nIn build.sbt I added a dependency to\n\"org.apache.commons\" % \"commons-dbcp2\" % \"2.1.1\"\nand in deepdive.conf I added \ndeepdive.db.default {\n      poolFactoryName: \"commons-dbcp2\"\n      ...\n   }\nNo other change was necessary for this.\n3. We now need to create the schema when we run deepdive initdb. I think we\n   should not let this command drop and create the entire DB; it should\n   just create a new schema for the next run. In util/deepdive-initdb, I added\ndeepdive-sql \"CREATE SCHEMA $SCHEMA_NAME;\"\n4. Finally, we need to make sure that JDBC uses the schema when\n   creating a connection. Unfortunately, the default search_path on\n   the database (ALTER DATABASE ... SET search_path = ...)\n   is ignored by JDBC, and it doesn't look like scalikejdbc supports\n   schemas or SQL queries for connection initialization. As a workaround, \n   I am now setting the schema in JdbcDataStore:\ndef borrowConnection() : Connection = {\n    val c = ConnectionPool.borrow()\n    c.setSchema(SCHEMA_NAME)\n    c\n   }\nSo, this this might not be the most elegant solution, but works. Any thoughts on whether we\nwant something like this and on how to make the integration better?\n. I added a branch with all the necessary changes.\nhttps://github.com/HazyResearch/deepdive/compare/84898b1c9b9337a64057c521def5008b34d8913f...schema_support\nIf you build this and then, in your DD app, run\nexport DEEPDIVE_SCHEMA=my\ndeepdive initdb\nthen all generated tables will be in schema my. You can verify this in psql by running\nSET search_path = my,public;\n\\d\n. We are now using schemas for our DeepDive app. It works well, and saves us a lot of time.\nIf you have access to our repo, you can see how we are now launching the extractors.\nSeparate commands for the schemas would be nice, but are not absolutely necessary.\nSomething that would be very useful though is a DDLOG annotation that marks a table as a user table which DeepDive should assume exists and not try to drop and re-create. That way DeepDive can follow the search path to select the table from a different schema during execution.\nI absolutely agree about replacing JDBC with calls to \"deepdive sql\". Also, it doesn't make sense why we are using a connection pool, since we don't have many concurrent requests and it just adds complexity.\nIf JDBC was replaced with deepdive sql, no change to the scala code base would be necessary in order to support schemas, since \"deepdive sql\" observes the DB-level search_path.\n@netj How difficult do you think it would be to replace JDBC?\n. @zhangce @SenWu @feiranwang Can you show us a few lines of ddlog code that does this copying of supervised examples? Or should we already do it inside our UDFs?\n. Thanks! That works! @alldefector I think we can close this issue.\n. +1. Turning it on by default (and supporting switch --disable_evidence_sampling) would also allow us to avoid having to edit deepdive.conf.\n. @zhangce I agree that there are different things you might want to do with DeepDive, and in some cases you do not want to sample evidence variables.\nWhen building extractors with noisy distant supervision rules, however, you likely do want to sample evidence variables. We may at least want to emphasize this setting in the tutorials and examples since some users may not be aware.\n. @SenWu Thanks! It looks like we ran into an issue with PGXL, not DDLOG. \nAfter restarting PGXL, the time for this query went down from 38 min to 3 min, and the query plans were good again. (We kept the old logs and query plans both before/after analyze in case they are useful.) Due to this and other issues, we decided to move our processing back to Greenplum.\n. Hi @netj, this new version of deepdive is really great! Here are a few notes I took while experimenting.\n1. When building on OSX, I got error that X11/Xlib.h couldn't be found. This error\n   was thrown by buildkit module for graphviz.\n/usr/include/tk.h:78:23: fatal error: X11/Xlib.h: No such file or directory\nFollowing these instructions, I fixed it by adding a symlink\nln -s /opt/X11/include/X11 /usr/local/include/X11\nAfterwards, it was not sufficient to rerun 'make'. I manually cd'ed to deepdive/depends/bundled/graphviz/graphviz-2.38.0 and ran ./configure && make && make install. Then I could run \"make\" in deepdive\n2. ddlog apps didn't work with the default submodule commit, but \u2013 following your\n   suggestions \u2013 everything worked fine after upgrading to ddlog commit HazyResearch/ddlog@ee0b359c9f018c19df5eddbf3bff60bdf076e091.\nNow everything works fine, but I've encountered two different problems in a small number of runs:\n1. Sometimes, stdin is no longer showing on the screen after a successful deepdive do\n   execution. That is, I can type commands and they execute, but the letters I type are not\n   being shown on the screen. I'm using iterm2.\n2. Sometimes (rarely), a mkmimo process kept running at 100% cpu usage, after deepdive do \n   terminated with an error (possibly related to missing input data).\nI'll try to investigate more carefully if I encounter these again. \n. @feiranwang @netj Wow, thanks guys, for fixing this so quickly!\n. I noticed when using deepdive.conf, but I think the issue applies to ddlog as well.\n. @netj Thanks for looking at this. I'm on commit 802ec0d5284704c1b8bdd71c8a788758553d19ef and am trying to run the chunking example. Let me send you the logs. \n. @netf found the problem: there are some conditions missing in a query in the example, see here https://github.com/HazyResearch/deepdive/blob/docathon-v0.8/examples/chunking/app.ddlog#L38 \nI added [word_id1 = word_id2 + 1], word1 != NULL, but am still investigating why results are not the same as with old chunking example.\n. Problem is now fixed with https://github.com/HazyResearch/deepdive/pull/485\n. This works really great! Ready to merge.\n. Wow, thanks for fixing this so quickly!\n. Great! That worked!\n. Tested fix on the chunking example, but I'm still getting the same problem. The sequence assignment output in the log is now\n2016-02-21 10:27:33.852620 + deepdive db assign_sequential_id tag id 0\n2016-02-21 10:27:33.908521 NOTICE:  language \"plpgsql\" already exists, skipping\n2016-02-21 10:27:33.908568 CREATE LANGUAGE\n2016-02-21 10:27:34.194183 CREATE LANGUAGE\n2016-02-21 10:27:34.410781 NOTICE:  table \"tmp_gpsid_count\" does not exist, skipping\n2016-02-21 10:27:34.410909 CONTEXT:  SQL statement \"drop table if exists tmp_gpsid_count cascade;\"\n2016-02-21 10:27:34.410954 PL/pgSQL function \"fast_seqassign\" line 2 at execute statement\n2016-02-21 10:27:34.410984 NOTICE:  table \"tmp_gpsid_count_noagg\" does not exist, skipping\n2016-02-21 10:27:34.411008 CONTEXT:  SQL statement \"drop table if exists tmp_gpsid_count_noagg cascade;\"\n2016-02-21 10:27:34.411035 PL/pgSQL function \"fast_seqassign\" line 3 at execute statement\n2016-02-21 10:27:35.037462 NOTICE:  EXECUTING _fast_seqassign()...\n2016-02-21 10:27:35.703437  fast_seqassign\n2016-02-21 10:27:35.703535 ----------------\n2016-02-21 10:27:35.703555\n2016-02-21 10:27:35.703572 (1 row)\n2016-02-21 10:27:35.703588\n2016-02-21 10:27:35.704557 + deepdive db generate_series dd_categories_tag category 0 12\n2016-02-21 10:27:35.980159 CREATE VIEW\n2016-02-21 10:27:35.981578 mark_done process/grounding/variable/tag/assign_id\nSo, the fix removed the error message at this point, but we later still crash with:\n2016-02-21 10:57:45.739665 sampler-dw.bin: src/dstruct/factor_graph/factor_graph.cpp:375: void dd::FactorGraph::safety_check(): Assertion `this->weights[i].id == i' failed.\n2016-02-21 10:57:45.914532 process/model/learning/run.sh: line 22: 135592 Aborted                 (core dumped) sampler-dw gibbs -w <(flatten factorgraph/weights) -v <(flatten factorgraph/variables) -f <(flatten factorgraph/factors) -m factorgraph/meta -o weights -l 1000 -s 1 -i 1000 --alpha 0.01 --sample_evidence\nThe following query shows that there are still duplicate IDs in the database:\n```\nselect count(distinct id) from dd_weightsmulti_inf_istrue_tag;\n\n20676\n(1 row)\nselect count(*) from dd_weightsmulti_inf_istrue_tag;\n count\n\n268632\n(1 row)\n```\nIn the case of postgres, the count is 268632 in both cases.\n. Computing the skip-chain factors in a python UDF now.\n. We are using deepdive sql eval ... format=json to export data from greenplum to python. For integer and floating point columns, however, this export only worked for fields that did not contain NULL values. With this fix, NULL values get translated to python's None value.\n. ",
    "billgreenwald": "I am running into this error too, both on the JDBC removal and normal branches.  Has this been resolved/is there a workaround?\n. I am running into this error too, both on the JDBC removal and normal branches.  Has this been resolved/is there a workaround?\n. ",
    "nlothian": "Master\n. It's pretty long. See http://pastebin.com/un3A01j5\n. Thanks. That's looking better.\n. gnuplot is also a dependency (though it doesn't seem to fail without it?)\n. Master\n. It's pretty long. See http://pastebin.com/un3A01j5\n. Thanks. That's looking better.\n. gnuplot is also a dependency (though it doesn't seem to fail without it?)\n. ",
    "tiangolo": "I'm glad to help!\n. I'm glad to help!\n. ",
    "robinjia": "Chiming in because this bug affected me too...why don't we create a test corresponding to brahmaneya's example?  Or more generally some sort of latent variable model (maybe a simple HMM?) where every edge is connected to at least one node that is not observed.\n. Chiming in because this bug affected me too...why don't we create a test corresponding to brahmaneya's example?  Or more generally some sort of latent variable model (maybe a simple HMM?) where every edge is connected to at least one node that is not observed.\n. ",
    "coveralls": "\nChanges Unknown when pulling 6c6a0158930f8a526179c4165477da9d2c4d146b on sen-coverage into * on develop*.\n. \nChanges Unknown when pulling c3861753fac6012ed85196a03cf5c06bc9444976 on sen-coverage into * on develop*.\n. \nChanges Unknown when pulling f658718d05ddf5a7300f8ab397924725ea4fcfc8 on sen-coverage into * on develop*.\n. \nChanges Unknown when pulling f658718d05ddf5a7300f8ab397924725ea4fcfc8 on sen-coverage into * on develop*.\n. \nCoverage remained the same at 70.43% when pulling 6f295fe207f1a7c71ec51da9d790a78b03b9a7a5 on dark-data-edits into 77377faeb62be257aaf830ddb5e41f0ca137a18b on master.\n. \nCoverage remained the same at 70.43% when pulling 347971eb4cc34bf5f020f22bb23eb3ff60ad80c1 on dark-data-edits into 0a5457e6f3576dcda5e432cfc2a346b4cc93a026 on master.\n. \nCoverage increased (+0.01%) to 78.39% when pulling 01c273ebb2f225b7f16b5f74e98271dd735a998b on raphael-pgxl into b3a0b40d17f05fb34b040324211507d37d9cb6e5 on develop.\n. \nCoverage increased (+0.01%) to 78.39% when pulling 01c273ebb2f225b7f16b5f74e98271dd735a998b on raphael-pgxl into b3a0b40d17f05fb34b040324211507d37d9cb6e5 on develop.\n. \nCoverage increased (+0.01%) to 78.39% when pulling 6714584f26cf046840eb10629ce607239cd688e4 on raphael-pgxl into b3a0b40d17f05fb34b040324211507d37d9cb6e5 on develop.\n. \nCoverage decreased (-0.18%) to 78.2% when pulling 5634f1028220ea98299ddcb81367cfe2fe924fb9 on raphael-pgxl into b3a0b40d17f05fb34b040324211507d37d9cb6e5 on develop.\n. \nCoverage decreased (-0.18%) to 78.2% when pulling 817ab443bd3b1aa9ba20c1ea90a86c9b41334a51 on raphael-pgxl into b3a0b40d17f05fb34b040324211507d37d9cb6e5 on develop.\n. \nCoverage decreased (-0.18%) to 78.2% when pulling 97f28e58f93bd43e62dd11784dd00b34315c7989 on raphael-pgxl into b3a0b40d17f05fb34b040324211507d37d9cb6e5 on develop.\n. \nCoverage remained the same at 78.53% when pulling 91ceea70f08167838c64ad76510386066ac43ca9 on doc into 9b1425b37daf9edcb9ef60a45860072dc466791d on develop.\n. \nCoverage remained the same at 78.53% when pulling 91ceea70f08167838c64ad76510386066ac43ca9 on doc into 9b1425b37daf9edcb9ef60a45860072dc466791d on develop.\n. \nCoverage remained the same at 78.53% when pulling 79a8dabd8d028e8dcf5d8b89265c47e9b9140a1f on update_samplers into 948fa395ff32a81478eeff221ac8726060ef5904 on develop.\n. \nCoverage remained the same at 78.53% when pulling 79a8dabd8d028e8dcf5d8b89265c47e9b9140a1f on update_samplers into 948fa395ff32a81478eeff221ac8726060ef5904 on develop.\n. \nCoverage increased (+0.35%) to 70.77% when pulling 364bf0224462432ec98d75d965c9ee4d869b8b74 on dark-data-banner into 88b37d31bebcd5cb3bf7c91551a44956762f19f5 on master.\n. \nCoverage increased (+0.35%) to 70.77% when pulling 2e716756ed0e827e6e97e754a9bfe0c1e3734e69 on dark-data-banner into 88b37d31bebcd5cb3bf7c91551a44956762f19f5 on master.\n. \nCoverage increased (+0.35%) to 70.77% when pulling 2e716756ed0e827e6e97e754a9bfe0c1e3734e69 on dark-data-banner into 88b37d31bebcd5cb3bf7c91551a44956762f19f5 on master.\n. \nCoverage increased (+0.35%) to 70.77% when pulling d4c1e75cd9cae7a38dfcbfa4bbddef812572b75d on hotfix-docs-variable into 88b37d31bebcd5cb3bf7c91551a44956762f19f5 on master.\n. \nCoverage remained the same at 70.43% when pulling d670c238bf238add149319dce93e244c3bd97915 on hotfix-docs-variable into 88b37d31bebcd5cb3bf7c91551a44956762f19f5 on master.\n. \nCoverage remained the same at 78.53% when pulling 79186676dc93a8c6dec51c911bfa149e4e6d8215 on hotfix-develop-docs-skip_learning into 948fa395ff32a81478eeff221ac8726060ef5904 on develop.\n. \nCoverage remained the same at 78.53% when pulling 79186676dc93a8c6dec51c911bfa149e4e6d8215 on hotfix-develop-docs-skip_learning into 948fa395ff32a81478eeff221ac8726060ef5904 on develop.\n. \nCoverage remained the same at 78.53% when pulling eb88894a09585711438a39073a0abfafb0c9ad8e on hotfix-develop-docs-skip_learning into 948fa395ff32a81478eeff221ac8726060ef5904 on develop.\n. \nCoverage remained the same at 78.53% when pulling 254b94f63116f5a5877a56a7251dc8e88892ef9b on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage remained the same at 78.53% when pulling e5e303a9ce6ac2667ab85504301f2b9c6afcd1f5 on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage remained the same at 78.53% when pulling 017b363a18575b039f094f79881f47090a8340fe on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage remained the same at 78.53% when pulling 93dd3c2dfd3e51e182f16014009d20ca269e12c7 on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage remained the same at 78.53% when pulling a56e4993a2243f9f07a2a0c2969d748f462a19a4 on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage remained the same at 78.53% when pulling a56e4993a2243f9f07a2a0c2969d748f462a19a4 on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage remained the same at 78.53% when pulling a56e4993a2243f9f07a2a0c2969d748f462a19a4 on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage remained the same at 78.53% when pulling a56e4993a2243f9f07a2a0c2969d748f462a19a4 on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage remained the same at 78.53% when pulling 7d072db5bed863bab94af9f29d72e507086722e4 on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage increased (+1.06%) to 79.58% when pulling bfbe1b3073a454020cbfe3d31f7fa13b1f716453 on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage increased (+1.06%) to 79.58% when pulling e52ba0e6b2bbb8fd6bc0cf78191cee441c2fb76c on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage increased (+1.06%) to 79.58% when pulling e52ba0e6b2bbb8fd6bc0cf78191cee441c2fb76c on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nChanges Unknown when pulling 229211d8e2e7e2a4b06adb56449a6fef5de34e89 on raphael-pgxl-multinomial into * on develop*.\n. \nChanges Unknown when pulling db797771c179b80cc361d62f4abdcb709f378698 on update_samplers into * on develop*.\n. \nChanges Unknown when pulling db797771c179b80cc361d62f4abdcb709f378698 on update_samplers into * on develop*.\n. \nChanges Unknown when pulling db797771c179b80cc361d62f4abdcb709f378698 on update_samplers into * on develop*.\n. \nChanges Unknown when pulling e6f58cc89f0a9949baac490bb723c2861ccb507b on update_samplers into * on develop*.\n. \nChanges Unknown when pulling 6c6a0158930f8a526179c4165477da9d2c4d146b on sen-coverage into * on develop*.\n. \nChanges Unknown when pulling c3861753fac6012ed85196a03cf5c06bc9444976 on sen-coverage into * on develop*.\n. \nChanges Unknown when pulling f658718d05ddf5a7300f8ab397924725ea4fcfc8 on sen-coverage into * on develop*.\n. \nChanges Unknown when pulling f658718d05ddf5a7300f8ab397924725ea4fcfc8 on sen-coverage into * on develop*.\n. \nCoverage remained the same at 70.43% when pulling 6f295fe207f1a7c71ec51da9d790a78b03b9a7a5 on dark-data-edits into 77377faeb62be257aaf830ddb5e41f0ca137a18b on master.\n. \nCoverage remained the same at 70.43% when pulling 347971eb4cc34bf5f020f22bb23eb3ff60ad80c1 on dark-data-edits into 0a5457e6f3576dcda5e432cfc2a346b4cc93a026 on master.\n. \nCoverage increased (+0.01%) to 78.39% when pulling 01c273ebb2f225b7f16b5f74e98271dd735a998b on raphael-pgxl into b3a0b40d17f05fb34b040324211507d37d9cb6e5 on develop.\n. \nCoverage increased (+0.01%) to 78.39% when pulling 01c273ebb2f225b7f16b5f74e98271dd735a998b on raphael-pgxl into b3a0b40d17f05fb34b040324211507d37d9cb6e5 on develop.\n. \nCoverage increased (+0.01%) to 78.39% when pulling 6714584f26cf046840eb10629ce607239cd688e4 on raphael-pgxl into b3a0b40d17f05fb34b040324211507d37d9cb6e5 on develop.\n. \nCoverage decreased (-0.18%) to 78.2% when pulling 5634f1028220ea98299ddcb81367cfe2fe924fb9 on raphael-pgxl into b3a0b40d17f05fb34b040324211507d37d9cb6e5 on develop.\n. \nCoverage decreased (-0.18%) to 78.2% when pulling 817ab443bd3b1aa9ba20c1ea90a86c9b41334a51 on raphael-pgxl into b3a0b40d17f05fb34b040324211507d37d9cb6e5 on develop.\n. \nCoverage decreased (-0.18%) to 78.2% when pulling 97f28e58f93bd43e62dd11784dd00b34315c7989 on raphael-pgxl into b3a0b40d17f05fb34b040324211507d37d9cb6e5 on develop.\n. \nCoverage remained the same at 78.53% when pulling 91ceea70f08167838c64ad76510386066ac43ca9 on doc into 9b1425b37daf9edcb9ef60a45860072dc466791d on develop.\n. \nCoverage remained the same at 78.53% when pulling 91ceea70f08167838c64ad76510386066ac43ca9 on doc into 9b1425b37daf9edcb9ef60a45860072dc466791d on develop.\n. \nCoverage remained the same at 78.53% when pulling 79a8dabd8d028e8dcf5d8b89265c47e9b9140a1f on update_samplers into 948fa395ff32a81478eeff221ac8726060ef5904 on develop.\n. \nCoverage remained the same at 78.53% when pulling 79a8dabd8d028e8dcf5d8b89265c47e9b9140a1f on update_samplers into 948fa395ff32a81478eeff221ac8726060ef5904 on develop.\n. \nCoverage increased (+0.35%) to 70.77% when pulling 364bf0224462432ec98d75d965c9ee4d869b8b74 on dark-data-banner into 88b37d31bebcd5cb3bf7c91551a44956762f19f5 on master.\n. \nCoverage increased (+0.35%) to 70.77% when pulling 2e716756ed0e827e6e97e754a9bfe0c1e3734e69 on dark-data-banner into 88b37d31bebcd5cb3bf7c91551a44956762f19f5 on master.\n. \nCoverage increased (+0.35%) to 70.77% when pulling 2e716756ed0e827e6e97e754a9bfe0c1e3734e69 on dark-data-banner into 88b37d31bebcd5cb3bf7c91551a44956762f19f5 on master.\n. \nCoverage increased (+0.35%) to 70.77% when pulling d4c1e75cd9cae7a38dfcbfa4bbddef812572b75d on hotfix-docs-variable into 88b37d31bebcd5cb3bf7c91551a44956762f19f5 on master.\n. \nCoverage remained the same at 70.43% when pulling d670c238bf238add149319dce93e244c3bd97915 on hotfix-docs-variable into 88b37d31bebcd5cb3bf7c91551a44956762f19f5 on master.\n. \nCoverage remained the same at 78.53% when pulling 79186676dc93a8c6dec51c911bfa149e4e6d8215 on hotfix-develop-docs-skip_learning into 948fa395ff32a81478eeff221ac8726060ef5904 on develop.\n. \nCoverage remained the same at 78.53% when pulling 79186676dc93a8c6dec51c911bfa149e4e6d8215 on hotfix-develop-docs-skip_learning into 948fa395ff32a81478eeff221ac8726060ef5904 on develop.\n. \nCoverage remained the same at 78.53% when pulling eb88894a09585711438a39073a0abfafb0c9ad8e on hotfix-develop-docs-skip_learning into 948fa395ff32a81478eeff221ac8726060ef5904 on develop.\n. \nCoverage remained the same at 78.53% when pulling 254b94f63116f5a5877a56a7251dc8e88892ef9b on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage remained the same at 78.53% when pulling e5e303a9ce6ac2667ab85504301f2b9c6afcd1f5 on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage remained the same at 78.53% when pulling 017b363a18575b039f094f79881f47090a8340fe on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage remained the same at 78.53% when pulling 93dd3c2dfd3e51e182f16014009d20ca269e12c7 on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage remained the same at 78.53% when pulling a56e4993a2243f9f07a2a0c2969d748f462a19a4 on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage remained the same at 78.53% when pulling a56e4993a2243f9f07a2a0c2969d748f462a19a4 on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage remained the same at 78.53% when pulling a56e4993a2243f9f07a2a0c2969d748f462a19a4 on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage remained the same at 78.53% when pulling a56e4993a2243f9f07a2a0c2969d748f462a19a4 on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage remained the same at 78.53% when pulling 7d072db5bed863bab94af9f29d72e507086722e4 on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage increased (+1.06%) to 79.58% when pulling bfbe1b3073a454020cbfe3d31f7fa13b1f716453 on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage increased (+1.06%) to 79.58% when pulling e52ba0e6b2bbb8fd6bc0cf78191cee441c2fb76c on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nCoverage increased (+1.06%) to 79.58% when pulling e52ba0e6b2bbb8fd6bc0cf78191cee441c2fb76c on netj-installers into 7523ac6122296d177ced1281806220c764f29943 on develop.\n. \nChanges Unknown when pulling 229211d8e2e7e2a4b06adb56449a6fef5de34e89 on raphael-pgxl-multinomial into * on develop*.\n. \nChanges Unknown when pulling db797771c179b80cc361d62f4abdcb709f378698 on update_samplers into * on develop*.\n. \nChanges Unknown when pulling db797771c179b80cc361d62f4abdcb709f378698 on update_samplers into * on develop*.\n. \nChanges Unknown when pulling db797771c179b80cc361d62f4abdcb709f378698 on update_samplers into * on develop*.\n. \nChanges Unknown when pulling e6f58cc89f0a9949baac490bb723c2861ccb507b on update_samplers into * on develop*.\n. ",
    "abresler": "Brew install postgresql\n. Clean reinstalled and got it working sorry about that!  Video or screen\ncast on setup could be super helpful though\n\nAlex Bresler\nabresler@asbcllc.com\n\u200bwww.asbcllc.com\u200b\n917-455-0239\u200b (cell)\u200b\nOn May 24, 2015 2:01 PM, \"chrismre\" notifications@github.com wrote:\n\ndid we get any closure on this? I think brew install doesn't automatically\ncreatedb appropriately, perhaps we can add a note about this in the docs?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/294#issuecomment-105043291\n.\n. You have no idea how amazing that would be, the wild west of sports data\nscience is eagerly awaiting, been talking up deepdive because from what\nI've been able to tell this could be a game changer in the space.\n\nKeep up the great work everyone.\n\nAlex Bresler\nabresler@asbcllc.com\n\u200bwww.asbcllc.com\u200b\n917-455-0239\u200b (cell)\u200b\nOn May 24, 2015 2:12 PM, \"chrismre\" notifications@github.com wrote:\n\nMakes sense! We are thinking of a few screencasts... we just have to wait\nfor summer for our day jobs to simmer down!\nOn Sun, May 24, 2015 at 11:10 AM, Alex Bresler notifications@github.com\nwrote:\n\nClean reinstalled and got it working sorry about that! Video or screen\ncast on setup could be super helpful though\n\nAlex Bresler\nabresler@asbcllc.com\n\u200bwww.asbcllc.com\u200b\n917-455-0239\u200b (cell)\u200b\nOn May 24, 2015 2:01 PM, \"chrismre\" notifications@github.com wrote:\n\ndid we get any closure on this? I think brew install doesn't\nautomatically\ncreatedb appropriately, perhaps we can add a note about this in the\ndocs?\n\u2014\nReply to this email directly or view it on GitHub\n<\n\nhttps://github.com/HazyResearch/deepdive/issues/294#issuecomment-105043291\n\n.\n\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/issues/294#issuecomment-105045505\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/294#issuecomment-105045587\n.\n. Brew install postgresql\n. Clean reinstalled and got it working sorry about that!  Video or screen\ncast on setup could be super helpful though\n\n\nAlex Bresler\nabresler@asbcllc.com\n\u200bwww.asbcllc.com\u200b\n917-455-0239\u200b (cell)\u200b\nOn May 24, 2015 2:01 PM, \"chrismre\" notifications@github.com wrote:\n\ndid we get any closure on this? I think brew install doesn't automatically\ncreatedb appropriately, perhaps we can add a note about this in the docs?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/294#issuecomment-105043291\n.\n. You have no idea how amazing that would be, the wild west of sports data\nscience is eagerly awaiting, been talking up deepdive because from what\nI've been able to tell this could be a game changer in the space.\n\nKeep up the great work everyone.\n\nAlex Bresler\nabresler@asbcllc.com\n\u200bwww.asbcllc.com\u200b\n917-455-0239\u200b (cell)\u200b\nOn May 24, 2015 2:12 PM, \"chrismre\" notifications@github.com wrote:\n\nMakes sense! We are thinking of a few screencasts... we just have to wait\nfor summer for our day jobs to simmer down!\nOn Sun, May 24, 2015 at 11:10 AM, Alex Bresler notifications@github.com\nwrote:\n\nClean reinstalled and got it working sorry about that! Video or screen\ncast on setup could be super helpful though\n\nAlex Bresler\nabresler@asbcllc.com\n\u200bwww.asbcllc.com\u200b\n917-455-0239\u200b (cell)\u200b\nOn May 24, 2015 2:01 PM, \"chrismre\" notifications@github.com wrote:\n\ndid we get any closure on this? I think brew install doesn't\nautomatically\ncreatedb appropriately, perhaps we can add a note about this in the\ndocs?\n\u2014\nReply to this email directly or view it on GitHub\n<\n\nhttps://github.com/HazyResearch/deepdive/issues/294#issuecomment-105043291\n\n.\n\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/HazyResearch/deepdive/issues/294#issuecomment-105045505\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/294#issuecomment-105045587\n.\n. \n",
    "minafarid": "Thank you!\n. You were right, it was an older version of deepdive (deepdive version results shown below).\n```\n\u279c  longtail git:(master) deepdive version\ndeepdive v0.8.0-79-g28a58de (Linux x86_64)\nInformation on this build of deepdive follows.\ndeepdive version: v0.8.0-79-g28a58de\n  deepdive Git commit: 28a58de9eaf92ef16c4777b007ef2d16cb7f0548\nBuild Date: 2016-04-26T15:21:44+00:00\n  Build Host: testing-worker-linux-docker-babe303d-3437-linux-5.prod.travis-ci.org\n  Build Operating System: Linux\n  Build Machine: x86_64\nRunning Operating System: Linux\n  Running Machine: x86_64\n\u279c  longtail git:(master) \n```\nI tried to install a new version of deepdive but it failed at stage after building most modules. \n\nI also tried to build it in a Docker container, but it gives me the following error:\n\nDo you have any recommendations, if you ran into this before?\n. Issue fixed. Application compiles and runs successfully. Thank you for your help.\n. Great!! I suggest this becomes a part of deepdive run.\n. Thank you!\n. You were right, it was an older version of deepdive (deepdive version results shown below).\n```\n\u279c  longtail git:(master) deepdive version\ndeepdive v0.8.0-79-g28a58de (Linux x86_64)\nInformation on this build of deepdive follows.\ndeepdive version: v0.8.0-79-g28a58de\n  deepdive Git commit: 28a58de9eaf92ef16c4777b007ef2d16cb7f0548\nBuild Date: 2016-04-26T15:21:44+00:00\n  Build Host: testing-worker-linux-docker-babe303d-3437-linux-5.prod.travis-ci.org\n  Build Operating System: Linux\n  Build Machine: x86_64\nRunning Operating System: Linux\n  Running Machine: x86_64\n\u279c  longtail git:(master) \n```\nI tried to install a new version of deepdive but it failed at stage after building most modules. \n\nI also tried to build it in a Docker container, but it gives me the following error:\n\nDo you have any recommendations, if you ran into this before?\n. Issue fixed. Application compiles and runs successfully. Thank you for your help.\n. Great!! I suggest this becomes a part of deepdive run.\n. ",
    "kconor": "No, I'm following http://deepdive.stanford.edu/doc/basics/walkthrough/walkthrough.html.\n. No, I'm following http://deepdive.stanford.edu/doc/basics/walkthrough/walkthrough.html.\n. ",
    "ajratner": "These ideas are all awesome!\nIn our dd-genomics repo (still pre-0.7.0) we just split our create_schema.sh utility script into two-\n- one for the \"input\" fields (e.g. the sentences table, anything else that might have been the result of expensive pre-processing)\n- one for everything else (stuff that would be re-computed each full run of dd anyway)\nThis has been good enough for us for the most part.  But fine grained control such as the above would obviously be even better\nIn particular I really like your first point/idea.  The schema file seems so absolutely central to a deepdive app that it really would be great to have it specified in a single file only (and even better, a json file :) )\n. For reference, in case of use to anyone, a simple solution with sed is\nbash\njava -jar ${DEEPDIVE_HOME}/util/ddlog.jar compile app.ddl | sed -e 's/^\\(.*\\)\\(style: \"tsv_extractor\"\\)/\\1\\2XXNXX\\1parallelism: 8/g' -e $'s/XXNXX/\\\\\\n/g' > deepdive.conf\n. Sorry, for future reference should I have put this somewhere else?\n. Okay cool\nTo clarify, for me this is not a big issue- for now I just changed the column names in deepdive.conf manually and is working fine\nGood point on Ce's example too- that is another easy way for me to temp fix.  So no rush from my end.  Thanks!\n. Closing- moving ddlog issues to ddlog repo.  Not yet re-opening this one there though, because from my end, defining these intermediate tables in the schema as you suggested- e.g. using separate tables rather than views- is a fine enough solution for now\n. Isn't this similar to a (slightly more simple-minded) issue I posted which seems to have been partly resolved: #351 ?\nTo chime in with my now tedious \"one time at band camp\"-style comments... One time at genomics camp, we also implemented a short term fix for this issue which was to have two versions of the \"initdb\" command equivalent, one which did and one which did not ever touch user data (again, see my comments in #351 )\nAgain, it was also simpler to have a TRUNCATE script run as a \"before\" command for every extractor in the pre-ddlog syntax\nIn my tables code in ddlog, I had a post-processing script which went through the ddlog-generated app.conf file, and added this before script command back in for every extractor...\n. Yeah sorry for not chiming in earlier, we ran into this problem in genomics and @feiranwang added the --sample_evidence flag for us pre-ddlog, so we had been using it\nConfirming that there is a simple way to incorporate this with ddlog would be great too!\n. > > 3 looks easy for the programs that mainly contain regressions. But what is 3's formal semantic in general factor graph? When you have many general correlations between variables, how should these copies been connected with other variables?\n@zhangce I think one of the big benefits of having the --sample-evidence flag is that users don't have to create copies, and we don't have to consider these issues.  In fact in the genomics code, before I was involved, I actually think there were some confusions due to having created copies- which is part of why I asked Feiran about making the sample evidence flag in the first place...\nIn general, I actually think it's fairly simple to keep things 'clean'- you just make sure that any e.g. precision numbers you report are only based on non-evidence variables; that way you get the benefit of potentially \"flipping\" one of the (noisy!) evidence variables to a correct value, but otherwise nothing else is really changed at all (since the evidence is still pinned in learning...)\n@thodrek You literally just beat me to it :)  My comment to you was going to be this (echoing @raphaelhoffmann 's point): clearly it seems like the noisier we suspect our evidence to be = the less accurate we think our distant supervision are, the more we would want to re-sample these variables after learning... this is what definitely makes this domain / application-specific (so sure, don't set the flag as true by default), but seems like we can model this in our framework for DSRs, and potentially set the --sample-evidence flag automatically, even differently for different sets of evidence variables coming from different DSRs of different noise levels according to the model... :)\n. Sorry! Actually had looked this over already, merging now!\nOn Fri, Jan 29, 2016 at 5:57 PM Jaeho Shin notifications@github.com wrote:\n\n@ajratner https://github.com/ajratner ping? Can we get this reviewed\nasap?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/468#issuecomment-177045613\n.\n. Vanilla gibbs sampling gets broken if you add in a \"hard\" constraint (i.e.\ninfinite / near-infinite weight) like OneIsTrue, because ergodicity gets\nbroken (you can't reach certain states from certain other states).  However\nyou could just do block gibbs, where you would sample from all the boolean\nvariables connected by this OneIsTrue factor (conditioned on all the other\nvars' values at that point) in a single gibbs step.\n\nOf course this problem goes away with actual categorical variables.\nHowever Jaeho and I were briefly chatting about some of the challenges\ninvolved in integrating categorical variables, including dynamic scoping of\nindividual categorical variables...\nI actually think implementing this simple block gibbs scheme might be\neasier?\nOn Thu, Jan 28, 2016 at 4:44 PM alldefector notifications@github.com\nwrote:\n\nYes, it is the one-is-true constraint. However, the sampler doesn't seem\nto support this constraint currently. There is a factor type by the same\nname, but it's not a constraint.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/471#issuecomment-176498739\n.\n. Thanks for this :)!\n. Sorry, I think I started but got interrupted by something else... @ThomasPalomares lmk if I can help here, will be around lab today\n. Hey @ThomasPalomares I know of one bug / annoying \"feature\" where I believe NULL values are dropped from psql arrays... could this be causing any problems perhaps?\n. I'm actually not sure... I think I just tried to avoid passing around\narrays of bools in my deepdive apps, come to think of it.  @netj any\nthoughts?\n\nOn Fri, Feb 26, 2016 at 2:39 PM Thomas Palomares notifications@github.com\nwrote:\n\nah maybe ! the error I have is:\nERROR: malformed array literal: \"{\"asdf\nqwer\\tzxcv\\n1234\",,\"NULL\",\"null\",\"\\N\",\"N\",\"\\I'm your father\",\"\\\" said\nDarth Vader.\\\"\",\"{\\csv in a json\\\": \\\"a\",\"b c\",\"\\\"\",\"\\\"\",\"\\\"line\n'1'\\nbogus\",\"NULL\",\"null\",\"\\N\",\"N\",\"line \\\"\\\"2\\\"\\\"\\\"\",\"\n\\\"foo\\\":123\",\"\\n\\\"bar\\\":45.678\",\" \\\"null\\\": \\\"\\N\\\"}\\\"\"}\"\nso it may be because of that. Do you know if there is a way to modify this\nin psql or should the ddlib/util.py be modified to take that into account ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/498#issuecomment-189513008\n.\n. @netj maybe i was only thinking about NULLs being dropped when\narray_to_string was used... Thomas and I were going to talk about this\nbriefly either way-I'm free all weekend!\nOn Fri, Feb 26, 2016 at 4:23 PM Jaeho Shin notifications@github.com wrote:\n@ajratner https://github.com/ajratner Is the bug confusing NULLs in\narrays as empty strings? I gave up this case because I was relying on the\ncsv package, but if we're parsing commas ourselves, I think the problem\nboiled down to a matter of distinguishing ...,,... from ...,\"\",....\n@ThomasPalomares https://github.com/ThomasPalomares I think the new\ntest is too large involving too many other parts, essentially everything\nfrom deepdive: ddlog, compiler, runner, local compute driver etc.\n- Can't we just feed the NastyTSV directly to the identity.py, then\n  check whether it outputs the same thing?\n@test \"ddlib.util (@tsv_extractor and @returns) works against nasty input\" {\n    diff -u <(echo \"$NastyTSV\")  <(python \"$BATS_TEST_DIRNAME\"/identity.py <<<\"$NastyTSV\")\n}\nidentity.py may be even just inlined in the bats file with -c.\n-\nThe rest of the content of the .bats file is duplicate and should be\n   removed.\n   -\nAlso, the test is not dependent on any database, so let's move it out\n   of test/postgresql/, say under ddlib/test/. You can look at other\n   subdir's env.sh.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/498#issuecomment-189536612\n.\n. LGTM!\n. These ideas are all awesome!\n\nIn our dd-genomics repo (still pre-0.7.0) we just split our create_schema.sh utility script into two-\n- one for the \"input\" fields (e.g. the sentences table, anything else that might have been the result of expensive pre-processing)\n- one for everything else (stuff that would be re-computed each full run of dd anyway)\nThis has been good enough for us for the most part.  But fine grained control such as the above would obviously be even better\nIn particular I really like your first point/idea.  The schema file seems so absolutely central to a deepdive app that it really would be great to have it specified in a single file only (and even better, a json file :) )\n. For reference, in case of use to anyone, a simple solution with sed is\nbash\njava -jar ${DEEPDIVE_HOME}/util/ddlog.jar compile app.ddl | sed -e 's/^\\(.*\\)\\(style: \"tsv_extractor\"\\)/\\1\\2XXNXX\\1parallelism: 8/g' -e $'s/XXNXX/\\\\\\n/g' > deepdive.conf\n. Sorry, for future reference should I have put this somewhere else?\n. Okay cool\nTo clarify, for me this is not a big issue- for now I just changed the column names in deepdive.conf manually and is working fine\nGood point on Ce's example too- that is another easy way for me to temp fix.  So no rush from my end.  Thanks!\n. Closing- moving ddlog issues to ddlog repo.  Not yet re-opening this one there though, because from my end, defining these intermediate tables in the schema as you suggested- e.g. using separate tables rather than views- is a fine enough solution for now\n. Isn't this similar to a (slightly more simple-minded) issue I posted which seems to have been partly resolved: #351 ?\nTo chime in with my now tedious \"one time at band camp\"-style comments... One time at genomics camp, we also implemented a short term fix for this issue which was to have two versions of the \"initdb\" command equivalent, one which did and one which did not ever touch user data (again, see my comments in #351 )\nAgain, it was also simpler to have a TRUNCATE script run as a \"before\" command for every extractor in the pre-ddlog syntax\nIn my tables code in ddlog, I had a post-processing script which went through the ddlog-generated app.conf file, and added this before script command back in for every extractor...\n. Yeah sorry for not chiming in earlier, we ran into this problem in genomics and @feiranwang added the --sample_evidence flag for us pre-ddlog, so we had been using it\nConfirming that there is a simple way to incorporate this with ddlog would be great too!\n. > > 3 looks easy for the programs that mainly contain regressions. But what is 3's formal semantic in general factor graph? When you have many general correlations between variables, how should these copies been connected with other variables?\n@zhangce I think one of the big benefits of having the --sample-evidence flag is that users don't have to create copies, and we don't have to consider these issues.  In fact in the genomics code, before I was involved, I actually think there were some confusions due to having created copies- which is part of why I asked Feiran about making the sample evidence flag in the first place...\nIn general, I actually think it's fairly simple to keep things 'clean'- you just make sure that any e.g. precision numbers you report are only based on non-evidence variables; that way you get the benefit of potentially \"flipping\" one of the (noisy!) evidence variables to a correct value, but otherwise nothing else is really changed at all (since the evidence is still pinned in learning...)\n@thodrek You literally just beat me to it :)  My comment to you was going to be this (echoing @raphaelhoffmann 's point): clearly it seems like the noisier we suspect our evidence to be = the less accurate we think our distant supervision are, the more we would want to re-sample these variables after learning... this is what definitely makes this domain / application-specific (so sure, don't set the flag as true by default), but seems like we can model this in our framework for DSRs, and potentially set the --sample-evidence flag automatically, even differently for different sets of evidence variables coming from different DSRs of different noise levels according to the model... :)\n. Sorry! Actually had looked this over already, merging now!\nOn Fri, Jan 29, 2016 at 5:57 PM Jaeho Shin notifications@github.com wrote:\n\n@ajratner https://github.com/ajratner ping? Can we get this reviewed\nasap?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/468#issuecomment-177045613\n.\n. Vanilla gibbs sampling gets broken if you add in a \"hard\" constraint (i.e.\ninfinite / near-infinite weight) like OneIsTrue, because ergodicity gets\nbroken (you can't reach certain states from certain other states).  However\nyou could just do block gibbs, where you would sample from all the boolean\nvariables connected by this OneIsTrue factor (conditioned on all the other\nvars' values at that point) in a single gibbs step.\n\nOf course this problem goes away with actual categorical variables.\nHowever Jaeho and I were briefly chatting about some of the challenges\ninvolved in integrating categorical variables, including dynamic scoping of\nindividual categorical variables...\nI actually think implementing this simple block gibbs scheme might be\neasier?\nOn Thu, Jan 28, 2016 at 4:44 PM alldefector notifications@github.com\nwrote:\n\nYes, it is the one-is-true constraint. However, the sampler doesn't seem\nto support this constraint currently. There is a factor type by the same\nname, but it's not a constraint.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/issues/471#issuecomment-176498739\n.\n. Thanks for this :)!\n. Sorry, I think I started but got interrupted by something else... @ThomasPalomares lmk if I can help here, will be around lab today\n. Hey @ThomasPalomares I know of one bug / annoying \"feature\" where I believe NULL values are dropped from psql arrays... could this be causing any problems perhaps?\n. I'm actually not sure... I think I just tried to avoid passing around\narrays of bools in my deepdive apps, come to think of it.  @netj any\nthoughts?\n\nOn Fri, Feb 26, 2016 at 2:39 PM Thomas Palomares notifications@github.com\nwrote:\n\nah maybe ! the error I have is:\nERROR: malformed array literal: \"{\"asdf\nqwer\\tzxcv\\n1234\",,\"NULL\",\"null\",\"\\N\",\"N\",\"\\I'm your father\",\"\\\" said\nDarth Vader.\\\"\",\"{\\csv in a json\\\": \\\"a\",\"b c\",\"\\\"\",\"\\\"\",\"\\\"line\n'1'\\nbogus\",\"NULL\",\"null\",\"\\N\",\"N\",\"line \\\"\\\"2\\\"\\\"\\\"\",\"\n\\\"foo\\\":123\",\"\\n\\\"bar\\\":45.678\",\" \\\"null\\\": \\\"\\N\\\"}\\\"\"}\"\nso it may be because of that. Do you know if there is a way to modify this\nin psql or should the ddlib/util.py be modified to take that into account ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/498#issuecomment-189513008\n.\n. @netj maybe i was only thinking about NULLs being dropped when\narray_to_string was used... Thomas and I were going to talk about this\nbriefly either way-I'm free all weekend!\nOn Fri, Feb 26, 2016 at 4:23 PM Jaeho Shin notifications@github.com wrote:\n@ajratner https://github.com/ajratner Is the bug confusing NULLs in\narrays as empty strings? I gave up this case because I was relying on the\ncsv package, but if we're parsing commas ourselves, I think the problem\nboiled down to a matter of distinguishing ...,,... from ...,\"\",....\n@ThomasPalomares https://github.com/ThomasPalomares I think the new\ntest is too large involving too many other parts, essentially everything\nfrom deepdive: ddlog, compiler, runner, local compute driver etc.\n- Can't we just feed the NastyTSV directly to the identity.py, then\n  check whether it outputs the same thing?\n@test \"ddlib.util (@tsv_extractor and @returns) works against nasty input\" {\n    diff -u <(echo \"$NastyTSV\")  <(python \"$BATS_TEST_DIRNAME\"/identity.py <<<\"$NastyTSV\")\n}\nidentity.py may be even just inlined in the bats file with -c.\n-\nThe rest of the content of the .bats file is duplicate and should be\n   removed.\n   -\nAlso, the test is not dependent on any database, so let's move it out\n   of test/postgresql/, say under ddlib/test/. You can look at other\n   subdir's env.sh.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/HazyResearch/deepdive/pull/498#issuecomment-189536612\n.\n. LGTM!\n. \n",
    "ghost": "I have similar error on macOS Sierra 10.12.5\n\n\ntar xzvf deepdive-v0.8-STABLE-Darwin.tar.gz -C /Users/andyck/local\ntar: Error opening archive: Failed to open 'deepdive-v0.8-STABLE-Darwin.tar.gz'\n Failed installation for deepdive_from_release\n Failed installation for deepdive\n Install what (enter to repeat options, a to see all, q to quit, or a number)? . Thank you alldefector :) It was exactly the problem\n. I'm trying to adapt DeepDive to my needs so I have to extract first of all \"Vehicles\" mentions for example (like you have done in spouse example to extract \"person\" mentions). I succeeded in adding Vehicles mentions extraction apart with CoreNLP using RegexNER but not with DeepDive. And now I want to do so with DeepDive. Can you advise me please @netj ?\n\n\nThank you\n. Finally i added my regular expressions in a file and i added the \"regexner\" option in the parser and now it works fine. Thank your for your answers\n. Hi @JBKBO,\nWith pleasure :) \nSo you have to modify the file \"yourproject\"\\udf\\bazaar\\parser\\src\\main\\scala\\com\\clearcut\\nlp\\Main.scala by adding:\n-> annotators: String = \"tokenize, cleanxml, ssplit, pos, lemma, ner, parse, regexner\" in Config Class\n-> props.put(\"regexner.mapping\", \"Path to your regular expressions file\") for me the file was in udf\\bazaar\\parser\\\nThen you have to rebuild the scala part by typing \"sbt/sbt compile\" when you are under *udf\\bazaar\\parser*\nI hope this will help you.\nNadia\n. In case someone else faces the same problem, it was because of the ratio between the CPU cores and the memory that i have on my machine (limited memory per core). Deepdive parallels the treatment between the different CPU of the machine so the number of CPU should be well chosen otherwise the machine freezes. To do that you can use \"export DEEPDIVE_NUM_PROCESSES=N\" with N represents the number of CPU core to use. Or modify it in the file \"/home/local/util/compute-driver/local/compute-execute\".\nNadia\n. Thanks a lot.\n. I have the same issue. I am running an EC2 instance with Ubuntu 16.04.\n. I tried to run the same thing on an EC2 instance with Ubuntu 14.04 and now I have issues with JQ ( tsv is not a valid format ).\n. I looked at the differences in the tables. I noticed that even when I have the same number of rows between two sentences tables they don't contain the same results. I looked quickly at the differences and nothing obvious pops up. In one case it considers \\u2013 as a group of four numbers instead of a single character. In another number nothing is clear in what the difference is.. Ok. I did multiple runs on my 5k spouse test to get different sentences results. I also created a small python program to compare the results. From what I see when I have differences it is always the last lines. It seems there is a reuse of a string variable somewhere and when there is a document that is smaller than the previous document then deepdive or coreNLP mixes both documents by adding the end of the big document to the small document.. I pushed further my tests. I think there is an issue with the CoreNLP. I have runs where in the same phrase of the same document words are cut in half such as \"available\" in one run and \"availa\", \"ble\" in the other run. . Hi Manning, we didn't solve this. We haven't tried to reproduce it by calling CoreNLP directly but we will probably do it in the future since we are using it.. Ok. I found a workaround by using deepdive sql eval and changing the way I provide the file name ( \"$( < filename)\" ).. +1\nmuch the same error:\n$ docker pull XXXXXX\nUsing default tag: latest\nlatest: Pulling from XXXX\nd5c6f90da05d: Already exists \n1300883d87d5: Already exists \nc220aa3cfc1b: Already exists \n2e9398f099dc: Already exists \ndc27a084064f: Already exists \n74dcc219f100: Extracting [==================================================>] 28.81 MB/28.81 MB\n4bce57e3a5e2: Download complete \nf7a3d7f68600: Download complete \nd3fc508d9cb5: Download complete \n59e80cd457c5: Download complete \n841fb2c67d2a: Download complete \nafbdb9c3f939: Download complete \n4220b73c202c: Download complete \n38832cec6dbc: Download complete \n62e3cdc8f355: Download complete \nfailed to register layer: lstat /var/lib/docker/overlay/6e37823abf301c77b44329c792b62822d9d42e839c3e27fdb87fa0fe5bf7634c: no such file or directory. I have similar error on macOS Sierra 10.12.5\n\n\ntar xzvf deepdive-v0.8-STABLE-Darwin.tar.gz -C /Users/andyck/local\ntar: Error opening archive: Failed to open 'deepdive-v0.8-STABLE-Darwin.tar.gz'\n Failed installation for deepdive_from_release\n Failed installation for deepdive\n Install what (enter to repeat options, a to see all, q to quit, or a number)? . Thank you alldefector :) It was exactly the problem\n. I'm trying to adapt DeepDive to my needs so I have to extract first of all \"Vehicles\" mentions for example (like you have done in spouse example to extract \"person\" mentions). I succeeded in adding Vehicles mentions extraction apart with CoreNLP using RegexNER but not with DeepDive. And now I want to do so with DeepDive. Can you advise me please @netj ?\n\n\nThank you\n. Finally i added my regular expressions in a file and i added the \"regexner\" option in the parser and now it works fine. Thank your for your answers\n. Hi @JBKBO,\nWith pleasure :) \nSo you have to modify the file \"yourproject\"\\udf\\bazaar\\parser\\src\\main\\scala\\com\\clearcut\\nlp\\Main.scala by adding:\n-> annotators: String = \"tokenize, cleanxml, ssplit, pos, lemma, ner, parse, regexner\" in Config Class\n-> props.put(\"regexner.mapping\", \"Path to your regular expressions file\") for me the file was in udf\\bazaar\\parser\\\nThen you have to rebuild the scala part by typing \"sbt/sbt compile\" when you are under *udf\\bazaar\\parser*\nI hope this will help you.\nNadia\n. In case someone else faces the same problem, it was because of the ratio between the CPU cores and the memory that i have on my machine (limited memory per core). Deepdive parallels the treatment between the different CPU of the machine so the number of CPU should be well chosen otherwise the machine freezes. To do that you can use \"export DEEPDIVE_NUM_PROCESSES=N\" with N represents the number of CPU core to use. Or modify it in the file \"/home/local/util/compute-driver/local/compute-execute\".\nNadia\n. Thanks a lot.\n. I have the same issue. I am running an EC2 instance with Ubuntu 16.04.\n. I tried to run the same thing on an EC2 instance with Ubuntu 14.04 and now I have issues with JQ ( tsv is not a valid format ).\n. I looked at the differences in the tables. I noticed that even when I have the same number of rows between two sentences tables they don't contain the same results. I looked quickly at the differences and nothing obvious pops up. In one case it considers \\u2013 as a group of four numbers instead of a single character. In another number nothing is clear in what the difference is.. Ok. I did multiple runs on my 5k spouse test to get different sentences results. I also created a small python program to compare the results. From what I see when I have differences it is always the last lines. It seems there is a reuse of a string variable somewhere and when there is a document that is smaller than the previous document then deepdive or coreNLP mixes both documents by adding the end of the big document to the small document.. I pushed further my tests. I think there is an issue with the CoreNLP. I have runs where in the same phrase of the same document words are cut in half such as \"available\" in one run and \"availa\", \"ble\" in the other run. . Hi Manning, we didn't solve this. We haven't tried to reproduce it by calling CoreNLP directly but we will probably do it in the future since we are using it.. Ok. I found a workaround by using deepdive sql eval and changing the way I provide the file name ( \"$( < filename)\" ).. +1\nmuch the same error:\n$ docker pull XXXXXX\nUsing default tag: latest\nlatest: Pulling from XXXX\nd5c6f90da05d: Already exists \n1300883d87d5: Already exists \nc220aa3cfc1b: Already exists \n2e9398f099dc: Already exists \ndc27a084064f: Already exists \n74dcc219f100: Extracting [==================================================>] 28.81 MB/28.81 MB\n4bce57e3a5e2: Download complete \nf7a3d7f68600: Download complete \nd3fc508d9cb5: Download complete \n59e80cd457c5: Download complete \n841fb2c67d2a: Download complete \nafbdb9c3f939: Download complete \n4220b73c202c: Download complete \n38832cec6dbc: Download complete \n62e3cdc8f355: Download complete \nfailed to register layer: lstat /var/lib/docker/overlay/6e37823abf301c77b44329c792b62822d9d42e839c3e27fdb87fa0fe5bf7634c: no such file or directory. ",
    "astrung": "After i change your Ubuntu mirror,it is fine.Thank you.\nBut it is neccessary to update system when install Deepdive??\n. If we dont have any positive and negative examples,how to learn the weight of a factor???we can generate a constant or depend on some variables for it,but if we have some negative candidates for a relation,i am not sure that deepdive can give small weights automatically for them without examples\n. @chrismre When do you complete your tools?:))\n. well,anyone can explain for me about this result???\n. But what is \"evidence variables\"??Even with some variables with same values(with spouses examples,it have same persons 1 and persons 2),some variables is not in output??I thinh that \"evidence\" is training data and testing data???? \n. After i change your Ubuntu mirror,it is fine.Thank you.\nBut it is neccessary to update system when install Deepdive??\n. If we dont have any positive and negative examples,how to learn the weight of a factor???we can generate a constant or depend on some variables for it,but if we have some negative candidates for a relation,i am not sure that deepdive can give small weights automatically for them without examples\n. @chrismre When do you complete your tools?:))\n. well,anyone can explain for me about this result???\n. But what is \"evidence variables\"??Even with some variables with same values(with spouses examples,it have same persons 1 and persons 2),some variables is not in output??I thinh that \"evidence\" is training data and testing data???? \n. ",
    "vsoch": "Great, thanks so much! If we figure it out, I'd be happy to write up a little documentation for it.\n. @SenWu there isn't any issue with git - the issue is when using deepdive and connecting to the database (eg, any command with deepdive X\" whether it be via a utils function to do initial database work, or a function embedded in the scala to work with the same database.) In the case of a url for postgres, it would be just added ?ssl=true. I'm hoping it will be as simple as adding an extra parser to read an ssl specification in the db.url, (and then parsed by the postres driver) and some module within the scala to do the same. I haven't worked with scala before, so I'm not much help, but here is a suggestion from one of the tech's at TACC:\n```\n\n\nActually, before going through all the trouble of rewriting code in a different language, I'd be interested in knowing if the following jdbc parameter would work in the case where we don't actually want to setup certificates.\n\n\nsslfactory=org.postgresql.ssl.NonValidatingFactory\n```\nAnd some more detail on the JDBC url:\n```\n\n\nComing in late to the conversation, so if this has already been discussed please forgive the extra mail, but with the JDBC you should be able to simply add properties to the JDBC URL. \n\n\nString url = \"jdbc:postgresql://localhost/test?user=fred&password=secret&ssl=true\";\n  Connection conn = DriverManager.getConnection(url);\n```\nhttps://jdbc.postgresql.org/documentation/80/connect.html\nI played around with editing the driver functions and setting environment variables to expect SSL, but couldn't get anything to work.\n. Fantastic, many thanks @SenWu.  There isn't huge rush because I was able to go through the entire walkthrough on my local machine, and am not held back in any way to start developing our methods locally. It's really a beautiful piece of software - I of course expect to have bugs and what not, but the workflow with the database, making extraction rules, and running pipelines is very intuitive! :L)\n. oh yes, definitely coding! :o)\n\n. Thanks @SenWu! I will give it a try asap - there might be some delay because my Brother is getting married tomorrow! :wedding: \n. @SunWu, we are making progress! The ssl part seems to be working, and there are some additional bugs that are likely related to my installation (that I will look into). Here is how I tested, despite some installation bugs: \nFirst, I tried it as a user would, from the project directory:\ndeepdive init-db\nAnd here was the error:\ndb-parse operation not available for \"postgresql\n  db-init operation not available for \"postgresql\n  db-execute operation not available for \"postgresql\n  db-query operation not available for \"postgresql\n  ~/local/util/load-db-driver.sh: line 61: db-parse: command not found\n  ~/local/util/load-db-driver.sh: eval: line 37: unexpected EOF while looking for matching `\"'\nAnd this happens because there must be some issue adding the driver to the path in ~/local/util/load-db-driver.sh. I didn't look into this, instead I decided to keep going and try parsing the url:\ncd ~/local/util/\n  url=\"postgresql://vsochat@xxx.xxxxxxxx.xxx.xxxx.edu:5432/database_name?ssl=true\"\n  ~/local/util/driver.postgresql/db-parse $url\nHere it spit out all the variables into the window, and I copy pasted to execute, and the only issue being that the DEEPDIVE_JDBC_URL needed quotes since it had a \"?\" in it.\nI'm not allowed to create the database itself (so I can't test dbinit) but I can execute stuff, so I decided to test that:\ncd driver.postgresql\n  db-execute \"\\d+\"\n  Password for user vsochat: \n  No relations found.\nBoom! Success! So - here are my thoughts. The ssl issue is definitely resolved. Setting PGSSLMODE to require and specifying ?ssl=true for the DEEPDIVE_JDBC_URL were spot on! I don't think that you need to do any more debugging - I want to do the entire thing from start to end fresh. I'm not convinced that some of my local changes aren't still lurking in code somewhere and mucking something up, because it works perfectly on my local machine. I am finishing up work for today, but I will be able to test this out this weekend. I will post an update!\n. ok, I can confirm with a fresh install that the previously mentioned bugs to use the driver are resolved! Given that I don't have permissions to create the database, the standard initdb commands don't work, however I can write custom scripts to do the database setup and loading. Thanks again for your help with this, and I'll be sure to post if I run into any trouble!\n. Is there a branch I could look at for inspiration? Even just a push in the right direction I think I could get something working for our analysis.\n. I think I found a lot of examples here that will be good to get me started!\n. Thanks for the feedback, and the example! That strategy is very logical, I agree that the candidate mapping is likely to evolve, and actually it's the extractors that seem like they would be the most variable out of all things in the application. I will move forward with enveloping the regular expressions there.\n. I think we should be able to set up a dedicated database with superuser - I will know more information on Monday! If not, it would definitely be worth a chat. I can imagine many scenarios of wanting to use DeepDive without any kind of sudo.\n. Once I know more about the database, it might be needed to setup a Skype to talk about the different options for setting this up to run in parallel. I have a few ideas, will likely try a few things, and very likely will have questions!\n. +1 !\n. @zhangce, wicked awesome. Looking forward to it.\n. Hi Hazy! Any updates on superuser? Did you mean this passed weekend, or next weekend? We are paused in developing on TACC so I wanted to make sure that I didn't miss the update!\n. Excellent! I will update this morning and let you know if I run into trouble.\n. Ah! That's what I get for renaming my pipeline to has_related_concept from has_cognitive_concept...\noh my gosh!! It's working!! It's working!!\n. Thanks for the help, you just made my Friday and brought immense happiness :O)\n. Great, thanks so much! If we figure it out, I'd be happy to write up a little documentation for it.\n. @SenWu there isn't any issue with git - the issue is when using deepdive and connecting to the database (eg, any command with deepdive X\" whether it be via a utils function to do initial database work, or a function embedded in the scala to work with the same database.) In the case of a url for postgres, it would be just added ?ssl=true. I'm hoping it will be as simple as adding an extra parser to read an ssl specification in the db.url, (and then parsed by the postres driver) and some module within the scala to do the same. I haven't worked with scala before, so I'm not much help, but here is a suggestion from one of the tech's at TACC:\n```\n\n\nActually, before going through all the trouble of rewriting code in a different language, I'd be interested in knowing if the following jdbc parameter would work in the case where we don't actually want to setup certificates.\n\n\nsslfactory=org.postgresql.ssl.NonValidatingFactory\n```\nAnd some more detail on the JDBC url:\n```\n\n\nComing in late to the conversation, so if this has already been discussed please forgive the extra mail, but with the JDBC you should be able to simply add properties to the JDBC URL. \n\n\nString url = \"jdbc:postgresql://localhost/test?user=fred&password=secret&ssl=true\";\n  Connection conn = DriverManager.getConnection(url);\n```\nhttps://jdbc.postgresql.org/documentation/80/connect.html\nI played around with editing the driver functions and setting environment variables to expect SSL, but couldn't get anything to work.\n. Fantastic, many thanks @SenWu.  There isn't huge rush because I was able to go through the entire walkthrough on my local machine, and am not held back in any way to start developing our methods locally. It's really a beautiful piece of software - I of course expect to have bugs and what not, but the workflow with the database, making extraction rules, and running pipelines is very intuitive! :L)\n. oh yes, definitely coding! :o)\n\n. Thanks @SenWu! I will give it a try asap - there might be some delay because my Brother is getting married tomorrow! :wedding: \n. @SunWu, we are making progress! The ssl part seems to be working, and there are some additional bugs that are likely related to my installation (that I will look into). Here is how I tested, despite some installation bugs: \nFirst, I tried it as a user would, from the project directory:\ndeepdive init-db\nAnd here was the error:\ndb-parse operation not available for \"postgresql\n  db-init operation not available for \"postgresql\n  db-execute operation not available for \"postgresql\n  db-query operation not available for \"postgresql\n  ~/local/util/load-db-driver.sh: line 61: db-parse: command not found\n  ~/local/util/load-db-driver.sh: eval: line 37: unexpected EOF while looking for matching `\"'\nAnd this happens because there must be some issue adding the driver to the path in ~/local/util/load-db-driver.sh. I didn't look into this, instead I decided to keep going and try parsing the url:\ncd ~/local/util/\n  url=\"postgresql://vsochat@xxx.xxxxxxxx.xxx.xxxx.edu:5432/database_name?ssl=true\"\n  ~/local/util/driver.postgresql/db-parse $url\nHere it spit out all the variables into the window, and I copy pasted to execute, and the only issue being that the DEEPDIVE_JDBC_URL needed quotes since it had a \"?\" in it.\nI'm not allowed to create the database itself (so I can't test dbinit) but I can execute stuff, so I decided to test that:\ncd driver.postgresql\n  db-execute \"\\d+\"\n  Password for user vsochat: \n  No relations found.\nBoom! Success! So - here are my thoughts. The ssl issue is definitely resolved. Setting PGSSLMODE to require and specifying ?ssl=true for the DEEPDIVE_JDBC_URL were spot on! I don't think that you need to do any more debugging - I want to do the entire thing from start to end fresh. I'm not convinced that some of my local changes aren't still lurking in code somewhere and mucking something up, because it works perfectly on my local machine. I am finishing up work for today, but I will be able to test this out this weekend. I will post an update!\n. ok, I can confirm with a fresh install that the previously mentioned bugs to use the driver are resolved! Given that I don't have permissions to create the database, the standard initdb commands don't work, however I can write custom scripts to do the database setup and loading. Thanks again for your help with this, and I'll be sure to post if I run into any trouble!\n. Is there a branch I could look at for inspiration? Even just a push in the right direction I think I could get something working for our analysis.\n. I think I found a lot of examples here that will be good to get me started!\n. Thanks for the feedback, and the example! That strategy is very logical, I agree that the candidate mapping is likely to evolve, and actually it's the extractors that seem like they would be the most variable out of all things in the application. I will move forward with enveloping the regular expressions there.\n. I think we should be able to set up a dedicated database with superuser - I will know more information on Monday! If not, it would definitely be worth a chat. I can imagine many scenarios of wanting to use DeepDive without any kind of sudo.\n. Once I know more about the database, it might be needed to setup a Skype to talk about the different options for setting this up to run in parallel. I have a few ideas, will likely try a few things, and very likely will have questions!\n. +1 !\n. @zhangce, wicked awesome. Looking forward to it.\n. Hi Hazy! Any updates on superuser? Did you mean this passed weekend, or next weekend? We are paused in developing on TACC so I wanted to make sure that I didn't miss the update!\n. Excellent! I will update this morning and let you know if I run into trouble.\n. Ah! That's what I get for renaming my pipeline to has_related_concept from has_cognitive_concept...\noh my gosh!! It's working!! It's working!!\n. Thanks for the help, you just made my Friday and brought immense happiness :O)\n. ",
    "AlexisBRENON": "Even if it's faster, I think that JSON extractor, based on column name and so, that gives some semantic to your extractor, is far better for maintenance: in case you have to add or remove some attributes, you can easily check its existence with JSON, but not with TSV. Both are useful, depending on your goal : speed or continued existence.\n. Maybe there is other pages with code snippets that need to be fixed. Let me know, I will do it.\n. Even if it's faster, I think that JSON extractor, based on column name and so, that gives some semantic to your extractor, is far better for maintenance: in case you have to add or remove some attributes, you can easily check its existence with JSON, but not with TSV. Both are useful, depending on your goal : speed or continued existence.\n. Maybe there is other pages with code snippets that need to be fixed. Let me know, I will do it.\n. ",
    "thodrek": "@zhangce Fixed some typos in the incremental.md file.\n. @netj @zhangce I'll do a pass over the documentation as well. Starting now. \n. @raphaelhoffmann We are actually developing a formal framework that will address exactly this issue of noisy \"evidence\" from distant supervision rules. Maybe we can talk more in person. Indeed sampling evidence variables is interesting in that context. As a new deepdive user myself I've already spent sometime playing with the -sample_evidence flag and I would like to second the argument that sample_evidences shouldn't be set to true by default. As @zhangce pointed out earlier it may lead to weird results. \n. OK but we need to make sure cause I tried compiling and it gave me an error. \n. @netj : Those where already there. Not sure about the initial motivation. The reason I was asking for those was to be able to introduce observed variables in a factor graph and have a single factor outputting different values based on the value of those observed variables. At that point factors in DD didn't support constants as input. \n. This is awesome! I'll test it once with distributed NS and let you know. Once done with the tests I'll go over the pull request carefully :) \n. Yep go ahead :)\n. @rudaoshi Regarding the NULL issue alone, you should try to use casting within ddlog. Writing = NULL::boolean instead of = NULL will take care of the label casting issue. . @zhangce Fixed some typos in the incremental.md file.\n. @netj @zhangce I'll do a pass over the documentation as well. Starting now. \n. @raphaelhoffmann We are actually developing a formal framework that will address exactly this issue of noisy \"evidence\" from distant supervision rules. Maybe we can talk more in person. Indeed sampling evidence variables is interesting in that context. As a new deepdive user myself I've already spent sometime playing with the -sample_evidence flag and I would like to second the argument that sample_evidences shouldn't be set to true by default. As @zhangce pointed out earlier it may lead to weird results. \n. OK but we need to make sure cause I tried compiling and it gave me an error. \n. @netj : Those where already there. Not sure about the initial motivation. The reason I was asking for those was to be able to introduce observed variables in a factor graph and have a single factor outputting different values based on the value of those observed variables. At that point factors in DD didn't support constants as input. \n. This is awesome! I'll test it once with distributed NS and let you know. Once done with the tests I'll go over the pull request carefully :) \n. Yep go ahead :)\n. @rudaoshi Regarding the NULL issue alone, you should try to use casting within ddlog. Writing = NULL::boolean instead of = NULL will take care of the label casting issue. . ",
    "ukliu": "@netj  Thanks so much for the prompt reply! I tried it, but it failed for most of the tests. The error message is like this.\n``\n...\nremote: Counting objects: 574, done.\nremote: Total 574 (delta 0), reused 0 (delta 0), pack-reused 574\nReceiving objects: 100% (574/574), 114.06 KiB | 0 bytes/s, done.\nResolving deltas: 100% (270/270), done.\nChecking connectivity... done.\nTesting against database TEST_DBNAME=deepdive_test_cliu running at TEST_DBHOST=localhost\nRunning 13 tests defined in 6 .bats files: test/postgresql/biased_coin_example.bats test/postgresql/broken_example.bats test/postgresql/chunking_example.bats test/postgresql/spouse_example.bats test/postgresql/spouse_example_incremental.bats test/test_plpy.bats\n \u2713 postgresql biased coin example\n \u2713 postgresql DeepDive returns error for a broken application\n \u2717 postgresql chunking example\n   (in test file test/postgresql/chunking_example.bats, line 11)DEEPDIVE_CONFIG_EXTRA='deepdive.calibration.holdout_query: \"INSERT INTO dd_graph_variables_holdout(variable_id) SELECT id FROM words WHERE word_id > '${SUBSAMPLE_NUM_WORDS_TRAIN}'\"' \\' failed with status 137\n   CREATE TABLE\n   CREATE TABLE\n   CREATE TABLE\n   COPY 6000\n   20:07:41 [] INFO  Slf4jLogger started\n   20:07:41 [EventStream(akka://deepdive)] DEBUG logger log1-Slf4jLogger started\n   20:07:41 [EventStream(akka://deepdive)] DEBUG Default Loggers started\n   20:07:41 [Main$(akka://deepdive)] INFO  Running pipeline with configuration from /var/folders/f7/4rfwnx6j1vv_rksgtbydk57r0000gn/T/deepdive.conf.iezYANp\n   20:07:41 [SettingsParser$(akka://deepdive)] INFO  Database settings: user cliu, dbname deepdive_test_cliu, host localhost, port 5432.\n   20:07:41 [SettingsParser$(akka://deepdive)] DEBUG samplerArgs: -l 300 -s 1 -i 500 --alpha 0.1\n   20:07:41 [DeepDive$(akka://deepdive)] DEBUG relearnFrom=null\n   20:07:41 [DeepDive$(akka://deepdive)] DEBUG outputDir=/Users/cliu/deepdive-0.7.x/examples/chunking/run/20150921/200740.N\n   20:07:41 [JdbcDataStoreObject$(akka://deepdive)] INFO  Intializing all JDBC data stores\n   20:07:41 [profiler] INFO  starting at akka://deepdive/user/profiler\n   20:07:41 [taskManager] INFO  starting at akka://deepdive/user/taskManager\n   20:07:41 [inferenceManager] INFO  Starting\n   20:07:41 [extractionManager] INFO  starting\n   20:07:41 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback... SELECT EXISTS (\n         SELECT 1\n         FROM   pg_language\n         WHERE  lanname = 'plpgsql');\n   20:07:41 [DeepDive$(akka://deepdive)] DEBUG Total number of extractors: 2\n   20:07:41 [DeepDive$(akka://deepdive)] DEBUG Total number of factors: 3\n   20:07:41 [DeepDive$(akka://deepdive)] DEBUG Number of active factors: 3\n   20:07:41 [DeepDive$(akka://deepdive)] INFO  Running pipeline=_default with tasks=List(ext_features, ext_training, inference_grounding, inference, calibration, report, shutdown)\n   20:07:41 [taskManager] INFO  Added task_id=ext_features\n   20:07:41 [taskManager] INFO  Added task_id=ext_training\n   20:07:41 [taskManager] DEBUG Sending task_id=ext_training to Actor[akka://deepdive/user/extractionManager#-189704169]\n   20:07:41 [profiler] DEBUG starting report_id=ext_training\n   20:07:41 [taskManager] INFO  Added task_id=inference_grounding\n   20:07:41 [taskManager] INFO  Added task_id=inference\n   20:07:41 [taskManager] INFO  Added task_id=calibration\n   20:07:41 [taskManager] INFO  Added task_id=report\n   20:07:41 [taskManager] INFO  Added task_id=shutdown\n   20:07:41 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback... SELECT EXISTS (\n         SELECT 1\n         FROM   pg_language\n         WHERE  lanname = 'plpythonu');\n   20:07:41 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: CREATE LANGUAGE plpythonu\n   20:07:41 [PostgresDataStore(akka://deepdive)] ERROR org.postgresql.util.PSQLException: ERROR: could not access file \"$libdir/plpython2\": No such file or directory\n   20:07:41 [extractionManager] ERROR ERROR: could not access file \"$libdir/plpython2\": No such file or directory\n   akka.actor.ActorInitializationException: exception during creation\n    at akka.actor.ActorInitializationException$.apply(Actor.scala:166) ~[deepdive.jar:0.7.0]\n    at akka.actor.ActorCell.create(ActorCell.scala:571) ~[deepdive.jar:0.7.0]\n    at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:431) ~[deepdive.jar:0.7.0]\n    at akka.actor.ActorCell.systemInvoke(ActorCell.scala:453) ~[deepdive.jar:0.7.0]\n    at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262) ~[deepdive.jar:0.7.0]\n    at akka.dispatch.Mailbox.run(Mailbox.scala:218) ~[deepdive.jar:0.7.0]\n    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:385) [deepdive.jar:0.7.0]\n    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [deepdive.jar:0.7.0]\n    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [deepdive.jar:0.7.0]\n    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [deepdive.jar:0.7.0]\n    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [deepdive.jar:0.7.0]\n   Caused by: org.postgresql.util.PSQLException: ERROR: could not access file \"$libdir/plpython2\": No such file or directory\n    at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2157) ~[deepdive.jar:0.7.0]\n    at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1886) ~[deepdive.jar:0.7.0]\n    at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255) ~[deepdive.jar:0.7.0]\n    at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:555) ~[deepdive.jar:0.7.0]\n    at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:417) ~[deepdive.jar:0.7.0]\n    at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:410) ~[deepdive.jar:0.7.0]\n    at org.apache.commons.dbcp.DelegatingPreparedStatement.execute(DelegatingPreparedStatement.java:172) ~[deepdive.jar:0.7.0]\n    at org.apache.commons.dbcp.DelegatingPreparedStatement.execute(DelegatingPreparedStatement.java:172) ~[deepdive.jar:0.7.0]\n    at org.deepdive.datastore.JdbcDataStore$$anonfun$executeSqlQueries$2.apply(JdbcDataStore.scala:90) ~[deepdive.jar:0.7.0]\n    at org.deepdive.datastore.JdbcDataStore$$anonfun$executeSqlQueries$2.apply(JdbcDataStore.scala:87) ~[deepdive.jar:0.7.0]\n    at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) ~[deepdive.jar:0.7.0]\n    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108) ~[deepdive.jar:0.7.0]\n    at org.deepdive.datastore.JdbcDataStore$class.executeSqlQueries(JdbcDataStore.scala:87) ~[deepdive.jar:0.7.0]\n    at org.deepdive.datastore.PostgresDataStore.executeSqlQueries(PostgresDataStore.scala:20) ~[deepdive.jar:0.7.0]\n    at org.deepdive.datastore.PostgresDataStore.init(PostgresDataStore.scala:182) ~[deepdive.jar:0.7.0]\n    at org.deepdive.extraction.ExtractionManager$class.preStart(ExtractionManager.scala:64) ~[deepdive.jar:0.7.0]\n    at org.deepdive.extraction.ExtractionManager$PostgresExtractionManager.preStart(ExtractionManager.scala:28) ~[deepdive.jar:0.7.0]\n    at akka.actor.Actor$class.aroundPreStart(Actor.scala:472) ~[deepdive.jar:0.7.0]\n    at org.deepdive.extraction.ExtractionManager$PostgresExtractionManager.aroundPreStart(ExtractionManager.scala:28) ~[deepdive.jar:0.7.0]\n    at akka.actor.ActorCell.create(ActorCell.scala:555) ~[deepdive.jar:0.7.0]\n    ... 9 common frames omitted\n   20:07:41 [taskManager] INFO  Actor[akka://deepdive/user/extractionManager#-189704169] was terminated. Canceling its tasks.\n   20:07:41 [profiler] DEBUG ending report_id=ext_training\n   20:07:41 [taskManager] INFO  Completed task_id=ext_training with Failure(java.lang.RuntimeException: worker was terminated)\n   20:07:41 [taskManager] ERROR task=ext_training Failed: java.lang.RuntimeException: worker was terminated\n   20:07:41 [taskManager] ERROR Forcing shutdown\n   20:07:41 [taskManager] ERROR Cancelling task=inference_grounding\n   20:07:41 [taskManager] ERROR Cancelling task=calibration\n   20:07:41 [taskManager] ERROR Cancelling task=inference\n   20:07:41 [taskManager] ERROR Cancelling task=ext_features\n   /Users/cliu/local/util/deepdive-run: line 80:  8035 Killed: 9               java org.deepdive.Main -c \"$extendedConfig\" -o \"$APP_HOME/$run_dir\"\n \u2717 postgresql spouse example (tsv_extractor)\n...\n \u2717 postgresql spouse example (json_extractor)\n..\n \u2717 postgresql spouse example (piggy_extractor)\n...\n - postgresql spouse example (plpy_extractor) (skipped)\n \u2717 incremental spouse example (F1, then F2 and Symmetry incrementally)\n...\n...\n...\n13 tests, 9 failures, 1 skipped\nFailed installation for run_deepdive_tests\n```\nCould you please take a look at it? Thanks!\n. @netj  Thank you so much! Now everything is working!\nOne of the 13 tests is skipped. I guess that is acceptable, right?\n```\nDeepDive installer for Mac\n\ncurl -fsSL https://github.com/HazyResearch/deepdive/raw/v0.7.x/util/install/install.Mac.sh\n\nStarting installation for run_deepdive_tests\nStarting installation for deepdive_examples_tests\nDeepDive examples and tests already downloaded at /Users/cliu/deepdive-0.7.x\nFinished installation for deepdive_examples_tests\n\nPATH=/Users/cliu/local/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin\ndeepdive-0.7.x/test/test-installed.sh\nTesting DeepDive installed at: /Users/cliu/local\nTesting against database TEST_DBNAME=deepdive_test_cliu running at TEST_DBHOST=localhost\nRunning 13 tests defined in 6 .bats files: test/postgresql/biased_coin_example.bats test/postgresql/broken_example.bats test/postgresql/chunking_example.bats test/postgresql/spouse_example.bats test/postgresql/spouse_example_incremental.bats test/test_plpy.bats\n \u2713 postgresql biased coin example\n \u2713 postgresql DeepDive returns error for a broken application\n \u2713 postgresql chunking example\n \u2713 postgresql spouse example (tsv_extractor)\n \u2713 postgresql spouse example (json_extractor)\n \u2713 postgresql spouse example (piggy_extractor)\npostgresql spouse example (plpy_extractor) (skipped)\n \u2713 incremental spouse example (F1, then F2 and Symmetry incrementally)\n \u2713 incremental spouse example's non-incremental run (F1+F2)\n \u2713 incremental spouse example's non-incremental run (F1+F2+Symmetry)\n \u2713 similarity of probability distributions of incremental and non-incremental spouse example (F1+F2)\n \u2713 similarity of probability distributions of incremental and non-incremental spouse example (F1+F2+Symmetry)\n \u2713 util/ddext.py should work as expected\n\n13 tests, 0 failures, 1 skipped\nFinished installation for run_deepdive_tests\n```\n. @netj Thanks a lot, it works! I did that step in the original installation, but did not do it when everything was reinstalled last night. \nI might keep bothering you in the future.  Thank you for your patience and help, I really appreciate them.\n. Thanks for the reply! I cannot open https://github.com/HazyResearch/pdd though.\n. @zhangce Thanks a lot! Also I sent an email to say hi and introduce myself. Thanks!\n. @chrismre Dear Chris, thanks for the reply and all your help. I meant to reply it sooner. I am afraid I will keep asking silly questions to learn deepdive. I do not know how many people are trying to learn it, but if there are a lot, maybe it is worth setting up an online course. \n. @netj  Thanks so much for the prompt reply! I tried it, but it failed for most of the tests. The error message is like this.\n``\n...\nremote: Counting objects: 574, done.\nremote: Total 574 (delta 0), reused 0 (delta 0), pack-reused 574\nReceiving objects: 100% (574/574), 114.06 KiB | 0 bytes/s, done.\nResolving deltas: 100% (270/270), done.\nChecking connectivity... done.\nTesting against database TEST_DBNAME=deepdive_test_cliu running at TEST_DBHOST=localhost\nRunning 13 tests defined in 6 .bats files: test/postgresql/biased_coin_example.bats test/postgresql/broken_example.bats test/postgresql/chunking_example.bats test/postgresql/spouse_example.bats test/postgresql/spouse_example_incremental.bats test/test_plpy.bats\n \u2713 postgresql biased coin example\n \u2713 postgresql DeepDive returns error for a broken application\n \u2717 postgresql chunking example\n   (in test file test/postgresql/chunking_example.bats, line 11)DEEPDIVE_CONFIG_EXTRA='deepdive.calibration.holdout_query: \"INSERT INTO dd_graph_variables_holdout(variable_id) SELECT id FROM words WHERE word_id > '${SUBSAMPLE_NUM_WORDS_TRAIN}'\"' \\' failed with status 137\n   CREATE TABLE\n   CREATE TABLE\n   CREATE TABLE\n   COPY 6000\n   20:07:41 [] INFO  Slf4jLogger started\n   20:07:41 [EventStream(akka://deepdive)] DEBUG logger log1-Slf4jLogger started\n   20:07:41 [EventStream(akka://deepdive)] DEBUG Default Loggers started\n   20:07:41 [Main$(akka://deepdive)] INFO  Running pipeline with configuration from /var/folders/f7/4rfwnx6j1vv_rksgtbydk57r0000gn/T/deepdive.conf.iezYANp\n   20:07:41 [SettingsParser$(akka://deepdive)] INFO  Database settings: user cliu, dbname deepdive_test_cliu, host localhost, port 5432.\n   20:07:41 [SettingsParser$(akka://deepdive)] DEBUG samplerArgs: -l 300 -s 1 -i 500 --alpha 0.1\n   20:07:41 [DeepDive$(akka://deepdive)] DEBUG relearnFrom=null\n   20:07:41 [DeepDive$(akka://deepdive)] DEBUG outputDir=/Users/cliu/deepdive-0.7.x/examples/chunking/run/20150921/200740.N\n   20:07:41 [JdbcDataStoreObject$(akka://deepdive)] INFO  Intializing all JDBC data stores\n   20:07:41 [profiler] INFO  starting at akka://deepdive/user/profiler\n   20:07:41 [taskManager] INFO  starting at akka://deepdive/user/taskManager\n   20:07:41 [inferenceManager] INFO  Starting\n   20:07:41 [extractionManager] INFO  starting\n   20:07:41 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback... SELECT EXISTS (\n         SELECT 1\n         FROM   pg_language\n         WHERE  lanname = 'plpgsql');\n   20:07:41 [DeepDive$(akka://deepdive)] DEBUG Total number of extractors: 2\n   20:07:41 [DeepDive$(akka://deepdive)] DEBUG Total number of factors: 3\n   20:07:41 [DeepDive$(akka://deepdive)] DEBUG Number of active factors: 3\n   20:07:41 [DeepDive$(akka://deepdive)] INFO  Running pipeline=_default with tasks=List(ext_features, ext_training, inference_grounding, inference, calibration, report, shutdown)\n   20:07:41 [taskManager] INFO  Added task_id=ext_features\n   20:07:41 [taskManager] INFO  Added task_id=ext_training\n   20:07:41 [taskManager] DEBUG Sending task_id=ext_training to Actor[akka://deepdive/user/extractionManager#-189704169]\n   20:07:41 [profiler] DEBUG starting report_id=ext_training\n   20:07:41 [taskManager] INFO  Added task_id=inference_grounding\n   20:07:41 [taskManager] INFO  Added task_id=inference\n   20:07:41 [taskManager] INFO  Added task_id=calibration\n   20:07:41 [taskManager] INFO  Added task_id=report\n   20:07:41 [taskManager] INFO  Added task_id=shutdown\n   20:07:41 [PostgresDataStore(akka://deepdive)] DEBUG Executing SQL with callback... SELECT EXISTS (\n         SELECT 1\n         FROM   pg_language\n         WHERE  lanname = 'plpythonu');\n   20:07:41 [PostgresDataStore(akka://deepdive)] DEBUG Executing query via JDBC: CREATE LANGUAGE plpythonu\n   20:07:41 [PostgresDataStore(akka://deepdive)] ERROR org.postgresql.util.PSQLException: ERROR: could not access file \"$libdir/plpython2\": No such file or directory\n   20:07:41 [extractionManager] ERROR ERROR: could not access file \"$libdir/plpython2\": No such file or directory\n   akka.actor.ActorInitializationException: exception during creation\n    at akka.actor.ActorInitializationException$.apply(Actor.scala:166) ~[deepdive.jar:0.7.0]\n    at akka.actor.ActorCell.create(ActorCell.scala:571) ~[deepdive.jar:0.7.0]\n    at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:431) ~[deepdive.jar:0.7.0]\n    at akka.actor.ActorCell.systemInvoke(ActorCell.scala:453) ~[deepdive.jar:0.7.0]\n    at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262) ~[deepdive.jar:0.7.0]\n    at akka.dispatch.Mailbox.run(Mailbox.scala:218) ~[deepdive.jar:0.7.0]\n    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:385) [deepdive.jar:0.7.0]\n    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [deepdive.jar:0.7.0]\n    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [deepdive.jar:0.7.0]\n    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [deepdive.jar:0.7.0]\n    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [deepdive.jar:0.7.0]\n   Caused by: org.postgresql.util.PSQLException: ERROR: could not access file \"$libdir/plpython2\": No such file or directory\n    at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2157) ~[deepdive.jar:0.7.0]\n    at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1886) ~[deepdive.jar:0.7.0]\n    at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255) ~[deepdive.jar:0.7.0]\n    at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:555) ~[deepdive.jar:0.7.0]\n    at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:417) ~[deepdive.jar:0.7.0]\n    at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:410) ~[deepdive.jar:0.7.0]\n    at org.apache.commons.dbcp.DelegatingPreparedStatement.execute(DelegatingPreparedStatement.java:172) ~[deepdive.jar:0.7.0]\n    at org.apache.commons.dbcp.DelegatingPreparedStatement.execute(DelegatingPreparedStatement.java:172) ~[deepdive.jar:0.7.0]\n    at org.deepdive.datastore.JdbcDataStore$$anonfun$executeSqlQueries$2.apply(JdbcDataStore.scala:90) ~[deepdive.jar:0.7.0]\n    at org.deepdive.datastore.JdbcDataStore$$anonfun$executeSqlQueries$2.apply(JdbcDataStore.scala:87) ~[deepdive.jar:0.7.0]\n    at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) ~[deepdive.jar:0.7.0]\n    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108) ~[deepdive.jar:0.7.0]\n    at org.deepdive.datastore.JdbcDataStore$class.executeSqlQueries(JdbcDataStore.scala:87) ~[deepdive.jar:0.7.0]\n    at org.deepdive.datastore.PostgresDataStore.executeSqlQueries(PostgresDataStore.scala:20) ~[deepdive.jar:0.7.0]\n    at org.deepdive.datastore.PostgresDataStore.init(PostgresDataStore.scala:182) ~[deepdive.jar:0.7.0]\n    at org.deepdive.extraction.ExtractionManager$class.preStart(ExtractionManager.scala:64) ~[deepdive.jar:0.7.0]\n    at org.deepdive.extraction.ExtractionManager$PostgresExtractionManager.preStart(ExtractionManager.scala:28) ~[deepdive.jar:0.7.0]\n    at akka.actor.Actor$class.aroundPreStart(Actor.scala:472) ~[deepdive.jar:0.7.0]\n    at org.deepdive.extraction.ExtractionManager$PostgresExtractionManager.aroundPreStart(ExtractionManager.scala:28) ~[deepdive.jar:0.7.0]\n    at akka.actor.ActorCell.create(ActorCell.scala:555) ~[deepdive.jar:0.7.0]\n    ... 9 common frames omitted\n   20:07:41 [taskManager] INFO  Actor[akka://deepdive/user/extractionManager#-189704169] was terminated. Canceling its tasks.\n   20:07:41 [profiler] DEBUG ending report_id=ext_training\n   20:07:41 [taskManager] INFO  Completed task_id=ext_training with Failure(java.lang.RuntimeException: worker was terminated)\n   20:07:41 [taskManager] ERROR task=ext_training Failed: java.lang.RuntimeException: worker was terminated\n   20:07:41 [taskManager] ERROR Forcing shutdown\n   20:07:41 [taskManager] ERROR Cancelling task=inference_grounding\n   20:07:41 [taskManager] ERROR Cancelling task=calibration\n   20:07:41 [taskManager] ERROR Cancelling task=inference\n   20:07:41 [taskManager] ERROR Cancelling task=ext_features\n   /Users/cliu/local/util/deepdive-run: line 80:  8035 Killed: 9               java org.deepdive.Main -c \"$extendedConfig\" -o \"$APP_HOME/$run_dir\"\n \u2717 postgresql spouse example (tsv_extractor)\n...\n \u2717 postgresql spouse example (json_extractor)\n..\n \u2717 postgresql spouse example (piggy_extractor)\n...\n - postgresql spouse example (plpy_extractor) (skipped)\n \u2717 incremental spouse example (F1, then F2 and Symmetry incrementally)\n...\n...\n...\n13 tests, 9 failures, 1 skipped\nFailed installation for run_deepdive_tests\n```\nCould you please take a look at it? Thanks!\n. @netj  Thank you so much! Now everything is working!\nOne of the 13 tests is skipped. I guess that is acceptable, right?\n```\nDeepDive installer for Mac\n\ncurl -fsSL https://github.com/HazyResearch/deepdive/raw/v0.7.x/util/install/install.Mac.sh\n\nStarting installation for run_deepdive_tests\nStarting installation for deepdive_examples_tests\nDeepDive examples and tests already downloaded at /Users/cliu/deepdive-0.7.x\nFinished installation for deepdive_examples_tests\n\nPATH=/Users/cliu/local/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin\ndeepdive-0.7.x/test/test-installed.sh\nTesting DeepDive installed at: /Users/cliu/local\nTesting against database TEST_DBNAME=deepdive_test_cliu running at TEST_DBHOST=localhost\nRunning 13 tests defined in 6 .bats files: test/postgresql/biased_coin_example.bats test/postgresql/broken_example.bats test/postgresql/chunking_example.bats test/postgresql/spouse_example.bats test/postgresql/spouse_example_incremental.bats test/test_plpy.bats\n \u2713 postgresql biased coin example\n \u2713 postgresql DeepDive returns error for a broken application\n \u2713 postgresql chunking example\n \u2713 postgresql spouse example (tsv_extractor)\n \u2713 postgresql spouse example (json_extractor)\n \u2713 postgresql spouse example (piggy_extractor)\npostgresql spouse example (plpy_extractor) (skipped)\n \u2713 incremental spouse example (F1, then F2 and Symmetry incrementally)\n \u2713 incremental spouse example's non-incremental run (F1+F2)\n \u2713 incremental spouse example's non-incremental run (F1+F2+Symmetry)\n \u2713 similarity of probability distributions of incremental and non-incremental spouse example (F1+F2)\n \u2713 similarity of probability distributions of incremental and non-incremental spouse example (F1+F2+Symmetry)\n \u2713 util/ddext.py should work as expected\n\n13 tests, 0 failures, 1 skipped\nFinished installation for run_deepdive_tests\n```\n. @netj Thanks a lot, it works! I did that step in the original installation, but did not do it when everything was reinstalled last night. \nI might keep bothering you in the future.  Thank you for your patience and help, I really appreciate them.\n. Thanks for the reply! I cannot open https://github.com/HazyResearch/pdd though.\n. @zhangce Thanks a lot! Also I sent an email to say hi and introduce myself. Thanks!\n. @chrismre Dear Chris, thanks for the reply and all your help. I meant to reply it sooner. I am afraid I will keep asking silly questions to learn deepdive. I do not know how many people are trying to learn it, but if there are a lot, maybe it is worth setting up an online course. \n. ",
    "Atlas7": "@netj oh I see - thanks and keep up the great work!\n. @netj oh I see - thanks and keep up the great work!\n. ",
    "xiaoling": "Yeah, I was completely confused by why MindBender didn't include the supervision examples...\n. > Your insert into gives me a different result, but it seems you're inserting a literal backslash, and that's what you're seeing from the psql prompt.\nSorry for the confusion. My intention was to insert a literal backslash. In the original example, the field is for a json string where the unicode char is stored literally. I guess PG COPY is escaping the field that we don't need it to.\n\nHowever, if you use CSV, you get the literal text:\n\nThanks for the tip. But the problem is that this issue came up when the table is printed out as the input to a udf. I'm not sure if we can specify the csv format inside DeepDive & ddlog. Maybe in this case, I should do something like (borrowing the spouse app for example)?\nfunction nlp_markup over (\n        doc_id  text,\n        content text\n    ) returns rows like sentences\n    implementation \"udf/nlp_markup.sh\" handles *csv/json* lines.\nIs this supported?\n. @netj can you please confirm that init/app will not drop the database in any case?\nI see deepdive db init in init/app/run.sh. I don't think this command is supposed to drop the database because db-init in both postgres and greenplum drivers doesn't dropdb unless the number of argument is greater than zero. But I did experience some cases when it did drop the db.  Sorry can't recall the exact setting but was using the master branch a couple days ago.\n. Yeah, OPENBLAS_NUM_THREADS=1 seems to work without changing anything from deepdive.\n. it seems that the curl command didn't finish. \ncurl: (18) transfer closed with 2974881 bytes remaining to read \nprobably try again (e.g., deepdive do data/adult)?. Yeah, I was completely confused by why MindBender didn't include the supervision examples...\n. > Your insert into gives me a different result, but it seems you're inserting a literal backslash, and that's what you're seeing from the psql prompt.\nSorry for the confusion. My intention was to insert a literal backslash. In the original example, the field is for a json string where the unicode char is stored literally. I guess PG COPY is escaping the field that we don't need it to.\n\nHowever, if you use CSV, you get the literal text:\n\nThanks for the tip. But the problem is that this issue came up when the table is printed out as the input to a udf. I'm not sure if we can specify the csv format inside DeepDive & ddlog. Maybe in this case, I should do something like (borrowing the spouse app for example)?\nfunction nlp_markup over (\n        doc_id  text,\n        content text\n    ) returns rows like sentences\n    implementation \"udf/nlp_markup.sh\" handles *csv/json* lines.\nIs this supported?\n. @netj can you please confirm that init/app will not drop the database in any case?\nI see deepdive db init in init/app/run.sh. I don't think this command is supposed to drop the database because db-init in both postgres and greenplum drivers doesn't dropdb unless the number of argument is greater than zero. But I did experience some cases when it did drop the db.  Sorry can't recall the exact setting but was using the master branch a couple days ago.\n. Yeah, OPENBLAS_NUM_THREADS=1 seems to work without changing anything from deepdive.\n. it seems that the curl command didn't finish. \ncurl: (18) transfer closed with 2974881 bytes remaining to read \nprobably try again (e.g., deepdive do data/adult)?. ",
    "juhanaka": "I can take a look at this. \n. I can take a look at this. \n. ",
    "zhimingz": "I now can compile deepdive, It was my sbt version old  ,. Now I use 0.13.9, it works.\n. I now can compile deepdive, It was my sbt version old  ,. Now I use 0.13.9, it works.\n. ",
    "hartsantler": "i ran make depends it first prints out that Fedora is not supported,\nthen reminds me that i needs to install: git, make, bzip2, unzip and jdk, (which i already have), and then it prompts me if i want to continue.  I have to type y,\nthen it asks me again to make sure i have: bash, coreutils, xargs, bc, gnuplot, jre, and python (i have all these already).\ni wish that it would not prompt me to type y and just run, that way i can make an automated script to get the repo and rebuild from scratch.  how do i make a fully automated script when it asks multiple times for me to type y?\n. thanks netj, i got deepdive to compile.\nnote that in the new fedora the libnuma-devel package has been renamed to:\nnumactl-devel\n. i ran make depends it first prints out that Fedora is not supported,\nthen reminds me that i needs to install: git, make, bzip2, unzip and jdk, (which i already have), and then it prompts me if i want to continue.  I have to type y,\nthen it asks me again to make sure i have: bash, coreutils, xargs, bc, gnuplot, jre, and python (i have all these already).\ni wish that it would not prompt me to type y and just run, that way i can make an automated script to get the repo and rebuild from scratch.  how do i make a fully automated script when it asks multiple times for me to type y?\n. thanks netj, i got deepdive to compile.\nnote that in the new fedora the libnuma-devel package has been renamed to:\nnumactl-devel\n. ",
    "vipulved": "On recent Ubuntu, this package is called libnuma-dev.\n. On recent Ubuntu, this package is called libnuma-dev.\n. ",
    "elvin2014lee": "Thanks a lot !!!!!!! I encountered the same problem as you . \n. Thanks a lot !!!!!!! I encountered the same problem as you . \n. ",
    "ThomasPalomares": "Ahah sure. Also, concerning your previous comments, I added comments near the beginning and checked the trail spaces with a make checkstyle. I am not sure about how to highlight the new part of the code though \n. ok rebased to 959282f !\n. @chrismre: this fixes the issue about distributed_by in temporary tables. \nWith that + a bug in mkimo that Jaeho noticed, the time consuming extractions are now ~50x faster\n. No, the problem mentioned today is fixed in https://github.com/HazyResearch/deepdive/pull/453 ! This PR is for another (smaller) issue\n. Looks great @netj ! \nmake test ONLY=test/postgresql/deepdive_load.bats runs without any failure and it loads correctly the sentences_input.sql file in the genomics application, is it enough to merge this PR or is there another test I should try first ?\n. Hi @netj and @ajratner, \nthanks for the comments. I tried to create a test for parsing the NastyTSV but I have two issues:\n- the parsing fails when there is a 'boolean' in @tsv_extractor or @returns (surprisingly since it is only 't' and 'f' in the nasty TSV).\n- it works when I modify the booleans to the type 'text', but then fails on the last column of the NastyTSV. I am not sure why (and I am a bit lost in this last array with all the \"/\" and escapes), let me know if you know why it fails !\n. ah maybe ! the error I have is:\nERROR:  malformed array literal: \"{\"asdf  qwer\\tzxcv\\n1234\",,\"NULL\",\"null\",\"\\N\",\"N\",\"\\I'm your father\",\"\\\" said Darth Vader.\\\"\",\"{\\csv in a json\\\": \\\"a\",\"b c\",\"\\\\\"\",\"\\\\\"\",\"\\\\\"line '1'\\nbogus\",\"NULL\",\"null\",\"\\\\N\",\"N\",\"line \\\\\"\\\\\"2\\\\\"\\\\\"\\\"\",\"  \\\"foo\\\":123\",\"\\n\\\"bar\\\":45.678\",\" \\\"null\\\": \\\"\\\\N\\\"}\\\"\"}\"\nso it may be because of that. Do you know if there is a way to modify this in psql or should the ddlib/util.py be modified to take that into account ?\n. Ahah sure. Also, concerning your previous comments, I added comments near the beginning and checked the trail spaces with a make checkstyle. I am not sure about how to highlight the new part of the code though \n. ok rebased to 959282f !\n. @chrismre: this fixes the issue about distributed_by in temporary tables. \nWith that + a bug in mkimo that Jaeho noticed, the time consuming extractions are now ~50x faster\n. No, the problem mentioned today is fixed in https://github.com/HazyResearch/deepdive/pull/453 ! This PR is for another (smaller) issue\n. Looks great @netj ! \nmake test ONLY=test/postgresql/deepdive_load.bats runs without any failure and it loads correctly the sentences_input.sql file in the genomics application, is it enough to merge this PR or is there another test I should try first ?\n. Hi @netj and @ajratner, \nthanks for the comments. I tried to create a test for parsing the NastyTSV but I have two issues:\n- the parsing fails when there is a 'boolean' in @tsv_extractor or @returns (surprisingly since it is only 't' and 'f' in the nasty TSV).\n- it works when I modify the booleans to the type 'text', but then fails on the last column of the NastyTSV. I am not sure why (and I am a bit lost in this last array with all the \"/\" and escapes), let me know if you know why it fails !\n. ah maybe ! the error I have is:\nERROR:  malformed array literal: \"{\"asdf  qwer\\tzxcv\\n1234\",,\"NULL\",\"null\",\"\\N\",\"N\",\"\\I'm your father\",\"\\\" said Darth Vader.\\\"\",\"{\\csv in a json\\\": \\\"a\",\"b c\",\"\\\\\"\",\"\\\\\"\",\"\\\\\"line '1'\\nbogus\",\"NULL\",\"null\",\"\\\\N\",\"N\",\"line \\\\\"\\\\\"2\\\\\"\\\\\"\\\"\",\"  \\\"foo\\\":123\",\"\\n\\\"bar\\\":45.678\",\" \\\"null\\\": \\\"\\\\N\\\"}\\\"\"}\"\nso it may be because of that. Do you know if there is a way to modify this in psql or should the ddlib/util.py be modified to take that into account ?\n. ",
    "Colossus": "That's crazy\n. No\n\nOn Jan 14, 2016, at 19:54, chrismre notifications@github.com wrote:\ncc: @HazyResearch/genomics Does this fix the issue from today's meeting?\n\u2014\nReply to this email directly or view it on GitHub.\n. That's crazy\n. No\nOn Jan 14, 2016, at 19:54, chrismre notifications@github.com wrote:\ncc: @HazyResearch/genomics Does this fix the issue from today's meeting?\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "paidi": "Thanks @netj. That was what I figured and have completed the walkthrough using a single --reg_param \n. Thanks for the quick response! I will try that today.. Thanks @netj. That was what I figured and have completed the walkthrough using a single --reg_param \n. Thanks for the quick response! I will try that today.. ",
    "skprasadu": "There is this link, please see if it is useful.. for me somehow, the linking between Postgres and Docker was not working.. \nhttp://deepdive.stanford.edu/doc/advanced/docker.html\nIf you make it work, please let me know how it works.\nThanks\nKrishna\n. There is this link, please see if it is useful.. for me somehow, the linking between Postgres and Docker was not working.. \nhttp://deepdive.stanford.edu/doc/advanced/docker.html\nIf you make it work, please let me know how it works.\nThanks\nKrishna\n. ",
    "joshneland": "I have a working dockerfile to commit, but it's having a problem with the spouse_example in 0.8 that I'm troubleshooting first.\n. I have a working dockerfile to commit, but it's having a problem with the spouse_example in 0.8 that I'm troubleshooting first.\n. ",
    "lanphan": "@joshneland @netj \nIs there any progress on this task? I have some free time this weekend, and I already have Deepdive Docker to share with you. Just need some time to verify and add Docker Compose.\n. Hi @igozali\n\nEarly tests show that there are indeed some usefulness of this extension. Using the Infolab compute cluster, I was able to process the whole spouse articles (signalmedia-1m.jsonl, around 200MB after greping for wife, spouse, etc.) in an hour by using 25 nodes (as defined by PBS).\n\nAs I also ran spouse tutorial on 1 machine only, but I try with 2 different OSes:\n- MacOS (laptop, core i7, SSD, 16G RAM):\n  - full process (deepdive do probabilities): around 9 hours\n  - noted that I did use patch version mkmimo.sh from Jaeho (see #522) + use sed to remove \\r, not grep (see #523)\n- Ubuntu (PC, core i7, HDD, 32G RAM):\n  - deepdive do has_spouse: around 4h30. \"deepdive do probabilities\" got problem (see #524)\n  - noted that I did use sed to remove \\r, not grep (see #523)\nSo that I'd like want to know your configuration to see how much speedup it is. Would you please tell I know?\nThanks.\n. @igozali \nI'd like to ask you about configuration of your cluster (how many RAM, CPU power for each node in your cluster, etc ...)? Because I don't see much speedup comparing to result of my 1 PC only.\n. @netj \nThanks for your quick response.\nHowever, as in #509, and I review there is really a fix value for THROTTLE_SLEEP_MSEC in case of MacOS:\n```\n    # OS specific workarounds via tweaking the environment\n    case $(uname) in\n        Darwin)\n            # XXX mkmimo can reboot Mac unless its use of poll(2) is throttled\n            export THROTTLE_SLEEP_MSEC=1\n            ;;\n    esac\n```\nDoes that mean I need to customize value of THROTTLE_SLEEP_MSEC in deepdive?\nI'll try and report here soon.\n. @netj \nHi Jaeho,\nI did as you said, but it still crashed after around 9min.\nIs there anyway to workaround this bug? Should I increase THROTTLE_SLEEP_MSEC to 20 or 100?\nBelow is attached of my postgres db, comparing with the above image, data is increased a little, but still stuck in sentence table creation.\n\n. @netj \nBefore trying your proposed solution, I did report that I ran deepdive with THROTTLE_SLEEP_MSEC=50, deepdive runs well around 15 minute (I use \"run well\" because CPU and RAM usage is under control, CPU ~ 20% usage, RAM ~ 4G usage; and has only 1 java process), but it still crash after that. Data throughput is less than my 2nd try above.\nNow I'll try with new mkmimo patch and report soon here.\nThanks for your support.\n. @netj \nBelow is my result after trying your first point (using fix-for-mac-reboots branch of mkmimo):\n- 1st try:\n  export THROTTLE_SLEEP_USEC=1000\n  export DEEPDIVE_NUM_PROCESSES=1\nCrashed after 1h37m, attached is my data in postgres.\n\n- 2nd try:\n  export THROTTLE_SLEEP_USEC=500\n  export DEEPDIVE_NUM_PROCESSES=1\nCrashed after 7m, with data throughput is little over the case in my first comment.\n- 3rd try:\n  export THROTTLE_SLEEP_USEC=20000\n  export DEEPDIVE_NUM_PROCESSES=1\nCrashed after 5m, with data throughput is almost the same with 2nd try above.\n--> Conclusion: seems that it still has bug. I don't know why my first try with USEC=1000 can last to 1 hour 37 minute, but my second (USEC=500 < 1000) and third try (USEC=20000 > 1000) both failed so fast.\n. @netj \nI think it runs ok with your dumb version written in bash (your 2/ approach). However, after running 3 hours, I got error from posgres (issue #523).\nBelow is my quick comparison between dump version and official mkmimo:\n- dump version:\n      ++ run well\n      ++ resource consumption: around 80% - 90% CPU usage, 10GB - 14GB RAM\n- official mkmimo: \n      ++ got problem in MacOS\n      ++ resource consumption: around 15% - 25% CPU usage, 2GB - 4GB RAM\nI'm going to evaluate deepdive on Linux (Ubuntu) soon this week.\n. @netj \nI can run successfully with approach 2/ after issue #523 fixed.\nHowever, it took me 8 hours to finish \"deepdive do spouse_feature\". Will I wait for your fix to improve speed, Jaeho?\nIn the meantime, I switch to Ubuntu desktop PC (32G RAM, core i7) to see it can run well and faster.\n. Deepdive ran very well on Ubuntu, it finished \"deepdive do spouse_feature\" in around 3h40m (using official deepdive / mkmimo version, no patch).\n@netj I wonder that I can use your first patch in order to use _USEC (10 to 100) to improve performance?\n. @netj \nThanks for your tips, I'll try soon. Are these parameters (DEEPDIVE_NUM_PARALLEL_UNLOADS and DEEPDIVE_NUM_PARALLEL_LOADS) can be used with Greenplum too?\n. @netj \nSetting DEEPDIVE_NUM_PARALLEL_UNLOADS=3 and DEEPDIVE_NUM_PARALLEL_LOADS=3 (run on Ubuntu) help to increase performance much more: only took 2h30m to finish deepdive do has_spouse (has_spouse is a step after spouse_feature, and deepdive do spouse_feature in previous comment took 3h40m already). Are these parameters useful for other db (Greenplum, PostgresXL)?\n. @netj \nI only comment \"head -100 |\" in order to get all relevant data.\nI think current gsub is not enough, see my attached file below to see result of grep '\\\\r' input/articles-1m.tsv with articles-1m.tsv is an output of input/articles.tsv.sh\nstill_contains_r.txt\n. @netj \nI rename \"still_contains_r.txt\" to \"articles.tsv\" and run \"deepdive do sentences\", I got the same error.\nWould you please re-open this bug? (don't see Open or Reopen here).\nPs: thanks to your feedback, I understand that input/articles.tsv.sh is as pre-process step for Deepdive also (runs inside Deepdive, not separated step).\n. Hi all,\nSwitching to Mac, I'm able to run \"deepdive do probabilities\" on it. I guessed that bug above I got in Ubuntu caused by graphviz (I remote accessed to Ubuntu machine and don't know which softwares installed on it. For Mac, it's my PC and I installed full of software required).\nBTW, I got 1 ERROR line while running \"deepdive do probabilities\" on Mac, can I skip it? \n2016-03-18 16:38:30.169470 pbzip2: *ERROR: File [factorgraph/variables/has_spouse/variables.part-7.bin.bz2] is NOT a valid bzip2!  Skipping...\n. @netj \nThank Jaeho, I'll test this again with Deepdive 0.8.1.\nBTW, is there any process to upgrade Deepdive? Or, I'll install again to make it overwrite?\n. @chrismre \nNot yet tested again, I'll do it soon this week.\nHowever, it seems that new DeepDive got some problems with installation (comment from netza2 in gitter + my error report in #530), would you please take a look at these problems?\n. @joshneland @netj \nIs there any progress on this task? I have some free time this weekend, and I already have Deepdive Docker to share with you. Just need some time to verify and add Docker Compose.\n. Hi @igozali\n\nEarly tests show that there are indeed some usefulness of this extension. Using the Infolab compute cluster, I was able to process the whole spouse articles (signalmedia-1m.jsonl, around 200MB after greping for wife, spouse, etc.) in an hour by using 25 nodes (as defined by PBS).\n\nAs I also ran spouse tutorial on 1 machine only, but I try with 2 different OSes:\n- MacOS (laptop, core i7, SSD, 16G RAM):\n  - full process (deepdive do probabilities): around 9 hours\n  - noted that I did use patch version mkmimo.sh from Jaeho (see #522) + use sed to remove \\r, not grep (see #523)\n- Ubuntu (PC, core i7, HDD, 32G RAM):\n  - deepdive do has_spouse: around 4h30. \"deepdive do probabilities\" got problem (see #524)\n  - noted that I did use sed to remove \\r, not grep (see #523)\nSo that I'd like want to know your configuration to see how much speedup it is. Would you please tell I know?\nThanks.\n. @igozali \nI'd like to ask you about configuration of your cluster (how many RAM, CPU power for each node in your cluster, etc ...)? Because I don't see much speedup comparing to result of my 1 PC only.\n. @netj \nThanks for your quick response.\nHowever, as in #509, and I review there is really a fix value for THROTTLE_SLEEP_MSEC in case of MacOS:\n```\n    # OS specific workarounds via tweaking the environment\n    case $(uname) in\n        Darwin)\n            # XXX mkmimo can reboot Mac unless its use of poll(2) is throttled\n            export THROTTLE_SLEEP_MSEC=1\n            ;;\n    esac\n```\nDoes that mean I need to customize value of THROTTLE_SLEEP_MSEC in deepdive?\nI'll try and report here soon.\n. @netj \nHi Jaeho,\nI did as you said, but it still crashed after around 9min.\nIs there anyway to workaround this bug? Should I increase THROTTLE_SLEEP_MSEC to 20 or 100?\nBelow is attached of my postgres db, comparing with the above image, data is increased a little, but still stuck in sentence table creation.\n\n. @netj \nBefore trying your proposed solution, I did report that I ran deepdive with THROTTLE_SLEEP_MSEC=50, deepdive runs well around 15 minute (I use \"run well\" because CPU and RAM usage is under control, CPU ~ 20% usage, RAM ~ 4G usage; and has only 1 java process), but it still crash after that. Data throughput is less than my 2nd try above.\nNow I'll try with new mkmimo patch and report soon here.\nThanks for your support.\n. @netj \nBelow is my result after trying your first point (using fix-for-mac-reboots branch of mkmimo):\n- 1st try:\n  export THROTTLE_SLEEP_USEC=1000\n  export DEEPDIVE_NUM_PROCESSES=1\nCrashed after 1h37m, attached is my data in postgres.\n\n- 2nd try:\n  export THROTTLE_SLEEP_USEC=500\n  export DEEPDIVE_NUM_PROCESSES=1\nCrashed after 7m, with data throughput is little over the case in my first comment.\n- 3rd try:\n  export THROTTLE_SLEEP_USEC=20000\n  export DEEPDIVE_NUM_PROCESSES=1\nCrashed after 5m, with data throughput is almost the same with 2nd try above.\n--> Conclusion: seems that it still has bug. I don't know why my first try with USEC=1000 can last to 1 hour 37 minute, but my second (USEC=500 < 1000) and third try (USEC=20000 > 1000) both failed so fast.\n. @netj \nI think it runs ok with your dumb version written in bash (your 2/ approach). However, after running 3 hours, I got error from posgres (issue #523).\nBelow is my quick comparison between dump version and official mkmimo:\n- dump version:\n      ++ run well\n      ++ resource consumption: around 80% - 90% CPU usage, 10GB - 14GB RAM\n- official mkmimo: \n      ++ got problem in MacOS\n      ++ resource consumption: around 15% - 25% CPU usage, 2GB - 4GB RAM\nI'm going to evaluate deepdive on Linux (Ubuntu) soon this week.\n. @netj \nI can run successfully with approach 2/ after issue #523 fixed.\nHowever, it took me 8 hours to finish \"deepdive do spouse_feature\". Will I wait for your fix to improve speed, Jaeho?\nIn the meantime, I switch to Ubuntu desktop PC (32G RAM, core i7) to see it can run well and faster.\n. Deepdive ran very well on Ubuntu, it finished \"deepdive do spouse_feature\" in around 3h40m (using official deepdive / mkmimo version, no patch).\n@netj I wonder that I can use your first patch in order to use _USEC (10 to 100) to improve performance?\n. @netj \nThanks for your tips, I'll try soon. Are these parameters (DEEPDIVE_NUM_PARALLEL_UNLOADS and DEEPDIVE_NUM_PARALLEL_LOADS) can be used with Greenplum too?\n. @netj \nSetting DEEPDIVE_NUM_PARALLEL_UNLOADS=3 and DEEPDIVE_NUM_PARALLEL_LOADS=3 (run on Ubuntu) help to increase performance much more: only took 2h30m to finish deepdive do has_spouse (has_spouse is a step after spouse_feature, and deepdive do spouse_feature in previous comment took 3h40m already). Are these parameters useful for other db (Greenplum, PostgresXL)?\n. @netj \nI only comment \"head -100 |\" in order to get all relevant data.\nI think current gsub is not enough, see my attached file below to see result of grep '\\\\r' input/articles-1m.tsv with articles-1m.tsv is an output of input/articles.tsv.sh\nstill_contains_r.txt\n. @netj \nI rename \"still_contains_r.txt\" to \"articles.tsv\" and run \"deepdive do sentences\", I got the same error.\nWould you please re-open this bug? (don't see Open or Reopen here).\nPs: thanks to your feedback, I understand that input/articles.tsv.sh is as pre-process step for Deepdive also (runs inside Deepdive, not separated step).\n. Hi all,\nSwitching to Mac, I'm able to run \"deepdive do probabilities\" on it. I guessed that bug above I got in Ubuntu caused by graphviz (I remote accessed to Ubuntu machine and don't know which softwares installed on it. For Mac, it's my PC and I installed full of software required).\nBTW, I got 1 ERROR line while running \"deepdive do probabilities\" on Mac, can I skip it? \n2016-03-18 16:38:30.169470 pbzip2: *ERROR: File [factorgraph/variables/has_spouse/variables.part-7.bin.bz2] is NOT a valid bzip2!  Skipping...\n. @netj \nThank Jaeho, I'll test this again with Deepdive 0.8.1.\nBTW, is there any process to upgrade Deepdive? Or, I'll install again to make it overwrite?\n. @chrismre \nNot yet tested again, I'll do it soon this week.\nHowever, it seems that new DeepDive got some problems with installation (comment from netza2 in gitter + my error report in #530), would you please take a look at these problems?\n. ",
    "BrettMeehan": "@SenWu I'm curently working on adding checkpointing to the learning and inference steps of the Gibbs sampler so the application can be paused and resumed. Also browsing Feiran's partition code from 1 year ago to see what parts we can merge to the main branch.\n. @SenWu I'm curently working on adding checkpointing to the learning and inference steps of the Gibbs sampler so the application can be paused and resumed. Also browsing Feiran's partition code from 1 year ago to see what parts we can merge to the main branch.\n. ",
    "igozali": "@netj Should I do anything else on this? I need to pull this to my branch eventually, right?\n. Looks good to me! Just some questions below. \nWhen we were discussing the idea of having other compute-* (such as compute-status, compute-loadin, compute-unloadout, compute-submit) commands, we also talked about requiring DeepDive be installed on the submission nodes as well. I was wondering if running deepdive compute status (for example) should be allowed on both our workstation (laptop, etc.) and the submission node? \nOr would it only make sense to run these compute-* commands inside the context of compute-execute (i.e. compute-* commands would be \"private functions\" being used only by compute-execute)?\n. LGTM\n. Reminder in this pull request to discuss compute-* filenames; maybe rename it to something better?\n. Hi @lanphan, \nApologies for the delay, I haven't had time to maintain this pull request until now. \nThis branch requires that you have a cluster with more than one machine that is using the Torque scheduler so that you can utilize multiple nodes to run the jobs in the workflow. If you only have one machine, I'm afraid you can't use this feature. \n. We have somewhere between 10-30 nodes in the clusters, each having CPU cores in the teens, and sufficient memory. We don't seem to see much speedup due to our DFS configuration, but we haven't verified this yet.\n. @netj wow you're actually going over old pull requests! :)\n. @netj Should I do anything else on this? I need to pull this to my branch eventually, right?\n. Looks good to me! Just some questions below. \nWhen we were discussing the idea of having other compute-* (such as compute-status, compute-loadin, compute-unloadout, compute-submit) commands, we also talked about requiring DeepDive be installed on the submission nodes as well. I was wondering if running deepdive compute status (for example) should be allowed on both our workstation (laptop, etc.) and the submission node? \nOr would it only make sense to run these compute-* commands inside the context of compute-execute (i.e. compute-* commands would be \"private functions\" being used only by compute-execute)?\n. LGTM\n. Reminder in this pull request to discuss compute-* filenames; maybe rename it to something better?\n. Hi @lanphan, \nApologies for the delay, I haven't had time to maintain this pull request until now. \nThis branch requires that you have a cluster with more than one machine that is using the Torque scheduler so that you can utilize multiple nodes to run the jobs in the workflow. If you only have one machine, I'm afraid you can't use this feature. \n. We have somewhere between 10-30 nodes in the clusters, each having CPU cores in the teens, and sufficient memory. We don't seem to see much speedup due to our DFS configuration, but we haven't verified this yet.\n. @netj wow you're actually going over old pull requests! :)\n. ",
    "henryre": "Sounds good\n. Whoa what a throwback\n. Sounds good\n. Whoa what a throwback\n. ",
    "JBKBO": "@naddou14 can you please give some more hints where exactly one has to put the file? \nIm want to replace the NER model with my own model but have no clue where to do it. \n. @naddou14 can you please give some more hints where exactly one has to put the file? \nIm want to replace the NER model with my own model but have no clue where to do it. \n. ",
    "syadlowsky": "@netj Re: our discussion earlier\n. @thodrek\n. @netj Re: our discussion earlier\n. @thodrek\n. ",
    "raphaelhoff": "Here's what I would like to do. Let's assume we are building a NER tagger that assigns a label to each word in a document.\nOur set of labels is labels = { HOUSE_NUM, STREET, APT, SUITE, ZIP, CITY, ZIP, PO_BOX, STATE, O} and we create a categorical random variable v1, v2, v3 for each word. \nA labeled document:\nO     O     O  CITY       O O   O       O HOUSE_NUM STREET STREET  APT APT CITY       CITY CITY ZIP   O\nPeter lives in Washington . His address : 3432      New    Blvd    Apt 3   Washington ,    DC   20001 .\n1. We would like to learn how likely different mentions of the same word in a document will get the same label, so that the system will make the first mention of Washington a CITY and not a STATE. \n   We add the following ddlog factor:\n@weight(\"?\")\n   Multinomial(tag(word_id_1), tag(word_id_2)) :- ...\nThe problem is that we are now learning 100 weights, ie. for each combination of labels. But actually all other weights are irrelevant to us, we would only like to distinguish two cases:\n    a) two mentions of the same word have the same label\n    b) two mentions of the same word have different label\n2. We would also like to learn that certain label transitions are more likely than others. For example, we might see HOUSE_NUM STREET STREET or HOUSE_NUM STREET APT, but probably not HOUSE_NUM ZIP O or STREET APT STREET. Again, there's no need to learn a weight for every combination of labels, since most simply don't make sense.\n3. We may want to condition on some words. For example, when seeing \"address :\", the labels for following tokens may be HOUSE_NUM STREET or perhaps PO_BOX CITY. Again, we want to rule out impossible combinations, learning only a single weight for all of them combined (or perhaps not letting the sampler consider such combinations at all).\nNow, suppose we only had two labels { CITY, O }, then we could use our Boolean factor functions, eg.\nAnd(..,..) to achieve some of the above.\n. I think the \"equal pair\" and \"filter based on the categorical value\" would get us a long way. @alldefector\nCan you point me to the \"equal pair\" factor for Multinomials? @netj Can I do the filtering based on categorical value?\nSome more background: My main motivation is to improve quality, though performance may matter. The predicted sequence of labels sometimes \"doesn't make sense\", so I am thinking that factors over 2, 3, 4 variables (representing adjacent words) may help. \nHere is an example that shows why 3 or 4 variable factors may be useful: Suppose we have the following\nlabel sequence and we are unsure about the ?? label.\n?? CITY STATE ZIP\n?? CITY STATE O\nIn the first case, the ?? label is likely to be STREET, not O. In the second case, the ?? is likely to be O, but not STREET. So, the label on the fourth word helps disambiguate the first. Why? In the first case, it's more likely that we are seeing a full address, but in the second case it's more likely we are just seeing a reference to a city in a sentence since there is no ZIP code. I have more such examples, also for PERSON name extraction.\nUnfortunately, a factor over four variables adds 10,000 weights here, and more if we consider not just\nlocations.\nMost such \"wrong\" label sequences will never get generated, so their weight will be 0. However, some of the \"wrong\" label sequences will be knocked down during learning and will get a negative weight; with fewer weights, this negative weight would apply to other \"wrong\" sequences as well, and the overall quality of the system will likely improve.\n. @shahin The issue is that in my case the null values appear inside an array, so they are not captured by the check in main. Also, when in an array, gp seems to write nulls differently. See here:\n```\n$ psql -c \"COPY (select null::int) to stdout\"\n\\N\n$ psql -c \"COPY (select '{1,null}'::int[]) to stdout\"\n{1,NULL}\n```\n. @shahin That looks good, thanks! I made the change. The null values are also correctly handled for timestamps and booleans now. I see that text is handled separately, but I find the following example shocking: \n$ psql -c \"COPY (select '{\"NULL\",null}'::text[]) TO STDOUT\"\n{NULL,NULL}\nDo you know if we're able to distinguish between \"NULL\" and null?\n. Thanks @netj. Totally agree that we should switch to JSON eventually; for the short term, let's keep the PG TSV solution alive.\n. Here's what I would like to do. Let's assume we are building a NER tagger that assigns a label to each word in a document.\nOur set of labels is labels = { HOUSE_NUM, STREET, APT, SUITE, ZIP, CITY, ZIP, PO_BOX, STATE, O} and we create a categorical random variable v1, v2, v3 for each word. \nA labeled document:\nO     O     O  CITY       O O   O       O HOUSE_NUM STREET STREET  APT APT CITY       CITY CITY ZIP   O\nPeter lives in Washington . His address : 3432      New    Blvd    Apt 3   Washington ,    DC   20001 .\n1. We would like to learn how likely different mentions of the same word in a document will get the same label, so that the system will make the first mention of Washington a CITY and not a STATE. \n   We add the following ddlog factor:\n@weight(\"?\")\n   Multinomial(tag(word_id_1), tag(word_id_2)) :- ...\nThe problem is that we are now learning 100 weights, ie. for each combination of labels. But actually all other weights are irrelevant to us, we would only like to distinguish two cases:\n    a) two mentions of the same word have the same label\n    b) two mentions of the same word have different label\n2. We would also like to learn that certain label transitions are more likely than others. For example, we might see HOUSE_NUM STREET STREET or HOUSE_NUM STREET APT, but probably not HOUSE_NUM ZIP O or STREET APT STREET. Again, there's no need to learn a weight for every combination of labels, since most simply don't make sense.\n3. We may want to condition on some words. For example, when seeing \"address :\", the labels for following tokens may be HOUSE_NUM STREET or perhaps PO_BOX CITY. Again, we want to rule out impossible combinations, learning only a single weight for all of them combined (or perhaps not letting the sampler consider such combinations at all).\nNow, suppose we only had two labels { CITY, O }, then we could use our Boolean factor functions, eg.\nAnd(..,..) to achieve some of the above.\n. I think the \"equal pair\" and \"filter based on the categorical value\" would get us a long way. @alldefector\nCan you point me to the \"equal pair\" factor for Multinomials? @netj Can I do the filtering based on categorical value?\nSome more background: My main motivation is to improve quality, though performance may matter. The predicted sequence of labels sometimes \"doesn't make sense\", so I am thinking that factors over 2, 3, 4 variables (representing adjacent words) may help. \nHere is an example that shows why 3 or 4 variable factors may be useful: Suppose we have the following\nlabel sequence and we are unsure about the ?? label.\n?? CITY STATE ZIP\n?? CITY STATE O\nIn the first case, the ?? label is likely to be STREET, not O. In the second case, the ?? is likely to be O, but not STREET. So, the label on the fourth word helps disambiguate the first. Why? In the first case, it's more likely that we are seeing a full address, but in the second case it's more likely we are just seeing a reference to a city in a sentence since there is no ZIP code. I have more such examples, also for PERSON name extraction.\nUnfortunately, a factor over four variables adds 10,000 weights here, and more if we consider not just\nlocations.\nMost such \"wrong\" label sequences will never get generated, so their weight will be 0. However, some of the \"wrong\" label sequences will be knocked down during learning and will get a negative weight; with fewer weights, this negative weight would apply to other \"wrong\" sequences as well, and the overall quality of the system will likely improve.\n. @shahin The issue is that in my case the null values appear inside an array, so they are not captured by the check in main. Also, when in an array, gp seems to write nulls differently. See here:\n```\n$ psql -c \"COPY (select null::int) to stdout\"\n\\N\n$ psql -c \"COPY (select '{1,null}'::int[]) to stdout\"\n{1,NULL}\n```\n. @shahin That looks good, thanks! I made the change. The null values are also correctly handled for timestamps and booleans now. I see that text is handled separately, but I find the following example shocking: \n$ psql -c \"COPY (select '{\"NULL\",null}'::text[]) TO STDOUT\"\n{NULL,NULL}\nDo you know if we're able to distinguish between \"NULL\" and null?\n. Thanks @netj. Totally agree that we should switch to JSON eventually; for the short term, let's keep the PG TSV solution alive.\n. ",
    "RominYue": "Another problem when I just reuse the weight.\nAfter pre-processing of NLP step, I type in the following command:\n$ deepdive model weights reuse FOO\nand the step goes forward to combine_factorgraph, but then\n$ deepdive model infer\noccurs the following error:\n2016-05-07 12:50:49.932967 + mkdir -p probabilities\n2016-05-07 12:50:49.935240 + mv -f weights/inference_result.out.text probabilities/\n2016-05-07 12:50:49.938343 mv: cannot stat \u2018weights/inference_result.out.text\u2019: No such file or directory\n\u2018run/ABORTED\u2019 -> \u201820160507/125042.142224488\u2019\nThe detail info shows as below:\n\n\n\n. Another problem when I just reuse the weight.\nAfter pre-processing of NLP step, I type in the following command:\n$ deepdive model weights reuse FOO\nand the step goes forward to combine_factorgraph, but then\n$ deepdive model infer\noccurs the following error:\n2016-05-07 12:50:49.932967 + mkdir -p probabilities\n2016-05-07 12:50:49.935240 + mv -f weights/inference_result.out.text probabilities/\n2016-05-07 12:50:49.938343 mv: cannot stat \u2018weights/inference_result.out.text\u2019: No such file or directory\n\u2018run/ABORTED\u2019 -> \u201820160507/125042.142224488\u2019\nThe detail info shows as below:\n\n\n\n. ",
    "shahin": "@alldefector Good suggestion. I tried passing the timestamp string through unchanged, but jq can't handle that particular format. This suggests to me that other JSON tools won't either, even if ES does.\n@netj Our timestamps are time zone naive and don't have offsets (they use Postgres' timestamp without time zone type), so there is no time zone setting in psql that can result in a correct offset down the line. We could change these columns to timestamp with time zone but now we're asking the application to support time zones for a really obscure technical reason.\nI pushed 0a794ff in an attempt to resolve this. Before this change, pgtsv_to_json didn't support any timestamps. After this change, pgtsv_to_json explicitly supports the timestamp without time zone type without qualification. Looks to me like jq and Elastic support the output without changes elsewhere.\nPlease let me know if you think it produces inaccurate results, or if it doesn't support a feature that we need!\n. @netj I agree, it's weird and certainly incorrect to leave out time zone info! Unfortunately adding time zone support to our app is way beyond the scope of my current task (moving specific processing from Postgres to Greenplum).\nI don't think I made the motivation clear in my initial PR, and my original patch was definitely flawed. Sorry about that! The motivation for this PR is twofold:\n1. pgtsv_to_json terminates on a ValueError when dumping timestamps of any kind, and\n2. jq crashes when we send it psql-formatted timestamp strings, which are missing the \"T\" character and as you pointed out are not valid ISO 8601.\nSeems like setting psql's time zone to UTC for this one dump doesn't fix those problems, but does make our timestamps incorrect until we support time zones from the very beginning of the pipeline. As it stands the data source (PostgreSQL/Greenplum) doesn't contain time zone information, and no combination of psql runtime directives can create that information out of thin air.\nWould you be open to my approach in 0a794ff along with a promise to open a ticket for proper time zone handling in downstream apps? I'd love to fix that at some point.\n. @alldefector Looks like CI is failing, but I don't think it's caused by this diff. Can I merge and push on the command line anyway? Should I open a ticket for fixing CI here?\n. Closing in favor of #551 \n. @raphaelhoffmann there's a check for the null character \\\\N down below in main, before convert is called. \\\\N is the default for the null character. Are we seeing it dump NULL instead?\n. @raphaelhoffmann I see, makes sense. I can see this implementation causing unexpected results, since one might assume that \\\\N represents null for integers when reading main but in fact \\\\N or NULL represent null. Sounds like this only applies inside arrays. Could we isolate this change inside the array-handling code?\nOne approach might be to compose the existing conversion around line 51 with your NULL check in a lambda:\nconvert_or_null = lambda x: None if x == 'NULL' else convert(x)\nreturn map(convert_or_null, arr)\nWhat do you think?\n. @raphaelhoff wow, that is very surprising. I would not have expected it, but I think the issue there is the ::text[] type cast. Without it we get reasonable results:\n$ psql postgres -c \"copy (select ARRAY['NULL',null]) to stdout\"\n{\"NULL\",NULL}\nApparently ::<type>[] goes through every element and converts it to <type>. They call null a \"generic constant\", so I guess when you convert this generic constant to text it's no longer a null value. Very weird.\n. No, I'm not concerned about NULLs arrays here. Switching to (a) Postgres' CSV format and (b) Python 3 means we can handle unicode with way less custom escaping/unescaping. @alldefector pointed me in the right direction here.\nAll the Postgres TEXT format encoding caveats plus all the Python 2-specific encoding caveats added up to a pile of brittle exception code (see: most of my commits over the past few days).\nWe won't need to distinguish between ,\"\", and ,, as long as we use an explicit null as '\\\\N' or similar.\n. @netj Yeah, I would also like to switch back to Travis's container-based infra. It is possible that bash 4.3 is all that's needed. Two questions:\n1. Why do we bundle bash with DeepDive? This seems like a very low-level dependency to include in our distribution. As a DeepDive user or new developer, I imagine I'd be very surprised to find out (eventually) that it's not using the same bash that I'm using on my host system. Seems like an explicit dependency on a given bash version would be clearer.\n2. As long as we're bundling bash, should we be using it for all tests? An exception for a single .bats seems easily overlooked, and now our tests are targeting different platforms. You know more about bats than I do -- what's the best way to run them all on bash 4.3?\n. > Mac users wasted many hours/days if not weeks because of these since it's bash 3 and BSD.\nWas this before homebrew/macports/fink? All the Mac users I know install tons of packages with GNU/newer dependencies, but I don't think many of those actually bundle bash or coreutils. I wonder if we could revisit this later and simplify our build.\n. @netj @alldefector this is all tangential, sorry for hijacking this thread:\n\nthey often don't have control over the compute resources they want to run DeepDive on\n\nInteresting! Sounds like I need to learn more about our users (and what you mean by \"control\"). The deepdive-users list is an obvious place to start, but it's sparse and biased. Do we have other data about our user base that I could check out?\n\nMany users are in fact struggling and mentioning they waste most time at installing dependencies we don't bundle\n\nIsn't this exactly what dependency management tools are for?\n\nIf we start distributing releases via Homebrew, APT, yum, then yes we should install the runtime dependencies natively rather than bundling. However, that requires more maintenance effort\n\nIn many ways a package manager reduces maintenance effort. Package managers take things like CPU architecture, security patches, bug fixes, API-stable updates, custom shared libraries, etc. into account when resolving dependencies. If we're saving time on maintenance today then it's because we're ignoring all these things, not because we've found a way to do them better than a package manager!\n\nand lower coverage\n\nThe abstraction we get from ordinary dependency management seems likely to increase coverage, not lower it. If we just specify dependency names then DeepDive can be ported to any platform that implements those names, including the BSDs, Cygwin, or whatever. If we bundle dependency code or binaries, then it'll run only on platforms that we take the time to bundle dependencies for. What am I missing here?\n. This is very cool. Can you include stats on wall time and space savings for TSJ vs JSON, and TSJ vs TSV?\nI know that you wrote this specifically for our Deepdive use cases, but serde isn't a Deepdive-specific function. Can we break your core TSJ parsing/serialization code into a separate Python package? This will make it more testable, transparent and composable. If it's out of scope here, feel free to open a ticket so that we can get to it later.\nI see that DEEPDIVE_USE_TSV2TSJ lets us switch from the Python lib to a Perl version. When should we do this? The Perl stuff should probably be pulled out into its own installable module, too.\n. @netj Thanks for pasting those numbers. I remember seeing them before, but they don't contain any comparison to JSON lines. Since JSON solves our NULL-encoding problems and is very common, seems to me like this is the proper baseline for comparison.\nI agree that we can cook up any performance numbers we want by picking and choosing tables. I'm looking for persuasive numbers to convince new hires that they should learn and maintain this new format instead of JSON lines.\nMaybe we have a bunch of really wide tables with really long keys. Or Deepdive is peppered with cases where parsing with jq instead of cut is way too slow. I'm sure concrete examples of this seem obvious to those deeply familiar w/ Deepdive code! I guess I'm hoping we can dig up two or three of them and paste them here.\nPut another way, seems like we'd make these claims about JSON lines when making a case for TSJ (slightly modified from your TSJ version above):\n1. JSON lines parsing (decoding/deserialization) is super easy: ~~split each input line by TAB, then parse each field~~ parse each line with universally available JSON parser.\n2. JSON lines formatting (encoding/serialization) is super easy: encode each output ~~value~~ line in JSON, again universally available~~, then join with TAB to produce a line.~~\n3. Extra JSON lines encoding overhead is ~~negligible~~ not negligible compared to TSV.\nSince 1 and 2 are clear wins for JSON lines, our argument for TSJ seems to rest on 3. Can we show that JSON encoding overhead is not just positive but also non-negligible for our use cases, compared to TSV? Compared to TSJ?\n. Very convincing notebook, thanks @netj . Looks like ad-hoc inspection is the main use case. Other than what's here, do you have any bottlenecks in mind that we need to migrate to TSJ?\nOtherwise LGTM.\n. > Most benefits of conda you listed sounded like tautology to me, i.e., \"conda packaging is good for using conda,\"\nHa ha, I did use the word \"conda\" there many times. It probably reads tautologically because the word is semantically ambiguous. \"conda\" refers to any subset of:\n1. A package format,\n2. A dependency system,\n3. An environment manager.\nOn ubuntu you have deb/apt/nothing (or chroot?), in Python you have setup.py/pip/virtualenv, in Rust you have crate/cargo/multirust. The package format in this PR buys us a ticket to the dependency system (in C) and the environment manager (in D) the same way that Python packaging gets you pip and virtualenv. The conda/conda/conda naming situation we have here is pretty unfortunate.\n\nAlso, please keep in mind that DD has to allow two different PYTHONPATHs (or environments in general): user/app's and deepdive's own [...] Naively forcing all DD users to share a single PYTHONPATH and PATH with conda will make certain UDFs impossible to use with DD. Greenplum is a good example of such nightmare as users are misguided to switch to their internal python for accessing the database.\n\nThis isn't quite clear to me yet.\nGreenplum is a process that runs as a service. So on the one hand it's entitled to its own runtime environment, but on the other it's a huge pain to write and debug Greenplum UDFs because they depend on this special runtime environment that interacts with user environments in brittle ways. I think this is what your last sentence is referring to.\nIdeally, Greenplum would be installed in its own conda environment that can be activated and deactivated anywhere by anyone (I have no intention of doing this atm). UDFs would be written, tested, and deployed inside this conda environment.\nIt sounds like you're saying the Deepdive situation is similar to Greenplum's. Is that right? I'd be grateful for more details or a pointer to a good example UDF.\n. @alldefector Good suggestion. I tried passing the timestamp string through unchanged, but jq can't handle that particular format. This suggests to me that other JSON tools won't either, even if ES does.\n@netj Our timestamps are time zone naive and don't have offsets (they use Postgres' timestamp without time zone type), so there is no time zone setting in psql that can result in a correct offset down the line. We could change these columns to timestamp with time zone but now we're asking the application to support time zones for a really obscure technical reason.\nI pushed 0a794ff in an attempt to resolve this. Before this change, pgtsv_to_json didn't support any timestamps. After this change, pgtsv_to_json explicitly supports the timestamp without time zone type without qualification. Looks to me like jq and Elastic support the output without changes elsewhere.\nPlease let me know if you think it produces inaccurate results, or if it doesn't support a feature that we need!\n. @netj I agree, it's weird and certainly incorrect to leave out time zone info! Unfortunately adding time zone support to our app is way beyond the scope of my current task (moving specific processing from Postgres to Greenplum).\nI don't think I made the motivation clear in my initial PR, and my original patch was definitely flawed. Sorry about that! The motivation for this PR is twofold:\n1. pgtsv_to_json terminates on a ValueError when dumping timestamps of any kind, and\n2. jq crashes when we send it psql-formatted timestamp strings, which are missing the \"T\" character and as you pointed out are not valid ISO 8601.\nSeems like setting psql's time zone to UTC for this one dump doesn't fix those problems, but does make our timestamps incorrect until we support time zones from the very beginning of the pipeline. As it stands the data source (PostgreSQL/Greenplum) doesn't contain time zone information, and no combination of psql runtime directives can create that information out of thin air.\nWould you be open to my approach in 0a794ff along with a promise to open a ticket for proper time zone handling in downstream apps? I'd love to fix that at some point.\n. @alldefector Looks like CI is failing, but I don't think it's caused by this diff. Can I merge and push on the command line anyway? Should I open a ticket for fixing CI here?\n. Closing in favor of #551 \n. @raphaelhoffmann there's a check for the null character \\\\N down below in main, before convert is called. \\\\N is the default for the null character. Are we seeing it dump NULL instead?\n. @raphaelhoffmann I see, makes sense. I can see this implementation causing unexpected results, since one might assume that \\\\N represents null for integers when reading main but in fact \\\\N or NULL represent null. Sounds like this only applies inside arrays. Could we isolate this change inside the array-handling code?\nOne approach might be to compose the existing conversion around line 51 with your NULL check in a lambda:\nconvert_or_null = lambda x: None if x == 'NULL' else convert(x)\nreturn map(convert_or_null, arr)\nWhat do you think?\n. @raphaelhoff wow, that is very surprising. I would not have expected it, but I think the issue there is the ::text[] type cast. Without it we get reasonable results:\n$ psql postgres -c \"copy (select ARRAY['NULL',null]) to stdout\"\n{\"NULL\",NULL}\nApparently ::<type>[] goes through every element and converts it to <type>. They call null a \"generic constant\", so I guess when you convert this generic constant to text it's no longer a null value. Very weird.\n. No, I'm not concerned about NULLs arrays here. Switching to (a) Postgres' CSV format and (b) Python 3 means we can handle unicode with way less custom escaping/unescaping. @alldefector pointed me in the right direction here.\nAll the Postgres TEXT format encoding caveats plus all the Python 2-specific encoding caveats added up to a pile of brittle exception code (see: most of my commits over the past few days).\nWe won't need to distinguish between ,\"\", and ,, as long as we use an explicit null as '\\\\N' or similar.\n. @netj Yeah, I would also like to switch back to Travis's container-based infra. It is possible that bash 4.3 is all that's needed. Two questions:\n1. Why do we bundle bash with DeepDive? This seems like a very low-level dependency to include in our distribution. As a DeepDive user or new developer, I imagine I'd be very surprised to find out (eventually) that it's not using the same bash that I'm using on my host system. Seems like an explicit dependency on a given bash version would be clearer.\n2. As long as we're bundling bash, should we be using it for all tests? An exception for a single .bats seems easily overlooked, and now our tests are targeting different platforms. You know more about bats than I do -- what's the best way to run them all on bash 4.3?\n. > Mac users wasted many hours/days if not weeks because of these since it's bash 3 and BSD.\nWas this before homebrew/macports/fink? All the Mac users I know install tons of packages with GNU/newer dependencies, but I don't think many of those actually bundle bash or coreutils. I wonder if we could revisit this later and simplify our build.\n. @netj @alldefector this is all tangential, sorry for hijacking this thread:\n\nthey often don't have control over the compute resources they want to run DeepDive on\n\nInteresting! Sounds like I need to learn more about our users (and what you mean by \"control\"). The deepdive-users list is an obvious place to start, but it's sparse and biased. Do we have other data about our user base that I could check out?\n\nMany users are in fact struggling and mentioning they waste most time at installing dependencies we don't bundle\n\nIsn't this exactly what dependency management tools are for?\n\nIf we start distributing releases via Homebrew, APT, yum, then yes we should install the runtime dependencies natively rather than bundling. However, that requires more maintenance effort\n\nIn many ways a package manager reduces maintenance effort. Package managers take things like CPU architecture, security patches, bug fixes, API-stable updates, custom shared libraries, etc. into account when resolving dependencies. If we're saving time on maintenance today then it's because we're ignoring all these things, not because we've found a way to do them better than a package manager!\n\nand lower coverage\n\nThe abstraction we get from ordinary dependency management seems likely to increase coverage, not lower it. If we just specify dependency names then DeepDive can be ported to any platform that implements those names, including the BSDs, Cygwin, or whatever. If we bundle dependency code or binaries, then it'll run only on platforms that we take the time to bundle dependencies for. What am I missing here?\n. This is very cool. Can you include stats on wall time and space savings for TSJ vs JSON, and TSJ vs TSV?\nI know that you wrote this specifically for our Deepdive use cases, but serde isn't a Deepdive-specific function. Can we break your core TSJ parsing/serialization code into a separate Python package? This will make it more testable, transparent and composable. If it's out of scope here, feel free to open a ticket so that we can get to it later.\nI see that DEEPDIVE_USE_TSV2TSJ lets us switch from the Python lib to a Perl version. When should we do this? The Perl stuff should probably be pulled out into its own installable module, too.\n. @netj Thanks for pasting those numbers. I remember seeing them before, but they don't contain any comparison to JSON lines. Since JSON solves our NULL-encoding problems and is very common, seems to me like this is the proper baseline for comparison.\nI agree that we can cook up any performance numbers we want by picking and choosing tables. I'm looking for persuasive numbers to convince new hires that they should learn and maintain this new format instead of JSON lines.\nMaybe we have a bunch of really wide tables with really long keys. Or Deepdive is peppered with cases where parsing with jq instead of cut is way too slow. I'm sure concrete examples of this seem obvious to those deeply familiar w/ Deepdive code! I guess I'm hoping we can dig up two or three of them and paste them here.\nPut another way, seems like we'd make these claims about JSON lines when making a case for TSJ (slightly modified from your TSJ version above):\n1. JSON lines parsing (decoding/deserialization) is super easy: ~~split each input line by TAB, then parse each field~~ parse each line with universally available JSON parser.\n2. JSON lines formatting (encoding/serialization) is super easy: encode each output ~~value~~ line in JSON, again universally available~~, then join with TAB to produce a line.~~\n3. Extra JSON lines encoding overhead is ~~negligible~~ not negligible compared to TSV.\nSince 1 and 2 are clear wins for JSON lines, our argument for TSJ seems to rest on 3. Can we show that JSON encoding overhead is not just positive but also non-negligible for our use cases, compared to TSV? Compared to TSJ?\n. Very convincing notebook, thanks @netj . Looks like ad-hoc inspection is the main use case. Other than what's here, do you have any bottlenecks in mind that we need to migrate to TSJ?\nOtherwise LGTM.\n. > Most benefits of conda you listed sounded like tautology to me, i.e., \"conda packaging is good for using conda,\"\nHa ha, I did use the word \"conda\" there many times. It probably reads tautologically because the word is semantically ambiguous. \"conda\" refers to any subset of:\n1. A package format,\n2. A dependency system,\n3. An environment manager.\nOn ubuntu you have deb/apt/nothing (or chroot?), in Python you have setup.py/pip/virtualenv, in Rust you have crate/cargo/multirust. The package format in this PR buys us a ticket to the dependency system (in C) and the environment manager (in D) the same way that Python packaging gets you pip and virtualenv. The conda/conda/conda naming situation we have here is pretty unfortunate.\n\nAlso, please keep in mind that DD has to allow two different PYTHONPATHs (or environments in general): user/app's and deepdive's own [...] Naively forcing all DD users to share a single PYTHONPATH and PATH with conda will make certain UDFs impossible to use with DD. Greenplum is a good example of such nightmare as users are misguided to switch to their internal python for accessing the database.\n\nThis isn't quite clear to me yet.\nGreenplum is a process that runs as a service. So on the one hand it's entitled to its own runtime environment, but on the other it's a huge pain to write and debug Greenplum UDFs because they depend on this special runtime environment that interacts with user environments in brittle ways. I think this is what your last sentence is referring to.\nIdeally, Greenplum would be installed in its own conda environment that can be activated and deactivated anywhere by anyone (I have no intention of doing this atm). UDFs would be written, tested, and deployed inside this conda environment.\nIt sounds like you're saying the Deepdive situation is similar to Greenplum's. Is that right? I'd be grateful for more details or a pointer to a good example UDF.\n. ",
    "timofeytt": "+1 for documentation. +1 for documentation. ",
    "clfarron4": "Apologies for the delay. Here is the information requested.\n```\n[claire@localhost ~]$ cat /etc/redhat-release\nFedora release 24 (Twenty Four)\n[claire@localhost ~]$ perl -v\nThis is perl 5, version 22, subversion 2 (v5.22.2) built for x86_64-linux-thread-multi\n```\n. Apologies for the delay. Here is the information requested.\n```\n[claire@localhost ~]$ cat /etc/redhat-release\nFedora release 24 (Twenty Four)\n[claire@localhost ~]$ perl -v\nThis is perl 5, version 22, subversion 2 (v5.22.2) built for x86_64-linux-thread-multi\n```\n. ",
    "philipperemy": "Use https://github.com/euske/pdfminer\n. Use https://github.com/euske/pdfminer\n. ",
    "vpsm": "@raphaelhoff @alldefector I see the same issue when I do make depends. Do you recall what exactly you had to do to fix the problem? I tried installing the brew packages you indicated but still have the issue.\nThe error on Mac is:\n```\nThis program built for i386-apple-darwin11.3.0\n+ has gnuplot\nFinished installation for _deepdive_runtime_deps\nsha1sum util/install.sh util/install/install.Mac.sh util/install/install.RedHat.sh util/install/install.Ubuntu.pgxl.sh util/install/install.Ubuntu.sh >.build/depends\n/bin/sh: .build/depends: No such file or directory\nmake: *** [.build/depends] Error 1\n```\n. @alldefector @raphaelhoff I tried the following on an Ubuntu machine as well but still get an error\ngit clone https://github.com/HazyResearch/deepdive.git\ncd deepdive\ngit submodule update --init\nmake depends\nThe error is:\nsha1sum util/install.sh util/install/install.RedHat.sh util/install/install.Ubuntu.pgxl.sh util/install/install.Ubuntu.sh util/install/install.Mac.sh >.build/depends\n/bin/sh: 1: cannot create .build/depends: Directory nonexistent\nMakefile:45: recipe for target '.build/depends' failed\nmake: *** [.build/depends] Error 2\n. Thanks @chrismre @alldefector for the quick response.\n@alldefector is the following understanding correct?\nobject_cat?(\n    @key obj_id text,\n    @key cat_type text\n).\nwill create a boolean variable for each possible combination of obj_id and cat_type\nobject_cat?(\n    @key obj_id text,\n    cat_type text\n).\nwill create a categorical variable for each obj_id.\n. Thanks @alldefector \nI'm facing a few issues compiling and getting the master branch to run. Will report further once I sort those issues out in the other threads :).\nThanks.\n. Thanks, that fixed it.\n. @raphaelhoff @alldefector I see the same issue when I do make depends. Do you recall what exactly you had to do to fix the problem? I tried installing the brew packages you indicated but still have the issue.\nThe error on Mac is:\n```\nThis program built for i386-apple-darwin11.3.0\n+ has gnuplot\nFinished installation for _deepdive_runtime_deps\nsha1sum util/install.sh util/install/install.Mac.sh util/install/install.RedHat.sh util/install/install.Ubuntu.pgxl.sh util/install/install.Ubuntu.sh >.build/depends\n/bin/sh: .build/depends: No such file or directory\nmake: *** [.build/depends] Error 1\n```\n. @alldefector @raphaelhoff I tried the following on an Ubuntu machine as well but still get an error\ngit clone https://github.com/HazyResearch/deepdive.git\ncd deepdive\ngit submodule update --init\nmake depends\nThe error is:\nsha1sum util/install.sh util/install/install.RedHat.sh util/install/install.Ubuntu.pgxl.sh util/install/install.Ubuntu.sh util/install/install.Mac.sh >.build/depends\n/bin/sh: 1: cannot create .build/depends: Directory nonexistent\nMakefile:45: recipe for target '.build/depends' failed\nmake: *** [.build/depends] Error 2\n. Thanks @chrismre @alldefector for the quick response.\n@alldefector is the following understanding correct?\nobject_cat?(\n    @key obj_id text,\n    @key cat_type text\n).\nwill create a boolean variable for each possible combination of obj_id and cat_type\nobject_cat?(\n    @key obj_id text,\n    cat_type text\n).\nwill create a categorical variable for each obj_id.\n. Thanks @alldefector \nI'm facing a few issues compiling and getting the master branch to run. Will report further once I sort those issues out in the other threads :).\nThanks.\n. Thanks, that fixed it.\n. ",
    "bjonnh": "On debian sid, there is no need for this PPA as openjdk is already 8.\nGood question for jessie.\nAlso to run tests (postgres spouse_example) I had do add pyhocon (using pip), pbzip2 and moreutils\n. Oh I see, I was going to answer saying that using the last buildkit provided that file.\nThanks.\n. On debian sid, there is no need for this PPA as openjdk is already 8.\nGood question for jessie.\nAlso to run tests (postgres spouse_example) I had do add pyhocon (using pip), pbzip2 and moreutils\n. Oh I see, I was going to answer saying that using the last buildkit provided that file.\nThanks.\n. ",
    "profmocs": "I am running deepdive on Centos 7. \n. I am running deepdive on Centos 7. \n. ",
    "profressor": "Tried the docker images. Didnt work\n\nuser@vm:~$ bash <(curl -fsSL git.io/getdeepdive)\n++ get-url https://github.com/HazyResearch/deepdive/raw/master/util/install.sh\n++ curl -fsSL https://github.com/HazyResearch/deepdive/raw/master/util/install.sh\nDeepDive installer for Ubuntu\n1) deepdive             6) postgres\n2) deepdive_docker_sandbox    7) postgres_xl\n3) deepdive_example_notebook  8) run_deepdive_tests\n4) deepdive_from_release      9) spouse_example\n5) jupyter_notebook\nInstall what (enter to repeat options, a to see all, q to quit, or a number)? 2\nStarting installation for deepdive_docker_sandbox\n\nget-url https://github.com/HazyResearch/deepdive/archive/v0.8-STABLE.tar.gz\ncurl -fsSL https://github.com/HazyResearch/deepdive/archive/v0.8-STABLE.tar.gz\ntar xvzf - -C deepdive-0.8-STABLE.download deepdive-0.8-STABLE/sandbox deepdive-0.8-STABLE/test/postgresql/Dockerfile.postgres\n  tar: deepdive-0.8-STABLE/sandbox: Not found in archive\n  tar: deepdive-0.8-STABLE/test/postgresql/Dockerfile.postgres: Not found in archive\n  tar: Exiting with failure status due to previous errors\n  ## Failed installation for deepdive_docker_sandbox\n  # Install what (enter to repeat options, a to see all, q to quit, or a number)? \n\n\nIs there a way to switch from stable to master build ? Or any other fix for this problem?\nWhen will v0.9 be released?\n. Using that command gives me another error\n\nStep 5 : USER jovyan\n ---> Using cache\n ---> d04688cef943\nStep 6 : ADD install.sh /deepdive/\nERROR: Service 'deepdive-notebooks' failed to build: lstat install.sh: no such file or directory\nFailed installation for deepdive_docker_sandbox\n\nAny ideas? \n. The Docker sandbox installation works now.  However Im not familiar how to use it to build and run my own DD apps what I intend to do. Using my prefered method No. 1. (Deepdive installer) however shows some error\n\n\u2717 partition_integers works\n   (in test file util/test/partition_integers.bats, line 43)\n     `partition_integers $m $n | check_correct_output $m $n' failed\n   partitioning 0 by 2\n   /tmp/bats.72064.src: line 6: incorrect_lines: Permission denied\n \u2713 partition_integers fails on non-positive partitions\n125 tests, 1 failure, 5 skipped\nFailed installation for run_deepdive_tests\n\nLooks like a tiny perm. error which can be fixed easily. Back to topic, Im still stuck on the spouse example doing the sentences. Im using the updated version which still gives me (another) error:\n\nuser@vm:~/spouse$ deepdive compile\n(OK)\nuser@vm:~/spouse$ deepdive do sentences\n\u2018run/RUNNING\u2019 -> \u201820161118/002015.669047933\u2019\n\u2018run/LATEST\u2019 -> \u201820161118/002015.669047933\u2019\n2016-11-18 00:20:19.019593 # on vm: deepdive do sentences\n2016-11-18 00:20:19.019657 # run/20161118/002015.669047933/plan.sh\n2016-11-18 00:20:19.019670 # execution plan for data/sentences\n2016-11-18 00:20:19.019677 \n2016-11-18 00:20:19.019686 : ## process/init/app ##########################################################\n2016-11-18 00:20:19.019693 : # Done: 2016-11-18T00:06:32+0100 (13m 43s ago)\n2016-11-18 00:20:19.019699 : process/init/app/run.sh\n2016-11-18 00:20:19.019705 : mark_done process/init/app\n2016-11-18 00:20:19.019712 : ##############################################################################\n2016-11-18 00:20:19.019721 \n2016-11-18 00:20:19.019727 \n2016-11-18 00:20:19.019733 : ## process/init/relation/articles ############################################\n2016-11-18 00:20:19.019739 : # Done: 2016-11-18T00:06:32+0100 (13m 43s ago)\n2016-11-18 00:20:19.019745 : process/init/relation/articles/run.sh\n2016-11-18 00:20:19.019750 : mark_done process/init/relation/articles\n2016-11-18 00:20:19.019756 : ##############################################################################\n2016-11-18 00:20:19.019764 \n2016-11-18 00:20:19.019770 \n2016-11-18 00:20:19.019776 : ## data/articles #############################################################\n2016-11-18 00:20:19.019782 : # Done: 2016-11-18T00:06:32+0100 (13m 43s ago)\n2016-11-18 00:20:19.019787 : # no-op\n2016-11-18 00:20:19.019793 : mark_done data/articles\n2016-11-18 00:20:19.019800 : ##############################################################################\n2016-11-18 00:20:19.019807 \n2016-11-18 00:20:19.019812 \n2016-11-18 00:20:19.019818 ## process/ext_sentences_by_nlp_markup #######################################\n2016-11-18 00:20:19.019825 # Done: N/A\n2016-11-18 00:20:19.019831 process/ext_sentences_by_nlp_markup/run.sh\n2016-11-18 00:20:19.019837 ++ dirname process/ext_sentences_by_nlp_markup/run.sh\n2016-11-18 00:20:19.019844 + cd process/ext_sentences_by_nlp_markup\n2016-11-18 00:20:19.019850 + export DEEPDIVE_CURRENT_PROCESS_NAME=process/ext_sentences_by_nlp_markup\n2016-11-18 00:20:19.019856 + DEEPDIVE_CURRENT_PROCESS_NAME=process/ext_sentences_by_nlp_markup\n2016-11-18 00:20:19.019862 + export DEEPDIVE_LOAD_FORMAT=tsv\n2016-11-18 00:20:19.019869 + DEEPDIVE_LOAD_FORMAT=tsv\n2016-11-18 00:20:19.019875 + deepdive compute execute 'input_sql= SELECT R0.id AS \"articles.R0.id\", R0.content AS \"articles.R0.content\"\n2016-11-18 00:20:19.019883 FROM articles R0\n2016-11-18 00:20:19.019889\n2016-11-18 00:20:19.019894           ' 'command=\"$DEEPDIVE_APP\"/udf/nlp_markup.sh' output_relation=sentences\n2016-11-18 00:20:19.027645 Executing with the following configuration:\n2016-11-18 00:20:19.027708  DEEPDIVE_NUM_PROCESSES=3\n2016-11-18 00:20:19.027720  DEEPDIVE_NUM_PARALLEL_UNLOADS=1\n2016-11-18 00:20:19.027727  DEEPDIVE_NUM_PARALLEL_LOADS=1\n2016-11-18 00:20:19.027734  output_relation_tmp=dd_tmp_sentences\n2016-11-18 00:20:19.027739 \n2016-11-18 00:20:19.116391 CREATE TABLE\n2016-11-18 00:20:19.237686 CREATE TABLE\n2016-11-18 00:20:19.275085 unloading to feed_processes-1: ' SELECT R0.id AS \"articles.R0.id\", R0.content AS \"articles.R0.content\"\n2016-11-18 00:20:19.275149 FROM articles R0\n2016-11-18 00:20:19.275160\n2016-11-18 00:20:19.275168           '\n> 2016-11-18 00:20:19.278372 No Bazaar/Parser set up at: /home/user/spouse/udf/bazaar/parser\n2016-11-18 00:20:19.278409 /home/user/local/util/compute-driver/local/compute-execute: line 140: kill: (5382) - No such process\n2016-11-18 00:20:19.279831 [ERROR] command='\"$DEEPDIVE_APP\"/udf/nlp_markup.sh': PID 5382: finished with non-zero exit status (0)\n2016-11-18 00:20:19.279923 /home/user/local/util/compute-driver/local/compute-execute: line 138:  5383 Terminated              DEEPDIVE_CURRENT_PROCESS_INDEX=$i bash -c \"$command\" < process-$i.input > process-$i.output\n2016-11-18 00:20:19.279942 /home/user/local/util/compute-driver/local/compute-execute: line 138:  5384 Terminated              DEEPDIVE_CURRENT_PROCESS_INDEX=$i bash -c \"$command\" < process-$i.input > process-$i.output\n2016-11-18 00:20:19.279949 /home/user/local/util/compute-driver/local/compute-execute: line 138:  5388 Terminated              mkmimo process-.output > output_computed-\n2016-11-18 00:20:19.279971 /home/user/local/util/compute-driver/local/compute-execute: line 138:  5389 Terminated              deepdive-load \"$output_relation_tmp\" output_computed-\n2016-11-18 00:20:19.279984 /home/user/local/util/compute-driver/local/compute-execute: line 138:  5395 Terminated              deepdive-db unload \"$input_sql\" \"$DEEPDIVE_LOAD_FORMAT\" feed_processes-\nunloading: 0:00:00    0     0 /sil/compute-driver/local/compute-execute: line 138:  5396 Terminated              mkmimo feed_processes-* > process-*.input\nunloading: 0:00:00   12KiB  546KiB/s\n\u2018run/ABORTED\u2019 -> \u201820161118/002015.669047933\u2019\n\nCan this be fixed? I need to run the spouse example in order to understand how DD works. I need to get it working to finish my own research on my masters thesis about DeepDive. Any help appreciated!\n. Hi Jaeho,\nIm using the sandbox image now but there is a problem connecting to the database. Im not too familiar with docker :/\n```\nbash <(curl -fsSL git.io/getdeepdive)\n...\ndeepdive-notebooks_1  | [W 14:10:01.971 NotebookApp] Widgets are unavailable. Please install widgetsnbextension or ipywidgets 4.0\ndeepdive-notebooks_1  | [I 14:10:01.990 NotebookApp] Serving notebooks from local directory: /ConfinedWater\ndeepdive-notebooks_1  | [I 14:10:01.990 NotebookApp] 0 active kernels \ndeepdive-notebooks_1  | [I 14:10:01.990 NotebookApp] The Jupyter Notebook is running at: http://0.0.0.0:8888/\ndeepdive-notebooks_1  | [I 14:10:01.990 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\nuser@vm:~$ docker ps\nCONTAINER ID        IMAGE                   COMMAND                  CREATED             STATUS              PORTS               NAMES\ndc7ee2c58466        hazyresearch/postgres   \"/docker-entrypoint.s\"   4 days ago          Up 24 minutes       5432/tcp            sandbox_database_1\nuser@vm:~$ docker run -i -t -v $HOME:/vm hazyresearch/deepdive:latest /bin/bash\njovyan@6353d26f2fd8:/vm/spouse$ ls\napp.ddlog  db.url  deepdive.conf  input  run  udf\njovyan@6353d26f2fd8:/vm/spouse$ deepdive compile\n\u2018run/compiled\u2019 -> \u201820161122/143702.284636268\u2019\njovyan@6353d26f2fd8:/vm/spouse$ deepdive do sentences\n\u2018run/RUNNING\u2019 -> \u201820161122/143608.888898717\u2019\n2016-11-22 14:36:09.071365 process/ext_sentences_by_nlp_markup/run.sh\n2016-11-22 14:36:09.071107 ################################################################################\n2016-11-22 14:36:09.071216 # Host: 6353d26f2fd8\n2016-11-22 14:36:09.071236 # DeepDive: deepdive v0.8.0-742-g4b812a1 (Linux x86_64)\n2016-11-22 14:36:09.071247 export PATH='/usr/local/bin':\"$PATH\"\n2016-11-22 14:36:09.071256 export DEEPDIVE_PWD='/vm/spouse'\n2016-11-22 14:36:09.071265 export DEEPDIVE_APP='/vm/spouse'\n2016-11-22 14:36:09.071278 cd \"$DEEPDIVE_APP\"/run\n2016-11-22 14:36:09.071288 export DEEPDIVE_RUN_ID='20161122/143608.888898717'\n2016-11-22 14:36:09.071297 # Plan: 20161122/143608.888898717/plan.sh\n2016-11-22 14:36:09.071306 # Targets: sentences\n2016-11-22 14:36:09.071315 ################################################################################\n2016-11-22 14:36:09.071324 \n2016-11-22 14:36:09.071334     # process/init/app/run.sh ####################################### last done: 2016-11-21T23:38:35+0000 (14h 57m 33s ago)\n2016-11-22 14:36:09.071344     # process/init/relation/articles/run.sh ######################### last done: 2016-11-21T23:38:35+0000 (14h 57m 33s ago)\n2016-11-22 14:36:09.071353     # deepdive mark 'done' data/articles ############################ last done: 2016-11-21T23:38:35+0000 (14h 57m 33s ago)\n2016-11-22 14:36:09.071365 process/ext_sentences_by_nlp_markup/run.sh ########################## last done: N/A\n2016-11-22 14:36:09.071375 ++ dirname process/ext_sentences_by_nlp_markup/run.sh\n2016-11-22 14:36:09.071384 + cd process/ext_sentences_by_nlp_markup\n2016-11-22 14:36:09.071407 + : dd_tmp_ dd_old_\n2016-11-22 14:36:09.071436 + export DEEPDIVE_CURRENT_PROCESS_NAME=process/ext_sentences_by_nlp_markup\n2016-11-22 14:36:09.071467 + DEEPDIVE_CURRENT_PROCESS_NAME=process/ext_sentences_by_nlp_markup\n2016-11-22 14:36:09.071487 + export DEEPDIVE_LOAD_FORMAT=tsv\n2016-11-22 14:36:09.071524 + DEEPDIVE_LOAD_FORMAT=tsv\n2016-11-22 14:36:09.071553 + output_relation=sentences\n2016-11-22 14:36:09.071568 + output_relation_tmp=dd_tmp_sentences\n2016-11-22 14:36:09.071579 + output_relation_old=dd_old_sentences\n2016-11-22 14:36:09.071591 + deepdive create table-if-not-exists sentences\n2016-11-22 14:36:09.192254 psql: could not connect to server: Connection refused\n2016-11-22 14:36:09.192330  Is the server running on host \"localhost\" (::1) and accepting\n2016-11-22 14:36:09.192347  TCP/IP connections on port 5432?\n2016-11-22 14:36:09.192367 could not connect to server: Connection refused\n2016-11-22 14:36:09.192377  Is the server running on host \"localhost\" (127.0.0.1) and accepting\n2016-11-22 14:36:09.192390  TCP/IP connections on port 5432?\n2016-11-22 14:36:09.227064 psql: could not connect to server: Connection refused\n2016-11-22 14:36:09.227139  Is the server running on host \"localhost\" (::1) and accepting\n2016-11-22 14:36:09.227156  TCP/IP connections on port 5432?\n2016-11-22 14:36:09.227166 could not connect to server: Connection refused\n2016-11-22 14:36:09.227187  Is the server running on host \"localhost\" (127.0.0.1) and accepting\n2016-11-22 14:36:09.227196  TCP/IP connections on port 5432?\n2016-11-22 14:36:09.261292 psql: could not connect to server: Connection refused\n2016-11-22 14:36:09.261378  Is the server running on host \"localhost\" (::1) and accepting\n2016-11-22 14:36:09.261396  TCP/IP connections on port 5432?\n2016-11-22 14:36:09.261416 could not connect to server: Connection refused\n2016-11-22 14:36:09.261426  Is the server running on host \"localhost\" (127.0.0.1) and accepting\n2016-11-22 14:36:09.261439  TCP/IP connections on port 5432?\njovyan@6353d26f2fd8:/ConfinedWater$ psql\npsql: could not connect to server: No such file or directory\n    Is the server running locally and accepting\n    connections on Unix domain socket \"/var/run/postgresql/.s.PGSQL.5432\"?\n```\nAny hint?\nAs I don't don't particularly like Docker (on top of ubuntu@vmware@windows), I'd prefer to install and use Deepdive. I have three question:\n(1) Is there an older version I can use which will work?\n(2) Any ETA on version 0.9?\n(3) Will v0.9 include a DD installer, or are you going to stick with Docker?\nThank you in advance! Keep up the good work.. Tried the docker images. Didnt work\n\nuser@vm:~$ bash <(curl -fsSL git.io/getdeepdive)\n++ get-url https://github.com/HazyResearch/deepdive/raw/master/util/install.sh\n++ curl -fsSL https://github.com/HazyResearch/deepdive/raw/master/util/install.sh\nDeepDive installer for Ubuntu\n1) deepdive             6) postgres\n2) deepdive_docker_sandbox    7) postgres_xl\n3) deepdive_example_notebook  8) run_deepdive_tests\n4) deepdive_from_release      9) spouse_example\n5) jupyter_notebook\nInstall what (enter to repeat options, a to see all, q to quit, or a number)? 2\nStarting installation for deepdive_docker_sandbox\n\nget-url https://github.com/HazyResearch/deepdive/archive/v0.8-STABLE.tar.gz\ncurl -fsSL https://github.com/HazyResearch/deepdive/archive/v0.8-STABLE.tar.gz\ntar xvzf - -C deepdive-0.8-STABLE.download deepdive-0.8-STABLE/sandbox deepdive-0.8-STABLE/test/postgresql/Dockerfile.postgres\n  tar: deepdive-0.8-STABLE/sandbox: Not found in archive\n  tar: deepdive-0.8-STABLE/test/postgresql/Dockerfile.postgres: Not found in archive\n  tar: Exiting with failure status due to previous errors\n  ## Failed installation for deepdive_docker_sandbox\n  # Install what (enter to repeat options, a to see all, q to quit, or a number)? \n\n\nIs there a way to switch from stable to master build ? Or any other fix for this problem?\nWhen will v0.9 be released?\n. Using that command gives me another error\n\nStep 5 : USER jovyan\n ---> Using cache\n ---> d04688cef943\nStep 6 : ADD install.sh /deepdive/\nERROR: Service 'deepdive-notebooks' failed to build: lstat install.sh: no such file or directory\nFailed installation for deepdive_docker_sandbox\n\nAny ideas? \n. The Docker sandbox installation works now.  However Im not familiar how to use it to build and run my own DD apps what I intend to do. Using my prefered method No. 1. (Deepdive installer) however shows some error\n\n\u2717 partition_integers works\n   (in test file util/test/partition_integers.bats, line 43)\n     `partition_integers $m $n | check_correct_output $m $n' failed\n   partitioning 0 by 2\n   /tmp/bats.72064.src: line 6: incorrect_lines: Permission denied\n \u2713 partition_integers fails on non-positive partitions\n125 tests, 1 failure, 5 skipped\nFailed installation for run_deepdive_tests\n\nLooks like a tiny perm. error which can be fixed easily. Back to topic, Im still stuck on the spouse example doing the sentences. Im using the updated version which still gives me (another) error:\n\nuser@vm:~/spouse$ deepdive compile\n(OK)\nuser@vm:~/spouse$ deepdive do sentences\n\u2018run/RUNNING\u2019 -> \u201820161118/002015.669047933\u2019\n\u2018run/LATEST\u2019 -> \u201820161118/002015.669047933\u2019\n2016-11-18 00:20:19.019593 # on vm: deepdive do sentences\n2016-11-18 00:20:19.019657 # run/20161118/002015.669047933/plan.sh\n2016-11-18 00:20:19.019670 # execution plan for data/sentences\n2016-11-18 00:20:19.019677 \n2016-11-18 00:20:19.019686 : ## process/init/app ##########################################################\n2016-11-18 00:20:19.019693 : # Done: 2016-11-18T00:06:32+0100 (13m 43s ago)\n2016-11-18 00:20:19.019699 : process/init/app/run.sh\n2016-11-18 00:20:19.019705 : mark_done process/init/app\n2016-11-18 00:20:19.019712 : ##############################################################################\n2016-11-18 00:20:19.019721 \n2016-11-18 00:20:19.019727 \n2016-11-18 00:20:19.019733 : ## process/init/relation/articles ############################################\n2016-11-18 00:20:19.019739 : # Done: 2016-11-18T00:06:32+0100 (13m 43s ago)\n2016-11-18 00:20:19.019745 : process/init/relation/articles/run.sh\n2016-11-18 00:20:19.019750 : mark_done process/init/relation/articles\n2016-11-18 00:20:19.019756 : ##############################################################################\n2016-11-18 00:20:19.019764 \n2016-11-18 00:20:19.019770 \n2016-11-18 00:20:19.019776 : ## data/articles #############################################################\n2016-11-18 00:20:19.019782 : # Done: 2016-11-18T00:06:32+0100 (13m 43s ago)\n2016-11-18 00:20:19.019787 : # no-op\n2016-11-18 00:20:19.019793 : mark_done data/articles\n2016-11-18 00:20:19.019800 : ##############################################################################\n2016-11-18 00:20:19.019807 \n2016-11-18 00:20:19.019812 \n2016-11-18 00:20:19.019818 ## process/ext_sentences_by_nlp_markup #######################################\n2016-11-18 00:20:19.019825 # Done: N/A\n2016-11-18 00:20:19.019831 process/ext_sentences_by_nlp_markup/run.sh\n2016-11-18 00:20:19.019837 ++ dirname process/ext_sentences_by_nlp_markup/run.sh\n2016-11-18 00:20:19.019844 + cd process/ext_sentences_by_nlp_markup\n2016-11-18 00:20:19.019850 + export DEEPDIVE_CURRENT_PROCESS_NAME=process/ext_sentences_by_nlp_markup\n2016-11-18 00:20:19.019856 + DEEPDIVE_CURRENT_PROCESS_NAME=process/ext_sentences_by_nlp_markup\n2016-11-18 00:20:19.019862 + export DEEPDIVE_LOAD_FORMAT=tsv\n2016-11-18 00:20:19.019869 + DEEPDIVE_LOAD_FORMAT=tsv\n2016-11-18 00:20:19.019875 + deepdive compute execute 'input_sql= SELECT R0.id AS \"articles.R0.id\", R0.content AS \"articles.R0.content\"\n2016-11-18 00:20:19.019883 FROM articles R0\n2016-11-18 00:20:19.019889\n2016-11-18 00:20:19.019894           ' 'command=\"$DEEPDIVE_APP\"/udf/nlp_markup.sh' output_relation=sentences\n2016-11-18 00:20:19.027645 Executing with the following configuration:\n2016-11-18 00:20:19.027708  DEEPDIVE_NUM_PROCESSES=3\n2016-11-18 00:20:19.027720  DEEPDIVE_NUM_PARALLEL_UNLOADS=1\n2016-11-18 00:20:19.027727  DEEPDIVE_NUM_PARALLEL_LOADS=1\n2016-11-18 00:20:19.027734  output_relation_tmp=dd_tmp_sentences\n2016-11-18 00:20:19.027739 \n2016-11-18 00:20:19.116391 CREATE TABLE\n2016-11-18 00:20:19.237686 CREATE TABLE\n2016-11-18 00:20:19.275085 unloading to feed_processes-1: ' SELECT R0.id AS \"articles.R0.id\", R0.content AS \"articles.R0.content\"\n2016-11-18 00:20:19.275149 FROM articles R0\n2016-11-18 00:20:19.275160\n2016-11-18 00:20:19.275168           '\n> 2016-11-18 00:20:19.278372 No Bazaar/Parser set up at: /home/user/spouse/udf/bazaar/parser\n2016-11-18 00:20:19.278409 /home/user/local/util/compute-driver/local/compute-execute: line 140: kill: (5382) - No such process\n2016-11-18 00:20:19.279831 [ERROR] command='\"$DEEPDIVE_APP\"/udf/nlp_markup.sh': PID 5382: finished with non-zero exit status (0)\n2016-11-18 00:20:19.279923 /home/user/local/util/compute-driver/local/compute-execute: line 138:  5383 Terminated              DEEPDIVE_CURRENT_PROCESS_INDEX=$i bash -c \"$command\" < process-$i.input > process-$i.output\n2016-11-18 00:20:19.279942 /home/user/local/util/compute-driver/local/compute-execute: line 138:  5384 Terminated              DEEPDIVE_CURRENT_PROCESS_INDEX=$i bash -c \"$command\" < process-$i.input > process-$i.output\n2016-11-18 00:20:19.279949 /home/user/local/util/compute-driver/local/compute-execute: line 138:  5388 Terminated              mkmimo process-.output > output_computed-\n2016-11-18 00:20:19.279971 /home/user/local/util/compute-driver/local/compute-execute: line 138:  5389 Terminated              deepdive-load \"$output_relation_tmp\" output_computed-\n2016-11-18 00:20:19.279984 /home/user/local/util/compute-driver/local/compute-execute: line 138:  5395 Terminated              deepdive-db unload \"$input_sql\" \"$DEEPDIVE_LOAD_FORMAT\" feed_processes-\nunloading: 0:00:00    0     0 /sil/compute-driver/local/compute-execute: line 138:  5396 Terminated              mkmimo feed_processes-* > process-*.input\nunloading: 0:00:00   12KiB  546KiB/s\n\u2018run/ABORTED\u2019 -> \u201820161118/002015.669047933\u2019\n\nCan this be fixed? I need to run the spouse example in order to understand how DD works. I need to get it working to finish my own research on my masters thesis about DeepDive. Any help appreciated!\n. Hi Jaeho,\nIm using the sandbox image now but there is a problem connecting to the database. Im not too familiar with docker :/\n```\nbash <(curl -fsSL git.io/getdeepdive)\n...\ndeepdive-notebooks_1  | [W 14:10:01.971 NotebookApp] Widgets are unavailable. Please install widgetsnbextension or ipywidgets 4.0\ndeepdive-notebooks_1  | [I 14:10:01.990 NotebookApp] Serving notebooks from local directory: /ConfinedWater\ndeepdive-notebooks_1  | [I 14:10:01.990 NotebookApp] 0 active kernels \ndeepdive-notebooks_1  | [I 14:10:01.990 NotebookApp] The Jupyter Notebook is running at: http://0.0.0.0:8888/\ndeepdive-notebooks_1  | [I 14:10:01.990 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\nuser@vm:~$ docker ps\nCONTAINER ID        IMAGE                   COMMAND                  CREATED             STATUS              PORTS               NAMES\ndc7ee2c58466        hazyresearch/postgres   \"/docker-entrypoint.s\"   4 days ago          Up 24 minutes       5432/tcp            sandbox_database_1\nuser@vm:~$ docker run -i -t -v $HOME:/vm hazyresearch/deepdive:latest /bin/bash\njovyan@6353d26f2fd8:/vm/spouse$ ls\napp.ddlog  db.url  deepdive.conf  input  run  udf\njovyan@6353d26f2fd8:/vm/spouse$ deepdive compile\n\u2018run/compiled\u2019 -> \u201820161122/143702.284636268\u2019\njovyan@6353d26f2fd8:/vm/spouse$ deepdive do sentences\n\u2018run/RUNNING\u2019 -> \u201820161122/143608.888898717\u2019\n2016-11-22 14:36:09.071365 process/ext_sentences_by_nlp_markup/run.sh\n2016-11-22 14:36:09.071107 ################################################################################\n2016-11-22 14:36:09.071216 # Host: 6353d26f2fd8\n2016-11-22 14:36:09.071236 # DeepDive: deepdive v0.8.0-742-g4b812a1 (Linux x86_64)\n2016-11-22 14:36:09.071247 export PATH='/usr/local/bin':\"$PATH\"\n2016-11-22 14:36:09.071256 export DEEPDIVE_PWD='/vm/spouse'\n2016-11-22 14:36:09.071265 export DEEPDIVE_APP='/vm/spouse'\n2016-11-22 14:36:09.071278 cd \"$DEEPDIVE_APP\"/run\n2016-11-22 14:36:09.071288 export DEEPDIVE_RUN_ID='20161122/143608.888898717'\n2016-11-22 14:36:09.071297 # Plan: 20161122/143608.888898717/plan.sh\n2016-11-22 14:36:09.071306 # Targets: sentences\n2016-11-22 14:36:09.071315 ################################################################################\n2016-11-22 14:36:09.071324 \n2016-11-22 14:36:09.071334     # process/init/app/run.sh ####################################### last done: 2016-11-21T23:38:35+0000 (14h 57m 33s ago)\n2016-11-22 14:36:09.071344     # process/init/relation/articles/run.sh ######################### last done: 2016-11-21T23:38:35+0000 (14h 57m 33s ago)\n2016-11-22 14:36:09.071353     # deepdive mark 'done' data/articles ############################ last done: 2016-11-21T23:38:35+0000 (14h 57m 33s ago)\n2016-11-22 14:36:09.071365 process/ext_sentences_by_nlp_markup/run.sh ########################## last done: N/A\n2016-11-22 14:36:09.071375 ++ dirname process/ext_sentences_by_nlp_markup/run.sh\n2016-11-22 14:36:09.071384 + cd process/ext_sentences_by_nlp_markup\n2016-11-22 14:36:09.071407 + : dd_tmp_ dd_old_\n2016-11-22 14:36:09.071436 + export DEEPDIVE_CURRENT_PROCESS_NAME=process/ext_sentences_by_nlp_markup\n2016-11-22 14:36:09.071467 + DEEPDIVE_CURRENT_PROCESS_NAME=process/ext_sentences_by_nlp_markup\n2016-11-22 14:36:09.071487 + export DEEPDIVE_LOAD_FORMAT=tsv\n2016-11-22 14:36:09.071524 + DEEPDIVE_LOAD_FORMAT=tsv\n2016-11-22 14:36:09.071553 + output_relation=sentences\n2016-11-22 14:36:09.071568 + output_relation_tmp=dd_tmp_sentences\n2016-11-22 14:36:09.071579 + output_relation_old=dd_old_sentences\n2016-11-22 14:36:09.071591 + deepdive create table-if-not-exists sentences\n2016-11-22 14:36:09.192254 psql: could not connect to server: Connection refused\n2016-11-22 14:36:09.192330  Is the server running on host \"localhost\" (::1) and accepting\n2016-11-22 14:36:09.192347  TCP/IP connections on port 5432?\n2016-11-22 14:36:09.192367 could not connect to server: Connection refused\n2016-11-22 14:36:09.192377  Is the server running on host \"localhost\" (127.0.0.1) and accepting\n2016-11-22 14:36:09.192390  TCP/IP connections on port 5432?\n2016-11-22 14:36:09.227064 psql: could not connect to server: Connection refused\n2016-11-22 14:36:09.227139  Is the server running on host \"localhost\" (::1) and accepting\n2016-11-22 14:36:09.227156  TCP/IP connections on port 5432?\n2016-11-22 14:36:09.227166 could not connect to server: Connection refused\n2016-11-22 14:36:09.227187  Is the server running on host \"localhost\" (127.0.0.1) and accepting\n2016-11-22 14:36:09.227196  TCP/IP connections on port 5432?\n2016-11-22 14:36:09.261292 psql: could not connect to server: Connection refused\n2016-11-22 14:36:09.261378  Is the server running on host \"localhost\" (::1) and accepting\n2016-11-22 14:36:09.261396  TCP/IP connections on port 5432?\n2016-11-22 14:36:09.261416 could not connect to server: Connection refused\n2016-11-22 14:36:09.261426  Is the server running on host \"localhost\" (127.0.0.1) and accepting\n2016-11-22 14:36:09.261439  TCP/IP connections on port 5432?\njovyan@6353d26f2fd8:/ConfinedWater$ psql\npsql: could not connect to server: No such file or directory\n    Is the server running locally and accepting\n    connections on Unix domain socket \"/var/run/postgresql/.s.PGSQL.5432\"?\n```\nAny hint?\nAs I don't don't particularly like Docker (on top of ubuntu@vmware@windows), I'd prefer to install and use Deepdive. I have three question:\n(1) Is there an older version I can use which will work?\n(2) Any ETA on version 0.9?\n(3) Will v0.9 include a DD installer, or are you going to stick with Docker?\nThank you in advance! Keep up the good work.. ",
    "rudaoshi": "I removed the last columns of both person_has_cancer.tsv and person_smoke.tsv. The above error disappear but following error occurs:\n2016-12-20 11:29:24.266355 + sampler-dw gibbs -w /dev/fd/63 -v /dev/fd/62 -f /dev/fd/61 -m factorgraph/meta -o weights -l 0 -i 1000 --alpha 0.01\n2016-12-20 11:29:24.266373 ++ find -L factorgraph/factors -type f -exec pbzip2 -c -d -k '{}' +\n2016-12-20 11:29:24.282508 pbzip2: ERROR: File [factorgraph/variables/person_has_cancer/variables.part-2.bin.bz2] is NOT a valid bzip2!  Skipping...\n2016-12-20 11:29:24.282627 -------------------------------------------\n2016-12-20 11:29:24.282648 pbzip2: ERROR: File [factorgraph/variables/person_has_cancer/variables.part-3.bin.bz2] is NOT a valid bzip2!  Skipping...\n2016-12-20 11:29:24.282666 -------------------------------------------\n2016-12-20 11:29:24.282910 pbzip2: *ERROR: File [factorgraph/variables/person_smokes/variables.part-2.bin.bz2] is NOT a valid bzip2!  Skipping...\n2016-12-20 11:29:24.283018 -------------------------------------------\n2016-12-20 11:29:24.283069 pbzip2: *ERROR: File [factorgraph/variables/person_smokes/variables.part-3.bin.bz2] is NOT a valid bzip2!  Skipping...\n2016-12-20 11:29:24.283109 -------------------------------------------\n2016-12-20 11:29:24.291100 PARSE ERROR:\n2016-12-20 11:29:24.291156              Required argument missing: n_samples_per_learning_epoch\n2016-12-20 11:29:24.291170\n2016-12-20 11:29:24.291187 Brief USAGE:\n2016-12-20 11:29:24.291267    sampler-dw gibbs  [--learn_non_evidence] ...  [--sample_evidence] ...\n2016-12-20 11:29:24.291354                      [-q] ...  [--regularization ] ...  [-b\n2016-12-20 11:29:24.291436                      ] ...  [-d ] ...  [-p ] ...\n2016-12-20 11:29:24.291527                      [-a ] ...  [-c ] ...  [--burn_in ]\n2016-12-20 11:29:24.291609                      ...  -i  ...  -s  ...  -l  ...  [-j\n2016-12-20 11:29:24.291782                      ] [-r ] [-o ] [-w ]\n2016-12-20 11:29:24.291875                      [-e ] [-f ] [-v ] [-m\n2016-12-20 11:29:24.291955                      ] [--] [--version] [-h]. After add \"--n_samples_per_learning_epoch 3\"  in the parameter list\uff0c the program runs smoothly. \nThe data file, config and document may need update.. When I want to define a unsupervised model by using \"p(x,y)=NULL: ....\", the program report: \ncolumn \"label\" is of type boolean but expression is of type text\nIt seems that the error is about a bug in sql generation module.  . @chrismre  Thank you for mentioning another amazing projects. I'll look at it. \nHowever, currently I'd rather like to know how to do this by deepdive. . Thank you very much to every one. I'll try your recommended solutions. \n. @thodrek I got following error using Deepdive 0.8:\n2016-12-21 11:43:00.679659 [error] app.ddlog[176.44] failure: :-' expected but:' found\n2016-12-21 11:43:00.679741\n2016-12-21 11:43:00.679756 has_relation(p1_id, p2_id, relation) = NULL:boolean\n2016-12-21 11:43:00.679767\n2016-12-21 11:43:00.679947                                                                       ^\nDoes this feature need newer version cloned from github?. I removed the last columns of both person_has_cancer.tsv and person_smoke.tsv. The above error disappear but following error occurs:\n2016-12-20 11:29:24.266355 + sampler-dw gibbs -w /dev/fd/63 -v /dev/fd/62 -f /dev/fd/61 -m factorgraph/meta -o weights -l 0 -i 1000 --alpha 0.01\n2016-12-20 11:29:24.266373 ++ find -L factorgraph/factors -type f -exec pbzip2 -c -d -k '{}' +\n2016-12-20 11:29:24.282508 pbzip2: ERROR: File [factorgraph/variables/person_has_cancer/variables.part-2.bin.bz2] is NOT a valid bzip2!  Skipping...\n2016-12-20 11:29:24.282627 -------------------------------------------\n2016-12-20 11:29:24.282648 pbzip2: ERROR: File [factorgraph/variables/person_has_cancer/variables.part-3.bin.bz2] is NOT a valid bzip2!  Skipping...\n2016-12-20 11:29:24.282666 -------------------------------------------\n2016-12-20 11:29:24.282910 pbzip2: *ERROR: File [factorgraph/variables/person_smokes/variables.part-2.bin.bz2] is NOT a valid bzip2!  Skipping...\n2016-12-20 11:29:24.283018 -------------------------------------------\n2016-12-20 11:29:24.283069 pbzip2: *ERROR: File [factorgraph/variables/person_smokes/variables.part-3.bin.bz2] is NOT a valid bzip2!  Skipping...\n2016-12-20 11:29:24.283109 -------------------------------------------\n2016-12-20 11:29:24.291100 PARSE ERROR:\n2016-12-20 11:29:24.291156              Required argument missing: n_samples_per_learning_epoch\n2016-12-20 11:29:24.291170\n2016-12-20 11:29:24.291187 Brief USAGE:\n2016-12-20 11:29:24.291267    sampler-dw gibbs  [--learn_non_evidence] ...  [--sample_evidence] ...\n2016-12-20 11:29:24.291354                      [-q] ...  [--regularization ] ...  [-b\n2016-12-20 11:29:24.291436                      ] ...  [-d ] ...  [-p ] ...\n2016-12-20 11:29:24.291527                      [-a ] ...  [-c ] ...  [--burn_in ]\n2016-12-20 11:29:24.291609                      ...  -i  ...  -s  ...  -l  ...  [-j\n2016-12-20 11:29:24.291782                      ] [-r ] [-o ] [-w ]\n2016-12-20 11:29:24.291875                      [-e ] [-f ] [-v ] [-m\n2016-12-20 11:29:24.291955                      ] [--] [--version] [-h]. After add \"--n_samples_per_learning_epoch 3\"  in the parameter list\uff0c the program runs smoothly. \nThe data file, config and document may need update.. When I want to define a unsupervised model by using \"p(x,y)=NULL: ....\", the program report: \ncolumn \"label\" is of type boolean but expression is of type text\nIt seems that the error is about a bug in sql generation module.  . @chrismre  Thank you for mentioning another amazing projects. I'll look at it. \nHowever, currently I'd rather like to know how to do this by deepdive. . Thank you very much to every one. I'll try your recommended solutions. \n. @thodrek I got following error using Deepdive 0.8:\n2016-12-21 11:43:00.679659 [error] app.ddlog[176.44] failure: :-' expected but:' found\n2016-12-21 11:43:00.679741\n2016-12-21 11:43:00.679756 has_relation(p1_id, p2_id, relation) = NULL:boolean\n2016-12-21 11:43:00.679767\n2016-12-21 11:43:00.679947                                                                       ^\nDoes this feature need newer version cloned from github?. ",
    "eadren": "DO NOT download zip file. Use git clone instead!. Hi! https://github.com/LatticeData/deepdive/blob/alpha/doc/installation.md is NOT FOUND. I used the Quick Installation. In other words, I mean I have the same problem as issue #604 . \n        Sorry but not solved with me.\n        On 04/23/2017 16:01, dqqian wrote: Have you solved it? That also goes wrong with me\n\n\u2014You are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or mute the thread.\n{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/HazyResearch/deepdive\",\"title\":\"HazyResearch/deepdive\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/HazyResearch/deepdive\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@dqqian in #631: Have you solved it? That also goes wrong with me\"}],\"action\":{\"name\":\"View Issue\",\"url\":\"https://github.com/HazyResearch/deepdive/issues/631#issuecomment-296426548\"}}}. DO NOT download zip file. Use git clone instead!. Hi! https://github.com/LatticeData/deepdive/blob/alpha/doc/installation.md is NOT FOUND. I used the Quick Installation. In other words, I mean I have the same problem as issue #604 . \n        Sorry but not solved with me.\n        On 04/23/2017 16:01, dqqian wrote: Have you solved it? That also goes wrong with me\n\n\u2014You are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or mute the thread.\n{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/HazyResearch/deepdive\",\"title\":\"HazyResearch/deepdive\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/HazyResearch/deepdive\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@dqqian in #631: Have you solved it? That also goes wrong with me\"}],\"action\":{\"name\":\"View Issue\",\"url\":\"https://github.com/HazyResearch/deepdive/issues/631#issuecomment-296426548\"}}}. ",
    "melporto": "I still haven't managed, do you have any hint? thanks, . I still haven't managed, do you have any hint? thanks, . ",
    "cognitronz": "after some googling, found this thread on stackoverflow.\nhttp://stackoverflow.com/questions/6784463/error-trustanchors-parameter-must-be-non-empty\nfor me, i had to do the following, hope it helps others who have come across this problem.\nsudo update-ca-certificates -f. after some googling, found this thread on stackoverflow.\nhttp://stackoverflow.com/questions/6784463/error-trustanchors-parameter-must-be-non-empty\nfor me, i had to do the following, hope it helps others who have come across this problem.\nsudo update-ca-certificates -f. ",
    "aabhaschauhan": "Hey @cognitronz , can you explain in brief how you created the dummy file? Thanks!. Hey @cognitronz , can you explain in brief how you created the dummy file? Thanks!. ",
    "manning": "Did you ever solve this? Can you provide a document where this can be directly reproduced just calling CoreNLP?. I'd retitle this \"Deepdive character encoding problem\". As you've already determined, the problem isn't with CoreNLP, which handles French and character encodings just fine\u2026.. Did you ever solve this? Can you provide a document where this can be directly reproduced just calling CoreNLP?. I'd retitle this \"Deepdive character encoding problem\". As you've already determined, the problem isn't with CoreNLP, which handles French and character encodings just fine\u2026.. ",
    "dqqcasia": "Have you solved it? That also goes wrong with me. Have you solved it? That also goes wrong with me. ",
    "mcavdar": "Hi I have exactly same problem in Ubuntu 16.04 with DeepDive (v0.9.0rc2-1063-g7b82f5a). Memory was free enough, OS doesn't even use swap space.. Problem is solved for me by adding env variable \"CORENLP_JAVAOPTS=-Xmx8g\". . This problem occurs if you work with a modified spouse example by make test: use clean spouse example (without run folder). It's for French but, this can be helpful for you.\nAfter change, don't forget to build deepdive with make install. More information about build\n. In the first log:\n\n/data/deepdive/examples/spouse: Not compiled yet, please run first: deepdive compile\n\nI think you missed compilation step. Before load data try to compile deepdive config file:\ndeepdive compile. @eromoe @nlpjoe You are right. Even in the first error, deepdive compile line is executed.\nIt can be occured because of \"udf/nlp_markup.sh\" file's permissions. I always give all permissions(777) to udf files.. @nlpjoe Did you touch app.ddlog file? What version of deepdive do you use ?(Also spouse example?) BTW I just realised a strange line in the first output.\npsql: FATAL:  database \"deepdive_spous\" does not exist\nIt should be deepdive_spouse_$USER. \nCan you please let us know which steps do you execute, from scratch.. Hi,\nIt's normal that takes a lot of time compared to other steps. Did it create \"dd_tmp_sentences\" table or not ?\nAnd which dataset you use? I mean how big is your dataset, number of sentence etc..?. Hi @Balachandar-R ,\n\n2017-08-24 04:29:40.458075 ERROR: missing data for column \"sentence_index\"\n\nAccording to this, deepdive doesn't get clearly result/output of CoreNLP.\nI suggest to run this piece of code from terminal, when your CoreNLP system is running.\nwget --post-data \"Hello friend!\" 'http://localhost:24688/?properties={\"annotators\": \"tokenize, ssplit, pos, lemma,ner\",\"outputFormat\": \"json\"}' -O -\nExpected output is something like that, with sentences and index tags:\n\n{\"sentences\":[{\"index\":0,\"tokens\":[{\"index\":1,\"word\":\"Hello\",\"originalText\":\"Hello\",\"lemma\":\"hello\",\"characterOffsetBegin\":0,\"characterOffsetEnd\":5,\"pos\":\"PROPN\",\"ner\":\"O\"},{\"index\":2,\"word\":\"friend\",\"originalText\":\"friend\",\"lemma\":\"friend\",\"characterOffsetBegin\":6,\"characterOffsetEnd\":12,\"pos\":\"VERB\",\"ner\":\"O\"},{\"index\":3,\"word\":\"!\",\"originalText\":\"!\",\"lemma\":\"!\",\"characterO-\n\nIs it same for you?\nAnd are you sure that Deepdive create articles table and load contents properly?. To start CoreNLP, run:\nexport CORENLP_JAVAOPTS=-Xmx4g\ndeepdive corenlp start\nIf you didn't install Corenlp yet, install it before start:\ndeepdive corenlp install. Hi @Balachandar-R ,\nI tried to run spouse example on Deepdive 0.8. Actually, it uses an old  Shift-Reduce Constituency Parser model. (srparser-2014-10-23-models) So I'm not sure but it doesn't seem like it finds the NER.\nI hardly suggest you to update Deepdive. Thus, you can use CoreNLP either with NER functionality or with Regexner functionality. See for train your own NER. Also see Regexner example. If you are interested just with some predefined keyword, Regexner is easy way to do.\n . To build and install last version from source code: link. If you want to install from specific commit, do git checkout <commit_hash> after git clone https://github.com/HazyResearch/deepdive.git. Hi I have exactly same problem in Ubuntu 16.04 with DeepDive (v0.9.0rc2-1063-g7b82f5a). Memory was free enough, OS doesn't even use swap space.. Problem is solved for me by adding env variable \"CORENLP_JAVAOPTS=-Xmx8g\". . This problem occurs if you work with a modified spouse example by make test: use clean spouse example (without run folder). It's for French but, this can be helpful for you.\nAfter change, don't forget to build deepdive with make install. More information about build\n. In the first log:\n\n/data/deepdive/examples/spouse: Not compiled yet, please run first: deepdive compile\n\nI think you missed compilation step. Before load data try to compile deepdive config file:\ndeepdive compile. @eromoe @nlpjoe You are right. Even in the first error, deepdive compile line is executed.\nIt can be occured because of \"udf/nlp_markup.sh\" file's permissions. I always give all permissions(777) to udf files.. @nlpjoe Did you touch app.ddlog file? What version of deepdive do you use ?(Also spouse example?) BTW I just realised a strange line in the first output.\npsql: FATAL:  database \"deepdive_spous\" does not exist\nIt should be deepdive_spouse_$USER. \nCan you please let us know which steps do you execute, from scratch.. Hi,\nIt's normal that takes a lot of time compared to other steps. Did it create \"dd_tmp_sentences\" table or not ?\nAnd which dataset you use? I mean how big is your dataset, number of sentence etc..?. Hi @Balachandar-R ,\n\n2017-08-24 04:29:40.458075 ERROR: missing data for column \"sentence_index\"\n\nAccording to this, deepdive doesn't get clearly result/output of CoreNLP.\nI suggest to run this piece of code from terminal, when your CoreNLP system is running.\nwget --post-data \"Hello friend!\" 'http://localhost:24688/?properties={\"annotators\": \"tokenize, ssplit, pos, lemma,ner\",\"outputFormat\": \"json\"}' -O -\nExpected output is something like that, with sentences and index tags:\n\n{\"sentences\":[{\"index\":0,\"tokens\":[{\"index\":1,\"word\":\"Hello\",\"originalText\":\"Hello\",\"lemma\":\"hello\",\"characterOffsetBegin\":0,\"characterOffsetEnd\":5,\"pos\":\"PROPN\",\"ner\":\"O\"},{\"index\":2,\"word\":\"friend\",\"originalText\":\"friend\",\"lemma\":\"friend\",\"characterOffsetBegin\":6,\"characterOffsetEnd\":12,\"pos\":\"VERB\",\"ner\":\"O\"},{\"index\":3,\"word\":\"!\",\"originalText\":\"!\",\"lemma\":\"!\",\"characterO-\n\nIs it same for you?\nAnd are you sure that Deepdive create articles table and load contents properly?. To start CoreNLP, run:\nexport CORENLP_JAVAOPTS=-Xmx4g\ndeepdive corenlp start\nIf you didn't install Corenlp yet, install it before start:\ndeepdive corenlp install. Hi @Balachandar-R ,\nI tried to run spouse example on Deepdive 0.8. Actually, it uses an old  Shift-Reduce Constituency Parser model. (srparser-2014-10-23-models) So I'm not sure but it doesn't seem like it finds the NER.\nI hardly suggest you to update Deepdive. Thus, you can use CoreNLP either with NER functionality or with Regexner functionality. See for train your own NER. Also see Regexner example. If you are interested just with some predefined keyword, Regexner is easy way to do.\n . To build and install last version from source code: link. If you want to install from specific commit, do git checkout <commit_hash> after git clone https://github.com/HazyResearch/deepdive.git. ",
    "vochicong": "On my Mac, the problem only solves by setting Docker Memory limit to 8.0 GB and \n%env CORENLP_JAVAOPTS=-Xmx4g in the Jupyter notebook.. On my Mac, the problem only solves by setting Docker Memory limit to 8.0 GB and \n%env CORENLP_JAVAOPTS=-Xmx4g in the Jupyter notebook.. ",
    "zian92": "check if its related to string literals (tabs etc). ab tab in your content may break it if you don't escape it properly. I experienced DD to be a little sloppy with encoding (may be related to python 2.7). \nE.g.  \n@tsv_extractor\n@returns(lambda\n                 doc_id=\"text\"\n         : [])\ndef extract(\n        id=\"text\",\n):\n    yield \"\\t\"\nproduces \"ERROR:  extra data after last expected column\" as the string is not encoded properly and DD detects a 2nd column (which it doesn't expect). It took me some time to get this. \nI am sure to be ran into this problem but unable to reproduce ist. @hugochan can you identify the text that produces the error? And at which step?\n. maybe i was a little wrong here: \ni have the following UDF \n@tsv_extractor\n@returns(lambda\n                 doc_id=\"text\",\n                 feature=\"text\",\n         : [])\ndef extract(\n        doc_id=\"text\",\n        feature=\"text\",\n        counter=\"int\",\n):\n    #(1)\n    print sys.stderr, doc_id, feature, counter\n    for _ in range(counter):\n        yield [doc_id, feature]\n    #(2)\n    yield [doc_id, feature + \" \" + str(counter)]\nif (1) is used ((2) as comment), then the UDF fails: ERROR:  missing data for column \"feature\"\nif (2) is used, then it works.  I dont know why it's like that and don't see a difference.\n. @Balachandar-R : i don't see a relation to the original topic of this ticket ;) \nIt may be necessary to decode the rows from the database and to encode your results to be stored in the db. I am not familiar with BOM, but the web should provide an answer to your problem. . deepdive version does the trick. fixed by manually forcing the package update of JSON::XS\nsudo cpanm JSON::XS --force. You can set the number of processes like this: export DEEPDIVE_NUM_PROCESSES=1\n(not sure if and how you can set it for the NLP task). check if its related to string literals (tabs etc). ab tab in your content may break it if you don't escape it properly. I experienced DD to be a little sloppy with encoding (may be related to python 2.7). \nE.g.  \n@tsv_extractor\n@returns(lambda\n                 doc_id=\"text\"\n         : [])\ndef extract(\n        id=\"text\",\n):\n    yield \"\\t\"\nproduces \"ERROR:  extra data after last expected column\" as the string is not encoded properly and DD detects a 2nd column (which it doesn't expect). It took me some time to get this. \nI am sure to be ran into this problem but unable to reproduce ist. @hugochan can you identify the text that produces the error? And at which step?\n. maybe i was a little wrong here: \ni have the following UDF \n@tsv_extractor\n@returns(lambda\n                 doc_id=\"text\",\n                 feature=\"text\",\n         : [])\ndef extract(\n        doc_id=\"text\",\n        feature=\"text\",\n        counter=\"int\",\n):\n    #(1)\n    print sys.stderr, doc_id, feature, counter\n    for _ in range(counter):\n        yield [doc_id, feature]\n    #(2)\n    yield [doc_id, feature + \" \" + str(counter)]\nif (1) is used ((2) as comment), then the UDF fails: ERROR:  missing data for column \"feature\"\nif (2) is used, then it works.  I dont know why it's like that and don't see a difference.\n. @Balachandar-R : i don't see a relation to the original topic of this ticket ;) \nIt may be necessary to decode the rows from the database and to encode your results to be stored in the db. I am not familiar with BOM, but the web should provide an answer to your problem. . deepdive version does the trick. fixed by manually forcing the package update of JSON::XS\nsudo cpanm JSON::XS --force. You can set the number of processes like this: export DEEPDIVE_NUM_PROCESSES=1\n(not sure if and how you can set it for the NLP task). ",
    "Balachandar-R": "Hi zian92,\nCan u elaborate your explanation with an example.So many of us would get befitted.\nThanks,\nBala. Hi Zian92,\nThanks for your explanation.\nI have one more issue with Python Encoding Character with BOM .\nI have my python code to extract the contents from the documents and writing it in a tsv file but at this stage everything goes fine.\nWhile i am processing the same(tsv) file with Deepdive. Deepdive identifies the character(1\u00ef\u00bb\u00bfyQ11CQEAP1X ) from the tsv file and it causes a failure in deepdive do sentences.\nBut I am not sure that this special character causes this issue.\nCould u please help me to get rid off this issue.?. Hi Zian92,\nThanks for your answers.\nI will have the following issue while \"deepdive do sentences\"\nuser@Azmachine:~/pedia$ deepdive do sentences\n\u00e2run/RUNNING\u00e2 -> \u00e220170817/042914.419451517\u00e2\n2017-08-17 04:29:14.710491 process/ext_sentences_by_nlp_markup/run.sh\nunloading: 0:00:00  715KiB [2.29MiB/s] ([2.29MiB/s])\nunloading: 0:00:00    2  [6.55 /s] ([6.55 /s])\nloading dd_tmp_sentences: 1:33:29  277 B [50.6miB/s] ([   0 B/s])\nloading dd_tmp_sentences: 1:33:29    5  [ 891u/s] ([   0 /s])\nalong with \n2017-08-17 04:31:10.411673 Loading parser from serialized file edu/stanford/nlp/models/srparser/englishSR.ser.gz ...OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x000000072b980000, 113770496, 0) failed; error='Cannot allocate memory' (errno=12)\nI am using Deepdive 0.8 stable version.\nThanks in advance,\nBala\n. Hi mcavdar,\nThanks for you reply.\nI have extracted the contents of various documents from a repository.( nearly 700 documents of PPT,PDF,DOCX and weg pages). I have created an article.tsv file which is having all the contents along with a content_id. \nI can execute the deepdive do articles and which created a table of all these contents.\nBut, when i execute the deepdive do sentences command, which will create the dd_tmp_sentences and sentences table.I am using deepdive 0.8 version.\nOn executing the deepdive do sentences,\nCoreNLP processes got started  and finished with an error. I have pasted my logs here for your reference.\nLast few lines for your reference.\n2017-08-24 04:29:04.853776 INFO: Ignoring inactive rule: null\n2017-08-24 04:29:04.854532 Aug 24, 2017 4:29:04 AM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules\n2017-08-24 04:29:04.854563 INFO: Ignoring inactive rule: temporal-composite-8:ranges\n2017-08-24 04:29:04.854789 Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.holidays.sutime.txt\nloading dd_tmp_sentences: 0:01:06    1  [15.1m/s] ([   0 /s])2017-08-24 04:29:40.044975 Loading parser from serialloading dd_tmp_sentences: 0:01:07    1  [14.9m/s] ([   0 /s])2017-08-24 04:29:40.415900 Warning: skipped malformedloading dd_tmp_sentences: 0:01:07    1  [14.9m/s] ([14.9m/s])B/s])ns, this is my first sentence\"}\n2017-08-24 04:29:40.458075 ERROR:  missing data for column \"sentence_index\"\n2017-08-24 04:29:40.458141 CONTEXT:  COPY dd_tmp_sentences, line 1: \"#\"\n2017-08-24 04:29:40.460150 /home/u5/local/util/compute-driver/local/compute-execute: line 140: kill: (50040) - No such process\n2017-08-24 04:29:40.460183 /home/u5/local/util/compute-driver/local/compute-execute: line 140: kill: (50044) - No such process\n2017-08-24 04:29:40.460196 /home/u5/local/util/compute-driver/local/compute-execute: line 140: kill: (50045) - No such process\n2017-08-24 04:29:40.460204 /home/u5/local/util/compute-driver/local/compute-execute: line 140: kill: (50051) - No such process\n2017-08-24 04:29:40.460213 /home/u5/local/util/compute-driver/local/compute-execute: line 140: kill: (50052) - No such process\n\u00e2run/ABORTED\u00e2 -> \u00e220170824/042830.108135291\u00e2\nI also want to know the actual nlp_backup.sh content.\nplease help me to resolve it.\nThanks, \nBala\n. Hi @mcavdar,\nThanks for your responses.\nSince i am  using deepdive 0.8, Bazaar parser is doing document parsing process.So that i could start CoreNLP as a service in this version.  After exporting a variable called DEEPDIVE_NUM_PROCESSES=1 the parser can successfully running on  all the documents without any errors.\nThanks @mcavdar  \n. @mcavdar \nI need to know how to customize the NER to identify some of the DOMAINS(for example if my docs contains any of the keywords like Banking,Healthcare  and Education etc) in the given document content.\nI have followed the official corenlp site to test the Customized NER in the parsers by coding. But i dont know where exactly we need to update this in DeepDive?\nit would be great if  you are able to help me out here.\nThanks,\nBalachandar. HI @mcavdar,\nThanks man.. I have updated DeepDive and now i can create the sentences table as well.\nBut i have one more question.\n1.I have extracted the content from various repositories and writing it in a single file(.tsv) using python.\nBut the fileformat of this .tsv file was \nUTF-8 Unicode (with BOM) text, with very long lines\nBut the fileformats available in the spouse example is something like \nUTF-8 Unicode text, with very long lines\nAnd the Postgres sql also follows the UTF8 encoding scheme.\nHow could you see it from your perspective ?\nAny Suggestion?\nThanks \nBalachandar. Hi zian92,\nCan u elaborate your explanation with an example.So many of us would get befitted.\nThanks,\nBala. Hi Zian92,\nThanks for your explanation.\nI have one more issue with Python Encoding Character with BOM .\nI have my python code to extract the contents from the documents and writing it in a tsv file but at this stage everything goes fine.\nWhile i am processing the same(tsv) file with Deepdive. Deepdive identifies the character(1\u00ef\u00bb\u00bfyQ11CQEAP1X ) from the tsv file and it causes a failure in deepdive do sentences.\nBut I am not sure that this special character causes this issue.\nCould u please help me to get rid off this issue.?. Hi Zian92,\nThanks for your answers.\nI will have the following issue while \"deepdive do sentences\"\nuser@Azmachine:~/pedia$ deepdive do sentences\n\u00e2run/RUNNING\u00e2 -> \u00e220170817/042914.419451517\u00e2\n2017-08-17 04:29:14.710491 process/ext_sentences_by_nlp_markup/run.sh\nunloading: 0:00:00  715KiB [2.29MiB/s] ([2.29MiB/s])\nunloading: 0:00:00    2  [6.55 /s] ([6.55 /s])\nloading dd_tmp_sentences: 1:33:29  277 B [50.6miB/s] ([   0 B/s])\nloading dd_tmp_sentences: 1:33:29    5  [ 891u/s] ([   0 /s])\nalong with \n2017-08-17 04:31:10.411673 Loading parser from serialized file edu/stanford/nlp/models/srparser/englishSR.ser.gz ...OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x000000072b980000, 113770496, 0) failed; error='Cannot allocate memory' (errno=12)\nI am using Deepdive 0.8 stable version.\nThanks in advance,\nBala\n. Hi mcavdar,\nThanks for you reply.\nI have extracted the contents of various documents from a repository.( nearly 700 documents of PPT,PDF,DOCX and weg pages). I have created an article.tsv file which is having all the contents along with a content_id. \nI can execute the deepdive do articles and which created a table of all these contents.\nBut, when i execute the deepdive do sentences command, which will create the dd_tmp_sentences and sentences table.I am using deepdive 0.8 version.\nOn executing the deepdive do sentences,\nCoreNLP processes got started  and finished with an error. I have pasted my logs here for your reference.\nLast few lines for your reference.\n2017-08-24 04:29:04.853776 INFO: Ignoring inactive rule: null\n2017-08-24 04:29:04.854532 Aug 24, 2017 4:29:04 AM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules\n2017-08-24 04:29:04.854563 INFO: Ignoring inactive rule: temporal-composite-8:ranges\n2017-08-24 04:29:04.854789 Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.holidays.sutime.txt\nloading dd_tmp_sentences: 0:01:06    1  [15.1m/s] ([   0 /s])2017-08-24 04:29:40.044975 Loading parser from serialloading dd_tmp_sentences: 0:01:07    1  [14.9m/s] ([   0 /s])2017-08-24 04:29:40.415900 Warning: skipped malformedloading dd_tmp_sentences: 0:01:07    1  [14.9m/s] ([14.9m/s])B/s])ns, this is my first sentence\"}\n2017-08-24 04:29:40.458075 ERROR:  missing data for column \"sentence_index\"\n2017-08-24 04:29:40.458141 CONTEXT:  COPY dd_tmp_sentences, line 1: \"#\"\n2017-08-24 04:29:40.460150 /home/u5/local/util/compute-driver/local/compute-execute: line 140: kill: (50040) - No such process\n2017-08-24 04:29:40.460183 /home/u5/local/util/compute-driver/local/compute-execute: line 140: kill: (50044) - No such process\n2017-08-24 04:29:40.460196 /home/u5/local/util/compute-driver/local/compute-execute: line 140: kill: (50045) - No such process\n2017-08-24 04:29:40.460204 /home/u5/local/util/compute-driver/local/compute-execute: line 140: kill: (50051) - No such process\n2017-08-24 04:29:40.460213 /home/u5/local/util/compute-driver/local/compute-execute: line 140: kill: (50052) - No such process\n\u00e2run/ABORTED\u00e2 -> \u00e220170824/042830.108135291\u00e2\nI also want to know the actual nlp_backup.sh content.\nplease help me to resolve it.\nThanks, \nBala\n. Hi @mcavdar,\nThanks for your responses.\nSince i am  using deepdive 0.8, Bazaar parser is doing document parsing process.So that i could start CoreNLP as a service in this version.  After exporting a variable called DEEPDIVE_NUM_PROCESSES=1 the parser can successfully running on  all the documents without any errors.\nThanks @mcavdar  \n. @mcavdar \nI need to know how to customize the NER to identify some of the DOMAINS(for example if my docs contains any of the keywords like Banking,Healthcare  and Education etc) in the given document content.\nI have followed the official corenlp site to test the Customized NER in the parsers by coding. But i dont know where exactly we need to update this in DeepDive?\nit would be great if  you are able to help me out here.\nThanks,\nBalachandar. HI @mcavdar,\nThanks man.. I have updated DeepDive and now i can create the sentences table as well.\nBut i have one more question.\n1.I have extracted the content from various repositories and writing it in a single file(.tsv) using python.\nBut the fileformat of this .tsv file was \nUTF-8 Unicode (with BOM) text, with very long lines\nBut the fileformats available in the spouse example is something like \nUTF-8 Unicode text, with very long lines\nAnd the Postgres sql also follows the UTF8 encoding scheme.\nHow could you see it from your perspective ?\nAny Suggestion?\nThanks \nBalachandar. ",
    "eromoe": "Does deepdive only need python 2 ??\nI found it was by file name empty, because import urllib,sys;print urllib.unquote(sys.argv[1]) not valid in python 3. \nI changed that to  file=$(python -c 'import sys;from urllib.parse import unquote;print(unquote(sys.argv[1]))' \"$file\"). Anyway, installation is buggy on python3 , I also see some error as\nSetting up python-software-properties (0.92.37.8) ...\n+ false\n+ sudo add-apt-repository -y ppa:openjdk-r/ppa\n+ [[ 0 = 0 ]]\n+ add-apt-repository -y ppa:openjdk-r/ppa\nTraceback (most recent call last):\n  File \"/usr/bin/add-apt-repository\", line 11, in <module>\n    from softwareproperties.SoftwareProperties import SoftwareProperties, shortcut_handler\n  File \"/usr/lib/python3/dist-packages/softwareproperties/SoftwareProperties.py\", line 27, in <module>\n    import apt_pkg\nImportError: No module named 'apt_pkg'\nFinally , I give up and turn to python2 ,   and success.\n. And deepdive would not automaticlly create database\nroot@2f46265d3bfb:/data/deepdive/examples/spouse# deepdive load articles input/articles-1000.tsv.bz2\n/data/deepdive/examples/spouse: Not compiled yet, please run first: deepdive compile\nLoading articles from input/articles-1000.tsv.bz2 (tsv format)\npsql: FATAL:  database \"deepdive_spous\" does not exist\nloading articles: 0:00:00    0  [   0 /s] ([   0 /s])\nloading articles: 0:00:00  192KiB [2.12MiB/s] ([2.12MiB/s])\n```\npostgres=# \\l\n                             List of databases\n   Name    |  Owner   | Encoding  | Collate | Ctype |   Access privileges\n-----------+----------+-----------+---------+-------+-----------------------\n postgres  | postgres | SQL_ASCII | C       | C     |\n template0 | postgres | SQL_ASCII | C       | C     | =c/postgres          +\n           |          |           |         |       | postgres=CTc/postgres\n template1 | postgres | SQL_ASCII | C       | C     | =c/postgres          +\n           |          |           |         |       | postgres=CTc/postgres\n(3 rows)\n```\nAfter I manually create database, then got:\nLoading articles from input/articles-100.tsv.bz2 (tsv format)\nERROR:  relation \"articles\" does not exist\nloading articles: 0:00:00   12  [ 125 /s] ([ 125 /s])\nloading articles: 0:00:00  132KiB [1.34MiB/s] ([1.34MiB/s])\nIs there any real working  guide ??. Does deepdive only need python 2 ??\nI found it was by file name empty, because import urllib,sys;print urllib.unquote(sys.argv[1]) not valid in python 3. \nI changed that to  file=$(python -c 'import sys;from urllib.parse import unquote;print(unquote(sys.argv[1]))' \"$file\"). Anyway, installation is buggy on python3 , I also see some error as\nSetting up python-software-properties (0.92.37.8) ...\n+ false\n+ sudo add-apt-repository -y ppa:openjdk-r/ppa\n+ [[ 0 = 0 ]]\n+ add-apt-repository -y ppa:openjdk-r/ppa\nTraceback (most recent call last):\n  File \"/usr/bin/add-apt-repository\", line 11, in <module>\n    from softwareproperties.SoftwareProperties import SoftwareProperties, shortcut_handler\n  File \"/usr/lib/python3/dist-packages/softwareproperties/SoftwareProperties.py\", line 27, in <module>\n    import apt_pkg\nImportError: No module named 'apt_pkg'\nFinally , I give up and turn to python2 ,   and success.\n. And deepdive would not automaticlly create database\nroot@2f46265d3bfb:/data/deepdive/examples/spouse# deepdive load articles input/articles-1000.tsv.bz2\n/data/deepdive/examples/spouse: Not compiled yet, please run first: deepdive compile\nLoading articles from input/articles-1000.tsv.bz2 (tsv format)\npsql: FATAL:  database \"deepdive_spous\" does not exist\nloading articles: 0:00:00    0  [   0 /s] ([   0 /s])\nloading articles: 0:00:00  192KiB [2.12MiB/s] ([2.12MiB/s])\n```\npostgres=# \\l\n                             List of databases\n   Name    |  Owner   | Encoding  | Collate | Ctype |   Access privileges\n-----------+----------+-----------+---------+-------+-----------------------\n postgres  | postgres | SQL_ASCII | C       | C     |\n template0 | postgres | SQL_ASCII | C       | C     | =c/postgres          +\n           |          |           |         |       | postgres=CTc/postgres\n template1 | postgres | SQL_ASCII | C       | C     | =c/postgres          +\n           |          |           |         |       | postgres=CTc/postgres\n(3 rows)\n```\nAfter I manually create database, then got:\nLoading articles from input/articles-100.tsv.bz2 (tsv format)\nERROR:  relation \"articles\" does not exist\nloading articles: 0:00:00   12  [ 125 /s] ([ 125 /s])\nloading articles: 0:00:00  132KiB [1.34MiB/s] ([1.34MiB/s])\nIs there any real working  guide ??. ",
    "winnerineast": "It seems true based my case.. It seems true based my case.. ",
    "nlpjoe": "@mcavdar He said that \"I also tried the notebook inside that example, in !deepdive complile section got error\". @mcavdar \nI still can't understand why it can't work\n\u279c  spouse git:(master) \u2717 ll udf/nlp_markup.sh\n-rwxrwxrwx  1 Echo  staff   1.1K 11  9 16:59 udf/nlp_markup.sh\n\u279c  spouse git:(master) \u2717 ll input\ntotal 31112\n-rw-r--r--  1 Echo  staff   136K 11  9 16:59 articles-100.tsj.bz2\n-rw-r--r--  1 Echo  staff   136K 11  9 16:59 articles-100.tsv.bz2\n-rw-r--r--  1 Echo  staff   1.2M 11  9 16:59 articles-1000.tsj.bz2\n-rw-r--r--  1 Echo  staff   1.2M 11  9 16:59 articles-1000.tsv.bz2\n-rwxr-xr-x  1 Echo  staff   711B 11  9 16:59 articles.tsj.sh\n-rwxrwxrwx  1 Echo  staff   209K 11 10 14:45 has_spouse\n-rw-r--r--  1 Echo  staff   653K 11  9 16:59 sentences-100.tsj.bz2\n-rw-r--r--  1 Echo  staff   642K 11  9 16:59 sentences-100.tsv.bz2\n-rw-r--r--  1 Echo  staff   5.6M 11  9 16:59 sentences-1000.tsj.bz2\n-rw-r--r--  1 Echo  staff   5.4M 11  9 16:59 sentences-1000.tsv.bz2\n-rwxrwxrwx  1 Echo  staff    76K 11  9 16:59 spouses_dbpedia.csv.bz2\nerror is as same as:\n2017-07-20 01:35:02.525573  [ERROR] base relation 'has_spouse' must have data to load at: input/has_spouse.*\n2017-07-20 01:35:02.525592  [ERROR] FAILED deepdive check compiled_base_relations_have_input_data. @mcavdar He said that \"I also tried the notebook inside that example, in !deepdive complile section got error\". @mcavdar \nI still can't understand why it can't work\n\u279c  spouse git:(master) \u2717 ll udf/nlp_markup.sh\n-rwxrwxrwx  1 Echo  staff   1.1K 11  9 16:59 udf/nlp_markup.sh\n\u279c  spouse git:(master) \u2717 ll input\ntotal 31112\n-rw-r--r--  1 Echo  staff   136K 11  9 16:59 articles-100.tsj.bz2\n-rw-r--r--  1 Echo  staff   136K 11  9 16:59 articles-100.tsv.bz2\n-rw-r--r--  1 Echo  staff   1.2M 11  9 16:59 articles-1000.tsj.bz2\n-rw-r--r--  1 Echo  staff   1.2M 11  9 16:59 articles-1000.tsv.bz2\n-rwxr-xr-x  1 Echo  staff   711B 11  9 16:59 articles.tsj.sh\n-rwxrwxrwx  1 Echo  staff   209K 11 10 14:45 has_spouse\n-rw-r--r--  1 Echo  staff   653K 11  9 16:59 sentences-100.tsj.bz2\n-rw-r--r--  1 Echo  staff   642K 11  9 16:59 sentences-100.tsv.bz2\n-rw-r--r--  1 Echo  staff   5.6M 11  9 16:59 sentences-1000.tsj.bz2\n-rw-r--r--  1 Echo  staff   5.4M 11  9 16:59 sentences-1000.tsv.bz2\n-rwxrwxrwx  1 Echo  staff    76K 11  9 16:59 spouses_dbpedia.csv.bz2\nerror is as same as:\n2017-07-20 01:35:02.525573  [ERROR] base relation 'has_spouse' must have data to load at: input/has_spouse.*\n2017-07-20 01:35:02.525592  [ERROR] FAILED deepdive check compiled_base_relations_have_input_data. ",
    "nicolaichuk": "+1\nsame issue.\n. @fustbariclation \nSee more https://techcrunch.com/2017/05/13/apple-acquires-ai-company-lattice-data-a-specialist-in-unstructured-dark-data/\nI think that this project is no longer public supported. Try build from source.... +1\nsame issue.\n. @fustbariclation \nSee more https://techcrunch.com/2017/05/13/apple-acquires-ai-company-lattice-data-a-specialist-in-unstructured-dark-data/\nI think that this project is no longer public supported. Try build from source.... ",
    "gerard0315": "@fustbariclation yea, it's commercialized. I've been using Snorkel instead. @fustbariclation yea, it's commercialized. I've been using Snorkel instead. ",
    "romizc": "Good morning, \nI'm having the same issue, I've downloaded the stable version of deepdive and the spouse example. Unfortunately I can't pass the process of NLP_markup, it takes too much time and memory and the process is aborted by the same deepdive. I've tried to update my version because I do not have the command \"deepdive corenlp \", with no luck. \nWill you be so kind to tell me exactly how to make the deepdive update, please.\nThanks in advance.\nMy deepdive version is: \ndeepdive v0.8.0-79-g28a58de (Linux x86_64)\nInformation on this build of deepdive follows.. Good morning, \nI'm having the same issue, I've downloaded the stable version of deepdive and the spouse example. Unfortunately I can't pass the process of NLP_markup, it takes too much time and memory and the process is aborted by the same deepdive. I've tried to update my version because I do not have the command \"deepdive corenlp \", with no luck. \nWill you be so kind to tell me exactly how to make the deepdive update, please.\nThanks in advance.\nMy deepdive version is: \ndeepdive v0.8.0-79-g28a58de (Linux x86_64)\nInformation on this build of deepdive follows.. ",
    "yayitswei": "whops wrong repo, closing. whops wrong repo, closing. "
}
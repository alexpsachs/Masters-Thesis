{
    "rbgirshick": "My guess is that you have another installation of Caffe already in your PYTHONPATH. You'll need to use the included caffe-fast-rcnn git submodule, which defines a couple of new layers, including the RoI pooling layer.\n. Good to hear.\n. Unfortunately you won't be able to use VGG16 with these GPUs, they just don't have enough memory. Even if you go down to using a single image per batch (controlled in lib/fast_rcnn/config.py, btw), you'll still need ~5G of memory.\nYou'll be able to train VGG_CNN_M_1024 and CaffeNet on these cards.\n. Things may have changed, but last I tried cuDNN was slower than Caffe's default conv engine for VGG16. Fiddling with the conv engine may speed up other models, though. Feel free to play around and let us know what you find.\n. Hi @riddhiman-dasgupta, the caffe-users google group is the best place for this type of question. An existing data layer may fit your needs. If not, a simple and flexible option is to write a Python layer (you can model it after the roi_data_layer).\n. Yes, I was lazy and didn't implement a CPU version of backwards here because it would likely never see use. I'll look into how to disable this test so that make runtest can execute cleanly.\n. I've disabled the roi_pooling_layer CPU device test for now https://github.com/rbgirshick/caffe-fast-rcnn/commit/bcd9b4eadc7d8fbc433aeefd564e82ec63aaf69c. Better tests are a todo.\n. Thanks! The demo would work fine when run from the root dir, but this fixes it so it can be run from any dir. I've merged the PR, after a minor formatting edit.\n. Thanks! (I have done very little CPU mode testing.)\n. train_net.py requires some minor code changes to support CPU training (add a --cpu option to argparser and then handle it appropriately; see demo.py for an example of this). I might add this to the code, but for my workflow CPU-based training is not very useful. Feel free to PR the change (if you do, you should also add a CPU option to test_net.py).\n. This appears to be an issue with Caffe and your system. You can seek help in the caffe-users group.\n. It looks like you have 3 classes. In the train.prototxt and test.prototxt files that you're using, you'll need to change num_output from 21 to 3 in the cls_score layer and from 84 to 12 in the bbox_pred layer. You'll also need to change num_classes from 21 to 3 in the Python layer that provides data to the net (the very first layer).\n. Yes, please see issue #9. A rough rule of thumb for running convnets on current GPUs vs CPUs: GPU will run the forward pass about 10x faster than an 8-core CPU.\n. I've been working on a COCO baseline and have posted the code to the coco branch.\nMy version of lib/datasets/coco.py is a bit different in that it handles crowd annotations, uses the python coco api for evaluation, and supports proposals in the format generated by Hosang et al..\n. I haven't tried using it with Python 3, so it's almost certain that the code would need some patching.\n. My recommendation is to look for help in the caffe-users group.\n. Hi @zeyuanxy, I'm curious to know what mAP you get on INRIA?\nI won't be able to merge this PR in its current form because it includes the selective search code, which is under a license that does not permit redistribution.\n. In my experiments I have just used the pixel mean values from VGG16, even for the CaffeNet and CNN_M experiments. @tzutalin: have you found any impact on mAP for CaffeNet when changing the pixel mean values? (I suspect the effect will be very small.)\n. For that particular image, I used the pre-computed selective search boxes from http://homepages.inf.ed.ac.uk/juijling/index.php#page=projects1. The authors say they used \"fast mode\", but perhaps they computed those boxes with a slightly different version of selective search than what they released?\nFor datasets after VOC 2007, I used the code here: https://github.com/rbgirshick/rcnn/tree/master/selective_search to compute boxes myself. Those should be reproducible. It's unfortunate that the boxes they distributed for 2007 seem hard to reproduce (all the more reason to move beyond selective search: http://arxiv.org/abs/1506.01497).\n. Thanks! This issue is fixed in https://github.com/rbgirshick/py-faster-rcnn/commit/45e0da9a246fab5fd86e8c96dc351be7f145499f.\n. 1/16 comes from the fact that the network performs a 16x spatial reduction through max pooling or strided convolution.\n. You should only apply NMS to detections that have a score greater than some threshold. For example, even a low threshold of 0.05 greatly reduces the number of detections to process, and thus speeds up the NMS calls. For the demo, I've updated it to use the conf threshold that's already in the code.\n. The boxes for the demo image come from the VOC 2007 download on Jasper Uijling's homepage: http://homepages.inf.ed.ac.uk/juijling/index.php#page=projects1. My understanding is that these boxes were generated with a deprecated version of selective search, not the IJCV version.\n. I haven't run into this issue with scipy 0.15.1 (via the anaconda distro).\nOn Tue, Sep 22, 2015 at 3:15 AM, markusnagel notifications@github.com\nwrote:\n\nI had the same issue. Further investigations showed that the error comes\nat 'image 29'. This image has 0 gt annotations. Somehow it seems that\nscipy.sparse.vstack have a bug with concatenating sparse arrays that have a\ndimension with 0 length. You can manually handle that case in imdb.py by\nchanging line 224 to:\nif np.shape(a[i]['gt_overlaps'])[0] > 0:\na[i]['gt_overlaps'] = scipy.sparse.vstack([a[i]['gt_overlaps'],\nb[i]['gt_overlaps']])\nelse:\na[i]['gt_overlaps'] = b[i]['gt_overlaps']\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rbgirshick/fast-rcnn/issues/54#issuecomment-142238339\n.\n. Please see the note about reproducing results \"up to stochastic variation\" here.\n\nUnfortunately I did not use a fixed random seed for the value of 66.9%. Subsequently, I fixed the rng seed in the code to make results reproducible. Performance is around mAP = 67.2% +/- 0.2 based on multiple runs with different seeds. I didn't have enough GPUs and time while preparing the paper to run each experiment multiple times. Ideally I would provide information on the mAP distribution for each experiment.\n. Thanks for spotting this issue; it is a bug. The bias shouldn't be scaled. This bug will lead to incorrect decision function values. However, it shouldn't change the class APs returned by the post-hoc SVM network. The scaled bias will just shift the scores of all detections by the same constant offset. That shift leaves the relative order of the detections unchanged. Nevertheless, this needs fixing...\n. Good point. Hopefully I'll have cycles to see how this effects the SVM experiments. Or maybe you already have some results?\n. I re-trained models from scratch and then tested softmax vs. svm after fixing the scaled-bias bug. Here are the new results. (The softmax results differ from those in the paper because these runs use a different random seed.)\nmAP on VOC2007 test:\n| model | softmax | svm |\n| --- | --- | --- |\n| CaffeNet | 56.4 | 55.9 |\n| VGG M | 59.6 | 59.3 |\n| VGG 16 | 67.3 | 67.0 |\nThe feature_scale ranged from around .7 to .9 and so it was shrinking the negative biases towards 0 in the buggy experiments. This bias shrinkage had the effect of including more negative examples, not fewer, which is why the overall conclusion didn't change.\n. Fixed in master.\n. Thanks for the PR to fix the links.\n. Thanks for catching this typo. I missed it after refactoring. It's fixed in https://github.com/rbgirshick/fast-rcnn/commit/364344b6658b98558c2a447fb472bf3e6f0421bc.\n. Including the gt boxes for 2007 test is intentional for two reasons:\n1) Sometimes we want to use 2007 test for training (e.g., train on 2007 train+val+test + 2012 train+val, then test on 2012 test).\n2) It's interesting to see how much mAP improves if you include the gt annotations at test time.\nThe testing code excludes the gt boxes, so including them in 2007 test doesn't cause any problems.\n. @nw362 the paper you referenced is previous work. This paper is the correct one for this code: http://arxiv.org/pdf/1504.08083.pdf.\n. Thanks for the bug report! I've fixed this in https://github.com/rbgirshick/py-faster-rcnn/commit/45e0da9a246fab5fd86e8c96dc351be7f145499f with a slightly different approach.\n. Fixed in https://github.com/rbgirshick/py-faster-rcnn/commit/45e0da9a246fab5fd86e8c96dc351be7f145499f.\n. My guess is that you have another installation of Caffe already in your PYTHONPATH. You'll need to use the included caffe-fast-rcnn git submodule, which defines a couple of new layers, including the RoI pooling layer.\n. Good to hear.\n. Unfortunately you won't be able to use VGG16 with these GPUs, they just don't have enough memory. Even if you go down to using a single image per batch (controlled in lib/fast_rcnn/config.py, btw), you'll still need ~5G of memory.\nYou'll be able to train VGG_CNN_M_1024 and CaffeNet on these cards.\n. Things may have changed, but last I tried cuDNN was slower than Caffe's default conv engine for VGG16. Fiddling with the conv engine may speed up other models, though. Feel free to play around and let us know what you find.\n. Hi @riddhiman-dasgupta, the caffe-users google group is the best place for this type of question. An existing data layer may fit your needs. If not, a simple and flexible option is to write a Python layer (you can model it after the roi_data_layer).\n. Yes, I was lazy and didn't implement a CPU version of backwards here because it would likely never see use. I'll look into how to disable this test so that make runtest can execute cleanly.\n. I've disabled the roi_pooling_layer CPU device test for now https://github.com/rbgirshick/caffe-fast-rcnn/commit/bcd9b4eadc7d8fbc433aeefd564e82ec63aaf69c. Better tests are a todo.\n. Thanks! The demo would work fine when run from the root dir, but this fixes it so it can be run from any dir. I've merged the PR, after a minor formatting edit.\n. Thanks! (I have done very little CPU mode testing.)\n. train_net.py requires some minor code changes to support CPU training (add a --cpu option to argparser and then handle it appropriately; see demo.py for an example of this). I might add this to the code, but for my workflow CPU-based training is not very useful. Feel free to PR the change (if you do, you should also add a CPU option to test_net.py).\n. This appears to be an issue with Caffe and your system. You can seek help in the caffe-users group.\n. It looks like you have 3 classes. In the train.prototxt and test.prototxt files that you're using, you'll need to change num_output from 21 to 3 in the cls_score layer and from 84 to 12 in the bbox_pred layer. You'll also need to change num_classes from 21 to 3 in the Python layer that provides data to the net (the very first layer).\n. Yes, please see issue #9. A rough rule of thumb for running convnets on current GPUs vs CPUs: GPU will run the forward pass about 10x faster than an 8-core CPU.\n. I've been working on a COCO baseline and have posted the code to the coco branch.\nMy version of lib/datasets/coco.py is a bit different in that it handles crowd annotations, uses the python coco api for evaluation, and supports proposals in the format generated by Hosang et al..\n. I haven't tried using it with Python 3, so it's almost certain that the code would need some patching.\n. My recommendation is to look for help in the caffe-users group.\n. Hi @zeyuanxy, I'm curious to know what mAP you get on INRIA?\nI won't be able to merge this PR in its current form because it includes the selective search code, which is under a license that does not permit redistribution.\n. In my experiments I have just used the pixel mean values from VGG16, even for the CaffeNet and CNN_M experiments. @tzutalin: have you found any impact on mAP for CaffeNet when changing the pixel mean values? (I suspect the effect will be very small.)\n. For that particular image, I used the pre-computed selective search boxes from http://homepages.inf.ed.ac.uk/juijling/index.php#page=projects1. The authors say they used \"fast mode\", but perhaps they computed those boxes with a slightly different version of selective search than what they released?\nFor datasets after VOC 2007, I used the code here: https://github.com/rbgirshick/rcnn/tree/master/selective_search to compute boxes myself. Those should be reproducible. It's unfortunate that the boxes they distributed for 2007 seem hard to reproduce (all the more reason to move beyond selective search: http://arxiv.org/abs/1506.01497).\n. Thanks! This issue is fixed in https://github.com/rbgirshick/py-faster-rcnn/commit/45e0da9a246fab5fd86e8c96dc351be7f145499f.\n. 1/16 comes from the fact that the network performs a 16x spatial reduction through max pooling or strided convolution.\n. You should only apply NMS to detections that have a score greater than some threshold. For example, even a low threshold of 0.05 greatly reduces the number of detections to process, and thus speeds up the NMS calls. For the demo, I've updated it to use the conf threshold that's already in the code.\n. The boxes for the demo image come from the VOC 2007 download on Jasper Uijling's homepage: http://homepages.inf.ed.ac.uk/juijling/index.php#page=projects1. My understanding is that these boxes were generated with a deprecated version of selective search, not the IJCV version.\n. I haven't run into this issue with scipy 0.15.1 (via the anaconda distro).\nOn Tue, Sep 22, 2015 at 3:15 AM, markusnagel notifications@github.com\nwrote:\n\nI had the same issue. Further investigations showed that the error comes\nat 'image 29'. This image has 0 gt annotations. Somehow it seems that\nscipy.sparse.vstack have a bug with concatenating sparse arrays that have a\ndimension with 0 length. You can manually handle that case in imdb.py by\nchanging line 224 to:\nif np.shape(a[i]['gt_overlaps'])[0] > 0:\na[i]['gt_overlaps'] = scipy.sparse.vstack([a[i]['gt_overlaps'],\nb[i]['gt_overlaps']])\nelse:\na[i]['gt_overlaps'] = b[i]['gt_overlaps']\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rbgirshick/fast-rcnn/issues/54#issuecomment-142238339\n.\n. Please see the note about reproducing results \"up to stochastic variation\" here.\n\nUnfortunately I did not use a fixed random seed for the value of 66.9%. Subsequently, I fixed the rng seed in the code to make results reproducible. Performance is around mAP = 67.2% +/- 0.2 based on multiple runs with different seeds. I didn't have enough GPUs and time while preparing the paper to run each experiment multiple times. Ideally I would provide information on the mAP distribution for each experiment.\n. Thanks for spotting this issue; it is a bug. The bias shouldn't be scaled. This bug will lead to incorrect decision function values. However, it shouldn't change the class APs returned by the post-hoc SVM network. The scaled bias will just shift the scores of all detections by the same constant offset. That shift leaves the relative order of the detections unchanged. Nevertheless, this needs fixing...\n. Good point. Hopefully I'll have cycles to see how this effects the SVM experiments. Or maybe you already have some results?\n. I re-trained models from scratch and then tested softmax vs. svm after fixing the scaled-bias bug. Here are the new results. (The softmax results differ from those in the paper because these runs use a different random seed.)\nmAP on VOC2007 test:\n| model | softmax | svm |\n| --- | --- | --- |\n| CaffeNet | 56.4 | 55.9 |\n| VGG M | 59.6 | 59.3 |\n| VGG 16 | 67.3 | 67.0 |\nThe feature_scale ranged from around .7 to .9 and so it was shrinking the negative biases towards 0 in the buggy experiments. This bias shrinkage had the effect of including more negative examples, not fewer, which is why the overall conclusion didn't change.\n. Fixed in master.\n. Thanks for the PR to fix the links.\n. Thanks for catching this typo. I missed it after refactoring. It's fixed in https://github.com/rbgirshick/fast-rcnn/commit/364344b6658b98558c2a447fb472bf3e6f0421bc.\n. Including the gt boxes for 2007 test is intentional for two reasons:\n1) Sometimes we want to use 2007 test for training (e.g., train on 2007 train+val+test + 2012 train+val, then test on 2012 test).\n2) It's interesting to see how much mAP improves if you include the gt annotations at test time.\nThe testing code excludes the gt boxes, so including them in 2007 test doesn't cause any problems.\n. @nw362 the paper you referenced is previous work. This paper is the correct one for this code: http://arxiv.org/pdf/1504.08083.pdf.\n. Thanks for the bug report! I've fixed this in https://github.com/rbgirshick/py-faster-rcnn/commit/45e0da9a246fab5fd86e8c96dc351be7f145499f with a slightly different approach.\n. Fixed in https://github.com/rbgirshick/py-faster-rcnn/commit/45e0da9a246fab5fd86e8c96dc351be7f145499f.\n. ",
    "songjun54cm": "Thank you very much!\nBecause i clone the caffe-fast-rcnn by hand, so it is not on the correct branch.\nAfter checking out the fast-rcnn branch and re-compiling the code, the error is gone.\nThank you for give the clue to figure it out.\n. As I have explained, I check out the correct branch and then recompile the code, the error was gone.\nI think you should to check that you checked out the correct branch fast-rcnn in the caffe-fast-rcnn root.\n. Thank you very much!\nBecause i clone the caffe-fast-rcnn by hand, so it is not on the correct branch.\nAfter checking out the fast-rcnn branch and re-compiling the code, the error is gone.\nThank you for give the clue to figure it out.\n. As I have explained, I check out the correct branch and then recompile the code, the error was gone.\nI think you should to check that you checked out the correct branch fast-rcnn in the caffe-fast-rcnn root.\n. ",
    "xprincehf": "hi, I have the same error as you,  how do you re-compile the code, I recompile under the caffe-fast-rcnn root as caffe install guide,but  after that ,the error still exist.\n. hi, I have the same error as you,  how do you re-compile the code, I recompile under the caffe-fast-rcnn root as caffe install guide,but  after that ,the error still exist.\n. ",
    "yuanli12139": "I am not quite familiar with git. Could you please explain it more specifically? Thanks a lot!\n. Hi, \nI ran \"git checkout master\" on my current caffe-fast-rcnn root. Before recompiling, I do \"make clean\". However the following error occurs: \n/bin/sh: /usr/local/cuda/bin/nvcc: No such file or directory\n(standard_in) 1: parse error\nThen I keep moving on, \"make all\", \"make runtest\"... No more error occurs. When I try to run the demo again, I however still encounter the same error as before:\n$ ./tools/demo.py --cpu\n[libprotobuf ERROR google/protobuf/text_format.cc:274] Error parsing text-format caffe.NetParameter: 392:21: Message type \"caffe.LayerParameter\" has no field named \"roi_pooling_param\".\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nF0720 21:37:12.831410 1969263360 upgrade_proto.cpp:928] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: /Users/TRotK/fast-rcnn/models/VGG16/test.prototxt\n* Check failure stack trace: *\nAbort trap: 6 \nAny suggestion is appreciated!\n. I am not quite familiar with git. Could you please explain it more specifically? Thanks a lot!\n. Hi, \nI ran \"git checkout master\" on my current caffe-fast-rcnn root. Before recompiling, I do \"make clean\". However the following error occurs: \n/bin/sh: /usr/local/cuda/bin/nvcc: No such file or directory\n(standard_in) 1: parse error\nThen I keep moving on, \"make all\", \"make runtest\"... No more error occurs. When I try to run the demo again, I however still encounter the same error as before:\n$ ./tools/demo.py --cpu\n[libprotobuf ERROR google/protobuf/text_format.cc:274] Error parsing text-format caffe.NetParameter: 392:21: Message type \"caffe.LayerParameter\" has no field named \"roi_pooling_param\".\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nF0720 21:37:12.831410 1969263360 upgrade_proto.cpp:928] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: /Users/TRotK/fast-rcnn/models/VGG16/test.prototxt\n* Check failure stack trace: *\nAbort trap: 6 \nAny suggestion is appreciated!\n. ",
    "flipflop98": "Hi,\nI have similar situation with above.\nI'm using caffe-fast-rcnn the sam with fast-rcnn and checked \"PYTHONPATH\" parameter indicate the current directory by writing \"export PYTHONPATH=/home/ch723.kim/samba/py-faster-rcnn-master/caffe-fast-rcnn/python:$PYTHONPATH\" in the ~/.bashrc file.\nBut I have following error when playing demo.py as follows.\nch723.kim@ubuntu:~/samba/py-faster-rcnn-master$ ./tools/demo.py\n[libprotobuf ERROR google/protobuf/text_format.cc:274] Error parsing text-format caffe.NetParameter: 310:18: Message type \"caffe.LayerParameter\" has no field named \"reshape_param\".\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nF1205 21:22:31.945125 17247 upgrade_proto.cpp:928] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: /home/ch723.kim/samba/py-faster-rcnn-master/models/VGG16/faster_rcnn_alt_opt/faster_rcnn_test.pt\n* Check failure stack trace: *\nAborted (core dumped)\nAlso I have the same error when I run \"./experiments/scripts/faster_rcnn_alt_opt.sh 3 ZF\" as follows.\nI1205 21:16:10.417134 17205 solver.cpp:32] Initializing solver from parameters: \ntrain_net: \"models/ZF/faster_rcnn_alt_opt/stage1_rpn_train.pt\"\nbase_lr: 0.001\ndisplay: 20\nlr_policy: \"step\"\ngamma: 0.1\nmomentum: 0.9\nweight_decay: 0.0005\nstepsize: 60000\nsnapshot: 0\nsnapshot_prefix: \"zf_rpn\"\naverage_loss: 100\nI1205 21:16:10.417202 17205 solver.cpp:61] Creating training net from train_net file: models/ZF/faster_rcnn_alt_opt/stage1_rpn_train.pt\n[libprotobuf ERROR google/protobuf/text_format.cc:274] Error parsing text-format caffe.NetParameter: 221:18: Message type \"caffe.LayerParameter\" has no field named \"reshape_param\".\nF1205 21:16:10.417758 17205 upgrade_proto.cpp:928] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: models/ZF/faster_rcnn_alt_opt/stage1_rpn_train.pt\n* Check failure stack trace: *\nPlease give me any suggestion.\nThank you.\n. Hi,\nI have similar situation with above.\nI'm using caffe-fast-rcnn the sam with fast-rcnn and checked \"PYTHONPATH\" parameter indicate the current directory by writing \"export PYTHONPATH=/home/ch723.kim/samba/py-faster-rcnn-master/caffe-fast-rcnn/python:$PYTHONPATH\" in the ~/.bashrc file.\nBut I have following error when playing demo.py as follows.\nch723.kim@ubuntu:~/samba/py-faster-rcnn-master$ ./tools/demo.py\n[libprotobuf ERROR google/protobuf/text_format.cc:274] Error parsing text-format caffe.NetParameter: 310:18: Message type \"caffe.LayerParameter\" has no field named \"reshape_param\".\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nF1205 21:22:31.945125 17247 upgrade_proto.cpp:928] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: /home/ch723.kim/samba/py-faster-rcnn-master/models/VGG16/faster_rcnn_alt_opt/faster_rcnn_test.pt\n* Check failure stack trace: *\nAborted (core dumped)\nAlso I have the same error when I run \"./experiments/scripts/faster_rcnn_alt_opt.sh 3 ZF\" as follows.\nI1205 21:16:10.417134 17205 solver.cpp:32] Initializing solver from parameters: \ntrain_net: \"models/ZF/faster_rcnn_alt_opt/stage1_rpn_train.pt\"\nbase_lr: 0.001\ndisplay: 20\nlr_policy: \"step\"\ngamma: 0.1\nmomentum: 0.9\nweight_decay: 0.0005\nstepsize: 60000\nsnapshot: 0\nsnapshot_prefix: \"zf_rpn\"\naverage_loss: 100\nI1205 21:16:10.417202 17205 solver.cpp:61] Creating training net from train_net file: models/ZF/faster_rcnn_alt_opt/stage1_rpn_train.pt\n[libprotobuf ERROR google/protobuf/text_format.cc:274] Error parsing text-format caffe.NetParameter: 221:18: Message type \"caffe.LayerParameter\" has no field named \"reshape_param\".\nF1205 21:16:10.417758 17205 upgrade_proto.cpp:928] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: models/ZF/faster_rcnn_alt_opt/stage1_rpn_train.pt\n* Check failure stack trace: *\nPlease give me any suggestion.\nThank you.\n. ",
    "Mhalla": "Hello,\nI want to start with Fast RCNN but I have this  error\nlinux@linux-XPS-630i:~/FRCN_ROOT$ ./tools/demo.py --cpu\n[libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 4:13: Message type \"caffe.NetParameter\" has no field named \"input_shape\".\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nF1206 20:10:44.134786 28081 upgrade_proto.cpp:928] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: /home/linux/FRCN_ROOT/models/VGG16/test.prototxt\n* Check failure stack trace: *\nAbandon (core dumped)\nDon't really know how to fix this error. I Hope you can help me.\n\n. Hello,\nYou have a problem in the  download of fast RCNN, you must use this command\nfor donlowed the fast RCNN (git clone --recursive\nhttps://github.com/rbgirshick/fast-rcnn.git), because you have\nincompatibility between caffe and Fast RCNN (you dowlonded Fast with\nDownload Zip? )\n2016-05-16 3:26 GMT+02:00 oversnowgrass notifications@github.com:\n\nI have similar situation with above.When i run python demo.py --cpu,I have\nthe error [image: :+1:]\nC:\\Users\\Administrator>f:\nF:>cd caffe\\fast-rcnn-master\\tools\nF:\\caffe\\fast-rcnn-master\\tools>python demo.py --cpu\nD:\\caffe\\caffe-master\\Build\\x64\\Release\\pycaffe\\caffe\\pycaffe.py:13:\nRuntime\n[libprotobuf ERROR ..\\src\\google\\protobuf\\text_format.cc:274] Error\nparsing text\n-format caffe.NetParameter: 392:21: Message type \"caffe.LayerParameter\"\nhas no f\nield named \"roi_pooling_param\".\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nF0516 08:40:48.301647 5344 upgrade_proto.cpp:79] Check failed:\nReadProtoFromTex\ntFile(param_file, param) Failed to parse NetParameter file:\nF:\\caffe\\fast-rcnn-m\naster\\models\\VGG16\\test.prototxt\n* Check failure stack trace: *\nF:\\caffe\\fast-rcnn-master\\tools>\nI have found lots of solution but still don't solve it.Please can you help\nme?This is my fast-rcnn folder:\n[image: qq 20160516092511]\nhttps://cloud.githubusercontent.com/assets/19328128/15278235/3ab64970-1b48-11e6-8199-81c3e49e00c5.png\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\nhttps://github.com/rbgirshick/fast-rcnn/issues/1#issuecomment-219329301\n. Hello,\nI want to start with Fast RCNN but I have this  error\n\nlinux@linux-XPS-630i:~/FRCN_ROOT$ ./tools/demo.py --cpu\n[libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 4:13: Message type \"caffe.NetParameter\" has no field named \"input_shape\".\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nF1206 20:10:44.134786 28081 upgrade_proto.cpp:928] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: /home/linux/FRCN_ROOT/models/VGG16/test.prototxt\n* Check failure stack trace: *\nAbandon (core dumped)\nDon't really know how to fix this error. I Hope you can help me.\n\n. Hello,\nYou have a problem in the  download of fast RCNN, you must use this command\nfor donlowed the fast RCNN (git clone --recursive\nhttps://github.com/rbgirshick/fast-rcnn.git), because you have\nincompatibility between caffe and Fast RCNN (you dowlonded Fast with\nDownload Zip? )\n2016-05-16 3:26 GMT+02:00 oversnowgrass notifications@github.com:\n\nI have similar situation with above.When i run python demo.py --cpu,I have\nthe error [image: :+1:]\nC:\\Users\\Administrator>f:\nF:>cd caffe\\fast-rcnn-master\\tools\nF:\\caffe\\fast-rcnn-master\\tools>python demo.py --cpu\nD:\\caffe\\caffe-master\\Build\\x64\\Release\\pycaffe\\caffe\\pycaffe.py:13:\nRuntime\n[libprotobuf ERROR ..\\src\\google\\protobuf\\text_format.cc:274] Error\nparsing text\n-format caffe.NetParameter: 392:21: Message type \"caffe.LayerParameter\"\nhas no f\nield named \"roi_pooling_param\".\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nF0516 08:40:48.301647 5344 upgrade_proto.cpp:79] Check failed:\nReadProtoFromTex\ntFile(param_file, param) Failed to parse NetParameter file:\nF:\\caffe\\fast-rcnn-m\naster\\models\\VGG16\\test.prototxt\n* Check failure stack trace: *\nF:\\caffe\\fast-rcnn-master\\tools>\nI have found lots of solution but still don't solve it.Please can you help\nme?This is my fast-rcnn folder:\n[image: qq 20160516092511]\nhttps://cloud.githubusercontent.com/assets/19328128/15278235/3ab64970-1b48-11e6-8199-81c3e49e00c5.png\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\nhttps://github.com/rbgirshick/fast-rcnn/issues/1#issuecomment-219329301\n. \n",
    "oversnowgrass": "I have similar situation with above.When i run python demo.py --cpu,I have the error :+1: \nC:\\Users\\Administrator>f:\nF:>cd caffe\\fast-rcnn-master\\tools\nF:\\caffe\\fast-rcnn-master\\tools>python demo.py --cpu\nD:\\caffe\\caffe-master\\Build\\x64\\Release\\pycaffe\\caffe\\pycaffe.py:13: Runtime\n[libprotobuf ERROR ..\\src\\google\\protobuf\\text_format.cc:274] Error parsing text\n-format caffe.NetParameter: 392:21: Message type \"caffe.LayerParameter\" has no f\nield named \"roi_pooling_param\".\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nF0516 08:40:48.301647  5344 upgrade_proto.cpp:79] Check failed: ReadProtoFromTex\ntFile(param_file, param) Failed to parse NetParameter file: F:\\caffe\\fast-rcnn-m\naster\\models\\VGG16\\test.prototxt\n* Check failure stack trace: *\nF:\\caffe\\fast-rcnn-master\\tools>\nI have found lots of solution but still don't solve it.Please can you help me?This is my fast-rcnn folder:\n\n. Thank you for your letter while sleeping.I have downloaded the fast-rcnn in the right way,but it still appear the problem \n\n\nDo you think there have any other solution to the problem?I'm looking forward to your letter.\n. I have similar situation with above.When i run python demo.py --cpu,I have the error :+1: \nC:\\Users\\Administrator>f:\nF:>cd caffe\\fast-rcnn-master\\tools\nF:\\caffe\\fast-rcnn-master\\tools>python demo.py --cpu\nD:\\caffe\\caffe-master\\Build\\x64\\Release\\pycaffe\\caffe\\pycaffe.py:13: Runtime\n[libprotobuf ERROR ..\\src\\google\\protobuf\\text_format.cc:274] Error parsing text\n-format caffe.NetParameter: 392:21: Message type \"caffe.LayerParameter\" has no f\nield named \"roi_pooling_param\".\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nF0516 08:40:48.301647  5344 upgrade_proto.cpp:79] Check failed: ReadProtoFromTex\ntFile(param_file, param) Failed to parse NetParameter file: F:\\caffe\\fast-rcnn-m\naster\\models\\VGG16\\test.prototxt\n* Check failure stack trace: *\nF:\\caffe\\fast-rcnn-master\\tools>\nI have found lots of solution but still don't solve it.Please can you help me?This is my fast-rcnn folder:\n\n. Thank you for your letter while sleeping.I have downloaded the fast-rcnn in the right way,but it still appear the problem \n\n\nDo you think there have any other solution to the problem?I'm looking forward to your letter.\n. ",
    "cervantes-loves-ai": "how to solve this ?\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nW0718 17:01:06.813407 26562 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface\nW0718 17:01:06.813459 26562 _caffe.cpp:123] Use this instead (with the named \"weights\" parameter):\nW0718 17:01:06.813477 26562 _caffe.cpp:125] Net('/home/rvlab/Documents/fast-rcnn-master/models/VGG16/test.prototxt', 1, weights='/home/rvlab/Documents/fast-rcnn-master/data/fast_rcnn_models/vgg16_fast_rcnn_iter_40000.caffemodel')\n[libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 392:21: Message type \"caffe.LayerParameter\" has no field named \"roi_pooling_param\".\nF0718 17:01:06.815565 26562 upgrade_proto.cpp:79] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: /home/rvlab/Documents/fast-rcnn-master/models/VGG16/test.prototxt\n* Check failure stack trace: \nAborted (core dumped\n. how to solve this ?\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nW0718 17:01:06.813407 26562 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface\nW0718 17:01:06.813459 26562 _caffe.cpp:123] Use this instead (with the named \"weights\" parameter):\nW0718 17:01:06.813477 26562 _caffe.cpp:125] Net('/home/rvlab/Documents/fast-rcnn-master/models/VGG16/test.prototxt', 1, weights='/home/rvlab/Documents/fast-rcnn-master/data/fast_rcnn_models/vgg16_fast_rcnn_iter_40000.caffemodel')\n[libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 392:21: Message type \"caffe.LayerParameter\" has no field named \"roi_pooling_param\".\nF0718 17:01:06.815565 26562 upgrade_proto.cpp:79] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: /home/rvlab/Documents/fast-rcnn-master/models/VGG16/test.prototxt\n** Check failure stack trace: *\nAborted (core dumped\n. how to solve this ?\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nW0718 17:01:06.813407 26562 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface\nW0718 17:01:06.813459 26562 _caffe.cpp:123] Use this instead (with the named \"weights\" parameter):\nW0718 17:01:06.813477 26562 _caffe.cpp:125] Net('/home/rvlab/Documents/fast-rcnn-master/models/VGG16/test.prototxt', 1, weights='/home/rvlab/Documents/fast-rcnn-master/data/fast_rcnn_models/vgg16_fast_rcnn_iter_40000.caffemodel')\n[libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 392:21: Message type \"caffe.LayerParameter\" has no field named \"roi_pooling_param\".\nF0718 17:01:06.815565 26562 upgrade_proto.cpp:79] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: /home/rvlab/Documents/fast-rcnn-master/models/VGG16/test.prototxt\n* Check failure stack trace: \nAborted (core dumped\n. how to solve this ?\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nW0718 17:01:06.813407 26562 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface\nW0718 17:01:06.813459 26562 _caffe.cpp:123] Use this instead (with the named \"weights\" parameter):\nW0718 17:01:06.813477 26562 _caffe.cpp:125] Net('/home/rvlab/Documents/fast-rcnn-master/models/VGG16/test.prototxt', 1, weights='/home/rvlab/Documents/fast-rcnn-master/data/fast_rcnn_models/vgg16_fast_rcnn_iter_40000.caffemodel')\n[libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 392:21: Message type \"caffe.LayerParameter\" has no field named \"roi_pooling_param\".\nF0718 17:01:06.815565 26562 upgrade_proto.cpp:79] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: /home/rvlab/Documents/fast-rcnn-master/models/VGG16/test.prototxt\n** Check failure stack trace: *\nAborted (core dumped\n. ",
    "dawin2015": "hi,@rbgirshick @songjun54cm \nI am in China where some places cannot use the git clone --recursive https://github.com/rbgirshick/fast-rcnn.git. Once I did that, it would print time run out error. So I manually downloaded your zip fast-rcnn-master.zip and unzip into the file FRCN_ROOT. Then I followed your  README.md and began from \"3.Build the Cython modules\": cd $FRCN_ROOT/lib &&make. Everything was fine except for this\n dawin@dawin-Lenovo:~/Downloads/FRCN_ROOT$ sudo ./tools/demo.py\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nW0829 15:08:30.115890  7010 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface\nW0829 15:08:30.115929  7010 _caffe.cpp:123] Use this instead (with the named \"weights\" parameter):\nW0829 15:08:30.115936  7010 _caffe.cpp:125] Net('/home/dawin/Downloads/FRCN_ROOT/models/VGG16/test.prototxt', 1, weights='/home/dawin/Downloads/FRCN_ROOT/data/fast_rcnn_models/vgg16_fast_rcnn_iter_40000.caffemodel')\n[libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 392:21: Message type \"caffe.LayerParameter\" has no field named \"roi_pooling_param\".\nF0829 15:08:30.118136  7010 upgrade_proto.cpp:79] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: /home/dawin/Downloads/FRCN_ROOT/models/VGG16/test.prototxt\n*** Check failure stack trace: ***\nI had read all the blogs here and I tried, but I cannot figure out how to deal it.\nI would appreciate your suggestions.\n. my problem is like yours and i run it in ubuntu14.04\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nW0829 12:23:33.219240 29104 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface\nW0829 12:23:33.219276 29104 _caffe.cpp:123] Use this instead (with the named \"weights\" parameter):\nW0829 12:23:33.219281 29104 _caffe.cpp:125] Net('/home/dawin/Downloads/FRCN_ROOT/fast-rcnn/models/CaffeNet/test.prototxt', 1, weights='/home/dawin/Downloads/FRCN_ROOT/fast-rcnn/data/fast_rcnn_models/caffenet_fast_rcnn_iter_40000.caffemodel')\n[libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 195:21: Message type \"caffe.LayerParameter\" has no field named \"roi_pooling_param\".\nF0829 12:23:33.233490 29104 upgrade_proto.cpp:79] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: /home/dawin/Downloads/FRCN_ROOT/fast-rcnn/models/CaffeNet/test.prototxt\n*** Check failure stack trace: ***\n. hi,@rbgirshick @songjun54cm \nI am in China where some places cannot use the git clone --recursive https://github.com/rbgirshick/fast-rcnn.git. Once I did that, it would print time run out error. So I manually downloaded your zip fast-rcnn-master.zip and unzip into the file FRCN_ROOT. Then I followed your  README.md and began from \"3.Build the Cython modules\": cd $FRCN_ROOT/lib &&make. Everything was fine except for this\n dawin@dawin-Lenovo:~/Downloads/FRCN_ROOT$ sudo ./tools/demo.py\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nW0829 15:08:30.115890  7010 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface\nW0829 15:08:30.115929  7010 _caffe.cpp:123] Use this instead (with the named \"weights\" parameter):\nW0829 15:08:30.115936  7010 _caffe.cpp:125] Net('/home/dawin/Downloads/FRCN_ROOT/models/VGG16/test.prototxt', 1, weights='/home/dawin/Downloads/FRCN_ROOT/data/fast_rcnn_models/vgg16_fast_rcnn_iter_40000.caffemodel')\n[libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 392:21: Message type \"caffe.LayerParameter\" has no field named \"roi_pooling_param\".\nF0829 15:08:30.118136  7010 upgrade_proto.cpp:79] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: /home/dawin/Downloads/FRCN_ROOT/models/VGG16/test.prototxt\n*** Check failure stack trace: ***\nI had read all the blogs here and I tried, but I cannot figure out how to deal it.\nI would appreciate your suggestions.\n. my problem is like yours and i run it in ubuntu14.04\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nW0829 12:23:33.219240 29104 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface\nW0829 12:23:33.219276 29104 _caffe.cpp:123] Use this instead (with the named \"weights\" parameter):\nW0829 12:23:33.219281 29104 _caffe.cpp:125] Net('/home/dawin/Downloads/FRCN_ROOT/fast-rcnn/models/CaffeNet/test.prototxt', 1, weights='/home/dawin/Downloads/FRCN_ROOT/fast-rcnn/data/fast_rcnn_models/caffenet_fast_rcnn_iter_40000.caffemodel')\n[libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 195:21: Message type \"caffe.LayerParameter\" has no field named \"roi_pooling_param\".\nF0829 12:23:33.233490 29104 upgrade_proto.cpp:79] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: /home/dawin/Downloads/FRCN_ROOT/fast-rcnn/models/CaffeNet/test.prototxt\n*** Check failure stack trace: ***\n. ",
    "lindylin1817": "You could directly check the src/caffe/proto/caffe.proto. If you couldn't find anything with \"ROI\", you could add it manually. \nInsert the following code in the line around 690. \nmessage ROIPoolingParameter {\n  // Pad, kernel size, and stride are all given as a single value for equal\n  // dimensions in height and width or as Y, X pairs.\n  optional uint32 pooled_h = 1 [default = 0]; // The pooled output height\n  optional uint32 pooled_w = 2 [default = 0]; // The pooled output width\n  // Multiplicative spatial scale factor to translate ROI coords from their\n  // input scale to the scale used when pooling\n  optional float spatial_scale = 3 [default = 1];\n}\nInsert the following line in the line around 320\noptional ROIPoolingParameter roi_pooling_param = 8266711;\nThen build the caffe again with \"make all; make pycaffe\"\n. You could directly check the src/caffe/proto/caffe.proto. If you couldn't find anything with \"ROI\", you could add it manually. \nInsert the following code in the line around 690. \nmessage ROIPoolingParameter {\n  // Pad, kernel size, and stride are all given as a single value for equal\n  // dimensions in height and width or as Y, X pairs.\n  optional uint32 pooled_h = 1 [default = 0]; // The pooled output height\n  optional uint32 pooled_w = 2 [default = 0]; // The pooled output width\n  // Multiplicative spatial scale factor to translate ROI coords from their\n  // input scale to the scale used when pooling\n  optional float spatial_scale = 3 [default = 1];\n}\nInsert the following line in the line around 320\noptional ROIPoolingParameter roi_pooling_param = 8266711;\nThen build the caffe again with \"make all; make pycaffe\"\n. ",
    "markotitel": "Thank you for helping.\n. @forresti Interesting that after installing CUDNN I got no memory trouble.\n. @erohan Hi, unfortunately I am a mere copy paster and google search user. It was more than a year now, I cant help no further on this. But I would advise to start from scratch and follow every step fully.\nI may have somewhere Ansible script for setting up the instance though. But don't think it will help a lot hence in a year period guess a lot has changed.\n. Thank you for helping.\n. @forresti Interesting that after installing CUDNN I got no memory trouble.\n. @erohan Hi, unfortunately I am a mere copy paster and google search user. It was more than a year now, I cant help no further on this. But I would advise to start from scratch and follow every step fully.\nI may have somewhere Ansible script for setting up the instance though. But don't think it will help a lot hence in a year period guess a lot has changed.\n. ",
    "forresti": "If we reduce BATCH_SIZE in config.py, could that enable fast-rcnn to run on low-memory GPUs? (I played with this a bit and it still ran out of memory... but I haven't investigated deeply yet.)\n. If we reduce BATCH_SIZE in config.py, could that enable fast-rcnn to run on low-memory GPUs? (I played with this a bit and it still ran out of memory... but I haven't investigated deeply yet.)\n. ",
    "bipul21": "I am facing the same problem AWS g2.2xlarge when running demo.py\nShould it work there? Or any option ?\n. Edit the same. Try the caffenet model.\nBy running ./tools/demo.py --net=caffenet\nOn Sun, Oct 18, 2015 at 10:56 PM, Sid notifications@github.com wrote:\n\nCould anyone solve this issue yet?I reduced the BATCH_SIZE from 128 to 16\nin the config.py file.Still running out of memory!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rbgirshick/fast-rcnn/issues/2#issuecomment-149031925.\n\n\nRegards\nBipul Jain\n+91-9845885679\n. I am facing the same problem AWS g2.2xlarge when running demo.py\nShould it work there? Or any option ?\n. Edit the same. Try the caffenet model.\nBy running ./tools/demo.py --net=caffenet\nOn Sun, Oct 18, 2015 at 10:56 PM, Sid notifications@github.com wrote:\n\nCould anyone solve this issue yet?I reduced the BATCH_SIZE from 128 to 16\nin the config.py file.Still running out of memory!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rbgirshick/fast-rcnn/issues/2#issuecomment-149031925.\n\n\nRegards\nBipul Jain\n+91-9845885679\n. ",
    "sid027": "Could anyone solve this issue yet?I reduced the BATCH_SIZE from 128 to 16 in the config.py file.Still running out of memory!\n. @IchibanKanobee were you able to figure out if one has to create subfolders?\n. @IchibanKanobee I think it is the corner.I think annotations are needed for all classes except the background.I am trying to train the detector for the ILSVRC2012 imagenet dataset.Did you use the same dataset?\n. Hi,\nWere you able to find these prototxt files for the imagenet models?Did you write it yourself?\nI need them as well.\n. is there a particular reason you used 201/804.....do you have 201 classes?\n. one more question on the bounding boxes.Are they part of the training.Can I give 1000 instead of 804.Is there some specific rule that it needs to be four times?\n. ahh!thanks a lot.I will try this.\n. @liuwenran have a question of image database structure.Do I put each class in a subfolder?What about the background?\n. I have also looked for this.It doesn't seem to exist.\nI also want to do the same as you.Maybe a list of randomly generated overlapping bounding boxes in the same .mat file as it requires should do it.\nSid\n. Yes I trained my own model on imagenet data.\n. You also have to generate the IMDB and run selective search for generating region proposals (requires MATLAB).\nYes I had to modify the pascal_voc.py file to get the one I wanted.Do you want me to post it here?The file is quite big.The following function should be modified according to your setup - _load_selective_search_roidb, _load_inria_annotation. The following class variable should also be modified - self._classes, self._image_ext.\n. @alilemus good to hear.I think the link is also useful for the rest of the people here!\n. Yes,I think.There are modules used which are only available in cuda.\n. you can you selective search for this.There is a python wrapper for this.\n. Could anyone solve this issue yet?I reduced the BATCH_SIZE from 128 to 16 in the config.py file.Still running out of memory!\n. @IchibanKanobee were you able to figure out if one has to create subfolders?\n. @IchibanKanobee I think it is the corner.I think annotations are needed for all classes except the background.I am trying to train the detector for the ILSVRC2012 imagenet dataset.Did you use the same dataset?\n. Hi,\nWere you able to find these prototxt files for the imagenet models?Did you write it yourself?\nI need them as well.\n. is there a particular reason you used 201/804.....do you have 201 classes?\n. one more question on the bounding boxes.Are they part of the training.Can I give 1000 instead of 804.Is there some specific rule that it needs to be four times?\n. ahh!thanks a lot.I will try this.\n. @liuwenran have a question of image database structure.Do I put each class in a subfolder?What about the background?\n. I have also looked for this.It doesn't seem to exist.\nI also want to do the same as you.Maybe a list of randomly generated overlapping bounding boxes in the same .mat file as it requires should do it.\nSid\n. Yes I trained my own model on imagenet data.\n. You also have to generate the IMDB and run selective search for generating region proposals (requires MATLAB).\nYes I had to modify the pascal_voc.py file to get the one I wanted.Do you want me to post it here?The file is quite big.The following function should be modified according to your setup - _load_selective_search_roidb, _load_inria_annotation. The following class variable should also be modified - self._classes, self._image_ext.\n. @alilemus good to hear.I think the link is also useful for the rest of the people here!\n. Yes,I think.There are modules used which are only available in cuda.\n. you can you selective search for this.There is a python wrapper for this.\n. ",
    "aaronpolhamus": "@bipul21: what't the process for passing caffenet into the demo as a valid option? currently executing your command results in demo.py: error: argument --net: invalid choice: 'caffenet' (choose from 'vgg16', 'zf')\n. ah, my mistake, i'm using faster-py-rcnn and was referencing the issues page for fast-rcnn. any guidance for troubleshooting this issue with py-faster-rcnn?\n. @bipul21: what't the process for passing caffenet into the demo as a valid option? currently executing your command results in demo.py: error: argument --net: invalid choice: 'caffenet' (choose from 'vgg16', 'zf')\n. ah, my mistake, i'm using faster-py-rcnn and was referencing the issues page for fast-rcnn. any guidance for troubleshooting this issue with py-faster-rcnn?\n. ",
    "erohan": "@markotitel  Although i installed CudNN5 from https://developer.nvidia.com/rdp/cudnn-download and copied header files to include and shared so libraries to lib64 memory problem continued.\nI am trying to run demo of Fast R-CNN on Ubuntu 14.04. Lastly i run it through ./demo.py --net=zf and it gave an output of detected person, cat, dog, horse, bus.. etc. So, VGG16 database is big and i think everybody must start wt. a small sample. Currently my solution is this for Fast R-CNN\n. @markotitel Lastly I installed nVidia CUDA drivers and because of 2048 MB memory of my graphic card, the VGG16 does not run; it needs approximately 5 GB.\nSo, i run VGG16 with --cpu option to see @ demo phase. --cpu realy performs very slower whan you compare it wt. default option (GPU) but i can try some work so no need to investigate much more about this memory problem.\nThanks again for your answer. Regards.\n. @markotitel  Although i installed CudNN5 from https://developer.nvidia.com/rdp/cudnn-download and copied header files to include and shared so libraries to lib64 memory problem continued.\nI am trying to run demo of Fast R-CNN on Ubuntu 14.04. Lastly i run it through ./demo.py --net=zf and it gave an output of detected person, cat, dog, horse, bus.. etc. So, VGG16 database is big and i think everybody must start wt. a small sample. Currently my solution is this for Fast R-CNN\n. @markotitel Lastly I installed nVidia CUDA drivers and because of 2048 MB memory of my graphic card, the VGG16 does not run; it needs approximately 5 GB.\nSo, i run VGG16 with --cpu option to see @ demo phase. --cpu realy performs very slower whan you compare it wt. default option (GPU) but i can try some work so no need to investigate much more about this memory problem.\nThanks again for your answer. Regards.\n. ",
    "Vandertic": "I found a solution somewhere else and it worked for me.\nYou have to use CUDNN libraries, but not the most recent ones (presently 5.1) the caffe branch inside py-faster-rcnn/caffe-faster-rcnn, only supports cudnn 4.\nLibraries cudnn 4 were originally developed for cuda 7.0 but they work without problems on cuda 8.0 and ubuntu 16.04. With those installed, after compiling caffe (with USE_CUDNN:=1 in the Makefile.config), everything worked fine. The demo.py completed successfully on a 2Gb GeForce GTX 950.. I found a solution somewhere else and it worked for me.\nYou have to use CUDNN libraries, but not the most recent ones (presently 5.1) the caffe branch inside py-faster-rcnn/caffe-faster-rcnn, only supports cudnn 4.\nLibraries cudnn 4 were originally developed for cuda 7.0 but they work without problems on cuda 8.0 and ubuntu 16.04. With those installed, after compiling caffe (with USE_CUDNN:=1 in the Makefile.config), everything worked fine. The demo.py completed successfully on a 2Gb GeForce GTX 950.. ",
    "ssakhavi": "OK.\nThanks.\n. Maybe the main reason I'm asking this is that for memory heavy nets (Like\nGoogLeNet), CPU is sometimes the only option. (Due to limited GPU memory).\nI have tried the CPU mode in the demo and it isn't as fast as GPU.\nYou are right that. The whole point of FRCN is to be faster.\nThanks for the reply.\n. @SunShineMin @rbgirshick  @nascimentocrafael \nSome of the forks from this repo are actually trying (or succeeded) to do this. \nI refer you to:\nhttps://github.com/EdisonResearch/fast-rcnn @EdisonResearch\nhttps://github.com/raingo/fast-rcnn @raingo\n. Hi\n@fariba-abbasi I refer you to the following issue #9 .\n. I would refer you to the issue #11 , but I'm guessing that it isn't the answer to your question. \nActually, I have the same problem as above @rbgirshick. Maybe I can rephrase the question in a more general way:\nIn what format and where should the data be written in order to be used in the Python Layer?\n. Solved\n. OK.\nThanks.\n. Maybe the main reason I'm asking this is that for memory heavy nets (Like\nGoogLeNet), CPU is sometimes the only option. (Due to limited GPU memory).\nI have tried the CPU mode in the demo and it isn't as fast as GPU.\nYou are right that. The whole point of FRCN is to be faster.\nThanks for the reply.\n. @SunShineMin @rbgirshick  @nascimentocrafael \nSome of the forks from this repo are actually trying (or succeeded) to do this. \nI refer you to:\nhttps://github.com/EdisonResearch/fast-rcnn @EdisonResearch\nhttps://github.com/raingo/fast-rcnn @raingo\n. Hi\n@fariba-abbasi I refer you to the following issue #9 .\n. I would refer you to the issue #11 , but I'm guessing that it isn't the answer to your question. \nActually, I have the same problem as above @rbgirshick. Maybe I can rephrase the question in a more general way:\nIn what format and where should the data be written in order to be used in the Python Layer?\n. Solved\n. ",
    "XericZephyr": "I got first two files from baidu cloud, a famous Chinese cloud storage service. However, I cannot get the last devkit which sizes only 250 kb.  \nI don't know whether it's legal to redistribute the dataset. So I guess you can do some google search towards baidu cloud to get the dataset (though without the devkit). \n. I got first two files from baidu cloud, a famous Chinese cloud storage service. However, I cannot get the last devkit which sizes only 250 kb.  \nI don't know whether it's legal to redistribute the dataset. So I guess you can do some google search towards baidu cloud to get the dataset (though without the devkit). \n. ",
    "laotao": "https://web.archive.org/web/20140815141459/http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2012/index.html\n. https://web.archive.org/web/20140815141459/http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2012/index.html\n. ",
    "wangdelp": "CyrusChiu found one on oxford website, tested work for me. \nwget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\nwget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\nwget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar\n. CyrusChiu found one on oxford website, tested work for me. \nwget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\nwget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\nwget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar\n. ",
    "ltl315": "Hi, I encounter the same problem when I tried to make runtest. Hope someone give some tips to solve it.\n[----------] 1 test from ROIPoolingLayerTest/1, where TypeParam = caffe::DoubleCPU\n[ RUN      ] ROIPoolingLayerTest/1.TestGradient\nF0505 11:12:48.980974  8613 roi_pooling_layer.cpp:130] Not Implemented Yet\n* Check failure stack trace: \n    @     0x2b699bb6edaa  (unknown)\n    @     0x2b699bb6ece4  (unknown)\n    @     0x2b699bb6e6e6  (unknown)\n    @     0x2b699bb71687  (unknown)\n    @     0x2b699ca38930  caffe::ROIPoolingLayer<>::Backward_cpu()\n    @           0x4644e5  caffe::Layer<>::Backward()\n    @           0x469c31  caffe::GradientChecker<>::CheckGradientSingle()\n    @           0x46d14b  caffe::GradientChecker<>::CheckGradientExhaustive()\n    @           0x6520f0  caffe::ROIPoolingLayerTest_TestGradient_Test<>::TestBody()\n    @           0x6f6993  testing::internal::HandleExceptionsInMethodIfSupported<>()\n    @           0x6ed5d7  testing::Test::Run()\n    @           0x6ed67e  testing::TestInfo::Run()\n    @           0x6ed785  testing::TestCase::Run()\n    @           0x6f0ac8  testing::internal::UnitTestImpl::RunAllTests()\n    @           0x6f0d57  testing::UnitTest::Run()\n    @           0x442a1a  main\n    @     0x2b699d58cec5  (unknown)\n    @           0x447a29  (unknown)\n    @              (nil)  (unknown)\nmake: ** [runtest] Aborted (core dumped)\n. Hi, I encounter the same problem when I tried to make runtest. Hope someone give some tips to solve it.\n[----------] 1 test from ROIPoolingLayerTest/1, where TypeParam = caffe::DoubleCPU\n[ RUN      ] ROIPoolingLayerTest/1.TestGradient\nF0505 11:12:48.980974  8613 roi_pooling_layer.cpp:130] Not Implemented Yet\n* Check failure stack trace: \n    @     0x2b699bb6edaa  (unknown)\n    @     0x2b699bb6ece4  (unknown)\n    @     0x2b699bb6e6e6  (unknown)\n    @     0x2b699bb71687  (unknown)\n    @     0x2b699ca38930  caffe::ROIPoolingLayer<>::Backward_cpu()\n    @           0x4644e5  caffe::Layer<>::Backward()\n    @           0x469c31  caffe::GradientChecker<>::CheckGradientSingle()\n    @           0x46d14b  caffe::GradientChecker<>::CheckGradientExhaustive()\n    @           0x6520f0  caffe::ROIPoolingLayerTest_TestGradient_Test<>::TestBody()\n    @           0x6f6993  testing::internal::HandleExceptionsInMethodIfSupported<>()\n    @           0x6ed5d7  testing::Test::Run()\n    @           0x6ed67e  testing::TestInfo::Run()\n    @           0x6ed785  testing::TestCase::Run()\n    @           0x6f0ac8  testing::internal::UnitTestImpl::RunAllTests()\n    @           0x6f0d57  testing::UnitTest::Run()\n    @           0x442a1a  main\n    @     0x2b699d58cec5  (unknown)\n    @           0x447a29  (unknown)\n    @              (nil)  (unknown)\nmake: ** [runtest] Aborted (core dumped)\n. ",
    "Tingting-Chang": "Hi, @rbgirshick I do not know why I still get this error. \n[ RUN      ] ROIPoolingLayerTest/1.TestGradient\nF0605 18:07:55.833154  9241 roi_pooling_layer.cpp:130] Not Implemented Yet\n Check failure stack trace: \n    @     0x2b907352adbd  google::LogMessage::Fail()\n    @     0x2b907352cc5d  google::LogMessage::SendToLog()\n    @     0x2b907352a9ac  google::LogMessage::Flush()\n    @     0x2b907352d57e  google::LogMessageFatal::~LogMessageFatal()\n    @     0x2b9075558f20  caffe::ROIPoolingLayer<>::Backward_cpu()\n    @           0x4601d5  caffe::Layer<>::Backward()\n    @           0x463459  caffe::GradientChecker<>::CheckGradientSingle()\n    @           0x4a4816  caffe::ROIPoolingLayerTest_TestGradient_Test<>::TestBody()\n    @           0x6cb453  testing::internal::HandleExceptionsInMethodIfSupported<>()\n    @           0x6c20d7  testing::Test::Run()\n    @           0x6c217e  testing::TestInfo::Run()\n    @           0x6c2285  testing::TestCase::Run()\n    @           0x6c55c8  testing::internal::UnitTestImpl::RunAllTests()\n    @           0x6c5857  testing::UnitTest::Run()\n    @           0x4560f2  main\n    @     0x2b907600af45  (unknown)\n    @           0x45cdd9  (unknown)\nAborted (core dumped)\nmake: *** [runtest] Error 134. Hi, @rbgirshick I do not know why I still get this error. \n[ RUN      ] ROIPoolingLayerTest/1.TestGradient\nF0605 18:07:55.833154  9241 roi_pooling_layer.cpp:130] Not Implemented Yet\n Check failure stack trace: \n    @     0x2b907352adbd  google::LogMessage::Fail()\n    @     0x2b907352cc5d  google::LogMessage::SendToLog()\n    @     0x2b907352a9ac  google::LogMessage::Flush()\n    @     0x2b907352d57e  google::LogMessageFatal::~LogMessageFatal()\n    @     0x2b9075558f20  caffe::ROIPoolingLayer<>::Backward_cpu()\n    @           0x4601d5  caffe::Layer<>::Backward()\n    @           0x463459  caffe::GradientChecker<>::CheckGradientSingle()\n    @           0x4a4816  caffe::ROIPoolingLayerTest_TestGradient_Test<>::TestBody()\n    @           0x6cb453  testing::internal::HandleExceptionsInMethodIfSupported<>()\n    @           0x6c20d7  testing::Test::Run()\n    @           0x6c217e  testing::TestInfo::Run()\n    @           0x6c2285  testing::TestCase::Run()\n    @           0x6c55c8  testing::internal::UnitTestImpl::RunAllTests()\n    @           0x6c5857  testing::UnitTest::Run()\n    @           0x4560f2  main\n    @     0x2b907600af45  (unknown)\n    @           0x45cdd9  (unknown)\nAborted (core dumped)\nmake: *** [runtest] Error 134. ",
    "sunshineatnoon": "@ssakhavi Have you tried to train rcnn with cpu? \n. @rbgirshick hi~ It seems that the SmoothL1LossLayer has not been implemented on CPU, so even after changing code in train_net.py, I still cannot train with CPU.\n. @anuchandra Have you solved the assertion problem? I trained on image net and have the same problem, by deleting -1 operation, I still have the same problem.\n. @anuchandra Thanks for you help. I added those lines too, but I still get this assert error, did I miss anything else here?\n. @zeyuanxy Hi~ Thanks to your INRIA docs, I successfully train a model on INRIA. I don't have matlab on my computer, so I use dlib's selective search. I want to create a version that can be ran without matlab. So I need to wrtire an evaluation script in python. But I don't know how to evaluate prediction precision on INRIA. Could you pls give me some hints? Thanks.\n. @anuchandra Thx, getting rid of the -1 operation solves my problem.\n. @catsdogone Thx, I wrote some test code myself.\n. @zeyuanxy Hi~, How did you generate images for the background class? How many images are needed for this class?\n. @pradeepj247 \nthese links might help:\nhttps://github.com/zeyuanxy/fast-rcnn/tree/master/help/train\nhttp://sunshineatnoon.github.io/Train-fast-rcnn-model-on-imagenet-without-matlab/\n. @pradeepj247 You don't need to mention which class the image belongs to in the train.txt file, its only the names of the images, if you see classes included in images' names, that's pure coincidence. The annotation files should indicate which class a image belongs to.\nFor instance, a pascal voc annotation file looks like this:\n<object>\n        <name>dog</name>\n        <pose>Left</pose>\n        <truncated>1</truncated>\n        <difficult>0</difficult>\n        <bndbox>\n            <xmin>48</xmin>\n            <ymin>240</ymin>\n            <xmax>195</xmax>\n            <ymax>371</ymax>\n        </bndbox>\n    </object>\nThen the name tag tells you which class this object belongs to.\nYes, you need to mention the full path to the images in factory.py.\nAll these three files we modified tell fast rcnn how to find data and how to parse the annotation files.\n. 1) yes, only imagenet loop is needed, you can deleted others. As I mentioned, I referred here for how to modify the code, so I didn't delete what the author wrote initially.\n2) this error is caused by code in lib/datasets/init.py, so just comment it out:\nif _which(MATLAB) is None:\n    msg = (\"MATLAB command '{}' not found. \"\n           \"Please add '{}' to your PATH.\").format(MATLAB, MATLAB)\n    raise EnvironmentError(msg)'''\n. @WilsonWangTHU Hi~ I want to train fast rcnn on ImageNet(2 classes+background), then how should I generate images background class?\n. @mdering Did you finished getting a fork up? I want to try training on the imagenet 200 Detection Dataset too, where should I look first? Thanks.\n. @LarsHH Thanks for your reply. I acctually want to do some detection on imagenet 200 detection dataset, will it be possible to use the bounding box provided by rcnn?\n. @LarsHH Thanks, I am trying dlib's selective search now, I changed some code in demo.py to use dlib\u2019s selective search to generate proposal bounding boxes, so no _boxes.mat is needed for demo. It works fine for the demo. But I don't know if dlib's implementation can give equal accuracy compared to the original selective search. \nI actually only interested in few categories in the imagenet detection database.\n. hi, @paulinder . I am trying to use dlib's selective search too. I changed some code in demo.py to use dlib\u2019s selective search to generate proposal bounding boxes, so no _boxes.mat is needed for demo. You can check here for details.\n For the demo dlib's selective search works fine, but do you know if dlib's implementation gives equal accuracy compared to the original matlab selective search implementation for larger dataset such as imagenet detection?\nI haven't figured out how to use dlib's selective search to train on new dataset yet.\n. @paulinder I suppose you use find_candidate_object_locations to generate bounding boxes. Did you try to set the min_size parameter of this function to a greater value so that less bounding box will be generated?\nAlso, I am trying to use dlib's SS to train on new data set, did you try this?\n. @haimansx No, I have not solved the problem, I think the only way to solve the problem is by implementing the two functions above in c++\n. @tanjoreg No, I haven't solved this issue. Fast Rcnn can only be trained on GPU as far as I know. But it can be tested on CPU.\n. As I mentioned above, I think the way to solve this issue is to implement the two functions above in c++. \n. I don't have matlab either, but I use dlib's selective search and wrote the evaluation code myself, you can also find a python evaluation at https://github.com/rbgirshick/fast-rcnn/pull/33.\n. @ssakhavi Have you tried to train rcnn with cpu? \n. @rbgirshick hi~ It seems that the SmoothL1LossLayer has not been implemented on CPU, so even after changing code in train_net.py, I still cannot train with CPU.\n. @anuchandra Have you solved the assertion problem? I trained on image net and have the same problem, by deleting -1 operation, I still have the same problem.\n. @anuchandra Thanks for you help. I added those lines too, but I still get this assert error, did I miss anything else here?\n. @zeyuanxy Hi~ Thanks to your INRIA docs, I successfully train a model on INRIA. I don't have matlab on my computer, so I use dlib's selective search. I want to create a version that can be ran without matlab. So I need to wrtire an evaluation script in python. But I don't know how to evaluate prediction precision on INRIA. Could you pls give me some hints? Thanks.\n. @anuchandra Thx, getting rid of the -1 operation solves my problem.\n. @catsdogone Thx, I wrote some test code myself.\n. @zeyuanxy Hi~, How did you generate images for the background class? How many images are needed for this class?\n. @pradeepj247 \nthese links might help:\nhttps://github.com/zeyuanxy/fast-rcnn/tree/master/help/train\nhttp://sunshineatnoon.github.io/Train-fast-rcnn-model-on-imagenet-without-matlab/\n. @pradeepj247 You don't need to mention which class the image belongs to in the train.txt file, its only the names of the images, if you see classes included in images' names, that's pure coincidence. The annotation files should indicate which class a image belongs to.\nFor instance, a pascal voc annotation file looks like this:\n<object>\n        <name>dog</name>\n        <pose>Left</pose>\n        <truncated>1</truncated>\n        <difficult>0</difficult>\n        <bndbox>\n            <xmin>48</xmin>\n            <ymin>240</ymin>\n            <xmax>195</xmax>\n            <ymax>371</ymax>\n        </bndbox>\n    </object>\nThen the name tag tells you which class this object belongs to.\nYes, you need to mention the full path to the images in factory.py.\nAll these three files we modified tell fast rcnn how to find data and how to parse the annotation files.\n. 1) yes, only imagenet loop is needed, you can deleted others. As I mentioned, I referred here for how to modify the code, so I didn't delete what the author wrote initially.\n2) this error is caused by code in lib/datasets/init.py, so just comment it out:\nif _which(MATLAB) is None:\n    msg = (\"MATLAB command '{}' not found. \"\n           \"Please add '{}' to your PATH.\").format(MATLAB, MATLAB)\n    raise EnvironmentError(msg)'''\n. @WilsonWangTHU Hi~ I want to train fast rcnn on ImageNet(2 classes+background), then how should I generate images background class?\n. @mdering Did you finished getting a fork up? I want to try training on the imagenet 200 Detection Dataset too, where should I look first? Thanks.\n. @LarsHH Thanks for your reply. I acctually want to do some detection on imagenet 200 detection dataset, will it be possible to use the bounding box provided by rcnn?\n. @LarsHH Thanks, I am trying dlib's selective search now, I changed some code in demo.py to use dlib\u2019s selective search to generate proposal bounding boxes, so no _boxes.mat is needed for demo. It works fine for the demo. But I don't know if dlib's implementation can give equal accuracy compared to the original selective search. \nI actually only interested in few categories in the imagenet detection database.\n. hi, @paulinder . I am trying to use dlib's selective search too. I changed some code in demo.py to use dlib\u2019s selective search to generate proposal bounding boxes, so no _boxes.mat is needed for demo. You can check here for details.\n For the demo dlib's selective search works fine, but do you know if dlib's implementation gives equal accuracy compared to the original matlab selective search implementation for larger dataset such as imagenet detection?\nI haven't figured out how to use dlib's selective search to train on new dataset yet.\n. @paulinder I suppose you use find_candidate_object_locations to generate bounding boxes. Did you try to set the min_size parameter of this function to a greater value so that less bounding box will be generated?\nAlso, I am trying to use dlib's SS to train on new data set, did you try this?\n. @haimansx No, I have not solved the problem, I think the only way to solve the problem is by implementing the two functions above in c++\n. @tanjoreg No, I haven't solved this issue. Fast Rcnn can only be trained on GPU as far as I know. But it can be tested on CPU.\n. As I mentioned above, I think the way to solve this issue is to implement the two functions above in c++. \n. I don't have matlab either, but I use dlib's selective search and wrote the evaluation code myself, you can also find a python evaluation at https://github.com/rbgirshick/fast-rcnn/pull/33.\n. ",
    "ericromanenghi": "there is some layer that does not support CPU because they are only implemented for GPU\n. I have a similar doubt that you have.\nMy training dataset are cropped image, with almost no background. I don't know if this will break up the training.\nCould you train your net? Did you get good results?\n. i follow that tutorial and i could train a model, but the model don't display the accuracy while training. Also, there is no information about the test script.\nCould you help me please?\n. Could you find the way to add negative samples?\n. I had the same problem and compiling caffe with CUDDN solved the issue.\nIf you still need help, tell me.\n. there is some layer that does not support CPU because they are only implemented for GPU\n. I have a similar doubt that you have.\nMy training dataset are cropped image, with almost no background. I don't know if this will break up the training.\nCould you train your net? Did you get good results?\n. i follow that tutorial and i could train a model, but the model don't display the accuracy while training. Also, there is no information about the test script.\nCould you help me please?\n. Could you find the way to add negative samples?\n. I had the same problem and compiling caffe with CUDDN solved the issue.\nIf you still need help, tell me.\n. ",
    "zhouphd": "@sunshineatnoon\nI tried to implement the CPU mode, but this still can not pass the runtest. Hope it helps, :)\n`// ------------------------------------------------------------------\n// Fast R-CNN\n// Copyright (c) 2015 Microsoft\n// Licensed under The MIT License [see fast-rcnn/LICENSE for details]\n// Written by Ross Girshick\n// ------------------------------------------------------------------\ninclude \"caffe/fast_rcnn_layers.hpp\"\nnamespace caffe {\ntemplate \nvoid SmoothL1LossLayer::LayerSetUp(\n  const vector>& bottom, const vector>& top) {\n  SmoothL1LossParameter loss_param = this->layer_param_.smooth_l1_loss_param();\n  sigma2_ = loss_param.sigma() * loss_param.sigma();\n  has_weights_ = (bottom.size() >= 3);\n  if (has_weights_) {\n    CHECK_EQ(bottom.size(), 4) << \"If weights are used, must specify both \"\n      \"inside and outside weights\";\n  }\n}\ntemplate \nvoid SmoothL1LossLayer::Reshape(\n  const vector>& bottom, const vector>& top) {\n  LossLayer::Reshape(bottom, top);\n  CHECK_EQ(bottom[0]->channels(), bottom[1]->channels());\n  CHECK_EQ(bottom[0]->height(), bottom[1]->height());\n  CHECK_EQ(bottom[0]->width(), bottom[1]->width());\n  if (has_weights_) {\n    CHECK_EQ(bottom[0]->channels(), bottom[2]->channels());\n    CHECK_EQ(bottom[0]->height(), bottom[2]->height());\n    CHECK_EQ(bottom[0]->width(), bottom[2]->width());\n    CHECK_EQ(bottom[0]->channels(), bottom[3]->channels());\n    CHECK_EQ(bottom[0]->height(), bottom[3]->height());\n    CHECK_EQ(bottom[0]->width(), bottom[3]->width());\n  }\n  diff_.Reshape(bottom[0]->num(), bottom[0]->channels(),\n      bottom[0]->height(), bottom[0]->width());\n  errors_.Reshape(bottom[0]->num(), bottom[0]->channels(),\n      bottom[0]->height(), bottom[0]->width());\n  // vector of ones used to sum\n  ones_.Reshape(bottom[0]->num(), bottom[0]->channels(),\n      bottom[0]->height(), bottom[0]->width());\n  for (int i = 0; i < bottom[0]->count(); ++i) {\n    ones_.mutable_cpu_data()[i] = Dtype(1);\n  }\n}\ntemplate \nvoid SmoothL1LossLayer::Forward_cpu(const vector>& bottom,\n    const vector>& top) {\n  // NOT_IMPLEMENTED; \n  int count = bottom[0]->count();\n  //int num = bottom[0]->num();\n  const Dtype* in = diff_.cpu_data();\n  Dtype* out = errors_.mutable_cpu_data();\n  caffe_set(errors_.count(), Dtype(0), out);\ncaffe_sub(\n      count,\n      bottom[0]->cpu_data(),\n      bottom[1]->cpu_data(),\n      diff_.mutable_cpu_data());    // d := b0 - b1\n  if (has_weights_) {\n    // apply \"inside\" weights\n    caffe_mul(\n        count,\n        bottom[2]->cpu_data(),\n        diff_.cpu_data(),\n        diff_.mutable_cpu_data());  // d := w_in * (b0 - b1)\n  }\nfor (int index = 0;index < count; ++index){\n    Dtype val = in[index];\n    Dtype abs_val = abs(val);\n    if (abs_val < 1.0 / sigma2_) {\n      out[index] = 0.5 * val * val * sigma2_;\n    } else {\n      out[index] = abs_val - 0.5 / sigma2_;\n    }\n  }\nif (has_weights_) {\n    // apply \"outside\" weights\n    caffe_mul(\n        count,\n        bottom[3]->cpu_data(),\n        errors_.cpu_data(),\n        errors_.mutable_cpu_data());  // d := w_out * SmoothL1(w_in * (b0 - b1))\n  }\nDtype loss = caffe_cpu_dot(count, ones_.cpu_data(), errors_.cpu_data());\n  top[0]->mutable_cpu_data()[0] = loss / bottom[0]->num();\n}\ntemplate \nvoid SmoothL1LossLayer::Backward_cpu(const vector>& top,\n    const vector& propagate_down, const vector>& bottom) {\n  // NOT_IMPLEMENTED; \n  int count = diff_.count();\n  //int num = diff_.num();\n  const Dtype* in = diff_.cpu_data();\n  Dtype* out = errors_.mutable_cpu_data();\n  caffe_set(errors_.count(), Dtype(0), out);\nfor (int index = 0;index < count; ++index){\n    Dtype val = in[index];\n    Dtype abs_val = abs(val);\n    if (abs_val < 1.0 / sigma2_) {\n      out[index] = sigma2_ * val;\n    } else {\n      out[index] = (Dtype(0) < val) - (val < Dtype(0));\n    }\n  }\nfor (int i = 0; i < 2; ++i) {\n    if (propagate_down[i]) {\n      const Dtype sign = (i == 0) ? 1 : -1;\n      const Dtype alpha = sign * top[0]->cpu_diff()[0] / bottom[i]->num();\n      caffe_cpu_axpby(\n          count,                           // count\n          alpha,                           // alpha\n          diff_.cpu_data(),                // x\n          Dtype(0),                        // beta\n          bottom[i]->mutable_cpu_diff());  // y\n      if (has_weights_) {\n        // Scale by \"inside\" weight\n        caffe_mul(\n            count,\n            bottom[2]->cpu_data(),\n            bottom[i]->cpu_diff(),\n            bottom[i]->mutable_cpu_diff());\n        // Scale by \"outside\" weight\n        caffe_mul(\n            count,\n            bottom[3]->cpu_data(),\n            bottom[i]->cpu_diff(),\n            bottom[i]->mutable_cpu_diff());\n      }\n    }\n  }\n}\nifdef CPU_ONLY\nSTUB_GPU(SmoothL1LossLayer);\nendif\nINSTANTIATE_CLASS(SmoothL1LossLayer);\nREGISTER_LAYER_CLASS(SmoothL1Loss);\n}  // namespace caffe\n`\n. @sunshineatnoon\nI tried to implement the CPU mode, but this still can not pass the runtest. Hope it helps, :)\n`// ------------------------------------------------------------------\n// Fast R-CNN\n// Copyright (c) 2015 Microsoft\n// Licensed under The MIT License [see fast-rcnn/LICENSE for details]\n// Written by Ross Girshick\n// ------------------------------------------------------------------\ninclude \"caffe/fast_rcnn_layers.hpp\"\nnamespace caffe {\ntemplate \nvoid SmoothL1LossLayer::LayerSetUp(\n  const vector>& bottom, const vector>& top) {\n  SmoothL1LossParameter loss_param = this->layer_param_.smooth_l1_loss_param();\n  sigma2_ = loss_param.sigma() * loss_param.sigma();\n  has_weights_ = (bottom.size() >= 3);\n  if (has_weights_) {\n    CHECK_EQ(bottom.size(), 4) << \"If weights are used, must specify both \"\n      \"inside and outside weights\";\n  }\n}\ntemplate \nvoid SmoothL1LossLayer::Reshape(\n  const vector>& bottom, const vector>& top) {\n  LossLayer::Reshape(bottom, top);\n  CHECK_EQ(bottom[0]->channels(), bottom[1]->channels());\n  CHECK_EQ(bottom[0]->height(), bottom[1]->height());\n  CHECK_EQ(bottom[0]->width(), bottom[1]->width());\n  if (has_weights_) {\n    CHECK_EQ(bottom[0]->channels(), bottom[2]->channels());\n    CHECK_EQ(bottom[0]->height(), bottom[2]->height());\n    CHECK_EQ(bottom[0]->width(), bottom[2]->width());\n    CHECK_EQ(bottom[0]->channels(), bottom[3]->channels());\n    CHECK_EQ(bottom[0]->height(), bottom[3]->height());\n    CHECK_EQ(bottom[0]->width(), bottom[3]->width());\n  }\n  diff_.Reshape(bottom[0]->num(), bottom[0]->channels(),\n      bottom[0]->height(), bottom[0]->width());\n  errors_.Reshape(bottom[0]->num(), bottom[0]->channels(),\n      bottom[0]->height(), bottom[0]->width());\n  // vector of ones used to sum\n  ones_.Reshape(bottom[0]->num(), bottom[0]->channels(),\n      bottom[0]->height(), bottom[0]->width());\n  for (int i = 0; i < bottom[0]->count(); ++i) {\n    ones_.mutable_cpu_data()[i] = Dtype(1);\n  }\n}\ntemplate \nvoid SmoothL1LossLayer::Forward_cpu(const vector>& bottom,\n    const vector>& top) {\n  // NOT_IMPLEMENTED; \n  int count = bottom[0]->count();\n  //int num = bottom[0]->num();\n  const Dtype* in = diff_.cpu_data();\n  Dtype* out = errors_.mutable_cpu_data();\n  caffe_set(errors_.count(), Dtype(0), out);\ncaffe_sub(\n      count,\n      bottom[0]->cpu_data(),\n      bottom[1]->cpu_data(),\n      diff_.mutable_cpu_data());    // d := b0 - b1\n  if (has_weights_) {\n    // apply \"inside\" weights\n    caffe_mul(\n        count,\n        bottom[2]->cpu_data(),\n        diff_.cpu_data(),\n        diff_.mutable_cpu_data());  // d := w_in * (b0 - b1)\n  }\nfor (int index = 0;index < count; ++index){\n    Dtype val = in[index];\n    Dtype abs_val = abs(val);\n    if (abs_val < 1.0 / sigma2_) {\n      out[index] = 0.5 * val * val * sigma2_;\n    } else {\n      out[index] = abs_val - 0.5 / sigma2_;\n    }\n  }\nif (has_weights_) {\n    // apply \"outside\" weights\n    caffe_mul(\n        count,\n        bottom[3]->cpu_data(),\n        errors_.cpu_data(),\n        errors_.mutable_cpu_data());  // d := w_out * SmoothL1(w_in * (b0 - b1))\n  }\nDtype loss = caffe_cpu_dot(count, ones_.cpu_data(), errors_.cpu_data());\n  top[0]->mutable_cpu_data()[0] = loss / bottom[0]->num();\n}\ntemplate \nvoid SmoothL1LossLayer::Backward_cpu(const vector>& top,\n    const vector& propagate_down, const vector>& bottom) {\n  // NOT_IMPLEMENTED; \n  int count = diff_.count();\n  //int num = diff_.num();\n  const Dtype* in = diff_.cpu_data();\n  Dtype* out = errors_.mutable_cpu_data();\n  caffe_set(errors_.count(), Dtype(0), out);\nfor (int index = 0;index < count; ++index){\n    Dtype val = in[index];\n    Dtype abs_val = abs(val);\n    if (abs_val < 1.0 / sigma2_) {\n      out[index] = sigma2_ * val;\n    } else {\n      out[index] = (Dtype(0) < val) - (val < Dtype(0));\n    }\n  }\nfor (int i = 0; i < 2; ++i) {\n    if (propagate_down[i]) {\n      const Dtype sign = (i == 0) ? 1 : -1;\n      const Dtype alpha = sign * top[0]->cpu_diff()[0] / bottom[i]->num();\n      caffe_cpu_axpby(\n          count,                           // count\n          alpha,                           // alpha\n          diff_.cpu_data(),                // x\n          Dtype(0),                        // beta\n          bottom[i]->mutable_cpu_diff());  // y\n      if (has_weights_) {\n        // Scale by \"inside\" weight\n        caffe_mul(\n            count,\n            bottom[2]->cpu_data(),\n            bottom[i]->cpu_diff(),\n            bottom[i]->mutable_cpu_diff());\n        // Scale by \"outside\" weight\n        caffe_mul(\n            count,\n            bottom[3]->cpu_data(),\n            bottom[i]->cpu_diff(),\n            bottom[i]->mutable_cpu_diff());\n      }\n    }\n  }\n}\nifdef CPU_ONLY\nSTUB_GPU(SmoothL1LossLayer);\nendif\nINSTANTIATE_CLASS(SmoothL1LossLayer);\nREGISTER_LAYER_CLASS(SmoothL1Loss);\n}  // namespace caffe\n`\n. ",
    "zeyuanxy": "Cool, thanks :)\n. @SunShineMin @rbgirshick @nascimentocrafael  @ssakhavi  I have succeeded training it on INRIA Person, see https://github.com/EdisonResearch/fast-rcnn  for more details. Thanks!\n. How to Train Fast-RCNN on Another Dataset\nhttps://github.com/EdisonResearch/fast-rcnn/tree/master/help/train\n. @SunShineMin  Which dataset did you train Fast-RCNN on? I am so glad to meet a Tsinghua peer here.\n. @catsdogone  It seems that there is some problem with your selective search file.\n. Hi @revilokeb , I am sorry that I did not include negative examples because I do not know how to make it and that was why I made this issue. Maybe @rbgirshick  will help me and you.\n. Hi @rbgirshick  It is about 87.8, a very high number.\n. @sunshineatnoon  I am sorry that I did not remember that. Maybe I have not used any\nbackground images.\nOn Mon, Sep 14, 2015 at 3:19 AM, SunshineAtNoon notifications@github.com\nwrote:\n\n@zeyuanxy https://github.com/zeyuanxy Hi~, How did you generate images\nfor the background class? How many images are needed for this class?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rbgirshick/fast-rcnn/pull/21#issuecomment-139982672.\n\n\nBest Regards,\nZeyuan Shang\nDepartment of Computer Science & Technology\nTsinghua University\nMobile: +86 15635986068\n. @cuatristapr I am sorry that the models were stored on the server of the company I interned. You can follow the steps I have provided and it is easy to reproduce one.\n. Cool, thanks :)\n. @SunShineMin @rbgirshick @nascimentocrafael  @ssakhavi  I have succeeded training it on INRIA Person, see https://github.com/EdisonResearch/fast-rcnn  for more details. Thanks!\n. How to Train Fast-RCNN on Another Dataset\nhttps://github.com/EdisonResearch/fast-rcnn/tree/master/help/train\n. @SunShineMin  Which dataset did you train Fast-RCNN on? I am so glad to meet a Tsinghua peer here.\n. @catsdogone  It seems that there is some problem with your selective search file.\n. Hi @revilokeb , I am sorry that I did not include negative examples because I do not know how to make it and that was why I made this issue. Maybe @rbgirshick  will help me and you.\n. Hi @rbgirshick  It is about 87.8, a very high number.\n. @sunshineatnoon  I am sorry that I did not remember that. Maybe I have not used any\nbackground images.\nOn Mon, Sep 14, 2015 at 3:19 AM, SunshineAtNoon notifications@github.com\nwrote:\n\n@zeyuanxy https://github.com/zeyuanxy Hi~, How did you generate images\nfor the background class? How many images are needed for this class?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rbgirshick/fast-rcnn/pull/21#issuecomment-139982672.\n\n\nBest Regards,\nZeyuan Shang\nDepartment of Computer Science & Technology\nTsinghua University\nMobile: +86 15635986068\n. @cuatristapr I am sorry that the models were stored on the server of the company I interned. You can follow the steps I have provided and it is easy to reproduce one.\n. ",
    "SunShineMin": "Thank you very much for your reply and I have solved the problem with your help.But  when I increased the number of classes to 12 ,an AssertionError occured in imdb.append_flipped_images----assert (boxes[:, 2] >= boxes[:, 0]).all() .Is there something wrong with my proposals? But I have used these proposals in RCNN successfully. @rbgirshick \nPS: I found that one of the boxes[:,0]=65535 that results in the error ,but how does this issue come ?\n. @ssakhavi @zeyuanxy @rbgirshick Thank you very much for above help. I have trained Fast R-CNN on my own dataset, but the mAP is lower than R-CNN . I changed the learning rate and lterations, with little improvement . Which arguments can I modify else to improve mAP ?\n. @nascimentocrafael  I have sent you an email attached with VOC dataset , please have a check\n. Thank you very much for your reply and I have solved the problem with your help.But  when I increased the number of classes to 12 ,an AssertionError occured in imdb.append_flipped_images----assert (boxes[:, 2] >= boxes[:, 0]).all() .Is there something wrong with my proposals? But I have used these proposals in RCNN successfully. @rbgirshick \nPS: I found that one of the boxes[:,0]=65535 that results in the error ,but how does this issue come ?\n. @ssakhavi @zeyuanxy @rbgirshick Thank you very much for above help. I have trained Fast R-CNN on my own dataset, but the mAP is lower than R-CNN . I changed the learning rate and lterations, with little improvement . Which arguments can I modify else to improve mAP ?\n. @nascimentocrafael  I have sent you an email attached with VOC dataset , please have a check\n. ",
    "nascimentocrafael": "Hi SunShineMin,\nCould you please share the steps to train fast-rcnn ConvNet on another dataset? Also, I'm having problems downloading the PASCAL dataset, it seems that the links are broken. Did you have some problem downloading the database?. If you help me I'll be extremely thankful. If you prefer, you could e-mail me.\nThank you in advance.\n. Hi SunShineMin,\nCould you please share the steps to train fast-rcnn ConvNet on another dataset? Also, I'm having problems downloading the PASCAL dataset, it seems that the links are broken. Did you have some problem downloading the database?. If you help me I'll be extremely thankful. If you prefer, you could e-mail me.\nThank you in advance.\n. ",
    "github-anurag": "@rbgirshick @zeyuanxy @SunShineMin \nHow do we go about training for the negative/background class? After reading the paper, I understand that if the IoU between the bbox proposals and our labelled bbox is < 0.3 we consider those bboxes to be negative. Do we specify these bboxes as background class for each image while creating the training set imdb's? \nI imagine a caffe layer can do this .. does something like that exist already? thanks\n. @anuchandra \nThis assertion error is due to some problems with your annotation files. The annotations have the bounding box informations as column 0 - xmin, column 1- ymin , column 2- xmax, column 3 as ymax. The assertion fails as you have xmin > xmax for one or possibly more of your annotation files. Try to fix this.\n. @rbgirshick @zeyuanxy @SunShineMin \nHow do we go about training for the negative/background class? After reading the paper, I understand that if the IoU between the bbox proposals and our labelled bbox is < 0.3 we consider those bboxes to be negative. Do we specify these bboxes as background class for each image while creating the training set imdb's? \nI imagine a caffe layer can do this .. does something like that exist already? thanks\n. @anuchandra \nThis assertion error is due to some problems with your annotation files. The annotations have the bounding box informations as column 0 - xmin, column 1- ymin , column 2- xmax, column 3 as ymax. The assertion fails as you have xmin > xmax for one or possibly more of your annotation files. Try to fix this.\n. ",
    "hengck23": "@ github-anurag \nIf you look at \" fast-rcnn/lib/datasets/pascal_voc.py\",\n              def selective_search_roidb(self):\n                   if int(self._year) == 2007 or self._image_set != 'test':\nline 124:            gt_roidb = self.gt_roidb()\nline 125:            ss_roidb = self._load_selective_search_roidb(gt_roidb)\nline 126:            roidb = datasets.imdb.merge_roidbs(gt_roidb, ss_roidb)\nIt means that during training, the ground truth annotation is first read in line 124. These are positive samples. Then selective search boxes are loaded, those that do not overlap with the ground truth are marked as backgrounds (aka negative samples), as in line 125. Finally, both positive and negative samples are combined into roidb and used for training in line 126.\nnote that \"selective_search_roidb()\" is always called when you use imdb.roidb(), see\n\" fast-rcnn/lib/datasets/imdb.py:\" \n    @property\n    def roidb(self):\nimdb is your train database and roidb is the roi of your train database.\nin short, you need to supply ground truth annotation and a set of box list (e.g. from selective search) for training. Hope this explanation helps.\n. @ github-anurag \nIf you look at \" fast-rcnn/lib/datasets/pascal_voc.py\",\n              def selective_search_roidb(self):\n                   if int(self._year) == 2007 or self._image_set != 'test':\nline 124:            gt_roidb = self.gt_roidb()\nline 125:            ss_roidb = self._load_selective_search_roidb(gt_roidb)\nline 126:            roidb = datasets.imdb.merge_roidbs(gt_roidb, ss_roidb)\nIt means that during training, the ground truth annotation is first read in line 124. These are positive samples. Then selective search boxes are loaded, those that do not overlap with the ground truth are marked as backgrounds (aka negative samples), as in line 125. Finally, both positive and negative samples are combined into roidb and used for training in line 126.\nnote that \"selective_search_roidb()\" is always called when you use imdb.roidb(), see\n\" fast-rcnn/lib/datasets/imdb.py:\" \n    @property\n    def roidb(self):\nimdb is your train database and roidb is the roi of your train database.\nin short, you need to supply ground truth annotation and a set of box list (e.g. from selective search) for training. Hope this explanation helps.\n. ",
    "anuchandra": "@zeyuanxy @SunShineMin\nThanks for the instructions on how to train on your own dataset. I tried to follow them using one imagenet class and a background. But I'm getting an error which is exactly the same as SunShineMin did.\nAs background, I computed the selective search proposals. Then ran the train_net.py script. Any suggestions? SunShineMin, how did you fix this? Anyone else have any suggestions?\nTraceback (most recent call last):\n  File \"./tools/train_net.py\", line 80, in \n    roidb = get_training_roidb(imdb)\n  File \"/home/ubuntu/fast-rcnn/tools/../lib/fast_rcnn/train.py\", line 107, in get_training_roidb\n    imdb.append_flipped_images()\n  File \"/home/ubuntu/fast-rcnn/tools/../lib/datasets/imdb.py\", line 104, in append_flipped_images\n    assert (boxes[:, 2] >= boxes[:, 0]).all()\nAssertionError\nubuntu@ip-172-31-8-77:~/fast-rcnn$ \n. @YihangLou \nYou're absolutely right. I added the checks for negative x1 and y1 in _load_pascal_annotation and this fixed a couple of issues. Firstly, the assert (boxes[:, 2] >= boxes[:, 0]).all() were no longer called. The second big thing was I was getting nans appearing in my loss_bbox during training. These have gone as well. I carefully compared the annotations which were being made negative x1s with those caught by the assert. It seems the assert wasn't catching all of them. Thanks!\n. @sunshineatnoon I kept the -1 operations. Instead I added checks in _load_pascal_annotation() \nIn_load_pascal_annotation(), after these lines, \n```\nMake pixel indexes 0-based\nx1 = float(get_data_from_tag(obj, 'xmin')) - 1\ny1 = float(get_data_from_tag(obj, 'ymin')) - 1\nx2 = float(get_data_from_tag(obj, 'xmax')) - 1\ny2 = float(get_data_from_tag(obj, 'ymax')) - 1\n```\nI added:\nif x1 < 0:\n        x1 = 0\nif y1 < 0:\n        y1 = 0\nThat took care of annotations where x1 or y1 were 0.\n. @sunshineatnoon Similarly in addition, you could try checking the -1 operation in _load_selective_search_roidb. See @YihangLou suggestion above. This didn't prove an issue for my dataset but it could be with yours.\nbox_list.append(raw_data[i][:, (1, 0, 3, 2)] - 1)\n. @IchibanKanobee The easiest way to figure out how fastrcnn works is to take the working PASCAL VOC 2007 example and strip it down to one image in one class. You can use the Edison training guide - it'll help you do that.\n. The instructions for the working example are on the home page.\nhttps://github.com/rbgirshick/fast-rcnn#beyond-the-demo-installation-for-training-and-testing-models\nAll the info you need is on these discussion forums. \n. @siddharthm83 This is useful to know for the future.\n. @attiliotnt I can give you a generic suggestion. When I run into a tough problem that no one else can seem to help with, I break it down. The way I got fast-rcnn building a model was to take the working example - see repo frontpage ie VOC2007 datasets. Stripped them down from 20 classes to 1 class. Stripped the one class from tons of images to 1 image. My first fast-rcnn model was 1 class with 1 image. Subsequently, I added images in small batches. This approach meant that when I had an error like you do, I could pinpoint down to a single image.\n. @attiliotnt If you add imagenet images & annotations in progressively smaller batches, you should be able to identify which image or annotation file in the imagenet dataset is causing the assert. You can write scripts which speed it up. This is literally what I did at one point. \n. @abhisheksgumadi Most likely an issue with the annotation files. You should be able to print out which annotation file is causing this exception. I forget which bit of code causes this error but add a print somewhere which prints out the name of the annotation file. This should help you pinpoint the annotation file in question. The other way is to try and reduce the number of annotations to a small working set and progressively add annotations until you find the one causing the problem.\n. A few of us have faced this issue. You can find that discussion here.\nhttps://github.com/rbgirshick/fast-rcnn/issues/11\n. @zeyuanxy @SunShineMin\nThanks for the instructions on how to train on your own dataset. I tried to follow them using one imagenet class and a background. But I'm getting an error which is exactly the same as SunShineMin did.\nAs background, I computed the selective search proposals. Then ran the train_net.py script. Any suggestions? SunShineMin, how did you fix this? Anyone else have any suggestions?\nTraceback (most recent call last):\n  File \"./tools/train_net.py\", line 80, in \n    roidb = get_training_roidb(imdb)\n  File \"/home/ubuntu/fast-rcnn/tools/../lib/fast_rcnn/train.py\", line 107, in get_training_roidb\n    imdb.append_flipped_images()\n  File \"/home/ubuntu/fast-rcnn/tools/../lib/datasets/imdb.py\", line 104, in append_flipped_images\n    assert (boxes[:, 2] >= boxes[:, 0]).all()\nAssertionError\nubuntu@ip-172-31-8-77:~/fast-rcnn$ \n. @YihangLou \nYou're absolutely right. I added the checks for negative x1 and y1 in _load_pascal_annotation and this fixed a couple of issues. Firstly, the assert (boxes[:, 2] >= boxes[:, 0]).all() were no longer called. The second big thing was I was getting nans appearing in my loss_bbox during training. These have gone as well. I carefully compared the annotations which were being made negative x1s with those caught by the assert. It seems the assert wasn't catching all of them. Thanks!\n. @sunshineatnoon I kept the -1 operations. Instead I added checks in _load_pascal_annotation() \nIn_load_pascal_annotation(), after these lines, \n```\nMake pixel indexes 0-based\nx1 = float(get_data_from_tag(obj, 'xmin')) - 1\ny1 = float(get_data_from_tag(obj, 'ymin')) - 1\nx2 = float(get_data_from_tag(obj, 'xmax')) - 1\ny2 = float(get_data_from_tag(obj, 'ymax')) - 1\n```\nI added:\nif x1 < 0:\n        x1 = 0\nif y1 < 0:\n        y1 = 0\nThat took care of annotations where x1 or y1 were 0.\n. @sunshineatnoon Similarly in addition, you could try checking the -1 operation in _load_selective_search_roidb. See @YihangLou suggestion above. This didn't prove an issue for my dataset but it could be with yours.\nbox_list.append(raw_data[i][:, (1, 0, 3, 2)] - 1)\n. @IchibanKanobee The easiest way to figure out how fastrcnn works is to take the working PASCAL VOC 2007 example and strip it down to one image in one class. You can use the Edison training guide - it'll help you do that.\n. The instructions for the working example are on the home page.\nhttps://github.com/rbgirshick/fast-rcnn#beyond-the-demo-installation-for-training-and-testing-models\nAll the info you need is on these discussion forums. \n. @siddharthm83 This is useful to know for the future.\n. @attiliotnt I can give you a generic suggestion. When I run into a tough problem that no one else can seem to help with, I break it down. The way I got fast-rcnn building a model was to take the working example - see repo frontpage ie VOC2007 datasets. Stripped them down from 20 classes to 1 class. Stripped the one class from tons of images to 1 image. My first fast-rcnn model was 1 class with 1 image. Subsequently, I added images in small batches. This approach meant that when I had an error like you do, I could pinpoint down to a single image.\n. @attiliotnt If you add imagenet images & annotations in progressively smaller batches, you should be able to identify which image or annotation file in the imagenet dataset is causing the assert. You can write scripts which speed it up. This is literally what I did at one point. \n. @abhisheksgumadi Most likely an issue with the annotation files. You should be able to print out which annotation file is causing this exception. I forget which bit of code causes this error but add a print somewhere which prints out the name of the annotation file. This should help you pinpoint the annotation file in question. The other way is to try and reduce the number of annotations to a small working set and progressively add annotations until you find the one causing the problem.\n. A few of us have faced this issue. You can find that discussion here.\nhttps://github.com/rbgirshick/fast-rcnn/issues/11\n. ",
    "YihangLou": "@anuchandra \nI know what caused this error. Because I have met this error too and the solved. The reason is probably due to the \"-1\" operation during reading the anontation file or your selectivesearchBox.mat file. This assert expression is to check whether the horizontal flipped is success or not. According to this flipped operation, x2 will be larger than x1. You should notice your pixel coordinates are whether starting from 0 or 1, if you coordiantes is starting from 0, do not perform \"-1\" operation which you can see in the pascal_voc.py, or the x coordinates would be negative number that will lead to this error. And this operation you can find in function _load_pascal_annotation() and _load_selective_search_roidb. :)\n. @anuchandra \nI know what caused this error. Because I have met this error too and the solved. The reason is probably due to the \"-1\" operation during reading the anontation file or your selectivesearchBox.mat file. This assert expression is to check whether the horizontal flipped is success or not. According to this flipped operation, x2 will be larger than x1. You should notice your pixel coordinates are whether starting from 0 or 1, if you coordiantes is starting from 0, do not perform \"-1\" operation which you can see in the pascal_voc.py, or the x coordinates would be negative number that will lead to this error. And this operation you can find in function _load_pascal_annotation() and _load_selective_search_roidb. :)\n. ",
    "IchibanKanobee": "I have a couple of questions about training the fast r-cnn on another dataset, as it is described in\nhttps://github.com/EdisonResearch/fast-rcnn/tree/master/help/train\nDo I put negative and positive images under the same \"INRIA/data/Images\" folder, or separate them into sub-folders, like \"INRIA/data/Images/positive\" and \"INRIA/data/Images/negative\". The more general question is, if I have several categories, do I put all of their images into the same \"INRIA/data/Images\" folder, or into separate  \"INRIA/data/Images/Category1\"... \"INRIA/data/Images/CategoryN\" sub-folders?\nThe second question is, do I need annotations for negative samples as well, or only for positive ones?\nThanks a lot for your help!\n. I am still not able to train on my own data set, so I might be wrong, but my understanding is that it is not necessary to separate the images into sub folders. At least the selective_search.py is written in a way that all images are under the same folder for generating rois.\nI also understand that annotations are needed for negative samples as well, but I am not sure if the region of interest in this case is the whole image with the center pointing to the center of the image, or it is [0, 0, 0, 0] with the center pointing to [0,0]. \n. @sid027 I am following the training instructions in readme.md in help folder and using the VGG_CNN_M_1024 model. \nIf I don't have annotations for background files, I am getting an error:\nFile \"./tools/train_net.py\", line 80, in \n    roidb = get_training_roidb(imdb)\n  File \"/home/GitHub/fast-rcnn/tools/../lib/fast_rcnn/train.py\", line 111, in get_training_roidb\n    rdl_roidb.prepare_roidb(imdb)\n  File \"/home/GitHub/fast-rcnn/tools/../lib/roi_data_layer/roidb.py\", line 23, in prepare_roidb\n    roidb[i]['image'] = imdb.image_path_at(i)\nIndexError: list index out of range\nThat's why I thought the annotations for background files are required. But I didn't get the training working, so I might be wrong.\n. @zeyuanxy Do you include background files in the training set? If they are included in the training set, do you include annotations for the background files? And, if you do include annotations for the background files, what are the bounding boxes - the whole image, or [0,0,0, 0]?\nThanks!\n. @anuchandra Thank you for the tip! Can you please point me to the working PASCAL VOC 2007 sample?\n. @attiliotnt Make sure to delete the old cache folder before starting the new training\n. @attiliotnt Yes, it is. If you want recognize facial features, for example, you specify eyes, mouth, nose, etc. on the same image.\n. I have a couple of questions about training the fast r-cnn on another dataset, as it is described in\nhttps://github.com/EdisonResearch/fast-rcnn/tree/master/help/train\nDo I put negative and positive images under the same \"INRIA/data/Images\" folder, or separate them into sub-folders, like \"INRIA/data/Images/positive\" and \"INRIA/data/Images/negative\". The more general question is, if I have several categories, do I put all of their images into the same \"INRIA/data/Images\" folder, or into separate  \"INRIA/data/Images/Category1\"... \"INRIA/data/Images/CategoryN\" sub-folders?\nThe second question is, do I need annotations for negative samples as well, or only for positive ones?\nThanks a lot for your help!\n. I am still not able to train on my own data set, so I might be wrong, but my understanding is that it is not necessary to separate the images into sub folders. At least the selective_search.py is written in a way that all images are under the same folder for generating rois.\nI also understand that annotations are needed for negative samples as well, but I am not sure if the region of interest in this case is the whole image with the center pointing to the center of the image, or it is [0, 0, 0, 0] with the center pointing to [0,0]. \n. @sid027 I am following the training instructions in readme.md in help folder and using the VGG_CNN_M_1024 model. \nIf I don't have annotations for background files, I am getting an error:\nFile \"./tools/train_net.py\", line 80, in \n    roidb = get_training_roidb(imdb)\n  File \"/home/GitHub/fast-rcnn/tools/../lib/fast_rcnn/train.py\", line 111, in get_training_roidb\n    rdl_roidb.prepare_roidb(imdb)\n  File \"/home/GitHub/fast-rcnn/tools/../lib/roi_data_layer/roidb.py\", line 23, in prepare_roidb\n    roidb[i]['image'] = imdb.image_path_at(i)\nIndexError: list index out of range\nThat's why I thought the annotations for background files are required. But I didn't get the training working, so I might be wrong.\n. @zeyuanxy Do you include background files in the training set? If they are included in the training set, do you include annotations for the background files? And, if you do include annotations for the background files, what are the bounding boxes - the whole image, or [0,0,0, 0]?\nThanks!\n. @anuchandra Thank you for the tip! Can you please point me to the working PASCAL VOC 2007 sample?\n. @attiliotnt Make sure to delete the old cache folder before starting the new training\n. @attiliotnt Yes, it is. If you want recognize facial features, for example, you specify eyes, mouth, nose, etc. on the same image.\n. ",
    "catsdogone": "@zeyuanxy \nI try to train on my database as it is describe in https://github.com/EdisonResearch/fast-rcnn/tree/master/help/train, while there is something wrong:\n... \n File \"/usr/local/fast-rcnn/tools/../lib/datasets/imdb.py\", line 167, in create_roidb_from_box_list\n    argmaxes = gt_overlaps.argmax(axis=1)\nValueError: attempt to get argmax of an empty sequence\nThanks a lot for your help!\n. @sunshineatnoon \nMaybe this is what you need\uff1a\nhttps://github.com/rbgirshick/fast-rcnn/pull/33/files\n. layer {\nname: \"conv1/7x7_s2\"\ntype: \"Convolution\"\nbottom: \"data\"\ntop: \"conv1/7x7_s2\"\nparam {\nlr_mult: 1\ndecay_mult: 1\n}\nIn /fast-rcnn/models/VGG16(and CaffeNet) /train.prototxt, the parameter lr_mult and decay_mult is 0, so why you set them 1. I still can not really understand the definition of the prototxt. Do you have any references? Thank you very much.\n. @jond55 I think the input image doesn't need to be resized, the roi_pooling layer is where amazing happens. The input image can be any ratio but I think the minimum size should larger than 224. \n. @marutiagarwal \n  roi_pooling_param {\n    pooled_w: 7\n    pooled_h: 7\n    spatial_scale: 0.0625 # 1/16\n  }\nIs your spatial_scale compatible with the original model?\n. @marutiagarwal I think this value is determined by the stride of layers.\n. @marutiagarwal I think the parameter spatial_scale: 0.0625 # 1/16 is very important, since it is the scaling parameter for coordinate projection. For VGG16, if the input images is (224,224), the conv5_3 is (14, 14), so the spatial_scale is 0.0625 #1/16.   (224/14=16)\n I think your layer {name: \"roi_pool3\" ...} 's roi_pooling_param  should be:\n   roi_pooling_param {\n    pooled_w: 7\n    pooled_h: 7\n    spatial_scale: 0.03125 # 1/32\n  }\n since the dim of inception_5b/output is (7,7) when the input image is (224,224)\nBut, I get a worse accuracy use this setting. May be I was wrong.\n. @201power Could you please share your parameters? \nWill highly appreciate that. Thanks.\n. @rbgirshick @marutiagarwal @XiongweiWu Ignore loss1 and loss2 layers, I just add rpn-layers after layer \u300aname: \"inception_5b/output\"\u300b and set \"'feat_stride': 32\" of layer \u300aname: 'rpn-data_faster3'>\u300band layer \u300aname: 'proposal_faster3'\u300b,about the pooling layer, the parameters are(since there are five 2*2 max-pooling layers):\nlayer {\n  name: \"roi_pool5_faster3\"\n  type: \"ROIPooling\"\n  bottom: \"inception_5b/output\"\n  bottom: \"rois_faster3\"\n  top: \"pool5_faster3\"\n  roi_pooling_param {\n    pooled_w: 7\n    pooled_h: 7\n    spatial_scale: 0.03125 # 1/32\n  }\n}\nBut my precision was still lower than 10%. Is there something wrong? \nThank you for your help.\nThis is my prototxt:\ntest.txt\ntrain.txt\n. have you successfully trained a Googlenet/Inception network definition that works in conjunction with  Fast R-CNN/faster RCNN? @kamadforge @marutiagarwal @jainanshul \n. @marutiagarwal Could you share your parameter setting or experience in your training \uff1fI have try many times with 1 or 3 fc classifiers while all failed, the ap value always just about 2%. Except the \"spatial-scale\" my net definition is similar with yours.\nThank you very much.\n. hi.\nYou can use 'selective search' or other methods for generating region proposals.\n. you are right. Sorry for my carelessness.\n. @zeyuanxy \nI try to train on my database as it is describe in https://github.com/EdisonResearch/fast-rcnn/tree/master/help/train, while there is something wrong:\n... \n File \"/usr/local/fast-rcnn/tools/../lib/datasets/imdb.py\", line 167, in create_roidb_from_box_list\n    argmaxes = gt_overlaps.argmax(axis=1)\nValueError: attempt to get argmax of an empty sequence\nThanks a lot for your help!\n. @sunshineatnoon \nMaybe this is what you need\uff1a\nhttps://github.com/rbgirshick/fast-rcnn/pull/33/files\n. layer {\nname: \"conv1/7x7_s2\"\ntype: \"Convolution\"\nbottom: \"data\"\ntop: \"conv1/7x7_s2\"\nparam {\nlr_mult: 1\ndecay_mult: 1\n}\nIn /fast-rcnn/models/VGG16(and CaffeNet) /train.prototxt, the parameter lr_mult and decay_mult is 0, so why you set them 1. I still can not really understand the definition of the prototxt. Do you have any references? Thank you very much.\n. @jond55 I think the input image doesn't need to be resized, the roi_pooling layer is where amazing happens. The input image can be any ratio but I think the minimum size should larger than 224. \n. @marutiagarwal \n  roi_pooling_param {\n    pooled_w: 7\n    pooled_h: 7\n    spatial_scale: 0.0625 # 1/16\n  }\nIs your spatial_scale compatible with the original model?\n. @marutiagarwal I think this value is determined by the stride of layers.\n. @marutiagarwal I think the parameter spatial_scale: 0.0625 # 1/16 is very important, since it is the scaling parameter for coordinate projection. For VGG16, if the input images is (224,224), the conv5_3 is (14, 14), so the spatial_scale is 0.0625 #1/16.   (224/14=16)\n I think your layer {name: \"roi_pool3\" ...} 's roi_pooling_param  should be:\n   roi_pooling_param {\n    pooled_w: 7\n    pooled_h: 7\n    spatial_scale: 0.03125 # 1/32\n  }\n since the dim of inception_5b/output is (7,7) when the input image is (224,224)\nBut, I get a worse accuracy use this setting. May be I was wrong.\n. @201power Could you please share your parameters? \nWill highly appreciate that. Thanks.\n. @rbgirshick @marutiagarwal @XiongweiWu Ignore loss1 and loss2 layers, I just add rpn-layers after layer \u300aname: \"inception_5b/output\"\u300b and set \"'feat_stride': 32\" of layer \u300aname: 'rpn-data_faster3'>\u300band layer \u300aname: 'proposal_faster3'\u300b,about the pooling layer, the parameters are(since there are five 2*2 max-pooling layers):\nlayer {\n  name: \"roi_pool5_faster3\"\n  type: \"ROIPooling\"\n  bottom: \"inception_5b/output\"\n  bottom: \"rois_faster3\"\n  top: \"pool5_faster3\"\n  roi_pooling_param {\n    pooled_w: 7\n    pooled_h: 7\n    spatial_scale: 0.03125 # 1/32\n  }\n}\nBut my precision was still lower than 10%. Is there something wrong? \nThank you for your help.\nThis is my prototxt:\ntest.txt\ntrain.txt\n. have you successfully trained a Googlenet/Inception network definition that works in conjunction with  Fast R-CNN/faster RCNN? @kamadforge @marutiagarwal @jainanshul \n. @marutiagarwal Could you share your parameter setting or experience in your training \uff1fI have try many times with 1 or 3 fc classifiers while all failed, the ap value always just about 2%. Except the \"spatial-scale\" my net definition is similar with yours.\nThank you very much.\n. hi.\nYou can use 'selective search' or other methods for generating region proposals.\n. you are right. Sorry for my carelessness.\n. ",
    "yawadugyamfi": "When passing argument for training the INRIA person dataset what does inria_train represent in the code below. Is it the train.mat file generated from selective search? or the folder containing images of your training data?\n./tools/train_net.py --gpu 0 --solver models/VGG_CNN_M_1024/solver.prototxt \\\n    --weights data/imagenet_models/VGG_CNN_M_1024.v2.caffemodel --imdb inria_train\n. When passing argument for training the INRIA person dataset what does inria_train represent in the code below. Is it the train.mat file generated from selective search? or the folder containing images of your training data?\n./tools/train_net.py --gpu 0 --solver models/VGG_CNN_M_1024/solver.prototxt \\\n    --weights data/imagenet_models/VGG_CNN_M_1024.v2.caffemodel --imdb inria_train\n. ",
    "zsc-tju": "@zeyuanxy @rbgirshick \nI have a question,now that we set the scale=600 in the config.py, why in the roi_data_layer\\layer.py,still to reshape the top[0] to 100x100,and in the test phase the iunput size is 227x227,please enlighten me,thank you very much!\n. @zeyuanxy @rbgirshick \nI have a question,now that we set the scale=600 in the config.py, why in the roi_data_layer\\layer.py,still to reshape the top[0] to 100x100,and in the test phase the iunput size is 227x227,please enlighten me,thank you very much!\n. ",
    "siddharthm83": "@anuchandra @SunShineMin @sunshineatnoon \nI was working on the imagenet dataset and I got the assertion error  in imdb.append_flipped_images----assert (boxes[:, 2] >= boxes[:, 0]).all() \nFrom: http://image-net.org/download-bboxes\nRemark: In the bounding box annotations, there are two fields(<width> and <height>) indicating the size of the image. The location and size of a bounding box in the annotation file are relative to this size. However, this size may not be identical to the real image size in the downloaded package. (The reason is that the size in the annotation file is the displayed size in which the image was shown to an annotator). Therefore to locate the actual pixels on the original image, you might need to rescale the bounding boxes accordingly.\nIt appears that one would need to normalize values in xml to the true size of the image. \n. @anuchandra @SunShineMin @sunshineatnoon \nI was working on the imagenet dataset and I got the assertion error  in imdb.append_flipped_images----assert (boxes[:, 2] >= boxes[:, 0]).all() \nFrom: http://image-net.org/download-bboxes\nRemark: In the bounding box annotations, there are two fields(<width> and <height>) indicating the size of the image. The location and size of a bounding box in the annotation file are relative to this size. However, this size may not be identical to the real image size in the downloaded package. (The reason is that the size in the annotation file is the displayed size in which the image was shown to an annotator). Therefore to locate the actual pixels on the original image, you might need to rescale the bounding boxes accordingly.\nIt appears that one would need to normalize values in xml to the true size of the image. \n. ",
    "aragon111": "@IchibanKanobee  how did you fixed the \"IndexError: list index out of range\"?\nI'm stucked there since two days trying to train my own dataset.\n. Thank you for the answer @IchibanKanobee.\nMy problem was that the annotation files I got from Matlab didn't have the fields folder and name so I edit by myself.\n. Hello, I would like to ask which script you chose to run for detection? I tried to use my webcam but the results are not so good.\n. @MinaRe  very interesting. Do you use images or video for test?\n. @MinaRe  No, unfortunately. I worked just with detection.\n. I would like to ask an advice about the model to use during the training. I need to detect some object underwater and I used CaffeNet.v2.caffemodel. The result I obtained are really bad until now. Any advice?\n. I have a little doubt. During the labeling of the training set, is it possible to label more times items (ROIs) in the same image (of the same class)?\n. @SunShineMin  @anuchandra @siddharthm83   I'm having the assertion error too.\nI inserted the check after _load_annotation() but it's the same.\nassert all(max_classes[nonzero_inds] != 0)\nIf I remove the -1 in box_list.append(raw_data[i][:, (1, 0, 3, 2)] - 1) I still get the error but without the above part.\nShould I normalize the values in my xml files? And in case how?\n. @IchibanKanobee  thank you for the explanation!\n. Could anybody give me a suggestion about how to get rid of AssertionError?\n. Thanks @anuchandra for the suggestion. Actually I was trying to improve my previous dataset adding more pictures (I only use two classes including the background). The error I got happened using pictures from Imagenet database and I temporarily solve it just don't using them.\n. @deeha  You can easily solve it leaving temporarily a copy of the train.txt in the Images folder.\n. Is there already any script for testing fast-rcnn on videos?\n. Hello,\nwhen I try to train, I get this error\nFile \"/home/attilio/fast-rcnn/tools/../lib/datasets/amph.py\", line 190, in get_data_from_tag\n    return node.getElementsByTagName(tag)[0].childNodes[0].data\nIndexError: list index out of range\nWould you help to understand how to fix it?\nI don't know if it depends on the Annotation file I have or some editing error of python files (I use just one class)\n. I fixed it adding to the .xml files (which I got from Matlab) the fields name and folder. \n. Can you tell me why running detection, the frame detect just one item at a time?\n. @nw362  exactly :)\n. In order to detect more ROIs in the same frame there is anything I could do? \n. @nw362 I checked and it seems to me that it does not send only one ROI. \n. I have a little doubt. During the labeling of the training set, is it possible to label more items (ROIs) in the same image?\n. @nw362 Do you mean that is not a problem if there are more bounding boxes for the same .xml file?\n. @nw362  I'm not sure about what you mean with many data sets. I have only two classes (background and amphora) and in some image from the data set there are many amphoras.\n. @IchibanKanobee  how did you fixed the \"IndexError: list index out of range\"?\nI'm stucked there since two days trying to train my own dataset.\n. Thank you for the answer @IchibanKanobee.\nMy problem was that the annotation files I got from Matlab didn't have the fields folder and name so I edit by myself.\n. Hello, I would like to ask which script you chose to run for detection? I tried to use my webcam but the results are not so good.\n. @MinaRe  very interesting. Do you use images or video for test?\n. @MinaRe  No, unfortunately. I worked just with detection.\n. I would like to ask an advice about the model to use during the training. I need to detect some object underwater and I used CaffeNet.v2.caffemodel. The result I obtained are really bad until now. Any advice?\n. I have a little doubt. During the labeling of the training set, is it possible to label more times items (ROIs) in the same image (of the same class)?\n. @SunShineMin  @anuchandra @siddharthm83   I'm having the assertion error too.\nI inserted the check after _load_annotation() but it's the same.\nassert all(max_classes[nonzero_inds] != 0)\nIf I remove the -1 in box_list.append(raw_data[i][:, (1, 0, 3, 2)] - 1) I still get the error but without the above part.\nShould I normalize the values in my xml files? And in case how?\n. @IchibanKanobee  thank you for the explanation!\n. Could anybody give me a suggestion about how to get rid of AssertionError?\n. Thanks @anuchandra for the suggestion. Actually I was trying to improve my previous dataset adding more pictures (I only use two classes including the background). The error I got happened using pictures from Imagenet database and I temporarily solve it just don't using them.\n. @deeha  You can easily solve it leaving temporarily a copy of the train.txt in the Images folder.\n. Is there already any script for testing fast-rcnn on videos?\n. Hello,\nwhen I try to train, I get this error\nFile \"/home/attilio/fast-rcnn/tools/../lib/datasets/amph.py\", line 190, in get_data_from_tag\n    return node.getElementsByTagName(tag)[0].childNodes[0].data\nIndexError: list index out of range\nWould you help to understand how to fix it?\nI don't know if it depends on the Annotation file I have or some editing error of python files (I use just one class)\n. I fixed it adding to the .xml files (which I got from Matlab) the fields name and folder. \n. Can you tell me why running detection, the frame detect just one item at a time?\n. @nw362  exactly :)\n. In order to detect more ROIs in the same frame there is anything I could do? \n. @nw362 I checked and it seems to me that it does not send only one ROI. \n. I have a little doubt. During the labeling of the training set, is it possible to label more items (ROIs) in the same image?\n. @nw362 Do you mean that is not a problem if there are more bounding boxes for the same .xml file?\n. @nw362  I'm not sure about what you mean with many data sets. I have only two classes (background and amphora) and in some image from the data set there are many amphoras.\n. ",
    "MinaRe": "Dear All\nfinaly i run fast-rcnn on my dataset and got acceptable results on object detection task, but I was wondering to know about the segmentation task? has anyone done it before?\n. @attiliotnt I apply fast-rcnn for cancer cell detection.\n. 3D images :)\n do you know about the segmentation task by fast rcnn?\n. I did training on Asus-G751JT on CPU.\n. @deeha  I followed step by step of https://recordnotfound.com/fast-rcnn-coldmanck-53618 ,let me know if you have any problem after reading this page :)\n. Dear All, (@zeyuanxy ) \ncan you please kindly tell me how can I change batch_size?\n. thanks @IdiosyncraticDragon \nActually my training stop during  \"Computing bounding-box regression targets...\" ,do you have any Idea? is this problem because of memory size?  I have 50k images, How much CPU memory do I need for training?\n. Hi Guys\nI have received this problem when I want to run demo ,I dont have any idea ,can you please help me.\nThanks in advance!\nOptiPlex-990:~/selective_search_py$ ./demo_showcandidates.py --image image.jpg\nTraceback (most recent call last):\n  File \"./demo_showcandidates.py\", line 12, in \n    import selective_search\n  File \"/home/minarezaei/selective_search_py/selective_search.py\", line 10, in \n    import segment\nImportError: /usr/lib/x86_64-linux-gnu/libboost_python-py34.so.1.54.0: undefined symbol: PyUnicode_AsUTF8String\n. Just put \"sudo\" before your comment, and if the problem still ,you can change number  of batch.\n. Dear All\nfinaly i run fast-rcnn on my dataset and got acceptable results on object detection task, but I was wondering to know about the segmentation task? has anyone done it before?\n. @attiliotnt I apply fast-rcnn for cancer cell detection.\n. 3D images :)\n do you know about the segmentation task by fast rcnn?\n. I did training on Asus-G751JT on CPU.\n. @deeha  I followed step by step of https://recordnotfound.com/fast-rcnn-coldmanck-53618 ,let me know if you have any problem after reading this page :)\n. Dear All, (@zeyuanxy ) \ncan you please kindly tell me how can I change batch_size?\n. thanks @IdiosyncraticDragon \nActually my training stop during  \"Computing bounding-box regression targets...\" ,do you have any Idea? is this problem because of memory size?  I have 50k images, How much CPU memory do I need for training?\n. Hi Guys\nI have received this problem when I want to run demo ,I dont have any idea ,can you please help me.\nThanks in advance!\nOptiPlex-990:~/selective_search_py$ ./demo_showcandidates.py --image image.jpg\nTraceback (most recent call last):\n  File \"./demo_showcandidates.py\", line 12, in \n    import selective_search\n  File \"/home/minarezaei/selective_search_py/selective_search.py\", line 10, in \n    import segment\nImportError: /usr/lib/x86_64-linux-gnu/libboost_python-py34.so.1.54.0: undefined symbol: PyUnicode_AsUTF8String\n. Just put \"sudo\" before your comment, and if the problem still ,you can change number  of batch.\n. ",
    "deeha": "@MinaRe i really need to know , how you run it? i need steps.  kindly guide me. thanks alot \n. @MinaRe i have problem like this:\nselective_search\nError using textread (line 165)\nFile not found.\nError in selective_search (line 14)\nimage_filenames = textread([image_db '/data/ImageSets/train.txt'], '%s', 'delimiter', '\\n');\n. @attiliotnt  still same error. i think error is \"textread\".\n. @MinaRe i really need to know , how you run it? i need steps.  kindly guide me. thanks alot \n. @MinaRe i have problem like this:\nselective_search\nError using textread (line 165)\nFile not found.\nError in selective_search (line 14)\nimage_filenames = textread([image_db '/data/ImageSets/train.txt'], '%s', 'delimiter', '\\n');\n. @attiliotnt  still same error. i think error is \"textread\".\n. ",
    "karaspd": "@zeyuanxy , @SunShineMin , @IchibanKanobee, @MinaRe I did training on INRIA dataset but when I do demo I saw lots of  little boxes with confidence of higher than 0.5 (and all same) at the bottom or side of demo images and some correct boxes on the object. Please see the attached image. I was wondering if any of you see this issue previously. I was suspected of train scales however changing it to [300,400,500,600,700,800] did not solve it. Please let me know if you have any solution to this problem.\n(Also I saw that it can only detect objects inside a square with the length of shortest side of the image!! )\n\n. I found my problem :) for future reference, the problem was because of object proposal coming from matlab selective search and demo.py. selective search was given boxes in the format of (y1,x1,y2,x2) \nobj_proposal = sio.loadmat(box_file)['boxes']\nI swapped x1 with y1 and x2 with y2.\n        for i in range(len(obj_proposal)):\n            obj_proposal[i,0], obj_proposal[i,1]=obj_proposal[i,1], obj_proposal[i,0]\n            obj_proposal[i,2], obj_proposal[i,3]=obj_proposal[i,3], obj_proposal[i,2]\n. @zeyuanxy , @SunShineMin , @IchibanKanobee, @MinaRe I did training on INRIA dataset but when I do demo I saw lots of  little boxes with confidence of higher than 0.5 (and all same) at the bottom or side of demo images and some correct boxes on the object. Please see the attached image. I was wondering if any of you see this issue previously. I was suspected of train scales however changing it to [300,400,500,600,700,800] did not solve it. Please let me know if you have any solution to this problem.\n(Also I saw that it can only detect objects inside a square with the length of shortest side of the image!! )\n\n. I found my problem :) for future reference, the problem was because of object proposal coming from matlab selective search and demo.py. selective search was given boxes in the format of (y1,x1,y2,x2) \nobj_proposal = sio.loadmat(box_file)['boxes']\nI swapped x1 with y1 and x2 with y2.\n        for i in range(len(obj_proposal)):\n            obj_proposal[i,0], obj_proposal[i,1]=obj_proposal[i,1], obj_proposal[i,0]\n            obj_proposal[i,2], obj_proposal[i,3]=obj_proposal[i,3], obj_proposal[i,2]\n. ",
    "mksarker": "hi all,\nI have followed the below web instruction for training my own dataset as INRIA,\nhttps://github.com/zeyuanxy/fast-rcnn/blob/master/help/train/README.md\nand found the below problem,\n:~$ cd fast-rcnn\n:~/fast-rcnn$ ./tools/train_net.py --gpu 0 --solver models/pascal_voc/VGG_CNN_M_1024/fast-rcnn/solver.prototxt --weights data/faster_rcnn_models/VGG16_faster_rcnn_final.caffemodel --imdb inria_train\n/home/sarker/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\nPython 2.7.11 |Anaconda custom (64-bit)| (default, Jun 15 2016, 15:21:30) \nType \"copyright\", \"credits\" or \"license\" for more information.\nIPython 4.2.0 -- An enhanced Interactive Python.\n?         -> Introduction and overview of IPython's features.\n%quickref -> Quick reference.\nhelp      -> Python's own help system.\nobject?   -> Details about 'object', use 'object??' for extra details.\nIn [1]: \nwhen I run the training code on terminal the IPython console is open? I don\u2019t know what is the problem? I am a beginner  in this area. Please help me.\nThanks in advanced....\n. hi all,\nI have followed the below web instruction for training my own dataset as INRIA,\nhttps://github.com/zeyuanxy/fast-rcnn/blob/master/help/train/README.md\nand found the below problem,\n:~$ cd fast-rcnn\n:~/fast-rcnn$ ./tools/train_net.py --gpu 0 --solver models/pascal_voc/VGG_CNN_M_1024/fast-rcnn/solver.prototxt --weights data/faster_rcnn_models/VGG16_faster_rcnn_final.caffemodel --imdb inria_train\n/home/sarker/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\nwarnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\nPython 2.7.11 |Anaconda custom (64-bit)| (default, Jun 15 2016, 15:21:30)\nType \"copyright\", \"credits\" or \"license\" for more information.\nIPython 4.2.0 -- An enhanced Interactive Python.\n? -> Introduction and overview of IPython's features.\n%quickref -> Quick reference.\nhelp -> Python's own help system.\nobject? -> Details about 'object', use 'object??' for extra details.\nIn [1]:\nwhen I run the training code on terminal the IPython console is open? I don\u2019t know what is the problem? I am a beginner in this area. Please help me.\nThanks in advanced....\n. hi all,\nI have followed the below web instruction for training my own dataset as INRIA,\nhttps://github.com/zeyuanxy/fast-rcnn/blob/master/help/train/README.md\nand found the below problem,\n:~$ cd fast-rcnn\n:~/fast-rcnn$ ./tools/train_net.py --gpu 0 --solver models/pascal_voc/VGG_CNN_M_1024/fast-rcnn/solver.prototxt --weights data/faster_rcnn_models/VGG16_faster_rcnn_final.caffemodel --imdb inria_train\n/home/sarker/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\nPython 2.7.11 |Anaconda custom (64-bit)| (default, Jun 15 2016, 15:21:30) \nType \"copyright\", \"credits\" or \"license\" for more information.\nIPython 4.2.0 -- An enhanced Interactive Python.\n?         -> Introduction and overview of IPython's features.\n%quickref -> Quick reference.\nhelp      -> Python's own help system.\nobject?   -> Details about 'object', use 'object??' for extra details.\nIn [1]: \nwhen I run the training code on terminal the IPython console is open? I don\u2019t know what is the problem? I am a beginner  in this area. Please help me.\nThanks in advanced....\n. hi all,\nI have followed the below web instruction for training my own dataset as INRIA,\nhttps://github.com/zeyuanxy/fast-rcnn/blob/master/help/train/README.md\nand found the below problem,\n:~$ cd fast-rcnn\n:~/fast-rcnn$ ./tools/train_net.py --gpu 0 --solver models/pascal_voc/VGG_CNN_M_1024/fast-rcnn/solver.prototxt --weights data/faster_rcnn_models/VGG16_faster_rcnn_final.caffemodel --imdb inria_train\n/home/sarker/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\nwarnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\nPython 2.7.11 |Anaconda custom (64-bit)| (default, Jun 15 2016, 15:21:30)\nType \"copyright\", \"credits\" or \"license\" for more information.\nIPython 4.2.0 -- An enhanced Interactive Python.\n? -> Introduction and overview of IPython's features.\n%quickref -> Quick reference.\nhelp -> Python's own help system.\nobject? -> Details about 'object', use 'object??' for extra details.\nIn [1]:\nwhen I run the training code on terminal the IPython console is open? I don\u2019t know what is the problem? I am a beginner in this area. Please help me.\nThanks in advanced....\n. ",
    "abhisheksgumadi": "I still get this error\nargmaxes = gt_overlaps.argmax(axis=1)\nValueError: attempt to get argmax of an empty sequence\nAny idea why this is happening? I am training on my own set of images and all annotation files look fine for me.\nThanks ! \n. Thanks @anuchandra  I got the error. It was because I had few annotation files that did not have any object annotations (bounding boxes) in them. I eliminated them and now it runs fine.\nHaving solved that, I have another question. How should the annotation files look if I have to include images that should completely be treated as background images? Should I have a object annotation in the XML file with the bounding box covering the entire image and the name  of the object as  \"background\" ?\n. Ho can we identify which images in our dataset is causing this problem?\n. I still get this error\nargmaxes = gt_overlaps.argmax(axis=1)\nValueError: attempt to get argmax of an empty sequence\nAny idea why this is happening? I am training on my own set of images and all annotation files look fine for me.\nThanks ! \n. Thanks @anuchandra  I got the error. It was because I had few annotation files that did not have any object annotations (bounding boxes) in them. I eliminated them and now it runs fine.\nHaving solved that, I have another question. How should the annotation files look if I have to include images that should completely be treated as background images? Should I have a object annotation in the XML file with the bounding box covering the entire image and the name  of the object as  \"background\" ?\n. Ho can we identify which images in our dataset is causing this problem?\n. ",
    "anewlearner": "@MinaRe\nHello, MinaRe\nI already know how to train on new 2d datasets.\nWhat should I do if I want to apply fast-rcnn to 3d images? I am not sure if there is any other changes except for train.prototxt and test.prototxt. It would be better if you can provide an example.\nThanks.\n. @MinaRe\nHello, MinaRe\nI already know how to train on new 2d datasets.\nWhat should I do if I want to apply fast-rcnn to 3d images? I am not sure if there is any other changes except for train.prototxt and test.prototxt. It would be better if you can provide an example.\nThanks.\n. ",
    "xinleipan": "What does \" Write the function for parsing annotations, see _load_sgs_annotation in sgs.py\" and \"Do not forget to add import syntaxes in your own python file and other python files in the same directory\" mean? \nCan any one give a step by step detailed instruction on how to train Faster-RCNN on customized dataset? Thanks!. What does \" Write the function for parsing annotations, see _load_sgs_annotation in sgs.py\" and \"Do not forget to add import syntaxes in your own python file and other python files in the same directory\" mean? \nCan any one give a step by step detailed instruction on how to train Faster-RCNN on customized dataset? Thanks!. ",
    "sharmapulkit": "I am trying to train fast-RCNN on INRIA dataset following the instructions as given by @zeyuanxy  at https://github.com/zeyuanxy/fast-rcnn/blob/master/help/train/README.md. But I have not been able to achieve any appreciable results. The output of the trained fast-RCNN gives all the input selective search bounding boxes and all with the same score. Below is a brief of approach I followed:\nI have prepared the 'inria.py' file, put the required code in factory.py. I have computed the selective search boxes using the code by EdisonResearch. Then I have modified Prototxt number of classes, and running train_net.py.\nCan any one suggest me where the problem might be originating? It is quite odd to get every proposal's score to be exactly same everytime I train a model.\nThank You!. @ruqiang826 Well, Earlier I was not using the pre-trained model on imagenet and training it from the scratch. Using pretrained imagenet models does improve the results a bit. See if it works for you.. I am trying to train fast-RCNN on INRIA dataset following the instructions as given by @zeyuanxy  at https://github.com/zeyuanxy/fast-rcnn/blob/master/help/train/README.md. But I have not been able to achieve any appreciable results. The output of the trained fast-RCNN gives all the input selective search bounding boxes and all with the same score. Below is a brief of approach I followed:\nI have prepared the 'inria.py' file, put the required code in factory.py. I have computed the selective search boxes using the code by EdisonResearch. Then I have modified Prototxt number of classes, and running train_net.py.\nCan any one suggest me where the problem might be originating? It is quite odd to get every proposal's score to be exactly same everytime I train a model.\nThank You!. @ruqiang826 Well, Earlier I was not using the pre-trained model on imagenet and training it from the scratch. Using pretrained imagenet models does improve the results a bit. See if it works for you.. ",
    "micros-uav": "@attiliotnt I also have this error:\nassert all(max_classes[nonzero_inds] != 0)\nAssertionError\nHave you found the solution?. @attiliotnt I also have this error:\nassert all(max_classes[nonzero_inds] != 0)\nAssertionError\nHave you found the solution?. ",
    "ruqiang826": "@Terminator1500 I have the same problem of same score. have you found the solution?. @Terminator1500 I have the same problem of same score. have you found the solution?. ",
    "smajida": "@Terminator1500 I am also having the same problem of same scores for different objects. The results are strange as I get same detection bboxes for different classes. Have you found the solution?. @asbroad \nHi!\nI am going to make a live demo of Faster RCNN by using my laptops webcam. Is that possible to just pulling your changes in fast-rcnn make this demo run?\nsomething like the second movie in here: http://pjreddie.com/darknet/yolo/\nwhich is the real-time demo of \"You only look once\" paper.\nI am interested to do so for Faster RCNN which is the fastest in the family of RCNNs.\nThanks in advance\n. yes as far as I know.\n2016-01-14 11:09 GMT+01:00 arushk1 notifications@github.com:\n\nSo if I replace 'person' in your code to say 'dog'. It should detect it\nright?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rbgirshick/fast-rcnn/pull/29#issuecomment-171594240.\n. @Terminator1500 I am also having the same problem of same scores for different objects. The results are strange as I get same detection bboxes for different classes. Have you found the solution?. @asbroad \nHi!\nI am going to make a live demo of Faster RCNN by using my laptops webcam. Is that possible to just pulling your changes in fast-rcnn make this demo run?\nsomething like the second movie in here: http://pjreddie.com/darknet/yolo/\nwhich is the real-time demo of \"You only look once\" paper.\nI am interested to do so for Faster RCNN which is the fastest in the family of RCNNs.\nThanks in advance\n. yes as far as I know.\n\n2016-01-14 11:09 GMT+01:00 arushk1 notifications@github.com:\n\nSo if I replace 'person' in your code to say 'dog'. It should detect it\nright?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rbgirshick/fast-rcnn/pull/29#issuecomment-171594240.\n. \n",
    "indsak": "Hi\ni am trying to train faster RCNN as per https://github.com/deboc/py-faster-rcnn/tree/master/help\nWhen i try to train using the command\n./tools/train_faster_rcnn_alt_opt.py --gpu 0 --net_name fishclassify --weights data/imagenet_models/VGG_CNN_M_1024.v2.caffemodel --imdb fishclassify --cfg /home/alpha/Indhu/fast-rcnn/py-faster-rcnn/config.yml\nI am getting the following error\nFile \"/fast-rcnn/py-faster-rcnn/tools/../lib/datasets/factory.py\", line 46, in get_imdb\n    raise KeyError('Unknown dataset: {}'.format(name))\nAny help on what might be causing this?\nI had created fishclassify.py and fishclassify_eval.py under lib/datasets as well as modified factory.py\n. Hi,\n I successfully executed demo.py. But i am not able to train my own dataset.\n./tools/train_faster_rcnn_alt_opt.py --net_name fish_classify --weights data/imagenet_models/VGG_CNN_M_1024.v2.caffemodel \nCalled with args:\nNamespace(cfg_file=None, gpu_id=0, imdb_name='voc_2007_trainval', net_name='fish_classify', pretrained_model='data/imagenet_models/VGG_CNN_M_1024.v2.caffemodel', set_cfgs=None)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nStage 1 RPN, init from ImageNet model\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nInit model: data/imagenet_models/VGG_CNN_M_1024.v2.caffemodel\nUsing config:\n{'DATA_DIR': '/root/deep-learning/FRCNN/py-faster-rcnn/data',\n 'DEDUP_BOXES': 0.0625,\n 'EPS': 1e-14,\n 'EXP_DIR': 'default',\n 'GPU_ID': 0,\n 'MATLAB': 'matlab',\n 'MODELS_DIR': '/root/deep-learning/FRCNN/py-faster-rcnn/models/pascal_voc',\n 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),\n 'RNG_SEED': 3,\n 'ROOT_DIR': '/root/deep-learning/FRCNN/py-faster-rcnn',\n 'TEST': {'BBOX_REG': True,\n          'HAS_RPN': False,\n          'MAX_SIZE': 1000,\n          'NMS': 0.3,\n          'PROPOSAL_METHOD': 'selective_search',\n          'RPN_MIN_SIZE': 16,\n          'RPN_NMS_THRESH': 0.7,\n          'RPN_POST_NMS_TOP_N': 300,\n          'RPN_PRE_NMS_TOP_N': 6000,\n          'SCALES': [600],\n          'SVM': False},\n 'TRAIN': {'ASPECT_GROUPING': True,\n           'BATCH_SIZE': 128,\n           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],\n           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],\n           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],\n           'BBOX_NORMALIZE_TARGETS': True,\n           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,\n           'BBOX_REG': False,\n           'BBOX_THRESH': 0.5,\n           'BG_THRESH_HI': 0.5,\n           'BG_THRESH_LO': 0.1,\n           'FG_FRACTION': 0.25,\n           'FG_THRESH': 0.5,\n           'HAS_RPN': True,\n           'IMS_PER_BATCH': 1,\n           'MAX_SIZE': 1000,\n           'PROPOSAL_METHOD': 'gt',\n           'RPN_BATCHSIZE': 256,\n           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],\n           'RPN_CLOBBER_POSITIVES': False,\n           'RPN_FG_FRACTION': 0.5,\n           'RPN_MIN_SIZE': 16,\n           'RPN_NEGATIVE_OVERLAP': 0.3,\n           'RPN_NMS_THRESH': 0.7,\n           'RPN_POSITIVE_OVERLAP': 0.7,\n           'RPN_POSITIVE_WEIGHT': -1.0,\n           'RPN_POST_NMS_TOP_N': 2000,\n           'RPN_PRE_NMS_TOP_N': 12000,\n           'SCALES': [600],\n           'SNAPSHOT_INFIX': 'stage1',\n           'SNAPSHOT_ITERS': 10000,\n           'USE_FLIPPED': True,\n           'USE_PREFETCH': False},\n 'USE_GPU_NMS': False}\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nF0608 16:24:47.054726  7419 common.cpp:66] Cannot use GPU in CPU-only Caffe: check mode.\n Check failure stack trace: \n. Hi\n i started to train faster RCNN as per https://github.com/deboc/py-faster-rcnn/tree/master/help\nWhen i try to train using the command\n./tools/train_faster_rcnn_alt_opt.py --gpu 0 --net_name fishclassify --weights data/imagenet_models/VGG_CNN_M_1024.v2.caffemodel --imdb fishclassify --cfg /home/fast-rcnn/py-faster-rcnn/config.yml\nI am getting the following error\nFile \"/fast-rcnn/py-faster-rcnn/tools/../lib/datasets/factory.py\", line 46, in get_imdb\nraise KeyError('Unknown dataset: {}'.format(name))\nAny help on what might be causing this?\nI had created fishclassify.py and fishclassify_eval.py under lib/datasets as well as modified factory.py. @baristahell \nhow you solved this error? I am experiencing the same problem and would like to know where I missed the modification? Pls help.. Hi\ni am trying to train faster RCNN as per https://github.com/deboc/py-faster-rcnn/tree/master/help\nWhen i try to train using the command\n./tools/train_faster_rcnn_alt_opt.py --gpu 0 --net_name fishclassify --weights data/imagenet_models/VGG_CNN_M_1024.v2.caffemodel --imdb fishclassify --cfg /home/alpha/Indhu/fast-rcnn/py-faster-rcnn/config.yml\nI am getting the following error\nFile \"/fast-rcnn/py-faster-rcnn/tools/../lib/datasets/factory.py\", line 46, in get_imdb\n    raise KeyError('Unknown dataset: {}'.format(name))\nAny help on what might be causing this?\nI had created fishclassify.py and fishclassify_eval.py under lib/datasets as well as modified factory.py\n. Hi,\n I successfully executed demo.py. But i am not able to train my own dataset.\n./tools/train_faster_rcnn_alt_opt.py --net_name fish_classify --weights data/imagenet_models/VGG_CNN_M_1024.v2.caffemodel \nCalled with args:\nNamespace(cfg_file=None, gpu_id=0, imdb_name='voc_2007_trainval', net_name='fish_classify', pretrained_model='data/imagenet_models/VGG_CNN_M_1024.v2.caffemodel', set_cfgs=None)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nStage 1 RPN, init from ImageNet model\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nInit model: data/imagenet_models/VGG_CNN_M_1024.v2.caffemodel\nUsing config:\n{'DATA_DIR': '/root/deep-learning/FRCNN/py-faster-rcnn/data',\n 'DEDUP_BOXES': 0.0625,\n 'EPS': 1e-14,\n 'EXP_DIR': 'default',\n 'GPU_ID': 0,\n 'MATLAB': 'matlab',\n 'MODELS_DIR': '/root/deep-learning/FRCNN/py-faster-rcnn/models/pascal_voc',\n 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),\n 'RNG_SEED': 3,\n 'ROOT_DIR': '/root/deep-learning/FRCNN/py-faster-rcnn',\n 'TEST': {'BBOX_REG': True,\n          'HAS_RPN': False,\n          'MAX_SIZE': 1000,\n          'NMS': 0.3,\n          'PROPOSAL_METHOD': 'selective_search',\n          'RPN_MIN_SIZE': 16,\n          'RPN_NMS_THRESH': 0.7,\n          'RPN_POST_NMS_TOP_N': 300,\n          'RPN_PRE_NMS_TOP_N': 6000,\n          'SCALES': [600],\n          'SVM': False},\n 'TRAIN': {'ASPECT_GROUPING': True,\n           'BATCH_SIZE': 128,\n           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],\n           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],\n           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],\n           'BBOX_NORMALIZE_TARGETS': True,\n           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,\n           'BBOX_REG': False,\n           'BBOX_THRESH': 0.5,\n           'BG_THRESH_HI': 0.5,\n           'BG_THRESH_LO': 0.1,\n           'FG_FRACTION': 0.25,\n           'FG_THRESH': 0.5,\n           'HAS_RPN': True,\n           'IMS_PER_BATCH': 1,\n           'MAX_SIZE': 1000,\n           'PROPOSAL_METHOD': 'gt',\n           'RPN_BATCHSIZE': 256,\n           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],\n           'RPN_CLOBBER_POSITIVES': False,\n           'RPN_FG_FRACTION': 0.5,\n           'RPN_MIN_SIZE': 16,\n           'RPN_NEGATIVE_OVERLAP': 0.3,\n           'RPN_NMS_THRESH': 0.7,\n           'RPN_POSITIVE_OVERLAP': 0.7,\n           'RPN_POSITIVE_WEIGHT': -1.0,\n           'RPN_POST_NMS_TOP_N': 2000,\n           'RPN_PRE_NMS_TOP_N': 12000,\n           'SCALES': [600],\n           'SNAPSHOT_INFIX': 'stage1',\n           'SNAPSHOT_ITERS': 10000,\n           'USE_FLIPPED': True,\n           'USE_PREFETCH': False},\n 'USE_GPU_NMS': False}\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nF0608 16:24:47.054726  7419 common.cpp:66] Cannot use GPU in CPU-only Caffe: check mode.\n Check failure stack trace: \n. Hi\n i started to train faster RCNN as per https://github.com/deboc/py-faster-rcnn/tree/master/help\nWhen i try to train using the command\n./tools/train_faster_rcnn_alt_opt.py --gpu 0 --net_name fishclassify --weights data/imagenet_models/VGG_CNN_M_1024.v2.caffemodel --imdb fishclassify --cfg /home/fast-rcnn/py-faster-rcnn/config.yml\nI am getting the following error\nFile \"/fast-rcnn/py-faster-rcnn/tools/../lib/datasets/factory.py\", line 46, in get_imdb\nraise KeyError('Unknown dataset: {}'.format(name))\nAny help on what might be causing this?\nI had created fishclassify.py and fishclassify_eval.py under lib/datasets as well as modified factory.py. @baristahell \nhow you solved this error? I am experiencing the same problem and would like to know where I missed the modification? Pls help.. ",
    "raingo": "@rohitgirdhar It's the results of debugging. I don't know why it should be like that. There are two variables in the mat file. One is a cell array of path strings, and the other is a cell array of matrices (bbox).\n. @rohitgirdhar It's the results of debugging. I don't know why it should be like that. There are two variables in the mat file. One is a cell array of path strings, and the other is a cell array of matrices (bbox).\n. ",
    "damro": "@raingo Could you please share your trained models for COCO?\n. @raingo Could you please share your trained models for COCO?\n. ",
    "acpn": "Anybody managed? Same problem here.\n. Anybody managed? Same problem here.\n. ",
    "revilokeb": "Hi @zeyuanxy, thanks a lot for your very helpful INRIA example and your detailed recipe. I now can learn on positive examples (i.e. having images with annotations of bboxes and classes), but I am still failing to include negative examples (i.e. when there are no annotations). How have you managed to include those when learning INRIA? Setting the objs in _load_pascal_annotation to be empty as you suggest / ask results in python variables boxes, gt_classes etc to be arrays of shape (0,4) which gives problems when later in create_roidb_from_box_list the overlaps are calculated between those arrays and region proposals from selective search. What am I missing? Thanks so much!\n. you might use any, just have to adjust _load_pascal_annotation in pascal_voc.py accordingly. I am using sloth for annotating own datasets, save it as JSON and read from there.\n. Hi @zeyuanxy, thanks a lot for your very helpful INRIA example and your detailed recipe. I now can learn on positive examples (i.e. having images with annotations of bboxes and classes), but I am still failing to include negative examples (i.e. when there are no annotations). How have you managed to include those when learning INRIA? Setting the objs in _load_pascal_annotation to be empty as you suggest / ask results in python variables boxes, gt_classes etc to be arrays of shape (0,4) which gives problems when later in create_roidb_from_box_list the overlaps are calculated between those arrays and region proposals from selective search. What am I missing? Thanks so much!\n. you might use any, just have to adjust _load_pascal_annotation in pascal_voc.py accordingly. I am using sloth for annotating own datasets, save it as JSON and read from there.\n. ",
    "LarsHH": "Hi. Is there an update on this? I understood that F-R-CNN picked negative examples from the selective search boxes that have overlap with the ground-truth smaller than the threshold specified in the F-R-CNN config. Is that correct?\n. Do you mean to test the model without fine-tuning or to train it from scratch?\n. If you just want to train on Pascal Voc 2007 you just go to /lib/datasets/pascal_voc and modify the _load_selective_search_roidb function\nThe proposals from your method should be in the same order as your ground-truth images. So for Pascal Voc this would be the order as in the list VOC2007/ImageSets/Main/train.txt\nThen in _load_selective_search_roidb you load the proposals as a list of numpy arrays where each array represents the proposals for one image. The elements of the list should be the same order as the groundtruth like I said above\n. I think fast r-cnn only provides selective search proposals for Pascal VOC 2007\nIf I'm right the ILSVRC 2014 Detection challenge has 40152 testing images?\nYou can try to find proposals for this on the internet. In a quick 1min search I didn't find anything but you may have more luck. Otherwise you would have to compute the bounding box proposals yourself. You can use selective search for that.\nFrom my experience the selective_search_IJCV_for_python is very slow and crashes when you feed too many images into it. For me it took 1 day for 10000 images at ~2000 proposals per image with a script that feeds 300 images at a time. It also requires Matlab. You could check out dlib's selective search as in issue #51 which might be faster and more robust\nOnce you have the proposals you can go to tools/demo.py and modify it to use your images and proposals.\n. No solution here but I stayed away from computing selective search for my entire dataset (220000 images) as I calculated it would take 15 days of computing time.\nI used some other proposals I had for the data set so I can't help you. Just had a similar experience\n. Hi. Is there an update on this? I understood that F-R-CNN picked negative examples from the selective search boxes that have overlap with the ground-truth smaller than the threshold specified in the F-R-CNN config. Is that correct?\n. Do you mean to test the model without fine-tuning or to train it from scratch?\n. If you just want to train on Pascal Voc 2007 you just go to /lib/datasets/pascal_voc and modify the _load_selective_search_roidb function\nThe proposals from your method should be in the same order as your ground-truth images. So for Pascal Voc this would be the order as in the list VOC2007/ImageSets/Main/train.txt\nThen in _load_selective_search_roidb you load the proposals as a list of numpy arrays where each array represents the proposals for one image. The elements of the list should be the same order as the groundtruth like I said above\n. I think fast r-cnn only provides selective search proposals for Pascal VOC 2007\nIf I'm right the ILSVRC 2014 Detection challenge has 40152 testing images?\nYou can try to find proposals for this on the internet. In a quick 1min search I didn't find anything but you may have more luck. Otherwise you would have to compute the bounding box proposals yourself. You can use selective search for that.\nFrom my experience the selective_search_IJCV_for_python is very slow and crashes when you feed too many images into it. For me it took 1 day for 10000 images at ~2000 proposals per image with a script that feeds 300 images at a time. It also requires Matlab. You could check out dlib's selective search as in issue #51 which might be faster and more robust\nOnce you have the proposals you can go to tools/demo.py and modify it to use your images and proposals.\n. No solution here but I stayed away from computing selective search for my entire dataset (220000 images) as I calculated it would take 15 days of computing time.\nI used some other proposals I had for the data set so I can't help you. Just had a similar experience\n. ",
    "hongpingcai": "I also had such a problem. \nI have spent a long time to change a few codes in the lib/dataset/.py to distinguish if there exist ground-truth boxes or not. But still got wrong in /lib/roi_data_layer/minibatch.py, it seems I need do more changes. It is very frunstrating ......\n @rbgirshick \n. I also had such a problem. \nI have spent a long time to change a few codes in the lib/dataset/.py to distinguish if there exist ground-truth boxes or not. But still got wrong in /lib/roi_data_layer/minibatch.py, it seems I need do more changes. It is very frunstrating ......\n @rbgirshick \n. ",
    "DebangLi": "I have the same questions.\nI also change some code in the lib/dataset, and when the image is a negative sample, I just put the RoIs form the selective search included in the roidb. But when I start to train, it will  aborted (core dumped), and the information is \"F0326 10:14:48.913087  3132 roi_pooling_layer.cu:91] Check failed: error == cudaSuccess (9 vs. 0)  invalid configuration argument\"\n. I have the same questions.\nI also change some code in the lib/dataset, and when the image is a negative sample, I just put the RoIs form the selective search included in the roidb. But when I start to train, it will  aborted (core dumped), and the information is \"F0326 10:14:48.913087  3132 roi_pooling_layer.cu:91] Check failed: error == cudaSuccess (9 vs. 0)  invalid configuration argument\"\n. ",
    "nwestlake": "Please see https://github.com/rbgirshick/fast-rcnn/pull/102 \nNB by default none of the ROIs in negative image will be used, however you can set TRAIN.BG_THRESH_LO to 0.0.\n. @leejiajun\nTry applying the changes proposed here: https://github.com/rbgirshick/fast-rcnn/pull/102\ne.g. \ngit fetch git@github.com:nw362/fast-rcnn.git negativeImages:negativeImages\ngit merge negativeImages\n(I am interesting in the pull request too, so I might later try merging it with mine.)\n. This looks to be code you have written (amph.py)? Hard to say without seeing both the code and the annotation file. \n. One ROI at a time? \n. The convolutional layers of the CNN are calculated on the whole image (resized before hand). The final polling layer pools over only the ROI and so the CNN must run the forward steps over these layers for every ROI.\n. It should happen already \"lib/fast_rcnn/test.py\": _get_blobs will take in one image but multiple ROIs. In im_detect(net, im, boxes), the CNN will be called forward only once for an image and a set of ROIs. Is only one ROI being sent to this method? \n. Yes, e.g. the xml format used for annotations.\n. No problem at all. This is the case for many data sets. \n. Set TRAIN.BG_THRESH_LO to 0.0?\n. Are there images without any annotations/GT boxes?\n. A different issue then. It's a shame it doesn't give a file name and line number to assist.\n. I don't think it knows about the dataset until line 46 but I may be wrong. Does it work if you change dataset back to voc? \nline 46: self.solver.net.layers[0].set_roidb(roidb)\nBut just in case it's the same issue I had, apply 75b3fdd083f0490244441933b96581dcea719873 which gives a nicer error message if it was this. \n. Probably, especially if it's missing a layer that the fast-rcnn-caffe has!\n. Thanks!\nOn 26 January 2016 at 06:16, Ross Girshick notifications@github.com wrote:\n\n@nw362 https://github.com/nw362 the paper you referenced is previous\nwork. This paper is the correct one for this code:\nhttp://arxiv.org/pdf/1504.08083.pdf.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rbgirshick/fast-rcnn/issues/99#issuecomment-174851101\n.\n. Please see https://github.com/rbgirshick/fast-rcnn/pull/102 \n\nNB by default none of the ROIs in negative image will be used, however you can set TRAIN.BG_THRESH_LO to 0.0.\n. This sounds to me like it's running the wrong version of Caffe, i.e. not the one required for  and included with fast-rcnn which includes the appropriate layer.\nI suspect running \"locate roi_pooling_layer.hpp\" in the terminal would show another version to be installed. Otherwise, I'd check that caffe-fast-rcnn is on the right commit. \n. Please see https://github.com/rbgirshick/fast-rcnn/pull/102 \nNB by default none of the ROIs in negative image will be used, however you can set TRAIN.BG_THRESH_LO to 0.0.\n. @leejiajun\nTry applying the changes proposed here: https://github.com/rbgirshick/fast-rcnn/pull/102\ne.g. \ngit fetch git@github.com:nw362/fast-rcnn.git negativeImages:negativeImages\ngit merge negativeImages\n(I am interesting in the pull request too, so I might later try merging it with mine.)\n. This looks to be code you have written (amph.py)? Hard to say without seeing both the code and the annotation file. \n. One ROI at a time? \n. The convolutional layers of the CNN are calculated on the whole image (resized before hand). The final polling layer pools over only the ROI and so the CNN must run the forward steps over these layers for every ROI.\n. It should happen already \"lib/fast_rcnn/test.py\": _get_blobs will take in one image but multiple ROIs. In im_detect(net, im, boxes), the CNN will be called forward only once for an image and a set of ROIs. Is only one ROI being sent to this method? \n. Yes, e.g. the xml format used for annotations.\n. No problem at all. This is the case for many data sets. \n. Set TRAIN.BG_THRESH_LO to 0.0?\n. Are there images without any annotations/GT boxes?\n. A different issue then. It's a shame it doesn't give a file name and line number to assist.\n. I don't think it knows about the dataset until line 46 but I may be wrong. Does it work if you change dataset back to voc? \nline 46: self.solver.net.layers[0].set_roidb(roidb)\nBut just in case it's the same issue I had, apply 75b3fdd083f0490244441933b96581dcea719873 which gives a nicer error message if it was this. \n. Probably, especially if it's missing a layer that the fast-rcnn-caffe has!\n. Thanks!\nOn 26 January 2016 at 06:16, Ross Girshick notifications@github.com wrote:\n\n@nw362 https://github.com/nw362 the paper you referenced is previous\nwork. This paper is the correct one for this code:\nhttp://arxiv.org/pdf/1504.08083.pdf.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rbgirshick/fast-rcnn/issues/99#issuecomment-174851101\n.\n. Please see https://github.com/rbgirshick/fast-rcnn/pull/102 \n\nNB by default none of the ROIs in negative image will be used, however you can set TRAIN.BG_THRESH_LO to 0.0.\n. This sounds to me like it's running the wrong version of Caffe, i.e. not the one required for  and included with fast-rcnn which includes the appropriate layer.\nI suspect running \"locate roi_pooling_layer.hpp\" in the terminal would show another version to be installed. Otherwise, I'd check that caffe-fast-rcnn is on the right commit. \n. ",
    "cuatristapr": "@zeyuanxy Do you have a trained model that I can already use in my project? \n. How do I use it futurely? Where do I add that line to point to the python wrapper of Selective Search?\n. Hello asbroad,\nCan you send me the webcam demo file? I'm new to all of this and I'm trying to understand how will this work for real-time implementation on a Jetson TK1. Thanks!\n. I got the demo running, it works. I have a few questions, I still have no idea what does the scale factor actually does, and it's running like with 3-4 seconds delay on the Jetson TK1, but it's a start! \n. @tsaiJN Maybe your memory issue has to deal with the libraries. Check that all the dependencies are working good, and that you use the openCV designed for the Jetson (it has a different implementation for the CUDA framework).\n. @kesonyk try to use the COCO, it is a small net. Also, depends on what you want to train for. If you want for the ImageNET or PASCAL, it will be different for each. You can also train a fast-rcnn model on a imageset, which you can create and train that model on. That way, it will be smaller and the Jetson will run it better. NOTE: you are not looking to train for a large dataset, you want something small and concise, that way it runs well on the Jetson.\n. @zeyuanxy Do you have a trained model that I can already use in my project? \n. How do I use it futurely? Where do I add that line to point to the python wrapper of Selective Search?\n. Hello asbroad,\nCan you send me the webcam demo file? I'm new to all of this and I'm trying to understand how will this work for real-time implementation on a Jetson TK1. Thanks!\n. I got the demo running, it works. I have a few questions, I still have no idea what does the scale factor actually does, and it's running like with 3-4 seconds delay on the Jetson TK1, but it's a start! \n. @tsaiJN Maybe your memory issue has to deal with the libraries. Check that all the dependencies are working good, and that you use the openCV designed for the Jetson (it has a different implementation for the CUDA framework).\n. @kesonyk try to use the COCO, it is a small net. Also, depends on what you want to train for. If you want for the ImageNET or PASCAL, it will be different for each. You can also train a fast-rcnn model on a imageset, which you can create and train that model on. That way, it will be smaller and the Jetson will run it better. NOTE: you are not looking to train for a large dataset, you want something small and concise, that way it runs well on the Jetson.\n. ",
    "futurely": "You don't have to stick with the MATLAB implementation of selective search. There's a pure python version.\nUsage is a super easy one-liner rects = dlibss.selective_search(img, 50, 200, 3, 20, 50).\n. Custom datasets need to implement their own imdb classes following the PASCAL VOC examples.\nhttps://github.com/rbgirshick/fast-rcnn/blob/master/lib/datasets/imdb.py\nhttps://github.com/rbgirshick/fast-rcnn/blob/master/lib/datasets/pascal_voc.py\nhttps://github.com/rbgirshick/fast-rcnn/blob/master/lib/datasets/factory.py\n. https://github.com/rbgirshick/fast-rcnn/tree/master/experiments\n. You don't have to stick with the MATLAB implementation of selective search. There's a pure python version.\nUsage is a super easy one-liner rects = dlibss.selective_search(img, 50, 200, 3, 20, 50).\n. Custom datasets need to implement their own imdb classes following the PASCAL VOC examples.\nhttps://github.com/rbgirshick/fast-rcnn/blob/master/lib/datasets/imdb.py\nhttps://github.com/rbgirshick/fast-rcnn/blob/master/lib/datasets/pascal_voc.py\nhttps://github.com/rbgirshick/fast-rcnn/blob/master/lib/datasets/factory.py\n. https://github.com/rbgirshick/fast-rcnn/tree/master/experiments\n. ",
    "DaChaoXc": "@zeyuanxy   Hi,I haved tried your mothds,but i have some problems.Did you meet this? File \"./tools/train_net.py\", line 80, in \n    roidb = get_training_roidb(imdb)\n  File \"/home/xc/fast-rcnn-master/tools/../lib/fast_rcnn/train.py\", line 107, in get_training_roidb\n    imdb.append_flipped_images()\n  File \"/home/xc/fast-rcnn-master/tools/../lib/datasets/imdb.py\", line 99, in append_flipped_images\n    boxes = self.roidb[i]['boxes'].copy()\n  File \"/home/xc/fast-rcnn-master/tools/../lib/datasets/imdb.py\", line 63, in roidb\n    self._roidb = self.roidb_handler()\n  File \"/home/xc/fast-rcnn-master/tools/../lib/datasets/inria.py\", line 116, in selective_search_roidb\n    ss_roidb = self._load_selective_search_roidb(gt_roidb)\n  File \"/home/xc/fast-rcnn-master/tools/../lib/datasets/inria.py\", line 138, in _load_selective_search_roidb\n    return self.create_roidb_from_box_list(box_list, gt_roidb)\n  File \"/home/xc/fast-rcnn-master/tools/../lib/datasets/imdb.py\", line 167, in create_roidb_from_box_list\n    argmaxes = gt_overlaps.argmax(axis=1)\nValueError: attempt to get argmax of an empty sequence\n. @nicci1771  NO,i am still confused.\n. @zeyuanxy   Hi,I haved tried your mothds,but i have some problems.Did you meet this? File \"./tools/train_net.py\", line 80, in \n    roidb = get_training_roidb(imdb)\n  File \"/home/xc/fast-rcnn-master/tools/../lib/fast_rcnn/train.py\", line 107, in get_training_roidb\n    imdb.append_flipped_images()\n  File \"/home/xc/fast-rcnn-master/tools/../lib/datasets/imdb.py\", line 99, in append_flipped_images\n    boxes = self.roidb[i]['boxes'].copy()\n  File \"/home/xc/fast-rcnn-master/tools/../lib/datasets/imdb.py\", line 63, in roidb\n    self._roidb = self.roidb_handler()\n  File \"/home/xc/fast-rcnn-master/tools/../lib/datasets/inria.py\", line 116, in selective_search_roidb\n    ss_roidb = self._load_selective_search_roidb(gt_roidb)\n  File \"/home/xc/fast-rcnn-master/tools/../lib/datasets/inria.py\", line 138, in _load_selective_search_roidb\n    return self.create_roidb_from_box_list(box_list, gt_roidb)\n  File \"/home/xc/fast-rcnn-master/tools/../lib/datasets/imdb.py\", line 167, in create_roidb_from_box_list\n    argmaxes = gt_overlaps.argmax(axis=1)\nValueError: attempt to get argmax of an empty sequence\n. @nicci1771  NO,i am still confused.\n. ",
    "nicci1771": "@DaChaoXc I just met the same problem when I do training(btw, I tried to train INRIA). Have you solved this problem? Thank you in advance!\n. @DaChaoXc I just met the same problem when I do training(btw, I tried to train INRIA). Have you solved this problem? Thank you in advance!\n. ",
    "pradeepj247": "@zeyuanxy, @rbgirshick \nI have a naive question.on training on custom dataset\ni have got the installation done and got the demo.py to work successfully\ni have read about the alternating optimization also and my query is about training fast rCNN on a custom dataset.\nlet's say I have oxford flowers dataset, for which I would be training a model using AlexNet. And I want to use the power of region proposals to improve accuracy.\nSpecifially, my question is on the region proposals - since they would come from the training done on PASCAL VOC 2007 dataset. But my detections are going to be on Flowers.\nDoes it even matter? How does learning regions on a VOC dataset, help in proposing for Flowers or some other objects on a different model.\nWhat is the correct way of understanding this and resolving. Can someone pls help here?\nmany thanks\n. thanks @futurely, @zeyuanxy,\nI found the relevant documentation for creating the 3 python files as suggested and I have also created the dataset as suggested in the INRIA example. \nNow, in what sequence should I run these 3 Python files? and after running them I should call the train net ? \n. @futurely,\nthanks for responding regularly. this is exciting and it feels like am on a treasure hunt and you dropping hints to help move further along! \nbut honestly this time, i really am unable to connect the dots from the 3 python files to these scripts.\nam going to use the all_caffenet.sh but i don't quite see where the earlier mentioned python files (imdb, factory etc.) are being invoked from the scripts inside this one. and also I didn't get how to use the cfgs \n. @sunshineatnoon  thanks!\nthis blog is informative. i have one question on the dataset preparation. \nin train.txt, i see that you have the entries in the form of \"classname_image#\". So if we have 20,000 images belonging to 200 classes, then each image should be listed here in this way along with the class it belongs to? \nand about the full path to the images - i presume we mention it through Factory.py? \nthanks\n. @sunshineatnoon, just 2 more questions: \nI) in your Factory.py, there are 3 for..loops outside the functions, and they contain, the imagenet_devkit_path = '/home/xuetingli/imagenet' and towncenter_devkit_path = '/home/szy/TownCenter' and also inria_devkit_path = '/home/xuetingli/test/INRIA'. This is a bit confusing for me. if you are dealing with only one dataset (i.e imagenet), then only the imagenet should suffice? \n2) after setting up everything as mentioned by you and when I run train, i see this error: \nEnvironmentError: MATLAB command 'matlab' not found. Please add 'matlab' to your PATH.\ndo we still need MATLAB? I thought this was a pure Python implementation, especially, if we bring in our own selective search proposals and train.mat\n. Thanks @sunshineatnoon,\nI crossed all those stages -  it loaded the dataset, annotations and the caffeNet model required for training. \nBut just before starting the training, it failed with the following error: \nCheck failed: error == cudaSuccess (8 vs. 0) invalid device\nThis seems to be quite a popular error and I read quite a few posts on the forums, but couldn't resolve it. \nI am running this on AWS g2.2x and my caffe device_query call works fine. \nAny suggestions on how to get past this one? \n. @sid027 can you pls post your imagenet.py (pascal_voc.py that you modified) and also the factory.py\ni think it would be very helpful for understanding this. \n. @zeyuanxy, @rbgirshick \nI have a naive question.on training on custom dataset\ni have got the installation done and got the demo.py to work successfully\ni have read about the alternating optimization also and my query is about training fast rCNN on a custom dataset.\nlet's say I have oxford flowers dataset, for which I would be training a model using AlexNet. And I want to use the power of region proposals to improve accuracy.\nSpecifially, my question is on the region proposals - since they would come from the training done on PASCAL VOC 2007 dataset. But my detections are going to be on Flowers.\nDoes it even matter? How does learning regions on a VOC dataset, help in proposing for Flowers or some other objects on a different model.\nWhat is the correct way of understanding this and resolving. Can someone pls help here?\nmany thanks\n. thanks @futurely, @zeyuanxy,\nI found the relevant documentation for creating the 3 python files as suggested and I have also created the dataset as suggested in the INRIA example. \nNow, in what sequence should I run these 3 Python files? and after running them I should call the train net ? \n. @futurely,\nthanks for responding regularly. this is exciting and it feels like am on a treasure hunt and you dropping hints to help move further along! \nbut honestly this time, i really am unable to connect the dots from the 3 python files to these scripts.\nam going to use the all_caffenet.sh but i don't quite see where the earlier mentioned python files (imdb, factory etc.) are being invoked from the scripts inside this one. and also I didn't get how to use the cfgs \n. @sunshineatnoon  thanks!\nthis blog is informative. i have one question on the dataset preparation. \nin train.txt, i see that you have the entries in the form of \"classname_image#\". So if we have 20,000 images belonging to 200 classes, then each image should be listed here in this way along with the class it belongs to? \nand about the full path to the images - i presume we mention it through Factory.py? \nthanks\n. @sunshineatnoon, just 2 more questions: \nI) in your Factory.py, there are 3 for..loops outside the functions, and they contain, the imagenet_devkit_path = '/home/xuetingli/imagenet' and towncenter_devkit_path = '/home/szy/TownCenter' and also inria_devkit_path = '/home/xuetingli/test/INRIA'. This is a bit confusing for me. if you are dealing with only one dataset (i.e imagenet), then only the imagenet should suffice? \n2) after setting up everything as mentioned by you and when I run train, i see this error: \nEnvironmentError: MATLAB command 'matlab' not found. Please add 'matlab' to your PATH.\ndo we still need MATLAB? I thought this was a pure Python implementation, especially, if we bring in our own selective search proposals and train.mat\n. Thanks @sunshineatnoon,\nI crossed all those stages -  it loaded the dataset, annotations and the caffeNet model required for training. \nBut just before starting the training, it failed with the following error: \nCheck failed: error == cudaSuccess (8 vs. 0) invalid device\nThis seems to be quite a popular error and I read quite a few posts on the forums, but couldn't resolve it. \nI am running this on AWS g2.2x and my caffe device_query call works fine. \nAny suggestions on how to get past this one? \n. @sid027 can you pls post your imagenet.py (pascal_voc.py that you modified) and also the factory.py\ni think it would be very helpful for understanding this. \n. ",
    "KrasusC": "@DaChaoXc I met the problem you mentioned. Have you figured it out?\n. @DaChaoXc I met the problem you mentioned. Have you figured it out?\n. ",
    "leejiajun": "@DaChaoXc \n@KrasusC \n@nicci1771 \nI met the problem you mentioned. Have you figured it out?\n\"./tools/train_net.py\", line 80, in \nroidb = get_training_roidb(imdb)\nFile \"/home/xc/fast-rcnn-master/tools/../lib/fast_rcnn/train.py\", line 107, in get_training_roidb\nimdb.append_flipped_images()\nFile \"/home/xc/fast-rcnn-master/tools/../lib/datasets/imdb.py\", line 99, in append_flipped_images\nboxes = self.roidb[i]['boxes'].copy()\nFile \"/home/xc/fast-rcnn-master/tools/../lib/datasets/imdb.py\", line 63, in roidb\nself._roidb = self.roidb_handler()\nFile \"/home/xc/fast-rcnn-master/tools/../lib/datasets/inria.py\", line 116, in selective_search_roidb\nss_roidb = self._load_selective_search_roidb(gt_roidb)\nFile \"/home/xc/fast-rcnn-master/tools/../lib/datasets/inria.py\", line 138, in _load_selective_search_roidb\nreturn self.create_roidb_from_box_list(box_list, gt_roidb)\nFile \"/home/xc/fast-rcnn-master/tools/../lib/datasets/imdb.py\", line 167, in create_roidb_from_box_list\nargmaxes = gt_overlaps.argmax(axis=1)\nValueError: attempt to get argmax of an empty sequence\n. @nw362 \nnice work! I will fetch you code, thank you. \n. @ouxinyu I am also in this trouble. How to change the VOC dataset initialize? \nThank you.\n. @cuatristapr you'd better retrain a smaller model, otherwise it would run slowly.\n. @DaChaoXc \n@KrasusC \n@nicci1771 \nI met the problem you mentioned. Have you figured it out?\n\"./tools/train_net.py\", line 80, in \nroidb = get_training_roidb(imdb)\nFile \"/home/xc/fast-rcnn-master/tools/../lib/fast_rcnn/train.py\", line 107, in get_training_roidb\nimdb.append_flipped_images()\nFile \"/home/xc/fast-rcnn-master/tools/../lib/datasets/imdb.py\", line 99, in append_flipped_images\nboxes = self.roidb[i]['boxes'].copy()\nFile \"/home/xc/fast-rcnn-master/tools/../lib/datasets/imdb.py\", line 63, in roidb\nself._roidb = self.roidb_handler()\nFile \"/home/xc/fast-rcnn-master/tools/../lib/datasets/inria.py\", line 116, in selective_search_roidb\nss_roidb = self._load_selective_search_roidb(gt_roidb)\nFile \"/home/xc/fast-rcnn-master/tools/../lib/datasets/inria.py\", line 138, in _load_selective_search_roidb\nreturn self.create_roidb_from_box_list(box_list, gt_roidb)\nFile \"/home/xc/fast-rcnn-master/tools/../lib/datasets/imdb.py\", line 167, in create_roidb_from_box_list\nargmaxes = gt_overlaps.argmax(axis=1)\nValueError: attempt to get argmax of an empty sequence\n. @nw362 \nnice work! I will fetch you code, thank you. \n. @ouxinyu I am also in this trouble. How to change the VOC dataset initialize? \nThank you.\n. @cuatristapr you'd better retrain a smaller model, otherwise it would run slowly.\n. ",
    "IdiosyncraticDragon": "Editing the config file  FAST-RCNN-ROOT/lib/fast_rcnn/config.py\n\u53d1\u81ea\u6211\u7684 iPhone\n\n\u5728 2016\u5e744\u670819\u65e5\uff0c\u4e0b\u53485:35\uff0cMinaRe notifications@github.com \u5199\u9053\uff1a\nDear All,\ncan you please kindly tell me how can I change batch_size?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\n. Well, that's because of too much training data. I don't know whether it will be helpful if you have more memory size.\n\n\u53d1\u81ea\u6211\u7684 iPhone\n\n\u5728 2016\u5e744\u670819\u65e5\uff0c\u4e0b\u53488:11\uff0cMinaRe notifications@github.com \u5199\u9053\uff1a\nthanks @IdiosyncraticDragon \nActually my training stop during \"Computing bounding-box regression targets...\" ,do you have any Idea? is this problem because of memory size?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\n. @mdering hello, I am tring to train the fast rcnn on 200 DET these days, too. But I met a problem while generating the proposals for the 200 DET, it looks like some of the images in the ImageNet dataset are gray images,  and it cost the rgb2hsv function (This function is called by selective search)  to be failed. So I'm wondering did you do some converting before generating proposals by selective search? Thanks.\n. @kyuusaku @xksteven I have met the same problem, do you guys get some effective solutions?Thanks\n. What's your exact solution? I met this problem, too. And I have no idea with it.\nThank you!\n. Thanks very much! \n. I thought this may be caused by the cache file, you can delete the specific cache files for your training data under the folder fast-rcnn-master/data/cache/, and try again\n. @yileo19920925 you are welcome~~\n. You can check this answer #11 , and the link https://github.com/zeyuanxy/fast-rcnn/tree/master/help/train . \n. Editing the config file  FAST-RCNN-ROOT/lib/fast_rcnn/config.py\n\n\u53d1\u81ea\u6211\u7684 iPhone\n\n\u5728 2016\u5e744\u670819\u65e5\uff0c\u4e0b\u53485:35\uff0cMinaRe notifications@github.com \u5199\u9053\uff1a\nDear All,\ncan you please kindly tell me how can I change batch_size?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\n. Well, that's because of too much training data. I don't know whether it will be helpful if you have more memory size.\n\n\u53d1\u81ea\u6211\u7684 iPhone\n\n\u5728 2016\u5e744\u670819\u65e5\uff0c\u4e0b\u53488:11\uff0cMinaRe notifications@github.com \u5199\u9053\uff1a\nthanks @IdiosyncraticDragon \nActually my training stop during \"Computing bounding-box regression targets...\" ,do you have any Idea? is this problem because of memory size?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\n. @mdering hello, I am tring to train the fast rcnn on 200 DET these days, too. But I met a problem while generating the proposals for the 200 DET, it looks like some of the images in the ImageNet dataset are gray images,  and it cost the rgb2hsv function (This function is called by selective search)  to be failed. So I'm wondering did you do some converting before generating proposals by selective search? Thanks.\n. @kyuusaku @xksteven I have met the same problem, do you guys get some effective solutions?Thanks\n. What's your exact solution? I met this problem, too. And I have no idea with it.\nThank you!\n. Thanks very much! \n. I thought this may be caused by the cache file, you can delete the specific cache files for your training data under the folder fast-rcnn-master/data/cache/, and try again\n. @yileo19920925 you are welcome~~\n. You can check this answer #11 , and the link https://github.com/zeyuanxy/fast-rcnn/tree/master/help/train . \n. \n",
    "tzutalin": "Yes, I encounter the sample question.\nI change the mean as bellow:\n104.00698793,  116.66876762,  122.67891434]\n. Yes, I encounter the sample question.\nI change the mean as bellow:\n104.00698793,  116.66876762,  122.67891434]\n. ",
    "rosenfeldamir": "Nevermind, I solved it through the net.blob interface. \nthanks,\n-Amir\n. Nevermind, I solved it through the net.blob interface. \nthanks,\n-Amir\n. ",
    "WilsonWangTHU": "Hi tamirru,\nthe background class is necessary.\nThe fast rcnn has a number of region proposals as the input for each image, so you need a \"background\" class to denote the proposals that have \"nothing\" in the region (remember only a few of them have object in it).\nI strongly recommend you read the orginal paper here: http://arxiv.org/abs/1504.08083\n. It's in the ./lib/fast-rcnn/config.py\n. @xksteven  I guess you would like to do the validation during the training?\nI am not sure whether that's supported by the current fast-rcnn edition, as all the forward job is started from the python part and I don't think we have a testing function during the training for now. \nI am afraid in that way you might need to revise the code yourself.\n. @deartonym  you must rename at least the layers that has different shape, e.g.,  the layer 'bbox_pred' and 'cls_score'.\nAnd them you could finetune a new one.\nI suggest you finetune from the original caffenet, instead of the fast-rcnn caffenet model. \nIn that case you could simply change the \n{layer 'data': param_str: \"'num_classes': 2\"\nlayer 'cls_score': \"num_output: 2\",\nlayer 'bbox_pred': \"num_output: 8\". }\nand then start the finetuning.\n. Hi tamirru,\nthe background class is necessary.\nThe fast rcnn has a number of region proposals as the input for each image, so you need a \"background\" class to denote the proposals that have \"nothing\" in the region (remember only a few of them have object in it).\nI strongly recommend you read the orginal paper here: http://arxiv.org/abs/1504.08083\n. It's in the ./lib/fast-rcnn/config.py\n. @xksteven  I guess you would like to do the validation during the training?\nI am not sure whether that's supported by the current fast-rcnn edition, as all the forward job is started from the python part and I don't think we have a testing function during the training for now. \nI am afraid in that way you might need to revise the code yourself.\n. @deartonym  you must rename at least the layers that has different shape, e.g.,  the layer 'bbox_pred' and 'cls_score'.\nAnd them you could finetune a new one.\nI suggest you finetune from the original caffenet, instead of the fast-rcnn caffenet model. \nIn that case you could simply change the \n{layer 'data': param_str: \"'num_classes': 2\"\nlayer 'cls_score': \"num_output: 2\",\nlayer 'bbox_pred': \"num_output: 8\". }\nand then start the finetuning.\n. ",
    "jeanvbean": "Hi, you can use labelImg for the annotation of your own datasets. The output file are in line with the format of VOC2007, and can be directly used for input. The labelImg can be downloaded from here: https://github.com/tzutalin/labelImg. Hi, you can use labelImg for the annotation of your own datasets. The output file are in line with the format of VOC2007, and can be directly used for input. The labelImg can be downloaded from here: https://github.com/tzutalin/labelImg. ",
    "ouxinyu": "I change the boxsize and sigma, the proposals is change, but the result is worse than 2880 proposals for image 000004.jpg\n. @shicai I checked my files and followed the introduction to set all my files. train_net.py,compress.py, demo.py is well down. So I don;t konow how to fixed it\n. sorry, I forgotten change the VOC dataset initialize. I close this comment.\n. I change the boxsize and sigma, the proposals is change, but the result is worse than 2880 proposals for image 000004.jpg\n. @shicai I checked my files and followed the introduction to set all my files. train_net.py,compress.py, demo.py is well down. So I don;t konow how to fixed it\n. sorry, I forgotten change the VOC dataset initialize. I close this comment.\n. ",
    "GeorgiAngelov": "@rbgirshick, Do you know when the code will be released for the methods proposed in the paper you referred to? I am looking forward to this new setup of RPN + Fast-RCNN !\nP.S. A big THANK YOU for fast-rcnn and all of your hard work!\n. @rbgirshick, Do you know when the code will be released for the methods proposed in the paper you referred to? I am looking forward to this new setup of RPN + Fast-RCNN !\nP.S. A big THANK YOU for fast-rcnn and all of your hard work!\n. ",
    "deartonym": "I should read this issue earlier. I found the same problem, that their result for the 2880 proposals can hardly be reproduced (I tried many different parameters). I think we should not spend more time on this issue, and we can try other proposal methods as well.\n. I think I figure it out in some sense: I have to change the number of class and corresponding parameters in the 'train.txtproto'. \ne.g., for Caffe_net: \nlayer 'data': param_str: \"'num_classes': 2\" \nlayer 'cls_score': \"num_output: 2\", \nlayer 'bbox_pred': \"num_output: 8\". \nHowever, the problem is that if we change the structure of the network, it seems that we cannot use the 21-class PASCAL pre-trained network for fine-tuning anymore.\nAnyone has idea about this? How to make fine-tuning to work if given different number of classes?\nBest,\n. @WilsonWangTHU Thanks for your reply! Everything works fine according to your suggestion.\n. Hi @parhartanvir ,\nPlease check @WilsonWangTHU 's answer. You should change the number of class, number of output of both 'class score' layer and 'bounding box prediction' layer according to your class number. These parameters are set in the .prototxt.\nFurther, you also need to change the class names in the python code. I don't quite remember the name of the .py file. It should be some factory.py or pascal_voc.py. Just follow the code when debugging. I'm sure you can find it.\nBest,\n. I should read this issue earlier. I found the same problem, that their result for the 2880 proposals can hardly be reproduced (I tried many different parameters). I think we should not spend more time on this issue, and we can try other proposal methods as well.\n. I think I figure it out in some sense: I have to change the number of class and corresponding parameters in the 'train.txtproto'. \ne.g., for Caffe_net: \nlayer 'data': param_str: \"'num_classes': 2\" \nlayer 'cls_score': \"num_output: 2\", \nlayer 'bbox_pred': \"num_output: 8\". \nHowever, the problem is that if we change the structure of the network, it seems that we cannot use the 21-class PASCAL pre-trained network for fine-tuning anymore.\nAnyone has idea about this? How to make fine-tuning to work if given different number of classes?\nBest,\n. @WilsonWangTHU Thanks for your reply! Everything works fine according to your suggestion.\n. Hi @parhartanvir ,\nPlease check @WilsonWangTHU 's answer. You should change the number of class, number of output of both 'class score' layer and 'bounding box prediction' layer according to your class number. These parameters are set in the .prototxt.\nFurther, you also need to change the class names in the python code. I don't quite remember the name of the .py file. It should be some factory.py or pascal_voc.py. Just follow the code when debugging. I'm sure you can find it.\nBest,\n. ",
    "yao5461": "@shicai, I also met the same problem. Maybe you can try this way(matlab):\n1> Generate object proposals for the image, such as: boxes_tmp = selective_search_boxes(im);\n     the selective search matlab code: https://github.com/rbgirshick/rcnn/blob/master/selective_search\n2> Change the format: boxes = boxes_tmp(:, [2 1 4 3]) - 1;\n3> Save the boxes into a mat file and test it.\nI think more cars will be detected. \nFrom the source code, I found:\na. The format of proposals in demo mat file is: [left, top, right, bottom], 0-based index.\n    Line 86-86: https://github.com/rbgirshick/fast-rcnn/blob/master/lib/fast_rcnn/test.py\nb. The format of proposals produced by matlab code is: [top, left, bottom, right], 1-based index.\n    Line 145: https://github.com/rbgirshick/fast-rcnn/blob/master/lib/datasets/pascal_voc.py\nThe code for training changes the format of proposals to [left, top, right, bottom], 0-based index, but the test code doesn't. I guess it is the reason. Hope these will be helpful.\n. @shicai, I also met the same problem. Maybe you can try this way(matlab):\n1> Generate object proposals for the image, such as: boxes_tmp = selective_search_boxes(im);\n     the selective search matlab code: https://github.com/rbgirshick/rcnn/blob/master/selective_search\n2> Change the format: boxes = boxes_tmp(:, [2 1 4 3]) - 1;\n3> Save the boxes into a mat file and test it.\nI think more cars will be detected. \nFrom the source code, I found:\na. The format of proposals in demo mat file is: [left, top, right, bottom], 0-based index.\n    Line 86-86: https://github.com/rbgirshick/fast-rcnn/blob/master/lib/fast_rcnn/test.py\nb. The format of proposals produced by matlab code is: [top, left, bottom, right], 1-based index.\n    Line 145: https://github.com/rbgirshick/fast-rcnn/blob/master/lib/datasets/pascal_voc.py\nThe code for training changes the format of proposals to [left, top, right, bottom], 0-based index, but the test code doesn't. I guess it is the reason. Hope these will be helpful.\n. ",
    "shicai": "thanks @rbgirshick for your great job.\nbtw, when will you release the faster rcnn code?\nwe are looking forward to trying it:)\nthanks @yao5461, it's very helpful.\nrow major vs column major, and 0-based index vs 1-based major\nthat's the difference between python/c++ and matlab.\n. seems that file could not be found.\ncheck your file path\n. thanks @rbgirshick for your great job.\nbtw, when will you release the faster rcnn code?\nwe are looking forward to trying it:)\nthanks @yao5461, it's very helpful.\nrow major vs column major, and 0-based index vs 1-based major\nthat's the difference between python/c++ and matlab.\n. seems that file could not be found.\ncheck your file path\n. ",
    "345ishaan": "Hi Mina,\nDid your issue got resolved?. Hi Mina,\nDid your issue got resolved?. ",
    "vdksoda": "@mdering I noticed that you closed this issue. I was wondering if you figured out why this is happening? Any help would be appreciated. Thanks.\n. Drat. I wished this was not the situation but I too arrived at the same conclusion. I guess this means that the pre-trained Imagenet models with fast-rcnn are only trained over pascal voc object classes. Not all 200 Imagenet classes.\nThanks for your response! Much Appreciated.\n. Cheers. I will take a look.\n. @mdering I noticed that you closed this issue. I was wondering if you figured out why this is happening? Any help would be appreciated. Thanks.\n. Drat. I wished this was not the situation but I too arrived at the same conclusion. I guess this means that the pre-trained Imagenet models with fast-rcnn are only trained over pascal voc object classes. Not all 200 Imagenet classes.\nThanks for your response! Much Appreciated.\n. Cheers. I will take a look.\n. ",
    "mdering": "The models this software ships with have their final layer trained on the 20 +1 classes of the VOC challenge, if you wish to get different classes you must retrain it. I think thats the issue\n. I managed to get them trained on the 200 DET classes eventually, i will post some code in the coming weeks that helped me do it.\n. in the meantime, this group seems to have the right idea, though they're training on the INRIA dataset https://github.com/EdisonResearch/fast-rcnn\n. Thats a good question, basically no.\nThe short answer is that file index just needs to contain file names, so having a row like n03636649/n03636649_8522 will look for a file called n03636649_8522.JPEG in a folder called n03636649 under the images folder with minimal tweaking. That's perfectly fine. I'm going to try to get a fork up this week with at least my imagenet.py, which can do training, but doesn't do validation or testing just yet.\nThe only other thing you need to be aware of is that not all of the images in the training files actually contain an annotated box, which is a problem. also one of the annotations is incorrect, in that its xmin is greater than its xmax. \n. cudnn needs CUDA compute 3.0, so you can't use cudnn with that card\n. You might need to reboot if the drivers have updated recently\n. Right, I'm not sure what I did, but I think this is what fixed it for me. \n. The models this software ships with have their final layer trained on the 20 +1 classes of the VOC challenge, if you wish to get different classes you must retrain it. I think thats the issue\n. I managed to get them trained on the 200 DET classes eventually, i will post some code in the coming weeks that helped me do it.\n. in the meantime, this group seems to have the right idea, though they're training on the INRIA dataset https://github.com/EdisonResearch/fast-rcnn\n. Thats a good question, basically no.\nThe short answer is that file index just needs to contain file names, so having a row like n03636649/n03636649_8522 will look for a file called n03636649_8522.JPEG in a folder called n03636649 under the images folder with minimal tweaking. That's perfectly fine. I'm going to try to get a fork up this week with at least my imagenet.py, which can do training, but doesn't do validation or testing just yet.\nThe only other thing you need to be aware of is that not all of the images in the training files actually contain an annotated box, which is a problem. also one of the annotations is incorrect, in that its xmin is greater than its xmax. \n. cudnn needs CUDA compute 3.0, so you can't use cudnn with that card\n. You might need to reboot if the drivers have updated recently\n. Right, I'm not sure what I did, but I think this is what fixed it for me. \n. ",
    "liuchang8am": "Hi, @mdering ,  I'm trying to train on 200 DET too, did you re-organize the images and the annotations for 200 DET in the way that https://github.com/EdisonResearch/fast-rcnn/tree/master/help/train mentioned?  That is, to copy all the images into one \"image\" folder and all the annotations in one \"annotation\" folder?Thanks.\n. please check https://github.com/rbgirshick/py-faster-rcnn/tree/master/models\n. Hi, @mdering ,  I'm trying to train on 200 DET too, did you re-organize the images and the annotations for 200 DET in the way that https://github.com/EdisonResearch/fast-rcnn/tree/master/help/train mentioned?  That is, to copy all the images into one \"image\" folder and all the annotations in one \"annotation\" folder?Thanks.\n. please check https://github.com/rbgirshick/py-faster-rcnn/tree/master/models\n. ",
    "vvkdby": "In VOCCode, inside the VOCdevkit folder, there will be a file called VOCinit. \nIn that, on around line 31, change the VOC.testset to 'test' instead of 'val'\n. In VOCCode, inside the VOCdevkit folder, there will be a file called VOCinit.m . \nIn that, on around line 31, change the VOC.testset to 'test' instead of 'val'\n. In VOCCode, inside the VOCdevkit folder, there will be a file called VOCinit. \nIn that, on around line 31, change the VOC.testset to 'test' instead of 'val'\n. In VOCCode, inside the VOCdevkit folder, there will be a file called VOCinit.m . \nIn that, on around line 31, change the VOC.testset to 'test' instead of 'val'\n. ",
    "asbroad": "Hi Christopher,\nTo run the webcam demo you need two things in addition to Rob's fast-rcnn, (1) the webcam.py file and (2) Dlib.  To get the webcam file, you can get this pull request by either following these instructions (https://help.github.com/articles/checking-out-pull-requests-locally/) or, maybe more simply, by cloning my forked repository on my github page (the only differences are the webcam file and added information in the readme about how to run it).  If you're having trouble installing Dlib, be sure to consult http://dlib.net/compile.html - but it's an extremely easy to use library.\nOh, and you'll also need a webcam, but I assume that is obvious :)\nHope that helps,\nAlex\n. Hey Christopher,\nSorry for the delay in response, the scaling factor simply scales the webcam image.  So if your webcam produces an 640x480 image, a scaling factor of 2 will produce an image scaled down by that factor in each dimension (i.e. the image that is processed will be 320x240).  If you have any more specific questions, feel free to message me directly.\n- Alex\n. Hey @smajida , I haven't looked at the Faster RCNN code much, but from what I remember about the paper, they replace the selective search region proposal method with another neural net (a region proposal net) so you shouldn't need to use any of the additions from my pull request (this was just a work around to not use matlab and to just take the image from a computer's webcam).   Take a look at their github repo (https://github.com/rbgirshick/py-faster-rcnn) as it has a python implementation and a link to a matlab implementation - hope that helps and good luck!\n. @arushk1 yes, that is correct.  the pascal voc 2007 dataset includes the following 20 classes\nPerson: person\nAnimal: bird, cat, cow, dog, horse, sheep\nVehicle: aeroplane, bicycle, boat, bus, car, motorbike, train\nIndoor: bottle, chair, dining table, potted plant, sofa, tv/monitor\n. Hi Christopher,\nTo run the webcam demo you need two things in addition to Rob's fast-rcnn, (1) the webcam.py file and (2) Dlib.  To get the webcam file, you can get this pull request by either following these instructions (https://help.github.com/articles/checking-out-pull-requests-locally/) or, maybe more simply, by cloning my forked repository on my github page (the only differences are the webcam file and added information in the readme about how to run it).  If you're having trouble installing Dlib, be sure to consult http://dlib.net/compile.html - but it's an extremely easy to use library.\nOh, and you'll also need a webcam, but I assume that is obvious :)\nHope that helps,\nAlex\n. Hey Christopher,\nSorry for the delay in response, the scaling factor simply scales the webcam image.  So if your webcam produces an 640x480 image, a scaling factor of 2 will produce an image scaled down by that factor in each dimension (i.e. the image that is processed will be 320x240).  If you have any more specific questions, feel free to message me directly.\n- Alex\n. Hey @smajida , I haven't looked at the Faster RCNN code much, but from what I remember about the paper, they replace the selective search region proposal method with another neural net (a region proposal net) so you shouldn't need to use any of the additions from my pull request (this was just a work around to not use matlab and to just take the image from a computer's webcam).   Take a look at their github repo (https://github.com/rbgirshick/py-faster-rcnn) as it has a python implementation and a link to a matlab implementation - hope that helps and good luck!\n. @arushk1 yes, that is correct.  the pascal voc 2007 dataset includes the following 20 classes\nPerson: person\nAnimal: bird, cat, cow, dog, horse, sheep\nVehicle: aeroplane, bicycle, boat, bus, car, motorbike, train\nIndoor: bottle, chair, dining table, potted plant, sofa, tv/monitor\n. ",
    "arushk1": "So if I replace 'person' in your code to say 'dog'. It should detect it right?\n. So if I replace 'person' in your code to say 'dog'. It should detect it right?\n. ",
    "kesonyk": "@cuatristapr \nHi  I am now run the fast-rcnn on  jetson tk1.But when I cd $FRCN_ROOT/lib do make\nit appears  'arm-linux-gnueabihf-gcc' failed with exit status 1 \ndo you have this problem\n. I solve it\n. @cuatristapr \nHi ,I also use jetson tk1 to do fasr-rcnn but the given pretrained net is too big ,do you have a small net ,thx\n. @cuatristapr \nHi  I am now run the fast-rcnn on  jetson tk1.But when I cd $FRCN_ROOT/lib do make\nit appears  'arm-linux-gnueabihf-gcc' failed with exit status 1 \ndo you have this problem\n. I solve it\n. @cuatristapr \nHi ,I also use jetson tk1 to do fasr-rcnn but the given pretrained net is too big ,do you have a small net ,thx\n. ",
    "tsaiJN": "@kesonyk \nHi, I'm also trying to run human detection on Jetson TK1. If I run it in cpu mode, it works perfectly fine (very slow though), but whenever I run it in gpu mode, it always get \"killed\". I also guess this is the gpu out of memory issue, have you manage to solve it ?\n. @kesonyk \nHi, I'm also trying to run human detection on Jetson TK1. If I run it in cpu mode, it works perfectly fine (very slow though), but whenever I run it in gpu mode, it always get \"killed\". I also guess this is the gpu out of memory issue, have you manage to solve it ?\n. ",
    "miyamon11": "Hi, I'm trying to run your webcam.py, but I can not run.\nI changed the caffemodel and rewrite your code. When I run this script, an error is happened as below\nTraceback (most recent call last):\n  File \"realtime2.py\", line 150, in <module>\n    [im2, cls, dets, CONF_THRESH] = demo(net, frame, scale_factor, ('person',))\n  File \"realtime2.py\", line 87, in demo\n    scores, boxes = im_detect(net, im2, obj_proposals)\n  File \"/home/keisan/py-faster-rcnn/tools/../lib/fast_rcnn/test.py\", line 154, in im_detect\n    blobs_out = net.forward(**forward_kwargs)\n  File \"/home/keisan/py-faster-rcnn/tools/../caffe-fast-rcnn/python/caffe/pycaffe.py\", line 97, in _Net_forward\n    raise Exception('Input blob arguments do not match net inputs.')\nException: Input blob arguments do not match net inputs.\nI can not understand what is happened. What should I do?\nThe rewrited code is blow\nNETS = {'vgg16': ('VGG16',\n                  'VGG16_faster_rcnn_final.caffemodel'),\n        'zf': ('ZF',\n                    'ZF_faster_rcnn_final.caffemodel')}\nand\nprototxt = os.path.join(cfg.MODELS_DIR, NETS[args.demo_net][0], \n                        'faster_rcnn_alt_opt', 'faster_rcnn_test.pt')\ncaffemodel = os.path.join(cfg.DATA_DIR, 'faster_rcnn_models',\n                          NETS[args.demo_net][1])`\n. thank you for your reply,takemegod.\nI think this problem is not file connect  problem. I don't use Japanese as folder names at all.\nin fact, \ncfg.MODELS_DIR = ~/py-faster-rcnn/models/pascal_voc\nand\ncfg.DATA_DIR = ~/py-faster-rcnn/data\nI think that the cause may be in utils/cython_nms.py. I changed this module from utils/cython_nms.py to fast_rcnn/nms_wrapper.py .\nand the the error resource is below(~/py-faster-rcnn/caffe-fast-rcnn/python/caffe)\n`def _Net_forward(self, blobs=None, start=None, end=None, **kwargs):\n    \"\"\"\n    Forward pass: prepare inputs and run the net forward.\n```\nParameters\n\nblobs : list of blobs to return in addition to output blobs.\nkwargs : Keys are input blob names and values are blob ndarrays.\n         For formatting inputs for Caffe, see Net.preprocess().\n         If None, input is taken from data layers.\nstart : optional name of layer at which to begin the forward pass\nend : optional name of layer at which to finish the forward pass\n      (inclusive)\nReturns\nouts : {blob name: blob ndarray} dict.\n\"\"\"\nif blobs is None:\n    blobs = []\nif start is not None:\n    start_ind = list(self._layer_names).index(start)\nelse:\n    start_ind = 0\nif end is not None:\n    end_ind = list(self._layer_names).index(end)\n    outputs = set([end] + blobs)\nelse:\n    end_ind = len(self.layers) - 1\n    outputs = set(self.outputs + blobs)\nif kwargs:\n    if set(kwargs.keys()) != set(self.inputs):\n        raise Exception('Input blob arguments do not match net inputs.')\n    # Set input according to defined shapes and make arrays single and\n    # C-contiguous as Caffe expects.\n    for in_, blob in kwargs.iteritems():\n        if blob.shape[0] != self.blobs[in_].num:\n            raise Exception('Input is not batch sized')\n        self.blobs[in_].data[...] = blob\nself._forward(start_ind, end_ind)\nUnpack blobs to extract\nreturn {out: self.blobs[out].data for out in outputs}\n```\n`\nThis _Net_inputs function is called from im_detect() function in test.py.\nim_detect() function is below,\n`def im_detect(net, im, boxes=None):\n    \"\"\"Detect object classes in an image given object proposals.\n```\nArguments:\n    net (caffe.Net): Fast R-CNN network to use\n    im (ndarray): color image to test (in BGR order)\n    boxes (ndarray): R x 4 array of object proposals or None (for RPN)\nReturns:\n    scores (ndarray): R x K array of object class scores (K includes\n        background as object category 0)\n    boxes (ndarray): R x (4*K) array of predicted bounding boxes\n\"\"\"\nblobs, im_scales = _get_blobs(im, boxes)\nWhen mapping from image ROIs to feature map ROIs, there's some aliasing\n(some distinct image ROIs get mapped to the same feature ROI).\nHere, we identify duplicate feature ROIs, so we only compute features\non the unique subset.\nif cfg.DEDUP_BOXES > 0 and not cfg.TEST.HAS_RPN:\n    v = np.array([1, 1e3, 1e6, 1e9, 1e12])\n    hashes = np.round(blobs['rois'] * cfg.DEDUP_BOXES).dot(v)\n    _, index, inv_index = np.unique(hashes, return_index=True,\n                                    return_inverse=True)\n    blobs['rois'] = blobs['rois'][index, :]\n    boxes = boxes[index, :]\nif cfg.TEST.HAS_RPN:\n    im_blob = blobs['data']\n    blobs['im_info'] = np.array(\n        [[im_blob.shape[2], im_blob.shape[3], im_scales[0]]],\n        dtype=np.float32)\nreshape network inputs\nnet.blobs['data'].reshape((blobs['data'].shape))\nif cfg.TEST.HAS_RPN:\n    net.blobs['im_info'].reshape((blobs['im_info'].shape))\nelse:\n    net.blobs['rois'].reshape(*(blobs['rois'].shape))\ndo forward\nforward_kwargs = {'data': blobs['data'].astype(np.float32, copy=False)}\nif cfg.TEST.HAS_RPN:\n    forward_kwargs['im_info'] = blobs['im_info'].astype(np.float32, copy=False)\nelse:\n    forward_kwargs['rois'] = blobs['rois'].astype(np.float32, copy=False)\nblobs_out = net.forward(**forward_kwargs)\nif cfg.TEST.HAS_RPN:\n    assert len(im_scales) == 1, \"Only single-image batch implemented\"\n    rois = net.blobs['rois'].data.copy()\n    # unscale back to raw image space\n    boxes = rois[:, 1:5] / im_scales[0]\nif cfg.TEST.SVM:\n    # use the raw scores before softmax under the assumption they\n    # were trained as linear SVMs\n    scores = net.blobs['cls_score'].data\nelse:\n    # use softmax estimated probabilities\n    scores = blobs_out['cls_prob']\nif cfg.TEST.BBOX_REG:\n    # Apply bounding-box regression deltas\n    box_deltas = blobs_out['bbox_pred']\n    pred_boxes = bbox_transform_inv(boxes, box_deltas)\n    pred_boxes = clip_boxes(pred_boxes, im.shape)\nelse:\n    # Simply repeat the boxes, once for each class\n    pred_boxes = np.tile(boxes, (1, scores.shape[1]))\nif cfg.DEDUP_BOXES > 0 and not cfg.TEST.HAS_RPN:\n    # Map scores and predictions back to the original set of boxes\n    scores = scores[inv_index, :]\n    pred_boxes = pred_boxes[inv_index, :]\nreturn scores, pred_boxes\n```\n`\nWhat should I do?\nI'll appreciate for all tips, Thanks!\n. Hi, I'm trying to run your webcam.py, but I can not run.\nI changed the caffemodel and rewrite your code. When I run this script, an error is happened as below\nTraceback (most recent call last):\n  File \"realtime2.py\", line 150, in <module>\n    [im2, cls, dets, CONF_THRESH] = demo(net, frame, scale_factor, ('person',))\n  File \"realtime2.py\", line 87, in demo\n    scores, boxes = im_detect(net, im2, obj_proposals)\n  File \"/home/keisan/py-faster-rcnn/tools/../lib/fast_rcnn/test.py\", line 154, in im_detect\n    blobs_out = net.forward(**forward_kwargs)\n  File \"/home/keisan/py-faster-rcnn/tools/../caffe-fast-rcnn/python/caffe/pycaffe.py\", line 97, in _Net_forward\n    raise Exception('Input blob arguments do not match net inputs.')\nException: Input blob arguments do not match net inputs.\nI can not understand what is happened. What should I do?\nThe rewrited code is blow\nNETS = {'vgg16': ('VGG16',\n                  'VGG16_faster_rcnn_final.caffemodel'),\n        'zf': ('ZF',\n                    'ZF_faster_rcnn_final.caffemodel')}\nand\nprototxt = os.path.join(cfg.MODELS_DIR, NETS[args.demo_net][0], \n                        'faster_rcnn_alt_opt', 'faster_rcnn_test.pt')\ncaffemodel = os.path.join(cfg.DATA_DIR, 'faster_rcnn_models',\n                          NETS[args.demo_net][1])`\n. thank you for your reply,takemegod.\nI think this problem is not file connect  problem. I don't use Japanese as folder names at all.\nin fact, \ncfg.MODELS_DIR = ~/py-faster-rcnn/models/pascal_voc\nand\ncfg.DATA_DIR = ~/py-faster-rcnn/data\nI think that the cause may be in utils/cython_nms.py. I changed this module from utils/cython_nms.py to fast_rcnn/nms_wrapper.py .\nand the the error resource is below(~/py-faster-rcnn/caffe-fast-rcnn/python/caffe)\n`def _Net_forward(self, blobs=None, start=None, end=None, **kwargs):\n    \"\"\"\n    Forward pass: prepare inputs and run the net forward.\n```\nParameters\n\nblobs : list of blobs to return in addition to output blobs.\nkwargs : Keys are input blob names and values are blob ndarrays.\n         For formatting inputs for Caffe, see Net.preprocess().\n         If None, input is taken from data layers.\nstart : optional name of layer at which to begin the forward pass\nend : optional name of layer at which to finish the forward pass\n      (inclusive)\nReturns\nouts : {blob name: blob ndarray} dict.\n\"\"\"\nif blobs is None:\n    blobs = []\nif start is not None:\n    start_ind = list(self._layer_names).index(start)\nelse:\n    start_ind = 0\nif end is not None:\n    end_ind = list(self._layer_names).index(end)\n    outputs = set([end] + blobs)\nelse:\n    end_ind = len(self.layers) - 1\n    outputs = set(self.outputs + blobs)\nif kwargs:\n    if set(kwargs.keys()) != set(self.inputs):\n        raise Exception('Input blob arguments do not match net inputs.')\n    # Set input according to defined shapes and make arrays single and\n    # C-contiguous as Caffe expects.\n    for in_, blob in kwargs.iteritems():\n        if blob.shape[0] != self.blobs[in_].num:\n            raise Exception('Input is not batch sized')\n        self.blobs[in_].data[...] = blob\nself._forward(start_ind, end_ind)\nUnpack blobs to extract\nreturn {out: self.blobs[out].data for out in outputs}\n```\n`\nThis _Net_inputs function is called from im_detect() function in test.py.\nim_detect() function is below,\n`def im_detect(net, im, boxes=None):\n    \"\"\"Detect object classes in an image given object proposals.\n```\nArguments:\n    net (caffe.Net): Fast R-CNN network to use\n    im (ndarray): color image to test (in BGR order)\n    boxes (ndarray): R x 4 array of object proposals or None (for RPN)\nReturns:\n    scores (ndarray): R x K array of object class scores (K includes\n        background as object category 0)\n    boxes (ndarray): R x (4*K) array of predicted bounding boxes\n\"\"\"\nblobs, im_scales = _get_blobs(im, boxes)\nWhen mapping from image ROIs to feature map ROIs, there's some aliasing\n(some distinct image ROIs get mapped to the same feature ROI).\nHere, we identify duplicate feature ROIs, so we only compute features\non the unique subset.\nif cfg.DEDUP_BOXES > 0 and not cfg.TEST.HAS_RPN:\n    v = np.array([1, 1e3, 1e6, 1e9, 1e12])\n    hashes = np.round(blobs['rois'] * cfg.DEDUP_BOXES).dot(v)\n    _, index, inv_index = np.unique(hashes, return_index=True,\n                                    return_inverse=True)\n    blobs['rois'] = blobs['rois'][index, :]\n    boxes = boxes[index, :]\nif cfg.TEST.HAS_RPN:\n    im_blob = blobs['data']\n    blobs['im_info'] = np.array(\n        [[im_blob.shape[2], im_blob.shape[3], im_scales[0]]],\n        dtype=np.float32)\nreshape network inputs\nnet.blobs['data'].reshape((blobs['data'].shape))\nif cfg.TEST.HAS_RPN:\n    net.blobs['im_info'].reshape((blobs['im_info'].shape))\nelse:\n    net.blobs['rois'].reshape(*(blobs['rois'].shape))\ndo forward\nforward_kwargs = {'data': blobs['data'].astype(np.float32, copy=False)}\nif cfg.TEST.HAS_RPN:\n    forward_kwargs['im_info'] = blobs['im_info'].astype(np.float32, copy=False)\nelse:\n    forward_kwargs['rois'] = blobs['rois'].astype(np.float32, copy=False)\nblobs_out = net.forward(**forward_kwargs)\nif cfg.TEST.HAS_RPN:\n    assert len(im_scales) == 1, \"Only single-image batch implemented\"\n    rois = net.blobs['rois'].data.copy()\n    # unscale back to raw image space\n    boxes = rois[:, 1:5] / im_scales[0]\nif cfg.TEST.SVM:\n    # use the raw scores before softmax under the assumption they\n    # were trained as linear SVMs\n    scores = net.blobs['cls_score'].data\nelse:\n    # use softmax estimated probabilities\n    scores = blobs_out['cls_prob']\nif cfg.TEST.BBOX_REG:\n    # Apply bounding-box regression deltas\n    box_deltas = blobs_out['bbox_pred']\n    pred_boxes = bbox_transform_inv(boxes, box_deltas)\n    pred_boxes = clip_boxes(pred_boxes, im.shape)\nelse:\n    # Simply repeat the boxes, once for each class\n    pred_boxes = np.tile(boxes, (1, scores.shape[1]))\nif cfg.DEDUP_BOXES > 0 and not cfg.TEST.HAS_RPN:\n    # Map scores and predictions back to the original set of boxes\n    scores = scores[inv_index, :]\n    pred_boxes = pred_boxes[inv_index, :]\nreturn scores, pred_boxes\n```\n`\nWhat should I do?\nI'll appreciate for all tips, Thanks!\n. ",
    "takemegod": "it's look like file connect problem if you use Japanese or not English directory\nmaybe happed like folder file not find problem. and what about change folder file directory short?\nexception bug will be fix.\n\n\ubcf4\ub0b8 \uc0ac\ub78c: miyamon11 notifications@github.com\n\ubcf4\ub0b8 \ub0a0\uc9dc: 2016\ub144 10\uc6d4 13\uc77c \ubaa9\uc694\uc77c \uc624\uc804 8:40\n\ubc1b\ub294 \uc0ac\ub78c: rbgirshick/fast-rcnn\n\uc81c\ubaa9: Re: [rbgirshick/fast-rcnn] Webcam Demo (#29)\nHi, I'm trying to run your webcam.py, but I can not run.\nI changed the caffemodel and rewrite your code. When I run this script, an error is happened as below\nTraceback (most recent call last):\nFile \"realtime2.py\", line 150, in \n[im2, cls, dets, CONF_THRESH] = demo(net, frame, scale_factor, ('person',))\nFile \"realtime2.py\", line 87, in demo\nscores, boxes = im_detect(net, im2, obj_proposals)\nFile \"/home/keisan/py-faster-rcnn/tools/../lib/fast_rcnn/test.py\", line 154, in im_detect\nblobs_out = net.forward(**forward_kwargs)\nFile \"/home/keisan/py-faster-rcnn/tools/../caffe-fast-rcnn/python/caffe/pycaffe.py\", line 97, in _Net_forward\nraise Exception('Input blob arguments do not match net inputs.')\nException: Input blob arguments do not match net inputs.\nI can not understand what is happened. What should I do?\nThe rewrited code is blow\n`#!/usr/bin/env python\n-- coding: utf-8 --\nFast R-CNN\nCopyright (c) 2015 Microsoft\nLicensed under The MIT License [see LICENSE for details]\nWritten by Ross Girshick\n\"\"\"\nDemo script showing detections in sample images.\nSee README.md for installation instructions before running.\n\"\"\"\nimport _init_paths\nfrom fast_rcnn.config import cfg\nfrom fast_rcnn.test import im_detect\nfrom fast_rcnn.nms_wrapper import nms#\u3053\u3053\u304c\u9055\u3046\u5143\u306ffaster_rcnn.nms_wrapper\nfrom utils.timer import Timer\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.io as sio\nimport caffe, os, cv2 #sys\u304c\u306a\u3044\nimport argparse\nimport dlib #\u65b0\u3057\u3044\u30e2\u30b8\u30e5\u30fc\u30eb\nCLASSES = ('background',\n'aeroplane', 'bicycle', 'bird', 'boat',\n'bottle', 'bus', 'car', 'cat', 'chair',\n'cow', 'diningtable', 'dog', 'horse',\n'motorbike', 'person', 'pottedplant',\n'sheep', 'sofa', 'train', 'tvmonitor')\nNETS = {'vgg16': ('VGG16',\n'VGG16_faster_rcnn_final.caffemodel'),\n'zf': ('ZF',\n'ZF_faster_rcnn_final.caffemodel')}\ndef vis_detections(im, class_name, dets, thresh=0.5):\n\"\"\"Draw detected bounding boxes.\u3053\u306e\u5b9a\u7fa9\u306f\u3059\u3079\u3066demo.py\u3068\u4e00\u7dd2\"\"\"\ninds = np.where(dets[:, -1] >= thresh)[0]\nif len(inds) == 0:\nreturn\nim = im[:, :, (2, 1, 0)]\nfig, ax = plt.subplots(figsize=(12, 12))\nax.imshow(im, aspect='equal')\nfor i in inds:\n    bbox = dets[i, :4]\n    score = dets[i, -1]\nax.add_patch(\n    plt.Rectangle((bbox[0], bbox[1]),\n                  bbox[2] - bbox[0],\n                  bbox[3] - bbox[1], fill=False,\n                  edgecolor='red', linewidth=3.5)\n    )\nax.text(bbox[0], bbox[1] - 2,\n        '{:s} {:.3f}'.format(class_name, score),\n        bbox=dict(facecolor='blue', alpha=0.5),\n        fontsize=14, color='white')\nax.set_title(('{} detections with '\n              'p({} | box) >= {:.1f}').format(class_name, class_name,\n                                              thresh),\n              fontsize=14)\nplt.axis('off')\nplt.tight_layout()\nplt.draw()\ndef demo(net, im, scale_factor, classes):\n\"\"\"Detect object classes in an image using pre-computed object proposals.\n\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3053\u3053\u306e\u90e8\u5206\u306f\u307e\u3063\u305f\u304f\u9055\u3046\"\"\"\nim2 = cv2.resize(im, (0,0), fx=1.0/scale_factor, fy=1.0/scale_factor)\nobj_proposals_in = []\ndlib.find_candidate_object_locations(im2, obj_proposals_in, min_size=70)\nobj_proposals = np.empty((len(obj_proposals_in),4))\nfor idx in range(len(obj_proposals_in)):\n    obj_proposals[idx] = [obj_proposals_in[idx].left(), obj_proposals_in[idx].top(), obj_proposals_in[idx].right(), obj_proposals_in[idx].bottom()]\nDetect all object classes and regress object bounds\nscores, boxes = im_detect(net, im2, obj_proposals)\nVisualize detections for each class\nCONF_THRESH = 0.8\nNMS_THRESH = 0.3\nfor cls in classes:\n    cls_ind = CLASSES.index(cls)\n    cls_boxes = boxes[:, 4_cls_ind:4_(cls_ind + 1)]\n    cls_scores = scores[:, cls_ind]\n    dets = np.hstack((cls_boxes,\n                      cls_scores[:, np.newaxis])).astype(np.float32)\n    keep = nms(dets, NMS_THRESH)\n    dets = dets[keep, :]\nreturn [im2, cls, dets, CONF_THRESH]\ndef parse_args():\n\"\"\"Parse input arguments.\u3053\u3053\u306f\u540c\u3058\"\"\"\nparser = argparse.ArgumentParser(description='Train a Fast R-CNN network')\nparser.add_argument('--gpu', dest='gpu_id', help='GPU device id to use [0]',\ndefault=0, type=int)\nparser.add_argument('--cpu', dest='cpu_mode',\nhelp='Use CPU mode (overrides --gpu)',\naction='store_true')\nparser.add_argument('--net', dest='demo_net', help='Network to use [vgg16]',\nchoices=NETS.keys(), default='vgg16')\nargs = parser.parse_args()\nreturn args\nif name == 'main':\nargs = parse_args()\nprototxt = os.path.join(cfg.MODELS_DIR, NETS[args.demo_net][0],\n                        'faster_rcnn_alt_opt', 'faster_rcnn_test.pt')\ncaffemodel = os.path.join(cfg.DATA_DIR, 'faster_rcnn_models',\n                          NETS[args.demo_net][1])\nif not os.path.isfile(caffemodel):\n    raise IOError(('{:s} not found.\\nDid you run ./data/script/'\n                   'fetch_fast_rcnn_models.sh?').format(caffemodel))\nif args.cpu_mode:\n    caffe.set_mode_cpu()\nelse:\n    caffe.set_mode_gpu()\ncaffe.set_device(args.gpu_id)\nnet = caffe.Net(prototxt, caffemodel, caffe.TEST)\nprint '\\n\\nLoaded network {:s}'.format(caffemodel)\ncap = cv2.VideoCapture(0)\nwhile(True):\n    # Capture frame-by-frame\n    ret, frame = cap.read()\n```\nScaling the video feed can help the system run faster (and run on GPUs with less memory)\ne.g. with a standard video stream of size 640x480, a scale_factor = 4 will allow the system to run a < 1 sec/frame\nscale_factor = 4\n[im2, cls, dets, CONF_THRESH] = demo(net, frame, scale_factor, ('person',))\ninds = np.where(dets[:, -1] >= CONF_THRESH)[0]\nif len(inds) != 0:\n    for i in inds:\n        bbox = dets[i, :4]\n        cv2.rectangle(frame,(int(bbox[0]scale_factor),int(bbox[1]scale_factor)),(int(bbox[2]scale_factor),int(bbox[3]scale_factor)),(0,255,0),2)\nDisplay the resulting frame\ncv2.imshow('frame',frame)\nif cv2.waitKey(1) & 0xFF == ord('q'):\n    break\n```\nWhen everything done, release the capture\ncap.release()\ncv2.destroyAllWindows()`\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/rbgirshick/fast-rcnn/pull/29#issuecomment-253451063, or mute the threadhttps://github.com/notifications/unsubscribe-auth/ANj66yuHGvu3K-m9f70d477V5h5B7AsSks5qze6GgaJpZM4FAUIy.\n. it's look like file connect problem if you use Japanese or not English directory\nmaybe happed like folder file not find problem. and what about change folder file directory short?\nexception bug will be fix.\n\n\ubcf4\ub0b8 \uc0ac\ub78c: miyamon11 notifications@github.com\n\ubcf4\ub0b8 \ub0a0\uc9dc: 2016\ub144 10\uc6d4 13\uc77c \ubaa9\uc694\uc77c \uc624\uc804 8:40\n\ubc1b\ub294 \uc0ac\ub78c: rbgirshick/fast-rcnn\n\uc81c\ubaa9: Re: [rbgirshick/fast-rcnn] Webcam Demo (#29)\nHi, I'm trying to run your webcam.py, but I can not run.\nI changed the caffemodel and rewrite your code. When I run this script, an error is happened as below\nTraceback (most recent call last):\nFile \"realtime2.py\", line 150, in \n[im2, cls, dets, CONF_THRESH] = demo(net, frame, scale_factor, ('person',))\nFile \"realtime2.py\", line 87, in demo\nscores, boxes = im_detect(net, im2, obj_proposals)\nFile \"/home/keisan/py-faster-rcnn/tools/../lib/fast_rcnn/test.py\", line 154, in im_detect\nblobs_out = net.forward(**forward_kwargs)\nFile \"/home/keisan/py-faster-rcnn/tools/../caffe-fast-rcnn/python/caffe/pycaffe.py\", line 97, in _Net_forward\nraise Exception('Input blob arguments do not match net inputs.')\nException: Input blob arguments do not match net inputs.\nI can not understand what is happened. What should I do?\nThe rewrited code is blow\n`#!/usr/bin/env python\n-- coding: utf-8 --\nFast R-CNN\nCopyright (c) 2015 Microsoft\nLicensed under The MIT License [see LICENSE for details]\nWritten by Ross Girshick\n\"\"\"\nDemo script showing detections in sample images.\nSee README.md for installation instructions before running.\n\"\"\"\nimport _init_paths\nfrom fast_rcnn.config import cfg\nfrom fast_rcnn.test import im_detect\nfrom fast_rcnn.nms_wrapper import nms#\u3053\u3053\u304c\u9055\u3046\u5143\u306ffaster_rcnn.nms_wrapper\nfrom utils.timer import Timer\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.io as sio\nimport caffe, os, cv2 #sys\u304c\u306a\u3044\nimport argparse\nimport dlib #\u65b0\u3057\u3044\u30e2\u30b8\u30e5\u30fc\u30eb\nCLASSES = ('background',\n'aeroplane', 'bicycle', 'bird', 'boat',\n'bottle', 'bus', 'car', 'cat', 'chair',\n'cow', 'diningtable', 'dog', 'horse',\n'motorbike', 'person', 'pottedplant',\n'sheep', 'sofa', 'train', 'tvmonitor')\nNETS = {'vgg16': ('VGG16',\n'VGG16_faster_rcnn_final.caffemodel'),\n'zf': ('ZF',\n'ZF_faster_rcnn_final.caffemodel')}\ndef vis_detections(im, class_name, dets, thresh=0.5):\n\"\"\"Draw detected bounding boxes.\u3053\u306e\u5b9a\u7fa9\u306f\u3059\u3079\u3066demo.py\u3068\u4e00\u7dd2\"\"\"\ninds = np.where(dets[:, -1] >= thresh)[0]\nif len(inds) == 0:\nreturn\nim = im[:, :, (2, 1, 0)]\nfig, ax = plt.subplots(figsize=(12, 12))\nax.imshow(im, aspect='equal')\nfor i in inds:\n    bbox = dets[i, :4]\n    score = dets[i, -1]\nax.add_patch(\n    plt.Rectangle((bbox[0], bbox[1]),\n                  bbox[2] - bbox[0],\n                  bbox[3] - bbox[1], fill=False,\n                  edgecolor='red', linewidth=3.5)\n    )\nax.text(bbox[0], bbox[1] - 2,\n        '{:s} {:.3f}'.format(class_name, score),\n        bbox=dict(facecolor='blue', alpha=0.5),\n        fontsize=14, color='white')\nax.set_title(('{} detections with '\n              'p({} | box) >= {:.1f}').format(class_name, class_name,\n                                              thresh),\n              fontsize=14)\nplt.axis('off')\nplt.tight_layout()\nplt.draw()\ndef demo(net, im, scale_factor, classes):\n\"\"\"Detect object classes in an image using pre-computed object proposals.\n\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3053\u3053\u306e\u90e8\u5206\u306f\u307e\u3063\u305f\u304f\u9055\u3046\"\"\"\nim2 = cv2.resize(im, (0,0), fx=1.0/scale_factor, fy=1.0/scale_factor)\nobj_proposals_in = []\ndlib.find_candidate_object_locations(im2, obj_proposals_in, min_size=70)\nobj_proposals = np.empty((len(obj_proposals_in),4))\nfor idx in range(len(obj_proposals_in)):\n    obj_proposals[idx] = [obj_proposals_in[idx].left(), obj_proposals_in[idx].top(), obj_proposals_in[idx].right(), obj_proposals_in[idx].bottom()]\nDetect all object classes and regress object bounds\nscores, boxes = im_detect(net, im2, obj_proposals)\nVisualize detections for each class\nCONF_THRESH = 0.8\nNMS_THRESH = 0.3\nfor cls in classes:\n    cls_ind = CLASSES.index(cls)\n    cls_boxes = boxes[:, 4_cls_ind:4_(cls_ind + 1)]\n    cls_scores = scores[:, cls_ind]\n    dets = np.hstack((cls_boxes,\n                      cls_scores[:, np.newaxis])).astype(np.float32)\n    keep = nms(dets, NMS_THRESH)\n    dets = dets[keep, :]\nreturn [im2, cls, dets, CONF_THRESH]\ndef parse_args():\n\"\"\"Parse input arguments.\u3053\u3053\u306f\u540c\u3058\"\"\"\nparser = argparse.ArgumentParser(description='Train a Fast R-CNN network')\nparser.add_argument('--gpu', dest='gpu_id', help='GPU device id to use [0]',\ndefault=0, type=int)\nparser.add_argument('--cpu', dest='cpu_mode',\nhelp='Use CPU mode (overrides --gpu)',\naction='store_true')\nparser.add_argument('--net', dest='demo_net', help='Network to use [vgg16]',\nchoices=NETS.keys(), default='vgg16')\nargs = parser.parse_args()\nreturn args\nif name == 'main':\nargs = parse_args()\nprototxt = os.path.join(cfg.MODELS_DIR, NETS[args.demo_net][0],\n                        'faster_rcnn_alt_opt', 'faster_rcnn_test.pt')\ncaffemodel = os.path.join(cfg.DATA_DIR, 'faster_rcnn_models',\n                          NETS[args.demo_net][1])\nif not os.path.isfile(caffemodel):\n    raise IOError(('{:s} not found.\\nDid you run ./data/script/'\n                   'fetch_fast_rcnn_models.sh?').format(caffemodel))\nif args.cpu_mode:\n    caffe.set_mode_cpu()\nelse:\n    caffe.set_mode_gpu()\ncaffe.set_device(args.gpu_id)\nnet = caffe.Net(prototxt, caffemodel, caffe.TEST)\nprint '\\n\\nLoaded network {:s}'.format(caffemodel)\ncap = cv2.VideoCapture(0)\nwhile(True):\n    # Capture frame-by-frame\n    ret, frame = cap.read()\n```\nScaling the video feed can help the system run faster (and run on GPUs with less memory)\ne.g. with a standard video stream of size 640x480, a scale_factor = 4 will allow the system to run a < 1 sec/frame\nscale_factor = 4\n[im2, cls, dets, CONF_THRESH] = demo(net, frame, scale_factor, ('person',))\ninds = np.where(dets[:, -1] >= CONF_THRESH)[0]\nif len(inds) != 0:\n    for i in inds:\n        bbox = dets[i, :4]\n        cv2.rectangle(frame,(int(bbox[0]scale_factor),int(bbox[1]scale_factor)),(int(bbox[2]scale_factor),int(bbox[3]scale_factor)),(0,255,0),2)\nDisplay the resulting frame\ncv2.imshow('frame',frame)\nif cv2.waitKey(1) & 0xFF == ord('q'):\n    break\n```\nWhen everything done, release the capture\ncap.release()\ncv2.destroyAllWindows()`\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/rbgirshick/fast-rcnn/pull/29#issuecomment-253451063, or mute the threadhttps://github.com/notifications/unsubscribe-auth/ANj66yuHGvu3K-m9f70d477V5h5B7AsSks5qze6GgaJpZM4FAUIy.\n. ",
    "jiangyuguang": "@miyamon11  i meet same error , do u have solved the problem? \nerror :Traceback (most recent call last): File \"realtime2.py\", line 150, in  [im2, cls, dets, CONF_THRESH] = demo(net, frame, scale_factor, ('person',)) File \"realtime2.py\", line 87, in demo scores, boxes = im_detect(net, im2, obj_proposals) File \"/home/keisan/py-faster-rcnn/tools/../lib/fast_rcnn/test.py\", line 154, in im_detect blobs_out = net.forward(forward_kwargs) File \"/home/keisan/py-faster-rcnn/tools/../caffe-fast-rcnn/python/caffe/pycaffe.py\", line 97, in _Net_forward raise Exception('Input blob arguments do not match net inputs.') Exception: Input blob arguments do not match net inputs.\n. @miyamon11  i meet same error , do u have solved the problem? \nerror :Traceback (most recent call last): File \"realtime2.py\", line 150, in  [im2, cls, dets, CONF_THRESH] = demo(net, frame, scale_factor, ('person',)) File \"realtime2.py\", line 87, in demo scores, boxes = im_detect(net, im2, obj_proposals) File \"/home/keisan/py-faster-rcnn/tools/../lib/fast_rcnn/test.py\", line 154, in im_detect blobs_out = net.forward(forward_kwargs) File \"/home/keisan/py-faster-rcnn/tools/../caffe-fast-rcnn/python/caffe/pycaffe.py\", line 97, in _Net_forward raise Exception('Input blob arguments do not match net inputs.') Exception: Input blob arguments do not match net inputs.\n. ",
    "guptaanil2k1": "I am also getting the same issue... were you able to resolve this?\n. I resolved it by uncommenting WITH_PYTHON_LAYER := 1 and making clean build. It seems I have build the caffe without this flag and when I changed this flag did make again nothing got rebuild.\n.  changing this flag  WITH_PYTHON_LAYER := 1 in Makefile.config and doing make does not do anything.. I had to do clean and build\n. can you try clean build / removing build directory and building again\nafter WITH_PYTHON_LAYER\n:= 1\nRegards\nAnil\nOn Mon, Feb 20, 2017 at 11:10 AM, ran5515 notifications@github.com wrote:\n\nHello, I tried \"WITH_PYTHON_LAYER := 1\" but the error still exists.\nI am using Mac Sierra.\nCan anyone help me with it?\n\"I0220 11:06:37.499789 3922367424 layer_factory.hpp:77] Creating layer\nproposal\nF0220 11:06:37.499804 3922367424 layer_factory.hpp:81] Check failed:\nregistry.count(type) == 1 (0 vs. 1) Unknown layer type: Python (known\ntypes: AbsVal, Accuracy, ArgMax, BNLL, BatchNorm, BatchReindex, Bias,\nConcat, ContrastiveLoss, Convolution, Data, Deconvolution, Dropout,\nDummyData, ELU, Eltwise, Embed, EuclideanLoss, Exp, Filter, Flatten,\nHDF5Data, HDF5Output, HingeLoss, Im2col, ImageData, InfogainLoss,\nInnerProduct, LRN, Log, MVN, MemoryData, MultinomialLogisticLoss, PReLU,\nPooling, Power, ROIPooling, ReLU, Reduction, Reshape, SPP, Scale, Sigmoid,\nSigmoidCrossEntropyLoss, Silence, Slice, SmoothL1Loss, Softmax,\nSoftmaxWithLoss, Split, TanH, Threshold, Tile, WindowData)\n Check failure stack trace: \n\"\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/rbgirshick/fast-rcnn/issues/31#issuecomment-281160086,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AE-KNOR73i5oRWRp9uTABa7i3mASxHUIks5reeUzgaJpZM4FG2JE\n.\n\n\n-- \nRegards\nAnil Gupta\n. you can do make clean or delete the build folder manually. . I am also getting the same issue... were you able to resolve this?\n. I resolved it by uncommenting WITH_PYTHON_LAYER := 1 and making clean build. It seems I have build the caffe without this flag and when I changed this flag did make again nothing got rebuild.\n.  changing this flag  WITH_PYTHON_LAYER := 1 in Makefile.config and doing make does not do anything.. I had to do clean and build\n. can you try clean build / removing build directory and building again\nafter WITH_PYTHON_LAYER\n:= 1\nRegards\nAnil\nOn Mon, Feb 20, 2017 at 11:10 AM, ran5515 notifications@github.com wrote:\n\nHello, I tried \"WITH_PYTHON_LAYER := 1\" but the error still exists.\nI am using Mac Sierra.\nCan anyone help me with it?\n\"I0220 11:06:37.499789 3922367424 layer_factory.hpp:77] Creating layer\nproposal\nF0220 11:06:37.499804 3922367424 layer_factory.hpp:81] Check failed:\nregistry.count(type) == 1 (0 vs. 1) Unknown layer type: Python (known\ntypes: AbsVal, Accuracy, ArgMax, BNLL, BatchNorm, BatchReindex, Bias,\nConcat, ContrastiveLoss, Convolution, Data, Deconvolution, Dropout,\nDummyData, ELU, Eltwise, Embed, EuclideanLoss, Exp, Filter, Flatten,\nHDF5Data, HDF5Output, HingeLoss, Im2col, ImageData, InfogainLoss,\nInnerProduct, LRN, Log, MVN, MemoryData, MultinomialLogisticLoss, PReLU,\nPooling, Power, ROIPooling, ReLU, Reduction, Reshape, SPP, Scale, Sigmoid,\nSigmoidCrossEntropyLoss, Silence, Slice, SmoothL1Loss, Softmax,\nSoftmaxWithLoss, Split, TanH, Threshold, Tile, WindowData)\n Check failure stack trace: \n\"\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/rbgirshick/fast-rcnn/issues/31#issuecomment-281160086,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AE-KNOR73i5oRWRp9uTABa7i3mASxHUIks5reeUzgaJpZM4FG2JE\n.\n\n\n-- \nRegards\nAnil Gupta\n. you can do make clean or delete the build folder manually. . ",
    "xksteven": "Thanks for the tip! I was using an old caffe makefile as opposed to the .config.example provided.\nI'm confused about your error.  You mention you resolved the issue but then you say that nothing gets made when running make?\n. In that file I can change the time between snapshots and snapshot infix but nothing on using the snapshot during training.\nWould I just change in the solver.prototxt the snapshot number to reference the current snapshot?\n. @WilsonWangTHU \nYou know when using caffe you can provide the snapshot option such as -snapshot=model_iter_xxx.solverstate to restart the training from that point? Normally in caffe the solverstate and the caffemodel saved as model_iter_xxx.caffemodel are both in the same directory but with fast-rcnn I only see the caffemodel saved in the output/default/imdb_trainval.  I'd like to be able to restart the training using those weights stored there.  \nI'm running it on a cluster with a certain time limit and it will kill my process at certain time intervals. I just want to be able to restart the training from that snapshot.\n. Thanks for the tip! I was using an old caffe makefile as opposed to the .config.example provided.\nI'm confused about your error.  You mention you resolved the issue but then you say that nothing gets made when running make?\n. In that file I can change the time between snapshots and snapshot infix but nothing on using the snapshot during training.\nWould I just change in the solver.prototxt the snapshot number to reference the current snapshot?\n. @WilsonWangTHU \nYou know when using caffe you can provide the snapshot option such as -snapshot=model_iter_xxx.solverstate to restart the training from that point? Normally in caffe the solverstate and the caffemodel saved as model_iter_xxx.caffemodel are both in the same directory but with fast-rcnn I only see the caffemodel saved in the output/default/imdb_trainval.  I'd like to be able to restart the training using those weights stored there.  \nI'm running it on a cluster with a certain time limit and it will kill my process at certain time intervals. I just want to be able to restart the training from that snapshot.\n. ",
    "zhangjianagry": "but what should i do to remove the build?.  but what should i do to remove the build?. ",
    "sulth": "The faster RCNN was working when tried demo.py and got boundingbox detected.But while creating a model,syncedmem.cpp:56] Check failed: error == cudaSuccess (2 vs. 0)  out of memory\n Check failure stack trace: \nPlease help me. The faster RCNN was working when tried demo.py and got boundingbox detected.But while creating a model,syncedmem.cpp:56] Check failed: error == cudaSuccess (2 vs. 0)  out of memory\n Check failure stack trace: \nPlease help me. ",
    "alantrrs": "Does this mean I should be able to train on the VOT dataset without the\nEnvironmentError: MATLAB command 'matlab' not found. Please add 'matlab' to your PATH. error?\nI'm trying to test but I still get the error. Is this still planned to be merged?\n. Does this mean I should be able to train on the VOT dataset without the\nEnvironmentError: MATLAB command 'matlab' not found. Please add 'matlab' to your PATH. error?\nI'm trying to test but I still get the error. Is this still planned to be merged?\n. ",
    "senthilps8": "Should be fixed now.\nYou would still need to have the annotation mat files (train_anno.mat, trainval_anno.mat, test_anno.mat) in the $FRCN_ROOT/data/VOCdevkit2007/local/VOC2007/  folder.\nIf you can't generate them yourself, I think the ones available here(http://vision.cs.utexas.edu/voc/local/) should work.\n. Should be fixed now.\nYou would still need to have the annotation mat files (train_anno.mat, trainval_anno.mat, test_anno.mat) in the $FRCN_ROOT/data/VOCdevkit2007/local/VOC2007/  folder.\nIf you can't generate them yourself, I think the ones available here(http://vision.cs.utexas.edu/voc/local/) should work.\n. ",
    "deshelv": "How to generate the annotation mat files if I'm going to use VOC2012 dataset?\n. How to generate the annotation mat files if I'm going to use VOC2012 dataset?\n. ",
    "kyuusaku": "I have the same problem.\n. How to restart the training from a snapshot? Can anyone provide some tips? Thanks.\n. I have the same problem.\n. How to restart the training from a snapshot? Can anyone provide some tips? Thanks.\n. ",
    "lynetcha": "Make the following modifications and you will be able to use the --snapshot argument\nIn tools/train_net.py\n\n    def parse_args():\n        \"\"\"\n        Parse input arguments\n        \"\"\"\n        parser = argparse.ArgumentParser(description='Train a Fast R-CNN network')\n        parser.add_argument('--gpu', dest='gpu_id',\n                            help='GPU device id to use [0]',\n                            default=0, type=int)\n        parser.add_argument('--solver', dest='solver',\n                            help='solver prototxt',\n                            default=None, type=str)\n        parser.add_argument('--iters', dest='max_iters',\n                            help='number of iterations to train',\n                            default=40000, type=int)\n        parser.add_argument('--weights', dest='pretrained_model',\n                            help='initialize with pretrained model weights',\n                            default=None, type=str)\n        parser.add_argument('--snapshot', dest='previous_state',\n                            help='initialize with previous state',\n                            default=None, type=str) \n        parser.add_argument('--cfg', dest='cfg_file',\n                            help='optional config file',\n                            default=None, type=str)\n        parser.add_argument('--imdb', dest='imdb_name',\n                            help='dataset to train on',\n                            default='voc_2007_trainval', type=str)\n        parser.add_argument('--rand', dest='randomize',\n                            help='randomize (do not use a fixed seed)',\n                            action='store_true')\n        parser.add_argument('--set', dest='set_cfgs',\n                            help='set config keys', default=None,\n                            nargs=argparse.REMAINDER)\n\nIn lib/fast_rcnn/train.py\n\n        class SolverWrapper(object):\n            \"\"\"A simple wrapper around Caffe's solver.\n            This wrapper gives us control over he snapshotting process, which we\n            use to unnormalize the learned bounding-box regression weights.\n            \"\"\"\n            def __init__(self, solver_prototxt, roidb, output_dir,\n                         pretrained_model=None, previous_state=None):\n                \"\"\"Initialize the SolverWrapper.\"\"\"\n                self.output_dir = output_dir\n                print 'Computing bounding-box regression targets...'\n                self.bbox_means, self.bbox_stds = \\\n                        rdl_roidb.add_bbox_regression_targets(roidb)\n                print 'done'\n                self.solver = caffe.SGDSolver(solver_prototxt)\n                if pretrained_model is not None:\n                    print ('Loading pretrained model '\n                           'weights from {:s}').format(pretrained_model)\n                    self.solver.net.copy_from(pretrained_model)\n                 elif previous_state is not None:\n                    print ('Restoring State from '\n                              ' from {:s}').format(previous_state)\n                    self.solver.restore(previous_state)\n                self.solver_param = caffe_pb2.SolverParameter()\n                with open(solver_prototxt, 'rt') as f:\n                    pb2.text_format.Merge(f.read(), self.solver_param)\n                self.solver.net.layers[0].set_roidb(roidb)\n.\n.\n.\ndef train_net(solver_prototxt, roidb, output_dir,\n              pretrained_model=None, max_iters=40000,previous_state=None):\n    \"\"\"Train a Fast R-CNN network.\"\"\"\n    sw = SolverWrapper(solver_prototxt, roidb, output_dir,\n                       pretrained_model=pretrained_model,previous_state=previous_state)\n    print 'Solving...'\n    sw.train_model(max_iters)\n    print 'done solving'\n\n. Did you change \"snapshot: 0\"  to \"snapshot: 10000\" in your solver.prototxt?  That allows you to save the state at iteration 10000 for example. \n. Make the following modifications and you will be able to use the --snapshot argument\nIn tools/train_net.py\n\n    def parse_args():\n        \"\"\"\n        Parse input arguments\n        \"\"\"\n        parser = argparse.ArgumentParser(description='Train a Fast R-CNN network')\n        parser.add_argument('--gpu', dest='gpu_id',\n                            help='GPU device id to use [0]',\n                            default=0, type=int)\n        parser.add_argument('--solver', dest='solver',\n                            help='solver prototxt',\n                            default=None, type=str)\n        parser.add_argument('--iters', dest='max_iters',\n                            help='number of iterations to train',\n                            default=40000, type=int)\n        parser.add_argument('--weights', dest='pretrained_model',\n                            help='initialize with pretrained model weights',\n                            default=None, type=str)\n        parser.add_argument('--snapshot', dest='previous_state',\n                            help='initialize with previous state',\n                            default=None, type=str) \n        parser.add_argument('--cfg', dest='cfg_file',\n                            help='optional config file',\n                            default=None, type=str)\n        parser.add_argument('--imdb', dest='imdb_name',\n                            help='dataset to train on',\n                            default='voc_2007_trainval', type=str)\n        parser.add_argument('--rand', dest='randomize',\n                            help='randomize (do not use a fixed seed)',\n                            action='store_true')\n        parser.add_argument('--set', dest='set_cfgs',\n                            help='set config keys', default=None,\n                            nargs=argparse.REMAINDER)\n\nIn lib/fast_rcnn/train.py\n\n        class SolverWrapper(object):\n            \"\"\"A simple wrapper around Caffe's solver.\n            This wrapper gives us control over he snapshotting process, which we\n            use to unnormalize the learned bounding-box regression weights.\n            \"\"\"\n            def __init__(self, solver_prototxt, roidb, output_dir,\n                         pretrained_model=None, previous_state=None):\n                \"\"\"Initialize the SolverWrapper.\"\"\"\n                self.output_dir = output_dir\n                print 'Computing bounding-box regression targets...'\n                self.bbox_means, self.bbox_stds = \\\n                        rdl_roidb.add_bbox_regression_targets(roidb)\n                print 'done'\n                self.solver = caffe.SGDSolver(solver_prototxt)\n                if pretrained_model is not None:\n                    print ('Loading pretrained model '\n                           'weights from {:s}').format(pretrained_model)\n                    self.solver.net.copy_from(pretrained_model)\n                 elif previous_state is not None:\n                    print ('Restoring State from '\n                              ' from {:s}').format(previous_state)\n                    self.solver.restore(previous_state)\n                self.solver_param = caffe_pb2.SolverParameter()\n                with open(solver_prototxt, 'rt') as f:\n                    pb2.text_format.Merge(f.read(), self.solver_param)\n                self.solver.net.layers[0].set_roidb(roidb)\n.\n.\n.\ndef train_net(solver_prototxt, roidb, output_dir,\n              pretrained_model=None, max_iters=40000,previous_state=None):\n    \"\"\"Train a Fast R-CNN network.\"\"\"\n    sw = SolverWrapper(solver_prototxt, roidb, output_dir,\n                       pretrained_model=pretrained_model,previous_state=previous_state)\n    print 'Solving...'\n    sw.train_model(max_iters)\n    print 'done solving'\n\n. Did you change \"snapshot: 0\"  to \"snapshot: 10000\" in your solver.prototxt?  That allows you to save the state at iteration 10000 for example. \n. ",
    "chrert": "Thanks for the code but how to save the solverstate during fast r-cnn training? It looks like the method Solver::SnapshotSolverState isn't exported to pycaffe...\n. Ah, thanks! Didn't think of that...\n. Thanks for the code but how to save the solverstate during fast r-cnn training? It looks like the method Solver::SnapshotSolverState isn't exported to pycaffe...\n. Ah, thanks! Didn't think of that...\n. ",
    "smichalowski": "@lynetcha, one more modification:\nIn tools/train_net.py\n``\n    output_dir = get_output_dir(imdb)\n    print 'Output will be saved to{:s}`'.format(output_dir)\ntrain_net(args.solver, roidb, output_dir,\n          pretrained_model=args.pretrained_model,\n          max_iters=args.max_iters, **previous_state=args.previous_state**)\n\n```\nalso remember to omit --weights param\n. @lynetcha, one more modification:\nIn tools/train_net.py\n``\n    output_dir = get_output_dir(imdb)\n    print 'Output will be saved to{:s}`'.format(output_dir)\ntrain_net(args.solver, roidb, output_dir,\n          pretrained_model=args.pretrained_model,\n          max_iters=args.max_iters, **previous_state=args.previous_state**)\n\n```\nalso remember to omit --weights param\n. ",
    "twmht": "hi @po0ya \nwhat if I don't save the extra file for the last layer weights? would be bad mAP after retraining?\n. hi @po0ya \nwhat if I don't save the extra file for the last layer weights? would be bad mAP after retraining?\n. ",
    "po0ya": "Hello @twmht \nBasically it'll mess up the whole network if you want to continue training. The network is trained to work for zero mean and unit variance bboxes. For test time convenience, the weights and bias of the last layer is scaled by the std and shifted by the mean. If it has not been done, the prediction should've been scaled and shifted manually. It's for convenience in testing time, but the weights are not the ones that were learned by backprop, so retraining with these weights would be meaningless for the network. \nEDIT: Add these couple of lines to the end of SolverWrapper constructor init\n        found = False\n        for k in net.params.keys():\n            if 'bbox_pred' in k:\n                bbox_pred = k\n                found = True\n            print('[#] Renormalizing the final layers back')\n            net.params[bbox_pred][0].data[4:, :] = \\\n                (net.params[bbox_pred][0].data[4:, :] *\n                 1.0 / self.bbox_stds[4:, np.newaxis])\n            net.params[bbox_pred][1].data[4:] = \\\n                    (net.params[bbox_pred][1].data - self.bbox_means)[4:] * 1.0 / self.bbox_stds[4:]\n        if not found:\n            print('Warning layer \\\"bbox_pred\\\" not found'). Hello @twmht\n\nBasically it'll mess up the whole network if you want to continue training. The network is trained to work for zero mean and unit variance bboxes. For test time convenience, the weights and bias of the last layer is scaled by the std and shifted by the mean. If it has not been done, the prediction should've been scaled and shifted manually. It's for convenience in testing time, but the weights are not the ones that were learned by backprop, so retraining with these weights would be meaningless for the network. \nEDIT: Add these couple of lines to the end of SolverWrapper constructor init\n        found = False\n        for k in net.params.keys():\n            if 'bbox_pred' in k:\n                bbox_pred = k\n                found = True\n            print('[#] Renormalizing the final layers back')\n            net.params[bbox_pred][0].data[4:, :] = \\\n                (net.params[bbox_pred][0].data[4:, :] *\n                 1.0 / self.bbox_stds[4:, np.newaxis])\n            net.params[bbox_pred][1].data[4:] = \\\n                    (net.params[bbox_pred][1].data - self.bbox_means)[4:] * 1.0 / self.bbox_stds[4:]\n        if not found:\n            print('Warning layer \\\"bbox_pred\\\" not found').\n",
    "ds2268": "@po0ya but aren't the weights (.caffemodel) that are saved by the default solver already normalized (because they were never unnormalizied, because the caffemodel was not saved using provided snapshot functionality). So I guess the produced .solverstate is linked to the .caffemodel model that was not produced by the faster rcnn snapshot function. Using resuming functionality you get 2 versions of caffemodel, the one provided by the default solver snapshot and the one provided by the snapshot function in faster r-cnn that the weights are unnormalized before saving. So I guess that normalization is not needed.. @po0ya but aren't the weights (.caffemodel) that are saved by the default solver already normalized (because they were never unnormalizied, because the caffemodel was not saved using provided snapshot functionality). So I guess the produced .solverstate is linked to the .caffemodel model that was not produced by the faster rcnn snapshot function. Using resuming functionality you get 2 versions of caffemodel, the one provided by the default solver snapshot and the one provided by the snapshot function in faster r-cnn that the weights are unnormalized before saving. So I guess that normalization is not needed.. ",
    "grib0ed0v": "@marutiagarwal  actually here you don't use RoI Pooling layer. If you see the .prototxt for VGG or CaffeNet, they must contain this layer.\nlayer {\n  name: \"roi_pool5\"\n  type: \"ROIPooling\"\n  bottom: \"conv5_3\"\n  bottom: \"rois\"\n  top: \"pool5\"\n  roi_pooling_param {\n    pooled_w: 7\n    pooled_h: 7\n    spatial_scale: 0.0625 # 1/16\n  }\n}\n. @hermitman \"pool5\"  - just a name of an output blob. \nThe next layer have as bottom this \"pool5\" blob:\n...\nlayer {\n  name: \"inception_5b/output\"\n  type: \"Concat\"\n  bottom: \"inception_5b/1x1\"\n  bottom: \"inception_5b/3x3\"\n  bottom: \"inception_5b/5x5\"\n  bottom: \"inception_5b/pool_proj\"\n  top: \"inception_5b/output\"\n}\nlayer {\n  name: \"roi_pool5\"\n  type: \"ROIPooling\"\n  bottom: \"inception_5b/output\"\n  bottom: \"rois\"\n  top: \"pool5\"\n  roi_pooling_param {\n    pooled_w: 7\n    pooled_h: 7\n    spatial_scale: 0.0625 # 1/16\n  }\n}\nlayer {\n  name: \"fc6\"\n  type: \"InnerProduct\"\n  bottom: \"pool5\"\n  top: \"fc6\"\n  param {\n    lr_mult: 1\n    decay_mult: 1\n  }\n  param {\n    lr_mult: 2\n    decay_mult: 0\n  }\n  inner_product_param {\n    num_output: 4096\n    weight_filler {\n      type: \"gaussian\"\n      std: 0.01\n    }\n    bias_filler {\n      type: \"constant\"\n      value: 0\n    }\n  }\n}\n...\n. If you set lr_mult & decay_mult to 0 that actually means that you 'freeze' the layer and weights of this layer will not be updated during training.\n. @marutiagarwal  actually here you don't use RoI Pooling layer. If you see the .prototxt for VGG or CaffeNet, they must contain this layer.\nlayer {\n  name: \"roi_pool5\"\n  type: \"ROIPooling\"\n  bottom: \"conv5_3\"\n  bottom: \"rois\"\n  top: \"pool5\"\n  roi_pooling_param {\n    pooled_w: 7\n    pooled_h: 7\n    spatial_scale: 0.0625 # 1/16\n  }\n}\n. @hermitman \"pool5\"  - just a name of an output blob. \nThe next layer have as bottom this \"pool5\" blob:\n...\nlayer {\n  name: \"inception_5b/output\"\n  type: \"Concat\"\n  bottom: \"inception_5b/1x1\"\n  bottom: \"inception_5b/3x3\"\n  bottom: \"inception_5b/5x5\"\n  bottom: \"inception_5b/pool_proj\"\n  top: \"inception_5b/output\"\n}\nlayer {\n  name: \"roi_pool5\"\n  type: \"ROIPooling\"\n  bottom: \"inception_5b/output\"\n  bottom: \"rois\"\n  top: \"pool5\"\n  roi_pooling_param {\n    pooled_w: 7\n    pooled_h: 7\n    spatial_scale: 0.0625 # 1/16\n  }\n}\nlayer {\n  name: \"fc6\"\n  type: \"InnerProduct\"\n  bottom: \"pool5\"\n  top: \"fc6\"\n  param {\n    lr_mult: 1\n    decay_mult: 1\n  }\n  param {\n    lr_mult: 2\n    decay_mult: 0\n  }\n  inner_product_param {\n    num_output: 4096\n    weight_filler {\n      type: \"gaussian\"\n      std: 0.01\n    }\n    bias_filler {\n      type: \"constant\"\n      value: 0\n    }\n  }\n}\n...\n. If you set lr_mult & decay_mult to 0 that actually means that you 'freeze' the layer and weights of this layer will not be updated during training.\n. ",
    "hermitman": "@AlexGruzdev I have a question. Why the top connection is linked to pool5 instead of or_pool5? I don't see there is a pool5 layer in the prototxt file. Thank you very much~\n. @AlexGruzdev I have a question. Why the top connection is linked to pool5 instead of or_pool5? I don't see there is a pool5 layer in the prototxt file. Thank you very much~\n. ",
    "jond55": "I'm trying to use this model also, but running into a problem with the input dimensions and the loss1/fc and loss2/fc InnerProduct layers. I'm not fully understanding how Fast-RCNN is able to use the model when the image input dimensions are variable, as they are. I think the input images are resized (not cropped) in lib/utils/blob.py so that the input dimensions are not 224x224 consistently. They may be 150x224. This doesn't seem to affect the CaffeNet/VGG models, but it is affecting GoogleNet, since it causes varying input sizes to the loss InnerProduct layers.\nWhat is the best way to handle this? Should input images be resized to 224x224 to match the model, and lose the aspect ratio? Or should loss layers be modified? I tried changing loss1/fc to convolutional but it just pushed the problem a little further down. Or should these intermediate loss layers be removed?\nNot a Caffe expert by any means, so apologize if these questions seem basic.\n. I'm trying to use this model also, but running into a problem with the input dimensions and the loss1/fc and loss2/fc InnerProduct layers. I'm not fully understanding how Fast-RCNN is able to use the model when the image input dimensions are variable, as they are. I think the input images are resized (not cropped) in lib/utils/blob.py so that the input dimensions are not 224x224 consistently. They may be 150x224. This doesn't seem to affect the CaffeNet/VGG models, but it is affecting GoogleNet, since it causes varying input sizes to the loss InnerProduct layers.\nWhat is the best way to handle this? Should input images be resized to 224x224 to match the model, and lose the aspect ratio? Or should loss layers be modified? I tried changing loss1/fc to convolutional but it just pushed the problem a little further down. Or should these intermediate loss layers be removed?\nNot a Caffe expert by any means, so apologize if these questions seem basic.\n. ",
    "marutiagarwal": "I got the googLeNet working for fast-rcnn. Though, performance isn't as good as that of VGG16. I am wondering (significantly lower true positive for ~100 classes), why would that be?  \ngooglenet_test_caffe.prototxt.txt\ngooglenet_train_caffe.prototxt.txt\n. @catsdogone - No. I wasn't sure about its value. So used it as it is from VGG16. Rest everything looks ok. I haven't resized input images. Using their original size only. Although:\ninput_shape {\n  dim: 1\n  dim: 3\n  dim: 224\n  dim: 224\n}\nis present in test.prototxt, to take a crop from input? \n. @catsdogone - it's value is 0.0625 for VGG16 and VGG_CNN_M. I don't think this should affect the performance by such a large margin. I am still missing on what is causing such poor performance for googlenet-fastrcnn.\n. @catsdogone - any thoughts on that?\n. @201power - can you please upload your train, test, and solver proto files. May be we can help improving each others' system. Thanks !\n. @rbgirshick could you comment on this? Will highly appreciate that. Thanks.\n. @jainanshul - already shared my protofile, just few posts above.\n. yes ! Just that I got high fp. I think the problem might be coming from the\npart of having three fc classifiers instead of 1 as in most of the other\nnetworks. Since backprop is starting from all of these 3 fc classifier\nlayers.\n\u1427\nRegards,\nMaruti Agarwal\nOn Tue, Jul 12, 2016 at 12:16 PM, catsdogone notifications@github.com\nwrote:\n\nhave you successfully trained a Googlenet/Inception network definition\nthat works in conjunction with Fast R-CNN/faster RCNN? @kamadforge\nhttps://github.com/kamadforge @marutiagarwal\nhttps://github.com/marutiagarwal @jainanshul\nhttps://github.com/jainanshul\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/rbgirshick/fast-rcnn/issues/36#issuecomment-231951659,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/ACPuP6D6hHAJWA9ILr2D36VfQkXmGRSNks5qUzgvgaJpZM4FXUMo\n.\n. I got the googLeNet working for fast-rcnn. Though, performance isn't as good as that of VGG16. I am wondering (significantly lower true positive for ~100 classes), why would that be?  \n\ngooglenet_test_caffe.prototxt.txt\ngooglenet_train_caffe.prototxt.txt\n. @catsdogone - No. I wasn't sure about its value. So used it as it is from VGG16. Rest everything looks ok. I haven't resized input images. Using their original size only. Although:\ninput_shape {\n  dim: 1\n  dim: 3\n  dim: 224\n  dim: 224\n}\nis present in test.prototxt, to take a crop from input? \n. @catsdogone - it's value is 0.0625 for VGG16 and VGG_CNN_M. I don't think this should affect the performance by such a large margin. I am still missing on what is causing such poor performance for googlenet-fastrcnn.\n. @catsdogone - any thoughts on that?\n. @201power - can you please upload your train, test, and solver proto files. May be we can help improving each others' system. Thanks !\n. @rbgirshick could you comment on this? Will highly appreciate that. Thanks.\n. @jainanshul - already shared my protofile, just few posts above.\n. yes ! Just that I got high fp. I think the problem might be coming from the\npart of having three fc classifiers instead of 1 as in most of the other\nnetworks. Since backprop is starting from all of these 3 fc classifier\nlayers.\n\u1427\nRegards,\nMaruti Agarwal\nOn Tue, Jul 12, 2016 at 12:16 PM, catsdogone notifications@github.com\nwrote:\n\nhave you successfully trained a Googlenet/Inception network definition\nthat works in conjunction with Fast R-CNN/faster RCNN? @kamadforge\nhttps://github.com/kamadforge @marutiagarwal\nhttps://github.com/marutiagarwal @jainanshul\nhttps://github.com/jainanshul\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/rbgirshick/fast-rcnn/issues/36#issuecomment-231951659,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/ACPuP6D6hHAJWA9ILr2D36VfQkXmGRSNks5qUzgvgaJpZM4FXUMo\n.\n. \n",
    "201power": "I got googlenet working on faster rcnn as well, on VOC2007 I got~65% mAP, not as good as VGG. \n. I got googlenet working on faster rcnn as well, on VOC2007 I got~65% mAP, not as good as VGG. \n. ",
    "bultina0": "@201power - Can you share your train prototxt files? Thanks~\n. @201power - Can you share your train prototxt files? Thanks~\n. ",
    "RalphMao": "@201power I am also wondering about it\n. @201power I am also wondering about it\n. ",
    "jainanshul": "@marutiagarwal or @201power would you guys be able to share your parameters?\n. @marutiagarwal or @201power would you guys be able to share your parameters?\n. ",
    "XiongweiWu": "I have modify the prototxt and get 63.8% mAP in pascal voc 2007. Mainly about the spatial ratio issue and the last pooling issue\n. I have modify the prototxt and get 63.8% mAP in pascal voc 2007. Mainly about the spatial ratio issue and the last pooling issue\n. ",
    "kamadforge": "A note to anybody who would like to test the prototxt files from @catsdogone. The files assume 8 classes and they need to be edited accordingly if there is a different number of classes (e.g. 21 in Pascal VOC).\n. A note to anybody who would like to test the prototxt files from @catsdogone. The files assume 8 classes and they need to be edited accordingly if there is a different number of classes (e.g. 21 in Pascal VOC).\n. ",
    "Soccer-Video-Analysis": "Why is the 1st blob \ninput: \"data\"\ninput_shape {\n  dim: 1\n  dim: 3\n  dim: 224\n  dim: 224\n}\nWhy 224 x 224 ? I thought Fast RCNN take big input and propose region later ?. Why is the 1st blob \ninput: \"data\"\ninput_shape {\n  dim: 1\n  dim: 3\n  dim: 224\n  dim: 224\n}\nWhy 224 x 224 ? I thought Fast RCNN take big input and propose region later ?. ",
    "nw89": "I believe that it's rescaled for every image.. I believe that it's rescaled for every image.. ",
    "joyivan": "@marutiagarwal  can U share Ur prototxt files I need that for 1 class detecting  . @marutiagarwal  can U share Ur prototxt files I need that for 1 class detecting  . ",
    "haoyul": "@catsdogone Hi, I'm trying to train GoogleNet with rcnn and faced your same low AP problem by using your test/train prototxt. May I know how did you solve this? Many thanks!. Hi @marutiagarwal, I modified the original googlenet prototxt and it's more similar to @catsdogone's version. Why there's no RPN and RoI proposal layers in yours? \nAnyone here trained successfully with @catsdogone's prototxt? My training also only get about 0.1 mean AP on pascal 2007. Thanks for any advise!. @catsdogone Hi, I'm trying to train GoogleNet with rcnn and faced your same low AP problem by using your test/train prototxt. May I know how did you solve this? Many thanks!. Hi @marutiagarwal, I modified the original googlenet prototxt and it's more similar to @catsdogone's version. Why there's no RPN and RoI proposal layers in yours? \nAnyone here trained successfully with @catsdogone's prototxt? My training also only get about 0.1 mean AP on pascal 2007. Thanks for any advise!. ",
    "parhartanvir": "@deartonym  can you suggest me what changes did you make to the network. I have tried to change the name, load from a pre-trained model as well as train from scratch. But I still have the same error while saving the network snapshot. \nI too am training for detecting a single class of objects, so I have 2 object classes.\nThanks!\n. @deartonym  can you suggest me what changes did you make to the network. I have tried to change the name, load from a pre-trained model as well as train from scratch. But I still have the same error while saving the network snapshot. \nI too am training for detecting a single class of objects, so I have 2 object classes.\nThanks!\n. ",
    "Jonshoo": "Hi @deartonym\nNow I am also encountering this bug, and I have changed the three layers\u2018 coresponding num, but I still got the error. Could I gain your further help?  Thank u a lot.\nbtw, I am confusing about the sentence \"you must rename at least the layers that has different shape\" from @WilsonWangTHU. That means I should rename the layers's name?\n. Hi @deartonym\nNow I am also encountering this bug, and I have changed the three layers\u2018 coresponding num, but I still got the error. Could I gain your further help?  Thank u a lot.\nbtw, I am confusing about the sentence \"you must rename at least the layers that has different shape\" from @WilsonWangTHU. That means I should rename the layers's name?\n. ",
    "ArturoDeza": "I believe this is changed in the first lines of the files: /Faster-RCNN_TF/lib/networks/VGGnet_train.py and /Faster-RCNN_TF/lib/networks/VGGnet_test.py. I believe this is changed in the first lines of the files: /Faster-RCNN_TF/lib/networks/VGGnet_train.py and /Faster-RCNN_TF/lib/networks/VGGnet_test.py. ",
    "nightrome": "So what's your problem??\n. So what's your problem??\n. ",
    "saiprabhakar": "I am trying to train a system which has some images with no foreground in it, and it did break the code. I am currently modifying the code to make it work. Any leads on the above issue?\n. I think backgrounds are considered negative samples. So you just need to find a way to add bounding boxes with overlap<threshold_overlap to your 'selective search file'.\n. @HarisIqbal88 Thanks for the answer, that makes sense. So did you set the loss to be '0' where ever the bounding box is empty? Can you share the modified code. I am guessing you changed smooth_L1_loss_layer.cu ? Can you share the code. I am new to cuda.\n. @HarisIqbal88  thanks for the quick response. I think, I am giving enough number of proposal boxes. I am also working on Caltech dataset using ACF region proposal. I will look into it if this is the problem.\nI was thinking the problem is because there isnt enough number of pedestrian bounding boxes (and a lot of background bounding boxes). What do you think?\n. @HarisIqbal88 i see what you mean.\nI took a closer look and found that the problem is in the config file the threshold for considering a bounding box in training state was set to 0.5. When I decreased this to 0.1 the NaN vanished. I think the higher threshold prevented selecting background bounding boxes.\n. @HarisIqbal88 Thanks for the insight, that makes sense. The core problem seems to be lack of region proposals overlapping with sufficient IoU with ground truth. There seems to be handful of images with 1 or 2, ROIs having overlap greater than .1 with ground truth.\nNote: I did separated images from the data set that doesnt have and ground truth (pedestrians) in them.\nI am using the ACF model given along with the Caltech dataset (or with Piotr-toolbox I dont remember). It doesnt seems to produce good region proposals (with overlap).\nWhich region proposal did you use? If you used ACF proposal, did you train it or used the model they provided? What setting did you use for the region proposal (threshold, calibration)? \n. @HarisIqbal88 Ok I see. I changed my threshold to get ~1000 proposals. I want to confirm something, you said you added dummy boxes of zero width right, can you give more details on it. \nI am guessing you labelled them as ground truth. Did you select them randomly? Or, where they centered in your ground truth bounding box. After seeing the cost function, I think it will affect the performance of the network, am I right? I am thinking of avoiding it because of this.\n. @HarisIqbal88 Did you also retrain ACF (I think it used boosting ) or used the given model and changed parameter during testing?\n. I dont know the answer to how to identify which image is causing the problem yet, I will look into it. \nThe problem I had was, there was a internal swapping of x,y columns of bounding box array in either ground truth or region proposals (one of them I forgot which one), which I guess was correct for Pascal dataset. For Caltech this should not be done especially if your data arrangements are matching between ground truth and region proposals in the file stage. This decreased the number of positive boxes in the region proposal for me.\nWhen i removed it, it worked just fine even without the adding dummy boxes as it was suggested earlier.\n. I am trying to train a system which has some images with no foreground in it, and it did break the code. I am currently modifying the code to make it work. Any leads on the above issue?\n. I think backgrounds are considered negative samples. So you just need to find a way to add bounding boxes with overlap<threshold_overlap to your 'selective search file'.\n. @HarisIqbal88 Thanks for the answer, that makes sense. So did you set the loss to be '0' where ever the bounding box is empty? Can you share the modified code. I am guessing you changed smooth_L1_loss_layer.cu ? Can you share the code. I am new to cuda.\n. @HarisIqbal88  thanks for the quick response. I think, I am giving enough number of proposal boxes. I am also working on Caltech dataset using ACF region proposal. I will look into it if this is the problem.\nI was thinking the problem is because there isnt enough number of pedestrian bounding boxes (and a lot of background bounding boxes). What do you think?\n. @HarisIqbal88 i see what you mean.\nI took a closer look and found that the problem is in the config file the threshold for considering a bounding box in training state was set to 0.5. When I decreased this to 0.1 the NaN vanished. I think the higher threshold prevented selecting background bounding boxes.\n. @HarisIqbal88 Thanks for the insight, that makes sense. The core problem seems to be lack of region proposals overlapping with sufficient IoU with ground truth. There seems to be handful of images with 1 or 2, ROIs having overlap greater than .1 with ground truth.\nNote: I did separated images from the data set that doesnt have and ground truth (pedestrians) in them.\nI am using the ACF model given along with the Caltech dataset (or with Piotr-toolbox I dont remember). It doesnt seems to produce good region proposals (with overlap).\nWhich region proposal did you use? If you used ACF proposal, did you train it or used the model they provided? What setting did you use for the region proposal (threshold, calibration)? \n. @HarisIqbal88 Ok I see. I changed my threshold to get ~1000 proposals. I want to confirm something, you said you added dummy boxes of zero width right, can you give more details on it. \nI am guessing you labelled them as ground truth. Did you select them randomly? Or, where they centered in your ground truth bounding box. After seeing the cost function, I think it will affect the performance of the network, am I right? I am thinking of avoiding it because of this.\n. @HarisIqbal88 Did you also retrain ACF (I think it used boosting ) or used the given model and changed parameter during testing?\n. I dont know the answer to how to identify which image is causing the problem yet, I will look into it. \nThe problem I had was, there was a internal swapping of x,y columns of bounding box array in either ground truth or region proposals (one of them I forgot which one), which I guess was correct for Pascal dataset. For Caltech this should not be done especially if your data arrangements are matching between ground truth and region proposals in the file stage. This decreased the number of positive boxes in the region proposal for me.\nWhen i removed it, it worked just fine even without the adding dummy boxes as it was suggested earlier.\n. ",
    "christopher5106": "Thanks a lot !\n. Thanks a lot !\n. ",
    "futurecrew": "Thanks!!\n. Thanks!!\n. ",
    "tharuniitk": "Hello @only4hj @rbgirshick \nHow to tune NMS_threshold, Conf_threshold when we test faster rcnn on custom dataset. Grateful for  your valuable insights\n. Hello @only4hj @rbgirshick \nHow to tune NMS_threshold, Conf_threshold when we test faster rcnn on custom dataset. Grateful for  your valuable insights\n. ",
    "wanghao2020": "@pmuyan  Hello, I have the same problem with you . i try the selective search IJCV code and achieve author's  rcnn/selective_search/selective_search_boxes.m code .i also can't get the same result. i was wondering for a lot of time . \ncan anyone solve it ?\n. @pmuyan  Hello, I have the same problem with you . i try the selective search IJCV code and achieve author's  rcnn/selective_search/selective_search_boxes.m code .i also can't get the same result. i was wondering for a lot of time . \ncan anyone solve it ?\n. ",
    "ZiangYan": "The boxes for the demo are not generated from selective search IJCV code. I use IJCV version to re-generate .mat files and run fast-cnn using CaffeNet again, finally get 57.0 mAP (57.1 in fast-rcnn 2015 ICCV paper).\n. The boxes for the demo are not generated from selective search IJCV code. I use IJCV version to re-generate .mat files and run fast-cnn using CaffeNet again, finally get 57.0 mAP (57.1 in fast-rcnn 2015 ICCV paper).\n. ",
    "AndsonYe": "Problem solved. Thanks ; )\n. Problem solved. Thanks ; )\n. ",
    "kambstreat": "Hi...I am still facing the issue when I changed the VOC.testset to 'test' instead of 'val'. Please help me in resolving this ?\n. Hi...I am still facing the issue when I changed the VOC.testset to 'test' instead of 'val'. Please help me in resolving this ?\n. ",
    "Anfield-Uncle": "@vvkdby Thank you .It solved my problem.\n. @vvkdby Thank you .It solved my problem.\n. ",
    "TianyouChen": "@AndsonYe I have the same problem,how to solve?Thank you!\n. @AndsonYe I have the same problem,how to solve?Thank you!\n. ",
    "lihaixiang": "I have the same problem,i thinks this problem is caused by lacking files in your datasets\n.  I have the same problem,i thinks this problem is caused by lacking files in your datasets\n. ",
    "drozdvadym": "One possible problem - invalid input data. Please check. \nAlso you need to resize caffe::Net when  amount of proposal object is changed.\nAlso please show me next things:\n1) caffe net proto, which you use\n2) how you get the value of the network output\n. One possible problem - invalid input data. Please check. \nAlso you need to resize caffe::Net when  amount of proposal object is changed.\nAlso please show me next things:\n1) caffe net proto, which you use\n2) how you get the value of the network output\n. ",
    "vikiboy": "@mdering I got the same with a Tesla K20x. I am using a custom dataset and train_faster_rcnn_alt_opt. Any idea on how to solve this issue ? \n. I haven't updated the drivers. I could restart it and check again, however, since it's a server, it would be helpful if there were any other options that doesn't entail restarting. \n. @mdering I got the same with a Tesla K20x. I am using a custom dataset and train_faster_rcnn_alt_opt. Any idea on how to solve this issue ? \n. I haven't updated the drivers. I could restart it and check again, however, since it's a server, it would be helpful if there were any other options that doesn't entail restarting. \n. ",
    "Designbook1": "Hi, @javier-castan I've met the problem  and how to solve it? Could you tell me?. Hi, @javier-castan I've met the problem  and how to solve it? Could you tell me?. ",
    "lolongcovas": "from scracth. anyway, I've solved this problem by adding more training samples.\n. Thanks\n. from scracth. anyway, I've solved this problem by adding more training samples.\n. Thanks\n. ",
    "ghost": "Hmm - yeah the matlab is fairly slow, as it runs one image at a time. Using dlib's Python bindings and Python's multiprocessing library (to use all 8 cores on my CPU), I was able to get SS results on ~80k images in about 2 hours. The results, as mentioned above, were 78 GB in size... ~ 1 MB per image.\nI'm sure there's probably some way for me to read the SS files in batches from disk, as I'm sure the images are handled in that way. I guess I'll work toward that.\n. @sunshineatnoon I have been able to use dlib's SS on new images, with the trained fast-rcnn models available for download. I have no basis to claim that dlib's implementation gives equal accuracy, but I can say it does seem to work well. One difference is that it appears dlib's SS provides more boxes than the Matlab implementation. Passing all of the boxes into the fast-rcnn function at once would sometimes cause an out-of-memory error on my GPU, so I ended up passing them in batches.\n. @sunshineatnoon @kaifeichen @rbgirshick   I got the same problem. Did you solve this problem ???\n. Solved. According to test.py:\nfor j in xrange(1, imdb.num_classes):\n    inds = np.where((scores[:, j] > thresh[j]) &\n                    (roidb[i]['gt_classes'] == 0))[0]\n. got it, thanks.\n. do you fix it , i have same error , how to fix it ?\n. I have the same exact error on running demo example. I'm currently running rcnn on a different caffe build (which was setup before). Had to change _init_paths.py to point to that caffe installation. Any help to correct this would be great. Thanks.. Hmm - yeah the matlab is fairly slow, as it runs one image at a time. Using dlib's Python bindings and Python's multiprocessing library (to use all 8 cores on my CPU), I was able to get SS results on ~80k images in about 2 hours. The results, as mentioned above, were 78 GB in size... ~ 1 MB per image.\nI'm sure there's probably some way for me to read the SS files in batches from disk, as I'm sure the images are handled in that way. I guess I'll work toward that.\n. @sunshineatnoon I have been able to use dlib's SS on new images, with the trained fast-rcnn models available for download. I have no basis to claim that dlib's implementation gives equal accuracy, but I can say it does seem to work well. One difference is that it appears dlib's SS provides more boxes than the Matlab implementation. Passing all of the boxes into the fast-rcnn function at once would sometimes cause an out-of-memory error on my GPU, so I ended up passing them in batches.\n. @sunshineatnoon @kaifeichen @rbgirshick   I got the same problem. Did you solve this problem ???\n. Solved. According to test.py:\nfor j in xrange(1, imdb.num_classes):\n    inds = np.where((scores[:, j] > thresh[j]) &\n                    (roidb[i]['gt_classes'] == 0))[0]\n. got it, thanks.\n. do you fix it , i have same error , how to fix it ?\n. I have the same exact error on running demo example. I'm currently running rcnn on a different caffe build (which was setup before). Had to change _init_paths.py to point to that caffe installation. Any help to correct this would be great. Thanks.. ",
    "awayhome113": "Have you changed parameter of dlib, go to the source of dlib and find something looks like this: find_candidate_object_locations(img, rects, linspace(50, 200, 3), min_size).\n. You can refer to this link for help: https://groups.google.com/forum/#!topic/caffe-users/cI1omAfHxNw, and Have you install libboost-python-dev and python-dev?\n. It should be [x1,y1,x2,y2], meaning [left, top, right, bottom].\n. Have you changed parameter of dlib, go to the source of dlib and find something looks like this: find_candidate_object_locations(img, rects, linspace(50, 200, 3), min_size).\n. You can refer to this link for help: https://groups.google.com/forum/#!topic/caffe-users/cI1omAfHxNw, and Have you install libboost-python-dev and python-dev?\n. It should be [x1,y1,x2,y2], meaning [left, top, right, bottom].\n. ",
    "yihui-he": "I have this problem too. \nI solve it by add boost_regex to LIBRARIES in Makefile\n263     LIBRARIES += boost_thread stdc++ boost_regex\n. I have this problem too. \nI solve it by add boost_regex to LIBRARIES in Makefile\n263     LIBRARIES += boost_thread stdc++ boost_regex\n. ",
    "andeyeluguo": "brother yihui's solution can sovle my problem ,thank you.. brother yihui's solution can sovle my problem ,thank you.. ",
    "kaifeichen": "+1\n. Hi @haimansx,  I think @sunshineatnoon is right. I just used the GPU version and it worked.\n. +1\n. Hi @haimansx,  I think @sunshineatnoon is right. I just used the GPU version and it worked.\n. ",
    "tanjoreg": "Hi @sunshineatnoon I have followed your blog to train Fast-RCNN my own dataset and i have stumbled upon this error. I am using CPU Mode. Were you able to solve this issue?\n@kaifeichen Does that mean Fast-RCNN is not implemented for CPU mode? Any updates on this would help me.\n. @sunshineatnoon Thank you very much,for the swift reply. Your blog helped me a lot.\nI have tested the imagenet demo on CPU and it works. I was trying to train my own dataset, which resulted in the error. Does that mean I have hit the wall with CPU mode?\n. Hi @sunshineatnoon I have followed your blog to train Fast-RCNN my own dataset and i have stumbled upon this error. I am using CPU Mode. Were you able to solve this issue?\n@kaifeichen Does that mean Fast-RCNN is not implemented for CPU mode? Any updates on this would help me.\n. @sunshineatnoon Thank you very much,for the swift reply. Your blog helped me a lot.\nI have tested the imagenet demo on CPU and it works. I was trying to train my own dataset, which resulted in the error. Does that mean I have hit the wall with CPU mode?\n. ",
    "hnflee": "+1. +1. ",
    "athulase": "@sunshineatnoon: Were you able to train R-CNN in cpu? or is it still in the status of not running training on cpu mode?. @sunshineatnoon: Were you able to train R-CNN in cpu? or is it still in the status of not running training on cpu mode?. ",
    "HamdiHamed1992": "I am facing the same problem!\nAny help?\nIf someone implemented those 2 functions in cpp, can he post the code? . I am facing the same problem!\nAny help?\nIf someone implemented those 2 functions in cpp, can he post the code? . ",
    "markusnagel": "I had the same issue. Further investigations showed that the error comes at 'image 29'. This image has 0 gt annotations. Somehow it seems that scipy.sparse.vstack have a bug with concatenating sparse arrays that have a dimension with 0 length. You can manually handle that case in imdb.py by changing line 224 to:\nif np.shape(a[i]['gt_overlaps'])[0] > 0:\n    a[i]['gt_overlaps'] = scipy.sparse.vstack([a[i]['gt_overlaps'],\n                                                                    b[i]['gt_overlaps']])\nelse:\n    a[i]['gt_overlaps'] = b[i]['gt_overlaps']\n. I got a mAP of .196 on the validation set with fast rcnn using edge boxes 70 as proposals and the VGG16.  This is very similar what Ross have on the leader-board of the test-dev set. With using the VGG M net the mAP drops to .155 for me.\nHere are all scores (see coco evaluation for their definition)\nvgg_cnn_m_1024_fast_rcnn_iter_280000.caffemodel 0.155   0.315   0.138   0.023   0.150   0.270   0.179   0.260   0.266   0.059   0.292   0.445\nvgg16_fast_rcnn_iter_240000.caffemodel  0.196   0.375   0.186   0.034   0.204   0.332   0.209   0.302   0.308   0.079   0.352   0.505\n. I had the same issue. Further investigations showed that the error comes at 'image 29'. This image has 0 gt annotations. Somehow it seems that scipy.sparse.vstack have a bug with concatenating sparse arrays that have a dimension with 0 length. You can manually handle that case in imdb.py by changing line 224 to:\nif np.shape(a[i]['gt_overlaps'])[0] > 0:\n    a[i]['gt_overlaps'] = scipy.sparse.vstack([a[i]['gt_overlaps'],\n                                                                    b[i]['gt_overlaps']])\nelse:\n    a[i]['gt_overlaps'] = b[i]['gt_overlaps']\n. I got a mAP of .196 on the validation set with fast rcnn using edge boxes 70 as proposals and the VGG16.  This is very similar what Ross have on the leader-board of the test-dev set. With using the VGG M net the mAP drops to .155 for me.\nHere are all scores (see coco evaluation for their definition)\nvgg_cnn_m_1024_fast_rcnn_iter_280000.caffemodel 0.155   0.315   0.138   0.023   0.150   0.270   0.179   0.260   0.266   0.059   0.292   0.445\nvgg16_fast_rcnn_iter_240000.caffemodel  0.196   0.375   0.186   0.034   0.204   0.332   0.209   0.302   0.308   0.079   0.352   0.505\n. ",
    "twtygqyy": "Can you show the mAP of fast rcnn on MS COCO dataset?\n. @markusnagel Thanks a lot for your help. The problem solved after modifying imdb.py as you mentioned.\nI think we could close this issue.\n. Can you show the mAP of fast rcnn on MS COCO dataset?\n. @markusnagel Thanks a lot for your help. The problem solved after modifying imdb.py as you mentioned.\nI think we could close this issue.\n. ",
    "mbahacker": "Closing the issue as I figured out the solution by myself. It's just how the arrays are stored hierarchically\n. Here is the sample code I used. Let me know if you need further help:\npython\nwith h5py.File('/home/.hf5','r') as hf:\n        print('List of arrays in this file: \\n', hf.keys())\n        data = hf.get('box_0')\n        obj_proposals = np.array(data)\n        for i in range(1,(len(hf.keys())-1)):\n            keyName='box_'+str(i)\n            keyData=hf.get(keyName)\n            obj_proposals=np.concatenate((obj_proposals,np.array(keyData)),axis=0)\n. Closing the issue as I figured out the solution by myself. It's just how the arrays are stored hierarchically\n. Here is the sample code I used. Let me know if you need further help:\npython\nwith h5py.File('/home/.hf5','r') as hf:\n        print('List of arrays in this file: \\n', hf.keys())\n        data = hf.get('box_0')\n        obj_proposals = np.array(data)\n        for i in range(1,(len(hf.keys())-1)):\n            keyName='box_'+str(i)\n            keyData=hf.get(keyName)\n            obj_proposals=np.concatenate((obj_proposals,np.array(keyData)),axis=0)\n. ",
    "shinexunju": "Hi, I got the same error.\nCould you share your solutions? Thanks.\n. Thanks for your quick reply and good suggestion.\nIt is weird because the bounding box is generated by selective search in my experiments.\n. Thank for your hints @sxjzwq .\nI think the bounding-boxes returned from the bounding-box regresser may not follow the \"well-known\" convention. In other words, the area of returned bounding-box may be 0 or negative. @rbgirshick \n. Hi, I got the same error.\nCould you share your solutions? Thanks.\n. Thanks for your quick reply and good suggestion.\nIt is weird because the bounding box is generated by selective search in my experiments.\n. Thank for your hints @sxjzwq .\nI think the bounding-boxes returned from the bounding-box regresser may not follow the \"well-known\" convention. In other words, the area of returned bounding-box may be 0 or negative. @rbgirshick \n. ",
    "sxjzwq": "It is the problem of proposal bounding box.\nYou should check whether your proposal bounding box's width and height bigger than 0. MCG sometimes return a non-valued bounding box. Selective search has no such problem.\nAnyway, check your bounding box at first.\n. Yes. And if the area of the bounding-box is zero, there will be more problems in the training.\nIn fast-rcnn/lib/roi_data_layer/roidb.py, line114-line115, \n    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\nif the ex_weight == 0, the bounding-box regression targets will be a extremely large number.\nI simply remove those  zero area bounding boxes before this step, \n. It is the problem of proposal bounding box.\nYou should check whether your proposal bounding box's width and height bigger than 0. MCG sometimes return a non-valued bounding box. Selective search has no such problem.\nAnyway, check your bounding box at first.\n. Yes. And if the area of the bounding-box is zero, there will be more problems in the training.\nIn fast-rcnn/lib/roi_data_layer/roidb.py, line114-line115, \n    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\nif the ex_weight == 0, the bounding-box regression targets will be a extremely large number.\nI simply remove those  zero area bounding boxes before this step, \n. ",
    "liuwenran": "@sid027 I changed the num_output of \"cls_score\" layer  to 201 and the num_output of \"bboxes_pred\" layer to 804  in train.prototxt. And I trained it on imagenet dataset.\n. @sid027 because  there are 200 classes in imagenet dataset \uff0cand 201 classes with \u201cbackground\u201d \uff0c804  = 201 * 4 are bounding boxes \n. because a bounding box is [xmin,ymin,xmax,ymax]\uff0cthere are four numbers for a box. [xmin,ymin] is the left-top pixel location of this box in original image ,and [xmax,ymax] is the right-bottom.\n. @sid027 you needn't to put each class in a subfolder because images will be shuffled before training,and background will be generated automatically.\n. @sid027 I changed the num_output of \"cls_score\" layer  to 201 and the num_output of \"bboxes_pred\" layer to 804  in train.prototxt. And I trained it on imagenet dataset.\n. @sid027 because  there are 200 classes in imagenet dataset \uff0cand 201 classes with \u201cbackground\u201d \uff0c804  = 201 * 4 are bounding boxes \n. because a bounding box is [xmin,ymin,xmax,ymax]\uff0cthere are four numbers for a box. [xmin,ymin] is the left-top pixel location of this box in original image ,and [xmax,ymax] is the right-bottom.\n. @sid027 you needn't to put each class in a subfolder because images will be shuffled before training,and background will be generated automatically.\n. ",
    "alilemus": "Hi @liuwenran! I've been trying to train this with the imagenet dataset, but have no idea on how to build the dataset (I have the JPEG files and Annotations, etc), but cant understand how to build the imdb.\nany pointers?\n. @RafaRuiz did you ever manage to get the ImageNet model? I have been wanting to do that but have not yet succeeded :(\n. so @sid027, in order to train fast rcnn on imagenet data, according to: https://github.com/zeyuanxy/fast-rcnn/tree/master/help/train I have to:\n1. create a new imagenet.py (based on pascal_voc.py)\n2. modify factory.py\n3. run selective search\n4. modify prototxt\n5. train\nare those the right steps? I am pretty lost in the first step, do you happen to have a working example?\n. @sid027 Thanks for the quick reply!!!\nI'm sorry it took me so long to re-reply :(, I actually found this blog: http://sunshineatnoon.github.io/Train-fast-rcnn-model-on-imagenet-without-matlab/ which I followed and managed to train imagenet. \ngood luck!\n. Hi @liuwenran! I've been trying to train this with the imagenet dataset, but have no idea on how to build the dataset (I have the JPEG files and Annotations, etc), but cant understand how to build the imdb.\nany pointers?\n. @RafaRuiz did you ever manage to get the ImageNet model? I have been wanting to do that but have not yet succeeded :(\n. so @sid027, in order to train fast rcnn on imagenet data, according to: https://github.com/zeyuanxy/fast-rcnn/tree/master/help/train I have to:\n1. create a new imagenet.py (based on pascal_voc.py)\n2. modify factory.py\n3. run selective search\n4. modify prototxt\n5. train\nare those the right steps? I am pretty lost in the first step, do you happen to have a working example?\n. @sid027 Thanks for the quick reply!!!\nI'm sorry it took me so long to re-reply :(, I actually found this blog: http://sunshineatnoon.github.io/Train-fast-rcnn-model-on-imagenet-without-matlab/ which I followed and managed to train imagenet. \ngood luck!\n. ",
    "crazylyf": "I modified the corresponding numbers and run the demo, and found out that the result is very bad. what is the possible reason? Does anyone encounter such an problem? thanks!. I modified the corresponding numbers and run the demo, and found out that the result is very bad. what is the possible reason? Does anyone encounter such an problem? thanks!. ",
    "noureldien": "That would be helpful, thank you.\n. That would be helpful, thank you.\n. ",
    "gp335": "I ran into the same problem (using Matlab R2015a) - seems to be that some of the JPEG files I tried to load into Matlab (from ILSVRC2013_DET_val) were loading without the color channels.  You can test to see if this is the same issue by printing out the result of size() on the resulting matrix after you load it by using:\nim = imread(image_filenames{i});\nsize(im)\nIf size() returns with no third dimension or a third dimension of \"1\" then that particular image is going to throw the above error (i.e. it's losing color channels somewhere).  I was in a rush and so just had the matlab script skip over these images when it encountered them (and log their names to a file so I could remove them from the rest of the pipeline)... but would be curious at some point to understand better why they failed.  Occurred with about 2% of images.  Section of the selective_search_rcnn.m script I modified:\noutfileID = fopen([NAME OF LOG FILE], 'w');\nnum_errors = 0;\nall_boxes = {};\nfor i=1:length(image_filenames)\n    im = imread(image_filenames{i});\n    fprintf('filename: %s \\n', image_filenames{i});\n    [x, y, z] = size(im);\n    if z == 1\n        fprintf('Filename with bad dimensions: %s\\n', image_filenames{i});\n        fprintf(outfileID, '%s\\n', image_filenames{i});\n        num_errors = num_errors + 1;\n        continue;\n    end;\n   [REST OF THE FOR LOOP FOLLOWS HERE]\n. I ran into the same problem (using Matlab R2015a) - seems to be that some of the JPEG files I tried to load into Matlab (from ILSVRC2013_DET_val) were loading without the color channels.  You can test to see if this is the same issue by printing out the result of size() on the resulting matrix after you load it by using:\nim = imread(image_filenames{i});\nsize(im)\nIf size() returns with no third dimension or a third dimension of \"1\" then that particular image is going to throw the above error (i.e. it's losing color channels somewhere).  I was in a rush and so just had the matlab script skip over these images when it encountered them (and log their names to a file so I could remove them from the rest of the pipeline)... but would be curious at some point to understand better why they failed.  Occurred with about 2% of images.  Section of the selective_search_rcnn.m script I modified:\noutfileID = fopen([NAME OF LOG FILE], 'w');\nnum_errors = 0;\nall_boxes = {};\nfor i=1:length(image_filenames)\n    im = imread(image_filenames{i});\n    fprintf('filename: %s \\n', image_filenames{i});\n    [x, y, z] = size(im);\n    if z == 1\n        fprintf('Filename with bad dimensions: %s\\n', image_filenames{i});\n        fprintf(outfileID, '%s\\n', image_filenames{i});\n        num_errors = num_errors + 1;\n        continue;\n    end;\n   [REST OF THE FOR LOOP FOLLOWS HERE]\n. ",
    "naoto0804": "Thank you so much!!\n. Thank you so much!!\n. ",
    "heyulinxp": "Do you fix it? I have the same problem.. Do you fix it? I have the same problem.. ",
    "guofeng007": "mee too. mee too. ",
    "fengyuxi55": "you can refer to https://github.com/zeyuanxy/fast-rcnn/tree/master/help/train for some help\n. @frattezi , I think you should go to $FRCN_ROOT/lib and do 'make' before you make caffe and pycaffe\nThe problem is there is module named cython_bbox\n. you can refer to https://github.com/zeyuanxy/fast-rcnn/tree/master/help/train for some help\n. @frattezi , I think you should go to $FRCN_ROOT/lib and do 'make' before you make caffe and pycaffe\nThe problem is there is module named cython_bbox\n. ",
    "kshalini": "@alilemus can you pls post if your approach worked for fast-rCNN or py-faster-rCNN? if it was the former, then I still would like to know, if you generated the proposals file (.mat) for all the input files for all the classes of imagenet and then ran training? did you encounter any size issues with the mat file for so many files. i mean, it works in the example for VOC 2007 which has 20 classes, but when we do this at scale for 100s of categories with each 200+ images, does it work well? \n. @frattezi  do you have a complete set of steps that you can outline to help with the training? \ni am facing the same problem, but not able to find any clear set of steps to get through from end to end, ie what configs and code changes to make, to enable training on my custom dataset. thanks.\n. @alilemus can you pls post if your approach worked for fast-rCNN or py-faster-rCNN? if it was the former, then I still would like to know, if you generated the proposals file (.mat) for all the input files for all the classes of imagenet and then ran training? did you encounter any size issues with the mat file for so many files. i mean, it works in the example for VOC 2007 which has 20 classes, but when we do this at scale for 100s of categories with each 200+ images, does it work well? \n. @frattezi  do you have a complete set of steps that you can outline to help with the training? \ni am facing the same problem, but not able to find any clear set of steps to get through from end to end, ie what configs and code changes to make, to enable training on my custom dataset. thanks.\n. ",
    "arushigarg": "I need .mat files of Imagenet database to recognize objects using Voila Jones. Can anyone guide me from where I can get the same?\n. I need .mat files of Imagenet database to recognize objects using Voila Jones. Can anyone guide me from where I can get the same?\n. ",
    "frattezi": "I'm using the python wrapper described on the readme, you may have to change part of the code.\nOn selective_search. py  comment line 46 :  #os.remove(output_filename) for keeping the  _boxes.m on your tmp files (you can also change the final location of the files).\nYou may have to change the line 40 on the selective_search.m to :  save(output_filename, 'boxes', '-v7');.\nThat worked to me, good luck!\n. @fengyuxi55  Thanks, i did that and it worked!\n. I'm using the python wrapper described on the readme, you may have to change part of the code.\nOn selective_search. py  comment line 46 :  #os.remove(output_filename) for keeping the  _boxes.m on your tmp files (you can also change the final location of the files).\nYou may have to change the line 40 on the selective_search.m to :  save(output_filename, 'boxes', '-v7');.\nThat worked to me, good luck!\n. @fengyuxi55  Thanks, i did that and it worked!\n. ",
    "arash111": "anybody can help please!! I have the same problem and don't know how to prepare '_boxes.mat'file for my own datasets in matlab. I'm using the code proposed here: (https://github.com/vlfeat/matconvnet/blob/master/examples/fast_rcnn/fast_rcnn_demo.m). thanks,. anybody can help please!! I have the same problem and don't know how to prepare '_boxes.mat'file for my own datasets in matlab. I'm using the code proposed here: (https://github.com/vlfeat/matconvnet/blob/master/examples/fast_rcnn/fast_rcnn_demo.m). thanks,. ",
    "guyrose3": "I think another thing is the selection of hard negatives:\nwhen the net bias is wrong, It won't produce the expected svm decision function values,\nwhich in turn would cause to wrong selection of the hard negatives.\n. unfortunately not yet.\nI'm working with my own dataset, not Pascal VOC.\n. I think another thing is the selection of hard negatives:\nwhen the net bias is wrong, It won't produce the expected svm decision function values,\nwhich in turn would cause to wrong selection of the hard negatives.\n. unfortunately not yet.\nI'm working with my own dataset, not Pascal VOC.\n. ",
    "ajdroid": "Try it without the -j8 flag and see if you still get the error.\n. Try it without the -j8 flag and see if you still get the error.\n. Try it without the -j8 flag and see if you still get the error.\n. Try it without the -j8 flag and see if you still get the error.\n. ",
    "RyanCV": "Hi, \nI followed the steps of Fast R-CNN, however, when I run make -j16 && make pycaffe, I also have the following problem. I load cuda/7.5, opencv2.4.10, python2.7.10. \nsrc/caffe/layers/cudnn_conv_layer.cu(142): error: too few arguments in function call\n          detected during instantiation of \"void caffe::CuDNNConvolutionLayer::Backward_gpu(const std::vectorcaffe::Blob> &, const std::vector<__nv_bool, std::allocator<__nv_bool>> &, const std::vectorcaffe::Blob> &) [with Dtype=double]\" \n(159): here\n20 errors detected in the compilation of \"/tmp/tmpxft_0000923f_00000000-16_cudnn_conv_layer.compute_50.cpp1.ii\".\nmake: *** [.build_release/cuda/src/caffe/layers/cudnn_conv_layer.o] Error 1\n. Hi, \nI followed the steps of Fast R-CNN, however, when I run make -j16 && make pycaffe, I also have the following problem. I load cuda/7.5, opencv2.4.10, python2.7.10. \nsrc/caffe/layers/cudnn_conv_layer.cu(142): error: too few arguments in function call\n          detected during instantiation of \"void caffe::CuDNNConvolutionLayer::Backward_gpu(const std::vectorcaffe::Blob> &, const std::vector<__nv_bool, std::allocator<__nv_bool>> &, const std::vectorcaffe::Blob> &) [with Dtype=double]\" \n(159): here\n20 errors detected in the compilation of \"/tmp/tmpxft_0000923f_00000000-16_cudnn_conv_layer.compute_50.cpp1.ii\".\nmake: *** [.build_release/cuda/src/caffe/layers/cudnn_conv_layer.o] Error 1\n. ",
    "moyans": "\u4fee\u6539\u4e0a\u9762\u7684Makefile\u6587\u4ef6\uff08\u4e0d\u662fMakefile.config\uff09\uff1a\nLIBRARIES += glog gflags protobuf leveldb snappy \\  \nlmdb boost_system hdf5_hl hdf5 m \\  \nopencv_core opencv_highgui opencv_imgproc opencv_imgcodecs\n\u4e5f\u5c31\u662f\u5728libraries\u540e\u9762\uff0c\u52a0\u4e0aopencv\u7684\u76f8\u5173\u5e93\u6587\u4ef6\u3002. \u4fee\u6539\u4e0a\u9762\u7684Makefile\u6587\u4ef6\uff08\u4e0d\u662fMakefile.config\uff09\uff1a\nLIBRARIES += glog gflags protobuf leveldb snappy \\  \nlmdb boost_system hdf5_hl hdf5 m \\  \nopencv_core opencv_highgui opencv_imgproc opencv_imgcodecs\n\u4e5f\u5c31\u662f\u5728libraries\u540e\u9762\uff0c\u52a0\u4e0aopencv\u7684\u76f8\u5173\u5e93\u6587\u4ef6\u3002. ",
    "riadhayachi": "you need to use opencv 3\n. you can use python , and run demo.py\n. you need to use opencv 3\n. you can use python , and run demo.py\n. ",
    "bruceko": "Add opencv_imgproc in your Makefile might help.\nLIBRARIES += glog gflags protobuf leveldb snappy \\\nlmdb boost_system hdf5_hl hdf5 m \\\nopencv_core opencv_highgui opencv_imgproc\n. Add opencv_imgproc in your Makefile might help.\nLIBRARIES += glog gflags protobuf leveldb snappy \\\nlmdb boost_system hdf5_hl hdf5 m \\\nopencv_core opencv_highgui opencv_imgproc\n. ",
    "wkentaro": "@sunshineatnoon \nThat's what I was looking for.\nThanks.\n. @sunshineatnoon \nThat's what I was looking for.\nThanks.\n. ",
    "yileo19920925": "hi i met the same question did u make it ?\n. @IdiosyncraticDragon thx! the solution is exactly what u said..~\n. hi i met the same question did u make it ?\n. @IdiosyncraticDragon thx! the solution is exactly what u said..~\n. ",
    "smasoudn": "I still have this issue, although I cleaned the cache folder.\nThe thing is that I get this error when I use few training data! Any  idea?\n. I still have this issue, although I cleaned the cache folder.\nThe thing is that I get this error when I use few training data! Any  idea?\n. ",
    "zhongjunping": "hi, I also still have this problem, have you solved it? thx @smasoudn . hi, I also still have this problem, have you solved it? thx @smasoudn . ",
    "reddypruthvidhar": "I am facing the same problem, let me know if you figured out this @smasoudn @zhongjunping . Cleaning cache folder didn't help.. I am facing the same problem, let me know if you figured out this @smasoudn @zhongjunping . Cleaning cache folder didn't help.. ",
    "FangLinHe": "I just tried this way (but may not be trivial): For images without any target objects / annotation xml file, I make a \"fake\" class called fake_bg, and assign the whole image to this class.  The experimental results (qualitatively) show that it successfully reduces the false positive rate.  One may try it if they are interested.\n. You pressed Ctrl + Z accidentally.  Simply enter fg' in the shell and press Enter, and you will get your process back.\n. I just tried this way (but may not be trivial): For images without any target objects / annotation xml file, I make a \"fake\" class called fake_bg, and assign the whole image to this class.  The experimental results (qualitatively) show that it successfully reduces the false positive rate.  One may try it if they are interested.\n. You pressed Ctrl + Z accidentally.  Simply enterfg' in the shell and press Enter, and you will get your process back.\n. ",
    "xiaoxiongli": "I think you'd better to dig in the Faster RCNN source code and you will find it is easy to add negative samples to the net. make a \"fake\" class maybe has a problem that your \"negtive\" objects seems so different thus this \"fake\" class's output AP or precision is low. i am a \"small white\" in DL, so this is just my opinion...maybe someone else will have better idea\n. I think you'd better to dig in the Faster RCNN source code and you will find it is easy to add negative samples to the net. make a \"fake\" class maybe has a problem that your \"negtive\" objects seems so different thus this \"fake\" class's output AP or precision is low. i am a \"small white\" in DL, so this is just my opinion...maybe someone else will have better idea\n. ",
    "PhDStudentC": "i used  matlab and i could not find the same structure as the fast (FASTER are different)\ni can not find the num_classes and same other fonction \n. @rbgirshick  @ShaoqingRen\ncould you please tell me what  should i change to make it work on other base than the pascal VOC\n. i used  matlab and i could not find the same structure as the fast (FASTER are different)\ni can not find the num_classes and same other fonction \n. @rbgirshick  @ShaoqingRen\ncould you please tell me what  should i change to make it work on other base than the pascal VOC\n. ",
    "Pessanha24": "Hi @PhDStudentC ,\nDid you manage to train Faster-RCNN on your own database? Can you provide me more information about it?\n. Hi @PhDStudentC ,\nDid you manage to train Faster-RCNN on your own database? Can you provide me more information about it?\n. ",
    "debidatta": "I agree with this. \n. I agree with this. \n. ",
    "optimus1009": "i have also occur this problem\n. i have also occur this problem\n. ",
    "wangfei123": "hi\uff0c\n  how create annotation xml?\nthanks!\n. hi\uff0c\n  how create annotation xml?\nthanks!\n. ",
    "korkinof": "Anything on how to increase precision?\n. Anything on how to increase precision?\n. ",
    "Supersak80": "@leefionglee an updates on this? I'm also having the same problem with high recall and low precision due to false positives.\n@nw362 what is the intuition behind doing what you suggested?\n. @leefionglee an updates on this? I'm also having the same problem with high recall and low precision due to false positives.\n@nw362 what is the intuition behind doing what you suggested?\n. ",
    "roleiland": "I had the same error. If you just installed your Opencv library, try to update the paths with:\nldconfig\n. I had the same error. If you just installed your Opencv library, try to update the paths with:\nldconfig\n. ",
    "shengwu-asu": "I have the same problem. First, I compile and install OpenCV into my home directory (for lack of root privilege); Then, add LD_LIBRARY_PATH environment variable into my ~/.bash_profile, set it's value to OpenCV's lib directory; Finally, execute \"source ~/.bash_profile\" command to enable the update. The problem solved.\n. I have the same problem. First, I compile and install OpenCV into my home directory (for lack of root privilege); Then, add LD_LIBRARY_PATH environment variable into my ~/.bash_profile, set it's value to OpenCV's lib directory; Finally, execute \"source ~/.bash_profile\" command to enable the update. The problem solved.\n. ",
    "uracilForever": "I have found the mistake and solved it. The result of selective search is not good and it's not suitable to the objection that I remarked. \nThank you,@FangLinHe. It seems that I can't reply you directly.\n. I have found the mistake and solved it. The result of selective search is not good and it's not suitable to the objection that I remarked. \nThank you,@FangLinHe. It seems that I can't reply you directly.\n. ",
    "Cogito2012": "@nw362 No such images, all my training images had at lest one annotations/GT box\n. @The error ocurred at the line 36 in the file ./lib/fast_rcnn/train.py\nline 36: self.solver = caffe.SGDSolver(solver_prototxt)\n. @nw362 The same error happened when I run the original version with voc2007 datasets. I doubt the problem is that I'm not using rbg's caffe. My caffe comes from @happyear. (https://github.com/happynear/caffe-windows), Mybe his caffe is not support for fast-rcnn training...or my caffe compiling is not right.\n. @nw362 No such images, all my training images had at lest one annotations/GT box\n. @The error ocurred at the line 36 in the file ./lib/fast_rcnn/train.py\nline 36: self.solver = caffe.SGDSolver(solver_prototxt)\n. @nw362 The same error happened when I run the original version with voc2007 datasets. I doubt the problem is that I'm not using rbg's caffe. My caffe comes from @happyear. (https://github.com/happynear/caffe-windows), Mybe his caffe is not support for fast-rcnn training...or my caffe compiling is not right.\n. ",
    "sallymmx": "I have solved this problem, by change the \n./tools/test_net.py --gpu 1 --def models/VGG16/test.prototxt \\\n    --net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000.caffemodel\nto \n./tools/test_net.py --gpu 0 --def models/VGG16/test.prototxt \\\n    --net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000.caffemodel\n. I have solved this problem, by change the \n./tools/test_net.py --gpu 1 --def models/VGG16/test.prototxt \\\n    --net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000.caffemodel\nto \n./tools/test_net.py --gpu 0 --def models/VGG16/test.prototxt \\\n    --net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000.caffemodel\n. ",
    "zeroAska": "@ericromanenghi   Which version of CUDNN are you using? Thanks.\n. @ericromanenghi   Which version of CUDNN are you using? Thanks.\n. ",
    "urumican": "I have similar problem.\n[libprotobuf WARNING google/protobuf/io/coded_stream.cc:537] Reading dangerously large protocol message.  If the me                                                                                                                          ssage turns out to be larger than 1073741824 bytes, parsing will be halted for security reasons.  To increase the l                                                                                                                          imit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.                                                                                                                          h.\n[libprotobuf WARNING google/protobuf/io/coded_stream.cc:78] The total number of bytes read was 553432081\nSegmentation fault (core dumped)\n. I have similar problem.\n[libprotobuf WARNING google/protobuf/io/coded_stream.cc:537] Reading dangerously large protocol message.  If the me                                                                                                                          ssage turns out to be larger than 1073741824 bytes, parsing will be halted for security reasons.  To increase the l                                                                                                                          imit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.                                                                                                                          h.\n[libprotobuf WARNING google/protobuf/io/coded_stream.cc:78] The total number of bytes read was 553432081\nSegmentation fault (core dumped)\n. ",
    "ujsyehao": "@urumican the reason of the error is not having enough GPU memory. @urumican the reason of the error is not having enough GPU memory. ",
    "panfengli": "When output the model result, it will get this error:\nTraceback (most recent call last):\n  File \"./tools/train_net.py\", line 92, in <module>\n    max_iters=args.max_iters)\n  File \"/home/hust/fast-rcnn/tools/../lib/fast_rcnn/train.py\", line 123, in train_net\n    sw.train_model(max_iters)\n  File \"/home/hust/fast-rcnn/tools/../lib/fast_rcnn/train.py\", line 98, in train_model\n    self.snapshot()\n  File \"/home/hust/fast-rcnn/tools/../lib/fast_rcnn/train.py\", line 56, in snapshot\n    orig_0 = net.params['bbox_pred'][0].data.copy()\nKeyError: 'bbox_pred'\n. Oh, I find it is another error, since I have renamed bbox_pred to bbox_pred_inria in prototxt\n. When output the model result, it will get this error:\nTraceback (most recent call last):\n  File \"./tools/train_net.py\", line 92, in <module>\n    max_iters=args.max_iters)\n  File \"/home/hust/fast-rcnn/tools/../lib/fast_rcnn/train.py\", line 123, in train_net\n    sw.train_model(max_iters)\n  File \"/home/hust/fast-rcnn/tools/../lib/fast_rcnn/train.py\", line 98, in train_model\n    self.snapshot()\n  File \"/home/hust/fast-rcnn/tools/../lib/fast_rcnn/train.py\", line 56, in snapshot\n    orig_0 = net.params['bbox_pred'][0].data.copy()\nKeyError: 'bbox_pred'\n. Oh, I find it is another error, since I have renamed bbox_pred to bbox_pred_inria in prototxt\n. ",
    "ChangyuHu": "I get the same error to you \n. i chiose the version of  caffe-0.999,and want complie it ,but some error in the complie process,my project platform is latest version caffe +ubuntu 14.04+matlab 2014a. when i run the demo of (the project of rbgirshick )DeepPyramid .i get the error of Error using caffe Unknown command 'init'.who can tell me the reson .tanks a lot \n. I get the same error to you \n. i chiose the version of  caffe-0.999,and want complie it ,but some error in the complie process,my project platform is latest version caffe +ubuntu 14.04+matlab 2014a. when i run the demo of (the project of rbgirshick )DeepPyramid .i get the error of Error using caffe Unknown command 'init'.who can tell me the reson .tanks a lot \n. ",
    "suresh480": "Build succeeded.\n\"/Users/harikrishna/FirebaseBinding/objc-automatic-master/bindings/GTMSessionFetcher.Core/GTMSessionFetcher.Core.csproj\" (default target) (1) ->\n(_GenerateCompileInputs target) -> \n  /Library/Frameworks/Mono.framework/Versions/4.8.1/lib/mono/msbuild/15.0/bin/Microsoft.Common.CurrentVersion.targets(3105,5): warning MSB9004: ManifestResourceWithNoCulture item type is deprecated. Emit EmbeddedResource items instead, with metadata WithCulture='false', Type='Resx', and optional LogicalName. [/Users/harikrishna/FirebaseBinding/objc-automatic-master/bindings/GTMSessionFetcher.Core/GTMSessionFetcher.Core.csproj]\n1 Warning(s)\n0 Error(s)\n\nTime Elapsed 00:00:01.00\nAttempting to build package from 'GTMSessionFetcher.Core.nuspec'.\nSuccessfully created package '/Users/harikrishna/FirebaseBinding/objc-automatic-master/bindings/GTMSessionFetcher.Core/DT.Xamarin.GTMSessionFetcher.Core.1.1.11.nupkg'.\n~/FirebaseBinding/objc-automatic-master/bindings\nUnknown command: 'init'\nBuild succes but getting Unknown command: 'init' in terminal can you give an suggestion. \nBuild succeeded.\n\"/Users/harikrishna/FirebaseBinding/objc-automatic-master/bindings/GTMSessionFetcher.Core/GTMSessionFetcher.Core.csproj\" (default target) (1) ->\n(_GenerateCompileInputs target) -> \n  /Library/Frameworks/Mono.framework/Versions/4.8.1/lib/mono/msbuild/15.0/bin/Microsoft.Common.CurrentVersion.targets(3105,5): warning MSB9004: ManifestResourceWithNoCulture item type is deprecated. Emit EmbeddedResource items instead, with metadata WithCulture='false', Type='Resx', and optional LogicalName. [/Users/harikrishna/FirebaseBinding/objc-automatic-master/bindings/GTMSessionFetcher.Core/GTMSessionFetcher.Core.csproj]\n1 Warning(s)\n0 Error(s)\n\nTime Elapsed 00:00:01.00\nAttempting to build package from 'GTMSessionFetcher.Core.nuspec'.\nSuccessfully created package '/Users/harikrishna/FirebaseBinding/objc-automatic-master/bindings/GTMSessionFetcher.Core/DT.Xamarin.GTMSessionFetcher.Core.1.1.11.nupkg'.\n~/FirebaseBinding/objc-automatic-master/bindings\nUnknown command: 'init'\nBuild succes but getting Unknown command: 'init' in terminal can you give an suggestion. ",
    "yinyinl": "Also, I don't have access to MATLAB, so tried using octave to call the evaluations. It goes to the step that finished writing the detection boxes into \"comp4-id_det_test_class.txt\", then gives me error:\n\naeroplane: pr: load: 1862/2476\naeroplane: pr: load: 1916/2476\naeroplane: pr: load: 1971/2476\naeroplane: pr: load: 2021/2476\naeroplane: pr: load: 2076/2476\naeroplane: pr: load: 2132/2476\naeroplane: pr: load: 2191/2476\naeroplane: pr: load: 2244/2476\naeroplane: pr: load: 2293/2476\naeroplane: pr: load: 2347/2476\naeroplane: pr: load: 2403/2476\naeroplane: pr: load: 2462/2476\nerror: textread: could not open '/home/users/xxxxx/temp/fast-rcnn/data/VOCdevkit2007/results/VOC2007/Main/comp4-16041_det_val_aeroplane.txt' for reading\nerror: called from:\nerror:   /usr/share/octave/3.8.1/m/io/textread.m at line 82, column 5\nerror:   /home/users/xxxxx/temp/fast-rcnn/data/VOCdevkit2007/VOCcode/VOCevaldet.m at line 30, column 28\nerror:   /home/users/xxxxx/temp/fast-rcnn/lib/datasets/VOCdevkit-matlab-wrapper/voc_eval.m at line 36, column 19\nerror:   /home/users/xxxxx/temp/fast-rcnn/lib/datasets/VOCdevkit-matlab-wrapper/voc_eval.m at line 8, column 10\n\nSomewhere in the eval matlab script, it wants to look for \"....val..\" instead of \"....test...\", even though the scripts wrote into \"....test...\". Without too much digging myself, I am wondering if you've seen any error like this.\nThanks!\n. Thank you so much!\n. Also, I don't have access to MATLAB, so tried using octave to call the evaluations. It goes to the step that finished writing the detection boxes into \"comp4-id_det_test_class.txt\", then gives me error:\n\naeroplane: pr: load: 1862/2476\naeroplane: pr: load: 1916/2476\naeroplane: pr: load: 1971/2476\naeroplane: pr: load: 2021/2476\naeroplane: pr: load: 2076/2476\naeroplane: pr: load: 2132/2476\naeroplane: pr: load: 2191/2476\naeroplane: pr: load: 2244/2476\naeroplane: pr: load: 2293/2476\naeroplane: pr: load: 2347/2476\naeroplane: pr: load: 2403/2476\naeroplane: pr: load: 2462/2476\nerror: textread: could not open '/home/users/xxxxx/temp/fast-rcnn/data/VOCdevkit2007/results/VOC2007/Main/comp4-16041_det_val_aeroplane.txt' for reading\nerror: called from:\nerror:   /usr/share/octave/3.8.1/m/io/textread.m at line 82, column 5\nerror:   /home/users/xxxxx/temp/fast-rcnn/data/VOCdevkit2007/VOCcode/VOCevaldet.m at line 30, column 28\nerror:   /home/users/xxxxx/temp/fast-rcnn/lib/datasets/VOCdevkit-matlab-wrapper/voc_eval.m at line 36, column 19\nerror:   /home/users/xxxxx/temp/fast-rcnn/lib/datasets/VOCdevkit-matlab-wrapper/voc_eval.m at line 8, column 10\n\nSomewhere in the eval matlab script, it wants to look for \"....val..\" instead of \"....test...\", even though the scripts wrote into \"....test...\". Without too much digging myself, I am wondering if you've seen any error like this.\nThanks!\n. Thank you so much!\n. ",
    "ranju2015": "@shawn-tian Can I have your model file with Edge box proposal? Why did you choose Edge box over RPN? Please explain if possible? . @Parry26 Have you got any improvement?  Please let me know, I am looking for the same.. @shawn-tian Can I have your model file with Edge box proposal? Why did you choose Edge box over RPN? Please explain if possible? . @Parry26 Have you got any improvement?  Please let me know, I am looking for the same.. ",
    "Nicholas0304": "I have the same problem. Have you solved it?. I have the same problem. Have you solved it?. ",
    "xingkongliang": "@HarisIqbal88 hello!Did you solve this problem? I have the same issue like you, so could you please give me some suggestions?Thank you very much.\n. @alviur Hi, do you solve this problem? I have no idea.How do you get the object proposals?\n. @HarisIqbal88 alviur Thank you very much, I solved the problem. I found that there is negative number \uff08coordinate\uff09in the ground truth data. The negative number produce a nan problem, so I fix it to zero. This problem is solved. @alviur \n. @HarisIqbal88 hello!Did you solve this problem? I have the same issue like you, so could you please give me some suggestions?Thank you very much.\n. @alviur Hi, do you solve this problem? I have no idea.How do you get the object proposals?\n. @HarisIqbal88 alviur Thank you very much, I solved the problem. I found that there is negative number \uff08coordinate\uff09in the ground truth data. The negative number produce a nan problem, so I fix it to zero. This problem is solved. @alviur \n. ",
    "alviur": "Same issue here with my own dataset. drop the base_lr did not work. @xingkongliang  seems an issue with the input data, with some part of my dataset works.\n. Same issue here with my own dataset. drop the base_lr did not work. @xingkongliang  seems an issue with the input data, with some part of my dataset works.\n. ",
    "HarisIqbal88": "Hi all, yes I solved the problem. In my case, the problem was with the number of proposals per image. So, if reducing learning rate (base_lr=0) does not help, you might want to go to config file and see how many positive and negative proposals are be sampled from each image while training. Then, check if you have an image in your dataset with positive or negative proposals less than the numbers you got from config file, and there lies your problem. Basically, if some entries for calculating loss_bbx are missing, they are taken as nan and as the loss_bbx is defined as norm of difference between bbxs, it turns out to be nan.\n. @saiprabhakar well, it does not make sense to give an image with very few proposals for training. You should either increase number of proposal boxes by altering some parameters in you proposal generating algorithm or ignore the image entirely in python interface. There is no need to modify loss layer (and I did not do it as well).\n. @saiprabhakar   By number of proposals, I meant both positive and negative classes seperately will have to fulfill the condition mentioned in the config file. As far as Caltech with ACF is concerned, there are some images with absolutely no positive class. Then, you either ignore those images or you put some meaningless dummy bounding box of zero area so that they dont hurt the training process.\n. @saiprabhakar There are two things to consider here:\nFirst, decreasing threshold to 0.1 may remove the \"nan\" from training error but that does not mean its the right thing to do. Decreasing this thresholf to 0.1 means that a mere 10% overlap with ground truth will be considered a valid proposal. You have to decide for your problem if this is a right thing to do. Remmeber, the goal is not to magically vanish \"nan\" from the display screen or log file.\nSecond, there are images with absolutely no ground truth positive classes (i.e., images with no pedestrians) in Caltech Dataset. When those will come for training, your 0.1 technique is bound to fail for them because there will be no positive bounding boxes to consider for training which is being sought by the network for training. Thus, the only way around this problem is to either remove these images from the training or come up with some dummy bounding boxes in ground truth (as they will automatically be appended to proposals, you do not need to add them to proposals as well).\n. @saiprabhakar I used ACF proposal but I changed many variables in the code to produce many proposals (typically more than 500 per image). Unfortunately, now I can not access the code as it was in my internship account which is now closed. Therefore, I can not provide you with those variations. However, the quality of proposals does not matter much. If you can just duplicate already existing proposals with some suitable noise model, it should be good enough. Remember, you are also training your network. So, it will take care of that.\n. Hi all, yes I solved the problem. In my case, the problem was with the number of proposals per image. So, if reducing learning rate (base_lr=0) does not help, you might want to go to config file and see how many positive and negative proposals are be sampled from each image while training. Then, check if you have an image in your dataset with positive or negative proposals less than the numbers you got from config file, and there lies your problem. Basically, if some entries for calculating loss_bbx are missing, they are taken as nan and as the loss_bbx is defined as norm of difference between bbxs, it turns out to be nan.\n. @saiprabhakar well, it does not make sense to give an image with very few proposals for training. You should either increase number of proposal boxes by altering some parameters in you proposal generating algorithm or ignore the image entirely in python interface. There is no need to modify loss layer (and I did not do it as well).\n. @saiprabhakar   By number of proposals, I meant both positive and negative classes seperately will have to fulfill the condition mentioned in the config file. As far as Caltech with ACF is concerned, there are some images with absolutely no positive class. Then, you either ignore those images or you put some meaningless dummy bounding box of zero area so that they dont hurt the training process.\n. @saiprabhakar There are two things to consider here:\nFirst, decreasing threshold to 0.1 may remove the \"nan\" from training error but that does not mean its the right thing to do. Decreasing this thresholf to 0.1 means that a mere 10% overlap with ground truth will be considered a valid proposal. You have to decide for your problem if this is a right thing to do. Remmeber, the goal is not to magically vanish \"nan\" from the display screen or log file.\nSecond, there are images with absolutely no ground truth positive classes (i.e., images with no pedestrians) in Caltech Dataset. When those will come for training, your 0.1 technique is bound to fail for them because there will be no positive bounding boxes to consider for training which is being sought by the network for training. Thus, the only way around this problem is to either remove these images from the training or come up with some dummy bounding boxes in ground truth (as they will automatically be appended to proposals, you do not need to add them to proposals as well).\n. @saiprabhakar I used ACF proposal but I changed many variables in the code to produce many proposals (typically more than 500 per image). Unfortunately, now I can not access the code as it was in my internship account which is now closed. Therefore, I can not provide you with those variations. However, the quality of proposals does not matter much. If you can just duplicate already existing proposals with some suitable noise model, it should be good enough. Remember, you are also training your network. So, it will take care of that.\n. ",
    "cvDreamer": "You are missing Numpy in your include library, if you installed Numpy by brew, try the following:\nexport CFLAGS=-I/usr/local/Cellar/numpy/1.11.0/lib/python2.7/site-packages/numpy/core/include/\nthen make\n. You are missing Numpy in your include library, if you installed Numpy by brew, try the following:\nexport CFLAGS=-I/usr/local/Cellar/numpy/1.11.0/lib/python2.7/site-packages/numpy/core/include/\nthen make\n. ",
    "ivansong1988": "@DerekLee93 I have met the same problem, have you got some solutions?. @DerekLee93 I have met the same problem, have you got some solutions?. ",
    "deboc": "Seems like an issue with boost version :\nhttps://github.com/BVLC/caffe/issues/3494\n. Seems like an issue with boost version :\nhttps://github.com/BVLC/caffe/issues/3494\n. ",
    "dumka": "Hi, \nI faced the same problem. Did you manage to find a solution?\n. Hi, \nI faced the same problem. Did you manage to find a solution?\n. ",
    "gaurav16gupta": "Instead of train_net.py, I have used train_faster_rcnn_alt_opt.py. \npython ./tools/train_faster_rcnn_alt_opt.py --gpu 0 --net_name VGG_CNN_M_1024 --weights data/imagenet_models/VGG_CNN_M_1024.v2.caffemodel --cfg experiments/cfgs/faster_rcnn_alt_opt.yml\n. Instead of train_net.py, I have used train_faster_rcnn_alt_opt.py. \npython ./tools/train_faster_rcnn_alt_opt.py --gpu 0 --net_name VGG_CNN_M_1024 --weights data/imagenet_models/VGG_CNN_M_1024.v2.caffemodel --cfg experiments/cfgs/faster_rcnn_alt_opt.yml\n. ",
    "Utsavz": "Were you able to solve the issue. I am trying to train VOC2007 in fast-rcnn which does not have a .._alt_opt.py file. Please help.\n. Were you able to solve the issue. I am trying to train VOC2007 in fast-rcnn which does not have a .._alt_opt.py file. Please help.\n. ",
    "Lisandro79": "I am facing the same problem. @Stark123  Could you please explain how train_faster_rcnn_alt_opt.py solved your problem?\n. I am facing the same problem. @Stark123  Could you please explain how train_faster_rcnn_alt_opt.py solved your problem?\n. ",
    "Yan-Cy": "Hi guys, the real problem is probably that you used the Caffe compiled in py-faster-rcnn, and the author has changed smooth_L1_loss_layer.cpp file.\nSo recompile the Caffe in the fast-rcnn may help to solve the problem.\n@Lisandro79 @Utsavz @Stark123 \n. Hi guys, the real problem is probably that you used the Caffe compiled in py-faster-rcnn, and the author has changed smooth_L1_loss_layer.cpp file.\nSo recompile the Caffe in the fast-rcnn may help to solve the problem.\n@Lisandro79 @Utsavz @Stark123 \n. ",
    "LEXUSAPI": "i have meet the same problem ,it was because the different version in caffe .. i have meet the same problem ,it was because the different version in caffe .. ",
    "christopher-dG": "I don't think so, you can try to build with it but you will probably get some errors. I used v2 (for this reason I think).\n. I believe it is supposed to be 4classes, it was 84 in the original file with 21 classes. \n. Honestly I don't really remember any of the details. Sorry I can't be of more assistance \ud83d\ude1e . I figured it out, it's really horribly easy. Just change 'if 0:' to 'if 1:' in lib/fast-rcnn/test.py to enable.\n. I don't think so, you can try to build with it but you will probably get some errors. I used v2 (for this reason I think).\n. I believe it is supposed to be 4classes, it was 84 in the original file with 21 classes. \n. Honestly I don't really remember any of the details. Sorry I can't be of more assistance \ud83d\ude1e . I figured it out, it's really horribly easy. Just change 'if 0:' to 'if 1:' in lib/fast-rcnn/test.py to enable.\n. ",
    "Jilliansea": "I have meet the same problem, could you tell me the detail about this?. I meet the same problem in ubuntu14.04, have you solve this problem please?. I have meet the same problem, could you tell me the detail about this?. I meet the same problem in ubuntu14.04, have you solve this problem please?. ",
    "Aspirinkb": "Did you find the answer\uff1f\n. Did you find the answer\uff1f\n. ",
    "dongshuyan": "i have the same question!\n. i have the same question!\n. ",
    "marifnst": "me too\nany solution for this problem ?\n. me too\nany solution for this problem ?\n. ",
    "albanie": "Here is a slightly hacky fix I used to get it working (so use with caution):\n1. Starting in your fast-rcnn directory, delete the .so files in the subdirectories of lib (e.g. using the find command):\nfind lib -name \"*.so\" -delete\n2. Modify this line in setup.py from \"utils.cython_bbox\" to \"utils.bbox\".\n3. Rebuild (i.e. run make in the lib directory)\n4. Change any imports that used utils.cython_bbox to use utils.bbox instead.  This can be done with the following command (again from your fast-rcnn directory):\nfind lib -name \"*.py\" -exec sed -i '' 's/cython_bbox/bbox/g' {} +\nAnd (hopefully) it should now work.\nExplanation: This kind of import error can arise when the .pyx filename doesn't match the extension name given in setup.py.\n. @SHEKOLDA After running make in the third step, did you end up with a file named bbox.so in lib/utils ?\n. Here is a slightly hacky fix I used to get it working (so use with caution):\n1. Starting in your fast-rcnn directory, delete the .so files in the subdirectories of lib (e.g. using the find command):\nfind lib -name \"*.so\" -delete\n2. Modify this line in setup.py from \"utils.cython_bbox\" to \"utils.bbox\".\n3. Rebuild (i.e. run make in the lib directory)\n4. Change any imports that used utils.cython_bbox to use utils.bbox instead.  This can be done with the following command (again from your fast-rcnn directory):\nfind lib -name \"*.py\" -exec sed -i '' 's/cython_bbox/bbox/g' {} +\nAnd (hopefully) it should now work.\nExplanation: This kind of import error can arise when the .pyx filename doesn't match the extension name given in setup.py.\n. @SHEKOLDA After running make in the third step, did you end up with a file named bbox.so in lib/utils ?\n. ",
    "Sharon686": "Thanks to albanie!!! My problem was solved using your method!!!\n. Thanks to albanie!!! My problem was solved using your method!!!\n. ",
    "SHEKOLDA": "@albanie this solution didn't work out for me. I was getting an ImportError: no module named cython_bbox for seemingly the same reason. After following your instructions, the error message simply changed to \"no module named bbox\". Do you have any ideas why could that happen? Thank you.\n. @albanie this solution didn't work out for me. I was getting an ImportError: no module named cython_bbox for seemingly the same reason. After following your instructions, the error message simply changed to \"no module named bbox\". Do you have any ideas why could that happen? Thank you.\n. ",
    "xinyifcc": "There is an easy way to solve this problem\nAnd here's the code! \nunder your rcnn file     ~/rcnn/\n```javascript\ninstall cython\nsudo pip install cython scikit-image easydict\nbuild cython extension\nmake\n```\nproblem solved!\n. ## There is an easy way to solve this problem \nAnd here's the code! \nunder your rcnn file     ~/rcnn/\n```javascript\ninstall cython\nsudo pip install cython scikit-image easydict\nbuild cython extension\nmake\n```\nproblem solved!\n. ",
    "aakashpatel25": "If the issue is still not fixed. Follow the steps of @albanie. Make sure bbox.so is created. Eventually, instead of following step 4, comment the line that contains from utils.bbox. \nThis should make the class run.\nGenerally what happens in Linux machine is that it tries to import bbox.so instead of the python file and hence it throws error. . If the issue is still not fixed. Follow the steps of @albanie. Make sure bbox.so is created. Eventually, instead of following step 4, comment the line that contains from utils.bbox. \nThis should make the class run.\nGenerally what happens in Linux machine is that it tries to import bbox.so instead of the python file and hence it throws error. . ",
    "thangntt": "this issue is not fixed. I step by step follow of  @albanie.\nIn following step 4, I can't find cython_bbox.py file.\n. this issue is not fixed. I step by step follow of  @albanie.\nIn following step 4, I can't find cython_bbox.py file.\n. ",
    "Aminullah6264": "dear i have install cntk by using pip command \npip install \nnow when i run this FasterRCNN.py i got this error\nfrom utils.cython_modules.cython_bbox import bbox_overlaps\nModuleNotFoundError: No module named 'utils.cython_modules.cython_bbox'\nis there any make and build after pip installtion of CNTK\nhelp me to solve my problem\n. @FrancoisPl  i have solved problem.\ncan you tell me how can evaluate FasterRCNN for single image. dear i have install cntk by using pip command \npip install \nnow when i run this FasterRCNN.py i got this error\nfrom utils.cython_modules.cython_bbox import bbox_overlaps\nModuleNotFoundError: No module named 'utils.cython_modules.cython_bbox'\nis there any make and build after pip installtion of CNTK\nhelp me to solve my problem\n. @FrancoisPl  i have solved problem.\ncan you tell me how can evaluate FasterRCNN for single image. ",
    "FrancoisPl": "@Aminullah6264 @thangntt Have you checked whether there is no other utils module in your PYTHONPATH ? It might be searching at the wrong place (which is what happened to me), you don't need the cython_bbox.py (only the cython_bbox.so) for it to work.. @Aminullah6264 @thangntt Have you checked whether there is no other utils module in your PYTHONPATH ? It might be searching at the wrong place (which is what happened to me), you don't need the cython_bbox.py (only the cython_bbox.so) for it to work.. ",
    "pubEgg": "use git clone --recursive loaded,success\n. use git clone --recursive loaded,success\n. ",
    "huiba": "The implementation of ROIPooling layer is in Caffe. You can find it in\nCAFFE_ROOT/src/caffe/layers\n2016-07-12 22:03 GMT+02:00 Ben Johnson notifications@github.com:\n\nI see ROIPooling layers referenced (eg in\nfast-rcnn/models/CaffeNet/test.prototxt), but I'm having trouble finding\nwhere that layer is defined. Anyone have any guidance?\nThanks\nBen\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/rbgirshick/fast-rcnn/issues/127, or mute the thread\nhttps://github.com/notifications/unsubscribe/AOxApBDeL1ATVh_l4B77kYyC4600jZRBks5qU_MUgaJpZM4JKxNI\n.\n. The implementation of ROIPooling layer is in Caffe. You can find it in\nCAFFE_ROOT/src/caffe/layers\n\n2016-07-12 22:03 GMT+02:00 Ben Johnson notifications@github.com:\n\nI see ROIPooling layers referenced (eg in\nfast-rcnn/models/CaffeNet/test.prototxt), but I'm having trouble finding\nwhere that layer is defined. Anyone have any guidance?\nThanks\nBen\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/rbgirshick/fast-rcnn/issues/127, or mute the thread\nhttps://github.com/notifications/unsubscribe/AOxApBDeL1ATVh_l4B77kYyC4600jZRBks5qU_MUgaJpZM4JKxNI\n.\n. \n",
    "bkj": "Thanks! (I had thought it was a python layer, so I was looking in the wrong places..)\n. Thanks! (I had thought it was a python layer, so I was looking in the wrong places..)\n. ",
    "yw155": "'top[0]->cpu_diff()[0]' is not the same as the 'loss'.\n. 'top[0]->cpu_diff()[0]' is not the same as the 'loss'.\n. ",
    "mattm401": "It looks like you're trying to use a version of Caffe not included in fast-rcnn because your missing layers that aren't included in the main branch; you should try reinstalling using the version of caffe in the fast-rcnn branch.. I haven't had a chance to look into resolving this for myself yet, but it would be nice to find a solution.. I am similarly trying to figure out if it's possible to update the windows version of caffe to use this layer.. It looks like you're trying to use a version of Caffe not included in fast-rcnn because your missing layers that aren't included in the main branch; you should try reinstalling using the version of caffe in the fast-rcnn branch.. I haven't had a chance to look into resolving this for myself yet, but it would be nice to find a solution.. I am similarly trying to figure out if it's possible to update the windows version of caffe to use this layer.. ",
    "EvanJP": "Was fixed due to issue with roid_b where the params were being left empty due to issue reading annotation file of my dataset.\n. Was fixed due to issue with roid_b where the params were being left empty due to issue reading annotation file of my dataset.\n. ",
    "LiviuMarina": "Thank you for the fast answer.\nI am using Caffe on Windows.\nI have tried \"where\" instead of \"locate\" in CMD.\nThe file is found : ...FRCN_ROOT2\\caffe\\include\\caffe\\layers\\roi_pooling_layer.hpp.\nI was thinking to the same problem, that I am using the wrong Caffe version.\nBut I don't know which version should I use. An advice will be great for me.\n. Thank you for the fast answer.\nI am using Caffe on Windows.\nI have tried \"where\" instead of \"locate\" in CMD.\nThe file is found : ...FRCN_ROOT2\\caffe\\include\\caffe\\layers\\roi_pooling_layer.hpp.\nI was thinking to the same problem, that I am using the wrong Caffe version.\nBut I don't know which version should I use. An advice will be great for me.\n. ",
    "sunguanxiong": "I think maybe you can just label the box with 'horse', after training, for all labeled 'horse' boxes, you can manually add a label 'animal'...Maybe..\n. I think maybe you can just label the box with 'horse', after training, for all labeled 'horse' boxes, you can manually add a label 'animal'...Maybe..\n. ",
    "ankurgupta7": "Oh. I actually meant it could be horse - animal the first image and blue -  animal in the second image. So they are not exactly the same. In that case I would just have had label 'horseAnimal\". I think I'll edit the issue to reflect this. \nThanks anyways. \n. Oh. I actually meant it could be horse - animal the first image and blue -  animal in the second image. So they are not exactly the same. In that case I would just have had label 'horseAnimal\". I think I'll edit the issue to reflect this. \nThanks anyways. \n. ",
    "nimamahmoudi": "sorry, I should post this into the regarding repository.. sorry, I should post this into the regarding repository.. ",
    "owphoo": "@leyibjtu You can try this caffe(https://github.com/owphoo/caffe_fast_rcnn) which supports ssd, fast-rcnn and faster-rcnn.. @leyibjtu You can try this caffe(https://github.com/owphoo/caffe_fast_rcnn) which supports ssd, fast-rcnn and faster-rcnn.. ",
    "dorianHe": "Already found a solution ? What is it?. Already found a solution ? What is it?. ",
    "Usernamezhx": "the protobuf . i have two version protobuf in my linux.. the protobuf . i have two version protobuf in my linux.. ",
    "CaoYichao": "got it,caffemodel isn't complete.. got it,caffemodel isn't complete.. ",
    "bunschen": "I have the problem too, do you fixed it?. I have the problem too, do you fixed it?. ",
    "zhixianGuo": "@bunschen I have solved the problem.The explanation I found on Google was because it was compiled on different versions of the compiler, which is different from the GCC version.My method is to replace the faster  RCNN  caffe-rcnn inside the fast-rcnn,it worked fine. . @bunschen I have solved the problem.The explanation I found on Google was because it was compiled on different versions of the compiler, which is different from the GCC version.My method is to replace the faster  RCNN  caffe-rcnn inside the fast-rcnn,it worked fine. . ",
    "yayo13": "@zhixianGuo @bunschen what did u mean to 'replace the faster RCNN caffe-rcnn inside the fast-rcnn'? i only git the fast-rcnn.\nand i changed the gcc & g++ from 4.8 to 4.7 but got the same error. @zhixianGuo @bunschen what did u mean to 'replace the faster RCNN caffe-rcnn inside the fast-rcnn'? i only git the fast-rcnn.\nand i changed the gcc & g++ from 4.8 to 4.7 but got the same error. ",
    "Liangxianggithub": "Hello! I am new fast-rcnn learner. But  I could not download the imagenet_models.tgz as well as the selective_search_data.tgz with the link these days. I wonder if you could lend me a hand and email me the models? My email address is 1249408838@qq.com. Thank you very much!. Hello! I am new fast-rcnn learner. But  I could not download the imagenet_models.tgz as well as the selective_search_data.tgz with the link these days. I wonder if you could lend me a hand and email me the models? My email address is 1249408838@qq.com. Thank you very much!. ",
    "cdshdow": "How to get the proposal is isolated from the rcnn, the easiest way is make your newer proposals have same format with the origional ones.. Perhaps you can using the caffe version in faster r-cnn. adding \"import google.protobuf.text_format\" in the train.py. from the paper bottom[0] is 2, but bottom[1] is 128( rois taken from two images). How to get the proposal is isolated from the rcnn, the easiest way is make your newer proposals have same format with the origional ones.. Perhaps you can using the caffe version in faster r-cnn. adding \"import google.protobuf.text_format\" in the train.py. from the paper bottom[0] is 2, but bottom[1] is 128( rois taken from two images). ",
    "alexkreimer": "Do you use caffe as .so or .a?. Do you use caffe as .so or .a?. ",
    "AnshulBasia": "libcaffe.so. libcaffe.so. ",
    "fengsky401": "I have met the same thing. In the terminal ,it tried 20 times, showed\nvisionouc@visionouc-asus:~/LiuHongkun/py-faster-rcnn$ ./data/scripts/fetch_faster_rcnn_models.sh\nDownloading Faster R-CNN demo models (695M)...\n--2017-05-03 17:14:17--  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nResolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 45.58.69.37\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:16:25--  (try: 2)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:18:34--  (try: 3)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:20:45--  (try: 4)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... ^C\nvisionouc@visionouc-asus:~/LiuHongkun/py-faster-rcnn$ ./data/scripts/fetch_faster_rcnn_models.sh\nFile already exists. Checking md5...\nChecksum is incorrect. Need to download again.\nDownloading Faster R-CNN demo models (695M)...\n--2017-05-03 17:22:54--  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nResolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 45.58.69.37\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:25:02--  (try: 2)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:27:11--  (try: 3)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:29:22--  (try: 4)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:31:33--  (try: 5)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:33:45--  (try: 6)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:35:58--  (try: 7)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:38:13--  (try: 8)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:40:28--  (try: 9)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:42:44--  (try:10)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:45:01--  (try:11)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:47:19--  (try:12)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:49:36--  (try:13)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:51:53--  (try:14)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:54:10--  (try:15)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:56:28--  (try:16)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:58:45--  (try:17)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 18:01:02--  (try:18)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 18:03:19--  (try:19)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 18:05:36--  (try:20)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nGiving up.\nUnzipping...\ngzip: stdin: unexpected end of file\ntar: Child returned status 1\ntar: Error is not recoverable: exiting now\nDone. Please run this command again to verify that checksum = ac116844f66aefe29587214272054668.\n. ### I have met the same thing. In the terminal ,it tried 20 times, showed\nvisionouc@visionouc-asus:~/LiuHongkun/py-faster-rcnn$ ./data/scripts/fetch_faster_rcnn_models.sh\nDownloading Faster R-CNN demo models (695M)...\n--2017-05-03 17:14:17--  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nResolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 45.58.69.37\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:16:25--  (try: 2)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:18:34--  (try: 3)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:20:45--  (try: 4)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... ^C\nvisionouc@visionouc-asus:~/LiuHongkun/py-faster-rcnn$ ./data/scripts/fetch_faster_rcnn_models.sh\nFile already exists. Checking md5...\nChecksum is incorrect. Need to download again.\nDownloading Faster R-CNN demo models (695M)...\n--2017-05-03 17:22:54--  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nResolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 45.58.69.37\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:25:02--  (try: 2)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:27:11--  (try: 3)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:29:22--  (try: 4)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:31:33--  (try: 5)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:33:45--  (try: 6)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:35:58--  (try: 7)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:38:13--  (try: 8)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:40:28--  (try: 9)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:42:44--  (try:10)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:45:01--  (try:11)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:47:19--  (try:12)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:49:36--  (try:13)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:51:53--  (try:14)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:54:10--  (try:15)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:56:28--  (try:16)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 17:58:45--  (try:17)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 18:01:02--  (try:18)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 18:03:19--  (try:19)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nRetrying.\n--2017-05-03 18:05:36--  (try:20)  https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.37|:443... failed: Connection timed out.\nGiving up.\nUnzipping...\ngzip: stdin: unexpected end of file\ntar: Child returned status 1\ntar: Error is not recoverable: exiting now\nDone. Please run this command again to verify that checksum = ac116844f66aefe29587214272054668.\n. ",
    "ihciah": "Dropbox is blocked in some countries. Try to use a proxy. proxychains is a good choice.. @squirrel233 In China Dropbox is blocked. Just setup a proxy server like shadowsocks(or buy one), and then run a client to create a socks5 proxy. Change proxychains' setting and point it to your socks5 ip and port. Then the proxychains should work well.. Dropbox is blocked in some countries. Try to use a proxy. proxychains is a good choice.. @squirrel233 In China Dropbox is blocked. Just setup a proxy server like shadowsocks(or buy one), and then run a client to create a socks5 proxy. Change proxychains' setting and point it to your socks5 ip and port. Then the proxychains should work well.. ",
    "squirrel233": "@ihciah How to use it?\nI've tried:\n```\nme:~/fast-rcnn$ proxychains ./data/scripts/fetch_fast_rcnn_models.sh\nProxyChains-3.1 (http://proxychains.sf.net)\nDownloading Fast R-CNN demo models (0.96G)...\n--2017-07-12 17:43:44--  https://dl.dropboxusercontent.com/s/e3ugqq3lca4z8q6/fast_rcnn_models.tgz\nResolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... |DNS-request| dl.dropboxusercontent.com \n|S-chain|-<>-127.0.0.1:9050-<--timeout\n|DNS-response|: dl.dropboxusercontent.com does not exist\nfailed: Unknown error.\nwget: unable to resolve host address \u2018dl.dropboxusercontent.com\u2019\nUnzipping...\ngzip: stdin: unexpected end of file\ntar: Child returned status 1\ntar: Error is not recoverable: exiting now\nDone. Please run this command again to verify that checksum = 5f7dde9f5376e18c8e065338cc5df3f7.\nMaybe I made mistakes when using `proxychains`, plz note it, thanks!. @ihciah thanks~. @ihciah How to use it?\nI've tried:\nme:~/fast-rcnn$ proxychains ./data/scripts/fetch_fast_rcnn_models.sh\nProxyChains-3.1 (http://proxychains.sf.net)\nDownloading Fast R-CNN demo models (0.96G)...\n--2017-07-12 17:43:44--  https://dl.dropboxusercontent.com/s/e3ugqq3lca4z8q6/fast_rcnn_models.tgz\nResolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... |DNS-request| dl.dropboxusercontent.com \n|S-chain|-<>-127.0.0.1:9050-<--timeout\n|DNS-response|: dl.dropboxusercontent.com does not exist\nfailed: Unknown error.\nwget: unable to resolve host address \u2018dl.dropboxusercontent.com\u2019\nUnzipping...\ngzip: stdin: unexpected end of file\ntar: Child returned status 1\ntar: Error is not recoverable: exiting now\nDone. Please run this command again to verify that checksum = 5f7dde9f5376e18c8e065338cc5df3f7.\n``\nMaybe I made mistakes when usingproxychains`, plz note it, thanks!. @ihciah thanks~. ",
    "vabrishami": "I used train_net.py (endtoend) and it worked.. I used train_net.py (endtoend) and it worked.. ",
    "soulslicer": "It doesnt compile either. It doesnt compile either. "
}
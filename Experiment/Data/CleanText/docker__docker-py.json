{
    "johncosta": "I have the credentials ready to upload to PyPi.  Is there anything that needs to be done before we push it up? (Version bump, etc.)? /cc @shin- \n. Hi @nickstenning, Thank you for pushing it out there!  Could you transfer it over to docker?\n. Thanks @nickstenning!\n. I'm still seeing 500 errors when using this and calling client.container_create. \n```\ndocker version\nClient version: 0.4.0\nServer version: 0.4.0\nGo version: go1.0.3\n```\n. LGTM :D\n. +1 for these changes. I'm especially a fan of removing the \"smurf\" naming.\n. closed, is being addressed in another PR\n. thx @shin- LGTM\n. LGTM\n. LGTM\n. ",
    "nickstenning": "Hi all,\nI've taken the liberty of pushing this package to PyPI. I'm more than happy to hand over the package (https://pypi.python.org/pypi/docker-py) to you guys, I just needed it pushed!\n. Done.\n. ",
    "shin-": "Thanks!\n. see f69c179\n. Hi,\nYeah, I noticed the issue too. I'll see if there's a way to solve this programmatically, and if there isn't I'll patch the docs to warn about this.\nThanks!\n. That was my idea too, I'll commit a patch sometime today.\n. Done! https://github.com/dotcloud/docker-py/commit/a115b50ef50316ece612e211c6cd06a63fc2fc34\n. Thanks!\n. Seems reasonable - I'll add a ChangeLog in the near future.\n. Magic happened!\n. Looks good to me. Thanks! I'll merge it right now and it'll make it in the next release (0.0.6) by the end of the week.\n\nAlso why does the API reimplement the builder functionality?\n\nWe have a project (internal ATM) that uses docker-py for which having the log output and the final image ID separated is really useful, so reimplementing it made sense. Also at the time I started writing docker-py there was no builder in the docker server yet. We might reassess in the future but for now, it's fulfilling its purpose (except for the INSERT command which still needs to be implemented, but I'll get on that soon)\n. Hi,\nI had noticed it too but at the time wasn't able to use the docker logs command either. If this is working again I'll look into it. Thanks for the fix suggestions!\n. FIxed in b7e1ac7069d86b2568ce5c403f77d2aa601a558a\nShould be on pypi in 0.0.6 -- i.e. by the end of this week.\n. Hi,\nFIrst of all thanks for the contribution! Just wanted to ask before I merge it, is there any reason the stdin param is set to 1 here?\n. Thanks! Merged. =)\n. I'll look into it.\n. Not personally acquainted with sixbut after gathering some feedback, looks good to me! I'll try to be more Python3 aware in the future but don't hesitate to let me know if/when I miss the mark.\nThanks!\n. Hi!\nReviewing this at the moment (sorry for the delay, been caught up in other stuff). First of all, thanks a ton for the contribution! Most of it looks good but it's a bit disorienting because half of the commits are not related to the issue in any way.\nThat aside, regarding the changes to the README.md file, I noticed you removed (perhaps unintentionally) a lot of trailing spaces, but these are actually used to indicate line breaks in Markdown syntax. Can you undo this change?\nI'm not sold on the necessity of adding a Makefile either, maybe you can make it a separate pull request?\nFinally, I don't mind merging that change to the build() method, although it should be noted that it will probably disappear with the changes I'm preparing for 0.1.0 (will use server-side build system).\nOther than that, everything's looking good -- if you undo the changes to the README file and remove the Makefile (at least for now), I'll merge it asap.\nThanks again!\n. Awesome, thanks a lot! Merged, and the changes will make it into the next release (either 0.0.7 or 0.1.0)\n. Looks reasonable. Thanks Thatcher!\n. Good catch, thanks!\n. Closing, dotcloud/docker#1701 made it into master.\n. https://github.com/dotcloud/docker/issues/1292\n. The reason there we don't return exit codes (keep in mind also that you could potentially pass multiple IDs in a single start/stop call, just like with the docker CLI) is that the API call doesn't provide a response to the calls\n/containers/{name:.*}/start\n/containers/{name:.*}/stop\nIdeally you'd want to fix it in the API itself, then the client would be able to utilize the information. I'd suggest creating an issue in github.com/dotcloud/docker tagged as enhancement or better yet, a pull request!\n. Thanks @keeb !\n. Sounds good. Thanks!\n. Thanks for the report, I'll look into it.\nAs a workaround, you'll find that the first result of Client.build is the image ID, so you can do something like\nid, logs = client.build(...)\nclient.tag(id, ...)\nHope that helps!\n. Cool! Thanks. :)\n. Fixed: https://github.com/dotcloud/docker-py/commit/75d6f84e93c845670b8121df48d083de3abf763e\nThanks for the report!\n. Client.start is improperly documented indeed. Thanks for spotting that! For stop/restart/wait, I used the notation containers... which, in my experience, is the standard to describe variadic arguments. The line below even states that it behaves like its CLI counterpart, which also supports multiple arguments. How do you suggest we make it more clear?\ncreate_container returns a dict because it's a direct transcription of the API's response. Every other method in the library works that way, and I think it would be unwise to alter the responses -- or at least, I don't think it's the job docker-py is supposed to do.\nI am not opposed to the idea of start/stop/restart/wait supporting dicts (or a list thereof) also containing an Id, but it should still be possible to use plain IDs as well.\n. Not sure how I feel about this, it feels hackish, but at the same time it's a bit silly to do it for some methods and not for others.\nSo, yeah, go for it :)\n. Awesome, thanks!\n. Oh, unit tests would be cool, but as you point out it's a huge time sink to wirte and maintain a mock. \n\nI'm thinking this would be a good idea as we should not really be testing, for instance, if a given container is created/started but rather do we get a good response from the API and how we handle godd/bad responses in the client.\n\nIs it though? I am of the mind that making sure the library properly starts containers/creates images/etc. is more important, which is why I went for functional tests in the first place.\nThat said, I also believe unit tests are valuable, and as you said, it would make integrating with a CI tool like Travis easier too.\n. Just to clarify here, while I don't have much available bandwidth to write a whole unit test suite with mock-docker, it's definitely something I'd welcome into master if you wish to work on it!\n. Of course. Let me know if I can help! :)\n. Hey, I like what I've seen so far. Let's make it a pull request and continue the discussion there!\n. Alright, we've got unit tests and python 3 compatibility! =)\n. Thanks!\n. I wasn't sure this would work as expected when I first wrote this, but if people can use this I'm all for it.\nThanks!\n. Overall I'm feeling pretty good about those changes, one thing to not though is that by adding raise_for_status in methods such as wait, stop or start, these will stop on the first error they encounter as opposed to going all the way through and ignoring them, which is what they do now.\nFor example, that would mean that calling stop() on multiple containers where one of them is already stopped would lead to unreliable results depending on its position in the list.\nIt's also inconsistent with the way the docker CLI works right now.\nI think a custom error raised at the end of the procedure that recaps every container that failed to start/stop/... properly would be more appropriate in that case.\nWhat are your thoughts on this?\n. Hey,\nFirst off, I wish to apologize for the late response here -- I wanted to ponder this before taking a decision on that particular issue.\nWhen developing v0.1, I in fact tried to stick as close as possible to the docker CLI. At that time, the remote API was a newborn and most people were familiar with the command line interface.\nSince then, both this project and docker itself have evolved to a point where this may not be needed anymore. And I must admit your addendum was a compelling argument.\nSo, let's do this! The methods that are affected are:\n- kill\n- remove_container\n- remove_image\n- restart\n- start\n- stop\n- wait\nSince you offered, I'll let you start and implement this change. Because this is a breaking change, this will probably warrant releasing version 0.2.0.\nThanks!\n. I managed to find some time to work on this, so I went ahead and implemented the changes we talked about in the 0.2.0-dev branch (as well as merging this PR).\n. Looks good! Would you mind adding tests for these two?\n. Hey, this looks good. Just a few things:\n- You actually need to leave the double trailing whitespace in the docs, it's Markdown syntax for a line break.\n- Once that is done, can you rebase your changes?\nThanks!\n. No worries, it's happened to others before ;)\nOne last thing, can you rebase your changes against the current master branch? Once that's done I'll be able to merge it.\nThanks!\n. Perfect! Thanks :)\n. Added in e07e059, slated for 0.2.0 release.\n. I couldn't reproduce this. Can you give more information about your setup so we can figure out what's happening?\n```\npip freeze | grep requests\nrequests==1.2.3\nipython\nPython 2.7.4 (default, Apr 19 2013, 18:28:01) \nType \"copyright\", \"credits\" or \"license\" for more information.\nIPython 0.13.2 -- An enhanced Interactive Python.\n?         -> Introduction and overview of IPython's features.\n%quickref -> Quick reference.\nhelp      -> Python's own help system.\nobject?   -> Details about 'object', use 'object??' for extra details.\nIn [1]: import docker\nIn [2]:\n``\n. Have you tried force-installing the suggested version through pip (i.e.pip install -U requests==1.2.3) ? \n. Can you try https://github.com/dotcloud/docker-py/pull/45 and tell me if it works in your setup?\n. Merged -- thanks!\n. I'll try to release 0.2.1 by the end of this week with a couple other fixes.\n. Good catch! Fixed in cf2da51\n. Good catch, thanks!\n. Fixed in master (reading requirements.txt in setup.py so there's only one declarative source). Thanks for the report guys!\n. Thanks!\n. I think I merged a fix that addressed this issue earlier this week (#46). Not on pypi yet, but you can use master for now -- look forward to 0.2.1\n. Ok, I checked that it works and released 0.2.1 ; it should be on pypi in a few minutes.\n. Package shoud be fixed. Thanks for the heads up!\n. Hi, can you elaborate on the purpose here? I'm not sure I really understand why you'd want to be able to set both params at the same time.\n. The current behavior is, you either specify apathto a directory that contains aDockerfile(similar todocker build $path), or afileobjdescribing a Dockerfile (similar todocker build - < ./Dockerfile`). Having a hybrid of the two will, IMO, just serve to confuse people. \nCan you point out a practical use case where this change is necessary?\n. Thanks!\nAs for the reason it wasn't there, it's probably a recent addition that I hadn't noticed yet. Glad you noticed it =)\n. Noted -- thanks for the report!\n. Fixed in master\n. Thanks!\n. Just change the try-except to be narrower and add the data param to the README, then we're good to go :+1: \n. Fixed in 97bd69162a9e6ec3f8c8a6a45b084c9818745c46 (\"unix://\" will stay the default, but support for unix:/// has been added)\n. Thanks!\n. Thanks! I moved the comments in the README.md file to stay consistent with what we've been doing so far. I want to find a better solution to deliver documentation, but in the meantime let's make sure it all stays in one place :)\n. Thanks for the suggestion :)\nThere are multiple ways to do it, as @plausibility points out. I'd rather not make an \"official advice\" out of this because master is notoriously broken from time to time.\n. Hey, I started working on the things we talked about the other day. I'm hoping to have this merged today or tomorrow. :)\n. Why not have a ws=Falseparameter in attach_socket instead? I don't think this warrants adding a new method.\nIf you can change this (and rebase the PR), I'd be more than happy to accept it.\n. Cool, thanks for the update! I'll try to merge it by the end of this week.\n. So it looks like websocket-client doesn't support python 3 yet. I'm putting six.PY3 guards where it's used to avoid breaking the tests, and we'll have to watch for support in a future version (looks like they're actively working on it)\n. Thanks again for the contribution :)\n. The original intent was to clarify versus the behavior of the CLI which will try to pullan unknown image when doing docker run <unknown> ...\nBut yeah, with the new error handling system this seems a bit obsolete. :)\nThanks!\n. Fixed in 8be4c1f0d3f07adea63c95d5b1d293f9222f9ccc\n. Hey!\nI haven't been able to look into the links feature yet, even less into implementing it in docker-py. What you're proposing sounds good on paper, so sure, run with it :)\n. Yeah... Better docs is honestly the next step for docker-py at this point, having all the info crammed into README.md is clearly not enough anymore.\n. We have #87 for better syntax and #83 for better docs, so I'll close this =)\n. Solved! Thanks for the report.\n. It's honestly not on my list of priorities right now. I think requests 1.2.3 works fine. Is there a reason we would want to upgrade?\n. I think it's going to be non-trivial to upgrade to another major version of requests, especially because we would have to adapt the custom adapter classes and all that jazz that are used to support the unix:// protocol (see docker/unixconn/unixconn.py).\nLet's say it's low priority -- if someone wants to tackle it I'm not against it, but I'd rather focus on other issues.\n. Fixed by #151 \n. Thanks Michael, I'll have to delve deeper into the links feature first and see how exactly I'd like to include this.\n. I think #92 solves this, but I might be wrong. Let me know if these are 2 different things and I'll reopen.\n. Back when I started the project, writing to a running container was really complicated/weird and since then noone really requested or seemed to need it, so I don't think there's any way to do it with docker-py in its current status. Might add it on the pile of things I want to do, but if you feel up to the challenge, I'd gladly accept a pull request that implements it! ;)\n. See #239 \n. Hi,\nYeah, it looks like the container's JSON format has changed a bit, so port needs to be updated to work with 0.6.5. I'll look into it ASAP.\n. Thanks @rca, this is really helpful. I'll take a look today.\n. Merged #76, closing this!\n. Actually, most methods that work with containers accept either a container ID (string) or a dict containing and 'Id' key. commit is the exception to this rule for no real reason, and should be changed to accept both in a future version.\nHope that helps clarify! :)\n. Do we want to try and stay backward compatible with versions prior to 0.6.5?\n. Alright, here's what I suggest we do -- I'll merge this as is now, and when I've got some time to spare, and before releasing the next version, I'll look into making it backward compatible if it's possible. =)\nThanks a lot for the contribution, and the additional thoughts!\n. Re: backward compatibility, I'd say that I agree in theory, but in practice, there are people out here using docker in production, so I feel that maintaining it is at least something we should aim for (although it's no big deal if we fail sometimes)\nRe: create and run, as @rca said, it's how the remote API was designed. Also, the fact that both these commands already have a huge list of arguments and do a lot of things makes me want to keep them separate. If someone wants to wrap them in a single function in their own code, it shouldn't be too difficult. Finally, I think preserving atomicity (i.e. one method call = one API request) is a good thing.\n. It's not been implemented, but it should be easy enough to do, if you want to try your hand at it.\nYou can also do it \"by hand\", by calling client.remove_container(container_id), maybe after a call to client.wait to make sure it's done executing. container_id is the return value of client.create_container\nHTH!\n. You're right, it's not actually in the API, it's a feature specific to the docker CLI, and it does it using wait https://github.com/dotcloud/docker/blob/master/commands.go#L1899\nKnowing that, I'm actually questioning the legitimacy of having this be a parameter to Client.start. I feel that we should preserve the atomicity of the calls as much as possible (i.e. 1 method call = 1 API request at most) and this goes directly against that principle.\n. Yes, it doesn't seem like this is a big issue to anyone that commented, and we have to go forward, so I think we can close this one.\n. Hey, thanks for the contribution! Since you mention that it's hackish I'd like to take some time to take a closer look, but I'm just commenting to say that it's not going unnoticed!\n. I've been toying with this a bit, here's what I came up with -- can you give me your thoughts, maybe point out the fundamental flaw in my reasoning? :)\nOther than that, it should be good to go (I have a branch merged and ready to go if you're ok with it)!\n```\ndef events(self):\n    u = self._url(\"/events\")\nsocket = self._socket_connection(u, method='get', stream=True)\n\nwhile True:\n    chunk = socket.recv(4096)\n    if chunk:\n        # Messages come in the format of length, data, newline.\n        length, data = chunk.split(\"\\n\", 1)\n        length = int(length, 16)\n        if length > len(data):\n            data += socket.recv(length - len(data))\n        yield json.loads(data)\n    else:\n        break\n\n``\n. Merged -- thanks again :)\n. Thanks, I'll merge this. In the long run it would probably be a good idea to make the whole thing less tedious (by allowing input like{\"lxc.cgroup.cpu.shares\":\"1\"}(I'll create an issue)\n. Thanks for the contribution! I'll hold off on merging until you can add tests :)\n. Merged! Thanks Maxime :)\n. @tlindener Until we have good docs, I'd advise putting ourREADME.mdand the [remote API docs](http://docs.docker.io/en/latest/reference/api/docker_remote_api_v1.10/) side by side, which should (hopefully) clarify those differences.\n. I believe so!\n. It was originally introduced by PR #56 . I agree it's weird. In theory it's supposed to allow passing raw byte data to the import call. I'm guessing it's missing anif data is None:`. @kiorky do you have any additional insight on this?\n. Fixed!\n. Ah, tests are failing for Python 2.6 because assertIn doesn't exist in that version...\n. Fixed the issue with the tests and merged. Thanks for the contribution!\n. Yeah, I agree it kinda sucks the way it is at the moment. I'll try to take a jab at it, already started with the lxc_conf issue.\n. Thanks Thatcher, I'll definitely look into that!\n. Can you comment on #115 and tell me what you think?\n. Good point, thanks!\n. Thanks for the contribution! \nI like the idea and support the feature 100%, but does the logic really belong in the APIError class? It feels a bit tacky...\n. ah, sorry, I misread the diff. NEVERMIND :D\n. Thanks again!\n. Awesome, thanks for the contribution!\n. Thanks for the contribution!\nLooking at the code, I'm thinking there may be potential for factorization here, instead of adding timeout=self._timeout to each and every post/get call in the client. I'll gladly add it to my list of things to do, but if you get a chance to tackle it before I do that would be awesomely helpful!\n. Well, that was hell to merge with all the changes to the tests, but I made it >.<\nThanks again!\n. Great, thanks! I'll take a closer look and merge that asap.\n. I haven't started merging, so if you want to change that (and maybe rebase with the current master while you're at it), knock yourself out :)\n. Thanks, good catch!\n. Thanks!\n. Thanks!\n. I'd say it's a remote API bug and thus should be reported on the docker issue tracker. I'll leave it open here if there needs to be further discussion.\n. cool :)\n. duplicate of #71 :)\n. Thanks!\n. Continuing since this resurfaced with #228: I've had problems with non-pinned versions in the past, especially since pip's primitives when it comes to specifying version ranges are fairly limited in my opinion (compared to npm, for example).\nI'm inclined to say: call us out / send patches if pinned versions are getting unreasonably old, and use virtualenvs as much as possible since they will save you a lot of trouble. I'm happy to discuss this further, I may be missing some valid counter-arguments, but I genuinely believe unpinning versions will be worse for everyone (maintainers and users) in the long run.\n. > Perhaps not a problem right now, but imagine a situation where a version of requests is released which introduces a new feature and a (perhaps unknown) regression, both unrelated to how docker-py uses it. Do you accept the PR from person A which bumps your pin up? What do you do when person B opens a PR reverting that change because the regression breaks their app? And then what does person A do with their codebase that is using this shiny new requests feature?\nIn that example, my opinion is that person B should either update their code to support the newest version of requests, or stick with an older version of docker-py that doesn't require said version of requests. If they're okay with an \"outdated\" version of requests why wouldn't they be okay with an \"outdated\" version of docker-py?\n. Thanks for the contribution! Can you update the README to match, and rebase your changes? =)\n. Thanks!\n. Thanks :)\n. What's the status on this? Is it ready to merge (ignoring the problem with the tests in py33)?\n. Fixed by #292 \n. Thanks!\n. Thanks!\n. Closing since I believe it was solved. Feel free to reopen if that's incorrect!\n. Thanks!\n. Thanls!\n. Thanks!\n. Thanks!\n. Looks good overall -- thanks!\nAdded a couple comments -- mostly nitpicking about readability.\nAlso, can you look into making the tests pass?\nThanks!\n. Thanks!\n. LGTM! Thanks :)\n. Not too sure about this, let me play with it a bit first!\nIdeally, if we can accomodate both ways of doing things (i.e. passing arguments in the official CLI format + passing arguments in the python friendly format) I'd like us to do that.\n. Yes, the upgrade should be relatively painless. I'll give it a shot in the next few days, unless you want to tackle it before that!\n. I'd rather keep only the logs() method and just add a \"history\" or \"logs\" parameter, instead of adding yet another method . Is there a reason you think we shouldn't do that?\n. LGTM as is.\nThanks! =)\n. Thanks for the contribution! Can you make the small fix mentioned above? We can merge it after that.\n. Sure -- thanks! \n. Agreed that it's a bug, tcp should be the default. And yeah, if \"no protocol\" is invalid, Docker API shouldn't accept it.\n. You'll need to fix the associated unit test test_create_container_with_ports accordingly :)\n. Thanks! Merged =)\n. @mpetazzoni : unfortunately that's just how the remote API is, I don't think there's much we can do. \n. @bfirsh Thanks for the contribution!\n. Thanks!\n. Thanks!\n. > Could md5 work?\nI don't know, what source do we use, and what happens if/when the image changes? Comparing to a reference size poses the same problem.\nIt's clearly not an easy one to test and I can't think of a solution that satisfies me at the moment.\n. Thanks! I know a few people had asked for this before, so that's really cool.\n. Thanks! I like this solution too.\n. Thanks!\n. Thanks!\n. Thanks!\n. Just merged #161 (thanks @mpetazzoni ! :))\n\nIs this something that could be solved by inheritance, having different implementations for the various versions of the API, overloading on top of each other?\n\nWe could look into something like that, or maybe just decide that the next version of docker-py will only work with docker 0.8.0+. See discussion in #76 for more on backwards compatibility.\nAlso, once we've made sure that docker-py works with docker 0.8.0, we'll release a new version.\n. Is there a reason we're holding off on making 1.9 the default API version? I just updated the integration tests and they pass fine with both 1.8 and 1.9 (but they're admittedly not 100% coverage). What else do we need to make this release happen? I'll start working on the ChangeLog and README updates.\ncc @mpetazzoni \n. build works. It's not returning the produced image ID like it used to with more recent versions of the API, but that's something we can restore if/when we start properly parsing JSON streams. If that's the only thing, I think we can release 0.3.0 now.\n. Sure, sounds good to me.\n. tagged 0.3.0, and released on pypi as of now. I'm closing this issue, if there's anything that still needs to be discussed feel free to reopen it though.\n. Thanks! :)\n. Added installation instructions in the README.\n. Closing this since I merged #165, and a print statement is clearly not how we want to handle genuine errors.\n. Thanks!\nAs a side-note though, this is fine because it's a no-cost fix, but one shouldn't rely on packages several versions behind the one recommended in the requirements.txt to work with docker-py.\n. Already in master: see #140\n. I'll hold out on making changes for now because I believe ParseHost will undergo some changes very soon in the main project, which could invalidate or change the scope of this request. Leaving it open until then.\n. Thanks!\n. Fixed in b1f1e363a49753c8801b54480b01efe14c2c21f0\n. lgtm!\n. Thanks! Do you think you could write a unit test to ensure that this behavior doesn't break again further down the line? Otherwise this is looking good.\nCheers,\n. Thanks for the report! Which Python version are you using?\n. Yes, I think these are 2 different issues. Absence of image id for version >= 1.8 is intended, errors when trying to read from the stream are not.\n. Thanks for the contribution! This needs to be rebased first.\n. LGTM\n. have you tried increasing the timeout when instantiating the client?\nc = docker.Client(timeout=60)\n. Also, if you intend to use search frequently, hitting the index endpoint (https://index.docker.io/v1/search?q=<query>)directly might prove to be more efficient.\n. Does it work when you use docker search from the command line?\n. Thank you! \n. Odd. master seems to be passing though.\n. Client.create_container has a volumes_from parameter.\n. 0.3.1 should make it to pypi shortly. Sorry for the delay guys.\n. 3 months without response, closing.\n. Thanks for the report!\nDo you encounter the same problem when following these instructions?\n. LGTM! =)\n. Could the tls boolean be implied by the presence of cert and key?\n. Makes sense. I'd say we add the other parameters then and raise NotImplementedError when provided with an unsupported combination. What do you guys think?\n. Closing in favor of #226 \n. You need to upgrade docker. Current version is 0.9.x ! Also I'm pretty sure API version 1.6 came after docker 0.5.3. \nIf for some reason you need to stick with 0.5.x, try docker-py 0.2.2\n. client = docker.Client()\ncontainer = client.create_container('ubuntu', 'echo \"hello from docker\"')\nclient.start(container)\n. @rahul8590 You may need to add client.wait(container) between your start call and logs call.\n. Thanks! I think it's a good idea overall. Any reason why APIError has not been moved to the docker.errors module?\n. LGTM\n@mpetazzoni Any objections? Could potentially break for people currently importing APIError in their code, but I don't think it's a huge deal (easily fixed)\n. thanks!\n. Please make sure the unit tests pass.\n. Merged - Thanks!\n. LGTM\n. Thanks!\nCan you add unit tests too?\n. Since insert() disappears in API >= 1.12, I don't think working on improving it is a good use of anyone's time. Closing.\n. The ChangeLog for API 1.12 specifies that the insert endpoint is being removed. This is confirmed by the absence of said endpoint in the remote API code. It was removed in this commit. It looks like the fact that it still appears in the API documentation is an error. I created an issue for the documentation to be corrected: docker/docker#7305\n. Fixed 2d56149831b2b3197f6712ba6250b207c77eb00a\n. Thanks! Can you add the case to the unit tests, so we can avoid a regression in the future?\n. No that's great, thanks! (and sorry for the merging delay)\n. One last comment and this should be good to go :) \nThank you for your patience with this one.\n. Thanks!\n. Shouldn't you increase the global timeout when creating a new Client instead of adding local timeouts to some arbitrarily picked methods?\nThe whole idea bothers me a bit.\n. LGTM\n. Thanks!\n. You have to call Client.start on  the result of create_container. See the remote API reference for more information.\n. Fixed by #223 \n. Thanks for the contribution!\nSee my comment on the code. If you can just fix that I'll go ahead and merge it.\n. Thank you !\n. Feels like we've gone back and forth a few times on this method already. Have you had a chance to test this under both Python 2 and 3?\n. LGTM\n. Thanks!\n. See #200 \n. duplicate of #200 I believe.\n. Probably related to #200 -- can you try it with docker-py from master and see if it solves your issue?\n. @tianon : #101 \n. Fixed by #224 \n. Thanks! Can you add it to the README too?\n. Thanks!\nOk, so you'll want to rebase this first, especially because we already have an ~~exceptions~~ errors module in master which you'll want to merge with your own.\nOtherwise this looks good. Maybe extract all the additional checks and configs from __init__() and into a separate method so it doesn't become too crowded?\n. @momer Are you still working on this? \n. No worries, thanks for the quick reply.\n. I'm closing this since we're continuing the discussion on #250 :)\n. Right on all accounts! It should be relatively easy to add, do you want to take a crack at it?\n. Tried reproducing this today with Docker 1.0 and the current master but I didn't encounter the issue again. Are you still seeing this?\n. Closing, feel free to reopen if you're still seeing the issue with docker-py>=0.3.2 :)\n. Fixes #201 \n. This isn't the right fix. The type of the parameter changed in 0.10.0 from a comma-separated string to a proper list.\n. It's only used for API versions, so there's never a -dev suffix.\n. Thank you!\n. Where are you encountering this problem? pull() has a separate tag parameter, and push() doesn't accept a tag argument (at the docker level, you can only push an entire repo, not a limited amount of tags)\n. https://github.com/dotcloud/docker/blob/master/utils/utils.go#L827 Needs to be implemented in python and called before resolve_repository_name in pull(), like it is done in the CLI implementation.\n. My thoughts on the matter are still the same, we shouldn't have to do that, and it's a failure of the docker remote API that this issue even exists. With Docker 1.0, I expect the API to stabilize and this discussion to become obsolete anyway.\nWith 0.3.2 on the horizon, the default API version is 1.12 which shouldn't pose any problems if you're using Docker 1.0 (which everyone should, if only because of the security implications of using anything older than that)\n. I'd rather stick with having the default version and people being able to change that at their own risk. It's a very hands off approach but I think for an API client that's what makes the most sense.\n0.3.2 has 1.12 as its default, and if promises are kept, this should be stable going forward, so I'll close this issue. If we encounter more problems in the future we can reopen this and explore other solutions.\n. Thanks! Can you make sure the unit tests pass?\n. These are remote API issues, not docker-py issues.\n- One can't remove a container that's running.\n- One can't stop a container that's not running.\nBottom line is, you have to handle the exceptions thrown here and act accordingly. It's not the client library's job to decide how you want to handle server-reported exceptions.\n. Yeah, the whole auth is kind of designed that way. I'm sure there's something smart we could do here, but it's not a huge priority. If you want to contribute I'd be happy to merge a patch that improves this behavior.\nThanks!\n. docker.__version__ is incorrect in 0.3.1, sorry about that. We've made sure we have a single source of truth for version in 0.3.2 and up.\nRelated: #201\n. Requested docker-py==0.3.2, but installing version 0.3.1\nRemove your pip cache =)\n. Thanks!\n. Fixed by c3ebce91b0d76540280c0ef6f961c0a5305b30f3\n. Reverted for now, we'll figure out what to do after 0.3.2 is released\n. Thanks!\n. @denibertovic @momer If you guys can spare a minute to review this, it would help tremendously!\n. I fixed a couple bugs with the last commit (overlooks on my part, sorry about that) and acted upon some of @denibertovic 's comments. Are you guys able to make this work now? I'll see if I can add unit tests that make sense here later.\n. Thanks, fixed that one too.\n. @momer it's not removed, if you provide a TLSConfig() object with the ssl_version provided it will still work. i.e.\npython\nconfig = docker.tls.TLSConfig(True, ssl_version='...')\nc = docker.Client(host='<host>', tls=config)\nwill do the trick.\n. Don't worry, I'm not removing functionality, just squashing bugs at the moment =)\n. SSLError: [Errno 1] _ssl.c:510: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed`\nI think we can handle that one as a separate bug / investigation. \nAs discussed with @denibertovic over IRC, the only thing that's blocking this merge is the API itself. We try to stick with the CLI params but those are a mess already, and trying to translate them to python makes it worse still. We might want to do our own thing here. That doesn't invalidate the work done here since most of it will still be good, but making the interface more user-friendly would be great;\n. And it's in! Thanks everyone for your contributions and reviews!\n. Are you using docker-py 0.3.2 and docker 1.0+?\n. Are you using docker-py 0.3.2 and docker 1.0+?\n. Please do and let me know if you're still encountering the same problem.\n. Thanks, looking into it\n. Are you trying to provide a string argument (like \"64m\" or \"1G\" ?) \u2013 because this won't work, the remote API only interprets (long) integers. The following works for me:\nc.create_container('busybox', 'sleep 12', mem_limit=2.4117248e07)\npython\nIn [4]: c.create_container('busybox', 'sleep 12', mem_limit=2.4117248e07)\nOut[4]: \n{u'Id': u'0db7d6c5ad50732b448151493a50af2cb1789f901a8c7bdcd8fe44177df1b400',\n u'Warnings': [u'Your kernel does not support swap limit capabilities. Limitation discarded.']}\nIn [5]: c.inspect_container(_4)\nOut[5]: \n{u'Args': [u'12'],\n u'Config': { ...\n  u'Hostname': u'0db7d6c5ad50',\n  u'Image': u'busybox',\n  u'Memory': 24117248.0,\n  u'MemorySwap': -1,\n  u'NetworkDisabled': False,\n  ... }\n. In the second example you're using a string for mem_limit, which isn't valid.\n. Sure, I'd gladly merge that PR =)\n. Hi,\nThis is not a support channel. Please use the docker-user mailing list instead.\n. Not yet, to my knowledge. As always patches are welcome =)\n. Don't necessarily disagree. If that helps, here's the code I use in stackbrew to parse the result of Client.build.\npython\n            lines = [line for line in build_result]\n            try:\n                parsed_lines = [json.loads(e).get('stream', '') for e in lines]\n            except ValueError:\n                # sometimes all the data is sent on a single line ????\n                #\n                # ValueError: Extra data: line 1 column 87 - line 1 column\n                # 33268 (char 86 - 33267)\n                line = lines[0]\n                # This ONLY works because every line is formatted as\n                # {\"stream\": STRING}\n                parsed_lines = [\n                    json.loads(obj).get('stream', '') for obj in\n                    re.findall('{\\s*\"stream\"\\s*:\\s*\"[^\"]*\"\\s*}', line)\n                ]\n. Also related to #102 (same problem with Client.pull())\n. This is incorrect. We already test Python 3.4, see the tox.ini file.\n. Thank you for the report, I'll look into it.\n. We do try to sticking to these rules, I guess that was an oversight on my part, sorry about that. :)\n. This is still a breaking change, the original implementation doesn't allow to leave the port unspecified while docker-py currently does.\nWe can either stick with it and release a new major (it's kinda overdue anyway), or allow the port to be unspecified, with the potential security implications that this has. LMK what you think.\n. Thanks for the review guys :) merging!\n. Duplicate of #204\n. Thanks! Can you just fix the small flake8 issue so that the tests pass?\n. Great, thanks!\n. Thank you for the contribution! Please make sure the tests pass :)\n. Thanks!\n. Yes, I'm guessing this is implemented on the client side in Go, so it needs to be rewritten in Python. :)\n. I'll make it happen this weekend, or Monday at the latest.\n. docker-py==0.5.0 is now available on pypi, thanks everyone for your patience!\n. The bash -c \"trick\" is exactly what the official docker client does, actually! Try a docker build with a few RUN instructions and look at the commands for the containers it created, for example.\nI'm pretty sure that the remote API doesn't support chained commands without it. I don't think docker-py should do anything different here.\n. Awesome, thanks! Do you think you could add some unit tests?\n. Hey, sorry for the late response, I was MIA for a week and didn't get a chance to check back. \nI'd be interested in a couple unit tests for the utils.tar function specifically with different exclude values,   and checking that the resulting excludes the expected files (i.e. by listing the tarfile contents). Do you think that's something you could add? That way if we introduce a bug in the future it would be caught by the CI immediately (whereas integration tests are only run manually).\nThanks a ton for the work you've done so far!\n. Great, thanks a lot! Merging =)\n. Oh cool! coverage seems to be confused by our python 2/3 shenanigans (e.g. every if six.PY3: line is marked as \"partial\"), is there any way to fix this? Otherwise LGTM =)\n. Thanks! =)\n. Thanks! Is that necessary with a StringIO object too?\n. Fixed by #1965 . Thanks!\n. Fixed by #291\n. Thanks!\n. ah, that's definitely useful, thanks!\n. Closing in favor of #505 \n. Can you do pip freeze | grep docker and copy the output?\n. So first off, you should upgrade to 0.4.0 or at least 0.3.2 =)\nBut I checked and you're correct, docker-py doesn't support pushing a single tag at the moment (it was a recent addition to docker itself) -- it should be pretty easy to implement though, if you want to try your hand at a patch.\nThank you for the report!\n. We're pretty lax on this project in particular. Pretty much:\n- Fork the repo and submit a PR when it's ready (or before if you need help finishing it)\n- Make sure the tests pass (just run tox in the project's root folder).\n- There are flake8 tests, you might find them a bit annoying but they ensure we stay consistent on a project like that with tens of contributors.\nAlso, you should look into the Client.pull implementation which already does that kind of parsing, make sure you don't rewrite code that already exists. =)\nThanks!\n. Thanks!\n. I'm not fond of the idea, I really would rather have all public methods of client be primitives of docker. If one really needs to change the timeout \"in flight\", changing the _timeout attribute directly should be fine. WDYT?\n. WIth #475 merged, we can now close this. Thanks!\n. Thanks!\n. Looks good! Would it make sense to raise an error if the string isn't valid? i.e. doesn't end with a digit, 'm', 'k' or 'g' \u2013 or the remainder isn't a number?\n. Thanks! A few remarks still:\n- ~~Some pep8 errors need correcting (see the output of tox or the travis logs for directions).~~ You just updated this =)\n- Maybe a string like '123456' should count as a valid value too? This isn't a big deal but it would be nice to have, I think.\n- Can you maybe add a unit test to make sure the values are properly converted in the request? (See for example test_start_container_with_port_binds in tests/test.py)\n. Oh that's great. Thanks a lot! :)\n. That's a compromise I can work with. Thanks!\n. Thanks! To clarify, is that a Windows compatibility issue?\n. Thanks!\nPlease use six.string_types instead of str for Python 3 compatibility.\n. Thanks!\n. I'm pretty sure the same \"issue\" exists with the docker CLI. Dots in image names aren't valid.\n. Yes, that's an exception, you're supposed to catch it.\n. Thanks!\n. Thanks for reviewing guys, I've been kinda swamped with other stuff these past few days so I haven't had time to look into it yet, but I'll read and respond/adjust ASAP.\n. Closing for now, this is not ready / the right approach\n. Thank you!\n. Fixed by #441 ? Please re-open if the bug reappears in docker-py>=0.7.2\n. Can you share a code example that fails consistently? Also the more info you can provide, the better so we can start investigating.\n. You're using different protocols in your 2 examples.\n. Thanks :+1: \n. Sure go ahead! \n. Thanks!\n. Will be supported in the next release.\n. The issue is more than a year old, there have been many releases since then. What are you trying to do and what error are you getting?\n. Can you run pip freeze | grep docker-py && python --version && docker version and paste the output here? Also, being able to see your create_container call would definitely help if you don't mind sharing it.\n. Thanks, I'll look into it.\n. I haven't been able to reproduce your issue using identical versions of python, docker, and docker-py.\nCan you try running the following script and reporting with the output? Thanks.\n``` python\nimport docker\nc = docker.Client(version='auto')\nprint('VERSION: ', c.version())\nc.pull('busybox:latest')\nctnr_id = c.create_container(image='busybox:latest', mem_limit='5g')\nprint('CONTAINER ID: ', ctnr_id)\nctnr_data = c.inspect_container(ctnr_id)\nprint('Mem Limit (config): ', ctnr_data['Config']['Memory'])\nprint('Mem Limit (host config): ', ctnr_data['HostConfig']['Memory'])\n``\n. Thanks! Can we move thecpuset` param to the end of the param list? That way we don't break programs that may use unnamed parameters.\n. Great! LGTM =)\n. Thanks!\n. Should be fixed in 0.6.0 ! Let me know if it doesn't work <3\n. This seems pretty easy to do programmatically, I'm not sure I agree that we need to add this to the API.\n. Documentation is clear, I don't see the value in having 10 different ways to specify ports bindings.\n. Ah, my apologies, I didn't mean to offend, I just wanted to explain my reasoning. Genuinely sorry for the way it came off.\n. Closing in favor of #959 \n. Hi, thanks for the contribution! Can you make sure the tests pass before I review this?\n. From Travis:\ndocker/utils/utils.py:163:1: E302 expected 2 blank lines, found 1\ndocker/utils/utils.py:168:80: E501 line too long (81 > 79 characters)\ndocker/utils/utils.py:177:1: E302 expected 2 blank lines, found 1\ntests/test.py:652:40: E201 whitespace after '{'\ntests/test.py:652:80: E501 line too long (128 > 79 characters)\nJust coding style issues.\n. Resolved by #373 ?\n. LGTM, thanks!\n. python\nclient.pull('my.registry.expl/myrepo')\nclient.push('my.registry.expl/myrepo')\n. Oh you're right, sorry. I'll edit my previous response in case other people stumble on that issue in the future.\n. Thanks! @mpetazzoni any reservations about this before I merge it?\n. We're just mirroring Docker's behavior here, and the client will not accept to connect over HTTPS if the certificate is not valid, so I disagree with this PR.\n. Cleaning up, sorry about the necro-posting\nClosing via #681 \n. Thank you!\n. Thanks!\n. Thanks! I merged in #326 first, closing this as a result =)\n. With #448 merged we should be good to close this.\n. The issue tracker isn't a support forum... Please use the docker-user mailing list (docker-user@googlegroups.com) or StackOverflow for this kind of requests.\n. Thank you!\n. Thanks!\n. Thank you!\n. Number 2 is a misconfiguration of your private registry, _ping should be a public endpoint. This is the same behavior as the mainline docker client.\nAs for 1, I need to take another look at it with the changes that were made in docker 1.3.x\n. Probably fixed by #318 \n. I'm on vacation at the moment, I'll be sure to take a look when I return.\nPlease bear with me!\nOn Oct 13, 2014 5:44 PM, \"Guillaume Dedrie\" notifications@github.com\nwrote:\n\nHi @shin- https://github.com/shin-, is it possible to review this PR?\nRegards,\nGuillaume\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-py/pull/330#issuecomment-58862451.\n. Hi Guillaume,\n\nSorry for the delay. The policy so far has been to keep Client methods atomic (i.e. each method makes one and only one request to the API server). Since this behavior is pretty easy to replicate in three lines of code, I don't feel this change is necessary or warranted.\n. Hmm, looks like we've lost information on default values with the new docs. We'll need to fix that.\nFor the record, default in the Docker API/CLI used to be rm=False, so it was implemented as such in docker-py. It was then changed on the Docker side, but I'm wary of breaking this for all the people relying on the current default, considering there doesn't seem to be a compelling reason to do it other than the unfortunate mismatch.\n. Thank you!\n. @leth Can you rebase this? Thanks! =)\n. I need to have 0.6.0 out the door, but I'll make sure to look into this for the next release. Sorry for those affected.\n. Hi @sigmavirus24 , thank you for taking the time to look at this!\nTo comment on our usage of internal objects like resp.raw.fp._fp, we've actually had quite a history with those streaming methods. The first version used a simple response.iter_lines() loop, but issues arised as new versions of Docker (pre-1.0) were released and we ended up going for the fix that seemed to work for most people. It'd be interesting to see if, with the docker API being more stable now, we could make our way back to something less complex (i.e. using resp.raw as you suggest)\nTo answer your other question, anything that can make these methods simpler would be huge for us. Let me know if I can help in any way to make this happen.\n. Sorry for the late update, I've had some time to look into this today, I've replaced the _stream_helper method by a simple response.raw.readline() loop:\npython\n    def _stream_helper(self, response):\n        \"\"\"Generator for data coming from a chunked-encoded HTTP response.\"\"\"\n        reader = response.raw\n        while True:\n            data = reader.readline()\n            if not data:\n                break\n            yield data\nbut running the integration tests it seems that I'm getting multiple lines at once.\n```\nERROR: runTest (main.TestBuildStream)\nTraceback (most recent call last):\n  File \"integration_test.py\", line 889, in runTest\n    json.loads(chunk)  # ensure chunk is a single, valid JSON blob\n  File \"/usr/lib/python2.7/json/init.py\", line 338, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/lib/python2.7/json/decoder.py\", line 369, in decode\n    raise ValueError(errmsg(\"Extra data\", s, end, len(s)))\nValueError: Extra data: line 1 column 112 - line 2 column 1 (char 111 - 115)\n======================================================================\nERROR: runTest (main.TestPullStream)\n\nTraceback (most recent call last):\n  File \"integration_test.py\", line 796, in runTest\n    json.loads(chunk)  # ensure chunk is a single, valid JSON blob\n  File \"/usr/lib/python2.7/json/init.py\", line 338, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/lib/python2.7/json/decoder.py\", line 369, in decode\n    raise ValueError(errmsg(\"Extra data\", s, end, len(s)))\nValueError: Extra data: line 1 column 97 - line 2 column 1 (char 96 - 16658)\n```\n. LGTM so far. @leth and others watching this PR, is this in a satisfying state to be merged and be an improvement over the way things are done in the current stable?\n. Sounds good, thank you!\n. That's fine by me. Let me know if I can help in any way. \n. Thank you! :)\n. Umm, looks like we may have an issue still.\nhttps://travis-ci.org/docker/docker-py/builds/44372370\nDo you have an idea on what could be the cause?\n. I have implemented a simple retry-on-fail mechanism in the test itself for now. I would like to determine whether the bug is in the test itself or the result of a race condition in the new streaming code.\nIt looks like a race condition of some sort and I can't reproduce it at all locally, but on Travis it seems to happen more frequently.\nI've held off on the 0.7 release until we can figure this out, whether through a fix or a rollback.\n. Thanks!\n. LGTM\n. Thanks!\n. Thank you!\n. LGTM, but will probably warrant a version bump.\n. Also, can you rebase this? =) Thanks!\n. Fixed by e8c9e6e3f2e49c28ab744bb738fa2c8bf7fe7ff3\n. Sorry for the delay. First off, thanks for the great work on this! But onto the debate at hand:\n1. Yes please, let's continue to work with Markdown, I find it to be a more concise and intuitive format. It's also more widely supported. The fact that it's also what we use for docker mainline is just icing on that cake.\n2. As for moving the docs to the official API docs, I don't recommend it. It would be confusing as to why docker-py gets that privilege over other API clients out there I think, and it's not relevant to the majority of docker users.\n3. I do like what I'm seeing from MkDocs, and I wouldn't mind giving it a go.\n. Master's good!\n. Hm, it would probably be wise to require a more recent version of six. Thanks for reporting this.\n. #366 is a duplicate of this but has more tests.Do you see anything wrong with that other version?\n. Thanks, I merged #366 which means this can be closed =)\n. Does it work with command = ['/usr/bin/xvfb-run', '-a', '-s', '\"-screen 0 1600x1200x24\"', '...']?\n. Does it work if you use the remote API without docker-py (e.g. by using curl)? Looks like this isn't a docker-py issue at that point, but I could be mistaken.\n. LGTM\n. #363 \n. Thanks, LGTM! This needs to be rebased though :)\n. Thank you!\n. Thanks!\nHow about using shlex when the command is a string instead? Similar to what we do here\n. Thank you!\n. Does #360 help at all?\n. Sorry, I should have clarified ; I don't disagree with updating the docs to be clearer/better (although I'm probably not the best person to do this since I have limited experience with SSL usage). But I also want to know if kwargs_from_env is a viable solution to the general b2d woes. :)\n. Thank you! I'll look into updating the docs soon to include these instructions.\n. Docs updated!\n. LGTM\n. I'll close this for now, but let me know if you encounter that issue again.\n. docker-py version?\n. Hm, if I'm not mistaken Docker since 1.3.0 enforces SSL for its API. So you probably need to provide a TLSConfig to your Client constructor. See the README for more info.\n. Does #360 help at all?\n. Alright, thanks! I'll close this issue now, and update the docs when I have some time.\n. Thanks! _parse_devices should be placed in the utils module instead. =)\n. Looks good, can you\n1. Add the missing newline at the end of the utils.py file\n2. Add some documentation to the README file explaining what format is expected for the devices param, similar to this section\nThanks!\n. Thanks!\n. That's great, thanks! Looks like everyone's in favor so I'll merge it =)\n. pip freeze output? You may have outdated packages.\n. What happens if you pip install -U requests==2.5.3? Same issue?\n. Ah nevermind, this is something else...\n. No yeah, actually, try downgrading your version of requests (pip install -U requests==2.5.3) and see if it helps.\n. Glad you were able to figure it out! I'll close this then.\n. Thanks!\n. LGTM.\n@merll You made the last change to that part of the code, does this still work in your setup?\n. @merll Sounds good, thank you!\n@micahhausler Since the Dockerfile has already been updated by #354, can you revert the changes made to the Dockerfile? After that I'll be more than happy to merge!\n. I'll try to make something happen, if not today then early next week.\n. Done! Will update the README asap\nhttp://docker-py.readthedocs.org/en/latest/\n. @samuelvonstachelski see #776\n. #776 merged and should address this issue.\n. Thanks Eric! Is this a fix for #367 ?\n. Looks like you forgot to import the ssl package in ssladapter.py\n. Thanks! =)\n. Thanks, LGTM ; can you clean the trailing whitespace on line 294, and then we'll be good to merge!\n. Thank you!\n. Thanks, updated #386 with the change. :+1: \n. Thank you!\n. see #73 #239 #390\n. see #73 #239 \ntl;dr doesn't exist yet, pull request welcome =) I'll close as this is essentially a duplicate.\n. You either need to upgrade Docker to the latest version, or force docker-py to use API version 1.14 (with docker.Client(version='1.14')). Your first example uses API version 1.2 which is way too old, support for older versions of  the API can't be guaranteed.\n. Where build_result is the generator returned by Client.build.\n``` python\n            build_success_re = r'^Successfully built ([a-f0-9]+)\\n$'\n            lines = [line for line in build_result]\n            try:\n                parsed_lines = [json.loads(e).get('stream', '') for e in lines]\n            except ValueError:\n                # sometimes all the data is sent on a single line ????\n                #\n                # ValueError: Extra data: line 1 column 87 - line 1 column\n                # 33268 (char 86 - 33267)\n                line = lines[0]\n                # This ONLY works because every line is formatted as\n                # {\"stream\": STRING}\n                parsed_lines = [\n                    json.loads(obj).get('stream', '') for obj in\n                    re.findall('{\\s\"stream\"\\s:\\s\"[^\"]\"\\s*}', line)\n                ]\n        for line in parsed_lines:\n            match = re.match(build_success_re, line)\n            if match:\n                return match.group(1), parsed_lines\n        return None, parsed_lines\n\n``\n. Sorry, there's nothing in the remote API that would make this easier, to my knowledge\n. This has been added as a feature with the newDockerClient` in 2.0.0\npython\nimport docker\ndc = docker.from_env()\nimage = dc.images.build(path='.')\nprint(image.id) # sha256:e1a0edbefee23e8756909d07814cd325b245ac6a0ecbd7a1fa91ac1ad8d086c9. I'd rather take a more restrictive route and raise an exception if a non-string is passed, what do you think?\n. Hi, thank you for the submission! I think these instructions belong in their own file, if you can make this happen I'll be happy to merge!\n. No, add it to the mkdocs.yml file so it will show up in the sidebar menu in the hosted version\n. Streaming isn't implemented (or at least not properly) on that method, which is why you're seeing timeouts I think. It's a trivial call sure but you still have to receive all the binary content of the imafe you're requesting, so it's not really surprising that it would take a relatively long time.\nYou should look into reading the binary data in chunks (i.e. implement proper streaming), or increase the timeout value as needed.\n. No worries! The method you want is inspect_image most likely ;)\nOn Nov 12, 2014 12:55 PM, \"Tomas\" notifications@github.com wrote:\n\nOh, I feel so silly now. I though this call fetches only some metadata.\nShould have read the REST call. Sorry for the fuss and thanks for help.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-py/issues/395#issuecomment-62791637.\n. Hm, I'll see if I can reproduce. What version of docker-py and version of docker are you using?\n. Weird, I get even less lines than you do.\n\n``` python\nIn [22]: f = StringIO(u'FROM busybox\\nRUN echo testing')\nIn [23]: r = c.build(fileobj=f, rm=True, stream=True)   \nIn [24]: for line in r:                               \n    print line\n   ....:   \n{\"stream\":\" ---\\u003e Using cache\\n\"}\n{\"stream\":\" ---\\u003e e39d1400cba1\\n\"}\n{\"stream\":\"Successfully built e39d1400cba1\\n\"}\n```\n. Tentatively closing now that #336 has been merged in, please re-open if the bug still occurs with 0.7.x\n. Interesting -- What happened on the client side? Did the server issue a response with a 4xx or 5xx status? Did the request timeout? None of the above?\n. Weird, if streaming was disabled we should have gone through raise_for_status, and an exception should have occured in your program...\n. Haha, glad we elucidated that mystery ;)\n. Sounds to me like you're overloading your machine. What are the specs? How many containers are you running concurrently?\n. Closing, evidence points to this not being a docker-py issue.\n. No worries, thank you =)\n. Thanks! Can you look into fixing those pep8 errors?\ndocker/client.py:902:80: E501 line too long (96 > 79 characters)\ndocker/client.py:946:9: E303 too many blank lines (2)\n. Thank you!\n. https://github.com/docker/docker-py/blob/master/docker/client.py#L191\nhttps://github.com/docker/docker-py/blob/master/docker/client.py#L368\nhttps://github.com/docker/docker-py/blob/master/docker/client.py#L429\nhttps://github.com/docker/docker-py/blob/master/docker/client.py#L446\nhttps://github.com/docker/docker-py/blob/master/docker/client.py#L563\netc.\nWe use version for other things too, I don't think defaulting to the unversioned endpoint is wise. If ansible doesn't give you a good way to set this parameter, I think that's on them to improve.\nOn top of what J\u00e9r\u00f4me already said, the reason docker-py sets a specific default is because it is tested and assumed to run best against said version (integration tests notably are run before each release). \nProviding people with an option to go versionless is a possibility.\n. I... don't believe that should be necessary. Can you explain your thought process?\n. ```\nIn [10]: c.build(fileobj=df)                                                \nOut[10]: \nIn [13]: for line in _10:\n    print line\n   ....:   \n{\"stream\":\" ---\\u003e 5506de2b643b\\n\"}\n{\"stream\":\"Step 1 : RUN echo \\\"hello world\\\"\\n\"}\n{\"stream\":\" ---\\u003e Using cache\\n\"}\n{\"stream\":\" ---\\u003e ae03898ecbe2\\n\"}\n{\"stream\":\"Step 2 : RUN mkdir /a\\n\"}\n{\"stream\":\" ---\\u003e Running in 51d871806f98\\n\"}\n{\"stream\":\" ---\\u003e 0486101860a8\\n\"}\n{\"stream\":\"Successfully built 0486101860a8\\n\"}\nIn [14]: df = io.BytesIO('FROM ubuntu:14.04\\nRUN echo \"hello world\"\\nRUN mkdir /a')\nIn [16]: c.build(fileobj=df, nocache=True)                                 \nOut[16]: \nIn [17]: for line in _16:\n    print line\n{\"stream\":\" ---\\u003e 5506de2b643b\\n\"}\n{\"stream\":\"Step 1 : RUN echo \\\"hello world\\\"\\n\"}\n{\"stream\":\" ---\\u003e Running in 431836a7ad2f\\n\"}\n{\"stream\":\"hello world\\n\"}\n{\"stream\":\" ---\\u003e d337d1049bba\\n\"}\n{\"stream\":\"Step 2 : RUN mkdir /a\\n\"}\n{\"stream\":\" ---\\u003e Running in 78ee5bf79e5b\\n\"}\n{\"stream\":\" ---\\u003e 3f64c591368b\\n\"}\n{\"stream\":\"Successfully built 3f64c591368b\\n\"}\n```\nnocache seems to be working fine. Using latest docker-py and docker 1.3.2\n. Shouldn't be an issue anymore with #409 merged in.\nThanks!\n. Does this help? https://github.com/docker/docker-py/issues/424#issuecomment-67710118\n. Guys, you need to report that to the boot2docker maintainers, there's nothing I can do at my level about this issue.\n. Sure, but ideally assert_hostname=True should work in the context of boot2docker, too. But I'll look into the proposed fixes, thanks.\n. Hopefully this helps https://github.com/docker/docker-py/commit/7a623c4019a4eeae964a74369a4b0d92821672da\n. Also note that if the bug was indeed introduced in requests>=2.5.0, you shouldn't have any issue using the pinned dependencies from the requirements.txt file, which states requests==2.2.1.\n. I understand this, which is why we have strict requirements in requirements.txt and minimum requirements in setup.py (although if it was entirely up to me I'd pin versions everywhere ~ but that's another debate).\nI only wanted to provide as much information as possible for people that might be affected. :) next release will prioritize system-provided urllib3 over the vendored version, so we should cover most cases this way.\nThank you for helping resolve this.\n. Seems to be breaking in Python 3.2 but not Python 3.3, for some reason.\n. Ah, unicode literals aren't supported in python 3.2, they were reintroduced in 3.3 ...\n. Thanks!\n. Hmm, this issue is specific to python 3.2 (see https://www.python.org/dev/peps/pep-0414/), which I imagine is your default version of python if you're seeing these syntax errors. A real fix would actually be to use the explicit ctor unicode().\n. Used to be the case\nLooks like behavior changed in docker 1.3.0\n. Thanks!\n. Fixed by #428 !\n. Thanks! As it happens, we're already working on something similar in #336 . If you feel like you can contribute to this other PR in any way, feel free ; in any case we'll have to hold off on this one since it will probably create merge conflicts\n. Just a small comment, otherwise LGTM\n. And sorry for the delay here, haven't had much time for docker-py in the past few weeks.\n. Thanks!\n. Thanks @bfirsh for following up.\nI'm going to close this issue since this is now an upstream concern. Feel free to re-open or create a new issue if the situation changes.\n. https://github.com/docker/docker-py/blob/master/setup.py#L10\n. I'd be interested in this. Do you think it should live in the same project, or could it be a separate one that wraps docker-py?\n. #309 would fit into that package, #330 too. Maybe more.\nRe: module name, how about docker.efficiency? \n. I'll try to kickstart something today or tomorrow if it helps.\n. Thrown a branch together for starters, added a few commonly requested functions. See https://github.com/docker/docker-py/tree/efficiency ; Feel free to send PRs against this branch, even if it's WIP.\n. Thanks for the report.\nCan you include a snippet of the code you were trying to run?\n. Please add the argument at the end of the list! Other than that, LGTM =)\n. ~~All of the above! ;)~~ I was thinking of the method arguments specifically, as to not break for people who may depend on the arguments order/position.\n. Thanks!\n. THanks @dims - this is the correct answer =)\n. The fork you're linking to isn't up to date :( our docs leave out the version parameter in the example as you suggest.\n. Not a boot2docker user, but I'll see if I can get someone else to chime in.\n. I believe this can be closed, feel free to ping if that's not the case.\n. Thanks! I can see this being useful indeed. Can we rename the parameter while we're at it? root doesn't make much sense if it's going to point to a file. Call it config_path instead. Also needs to be rebased =)\n. Thanks!\n. Yay, fixes! Thanks =)\n. Thanks =)\n. Thank you! :)\n. Thanks, didn't realize unicode() stopped existing in python 3.x ...\n. Merging and releasing on pypi tomorrow.\n. python\ndef compare_version(v1, v2):\n    \"\"\"Compare docker versions\n    >>> v1 = '1.9'\n    >>> v2 = '1.10'\n    >>> compare_version(v1, v2)\n    1\n    >>> compare_version(v2, v1)\n    -1\n    >>> compare_version(v2, v2)\n    0\n    \"\"\"\n...\nDoes what it says on the label =)\n. Glad you were able to figure it out!\n. Can you share some details on your setup? I am able to ping() just fine, but my environment is pretty vanilla.\n. Default API version for docker-py==0.7.0 is 1.16, but docker 1.3.2 only serves up to 1.14 or 1.15. You should be able to use those two together with client = DockerClient(version='1.14')\n. Thanks!\n. Sorry for the delay. Looks good, any reason you didn't entirely replace _multiplexed_socket_stream_helper()? Also, have you tested it with Python 2 and 3?\n. Hey @objectified , thank you for testing! this is not a change brought up by this PR. We've always yielded bytestrings in Py3. \n. Sorry guys, I just want to run some tests on my end with this before merging, and I haven't had the time so far. This weekend at the latest. Thanks a lot for your help, and your patience.\n. Alright, sorry about the delay! This all looks good :) Thanks a ton for your contribution!\n. 0.7.2 released with that fix :)\n. That's a bug with the docker hub, there's no pagination on the API at the moment, and since ubuntu yields a ton of results, it hangs for a long time and eventually times out. \nNote that it's not the client timing out, but the daemon itself giving up on the request ar some point, hence the 500 error.\nThe hub team is already aware of the bug, and they're working on a fix.\n. Thank you!\n. Which Docker version are you using?\n. Hey, I tried to write some tests to reproduce this issue but couldn't. Could you run this code for me and share the output?\nThanks.\n. I'll close this for now, feel free to reopen if you're still encountering the issue and can share the results of the code snippet posted above.\n. ``` python\nimport docker\nc = docker.Client(version='1.15')\n```\nor update Docker to 1.4.1\n. Thanks! I'll look into this soon.\n. Woah, sorry it took so long. -.- Thanks again!\n. > Should I really parse the response json and look for \"error\" key in there?\nPersonally that is what I would do. I'll keep this open and see if anyone else using docker-py to build regularly has a better answer.\n. Is it always this same test that's failing? Always a timeout?\n```\nERROR: runTest (main.TestCommit)\nTraceback (most recent call last):\n  File \"tests/integration_test.py\", line 64, in tearDown\n    self.client.remove_container(container)\n  File \"/docker-py/docker/client.py\", line 885, in remove_container\n    params=params)\n  File \"/docker-py/docker/client.py\", line 84, in _delete\n    return self.delete(url, self._set_request_timeout(kwargs))\n  File \"/usr/lib/python2.7/dist-packages/requests/sessions.py\", line 526, in delete\n    return self.request('DELETE', url, kwargs)\n  File \"/usr/lib/python2.7/dist-packages/requests/sessions.py\", line 455, in request\n    resp = self.send(prep, send_kwargs)\n  File \"/usr/lib/python2.7/dist-packages/requests/sessions.py\", line 558, in send\n    r = adapter.send(request, kwargs)\n  File \"/usr/lib/python2.7/dist-packages/requests/adapters.py\", line 387, in send\n    raise Timeout(e)\nTimeout: UnixHTTPConnectionPool(host='localhost', port=None): Read timed out. (read timeout=5)\n\nRan 56 tests in 92.059s\nFAILED (errors=1)\n``\n. It used to be prevented in mainline docker. Indeed if that's not the case anymore I see no reason for this code to stay.\n. [Actually still there](https://github.com/docker/docker/blob/master/registry/registry.go#L214), just doesn't preventregistry.hub.docker.com` for some reason.\n. Thanks!\n. Hi there,\nThanks for chipping in and sharing your thoughts. I admit to being kind of torn on the issue of in-code documentation. It does have some nice upsides as you describe, but my main issue with it is adding bloat to files that should aspire to be as concise as possible (I know client.py fails big time there, and it's something I hope to be able to fix at some point). But in my opinion this makes reading and maintaning the actual code more tedious than it should be.\nBut I would love to hear other people's opinions on this, as like I said, I can see both sides of the argument here.\n. Thanks! IIUC that's the same thing as #468 and #463 ? If so we could still use the additional tests.\n. Closing as this was fixed by another PR in 0.7.2. Thanks again for submitting a patch!\n. Thank you for the report! Would you like to submit a fix?\n. Fix is in docker-py 0.7.2 !\n. That's because it's been deprecated for a long time already: see https://github.com/docker/docker-py/blob/master/docker/client.py#L438\nYou can use the following code snippet to get the built image ID: https://github.com/docker/docker-py/issues/392#issuecomment-62293783\nWe're working on having a tools module that could contain helpful code like this, hopefully soon.\n. You're right, it's not explicitly mentioned anywhere. I'll look into it.\n. Thanks! When I have some time I'll look into unifying the interface and merging. I'm not sure what's the best way to present that to developers yet.\n. Sensible. Thank you!\n. LGTM, thanks!\nWould it make sense to add regression tests to prevent breaking it again in the future?\n. I know how it is. If I find some time before you do I'll take care of it :)\n. Thanks, the fix and regression tests made it into 0.7.2 released today. \n. Thanks!\n. Hey everyone, did some investigating, can you try setting TLSConfig.verify to True explicitly and report back if that changed anything?\n. Sorry, clarifying. Assuming you're using kwargs_from_env,\npython\nkw = kwargs_from_env()\nkw['tls'].verify = True\nc = docker.Client(**kw)\nIf you're building TLSConfig:\npython\ntls = docker.utils.TLSConfig(..., verify=True)\nc = docker.Client(tls=tls)\nThanks!\n. Thank you for the feedback, seems like we should update the docs to include the verify=True option.\n. @wardi thanks for the feedback, can you test with current master and updated dependencies? We force a more recent version of requests now, which should help.\n. @cancan101 Code snippet please, and also details of your environment (OS, docker version, b2d version if applicable)\n. What's the output of pip freeze | grep requests ?\n. @cancan101 Can you try to downgrade OpenSSL as suggested here and see if that helps?\n@wardi Alright, thanks for keeping us posted!\n. Looks like the referenced PR has been merged on b2d's side. Closing this as a result.\n. Thank you!\n. Fix is in docker-py 0.7.2 !\n. Well, the issue here I think is that we have started streaming the pull logs (i.e. started writing the HTTP response body) which means we can't modify the HTTP status code. SInce no error was found at the moment the request is processed, the status code is 200.\nAn error during the pull is then simply reported in the logs themselves and since we have a very hands off approach to the data that comes out of there, docker-py doesn't transform it into an exception. The tradeoff is speed of execution (no JSON parsing / extra processing) VS convenience (Having an exception raised when an error occurs in the pull/push/build logs).\nFor this specifically:\njson\n{\"errorDetail\":{\"message\":\"HTTP code: 403\"},\"error\":\"HTTP code: 403\"}\nIt seems this was raised by either the registry or the DockerHub. The context should give you more information regarding which step failed. If you're working with a private registry, note that 0.7.2 fixed an auth bug with those, so this could be related.\n. I believe this has now been fixed in the engine.. I don't see it in the Changelog, can you link to the specific issue?\n. Okay, the issue we're trying to avoid is the one documented in #406 . If someone using boot2docker can test this specifically with requests 2.5.1 and confirm that it's been fixed, I'll be happy to merge this, but I don't see any explicit mention of this.\n. 9bed480c2745b06219c629dbbfd46b298c005447\n. #482 #483 #485 \n--memory-swap is already supported, the parameter is called memswap_limit.\n. I think you're on the wrong repository? The best way to suggest improvements to hub.docker.com is to send an e-mail to support@docker.com !\nFWIW, a public API for the hub is on our roadmap / in the works.\n. Should have been resolved by #336 - What docker-py version are you using?\n. LGTM, thanks! \n. Thanks!\nUm, I think that's all fine. Regarding the Changelog, I usually compile it myself when doing a release, so don't worry about it :)\n. LGTM, thanks! =)\n. You should be able to specify volumes without associated binds. I'm not sure what you mean by \"no support\"...\n. #480 \n. #480 \n. Hi, thanks for contributing! This looks good, my only issue is that, if I understand correctly, this would break for people currently using the events endpoint because of the different output format (strings VS dicts). Can we maybe add the decoding on the fly as an option, and leave the default as is to avoid breaking?\nThanks!\n. Awesome, thanks! LGTM.\n. Thanks for the heads up! I'll look into it.\n. Deactivated the wiki since we weren't using it anyway.\n. Thank you for jumpstarting this! I left a couple comments on the code.\n\n\nNeed to add unit tests\nNeed to add integration tests\n\n\nAye aye, let me know if I can help with those.\n\n\nI have code in place to add a maximum number of stats messages to return in case you don't want to stream indefinitely. Thinking whether this is relevant or not.\n\n\nIs this something that can't be achieved by:\npython\ni = 0\nfor line in client.stats():\n    if i >= MAX:\n        break\n    ...\n?\n. Thank you!\n. LGTM, thanks!\n. LGTM, thanks!\n. Hmm, I think this is a duplicate of #484 \n. I merged #484, so I will close this. Thank you for contributing!\n. Thanks, LGTM\n. Hi, I just merged a PR to integrate this. Please think to do a small search before posting an issue next time :)\n. #484 \n. Thanks, LGTM!\n. LGTM!\n. Thank you!\n. Thanks!\n. Hi,\nThanks for your report! This is working as intended.\n. > /containers/create in the remote api (which I believe docker-py is using) doesn't pull for you?\nYep\n\nis there a reason it doesn't?\n\nIt's a docker CLI feature for ease of use (if image isn't present, try to pull it, then re-run the create command). When using the API programmatically you want to privilege control over ease of use, and avoid side effects like that.\nHope that makes sense!\n. Thanks!\n. I'd like that. You can actually hit the unversioned API endpoint /version to fetch the server's version preemptively. We had a start here: #281 but the PR was abandoned by its author.\n. @HackToday see discussion on #505, but you will always be able to specify the API version manually no matter what.\n. Good catch! Thanks! :)\n. I'll take some time to look into this today, thank you for your contribution!\n. Looks good from a code standpoint. I have a couple issues with the design, mainly with the idea of having the version be fetched on the first request. I feel like we're hiding what really happens behind the scenes and if issues happen during that phase it will confuse the hell out of users.\nAdditionally, I'm not sure this should be the default behavior (mainly because I dislike the idea of making that extra request in the general case).\nWhat do you think of the following plan:\n1. Add a special value \"auto\" to the valid version values, which will trigger this behavior\n2. If version=None, use the DEFAULT_DOCKER_API_VERSION constant.\n3. If version=\"auto\", fetch the API version from the daemon inside the Client.__init__ method.\nAlternatively, if you think having a request inside the __init__ method is a bad idea, we could have an additional Client method called retrieve_server_version() or something similar that fetches the API version and sets the Client._version attribute. It's a bit extra work for the user but at least there's no implicit requests happening behind the scenes.\n. Merged, thanks a lot for your help!\n. Fixed by #507 \n. Thanks Aanand! Merged :)\n. Duplicate of #465 \n. Thanks!\n. That's the error from the daemon, so you'll need to bring it up to the main project instead.  :)\n. Do you declare the port you want to open in the create_container method?\n. Thanks, I'll look into it.\n. Ah, seems like this is an issue with network_mode=host. I'm not sure whether this is a bug or expected behavior, but it's definitely a docker engine issue:\n$ docker run -d -p 22 -P --net=host busybox /bin/sleep 30\na3ed61864c579635e6120aaae09a944a964c011c5218add99412da1da76d74d9\n$ docker port a3ed 22\nFATA[0000] Error: No public port '22/tcp' published for a3ed\n. Please create an issue in the docker/docker tracker instead.\n. Good catch! Thanks!\n. I'll look into it. \n\nIt would be nice if my system didn't have to stay with requests 2.4.3 just to support docker-py.\n\nWell to be fair, you could use Docker or virtualenv to isolate your environment :) But I understand the sentiment.\n. Fixed by 9bed480\n. Thanks!\n. Thanks!\n. Is that recent? Probably needs a rebase, I fixed it 2 weeks ago\n. Thank you for sharing your concerns! The example was written like this to outline the fact that there is no return value to client.start. There might be a better way to get the point across, but removing information definitely isn't the way to go here.\n. Thanks!\n. You're overloading your machine with more containers than it can handle. Docker doesn't get enough CPU time to respond in the 60 seconds timeout. You should either:\n- start less containers\n- use mem_limit in create_container to limit the amount of resources each container will occupy on your machine\nSince this is not a docker-py issue, I will now close this ticket. If you need additional help, please refer to the mailing list or #docker IRC channel.\n. Thanks! I'll look into this when preparing the next release \u2013 sorry for the delay.\n. Sorry, I don't have an ETA right now. My time is focused on a different project at the moment, but I will have more leeway by the end of the month.\n. Thank you for your patience. =)\n. LGTM, thanks!\n. LGTM, thanks guys!\n. Thanks! Nice catch.\n. GET /v1/_ping needs to be unprotected on your registry. As far as I know, this is also how the official docker client behaves.\n. I'll close if that's resolved.\n. Thanks! =]\n. Sorry for the absence of response, haven't had much time to spend on the project recently. Things could definitely be better in that regard, but there is a lot to consider with regards to backwards compatibility, ease of use and familiarity. I'll give this proposal another look later, and answer more thoroughly.\n. Thanks!\n. Thanks!\n. It's probably an engine limitation. Thanks for reporting it, I'll look into it.\n. See also #546 \nDocker Engine is more or less deprecating the passing of HostConfig values in start (the CLI exclusively uses create-time provided HostConfig now), so I doubt they would want to fix it.\n. Closing by #593 \n. It may have been deprecated in recent versions of the API without me noticing.\n. I'll look into it, might not happen right away since my efforts will go to 1.6 support first. Thanks!\n. LGTM, thanks for taking the time to do this! \n. Thank you for the report! Closing as a duplicate of #517 \n. Please provide the following information:\n- OS used\n- docker version\n- docker-py version\n. Thanks, I'll look into it for 1.2\n. Guys, I'm having trouble reproducing this, can you try and run the following code and report back with the results? https://gist.github.com/shin-/94bc282e8a41f0d09786\nThanks!\n. No, just running the file in an environment where docker-py is installed\nshould do the trick.\nOn Thu, Apr 23, 2015 at 1:04 AM, Tobias Munk notifications@github.com\nwrote:\n\n@shin- https://github.com/shin- How do I run the test case? Would I\nneed to get docker-py source on the broken system?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-py/issues/546#issuecomment-95484155.\n\n\nJoffrey F, Software Engineer @docker\ndocker-py maintainer\n. I'll give that a shot in the morning, but looking at the code of both projects there's no reason it would work differently from using the Docker CLI.\n. I think I've found the issue: can you try specifying the binds in the create_container call, like such\npython\nc.create_container(\n    img, cmd, volumes=volumes, \n    host_config=docker.utils.create_host_config(binds=binds)\n)\nAnd see if that works?\n. (and then call start without specifying binds)\n. Sorry, I meant in your own code, replacing img and cmd by the appropriate image and command that need to be run.\n. I've reported the issue upstream (https://github.com/docker/docker/issues/12759), but Ansible will probably need to be updated to provide HostConfig params in create_container instead of start\n. Closing, setting host config in start is deprecated starting with 1.2.3\n. Should we add @mpetazzoni and maybe @aanand in there? (I know we're kind of in maintainer-limbo, but if we're going to merge this officially let's take the chance to straighten it out.)\n. Closing as duplicate of #444 \n. Thanks! I actually took a different approach in #568, but end result is essentially the same.\n. Can you try to run the equivalent commands with the docker CLI and see if the problem happens there too?\n. cc @vieux Any insight on why that is needed? This seems really odd considering the request's body is empty.\n. @trwq63 can you confirm that this is fixed for you too?\n. I'll close, feel free to reopen if there's more info.\n. Thanks @aidanhs ! \nAs a general rule, docker-py almost never tampers with the way response data is formatted. I'll close this issue, but feel free to raise the issue on the docker repo\n. https://github.com/docker/docker/issues/12748\n. Looks like a docker engine issue. Feel free to report it on docker/docker.\n. The CLI is sending \"null\" which is automatically translated to None. I don't think docker-py should tamper with that.\n. Thank you for the contribution! Could you add unit tests to validate that this will behave as expected?\n. Thanks!\n. Unsurprisingly, --env-file is a client-side feature of the docker CLI, so it's debatable whether this should be included in that project or not. I'll explore the possibility.\n. Thanks! =)\n. I commented on https://github.com/projectatomic/atomic/issues/41 by the way. Need more info before I can look into this.\n. #999 ?\n. Nice catch, thanks!\n. Indeed. Thanks!\n. Do not pass host config to start() if you can help it.\n. Closing by #593 \n. ``` python\nimport docker\nfrom docker.utils.types import LogConfig\nc = docker.Client()\nhc = c.create_host_config(log_config={'type': LogConfig.types.SYSLOG, 'config': {}})\nctnr = c.create_container(image='busybox', command='ls -la', host_config=hc)\n```\nAlso, the syntax if you want to provide keyword arguments is **options (double star), *options being for positional arguments. In that case:\n``` python\nimport docker\nfrom docker.utils.types import LogConfig\noptions = {\n  'image': 'busybox',\n 'command': 'ls -la'\n}\nc = docker.Client()\noptions['host_config'] = c.create_host_config(log_config={'type': LogConfig.types.SYSLOG, 'config': {}})\nctnr = c.create_container(**options)\n``\n. Thank you for the report! Reproduction steps would be extremely helpful if possible (i.e. which image did you use, what parameters did you use in the create / start methods, etc.)\n. Closing via #669 \n. Gonna need: OS, docker-py version, docker version, value of yourDOCKER_HOST` environment (if any). Also you should report that on the ansible plugin repo as well.\n. Hey @mikexstudios , thank you for reporting this!\nI should have a patch for this by the end of the day, do you think you could help me test it with a real-life .dockerignore file?\n. Thank you for reporting this.\nSeems like a corner case... I think the information provided right now is sufficient. And I don't want to assume anything about the underlying system (keep in mind people are going to use different ways to contact the API (HTTP(S), TCP, UNIX...), a ConnectionError can mean many things. \nYou've got the perfect example of that by the way, with the CLI suggesting to use TLS when this has nothing to do with your issue.\nI'll close the issue but feel free to add to it if you think there's more to be discussed.\n. Thanks for contributing!\nDoes that work? It's not documented... Also, can you look into fixing the pep8 issue (the declaration line spans over 80+ characters)\n. Thanks!\n. for ev in client.events(decode=True):\n    assert isinstance(ev, dict)\nNeeds to be documented :)\n. Ha, I don't use the Makefile, so I missed those. Should be an easy fix (and they should run fine if you do python tests/integration_test.py in your vagrant box.\nI'll look into fixing, thanks for the report.\n. Fixed by #587 \n. Not implemented at the moment, since it never made it into the Changelog ... I'll look into adding it.\n. What is your docker version and what OS is it running on? I tested it and it works for me.\n```\nIn [9]: c.create_container('busybox', 'sleep 999', mem_limit='512m', memswap_limit=-1)\nOut[9]: \n{u'Id': u'f5372b4faf62aa1b414b892f26d72fdac7f5b595ea890c3c657e1721b56b0441',\n u'Warnings': None}\nIn [10]: data = c.inspect_container(_)\nIn [12]: data['Config']['Memory']\nOut[12]: 536870912\nIn [13]: data['HostConfig']['Memory']\nOut[13]: 536870912\nIn [14]: data['HostConfig']['MemorySwap']\nOut[14]: -1\nIn [15]: data['Config']['MemorySwap']\nOut[15]: -1\n```\n. Try this instead:\ncontainer = c.create_container('longgeek/ubuntu-14.04.1:base', '/bin/bash', mem_limit='512m', memswap_limit=-1, host_config=docker.utils.create_host_config(publish_all_ports=True))\nc.start(container)\nThis is related to what I mentioned here, we're going to deprecate passing host config in start because it's poorly supported in the docker engine\n. @mikexstudios would you like to take a look at this before I merge it? Thanks.\n. Thanks for contributing!\nI'm really uncomfortable with touching that part of the code. Do you happen to have a code snippet or integration test that would allow us to reproduce this and test the regression?\n. Thanks, and sorry for the delayed response. I'm taking a look now to make sure I understand correctly what's happening.\n. One last small nitpick, otherwise LGTM. Thanks for your help and patience through this!\n. Looks like something's gone awry with your rebase, did you check you were on the latest version of master?\n. Much butter, thanks :)\n. Squashed and merged 7f3692ceeda92ca3690394822cbd3c99378c0d7e\nThanks a ton!\n. Use HostConfig\n. Thanks! Please note that execute will disappear in the next release!\n. As stated in the link I provided, you should use exec_create and exec_start instead.\n. 1. In your example, create_res can also be used in exec_inspect (very useful to get the result of the exec command) and exec_resize. \n2. It's closer to the remote API design\n3. It doesn't violate one of the core design principles which is that each method in client should be atomic (i.e. make at most one call to the remote API).\n4. If none of that matters to you personally, you're still able to literally write a 2-line function that reproduces the behavior of execute.\nHopefully that clarifies the reason for the change.\n. Yes, this is correct.\n. Sorry for the delay on this - You're not getting an error because you're not doing volume configuration properly.\n```\nIn [1]: import docker\nIn [2]: c = docker.Client()\nIn [6]: cfg = docker.utils.create_host_config(binds={'./tests': { 'bind': '/vol1', 'ro': False }})\nIn [8]: container_id = c.create_container('busybox', 'ls /vol1', volumes=['/vol1'], host_config=cfg)\nAPIError: 500 Server Error: Internal Server Error (\"cannot bind mount volume: ./tests volume paths must be absolute.\")\n```\nWe could add more validation arguably (for example to not let you set a string as the \"volumes\" param) but there's a limit to how much we can really do I think.(i.e., if you specify volumes but not binds, there's no way for docker-py to tell if that's an error, or simply the intent to have unbound volumes)\n. Use docker.utils.kwargs_from_env. Docs here. As a design point, I think it's better that nothing be hidden from sight of the user (i.e. only use environment values if it's done explicitly).\n. Closing in favor of #604 \n. Thanks, I'll give that a serious look over later this week. Can you look into the test failures in the meantime? https://travis-ci.org/docker/docker-py/builds/62859246\n. You'll need to add pathspec >=0.3.3, <0.4.0 to the dependencies in setup.py.\n. This would be better done in the check_resource decorator.\n. Thanks!\n. Good catch, thanks!\n. Mmmh, my concerns are:\n- if we put it here the condition will also be unnecessarily evaluated in the case of image-oriented functions. Seems like a bad practice even though the overhead is negligible.\n- I'd rather have several short, one-purpose decorators than a long, single, multi-purpose one.\n. Thanks!\n. Thank you!\n. Thanks! :+1:  sorry about the delay.\n. I can't help if I don't know what code you're running.\n. Sorry, I didn't express myself properly. \nPlease provide me with a code snippet that is focused to the problem you're having (in that case, the code that creates and starts a container). I don't have the time or patience to go through your whole project to figure out how you're using docker-py.\n. Thank you!\n. End of June is the current plan.\n. Hm, alright, I've had some time to look into this. On your thoughts:\n- I feel the docs are quite specific about when -1 will be returned,\n\nReturns the value -1 if no StatusCode is returned by the API.\n\nI guess we could rephrase into\n\nReturns the value -1 if the API responds without a StatusCode attribute.\n- This is interesting, because right now this exception will be returned any time the specified HTTP timeout is reached, it applies across the board, The fact that you seem to point it out as a Client.wait specific problem indicates that there may be a debate to be had about having this particular method behave differently, depending on whether we value being explicit over being consistent. Also it's a breaking change for people depending on the current behavior. @mpetazzoni @aanand, thoughts on that point?\n. Fixed in 69ef9e5668254adf8c951571660b11d4c89b2529\n. duplicate of #621 \n. This is not the right way to solve this. requirements.txt will always pin dependencies, this has been discussed many times in the past. However, updating this dependency to a more recent version would be good.\n. Fixed by #624 \n. It's a 1.7 change. I'll take care of it today.\nOn Jun 2, 2015 9:37 AM, \"Scott Sanderson\" notifications@github.com wrote:\n(I'm seeing this issue as well by the way)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-py/issues/623#issuecomment-108008785.\n. https://github.com/docker/docker/issues/13663\n. On hold until Docker Engine 1.7 lands.\n. Thank you!\n. Thank you!\n. Thanks! Merged\n. Thanks :)\n. Closing via #669 \n. Thanks!\n. See #611 and docs\n. Thanks! Merged\n. Thanks! Merged\n. Um, works for me...\n\n$ pip install docker-py==1.2.2\nDownloading/unpacking docker-py==1.2.2\n[snip]\n$ pip freeze | grep docker\ndocker-py==1.2.2\n. Created #664 to keep track of that separately - I agree, but we have to keep in mind it's used when the version is too low as well as too high.\n. Fix the fix? ~\nJokes aside, the onus is on ansible to support the new way of passing those arguments (otherwise it will break with newer versions of Docker anyway). In the meantime, you can:\n- pin the docker-py version to 1.2.3 \n- somehow tell ansible to use a Docker API version < 1.19 (the version parameter in docker.Client())\n. Hm, that's definitely a bug. Thank you for reporting it.\n. Duplicate of #617 !\n. I agree that the end goal should be to have most of those outside of the class, but I was considering this as a first step in separating API code from internal code. I strongly believe it's still better than the current state of the code and it's relatively low effort.\n. LGTM. Thanks!\n. #660 was merged but I have to look into the test removal question still, so I'm keeping this ticket open for now.\n. Test re-added in #848 \n. Client.build(..., pull=False)\nSee also #622 #624\n. Fixed by #617 \n. Is this similar to #631?\n. Fixes #656 \n. Sorry, it was too late to make the cut, but I'll look into having it in a patch version.\n. Awesome, thanks!\n. Change your config to allow unauthenticated access to the ping endpoint, as seen here\n. Already addressed and the two issues you've linked to contain a code snippet that responds to your particular use case.\n. https://github.com/docker/docker-py/issues/657#issuecomment-116782795\n. Working on this as part of #771 \n. Merged into master!\n. Use Client.exec_inspect\n``` python\ncli.exec_inspect(ins)['ExitCode']\n\n\n\n2\n``\n. Yay!\n. You have several options: \n- upgrade docker to 1.7.0\n- indicate the API version corresponding to your current docker installation (1.18) in thedocker.Clientconstructor, like this:c = docker.Client(version='1.18')- Tell docker-py you want it to detect the version it needs to use automaticallyc = docker.Client(version='auto')`\n\n\n\nHTH!\n. LGTM!\n. lgtm!\n. I created #682 based off of this, with a change to support older versions of the API / Docker daemon. Feel free to review it! I will close this PR in favor of the new one as a result,\n. Thanks!\n. Thanks, lgtm!\n. Thanks!\n. Thanks for following up so closely @moutten !\n. Added in 3caaa0050b044462157b755f3810e44857d73761\n. Thanks!\n. python\nc = docker.Client()\nwith f = open('./fedora.tar', 'r');\n    c.load_image(f)\n. Can you address those few comments? Thanks!\n. Thanks, that's helpful - I'll take it from here.\n. See #698 \n. Thanks! Fixed via #716 :)\n. #729 was merged, closing!\n. Merged - thanks!\n. Thank you! :+1: \n. Do you mind giving it a whirl against your branch when you have some time? Thanks!\n. Merged, sorry for the delay!\n. Thanks!\n. --net=none and network_disabled are two different things. The command you're running with docker-py is actually equivalent to docker run --networking=false debian:sid cat /etc/hosts\n$ docker run -t --networking=false ubuntu cat /etc/hosts \nWarning: '--networking' is deprecated, it will be removed soon. See usage.\n$ docker run -t --net=none ubuntu cat /etc/hosts\n127.0.0.1   localhost\n::1 localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\nHere's the equivalent docker-py line you would use:\npython\nclient.create_container(\n    'debian:sid', \n    'cat /etc/hosts', \n    host_config=docker.utils.create_host_config(network_mode='none')\n)\nHTH!\n. Happy to help - docker-py should probably warn about network_disabled being deprecated somewhere.\n. LGTM\n. Hm, if you only need one or the other, you can set it up at the exec_create level by setting stderr or stdout to False respectively. Outside of that, as far as I know and as far as the remote API reference is concerned, there's no way to filter the logs of an exec instance.\n. I'm not privy to the plans of the engine team, so you'll have to ask those guys if an attach-like functionality is planned for exec instances. =/\nhttps://github.com/docker/docker/issues/9527 seems like a good start.\n. Resolved in #2150 . Also being discussed in #731 \n. Please make sure the unit tests pass. make unit-test can help you verify that.\n. Closing via #724 \n. There's a lot of commits that shouldn't be here - can you try and isolate your changes and submit a clean PR? Thanks :)\n. Hm, I'm not able to reproduce. Did you follow the instructions from the docs? What version of docker / docker-py are you using?\n```\nIn [1]: import docker\nIn [2]: c = docker.Client()\nIn [3]: c = docker.Client(version='1.18')\nIn [5]: c.create_container(\n  'busybox', 'ls', ports=[80, 443], \n  host_config=docker.utils.create_host_config(\n     port_bindings={ 80:8000, 443: 9000 }\n  )\n)\nOut[5]: \n{u'Id': u'5bebb07163697d2bef9e3d8642b28ef7f5208e864b1f10f64900e4a79f078931',\n u'Warnings': None}\nIn [6]: c.start(_5)\nIn [7]: c.inspect_container(_5)\nOut[7]: \n{\n...\n u'HostConfig': {\n  u'PortBindings': {u'443/tcp': [{u'HostIp': u'', u'HostPort': u'9000'}],\n   u'80/tcp': [{u'HostIp': u'', u'HostPort': u'8000'}]},\n  }\n }\n...\n}\n``\n. No response, closing. Feel free to reopen if you're still encountering this issue.\n. Thanks!\n. Fixed in master - thanks for the report!\n. Thanks! #724 \n. Fixed by #732 \n. Applied a few fixes and merged manually. Thanks!\n. I feel there's value in having the user be able to useLogConfig.types.FLUENTDinstead of providing an error-prone string value. Maybe we can do away with the validation... \n. Just a few comments, looks pretty good overall :+1: \n. Thanks.\n. LGTM - thanks!\n. The problem I have with that is the fact that in most cases, this method will be called directly by the API consumer, which makes the version detection process very error-prone. I don't disagree that it needs a solution, but I'm unconvinced this is the right approach to it.\n. Sorry - yeah, that seems to be the right solution for this. It rubs me the wrong way that we're adding still more stuff to theClientobject that already feels overcrowded, but that's a minor thing, it shouldn't block us.\n. Thanks, could you squash those 2 commits?\n. Thanks!\n. You're looking to use thedevicesparam indocker.utils.create_host_config`. Unfortunately it looks like it's missing from the documentation, but the feature does exist already.\n. Documentation updated in 5d95f24264d25991e9f7caf23e9c860bcdb11874\n. Correct - although, we should support both forms IMO, but it doesn't look like we do at the moment.\n. Thanks!\n. LGTM! \n. :+1:\n. That's actually what we recommend in the docs for boot2docker currently. SSL certs without a hostname are kind of an oddity as far as the general case goes. \nMaking the default more lenient could be an option (i.e. assume assert_hostname=False unless otherwise specified) but it really feels like a security faux-pas.\n. Thanks!\n. LGTM!\n. Thanks!\n. :+1: \n. #738 ... oops\n. Looks like you removed it from #721 - But your solution looks good. Feel free to merge it in.\n. Sorry for the delayed response, had to figure out priority for that, but I'll be working on this in the next week or two.\n. docker-py version?\n. So, merging that PR comes with the expectation that we'll support Pypy specific issues in the future. I'm not sure I want to commit to that.\nOf course, that doesn't mean that we'll intentionally break it, or that we won't make a reasonable effort to help fix it if it's broken.\n. It's a docker issue, not a docker-py issue. You should get a deprecation warning when passing arguments to start, and the current plan is to remove that ability entirely in the near future.\n. Thanks :)\n. Perfect - I was gonna ask yesterday but I forgot. LGTM :+1: \n. That's happening now - some improvements can and will be made in the near future so that they hopefully run faster than they are currently.\n. @aanand @dnephin PTAL!\n. How else would you test volume binds though?\n. I wasn't able to reproduce, but I noticed you're using python 2.6.1 which is extremely outdated - can you try using 2.6.9 and see if you notice any difference?\n. No response in 1 month, closing. Feel free to reopen if you're still having this issue.\n. #718 \n. Release will happen in the next week or so.\n. Included a fix for #769 as well.\n. Thanks!\n. Oh, yeah. It's only available in 1.8+. I'll add a skip. Thanks for the report.\n. Fixed via #764 \n. Should be good now.\n. #765 \n. Thanks! LGTM.\n. Closing in favor of #787 \n. Rebased!\n@docker/docker-py-maintainers Can we include this in the 1.5.0 rc? :)\n. Thanks!\n. As mentioned in the docker/docker#15567, this seems to be the wrong approach (we should maintain backwards compatibility at every level of the stack). \nAs a result, I am closing this for now until the issue is properly resolved upstream.\n. Good catch, thanks!\n. I'm going to merge this, we can always continue to improve it as we go if need be. Thanks everyone for your feedback!\n. I'll take a look.\n. I tried to reproduce this issue but was unable to (tested with python 2.7 and 3.4). Here is the code I'm running.\n```\n-- coding: utf-8 --\nfrom docker.utils import convert_volume_binds\ndata = {\n    '/mnt/\uc9c0\uc5f0': {\n        'bind': '/unicode/\ubc15',\n        'mode': 'rw'\n    }\n}\nprint(convert_volume_binds(data) == ['/mnt/\uc9c0\uc5f0:/unicode/\ubc15:rw'])\n```\n@ColinHuang , can you provide a short example of code that fails. Also, the stacktrace you shared seems to be truncated, can you provide a full stacktrace so we can look deeper into that issue?\nThanks!\n. @ColinHuang Can you give the patch in #787 a try and confirm it fixes the issue for you? Thanks!\n. Merged! Thanks.\n. I just added the most recent release tags to the automated build configuration so you should be able to pull 1.3.1 and 1.4.0 now. The tag latest should be synced with master. \nDid you have anything else in mind that would help?\n. We don't merge anything into master if the tests don't pass - so we should be safe there. Thanks for the feedback!\n. Added by https://github.com/docker/docker/pull/12856 in docker 1.8.0\n. Fixed by #802 \n. What would be an example of a use-case where accessing this package directly is necessary?\n. Did you install the package from pypi? Which version of Python are you using? I'm not seeing any issues with a fresh install on 2.7 or 3.4 :/\n. Ah, you're right. I was confused, sorry.\n. @aanand Thanks, didn't know about six.u. PTAL?\n. Okay, I've tried all sorts of combination, I've settled on just separating py2 and py3 tests completely (so it's 2+2 now). On the bright side, results were consistent all along (and we have tests to prove it)!\n. Thanks!\nLGTM - as a bonus, adding a small test for that feature would be nice.\n. Cool! LGTM :+1: \n. Actually, if something fails, the more verbose the better I think... It's not always useful but I'll take too much information over not enough of it. :x\n. Fixed the flake8 issues and merged :)\n. Thanks for the report!\nWhile this has no bearing on that particular bug, I would strongly encourage you to upgrade docker to a more recent version if you can. \n. LGTM :+1: \n. Updated version #850 - closing.\n. We wouldn't do that in create_container because we want to stick as close as possible to the API itself, which defines those two things separately (config part declares the volumes themselves, host_config part optionally defines the host folder mappings).\nHowever, It could be implemented as a helper, or a high-level wrapper on create_container. \n. I wonder if we should release that with the default API version set to 1.21? Given there's a good chance this goes out before Docker 1.9.0 releases, this might be very confusing to users.\n. Implemented @dnephin 's suggestions and rebased.\n. Thanks for the report! It's not related to JWT however, see https://github.com/docker/docker/blob/master/api/server/router/local/image.go#L172-L179\nI'll submit a patch.\n. Thanks for contributing!\nThis could use an integration test, otherwise LGTM.\n. Thanks!\n. docker-py 1.5.0 specifies API version 1.20 as the default, but your docker daemon only supports up to API 1.19 (1.20 was introduced with Docker 1.8.x)\nYou can easily fix this by pinning the version you want to use when constructing the Client, i.e.\npython\nimport docker\nc = docker.Client(version='1.19')\n. Assuming this is solved, feel free to reopen if that's not the case.\n. Thanks! Can you squash those into one single commit?\nOn a tangential note, I'm not the biggest fan of calling the client instance cli because of the \"Command Line Interface\" acronym. But we can take another pass at this later - it being consistent means it's easier to sed later down the line.\n. LGTM, thanks!\n. Yup, I was trying to import the integration/ module in the integration_test.py file to help transition but that doesn't work as I expected to. Will update soon. :)\n. ```\n$ pip install -U 'requests<2.8.0'Collecting requests<2.8.0\n  Using cached requests-2.7.0-py2.py3-none-any.whl\nInstalling collected packages: requests\n  Found existing installation: requests 2.8.0\n    Uninstalling requests-2.8.0:\n      Successfully uninstalled requests-2.8.0\nSuccessfully installed requests-2.7.0\n$ py.test -k647 tests/integration/regression_test.py \n============================= test session starts ==============================\nplatform linux2 -- Python 2.7.9 -- py-1.4.30 -- pytest-2.7.2 -- /home/joffrey/.envs/pydocker/bin/python\nrootdir: /home/joffrey/work/pydocker, inifile: pytest.ini\nplugins: cov\ncollected 6 items \ntests/integration/regression_test.py::TestRegressions::test_647_support_doubleslash_in_image_names PASSED\n======================== 5 tests deselected by '-k647' =========================\n==================== 1 passed, 5 deselected in 0.07 seconds ====================\n$ pip install -U 'requests==2.8.0'Collecting requests==2.8.0\n  Using cached requests-2.8.0-py2.py3-none-any.whl\nInstalling collected packages: requests\n  Found existing installation: requests 2.7.0\n    Uninstalling requests-2.7.0:\n      Successfully uninstalled requests-2.7.0\nSuccessfully installed requests-2.8.0\n(pydocker)joffrey@yuna:~/work/pydocker$ py.test -k647 tests/integration/regression_test.py \n============================= test session starts ==============================\nplatform linux2 -- Python 2.7.9 -- py-1.4.30 -- pytest-2.7.2 -- /home/joffrey/.envs/pydocker/bin/python\nrootdir: /home/joffrey/work/pydocker, inifile: pytest.ini\nplugins: cov\ncollected 6 items \ntests/integration/regression_test.py::TestRegressions::test_647_support_doubleslash_in_image_names FAILED\n=================================== FAILURES ===================================\n___ TestRegressions.test_647_support_doubleslash_in_image_names ____\ntests/integration/regression_test.py:30: in test_647_support_doubleslash_in_image_names\n    self.client.inspect_image('gensokyo.jp//kirisame')\ndocker/utils/decorators.py:21: in wrapped\n    return f(self, resource_id, *args, kwargs)\ndocker/api/image.py:143: in inspect_image\n    self._get(self._url(\"/images/{0}/json\", image)), True\ndocker/client.py:110: in _get\n    return self.get(url, self._set_request_timeout(kwargs))\n../../.envs/pydocker/local/lib/python2.7/site-packages/requests/sessions.py:483: in get\n    return self.request('GET', url, kwargs)\n../../.envs/pydocker/local/lib/python2.7/site-packages/requests/sessions.py:471: in request\n    resp = self.send(prep, send_kwargs)\n../../.envs/pydocker/local/lib/python2.7/site-packages/requests/sessions.py:600: in send\n    history = [resp for resp in gen] if allow_redirects else []\n../../.envs/pydocker/local/lib/python2.7/site-packages/requests/sessions.py:195: in resolve_redirects\n    adapter_kwargs\n../../.envs/pydocker/local/lib/python2.7/site-packages/requests/sessions.py:579: in send\n    r = adapter.send(request, kwargs)\n../../.envs/pydocker/local/lib/python2.7/site-packages/requests/adapters.py:339: in send\n    url = self.request_url(request, proxies)\n../../.envs/pydocker/local/lib/python2.7/site-packages/requests/adapters.py:278: in request_url\n    proxy = select_proxy(request.url, proxies)\n../../.envs/pydocker/local/lib/python2.7/site-packages/requests/utils.py:550: in select_proxy\n    proxy = proxies.get(urlparts.scheme+'://'+urlparts.hostname)\nE   TypeError: cannot concatenate 'str' and 'NoneType' objects\n======================== 5 tests deselected by '-k647' =========================\n==================== 1 failed, 5 deselected in 0.14 seconds ====================\n```\n. We won't support float values after all, see rationale here https://github.com/docker/docker-py/pull/880#issuecomment-166335547\n. Thanks!\nThe tests are failing because you have trailing whitespace and an indentation error in your patch. You can run make flake8 in the project's root to detect those locally.\nApart from the code style nitpicking:\n- It seems the option was added in API version 1.20 - can we add a client-side check for that? Just like we do for group_add here\n- The parameter should be added to the HostConfig documentation (docs/host_config.md).\n- It might also be valuable to add an integration test that verifies the value we pass is reflected in the inspect_container dict.\n. Closing via #818 \n. I've investigated this issue,\nthe problem with the solution proposed is that importing pyopenssl raises the following exception if no additional packages are installed:\nImportError: No module named ndg.httpsclient.ssl_peer_verification\nInstalling this package requires additional system packages (libffi notably, which in turn requires python-dev and libssl on Ubuntu) - which is a no-go as far as I'm concerned, given we want to keep the installation and usage of docker-py simple and universal.\nI'm confused as to why pyopenssl would raise a KeyError for any of those protocols. Can you provide more information about the platform you're running on, and the version of pyopenssl you have currently installed? Does upgrading pyopenssl to the latest version fix the issue?\n. @zbyte64 Thanks. Can you share the output of pip freeze and what OS/version you're running on as well?\n. Thanks for contributing!\nI'm going to wait for the 1.9.0 release to merge this to make sure things don't change last minute.\nIn the meantime, having some tests and mentioning the added methods in the documentation would be a valuable improvement.\n. Hi @rmb938 , do you mind rebasing this? We can take a closer look now that 1.9 is out.\n. Thanks for updating. A few things to change in the docs and a small nitpick in the create_ipam_config function, then we should be good.\nFeel free to squash it as well :)\n. Thanks, looking great - LGTM!\n. In addition to what @dnephin said, there is a discussion about using Sphinx here: #454 \n. Your last paragraph seems to point at some sort of quickstart guide, which could potentially be a good idea. It would probably assume the reader already knows python, though.\nWe could potentially separate the different API methods thematically, but that might cause confusion considering all of them still exist as methods of the same docker.Client class. Could be worked around by having a \"Client index\" that lists all the methods and links to the different pages, but the page hops might be irritating to some level.\n. I feel like that's a question that would much find better answers at the http://github.com/docker/swarm repo.\n. LGTM!\n. Thanks! LGTM.\n. Thanks!\n. Sorry, I didn't realize this had been sitting here for so long.\nYour issue is you're passing parameters to Client.start, which isn't working properly for recent versions of Docker. In fact, doing so should display a deprecation warning.\nFor more info on this, see #822 \nTo remedy your issue, the proper way to proceed is to provide a host config in create_container with a properly set dns parameter, as such:\n``` python\nimport docker\nc = docker.Client(base_url='unix:///var/run/docker.sock',version='1.18')\nimage_name = 'centos:6'\nargs = {'tty': True, 'name': 'kyle-test', 'hostname': 'instance-kyle-test', 'environment': {}, 'network_disabled': True, 'mem_limit': 536870912, 'cpu_shares': 1024}\nhost_config = c.create_host_config(dns=['8.8.8.8', '8.8.4.4'])\nctnr = c.create_container(image_name, host_config=host_config, **args)\nc.start(ctnr)\n```\n. RIP janky :angel: :rose: \n. LGTM. Thanks!\n. Thanks!\nSorry it took me a while to take another look. LGTM now. :+1: \n. This has been reported before, but I haven't been able to reproduce it. can you help me investigate? https://github.com/docker/docker-py/issues/305#issuecomment-149387309\n. @harpingon How do you call Client.start? Any arguments in addition to the container ID?\n. Here's a code sample you can run with a more recent version of docker (that other comment was in reply to someone using Docker 1.6.x)\n``` python\nimport docker\nc = docker.Client(version='auto')\nprint('VERSION: ', c.version())\nc.pull('busybox:latest')\nctnr_id = c.create_container(image='busybox:latest', host_config=c.create_host_config(mem_limit='5g'))\nprint('CONTAINER ID: ', ctnr_id)\nctnr_data = c.inspect_container(ctnr_id)\nprint('Mem Limit (config): ', ctnr_data['Config']['Memory'])\nprint('Mem Limit (host config): ', ctnr_data['HostConfig']['Memory'])\n```\n. you're running into #750 . \n. I didn't think of that, but you're right. Noting this down!\n. The PR for this has been merged - closing.\n. Thanks!\nOne small comment on the test. Additionally, can you squash those commits into one? Once that's done, LGTM.\n. Thanks! Looking back there's a few things still.\n1. Your commit is missing the signoff at the bottom.\n2. The test should require version 1.21 (the latest API version) instead of 1.9 (the engine version).\n3. In Client.build, if buildargs is provided, we should check the version is greater or equal 1.21 (utils.version_gte is a good way to do that) and raise an errors.InvalidVersion if it isn't.\n. There's a race condition where your test is progressing before the build operation is finished on the server side. I've updated the PR and submitted it as #846 . Let's follow up there.\n. Merged into master :+1: \n. LGTM :+1: \n. http://docker-py.readthedocs.org/en/1.5.0/api/#containers\n. Thanks, looks good! Just a small request to make it perfect :+1: \n. Thanks!\nLGTM.\n. When you say you installed form the github repo, did you do so using pip (pip install .) or setup.py (python setup.py install)? Or something else?\nThanks for helping us look into this!\n. I tried reproducing with the following Dockerfile but didn't encounter the import error. Could it be that you cloned a bugged version of the pip repo? What commit is the HEAD at?\nDockerfile\nFROM ubuntu:14.04\nWORKDIR /var/opt\nRUN apt-get install -y python-setuptools git\nRUN git clone https://github.com/pypa/pip\nRUN cd pip && python setup.py install\nRUN pip install docker-py\nENTRYPOINT python -c 'import docker; print(\"docker-py operational.\")'\n. I'm closing this now, but feel free to reopen if there are any new developments.\n. @dothebart What do you mean by \"target host\"?\n. The CLI behaves properly, so this is most likely a docker-py bug.\n. #796 - I still need to review it but I think it's looking well on its way.\n. #796 has been merged and is in 1.6.0 - closing this as a result.\n. Can you try and repost your images? I'm not able to see them at the moment, so I'm not sure what's wrong.\n. I mean your screenshots in the first post.\n. Thanks! They didn't show up for me before.\n. Your cfs variable is incorrect, you have the origin and destination reversed.\nThe syntax is <path_on_host_machine>:<path_inside_container>:<mode>.\nIn your case, you want to change it to cfs = ['/data/wwwroot:%s:rw' % volume]\n. Glad I could help!\nIssue has been resolved, so I'll close this ticket.\n. @dnephin Updated - should look better now.\n. Hm, the separation between create and start has always been there, and it certainly is the same in swarm as well.\nThis looks like a swarm issue to me. @vieux @aluzzardi ?\n. Yup, if that's the case, you should create an issue on docker/docker :)\n. The integration tests on Jenkins use Docker 1.9.0 as well (see the integration-dind task in Makefile). I also get successes testing outside of dind. I'll take a closer look at the test.\n. Mh, the only reason we go through that validation is to be able to extract the tag name from the repo name. It seems things have diverged substantially in the docker/docker codebase since the last time we touched this, notably with https://github.com/docker/docker/pull/8456\n. I'm working on a fix for this, but just to amend my previous comment: we also use the result of resolve_repository_name to figure out what auth config to send to the daemon.\nCurrently comparing to how the CLI does this nowadays to figure out the best way to proceed.\n. Updated with a couple tests for the @digest notation.\n. LGTM\n. Thanks! What purpose would that serve, specifically?\n. Seems fine. LGTM!\n. https://github.com/docker/docker/pull/17127\nLGTM!\n. LGTM, thanks!\n. See also: #77 #330 \n. That would be a swarm issue. Nothing we can do about it at the client level.\n. LGTM, although we might want to remove Mazz while we're at it :3\n. Thanks, LGTM :+1: \n. We're using wheel now since 1.7.0-rc1 and will be for future releases. Marking this as resolved. \n. docker-compose would just have to not use the default, so I don't think it's too much of an issue in that respect (given we handle the version bump properly etc).\nFixed default is just much easier to debug than 'auto' in my opinion though.\n. I'll try to check against the engine code base, I was in that part not too long ago for #861 \n. Ok, I checked against the engine code (which I agree is a mess) and this looks to imitate the behavior properly.\nLGTM.\n. docker-py doesn't control the output of the API. Please report this bug on the docker/docker issue tracker.\n. > Bad Request (\"b'client is newer than server (client API version: 1.21, server API version: 1.20)'\")\nTry initializing your client with c = docker.Client(version='1.20') - you're probably seeing a difference because your docker-py version is not the same in python2 and python3.\n. Looking at the engine code,\n- LxcConf is of type LxcConfig\n- LxcConfig.values  is a slice of KeyValuePair\n- KeyValuePair.Value is always a string\nAs a result, it seems to me that converting values to a string pre-emptively is always correct. Do you experience an issue caused by this specifically? \n. No worries, happy to help. :+1: \n. It's not imperative, but I don't see any downside to having the option be available!\n. Thanks, LGTM!\n. Thanks for your contribution! \nI made a few comments, and the branch also needs to be rebased.\n. LGTM.\n. Closed via #899 - Thanks!\n. Maybe change the test instead so that it tests a few specific values in the result - I would check for the presence (and validity) of Name, Id and Driver in particular. That would prevent the current deadlock as well.\n. Thanks - LGTM!\n. It's handled client-side. See also #870 #330 \n. It's fairly easy to implement the behavior using docker-py:\nclient.start(container_id)\nclient.wait(container_id)\nclient.remove_container(container_id)\n. You want to use cli.create_host_config(links=...) for the array to be converted into the format expected by the docker API, and pass the result of that command into the host_config parameter. Let me know if that helps.\n. Cool!\n. Available in 1.6.0\nhttps://github.com/docker/docker-py/pull/846\nhttps://github.com/docker/docker-py/blob/master/docs/change_log.md#features\n. LGTM\n. The documentation is explicit about needing lists here, so I don't consider that an issue. Conversion from any of these types to a list is trivial as well.\n. LGTM once tests pass :+1: \n. LGTM.\n. It's likely that docker-py will still work with python 3.2 for the foreseeable future, but we won't enforce it or test for it anymore.\n. What are the contents of your compose file? Specifically for the adminer service?\n. I'm not able to reproduce the issue:\na\u3084\u3088\u3044$ docker-compose build\nBuilding adminer\nStep 1 : FROM clue/adminer\n ---> ea11c9f031e5\nSuccessfully built ea11c9f031e5\nDo you still encounter the same issue with docker-compose 1.6.0?\n. Should be fixed in recent versions.. Hmm, I've tried to reproduce on Windows with the same user name, but this still works fine for me:\nPS C:\\Users\\\u0160arlat\u00e1n\\Documents\\feed\\docker2> docker-compose version\ndocker-compose version 1.23.1, build b02f130\ndocker-py version: 3.5.1\nCPython version: 3.7.1\nOpenSSL version: OpenSSL 1.1.0i  14 Aug 2018\nPS C:\\Users\\\u0160arlat\u00e1n\\Documents\\feed\\docker2> docker-compose build\nBuilding adminer\nStep 1/1 : FROM docker4w/nsenter-dockerd\n ---> cae870735e91\nSuccessfully built cae870735e91\nSuccessfully tagged docker2_adminer:latest\nWith Compose file:\nyaml\nversion: '2'\nservices:\n  adminer:\n    build: images/adminer\n    ports:\n      - 8012:80\n    restart: always\nI have a few questions if we want to figure this out:\n\nPlease post the full output of the docker-compose version command\nWhat terminal program are you using? Is it git-bash, Cygwin, something else?\nDoes it change anything if you run Compose from Powershell instead?\nWhat version of Windows are you running on?\nHow did you install Docker? (docker-machine or Toolbox, Docker Desktop for Windows, other?)\nHow did you install docker-compose?\nIf installed through pip, what version of Python are you running on? \nIf installed through pip, does the binary package exhibit the same issue?. > Last one I tried this is Windows 10 64bit, version 1803, build 17134.345, running in Virtualbox VM. I know docker won't work there, this is only to confirm this issue has been fixed.\n\nOh, if Docker isn't running, then the error you're seeing is normal and expected (it's not a great error message but we're getting that fixed soon)\nAll in all, I'm pretty confident it's been fixed, but please confirm on a machine running Docker so I can close this!. CI behaves properly and the tests run locally for the team as well, so I have no idea where this could be coming from. The stacktrace provided doesn't reference any of our code either.\n. Feel free to reopen if reproduction info can be provided.\n. Hard-coding the /tmp value is 100% the wrong approach. \n. LGTM. :+1: \n. Works for me?\n``` python\nIn [1]: import docker\nIn [2]: c= docker.Client()\nIn [3]: c.images(name='registry.com:2222/hello')\nOut[3]: \n[{u'Created': 1452128968,\n  u'Id': u'sha256:9c4901f3703a178bee19d02a0dd22794a93c6658ccbc53af44bada7e3703edc6',\n  u'Labels': None,\n  u'ParentId': u'',\n  u'RepoDigests': None,\n  u'RepoTags': [u'registry.com:2222/hello:latest'],\n  u'Size': 84894442,\n  u'VirtualSize': 84894442}]\nIn [4]: docker.version\nOut[4]: '1.6.0'\n. As it turns out, this is a Docker engine bug that will be fixed in the upcoming 1.10 release. See https://github.com/docker/docker/issues/18181 ;\nI couldn't reproduce the issue because I'm currently using the 1.10 RC.\n. I believe that is supported. Are you seeing different results?\n. It is handled here supposedly: https://github.com/docker/docker-py/blob/master/docker/utils/utils.py#L335\n. `ports=[8080]` in `create_container` should do what you need, unless I'm misunderstanding.\n. `port`, only lists the ports that are exposed to the host.You can use `inspect_container` instead to ensure the container is properly configured. :+1: \n. python\n\ninfo = client.info()\ndriver_status = dict(info['DriverStatus'])\nprint(driver_status)\n{u'Backing Filesystem': u'extfs',\n u'Dirperm1 Supported': u'true',\n u'Dirs': u'194',\n u'Root Dir': u'/var/lib/docker/aufs'}\n```\n\nLooks fine to me. We could do the dict conversion directly in the info() method, but the advantages seem marginal.\nAs for the \"extraneous characters\" you're seeing, they're present because your OS/storage driver combination prints them out when docker queries the info. It's not docker-py's responsibility to remove them.\n. This issue seems to be Swarm specific, and will be closed as a result - feel free to reopen if the situation evolves.\n. Did you create an issue on the Swarm tracker?\n. Yeah, the error message is obsolete - dns and volumes_from should be provided in the host config - thanks for bringing it up.\n. Just to clarify, the reason you're seeing this error is because you're trying to pass dns or volumes_from as a parameter to create_container directly. While that used to be valid for older versions of Docker, it has since been moved to the host_config. \nAlso note that you don't need to build the config dictionary by hand - just use Client.create_host_config as described here\n. The reason we bumped the version of the requests package way back then (release 1.1.0) was because a lot of people were encountering issues with SSL connections that were caused by the outdated version of requests.\nIf your package doesn't require connecting to a daemon over HTTPS, using an outdated version of requests might work, but there won't be any guarantee or support for it.\n. Should fix #816 \n. Cool, thank you for testing! We'll try to have it in a 1.7.1 soonish I think.\n. Thanks!\n. Thanks!\n. LGTM, thanks! :)\n. Looks like KwargsFromEnvTest.test_kwargs_from_env_tls_verify_false_no_cert is failing on Travis!\n. Cool, thanks!\n. Yeah, this is addressed by #988 \n. Thanks for the report, I'll look into it!\n. Closing via #1008 \n. Hmm, I tested against 1.9.0 and the same thing happens. What's your version of requests?\n. Is this what you're looking for?\n``` python\nimport docker\nfrom requests.exceptions import ReadTimeout\nc = docker.Client()\nctnr = c.create_container('busybox', 'sleep 45')\nc.start(ctnr)\ntry:\n  c.wait(ctnr, timeout=5)\nexcept ReadTimeout as e:\n  print('too slow!')\n  c.kill(ctnr)\n``\n. Yeah,Client.startis non-blocking.Client.stopwill typically block until the container is stopped, or the command has timed out (default 10 seconds).\n. Yeah,docker runis a combination ofcreate_container+start+wait(unless you use the-d` parameter).\nGlad I could help - feel free to reach out if you're still having trouble.\n. Closing via #936 \n. A few small comments, should be good after that.\n. LGTM, thank you! :+1: \n. Closing via #1228 . LGTM - not sure why the bot is freaking out.\n. LGTM, thanks!\n. Thanks!\n. I think you're missing ** in front of your dictionary for the create_container call.\n. No. You're calling create_container with a single dictionary argument. I assume you want it to be a kwargs dictionary, in which case you're missing the ** in front of it.\npython\nbackup_container = cli.create_container(**{\n   'image': 'xxxx',\n   'entrypoint': 'xxxx',\n   'command': 'xxxx',\n   'volumes': '/backups',\n   'host_config': conf\n})\n. Thank you for your contribution!\nHowever, we're not looking to switch to RST at the moment. The resulting lack of format on PyPi is unfortunate, but ultimately trivial.\n. #950 \n. LGTM!\n. This is the intended API's behavior. If you only intend to pull latest, you must specify it as a tag.\n. thanks!\n. Please read our guidelines about reporting issues on the tracker.\n. > A question: are the security concerns of doing assert_hostname=False true? [...]\n\nI think we would still have to specifically say \"if you are using Machine, this is safe\", because in the context of using a normal CA cert it would not be safe.\n\nThat's fair, it should probably be fine to do so for a Machine environment. I believe the warning was written in a broader context, which is also why the recommendation was kept fairly vague.\n. https://github.com/docker/docker-py/pull/947#issuecomment-188428967\n. :+1: once green\n. Does your registry have an index extension? As far as I recall, searching on private registries isn't available by default, which is why you're seeing this issue.\n. Ah, my bad. \nStill, in that case, it seems docker-py is behaving as expected.\n. Can't reproduce either - probably a bug in the docker engine dev version you're using.\n. Fixed, thanks for the report!\n. What happens if you unset DOCKER_TLS_VERIFY?\n. Could you run the following code snippet to help pinpoint the issue, and post the output here?\n``` python\nimport docker\nimport ssl\nprotocols = [\n    'PROTOCOL_SSLv23', 'PROTOCOL_TLSv1', 'PROTOCOL_TLSv1_1', 'PROTOCOL_TLSv1_2'\n]\nkwargs = docker.utils.kwargs_from_env(assert_hostname=False)\nclient = docker.Client(**kwargs)\ntls_config = kwargs['tls']\nfor proto in protocols:\n    if not hasattr(ssl, proto):\n        print('Protocol {0} not found in SSL protocol list'.format(proto))\n    tls_config.ssl_version = getattr(ssl, proto)\n    tls_config.configure_client(client)\n    try:\n        client.version()\n        print('Successful connection with protocol {0}'.format(proto))\n    except Exception as e:\n        print('Connection error with protocol {0}: \"{1}\"'.format(proto, e))\n``\n. Hmm, that's interesting. Python doc says [PROTOCOL_SSLv23`](https://docs.python.org/dev/library/ssl.html#ssl.PROTOCOL_SSLv23) selects the highest protocol supported by both server and client, but it seems to be causing issues for you.\nConnection error with protocol PROTOCOL_SSLv23: \"[Errno 1] _ssl.c:507: error:1407742E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert protocol version\"\nPROTOCOL_TLSv1 seems to work fine however.\n. A workaround for now would be to call kwargs_from_env with ssl_version=ssl.PROTOCOL_TLSv1 set.\n. That'd be great. If you can give #971 a spin as well and tell me how it behaves for you , that's even better. :+1:  thanks!\n. The issue can be worked around by passing an explicit ssl_version, so I don't think it calls for a patch version, especially considering 1.8.0 isn't too far out.\n. Sorry, I forgot that section was so poorly documented. This should get you on the right track:\n``` python\nimport docker\nfrom docker.utils import LogConfig\nc = docker.Client()\nhost_conf = c.create_host_config(log_config=LogConfig(type=LogConfig.types.FLUENTD))\nctnr = c.create_container('busybox', 'top', host_config=host_conf)\n```\n. Thanks! Can you rebase against master?\n. Yes, unfortunately.\nLGTM!\n. I don't think it should be a docker-py feature, to be honest. If you want to write a container ID into a file, it's currently very simple to do so from Python.. @AndreLouisCaron That's not what ContainerIDFile does - the file is created on the host, not inside the container.. @AndreLouisCaron Yeah - that issue is outside the scope of the Python SDK.. LGTM, thanks!\n. LGTM, thanks!\n. @TomasTomecek I don't plan to - see rationale here\n. Thank you for the detailed report. \nChange LGTM.\n. It'll be available in 1.8.0 : see #936 \n. You need to use custom_context. See #209 and the docs\n. Closing via #994 \n. Reported issue in https://github.com/docker/docker/issues/21069\nWe could do client-side validation to mitigate the issue, but the server closing the connection like that is definitely a bug.\n@aldenpeterson-wf Are you familiar with python generators? Using Client.build's output really shouldn't be a challenge. For your purposes, those 2 lines should suffice:\npython\nfor line in cli.build(...):\n  print line\nThere's also an example of usage in our API docs\n. Fixed in docker engine 1.11.0rc1 - closing this as a result.\n. I think both options should exist. For public repositories, just passing the git URL is definitely faster.\nFeel free to submit a patch to add that feature.\n. Ah, I just tested it and it seems you are correct - public repos seem to fail too here. However, the engine definitely recognizes the syntax since it's trying to process it (Cloning into '/var/lib/docker/tmp/docker-build-git307830553'...) - so I would argue that this is actually a bug at the engine level. I'm certain it used to work before as well.\nWe should open an issue on the docker/docker issue tracker to report it and see if this can be fixed.\nAs far as 2 is concerned, the way I'd implement it is have a function in utils to clone a git repository (maybe this should be in its own package - it's actually nontrivial) and tar it into a file object (pointing to a temp file on the disk ideally). Then this can be passed to the Client.build method through the fileobj param, with custom_context set to True. If those utils exist compose can easily leverage them afterwards.\n. My daemon is on the same machine as my client, so it should have access to my SSH config just fine, but I'm still experiencing the issue.\n. Ah, good point. After copying my .ssh folder into /root, I am able to build from a git-ssh URL.\n```\nIn [1]: import docker\nIn [2]: c = docker.Client()\nIn [3]: strm = c.build('git@github.com:docker/docker-py.git', stream=True)\nIn [4]: for line in strm:\n   ...:     print line\n   ...:   \n{\"stream\":\"Step 1 : FROM python:2.7\\n\"}\n{\"stream\":\" ---\\u003e 31093b2dabe2\\n\"}\n[...]\n{\"stream\":\"Successfully built 16580dbca2de\\n\"}\n```\n. Regardless, I still think the second part of https://github.com/docker/docker-py/issues/980#issuecomment-193987937 is the best approach.\n. > Just to be clear, you're saying clients should have the choice to either:\n\n\nUse docker.util.clone_repo (or something), and then pass with fileobj and custom_context\nOR, pass path=git@github... and expect it will be passed to the docker daemon\n\n\nYep! Thanks for offering to take it on!\n. This will continue to be a feature-request - docker-py passes on any remote URL it gets to the engine which knows how to process them. In some cases, missing configuration on the engine's host means this process will fail, but this is not this library's responsibility.\nI still welcome pull requests that would implement this additional feature, or people can use any of the workaround suggested in this thread.\n. Closing, see https://github.com/docker/docker-py/pull/1508#issuecomment-351596768 for rationale.. Try updating the engine to 1.10.3?\nhttps://github.com/docker/docker/issues/20638\n. Ah, my bad, 1.10.3 isn't out yet. However there's a release candidate if you want to test it. https://github.com/docker/docker/releases/tag/v1.10.3-rc1\n. Our networking documentation is seriously lacking, but the feature is there: https://github.com/docker/docker-py/blob/master/docker/api/network.py#L50\nExample usage from our test suite: https://github.com/docker/docker-py/blob/master/tests/integration/network_test.py#L131-L143\n. docker-py 1.7.x uses API version 1.21 by default. You can override that in the client constructor like this:\npython\nimport docker\nc = docker.Client(version='1.22')\nThe upcoming 1.8.0 will use 1.22 by default.\n. use Client.put_archive instead :)\n. Thank you for the report. Have you tried it with the fix 9ba0ddf267f853afe37441da7798987e199817dd ? We released 1.7.2 to try and address those TLS issues, but it's possible we might have missed a case.\n. Do you folks mind trying #988 (commit SHA: ad480ff0b16907c8cfea5aa4cc6fe9b8bc5d2494) and see if it solves the problems you're experiencing?\n. It seems to be working fine for me.\n``` python\nIn [1]: import docker\nIn [2]: c = docker.Client()\nIn [3]: c.create_host_config()\nOut[3]: {'NetworkMode': 'default'}\nIn [4]: c.create_container('busybox', 'top', host_config=_3)\nOut[4]: \n{u'Id': u'd8a885ec91c58e36c441ed4825528f7e047dfcde5b6678b867afe798a3b32c7d',\n u'Warnings': None}\nIn [5]: c.start(_4)\nIn [7]: c.containers()\nOut[7]: \n[{u'Command': u'top',\n  u'Created': 1457728245,\n  u'HostConfig': {u'NetworkMode': u'default'},\n  u'Id': u'd8a885ec91c58e36c441ed4825528f7e047dfcde5b6678b867afe798a3b32c7d',\n  u'Image': u'busybox',\n  u'ImageID': u'sha256:3240943c9ea3f72db51bea0a2428e83f3c5fa1312e19af017d026f9bcf70f84b',\n  u'Labels': {},\n  u'Names': [u'/high_galileo'],\n  u'NetworkSettings': {u'Networks': {u'bridge': {u'Aliases': None,\n     u'EndpointID': u'b7df815663775e2f12f2651a6901dfe29d649cac6bb39fc12ed8ed0f4d0f8597',\n     u'Gateway': u'172.17.0.1',\n     u'GlobalIPv6Address': u'',\n     u'GlobalIPv6PrefixLen': 0,\n     u'IPAMConfig': None,\n     u'IPAddress': u'172.17.0.2',\n     u'IPPrefixLen': 16,\n     u'IPv6Gateway': u'',\n     u'Links': None,\n     u'MacAddress': u'02:42:ac:11:00:02',\n     u'NetworkID': u''}}},\n  u'Ports': [],\n  u'Status': u'Up About a minute'}]\n```\nAlso, note that your host is on API version 1.18, so this shouldn't affect you either way. Maybe ansible is passing the default value to network_mode even though your daemon doesn't support it?\n. This also appears to be consistent with the CLI's behavior: https://github.com/docker/docker/blob/131e2bf12b2e1b3ee31b628a501f96bbb901f479/runconfig/hostconfig.go#L28-L35\n. That wouldn't work with your 1.6.2 daemon though (you'd get an error like this one APIError: 400 Client Error: Bad Request \"client is newer than server (client API version: 1.23, server API version: 1.22)\"\nAre you sure your setup is as you describe it?\n. Ah, that makes more sense. Your client is using the correct API version, but ansible makes unversioned calls to docker.utils.create_host_config, which it definitely shouldn't be doing. default is causing issues for your setup because the docker version you're using doesn't support that value, which was introduced later.\nThe reason we're not using bridge as the default value is that the default for Swarm engines is different. The bug should be reported to ansible - their code should be using Client.create_host_config instead of docker.utils.create_host_config.\n. default is just missing from the documentation. The daemon interprets it as \"default for the current platform\", which is bridge for Docker Engine, and overlay for Swarm.\nIn 1.20, specifying a value for network_mode became mandatory (the server will error out otherwise), which is why we have this special case where we insert a value if it's missing. API versions < 1.20 don't have that restriction, so we leave it to the user to do what they think is best.\nSee #698 and docker/docker#14530 for more info.\nHope that clears it up!\n. @bfirsh Yes, that's exactly what the default value means \"bridge for engine, overlay for swarm\". See discussion in #986 .\nI'll close this as a result.\n. Thanks! This won't be needed anymore with docker engine 1.11, so I'm going to close this.\n. Sure, feel free to make a PR for it! \n. Has the fix made it to a release yet? There isn't anything we can do on our end about server-side issues.\n. > That makes me less convinced it is a bug with docker itself but more how docker-py is wrapping things.\nNo, that just means the bug is only exhibited when we're using the same session / connection to issue both commands. As far as I can tell, it's still a server bug. \nDo you have a link to the issue on docker/docker?\n. It's a Go issue that has been fixed in 1.6.\nDocker 1.10.3 was compiled using Go 1.5.3\nThis PR upgrading Go to 1.6 has been merged in master a couple weeks ago, meaning it will be part of the next release cycle (Docker 1.11 onwards).\nIn the few cases where this is an issue, people are free to initiate a new connection (docker.Client) when necessary. Any solution we could implement client-side would amount to the same thing anyway.\nThe sane default is to have a persistent connection, for obvious reasons.\n. Minor docs comment, apart from that, LGTM.\n. The integration-dind-ssl target has a race condition in. You can add sleep 1 as the second line and it should work fine.\nBut yeah, it would be nice to have a good fix for it.\n. LGTM :+1: \n. I'm pretty sure you need to cli.start your container.\n. Is your container stopped when you look at port bindings? A stopped container releases the ports it binds while running. I assume that's what you're seeing. Use the top command instead of ls to have a long-running container and you should see your code working as expected.\n. Thanks, this is looking pretty good! Would you mind adding a test for it?\n. Thanks! LGTM.\n. Thank you!\n. LGTM, thank you!\n. Support for custom log drivers is already implemented.\nHere's what you want to do:\n``` python\nfrom docker import Client\nfrom docker.utils import LogConfig\nc = Client()\nlog_cfg = LogConfig(LogConfig.GELF, { 'gelf-address': 'udp://:12201' })\nhost_cfg = c.create_host_config(log_config=log_cfg)\nc.create_container('busybox', 'echo Hello Graylog', host_config=host_cfg)\n```\nRelated issues:\n718 #724\n. LGTM!\n. Thanks! Do you mind rebasing this against the current master? Changes LGTM.\n. Thanks!\nIf we expect this list will continue to grow, maybe a better solution would be to link to the source instead of trying to play catch up. Or at least add a disclaimer like \"A comprehensive list is available [here](link-docker-docs)\"\n. Ah, yeah, fair point. I don't think the docker docs have a dynamic link to the latest API docs page. Maybe we can just link to https://docs.docker.com/engine/reference/api/docker_remote_api/ for now.\n. Agreed. Do you mind making the change?\n. Awesome, thank you!\n. Can this be fixed by updating your setuptools package? I just tested it with Python 2.7.7 and simply doing pip install -U setuptools seems to solve the problem.\n. There is no \"fix\" planned - this is not a bug. Please update your version of setuptools to support the environment marker syntax.\n. Thanks!\n. Quite right :+1: . Yes, put_archive is the way to do this. As @TomasTomecek said, that's how the remote (docker engine) API works - it might seem tedious when working with a single file, but it's obviously a good solution when you want to copy a wide range of things from files big and small to entire directory structures.\n. Thank you for your help! This has been fixed by #1071, and I will close this issue as a result.\n. Thanks! I didn't know this had been backported already. Obviously it is much better than having to maintain it ourselves.\nJust one small issue with the requirements, otherwise this is good.\n. Yes, I think we want to keep the focus of docker-py on programmatic\ninteraction with the API. A project that leverages it into a human-friendly\nREPL is cool and would have its uses, but should remain separate from the\nlibrary.\nThanks for opening the discussion!\nOn Tue, Apr 26, 2016, 11:01 AM Aanand Prasad notifications@github.com\nwrote:\n\nI'm not sure this makes sense as a docker-py addition. That's mostly\nbecause I'm not clear on the use cases - but even if they are valid, it\nseems like having it as a separate tool would be fine.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-py/issues/1031#issuecomment-214830329\n. 1.22 is the API version for engine 1.10. We try to keep the default version close to that of the latest engine release (although in practice we've been one step behind lately).\n\nAs Daniel said, this is easy to configure when using docker-py by itself. I believe ansible gives you an option to do so as well, but my knowledge of the tool is limited.\n. Does #1039 solve your problem? It swaps the dependency to ipaddress, which seems to be the one you are using.\nI absolutely want to avoid requiring the user to perform manual steps before using the library if at all possible, so I don't think this PR is the right approach.\n. http://docker-py.readthedocs.org/en/stable/api/#update_container ?\n. Sorry, other things happened to take priority before I could take a look at this.\nIn my opinion, we don't want to change resolve_authconfig across the board, because it affects other parts of the code and might even break user programs if they were using that function.\nI would suggest modifying only Client.push instead.\n. Closing via #1068 \n. Closing via #1068 \n. We should make this easier. It doesn't look like create_network_config (or create_endpoint_config) currently expose the functionality.\n. We changed it a couple versions ago here: https://github.com/docker/docker-py/pull/874\nIt's odd that this would cause an issue, but I'll look into it. Thanks for the report.\n. @linlinlinlin docker-py doesn't modify your configuration file \nIf you call login(), it will simply keep the information for the duration of the session (as part of the Client instance) in memory, but we don't write in the JSON file.\n. I've been trying to reproduce the error but I haven't been able to do so. The current code seems to work just fine ( docker-py 1.8.0, docker 1.11.0).\nAre you still seeing the issue?\n``` python\nIn [1]: import docker\nIn [2]: c = docker.Client()\nIn [3]: c.login('joffrey', reauth=True)\nOut[3]: {u'Status': u'Login Succeeded'}\n``\n. Ok, I just tried with 1.10.2, and I can reproduce the issue.\nSince this is fixed in 1.11 and can be worked around for 1.10 (by specifying the registry manually), I don't think it is necessary to do anything.\n. Then again, #1050 is a more future-proof solution and should fix your issue.\n. What version of docker are you using? Do you see the same issue if you dowatch -n 2 docker psin your shell?\n. The way I see it, making therepo_name:tag_name` separation is trivial, but I'm not sure how well that matches with user expectations and consistency.\nThe remote API could gain to be more explicit about the issue it encountered as well.\n. Thanks, LGTM!\n. Ah, sorry I lost track of this, but I actually included that fix in #1178 . Thanks again for your help tracking this down!\n. It's been considered. A more recent branch that implements a similar behavior is efficiency ( https://github.com/docker/docker-py/commits/efficiency ), but it's not yet at a point where I'm satisfied with it in terms of code quality and robustness, and it's stayed fairly low priority compared to other tasks.\nThe reality is, not everyone needs or even wants an OO approach, so if any efforts are made in that direction, they should keep the experience intact for people using the library as it is currently.\n. I think this covers most use-cases:\n``` python\nimport docker\nclass DryRunClient(docker.Client):\n    def request(self, method, url, params=None, data=None, headers=None,\n                args, *kwargs):\n        return {\n            'method': method,\n            'url': url,\n            'params': params,\n            'data': data,\n            'headers': headers\n        }\ndef _result(self, response, *args, **kwargs):\n    return response\n\ndef _raise_for_status(self, *args, **kwargs):\n    pass\n\ndef _stream_helper(self, response, *args, **kwargs):\n    return response\n\ndef _get_result(self, container, stream, response):\n    return response\n\ndef _get_result_tty(self, stream, response, is_tty):\n    return response\n\n```\nSome methods like stop, pause or rename will not return any value regardless (because the API provides an empty response when those operations are successful) so if you're interested in those as well, you would need to tweak them manually.\n. dns_search is part of HostConfig\n. I need more information. Please follow the instructions for reporting issues\n. I believe that happens because requests in Ubuntu can't vendor in urllib3 (see here), so we had to construct a fallback that uses the version installed on the system instead.\nBecause the version of urllib3 that's available on Trusty is so old, docker-py's attempt at monkey-patching it is unfortunately failing. Installing a more recent version of urllib3 using pip may solve the issue. (pip install -U urllib3==1.14)\nI'll try and figure out if there's a better solution for us.\n. @mixmatch What about docker-py==1.8.1 + urllib3==1.14 ?\n. Also, what OS/version are you using?\n. @oryband Did you try pip-installing urllib3==1.14?\n. Closing via #1563 . Thanks!\n. Thanks! See my comment here: https://github.com/docker/docker-py/pull/505#issuecomment-77285993\nThe best experience using docker-py for users will always be to use the version it's tested and verified against. auto is provided as an option, but should not be the default.\n. I think there's an issue with requests concatenating the entire response when the connection closes before we start reading it. Do you think that could be what you're seeing? \nIf the objects are separated by CRLFs, we can easily parse that somewhere.\n. @pmarques A fix has been merged in master and will be in the upcoming 1.10. Feel free to try it out.\nAlso, since this is now in master, I'll go ahead and close this issue.\n. I don't want to commit to a date yet at that point, but it will be soon.\n. 1.10.0 is now available on pypi.\n. We implemented another fix in #1389 which is available in docker==2.0.2. It should solve this issue definitively.. @erhudy - Yes - unfortunately that is the way the response is streamed by then Engine API. The fix we implemented in 2.0.2 uses an improved JSON decoder that is able to parse those artifacts. You need to use decode=True when calling APIClient.build to take advantage of it.. docker-py doesn't update the config,json file. \nYou're right to point that out - we should update the documentation to reflect that more accurately.\n. The login token is kept in memory by the Client object, but it won't be available to the outside (including programs like docker-compose).\nIf you need to persist your credentials, you can either\n- Use the docker login command.\n- Write into the config.json file manually using python or some other script. docker-py, unfortunately, isn't designed for this task.\n. Thanks!\n. LGTM, thanks!\n. It is supported.\npython\nimport docker\nc = docker.Client(...)\nhost_cfg = c.create_host_config(cgroup_parent='test', ...)\nc.create_container(image, cmd, host_config=host_cfg, ...)\n. Changes LGTM. Thanks!\n. @jgiannuzzi : You should use a git sha to make sure the branch remains functional and can be tested. See for example https://github.com/docker/compose/commit/85c7d3e5ce821c7e8d6a7c85fc0b786f3a60ec93\n. HostConfig is the name of that object in the Docker API.\nAs I understand it, the rationale is this specific part of the configuration specifies how the container will interact with the host (run limits, volumes binds, port mappings...)\nHTH.\n. You're going to have to increase the HTTP timeout parameter. c = docker.Client(..., timeout=300)\n. I thought I had commented on there, sorry. Changes LGTM.\n. Use the DOCKER_CONFIG environment variable.\n. You probably need to remove the trailing slash in your base_url. The request URL you're showing is invalid because it has two slashes at the beginning of the path.\n. Ah, it seems like you were using an API version that's greater than the one your Docker engine supports. You can use the version parameter in the Client constructor to specify a lower version, or use the AutoVersionClient instead.\n. Agreed.\nFor now, the current workaround would be\n``` python\nfrom docker import Client\nfrom docker.utils import kwargs_from_env\nc = Client(version='auto', **kwargs_from_env())\n``\n. Fixed by #1928 .filters={'status': 'dead'}` should work. https://docs.docker.com/engine/reference/commandline/ps/#filtering\n. cc @aanand @dnephin \nhttps://github.com/docker/compose/issues/3170\n. Thank you!\nLooks like a good solution. Jenkins' failure is just a fluke. I'll go ahead and merge this.\n. Thanks!\nHm, that's interesting. I don't know if we can or should count on the response being a JSON object in that situation. On the other hand, if it's truly an error case like we assume, it doesn't matter whether we pass the json parameter or not.\nAll in all, seems fairly safe. LGTM!\n. cc @aanand because networking stuff :cat: \n. Updated the docs to mention the 1 network limit.\nI agree re: other methods, although those are already documented in api.md, so they're less of a priority. \nMoving forward, maybe separating the API docs by theme makes more sense given how the API has grown. Food for thought.\n. LGTM - thank you!\n. Sorry, no ETA for 1.10.0 yet.\n1127 and #1147 have been merged to master, so I'm closing this.\n. 1.10.0 now available on pypi.\n. Oops, that documentation is indeed incorrect. Here's how you need to proceed:\npython\nipam_pool = docker.utils.create_ipam_pool(subnet='xxx')\nipam_cfg = docker.utils.create_ipam_config(pool_configs=[ipam_pool])\nI am reworking the networking docs as part of #1083 , so I'll fix that mistake there as well.\n. Hi!\nThis actually works as intended - we're doing exactly the same thing the CLI does. The untagged image you're seeing is an intermediary image that hasn't been cleaned up because you didn't set rm=True when calling Client.build, which is False by default for backwards compatibility reasons.\n. Ah, right, sorry.\nFirst off, just to clarify for myself and others, the leftover image issue also happens while using the Docker CLI, as far as I can tell:\n$ docker build --build-arg foo=hello --build-arg bar=world .\nSending build context to Docker daemon 2.048 kB\nStep 1 : FROM alpine\n ---> 13e1761bf172\nStep 2 : MAINTAINER Joffrey F <joffrey@docker.com>\n ---> Running in 9c605066ca55\n ---> cb7211f9bf95\nRemoving intermediate container 9c605066ca55\nStep 3 : ARG foo\n ---> Running in 8f99338e84db\n ---> 2210f554aa58\nRemoving intermediate container 8f99338e84db\nStep 4 : RUN echo ${foo}\n ---> Running in a1a5c60a7331\nhello\n ---> 455c57454e96\nRemoving intermediate container a1a5c60a7331\nOne or more build-args [bar] were not consumed, failing build.\n$ docker images\nREPOSITORY                  TAG                 IMAGE ID            CREATED             SIZE\n<none>                      <none>              455c57454e96        5 seconds ago       4.797 MB\nAs for the matter of detecting build errors - the library will not do it for you. You would have to parse the stream and detect the line(s) that indicate failure (typically a JSON object containing an error key)\n. By design, docker-py doesn't modify the config file. \n. Thanks @thaJeztah !\n. Hi!\n1. Pretty sure you mean 1.20, not 1.2 - they're not the same thing.\n2. The version of Docker Engine you are currently using is too old - the put_archive method wasn't implemented at the time. You need to update the engine to a more recent version.\n. A better approach would be to call client.exec_inspect() on ex['Id'] and retrieve the status from the resulting dict. :)\npython\nex = cli.exec_create(container_id, cmd=\"bash -c 'cd /bin; ls'\")\nresp = cli.exec_start(ex)\ncli.exec_inspect(ex)['ExitCode']  # == 0\nAs for changing the workdir of an existing container, I don't think that's possible, at least not in a way that's exposed by the Docker API.\n. If the core project has explicitly decided not to support it, I don't think it would be a good idea for us to do so. As Tomas pointed out, you can use the fileobj tactic as a workaround (at your own risk), but for equivalent parameters, behavior should stay consistent with the engine.\n. These were added \"recently\", so we don't have support for them yet\n@linlinlinlin FYI, driver_opts is different. It corresponds to\n\nDriverOpts - A mapping of driver options and values. These options are passed directly to the driver and are driver specific.\n\nin the docs.\n@TomasTomecek those are logging drivers, not volume drivers :pensive: \n. Duplicate of #1085 \n. @dnephin Should look good now.\n. This would be the equivalent code using docker-py:\npython\nimport docker\nc = docker.Client()\nctnr = c.create_container(\n    'busybox:latest',\n    name='tanmay',\n    volumes=['/mnt/sda1/var/lib/docker/volumes/xldVolume/_data'], \n    detach=True, \n    host_config=c.create_host_config(\n        binds=['xldVolume:/mnt/sda1/var/lib/docker/volumes/xldVolume/_data'],\n        publish_all_ports=True\n    )\n)\nc.start(ctnr)\nEDIT: moved publish_all_ports to host_config\n. Thank you for your contribution!\nI have left a couple comments on the code, please take the time to address those when you can.\nAlso, @GordonTheTurtle is a bot, so he won't be able to reply to you :)\n. Great, thanks! One last thing before going forward is to add the parameter to the docs, specifically in docs/hostconfig.md - and also while you are at it, please squash your commits! :)\n. Thanks, LGTM!\n. Merged as part of #1147 . Thanks!\n. Thanks! Can we mention the parameter in the docs (docs/api.md) ?\n. Thank you, LGTM!\n. I believe this was fixed some time ago.. @lelit You're right that this is still missing, but I would consider it a separate issue from the ** pattern support. Do you mind opening a new ticket with those details so we can track it and work on a resolution?. @g0t4 does it work on Windows if you use backslashes instead in your .dockerignore?. Hi @anselal ,\nThis doesn't look like a docker-py issue. \n. Thanks!\nChanges make sense - would it be possible to add a test to validate it and make sure we don't break it in the future?\n. Thanks, tests look good!\nLGTM - cc @aanand @dnephin \n. From the message you posted, it seems your problem lies here:\n\n\"Your Authorization Token has expired. Please run \\'aws ecr get-login\\' to fetch a new one.\"\n\nThis is an AWS thing, unrelated to docker-py.\n. Thanks, LGTM!\n. I suppose we need some client-side code to handle the change of protocol?\nThe Connection Aborted issue only happens when we're executing the tests from inside a container, this is what I get running locally.\n```\n$ py.test tests/integration/exec_test.py \n============================= test session starts ==============================\nplatform linux2 -- Python 2.7.9 -- py-1.4.30 -- pytest-2.7.2\nrootdir: /home/joffrey/work/pydocker, inifile: pytest.ini\nplugins: cov\ncollected 7 items \ntests/integration/exec_test.py FFFF..F\n=================================== FAILURES ===================================\n___ ExecTest.test_exec_command_as_root ___\ntests/integration/exec_test.py:74: in test_exec_command_as_root\n    self.assertEqual(exec_log, b'root\\n')\nE   AssertionError: '' != 'root\\n'\n___ ExecTest.test_exec_command_as_user ___\ntests/integration/exec_test.py:58: in test_exec_command_as_user\n    self.assertEqual(exec_log, b'default\\n')\nE   AssertionError: '' != 'default\\n'\n___ ExecTest.test_exec_command_streaming ___\ntests/integration/exec_test.py:92: in test_exec_command_streaming\n    self.assertEqual(res, b'hello\\nworld\\n')\nE   AssertionError: '' != 'hello\\nworld\\n'\n____ ExecTest.test_exec_command_string ___\ntests/integration/exec_test.py:42: in test_exec_command_string\n    self.assertEqual(exec_log, b'hello world\\n')\nE   AssertionError: '' != 'hello world\\n'\n___ ExecTest.testexecute_command ___\ntests/integration/exec_test.py:26: in test_execute_command\n    self.assertEqual(s, b'hello\\n')\nE   AssertionError: '' != 'hello\\n'\n``\n. Thank you for your help, folks! I've been focused on other tasks lately but I should be able to pick this back up soon.\n. - I have rebased this on top of #1147 so I could use the swarm init methods in the integration tests.\n- Updated methods and added the missingupdate_service- Moved theTaskTemplate,ContainerSpecand similar classes to thedocker.types` package.\n- Started writing docs. API docs still need to be filled out.\nAt that point everything should work properly, but please leave a comment if you're testing this and running into issues. :)\n. Squashed and merged into master. Thanks!\n. Thanks, LGTM.\n. Hey @Knetic , sorry for the lack of feedback - we usually wait for two maintainers to LGTM a PR before it gets merged, but with the summer months it's a bit difficult to get 2 pairs of eyes on everything.\nI've merged it now, since the changes are fairly low-risk and everything checks out. Thank you for your patience!\n. Thank you for your contribution!\nWe went with #1178 instead, so I'll close this now that the feature is in master.\n. This will be in 1.10\n. I believe this is a duplicate of #1059 . Please refer to that issue for progress and resolution.\n. Don't pass any config in Client.start, it is deprecated for good reason.\nhttps://github.com/docker/docker/pull/17799\nhttps://github.com/docker/docker-py/issues/578\n. Do you think this is too early? The npipe code hasn't really been tested extensively yet. I'd wait for 1.10 \n. Cool, LGTM :+1: \n. Your issue is actually much simpler than that. You're using docker-py 1.8.1, which didn't support the ipv4_address parameter in create_endpoint_config. The documentation available here is for docker-py 1.9, which is currently at the \"release candidate\" stage. Sorry about the confusion that may have caused.\nI'd suggest using the release candidate (pip install -U docker-py==1.9.0rc2) for now, or wait a few more days for the 1.9.0 release. Alternatively, if you'd rather keep using 1.8.1 for now, you can construct the endpoint_config dictionary yourself.\npython\nnew_client.start(new_client.create_container(image=\"nginx\",name=\"test\",detach=True,\n             networking_config=new_client.create_networking_config({\n                  'docker_macvlan': {'IPv4Address': '10.10.10.14'}\n             })\n))\n. You probably do need the IPAMConfig part - I typed that last part a bit too quickly, sorry. \nAs for the docs, stable is the documentation for the current stable release (currently 1.8.1), while latest (as you realized) is extracted from the master branch. Supposedly, accessing http://docker-py.readthedocs.io/ directly should redirect you to the stable docs, but I understand it can be confusing when coming in from an external link.\n. Thanks! LGTM.\n. Thanks! I have just one request regarding the addition to the Client.start method. Rest is LGTM.\n. LGTM, thank you!\n. LGTM :+1: \n. #1085 \n. cc @dnephin This should be ready for review. :smile: \n. Closing via #1294 \n. Thank you for the report!\n. The docker CLI just runs a loop removing each volume 1 by 1, see https://github.com/docker/docker/blob/master/api/client/volume/remove.go#L46-L53\nSince it's not an API feature, I think it's reasonable to let the implementer handle that themselves.\n. Closing in favor of #1224 \n. Thanks!\nWe went with a case-by-case approach in #1167 instead, to avoid surprises and make it easier to maintain in the future.\nI'm going to close this PR as a result.\n. Thanks, changes LGTM!\n. Can you provide more info about the code you're running and what system / docker version you're running on?\n. Hey @antoineco , I believe this is a duplicate of #986 . Please refer to the explanation I gave there - it's an Ansible issue. If upgrading your Docker host to a more recent version is not an option, you can work around the issue by specifying network_mode=bridge in your host_config.\nIf I'm not mistaken (not much experience with ansible), that would be net: bridge in your local_action dictionary.\n. It's not part of the API - it's a feature of the docker client. This has been discussed at length before: #77 #330 #870 #890 \n. Thanks for bringing this to our attention!\nI just merged #1191 which should have the same effect.\n. Correct, sorry.\n. Thanks!\n. Hi @ashishjain14 , the commands you are referring to (docker stack ...) are actually handled by the docker CLI, meaning they are a client-side feature. Those commands then perform remote operations like creating services and managing tasks, which are implemented in the PRs @kleptog references.\nRe-implementing those features in python might or might not be outside of the scope of docker-py. As of now, it is not on the immediate roadmap.\n. I'm not ready to commit to a date yet, but it should happen soon. In the meantime, the master branch is fairly solid so feel free to play around with it until then.\n. 1.10.1 is on pypi now!\n. Can you clarify what you are trying to do? Is test.txt a file on your host, or inside the container? How would you accomplish the same thing with the docker CLI?\n. For the indirection (>>)to be understood as part of the command, you need to pass it as an argument to sh -c, like so: sh -c '/bin/echo test1 teip >> test.txt'. This applies to the docker CLI and docker-py.\n``` python\nfrom docker import Client\ncli = Client(base_url='unix://var/run/docker.sock')\nexec_instance=cli.exec_create(container='hello',cmd=\"sh -c '/bin/echo \\'test1 teip\\'>>/test.txt'\",user='root',tty=True)\ncli.exec_start(exec_id, stream=True,detach=False)\n``\n. Happy to help!\n. You can find detailed information on port bindings [in our docs](http://docker-py.readthedocs.io/en/stable/port-bindings/). Let me know if that answers your question!\n. You're definitely not using the syntax prescribed in the docs, and you're passing your port bindings instartas well, which is incorrect. Please review the documentation I linked to once more and make sure you're following the proper procedure.\n. Glad you could solve it!\n. @crosbymichael Inhack/make/test-docker-py`, you can change line 17 to something like\nsh\ntest_env PYTHONPATH=\"$dockerPy\" py.test --junitxml=/path/to/results.xml \"$dockerPy/tests/integration\"\nDoes that answer your question?\n. Closing via #1297 - Thank you!. Hi, I believe this is a duplicate of #1054 . Please refer to that other issue for an explanation of the bug and possible workarounds.\n. Oops. :disappointed: \nLGTM - thanks!\n. https://github.com/shin-/dockerpy-creds\n. Fixes #1023 \n. We do support it already. shm_size needs to be passed through the HostConfig.\n. #923 \n. Thanks!\n. The only thing that's outstanding to me still is this thread: https://github.com/docker/docker-py/pull/1186#discussion_r79425908\nBut I think we can address it separately before the release. Let's go ahead and merge this. :+1: . Nice. LGTM!\n. @TomasTomecek If you mean as opposed to making it a part of the docker-py project, then the answer is yes, most likely this will continue to be the case.\n. Hi!\nThis is what the Docker API returns. If you need to obtain more HostConfig data, you can do so using Client.inspect_container.\nHTH!\n. Glad you guys were able to figure it out! Thanks @TomasTomecek for helping get to the bottom of this. Since the problem has been identified as an engine bug, I'll go ahead and close this issue, but feel free to reopen if there's anything else.\n. It's been tagged for 1.13.0, which is the upcoming release. https://github.com/docker/docker/pull/25905\nIf you want, you can try the current 1.13 release candidate and see if it indeed solves your issue.. That's weird. docker.types is present and declared in setup.py. Installing and importing it in a fresh virtualenv also doesn't seem to raise any issue. Are there more details you can share on what's triggering the issue?\n. I guess I broke this https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/docker_common.py#L38 - my bad. I'll see if I can release a 1.10.1 with a fix.\n. Hey folks, I just pushed 1.10.1 to pypi, can you try to upgrade and check that it solves the issue? Thanks!\n. > No description provided.\nPlease enlighten me.\n. Yeah, that's on ansible, unfortunately. https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/docker_common.py#L154\nThe docker module has had a version_info for a while now (since 1.2), so there's no excuse for not using it.\n. I'm gonna close this since the issue is downstream, but feel free to ping me here if there's anything I can do to help.\n. Hey folks, sorry about the difficulties over the past couple days. I just submitted #1202 which should resolve most of the issues with the new docker-pycreds dependency. Thank you for your patience, I'll update again once 1.10.2 is on pypi.\n. 1.10.2 has been pushed to pypi. That version should solve\n1. The six dependency being too strict in docker-pycreds\n2. The error message about the missing test-requirements.txt\n3. The GitHub URL for the docker-pycreds package.\nPlease let me know if you're still having issues!\n. @dajose Yes, that would be released in 1.11, unless another patch version comes out inbetween in which case we might include that change as well.\nThank you for your help!\n. @dnephin We could. As I wrote, I increased the default from 10 to 25 which should be enough to cover the majority of use cases, but if we find out some people still have issues it's possible to make Compose smarter with handling those.\n. @dnephin PR updated - PTAL!\n. Tentatively using the python3 label, but I'm not sure yet what we're dealing with here. We did change the log streaming code recently so it's possible we missed something.\n. Hm, interesting. Might be the same underlying issue presented differently. I'll look into it. Thanks for the report!\n. That's fair. I went by the description given here: https://docs.docker.com/engine/reference/api/docker_remote_api_v1.24/#create-a-service\nBut clearly this is incorrect.\n. LGTM\n. :+1: \n. @bfirsh Looks like you accidentally included 7f64928 ?\n. I think we've recently ruled against doing something similar, see https://github.com/docker/docker-py/pull/1159#issuecomment-239861637 \nand the solution we went with instead: #1167 \nI think we should keep with the same guidelines here.\n. LGTM, thank you!\n. Not against it in principle, slightly worried about breaking stuff for people who might have a from docker.auth.auth import or similar. May be reasonable to put it in the 2.0 milestone?\n. LGTM now, thanks!\n. Merged via #1294 \n. Seems odd that this would appear on 1.10.3, considering we merged #1081 - is it possible you have multiple versions of docker-py installed (through virtualenv or something) and ansible is using an older one? \n. Ah, thanks for double-checking. It looks like #1081 didn't completely solve our issue, or maybe this is a different problem. \nWhat version of the engine are you using?\n. Would you mind installing docker-py from this source, and reporting here with the error it prints out?\npip install -e 'git+https://github.com/docker/docker-py.git@6b7a828400f46ea81374bc5764d8aa81bf38f6f7'\nHopefully it would help us identify what kind of data docker-py is choking on, and then we can write a patch to fix it.\n. That's interesting. Seems like the error might be in a different place than we originally suspected, since it wasn't caught here. Let me dig in some more.\n. Oh, I figured out why you're seeing the issue: https://github.com/ansible/ansible-modules-core/blob/devel/cloud/docker/docker_image.py#L428\nAnsible does the decoding of data chunks itself, so it doesn't rely on our JSON parsing code, causing the issue at their level when the API sometimes sends multiple chunks at a time. I'm afraid this is something you'll have to report there, as there's little to be done on our end.\n. Cool. Thanks for helping me diagnose this. :)\nI'll close the issue, but feel free to comment or reopen if there's more info or if you have other questions.\n. Thanks - LGTM!\n. Damn, I didn't realize updating/removing a node was possible. Those docs were added after the release :tired_face: \nAnyway, yeah if you're wiiling to make a PR for this that'd be sweet. \nLet's keep the node stuff inswarm.py. For implementation reference, I'd look at update_container which is very similar.\n. From the backports.ssl_match_hostname pypi page:\n\nIf you want to verify hosts match with certificates via ServerAltname \nIPAddress fields, you need to install the ipaddress module_. \nbackports.ssl_match_hostname will continue to work without ipaddress but \nwill only be able to handle ServerAltName DNSName fields, not IPAddress. \nSystem packagers (Linux distributions, et al) are encouraged to add \nthis as a hard dependency in their packages. \n. @TomasTomecek That sounds good. I'm sure other people will be confused by it.\n\nJust a small comment on wording, otherwise LGTM.\n. Thanks :+1: \n. LGTM - thank you!\n. @darkn3rd Can you try upgrading pip and try again? Another source I found suggested using --no-use-wheel when running pip install.\n. The name of the package is docker-py, not docker.py.\n. Hi @nathannis \nSorry for the delay on this, other priorities have cropped up since we released 1.10 . Code and docs look good to me, and you added tests too, so this looks perfect as far as I'm concerned.\nI'll merge to master for the time being, and I'll update once it makes it into a release.\n. LGTM, thanks!\n. We need to improve things on our end in terms of usability, but something like\npython\nclient.create_service(task_template, endpoint_config={\n  'Ports': [\n    { 'Protocol': 'tcp', 'PublishedPort': 8080, 'TargetPort': 80 },\n  ]\n})\nshould work, I think.\n. Yeah, I was unfortunately working with an outdated API documentation while implementing that feature. It seems it has been updated since and we need to fix the field's name in our request.\nThank you for reporting it.\n. LGTM, Thanks!\n. Thank you :+1: . Did you pull fedora:latest before calling get_image? It needs to be available locally first.\n. I tried to reproduce the issue with the following code snippet, but wasn't able to:\n``` python\nimport docker\nc = docker.from_env()\nhc = c.create_host_config(pids_limit=20)\ncid = c.create_container('busybox', 'true', host_config=hc)\nc_data = c.inspect_container(cid)\nassert c_data['HostConfig']['PidsLimit'] == 20\n```\nCan you try running this in your own environment and report back? Also, what OS / Python version are you using?\n. Thank you!\n. LGTM, thank you!\n. LGTM, thank you!\n. What version of docker-py are you using?\n. I can't reproduce the issue when using the same arguments on my machine. Can you try and run the following command: docker run --shm-size 8G busybox df -h? What size do you see then for /dev/shm?\n. Oh yeah I missed that in your first post, sorry. Do not pass any argument to start other than the container name / ID. It's deprecated and it doesn't work on newer engines.\n. By the way, I highly recommend you use fenced code blocks when pasting code or console output in Github issues - it makes it much easier to read.\n. Thank you!. @friism The problem is that we don't really have anything setup to do CI in a Windows + Docker environment, as far as I know. And the dependency on win32file and win32pipe makes it very difficult to do any sort of testing in other environments.\nMy manual tests on Windows didn't reveal the issue, either, so I'm not sure what exactly is causing this.\n. Fixes #1251 \n. Closing, including in #1254 \n. You want to look at Client.put_archive.\nFor an example of usage, you can refer to those integration tests\n. @christianbundy If you want, we have a dev build here that includes those fixes. Testing and feedback will be massively appreciated :)\n. Try mode={'Replicated': {'Replicas': n}}\nSorry the docs is misleading about that part. I'll look into fixing it.\n. LGTM - thank you!\n. Rather than modify the documentation, I think we should modify the code in ContainerSpec to call docker.utils.format_environment.\n. @manics Sorry, I seem to have missed your message. A temporary solution would be to do a delayed import like here: https://github.com/docker/docker-py/blob/master/docker/types/services.py#L41\n. Thank you - LGTM!\n. That's odd, that test has been here for a while - why would it start failing now?\n. It's kind of rough right now and it might change in a future version of docker-py, but the way to do it right now would be to call update_service with mode={'Replicated': {'Replicas': n}}. \n. #1260 \n. see https://github.com/moby/moby/issues/34116 and #1686 . I replied on the molecule issue thread.\n. Not sure why Gordon's freaking out on the DCO - changes LGTM. :+1: \n. Thank you for the report!\n. I believe this is now handled properly thanks to docker.utils.json_stream. Thanks :)\n. Thank you for the report! See discussion in #1264 . I'll close this as a duplicate.\n. LGTM, thank you!\n. The rebase is missing my changes I think. I'll reupload from my local branch.. This is an issue with the ansible docker module - please report it to the maintainers of that project.. Thanks, LGTM.. :+1: . LGTM!. @bfirsh \n- Moved the user guide part of services.md to docs/user_guides/swarm_services.md (To be converted / amended at a later date)\n- Moved docker.types.services docs to docstrings\nPTAL!. @TomasTomecek People will still be able to use 1.10.6 which is stable, and we're still considering whether or not to do some 1.x backports.. > A DockerClient object is part of a graph of objects that represent different things managed by the Docker Engine\n@aanand I'm not sure I follow how DockerClient is managed by the engine - the way I see it, it's an interface to the engine, but the engine doesn't have awareness of it as an entity.. LGTM. Thanks Tomas!\nI'm going to close this for the time being. If the feature makes it into older Python versions at one point we can revisit this.. Thanks Ben! Updated a lot of doc-related stuff and rebased, should look good now.. Thank you for the report folks - we'll exclude requests 2.12.2 in setup.py for the time being.. 2.0 should come out soon.. Let's hold off on that - https://github.com/kennethreitz/requests/issues/3735 suggests it's not resolved yet and I don't want to make hasty changes.. Can you share an excerpt of the code you're running? Also, what version of docker-py are you running?. yes. I have done a partial revert of my previous changes. Moving the GitHub repo and docs is too disruptive right now and we need to move forward with the release. There will still be time to make those renames after the dust has settled a bit.. The master branch contains the 2.0 changes, with Client being renamed APIClient. . Couple comments based on my current understanding that this option was introduced with API 1.24.. Hmm, I don't know what's the cause, but it doesn't look like the Attachable option is being registered by my Engine (1.13.0-rc2)\nIn [10]: net_id = c.create_network(name='helloworld', attachable=True)\nIn [12]: c.inspect_network('helloworld')['Attachable']\nOut[12]: False. @dnephin If you have a minute can you do a quick review of the changes here?. @mikedougherty Thanks! Took some time to read up on Groovy this morning too. Does this look decent?. Thanks - comments addressed :+1: . cc @dnephin @aanand @bfirsh \nHoping to release on Monday!. LGTM - Thank you!. Are you able to pip install anything else? Most likely there's an issue with your network setup.. cc @bfirsh . client.inspect_container(ctnr_id)['Env'] would do the trick, if I understand correctly what you're asking.. Thanks for the report, and the patch!. We should provide a streaming option. You can try the following code excerpt in the meantime:\n```python\nimport docker\nclient = docker.from_env()\nimg = client.images.get('busybox:latest')\nurl = client.api._url('/images/{0}/get', img.id)\nres = client.api._get(url, stream=True)\nwith open('/tmp/img.tar', 'w') as f:\n  for chunk in res.iter_content(chunk_size=None):\n    f.write(chunk)\n```. Just wanted to say - I read the messages, and it's on my radar - should be a simple fix. Thanks everyone for the reports.. Hey folks, please heed the following recommendations when reporting issues: https://github.com/docker/docker-py/blob/master/CONTRIBUTING.md#reporting-issues\nI can't help you if I don't know what version you're running.. You may need to uninstall the docker-py package as it may be conflicting with the new docker package.\nbash\npip uninstall docker docker-py\npip install docker\nLet me know if it helps.. Closing in favor of #1657 . https://msdn.microsoft.com/en-us/library/aa365247(VS.85).aspx#maxpath\nI guess there's some sort of hack we could implement. In the meantime this is something you could try:\n\nA registry key allows you to enable or disable the new long path behavior. To enable long path behavior set the registry key at HKLM\\SYSTEM\\CurrentControlSet\\Control\\FileSystem LongPathsEnabled (Type: REG_DWORD). The key's value will be cached by the system (per process) after the first call to an affected Win32 file or directory function (list follows). The registry key will not be reloaded during the lifetime of the process. In order for all apps on the system to recognize the value of the key, a reboot might be required because some processes may have started before the key was set.\nThe registry key can also be controlled via Group Policy at Computer Configuration > Administrative Templates > System > Filesystem > Enable NTFS long paths.. @mmalek06 The solution is to enable NTFS long paths as highlighted in my previous response. This is a Windows issue, not a Compose issue.. Working on a fix for this in Compose 1.23. in the meantime, installing Compose via pip seems to get rid of the issue. Yes. See https://github.com/d11wtq/dockerpty that does that.. Thank you for the report. We mostly run our tests inside containers (make integration-dind) but it's still worth looking into for sure.. Yes, this API was introduced in 2.0, but you're using docker-py 1.10.\nIf you'd like to install the newest version, you can do so by uninstalling docker-py (pip uninstall docker-py) then installing the docker package (pip install docker==2.0.0`)\nIf however you would like to keep using 1.10, you can find the documentation for that version here: http://docker-py.readthedocs.io/en/1.10.0/api/#containers. Fixed by #1632 . Thanks! Can you squash those commits together?. This is a very late answer, but this can already be donen thusly (if anyone else stumbles upon this issue):\n\n```python\nfrom docker.tls import TLSConfig\nfrom docker import DockerClient\ntls_cfg = TLSConfig(verify=False)\nclient = DockerClient(tls=tls_cfg)\n. Closing via #1663 . cc @bfirsh - Can you look into this when you have a moment? It looks like `disconnect` has the same problem.. The correct syntax would be the following:python\nclient.services.list(filters={ 'name': 'my-name' })\n``.APIClientshouldn't be different fromClientin 1.x in a significant way. If you were able to mock it before it should still behave the same.. Assuming you already had / have code that works with docker-py 1.x, I would simply use theAPIClientclass directly instead of going through a new layer of indirection withDockerClient. There doesn't appear to be a good reason in my mind to invokeDockerClient.api.inspect_containerversusAPIClient.inspect_container.\nThis would have the side-benefit of leaving your test-suite mostly intact (apart from some basicClient -> APIClientrename, I imagine). How did you install Docker? . VMs created using Toolbox don't expose a named pipe and should be connected to using their TCP address. Make sure you set the appropriate environment variables as indicated in thedocker-machine env` command output.. Should be fixed by #1634 . Sorry, that's a bug on our end. Here's a workaround you can use in the meantime:\npython\nimport docker\nclient = docker.DockerClient(base_url='unix://var/run/docker.sock')\nmounts = [\n  docker.types.Mount(target='/data', source='/data', type='bind'),\n]\nclient.services.create(\n  name=\"bab\", command=[\"sleep\", \"30000\"], image=\"busybox\", mounts=mounts\n)\nHTH. You can use client.api.build() to get the old (1.x) behavior.. Thank you for the report. It looks like this is an issue we overlooked.\ncc @bfirsh It seems like we're passing the volumes data to host_config no matter what, so even using the list syntax (volumes=['/hello/world:rw']) will result in an error from the engine because we're passing an invalid bind mount spec.. Yes, this is something we need to figure out. Thanks for bringing it to our attention.. @mitar Use the low-level API for this. There's no way for the client to detect reliably when the exec command has stopped running in an asynchronous manner. Closing: #1417 . Hi @narenst ,\nThe equivalent command to docker inspect is Client.inspect_container. The result of that command will include the State information on any version of the library you decide to use.. Thanks for the report!. Are you using 2.0? If not, do you think this could be it: #1327 ?. Thank you!. The newer version of docker-compose uses the docker package which unfortunately conflicts with the docker-py package. I'd suggest using virtualenv to isolate those projects, or make sure to only use docker (not docker-py) going forward.\nHTH.. @aboutlo You do not need docker-py for the most recent version of docker-compose. It has been replaced by the docker package.\nIf you absolutely MUST have docker-compose and docker-py in the same environment, your best bet is probably to use the binary version of docker-compose here. @james-stephenson \n\nOur workaround was to not use the ansible docker module (unfortunately), and instead use docker directly.\n\nIs there any reason you chose to do that over using the docker-compose binary, or using virtualenv to separate docker-compose and ansible? I'm mostly curious as this seems very radical.. Thank you for taking the time to clarify! . @dj-wasabi You already can - just instantiate a docker.types.Mount object.. docker/compose#4356. The returned type Image matches the documentation, as far as I can tell.  \nWhere did you see that client.images.build returns a generator? Might be an overlook on our part in another section of the docs.. Ah, this is a mistake - the stream parameter doesn't do anything (it's\ndeprecated as noted).\nFor streaming build logs, you'll want the low-level build method on\nAPIClient (client.api.build).\nhttp://docker-py.readthedocs.io/en/stable/api.html#module-docker.api.build. Glad I could help! The documentation is definitely misleading.. Closing via #1393 . I'm closing this in favor of #1632, but still want to thank you for submitting this PR!. Closing via #1481 \nThank you!. Thanks!. Thank you for the report!. I believe this was fixed by #1317 . The list() method returns a list of Container objects. All of them hold a complete set of information about the container in their .attrs member.\nSee http://docker-py.readthedocs.io/en/stable/containers.html#docker.models.containers.Container. LGTM, thank you both!. Thank you!. Actually, the direction I'd rather see us go in is to\n1. Remove the mention of the stream param in that docstring, since it's a no-op.\n2. In another update, add something to the DockerClient that lets one retrieve build logs (either a param when calling build or maybe a different method. Thank you for your contribution! However, I feel like that change will make our documentation more confusing, whereas the current version at least provides a clear, supported way to use this method.\nAs far as the test failures are concerned, I'm investigating, but I agree that they're obviously not a result of your changes :). @bfirsh Those params aren't referenced directly by the models, so documentation only needs to be updated on the classes themselves. Hi!\nThis is a known limitation of our implementation that is on our TODO list. See the original report: #1117 . Fixed. Hi,\nUnfortunately that looks like a bug on our end. I believe you should be able to do the same thing using the network_mode parameter, i.e\npython\napi.containers.run(\n  ...\n  network_mode='foo'\n)\nHope that helps. - This is closed because a fix has been merged in master.\n- networks will be removed in the next release, as it has never worked to begin with.\n- The documentation is updated in master along with the code itself.. FWIW, you can also use client.api.create_host_config() which will fill in the version for you.. What version of six do you currently have installed? (pip freeze | grep six). @twllight Can I ask what your client.containers.create call looks like? It looks like the problem might be coming from there.. Okay, I see the issue now. With docker.containers.create we attempted to flatten the arguments list for HostConfig directly into the call, but it can be a bit confusing when migrating over from 1.x. As a result, ports and port_bindings have been fused into one and you'll just need to provide the dictionary in the create call. Same idea for volumes and binds.\nYou'll want to try this instead:\n``python\ncont_create = docker.containers.create(image       = ship_opts['cont_image'],\n                                              hostname    = ship_opts['cont_hostname'],\n                                              ports       = {5520: 5520, 7088: 7088, 8088: 8088, 5020: 5020, 8181: 8181},\n                                              command     = ship_opts['cont_action'],\n                                              cpuset_cpus = ship_opts['cont_cpuset'],\n                                              labels      = ship_opts['cont_label'],\n                                              name        = ship_opts['cont_name'],\n                                              environment = ship_opts['cont_env'],\n                                              detach      = True,\n                                               tty         = True,\n                                              privileged     = True,\n                                               dns            = ship_opts['cont_dns'],\n                                    dns_search     = ship_opts['cont_search'],\n                                    mem_limit      = ship_opts['cont_memory'][0],\n                                    network_mode   = ship_opts['cont_network_type'],\n                                    binds          = mount_points,\n                                    restart_policy = {\"MaximumRetryCount\": 5, \"Name\": \"always\"}\n). Trycontainer.attrs['NetworkSettings']['Ports']. I believe you will have to callcontainer.reload()to refresh the container info after it's allocated a port (i.e. once it's been started)..ipv4_address` is an IP address, not an IP range.. Yes - the subnet mask of your network and the IP address of a container that is connected to it are not the same thing.\nEither way, glad you figured out a solution.. Thanks for the report - we'll look into it and figure out the best way to approach a fix.. See #1380 \n~~In your case though, volumes = ['/:/test:rw'] should work.~~ nevermind. Overall - volumes in DockerClient.containers.run needs fixing, we started with #1439 but there are more improvements, and possibly fixes, to make still. Thank you for your report.. python\nimg = client.images.get(img_id)\nprint(img.attrs['Size']). We will eventually rename the docker-py github repo. This was a pre-emptive change before we decided to delay the move. It's not critical, so I'll leave it like this for now.. Thanks! Integrated that fix in #1460 . See also: #873, #878 . Rebased and merged in 76eb0298c69c74545e9389e0458f27b2ea12fd88\nThank you!. Call container.reload(). What version of pypiwin32 is installed in your environment? pip freeze | grep pypiwin32 to find out.  It should be 219.. Rebased and merged manually - 13b5f785a7ab459960aae82fae00e4245e391387 8c6534d7be9130c7889164cd5ec054cb1e051569\nThank you!. decode is only applicable with stream=True. We should call this out in the docs probably.. Thank you!. If it's a valid configuration, I see no reason for us to add an arbitrary default where none is needed.. I believe this is a duplicate of #1400 . What does your .dockerignore look like?. Should have been fixed by #1679 - let me know if that's not the case and I'll reopen.. That error message is sent by the engine - there's a good chance this subnet is already taken by some other network on your machine, or you're somehow not allowed to allocate it, but unfortunately, there's not much we can do to help at the SDK level.. client = docker.from_env(version='1.25'). Closing via #1545 . I think you can only create a websocket connection over HTTP or HTTPS - The default UNIX socket likely won't work.\nAlso, origin is not a valid params entry - the list of valid params is here: https://docs.docker.com/engine/api/v1.26/#operation/ContainerAttachWebsocket. Thank you!. It's already available: APIClient.tasks. That's weird - that endpoint is pretty standard. What version of Swarm are you using? What version of the SDK?. Belated update if anyone's still watching - this is a Swarm issue and can't be resolved at the client level.. Yes, those are pretty useful additions. Thank you!. Technically, everything that's in requirements.txt is also required to run the tests. Why are we making an exception by adding docker-pycreds is test-requirements.txt?. Ideally the DockerClient would use auto_remove for API versions where it is available when remove=True is passed as an argument. That would also let us allow using detach and remove together for sufficient API versions, which would fix #1477 as well.. Closing via #1545 . Closing for the reasons explained in https://github.com/docker/docker-py/pull/1937#issuecomment-398198819. Merged via #1500 . Thanks!. Thanks for the report! Looks like the same issue as #1497 .\nYou should be able to use remove instead to the same effect.. See http://docker-py.readthedocs.io/en/stable/api.html\n\nversion (str) \u2013 The version of the API to use. Set to auto to automatically detect the server\u2019s version. Default: 1.24\n\nIf you know your Engine supports 1.25 (you can find out using docker version), you can instantiate APIClient with the appropriate version param.\nOr use auto.. Sorry for the (very) late response. While I'm sure this is useful to some, I don't think it belongs in the Docker SDK. I realize that's not something I expressed clearly in #980 and I apologize for that.\nI believe it would make sense as a complementary library though!. You're reading the documentation for the Docker Python SDK (docker>=2) but using the legacy docker-py  library (docker-py==1.10.6). The documentation for the version you're currently using is here, but please upgrade (pip uninstall docker-py ; pip install docker) when you get the chance!. We already do: https://github.com/docker/docker-py#installation. No problem :+1: Thank you for your patience!. docker pull ubuntu is actually translated into docker pull ubuntu:latest. Same thing for your rmi command which only untags the ubuntu:latest image (you probably have ubuntu:16.04 tagged as well which prevents the CLI for actually removing the associated layers). So your CLI command is very fast because it's not actually downloading any new data, just checking that the tag matches the version you already have locally and re-tagging it accordingly.\nOn the other hand, the API (and the Python API client) when asked to pull ubuntu, actually pulls the entire repository (all images tagged in the official ubuntu repository, of which there are a lot).\nIf you change your code to use equivalent pull commands, I believe you will see comparable execution times:\n```python\n    def test_hello_world_sdk_with_cli_pull(self):\n        client = docker.from_env()\n        call(['docker', 'pull', 'ubuntu'])\n        output = client.containers.run(\"ubuntu\", \"echo hello world\")\n        self.assertEqual(output, 'hello world\\n')\ndef test_hello_world_sdk_with_sdk_pull(self):\n    client = docker.from_env()\n    client.images.pull('ubuntu:latest')\n    output = client.containers.run(\"ubuntu\", \"echo hello world\")\n    self.assertEqual(output, 'hello world\\n')\n\n``. In your defense, our SDK docs say \"similar todocker pull`\" - it should probably clarify in which ways it's not similar!\nAnyway, glad I could help!. Are you using Docker Machine or Docker Toolbox? If so, you might be missing this command: docker-machine.exe env --shell=powershell dev | Invoke-Expression (https://docs.docker.com/machine/reference/env/)\n. try this instead:\n```python\nfrom docker.types import ContainerSpec, TaskTemplate\nc_spec = ContainerSpec(image='mysql', env={'MYSQL_ROOT_PASSWORD': 'mypassword'})\ntask_tmpl = TaskTemplate(c_spec)\nclient.create_service(task_tmpl)\n``. Are you using the low-level API (APIClient) or high-level API (DockerClient)?. Check out theattrsattribute on theImage,Container,NetworkandVolume` objects.\n\nhttp://docker-py.readthedocs.io/en/stable/containers.html#docker.models.containers.Container.attrs\nhttp://docker-py.readthedocs.io/en/stable/images.html#docker.models.images.Image.attrs\nhttp://docker-py.readthedocs.io/en/stable/networks.html#docker.models.networks.Network.attrs\netc.. Newer versions should handle everything properly now.. Yes - I agree it shouldn't be ignored. Raising an error if a float is provided would be the appropriate course of action.. Thank you!. You get different datasets because the engine version you're talking to is different. This has nothing to do with the SDK.. The fetch_current_spec parameter can be used for this in recent versions.. Thank you for the report!. Ah, right, that's going to be an issue. Thank you for reporting it!. LGTM, thanks! Such a weird behavior for requests to have!\n\n\nIs something up with Janky PR tests? It seems like all recent PR's to the project are stuck waiting for this build to be scheduled\n\nIt's obsolete, we need to remove the hook - I just haven't taken the time yet!. It was implemented in 2.1.0. This honestly is an issue with Ubuntu and its stubbornness with messing with the packaging / bundling of certain Python libraries.\nI won't add urllib3 to our requirements.txt because urllib3 is not a requirement of the docker SDK - requests is.\nIf you decide (or your OS decides) to use older versions of libraries than the ones we indicate and test our software with, you have to accept the consequences of those choices and the extra work it might entail. We have made reasonable concessions to support a wide range of setups (i.e. fallback on urllib3 import when requests.packages.urllib3 is unfortunately absent), but none that would compromise the quality of the software for the majority of users.\nYou may solve this issue by using virtualenv, or using a more recent version of Ubuntu.. As I outlined in #1526, I don't think this change is warranted.. The value is cached. Call container.reload() to retrieve the updated status.. Yes, the reload method was missing in our docs, that's now been fixed: http://docker-py.readthedocs.io/en/stable/containers.html#docker.models.containers.Container.reload\nAs to the rationale for caching, not all environments are low-latency, and server requests can be expensive, which is why it makes sense to me to give the developer control over when they want to retrieve new data from the remote with an explicit call. I hope that makes sense!. You can use Service.update or APIClient.update_service with an updated mode ServiceMode attribute.. APIClient is a class. It needs to be instantiated.. A 500 error is still a server-side (engine) issue and should be addressed at that level.\nOne significant difference between 2.1.0 and 2.2.1 is the default API version (1.24 vs 1.26). Can you reproduce the issue with 2.2.1 if you specify version=1.24 when instantiating the Client?. I'm going to close this, but feel free to reopen if your issue persists.. Thank you for the report. Does it work if you force the API  version to 1.20?. Merged in e17a545aa5b17a2aa8de486a2e2363a6274ea6a7\nThank you!. The output should match what is returned by the API. https://docs.docker.com/engine/api/v1.27/#operation/ContainerStats\nIt's very likely that percentages are computed by the CLI on the client-side.. It's possible, and yes you'll probably need threading.\nYou can refer to the CLI code to figure out how it's done there ( https://github.com/docker/cli/blob/master/cli/command/container/stats.go ) - it's in Go, but uses the same API.. client.images.list(name='deis/store-daemon')\nhttp://docker-py.readthedocs.io/en/stable/images.html#docker.models.images.ImageCollection.list. @joebloggs12 Thanks for the report. FWIW, client.images.pull('bsidescbrctf/2017-crypto-docuprotect:1') should work as well.. I agree! PRs are welcome :). Closing via #1583 . Thank you!\nDo you mind squashing those commits into one?. Closing via #1583 . LGTM, thanks!. Thank you for the report. I'm assuming you started a long-running container, like a server? In which case using detach=True makes a lot of sense.\nWhat behavior did you expect to see instead?. Yes - for long-running processes, either use detach=True or create + start.\n. Thanks Santhosh! Do you know which API version introduced that option? We probably want to do a client-side check to see if it's available to the user.. Merged via #1558 . I've only glanced at this, but maybe you could subclass APIClient into NvidiaAPIClient instead.\nMore thoughts when I have some time!. Duplicate of #1433 . I think we should leave it up to users to figure out if they need those additional packages. A sizable amount of users don't require TLS at all when communicating with the engine over a UNIX socket / Windows npipe.. Looks like building the cryptography module on AppVeyor fails because opensslv.h is missing.\n```\nFailed to build cryptography\nInstalling collected packages: mock, colorama, py, pytest, coverage, pytest-cov, mccabe, pyflakes, pep8, flake8, appdirs, asn1crypto, backports.ssl-match-hostname, pycparser, cffi, idna, six, pyparsing, packaging, cryptography, docker-pycreds, enum34, ipaddress, pyOpenSSL, requests, websocket-client\n  Running setup.py install for cryptography\n    Complete output from command c:\\projects\\docker-py.tox\\py35\\scripts\\python.exe -c \"import setuptools, tokenize;file='C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-build-mitiwy39\\cryptography\\setup.py';exec(compile(getattr(tokenize, 'open', open)(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" install --record C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-ltw8vmfe-record\\install-record.txt --single-version-externally-managed --compile --install-headers c:\\projects\\docker-py.tox\\py35\\include\\site\\python3.5\\cryptography:\n    running install\n    running build\n    running build_py\n    running egg_info\n    writing dependency_links to src\\cryptography.egg-info\\dependency_links.txt\n    writing requirements to src\\cryptography.egg-info\\requires.txt\n    writing top-level names to src\\cryptography.egg-info\\top_level.txt\n    writing src\\cryptography.egg-info\\PKG-INFO\n    writing entry points to src\\cryptography.egg-info\\entry_points.txt\n    warning: manifest_maker: standard file '-c' not found\nreading manifest file 'src\\cryptography.egg-info\\SOURCES.txt'\nreading manifest template 'MANIFEST.in'\nno previously-included directories found matching 'docs\\_build'\nwarning: no previously-included files matching '*' found under directory 'vectors'\nwriting manifest file 'src\\cryptography.egg-info\\SOURCES.txt'\nrunning build_ext\ngenerating cffi module 'build\\\\temp.win32-3.5\\\\Release\\\\_padding.c'\nalready up-to-date\ngenerating cffi module 'build\\\\temp.win32-3.5\\\\Release\\\\_constant_time.c'\nalready up-to-date\ngenerating cffi module 'build\\\\temp.win32-3.5\\\\Release\\\\_openssl.c'\nalready up-to-date\nbuilding '_openssl' extension\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Ic:\\python35\\include -Ic:\\python35\\include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.14393.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.14393.0\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.14393.0\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.14393.0\\winrt\" /Tcbuild\\temp.win32-3.5\\Release\\_openssl.c /Fobuild\\temp.win32-3.5\\Release\\build\\temp.win32-3.5\\Release\\_openssl.obj\n_openssl.c\nbuild\\temp.win32-3.5\\Release\\_openssl.c(434): fatal error C1083: Cannot open include file: 'openssl/opensslv.h': No such file or directory\nerror: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\VC\\\\BIN\\\\cl.exe' failed with exit status 2\n\n```\nDo we need those changes to requirements.txt?. https://github.com/pyca/cryptography/issues/3028\nNot sure what version of pip AppVeyor is using.. pip is up to date on Appveyor, seems like the solution is to force pip to install the wheel using --only-binary.\nThis should work: https://github.com/shin-/docker-py/commit/eeb29b92ad609cf8818854930d7f1b7740d55650. Collecting cryptography==1.8.1 (from -r C:\\projects\\docker-py/requirements.txt (line 5))\n  Could not find a version that satisfies the requirement cryptography==1.8.1 (from -r C:\\projects\\docker-py/requirements.txt (line 5)) (from versions: 0.2, 0.2.1, 0.2.2, 0.3, 0.4, 0.5, 0.5.1, 0.5.2, 0.5.3, 0.5.4, 0.6, 0.6.1, 0.7, 0.7.1, 0.7.2, 0.8, 0.8.1, 0.8.2, 0.9, 0.9.1, 0.9.2, 0.9.3, 1.0, 1.0.1, 1.0.2, 1.1, 1.1.1, 1.1.2, 1.2, 1.2.1, 1.2.2, 1.2.3, 1.3, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.4)\nNo matching distribution found for cryptography==1.8.1 (from -r C:\\projects\\docker-py/requirements.txt (line 5))\nI guess 1.8.1 is only available on UNIX platforms?. Sorry for neglecting this for so long, I thought I had merged it. Thank you!. Sorry, there is no such guarantee.. Thanks!\nFixed the flake8 error and merged in c5cc23884a53bb2d074b290ae7e3beb8b26d1703. Closing via #1568 . Hi Antoine,\nThank you for taking the time to write and submit a PR! Sorry for not responding earlier, I've been busy with other tasks lately. There's a few things I need before I can accept this PR:\n\nPlease sign your commits\nMake sure the test_create_with_volume_mount passes. You can verify this locally by running make test in the project's root folder.\nAs a bonus, if you were able to add a test checking that the behaviour that used to fail doesn't do so anymore, that would be really great. If that seems too daunting, let me know and I can add that myself.. Yes, every commit should be signed. Alternatively, you can squash them all into a single commit.\n\nChanges look good now. Thanks!. Fixed by #1573 . Thank you!\nRebased and merged in 9412e21f1ab4a9ff849885f7fcfd86f17ab1f59c. LGTM, thank you!. Thank you!. Thank you for the report!. It's a Jenkins/webhook config issue - we don't actually run the janky tests anymore.. Thank you!. see #1504 and related issues.. LGTM, thank you =). Thank you @Larsjep !\nSorry about the late response - my understanding is that this patch will not do anything unless it is used with a Docker Engine built with Go 1.9+, of which there has been none so far. Once one such Engine has been released, we can test those changes to ensure they have the expected result, and if confirmed, merge the PR.\n. Thanks for the report. See my comment there for a workaround: https://github.com/docker/docker-py/issues/1548#issuecomment-297853720. Thank you!. Thank you!. #1576 . #1477 . As pointed out in our docs, You should call stream() on imageBytes and iterate on that.\n```python\nIn [6]: img = c.images.get('docker/compose:1.13.0')\nIn [7]: f = open('/tmp/compose.tar', 'w')\nIn [8]: resp = img.save()\nIn [10]: for chunk in tqdm.tqdm(resp.stream(), leave=True, miniters=1):\n    f.write(chunk)\n   ....:   \n300it [00:00, 4722.71it/s]\nIn [11]: f.close()\nIn [12]: f = open('/tmp/compose.tar', 'w')\nIn [13]: resp = img.save()\nIn [14]: for chunk in tqdm.tqdm(resp, leave=True, miniters=1):\n    f.write(chunk)\n   ....:   \n71273it [01:25, 837.72it/s] \n. Looks like an engine breakage... that test definitely works with 17.04.0.. See #1577\nIt'll be fixed in the next release.. Untested, but something like this should work:\n```python\nfrom docker import APIClient\nfrom docker.types import Mount, ContainerSpec, TaskTemplate, DriverConfig\ncli = APIClient()\ndriver_cfg = DriverConfig('nfs4', options={\n  'o': 'addr=10.x.x.x',\n  'device': ':/docker/pgtest'\n})\nm = Mount(type='volume', target='/var/lib/postgresql/data', driver_config=driver_cfg)\nc_spec = ContainerSpec('medzilla/postgres', mounts=[m])\ntask_tmpl = TaskTemplate(c_spec)\ncli.create_service(task_tmpl, name='pgtest')\n```. That test works against the latest releases, so if there's an issue it's probably on the engine's end (it might not be caused by your changes though). We should test both 500 and 400 so the test still validates pre-existing releases.. The ideal situation from an API consumer standpoint would be for status code changes to happen as rarely as possible, and to not affect existing API versions. \nI understand why that's not necessarily possible, and I know from our previous discussions that you do not want anyone relying on status codes to detect and identify errors, but at this time, and until proper error codes become an integral part of the engine API, this is the only indicator users have. If that type of test can be a small reminder of that, and the possible consequences of that type of change in the real world, I don't necessarily see it as a bad thing.. Thank you!. The docker.types.Mount class provides everything that's needed now.. This behavior is intentional and documented. I'm not sure what the issue is here.. It's just a mistake in the documentation. stream has no effect on the high-level API.. Well no, it's always true in the sense that the logs are streamed in the background and processed to retrieve the image object that is then returned by the high-level API. As we mention higher in the docstring,\n\nIf you want to get the raw output of the build, use the build method in the low-level API.. #1521 #1503 . I think you have to pull master again and rebase your branch for the tests to pass here.. Unfortunately, the ID of the loaded image is not part of the API response on that endpoint: https://docs.docker.com/engine/api/v1.29/#operation/ImageLoad\n\nYou may want to create an issue on https://github.com/moby/moby/issues. Thanks! Would you mind adding an integration test for it as well?. Thank you!. We return a value as as soon as we receive a response from the API endpoint. As far as I know, there's no way to wait for the service to be \"ready\".. Docs needed a rebuild. Thanks for the report! Should be good now.. Please upgrade to 2.3.0. Please ignore the janky test... It's an obsolete test, but we somehow haven't been able to disable the corresponding webhook.. Yeah, I think that works! Thank you!. Try container.exec_run(['sh', '-c', 'echo 5 | python test.py']). ```python\nimport docker\nclient = docker.from_env()\nreplica_mode = docker.types.ServiceMode('replicated', replicas=4)\nclient.services.create(mode=replica_mode, ...)\n``. Thank you for the report. You should be able to work around this by omitting theexec_id=` part for now.. -_-\npython\nexe = llclient.exec_create(container=containerId, cmd=cmd)['Id']\nexe_start= llclient.exec_start(exe, detach=False, tty=False, stream=True, socket=False)\n. LGTM - should we do a similar thing with UnixAdapter and NpipeAdapter as well?. Great, thank you!. That is already handled in APIClient._post_json. Oops. Thanks for the report!. LGTM, thanks!. I keep getting flak when I want to restrict the range of versions for requests on this repo and compose, but this kind of issue is the exact reason I want to do so.\nThanks for the report! I'll take a look at that PR asap.. Should be fixed by #1646 . LGTM, thank you!. Thanks, LGTM!. @matthewtberry They certainly do. Can you create an issue / submit a PR? Thanks!. EndpointSpec is apparently properly set on the service object, so it's likely you're running into an engine issue at that point.. While it's a bit annoying that the check isn't performed when installing from the wheel, this is much less of a concern now that most people have upgraded from v1 to v2 (or for some, decided to stick with v1). As a result, this is not something we're looking to invest time into fixing.\nThank you for your understanding.. Thanks!. PR for this #1756 \nAs I wrote there, we'll need to wait for engine support before merging it.. @cecton Behavior is the same with _raise_for_status called before. If an error occurs while loading, the API has already started streaming the response, and the 200 status will already be in the header. I tested this locally just to be sure:\n```diff\n$ git diff\ndiff --git a/docker/api/image.py b/docker/api/image.py\nindex 41cc267..02b99b0 100644\n--- a/docker/api/image.py\n+++ b/docker/api/image.py\n@@ -292,10 +292,13 @@ class ImageApiMixin(object):\n         res = self._post(\n             self._url(\"/images/load\"), data=data, params=params, stream=True\n         )\n+\n+        self._raise_for_status(res)\n+\n         if utils.version_gte(self._version, '1.23'):\n             return self._stream_helper(res, decode=True)\n\nself._raise_for_status(res)\n\nreturn None\n@utils.minimum_version('1.25')\n def prune_images(self, filters=None):\n\n$ python setup.py develop\n/home/joffrey/.envs/pydocker/local/lib/python2.7/site-packages/setuptools/dist.py:341: UserWarning: Normalizing '2.6.0-dev' to '2.6.0.dev0'\n  normalized_version,\nrunning develop\nrunning egg_info\n[...]\nFinished processing dependencies for docker==2.6.0.dev0\n$ python\nPython 2.7.9 (default, Apr  2 2015, 15:33:21) \n[GCC 4.9.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport docker\nc = docker.APIClient()\nstrm = c.load_image(b'000')\nfor x in strm:\n...   print(x)\n... \n{u'errorDetail': {u'message': u'Error processing tar file(exit status 1): unexpected EOF'}, u'error': u'Error processing tar file(exit status 1): unexpected EOF'}\n``. I thinkupdate_serviceneeds work across the board. Thank you for bringing this particular issue to my attention!. Thanks everyone for the detailed reports. 2.4.1 should be out soon with the fix.. Hmm, alright, should be good with 2.4.2. @Evertude Compose 1.14 doesn't have this fix, but it shouldn't hit this case to begin with. Can you create an issue on https://github.com/docker/compose with your Compose file and the exact command you're running?. #1863 . I'm not actually sure how this is related to the Python SDK.. I don't think that's the issue - requests are cleaned up andNone` values removed from the payload if present, see here\n\n\n\n\n\nEDIT: As far as I can tell, that method is still conform to the current spec: https://docs.docker.com/engine/api/v1.32/#operation/SystemAuth . I suppose it's an issue specifically with ECR? Is there any documentation on what they expect their auth to look like somewhere?. #1863 . Appears to be an engine issue:\n$ docker container prune --filter name=foobar\nWARNING! This will remove all stopped containers.\nAre you sure you want to continue? [y/N] y\nError response from daemon: Invalid filter 'name'\nCan you report it to https://github.com/moby/moby ?. Indeed! Sorry for not updating this issue back then.. Thank you!. Are you using a credentials store?. Alright, thanks! I'll try to reproduce.. That's a server error - can you submit it to moby/moby?. https://github.com/moby/moby/issues/34116#issuecomment-315484688. Should work now with #1807 . Thank you! Pulling all tags when no tag is specified is indeed the API's behavior. Im sure there's valid use cases for it in pull, but we should probably override that in run.. Rebased and merged manually: b8fd8213364ae21c2981f3e51cee924738cb9b93\nThank you!. Updated :+1: Thank you!. The message is self-explanatory. You need to create an instance of APIClient and call create_endpoint_config with that instance.. APIClient is a class. You need an instance of that class. See here if you need a refresher on OOP terms.. Yes, that's not a valid option for run.\n```python\noptions = {\n    \"detach\": True,\n    # \"remove\": True,\n    \"name\": \"mbussim_\" + ip,\n    \"cap_add\": \"NET_ADMIN\",\n    \"network_mode\": \"none\",\n    \"volumes\": {self.instance_dir_src_dir: {'bind': '/src', 'mode': 'rw'}}\n}\n\nself.dockcon = self.dockerclient.containers.run(self.dimg, **options)\nnetwork = self.dockerclient.networks.get('br0')\nnetwork.connect(self.dockcon, ipv4_address='172.27.153.11')\n\n``. You're trying to connect to the same network multiple times, first by declaringnetworkin theoptionsdict and then a second time by callingconnect` on the same network.\nYou can either remove network from options and let the container connect to bridge by default, or you can disconnect then reconnect to the same network:\npython\nself.dockcon = self.dockerclient.containers.run(self.dimg, **options)\nnetwork = self.dockerclient.networks.get('br0')\nnetwork.disconect(self.dockcon)\nnetwork.connect(self.dockcon, ipv4_address='172.27.153.11'). Matches the engine API.. That's not a docker-py issue. Please recreate on moby/moby. We want to defer the default value to the Engine if possible, so I think we should keep None here, and update the docs instead to reflect the actual behavior.. I think that's fine - the definitive reference is the engine API, not the CLI.. Thanks!. LGTM! Just one last thing - do you mind squashing those commits together? Thanks!. Thank you!. Does it happen with the CLI as well? When you curl the API endpoint?. @guyisra What version of the library are you using?. Oh, that's an old version. Improvements to log streaming have been made in 2.x, upgrading will probably help if you can do so.. Good catch, thank you!. Thanks!. userns_mode in HostConfig.. Closing via #1807 . You're using 1.x, the example is for 2.x\npip uninstall docker-py && pip install -U docker. https://github.com/docker/docker-py/issues/1705#issuecomment-321703517. Sorry, looks like this got buried for a while. It's merged now - thank you for your help!. You can compute it yourself using ctnr.attrs['State']['StartedAt'], or you can use the client.api.containers() method where it's listed directly.. Pick one:\n- http://docker-py.readthedocs.io/en/stable/change-log.html\n- https://github.com/docker/docker-py/blob/master/docs/change-log.md\n- https://github.com/docker/docker-py/releases/tag/2.5.0. @jonathanchristison Does it work if you downgrade the docker python package to 2.4.2?. Also can you share the result of docker inspect on your test container?. @jonathanchristison Thank you for the update, I'll look into tackling this in 2.5.1. Thanks! Looks like I missed this. I ended up implementing as part of #1780. Sorry about that!. Thank you!. docker-py is the older, deprecated version of the library. :). Thank you - and sorry for the late review!. As far as I can tell that should not cause any issue. We're not trying to emulate the CLI's behavior.. stream affects the way you will receive the response. If it's set to False, you'll get a single chunk of data once the request completes. If True, it'll be a generator that streams data as it is received.\nFor most use-cases, people should use stream=True, unless you're looking at a very few lines of logs that you expect to get immediately.\nIf you set follow=True and stream=False, the method will wait for the request to complete (container exits or is stopped) or until the HTTP timeout is hit, whichever comes first. That combination is not the most useful, but those 2 parameters have separate meanings and they should stay separate.\nIs it possible that the containers you're getting strings for are TTY-enabled? That's admittedly something we've been fairly bad at keeping consistent.\nI'm happy to look at repro cases if you manage to provide one on that hanging logs thing. It's not something I've run in so far.. Please refer to the Docker documentation on this matter.. nano_cpus is the counterpart to --cpus. cpuset_cpus lets you assign CPUs. Here is a more comprehensive source: https://docs.docker.com/engine/admin/resource_constraints/#cpu which better describes what happens in a multi CPU scenario.\nI don't know that there is a way to say \"use any 2 CPUs\", but you may wanna ask on the forums as it's a bit outside my area of expertise.. Are you testing this on the latest version of docker-py (2.5.1)?. Thank you for confirming. I see now that we're making an incorrect interpretation of the protocol where we equate a 0-length chunk to an EOF signal, which isn't corroborated by the docs:https://docs.docker.com/engine/api/v1.30/#operation/ContainerAttach. I fixed the issue in master, please rebase your PR to get the changes :+1: \n. @tobiasfielitz It's an update call setting mode parameter with the appropriate number of replicas, i.e\npython\nmode = ServiceMode('replicated', replicas=5)\nservice.update(mode=mode, fetch_current_spec=True)\nThat said, it'd be reasonable to have a method for that since it's a fairly common operation.. We are aware of this, unfortunately not much can be done at this point without breaking the codebase for a section of users.. Thank you!. @njthykkathu What version of the library are you using? You might have better luck creating Mount instances instead of relying on string parsing: http://docker-py.readthedocs.io/en/stable/api.html#docker.types.Mount. @njthykkathu You should upgrade to 2.5.2 - The issue you're describing was fixed a couple versions ago. Thank you!. @treuherz You can access the inspect data using the attrs attribute on all model objects (e.g. https://docker-py.readthedocs.io/en/stable/networks.html#docker.models.networks.Network.attrs). client.tasks(filters={'service': service_id})\n(edited with correction suggested below). Closing via #1783 . Try resp = cli.api.get_image('fedora:latest') instead. I think image.save() will always use the image's ID as argument, which causes this to happen:\n\nIf name is an image ID, [...] only that image (and its parents) are returned, but with the exclusion of the repositories file in the tarball, as there were no image names referenced.\n\n[source]. LGTM, thanks!. This works fine for me on the newest versions of the API. Are you testing against Swarm / UCP?\n```python\nIn [1]: import docker\nIn [2]: c = docker.APIClient(version='1.30')\nIn [3]: c.images(name='postgres')\nOut[3]: \n[{u'Containers': -1,\n  u'Created': 1502405289,\n  u'Id': u'sha256:82b06f064259120143321ba826ce7c172bf04404f9634911f545b7bac5aa547e',\n  u'Labels': {},\n  u'ParentId': u'',\n  u'RepoDigests': [u'postgres@sha256:586320aba4a40f7c4ffdb69534f93c844f01c0ff1211c4b9d9f05a8bddca186f'],\n  u'RepoTags': [u'postgres:9'],\n  u'SharedSize': -1,\n  u'Size': 266933601,\n  u'VirtualSize': 266933601}]\nIn [4]: len(c.images())\nOut[4]: 85\n```. Probably some kind of 17.03 bug. I don't think anything needs to be done on our end.. Thanks! Do you mind adding the appropriate version check?. I tried adding a test for this, but I don't think this is supported by the engine yet as no matter what I do, I'm not able to set a different MAC address for the network. This is reinforced by the fact that there is currently no CLI option to achieve this: https://github.com/moby/moby/issues/33715\nUntil this actually works with the Docker Engine, I think we probably shouldn't merge this PR.. Thank you for your report.\nThe parameters map directly to the ones of the remote API. As such, I don't believe any adjustment is necessary.. Thanks for the report, we'll look into it!. Looks like an engine issue - it shouldn't respond to requests with an API version it doesn't recognize.. http://docker-py.readthedocs.io/en/stable/api.html#docker.types.ServiceMode. As far as I know, that's not an available feature at this point in time. You would need to request it on moby/moby first.. Thanks! Rebased and merged in #1793 . I think it's due to a difference in implementation at the system level. You might be able to achieve a simiar result on UNIX systems using cpuset_cpus. moby/moby is probably a better place to ask this :). We're not blocking it at the SDK level, but it's not going to work on Linux. The equivalent to the --cpus flag in the CLI is nano_cpus.. Yes, put_archive and get_archive are the methods to use to copy to and from containers.. You can take a look at what we do in our integration tests: https://github.com/docker/docker-py/blob/master/tests/helpers.py#L29-L38\nI don't think it's particularly necessary to add that kind of helper to the library. The current implementation matches what the API provides, has a clear interface and advertises clearly what the expected input/output is - that's not something I want to blur with additional convenience methods.. Thank you for the report! As far as I can tell, this is working as intended. Since the error only occurs inside the response after the HTTP status has been set, there is no way for the library to pre-emptively detect it.. Yeah... That's definitely not the most well-tested method in the library. Sorry about that!\nAs a workaround for the time being, you should be able to use the methods on the HTTPResponse object to read the response data.\npython\nfrom docker import APIClient\n...\nself.cli = APIClient(base_url=self.base_url, timeout=self.timeout)\n....\ndef archive(self):\n        \"\"\"Archive the container\"\"\"\n        # Returns the path to the exported file\n        try:\n            resp = self.cli.export(container=self.container.get('Id'))\n            for data in resp.stream():\n                yield data\n        except errors.APIError as ex:\n            raise ex. Yes, just f.write the data you receive into a new file.. Does it work if you call reload() on the Network object?. Yeah, what is happening is that the list() method generates the network objects from the /networks endpoint, which isn't as exhaustive as the /networks/<id> endpoint, resulting in some fields not being populated until you call reload. Obviously there's a speed tradeoff here, but maybe we could add a greedy parameter to the list method to indicate whether we want to load the complete object or not.. Thank you!. @pizzop Did you contact support about this? As far as I can tell, this has more to do with the behavior of Swarm clusters / UCP than it does with the SDK's, and I don't want to attempt an explanation that will probably turn out inaccurate.. Thank you, but I think the docs should be updated to reflect the code's behavior, not the other way around.\nAnd what's the deal with all the quote changes?. Ah sorry, I realize what's happened now ; the docstring was updated by https://github.com/docker/docker-py/pull/1694, but because we haven't released a new version since, the docs on the website still have the incorrect version.\nSince #1694 has been merged, no additional change needs to be made.\nThank you!. That was fixed for 2.3.0 onward by #1518 . Thanks!. Thanks! Do you mind adding the mode key to the docs as well? It doesn't need a test, just a mention in the docstring.. Our policy has been to not make version checks on filters, since we're often a bit behind engine releases, it'd cause valid filters to be refused until we add them to the list of accepted parameters, making the experience worse for users in the long run.\nSo no, just a mention in the docstring will be sufficient :) Thank you for being thorough!. Great, thank you for confirming!. Should come out in the next couple of weeks \ud83d\udc4d . Thanks!\nPutting a pin in this, because it'll conflict with the docker-compose requirements, but we should be able to reconcile.. Should be good now. Thank you!. pip uninstall docker-py; pip install -U docker\n. Thank you!. Thanks for the report. Can you specify:\n- what version of Python you're working with\n- which OS you're running on\n- which method you use to connect to the engine (Unix socket, https, named pipe?)\nThank you. Thanks! Python 3.6 is recurrent in the other reports as well. I'll look into it asap.. 2.6.1 is now on PyPi. Thank you for the detailed reports and sorry about that!. Duplicate of #1799, but I'll take the extra info into account, thanks!. @graingert Unfortunately, our CI only tests the HTTP(s) transport, not the unix socket transport, which is why this wasn't caught in the first place. In the medium term, we definitely need to test on both, as any of our exec_run or attach tests would have highlighted the problem.. Thanks!. Thank you for your suggestion.\nYou're welcome to do this if you want, but it would be a separate project. This SDK has built up a large user base that relies on the existing interface, and while an autogenerated is easier to keep up to date, I think there is something to be said for thoughtful, language-aware design as well.. Awesome, thank you!. Can you provide an example I could run locally that exhibits the bug? When you say \"a list of containers, do you mean container objects? If so, those should definitely not be assumed to be serializable / pickleable. That they currently are in some situations is an accident and not by design.\nPlease consider using the data dictionaries instead (accessible through the .attrs property on Container objects). Thank you for your contribution, but no change is needed - this can already be done:\n```python\nfrom docker.tls import TLSConfig\nfrom docker import DockerClient\ntls_cfg = TLSConfig(verify=False)\nclient = DockerClient(tls=tls_cfg)\n```. Thank you for your contribution!\nI'm not sure how to feel about this to be honest. It's going to be a hassle to maintain as more properties constantly get added, and I'm not sure the payoff is significant enough to justify it.. @rycus86 I saw your changes, just didn't have time to take a closer look yet with Thanksgiving break and other priorities. Thank you for your patience. :+1: . Thanks guys. As @feliperuhland points out, if you're interested in the container's output, you probably want to use remove over auto_remove. \nThat said, I submitted #1840 to make sure we don't attempt to get the logs after the wait() call has completed. On the other hand, if the container is removed before we're able to get the logs, I think raising an exception is the expected outcome in this case.. Thank you for your contribution!\nI think this is too situationally useful to warrant adding it as a model property. The data can easily be obtained otherwise through .attrs.. Thanks!. Thanks for the report! Here are a few things worth exploring:\n\nDoes it work with Python 3.5? We don't officially support 3.6 yet.\nAre there any named pipes in your build context, or inaccessible root-owned files? . Also, please check out #1825 to see if it fixes your issue.. Closing via #1825 , but please let me know if the issue persists despite that patch.. Hi @mtsmfm , thank you for the update!\n\nThe integration tests are being run on Jenkins. You can see the output here: https://jenkins.dockerproject.org/job/docker/job/docker-py/job/PR-1826/2/consoleFull. Hi @mtsmfm - sorry about the delay. I can't give you access to Jenkins, but let me take a look at it and see what I can find. I'll update as soon as I find out what the snag is.. So, I'm able to reproduce locally as well. Your assumption is that sending the detach sequence to the socket will close the socket (which you test with assert_socket_closed_with_keys), but that doesn't actually happen. As far as I can tell, all it does is tell the daemon \"stop sending data through that socket\". The client side is still responsible for closing the connection.\nHowever, I was able to trigger a socket.error (Broken pipe) using this code snippet:\n```python\nimport docker\nc = docker.APIClient()\nctnr = c.create_container('busybox', 'cat', detach=True, stdin_open=True, tty=True)\nc.start(ctnr)\nsock = c.attach_socket({'stdin': True, 'stdout': True, 'stream': True, 'detachKeys': 'ctrl-x'})\nsock.send('abc')\nprint(sock.recv(3))  # should read 'abc' back\ns.send(chr(24))  # ctrl-x\ns.send('abc')  # error: [Errno 32] Broken pipe\n```\nYou should be able to adapt your tests to use that instead - let me know if you need more help with it. \nBy the way, you should be able to run the same tests as Jenkins by running make test in the project folder, which will let you check the behavior faster.. Sorry for the delay, I had to take care of the Compose release and didn't have much time for SDK issues. I'll take another look at this this week.. Merged in #1879 \nThank you so much for your help and patience with this PR!. Thanks for the report! Does the patch at #1828 help at all?. Also, what version of requests are you using? Does the error change if you install a different version of requests?. @mbovo Thanks for the follow-up! Would you be able to share the snippet of code you're executing leading up to this error? . Thanks! I'll look into it asap. @mbovo I can't reproduce either, even inserting that snippet into a basic Flask app doesn't recreate the error for me. It's probably an issue with your app or your environment. Are you or a library you're using monkey-patching the builtin exceptions?. The test failure should clear up after you re-push.. This feels a bit like a stupid API trap - why do we have a mandatory parameter that only accepts a single value in the first place?. Even if you modify the library's code to autofill name, it's still an arbitrary breaking change for users of the API in general. I understand rejecting custom names, but I'd strongly recommend letting Swarmkit translate absence of value / null value to default.\n\nI did consider autofill in swarmkit, but I wanted to avoid hidden behavior.\n\nDefaults are filled in everywhere else in the API (e.g. creating a volume without specifying a driver), I wouldn't call that \"hidden behavior\". Since there's nothing to actually configure here, it's just a frustrating \"gotcha\" that serves no practical purpose.. Almost of these tests are for init_swarm, though (except this one) - is it reasonable to assume those do not need to be changed then?\nI get that the chosen protocol might present additional challenges for doing this right, but that doesn't change the fact that the user experience is atrocious for people who happen to rely on these endpoints.\nOne of the main reasons for running the SDK tests on moby/moby in the first place was to be able to catch and address breaking changes. I don't feel that updating the tests to match the new, arbitrary constraints really accomplishes that.. Thanks!. Thanks!. Set force_update to True on your TaskTemplate instance. . Sweet, thank you!. Thank you!. Check out https://github.com/d11wtq/dockerpty. Duplicate of #1841. Thank you for the report.\nI'm not sure there's much we can do about it. Honestly, I'd encourage anyone still using 1.12 to upgrade ASAP because it's really outdated anyway. If that's not an option, using create and start  is still an option and will give you more control over what your code is doing.. That flag looks like a client-only option.` The workers aren't querying the registry ; the Docker CLI is.\nIn other words, that's not something you need to worry about using the SDK.. Ah, it's possible the CLI uses the information it has locally to determine the sha associated with the image's tag instead of letting the daemon query the registry for it. You can probably imitate that with docker-py by doing something like\npython\nimage = 'local.registry.url:5000/foo/bar'\nimage_id = client.inspect_image(image)['Id']\nand using the image_id value in your create_service call. HTH. It's here. Probably a bug with the docs generation. Thanks for pointing it out.. Docs are now fixed. The generator will exit once it reaches EOF.. That's the engine's message - if you feel it is inadequate, please open an issue on the moby/moby repo.. Thanks!. Can you provide the necessary information as explained here? Thank you!. For 1.x, docker-py used to be the name of the pip package if you ever want to get it from pypi instead.. Hi @kairichard \nWhile it's not a project I as the SDK maintainer am personally interested in taking on, you're more than welcome to create your own package based on fake_api_client.py as long as it's published in a way that's compatible with the Apache Software License.\nHTH!. @feliperuhland Thanks!. You need to provide links as part of the networking_config argument in create_container. The links parameter in HostConfig is for legacy links, which is a deprecated feature that only applies to the default bridge network.. LGTM, thank you!. You should use the reload() method to obtain the updated status.. I'll take a look next time I'm on a Windows machine - thanks for the report.. http://docker-py.readthedocs.io/en/stable/api.html#docker.types.Placement. thanks!. There has yet to be a release of the SDK with official support for Python 3.6. This will be addressed before such a release happens.. Fixed by https://github.com/docker/docker-py/pull/1871\nBut again, the latest release (2.7.0) pins the required version of pypiwin32 strictly to 219. The recent change you're calling out was only ever in master.. For the first, runtime is available for APIClient.create_host_config and DockerClient.containers.run since 2.4.0. The service equivalent hasn't been worked on yet.. @mwilliammyers Yes, I believe it is available in the API (otherwise the CLI wouldn't be able to implement it). It is listed under Resources here\nFor the implementation, Resources would need to be updated along with the methods you pointed out. Let me know if this is something you're interested in taking on.. @greenmaid You're right - we don't have support for it currently, unfortunately. Thanks for bringing it to our attention.. cc @mtsmfm . docker-py is the old name of the docker package. New releases for the past year have only been made with the docker package name, so it is recommended that you use it if possible.. I've just updated the docker-py description on PyPi. Hopefully that will prevent similar confusion in the future.. I might have messed up some of the auth stuff - I'll look into it asap and keep you posted. Thanks for the report!. @harlowja Are you using the login() method to authenticate? . @harlowja I think that might be the issue ; can you test the patch in #1896 and make sure it fixes your pulls?. Tests for this should be in tests/integration/api_service_test.py :+1: . Thank you!\nRebased and merged manually in 820de848fa73f20cb80215ceb4b8cdafc855867e. No worries!\n\nI couldn't figure out exactly what version that was added in...\n\nYep, and I apparently got it wrong too :confounded:  It's not your fault, it's missing from the changelog. 8fd9d3c99e9314323228af4832054b22d2ac4966. thank you for the report!. Thanks for the report!\nCan you provide the information requested in the guidelines? It would help a lot knowing what version you're working with.\nAlso note that you don't need to provide auth_config in pull if you've logged in with the same client object, as the client retains the login information internally. Does it work if you don't pass auth_config? \ne.g. this should work:\npython\nimport docker\nclient = docker.from_env()\nclient.login(user, pwd, registry=my_ecr_registry)\nclient.images.pull(('000.dkr.ecr.us-west-2.amazonaws.com/my_repo_name'). Sorry for the late reply - I think you were originally running into the follwing issue: #1895 Have you upgraded to docker==3.0.1 since? If so that probably cleared part of the problem.\nAs for AWS issues specifically, I don't have experience with it, but you may find this comment useful: https://github.com/docker/docker-py/issues/1677#issuecomment-363283049\nLet me know if anything's still unclear.. At the moment there's no shortcut to get bind mounts in the SDK - as you point out, it's fairly easy to implement yourself if you need it. I don't personally think it warrants adding a property on the Container model.. Yep, I already saw the posts in #1352 - we can keep the discussion there :). @pvizeli It's not set in stone, but most likely 3.1.0 should come out late February or early March. 3.1.0 is out with the fix.. Thanks!. Thanks for the report!. Yeah, I think it's fine, these commits are all individually clean and self-contained. \ud83d\udc4d . http://docker-py.readthedocs.io/en/stable/client.html#creating-a-client. That's quite odd - config_dict is a valid argument for load_config in 3.x, so there's no reason that error should happen.\nIs it possible you have different versions of the package interfering with one another, maybe inside / outside a virtualenv?. > Is it possible you have different versions of the package interfering with one another, maybe inside / outside a virtualenv?. Fixed in 3.1.3\nhttps://github.com/docker/docker-py/issues/1968. You probably have old artifacts messing up your imports. Remove all docker and docker-py packages (inside/outside virtualenv) then reinstall.. You can't have docker and docker-py in the same environment, they're incompatible.. I think the idea is good!\nJust a short observation before I review this: could CancellableEvents implement the iterator interface with __next__ and __iter__ methods? Then we wouldn't need the cancellable=True parameter since the change would be backward-compatible.\nAlso, do we want to add this to other methods like logs and attach as well?. @rycus86 Yes, can we add this to attach and logs as well? I think those are where it makes the most sense as potentially \"infinite\" streams.\nOn top of that, can you squash these commits into one? Thanks!. I still need to take a closer look at this. Sorry it hasn't happened yet and thank you for being patient :cat: . Merged in #1965 \nThanks again!. Yeah, sadly that test is hit or miss on 17.06 - don't worry about it.. If you could just squash those commits into one, I can go ahead and merge it.. It's called nano_cpus in the API. It's not an actual version. It was pushed in order to update the PyPi page so people visiting it would be aware of its deprecation and pointed to the new package. Gordon is a bot, so it won't be able to answer ;\nThe thing it's requesting is for you to add the signoff line to your commit message (git commit -s). Using the commands it suggests should fix everything. GPG signing is a different thing entirely and isn't required.. Are you using an implementation other than CPython?. Oh, sorry ; are you on Python 3.7?. Okay ; unfortunately, that version isn't supported yet.. The pypiwin32 package has a 220 release that's compiled for Python 3.6 and a 219 release that works with 2.7-3.5. The newer 223 release may work with 3.7, but I haven't had time to validate it yet.. Next release (3.5.0) will have Python 3.7 support. Thank you for your patience.. ETA: an hour or so :slightly_smiling_face: . @mnottale Thanks! You were able to verify that this fixes the issue?. Cool, thank you! Maybe we'll make a 3.1.1 - this fix will make a lot of people happy.. DockerClient.api.close() should work!. Can you post an example of the config format that seems to be causing the issue? Our auth parsing supports at least 3 different versions of the file, but maybe we missed something.. The equivalent of your CLI command is path=., dockerfile=path/to/directory/DockerFile. This is an issue tracker for the Docker Python SDK. Try stackoverflow.com. I'm guessing this is due to list() being greedy and calling inspect on each container, which might block while the container is in a transitive state. Are you seeing the same issue with client.api.containers(all=True)?\nWe could probably add an option to list() to create Container objects based solely on the partial information provided by the /containers endpoint and let users call reload() explicitly to retrieve the complete attribute set.. Fixed by #1955 . Given the recent developments and the upcoming inclusion of buildkit into Docker, it's unlikely the official implementation of build-time mounts will look like RedHat's version. See https://github.com/moby/buildkit/pull/442 for details.\nI'm closing this as a result, but I'm happy to look at it again if things evolve in a different direction.. The -a option in the CLI is equivalent to setting the dangling filter to false. (according to https://github.com/docker/cli/blob/master/cli/command/image/prune.go#L59)\nIn short, docker image prune -a -f translates as such: client.images.prune(filters={'dangling': False}). Thank you!. Here's how the Docker CLI uses the ControlAvailable field: https://github.com/docker/cli/blob/master/cli/command/system/info.go#L274-L275 \nThis seems to indicate that it is synonymous to \"node is a Swarm manager\"\nHTH. Thanks! I went with something a bit more radical instead: accb9de52f6e383ad0335807f73c8c35bd6e7426. Merged in #2008 :+1: \n. I think it'd be a good feature to have, but I feel you may be overthinking the implementation a little bit. Why not add a demux parameter to attach() that, when set to True, returns a couple of (stdout_stream, stderr_stream)? That'd be simpler, more pythonic, and more in line with the way the rest of the library is written!. I'm not able to reproduce the issue with the example you describe:\n```\n$ tree \n.\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 foo\n    \u2514\u2500\u2500 bar\n        \u2514\u2500\u2500 baz.txt\n2 directories, 2 files\n$ ipython\n[...]\nIn [1]: import docker\nIn [2]: docker.version\nOut[2]: '3.1.1'\nIn [3]: c = docker.APIClient()\nIn [4]: out = c.build('.')\nIn [5]: for x in out:\n   ...:     print(x)\n   ...:   \n{\"stream\":\"Step 1/2 : FROM busybox\"}\n{\"stream\":\"\\n\"}\n{\"stream\":\" ---\\u003e 5b0d59026729\\n\"}\n{\"stream\":\"Step 2/2 : ADD . /\"}\n{\"stream\":\"\\n\"}\n{\"stream\":\" ---\\u003e 1f811cf2c4b2\\n\"}\n{\"aux\":{\"ID\":\"sha256:1f811cf2c4b2a26603b27d4980ea50e94d0c2262dc43cbcfd8a7740b5ffdbdbc\"}}\n{\"stream\":\"Successfully built 1f811cf2c4b2\\n\"}\n```\nIs there any more information you could provide that could help us diagnose this? Thanks!. One thing that could be relevant is that we updated the processing of the  .dockerignore file to be more in line with the docker CLI (#1914). You might want to check that you're not accidentally excluding a folder that's supposed to be there during your build?. Thanks for the report! . Thanks!. Thank you for submitting a PR!\nI'm concerned that this could be considered a breaking change. While the behavior is probably desirable in 99% of cases, I'm guessing there's scenarios where ignoring these failures isn't correct.\nMaybe we can make it an option for now (ignore_removed?), and make it the default for the next major version bump?. We're going to proceed carefully this time around and re-evaluate when v4 comes around. Thanks for your help!. Duplicate of #1502 . Hi @fkromer ,\nI looked at our options for using sphinx to produce PDF output, but it turns out that rst2pdf is abandoned and straight up breaks with recent versions of Python, and the latex output produces a file that is too complex to be properly converted to a PDF file.\nIf this were an easy fix I'd be happy to add it to our release process, but given the situation I'm afraid there isn't much I can do to help.. Ok, I enabled it on RTD: https://media.readthedocs.org/pdf/docker-py/stable/docker-py.pdf\nHTH!. It works fine as far as I can tell:\n```python\nimport docker\nc = docker.from_env()\nmaps = {'8080': '7009', '22': '7007', '2375': '7011', '7014': '7014', '7012': '7012', '7010': '7010'}\nctnr = c.containers.run('busybox', 'top', detach=True, ports=maps, hostname='abccdef')\nctnr.reload()\nprint(ctnr.attrs['NetworkSettings']['Ports'])\n{'22/tcp': [{'HostIp': '0.0.0.0', 'HostPort': '7007'}],\n'2375/tcp': [{'HostIp': '0.0.0.0', 'HostPort': '7011'}],\n'7010/tcp': [{'HostIp': '0.0.0.0', 'HostPort': '7010'}],\n'7012/tcp': [{'HostIp': '0.0.0.0', 'HostPort': '7012'}],\n'7014/tcp': [{'HostIp': '0.0.0.0', 'HostPort': '7014'}],\n'8080/tcp': [{'HostIp': '0.0.0.0', 'HostPort': '7009'}]}\n```\nMaybe it's an issue in your code?. Why would CPU or memory have anything to do with port allocation ?. Ah, I broke python 3 apparently :|. Thank you for testing / reviewing :+1: . https://github.com/docker/docker-py/issues/1946. Right, that's not supposed to happen. Thanks for the report - I'll take a look ASAP.. Yeah it's my fault, the wheel got packaged using a build directory that had some stale files in it. Will be fixed by #1969 . @marpetr Let me know if you're still seeing the issue with 3.1.3. I checked the 3.1.4 wheel and I can confirm it's clean. Try removing the old package and reinstalling with the --no-cache-dir option.. @wjhill Ansible may be installing the deprecated docker-py package which conflicts with the docker package. Make sure you only have one or the other installed.. Not an SDK issue.. Thank you for reviewing! :+1: . Thanks for the report!. Just released 3.2.1 with the fix!. Thanks! We're probably going to go with https://github.com/docker/docker-py/pull/1983 - do you mind checking that it fixes the issue for you as well?. @kant2002 Awesome, thanks for confirming!. you'll need to call reload() to get the latest state.. Client.containers.run creates and starts a new container. To start a container that already exists, you should do the following:\npython\nhub = client.containers.get('hub')\nhub.start()\nHTH!. It returns a tuple as specified in the docs. The exact type of the first member of the tuple is an implementation detail ; it should be iterated over to retrieve the file data.. We agree ; please make a PR if you feel the documentation needs to be clearer on that point.. This looks like an issue with permissions on your machine. Try to run as administrator and see if it clears it. Regardless, it doesn't have anything to do with the package.. Thank you!. Not an SDK issue.. http://docker-py.readthedocs.io/en/stable/images.html#docker.models.images.ImageCollection.build. This should work fine for all versions of the API.. You can check client.ping(), though you'll likely get the same kind of error will be raised if it can't establish a connection to the daemon.. Thank you for the suggestion; however, this is outside of the scope this library is designed to cover.. http://docker-py.readthedocs.io/en/stable/images.html#docker.models.images.ImageCollection.build. Thanks! I went ahead and removed the check altogether (accb9de52f6e383ad0335807f73c8c35bd6e7426) as I believe it is no longer useful.. Thanks! I went ahead and removed the check altogether (accb9de) as I believe it is no longer useful.\n. The error message is self-explanatory:\n\n\"Cannot locate specified Dockerfile: /home/fl1/documents/interface/webapp/docker/esp8266/Dockerfile\"\n\nMake sure the Dockerfile is present at the specified location and that the user running your code is allowed to read and access it.\nHere's a short article on Python iterators and generators: https://anandology.com/python-practice-book/iterators.html\nHere's the documentation for APIClient.build: http://docker-py.readthedocs.io/en/stable/api.html#module-docker.api.build\nThe most foolproof way to retrieve your image after building it is to give it a name using the tag argument. Alternatively, you can also look at how we do it in the higher-level DockerClient: https://github.com/docker/docker-py/blob/master/docker/models/images.py#L258-L276. @beldhia First and foremost, please refer to the documentation.\n\nIf\u00a0detach\u00a0is\u00a0True, a\u00a0Container\u00a0object is returned instead.\n\nYou can call logs() on that object to retrieve that container's output.. This is the intended behavior ; stream=False returns a binary string containing all logs for the container once they've been retrieved. stream=True returns a generator that iterates over log data as it is received. As a result, the correct way to use logs with stream=False is thus:\npython\nerr_logs = container.logs(stdout=False, stderr=True, stream=False)\nprint(err_logs.decode('utf-8'))\nHTH!. http://docker-py.readthedocs.io/en/stable/networks.html#docker.models.networks.Network.connect. You need to create the network first.. The documentation is up to date. You're likely using an outdated version of the library.. http://docker-py.readthedocs.io/en/stable/client.html#docker.client.DockerClient.login\n\nreauth (bool) \u2013 Whether or not to refresh existing authentication on the Docker server.. Thanks! CI failure looks unrelated.. You're using the wrong package. Uninstall docker-py and install docker instead.. You probably have some path conflicts.\nMake sure to remove all copies of docker-py with pip uninstall (inside/outside virtualenvs, etc.) first, then uninstall and reinstall the docker package.. Thanks @asottile !. Please do a cursory search before posting a question that has been asked ten times before already: https://github.com/docker/docker-py/issues/2015. WaitNamedPipe is not the issue. The Docker CLI uses it as well: https://github.com/docker/cli/blob/master/vendor/github.com/Microsoft/go-winio/pipe.go#L160 - and nothing indicates that it's not supposed to work from inside a container.. Looks like a pipenv bug. Dependencies resolve fine with pip install.. AFAICT, dockerpty still works fine. It's still used in some of our own projects (docker/compose being the most prominent example).. You can poll container.reload() -> container.status ; or just run container.wait() and your post- task in a thread or a celery worker?. Images are immutable. If you need to add something to an image, you'll need to create a new image ; the best way to do so is generally to use build, but run + commit can do the trick if you're only interested in adding, say, a single package to a base image.. @fkromer The API doesn't let you clear the logs. If you only need the output for the exec command, the return value of exec_run should handle that. Otherwise, use the since argument to limit the amount of logs you're getting each time?. @dzervas See also #1916 , #1968 . Thanks for the report! Feel free to submit a PR for it if you have a minute!. In my experience, pywin32==223 has been riddled with issues. I'll upgrade when I feel confident it doesn't break 50% of the windows userbase ; in the meantime, use a virtualenv.. From the error message, it seems like you're setting cpus when creating the container. The Docker daemon subsequently refuses to update period/quota as they're mutually exclusive options.. Thank you!. Not sure I understand what purpose this would serve. Do you have an example where this is necessary?. Use the recommended dictionary notation:\npython\nclient.containers.create(\"ubuntu:latest\", volumes={'/sys/bus/usb/devices/1-3:1.0': {'bind': '/sys/bus/usb/devices/1-3:1.0'}})\nAPI should support --mount functionality that allows explicit specification of source and and destination in parameter value. \n\nIt already does. Please consult the documentation.. Sorry for the confusion on the earlier thread.\n\nmounts and binds are 2 different things, as explained here (in the context of the CLI). mounts is more modern and a better choice 99% of the time.\nBecause of the way the binds API was designed, {'/test:dir': {'bind': '/test'}} has to be transformed into ['test:dir:/test'], which the daemon fails to parse (as it expects the colon to be a path separator). This is not a SDK issue. (See here for the Docker daemon API)\nAs @thaJeztah pointed out, the correct approach to this problem is to use mounts instead\n\npython\nimport docker\nc = docker.from_env()\nmnt = docker.types.Mount(type='bind', source='/tmp/colon:colon/', target='/colon')\nctnr = c.containers.create('busybox', mounts=[mnt]) # <Container: 38869806cb>\nHTH. Have you tried streaming the logs instead?. Using the stream=True parameter.\nOn Fri, Jun 1, 2018, 12:55 AM Tioborto notifications@github.com wrote:\n\nNo, I haven't tried. Using the \"attach\" method?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/docker/docker-py/issues/2043#issuecomment-393798239,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABCVnOH6GZZ5a1amSm5-ZkAvT-4VwWoOks5t4PNmgaJpZM4URGzn\n.\n. You're asking for the raw socket, so no decoding is done by the SDK, causing the stdout/stderr byte to appear.\n\nThis is working as intended. \ud83d\udc4d. Yes - just don't set socket to True?. Thanks for the report! Feel free to submit a PR!. Broken for Python 3.3 and 3.4. It should work ; make sure your credentials are in the Docker config file (or in the credential store you're using), or call client.login() first.. Hmm, the documentation doesn't indicate that /plugins/privileges accepts auth headers, but that may be an overlook. Looking at the engine code, it definitely looks that way.\nFeel free to submit a PR for it if you have a moment - you're better poised to test the change than I would be!. @ramkrishnan8994 Please read the documentation.\npython\nwith open(tarpath, 'r') as f:\n  client.images.load(f). Thank you!. Building strings like this in Python is very inefficient. Here's a blog post comparing different methods and their relative performance.\nIn your case, you can do: \npython\nlog_init = y.get_archive(service_log_path)\nlog_str = b''.join(chunk for chunk in log_init[0]).decode('utf-8'). Thank you!. You can't pass multiple commands (the documentation's language is intentional; \"a command\" is singular). the list format is available for cases where argument parsing is ambiguous and shlex doesn't have the expected behavior.\nYou can do sh -c 'echo \"Hello World!\"; echo \"Another echo!\"' instead.. Thank you!. You probably installed an old version of the package. Make sure you install the latest version.. docker-py is the old version of the package (latest release 1.10.6 which was several years ago). You can find the documentation for it here: http://docker-py.readthedocs.io/en/1.10.3/ - docker.from_env() was not part of the library back then.\nI highly recommend using the docker package for new projects. Use a virtual environment to avoid conflicts with ansible.. You can't pause a container that's already exited - since true exits right away, your pause instruction probably arrives way too late. This isn't a bug.\n\nI'm using SDK Version v1.24,\n\nDo you mean API version? It'd be useful to include the actual SDK version as well (you can find out using pip show docker). Read the docs.\nhttps://pypi.org/project/docker/. Do this instead:\npython\nfname='/home/magowiz/proj/Dockerfile-master'\npath = os.path.dirname(fname)\nimage = self.client.images.build(path=path, dockerfile='Dockerfile-master')\nBy default, fileobj sends a Dockerfile without any build context, which is why you can't ADD anything. fileobj also lets you pass an entire build context (tarball containing context + Dockerfile) if you set custom_context to True, but in your case, it's much simpler if you just set path and dockerfile and let the SDK take care of generating the context for you.. Should still work the same as it used to:\n```python\nimport docker\nc = docker.APIClient()\nstream = c.build('.', dockerfile='Dockerfile.run', rm=True)\nfor chunk in stream:\n    print(chunk)\n```\n. @devnore The APIClient.build method still works the same as it always has. DockerClient.images.build does behave as you describe - you should pick the tool that best matches your needs. . Thanks for the report! I ended up fixing this as part of the work I was doing on #2165 . The source of truth for questions such as this one is the API documentation.\nAs you can see, it makes no guarantee that the image list will be ordered a certain way, so I wouldn't rely on it behaving a certain way.. Thank you!. @ofek Done - we still need to pin on 219 for the older versions though.. Hmm, that official package is missing versions (including 219, which we still want to use for the foreseeable future), and I'm concerned about the potential conflicts that could arise from including the same package from two different sources, even if they're mutually exclusive. On top of that, when I've tried to use it in the past, it was just completely broken.\nI'm more comfortable with sticking what works for the time being. If pypiwin32 starts lagging behind significantly, we can revisit.. It is possible. Check https://github.com/d11wtq/dockerpty for example.. You probably need this: https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities. LGTM, thank you!. Something like this:\n```python\nimport os.path\nimport docker\nregistry_url = 'registry.example.com'\nimg_name = 'someuser/some-image'\nclient = docker.from_env()\nimg = client.images.get(img_name)\nassert img.tag(os.path.join(registry_url, img_name))\nclient.login(\n  username=registry_configs['USERNAME'],\n  password=registry_config['PASSWORD'],\n  registry=registry_url\n)\nclient.images.push(os.path.join(registry_url, img_name))\n```. https://github.com/docker/docker-py/search?q=must+be+an+int+or+float&type=Issues\nMake sure you use pip and don't depend on Ubuntu's vendored files for your Python dependencies (namely urllib3 and requests).. :wave: @bfirsh \nChanges LGTM - sorry for the delay!. Glad you could find it. For the record, the equivalent of inspect_network in the high-level API is client.networks.get(\"network_id\").attrs. http://docker-py.readthedocs.io/en/stable/images.html#docker.models.images.ImageCollection.push\n\ndecode\u00a0(bool) \u2013 Decode the JSON data from the server into dicts. Only applies with\u00a0stream=True\n\n. http://docker-py.readthedocs.io/en/stable/containers.html#docker.models.containers.ContainerCollection.run. Doesn't look like an issue with your pull or your authentication - the relevant error is \"OCI runtime create failed: json: cannot unmarshal object into Go struct field Process.capabilities of type []string: unknown, and it seems to happen when hitting the /v1.35/containers/<id>/start endpoint (usually through APIClient.start(), Container.start() or DockerClient.containers.run()). \nThis issue seems relevant: https://github.com/docker/for-linux/issues/61. No, if there was an error with the pull at any level that would be reported as such in the API, unless your proxy errors are really good at disguising as valid JSON for some reason.\nAs for the port used, since that is handled by the daemon rather than the client, there's no reason that it would use a different port if provided with the same parameters.. Should work if you just remove the redundant docker.io prefix, but I agree this shouldn't raise ideally.. You're right, sorry!. Hm, I tried using your zip file but I'm not able to reproduce (Windows 10, Python 3.6.4)\n```\npython\nPython 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nexit()\nPS C:\\Users\\joffrey\\work\\tests\\repro2117> python .\\errorTest.py\n.\n\n\n\n\nRan 1 test in 5.101s\nOK\nPS C:\\Users\\joffrey\\work\\tests\\repro2117> python .\\errorTest.py\n.\n\nRan 1 test in 5.185s\nOK\nPS C:\\Users\\joffrey\\work\\tests\\repro2117> python .\\errorTest.py\n.\n\nRan 1 test in 5.146s\nOK\nPS C:\\Users\\joffrey\\work\\tests\\repro2117> python .\\errorTest.py\n.\n\nRan 1 test in 5.098s\nOK\nPS C:\\Users\\joffrey\\work\\tests\\repro2117> python .\\errorTest.py\n.\n\nRan 1 test in 4.944s\nOK\n```. Hi @arycloud \nThanks for your suggestion. The purpose of this library is to specifically provide bindings for the Docker Engine API. I see how an API client for Docker Hub would be useful, but I think it'd be overextending too much to include it here.\nAdditionally, I don't think there's a public, official API for Hub.. Closing as a duplicate - but we do need to address this.. @TomasTomecek I don't know if I agree with that. I'm actually getting slightly better times with the Python API on average than I am with the CLI on my laptop, although with very high variance (getting 10s - 1m ranges on all 3 tests):\n```\n$ python --version\nPython 3.7.0\n$ pip show docker\nName: docker\nVersion: 3.5.0\n$ ls -lh\ntotal 2.0G\n-rw------- 1 joffrey joffrey 2.0G Aug 21 11:07 bigimg.tar\n-rw-rw-r-- 1 joffrey joffrey  115 Aug 21 11:15 test1.py\n-rw-rw-r-- 1 joffrey joffrey  108 Aug 21 11:15 test2.py\n$ time docker load -i bigimg.tar\nLoaded image: bigimg:latest\nreal    0m51.426s\nuser    0m0.416s\nsys 0m1.372s\n$ time python test1.py\nreal    0m38.432s\nuser    0m0.196s\nsys 0m1.968s\n$ time python test2.py\nreal    0m24.617s\nuser    0m1.164s\nsys 0m1.612s\n```\n```python\ntest1.py\nimport docker\nc = docker.APIClient(timeout=120)\nwith open('./bigimg.tar', 'rb') as f:\n    c.load_image(f.read())\n```\n```python\ntest2.py\nimport docker\nc = docker.APIClient(timeout=120)\nwith open('./bigimg.tar', 'rb') as f:\n    c.load_image(f)\n```\nTesting on a small EC2 instance gives me more stable results where performance is pretty much identical:\n```\n$ time docker load -i bigimg.tar \n6e53fbb1bdaf: Loading layer  2.147GB/2.147GB\nLoaded image: bigimg:latest\nreal    1m6.209s\nuser    0m0.812s\nsys 0m1.600s\n$ time python test2.py \nreal    1m6.354s\nuser    0m1.024s\nsys 0m1.924s\n```\n(Unfortunately this instance is too small to load the entire file in memory with f.read() resulting in a memory error for test1.py, but it's probably a bad idea to do that anyway). @wekay102200 Make sure your version of the docker library is up to date (should be 3.5.0) and that your version of Docker is up to date as well (latest is 18.06.0-ce). And if you're still seeing issues, try to do load_image(f) instead of load_image(f.read()), as the latter may saturate your RAM and cause significant slowdowns.. @kolyshkin Thanks! Just a style issue needs to be addressed for the CI to turn green it seems:\ntests\\integration\\api_container_test.py:876:80: E501 line too long (87 > 79 characters)\n. Another pep8 issue. FWIW you can run make flake8 to catch these quickly\ntests/integration/api_container_test.py:877:17: E128 continuation line under-indented for visual indent. Hi @wekay102200 \nI don't think this is a Python SDK issue. Please report it there instead: https://github.com/docker/for-linux/issues. Thanks for the report, that's indeed a bug on our end.\nYou can work around the issue by using client.api.get_image('<repo>/<image>:<tag>') in the meantime instead.. I imagine you could add a thin proxy in front of the Docker Engine that filters requests based on the declared User-Agent in the request's headers.. In your call to remove_container, you're setting link=True, which is probably not what you're looking to do.\n        link (bool): Remove the specified link and not the underlying\n            container\n\nTry container.remove(v=True, force=True) instead.. You need to call reload() to update attributes: https://docker-py.readthedocs.io/en/stable/containers.html#docker.models.containers.Container.reload. Thanks for the PR!\nI want to investigate the issue further and see if we can actually fix the issue that is limiting the chunk size over a UNIX socket, but until then a doc call-out is a good stopgap.. Hi @adw1n , thanks for the submission.\nI'm not sure that's the right approach. Forcing stream=False seems hamfisted. If the problem is the one you describe, maybe we simply need a warning in the DockerClient documentation that when setting stream=True, the generator must be consumed for the operation to complete?. Sorry this took longer than it should have! I've made a few updates and merged the code in #2186 .\nThank you for your help and your patience!. Hard to say what the issue is without seeing your code - try making sure your environment is populated inside the celery process?. Thanks for the report!. You're using the APIClient but you're trying to call methods from the DockerClient. Please refer to the documentation.. All the information you see there is the information returned by the remote API; docker-py doesn't manipulate it whatsoever.\nYou probably want this:\npython\nfor container in client.containers():\n     print(client.inspect_container(container)). Looks like you installed an old version of the package: https://github.com/docker/docker-py/issues/1796#issuecomment-342569508. Duplicate of #1860 \nDuplicate of #1985 \nDuplicate of #1534 . Thanks!. You can't pass a list of commands to run, only a single command.. Sorry - it looked like you still had test failures, but they're due to https://github.com/moby/moby/issues/37920\nCan you rebase against the current master to pick up a3111d9e00c15f957af19f5bb5e301cc606f9aeb and clear the CI failures?. @little-dude Thanks!\nDesign-wise, I don't think there's ever a case where the data we receive from the server is in the STDIN channel. If we could remove that and go from a 3-tuple to a 2-tuple on demux, I think it would make the experience better overall.. Looks like Jenkins caught a bit of a cold over the weekend! I kicked the build again, but everything looks solid now.. Thanks! I'll give it another round of review.. Merged - thank you for your help and your patience!. Thanks for the report! This is working as intended: https://docker-py.readthedocs.io/en/stable/images.html#docker.models.images.ImageCollection.pull. What version of the library are you using?\nYour credsStore probably overrides the login you provided through the command. Try removing that from your config file and see if that resolves the issue (not saying the current behavior is correct though, just that it's probably what's happening). Thanks for the suggestion! However, unless that functionality becomes part of the Docker Engine API at some point, it is out of scope for this project.\nYou may be interested in the following projects instead:\n- https://github.com/davedoesdev/dxf\n- https://github.com/yodle/docker-registry-client. I'll open a PR when it's ready :) Thank you for your patience!. #2165\n. @AkihiroSuda That should work already: https://github.com/docker/docker-py/pull/2165/files#diff-2dd7e931899af21cd0ae5d3b517c0f9eR87. It's not supported in Compose yet. It will be in the next release (1.24) which should come out sometime in January.. > docker.errors.DockerException: Install paramiko package to enable ssh:// support\nDamn, sometimes tracebacks give useful information, imagine that.. The best thing to do in that case is to create a proper bug report with all the information needed for us to reproduce the issue. :). Sorry, you're in the wrong place. This is the salt repo: https://github.com/saltstack/salt. This library doesn't use the docker command-line; it's a frontend for the Docker Engine API. As such, anything specific to quay.io would be out of scope.. Thanks!. As described here:\n```python\nep_spec = types.EndpointSpec(ports={\n  8080: 8080,\n  80: 80,\n  5678: (1234, 'udp'),\n})\nclient.services.create('image', 'sleep 999', endpoint_spec=ep_spec)\n```\nWhen in doubt, our integration tests also tend to be a good resource for syntax questions like this one.\nHTH. Duplicate of #1772 . That feature depends on buildkit, which is very difficult to support in the Python library in its current state. We're aware of it and some work will happen to make that integration easier, but there's no ETA at the moment.. Duplicate of #1773 . Thanks!. Thank you!. The original design was that because it was the high-level client, the result (image object or exception in case of failure) was more important to the user than the logs - and if that wasn't the case, one could still use the low-level API.\nAs you remark, it's been a recurring theme on this tracker for a while now, and we've tried to compromise somewhat already by returning the logs after the operation finishes. Maybe we'll look at doing something better in 4.0.. Try using a Placement object. Sorry about that! You're right that it's poorly documented at the moment, and quite confusing. I've pushed #2197 which should hopefully improve things significantly.. You can't retrieve logs of an exec session you've detached from. You probably want to do the following instead:\n```python\ncontainer.start()\nresult = container.exec_run('script.py', stream=True)\nsomething else...\nfor chunk in result.output:\n  print(chunk)\n```. Sorry, I forgot to trigger it. Should appear in a few minutes. Thanks for pointing it out!. My bad, it's a file I've been working on and forgot to clean up when cutting the release.\nIt doesn't affect the rest of the code in any way and is otherwise safe to ignore.. Thanks for submitting a PR @originalgremlin !\nFWIW, the library already has a fairly established way to handle parameters added in newer versions of the API. see here or here for example.  Please make sure to adopt the same idiom for this PR so we can move forward!. Merged via #2219 - thanks again!. Thanks guys! Rebased :). I'm not sure what's implied by \"human readable format\", so I'm at a loss as to how to answer your question. . This library doesn't provide any formatting utilities if that's what you're asking.\nYou could look at how these streams are handled in Compose and reuse it or write something similar.\nHTH!. The following should help:\n```python\nfrom docker.utils import kwargs_from_env\nfrom docker import APIClient\nclient = APIClient(**kwargs_from_env())\n``. The image name needs to be fully qualified (providing an explicit tag), e.g.client.images.get_registry_data('nginx:latest').. What version of the library are you using? The issue you described was fixed in 3.0.0: https://github.com/docker/docker-py/pull/1865.container.attrs['State']['ExitCode']`\nMake sure to call container.reload() first.. Failure on 2.7 is unrelated to this PR and probably needs some dependency wrangling, I'll have to take a closer look.. Thanks! Can you\n\nRebase this against the current master branch (should take care of the failures you're seeing and the ones in the CI)\nAdd it to the documentation of APIClient.create_host_config as well?\n. Your container doesn't include bash. Try sh instead.\n\nAlso this is the wrong repo.. Thanks for the report!. Yeah, it's the same as #1772 - the error isn't detected at the moment the request is made, meaning it only appears in the output stream after the HTTP status has already been set. The high-level DockerClient implementation actually does that kind of error detection for builds, and that could be extended to apply to push / pulls as well. OTOH I don't think we should change the design of the low-level APIClient to process the output beyond what we're currently doing.\nAlso, as far as this specific error is concerned, the Docker Engine really should do some trivial validation and detect if a tag doesn't exist locally and send a 4xx HTTP response rather than attempt a push anyway.. The idea is to avoid the proliferation of properties that in turn need to be maintained and may appear in different locations in the attribute dictionary depending on the API version. As a result, we limit the number of quick access attributes to a few essentials. And to that point, I would personally disagree with the notion that a health attribute is essential; most containers do not use healthchecks at all.. Yes, this is related to #953. While the behavior for pull is consistent with the API's design, as explained in #953, it's obvious that in the case of run, only latest should be pulled if the tag is omitted.. > how come this bug is not even looked at?\nMaintainers have lives too. But also, it helps to include as much information as possible when reporting an issue.\nIn this particular case, this is not a bug; you simply need to call reload() on the network object to see the updated list of containers.. The list endpoint returns a shortened representation of the Network object, omitting the list of connected containers among other things. You can also use the greedy parameter to load the full data, eg\npython\n[n.containers for n in client.networks.list(greedy=True)]. This is an issue tracker for the Python SDK. Please use the forums instead.\nNote also that while there is no officially supported library for Nodejs, the community has published properly maintained, stable packages, such as dockerode that you could use.. Hi @Sanderhuisman \nI don't think that question is specific to the Python SDK. I would recommend using Docker's forums or the community Slack channel for help.. The most likely explanation is that your Dockerfile.tar file is either corrupted or doesn't contain the expected data. It's difficult to say for sure without knowing how the file was generated or what it contains, but this is unlikely to be an SDK bug.. [source]\n\nports (dict) \u2013 Ports to bind inside the container.\nThe keys of the dictionary are the ports to bind inside the container, either as an integer or a string in the form port/protocol, where the protocol is either tcp or udp.\nThe values of the dictionary are the corresponding ports to open on the host, which can be either:\n\nThe port number, as an integer. For example, {'2222/tcp': 3333} will expose port 2222 inside the container as port 3333 on the host.\nNone, to assign a random host port. For example, {'2222/tcp': None}.\nA tuple of (address, port) if you want to specify the host interface. For example, {'1111/tcp': ('127.0.0.1', 1111)}.\nA list of integers, if you want to bind multiple host ports to a single container port. For example, {'1111/tcp': [1234, 4567]}.. Assuming your containers are connected to the same custom network, exposing ports shouldn't be necessary. If you have to use the default bridge for some reason, you can do the following using the low-level API:\n\n\n```python\nimport docker\nc = docker.from_env()\nctnr_id = c.api.create_container('image', 'command', ports=[2222, (3333, 'udp')], ...)\ncontainer = c.containers.get(ctnr_id)\n```\nMore about the low-level create_container method here.. Hi @corynezin !\nPlease refer to the instructions in the CONTRIBUTING.md document:\n\nIf you encounter [a bug] while using the SDK, please create an issue in the tracker with the following information:\n\nSDK version, Docker version and python version\npip freeze | grep docker && python --version && docker version\nOS, distribution and OS version\nThe issue you're encountering including a stacktrace if applicable\nIf possible, steps or a code snippet to reproduce the issue\n. The current behavior is defined here: https://github.com/docker/docker-py/blob/master/docker/utils/utils.py#L343-L349\n\n\nYou may want to set DOCKER_TLS_VERIFY= (empty value) in your environment and see if that solves the issue.. I think that's due to DEFAULT_NUM_POOLS. 25 is fine for everything else.\nMaybe as a quick fix, we can hard-set the num_pools for the SSH transport to (9/10), then look at making it configurable later.. Another option would be to do that here with something like\npython\nif kwargs['base_url'].startswith('ssh://'):\n  kwargs['num_pools'] = 10. @ssbarnea Do you have an example of a NameError being raised on that particular code path that isn't caused by a missing dependency? \nMy problem with this PR is that I would still like to give clear information to the user in the case they haven't installed the optional paramiko dependency. As such, a better fix may be to catch a more specific error, or to catch it earlier in the callstack to avoid false positives. Making the error message more generic and lower-level is not the right direction, IMO.. Thank you for your contribution! That said, I fail to see the upside of that breaking change, especially if applied to a single method, when many of the method signatures in our codebase use the same shorthand.\n. It breaks the code of anyone using named parameters when calling this method.. It generally makes the code more explicit and better at self-documenting. . What version of the library are you using?. See the stream format documentation.. Unfortunately, select.poll doesn't exist on Windows, so this'll definitely need more work -- maybe by continuing to use select.select if we're running on Windows.\nI'm also curious as to whether there's a non-manufactured case where more than 1024 sockets are open on a client machine at the same time? That seems like a lot.. I'm a bit wary of adding a logger just for a couple warnings. Can we do without?\n. Hm, I'd rather we fix auth.resolve_config and auth.encoude_header so that they don't raise a KeyError.  They should be able to handle the absence of authconfig\n. except IOError: ?\n. I'm not fond of this, because it's very unclear (at least to me) when the root parameter is evaluated.\n. Adding another level of indentation feels unnecessary, especially since we already have a try..catch clause anyway.\n. I know it's \"pythonic\", but in that particular case it's just more indentation for the same effect. I'm all for following Guido's commandments but that's pushing it IMO.\n. Can you break it down so lines stay under 80 characters? That would make  the flake8 test pass.\n. This part could be rewritten as\npython\nelif fileobj is not None:\n    context = utils.mkbuildcontext(fileobj)\nelif path.startswith(('http://', ...)):\n    ...\nand save us an extra level of indentation.\n. It bothers me that there's no better way to do this.\n. Seems like throwing an exception would be more appropriate. Any particular reason you opted for a warning?\n. We should do something to warn the user if one of these is provided but the version is < 1.10\n. I don't think TypeError is right, I'd rather see us create/use a subclass of DockerException.\n. What happens if $DOCKER_HOST isn't defined?\n. Will that work? Shouldn't protocols be ('PROTOCOL_TLSv1_2', 'PROTOCOL_TLSv1_1', 'PROTOCOL_TLSv1') or am I misunderstanding this?\n. Can you expand on those changes here? It seems to me that both versions would do exactly the same thing, am I mistaken?\n. Should we use _stream_helper here instead?\n. I'm unsure what the use case is for this param, can you clarify what your thought process was?\n. Per https://github.com/tianon/docker-py/blob/fix-host-assumptions/tests/integration_test.py#L50, all of the timeouts are set to 5 - any longer probably indicates a problem with the test anyway. Did you have any particular issue with this, or just curiosity?\n. We won't need to change the tests for 1.6? compare_version doesn't work well with the -dev suffix, and we want 1.5.0-dev (and any version after that) to take the else path here.\n. Missing a colon here...\n. Let's pin dependencies in requirements.txt ; setup.py can have version ranges.\n. just extract that line from the inner block instead?\n. I'm a bit miffed by that. It doesn't seem right that we would have to make additional API calls when streaming content. I understand why it's done obviously, but I'd like us to try to see if there's an alternative that doesn't incure additional calls.\n. If that's how the docker client does it, I guess we'll have to do it as well. Let's merge for now - the fix is definitely needed. We can revisit later as @aanand said.\n. Let's stay consistent across the board and use format() instead :)\n. Maybe store this message in constants.py\n. ping @aanand :3\n. This is going to break if the dt parameter is unspecified. Not that it ever should - I say we remove the default value altogether, what do you think?\n. From Josh's answer, seems like the change only affects 1.7.0 and was reverted in 1.7.1 - making this PR unnecessary. Users of 1.7.0 should upgrade to 1.7.1.\n@moutten Can you confirm that this change is unnecessary with Docker 1.7.1 ?\n. Shouldn't it be default instead to maintain backwards compatibility?\n. Why are we setting this explicitly here if it's already handled in create_container_config?\n. It feels like that snippet of code belongs to create_host_config instead. What's the rationale for having it here?\n. But since the pre-condition here is that API version is >= 1.20, Docker 1.6 shouldn't be a concern.\n. Well, the user can decide to provide their own dictionary for host_config instead of using create_host_config, but at that point they are making the conscious choice of not perusing the library's validation and transformation features the library provides, which I believe docker-py should let them do without tampering. AFAIK the tests always use create_host_config.\nThe only drawback is that create_host_config doesn't have access to the client's API version. Not sure we have a way to work around that.\n. Yeah, but that's the default value you're assigning not just for the tests but every other project using docker-py. It should be default.\n. It's so that if the user doesn't provide an explicit value, we avoid creating a non-empty config in start (see https://github.com/docker/docker-py/blob/jhowardmsft-14530-netmode/docker/utils/utils.py#L431 - not passing None will prevent this field from being populated). It's kind of a hack, but the problem is that if a host_config is provided in start it will override the one provided in create_container, so we need to prevent that from happening (see 09a5eb8ae79dcba5c232728c1df3644e5bf72c71)\nThere's an argument for removing config from start entirely but I don't know if we're quite there yet.\n. Can you explain the reasoning here? Seems unnecessary.\n. I'd argue against adding this - we've deprecated passing host config arguments in start, and will probably remove it entirely in a future version.\n. Gladly! You can see the documentation on that topic.\n. Let's stay consistent with the other test class names, please.\n. Is it safe to remove this? Have you tried running make integration-test ?\n. It doesn't hurt to continue testing that one IMO\n. We should use the cross-platform compliant alternative os.path.expanduser('~') instead.\n. Why are we using a list to store a single value? Am I missing something?\n. Is there a reason we don't use the @pytest.mark.skipif decorator here as well?\n. Fair enough, I didn't want to use str() and didn't bother to look what was available in six :)\n. It's part of the TestRegressions class, which all simply reference the issue number they're testing for. But yeah, I'll add the test for the correctness of the user ID.\n. Sure. My reasoning was that those tests won't be read unless there's an issue (regression) happening, in which case the person examining it will probably go to the original issue anyway. If you think there's value in renaming them that's fine too.\n. Couldn't we make platform optional?\n. > best practices says to make these a single line, otherwise you can run into a situation where the update is cached without the available package you need to install\nWIll update!\n\ninstead of easy_install pip why not add python-pip to the apt-get install line? If you want a newer version https://pip.pypa.io/en/latest/installing.html#install-pip suggests their install script.\n\n\"Surprisingly\" (har har), python-pip is outdated/broken, so I'm using easy_install to get the latest version. I guess I could fetch their install script and use that instead, but that seems really unnecessary for what we're doing.\n. What's the upside to that method? Seems unnecessarily complicated for docker-py.\n. No, it's just unneeded, and I forgot to remove it - we already have Dockerfiles for building docker-py at the root.\n. yep! Already does!\n. Isn't an iteration on a list of size 0 almost a no-op? It feels to me like we would make the code more complex without actually gaining anything from it.\n. Mh, we can mention it. It's never been an issue in docker-py's case from memory.\n. I guess that's fair. I considered it, but since we're not really in the spotlight when contributor events happen I thought it wasn't necessary. \nBut it would surely make the project more welcoming to a broader range of people.\n. I actually decided against it when writing this document because I feel like the major part is irrelevant to what we're doing here. I'd rather refer to specific subsections like we do in the \"sign your commits\" part. Do you feel like there's one part of the docker/docker document that should be explicitly mentioned but isn't?\n. Good point!\n. Since this is exclusively for testing purposes, I'd rather see this file moved to tests/\n. This actually hides a race condition where the client container isn't able to connect to the server if it's started right away. A bit tricky to resolve.\n. Shouldn't we use the same test in both cases? At first glance it seems checking for existence of the _sock attribute would be more robust than basing it upon the value of a publicly accessible high-level attribute like Client.base_url.\nWhat was the intent behind making those different?\n. Understood - I meant if you remove that exec line, tests will all error out with \"Connection refused\" when first trying to contact the server.\n. Sorry, to clarify: I'm fine with the way it works now, I'm just wondering about the difference between this line and that one. I like testing for the existence of _sock better than testing the format of base_url.\n. Okay, gotcha. This is all sorts of awful...\n. It's going to be tedious to keep track of this if we ever want to change the image we use for testing. This doesn't seem essential, can we get rid of it? \n. ditto above\n. use self._url('/networks/{0}', resource_id=net_id) instead to avoid URL tampering\n. ditto\n. same thing about the use of format for this and the few other occurrences below\n. params = { 'partial-container-id': partial_container_id } would do the same thing here in practice.\n. That's a bit odd... Shouldn't the API do that kind of lookup rather than the client?\n. The other aspect is having 2 API calls in one method might break user expectations.\n. yup, I think I fixed it afterwards but it may be in one of the pending PRs. Let me find it.\n. https://github.com/docker/docker-py/commit/6cf7758f25de72a3f53528de0a376eadba902ae9#diff-193b0cbc51c3ae87519912dd6aeadec4R522\n. I believe @mnowster meant adding the line c.inspect_image(BUSBOX) at the end here so if there's been an issue during the pull, it will be detected immediately, making it easier to pinpoint why the tests have failed if it occurred. Having the pull logs in addition to that is fine as far as I'm concerned :+1: \n. I have no issue parsing nxt into next personally, so that doesn't bother me. \n@mnowster Is there a specific issue with those? Considering we're operating in python where len and hasattr and def are common keywords, I believe there's leeway for at least some abbreviation that still preserves clarity and readability.\n. Looks like this appears twice now. We probably only want to keep the second occurrence.\n. Since this doesn't need to exist inside the Client (doesn't use self), I think it should be moved to a function inside the docker.utils module instead.\n. Also, it would be nice to have it renamed to create_ipam_config to be consistent with the currently existing create_host_config and create_container_config. :smile: \n. Why parse_bytes here? Isn't it supposed to be a simple integer value between 0 and 100?\n. I'd be in favor of keeping it as RestartPolicy since it's the name of the JSON property. Better for API users transitioning to docker-py, and still legible/understandable by all?\n. The created image needs to be cleaned up after the test has finished running. self.tmp_imgs.append('buildargs') should do the trick.\n. Can we test that version_gte(version, '1.19') is True and raise an InvalidVersion otherwise?\n. Yep. It's still there :)\n. https://github.com/docker/docker-py/blob/remove-ci-transition-files/tests/integration/init.py\n. Yes, socket as a param makes a lot of sense. :+1: \n. This line is longer than 80 characters :(\n. Is this intended to be used anywhere else?\n. No real reason that I can remember. I may have been on the fence at the time about returning False if the operation doesn't complete in non-exceptional contexts, but I don't think we have any of those, so it's a moot point.\n. This line seems unnecessary.\n. return res should happen regardless of its value.\n. this should be moved to decorators.py\n. Doesn't look like headers is being used anywhere.\n. pool_configs or [] would allow us to get rid of the first two lines here. Small readability improvement.\n. This file needs to be added to the pages list in mkdocs.yml\n. Also, the other documentation files are written using \"you\" instead of \"we\". This needs to use the same perspective.\n. Can we reformulate that sentence? I would go with:\n\nYou can also create more advanced networks with custom IPAM configurations. For example, setting the subnet to 192.168.52.0/24 and gateway to 192.168.52.254:\n. How about leaving this here, and only adding requires('1.22') to the few methods where it's necessary?\n. Added :+1: \n. If utils.tar() is called on a folder that doesn't contain a Dockerfile, it would break without this change. It doesn't affect docker-py directly I think (we have some tar unit tests that break), but it's part of our public API and some people seem to use it: https://github.com/docker/docker-py/pull/865\n. Fair point.\n. Yeah, set(). Implicit type cast coming to the rescue obviously :)\n. You've got an extra space here making flake8 break.\n. Let's spell out \"version\" entirely + typo 1.22\n. Let's call those ipv4_address an ipv6_address. The additional characters are worth the clarity they provide, I think.\n. I'm torn on the necessity of having this function in the module namespace when people could simply call docker.Client.from_env()\n. I dislike the phrasing \"we recommend using Docker Toolbox\", because while I understand it's the product we at Docker want people to use, I feel it's irrelevant to docker-py in particular.\n\nCan we keep a more neutral approach here?\n. This can probably be a method of utils_test.KwargsFromEnvTest instead.\n. ah, yeah, good point.\n. Yes. I tested that case locally with the docker CLI, and setting DOCKER_TLS_VERIFY to empty string and DOCKER_HOST to a TCP address let me connect to an unsecured daemon, so I believe this change is correct.\n. Yes - this in accordance with what people reported in #984.\n. This file needs to be indexed in mkdocs.yml\n. I think this is only needed for python < 3.3 ( https://docs.python.org/3/library/ipaddress.html )\n. ipaddress is available from python 3.3 onwards. Change in #1039 is what I meant and seems to be working fine.\n. Should be buildargs\n. According to the documentation, \n\nThe result is a tuple of (hr, string/PyOVERLAPPEDReadBuffer), where hr may be 0, ERROR_MORE_DATA or ERROR_IO_PENDING. \n\nNone of those are actually \"errors\". MORE_DATA is irrelevant in the context of a socket emulation, and IO_PENDING only occurs when we use an overlapped buffer, which we don't in our case.\nOther (actual) errors will be raised as standard Python exceptions.\n. > The result is a tuple of (errCode, nBytesWritten). If errCode is not zero, it will be ERROR_IO_PENDING (ie, it is an overlapped request). \nSince we're not messing with overlapped structures, we don't need to handle this.\n. This is incorrect - it defaults to bridge for regular Docker Engine, but the default for Swarm is overlay. Please remove the second part.\n. We are not adding new parameters to start() - anything but the container ID is deprecated anyways.\n. Let's stick close to the engine's names and use userns_mode instead.\n. This sentence is superfluous and can be removed.\n. Let's use format instead to be consistent with the rest of the codebase!\n. Maybe this should be a constant\n. Late answer, but yes, that's intentional. Without it, _read_from_socket cannot work properly. As far as the user is concerned, the output will still be presented according to their choice.\n. What do you mean by normal? Both are valid:\nhttp://v6decode.com/#address=fd12%3A%3A82d1\nhttp://v6decode.com/#address=fd12%3A5672%3A%3A12aa\nThe brackets are conform to RFC 2732\n. We don't want to add any start arguments, as their use has been deprecated for over a year now. Do you mind removing those changes?\n. Yeah, I need to differentiate a runtime error from a missing entry in dockerpycreds, then we can raise when there's an issue, and proceed with an unauthenticated push/pull if we just don't have credentials.\n. Yeah, the token holds all the information. I had Matt Bentley test it with\nUCP yesterday and he confirmed it works, too.\nOn Wed, Sep 14, 2016, 8:44 AM Daniel Nephin notifications@github.com\nwrote:\n\nIn docker/auth/auth.py\nhttps://github.com/docker/docker-py/pull/1210#discussion_r78775816:\n\n@@ -189,12 +189,17 @@ def parse_auth(entries, raise_on_error=False):\n             'Found entry (registry={0}, username={1})'\n             .format(repr(registry), repr(username))\n         )\n-        conf[registry] = {\n-            'username': username,\n-            'password': password,\n-            'email': entry.get('email'),\n-            'serveraddress': registry,\n-        }\n-        if 'identitytoken' in entry:\n-            conf[registry] = {\n-                'IdentityToken': entry['identitytoken']\n-            }\n\nIt looks like auth is still required otherwise this function returns\nearly. Is that expected or could someone has an identitytoken without an\nauth key?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/docker/docker-py/pull/1210/files/be7d0f01844d5c08ee157446ce96f5bc6381507c..4b46b792eb7bfa0d7cc218e103f43b200f7752a9#r78775816,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABCVnBWNUR-2ZV3xd-2LWjpupwpmpkeLks5qqBZTgaJpZM4J8SKp\n.\n. Technically, either the client is using a cred store and all entries are empty, or they're not and none should be, so returning an empty dict here is not a big problem - but just in case weird hybrid config.json files start popping up, this should allow it to continue working. \n. ah crud, yeah it needs a continue.\n. This part is incorrect - we do not want to update the start method with new parameters. Especially in this case, healthcheck is part of the ContainerConfig, not the HostConfig, so it's not valid to have here.\n. You can use version_gte(version, '1.24') here, which is more explicit.\n. If you fix that it should solve the issue with the tests as well.\n. This should not be changed - people still need to be able to update containers on 1.22\n. There needs to be a version_lt check here for 1.23 instead\n. Alternatively, we could have the high-level client class be called something like Docker - that way it's clear this is the main \"entrypoint\" for the library, and we get to keep the Client name for the original low-level API client. I agree that since this is 2.0 however, backward-compatibility is a very minor concern. On the other hand I'm not sure how I feel about having two very different things called some variation of Client - I think that may be confusing to newcomers and current users alike.\nFor example, if they access an attribute which doesn't exist, but does exist on APIClient, it gives them a useful error message telling them to use client.api.\n\nThat would mean getting stuck with a nasty __getattribute__ override until 3.0, which I'm not really keen on.\n. nit: Combine both lines: from ..errors import ContainerError, ImageNotFound\n. Do we want to treat this as an exceptional occurrence? I don't have my mind made up on it yet, but I'd like to discuss the pros and cons of this versus returning an (exit_status, logs) tuple and letting the consumer handle the situation on their own terms.\n. It's too bad that taking a look at the source code won't inform users about the parameters that are available for that method. I understand that having to maintain a list of arguments in two separate files is a hassle to maintain, just wondering if there's another solution.\n. I think this also needs *args and **kwargs\n. I don't think taking an object-oriented (and inheritance-based) approach here is harmful at all.\n. I disagree with that way of doing things. It's extremely confusing and it's going to be a nightmare to maintain as the list of arguments evolves. I understand the rationale for wanting to flatten the argument list to avoid asking users to build 2+ separate objects before calling create, but I really think we just need to bite the bullet here and recognize that the structure of a service object is complicated and should be manipulated as such.\nAlso this doesn't allow for the user to specify resources, restart_policy, placement and logdriver for the TaskTemplate.\n. I'd suggest changing the wording to\n\nWhile not imported explicitly, the ipaddress module is required for [...]\n. https://github.com/docker/docker-py/pull/1204\n. You can use any version >= 2.5.2, except 2.11.0. \n2.11.1 fixed the bug so that version or above is fine as well.\n. Yes - thank you for catching this!\n. I feel like this could be rewritten in a more intuitive way. I feel like n should not be reset inside the loop that checks for its value. Here's what I would suggest as an alternative:\n\npython\nwhile True:\n  n = next_frame_size(socket)\n  if n == 0:\n    break\n  while n > 0:\n    result = read(socket, n)\n    n -= len(result)\n    yield result\n. There's a \"user guide\" part that we'd like to preserve I think - let me move the relevant bits to a different folder and we can figure out converting to RST / where to include at a later time.. Maybe we should have attachable=None here, and only accept either value (True/False) if API version >= 1.24?. Only add the Attachable field to data if it's != None and API version >= 1.24. Missing a colon. dind doesn't start properly when inheriting ZFS from the host, IIRC.. You can just use None directly.. Related to https://github.com/docker/docker/issues/29192 which I mention in two other places - could add it here too.. It was overzealous - we don't really care if there's an error during the tearDown. I had to change it because the 1.13 RC seems more unstable, as far as Swarm mode in dind goes at least - this would regularly raise a 500 error when running the integration tests.. Docstring should be updated with a Returns: section.. We may not want to do this for API < 1.24 depending on what the content is befor engine 1.12.. Can you add this to dockertypes_test.py instead?. Please make this into 3 separate tests (3 methods) instead. I don't think str is a valid type here, is it?. This is broken in the case of Windows host paths. ah, nevermind, we just add it to a one-element list if it's a string. Oops!. I believe this needs to be an argument in create_host_config, not create_container. Should be requires_api_version('1.25'). Minimum version is 1.25. This needs to be removed, Init is not part of ContainerConfig. 'true' is not a valid boolean. This test would fail, but the decorator prevents it from being run in the first place.. Unnecessary. Unnecessary. It's not enough to test the presence of the item,  you need to test the value you provide is the same as the value you're getting back.. Additionally, the name of the field is Init, capitalized.. Minor: this should be if init is not None:. If the user explicitly specifies init=False, we still want to enter that branch.. Thanks - indeed! I'll fix it in a follow-up PR :). Please replace with:\n\nNumber of usable CPUs (Windows only).. Please replace with:\nUsable percentage of the available CPUs (Windows only).. Let's conform with the API here and use\nnano_cpus (int): CPU quota in units of 10-9 CPUs.\n. Let's have nano_cpus be the parameter instead, limit the acceptable types to int and avoid float uncertainty altogether.. What purpose does or [] serve here? \"Env\": None should be fine.. Actually, since this is not an API call, it should probably be added to the APIClient class instead.. reload_config returns None, so this is probably not correct.. Same here. nit: \"Force\" instead of \"Forces\". If we're going through all events anyway, the conversion to list we do above is wasteful. We should iterate on json_stream(resp) directly.. maybe we could interrupt the loop on any event that has an error key.. What's the minimum API version for this feature? Please add a version check similar to the other options in this method.. We only update the ChangeLog at release time. This should be removed.. Please add a @requires_api_version decorator with the minimum API version for this feature.. This might make more sense as a unit test, since we really just want to check that the method raises an error when passed invalid arguments, creating an actual container or connecting to the Docker Engine is unnecessary.. nitpick: It feels like there should be a way to have this raise statement only once, but it's not immediately obvious to me what is the best way to get there.. Looks fine to me.. Rather than make an additional call to the API, we should be able to determine the driver from kwargs['log_config']. We should set this to None instead, and explain the behavior in the docs.. This change seems unrelated to the issue at hand. Did you run into an issue with this specifically? format should work fine with None values.. This probably needs to be updated as a result of the changes.. @thaJeztah It always comes down to the same issue - the API direly needs error codes if we want to be able to handle these things gracefully. Until then, I'm afraid hardcoded values are the best solution here. Especially in this case, run's purpose is to be a convenience method that imitates docker run - for more fine-grained control, people will still be able to call logs on the container directly and handle the possible exception themselves.\n\nAs for logs over attach, the reason is 2-fold:\n1. We don't have an interactive mode, so both are functionally equivalent\n2. Historically logs works better than attach. No, it's all good, I was just curious. Thank you!. The _result method includes this check, so this line can be removed. This will conflict with the changes in #1657, which is a more thorough fix. This is also addressed in #1657 and can safely be removed.. We may be able to add a check for an {'Deleted': img_id} entry in that list.. I'm not sure about this, because it feels somewhat brittle, and conceptually it doesn't feel like it should be the responsibility of the ConnectionPool class.. It also doesn't handle exec_start(). Yup, test should be removed altogether if naming isn't supported.. init_swarm is called here: https://github.com/docker/docker-py/pull/1829/files#diff-5045b9ca2e2c5acfebac3a4ce041b23aR116. This should be None by default, otherwise any client using API version < 1.29 will fail unless the user explicitly unsets it.. Can we rename this to fetch_current_spec, so it's clearer that this will result in an additional roundtrip to retrieve the spec?. I'd suggest s/previous/current to keep the terminology consistent here.. nit:\npython\ninspect_defaults = None\nif utils.version_lt(self._version, '1.29'):\n  inspect_defaults = True. We can also shorten these statements to\npython\ndata['Name'] = current.get('Name') if name is None else name. I'd like this to be a module-level function like check_api_features, rather than a static method.. Instead of adding more conditional branches, I'd rather update this message to say \"datetime or positive int. What do you think?. Let's add this test to theAPIErrorTestclass instead. :). \ud83d\udc4d Good catch. This test is failing because the defaultmodefor a service isglobal. Make sure to set the initial mode toreplicated!. The problem here is that you're making the assumption that the only reason aConnectionError` could occur is because of an incorrect path issue. This will result in a lot of \"false positives\", misidentifying the issue and misleading the user.\nThe better fix would be to figure out why the ConnectionError is raised in the first place, and fix the cause. . I understand you were looking to follow the same pattern as with cpu_ and mem_, but I don't see a scenario where \"generic limits\" becomes a thing (especially because the Docker CLI already uses --generic-resource for this), so I think we should rename this to generic_resources. I'd like to allow users to provide this argument in list form (i.e. already parsed) as well ; we just need to have a isinstance(generic_reservations, list) that bypasses parsing.\nThis is useful for people grabbing the resources data by inspecting another service, for example. . Yeah, CLI-like notation is out of scope for this project ; exposing the original API format when possible is something we do because it allows people to reuse inspect results and gives them a workaround when new things make it into the API until we can implement them at the SDK level.. We should check that we're able to retrieve this data in res_template below (data sent by the server). Also it's fine to make a separate test for this.. Oops! I wasn't super clear in what I meant, sorry!\nI actually think we can have both notations, the dict form that you implemented originally with the list form as a fallback. See for example https://github.com/docker/docker-py/blob/master/docker/types/containers.py#L242-L246. '!' + '/*.py' can probably just be '!/*.py' now. Does this mean that ../xxx becomes xxx? If so, does it match the behavior of the engine?. More generally, having some tests for .. components would be nice . nit: split (or better yet, split_path). does patterns need to be a list? It looks like walk only ever uses it as an iterator.. We're using a modified fnmatch implementation that should handle ** patterns - did you find that not to be the case? If so, maybe we can get rid of the utils/fnmatch.py file as well and use Python's fnmatch module instead.. I think restart doesn't accurately represent the operation that's being performed here. In the Docker ecosystem, restart usually means \"take the existing containers with their configuration and stop them then start them again\". The same idea exists in other common software like nginx with the reload vs restart distinction. \nTo my understanding. in this case, the containers for the service will actually be recreated with the same configuration - maybe force_update or reload is a more accurate name?. Can't we simply call self._response.close()? http://docs.python-requests.org/en/master/api/#requests.Response.close. I think we can get rid of this, and simply always return a CancellableStream, provided it behaves the same as the regular stream.. maybe we can have a timeout of some kind, so the test doesn't stall forever if we accidentally break something in the future?. Can we add __next__ as well?. Cool, I'm fine with the new implementation if it works better than the old one :)\nCan we remove utils/fnmatch.py from the tree in this case?. Ok - it probably doesn't see a lot of use anyway. I say we keep it as is and we can always revisit it later if it's an issue.. can we rename the parameter to uts_mode across the board? That way it matches the API name, is more explicit, and if more uts-related parameters are introduced in the future, they'll be easier to integrate.. Can we add a version check here as well? I believe this was only introduced recently.. Can we have an integration test as well? They're a much better indicator of things working as intended for this kind of thing.. yup, my bad, looks like it has existed for a long time now.. This change concerns me because it means we're losing any logs that happen after the method is called. I don't think that is correct.\nWe might want to use attach instead?. That change looks fine to me at a glance yeah.. Done in utils.parse_host: https://github.com/docker/docker-py/pull/2165/commits/f302756599a61d6775fbdf2beab8f1de7e0022c4. Since this is the only place socket_raw_iter is used, maybe we can just make that change inside the function instead?. Should we rename this to next_frame_header or similar?. This method as well as _demux_adaptor should probably be in docker/utils/socket.py. Not sure this is the best place for such a long example, especially since it applies to attach() as well.. Maybe as a separate docs page. We can figure that out later though, it's not worth blocking the PR.. nit: now just (stdout, stderr). -stdin. 2-tuple*. Should we raise an exception if someone tries to use demux and tty together?\nLike https://github.com/docker/docker-py/blob/master/docker/api/container.py#L1101-L1104 but in reverse.. Do you mind reverting this? I don't think the additional verbosity is necessary in the CI. :). same thing for this file.. I know that this makes sense because of the behavior of update, but it feels intuitively wrong. Maybe do this instead:\nsuggestion\n        proxy_args = self._proxy_configs.get_environment()\n        for k, v in proxy_args.items():\n          buildargs.setdefault(k, v). I also think this should be disabled by default to avoid introducing breaking changes, and add a parameter use_config_proxy to enable it. Then once we do a v4 we can set it to True by default.. typo on \"environment\" here. I feel like you'd save a lot of effort by writing this as a ProxyConfig(dict) class and just adding a few @property decorators instead.. It doesn't look like you're handling URL-scoped proxies here?. Does this need to be removed?. nit: I like to have the closing parenthesis on the next line so it's easier to tell at a glance where the \"block\" ends.. The Docker CLI doesn't inject the proxy values in exec calls (intuitively they're probably already injected at the time the container is created), so I'm torn on whether it's a good idea to have the option to do so here.. I'd just remove the setters as there's no reason to modify the object IMO. Good point \ud83d\ude05 . should be a func call(). specifically I think this should be assert exc.value.is_error(). Removing .response should clear up the CI failure. @ulyssessouza https://docs.quantifiedcode.com/python-anti-patterns/correctness/mutable_default_value_as_argument.html. ",
    "pplante": "What about using shlex (http://docs.python.org/2/library/shlex.html) thats built into python to parse the command if it is a plain ole string?\n. ",
    "kencochrane": "Thank you.\n. Thanks.\n. Thank you\n. LGTM\n. LGTM\n. LGTM\n. ",
    "keeb": "Here's the error..\n```\n\n\n\nfrom docker import client\nc = client.Client()\nc.build('Dockerfile')\n2013-06-23 19:06:32,988 ERROR Invalid Dockerfile format: \"D\"\n(None, 'Invalid Dockerfile format: \"D\"\\n')\n``\n. RTFD and Code.. This is not an issue.\n. Can you please add a repeatable test case? Creating a container works fine for me in 2.7.\n. No problem! Happy it works :)\n. I've started working on a unix socket patch torequests` which will allow this to work.\n. Some minor notes so far. The goal will be to reuse as much of the HTTPAdapter as possible and only patch in the places that need to change. Requests is pretty modular, but has pretty complicated connectionpool logic.\n\n\n\nrequests.Session has a mount method that allows for custom protocol handlers to be registered with an associated adapter. \nMy thinking is that inheriting and initializing the requests.adapter.HTTPAdapter and overwriting the get_connection method is the least invasive way to do it.\n. See attached patch for a WIP: https://gist.github.com/keeb/bf39ef50677d4b2ae3dd\nThe biggest hurdle right now is that there is no reliable way to separate a unix socket file from the rest of the parameter string. Separating the two would mean that we'd have to change the interface, which is something I would like to avoid if at all possible.\nThis would be trivial if we did not use requests but instead used httplib directly.\nThoughts?\n. @kstaken - it's a little backward, but the problem is the URL you're passing in to the client. It should be unix://var/run/docker.sock (notice, only 2 /'s)\na simple test is\nimport docker\nc = docker.Client(base_url=\"unix://var/run/docker.sock\")\nprint c.version()\n. Patch incoming..\n. This fixes #27 \n. ",
    "ehazlett": "This is wrong.  Closing.  I'm an idiot :)\n. You are most welcome!  Thanks!\n. I'm having the same issue.  I've tried all sorts of combinations as mentioned in the readme but cannot get any host mappings.  Using master at 6fd343d76ad3\n. Ok this was a config error on my part.  It is working based on the docs as of 6fd343d.  Thanks!\n. ",
    "ricobl": "Nice! You're welcome! :)\n. Great! Thanks for the quick fix.\n. I've forked the project and the solution is almost done. I'll send a pull request soon.\n. Sure! I'll make the changes right now.\nSorry for the mess, most things should really be in separate pull requests.\n. Done!\n. Awesome!! Thank you! \n. ",
    "srobertson": "That's how I saw it done in the go code base, but since I'm not passing stdin at the moment it's not needed.\n. just an FYI, I see this error when I use an older version of requests\n. Btw this was the discussion I was reading\nhttps://github.com/kennethreitz/requests/issues/1847\n. I finally got this to work with code like this:\n``` python\nif not DOCKER_TLS_VERIFY:\n    tls_config = False\n  else:\n    cert_path = os.environ.get('DOCKER_CERT_PATH')\n    if cert_path:\n      ca_cert_path = os.path.join(cert_path,'ca.pem')\n      client_cert=(\n        os.path.join(cert_path, 'cert.pem'), \n        os.path.join(cert_path, 'key.pem')\n      )\ntls_config = docker.tls.TLSConfig(\n  ssl_version = ssl.PROTOCOL_TLSv1,\n  #ca_cert = ca_cert_path,\n  client_cert = client_cert,\n  verify=ca_cert_path,\n  assert_hostname=False\n)\n\nclient = docker.Client(\n    base_url=base_url,\n    version='1.12',\n    timeout=10,\n    tls=tls_config\n  )\n```\nThe trick is to set  ssl_version = ssl.PROTOCOL_TLSv1 when creating TLSConfig\nI think this should be done automatically for us devs. Maybe as part of  #359 \n. ",
    "shykes": "LGTM\n. Yes, that would be much better. That will need to be fixed upstream, in the\ndocker remote API. Pull requests welcome :)\nOn Mon, Jul 22, 2013 at 7:04 PM, Roberto Aguilar\nnotifications@github.comwrote:\n\nWhen calling Client.containers() the return value for Ports is a string\nthat looks like:\nu'Ports': u'5000->5000, 6000->6000'\nFor programmatic consumption, the value would be much more useful as a\nsequence of (public_port, private_port) tuples, like:\nu'Ports': ((5000, 5000), (6000, 6000))\nThoughts?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-py/issues/23\n.\n. Yes that would be useful, thanks. Note that we support UDP ports also now,\nso maybe the pairs should be triplets instead:\n\n((\"udp\", 5000, 5000), (\"tcp\", 5000, 5000)) etc.\nOn Sun, Aug 4, 2013 at 9:19 PM, Roberto Aguilar notifications@github.comwrote:\n\n@shykes https://github.com/shykes I may actually have some time to look\ninto this upstream; is it still something that should be looked at?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-py/issues/23#issuecomment-22086416\n.\n. \n",
    "jaredly": "? doesn't the remote api give you a string anyway? Would you rather they give you json? b/c it seems like you could just as easily do [pair.split('->') for pair in ports.split(', ')]\n. ",
    "rca": "That's almost exactly what I'm doing right now; I also int() the values.  The API should simply return integer pairs and leave string formatting to the user.\n. @shykes I may actually have some time to look into this upstream; is it still something that should be looked at?\n. Just opened up PR #76 for this issue.\nThanks!\n. Here's what I learned while working on this feature.  Note, my understanding of how things work under the Docker hood are still a bit fuzzy.\nMaking ports available on the host system is a two-step process across two function calls.  The ports argument in create_container() exposes the port within the container, but does not setup a port on the host.  The port_bindings param in the start() method does that.\nI think the client would need a mechanism to maintain state across these two calls in order to maintain backwards compatibility.  This may also introduce some magic; what if you really just wanted to expose the port in the container without making it available via the host system (I believe this is what the links feature does).\nThe docker run command gets away with backwards compatibility because it performs the create and the start action atomically.  The python client doesn't have such call.\nMy take is that backwards compat is doable, but there's some wiring and potential unintended side effects.  What is the general user expectation here?  Personally, I know I'm an early adopter and have subscribed to getting the rug pulled from under me.  But I do appreciate keeping the rug pulls to a minimum.\n. I agree with @denibertovic on the backwards compatibility issue.  Put effort into backwards compat once a stable version is released.\nFrom what I see, there is are separate create and start methods to have parity with Docker's internal API.  Docker's run command is actually calling these two for you.  Maybe docker-py could have a  run() convenience function that emulates the behavior.\n. @mpetazzoni Sure, no problem, but can you elaborate on the style error?  Also, going to ensure this is the right direction first.\n@denibertovic If this was done intentionally, then no problem.  I just went to the README and see that the format is well-documented, so my fault for not reading!  :)\nI agree with having one known way of doing things; close at your leisure.\n. Actually, what if the else statement is changed so that it will fail if the binding param is not a string or int.  A dict should not be stringified and used as the port; I spent a good amount of time figuring out why my port bindings stopped working.\n. Is silently doing the wrong thing better?\nI realize that the library is under heavy dev, so backwards incompatible changes are to be expected (and the docs clearly describe the parameter format), but just to illustrate, I ended up with a stringified dict as the HostPort setting, which is clearly wrong.  And Docker upstream, as far as I can tell, silently ignored the port value and simply assigned a dynamic port.\nAnyway, not trying to be a pain here, just describing the issue I ran into.\nThanks!\n. @denibertovic What are your thoughts on something like this?  I suppose you'd still consider this defensive coding, but do we agree that HostPort must be numeric?\n. Yes, it can raise a ValueError and TypeError, but I'm not processing that in the actual code, just using it in the test case.  I added a test that asserts the ValueError, just in case.\nHere's the binding that brought us here today.  Feed this into def _convert_port_binding(binding):\npython\n{'HostPort': '49159', 'HostIp': '0.0.0.0'}\nAnd get this back:\n{'HostPort': \"{'HostPort': '49159', 'HostIp': '0.0.0.0'}\", 'HostIp': ''}\n. @denibertovic And to answer your question, yes, port randomly assigned and IP ignored.\n. Unfortunately I've been force updating but the initial patch checked to see\nif the param  was a dict and had the right keys.\nI can resurrect the patch, but it will be Monday before I can do so.\nLet me know.\nOn Friday, January 10, 2014, Joffrey F wrote:\n\nNot too sure about this, let me play with it a bit first!\nIdeally, if we can accomodate both ways of doing things (i.e. passing\narguments in the official CLI format + passing arguments in the python\nfriendly format) I'd like us to do that.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-py/pull/135#issuecomment-32036414\n.\n. \n",
    "mvanveen": "I've got a demonstration of this working in a lib I'm working on.\nIt returns a list of tuples instead of a tuple of tuples, but that's a quick fix.\nI can submit a PR for the ((5000, 5000), (6000, 6000)) approach this week if people are interested.\n@shykes what version of the Docker Remote API exposes both TCP and UDP?  Is this new version published, and does it have an updated output format?\nIn my lib, I can branch behaviors based on the version specified in the Dock object (currently just a Client wrapper class). \u00a0Perhaps something similarly polymorphic could be done in the core library.\nHere's the output of my docker version for reference:\nClient version: 0.5.0\nServer version: 0.5.0\nGo version: go1.1\n. If you run the inspect_container method on docker.Client objects, you'll get a dictionary with a lot of interesting metadata for a given container.  Off of the status Status key is:\npython\n{u'ExitCode': 0,\n u'Ghost': False,\n u'Pid': 4388,\n u'Running': True,\n u'StartedAt': u'2013-08-13T03:26:54.198907319Z'}\nThis is the only way I know of to easily get at the ExitCode of the docker command you've specified, as well as details like whether or not the process is currently running.\nI totally agree with @adrien-f and think this would be useful information to have on-hand when you run start, stop, etc.\nI think it may be sufficient and useful to just return ExitCode as an int rather than a full payload, as that ExitCode int is pretty canonically understood, and immediately knowing about a non-zero return code is a pretty useful thing to have off hand in a lot of use cases.\n. ",
    "jakedt": "It works after today's upgrade to 0.1.2, I was still running the dinosaur release 0.1.1 from this morning.  Thanks for the quick response.\n. When will a release with this fix appear on pypi?\n. I am running into this too, however, when I run the same exact commands individually through ipython, the command blocks, (even with stream=True) but it ultimately succeeds. Is streaming disabled if the session is interactive or something?\n. I haven't changed the non-streaming code path at all, but I have just tested the things you asked and they work as expected.\nThe total list of things that I tested is:\nstreaming logs\nnon-streaming logs\nstreaming build\nstreaming push\n. The Id is ID and it's also missing. Additionally, the format for Repository is different and is now a list in RepoTags.\n. I don't think it has to be O(n^2), but it would require a custom json parser. There's no reason a properly coded parser couldn't take in chunks of text while tracking how many \"closing brackets\" it's waiting for, and emitting an object when it receives a complete properly formed json object, while preserving the \"remainder\" in the buffer for the next object.\nMore concerning to me is that iter_chunks wasn't working regardless of how small I made the chunk size. Does this mean that the daemon is delivering all results as a single chunk?\n. ",
    "kstaken": "How far along should the patch get? I get the error below but I'm guessing maybe that's expected at this stage.\nrequests.exceptions.InvalidURL: Invalid URL u'unix:///var/run/docker.sock/v1.4/containers/create': No host supplied\n. @keeb the URL came from your patch. The default base_url :-)\nAnd wouldn't 3 slashes actually be correct anyway? unix:// is the scheme and then the path is /var/run/docker.sock although it seems maybe that wouldn't work with requests.\n. @keeb do commands other than c.version() work for you? \n. oh, I see _extract_path is just hard coded to return /version at the moment.\n. ",
    "denibertovic": "Great. So we'll go with option 3. Support dicts and strings as well. Awesome. I'll get on that this weekend and submit a pull request.\nRegarding the docs. I'll come up with something and send you a pull request so you can see if it makes the matter more clear. Thanks. :)\n. For consistency should we also make the following commands also accept dicts?\nThey are all the same as start/restart/stop in the sense that they expect a container's id but \nit would be nice to be able to just pas in the whole dict from create_container.\nI hope i didn't miss any.\ninspect_container\nkill\nattach\ndiff\nexport\nport\nwait\n. I've sent a pull request  #40 \nI was thinking about making a decorator that would handle this but came to the conclusion that would not be really readable. Just checking for a dict and doing the appropriate mapping seemed really straightforward and readable.\nRegarding the docs. I've just changed the plurals where necessary. That should be fine I think. Also I've trimmed the trailing white space, couldn't help my self. :) Hope that's fine.\n. > Is it though? I am of the mind that making sure the library properly starts containers/creates images/etc. is more ? important, which is why I went for functional tests in the first place.\nThe way I see it docker-py does not start/create containers, docker does. The moment we test that we get a valid reply from the docker API we can conclude that the container is indeed started. Same goes for receiving an error from the API. \nThe more I think about it the more I don't really see mocking out the REST API as that huge of a time sink. Because really the API isn't going to change that much after the 0.8 release I'd say. If i remember the road map correctly.\nAnd maintaining it basically means just making sure that the JSON it returns is up2date with the real API.\nThat being said, I would still prefer if we leave the Integration tests in place just not call them automatically in tox and on CI tools. We could however make sure to run the integration tests manually before releasing a new version on pypi for instance. \nI have some time to spare and could do some work on this.\n. :+1: i'll let you know as soon as I have something. probably something this weekend\n. I've been holding off on this partially because of time constraints and partially because I wanted to wait for 0.2.0 to hit master. \nI might have this by the end of the weekend cause I've spent some time experimenting with the best approach.\n. is it just me or are the tests on the current master not runnable?\nI get AttributeError: 'UnixHTTPConnectionPool' object has no attribute 'strip' for all 31 tests.\n. ah yes...requests 2.0 was released a few days ago... :/\n. Anyway, I'd say I'm half way there... A lot of stuff (assertions) are not needed in the unit tests so I need to sift through one more time.\nI going to try and have something on my test branch tonight so you can take a look.\n. So I have an initial structure of how I wanted to do this. (Please see the 'test' branch on my fork).\nI wanted to keep things as simple as possible so I didn't want to introduce any new dependencies for testing but rather just monkey patched the methods that communicate with the network (ie, post, put, delete, get).\nEach method looks up a fake api response based on the url it's accessing (cause the url should be unique i think). Each fake api response returns a status code and some content. And what's most important each fake api response is just a simple function. I know I could have done some magic and made a generic response API but I wanted each  API call to have it's own function so we can easily change them to do what ever we need and so that's it's easy to add new ones. Also, we can just as easily have a separate function mimic a bad API response so we can test that the lib handles errors well.\nThe way I see it the content that the fake API's are returning is irrelevant, we only need to test that each wrapper function of the Client executes without raising any Exceptions before it does the network call.\nWhat's still left to do is to check that each API request is called with exactly the parameters we expect, so that's one more assertion i want to add to each test. This way we know that the wrappers did not raise any exceptions and that they constructed a valid request for the API. \nThen all it remains is to make the tests work with python 3.x (the current integration tests also fail py3x).\nPlease take a look and tell me it you think this approach is fine?\n. Although now that I think about it the Mock library is in the standard library in python 3.3 so it might be worth including it for the previous versions.\n. It seems i need mock to assert that the params are being constructed correctly, so I'm refactoring the tests to use the mock lib. Should be uploading shortly.\n. @shin- I've updated the test branch on my fork. Could  you take a look and give suggestions?\nWhat's left, in my opinion, is maybe some cleanup, and adding a few missing tests, and then getting the tests to run on python 3.x.\nOnce we agree that everything looks okay I can squash the commits and do a pull request.\n. @shin- weeee :+1: \n. Argh...I've even checked the generated Readme to see if it looks good but missed it somehow. I'll fix this and update the pull request.\n. Updated. Sorry about that. I'll be sure to increase my daily coffee intake. :)\n. Done. Sorry for the delay. timezone difference. :/\n. I'm not a fan of there being 2 places for defining dependencies... Is there some way that we can tell tox not to use setup.py but rather the environment it already has (ie. the virtualenv it's in, the docker/ folder it already has in git and so on) ? And reference the requirements.txt file in the setup.py...\nThis way we would only need to worry about the requirements.txt file right?\nAlso, should we fix the requests version of try and update to the new one? \n. This seems to introduce another error seeing as now tox won't start and gives the following error:\nIOError: [Errno 2] No such file or directory: './requirements.txt'\n. Figured it out. Just need to modify the tox.ini file a bit.\nWill make a pull request.\n. added pullrequest #48 here.\n. Linking issue #35 her as github failed to parse it from the pull-request title for some reason.\n. Awesome! :+1: \n. LGTM\n. @shin-  I didn't wan't to bring anything up in this pull request as it's offtopic but I was wondering do we really need a create and start method? Can't we just emulate the run method from the cli and make that command create and start the container?\nWhat are the benefits from having 2 separate commands for this? (I'm unfamiliar with the early design decisions so i might me talking nonsense :))\nAlso, do we really need to worry all that much about backwards compatibility before docker 1.0? Because, odds are there will be more breaking changes, and realistically people will upgrade to the newest versions (they can always hold of on upgrading, and user frozen versions of docker and docker-py until they are ready to upgrade). I see no point in complicating/cluttering the codebase if 1.0 if considered somewhat a stable(ish) release and everything before that is just something that's building up to that release. \nI vote, that we consider maintaining backwards compatibility after docker 1.0, or even better we follow docker as is. Meaning, when docker breaks backwards compatibility so does docker-py (at least before 1.0).\nFeel free to correct me if I'm wrong, I'm just thinking out loud here... \nP.S. I can open a separate issue for this so we can discuss it further.\n. @shin- Regarding run (create, start) I agree that seeing the remote API is designed this way that we should ad hear to that. And yes, it's fairly easy to wrap that in a single run command in my own code, if need be.\nRegarding backwards compatibility, I'm thinking that if people upgrade docker on their systems then they will also upgrade docker-py...if they continue to use and older version they can use the freezed version of docker-py that works with that version....I don't see a scenario where they would wan't to use the newest docker-py but use an old docker version on their system (at least before 1.0). Yes I agree that we should strive not to carelessly break backwards compatibility but I would argue that docker is in a state of flux before 1.0 so I'm not seeing any benefits from maintaining backwards compatibility for version that people will be upgrading soon anyway, and stop using them. I may be missing some edge cases though...I don't know, In my mind everyone  updates when docker releases the new version :D\n. @ureyes84 When using the docker CLI the run command has a -c flag that's used for setting CPU shares. \nAs for docker-py. the lxc_conf param in the Client.start method expects a list of dicts like so: \n[{\"Key\":\"lxc.cgroup.cpu.shares\",\"Value\":\"1\"}, {\"Key\": ..., \"Value\": }] \nAnd, both methods (CLI and docker-py) set that correctly in the lxc config (you can verify this in /var/lib/docker/containers/{container_id}/config.\nYou can test with:\nclient.create_container('ubuntu', '/bin/bash -c \"dd if=/dev/zero of=/dev/null | dd if=/dev/zero of=/dev/null | dd if=/dev/zero of=/dev/null | dd if=/dev/zero of=/dev/null\"')\nand then:\nc.start('04095d4fc948', lxc_conf=[{'Key': 'lxc.cgroup.cpuset.cpus', 'Value': '1'}])\nNote that the restriction we use to restrict the number of cores to be used is 'lxc.cgroup.cpuset.cpus'\nand not lxc.cgroup.cpu.shares (this one does something different and depends on the current usage of the CPU, ie. \nif nobody is using their shares the one container will get all the cores to it's disposal until some other container\nneeds it's shares.). \nI hope this helps.\n. You are right about the docs. And tnx for the pull request, I've commented there.\nRegarding cpu.shared, you are right, it's suited for giving priority to certain containers by setting a higher value.\n. I think we can close this.\n. @ureyes84 you should disable the autotrim feature of your editor for markdown files. The spaces at the end of the lines are actually required for new lines (I did this not so long ago XD)\nThis needs correcting but other than that it looks fine.\n. LGTM!\n. I think it wold be nice to test this part in the integration tests.\n. I propose we use sphinx and host the docs on readthedocs.org (but i'm open to other suggestions).\nI think I'm going to have some time this weekend to work on this.\nIf anyone has any ongoing work in this regards, now's the time to speak up. :)\n. @shin- can we close this issue?\n. I was thinking about this a little.\nRegarding ports...I would rather we use some other data\nstructure than list of strings....for instance list of tuples for the complex case.\nExample: \nCreate_container simple case: \nports = [22, 80, 123] \nJust a list of ints for the generic version.\nTCP is implied as on the CLI but we can change port to udp like so:\nports = [22, 80, (123, UDP)] \nTCP and UDP can be exposed constants so we avoid having to enter strings manually\nSame applies to the start command.\nIf it's just an int it implies that it needs to be exposed on all interfaces\nbut if it's a tuple then the first item is the interface and the second is the\nport.\nLike so:\nstart_container:\nports = [22, ('127.0.0.1', 80), ('127.0.0.1', 123)] \nNot sure if we should make sure the IP's are proper, or just pass the string to\nthe API?\nVolumes seem a little more straightforward:\ncreate:\nvolumes=[\"/volume1\", \"/volume2\"] \nstart:\nvolumes=[(\"/volume1\", \"/target/volume1\"), (\"/volume2\", \"/target/volume2\"]\nWhat do you guys think? Is this something you see as more usable?\n. I did some inline comments... hope it helps.\n. This can be closed i believe since it's been merged in #115 \n. We should really try and extract the api version from the links in the unit tests... the way we do in the fake_api module. That's my fault for not doing it from the get go.\n. LGTM :+1: \n. Seem a unit test is failing. I don't see it being related to this particular change though...\n. This is present in the current master as well (don't know why the build is green still).\nI can confirm that when running py27 tests everything works, but it fails with the ordering issue on py33.\n. awesome :+1:  LGTM\n. if you could fix the style errors that would be great.\nI'm not to fond of the flake test I must say...especially as I don't like 4 of the peps and regularly set them to ignore on my local machine. :D\n. I've actually been bitten by this a few days ago. LGTM. :+1: \n. Hi, can you explain what you mean by \"implementing it's own mock\"? Do you mean the functions in the fake_api module?\n. Have you looked at the way the unit tests are done? and the fake_api module? it's not exactly a reusable lib or anything, but maybe it can help.\n. Nice! :+1: \n. From a quick glance at the docker API code (cause this flag isn't documented in the docs) this looks fine.\nA few tests fail because a new param has been added and the test don't expect it. Could you please update your pull request with this param added to those tests? And thanks for the contribution.\n. Awesome! :+1: \n. I'm not too happy about having 2 ways to use ports. Am I the only one? I think we broke backwards compatibility with that feature on purpose as to have a more Pythonic API.\n. Personally I don't like doing defensive coding like that in a dynamic language. If one passes incorrect parameters the API will tell us. But maybe @mpetazzoni and @shin- can weigh in.\n. Yes we agree that we should not let wrong values just fly like that, i'm just trying to limit the number of times we need to do stuff with type and isistance :)\nI think it's maybe worth looking into why the API didn't raise an error if it got malformed values from the client.\nThis does seem better, although this can also raise a ValueError (ie int('some string')) not just a TypeError.\nCan you paste the example that you used that produced the initial error. What got sent to the API (and  the result was a random port assigned right?) \n. @shin- what do you tnink?\n. If I remember correctly when the new port bindings and volumes API was being implemented this was supposed to be the case, meaning to set /tcp if nothing was specified by the user, so this is most likely a bug.\n. @mpetazzoni no I mean when we were discussing how to make the docker-py methods more pythonic, we agreed to make 'tcp' the default.\nAlso, I think I've stumbled upon this myself. I just forgot about it because at the time i prepended 'tcp' manually in my code, whereas docker-py should do it. Upon inspecting the container I saw that the port was without the proto part.\nThis might be a bug in the Docker API as well, because the docker API should raise an error if no proto is specified right?\n. @mpetazzoni It does. You can mount the same host dir into many different locations inside the container\n. Ah I see what you mean. Currently there isn't a way to mount the same host dir into 2 different mount points in the container with docker-py. The same goes for ports I believe, but I'm not 100% sure.\n. This is better. But still, a better assertion (the first question) would make the second one moot. Could md5 work?\n@shin- what do you think?\n. This doesn't make sense, with these changes the variables (ports and volumes) that get sent to the API will be None. And that's not right. Or am I missing something?\n. Ah I see. Then LGTM.\n. ping @shin- \nlet me know what you think.\n. I was trying to mimic what we already have with the CLI: http://docs.docker.io/en/master/examples/https/#client-modes\nIf you look at the docs you will see that I've only covered the 3rd option, cause that's the one that i specifically need at this time. At some point we should probably cover the others but this one seems like the one people will need the most (ie. the client authenticating to the server with a client certificate).\n. @momer @shin- sorry I was busy for quite a while. I can take another crack at this over the weekend but if you feel that you can get it done before then (or do you already have everything?) feel free to work with shin to get it merged.\n. +1 on a global timeout\n. Also, when doing the procedure described in the docs i get this:\nAttributeError: 'TLSConfig' object has no attribute 'cert'\n. @shin- shoot, I've been swamped the last few days and forgot about this. Will take a look again later today and give feedback.\n. @shin- when trying to instantiate the client like so: client = docker.Client(base_url='https://127.0.0.1:2375', tls=True)\nI get the following error:\n/home/deni/.virtualenvs/test/src/dockerpy/docker/client.py in  init(self, base_url, version, timeout, tls)\n     66             tls.configure_client(self)\n     67         elif tls:\n---> 68             self.mount('https://', ssladapter.SSLAdapter(self.ssl_version))\n     69         else:\n     70             self.mount('http+unix://', unixconn.UnixAdapter(base_url, timeout))\nAttributeError: 'Client' object has no attribute 'ssl_version'\n. I can confirm that I can get it to work if I explicitly set tls_verify=False (ie. not check the server certificate with tls_ca_cert). \nIf i try to validate the server certificate with tls_ca_cert I get this error:\nSSLError: [Errno 1] _ssl.c:510: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed\n. @shin- sorry for taking so long to take a look at this. Other than the small nitpick above this seems very nice. :+1: \nI've tested it locally and the client certificate auth is working. I still can't get the server verification to work though (with a self signed certificate), but I'm guessing this is an issue somewhere in the libraries (requests possibly) and not our code. So I used  verify=False and that works just fine.\n. @shin- Sorry, I didn't get a chance to look at this yet. Will take a closer look tonight and reply.\n. @shin- So if I understand correctly the breaking change is that now for those 2 methods we always get a generator back instead of a blocking call and a final structure? This means that the code that relied on this being blocking and actually finishing when returning would break now... and the user would have to check for generator exhaustion to be sure the call actually finished?\nI see how this makes more sense, and it does simplify the code a lot. I'm fine with a breaking change as long as we make it clear in the Release notes that this release is in fact breaking this. I'm guessing that people will hit this the most when pulling images and trying to run them immediately, seeing as the generator will yield immediately but the image hasn't actually been pulled yet. \n. I like RTD a lot, and it's definitely a step up for docker-py. That being said I still vote for consistency. \n. very nice :+1: \n. ",
    "eschnou": "@denibertovic Same issue here when trying to deploy a container with a new setup using the latest from pip install. I assume something has changed in the requets module. Maybe worth freezing version number of dependencies?\n. Downgrading requests from 2.0.0 to 1.2.0 fixes the problem. This can be done easily with pip:\npip uninstall requests\npip install 'requests == 1.2.0'\n. I would keep using setup.py and simply parse the requirements.txt file to set the value of the install_requires field:\nhttps://github.com/dotcloud/docker-py/blob/master/setup.py#L14\n. ",
    "aanand": "Right. Good point. I get where you're coming from, and having had a think about it, I'm pretty sure the way docker-py implements those methods\u2014specifically, kill, remove_container, remove_image, restart, start, stop and wait\u2014is an anti-pattern. I propose that the above-named methods be modified such that each one (a) only takes a single container ID as an argument, and (b) calls _raise_for_status on the response.\nI understand the rationale behind making docker-py behave like the CLI, but if you have a method which only deletes one container at a time and raises exceptions for erroneous HTTP status codes, it's trivial to wrap a while and a try/except around that method to get the CLI behaviour, whereas it's impossible to achieve the opposite with the API that's currently sitting in master[1].\nYour proposed solution of a \"summary\" error object would certainly work, but I don't know if the benefits of mirroring the CLI are worth the additional complexity it would introduce into docker-py. Basically, I'm not sure anyone using docker-py would find such semantics useful.\nIf you agree, I'm happy to go ahead and implement this change!\n[1]: This has, incidentally, brought me to a personal crystallisation of something I already believed on an unarticulated level: the requirements and failure modes of a library/API are different from those of a command-line tool. There is less call for descriptive feedback and do-what-I-mean semantics, and more for predictability and composeability.\n. Fantastic! I'm using the 0.2.0-dev branch now and it's working great.\n. I'd argue that it's worth adding a new method for the same reason that the HTTP and Websockets endpoints have different URLs: methods where the returned value can be one of two completely different types are a bit weird.\nBut I don't have strong feelings either way, so I'm happy to make that change (some time in the next day or two, probably).\n. Whoops! Good call, forgot to test it on 3.\nOn Fri, Nov 8, 2013 at 1:13 PM, Joffrey F notifications@github.com wrote:\n\nThanks again for the contribution :)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-py/pull/62#issuecomment-28084568\n.\n. Alright, I've had a first pass: https://github.com/aanand/docker-py/compare/links\n\nNot used in anger yet, though, so I won't create a PR.\n. I think it just needs a second pass now that the API has stabilised\u2014not sure if create_link (POST /containers/link) is still a thing, for example. I'm a bit busy the next few days though, so if one of you fancies taking a crack at it...\n. Ha! I was just doing that. And now I've done it.\n. Naming, mostly - passing a logs parameter to logs seemed a bit odd.\nhistory is an option, yes, although then we're departing from Docker terminology... :| happy to defer to your preference here!\n. One option would be to ditch the attach method and rename container_output to attach. I'm not sure the current attach is actually useful, if it doesn't do any parsing of frames?\n. Having looked closely at attach, I'm pretty sure it's not very useful, so I've done what I described above.\nIf people do want raw output for whatever reason, they can always call attach_socket and deal directly with the socket.\n. @mpetazzoni makes a good point - the commit history'll be easier to read, so I've squashed.\n. Broke it out to one argument per line - that OK?\n. I haven't tested it under Python 3, no. Is there already a simple way to do so inside a Docker container? (I'll install Python 3 in a VM if I absolutely have to, but I'd rather avoid it).\n. OK, I've added checks to the integration test and can confirm that it runs under Python 3.3 (a flurry of failures in other tests notwithstanding; think that file needs a bit of love in general.)\n. I've tested it by hand by running the integration test against 0.8.0, 0.9.0 and 0.10.0 and Python 2.7/3.3, but is there a more robust/automated way to do it? (Related: other parts of the integration test break for me - is it maintained?)\n. My bad.\n. LGTM too. If the Docker CLI doesn't allow an unspecified port, I don't think docker-py should either - and if it's a breaking change, so be it!\n. @samalba I've taken another look at #334 and it's not quite right, so I've done a more correct fix on this branch (didn't think two separate PRs were worth it, but happy to split out if you prefer). This is now the only PR that needs merging for Fig.\n. LGTM\n. LGTM\n. I'm also investigating this over on https://github.com/docker/compose/issues/890.\nRelevant debug info (would be good to get the same from anyone who either can or can't reproduce):\n$ python -V\nPython 2.7.10\n$ python -c 'import ssl; print ssl.OPENSSL_VERSION'\nOpenSSL 1.0.2a 19 Mar 2015\n. @indygreg Agreed - using pyOpenSSL seems far preferable, and if you can do that, great.\nUnfortunately, using it in Compose is blocking on https://github.com/pyca/cryptography/issues/1524 (we use PyInstaller to package Compose, so any incompatibilities there are a showstopper for us). I've created https://github.com/docker/compose/issues/1530 to keep track of it.\n. Fixed the tests.\n. :+1: \n. Fixed the build.\n. Looks good, apart from minor API surface area issue (comment inline).\n. Is this the same issue as #630?\n. Closed by #721.\n. 1. How much of this logic is actually implemented in the Docker client? For example, validation of the Dockerfile seems to happen on the server side. We don't need to go beyond what the Docker client implements.\n2. Documentation would be great.\n. Thanks. Looking good.\n. I've continued this in https://github.com/docker/docker-py/pull/721.\n. I'm in favour of the wording change suggested by @shin-. As for ReadTimeout, I'm undecided, but I don't think we should let that discussion hold up the 1.3.0 release.\nAlso, what's up with the exception at the top of the Python 3 output?\nTypeError: getresponse() got an unexpected keyword argument 'buffering'\n. > I suppose I could split _stream_raw_result into two methods - one for TTY-enabled containers, and one for versions below 1.6. The one for versions below 1.6 could keep the old, line-by-line behavior.\nSounds sensible to me.\n\nAlso please let me know if any additional unit tests are needed.\n\nI'm not sure I fully understand your changes to the tests. Are they now exclusively testing the Tty=True case? If so, we should make sure to also test the Tty=False case.\n. Thanks for the contribution!\nFirst of all: @shin-, would you consider this functionality within scope for docker-py? I'm a bit nervous about writing out auth files - if the format or location ever changes, software which uses docker-py could end up breaking things, unless both docker-py and the Docker client are very careful about backwards-compatibility.\nI have a few comments/questions regarding the code:\n- \"authentification\" should be \"authentication\"\n- The return value of write_authconfig is unused. Why is it there?\n- We don't need both config_path and config_file variables.\n- If the path doesn't exist, should we create it, rather than silently continuing?\n- If the path does exist, are we sure that we're not overwriting any auth data that's already there? i.e have we definitely loaded it?\n. Nice.\nLGTM\n. Just saw that line, yes - we should fix that too (in a separate PR).\n. LGTM\n. @thieman +1, actually\n. LGTM\n. LGTM\nFor future reference, you can always git push --force to a PR's branch, rather than close it and open a new one.\n. LGTM\n. This is a Docker 1.7.0 bug (https://github.com/docker/docker/issues/14170), and will be fixed in 1.7.1.\n. You can work around it for now by passing entrypoint as a list, e.g. entrypoint=['/bin/sh']\n. LGTM\n. Sounds sensible. You've got some linting errors:\ndocker/errors.py:55:1: E302 expected 2 blank lines, found 1\ndocker/errors.py:58:1: E302 expected 2 blank lines, found 1\n. LGTM\n. LGTM\n. Updated:\n- Use format instead of %\n- Store the deprecation warning in constants.py to avoid reuse\n. LGTM\n. Thanks. LGTM.\n. LGTM\n. Thanks for the contribution!\n1. This needs tests, and they should at least cover the cases covered by the Docker client's env file parser tests (although the \"too long file\" is less important).\n2. The Docker client performs very straightforward string splitting. Is there a compelling reason to use csv here, rather than just do the same?\n. Also, it's not clear what happens when a variable is specified both in a file and in the environment key. Indeed, the developer might want to decide that for themselves (I can say from experience working on Compose that opinions differ on what should take precedence).\nArguably, docker-py should just provide a utility method to parse a dict of environment variables from a file, and leave the job of building the combined dict to the developer.\n. Sorry, I meant: what happens when the same variable (e.g. FOO) is specified in both places with a different value? Which one wins?\n\nI figured that it was better to use a built-in feature instead of re-inventing the wheel \n\nFair enough - I'm just concerned that it could have all kinds of unexpected extra behaviour. For example, does it have special ways of dealing with quotes, or escaped delimiters? Even if those are useful things, we shouldn't deviate from the Docker client's behaviour.\n. The problem is that the \"expected behaviour\" differs for different people. For example, with the Docker client, values specified with -e take precedence over those specified in environment files, but lots of Compose users would prefer environment files to take precedence over values specified in the environment section of a Compose file.\nThis is why I think that, as a first step, it'd be preferable to simply provide a utility function for parsing environment files and leave it to the user to construct an environment dict to pass into create_container. That way docker-py isn't dictating the order of precedence.\n. You wouldn't be duplicating any logic - the logic for parsing files would live in docker-py, and the logic for building the final environment dict would live in your project.\n``` python\nfrom docker.utils import parse_env_file\nenv = #... get env option\nenv_files = #... get env_file option\nfor filename in env_files:\n    env.update(parse_env_file(filename))\n``\n. Looks much better, thanks.\n1. After skimming the docs for thecsvmodule, I'm convinced we shouldn't use it. There's way too much logic in there for dealing with _actual_ CSV that might result in weird bugs for environment files. It would be best to follow [Docker's ParseEnvFile method](https://github.com/docker/docker/blob/master/opts/envfile.go#L19-L51) as closely as possible.\n2. Theif isinstance(env_file, str)andif os.path.isfile(env_file)checks are unnecessary and will swallow errors that should bubble up to the developer.\n3. After that, just needs tests I think.\n. 1. If you remove theisfilecheck, you can remove the first patch from your test.\n2. Rather than patchingopen(), just write the contents to a temporary file (usingtempfile) andopen()that directly.\n. Thanks!\n1. The Docker client raises an error if there are bad lines in the file, so we should do that too.\n2. It'd be more useful ifparse_env_filereturned a dict, so that multiple env files can be combined using theupdate()method on dict\n3. I'm not sure about the change to.gitignore`, but that should definitely be done in a separate PR.\n. Great stuff. Two last things:\n1.  Could you add a test for the \"invalid line\" case?\n2.  Could you change the error message wording to:\nInvalid line in environment file /path/to/file.env:\n   (offending line goes here)\n. Great. Needs squashing to one commit, then LGTM.\n. It's got my LGTM, but another maintainer needs to approve and merge it.\n. LGTM\n. Agreed. Perhaps we could write an format-like wrapper function which performs URL-escaping on its arguments before passing them to format.\n. Option 1 is no good from a security standpoint - if it allows /, then I can still mess with the path by putting / in an argument.\nI don't really understand Option 3. If passing a string is still allowed, isn't it just as easy to forget?\n. What's the purpose of all the network_mode='none' stuff in the tests?\n. Thanks. Could you squash to one commit?\n. LGTM\n. LGTM\n. Thanks. The tests need fixing, and it'd be good to have a test that explicitly checks the new behaviour.\n. LGTM\n. LGTM\n. @alunduil Are you certain it's happening on docker-py 1.3.1? The offending commit was merged into master after the 1.3.1 release. Do you have steps to reproduce?\n. Sounds like we should stringify the value you pass in.\n. LGTM\n. Compose has a progress_stream module which attempts to replicate the stream parsing and terminal output behaviour of the Docker client. It's out of date, but should be somewhat reusable. We should make it a utility module of docker-py or dockerpty.\n. For the sake of backwards compatibility, we should arguably at least keep the existing constants around; there's a case to be made for adding GELF and FLUENTD for consistency too.\nHowever, validating it on the client side is a huge pain. It's too much of a blocker to have to add a new logging driver to docker-py's whitelist, then update Compose's docker-py version (bringing in stuff we might not want to deal with yet), then release a new version, just so Compose users can use it.\n. LGTM\n. I like it :+1: \n. LGTM\n. Test setup is failing for Python 3.2 on this line, which is odd.\nCloning git://github.com/cpburnz/python-path-specification.git to ./.tox/py32/src/pathspec\n    Complete output from command python setup.py egg_info:\n    Traceback (most recent call last):\n      File \"<string>\", line 20, in <module>\n      File \"/home/travis/build/docker/docker-py/.tox/py32/src/pathspec/setup.py\", line 6, in <module>\n        from pathspec import __author__, __email__, __license__, __project__, __version__\n      File \"pathspec/__init__.py\", line 11\n        __copyright__ = u\"Copyright \u00a9 2013-2015 Caleb P. Burns\"\n                                                               ^\n    SyntaxError: invalid syntax\n. OK, after some investigation it turns out that pathspec isn't right for us. .dockerignore's syntax and semantics are dramatically different from .gitignores.\nI've written a fairly comprehensive test suite that covers the correct behaviour (which I  confirmed was correct by testing it against actual docker build invocations), and updated the code accordingly. There are no new external dependencies.\nHere's what's new:\n- Support all basic pattern forms: file, directory, *, ?, !\n- Fix handling of wildcard patterns and subdirectories - */a.py should match foo/a.py, but not foo/bar/a.py\n- Fix handling of directory patterns with a trailing slash - make sure they're handled equivalently to those without one (#581)\n- Fix handling of custom Dockerfiles - make sure they go in the tarball (#730)\nThis is ready for review.\n. @cpburnz thanks for responding, by the way, and sorry we didn't end up using your library in the end!\n. Fixed the build.\n. For consistency, I think an ideal scenario is that create_host_config requires a version arg, much like create_container_config.\nHowever, this will break things for many current users of docker-py, who are probably doing things like this:\n``` python\nfrom docker import Client\nfrom docker.utils import create_host_config\nclient = Client()\ncontainer = client.create_container(\n    'busybox',\n    'true',\n    host_config=create_host_config(privileged=True),\n)\n```\nSo how about this: we define new create_container_config and create_host_config methods on Client (which can always set the version correctly via self._version), and deprecate the direct use of their namesakes in docker.utils.\nThe above snippet would continue to work exactly as before, but a deprecation warning would be printed. Something like \"docker.utils.create_host_config() is deprecated. Please use client.create_host_config() instead\".\nThis would be the new idiomatic approach:\n``` python\nfrom docker import Client\nclient = Client()\ncontainer = client.create_container(\n    'busybox',\n    'true',\n    host_config=client.create_host_config(privileged=True),\n)\n```\nAs part of doing this, we have two additional opportunities for improvement:\n1. Move the methods to a module with a more descriptive name - say docker.config. We can then replace the ones in docker.utils with wrappers that print the warning and then pass through.\n2. Move all the API version checks currently done in start into client.create_host_config(), and eventually into docker.config.create_host_config() after a sufficient deprecation period.\nThoughts?\n. @shin- would you be happy with the above approach? This bug's a bit of a showstopper.\n. Thanks! LGTM\n. This is ready for more eyes.\nThe networking API doesn't appear to be present in Docker master (which is strange - @mavenugo, @dave-tucker, is that expected? When should it land?), so to test it you'll need to install experimental Docker. If not straightforward per se, it's at least fairly doable with Docker Machine:\ndocker-machine create -d virtualbox experimental\ndocker-machine ssh experimental 'sudo sh -c \"set -ex; /etc/init.d/docker stop || true; sleep 2; curl https://experimental.docker.com/builds/Linux/x86_64/docker-1.9.0-dev > /usr/local/bin/docker; /etc/init.d/docker start\"'\neval \"$(docker-machine env experimental)\"\nThen update DEFAULT_DOCKER_API_VERSION to '1.21' in docker/constants.py.\nFinally, run the tests:\nmake unit-test\nmake integration-test\nThere are unrelated failures in both test suites, caused by the version update.\n. This is now working against the latest iteration of the networking API, as implemented in https://github.com/docker/docker/pull/16645.\n. I've worked a fix into https://github.com/docker/docker-py/pull/721.\n. Closed by #721.\n. You can also pass assert_hostname=False directly to kwargs_from_env:\npython\nClient(**kwargs_from_env(assert_hostname=False))\n. LGTM\n. LGTM\n. Thanks for the cleanup!\nLGTM\n. Pushed another commit to stop people passing version into create_host_config, since it'll be overridden (and it makes no sense).\n. Thanks. LGTM\n. @rade I'm working on it :+1: \n. I'm a bit afraid of making assertions about the whole of the build stream output. I took a different approach in 380d279bf04ffcb5c414bc5893f4aa5f27f91fad, which I rolled into #721 - what do you think? I could submit it as an isolated PR.\n. Getting a lot of APIError: 500 Server Error: Internal Server Error (\"Cannot kill container <long id string>: [8] System error: permission denied\")\nOnly related issue I can find is https://github.com/docker/docker/issues/15101, which implies it's something to do with AppArmor and suggests that something's up with the Ubuntu installation. Maybe Travis isn't going to work for us.\n. I've separated the integration test improvements out into https://github.com/docker/docker-py/pull/747.\n. Closing as we're going to move to Jenkins.\n. :+1: this would be fantastic.\n. Yep. Fixed in https://github.com/docker/docker-py/pull/743, but since Travis' Docker support is uncooperative, I'm going to move it into a separate PR.\n. See https://github.com/docker/docker-py/pull/747\n. > Sounds great! In the meantime, maybe it'd be nice to retrieve current config from docker and check if it has any non-default settings? (And then e.g. raise an exception)\nUnfortunately, difficult (maybe impossible) to do reliably, due to how much munging of the config dictionary is done by both docker-py and the daemon.\n. I see the use case, but catching every unrecognised kwarg to Client() and passing it to the adapter seems extreme - it'll make it hard to add kwargs in future, because of potential name collisions.\nI think we should either:\n1. Just add a max_connections argument for now, which passes through to the adapter\n2. Add a named adapter_kwargs argument\n3. Allow the user to construct their own adapter and pass it in\n. \n. I still don't like that those tests require access to the daemon's filesystem - we'll have to change them if we want to test against Swarm (which we should do). Still, that's arguably out of scope for this.\nI think it could be squashed to fewer commits, then LGTM.\n. I'm not the biggest fan of mixins, but it's better than one huge class, and I never liked that ClientBase superclass.\nNeeds squashing into logical commits, then LGTM.\n. Those snippets look near-identical. Maybe there should be a test utility method?\npython\ndef get_volume_info(container, destination, version):\n    \"\"\"\n    Takes a container dictionary and volume destination path\n    and returns a tuple of (rw, origin), using the appropriate\n    fields for the specified API version\n    \"\"\"\n    if docker.utils.compare_version('1.20', version) < 0:\n        # old logic\n    else:\n        # new logic\n. Duplicate line aside, LGTM\n. LGTM\n. Build's green.\n. Cool. Perhaps the first test should be rewritten so it tests byte string input on all Python versions:\n``` python\n    def test_convert_volume_bindings_binary_input(self):\n        expected = [six.u('/mnt/\uc9c0\uc5f0:/unicode/\ubc15:rw')]\n    data = {\n        b'/mnt/\uc9c0\uc5f0': {\n            'bind': b'/unicode/\ubc15',\n            'mode': 'rw'\n        }\n    }\n    self.assertEqual(\n        convert_volume_binds(data), expected\n    )\n\n```\n. OK, I see the rationale, but instead of entirely separate test methods, could we just switch on the Python version inside the method?\n``` python\ndef test_convert_volume_binds_unicode_bytes_input(self):\n    if six.PY2:\n        expected = [unicode('/mnt/\uc9c0\uc5f0:/unicode/\ubc15:rw', 'utf-8')]\n        data = {\n            '/mnt/\uc9c0\uc5f0': {\n                'bind': '/unicode/\ubc15',\n                'mode': 'rw'\n            }\n        }\n    else:\n        expected = ['/mnt/\uc9c0\uc5f0:/unicode/\ubc15:rw']\n        data = {\n            bytes('/mnt/\uc9c0\uc5f0', 'utf-8'): {\n                'bind': bytes('/unicode/\ubc15', 'utf-8'),\n                'mode': 'rw'\n            }\n        }\nself.assertEqual(convert_volume_binds(data), expected)\n\ndef test_convert_volume_binds_unicode_unicode_input(self):\n    if six.PY2:\n        expected = [unicode('/mnt/\uc9c0\uc5f0:/unicode/\ubc15:rw', 'utf-8')]\n        data = {\n            unicode('/mnt/\uc9c0\uc5f0', 'utf-8'): {\n                'bind': unicode('/unicode/\ubc15', 'utf-8'),\n                'mode': 'rw'\n            }\n        }\n    else:\n        expected = ['/mnt/\uc9c0\uc5f0:/unicode/\ubc15:rw']\n        data = {\n            '/mnt/\uc9c0\uc5f0': {\n                'bind': '/unicode/\ubc15',\n                'mode': 'rw'\n            }\n        }\nself.assertEqual(convert_volume_binds(data), expected)\n\n```\n. @dnephin nice, I'll update it\n. LGTM\n. Nice.\n. LGTM\n. Apologies, I hadn't looked at the codebase in a while so I didn't realise it was copied. Your suggestion was a good one and the tests indeed look much better now, so thanks!\nI'd like to make one more suggestion, assuming it's possible - can the select call be moved to the top of the read_socket function?\n. LGTM\n. Nice. I agree with @kanzure's comments, plus it'd be good to document the logic of should_include.\n. Thanks! I think this can be squashed to one commit.\n. Thanks. I just noticed that we're now excluding parent directories of files mentioned in !-style exceptions. Is that in line with Docker client behaviour?\n. OK, so even if we're not passing the directories explicitly to tar, they'll still be created as needed? Sounds good. Glad the integration test now explicitly checks the \"exception rule for file in ignored subdirectory\" case.\nLGTM\n. LGTM\n. Thanks! I think test_create_network should be updated, specifically this line (to pass options) and this line (to check that they made it through to the HTTP request).\n. Great. I think this can be squashed to two commits: one to get rid of remove_volume's return value, and one for the new argument to create_network.\n. Thanks!\nLGTM\n. We deliberately didn't remove Mazz from Compose, so I don't think there's any need to do it here either.\n. LGTM\n. @TomasTomecek That would involve making an extra call to the Docker API on every docker-compose command, which is probably unacceptable.\n. @dnephin correct.\n. It's called restart_policy, and it's part of host config:\n``` python\nfrom docker.utils import create_host_config\nclient = # instantiate a client\nclient.create_container(host_config=create_host_config(restart_policy='always'))\n```\n. Is supporting decimal values important? The Docker CLI doesn't seem to do it:\nconsole\n$ docker run -d -m=1.5G busybox true\ndocker: invalid size: '1.5G'.\nSee 'docker run --help'.\nI'd argue it's more important to keep in line with the CLI than to add conveniences. This being a library, consumers can do their own maths pretty easily if they need to.\n. LGTM\n. Not sure what's going on with the py32 build on Travis, but it looks like a pip issue.\n. LGTM\n. @zbyte64 Do you mean to say that passing the network name results in port binding working, whereas passing the network ID results in it failing?\nping @mavenugo \n. LGTM\n. LGTM once green\n. OK, I see the problem, but I'm not sure docker-py should change its behaviour here - in general I like things to fail as early as possible, and it's also nice that you currently know exactly when the version check will be performed.\nI feel like you shouldn't be initialising a docker.Client at the top level of a module if you need to wait for a daemon to start first?\n. LGTM\n. LGTM - needs rebase\n. LGTM\n. @dvenza This should work (off the top of my head):\npython\nclient.create_container(\n  image='busybox',\n  command=['true'],\n  net='mynetwork',\n  networking_config=client.create_networking_config(\n    endpoints_config={\n      'mynetwork': client.create_endpoint_config(\n        aliases=['foo', 'bar'],\n      )\n    }\n  )\n)\n@shin- is right that the documentation is lacking - this is my fault, sorry!\n. Ah, it's because of the Engine version (1.9.0). I'll decorate the test.\n. Updated mkdocs.yml\n. LGTM\n. If we're going to fix this, I'd advocate for fixing it a bit more properly. Perhaps it's time to split the integration test process out into a script and use nc -z to check that the Docker daemon is accepting connections.\n. LGTM\n. Does this also mean we can stop passing assert_hostname=False in Compose's docker_client.py, since the IP addresses of Machine VMs will now be correctly matched?\nLGTM\n. Good catch - fixing it now.\n. Fixed in #1015\n. I'm not sure this makes sense as a docker-py addition. That's mostly because I'm not clear on the use cases - but even if they are valid, it seems like having it as a separate tool would be fine.\n. Could you add some unit and integration tests for this? The code is also a little dense and would benefit from being split up into smaller methods.\n. It's difficult to integration test because the API streaming endpoint won't necessarily always concatenate multiple JSON objects in one chunk. It's even difficult to unit test in its current state, because _stream_helper is coupled to an HTTP response object.\nIf you can decouple the inner logic so it just operates on an IO object, then you should be able to write a unit test that passes in a mock object which emits a chunk of multiple JSON objects when read from.\n. Code changes look good. One comment about the docs.\nWe could also use docs for the other methods related to networks - create_network, connect_container_to_network, etc, but that can be done in a separate PR.\n. Ah, I missed that. LGTM, just needs rebase.\n. LGTM\n. Thanks! Minor style nit and needs a rebase, otherwise fine.\n. LGTM\n. LGTM\n. Closing in favour of #1130 \n. You've got some style errors (you can test this locally with tox -e flake8):\ntests/unit/utils_test.py:123:80: E501 line too long (81 > 79 characters)\ntests/unit/utils_test.py:132:1: E302 expected 2 blank lines, found 1\n. Thanks. Can you squash and rebase please?\n. Thanks for this. Looks sound, but needs tests.\n. Great!\nLGTM\n. Makes sense to me. I'll move it to the 1.10 milestone.\n. LGTM\n. Thanks! Just one small code thing.\n. retest this please\n. retest this please\n. Thanks. This is definitely something we want in docker-py. I know it's WIP but I have a few comments about the code.\n. I haven't finished reading through all the code, but this looks really good to me. Thoughts on topics discussed so far:\n- Restricting it to Engine 1.12+ sounds good to me.\n- I think it's correct for run to raise an exception when the exit code is nonzero, as long as it doesn't prevent you from getting at any of the information you need in the case of such an exception (e.g. the container object, the exit code - which do indeed appear to be attributes on the error object).\n- I don't have any opinions on inheritance. If there's an alternate implementation route that preserves the same API, great.\n- The smart AttributeError looks good.\n. LGTM. LGTM\n. LGTM, needs rebase. LGTM\n. LGTM, needs rebase. Hm, think I broke the DCO check with the \"Update branch\" button. @joncotton, could you rebase on master and force push?. LGTM, needs rebase. LGTM. DockerClient is fine by me. It has the advantage that inspecting the object makes it clear what it is (it's very easy to imagine an application holding onto several \"client\" objects from different libraries, so being able to see instantly that this is a <DockerClient ...> object could save people a lot of debugging time.\n(The only nit is that, by this logic, APIClient should be DockerAPIClient. Thoughts?). Should it have Client in the name at all?\nA DockerClient object is part of a graph of objects that represent different things managed by the Docker Engine - ContainerCollection, Container, Swarm. If DockerClient has Client on the end, shouldn't each of them have Client on the end too? That would be tedious.\nMaybe the right name is Engine.. @TomasTomecek Correct. It's a 2.0 release, so a rare opportunity to break backwards compatibility.. \n. Arguably better, yes, but I wanted to stick close to the Docker client's behaviour. Not sure what's the stronger principle here!\n. I don't think to_port_range and add_port_mapping should be imported here - they're only used within ports.py, and I can't see them being useful outside it\n. This should probably just say \"Docker\" :smirk: \n. What's this constant used for?\n. Could this be scoped to the specific type(s) of exception we're expecting?\n. This can be replaced with just six.binary_type(), I believe.\n. Can we reduce the duplication here? This is very similar to the same code in attach().\n. Could we also test that whoami returns root if user isn't passed in?\n. As an optimisation, attach (and _get_result) could take an optional argument to force TTY or non-TTY behaviour. Syntax to be determined, but e.g.:\npython\nclient.attach(force_tty=True)\nclient.attach(force_no_tty=True)\nThe inspect_container call could then be skipped.\nThis wouldn't need to be implemented as part of this PR - the pressing concern is to fix the bug.\n. For some reason I thought that's what I did, but it clearly isn't. Updated.\n. Looks like the change went out in Docker 1.7.1, according to the milestone on this PR: https://github.com/docker/docker/pull/14139\nI don't see any switching being done on the API version, so it's possible that backwards compatibility is broken. I've asked for clarification: https://github.com/docker/docker/pull/14139#issuecomment-123246739\n. Rather than checking twice for line[0] != '#', I think this could be simplified to:\n``` python\nif line[0] == '#':\n    continue\nparse_line = line.strip().split('=')\nif len(parse_line) == 2:\n    # ...\nelse:\n    # ...\n``\n. What's the purpose of this?\n. Is there a compelling reason not to do it increate_host_config`?\nIt looks like the fallback check there is for network_mode is None. This will be false if network_mode is the empty string. So it seems like the behaviour of start and create_host_config will differ when the network_mode kwarg is omitted:\n- client.start(container) will start a container with a network_mode of default.\n- create_host_config() will return a host config object without a NetworkMode key.\nIf NetworkMode is now required in host config objects, I feel create_host_config should always set it.\n. OK, that makes sense. (I'd be in favour of removing start's arguments soon.)\nCould we make it clearer what's happening? Perhaps:\n``` python\nstart_config_kwargs = dict(\n    binds=binds, port_bindings=port_bindings, lxc_conf=lxc_conf,\n    publish_all_ports=publish_all_ports, links=links, dns=dns,\n    privileged=privileged, dns_search=dns_search, cap_add=cap_add,\n    cap_drop=cap_drop, volumes_from=volumes_from, devices=devices,\n    network_mode=network_mode, restart_policy=restart_policy,\n    extra_hosts=extra_hosts, read_only=read_only, pid_mode=pid_mode,\n    ipc_mode=ipc_mode, security_opt=security_opt, ulimits=ulimits\n)\nstart_config = None\nif any(v is not None for v in start_config_kwargs.values()):\n    start_config = create_host_config(**start_config_kwargs)\n``\n. Perhaps we should also test the \"no type arg\" and \"no config arg\" cases.\n. While it's not consistent, I think @mnowster's changes are improvements, so I'm fine with it. Would be good to get the rest of the suite updated along similar lines.\n. Yes - we can't evaluateexec_driver_is_native()at script execution time, because the check is done insetup_module()(as per pytest convention), which hasn't run yet.\n. A hack caused by my misunderstanding of what features existed in Python 2 and 3. Updated to useglobal.\n. FYI, you can just useassert` with pytest:\npython\nassert url == '{0}{1}'.format(url_prefix, 'hello/somename/world')\nAssertion failures are still formatted nicely with helpful information etc. It's rad as heck.\n. Feels a bit hacky - could we just say six.text_type(user)?\n. Could we give this test a more descriptive name? Also, we should assert that the container came out with the right uid.\n. OK... I'm not sure about that as a general thing. It places a rather unnecessary demand on people reading the code. Out of scope for this PR, but maybe we could rename them with descriptive names and include the URL to the GitHub issue in a docstring?\n. Ah, I didn't realise it was public API, so I guess we have to. Will fix.\n. Done.\n. Yeah, it also means you can run the tests against any arbitrary daemon by passing in the DOCKER_HOST environment variable (so down the line we can e.g. run it against a Swarm cluster).\nIt's also nice to have the Docker-in-Docker stuff isolated to a container that's not getting messed with.\n. True. Updated.\n. This seems to be a duplicate of the line above - merge conflict resolution mishap?\n. I think this should have been >, not < - if I set the DEFAULT_DOCKER_API_VERSION to '1.21', then this error gets raised.\n. ping @mavenugo - I'm not familiar enough with the sandbox/container distinction\nI can see a case for splitting it out in docker-py, yes - the combined logic would then live in Compose\n. OK, in that case I'll hold off on changing anything as it sounds like docker-py's API won't change.\n. Minor thing, but this style of conditional makes me do a double-take - plus we're immediately doing another check in the second branch. I think a single outer check and two inner checks would be more normal.\nAlso, could we use version_lt? I find it much easier to read.\npython\nif cpu_quota:\n    if not isinstance(cpu_quota, int):\n        raise TypeError(...)\n    if version_lt(version, '1.19'):\n        raise errors.InvalidVersion(...)\n    host_config['CpuQuota'] = cpu_quota\nSame for the cpu_period conditional.\n. Might be able to avoid this step if we go:\npython\nexpected = [six.u('/mnt/\uc9c0\uc5f0:/unicode/\ubc15:rw')]\n. These will be byte strings in Python 2 - is that correct? Do we care what happens when users pass in unicode strings in Python 2?\n. I feel like this could use a docstring.\n. This is a bit harder to read than it needs to be. How about just:\npython\nif socket:\n    stream = True\nAlso, this is just a personal preference, but I prefer to modify arguments at the very top of a function, so it's all in one place, so ideally this would go above the bit where we initialise data.\n. Small thing: although we don't use Python's built-in id function anywhere in this block, it'd still be nicer not to shadow it for the sake of clarity (for one thing, it gets picked up by syntax highlighters, which results in misleading colouring). Maybe container_id?\n. Could these functions be moved to the top level? Even if they're only used by this test, they make it hard to read.\n. What's the purpose of \"{0}\".format(line) here?\n. Could this loop be extracted into a read_data(next_size) helper function?\n. This try/finally could alternatively be done with a self.addCleanup(socket.close) line right after calling exec_start, which might be a bit more readable, as well as catching any errors raised by select (assuming it could raise any).\n. What's the purpose of this line?\n. OK, I don't think either method should return anything, since the return value is always the same and therefore meaningless. @shin- - what was the reason for remove_volume returning True?\n. Should we also document the dict form of elements in the devices list?\n. Yep, just anticipating future additions (these things have a tendency to grow), but it's probably unnecessary\n. create_endpoint_config needs to pass the version through, same as create_host_config. create_networking_config doesn't need to right now, but it might in the future, and I'd like to avoid having to do another deprecation like we did with create_host_config.\n. Yeah. Perhaps just endpoint_config.aliases, since the user will have explicitly called create_endpoint_config. I'll update it.\n. I wasn't sure if the two would interact weirdly, so I elected to be more explicit, but I don't have strong feelings about it\n. I think this print() can be removed, right? Presumably it was used for debugging while getting the test to run?\n. I like the neatness too. It's always a tiny mental speed-bump when I have to learn a new class when starting out with a library, so having that removed is :+1: \n. This is a little misleading. When creating a container, you can only connect it to one network, using the network_mode argument to create_host_config. You can only specify that one network's config in the networking_config dict (so the example below is fine).\nTo connect a container to multiple networks, you have to first create it with one of the networks specified, then call connect_container_to_network once for each remaining network, with the relevant endpoint config. We should probably be explicit about this.\n. Rather than a conditional followed by assert False, can't this just read assert next_data, \"Failed trying to read in the data\"?\n. Small thing, but there's a double underscore after test, which we don't do anywhere else. Same thing with the other new tests.\n. I think we should use six.text_type here - str means different things in Python 2/3.\n. (we can also remove the isinstance conditional)\n. You can use the host_config_version_error utility function here instead of writing a custom error string.\n. We need to raise an error if healthcheck isn't the right type. You can use host_config_type_error.\n. I don't think we should parse duration strings ourselves and introduce another dependency. Users of docker-py can instead use whatever library they prefer, whether it's timeparse or arrow or doing the arithmetic themselves.\n. Yeah. We should also provide a utility function for building the object, following the pattern we have with the networking data structures (e.g. create_ipam_pool):\npython\ndef create_healthcheck(test=None, interval=None, timeout=None, retries=None):\n    return {\n        'Test': test,\n        'Interval': interval,\n        'Timeout': timeout,\n        'Retries': retries,\n    }\n. Ah, my mistake.\n. This doesn't have to happen before merge, but it'd be really cool if this showed the name (if there is one) instead of the short id.\n. What's the rationale for get() versus [] in the two methods above? Could we go with just one?\n. I know I could have just used docker.from_env() in the test, but I wanted something as close as possible to what users of the old docker-py would actually have written. Let me know if you think this is silly.. ",
    "pwaller": "I suppose requests was coming from my system, which is an Ubuntu 13.10 daily image. Link to the source of said package.\n. @shin: it is version requests==1.2.3, so pip does nothing. It's just that the debian/ubuntu maintainers, in their wisdom, made it so that requests gets its urllib3 from the system and not from its requests.packages. Is it possible for you to instead depend on urllib3 or is there a strong reason to use something which is really an internal detail of requests?\n. Looks like this issue is solved by #45, thanks!\n. works for me.\n. > Apparently docker is the replacement for docker-py, but does not conflict with it, so they simply install over each other.\nI hit this running docker-compose after an upgrade of docker-compose, and filed https://github.com/docker/compose/issues/6339 there. Seems worth preventing that failure mode.. ",
    "Kaleesastha": "How to run a setup.py ..? I ran it throws the below error. Please help me to resolve this...\nusage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n   or: setup.py --help [cmd1 cmd2 ...]\n   or: setup.py --help-commands\n   or: setup.py cmd --help\nerror: no commands supplied\n. ",
    "dmp42": "I'll check the behavior on master then.\nThanks. \n. It's likely broken:\n```\nDownloading/unpacking docker-py\n  Downloading docker-py-0.2.1.tar.gz\n  Running setup.py egg_info for package docker-py\n    Traceback (most recent call last):\n      File \"\", line 16, in \n      File \"/private/var/folders/17/hvtvth_97bv_q1wnf4wykjjh0000gq/T/pip-build-dmp/docker-py/setup.py\", line 9, in \n        with open('./requirements.txt') as requirements_txt:\n    IOError: [Errno 2] No such file or directory: './requirements.txt'\n    Complete output from command python setup.py egg_info:\n    Traceback (most recent call last):\nFile \"\", line 16, in \nFile \"/private/var/folders/17/hvtvth_97bv_q1wnf4wykjjh0000gq/T/pip-build-dmp/docker-py/setup.py\", line 9, in \nwith open('./requirements.txt') as requirements_txt:\n\nIOError: [Errno 2] No such file or directory: './requirements.txt'\n```\n. Works as expected - with context on build('.') - from git master otherwise.\nJust the pypi package is broken.\nThanks.\n. Declaring docker as a namespace package is not enough unfortunately. init py files inside the namespaces have to contain specific content, and can NOT declare anything specific to the product (there is no guarantee the init file will come from one product or the other).\nSee how it's done in docker registry:\nhttps://github.com/dotcloud/docker-registry/blob/master/docker_registry/init.py\nOr in zope:\nhttps://github.com/zopefoundation/zope.schema/blob/master/src/zope/init.py\nCurrently, docker-py will break if any other package use the namespace: https://github.com/dotcloud/docker-py/blob/master/docker/init.py\nHope that helps.\n. Also, namespaces in python are dangerously broken (<- this is only my opinion, not the one of my employer :-)), and working with pip / tox with namespace can prove to be tricky. Careful with that! :)\n. @shin- ping me if you need help / opinion on that stuff\n. ",
    "jsdir": "In the api, you can specify a fileobj but you can't include any files and directories with it for the build. This change allows you to include all the files and directories in path.\n. The only practical use case for this is if you want to build a container from an in-memory Dockerfile and want include a local directory. Since only my application does this, I'll probably be better off implementing this application-side.\nAccording to the build command docs, docker build - < Dockerfile will always build the image without context. Because docker itself doesn't provide this functionality, I now see how implementing it here would be somewhat inconsistent.\n. ",
    "kiorky": "Fixes \n```\nTypeError: 'APIError does not take keyword arguments'\n```\nwhile having an exception on remove_image for an image which cant be deleted (parent)\n. Uhm, in fact this is an invalid pull request.\nMy problem was that i was an interferring old requests package from my system site-packages messing up with my local python install.\n. @shin- does it sounds good to you ?\n. You (@shin-) should have a new look now (rebased work)\n. @shin- does it sounds good to you ?\n. did you noticed the readme: https://github.com/dotcloud/docker-py/blob/master/README.md\nbasically you have to define mountpoint entries in create_container then you define the binds in client.start.\nc = docker.Client(base_url='unix://var/run/docker.sock')\nc.create_container('ubuntu', ..., volumes={\"/srv\": {}}\nc.start('ubuntu', ..., binds={\"/srv\": \"/mnt_in_host/srv\"}\n. @shin- We should add my previous comment in readme.md\n. Sry\nc.create_container('ubuntu', ..., volumes={\"/root/data\": {}}\n. It may also depend on his saltstack integration, but i'm guessing he is using official packages; so his only luck is to install dockerpydev inside python's global site packages or better, use an isolated local version of saltstack.\n. on py27, for sni, you need just to install a bunch of other py pkgs: install pyOpenSSL, ndg-httpsclient, pyasn1 to enable python, via monkeypatchs (in requests, that's automatic, the only requirement is that the packages are there) to support server name indication, as it would be on py3.\n. The traceback you get now is another error.\nMaybe trying to access to an old sslv3 only service (see https://github.com/shazow/urllib3/issues/487 ?)\n. Well the other solution is to be cowardly silent ^^\n. lookin into it\n. ",
    "elbaschid": "That's nice. I've struggled a bit with this myself trying to figure out how it works. I've also tried to work out the syntax for mounting volumes read-only. I ended up using bind={'/host': '/container:ro'} similar to how the commandline option works. This seems to work but I am not sure if this is the best way. Do you have any input on that?\n. ",
    "sysr-q": "You can throw something like -e git+https://github.com/dotcloud/docker-py.git into your requirements.txt as well.\n. ",
    "glonnon": "ugh.  wrong project.  sorry\n. ",
    "yukw777": "@aanand any update yet? It looks good to me. it'd be awesome if this could be merged as soon as possible, because I really want to try links out!\n. I'll take it.\n. @aanand can you close this?\n. Huh? Did you mean to ask if the logic belongs in Client class? \nAssuming you did, I think it does. I think as a client, you should be able to request the server to give a stream back.\n. haha ok. :)\n. Just removed some blank lines as the build was failing.\n. travis build passes now.\n. Also, there are five tests failing. Pretty easy to fix though! python tests/test.py\n. #66 \n. I just realized that I expect links to be passed in like this: [{path: alias}, {path: alias}, ...], but it's probably better to have it { path: alias, path: alias, ...}. Not sure you've already made the changes while merging. If not, I can fix it.\n. Rebased and fixed the format. Also added another test! (test_start_container_with_links)\n. Oops sorry should've searched before submitting!\n. I'm going to investigate this more. The format it expects a bit tricky. Will be back to it tonight.\n. Just had a conversation with @crosbymichael and he confirmed that what we had before is correct (https://github.com/dotcloud/docker/blob/master/container.go#L122) Closing.\n. Nice! So much more complete support than my PR haha. There are some stylistic erros but other than that looks good!\n. It's failing since the keys in the dictionary are unordered and the order ia different in py33. We should sort the keys before comparing.\n. ping @mpetazzoni \n. https://github.com/dotcloud/docker-py/pull/92 It's already been implemented.\n. Nice! I'm going to close my pull request.\n. It'd be better if it was\npublish_all_ports=False\nbecause you can then just do\nstart_config['PublishAllPorts'] = publish_all_ports\nwithout the if clause.\n. I'm setting the default timeout (self._timeout) to None (no timeout), since I have a very large image. Can we handle that case here also?\n. ",
    "ghost": "+1 @yukw777 \nI need the links for my current project : i made a layer in my app to emulate them but the be able to use the real links would be great!\n. docker-py==0.3.1\n. Sorry I disappeared, went camping then got sick. I'll try my hand at a patch this evening. Do you have documentation of the process you like to go through? This will be my first OSS contribution.\n. requests 2.4.3-2\n. +1\n. ",
    "agoma8": "Sorry, I just read it.\nI was confused. I thought \"create_container\" was the same as \"docker run ....\"\nOtherwise, The error persists, I try:\nc.create_container('ubuntu', ..., volumes={\"/root/data\": \"\"})\nAnd the result is the same trace of my previous comment.\n. Yep! That's works. Thank you very much.\n. ",
    "dhrp": "+1 for fixing this. \nJust for ease of use, can we make docker-py accept a normal array instead? I'd suggest making it look like the ports as much as possible.\nc.create_container('ubuntu', ..., volumes=[\"/root/data\"])\nand \nc.start('ubuntu', ..., binds=[\"/root/data:/container/data\"])\n. cool :-)\n. Because I needed it, I already implemented the logic in my own script. It's still incomplete, but might be of use for you. https://github.com/dhrp/docker-runner\n. perhaps @vieux also wants to take a look at this, as it is related to the remote-api\n. Thanks @shin- I pinged @vieux and it has been fixed in Docker\nhttps://github.com/dotcloud/docker/pull/2662\n. ",
    "aboe76": "Just too make distribution easier, if distros ship the new python package they don't have to keep the old one, \nI can understand a reject if the update isn't backward compatible.\n. Another security incentive:\n2.0.1 (2013-10-24)\nUpdated included CA Bundle with new mistrusts and automated process for the future\n. Thanks, tested on my archlinux system and it works, hope the unit test get fixed without too much trouble.\n. ",
    "scottbessler": "any plans to merge this? can I help at all?\n. ",
    "nikicat": ":+1: \n. :+1:  I'm waiting for this feature\n. Hi, I've faced the similar issue, that's how I've workarounded it https://github.com/yandex-sysmon/dominator/blob/1b4fe17f0556274887cc9b2fdb1826239d9288e2/dominator/actions.py#L209-L235\n. :+1: \n. :+1: \n. ",
    "j-bennet": "+1\n. It's not exactly a duplicate as this is requested for an exec instance, not a container instance. Please add!\n. ",
    "itsafire": "+1\nAre there any obstacles to be resolved for this feature to be implemented ? exec_create being able to stream stdin into a running container and this be processed by a given command would greatly simplify configuration. Right now this use case is to be solved quite awkwardly. \n. ",
    "pacoxu": "In docker 1.13.0, does the secret feature work as you expected ?. Any update on this? \nwe get error during update service\n{\"message\":\"Invalid service version '': strconv.ParseUint: parsing \\\"\\\": invalid syntax\"}\nas https://forums.docker.com/t/docker-swarm-scale-service-using-update-api/19589\n\nThe reason seems to be \"missing version parameter\"\n?version=**\n. @atakada \nFinally, we found that you have to use the update API like\n\nPOST /services/[service_id]/update?version=[current-version-index]\n{\n \"Name\": \"\"\n\"TaskTemplate\" : {*}\n}\n\nIf you use service name in the API URL path, you will get \"not found error\";\nif you use wrong version, you will get \"not in sequence error\"\nHope it can help you.\n. 1. Remote API 1.25: https://docs.docker.com/engine/reference/api/docker_remote_api_v1.25/#/create-a-service\n   RestartPolicy \u2013 Specification for the restart policy which applies to containers created as part of this service.\n   Condition \u2013 Condition for restart (none, on-failure, or any).\n2. Docker cli: https://github.com/docker/docker/blob/c2d6e76a704604a964d2a593b8634d0bcbc53cd0/docs/reference/commandline/service_create.md\n     --restart-condition string       Restart when condition is met (none, on-failure, or any)\n. @arkkanoid \nI can not reproduce it with docker-py 1.10.3 as well.\n\n\n\nimport docker\nc = docker.from_env()\nhc = c.create_host_config(pids_limit=20)\ncid = c.create_container('redis', 'true', host_config=hc)\nc_data = c.inspect_container(cid)\nassert c_data['HostConfig']['PidsLimit'] == 20\nassert c_data['HostConfig']['PidsLimit'] == 19\nTraceback (most recent call last):\n  File \"\", line 1, in \nAssertionError\n. According to docker remote API, https://docs.docker.com/engine/reference/api/docker_remote_api_v1.25/#/create-a-container\n\n\n\nShmSize - Size of /dev/shm in bytes. The size must be greater than 0. If omitted the system uses 64MB.\nI think your system may not have enough memory. That's why you meet this issue.\nTry 1 GB if your system can.\n. ut not passed, so close it and try to fix it. @shin-  commits are squashed and use 'None' directly.. Is there any plan or roadmap for 1.13?  \nCan we just create PR for docker 1.13 new features? For instance, \"docker system df\" will be supported in docker 1.13. Where can I find the new Remote API > 1.24 ?. you need login the docker repo first and then create service with registry auth.\nfor instance, the default repo is docker.io \n\ndocker login daocloud.io\ndocker service create --with-registry-auth -p 80  daocloud.io/daocloud/dao-2048:latest. auth is a header for creating service\n\nYou can just try this and the service can be run successfully.\n\nimport docker\nclient = docker.DockerClient(base_url='unix://var/run/docker.sock')\nclient.login('admin', 'admin' , registry='https://registry_ip/')\nclient.services.create(\"registry_ip/daocloud/dubbo\")\n\nAnd if you don't login, you will get error like \n\nimport docker\nclient = docker.DockerClient(base_url='unix://var/run/docker.sock')\nclient.services.create(\"registry_ip/daocloud/dubbo\")\n\ng0tkajlos1h9   \\_ infallible_kowalevski.1  registry_ip/daocloud/dubbo  HOST  Shutdown       Rejected 18 seconds ago  \"No such image: registry_ip\u2026\". ",
    "ureyes84": "Thank you very much! We are using docker in prod lol , we love it!\nPS: No rush, just wanted to share the excitement.\n. I haven't been able to allow port redirection with 0.6.5. I followed the docker blog  and now I'm starting the container explicitly indicating the port I want open:\npython\ncontainer = self.docker_client.create_container(repository, command = None, detach = True, ports=[str(self.private_port)])\nBut I invariably get None in the PortMapping json as shown below\nAlso docker ps shows 8787/tcp instead of the random (redirected) port in the host\n. Thanks!\n. The solution by hand would do for now, thank you!\nI'd like to help implementing this feature. I'm assuming this could be a parameter in start. However, the API doesn't seem to offer this option. Will it require changes on the API then?\nOr will it use client.wait?\n. I agree. The way to go is just to follow the example your provided above using the client.wait command. Maybe this should be added as a note to the docs :)\n. This doesn't seem to error out but I it doesn't work either (takes all CPUs):\npython\nself.client.start(container, lxc_conf = self.get_lxc_conf())\ndef get_lxc_conf(self):\nreturn[\n    #All containers get the same share\n    {'lxc.cgroup.cpu.shares':1},\n    #Restrict container to a core\n    {'lxc.cgroup.cpuset.cpus':self.cpu_number}\n]\nThe CLI counterpart is able to limit this CPU-intensive script to a single CPU:\ndocker run -i -t -lxc-conf=\"lxc.cgroup.cpu.shares=1\" -lxc-conf=\"lxc.cgroup.cpuset.cpus=0\" my_image /bin/bash stress_cpu.sh\n. Interestingly, after running \ndocker run -i -t -lxc-conf=\"lxc.cgroup.cpu.shares=1\" -lxc-conf=\"lxc.cgroup.cpuset.cpus=0\" my_image /bin/bash stress_cpu.sh\nI can just do\ndocker start 104059c2e613\nAnd the previous resource limits are applied. However if I do afterwards the equivalent (I think) with docker-py, the  resource limits are ignored:\npython\nself.docker_client.start('104059c2e613')\n. Thank you very much! They Key Value syntax helped. I didn't see that coming though, do you think the docs should be updated?\nAlso, thank you for explaining the cpu shares. My intent is to limit resources, I think restricting the number of cores is the way to go. \nQuick question regarding this, say I have a dedicated server that creates ALL containers with:\npython\n[{\"Key\":\"lxc.cgroup.cpu.shares\",\"Value\":\"1\"}]\nDoes it really make a difference adding this parameter? I was wondering whether this is useful only when you need to give more \"weight\" to a specific container and set the Value parameter to something higher.\n. Thanks again for your help\n. @denibertovic Sorry about that :) I've updated the pull request.\n. Why do tests fail in Travis?\nERROR: InvocationError: '/home/travis/build/dotcloud/docker-py/.tox/flake8/bin/flake8 docker tests'\n....\nERROR:   flake8: commands failed\n. Got it, I'll update and resubmit. Thanks\n. Happy to help :)\n. Oops, I may be confusing the flags for the daemon vs containers. My intent is to start the containers in lxc mode. The docker blog suggests the following:\ndocker -d -e lxc\nThis is in regards to this issue in the docker repo: https://github.com/dotcloud/docker/issues/4587\n. I'm assuming the blog post refers to the docker daemon. Please feel free to close.\n. We are also seeing the following error during the same conditions as above. It almost seems like docker is getting overwhelmed:\nFile \"/home/mark/.virtualenvs/dev/local/lib/python2.7/site-packages/docker/client.py\", line 975, in start\n\u2002\u2002\u2002\u2002self._raise_for_status(res)\n\u2002\u2002File \"/home/mark/.virtualenvs/dev/local/lib/python2.7/site-packages/docker/client.py\", line 89, in _raise_for_status\n\u2002\u2002\u2002\u2002raise errors.APIError(e, response, explanation=explanation)\nAPIError: 500 Server Error: Internal Server Error (\"Cannot start container 6b6e90804cd041fb4bbb5b7872bf2c4ec8b8086a4054a87cd76ec35826d2267a: Process could not be started\")\n. Seems plausible. We are lunching 14 containers with heavy processing on 1 computer with 32 cores and 128 GB of RAM\n. ",
    "joshuaconner": "+1 to not preserving backwards compatibility. 0.6.5 broke backwards compatibility so I'd expect docker-py to have done the same to track the upstream changes.\n. Is it because your server is bound to 127.0.0.1? Have you tried binding to 0.0.0.0 instead? I'm not sure this is a good solution but it might be a solution...\nEdit: never mind!\n. Is there a way to push this to pip? Would be super handy to be able to do\nimport docker\nif hasattr(docker, '__version__'):\n  # 0.3.0 stuff probably, check to be sure\nelse:\n  # < 0.3.0 stuff\nwith the new version\n. Is there any chance of doing a bugfix release for this? It would be awesome for docker-py to be fully functional under Docker 0.9.\n. ",
    "mpetazzoni": "As per discussion in #76, Docker itself broke backwards compatibility with 0.6.5. Should docker-py do the same and not care about Docker < 0.6.5?\n. Good point. I'll write something.\n. You're right. I should be able to easily overload the post, get and delete methods and just pass-through to super.{post,get,delete}() simply adding the timeout parameter. I'll see what I can do if I find some time today.\n. Yeah, I saw there was a lot of changes in the meantime for v1.6 API support :( Thanks for powering through though! :+1: \n. Let me rebase this on the most recent master so you don't have to resolve merge conflicts again.\n. There we go.\n. It seems like we're getting closer to the default behavior of the Docker daemon to be change towards automatically creating missing host-side bind-mounted directories. This change would thus no longer be needed. I'll close the pull request when that's confirmed.\n. Let me look into it. Hopefully we can get this merged in today.\n. Hmm, the unit tests are running fine here:\n```\npython -m unittest tests.test\n......................................................\n\nRan 54 tests in 0.033s\nOK\n```\nIt looks like just an ordering issue though. Is the order of the links returned deterministic?\n. Ah, it only fails with Python 3.3...\n. Ok, the problem is that the links argument to client.start() is a dictionary, which isn't sorted. I've just added a change that creates the Links parameter in a deterministic manner by using the sorted list of keys, which in turns allow the unit test to be deterministic.\n. When you set stream=True this output makes a lot more sense. But I agree that if you don't care about the real-time progress then we can probably \"summarize\" it into a more useful result data structure before returning. Would you be interested in taking a crack at it?\n. Can you fix the style errors on merge or do you want me to make an extra commit with the fixes?\n. Flake8 issues should be fixed. Build failed because of tests and will keep doing so until my other pull request gets merged in.\n. Rebased and improved with log streaming support.\n. This test issue was fixed in the other pull request.\n. Otherwise LGTM\n. urljoin() will not do the job here as urljoin() will not preserve the path in the base_url. For example:\n```\n\nurlparse.urljoin('unix://var/run/docker.sock', '/v1/images')\n'/v1/images'\n```\n\nThe simplest is to remove trailing slashes in the base_url in the constructor and let self._url do its job.\n. I believe this was fixed by #107 already.\n. There is, it's in the documentation of create_container(). But the non-wrapping makes it hard to see, I agree.\n. Ping @shin- \n. The tests are fine but not the flake8 validation (for code style). Look at https://travis-ci.org/dotcloud/docker-py/builds/15471718#L292 (from lines 292 to 313) for all the styling errors you need to fix for the build to pass.\n. No, thank you!\n. Tests and flake8 pass, also added integration test for the JSON config file load\n. /ping @shin- \n. That's correct. Feel free to take a crack at it, I'll happily review it!\n. Yeah I think I stumbled upon this too. I'll try to take a look.\n. Thanks!\n. Thank you!\n. Hmm, interesting. I did do some changes recently to log streaming so it certainly is the cause of your issue. Have you confirmed that this only happens with Python 3.3? (I would think so, since I have no issues with 2.7).\n. Thanks! Have you validated the rest of the functionality? Streamed and non-streamed logs, for example, is a good test.\n. Thanks!\n. Thanks!\n. Good idea! Thanks!\n. Could you fix the style error and squash these two commits together? Thanks!\n. That's an interesting change indeed, and overall LGTM. Can you fix the style issues (lines are too long, see the Travis CI output), and add documentation of that new method into the README.md?\nThanks!\n. I haven't looked closely at the code of all these methods, but on principle I'd tend to agree with @aanand here.\n. @aanand Would you mind reworking the history a little bit so that we don't see the rename and have it directly implemented as the new attach()? Pretty sure you can just squash all 4 commits into one with a good commit message. TIA!\n. Awesome! Thanks a lot!\n. LGTM. Thanks!\n. Hmm, what version of docker-py are you using? Is it latest master?\nI think you're not using the correct combination of parameters in the create_container() and start() calls. You had both right, but not at the same time. As you can see I'm not able to reproduce:\n```\n30> c.containers()\n30: []\n31> container = c.create_container('stackbrew/ubuntu:12.04',\n        command=['sleep', '15'], ports=[1338])\n32> c.start(container, port_bindings={1337: ('0.0.0.0', 1338)})\n33> c.inspect_container(container)\n33:\n{u'Args': [u'15'],\n u'Config': {u'AttachStderr': True,\n  u'AttachStdin': False,\n  u'AttachStdout': True,\n  u'Cmd': [u'sleep', u'15'],\n  u'CpuShares': 0,\n  u'Dns': None,\n  u'Domainname': u'',\n  u'Entrypoint': None,\n  u'Env': [u'HOME=/',\n   u'PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'],\n  u'ExposedPorts': {u'1338': {}},\n  u'Hostname': u'a273f19ab1ba',\n  u'Image': u'stackbrew/ubuntu:12.04',\n  u'Memory': 0,\n  u'MemorySwap': 0,\n  u'NetworkDisabled': False,\n  u'OpenStdin': False,\n  u'PortSpecs': None,\n  u'StdinOnce': False,\n  u'Tty': False,\n  u'User': u'',\n  u'Volumes': None,\n  u'VolumesFrom': u'',\n  u'WorkingDir': u''},\n u'Created': u'2014-01-14T18:07:59.031292786Z',\n u'Driver': u'aufs',\n u'HostConfig': {u'Binds': None,\n  u'ContainerIDFile': u'',\n  u'Links': None,\n  u'LxcConf': None,\n  u'PortBindings': {u'1337/tcp': [{u'HostIp': u'0.0.0.0',\n     u'HostPort': u'1338'}],\n   u'1338': None},\n  u'Privileged': False,\n  u'PublishAllPorts': False},\n u'HostnamePath': u'/var/lib/docker/containers/a273f19ab1babb363fe8b209a55c64c6cf9430eb5fc65f42eb1a0668529ba38c/hostname',\n u'HostsPath': u'/var/lib/docker/containers/a273f19ab1babb363fe8b209a55c64c6cf9430eb5fc65f42eb1a0668529ba38c/hosts',\n u'ID': u'a273f19ab1babb363fe8b209a55c64c6cf9430eb5fc65f42eb1a0668529ba38c',\n u'Image': u'3aa646e4f1d2bd718c20174a38a87ac33a223ccd70fde655185197541ab6fcab',\n u'Name': u'/prickly_thompson4',\n u'NetworkSettings': {u'Bridge': u'docker0',\n  u'Gateway': u'172.17.42.1',\n  u'IPAddress': u'172.17.0.62',\n  u'IPPrefixLen': 16,\n  u'PortMapping': None,\n  u'Ports': {u'1337/tcp': [{u'HostIp': u'0.0.0.0', u'HostPort': u'1338'}],\n   u'1338': None}},\n u'Path': u'sleep',\n u'ResolvConfPath': u'/etc/resolv.conf',\n u'State': {u'ExitCode': 0,\n  u'FinishedAt': u'0001-01-01T00:00:00Z',\n  u'Ghost': False,\n  u'Pid': 17661,\n  u'Running': True,\n  u'StartedAt': u'2014-01-14T18:08:02.908042711Z'},\n u'Volumes': {},\n u'VolumesRW': {}}\n34> c.containers()\n34:\n[{u'Command': u'sleep 15',\n  u'Created': 1389722879,\n  u'Id': u'a273f19ab1babb363fe8b209a55c64c6cf9430eb5fc65f42eb1a0668529ba38c',\n  u'Image': u'stackbrew/ubuntu:12.04',\n  u'Names': [u'/prickly_thompson4'],\n  u'Ports': [{u'IP': u'0.0.0.0',\n    u'PrivatePort': 1337,\n    u'PublicPort': 1338,\n    u'Type': u'tcp'},\n   {u'IP': u'', u'PrivatePort': 0, u'PublicPort': 1338, u'Type': u'tcp'}],\n  u'SizeRootFs': 0,\n  u'SizeRw': 0,\n  u'Status': u'Up 8 seconds'}]\n35>\n``\n. Ok, I found what the problem was. When creating the container, if you don't define the protocol type (TCP or UDP), theHostConfigwill contain an invalid port mapping for the same port that was requested on thestart()` call:\nu'Ports': {u'8000': None,\n   u'8000/tcp': [{u'HostIp': u'0.0.0.0', u'HostPort': u'8082'}]}},\nThis seems to confuse Docker and it doesn't create the iptables NAT rule. If you do the following, it works:\n```\n43> container = c.create_container('stackbrew/ubuntu:12.04',\n    command=['python', '-m', 'SimpleHTTPServer'], ports=[(8000, 'tcp')])\n43> c.start(container, port_bindings={8000: ('0.0.0.0', 8082)})\n44> c.inspect_container(container)['HostConfig']\n44:\n{u'Binds': None,\n u'ContainerIDFile': u'',\n u'Links': None,\n u'LxcConf': None,\n u'PortBindings': {u'8000/tcp': [{u'HostIp': u'0.0.0.0',\n    u'HostPort': u'8082'}]},\n u'Privileged': False,\n u'PublishAllPorts': False}\n45> c.inspect_container(container)['NetworkSettings']\n45:\n{u'Bridge': u'docker0',\n u'Gateway': u'172.17.42.1',\n u'IPAddress': u'172.17.0.99',\n u'IPPrefixLen': 16,\n u'PortMapping': None,\n u'Ports': {u'8000/tcp': [{u'HostIp': u'0.0.0.0', u'HostPort': u'8082'}]}}\n```\nChecks out:\n$ sudo iptables -t nat -L -v -n | grep 8000\n    0     0 DNAT       tcp  --  !docker0 *       0.0.0.0/0            0.0.0.0/0            tcp dpt:8082 to:172.17.0.99:8000\n$ docker ps\nCONTAINER ID        IMAGE                                                           COMMAND                CREATED             STATUS              PORTS                                                                                              NAMES\n159381063215        stackbrew/ubuntu:12.04                                          python -m SimpleHTTP   26 seconds ago      Up 6 seconds        0.0.0.0:8082->8000/tcp                                                                             tender_archimedes\n$ echo \"HEAD / HTTP/1.0\" | nc localhost 8082\nHTTP/1.0 200 OK\nServer: SimpleHTTP/0.6 Python/2.7.3\nDate: Wed, 15 Jan 2014 00:28:16 GMT\nContent-type: text/html; charset=ANSI_X3.4-1968\nContent-Length: 846\n. @shin- If TCP is the default for port redirects, I think we should modify _container_config() to set proto to tcp if it's not provided, defined or is empty, to avoid this problem. What do you think?\n. @denibertovic In Docker itself you mean? I really find it strange that Docker reports a port number without either /udp or /tcp at the end.\n. So, what's the final word? I agree that the Docker API shouldn't accept it, but to prevent people from falling into the trap/bug while using docker-py we could default to tcp in create_container() if the protocol is not specified, what do you think?\n. Thanks!\n. Damn, I ran the test locally and they passed, but I it didn't do a relative import so it was not running the new code. Sorry about that, should be good now.\n. LGTM. I'm just concerned at the large number of parameters of the create_container() method. @shin- what's your take on this?\n. While we're on the subject, does this mean Docker does not support mounting the same host directory in two different locations inside the container?\n. One the command-line yes, but what about through the API? If it takes in a dictionary of host_dir -> bind_dir, you're hosed.\n. Thanks!\n. From @jakedt:\n\nIt looks like it's only broken if you tell the client to user API revision\n1.9, which switches to the json block generating StreamFormatter in the\ndaemon. It doesn't have newlines between the json chunks, and i couldn't\nget requests to dump chunks with iter_chunks either.\n. Ah yes sorry overlooked that. I actually stumbled upon that problem in Maestro recently. I yields a slightly bigger question though for docker-py as to how we want to handle multiple versions of the Docker remote API in our code. It will quickly become very messy. Is this something that could be solved by inheritance, having different implementations for the various versions of the API, overloading on top of each other?\n. Thanks. I agree. We should definitely release a 0.8.0-compatible version soon (see the 0.2.4 milestone I created recently). I also think we should freeze docker-py versions that match Docker versions, hopefully as soon as possible after each Docker release. What do you think?\n. We might need to update the build() method, since the endpoint changed quite a bit in recent Docker versions.\n. I think we should fix the JSON stream parsing first. I'll take a crack at it today. If I don't get it working, we can release 0.3.0 tomorrow. Sounds good?\n. See #167 . There isn't much we can do to parse this correctly as is without being stupidly O(n^2) because the JSON objects are not delimited.\n. @jakedt Not sure why we never got it to work before, but with my changes in #167 I have it working on API version 1.9 without issues. We were just not using the right stream helper. Feel free to give https://github.com/mpetazzoni/docker-py/tree/build-output a try and let me know.\n. Yay!\n. @mmerickel yes that's a known issue due to a change in Docker itself where they no longer separate each JSON block by a newline character. It makes it incredibly harder to extract out the individual JSON blocks for parsing in an efficient manner (aka without being O(n^2)). Figuring out a solution to this problem is on my todo list.\n\nSee #159. I'll close this issue as a duplicate. Thanks for reporting it though!\n. Hi @pate,\nYou're right, this in missing from the documentation. We're in a bit of a limbo state right now because the last release available on PiPy (for pip install docker-py or easy_install docker-py) is showing its age and probably not compatible with the couple last releases of Docker.\nWe're in the process of putting together a new release of docker-py, when we do so we'll make sure we provide accurate installation steps in the documentation.\nIn the meantime, I recommend you use (without doing a git clone or git pull):\n$ pip install --user --upgrade git:git://github.com/dotcloud/docker-py\n. ping @shin- \n. Alright, and this should fix events() too.\n. Thanks!\n. That would require pushing a 0.3.1 I think. /cc @shin- \n. Thanks!\n. Thanks!\nI think recvall should return a string in all cases, since that's the type that's expected downstream. Could you look into that? Also, i think it'd be cleaner and more Pythonic (albeit, equivalent) to use something like:\npython\ndata = bytes() if six.PY3 else str()\n. I'm not sure there is much we can do here. I don't think there is a way to specify different dependencies based on the Python version with pip/requirements.txt.\n. I think explicit key name is better, but it should remain read-write by default, since that's the current behavior.\npython\n{\"/home/host-data/\": {\n    \"bind\": \"/mnt/data\",\n    \"ro\": True,\n}}\nWith the modification to start() that goes along with it obviously.\n. The reason for this is that from API version 1.8 onwards the /build endpoint IIRC always returns a stream.\n. Thanks!\n. I don't believe docker-py follows the same contribution rules as the main Docker project, so for now it's fine.\n. I can't reproduce either. It's most likely a legitimate timeout, especially on such a common term like ubuntu. It entirely depends on the speed at which the Docker index replies, which can be a while at times.\n. Thanks. cmp() doesn't exist in Python 3 though, as shown by the build failing. Could you fix this in your change? Thanks!\n. Awesome, thanks!\n. /cc @shin- \n. The /build endpoint requires stream=True from API version 1.8 onwards, which is why your stream=False is not respected if you don't use an older API version.\nWe just merged some changes that might fix this too. Could you double check with the very latest code from master?\nThanks~\n. I've run into this problem as well and I believe it was reported before. Wasn't this fixed by #141 ?\n. Similarly, can't it be implied by the fact that the base_url starts with https:// ?\n. +1\n. What version of docker-py are you running? Can you look at your Docker daemon's logs (in /var/log/upstart/docker.log) and check what endpoint is being called by images()?\n. Does it work without quiet=True?\nI wasn't able to reproduce your problem here with Docker 0.8.0, hitting both API version 1.6 and 1.9 (latest). I ran into a distinct problem with quiet=True with API version 1.6, but downstream from the error you're seeing.\nWhat version of Docker are you running? Can you try without quiet=True and with different API versions?\n. Is the -e an command-line flag to the Docker daemon itself? If yes, then docker-py cannot control that, as it merely talks to an already running Docker daemon.\n. client.logs(container)\n. client.logs(container) will return a string, not display it. If you don't do anything with it in your program, it's not going to show anything.\n. What version of Docker are you running? And what version of docker-py?\nI can't reproduce the problem:\n1> import docker\n2> client = docker.client.Client(base_url='http://192.168.10.2:4243')\n3> container = client.create_container('stackbrew/ubuntu', 'echo \"hello world\"')\n4> container\n4:\n{u'Id': u'3e8dcd68d7c4a4b0e588ab0e848867ec061901e2f4fb89b3c410560157d29d2d',\n u'Warnings': None}\n5> client.start(container)\n6> print client.logs(container)\nhello world\n7> client.version()\n7:\n{u'Arch': u'amd64',\n u'GitCommit': u'cc3a8c8',\n u'GoVersion': u'go1.2',\n u'KernelVersion': u'3.8.0-34-generic',\n u'Os': u'linux',\n u'Version': u'0.8.0'}\n8> docker.__version__\n8: '0.3.0'\n. That's fine. Projects that depend on docker-py should specify what version they depend on anyway. Go ahead and merge!\n. Thanks for reporting this. I don't think we've validated docker-py with Docker 0.10 yet. Correctly returning the streamed lines has been a constant challenge because Docker changed the outputted stream format, and it's wasn't consistent across all endpoints. Hopefully 0.10 brings some sanity to this.\n. Thanks. See my comments, make sure flake8 is happy and update README and this should be good to go!\n. See the Travis CI output for the remaining flake8 errors. Can you also make sure you squash the two commits? The second one is just a revert of a change made in the first one...\nThanks!\n. Duplicate of #203 \n. Thanks!\n. Looks fine. @shin-, what do you think?\n. @bfirsh Very good point. It should be the role of the program calling/using the library.\n. Cool. Have you confirmed this also still works correctly with Python2? Any change you can come up with a unit test for the change?\n. Awesome. Thanks a lot!\n. We've been back and forth on this several times indeed. If this has been successfully tested (on all methods!) with both Python 2.x and 3.x, talking to Docker daemons 0.7, 0.8, 0.9 and 0.10, then I'm ok with the change.\n. LGTM then. Thanks a lot!\n. Any particular reason why this can't be achieved with this?\n```\napt-get install python-pip\npip install docker-py\n```\n. Agreed. I personally abhor language-specific package maintainers because you end up with a mixed soup of distribution-installed and language-installed packages. But the problem with distribution packages is that it becomes impossible to have multiple versions of the same package installed on the system (hence virtualenvs, etc).\nThat being said, that's a problem that the user can deal with if they have/want to, and doesn't really prevent us from creating .deb and .rpm packages of docker-py. Do you feel like you'd be able to take a crack at it and send a pull request?\nThe most difficult part is to then get these packages into the distribution, but in the meantime maybe they can be distributed on `get.docker.io?\n. Thanks for reporting this. This is a known issue we need to look into. We still need a way for the library to advertise, in some way, the minimum requirements.\nClosing as a duplicate of #101.\n. Could it be a timeout issue? What if you have RUN sleep 11 in your Dockerfile?\n. What versions of docker-py and Docker are you running? I can't reproduce the issue with Docker 1.0 and the current docker-py master.\n. Didn't you mean to use /v1/_ping in the command executed inside the VM too?\n. @pmyjavec My understanding is that even though the __version__ says 0.3.0 you actually have the 0.3.1 code installed, as the docker-py-0.3.1.tar.gz tarball was uploaded to PiPy when 0.3.1 was released, not when 0.3.0 was released.\n. Ah yep, that would be an issue. I think a 0.3.2 isn't too far ahead though, if you can bear to wait.\n. For comparison, what's the command line that you use to push the image (that works)?\nCould you also try using stream=True on the push() call and printing out the output:\npython\nfor line in c.push(..., stream=True):\n    print line\nAnd give us the output? Also, what version of docker-py and of Docker are you using?\nThanks!\n. +1 for a 0.3.2 release\n. That'd be cool, then I can remove the code that does that from Maestro :)\nIf you don't want to have to think about how to implement it, just rip it off https://github.com/signalfuse/maestro-ng/blob/a4c3774b15cd06db9f16b8e339a4dd7c64e1a689/maestro/entities.py#L301\n. I agree, the command is not meant to be a full shell pipeline of commands, but a single process. bash -c is the way to go.\n. :shipit: \n. Updated with fixes to the test which I forgot to run before ;)\n. I'd need to take a closer look. Ping me again if I haven't replied in a few days.\n. Ok, so if I understand correctly, all these functions now always return a generator, and the stream argument decides whether or not it will become blocking to wait for new content, or be a \"finite\" generator over what was available in the response at the time of the call?\n. FWIW LGTM\n(lol acronyms)\n. LGTM\n. I agree with the second part of the statement here. Pinging the registry from the app that uses docker-py doesn't guarantee anything as to whether the (most likely remote) Docker daemon will be able to push/pull or not.\n. LGTM. @shin- ?\n. LGTM. This is a breaking change for people relying on history() though, so it should be documented as such in the ChangeLog.\n. I'm not sure I understand your first question. For your second point, you can get that easily by inspecting the container as you mentioned:\npython\ndetails = c.inspect_container(container)\nip = details['NetworkSettings']['IPAddress']\n. That's normal. Port bindings are declared when creating the container, but also activated when starting it. The docker run command does both. When using docker-py you need to specify the port bindings in c.create(...) and c.start(...).\n. FWIW when you use a list for the command, you don't need quotes at all. Each element of the list is exactly one command line argument, even if it contains spaces.\npython\ncommand = ['/usr/bin/xvfb-run', '-a', '-s', '-screen 0 1600x1200x24', '...']\nHTH\n. Are you connecting to your Docker daemon via a UNIX socket? If yes, I believe this was fixed is 0.5.3. Re-open this issue if you're up-to-date and still see the problem.\n. Just made the tag and pushed it.\nhttps://github.com/docker/docker-py/releases/tag/0.5.3\n. I agree with @shin-. If for some reason Python's str() returns something strange (like \"1.14000000001\") then you'll hit the wrong endpoint without you knowing (other than via a 404).\n. Thanks!\n. Just ran into this, can confirm this is the root cause (and the proper fix).\n. :+1: \n. FWIW I'm still watching over what's going on here with a lot of attention and can definitely jump in whenever I'm needed. The only thing I can't do I think is push releases to PiPy.\n. Seems reasonable. Feel free to send a PR!\n. LGTM. @shin- ?\n. Thanks!\nConfirmed from https://docs.docker.com/reference/api/docker_remote_api_v1.18/#create-a-container\n. :+1:\nAs a side node, it'd be nice if the InvalidVersion exception actually took the minimum required version as an argument so that we can emit, in all cases, a more structured error message.\n. Can you write a better commit message for this one? Out of context, it's hard to know what \"adapter\" you're talking about, or what the bug was.\n. Unless you expect to have other subclasses of ClientBase, I don't really see the point.\nIf readability of the Client class is a concern, then yes some utility methods need to be extract, but in a way that doesn't depend on instance state (aka by taking what they need as arguments). Then you can extract those methods into one or more side helper modules.\n. Alright, let's work from there.\n. :+1: \n. :+1:  LGTM. Might be worth squashing those together.\n. Awesome!\n. :+1: \n. LGTM. Like @aanand not a huge fan of mixins either, but this was getting out of hand so this will definitely be more manageable!\n. You're right it's actually evaluated when the module is loaded, not when the function is called, which is not necessarily very intuitive. I'll fix that.\n. Using with is just a more \"Pythonic\" way.\n. I don't find this to be any better: https://gist.github.com/mpetazzoni/d7e4df97d9be006d21a0 since you need to manage file closing.\n. ",
    "paultag": "This is a massive hack. Not proud of it, but I figure I'll send something in to talk about.\n. nice, LGTM, +1\nNice, thanks, @shin- !\n. This is going to break if we get a data bit longer than 4096, since we parse json\n. (something to fix?)\n. ",
    "tlindener": "Hi, I'm currently trying to get started with docker-py but I don't really get the results I get with running the native commands.\nI would be very grateful to get some more documentation on how this works :-)\n. ",
    "bsdlp": "volume usage is ambiguous in the readme.\nare the volumes defined in create_container paths on the host or in the container?\n. ",
    "micahhausler": "I'll be building out sphinx docs today\n. @shin- @mpetazzoni \nWould you all like a PR to the master branch or an enhancement/documentation branch?\n. @leth are you going to try to fix those broken paths? I can make a stab at it, but I didn't want to duplicate any work you'd already done.\n. @SvenDowideit @fredlf Would you all prefer just axing the /docs folder and converting the RST in the docstrings to MarkDown? Do you prefer just .md files or something like MkDocs? \n. @shin-  or anyone else, any update? Markdown for sure? Using MkDocs or just Markdown files?\n. Thanks!\n. @shin- Great. Do you want a PR to master or a different branch?\n. I really think expansion on /docs/tls.md would be the most useful. Perhaps explaining what  \"Equivalent CLI options\" means. Like, \"If the daemon is running with these options ... you must connect like so ...\"\n. Personally, I just rebuilt b2d with the boot2docker/boot2docker#572  and disabled TLS. \n. I just realized there is a ChangeLog.md in the repo, I'm going to port that to the /docs/ directory\n. Just so you all can see the compiled documentation, I put it in S3 if you want to review that too. \nhttp://docker-py.s3-website-us-east-1.amazonaws.com/\n. This actually doesn't work.\n. @shin- sorry about that! Thanks for the heads up! Any chance you could cut a release soon with this fix? For now I'm left pointing my docker-py version to  pip install git+https://github.com/micahhausler/docker-py.git@1476ad302dd1b6622b7c53d3bea2270e658b0248\n. Thanks much!\n. ",
    "shreyu86": "@micahhausler +1\n. :thumbsup: \n. :thumbsup: thanks @shin- \n. @shin- I will remove it. I was over thinking :) \n. @discordianfish and @shin- added I can add a line or two in README.md for documenting this change is that OK?\n. added documentation. \n. :thumbsup: \n. +1\n. It was a 4XX from server. And when I used the docker cli directly I could see the Error but somehow when I pulled the image with the API it failed to show/notify me of that error as I did not print out the stream. \n. Actually false alarm! I can verify that it does raise an error, the wrapper that I used was swallowing it :(, I had wrapped boto and docker-py together to pull image on multiple hosts. Closing this issue thanks @shin- \n. +1 this sounds very interesting. \n. Because registry version below 1.0 does not support the restart flag, if I am wrong I can rip that out. \n. ",
    "bobobo1618": "Provisioning is so much faster now I don't have to use my hacked SSH command... Look forward to seeing this merged :grinning: \n. ",
    "dangra": "@denibertovic I updated the PR. thanks for reviewing it\n. :+1: I was looking for this.\n. - @mpetazzoni\n. travis-ci is failing due to unrelated issue IMHO: https://travis-ci.org/dotcloud/docker-py/builds/14236278\n```\npy33 runtests: commands[0] | /home/travis/build/dotcloud/docker-py/.tox/py33/bin/python tests/test.py\n...........................................F............\n======================================================================\nFAIL: test_start_container_with_links (main.DockerClientTest)\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.3/unittest/mock.py\", line 1087, in patched\n    return func(args, *keywargs)\n  File \"tests/test.py\", line 374, in test_start_container_with_links\n    \"Links\": [\"path2:alias2\", \"path1:alias1\"]\nAssertionError: {'Links': ['path1:alias1', 'path2:alias2'], 'PublishAllPorts': False} != {'Links': ['path2:alias2', 'path1:alias1'], 'PublishAllPorts': False}\n- {'Links': ['path1:alias1', 'path2:alias2'], 'PublishAllPorts': False}\n?                          ----------------\n+ {'Links': ['path2:alias2', 'path1:alias1'], 'PublishAllPorts': False}\n?            ++++++++++++++++\n\nRan 56 tests in 0.066s\n```\n. @mpetazzoni : thanks for pointing it. should be fixed now.\n. is trunk broken at the moment?  some tests failing with unrelated error:\nAssertionError: Command should not raise exception: local variable 'authcfg' referenced before assignment\n. the new compare_version was returning the result inverted and that triggered two hidden bugs in support code for older API versions.\nI rebased and included a fix for those errors too, but as the codepath for old API versions is not tested it is hard to tell when it will break again, how long do you plan to support old API versions in docker-py? \n. I haven't time to finish this PR at the moment, feel free to submit changes on top of it.\nthat said, setting None is a problem right now on Python3\n```\n\n\n\nmax(1, None)\nTraceback (most recent call last):\n  File \"\", line 1, in \nTypeError: unorderable types: NoneType() > int()\n```\n. \n\n\n",
    "jimfulton": "BTW, thanks for this module. It's going to make a current project a lot easier. :)\n. At the risk of repeating myself, best practice:\n- Libraries: either don't restrict versions of package dependencies, or restrict them to\n  ranges, ideally without upper bounds.\n- Applications: pin dependencies to particular versions.\nPinning library dependencies to specific versions makes it hard to reuse the library.\n. Or it might make sense not to return anything and error if you get a 404. Or return the fetched image id and error on 404.\nIt turns out I'm not using the output of pull, but calling images to get the image id for the pulled image, so I can detect a failure if the pulled image isn't returned from images.\n. Sure.\n. Nice compromise.\n. ",
    "OddBloke": "@shin- virtualenvs don't solve this problem if someone has something that depends on docker-py and one of your pinned dependencies; you'll still be forcing them to use a particular version of that dependency.\nPerhaps not a problem right now, but imagine a situation where a version of requests is released which introduces a new feature and a (perhaps unknown) regression, both unrelated to how docker-py uses it. Do you accept the PR from person A which bumps your pin up? What do you do when person B opens a PR reverting that change because the regression breaks their app? And then what does person A do with their codebase that is using this shiny new requests feature?\nIf you specified a minimum version that you require (rather than pinning at a specific version), then person A could use the shiny new feature and person B could stick with the version before regression.\n. ",
    "tianon": "So, as I mentioned on #218, I'm looking into this specifically in the context of packaging the library.  Currently, \"python-websocket\" is at 0.12 in Debian (which is the package for websocket-client), but the dep here is ==0.11, so I get fun errors that aren't easy to workaround without patching the requirements.txt myself, which I don't think you as an upstream would appreciate much.\n. Force pushed an amendment to fix my flake8 issue. :)\n. Fair enough. :)\n. This is already done in Debian via python-docker, which I'm working on updating to help out @paultag, but I've run into a snag where the requirements.txt file specifies exact versions (==) of deps instead of >=; is there a good reason for this that someone can point me to?\n. Thanks for the pointer @shin-\n. Is there a particular reason this one has a custom timeout value?\n. Just curiosity! :)\n. ",
    "lphoward": "You should heed sound advice, born of great experience, gently offered twice.\n. ",
    "vmalloc": "+1\nstream=True only makes partial sense IMO, because waiting too long before polling the generator once again concatenates strings:\n``` python\n\n\n\nc = docker.Client()\ngen = c.pull(\"ubuntu\", tag=\"latest\", stream=True)\ntime.sleep(60)\nnext(gen)\n'{\"status\":\"Pulling image (latest) from ubuntu\",\"progressDetail\":{},\"id\":\"ad892dd21d60\"}{\"status\":\"Pulling image (latest) from ubuntu, endpoint: https://cdn-registry-1.docker.io/v1/\",\"progressDetail\":{},\"id\":\"ad892dd21d60\"}{\"status\":\"Pulling dependent layers\",\"progressDetail\":{},\"id\":\"ad892dd21d60\"}{\"status\":\"Download complete\",\"progressDetail\":{},\"id\":\"511136ea3c5a\"}{\"status\":\"Download complete\",\"progressDetail\":{},\"id\":\"e465fff03bce\"}{\"status\":\"Download complete\",\"progressDetail\":{},\"id\":\"23f361102fae\"}{\"status\":\"Download complete\",\"progressDetail\":{},\"id\":\"9db365ecbcbb\"}{\"status\":\"Download complete\",\"progressDetail\":{},\"id\":\"ad892dd21d60\"}{\"status\":\"Download complete\",\"progressDetail\":{},\"id\":\"ad892dd21d60\"}'\n```\n. \n\n\n",
    "bfirsh": "Compose's json_stream helper is now included in the development version which will help enormously with this. See here for an example of an implementation with build:\nhttps://github.com/docker/docker-py/blob/1984f68730512a1c07017118f4e229c7949ff8a8/docker/models/images.py#L96-L169. Are there any existing mock implementations that could be improved and pushed upstream? We need a mock to test https://github.com/orchardup/fig and we're not sure whether to start from scratch or not.\n. It's far from complete, but it exists now! :tada: \nhttps://github.com/docker/docker-py/blob/master/tests/unit/fake_api_client.py. Perhaps worth updating requirements.txt too?\n. (1) implemented in #153. Although I like the idea of the dictionary being the \"right\" way around, I also like how this way is backwards compatible and the same way round as the links option in docker run.\n. Sorry, I should have been clearer. docker-py supports some of the values of DOCKER_HOST. Here are some things it doesn't support, for example:\n- tcp://\n- :4243\n- localhost\n- localhost:4243\nIt should all work if the logic from Docker's ParseHost is reproduced.\n. I'm not sure I like the idea of a library reading environment variables \u2013\u00a0that feels like the job of the thing calling the library. E.g. docker.Client(os.environ.get('DOCKER_HOST')).\nAlso see #140.\n. Might be worth documenting that usage \u2013\u00a0in particular, that base_url is intended to be the same format as the DOCKER_HOST environment variable.\n. https://apps.fedoraproject.org/packages/python-docker-py\nhttp://packages.ubuntu.com/search?keywords=python-docker\nLooks like we're all good here.. Closing in favour of #946 which seems to be more complete and less ancient. Feel free to reopen if you have an implementation, @Piwosz!. Compose's json_stream helper is now included in the development version. For an example of how to use it, see this implementation:\nhttps://github.com/docker/docker-py/blob/1984f68730512a1c07017118f4e229c7949ff8a8/docker/models/images.py#L96-L169. LGTM\n. /cc @aanand\n. There's an implementation here: https://github.com/orchardup/fig/pull/294\n. What @mminer said. :+1:. This has now been replaced by #347.\n. LGTM\n. Using the API without a version will be deprecated in the next release: https://github.com/docker/docker/pull/27830\nThis makes this a issue a moot point, but I think version='auto' solves the problem pretty well anyway. :). Work on supporting libtrust in Python is happening here: https://github.com/public/python-libtrust\n. @public \u2013\u00a0is there any way @dlorenc can help out?\n. @aanand might know.\n. Work on identity auth has been stalled indefinitely. :(\nMight be worth closing this issue...\n. @dlorenc It's stalled in general I'm afraid. Machine is going to use TLS certs.\nThe only discussion is here, I think: https://github.com/docker/docker/pull/8265\n. https://github.com/docker/docker-py/issues/1186. :tada:. Closing as a dupe of #998.. Fixed in #1186!. This is mostly fixed in #1186 now, but it doesn't include some external exceptions like timeouts. Perhaps we should close this is favour of handling those on a case-by-case basis.. This probably makes sense as a separate library - docker-py is just for the Engine API.. @aabdulwahed Are you still having this issue? If so, feel free to re-open the issue and we can take a look. :). There's now a mock fake client: https://github.com/docker/docker-py/blob/master/tests/unit/fake_api_client.py\nDo you think something more elaborate than this is required?. Some of Compose's functionality is now included in docker-py. Here's an example of the implementation:\nhttps://github.com/docker/docker-py/blob/1984f68730512a1c07017118f4e229c7949ff8a8/docker/models/images.py#L96-L169. Carried and merged in https://github.com/docker/docker-py/issues/1186. :tada:. Fixed in #1186.. Ah, sorry. It's not actually released yet with documentation, but in 2.0 you'll be able to do things along the lines of: client.containers.run(\"busybox\", volumes=['/var/www:/mnt/vol1:ro', '/home/user1/:/mnt/vol2']). Ditto create().. Fixed in #1186! Also, continued \"clean up documentation\" discussion can continue in #836. :). LGTM. rebased!. The development version now includes a build function which raises an exception on failure.. I've added some check boxes to see how we're doing here! I think the only things left are:\n\nReally comprehensive exception docs. This is much improved in #1186, which at least documents what internal exceptions are raised, but I think documenting external exceptions is far too large of a job. We could handle on a case-by-case basis based on what bugs we get, etc.\nulimits docs. Don't think this is done, yet.\nSome users guides are in #1186, the rest will be in https://github.com/docker/docker.github.io/pull/210. Shouldn't this be in the part of the documentation about instantiating clients? Creating an FAQ is always a warning sign for me that the main documentation isn't good enough.\n. This would be great, but unfortunately we've had a pretty major API change in the development version. Would you mind rebasing, @intrepidlemon?. @shin- Updated the docs to say that assert_hostname=False is safe if you're using Machine.\n. Rebased - thanks.\n. Seems to be there now!. IIRC, we should just pass no value by default, because Swarm often uses the overlay driver by default, and this would override that default.\n. Thanks for the contribution! This should be live now.. Fixed in #1160.. Agreed this doesn't make sense as part of docker-py, but would make a great 3rd party project! Closing this now as there seems to be consensus on this from the maintainers.. https://github.com/docker/docker-py/issues/1186. :tada:. @ghostsquad Feel free! I'm not working on it at the moment. :)\n. A prototype here, if anyone's interested: https://github.com/bfirsh/docker-sdk-python\n. Implemented in https://github.com/docker/docker-py/issues/1186!. replaced by #1316. Looks like this is sending it as part of the request payload rather than as header. You probably want to pass it as a headers argument to _post_json.\n. Rebased. ping @shin- @aanand \n. Oops, see this also includes stack commands. Re-opening and renaming.. @shin- @dnephin @aanand This is now more-or-less feature complete, so it'd be great to get comments on the design now before I spend any more time tidying things up and documenting it.\n\nIn lieu of docs, here's some basic usage. For everything else, the models are pretty readable.\n. @shin- @dnephin @aanand Okay, this is ready for a proper review! \ud83d\ude05 \nI haven't squashed the commits into a logical series of commits yet, but if you'd rather I do that first, I'm happy to. The advantage of having all the commits is you can see my working and thought process.\n. @shin- @dnephin @aanand I've done a big squash, which makes this review much more palatable! \ud83d\ude0b \nThe refactoring to the old stuff is done first in a series of small commits, then there are two big commits which add the new API and new documentation.\nA big unknown so far is how to do the versioning for the new API. I've only really tested it on Docker 1.12, and it's rather hard to test on anything else. One thing we could do (which would be convenient) is to say the new API is only for Docker >=1.12. Alternatively, we could try setting up CI for old versions so we can actually test it works.\n. @dnephin @shin- Opinions on restricting the new API to Engine 1.12+?\nWe could even say 2.0 only works with 1.12+, even with the low-level API. That would simplify our explanation about what version of Docker is compatible, and allows us to remove a whole load of cruft for supporting old API versions. If users want to use older versions of Docker, they can just use docker-py 1.x.\n. Had an offline discussion with @shin- and he seems fine with dropping support, too.\nTurns out dropping support everywhere is a big job, though, so in the meantime I've just added a warning if you're using an API version that is <1.24. We can remove support in a follow-up PR, or even leave in support for little bit as a bridge to 2.x and remove it in 2.1 or whatever.\n. Yippee, thank you!\n\n. Rebased on top of #1217.\n. If we're going to wait for 2.0 anyway, we may as well roll this into #1186.\n. Rebased. /cc @docker/core-docker-py-maintainers \n. Intentionally -- it allows timeout to be passed to from_env().\nOn Monday, 26 September 2016, Joffrey F notifications@github.com wrote:\n\n@bfirsh https://github.com/bfirsh Looks like you accidentally included\n7f64928\nhttps://github.com/docker/docker-py/commit/7f64928dc60b7d99d1be6ddd5e2cf6822d6127b7\n?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/docker/docker-py/pull/1217#issuecomment-249689706,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AACfymN6-ZeG2XYKYLjrd9emngrWXXFIks5quCzKgaJpZM4J9xRm\n.\n. Hm, alright. from_env() should accept all kwargs for Client though, so I'll pull that out into from_env().\n. @shin- Made a simpler version which just adds timeout to from_env(). :)\n. ping @shin- \n. Rebased. /cc @docker/core-docker-py-maintainers \n. Sure! Sounds reasonable to me - it's plausible that it could break code and\nit isn't urgent.\n\nOn Monday, 26 September 2016, Joffrey F notifications@github.com wrote:\n\nNot against it in principle, slightly worried about breaking stuff for\npeople who might have a from docker.auth.auth import or similar. May be\nreasonable to put it in the 2.0 milestone?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/docker/docker-py/pull/1218#issuecomment-249662267,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AACfyu1O_vx31e2z56jemOKxz4AQm028ks5quBRugaJpZM4J9373\n.\n. Rolled into #1186 \n. ping @shin- @dnephin . LGTM. Thanks!. LGTM\n\nThanks!. We had a big docs refactor and none of this exists any longer. Thanks for the contribution, though!. Carried, rebased, and squashed in #1317. Thanks @joncotton!. rebased. Oh, weird. Perhaps they were cancelled out by an upstream changes? I just did git rebase master and resolved the doc conflict.. LGTM. LGTM ^_^. LGTM\n@dnephin @aanand any better name ideas?. The whole point of this is to disambiguate DockerClient and APIClient, so continuing that and your logic, they should be DockerSomethingClient and DockerAPIClient. (This will end in madness.). I think his point is that it represents the Engine.\nBut, perhaps we also need the word \"client\" in there just for disambiguation between the client and server halves of the Engine, and to make it clear that it is connecting to something over the network.\nIt does work kinda neatly, though:\nimport docker\nengine = docker.from_env()\nengine.containers.run(...)\nBut perhaps not neatly enough to justify the change from the common pattern of these things being called \"clients\".\nNow it doesn't matter, here's another bike shed: EngineClient. ;). Fixed in #1305.\nmaster is a bit unstable at the moment - it's recommended you pin to a version. :). Thanks for the contribution! A more resilient solution is in #1305.. Oh, gawd. Thanks for the report.\nA better solution would be to use --, which is the ASCII equivalent of an em-dash. An even better solution would be for it to not break when reading utf-8, which will trip us up again in the future. Does https://github.com/docker/docker-py/pull/1314 solve this issue for you?. :+1:\nTest failures, though.. Looks good apart from docs!. LGTM! :tada:. LGTM. LGTM. Nice! I am travelling without a computer until next week, but I trust that this is all good. ;). yay :sparkles:. LGTM, thank you!. Ping @shin- \nNew 1.13 docs are out now which don't reference \"Remote API\" at all, so it'd be nice to get the Python SDK home page consistent. (Will the docs got auto deployed?). @shin- @aanand rebased :sparkles:. @qazbnm456 This should probably just be moved to create_host_config. Also needs a rebase (sorry!).. Gosh, what a mess. Fixes for both things here: https://github.com/docker/docker-py/pull/1439\nSorry all!. LGTM. LGTM. Fix is in #1393. Yeah, that should be clearer. Sorry about that.\nhttps://github.com/docker/docker-py/issues/1431. I'm not sure I understand what your question is. Could you clarify / reword?. Documentation for the model needs updating too, doesn't it?. In retrospect I think we should have dropped support for the string format in 2.0. ;)\nPerhaps we can phase it out over a few releases.... Whoops! LGTM. It is the easiest way to get started, though! It's what 95% of users will want to do.\n. I felt it was more suitable to associate it with Client because it was testing a code path in there.\n. I really like the neatness of it. :D \n@aanand WDYT?\n. To be clear: the vast majority of first-time users in development are going to have the best experience if we recommend using Toolbox, so we should put that front and centre. For users doing something more complex, they are probably knowledgeable enough that they don't need this handholding.\n. (Also \u2013 not all of them, but I talk to a lot of users so I have a pretty good sample size. ;)\n. Fixed :)\n. Thanks - fixed.\n. Yes, good idea. I think a more general DockerException (or something) makes sense \u2013\u00a0it's technically not an error from the API.\n. I would much prefer to make this the primary API (Client) and have the low-level thing as SomeQualifierClient (i.e. what is now APIClient). Otherwise, the people using the primary API (i.e. most people) will get confused because they're using a thing which doesn't look like the primary API. If we're going 2.0, we have the opportunity to break stuff like this.\nThe risk is that we confuse existing users who upgrade to the new API. I wonder if we can do something clever to warn them. For example, if they access an attribute which doesn't exist, but does exist on APIClient, it gives them a useful error message telling them to use client.api.\n. No strong opinions about this, but I'm not sure I understand why using a base class is a bad thing.\n. Thanks!\n. For run, certainly. This is a convenience for the 80% case where an error is bad. If for some reason you don't want to be handling exceptions, you can still do c = client.containers.run(..., detach=True); c.wait().\nGenerally, this is part of a broader direction we're trying to take the SDKs. This is not about running containers as such, this is about making this a toolkit for packaging up part of your app as containers. This allows you to treat containers as functions that could be anywhere, in any language.\nFor example, suppose you were writing a Python app and you wanted to call a piece of functionality in Node.js called \"reticulate-splines\":\nreticulated_splines = client.containers.run('tasks/reticulate-splines', splines)\nIf that fails, it's an error in your application.\nHappy to take this discussion offline if you'd like, because it is a much bigger discussion than a GitHub comment. ;)\n. Hopefully the docs will be good enough that they don't have to look at the source code. ;)\nAgreed though, it's a bit annoying. If this turns out to be a nuisance down the line, we can trivially duplicate the args here.\n. We need to find some shortcut to allow users to do the most common things with a flat list of args, because constructing objects like this is a nightmare for the user.\nI disagree that it is inherently complicated. It works fine in the CLI, and this is what the high-level API should be replicating.\nPerhaps client.services.create() should have these arguments:\n```\n$ docker service create --help                                                      !65\nUsage:  docker service create [OPTIONS] IMAGE [COMMAND] [ARG...]\nCreate a new service\nOptions:\n      --constraint value               Placement constraints (default [])\n      --container-label value          Container labels (default [])\n      --endpoint-mode string           Endpoint mode (vip or dnsrr)\n  -e, --env value                      Set environment variables (default [])\n      --help                           Print usage\n  -l, --label value                    Service labels (default [])\n      --limit-cpu value                Limit CPUs (default 0.000)\n      --limit-memory value             Limit Memory (default 0 B)\n      --log-driver string              Logging driver for service\n      --log-opt value                  Logging driver options (default [])\n      --mode string                    Service mode (replicated or global) (default \"replicated\")\n      --mount value                    Attach a mount to the service\n      --name string                    Service name\n      --network value                  Network attachments (default [])\n  -p, --publish value                  Publish a port as a node port (default [])\n      --replicas value                 Number of tasks (default none)\n      --reserve-cpu value              Reserve CPUs (default 0.000)\n      --reserve-memory value           Reserve Memory (default 0 B)\n      --restart-condition string       Restart when condition is met (none, on-failure, or any)\n      --restart-delay value            Delay between restart attempts (default none)\n      --restart-max-attempts value     Maximum number of restarts before giving up (default none)\n      --restart-window value           Window used to evaluate the restart policy (default none)\n      --stop-grace-period value        Time to wait before force killing a container (default none)\n      --update-delay duration          Delay between updates\n      --update-failure-action string   Action on update failure (pause|continue) (default \"pause\")\n      --update-parallelism uint        Maximum number of tasks updated simultaneously (0 to update all at once) (default 1)\n  -u, --user string                    Username or UID\n      --with-registry-auth             Send registry authentication details to swarm agents\n  -w, --workdir string                 Working directory inside the container\n```\nIf you want to do anything else more complex, you can use the low-level APIClient.create_service() method, or pass in a TaskTemplate or something.\n. Thanks!\n. Looks like we already had DockerException, so I made APIError inherit from that.\n. Thanks!\n. @shin- @dnephin @aanand WDYT about a design which mirrors the docker service create options? If users want to do more complex stuff they can use APIClient.create_service and the various objects.\n. @shin- Yeah, agreed that two things called \"client\" is confusing. The reason it's that way is because I couldn't think of a better noun, and I'm open to suggestions. Docker could work, but I'm not sure it's particularly descriptive for the thing it is. Also... from docker import Docker; docker = Docker(...) welp\nI don't think users will run into the Client/APIClient distinction enough for it to be confusing. If you're using APIClient, you're already doing something quite advanced, and will be looking into it in enough depth that you'll notice the difference.\nOne possibility is to make the default behaviour to not expose a client at all to the user:\nimport docker\nprint docker.containers.run(\"alpine\", \"echo hello world\")\nHas problems, but sure is neat for the 80% case!\n. > That would mean getting stuck with a nasty __getattribute__ override until 3.0, which I'm not really keen on.\nI don't think it's that nasty \u2013\u00a0it'll just be outputting a different message for the AttributeError.\n. For a really simple use case, this is how it currently works, from the docs:\npython\ncontainer_spec = docker.types.ContainerSpec(\n    image='busybox', command=['echo', 'hello']\n)\ntask_tmpl = docker.types.TaskTemplate(container_spec)\nservice_id = client.create_service(task_tmpl, name=name)\nI want it to be more like this:\npython\nservice = client.services.create(image='busybox', command=['echo', 'hello'])\nIf you want me to explain why that is better for the user, I can, but hopefully it's self-explanatory. ;)\n. Now implemented properly with roughly the same options as docker service create.\n. Not too bad... https://github.com/docker/docker-py/pull/1186/commits/315954a33f6975dd27ec2939cdc829aa8ffb3998\n. When squashing everything I left this as a separate commit in case we want to remove it. :)\n. Yeah, good point. I think it's because the version doesn't exist sometimes for some reason, but the name always does. But I forget when this is the case.\nPerhaps we could just use get() for both. Returning None if there isn't a name seems reasonable.\n. This is silly in a good, thoughtful way.. I think this should just be \"Client\" -- it's the title of the documentation page about clients, not the specific object name.\nIn the same way below, it says \"Creating a client\", not \"Creating a DockerClient\".. Oops \u2013 I think this file is redundant now. The content has been moved to docstrings and api.rst. There is also a file called services.rst which will be displayed instead of this file.\nWe can just delete it, I think.. This won't actually link to anywhere unless they are represented in a docs file somewhere.. Perhaps we could replace this with a \"types\" section to actually document them? (See my other comments.) Ditto the HostConfig object, and so on.. A tip, but not important: these types can be links, like :py:class:`~docker.types.services.ContainerSpec`.. It's a proper noun, so should be just \"Docker SDK for Python\".. same - s/the//. ",
    "GaretJax": "+1 for this\n. ",
    "wallrj": "I started some work on this in #693. Please comment on the PR if you have time.\n. Hey @jongiddy \nThis branch needs merging forward.\nI'm upgrading to docker-py 1.3.1  and noted that the pull method still doesn't have a timeout method so this is still needed.\n. > Is there any updates on getting this branch merged or something like it implemented?\nSorry @rmb938, I'm useless and never finished it off. Not sure when I'm going to get around to it.\n. ",
    "ticosax": "This PR, raise another question regarding release management.\nToday on pypi, the current version (0.2.2) of docker-py supports only docker api 1.4.\nAs today 1.7 is the stable version, wouldn't be better to catchup a bit ? It seems that current master is ready to ship 1.6.\n. @shin thank you for the review, I updated the PR.\n. @shin- I addressed your last comment.\n. thank you :smile:\n. thanks @blueyed I took your comment into consideration and pushed a new version.\n. You are right, changing...\n. ok\n. Good catch! I didn't notice this change.\n. updated.\n. I chose TypeError because this is what python raises if you call a function with a wrong argument.\nBut I do not have strong opinion on this, So I'll follow your recommendation and let's get it merged. I'm eager :smile: \n. ",
    "maasg": "@ureyes84  Thanks for this PR. It was a blessing after hours of chasing weird Docker deserialization errors.\n. ",
    "eltimn": "Sorry about that. I was looking for them on the create_container function, but makes sense they are on start.\n. ",
    "marcus233": "No. I have only tested on 3.3\n. ",
    "namin": "The tests now pass :) Thanks!\n. ",
    "metakermit": "I'm using docker-py 0.2.3 (installed using pip) and Python 3.3.3.\nMy problem is not that the container won't start (everything appears normal), but that I can't connect to it through the public port (and this does work when I start the container using the docker cli).\n``` python\nIn [7]: container = c.create_container('kermit/hellonode', name='hello', ports=[1337])\nIn [8]: c.start(container, port_bindings={1337: ('0.0.0.0', 1338)})\nIn [9]: c.inspect_container(container)\nOut[9]:\n{'Args': ['-c', 'node /hello.js'],\n 'Config': {'AttachStderr': True,\n  'AttachStdin': False,\n  'AttachStdout': True,\n  'Cmd': ['/bin/sh', '-c', 'node /hello.js'],\n  'CpuShares': 0,\n  'Dns': None,\n  'Domainname': '',\n  'Entrypoint': None,\n  'Env': ['HOME=/',\n   'PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'],\n  'ExposedPorts': {'1337': {}},\n  'Hostname': '2c4f953e10fc',\n  'Image': 'kermit/hellonode',\n  'Memory': 0,\n  'MemorySwap': 0,\n  'NetworkDisabled': False,\n  'OpenStdin': False,\n  'PortSpecs': None,\n  'StdinOnce': False,\n  'Tty': False,\n  'User': '',\n  'Volumes': None,\n  'VolumesFrom': '',\n  'WorkingDir': ''},\n 'Created': '2014-01-14T20:44:36.33498228Z',\n 'Driver': 'aufs',\n 'HostConfig': {'Binds': None,\n  'ContainerIDFile': '',\n  'Links': None,\n  'LxcConf': None,\n  'PortBindings': {'1337': None,\n   '1337/tcp': [{'HostIp': '0.0.0.0', 'HostPort': '1338'}]},\n  'Privileged': False,\n  'PublishAllPorts': False},\n 'HostnamePath': '/var/lib/docker/containers/2c4f953e10fcf2ca1ba0c6f100be1191de5f9d95b672b9cf5d4be9b66b0dbdde/hostname',\n 'HostsPath': '/var/lib/docker/containers/2c4f953e10fcf2ca1ba0c6f100be1191de5f9d95b672b9cf5d4be9b66b0dbdde/hosts',\n 'ID': '2c4f953e10fcf2ca1ba0c6f100be1191de5f9d95b672b9cf5d4be9b66b0dbdde',\n 'Image': 'fbccec340a0ee0f8248a69a23f6dee612464fec24ceb262f209ca413fa26ea5a',\n 'Name': '/hello',\n 'NetworkSettings': {'Bridge': 'docker0',\n  'Gateway': '172.17.42.1',\n  'IPAddress': '172.17.0.25',\n  'IPPrefixLen': 16,\n  'PortMapping': None,\n  'Ports': {'1337': None,\n   '1337/tcp': [{'HostIp': '0.0.0.0', 'HostPort': '1338'}]}},\n 'Path': '/bin/sh',\n 'ResolvConfPath': '/etc/resolv.conf',\n 'State': {'ExitCode': 0,\n  'FinishedAt': '0001-01-01T00:00:00Z',\n  'Ghost': False,\n  'Pid': 16423,\n  'Running': True,\n  'StartedAt': '2014-01-14T20:44:38.984802489Z'},\n 'Volumes': {},\n 'VolumesRW': {}}\nIn [10]: ! curl blimpyard.cloudfleet.io:1338\ncurl: (7) Failed connect to blimpyard.cloudfleet.io:1338; Connection refused\n```\nThese steps should be reproducible (aside from the url passed to curl) - I pushed the container to the docker index. The web app is the helloworld example from node's homepage.\njavascript\nvar http = require('http');\nhttp.createServer(function (req, res) {\n  res.writeHead(200, {'Content-Type': 'text/plain'});\n  res.end('Hello World\\n');\n}).listen(1337, '127.0.0.1');\nconsole.log('Server running at http://127.0.0.1:1337/');\n. @mpetazzoni thanks, that worked! Yes, I also think it should either be set to TCP automatically to avoid these mishaps or at least documented properly as the docs currently state that TCP is the default.\n\nIf you wish to use UDP instead of TCP (default), you can declare it like such:\n   c.create_container('busybox', 'ls', ports=[(1111, 'udp'), 2222])\n\n@joshuaconner good spotting, but it's a typing mistake because I just pasted the code from the node website, not from my actual script - I do set the web app to listen to 0.0.0.0. Anyway, it worked for the same container image with the docker cli.\n. ",
    "cglewis": "Nevermind, found publish_all_ports for start.\n. also seeing this behavior using b2d 1.3.0 and docker-py 0.5.3.  @rwestgeest thanks for suggesting that work around, but really need something that doesn't require another container running.  to the maintainers, is the recommended workaround to disable TLS on b2d, or is there some other way to make docker-py TLS connections work with the current b2d release?\n. ping @shin- @ewindisch \n. @shin- I'll give #360 a shot when 0.6.0 lands.\n. @shin- I was able to get b2d 1.3.1 working with TLS and docker-py 0.6.0 using the kwargs_from_env as suggested in #360.  \nFWIW, here's exactly what I did to make it work:\nadd 192.168.59.103 boot2docker to /etc/hosts\n$ $(boot2docker shellinit)\n $ export DOCKER_HOST=\"tcp://boot2docker:2376\"\n $ export DOCKER_TLS_VERIFY=\"1\"\n```\n $ python\n\n\n\nfrom docker.client import Client\nfrom docker.utils import kwargs_from_env\nclient = Client(**kwargs_from_env())\nprint client.version()\n {u'KernelVersion': u'3.16.4-tinycore64', u'Arch': u'amd64', u'ApiVersion': u'1.15', u'Version': u'1.3.1', u'GitCommit': u'4e9bbfa', u'Os': u'linux', u'GoVersion': u'go1.3.3'}\noutput = client.build(path=\"dockerfile_path\")\ncontainer = client.create_container(\"89316a67e70e\", detach=True)\nclient.start(container, publish_all_ports=True)\n\n\n\n$ docker ps\n CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS                     NAMES\n 03e541301e6c        89316a67e70e        \"python runserver.py   41 seconds ago      Up 3 seconds        0.0.0.0:49153->5000/tcp   stupefied_mayer   \n``\n. +1; I too would like to see an option like this too. Even if it just gave back results per step, instead of everything at the end, that would be great.\n. I was able to fix this by adding an entry to /etc/hosts on OSX:192.168.59.103 boot2dockerthen exporting like so:export DOCKER_HOST=tcp://boot2docker:2376`\nThis is because SSL certs are signed against hostnames not IP addresses.  Alternatively you could turn off the TLS_VERIFY, and it will ignore hostname mismatches.\n. add an entry in your /etc/hosts for 192.168.59.103 boot2docker, or whatever boot2docker ip says the ip is.\n. ",
    "ibuildthecloud": "I created PR https://github.com/dotcloud/docker-py/pull/151 for this.  After discussion with the requests folks on issue https://github.com/kennethreitz/requests/issues/1879 it seemed that the right approach was to change the scheme of the URL.\n. It seems this change has extended some of the lines past 80 characters and caused flake8 issues.  I will fix and update.\n. Merging this PR has broken the build because there are newer unit tests still using the old unix: scheme.  I'll put in another PR to fix that\n. Docker 0.8 has seemingly broken the <=1.6 version of the images API:\nsh\ncurl http://localhost:8085/v1.8/images/json\njson\n[\n  {\n    \"Created\":1391493237,\n    \"Id\":\"94041e8be8ec54d6557bf88e02ce00cd1204f401d955fd2bb2840d2d3c1ee027\",\n    \"ParentId\":\"7527bf080f7fa1f90fd477bc4dab20d16d4b8a2bc6e3f5d27aca69dba4487f35\",\n    \"RepoTags\":[\"ibuildthecloud/dstack:latest\"],\n    \"Size\":0,\n    \"VirtualSize\":543223874\n  }\n]\nsh\ncurl http://localhost:8085/v1.6/images/json\njson\n[\n  {\n    \"Created\":1391493237,\n    \"ID\":\"\",\n    \"Repository\":\"ibuildthecloud/dstack\",\n    \"Size\":0,\n    \"Tag\":\"latest\",\n    \"VirtualSize\":543223874\n  }\n]\n. May not be obvious, its broken because 'Id' is \"ID\"  I found this is a simple bug in docker v0.8 and I'll submit a PR there shortly\n. I'm getting this issue too but on pull(stream=True).  Ubuntu 13.10, Python 2.6.9 (yeah, i know that's a little odd), Docker 0.9.0, Docker-py 0.3.0.\n```\n    def _stream_helper(self, response):\n        \"\"\"Generator for data coming from a chunked-encoded HTTP response.\"\"\"\n        socket_fp = self._get_raw_response_socket(response)\n        socket_fp.setblocking(1)\n        socket = socket_fp.makefile()\n        while True:\n\n      size = int(socket.readline(), 16)\n\nE           ValueError: invalid literal for int() with base 16: ''\n```\n\nIs the root cause of this know?  I'm going to dig into this tonight, as it is breaking some stuff for me.  Let me know if the correct fix is already known.\n. So basically what I'm seeing is that between 0.8.x and 0.9.0, due to https://github.com/dotcloud/docker/pull/4276, docker now sends empty newline chunks in streamed response.\nSo before the response would look line\n42\\r\\n\n{......}\\r\\n\n42\\r\\n\n{......}\\r\\n\nNow it looks like \n42\\r\\n\n{......}\\r\\n\n\\r\\n\n42\\r\\n\n{......}\\r\\n\n\\r\\n\nSo I think just a simple check to ignore the newlines will probably fix it.  I'll try it out.\n. Ah, I see what's happening now.  So _stream_helper reads the chunk size, but then doesn't actually use the size to read the data\npython\n    def _stream_helper(self, response):\n        \"\"\"Generator for data coming from a chunked-encoded HTTP response.\"\"\"\n        socket_fp = self._get_raw_response_socket(response)\n        socket_fp.setblocking(1)\n        socket = socket_fp.makefile()\n        while True:\n            size = int(socket.readline(), 16)\n            if size <= 0:\n                break\n            data = socket.readline()\n            if not data:\n                break\n            yield data\nSo if the data has a newline in it (which in 0.8.x never happened), then socket.readline() will read a partial data chunk.  Then when the loop starts again, where you are expecting to read an int, you are really still reading the chunk.\nI'm going to experiment with switching to using Request.iter_lines() so that docker-py doesn't have to be so low level and read chunks\n. I'd also appreciate a release for this.  I'm disabling certain functionality in my app because I can't use pull(stream=True).\n. :+1: \nThis patch has the added bonus of separating out exec create and exec start into two separate methods.\n. +1\nAny chance this can be merged?  Seems like a straight forward PR and an obvious API gap between docker-py and Docker Remote API.\n. Oops, this was the wrong approach\n. @shin Updated as per comment\n. @shin Can this be merged now?\n. Great question... our stuff fails on master because it can't find api.  Are you saying this package does not need to be installed?\n. @shin- Seems to be referenced from here: https://github.com/docker/docker-py/blob/4475310eeac905e0def4e0eeec6f3573a65bb028/docker/client.py#L25\n. The issue is with master only, so git clone and setup.py install.  1.4.0 is fine because api didn't exist.\n. python\n.tox/py27/local/lib/python2.7/site-packages/docker/__init__.py:20: in <module>\n    from .client import Client, AutoVersionClient # flake8: noqa\n.tox/py27/local/lib/python2.7/site-packages/docker/client.py:25: in <module>\n    from . import api\nE   ImportError: cannot import name api\n. The 32 chunk_size is because request will block until the chunk size is read.  The default is 512, so if your status is currently less than 512 length, you will get no content.  If your reading the response to get progress updates, you want a smaller chunk_size.  Setting to 1 would ensure that you get pretty close to real time status updates, but then you'll be looping a lot more.\n. ",
    "markshao": "How about get the size of the \"busybox\" as the benchmark , and calculate the export file size should >= the benchmark , this may solve the second question I think.\n. ",
    "a-ba": "The point is that container_config() accepts two representations for \nthese parameters : dict and list, whereas the REST API supports only \ndict. But if given a list, it does the conversion _only if the list is \nnot empty. If the list is empty, it sends it verbatim and cause an error \non the server.\n. It is a different issue. #631 deals with choosing the right reader \n('multiplexed' or 'raw')\nThis PR fixes a bug in the 'multiplexed' reader.\n. ",
    "fufie": "This would be very useful - especially if the docker load command was also supported. \n. ",
    "Itxaka": "Actually the PiPy version is compatible with docker 0.8 with no problems, as Im using it to build my flask gui Gobolino and Im even using the 1.8 api version for the client. \nThere are some changes but it seems to work with no problems!\n. ",
    "stpierre": "I'm not sure this is a complete fix for the issue.  The problem seems to be actually in docker.client.APIError, which assumes that requests.exceptiosn.HTTPError takes keyword arguments:\nclass APIError(requests.exceptions.HTTPError):\n    def __init__(self, message, response, explanation=None):\n        super(APIError, self).__init__(message, response=response)\nThis is the case in requests 1.2.3 (which is what I happen to have handy), but not in 1.1.0, which is the version available in CentOS 6.\nIt seems like the better fix would be something like\nclass APIError(requests.exceptions.HTTPError):\n    def __init__(self, message, response, explanation=None):\n        super(APIError, self).__init__(message)\n        self.response = response\nThis would work for both requests 1.1.0 and 1.2.3.\nI'll open another PR to fix this.\n. cc @lsm5\n. ",
    "macobo": "Note that this touches #129 \n. Updated the PR as per instructions. While I was there, I pulled out the string concatenation - see http://stackoverflow.com/questions/3055477/how-slow-is-pythons-string-concatenation-vs-str-join.\nI'm not awfully certain about the decoding part (is it utf-8?), so that needs to be looked at.\n. @rutsky, could you specify what version (3.3?) exactly you are using and if it is vanilla python?\n@shin-, I'll look into it when I have free time.\n. ",
    "rutsky": "This PR doesn't work for me on Python 3.\n'\\n'.join(docker_client.logs(container_id, stream=True))\nfails with:\nFile \"/home/bob/env/lib/python3.3/site-packages/docker/client.py\", line 290, in _multiplexed_socket_stream_helper\n    _, length = struct.unpack('>BxxxL', header)\nTypeError: 'str' does not support the buffer interface\n. @macobo, Python 3.3.2 on Ubuntu 13.10:\n$ python3 --version\nPython 3.3.2+\nI think observed by me error on Python 3 is expected: struct.unpack() works only on buffer objects/objects that provide buffer protocol and you changed recvall() result type to str.\n. I made proof-of-concept PR https://github.com/dotcloud/docker-py/pull/178 for using different requirements lists in Python 2 and Pythin 3.\nIn setup.py we can select dependencies based on current sys.version_info and in tox.ini we can select requirements based on environment name (too bad tox doesn't allow to specify multiple environments in single \"[testenv]\", I made feature request for this: https://bitbucket.org/hpk42/tox/issue/159/support-multiple-environments)\n. Shouldn't contributions to docker-py be with signed Docker Developer Certificate of Origin?\nI forgot to sign it.\n. I agree that new syntax is more flexible, just want to mention that this is backward incompatible change and should be at least documented in changelog.\nAlso it's quite simple to support old syntax with old behavior (probably, with producing warning about obsolete syntax) and library user wouldn't be surprised that old syntax doesn't work anymore.\n. Any progress on this?\n@Slix's workaround doesn't work in Python 3 (s doesn't have send method) and I don't see any documentation for client.attach_socket.\n. 1. Currently API documented in Markdown looks visually awful: there is no distinction between argument names, types, they all just plain text, e.g.:\n\nParams:\n- path (str): Path to the directory containing the Dockerfile\n- tag (str): A tag to add to the final image\n- quiet (bool): Whether to return the status\n\nThey can be manually formatted, but it's painful and not obvious with limited abilities of Markdown, e.g.:\n\nParams:\n- path (str): Path to the directory containing the Dockerfile\n- tag (str): A tag to add to the final image\n- quiet (bool): Whether to return the status\n\nSphinx provides good formatting out of the box --- just see example, provided by @gtaylor.\n2. IMO, having documentation close to source code is much better that having it in separate place.\n   It's easier see what function does in IDE, it's easier to sync function behaviour changes with documentation.\n. ",
    "blalor": "Thanks for the hint, @rutsky.  Would like to see proper support for this, as well.\n. You'd want the mode to be part of the individual bind; maybe {\"/home/host-data/\": (\"/mnt/data\", False)} to indicate that writing is forbidden.  If it's a string, or if the 2nd value of the tuple's True then it would be read-only.  Feels kludgy, but you certainly don't want to force all binds to be rw or ro.\n. Along these lines, pulling a non-existent repository doesn't result in an error.  It should throw an exception, but in my case I'm just getting\n{\"status\":\"Pulling repository apps/skydock-nginx\"}\n{\"errorDetail\":{\"message\":\"HTTP code: 404\"},\"error\":\"HTTP code: 404\"}\nI don't think it warrants its own issue, but it is a problem I've run into.\n. ",
    "wallnerryan": "@blalor , @rutsky, Curious in what manner would you like to see this properly supported?Something like..\npython\nc.start(container_id, binds={\"/home/host-data/\": \"/mnt/data\"}, mode=\"ro\")\n???\n. agreed, I think the idea was to be explicit in that you setting mode to rw or ro. \n:+1: for keeping it part of the individual bind. \nSome ideas,\nI believe the True/False dictation may be useful, if you use \"docker inspect\" volumes come up as \n...stuff\n\"VolumesRW\": {\n      \"/foo/bar/myvol\": true\n}\n...more stuff\nthough its not very informative, heres a sample of two ways, yours and another.\nYour example\n{\"/home/host-data/\": (\"/mnt/data\", False)}\nchange start () from\nif binds:\n      bind_pairs = [\n          '{0}:{1}'.format(host, dest) for host, dest in binds.items()\n      ]\n      start_config['Binds'] = bind_pairs\nto (setting rw if True is set, ro if False)\nif binds:\n      bind_pairs = [\n          '%s:%s:rw' % (h,d[0]) if d[1] else '%s:%s:ro' % (h,d[0]) for h, d in binds.items()]\n      start_config['Binds'] = bind_pairs\nOr use more explicit keys\n{\"/home/host-data/\": { \"bind\" : \"/mnt/data\", \"rw\" : True }}\nThen in the start() def we could change it to\nif binds:\n      bind_pairs = [\n          '%s:%s:rw' % (h,d['bind']) if d['rw'] else '%s:%s:ro' % (h,d['bind']) for h, d in binds.items()\n       ]\n      start_config['Binds'] = bind_pairs\n. forgot to change README with this, can always do this after or redo a pull request if this is something that eventually gets merged. \n. @mpetazzoni , this should reflect changes, mind the small second pull request, i had a small comma typo in the README, the parent of that has the changes.\n. commits squashed, flake8 should be happy this time. thanks!\n. @rutsky : the idea was to have more flexibility here and to make it obvious of the ro/rw choice for volumes. I dont know if we would want to support both the older way and the newer way because you can achieve the same results so it would be redundant.\n. ",
    "rosenhouse": "ah, I figured I'd forgotten something relevant: 2.7.3.     Cheers.\n. ",
    "ereyes01": "Same problem in Python 2.7.5.\nWorkaround for me was to force version='1.6' in the Client constructor.\n. Sorry, I looked more closely, and my problem is close, but not the same as rosenhouse's problem.  I get this problem without specifying stream=True in 0.3.0, but it does involve the same _stream_helper() code.\nSorry if I'm hijacking this for something that should be a different issue, but I'll post my findings.\nHere is what I see:\n```\nIn [3]: c = docker.Client(base_url='http://localhost:4243')\nIn [4]: r = c.build(path='/tmp/tmpdEEGN8', tag='eddy-test')\nIn [5]: r\nOut[5]: \n...\nIn [7]: r.next()\nAttributeError                            Traceback (most recent call last)\n in ()\n----> 1 r.next()\n/home/ereyes/work/new-env/local/lib/python2.7/site-packages/docker/client.pyc in _stream_helper(self, response)\n    233     def _stream_helper(self, response):\n    234         \"\"\"Generator for data coming from a chunked-encoded HTTP response.\"\"\"\n--> 235         socket_fp = self._get_raw_response_socket(response)\n    236         socket_fp.setblocking(1)\n    237         socket = socket_fp.makefile()\n/home/ereyes/work/new-env/local/lib/python2.7/site-packages/docker/client.pyc in _get_raw_response_socket(self, response)\n    229     def _get_raw_response_socket(self, response):\n    230         self._raise_for_status(response)\n--> 231         return response.raw._fp.fp._sock\n    232 \n    233     def _stream_helper(self, response):\nAttributeError: 'NoneType' object has no attribute '_sock'\n```\nThe image does get built, but my code was relying on the return value of build() to get at the image ID.\nIt looks like, by default as of \"version 1.8\", build will always assume you want to stream the output back:\npython\n        if stream or utils.compare_version('1.8', self._version) >= 0:\n            return self._stream_helper(response)\n        else:\n            output = self._result(response)\nThis is why the workaround for me was to force version='1.6' in the client.  This worked for me because I didn't want to stream at all.  I would imagine that if you really want to do stream=True, you may still be stuck if you force the version to something older like 1.6.\n. Thanks for confirming. I don't think I'll have a chance to work on this until late next week. If it hasn't yet been resolved by then, I'll take a stab at it.\n. Thanks very much for knocking this out while I was gone!\n. ",
    "magicshui": "@shin- the first solution not work:(\n. yes,it's ok with cli\n. ",
    "HackToday": "I hit the same issue for docker-py\n2015-03-04 06:02:27.813 19496 ERROR oslo_messaging.rpc.dispatcher [-] Exception during message handling: UnixHTTPConnectionPool(host='localhost', port=None): Max retries exceeded with url: /run/docker.sock/v1.17/containers/2a686955856811a1cf8a2c90718a235d132b72c022cf379e02902eecff9e6f2a/restart?t=10 (Caused by ReadTimeoutError(\"UnixHTTPConnectionPool(host='localhost', port=None): Read timed out. (read timeout=10)\",))\nwe used the docker-py to call docker\n. do you know what's the issue ? @mpetazzoni \n. I think it is not good to automatic check the versions, the right ways, should the client specify what version of API to talk.\n. hi @shin-  If execute disapper, what is the replacement for execute ?\nLike cli\ndocker exec   would disappear?\n. OK. Thanks. Will use Client.exec_start\n. Hi @shin-  I am little confused for that.\nDo you mean for docker exec function to work.\nI need to code like this ?\n```\n    create_res = self.exec_create(\n        container, cmd, detach, stdout, stderr, tty\n    )\nself.exec_start(create_res, detach, tty, stream)\n```\nI did not understand what's benifit for developers to manual code like this, instead we just call \nexecute(...) , execute(..) is more simple, it only have one call, but now I need to call exec_create and exec_start two calls manually.\nMaybe I missed your point. Could you explain that to me ? Thanks\n. Hi @shin-  OK I got know this history.\nSo for https://docs.docker.com/reference/commandline/cli/\nif we want to support cli like docker exec, it means we have to combine like this\ncreate_res = self.exec_create(\n        container, cmd, detach, stdout, stderr, tty\n    )\nself.exec_start(create_res, detach, tty, stream)\nRight ? it seems have no other option\n. Hi @shin-  could you give your feedback about above comments ? Thanks\n. because I used docker swarm, which by default the docker swarm pull images on every node. It was slow for cirros. But finally long time can get that. So it is swarm performance issue, Not docker-py issue.\n. ",
    "vollnhals": "A workaround that currently works is to specify API version 1.7 when creating the client, so that streaming support is disabled internally.\nJust passing stream=False to client.build does not work, because streaming is enabled anyway. I would have expected that my stream=False is respected.\n. ",
    "momer": "@denibertovic Are you going to implement @shin- feedback? I missed this issue in my search for SSL, and rewrote it just a lil bit ago. Glad there's already a PR, but can we get the feedback implemented and merge this?\n. I'll send it through in a second here :+1: for your work on this so far.\n. Haven't heard back yet regarding this @shin- https://github.com/dotcloud/docker-py/pull/226\n. Thoughts?\n. Sounds good - will get to this tonight/later this week\n. @shin- sorry for the delay had a hectic deadline to meet. Changes in docker-py have since made my updates break tests in master. Would love it if you could pick up here and patch forward to current version. I may have time in the next month to get to this, but it's low on my priority unfortunately.\n. If tests pass and this is able to connect to daemons with TLS enabled, then I'd say merge it in! Nice tag team action all,\nMo\n. Right - however I was unable to get self.verify to connect even with my own CA file; hence the caveat in comments. If you're able to get it to work, then by all means go for it.\nI'd done some cleaning up on another branch, re the type error, but other changes in master caused issues that I don't have time to dig into atm.\n. @shin- and @denibertovic I strongly disagree with removing that little argument.\nThe entire reason for the SSLAdapter was to address an issue in many clients (I believe Ubuntu 12.04 and 14.04 are affected, at least my 12.04 was) where the SSL Version needed to be specified.\nThe reason the error popped up was because in the changes made since my initial commit (which worked on the branch at the time of it's parent) somewhere was removed the initialization of ssl_version to None.\nPlease see the comments at the top of https://github.com/momer/docker-py/blob/7ce73de4a710b6ccd334673bd3c4d1ee667addea/docker/ssladapter/ssladapter.py\nEdit: Also note that without that argument, if a user is affected (as I was), they will be unable to connect to their docker instance using TLS.\n. Right on! I'd thought it was en route to being removed as a parameter.\n. That's the same issue I ran into, which is why TLS Verify was only enabled if the TLS ca cert file was provided (which worked) OR explicitly specified. That should be caveated as I had in the comments, and addressed in the docs. It seems the issue is somewhere up the chain.\n:+1: for removing the original restriction of docker API mapping\n. ",
    "fermayo": "+1\n. It makes a lot of sense, If I get the time!\n. This is now a duplicate of #468\n. :heart_eyes: \n. +1\n. ",
    "bernardopericacho": "+1\n. +1\n. ",
    "Ranjith42k": "I got log as like this while executing above python code.\n2014/03/24 23:04:18 GET /v1.6/images/json?only_ids=1&all=0\n. I installed lxc-docker in ubuntu 13.04 using apt-get. \nWhile typing 'docker version' in terminal i am getting version as 0.5.3 \n. Thank you mpetazzoni and shin for your valuable time spend to help me.  Thank you very much. \n. ",
    "prune998": "in fact I can't see anything in the latest commit of the docker module that would make it work... \n. ",
    "rahul8590": "How will I  fetch the output of the command executed inside the container ? \n. Nope , it doesnt seem to work. This is the following code I am executing. \n``` python\nimport docker,sys \nclient = docker.Client()\ncontainer = client.create_container(sys.argv[1], sys.argv[2])\nif container: print \"successfully created container\"\nprint \"container id: \" , container[\"Id\"]\nclient.start(container)\nclient.logs(container)\nprint \"executed program\"\n```\n. This is the complete program .\nprogram \npython\nimport docker,sys \nclient = docker.Client()\ncontainer = client.create_container('ubuntu','echo \"hello world\"')\nif container: print \"successfully created container\"\nprint \"container id: \" , container[\"Id\"]\nclient.start(container)\na = client.logs(container)\nprint \"log is \" , a \nprint \"executed program\"\nOutput:\nshell\npython create_container.py \nsuccessfully created container\ncontainer id:  8d532166956c785c6e8d1c0015c576c27a7c8270a33526048cbc7a972b354035\nlog is  \nexecuted program\nclient.logs(container) doesnt seem to return anything in my case.\n. docker-py version = 0.3.0  \nbash\nrahul@ip-172-31-23-181_ormain:~ $dpkg -l | grep docker\nrc  docker                                1.4-5ubuntu1                        System tray for KDE3/GNOME2 docklet applications\nii  lxc-docker                            0.9.0                               Linux container runtime\nii  lxc-docker-0.9.0                      0.9.0                               Linux container runtime\n``` python\n\n\n\nclient.version()\n{u'KernelVersion': u'3.2.0-40-virtual', u'Arch': u'amd64', u'Version': u'0.9.0', u'GitCommit': u'2b3fdf2', u'Os': u'linux', u'GoVersion': u'go1.2.1'}\n```\n. ohh great. That worked for me. \nThanks a lot! \n. \n\n\n",
    "dustinlacewell": "Can you be more specific as to what exactly doesn't work? How far did this script go? What was the output? What's the output of docker ps -a after the script has run. What does docker logs <containerid> show?\n. ",
    "rustyrobot": "@shin- I've rebased my patches and moved APIError to docker.errors module, could you please take a look at my changes?\n. Hi @shin- , what do you think about this patch? I had the same problem and it looks like it affects docker 0.10, not only 0.10 api version, I set api version to 0.9, 0.8 and it didn't help.\n. Hi @ticosax , I used your patch and had here a problem docker api version 0.10 accepts VolumesFrom as a list, not as a string. Here is a link on the documentation.\n. ",
    "josephschorr": "Odd... looks like Travis doesn't like the change of the protocol from TCP to HTTP. I tested this successfully with HTTP when hitting my local Docker instance, but I can change it back. Any idea why it would not work with HTTP?\n. Should I change this back to using TCP, try to fix it for HTTP, something else?\n. Just that the extra for loop isn't needed which makes my implementation slightly more compact.\n. ",
    "rail44": "The parameter VolumesFrom should be JSON Array, shouldn't it?\n. I see.\nAnd...\n. This is mistake.\nThere is bug only in _container_config\n. ",
    "stefanfoulis": "In some cases there might be an error, where the error is only visible. Here a design decision needs to be made, if that should raise an Exception or require the developer to look into the result dictionary.\n. The same problem applies to docker.pull()\n. Is there a replacement for the functionality of insert somewhere else in the api?\n. I can't find anything about depreciating insert in the remote api docs. It's still listed there for v1.13: https://docs.docker.com/reference/api/docker_remote_api_v1.13/#insert-a-file-in-an-image\nIs it only being depreciated in docker-py, or am I missing something?\nIs there a other some other way to achieve the same thing with docker-py?\n. what does the if urllib3 accomplish? It will always be there (and it was already used when extracting urllib_ver. If it were not available at all, there would be an ImportError\nMaybe it should be if urllib_ver for the cases where that ends up being an empty string?\n. ",
    "fcoelho": "Done, the test just checks whether VolumesFrom is passed or not to the daemon. I didn't write code that actually triggers the exception, as I thought that could change too much of the fake requests data. If you think I should go for that, I'll add changes there too\n. ",
    "samriley": "I have checked python2 - still working, will get a unit test added.\n. Just brought branch up to date, no changes to original commit\n. ",
    "tarnfeld": "I've made the proposed fix @shin- :smile: looking good now.\n:shipit: \n. ",
    "tifayuki": "This works well on 0.11. we need this fix.\n. ",
    "mway": "Experiencing the same issue here, both using the docker module directly and dockerio in salt.  Interesting that the json payload definitely includes VolumesFrom:\njson\n{\n    \"Tty\": true,\n    \"NetworkDisabled\": false, \n    \"Image\": \"core\", \n    \"Hostname\": \"core\", \n    \"StdinOnce\": false, \n    \"ExposedPorts\": {\n        \"443/tcp\": {}, \n        \"80/tcp\": {}\n    }, \n    \"AttachStdout\": false, \n    \"Memory\": 0, \n    \"AttachStdin\": false, \n    \"AttachStderr\": false, \n    \"VolumesFrom\": \"storage\", \n    \"OpenStdin\": false\n}\nThinking this might be an issue with the docker server, checking into that now.\n. ",
    "therealbill": "Sure, I'll try it tomorrow and let you know. \nThanks,\nBill\n\nOn May 1, 2014, at 11:47, Joffrey F notifications@github.com wrote:\nProbably related to #200 -- can you try it with docker-py from master and see if it solves your issue?\n\u2014\nReply to this email directly or view it on GitHub.\n. @shin yes, pulling from master gave me docker-py==0.3.2-dev and it appears to be working fine no. Thanks. :)\n. \n",
    "julienvey": "I think the main reason is to be able to create packages for openstack services, such that when installing, for instance nova, it will directly fetch all packaged dependencies, without having to use pip.\n. ",
    "eliasp": "I agree with @julienvey here.\nProper distribution packages should always be preferred to language-specific packager managers, as they act outside the actual package management and just introduce a lot of inconsistencies and unmanaged/dead files.\n. ",
    "Frodox": "@tianon \n@bfirsh \nhi! what about supporting docker package also?\nI used python-docker, but just faced with https://github.com/ansible/ansible/issues/42162 , and there are info \n\ndocker-py is no longer maintained (last release from November 2016), you should install docker instead\n\nSo... after uninstalling python-docker and installing pip install docker it works, but it would be great to install it as deb-package, and not pip-package.. ",
    "pilwon": "This issue is blocking Ansible Issue #7324\n. ",
    "discordianfish": "(This wasn't the actual problem, this is already implemented in master)\n. :-1: Please keep this backward compatible, otherwise it will break a application using the coma separated syntax\n. /cc @shin- Any thoughts on this?\n. @shin- Okay, could we then just error out if one provides any version below 1.12 since it's not working properly anyways? And it seems like this should be discussed directly with @vieux (since he owns the api, right?). I personally will never use anything below 1.12 (after the port allocation issue is solved), so to me it's fine either way but even with 1.0.0 out there people will still run into those issues and get a frustrating experience using docker for quite some time.\n. LGTM\n@shin- Can we get this merged soon? I need this for our infra. I'll prepare a PR for ansible which would depend on this.\n. @jpetazzo / @shin- I got your point and I agree. The most deterministic and stable setup is by using a explicit version.\nIn that case I would suggest we go for option 1): Require the user of the library to provide a explicit version. This would force people like the ansible module authors to require a specific version instead of having some admin implicitly set the version by deploying some version of docker-py. Right now things look like it will just work if you don't specific a version but in fact it will break if you upgrade docker-py etc. \n. @shreyu86 Why the deprecation working?\n. ",
    "miracle2k": "Actually, setuptools gives me error: Installed distribution requests 2.3.0 conflicts with requirement requests==2.2.1 whenever there is any other requests requirements other than ==2.2.1.\n. ",
    "eembsen": "Looking at the logs more closely I now see that the IP address in the \"connection reset by peer\" is the IP address from the client that runs docker py function c.build.\n. BTW, logged the same issue in the docker project (https://github.com/dotcloud/docker/issues/6050) because I do not know which end is causing the trouble.\n. I played around with timeout settings but AFAIK that has no effect on the outcome.\n. ",
    "mattjbray": "My bad, this is actually a problem in Ansible's docker module.  I will raise a PR there.  Sorry for the noise.\n. On closer inspection it seems that pull() will extract the tag for you if you pass \"stackbrew/registry:0.7.0\" or \"localhost:5000/registry:0.7.0\" (see here), but not \"registry:0.7.0\" because it fails in resolve_repository_name().  Is that the expected behaviour?\n. ",
    "jcredland": "I just spent four hours on the same problem.\nOn a similar note there is a warning message when you try this.  But it doesn't appear.\nDepreciation warnings are not displayed by default in python.  \"Starting from Python 2.7, DeprecationWarning and its descendants are ignored by default.\".  This means that the user of volumes_from silently fails when the API version number is set incorrectly. Can we have a louder warning!\nthank you!\n. You could just swap Depreciation Warning for User Warning and magically error messages would appear.\nUserWarning The default category for warn().\nDeprecationWarning  Base category for warnings about deprecated features (ignored by default).\nIt would also help if the README.md didn't open with this: \nTo instantiate a Client class that will allow you to communicate with a Docker daemon, simply do:\nc = docker.Client(base_url='unix://var/run/docker.sock',\n                  version='1.9',\n                  timeout=10)\nBut rather specified a later version of the API.\nOn 21 Jun 2014, at 01:53, Joffrey F notifications@github.com wrote:\n\nMy thoughts on the matter are still the same, we shouldn't have to do that, and it's a failure of the docker remote API that this issue even exists. With Docker 1.0, I expect the API to stabilize and this discussion to become obsolete anyway.\nWith 0.3.2 on the horizon, the default API version is 1.12 which shouldn't pose any problems if you're using Docker 1.0 (which everyone should, if only because of the security implications of using anything older than that)\n?\nReply to this email directly or view it on GitHub.\n. \n",
    "Piwosz": "I will take care this soon.\n. ",
    "mmerickel": "So I really want this feature but I'm having trouble getting things to actually work out.\nBased on snooping traffic from ID=$(echo foo | docker run -i -a stdin busybox cat); docker wait $ID; docker logs $ID I can tell you that the following test gives almost the exact same traffic. The only difference is that while the docker CLI does an attach prior to start, it does not send the stdin data until start is sent. There's no way to coordinate this that I can see and I'd expect it to work out anyway.\nI'll continue to tinker with it but I'm kind of stumped based on the traffic I see. Both the docker CLI container and the one in the test are created with the same options and the attach is the same.\nclass TestAttachWithStdin(BaseTestCase):\n    def runTest(self):\n        container = self.client.create_container(\n            'busybox', 'cat', stdin_open=True)\n        container_id = container['Id']\n        self.tmp_containers.append(container_id)\n        self.client.attach(container_id, stdin='hello world')\n        self.client.start(container_id)\n        ret = self.client.wait(container_id)\n        self.assertEqual(ret, 0)\n        result = self.client.logs(container_id, stdout=True, stderr=False)\n        self.assertEqual(result, b'hello world')\nSide note, it's super easy to snoop the traffic from both the CLI and the tests by rebasing this against master and then using socat.\nsocat -x -v UNIX-LISTEN:/tmp/docker-proxy.sock,fork UNIX-CONNECT:/var/run/docker.sock &\nexport DOCKER_HOST=unix:///tmp/docker-proxy.sock\nenv/bin/python setup.py test\nID=$(echo foo | docker run -i -a stdin busybox cat); docker wait $ID; docker logs $ID\n. Would've been nice to address the issues with the integration tests, but anyway I'll just submit another PR to fix them.\n. Looks like the tests pass now, let me know if you need any other changes.\n. This is an issue with the homebrew version of Python. I have a custom version of python I compiled myself, linked against the system openssl 0.9.8, and everything works fine with it, but the homebrew Python (2.7.9), linked against openssl 1.0.2, is broken. I think this is something related to the new openssl 1.0.2 release last week but I haven't tracked down the root issue yet.\n. Setting kw['tls'].assert_hostname = False solves the issues but this really shouldn't be necessary, right?\n. @shin- How does the docker cli manage to handle this properly?\n. So we're back to workarounds. Is there a real solution here? Does docker-machine just need to generate better certificates? It seems to me that even it does that we're still broken unless it starts adding entries to /etc/hosts as well, no? Would it be possible for it to generate a new cert using the ip address as the subjectAltName each time the ip address changes? I ask what the docker cli does to know if there's a change we could make to docker-py to be more consistent.\n. I actually got a different error when trying this on Python 2.7.12, but amusingly in the same area. :-) I also confirmed that this works fine on the previous docker-py 1.9.0.\nTraceback (most recent call last):\n  File \"/Users/michael/work/oss/marina/marina/build.py\", line 697, in watcher\n    if encoding:\n  File \".venv/lib/python2.7/site-packages/docker/utils/socket.py\", line 67, in frames_iter\n    yield read(socket, n)\n  File \".venv/lib/python2.7/site-packages/docker/utils/socket.py\", line 24, in read\n    return socket.recv(n)\nOverflowError: signed integer is greater than maximum\n. I was reproducing it last night with 1.12.2-rc1-beta27. Docker for mac has not had a stable release so testing with that would be difficult.\n. What's the status here? If I rebase it will someone promise to review it this time?. @aanand Ok rebased for the second time.... Standard Python semantics on dict.get is that it returns None if the key doesn't exist.\n. Updated in cd53960.. ",
    "slix": ":+1: This is something I need.\nAre the failing unit tests the only blocker for this being merged?\n. I tried out the pull request, but I couldn't get the changes to work. requests was passing the \"stdin\" parameter to the /attach endpoint as data, but no matter what I did, I could never get the stdin content to show up in the container. (Or whatever I did was clobbering stdout, which is how I was testing this out.)\nI managed to get stdin to work via attach_socket though as a workaround! It doesn't require this pull request. I hope this helps anyone trying to use stdin:\npy\nclient.create_container(\n    image=\"ubuntu:14.04\",\n    command=\"whatever\",\n    stdin_open=True)\nclient.start(container)\ns = client.attach_socket(container, params={'stdin': 1, 'stream': 1})\ns.send(\"Hello, world! This is on stdin!\")\ns.close()\nI don't know if that's the correct way to do this, so if anyone knows a better way, that'd be awesome. But this works for me!\nAlso, if you rebase this patch onto a recent commit, the unit tests no longer fail.\n. ",
    "SakuraSound": "@rutsky @Slix's workaround does work in Python3. Make sure that when you create your container, you set the parameter stdin_open to True, since it defaults as False.\nmy example \nstdin-test.py\n``` python\nimport sys\nimport json\nimport logging\nprint(\"STARTED\")\nwhile True:\n    line = sys.stdin.readline()\n    print(\"GOT DATA\")\n    print(line)\n    print(\"Above is data\")\n    data = json.loads(line)\n    print(data)\n    data['success'] = True\n    logging.info(json.dumps(data))\n    break\nprint(\"Finished\")\n```\nRunner.py\n``` python\nimport json\nfrom docker.client import Client\nfrom docker.utils import kwargs_from_env\ndata = {'msg': 'hello-world'}\ncli = Client(**kwargs_from_env())\nprint(cli.version())\ncontainer = cli.create_container(image=\"example/stdin-test\" ,stdin_open=True)\ncli.start(container)\ns = cli.attach_socket(container, params={'stdin': 1, 'stream': 1})\nencoded = json.dumps(data).encode()\nprint(encoded)\nsent = s.send(encoded)\nprint(sent)\ns.close()\nprint(cli.logs(container))\n```\nOutput\nshell\n$ docker logs <WHATEVER THE NAME OF THE CONTAINER WAS THAT RAN>\nSTARTED\nGOT DATA\n{\"msg\": \"hello-world\"}\nAbove is data\n{'msg': 'hello-world'}\nFinished\n. Is there any status for adding futures support to docker-py? Would like to see a solution within docker-py that isn't just utilizing asyncio.run_in_executor(INSERT DOCKER-PY COMMAND).\nEDIT: perhaps this could be a separate project? The \"simple\" path would be duplication of code, but i could see how that would be messy... \n. I think a BYO-Netlib could be good way to deal with this, though I know that would be a much larger effort. My guess is that we would still need an asynchronous docker client class to enable async/await for all the functions\n. I commented in #239 about a workaround for Python 3 that was brought up by @Slix. While I would like an easier way to pass something into stdin upon creation of a container, i think the workaround is actually the right way to go about it if you want to keep your container running and accept more stdin input.\n. I issued a pr for updating the documentation. Am I missing something. I want to help at least clean up the docs on this project\n. Done\n. I could probably add that at the bottom of the updated docs, and it should work as long as the link carries with each new version.\n. Looking at the docs link, it seems we may be able to get away with using links:\nhttps://docs.docker.com/engine/reference/commandline/ps/ seems like it carries across versions.\n. Can do\nSent from my iPhone\n\nOn Apr 19, 2016, at 17:17, Joffrey F notifications@github.com wrote:\nAgreed. Do you mind making the change?\n\u2015\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n. \n",
    "max-sixty": "\n@rutsky @Slix's workaround does work in Python3. Make sure that when you create your container, you set the parameter stdin_open to True, since it defaults as False.\n\nI get the same error:\n``` python\n{'KernelVersion': '4.4.15-moby', 'BuildTime': '2016-07-28T21:15:28.963402499+00:00', 'Version': '1.12.0', 'GitCommit': '8eab29e', 'Os': 'linux', 'GoVersion': 'go1.6.3', 'Arch': 'amd64', 'ApiVersion': '1.24'}\nb'{\"msg\": \"hello-world\"}'\n\nAttributeError                            Traceback (most recent call last)\n in ()\n     13 encoded = json.dumps(data).encode()\n     14 print(encoded)\n---> 15 sent = s.send(encoded)\n     16 print(sent)\n     17 s.close()\nAttributeError: 'SocketIO' object has no attribute 'send'\n``\n. However changings.sendtos._sock.send` makes it work:\n``` python\nimport json\nfrom docker.client import Client\nfrom docker.utils import kwargs_from_env\ndata = {'msg': 'hello-world'}\ncli = Client(**kwargs_from_env())\nprint(cli.version())\ncontainer = cli.create_container(image=\"python:2\" ,stdin_open=True)\ncli.start(container)\ns = cli.attach_socket(container, params={'stdin': 1, 'stream': 1})\nencoded = json.dumps(data).encode()\nprint(encoded)\nsent = s._sock.send(encoded) # <-- here\nprint(sent)\ns.close()\nprint(cli.logs(container))\n```\n. ",
    "y0no": "Here is versions information:\n[~]$ pip freeze | grep docker\ndocker-py==0.3.1\n[~]$ docker version\nClient version: 1.0.0\nClient API version: 1.12\nGo version (client): go1.2.2\nGit commit (client): 63fe64c\nI just test with the current docker-py master, I've another result for the same code:\npython\nTraceback (most recent call last):\n  File \"docker_poc.py\", line 7, in <module>\n    for line in c.logs(container, stream=True):\n  File \"/home/y0no/.virtualenvs/test/lib/python2.7/site-packages/docker/client.py\", line 258, in _multiplexed_socket_stream_helper\n    socket = self._get_raw_response_socket(response)\n  File \"/home/y0no/.virtualenvs/test/lib/python2.7/site-packages/docker/client.py\", line 215, in _get_raw_response_socket\n    self._raise_for_status(response)\n  File \"/home/y0no/.virtualenvs/test/lib/python2.7/site-packages/docker/client.py\", line 82, in _raise_for_status\n    raise errors.APIError(e, response, explanation=explanation)\ndocker.errors.APIError: 404 Client Error: Not Found (\"No such container: {u'Id': u'58d7630019fe8bd9944c3143282ebfb7cc962a91a19c46260d4a4efbf8e63b4f', u'Warnings': None}\")\nI think that I'm missing something in the code but I don't know what.\n. ",
    "BertrandBordage": "I met the same issues.  There are several problems here.\n(For this minimal example, I assume you downloaded the official ubuntu:14.04 image)\nI use docker 1.0.0 (so API 1.12) & docker-py master (3dd8d9eb90363977787a7a7606fa7a4a3e88f62e):\nSuppose I want to remove a container after running some code:\n``` python\nimport docker\nc = docker.Client()\nctr = c.create_container('ubuntu:14.04', 'python3 -c \"print(\\'test\\')\"')\nc.start(ctr)\nfor s in c.logs(ctr, stream=True):\n    pass\nc.remove_container(ctr)\n```\nI get a 406 APIError telling me I can't remove a running container and advice me to use force=True.  But when I use it a 500 APIError is raised, telling me the process isn't running.\nUsing .stop or .kill gives the same issue.\nWhat works is c.wait(ctr).  But of course, if you gave an endless task like while True: pass, wait will never end.\nSo we can't reliably remove nor stop a container.\nAnd if you try with c = docker.Client(version='1.12'), .logs breaks with a 404 APIError, telling you the container couldn't be find.  Same thing with version='1.11'.\n. ",
    "dlip": "I have gotten it working by changing \nregistry=\"docker.mytrax.co.jp\" to registry=\"https://docker.mytrax.co.jp/v1/\".\nI'm not sure why its so sensitive, but these say login succeeded, but I can't pull:\ndocker.mytrax.co.jp\nhttps://docker.mytrax.co.jp\n. ",
    "coulix": "That did the trick indeed :+1: \n. ",
    "dverbeek84": "I have the same problem\nit is in the requests library docker-py use this for ping(if site is online)\nif i use request from my mac:\n```\n\n\n\nimport requests\nrequests.get(\"https://index.onegini.com/v1/_ping\", timeout=3)\n\n```\n\n\n\nand in my VM\n```\n\n\n\nimport requests\nrequests.get(\"https://index.onegini.com\", timeout=3)\n\n```\n\n\n\nSo there is a problem\nAlso the request from my MAC is forward by HAProxy to the docker index but from my VM not.\nI see the request in my HAProxy but not in the docker index.\n. Sorry it was my onw fold. don't call your programme docker.py :p\n. ",
    "tonicbupt": "I got this problem, too.\nIf I set registry without '/v1/', client told me that I had successfully logged in, but I couldn't do any push / pull. It seems I have to set registry ends with '/v1/'... Don't you think it weird?\n. hmm, actually someone has contributed it into master of docker ... see this PR, I did the advanced thing... so maybe we just let it hang up here and wait for the release of this feature and then merge it... XD\n. ",
    "pmyjavec": "I should also mention I've tested this on two completely different machines. Both on my Mac and on CentOS 6 so I don't believe it's a PEBCAK 0_o\n. @shin- \nOk that's fine but what happens in the meantime? Why not just release a fix for the version issue (because it's a pretty serious bug) as 0.3.2 and then release everything else on 0.3.3?\nIf you feel that isn't a great idea do you have an ETA for 0.3.2?\nYou may or not be surprised to hear that a lot of people are relying on this library now! ;)\nThanks mate,\nPauly\n. That's not the case, at least from the behaviour I saw last night, though I could be wrong.\nGenerally the problem is code checking for version > 0.3.0 if so, pass volumes_from etc\n. Ok, we can hold out ;)\nCheers\nOn Wed, Jun 18, 2014 at 4:17 AM, Maxime Petazzoni notifications@github.com\nwrote:\n\nAh yep, that would be an issue. I think a 0.3.2 isn't too far ahead\nthough, if you can bear to wait.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/dotcloud/docker-py/issues/242#issuecomment-46344879.\n\n\nPauly Myjavec\nBUZ BOX\n. ",
    "lenfree": "[root@registry01 packer]# pip install docker-py==0.3.2\nDownloading/unpacking docker-py==0.3.2\n  Running setup.py egg_info for package docker-py\n  Requested docker-py==0.3.2, but installing version 0.3.1\nRequirement already satisfied (use --upgrade to upgrade): requests==2.2.1 in /usr/lib/python2.6/site-packages (from docker-py==0.3.2)\nRequirement already satisfied (use --upgrade to upgrade): six>=1.3.0 in /usr/lib/python2.6/site-packages (from docker-py==0.3.2)\nRequirement already satisfied (use --upgrade to upgrade): websocket-client==0.11.0 in /usr/lib/python2.6/site-packages (from docker-py==0.3.2)\nInstalling collected packages: docker-py\n  Running setup.py install for docker-py\nSuccessfully installed docker-py\nCleaning up...\n[root@registry01 packer]# python\nPython 2.6.6 (r266:84292, Jan 22 2014, 09:42:36)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport docker\ndocker.version\n'0.3.0'\n\n\n\nBut, running this easy_install --upgrade docker-py does actually do the version upgrade.\n. ",
    "d11wtq": "My bad. Was trying to do it while the container was stopped. Works a treat!\n. Could you add the resize() method to this release? :octocat: \n. Oh, of course that will \"just happen\" as it's on master. Mis-read what this PR was. :+1:. Fig is depending on a bunch of things in 0.3.2.\n. I just released a project for this, standalone. I plan to add key-bindings to detach, much like the native docker client.\nhttps://github.com/d11wtq/dockerpty\n. ",
    "delfick": "Yeah, what I meant to say is how to do I interact with a tty?\n(i.e. even if I wanted to just run /bin/bash in the container)\nUnfortunately, I get this feeling that's not trivial.\n. Well, I got something to work.\nBut I have no idea what I'm doing, so it's a bit messy.\n```\ndef attach(self, container, stdin=True, stdout=True, stderr=True, stream=True, logs=False):\n    from docker.unixconn.unixconn import UnixHTTPConnection\n    import threading\n    import readchar\n    import urllib\n    import socket\n    import Queue\n    import time\nif isinstance(container, dict):\n    container = container.get('Id')\n\nparams = {\n    'logs': logs and 1 or 0,\n    'stdin': stdin and 1 or 0,\n    'stdout': stdout and 1 or 0,\n    'stderr': stderr and 1 or 0,\n    'stream': stream and 1 or 0,\n}\nencoded_params = urllib.urlencode(params)\npath = \"/v{0}/containers/{1}/attach\".format(self._version, container)\nrequest = \"POST {0}?{1} HTTP/1.1\\r\\n 'Content-Type: application/vnd.docker.raw-stream\\r\\n\\r\\n\".format(path, encoded_params)\n\n# So, docker uses http hijacking\nconnection = UnixHTTPConnection(\"http+unix://var/run/docker.sock\", \"/var/run/docker.sock\")\nconnection.connect()\nconnection.sock.send(request)\nconnection.sock.setblocking(False)\n\nbuf = []\nstarted = time.time()\nwhile time.time() - started < 5:\n    try:\n        data = connection.sock.recv(10)\n        if data:\n            buf.append(data)\n        else:\n            break\n    except socket.error:\n        if buf:\n            break\n        time.sleep(0.5)\n\nconnection.sock.setblocking(True)\nresponse = ''.join(buf)\nif not response.startswith(\"HTTP/1.1 200\"):\n    raise Exception(\"Couldn't attach to the container\\tresponse={0}\".format(response))\n\nprintout = False\nfor line in response.split(\"\\r\\n\"):\n    if not line:\n        printout = True\n\n    if line and printout:\n        sys.stdout.write(line)\n\nqueue = Queue.Queue()\nstopper = {\"stop\": False, \"pause\": False}\n\ntry:\n    def input_getter(queue):\n        \"\"\"Read in keys\"\"\"\n        while True:\n            if stopper[\"stop\"]:\n                break\n\n            if stopper[\"pause\"]:\n                time.sleep(1)\n                continue\n            queue.put((readchar.readkey(), None, False))\n\n    def output_getter(queue, sock):\n        \"\"\"Read from the socket\"\"\"\n        while True:\n            try:\n                nxt = sock.recv(1)\n                if nxt:\n                    queue.put((None, nxt, False))\n                else:\n                    queue.put((None, None, True))\n                    break\n            except socket.error:\n                queue.put((None, None, True))\n                break\n\n            if stopper[\"stop\"]:\n                break\n\n    threading.Thread(target=input_getter, args=(queue, )).start()\n    threading.Thread(target=output_getter, args=(queue, connection.sock)).start()\n    while True:\n        try:\n            result = queue.get(block=False, timeout=0.1)\n        except Queue.Empty:\n            result = None\n\n        if result:\n            inp, out, closed = result\n            if closed:\n                stopper[\"stop\"] = True\n                sys.stdin.write(\" \")\n                break\n\n            if out:\n                sys.stdout.write(out)\n                sys.stdout.flush()\n\n            if inp:\n                try:\n                    connection.sock.send(inp)\n                except socket.error as error:\n                    log.error(\"Failed to send information back to docker\\terror=%s\", error)\n                    break\n\nfinally:\n    stopper[\"stop\"] = True\n\n```\n. Meanwhile, it looks like fig has a solution https://github.com/d11wtq/fig/blob/bugfix/tty_size/fig/cli/main.py#L219\nAttach_socket makes more sense when you use it to attach to stdin, stdout and stderr separately.....\n. Awesome\n. Thanks to @d11wtq dockerpty solves the problem magnificently.\n. Fair enough.\nI finally found some time to look into this. I'm struggling to write a test that breaks without adding _sock but my harpoon tests are still failing without adding the _sock, so I'll investigate some more.\n. I believe the difference is connecting to the docker daemon over https as opposed to http+docker.\n. hi @shin, I've force pushed to this branch with the following changes:\n- A function to encapsulate making the docker context in the integration tests\n- A test that only fails when using python3 and connecting to the docker daemon with tls\n- The fix for that failing test\n- A task in the makefile that starts a docker-machine, activates it and runs the integration tests against it so you can switch between a virtualenv on your machine using python2 or python3 and then test connecting to the docker daemon over tls with it.\n:)\n. @shin, hello, I was wondering if you had a chance to look at this pull request since my changes?\nThanks\n. I force pushed again after applying the change on top of the latest master.\n. @shin, I've rebased and force pushed again. Now it's four commits:\n1. adding integration-dind-ssl make task\n2. Make it more obvious why integration test pauses for so long at the start (pulling busybox)\n3. adding a test that fails\n4. fixing the test\nWithout the fixing commit it will fail in the integration-dind-ssl Python3 run and nowhere else.\n. meanwhile, I've rebased ontop of origin/master again.\n. No probs.\nThanks for taking the time to review :)\n. and I've repushed again :)\n. that was weird, I totes did that correctly.\noh well, pushed again.\nWe'll get there eventually :)\n. Fixed the typo :)\n. Excellent\nThank you so much!\nOn Thu, 15 Oct 2015 11:36 Joffrey F notifications@github.com wrote:\n\nClosed #594 https://github.com/docker/docker-py/pull/594.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-py/pull/594#event-435877035.\n. https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L253 is the block that uses something other than self.poolmanager to create the connection\n. I agree that it would be nice if requests actually passed assert_hostname around.\n\nI'm not sure how that change would look though.\n. fair enough, I'll move it\n. It'll still pull down busybox in the tests if this fails...\n. I did this because at this point all that matters is whether it has _sock or not, rather than what causes us to choose _sock inside _get_raw_response_socket.\nIt's possible though that we don't need the six.PY3 check in this case. I'll have a look at the behavior in Python2 later on today.\n. ahhh, ok\n. Alternatively, what we could do is settimeout(None) on both the socket and socket._sock.\n. Nup, in python3 the socket is SocketIO object and has no settimout.\nI'll instead go for\nif hasattr(socket, \"settimeout\"):\n        socket.settimeout(None)\n    if hasattr(socket, \"_sock\") and hasattr(socket._sock, \"settimeout\"):\n        socket._sock.settimeout(None)\nI think it's reasonable to expect one of them to always have settimeout and that setting it on both shouldn't have bad effects.\nThis way we also aren't tying this piece of code to whatever conditions are making this distinction necessary.\n. Fair enough, The problem is that in Python3 _sock still exists even if ssl isn't used, but if we use _sock in that condition then it breaks. Which means we can only use _sock if it's using ssl.\n. yeap, just a bit \n. agreed.\n. yeah, that comes about because next is a special word in python, so I write nxt to not overwrite it.\n. I do this so it's obvious that the tests aren't hanging, otherwise it can stay here for quite a while seemingly not doing anything.\n. ah, that makes sense :)\n. oops, fixed.\n. ",
    "tomprince": "\nIt's logical for these projects to use modules like docker.treeviz or some such.\n\nIt isn't clear to me why this is better than docker_treeviz. In particular, it is clear that the later is a separate package, rather than part of docker-py. If there is going to be such a namespace, it should at least be something like docker.contrib.treeviz to make the provenance clear.\n. ",
    "tinytub": "Good work guys , hope this function merge in as soon as possible.\n. ",
    "madhavmurthy93": "Yeah. I figured it out. Thanks.\n. @defunctzombie Sorry for the late response. The issue was that I was trying to get the logs before the container had finished running. I fixed it by checking the state of the container and getting the logs only when the state was 'Exited'.\n. ",
    "defunctzombie": "@madhavmurthy93 what was the issue?\n. Can't you use stream=True and get the streaming output as it is created?\n. @shin- well.. yes ... that is one way to do it of course. No better way than that is available? I suppose the docker API itself doesn't provide something useful for this use case?\n. No worries. I appreciate the code snippet. Maybe something to bake into\ndocker-py to help others.\nOn Thursday, November 13, 2014, Joffrey F notifications@github.com wrote:\n\nSorry, there's nothing in the remote API that would make this easier.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-py/issues/392#issuecomment-62985026.\n. \n",
    "sharky93": "i'm using docker-py 0.3.1 and docker 1.0.0, with API 1.12. can i pip upgrade docker-py?\n. Yep, i upgraded my docker-py to 0.3.2. Still not working.\n. hmm, interesting.\nthis works for me too -> container = c.create_container(image, 'python', mem_limit=2.4117248e06)\nbut the following doesn't,\ncontainer = c.create_container(image, command='python', hostname=None, user=None, detach=False, stdin_open=True, tty=False, mem_limit='2.4117248e06', ports=None, environment=None, dns=None, volumes=None, volumes_from=None, network_disabled=False, name=None, entrypoint=None, cpu_shares=None, working_dir=None,)\n. i see. initially i was indeed trying out with a string argument. thanks.\n. ",
    "leonidlm": "@shin- Do you think it can be a good idea to add support for the '128m' format (the same as docker command line)? If so, I can add it...\n. @mpetazzoni thank you for the link, I used your code from there.\n@shin- the PR #287 is waiting for your approval =)\n. thank you @mpetazzoni for sharing the code which became the basis for this PR\n. @shin- good point, I think raising an exception is a good practice in this case. I will add it now.\n. @shin- I added the checks for the mem_limit parameter. I would be glad hearing your thoughts on this :)\n. @shin- I totally agree. I will work on that later today.\n. @shin- I added the tests and the support for strings like '12341'.\n. ",
    "dims": "Ping. #306 would be very handy +1 :)\n. We need this for Nova Docker.  Nova Docker driver has a custom http client which we are trying\nto replace with docker-py and during the initial review we ran into this. \nhttps://github.com/stackforge/nova-docker/blob/master/novadocker/virt/docker/client.py#L177\n. Gah! looks like there's a pending request already - https://github.com/docker/docker-py/pull/338/\n. looks like my changesets went berserk :)\n. Looks like there's one PR outstanding already\n. https://github.com/docker/docker-py/pull/306\n. Please see new pull request. my github-fu is broken, so ended up creating a fresh branch/PR.\nhttps://github.com/docker/docker-py/pull/373\n. @shin- I've addressed the 2 suggestions, can you please take alook when you get a chance?\n. please try version higher than 1.10 say version='1.13' to pass this check https://github.com/docker/docker-py/blob/master/docker/client.py#L865\nDocker Remote API has its own versioning \nhttps://docs.docker.com/reference/api/docker_remote_api/\n. Which version of requests?\n. Can you try a direct remote api call and see if you get the data?\nIf you are running docker on unix socket, you can use netcat\necho -e \"GET /containers/json?all=1 HTTP/1.0\\r\\n\" | nc -U /var/run/docker.sock\nIf you are running docker over http, you can use curl\ncurl -v http://localhost:2375/containers/json?all=1\n. ok, let's try the bare minimum python code now\nstack@bigblue:~$ python\nPython 2.7.6 (default, Mar 22 2014, 22:59:56)\n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport docker\nc = docker.Client()\nprint len(c.containers())\n0\nprint len(c.containers(all=True))\n21\nprint docker.version\n0.5.1\n. @shin- this error seems to be in the same area of code as mentioned in https://github.com/docker/docker-py/issues/425 - the socket fd vs response.raw problem where some of the socket is already read and hence recvall is reading bad data and the length goes bad.\n. the version parameter is the version number of the Docker Remote API, latest is 1.16. If you switch that, your code will work\n\n\n\nhttps://docs.docker.com/reference/api/docker_remote_api_v1.16/\nhttps://github.com/docker/docker-py/blob/master/docker/client.py#L37\n. @leth - i have a fix for at least logs in https://github.com/docker/docker-py/pull/441 \n. @leth thanks. If there is consensus, i can switch over attach and execute as well to the new method in a subsequent PR\n. @kevinastone added the settimeout in my patch. thanks! \n@objectified can you please try the updated patch?\n. @objectified thanks for the quick response!\n. @shin- Any feedback on this PR? Please let me know.\n. @objectified Am on it. thanks for the poke\n. @shin-, @objectified - tested with python2.7 and python3.4 using this snippet - http://paste.openstack.org/show/158892/\n. @shin- ping :)\n. Thanks @shin- looking forward to feedback\n. Thanks @shin- \n. @spinus just checked the Docker Remote API that docker-py calls under the covers, there seems to be no support for getting the status code there (https://docs.docker.com/reference/api/docker_remote_api_v1.16/) so docker-py won't be able to implement this i think\n. @spinus yep, should be possible to add the support. definitely promising.\n. Looks like the docker CLI calls /exec/xxxxxx/json to get information about the executed process and there is a \"ExitCode\" in the response json.\nhttp://paste.openstack.org/show/155353/\n. @lorin does adding volumes_from=\"source\" to the create_container (and not the start) help?\n. Weird! here's the http traffic i see when i run the CLI - http://paste.openstack.org/show/155516/\n. @lorin I can't seem to recreate the problem, please see http://paste.openstack.org/show/155542/\n. @GordonTheTurtle thanks! done.\nAlso, fyi, here is some context about the problem we encountered in OpenStack:\nhttp://markmail.org/message/sbjqgzcj7tjcfulg\n. @shin- yep, switching to ipaddress works for us. thanks! \n. @shin- this looks good! looking forward to a release with the ipaddress dependency.\n. ",
    "Oloremo": "Folks, I have kinda the same problem described here as \"sometimes all the data is sent on a single line ????\". Sometimes generator returns multiple lines, like(my debug):\nString is: {\"stream\":\"\\r\\n\"}\n{\"stream\":\"Preparing to unpack .../libkrb5-3_1.12.1+dfsg-19+deb8u2_amd64.deb ...\\r\\n\"}\nIs it known issue? How can I avoid it? For some reason I only experience it on docker 1.12 mac os, and on linux it seems to work just fine.\n. ",
    "adamleko": "This is either driver error or collateral damage from 073fdaf671aebd96ce2b39a6e2b91d4565e94213 for Python 3.x. I haven't tested with Python 2.x but I suspect it works just fine under that version.\n. Confirmed.\n```\n$ python --version\nPython 2.7.6\n$ python build.py \n{\"stream\":\" ---\\u003e b750fe79269d\\n\"}\n{\"stream\":\"Step 1 : RUN echo foo\\n\"}\n{\"stream\":\" ---\\u003e Using cache\\n\"}\n{\"stream\":\" ---\\u003e c066f8146b28\\n\"}\n{\"stream\":\"Step 2 : CMD /bin/true\\n\"}\n{\"stream\":\" ---\\u003e Using cache\\n\"}\n{\"stream\":\" ---\\u003e e50e6e31ce4f\\n\"}\n{\"stream\":\"Successfully built e50e6e31ce4f\\n\"}\n```\n. Just confirmed that 9170219188103022c06afd9c0caebf6aeea7eb5c fixes the problem. Thanks for the quick turnaround.\n. ",
    "ryanbrainard": "I just realized that this will not work if the environment is passed in as a dict because of the conversion that happens beforehand does not handle None correctly. I'm going to close this for now and submit a separate fix for that.\n. ",
    "vieux": "@hamiltont Are you using 1.1.0 ?\n. @hamiltont what are the permission of your .dockerignore file ? can you paste us a ls -la please ?\n. @hamiltont also, please paste us the output of sudo docker -D build .\n. @hamiltont sorry to ask, but are you sure you test the right image ? \nWe can see [debug] archive.go:350 Skipping excluded path: .git so it is skipped.\n. @hamiltont oh sorry I thought we were in the docker repo, not docker-py :)\nping @shin- \n. @trwq63 @shin- I just tested with the version of docker-py currently in pip, the pull worked as expected.\n```\n$ pip install docker-py\n$ python\nPython 2.7.6 (default, Sep  9 2014, 15:04:36)\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nfrom docker import Client\nc = Client(base_url=\"tcp://0.0.0.0:2375\")\nfor line in c.pull('busybox', stream=True):\n...     print line\n...\n{\"id\":\"ubuntu-1\",\"status\":\"Pulling busybox...\",\"progressDetail\":{}}\n{\"id\":\"fedora-1\",\"status\":\"Pulling busybox...\",\"progressDetail\":{}}\n{\"id\":\"ubuntu-2\",\"status\":\"Pulling busybox...\",\"progressDetail\":{}}\n{\"id\":\"ubuntu-1\",\"status\":\"Pulling busybox... : downloaded\",\"progressDetail\":{}}\n{\"id\":\"fedora-1\",\"status\":\"Pulling busybox... : downloaded\",\"progressDetail\":{}}\n{\"id\":\"ubuntu-2\",\"status\":\"Pulling busybox... : downloaded\",\"progressDetail\":{}}\n^D\n```\n. LGTM\n. \n\n\n",
    "hamiltont": "Yes, and just running a sudo docker build . works as expected. Running the same build command from docker-py does not \n. For reference: \nhamiltont@3:/localhdd$ sudo docker info\nContainers: 6\nImages: 37\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Dirs: 49\nExecution Driver: native-0.2\nKernel Version: 3.11.0-24-generic\nWARNING: No swap limit support\nhamiltont@3:/localhdd$ sudo docker version\nClient version: 1.1.0\nClient API version: 1.13\nGo version (client): go1.2.1\nGit commit (client): 79812e3\nServer version: 1.1.0\nServer API version: 1.13\nGo version (server): go1.2.1\nGit commit (server): 79812000\n. hamiltont@3:/localhdd$ ls -la | grep dock\n-rw-rw-r--   1 hamiltont hamiltont       5 Jul  7 23:26 .dockerignore\nHere you go. It should not be relevant, but my cwd when I call c.build is different from the path I provide to c.build(path=foo).  \nhamiltont@3:/localhdd$ sudo docker -D build . | tee output.log\n[debug] archive.go:350 Skipping excluded path: .git\nSending build context to Docker daemon 263.8 MB\nSending build context to Docker daemon\nStep 0 : FROM ubuntu:14.04\n ---> e54ca5efa2e9\nStep 1 : RUN locale-gen en_US.UTF-8\n ---> Running in e5b6ad76d8e5\nGenerating locales...\n  en_US.UTF-8... done\nGeneration complete.\n ---> 21733509e288\nRemoving intermediate container e5b6ad76d8e5\nStep 2 : ENV LANG en_US.UTF-8\n ---> Running in aa17977bfe5a\n<snip>\n. I mentioned above that it works correctly if I run sudo build $MYPATH myself, the problem is that when I run c.build(path=mypath) the .dockerignore file has no effect.\n. I wonder if this tar function is the culprit. Is it successfully including .dockerignore in the tarred context? \nEDIT: Don't think that is it, the .dockerignore file is still inside the container that I build with docker-py\n. ha ha, I was wondering...\nI'm thinking the issue is that the client needs to detect .dockerignore, build a list of excludes, and ensure the are not tarred into the context. docker probably does this in go, but docker-py probably needs to be trained to look for a .dockerignore file and react to it. tarfile has a filter option that would do nicely\n. thanks @shin- and @brutasse !\n. For anyone else having this issue, you can hack it together by making your \"one command\" be a shell invocation as so:\ncont=c.create_container('ubuntu', command='bash -c \"cp /etc/adduser.conf adduser && echo -q hi\"')\n. Thanks a lot @brutasse and @shin- !\n. So I removed the loop, and occasionally I see this: \nProcess Process-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/tmp/FrameworkBenchmarks/toolset/benchmark/benchmarker.py\", line 653, in __run_test_in_container\n    for line in output:\n  File \"/usr/local/lib/python2.7/dist-packages/docker/client.py\", line 332, in _multiplexed_socket_stream_helper\n    header = recvall(socket, STREAM_HEADER_SIZE_BYTES)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/client.py\", line 319, in recvall\n    block = socket.recv(size)\nerror: [Errno 4] Interrupted system call\nStill trying to figure out how to best debug, as this seems to happen sporadically \n. Yea, that seems a likely subject. Running this program in a loop, I see this error about 1/1000 times, just so you have some debugging context \n. Oh can't close this on mobile....I'll close it later \n. @defunctzombie that option reports the output for all the dockerfile commands. However, running docker build requires two steps - 1) tar the current directory 2) run docker build. I'm asking for a way to report the progress of the \"tar\" step, which can take quite a while in a large directory. Here's a minor explanation of why it tars stolen from here:\n\nthe client is tar/compressing the directory (and all subdirectories) where you executed docker build. Yeah, that's right. If you execute this in your root directory, your whole drive will get tar'd and sent to the docker daemon. Caveat something. Generally that's a mistake you only make once.\nAnyways, the build gets run by the daemon, not the client, so the daemon needs the whole directory that included (hopefully) the Dockerfile and any other local files needed for the build. That's the context.\n. FWIW, I'm thinking a helper API would be a simple procedural thing to start with, and hopefully limited to the 10-15 functions that expose the most utility (not 100 functions that do simple things, that's just silly). Here's an example that needs better function names and organization but showcases my thoughts:\n\ndef does_container_exist(name=None)\ndef get_progress_percent(build_generator_item)\ndef convert_docker_timestamp_to_datetime(timestamp)\ndef get_intermediate_build_images(image)\ndef total_docker_diskspace_used()\ndef human_readable_created(container)\ndef container_name_from_cid(cid)\n...etc\nI'd like to avoid the OO-based approach shown below, at least initially, because this requires much more architecting/thought (and depends on the above procedural approach anyway, so why not start there?)\nclass Container():\n    # some container properties\nclass Image():\n    # some image properties\n...etc\n. Oh, great! I'm thinking this should be a subpackage of docker-py as it's a pretty minor addition, at least to start with. Here's an example: \n```\nImport core API that mirrors docker API\nimport docker\nImport helper functions that depend on core\nimport docker.helper\n```\nI welcome any suggestions on\n- A better subpackage name than helper\n- Helper functions (or function names) that we may want to include\nDrop suggestions anytime and I'll try to get a PR in soon with some basic functions\n. @Julian great ideas! FWIW I've got some basic code but it's a bit stalled, if anyone wants to take point on a PR for this I'm happy to deposit my current code into a gist in case it's useful \n. Here's a simple idea that I've used a few times, maybe it will help out here. \n```\nimport os\nimport threading\nimport sys\ndef log_for_docker(generator, line_prefix='', print_empty=False):\n  '''\n  To get generator, use client.logs(container=cid, stream=True)\n  It's perfectly reasonable to use this method once for STDOUT and \n  once for STDERR \n  '''\n  def log_docker(generator, print_empty, line_prefix):\n    for line in generator:\n      if print_empty or line.strip():\n        sys.stdout.write(line_prefix)\n        sys.stdout.write(line)\n  r_log = threading.Thread(target=log_docker, args=(generator,print_empty, line_prefix))\n  r_log.daemon = True\n  r_log.start()\n```\nNaturally this might cause output to be mixed when using multiple threads, but I've yet to have such serious issues that I've cared. If someone wants to modify this to take an output mutex, they could easily make it such that only one background thread is printing at a single time without hurting the performance of the main thread of execution. That's no guarantee that the main thread and the currently-printing background thread won't write at the same time, it would just help if you've got many container logs being tracked in one application\n. Usage for above code\nlog_generator = cli.logs(container=cid, stream=True)\n    log_for_docker(log_generator, 'RabbitMQ: ', True)\nOutput: \nRabbitMQ: \nRabbitMQ:               RabbitMQ 3.4.4. Copyright (C) 2007-2014 GoPivotal, Inc.\nRabbitMQ:   ##  ##      Licensed under the MPL.  See http://www.rabbitmq.com/\nRabbitMQ:   ##  ##\nRabbitMQ:   ##########  Logs: tty\nRabbitMQ:   ######  ##        /var/log/rabbitmq/rabbit@WebJuice-sasl.log\nRabbitMQ:   ##########\nRabbitMQ:               Starting broker...\nRabbitMQ: =INFO REPORT==== 28-Feb-2015::23:55:38 ===\n. Another one that may be useful here while I'm at it. Ensures a container dies with the program. \nNaturally this could be modified to support it's own kwargs (stuff like should_rm or timeout) that are stripped from the dictionary passed to start\n```\nimport atexit\ndef safe_start(client, args, *kwargs):\n  def container_cleanup(client, cid):\n    client.stop(cid)\nclient.start(args, *kwargs)\n  if len(args) == 1:\n    atexit.register(container_cleanup, client, args[0])\n  else:\n    atexit.register(container_cleanup, client, kwargs['container'])\n```\nCalled as: \n# Both method work fine\n  safe_start(cli, cid, publish_all_ports=True)\n  safe_start(cli, container=cid, publish_all_ports=True)\n. ",
    "zedtux": "When a release will be created including this fix ?\n. ",
    "ricardosasilva": "I'm using the Ansible docker module (http://docs.ansible.com/docker_image_module.html) and having the same problem. This fix would be fantastic.\n. Nice!! Thanks @shin!\n. Thank you @shin!!\n. ",
    "gmlewis": "Update... this is not a docker-py issue... tested using the following:\nsh\n$ docker rmi c4af5740a8e79f0899dff7d4ff121d1aaf8cdb7e61704b3c4e345716c7afaedc\nError response from daemon: No such id: 48e533d2b3cc3c789b65a1f0fc04b8256f03ef47895bf79cf8921e1b74ead323\n2014/07/08 17:17:23 Error: failed to remove one or more images\nClosing issue.  Filing on main Docker site.\n. ",
    "brutasse": "@shin- I just added an integration test\n. Is there anything else that needs to be done to help getting this merged?\n. @shin- done. I've added a unit test for utils.tar that validates a couple of scenarios. In addition:\n- Some tests were leaking temp dirs, I added a couple of addCleanup calls to properly delete them\n- Python 2.6 doesn't have addCleanup() so I added an implementation for 2.6 that's loosely similar to what unittest does in Python >= 2.7\nLet me know if anything else is needed.\n. :sparkles: :cake: :metal:\n. ",
    "Aigeruth": "@shin- There is a solution for this problem. It is called combine. I will check it out and update the PR.\n. I've updated the PR. Coverage reports should be fine now.\n. ",
    "robertglen": "It would be necessary there as well, I hadn't looked at the StringIO option\nas I am using python3 in which case the module requires BytesIO.\nBut to answer your question I'm sure the same line would be required for\nthat as well a few lines up.  I've added another commit to my branch.\nOn Wed, Jul 9, 2014 at 7:14 PM, Joffrey F notifications@github.com wrote:\n\nIs that necessary with a StringIO object too?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/dotcloud/docker-py/pull/271#issuecomment-48558053.\n\n\nRobert Glen\n. ",
    "adewes": "I also encountered this problem when trying to read from the results of the Container.logs(stream=True) endpoint. It would be great if there was a way to just get the HTTPResponse object from requests here, as this allows non-blocking reads from the result (AFAIK). Otherwise it would be great to add an async parameter to the function so that it can be used in non-blocking context.\nMy use case:\nI'm trying to use the function in a non-blocking generator loop where on every iteration I want to check the status of the container, return the new log output, and yield control to the caller. With the current implementation of the logs endpoint this seems not possible.. ",
    "eamonnfaherty": "resolved this.  I had a file caching issue that meant my changes were not appearing on my remote machine.\n. ",
    "codebynumbers": "Appears to be a duplicate of #267, my bad.\n. ",
    "thebenwaters": "+1\n. ",
    "fxdgear": ":shipit:\n. ",
    "wojons": "can we get this merged\n. ",
    "wt": "I am not a huge fan of modifying an _ attr since I understood those attrs are conventionally not supposed to be altered from the outside. I would prefer to have a public api to do it, be it a property or a method.\n. ",
    "gtaylor": "@wt Agreed. Referencing a private attribute gets flagged as a warning in Pylint and PEP8, IIRC. \nPerhaps we could rename _timeout to timeout, thus bringing it public? Probably not enough going on here to warrant a full method. A property or a public attrib would be peachy.\nI don't know if docker-py has a policy on this yet, but Django and many other projects offer no backwards compatibility guarantees for private vars/attribs/funcs/methods. Worst case, we could easily alias _timeout to timeout if we were concerned.\n. Sent a pull request just for the --rm=true/rm=False default mis-match. I would have never noticed that difference until something unexpected started manifesting.\nIt looks like forcerm now exists and is documented, addressing the other issue mentioned.\n. We had to figure out how to handle this very issue with boto a few years ago. Initially, I felt that a separate package with the more sane API would make the most sense. However, that would mean juggling two projects with two potentially different release cadences, with a really confusing backwards compatibility story (boto evolves/evolved fast, like Docker).\nWe eventually settled on a layered approach. The official Amazon Java SDK was imitated closely in layer1, and a more humane API took form in layer2. You can see this in practice in the dynamodb2 module. Amazon's DynamoDB API was particularly difficult to deal with at first, so this second layer brought some much-needed sanity and usability to the table.\nHowever, some developers really do need access to the \"native\" level in order to specially optimize or handle some more complex cases. Most end up using layer2, because it's easier, but this setup allows everyone to be happy.\nI don't know what this would look like in docker-py, but figured I'd share experiences from another Python module that also had to track a rapidly changing upstream API. It sounds like you are leaning that way, anyway.\nFWIW, I'd gravitate towards a module name that suggests its primary purpose (helpers, conveniences, friendliness). Maybe something like docker.humane.\n. I don't care as much to argue about which documentation system to use (though I do subjectively prefer Sphinx), but almost any modern editor is able to collapse (or auto-collapse) docstrings. As a consequence, I tend to see the \"bloat\" counter-argument as a weak one.\nAs @nir0s says, you do get some maintainability benefits, and you also pick up on nice features like being able to cross-reference other modules/classes/functions in the docs (:py:class:somemodule.Class``). You can still do conversational/tutorial style docs to complement your API reference pages, too.\n. Not entirely your particular set of symptoms, but I was playing around with pull() on my own and noticed that passing in an invalid image/tag results in something like this:\n```\nStatus Code: 200\n{\"status\":\"Pulling repository blartyblartyblarty\"}\n{\"errorDetail\":{\"message\":\"Error: image library/blartyblartyblarty: not found\"},\"error\":\"Error: image library/blartyblartyblarty: not found\"}\n```\nA look at the Docker API docs showed that sure enough, only 200 or 500 are listed as probable response codes, though I'd expect a 403 to be handled by some form of wider-reaching middleware. On the docker-py side, raise_for_status() is being called correctly, but the Docker API is sending a 200 (meaning no exception is raised).\nIt might be worth checking and seeing if you are actually getting an HTTP 200 instead of a 403, even though the body is saying 403. I may be way off on this, but figured I'd share a somewhat related experience, since this tripped me up as well.\n. I'm not too in touch with how the Docker API was designed, so I'm not sure if these 200's with error messages were intentionally done this way for a reason. From my uninformed third party perspective, I'd definitely expect a 4xx when attempting to build an invalid image (as per the previously shown example).\nWhether this means filing an upstream bug in docker itself (seems like someone would have complained about this by now), or figuring out how to handle it in docker-py, I have no idea.\nHopefully my outlined case is just another relevant data point :)\n. Subscribed! At least it's been reported before. I would have figured there'd be more than a few people asking about this.\n. Thinking he may be talking about the Python 3-specific issue outlined here kennethreitz/requests/pull/2344\nThis looked like the fix: https://github.com/kennethreitz/requests/commit/bd3cf95e34aa49c8d764c899672048df107e0d70\n. I just realized that I renamed _timeout, but now _version ends up being inconsistent in that it is also a kwarg and of general interest, yet private.\nWould you accept another PR that renamed _version to version?\n. ",
    "dnephin": "Now that #288 is merged, you probably don't need this, although it wouldn't hurt to test against the latest requests module either.\n. Thanks!\n. If you get a timeout on the client it's almost always an issue with the server. 60s should be more than enough time to wait for a response.  Since you're running such an old version of docker, it's probably worth upgrading.\n. @cpuguy83 \n\nIn 1.4 passing in HostConfig on start is deprecated\n\nIt sounds like it's actually been removed entirely, not just deprecated\n. @cpuguy83 awesome, thanks!\n. Thanks for the contribution! Unfortunately I don't think this is something we want to support.\nWe'd like .dockerignore to work identically to the docker cli implementation, and I don't think it has this special case for .git.\n. I think this is missing label, which is also missing from the docker remote API docs. I have https://github.com/docker/docker/pull/12798 open to try and get it added.\n. I'd be ok with the idea as long as we make it opt-in. Adding a new kwarg that defaults to False, and when it's true we can write the config.\n. This is fixed in the 1.4.0 release\n. LGTM, agree we can adjust test names in followup\n. Either that one or dnephin@docker.com works for me. I guess I should update my gitconfig\n. Ah, my searches missed it because I only looked in issues not PRs!\nThanks!\n. LGTM\n. LGTM\n. LGTM (once janky is done and happy)\nI think the docs for filter format could be improved (currently undefined format), but we can do that as a followup\n. LGTM\n. I noticed this while adding python3 support to compose. The streams that you receive from stream=True calls are bytes (I was expecting str).\nThere is an option to have the endpoint return already loaded (which handles the encoding).  It would be nice to get streams of str, but that would be a breaking change, so I'm not sure we can do it initially.\nWe can at least update the examples to include the encode.\n. LGTM\n. overall LGTM\n. I suspect this is an issue in python2 (and not python3). Can you confirm?\n. LGTM\n. LGTM\n. If you want to just make this default, you could do this https://pytest.org/latest/customize.html#adding-default-options\n. I personally always use --tb=short, you still get the full traceback (filename and line number) for the relevant stack frames, it just doesn't show you the full source for every function.\nhttps://pytest.org/latest/usage.html#modifying-python-traceback-printing\nLGTM\n. LGTM\n. I'm running that test in a loop (against docker 1.8.1) and not seeing any failures yet (after 30+ iterations). What version of docker are you running against?\nI can also try with more tests in case it's test pollution.\n. I've re-run the travis build for master a few times https://travis-ci.org/docker/docker-py and it isn't failing there yet either. I'll try a couple more.\n. I've been running the entire suite in a loop and haven't been able to reproduce the error.  Is it possible that the error is triggered by your configuration? \nMaybe try modifying the test to expose more details about the failure? (from the current error it's not clear what is None, or what state the container is in)\n. LGTM\n. LGTM\n. That fix isn't in a release yet though, so we'll have to wait for 1.9 before we can merge this.\n. Maybe just add that to the release notes (that the new default version is 1.9.0) ?\n. LGTM\n. LGTM\n. some CI failures, otherwise looks good\n. LGTM\n. I didn't look over everything, but I assume it's all just moving stuff around.\nI think the structure looks great, but there is a test failure on janky. I think something needs to be updated for the new structure.\n. LGTM\n. Oops, nevermind. We're just using it wrong in compose.\n. LGTM\n. The api docs (http://docker-py.readthedocs.org/en/latest/api/) are ordered alphabetically, which seems logical to me. How would you suggest changing it?\n. Might be worth thinking about any other breaking changes you'd like to make before doing a 2.0 release.  Sounds reasonable to me.\n. LGTM\n. Very interesting.  How would this fail if there was unicode? Would it crash or just fail to authenticate?\n. LGTM \nless setup is faster tests!\n. It's not pretty, but you can see how we do it in compose here: https://github.com/docker/compose/blob/b6b9b002/compose/service.py#L738-L745\n. Actually, I think I linked you to the wrong code. The actual error handling is here:\nhttps://github.com/docker/compose/blob/b6b9b002e5c0b1687fac3e5e1e5c095b1c97bf8d/compose/progress_stream.py#L46-L47\nI just took a look at the output from the API. I'm seeing this as the last message in the stream:\n{\n  'errorDetail': {\n    'message': u\"The command '/bin/sh -c exit -1' returned a non-zero code: 2\",\n    'code': 2\n  },\n  'error': u\"The command '/bin/sh -c exit -1' returned a non-zero code: 2\"\n}\n. LGTM\n. LGTM\n. LGTM\n. I think I remember seeing something about earlier 3.x kernels not supporting this option. What version does uame -a report?\n. If I could add one more point, I think it would be awesome to have a link from the docker-py docs to the official API for the endpoint as well.\n. Thanks for the PR!\nA test for this case would be great.\nI don't think we have a set release date for the 1.6.0 milestone.\n. Code changes look good to me, but the jenkins build is broken\n. LGTM\n. Code change looks ok to me.  \nDo you think this is a bug in requests ? I'm not sure I understand why we have to handle this\n. LGTM, just needs a rebase to be merged\n. I believe this is the behaviour of the API (http://docs.docker.com/engine/reference/api/docker_remote_api_v1.21/#get-container-logs), so we shouldn't be modifying that on the client.  You might need to open this issue on the docker project.\n. Ah, good call\n. LGTM\n. LGTM, some minor suggestions, but no blockers\n. LGTM\n. I like it, left some comments\n. LGTM, but I think it needs a rebase.\nIt might be that the gihub \"branch protection\" is preventing me from merging this because I'm not an owner on this repo.\n. looks like there is one flake8 failure, otherwise LGTM\n. This was already added in #846, it's in master and will be in the next release.\n. LGTM\n. I think you're on the right path.  A PR for this would be great (once you've got the test passing).\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. I think we should add the options field. This PR requires changes to the docs and tests.\nThere is a much easier way to disable the network for a container by create it with net=\"none\"\n. LGTM\n. LGTM\n. I believe that functionality is handled by the client. That's how we handle it in Compose: https://github.com/docker/compose/blob/master/compose/cli/main.py#L639-L641\nThis is likely why --rm is incompatible with -d, because the client wouldn't have any idea when the container exited.\n. You'll have to set the api version if you aren't using the latest server. You can use either docker.Client(version='1.20') or docker.Client(version='auto')\n. Seems reasonable to me. \nPreviously we weren't converting index.docker.io to docker.io which caused the \"find auth config for hostname\" to fail for a private repo on the hub.  After this change we normalize to docker.io.  Is that right?\n. I think this is by design. If you use a fileobj that file is the context for your build. It should be a tarball, and any ADD directives would look in the tarball for the files.\nI think this is incompatible with path, where it builds the tarball for you first.\n. LGTM\n. LGTM\n. > But I don't want to install another binary package for docker client  ... If there was a command line tool dockerpy\nThese are conflicting requirements, you don't want to install one binary, but you want to create another one that will be installed?\nI don't think there is any reason to create a dockerpy binary that would be a clone of the docker cli binary.\nIt is possible to build a client-only binary from the current docker/docker repo. \n. I think something like this is probably a good idea. I'm not sure if \"Pitfalls\" is the right term. Maybe \"Gotchas\" or just FAQ?\n. LGTM\n. LGTM\n. Couple small nits, otherwise LGTM\n. LGTM\n. LGTM\n. From the error message \"/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/virtualenv_support/pip-8.0.0-py2.py3-none-any.whl/pip/_vendor/pkg_resources/init.py:87: UserWarning: Support for Python 3.0-3.2 has been dropped. Future versions will fail here.\"\nWe should stop testing against python3.2 if pip has dropped support for it.\n. LGTM\n. LGTM\n. @TJNII I think that is unrelated. You can specify stream=True, which returns a list of dicts as you would expect. It's not really possible to go changing all the response types across the entire library without releasing a V2 since it's a backwards incompatible change. I think most people are just using the stream=True option.\nIf you want to open a new issue about pull() raising NotFound that sounds appropriate for a feature request.\n. LGTM\n. I would recommend http://docs.python-guide.org/en/latest/dev/virtualenvs/\n. I use https://github.com/spotify/dh-virtualenv to create deb packages, so that I can use the virtualenv\n. LGTM\n. LGTM\n. I haven't looked over the docker cli logic, but I think this makes sense. I know we've had multiple requests for this in Compose.\nLGTM\n. LGTM\n. LGTM\n. I don't think it's just about performance. It's to support follow=False, stream=True. \nStream is a client-side option, follow is a server side option. We want to be able to stream (read the http response line by line and return an iterator instead of buffering the entire response), and not follow the logs.\nOtherwise running logs(stream=False) on a container with lots of logs would cause the process to use lots of memory (which I guess in a sense is performance related).\n. Thanks for the PR. I left aa couple comments.\nOnce it's ready to go, we'll also ask you to squash your commits down to one or two.\n. LGTM\n. The expected output doesn't match the actual output. Look at tmpfs /mnt\n. Merged as part of #994\n. A test case would be good\n. Looks good, just one consideration for backwards compatibility.\n. LGTM\n. I don't think we should duplicate the list, but linking to it sounds appropriate.\n. LGTM\n. This is what we do for Compose to convert the markdown to rst before publishing to pypi:\nhttps://github.com/docker/compose/blob/master/script/release/push-release#L57-L66\n. LGTM, based on my understanding of the problem\nAn integration test against a mock tls server would be great, but not something we have time for atm.\n. a test case would be good\n. Are you sure this is a client-side feature? Since it's configured by HostConfig I would expect it to be handled by the daemon.\n. you're right it is client side.\n. LGTM\n. LGTM\nThis seems to be the most reliable option for now, and users can always override the default if they really care, and know that a newer version is available to them.\nI did a quick search and this seems to be the suggested fix in any of the related issues I found.\n. LGTM\n. LGTM\n. I think the daemon probably runs as a different user, so it won't be looking in the right directory for the ssh config?\n. LGTM\n. LGTM\n. Code looks good, but the test is failing for some reason\n. LGTM\n. LGTM\n. LGTM\n. I think the OP was able to track down the differences already:\n\nThe only real differences seem to be the inclusion of the owner user and group names in the latter, and a flag in the file mode header (grep for 0100644 vs. 0000644)\n\nThat would be a good place to start. If we can make those consistent I think it's very likely that the tarballs  will match.\nSo we need to set tarinfo.uname and tarinfo.gname to the empty string (https://docs.python.org/2/library/tarfile.html#tarfile.TarInfo.uname), and figure out how to set that filemode flag correctly.\n. LGTM\n. It was merged in #994, it's in master now and will be in the 1.8.0 release\n. I don't understand the question. It accepts the param and passes it to the docker API which adds the tmpfs to the container. What behaviour do you expect from it?\n. Thanks, I think this is a necessary fix. The commit just needs to be signed.\n. LGTM\n. LGTM, test failure seems unrelated?\nbut I guess a test case wouldn't hurt either\n. LGTM when build is green\n. LGTM\n. LGTM when build is green. Not sure why they all failed, maybe just a jenkins issue?\n. LGTM\n. Pass version to the Client constructor: http://docker-py.readthedocs.org/en/stable/api/\n. You need a more recent version of pip I believe. I think > 6.x\n. I believe these are specified under networking_config:\n``` python\ncreate_container(..., networking_config={\n  'EndpointsConfig': {\n    'thenetworkname': {\n      'IPAMConfig': {\n        'IPv4Address': '...',\n        'IPv6Address': '...',\n      }\n    }\n  }\n})\n``\n. LGTM\n. Will this fix https://github.com/docker/compose/issues/3393 ?\n. Code change looks good. How about a test to cover this case?\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. This looks great. Just a couple minor comments. \n. LGTM\n. LGTM\n. LGTM\n. Looks like the integration test is unhappy.\n. LGTM\n. Cool, missed the earlier one\n. LGTM\n. LGTM\n. code LGTM, waiting on janky\n. LGTM, might need a rebase\n. LGTM\n. They are supported. They are passed inhost_config`.\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. I don't think I like this idea. You can always do this:\n``` python\nparams = kwargs_from_env(...)\nparams.update(custom_params)\n```\nor, if you want to override defaults:\n``` python\nparams = {...}\nparams.update(kwargs_from_env(...))\n```\nPassing in defaults here is unnecessary, it makes it very difficult to expand the params of this function in the future.\n. LGTM\n. The rm option was actually just moved to the server side: https://github.com/docker/docker/pull/20848\nIt will be in the engine 1.13 release, so API version 1.25. It's possible docker-py my be able to support it in the 1.10 release\n. LGTM, just a minor nit\n. LGTM\n. LGTM\n. There are a few options: http://stackoverflow.com/a/19126329/444646\nI like the idea of using a pytest since it's an awesome test framework: http://doc.pytest.org/en/latest/usage.html#creating-junitxml-format-files\nI believe docker-py already uses it as a test runner.\n. LGTM\n. LGTM, just one comment.\nI'm not all that familiar with these credential stores, but the code looks good to me.\n. LGTM\n. Seems fine to me\n. LGTM\n. LGTM\n. How will this integrate with Compose? Will we set the number of concurrent operations based on this constant in docker-py?\n. I think that would be good.\nLGTM\n. LGTM\n. LGTM\n. LGTM\n. This LGTM, but I'd like to make sure @shin- is ok with it.\n. LGTM\n. It looks like #1192 is in the 2.0 milestone, so probably won't be merged right away. It would be great to get this in earlier. It's been a commonly raised issue in Compose and docker-py.\nIf you could find a way to backport filter= that would be great.\n. LGTM. A test would be great\n. LGTM. Only the CS release had it in 1.24 I think, but I guess that's still more correct than no check.. maybe use  https://docs.python.org/2/library/urlparse.html#urlparse.urlparse instead ? It should be more robust to unexpected parts of the url.\nIt's available as six.moves.urllib.parse.urlparse\n. Couple minor suggestions here:\n- best practices says to make these a single line, otherwise you can run into a situation where the update is cached without the available package you need to install\n- instead of easy_install pip why not add python-pip  to the apt-get install line? If you want a newer version https://pip.pypa.io/en/latest/installing.html#install-pip suggests their install script.\n. Instead of  running both processes in the same container, what do you think about doing this:\n- change the base of this Dockerfile to be something like debian:jessie\n- change the docker run link to the dind image instead\ndocker run -d --name dpy-dind --privileged dockerswarm/dind:1.8.1 docker -d  -H tcp://0.0.0.0:2375\ndocker run --rm --link=dpy-dind:docker --env=\"DOCKER_HOST=tcp://docker:2375\"\ndocker rm -vf dpy-dind\n(which would allow the CMD in the Dockerfile to change to just py.test -rxs tests/integration_test.py\ndpy-dind could be a variable even, which would allow for jenkins to set a unique value for each build.\nThis is the approach aanand used in https://github.com/docker/compose/pull/1783\n. I think one advantage is that you can use the tests/Dockerfile without dind if you want to, which can make it easier to debug problems.\nIt also makes it easier to see errors from the daemon (since the current setup sends them to /dev/null).\n. Is this being done as part of the jenkins config? Maybe just change this to a comment so that users can run it locally ?\n. Ah I see, so should the integration-dind target depend on build and build-py3 ?\n. oops, right!\n. I'm also a fan of having a descriptive name and linking to the issue in the git commit\n. This check is rather verbose (and needs to be repeated in a few places).  You can actually do something like this to re-use the mark (https://pytest.org/latest/skipping.html#skipif):\n``` python\ndef requires_api_version(version):\n    return pytest.mark.skipif(\n        docker.utils.compare_version(\n            version,\n            docker.constants.DEFAULT_DOCKER_API_VERSION\n        ) < 0,\n        reason=\"API version too low\"\n    )\n@requires_api_version('1.21')\ndef test...\n```\n. \"currently\"\n. I'd suggest parameterizing this decorator with the version, so it can be used anywhere.\n. This new decorator is great!\n. Should we delete these 2 lines as well?\n. delete these two as well?\n. It would be good to document which exception.\nUsing rst and sphinx you can do :class:docker.errors.APIError and get it all linked up :smiling_imp: \n. This is one place where pytest can really improve the test code. Instead of having every test attempt this, you can use a https://pytest.org/latest/yieldfixture.html#yieldfixture (or https://pytest.org/latest/fixture.html#finalization) in just the tests that use a volume to do the proper cleanup.\nWe don't have to make that change now, but I think we should move in that direction eventually.\n. Minor, but if the tests crash or are killed between line 1394 and 1395 the volume won't be cleaned up. Same problem if the client call creates the volume, but then hits a timeout.\nI think this needs to be:\npython\nname = 'perfectcherryblossom'\nself.tmp_volumes.append(name)\nresult = self.client.create_volume(name)\nSame for the tests below\n. I don't think the improvement is in performance, it's in clarify.\nWith a single base class for all tests, it's not clear which tests actually make use of this feature.  Refactoring and moving the test suite around becomes harder because everything needs a single base.\nWith a yield fixture it's explicit that the test requires volume_cleanup. \nNot sure how it would make the code more complex.\n. I think these two lines need (123, 124) to be swapped, otherwise container start failures will prevent a container from being removed.\n. efficiency seems a bit strange to me for a module name.  What do you think about just making this docker.build ?\n. This sounds like a bug, do you know we can reproduce it?\nIf there is an open issue on engine, maybe add the url to the issue here\n. What do you think about making these factory classmethods on BuildContext instead of separate functions?\n``` python\nclass BuildContext(BuildCtxTuple):\n@classmethod\ndef from_tarball(cls, path, dockerfile='Dockerfile'):\n   return cls(...)\n\n@classmethod\ndef from_dockerfile(cls, ...):\n    ...\n\n```\n. Base classes like this are a common pattern in java, but I always avoid them at all costs.\nThe code re-use is pretty minimal here. Instead of the @property _id , you could just assign to self.id, right ?\nSo instead of having two base classes you have to do two assignments in a couple constructors, but it means that developers don't have to worry about things that might be hidden away in a base class.\n. I think we should to make this class compatible with compose.container.Container so that we can replace it with this one.\n. I think this is a bad idea. Globals make libraries much more difficult to use.\nI would pass in a client instead of registering a global. It's going to make this API much more re-usable, easier to test, and easier to reason about.\n. I think normally we want to raise an error if someone is using an option that isn't supported by the API version.  It would be nice if the server did this, but I don't think that's the case most of the time.\n. I'd like to make a suggestion here that I think will make it easier to test.  I think the logic for \"find config file\" can be separated from \"read contents of config file\".\n``` python\ndef find_config(config_path=None):\n    for path in (\n        config_path, \n        os.path.join(os.environ.get('DOCKER_CONFIG'), os.path.basename(DOCKER_CONFIG_FILENAME)),\n        os.path.join(os.path.expanduser('~'), DOCKER_CONFIG_FILENAME),\n        os.path.exists(os.path.join(os.path.expanduser('~'), LEGACY_DOCKER_CONFIG_FILENAME))\n    ):\n        if path and os.path.exists(path):\n            return path\nreturn None\n\n```\n. This could be done with \n``` python\nwith mock.patch.dict(os.environ):\n    ...\n``\n. Now that the tests use py.test, you could just useassert x not in y` and get a really nice error message when it fails! (instead of having this wrapper)\n. I think a comment about why this needs to exist would be great (so we don't forget). Something about which API version would hit which branch.\nThat way we know when we can remove it as well.\n. minor: I think adding it to tmp_containers should happen before self.client.start(), otherwise start failures would leave a created container lying around\n. very minor: this is equivalent to logs = six.binary_type()\n. same minor ordering issue here\n. Don't you still need an empty __init__.py here for the tests to be considered python modules?\n. Ah, I should have looked at the expanded changed files. cool\n. This does follow the pattern established by attach()/attach_socket() but I wonder if it could be just another kwarg like stream?\nWhat do you think @shin-  ?\n``` python\ndef exec_start(self, exec_id, detach=False, tty=False, stream=False, socket=False):\n    ...\n    func = get_response_handler(stream, socket)\n    return func(res)\ndef get_response_handler(stream, socket):\n    if stream and socket:\n        raise ValueError(...)\nif stream:\n    return functools.partial(self._get_result_tty, True)\nif socket:\n    return  self._get_raw_response_socket\nreturn  functools.partial(self._get_result_tty, False)\n\n``\n. thisself.tmp_containers.append(id)line should come beforeself.client.start(id)` so that the container gets cleaned up if start fails.\n. I think these should be args, not kwargs, since they are both required.\nSame for host_config_type_error\n. Removing the container here isn't safe. Any test failures will prevent this cleanup.\nDo you need this line? I think the convention is to use self.tmp_containers ?\n. I think if you use print(..., file=sys.stderr) with from __future__ import print_function you won't need the encoding.\n. pytest provides an interesting way to do this without having to import it into every module.  https://pytest.org/latest/fixture.html#autouse-fixtures-xunit-setup-on-steroids\nYou can use a conftest.py module, and @pytest.fixture(autouse=True, scope='session') which will run it once for every test run.\n. I think putting the file at tests/integration/conftest.py would do what you want\n. A test with an @ digest would be good I think\n. I think this could use a new test case. I don't see anything asserting these are read properly.\n. could be a single line\n. These methods don't seem to be very much, do we need them?\n. Should this be 'networking_config.endpoint_config.aliases' since the error is about a key relative to the host_config?\n. Ok\n. Minor: I think could be clarified as \"Any value or 'unset' counts as true\"\nI had to look at this a couple times before I got it.\n. I don't think this is being gzipped.\nThe formatting is off a bit, there should be spaces before and after %.\nI believe it's also common practice to use double quotes for strings that are messages to users (and single quotes for strings that are internal constants). \n. This function seems unnecessary. Why not just move the implementation of chk_print here?\n. I like the idea of this abstraction, however I think reporting total reads is a bit weird.\nI think it would be good to first get the full size of file, and report \"send / total\" like the docker-cli does.\nThis class implements both the iterator interface and part of the file interface. Are both necessary?\n. I think seek(), and tell() should be available on the file object already\n. I think there should be a kwarg gzip_context (default to False) to enable it. There may be cases where it's undesirables, and defaulting it to false keeps it backwards compatible.\nThe new kwarg will need a small docs addition.\n. Minor: I think the kwarg could be slightly more verbose, maybe warn_on_error=True or raise_on_error=False ?\n. Or considering this is only required for parsing the legacy config, maybe include_legacy=True ?\n. I doesn't seem like we do this os.path.exists() check in other places, is it necessary?\n. and I believe the default should be set() not {}, right?\n. I don't think this if i.isref() branch is necessary. I think the warning in the docs is only for the case where you're passing a fileobj. I don't think it means that it is required to pass it on windows.\nDid you run into problems without it?\n. This looked weird, but it does match pkg/archive/archive_windows.go in docker/docker, so I guess we should keep it this way. Not sure why write is stripped from group/other.\n. This isn't correct, the function needs to stay at 1.21. You can add an if branch within the function to require a higher version for specific params (there are examples of this already).\n. same here\n. I think this needs to be the other way around. \n``` python\nif version_lt(...):\n    raise errors.InvalidVersion(...)\n``\n. I wonder if we should setfollow=None` and do:\n``` python\n'follow': follow if follow is not None else int(stream)\n```\nto keep it backwards compatible\n. There seems to be some inconsistencies in the error messages here. This one says must be a list, but the previous branch accepted dicts.  The next one says they must be something else.\n. hmm, I just looked at this again, and maybe this needs to be if value is None\n. Minor: This could probably be two separate tests\n. From this test change it's not clear to me what the expected behaviour is supposed to be.\nIt seems like we're reverting part of the recent change, is that right?\n. So in the case where DOCKER_TLS_VERIFY= we're changing from tls being enabled by default, to tls being disabled by default?\n. This is failing with a KeyError for some reason, I'm not sure why, it seems to be correct to me.\n. It seems a bit strange to me that a library would report itself in the error message.\nUsually a library is consumed by some other application, which probably doesn't want to expose these details in error messages.\nI don't really know how I feel about this change. I think a better solution might be to just include the error string in the docs, that way if someone hits the issue and they search for it, they'll find the docs about how to set the version.\n. I don't think we want to remove the tests for PY2. Having them in separate test cases with pytest.mark.skip would be fine.\n. Oh, I missed that, sounds good.\n. some commented out code here\n. err is not used here, should it raise an exception if there is an error?\n. same with this err\n. cool sgtm\n. stream changed here to always True. Is that correct?\n. Well that's fun\n. Are these normal ipv6 addresses? I usually see something like fe80::4216:7eff:fee8:f206/64\n. Ah cool, that's what I was wondering. Thanks!\n. commented out import\n. The CLI doesn't support this syntax for mounts, so I'm not sure it should be supported here\n. I think you can write this as :\npython\nversion  = kwargs.pop('version', None)\n. I wonder if this should raise an exception? The user explicitly asked to use the credential store, right ?\n. s/406/httplib.NOT_ACCEPTABLE/\n. It looks like auth is still required otherwise this function returns early. Is that expected or could someone has an identitytoken without an auth key?\n. I think there is a missing return or continue here, no?\n. I think this might be better as a type ImageNotFound(APIError) instead of a method on the general APIError\n. I often find it useful to have all the errors from a package extend a common base class. That way if you want to catch every error from a package, you can do it without catching errors from other packages.\nSo maybe this should extend APIError ? Or we could create a more general error as a base for both?\n. I usually try to avoid inheritance. Right now these base classes are small, but their existence will tempt contributors to add more and more methods to these base classes.\nCollection isn't doing much right now. prepare_model() could be just a function prepare_model(model, attrs), it doesn't need to be a method at all.\nThe only other implemented method is __init__, which could be moved to the sub classes. The url param doesn't appear to be used, so it's really just two lines. Sometimes I think it's better to have that very minimal duplication instead of the risk introduced by a base class.\n. I think it would be nice to preserve backwards compatibility here.  Maybe we could leave the existing Client and from_env and add new ones for the new higher-level api?\n. A comment explaining why this is necessary would be great\n. >  because constructing objects like this is a nightmare for the user.\nI don't really understand why this would be. Could we maybe support input as dicts instead of objects?\nThe nested form really seems like it's a lot nicer to work with, but I'm not against providing both.\n. I see what you mean. I might say the issue here is that the most common options are deep in the nested structure. I agree that the most important options for the service should be easy to include.\nI guess I'm ok with having both interfaces, a higher-level flattened one, and the lower-level one that matches the API types.\n. Is this class basically just a namedtuple?\n. Sorry, I pushed this commit in a rush yesterday since I was working on a different laptop, and didn't have a chance to comment. Thanks for carrying the commits.. What's this change about?. Comment why it would be one or the other?. Sorry I added this empty test stub. I think we can remove it now that there are other tests.. Thanks, makes sense. ",
    "wgaggioli": "resolved by #288 \n. ",
    "merll": "Yes, I assume that this issue only occurs on Windows. On Mac OS X and Linux I could not reproduce it.\n. Rebased with new branch in #316.\n. The docker service can have a different certificate configuration, e.g. a self-signed certificate that the docker-py app is not aware of. In that case, the behavior between docker-py and the client (e.g. command line) is different.\nI would actually suggest for the ping to be optional altogether, since that also requires for a direct connection from the app to the registry. If the remote docker service is performing push or pull operations, it can be assumed that it has access to the registry; the connection from the app to the Docker Remote API may however run through a private network without external access.\nIn my opinion the optimal solution would be adding a ping command to the Docker Remote API.\n. I no longer have this setup, but since that part of the change is easy to follow I find it reasonable to assume that it would still work.\n. ",
    "kvchen": "It's likely that you're connecting to the wrong IP/port, or you're behind a firewall of some sort. Check your setup to make sure that's not the case!\n. ",
    "wangzhezhe": "Thanks for your suggestion!\nMy docker deamon didn's listen to the right ports,I was already fixed that problem.\n. ",
    "anselal": "I get a similar error when I try to connect to docker daemon after using the remove_container method. My output is:\n127.0.0.1 - - [04/Jul/2016 13:07:25] \"GET /docker/images?cmd=get-records&limit=100&offset=0&sort[0][field]=RepoTags&sort[0][direction]=asc HTTP/1.1\" 500 -\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 2000, in call\n    return self.wsgi_app(environ, start_response)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1991, in wsgi_app\n    response = self.make_response(self.handle_exception(e))\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1567, in handle_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1988, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1641, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1544, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1639, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1625, in dispatch_request\n    return self.view_functionsrule.endpoint\n  File \"/home/anselal/docker-isthmos/app/views.py\", line 21, in docker_images\n    images = client.images()\n  File \"/usr/local/lib/python2.7/dist-packages/docker/api/image.py\", line 39, in images\n    res = self._result(self._get(self._url(\"/images/json\"), params=params),\n  File \"/usr/local/lib/python2.7/dist-packages/docker/utils/decorators.py\", line 47, in inner\n    return f(self, _args, _kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/client.py\", line 120, in _get\n    return self.get(url, _self._set_request_timeout(kwargs))\n  File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 487, in get\n    return self.request('GET', url, _kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 475, in request\n    resp = self.send(prep, _send_kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 585, in send\n    r = adapter.send(request, *_kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/requests/adapters.py\", line 467, in send\n    raise ConnectionError(e, request=request)\nConnectionError: HTTPConnectionPool(host='127.0.0.1', port=2375): Max retries exceeded with url: /v1.22/images/json?only_ids=0&all=0 (Caused by NewConnectionError(': Failed to establish a new connection: [Errno 111] Connection refused',))\n. I am actually using unix socket to connect to docker. I do not understand why it tries to reconnect to the daemon with tcp.\n. from docker import Client\nclient = Client(base_url='unix://var/run/docker.sock')\n. Well, ok. I will try to debug it. Thanks anyways\n. Hi @shin- I am really really sorry. I posted on the wrong project.\n. ",
    "TomasTomecek": "@anselal you're connecting to docker engine running on 127.0.0.1:2375 but your flask app is not able to connect. Can you verify that engine is running on the port and accepting connections? This can be easily done with curl:\n$ curl -v 127.0.0.1:2375/info\n. This is the code of docker-py which results into failure:\nFile \"/home/anselal/docker-isthmos/app/views.py\", line 21, in docker_images\n  images = client.images()\nHow's the client being initiated?\n. But the exception mentions localhost, not unix socket. Looks like the app either\na) changes the base_url of client instance\nb) initiates client multiple times\nAnyway, this is certainly not an issue of docker-py.\n. same thing as #516 \nResolved in 1.0.0.\n. Oh, I feel so silly now. I though this call fetches only some metadata. Should have read the REST call. Sorry for the fuss and thanks for help.\n. I've already started working on a tool where one of the functionalities is to provide such helper functions:\nhttps://github.com/DBuildService/dock/blob/master/dock/core.py#L151\n. @leth \nUnfortunately there is no specification, just \"example response\". Would be nice to have proper specification what one can get from the /build API call.\n@shin- \nThanks for info. I guess that we can close this.\n. okay, will try to create a PR (no promises)\n. @shin- just checked the original PR. Are you fine with detecting api version in Client's constructor? That's gonna be a change in behavior. Right now there are no requests in constructor. I was actually thinking about hooking api discovery before first request (downside: first request will take longer). \n. rebased, fixed flake8 errors\n. Makes total sense to me. I'm in favor of the auto keyword \u2014 it's short and super descriptive.\nMy primary use case is that when I want to try something out with docker-py, I'll open ipython and do:\n``` python\nIn [1]: import docker\nIn [2]: d = docker.Client()\nIn [3]: d.images()\nAPIError                                  Traceback (most recent call last)\n in ()\n----> 1 d.images()\n        global d.images = >\n/usr/lib/python2.7/site-packages/docker/client.pyc in images(self=, name=None, quiet=False, all=False, viz=False, filters=None)\n    647             params['filters'] = utils.convert_filters(filters)\n    648         res = self._result(self._get(self._url(\"/images/json\"), params=params),\n--> 649                            True)\n        global True = undefined\n    650         if quiet:\n    651             return [x['Id'] for x in res]\n/usr/lib/python2.7/site-packages/docker/client.pyc in _result(self=, response=, json=True, binary=False)\n     96     def _result(self, response, json=False, binary=False):\n     97         assert not (json and binary)\n---> 98         self._raise_for_status(response)\n        self._raise_for_status = >\n        response = \n     99 \n    100         if json:\n/usr/lib/python2.7/site-packages/docker/client.pyc in _raise_for_status(self=, response=, explanation=None)\n     92             response.raise_for_status()\n     93         except requests.exceptions.HTTPError as e:\n---> 94             raise errors.APIError(e, response, explanation=explanation)\n        global errors.APIError = \n        e = HTTPError('404 Client Error: Not Found',)\n        response = \n        explanation = None\n     95 \n     96     def _result(self, response, json=False, binary=False):\nAPIError: 404 Client Error: Not Found (\"client and server don't have same version (client : 1.18, server: 1.17)\")\n\n/usr/lib/python2.7/site-packages/docker/client.py(94)_raise_for_status()\n     93         except requests.exceptions.HTTPError as e:\n---> 94             raise errors.APIError(e, response, explanation=explanation)\n     95 \n\nipdb> \n```\nAnd now I have to create client again. Therefore, how about if we created new subclass of client with automatic version detection?\n\nIf version=None, use the DEFAULT_DOCKER_API_VERSION constant.\n\nI totally agree with not changing the current default behavior. We certainly don't want to break existing tooling/scripts written on top of docker-py.\n. Rewritten, fixed flake errors.\n. Happy to help. \n. I am using docker-py with latest requests. Just uncommented the < 2.5.0 line and installed it like that.\n. > you could use Docker or virtualenv to isolate your environment\nOr user install:\nhttps://packaging.python.org/en/latest/installing.html#installing-to-the-user-site\n. recent version of docker-py returns generator (or ask for generator directly stream=True) using logs method: logs are returned as they asynchronously arrive from docker daemon:\npython\nfor log in client.logs(container_id, stream=True):\n  process_log_line(log)\n. what version of docker-py and requests are you using?\nI can reproduce your issue with 0.7.1 and requests 2.3.0. But, it works just fine with docker-py 1.0.0 and requests 2.4.3\n. I do have hard times reproducing your issue:\nbash\n$ git clone https://github.com/docker/docker-py\n$ cd docker-py\n$ pip install --user requests==2.4.3\n$ export PYTHONPATH=\"$(pwd)\"\n$ ipython\n``` python\nIn [1]: import docker\nIn [2]: docker?\nType:        module\nString form: \nFile:        /home/ttomecek/git/docker-py/docker/init.py\nDocstring:   \nIn [3]: d = docker.Client()\nIn [4]: c = d.create_container(\"fedora\", command='bash -c \"C=1024 ; while [ ${C} -gt 0 ] ; do echo \\\"$C\\\" ; C=$(( C - 1 )) ; sleep 1 ; done\"')\nIn [5]: d.start(c)\nIn [6]: for l in d.logs(c, stream=True):\n   ...:     print l,\n   ...:   \n1024\n1023\n1022\n1021\n1020\n\n^c\n```\nCan you provide a minimal example please?\n. Now I get it.\ndiff\n-        \"stdin_open\": True,\n-        \"tty\": True,\n+        \"stdin_open\": False,\n+        \"tty\": False,\nAllocating tty and making stdin open only makes sense when you want to attach to container. If you are interested only in logs, don't allocate tty and don't keep stdin open.\n. > Can I modify configs in order to make API respond faster?\nNot sure about that one. May be worth raising this at docker/docker.\nIf you are not interested in all logs, you can get just N last lines with tail argument of logs method.\n. It's not nothing. It's None. I think it's pretty important to state that start returns None. If it's not there, everyone will be curious what start returns (and will think that start actually returns something more meaningful).\n. How about pip install --user docker-py? \n. First thing: this should be definitely reported in ubuntu's, request's and/or pip's issue tracker. It's not docker-py's fault that pip can't handle new version of requests.\nWhat may work is to update pip first, then install docker-py. (wanted to try it out, but it takes ages to install pip from ubuntu's repositories):\nshell\n$ pip install --upgrade pip\n$ pip install docker-py\n. This has worked for me:\n``` shell\n$ apt-get install -y python-pip\n$ pip install -U --user pip\n$ pip install --user docker-py\n$ ~/.local/bin/pip\nUsage: \n  pip  [options]\n...\n``\n. exec syscall doesn't understand shell syntax, therefore all the arguments are passed tols, even|,grepandsys`. Hence the error message:\nls: cannot access |\n...\nls: cannot access grep\nYou need to use shell to interpret it.\nThere is already solution to your problem in your description:\npython\nIn [40]: c.execute(\"288807576b6c\", \"bash -c \\\"ls -l / | grep proc\\\"\")\nOut[40]: 'dr-xr-xr-x. 297 root root    0 Mar 27 03:02 proc\\n'\n. > That's a nice way to get around the issue.\nI'm sorry, I don't follow. What is the precise issue you are having? Is it that execute should be run in a shell? \n. This is really interesting. Looking at spec of the remote API, there is nothing related to truncation: https://docs.docker.com/reference/api/docker_remote_api_v1.17/#list-containers\nI guess it would be worth checking how docker ps works. \n. Have you tried putting it directly to requests? \n. Right. Didn't know that requests has such philosophy. Thanks for sharing the PR.\nGood job on making it your own upstream project!\nI think it would be good idea to use your project (but you gotta wait for more official comment from someone from Docker).\n. If you check the code, none of the methods check image nor container. To me, it would make sense to add such checks. At least for None, as you pointed out.\n. I would be in favor of something like this:\nCan't access docker daemon: 127.0.0.1:12345\nor\nCan't access docker daemon: /var/run/docker.sock\nFor me, endpoint access and TLS mismatch are two different things.\n. If you look into API spec of registry, you can find there several DELETE calls (and use those to remove images). Another way is to go to /var/lib/docker-registry and remove those images manually. Not sure if there are some tools available for this job.\n. Well, since docker-py is a thin docker engine client, I don't see a problem how the methods could be thread unsafe. The only thing from my PoV could be the HTTP transport: if requests/urllib3 connection is thread safe, I assume that docker-py is too.\n. Not sure what you are trying to achieve but I guess that you can try the thread safety within your dev environment. \n. +1\n. Maybe this?\nhttps://github.com/docker/docker/pull/11480\nhttps://github.com/docker/docker/issues/11955\n. @odigity I think that @choleraehyq meant code in docker-py, not engine\n. I did ask the same thing back then: https://github.com/docker/docker-py/issues/449\n. containers()\n. To me it looks like that you install docker-py for python 2 and you are trying to import it in python 3. Or your $PYTHONPATH is misconfigured.\nEDIT: does any of these work?\n$ python2 -c \"import docker\"\n$ python3 -c \"import docker\"\n. @containscafeine good point\nLooks like I have two installations of docker:\npython\nfor d in sys.path:\n    if os.path.exists(os.path.join(d, \"docker\")):\n        print(\"docker is at %r\" % d)\n   ....:         \ndocker is at '/usr/bin'\ndocker is at '/home/user/.local/lib/python2.7/site-packages'\ndocker is at '/usr/lib/python2.7/site-packages'\n. Regarding the integration test, it works fine for me locally:\n```\nplatform linux2 -- Python 2.7.10, pytest-2.8.2, py-1.4.30, pluggy-0.3.1\nplugins: capturelog-0.7\ncollected 8 items \ntests/integration/exec_test.py .\n```\ndocker 1.9.0\nIs this a regression between 1.8 and 1.9?\n. ~~I think that the issue could be with terminal allocation, since test runner probably doesn't allocate tty in kernel.~~\nEdit: ^ that doesn't make any sense, right? tty is allocated in container and exec api call just attaches to container so client doesn't need to allocate tty in test runner afaik\n. After doing some brief testing the issue with integration test seems like race:\nFirst run:\ntests/integration/exec_test.py:141: in test_exec_start_socket\n    self.assertEqual(next_size, len(line))\nE   AssertionError: 0 != 22\nconnectionpool.py          383 DEBUG    \"POST /v1.20/containers/create HTTP/1.1\" 201 90\nconnectionpool.py          383 DEBUG    \"POST /v1.20/containers/0d8a59766345c5724aabd31a4aacb259b85cb2be2f9d21bf88a8277ff7369471/start HTTP/1.1\" 204 0\nconnectionpool.py          383 DEBUG    \"POST /v1.20/containers/0d8a59766345c5724aabd31a4aacb259b85cb2be2f9d21bf88a8277ff7369471/exec HTTP/1.1\" 201 74\nconnectionpool.py          383 DEBUG    \"POST /v1.20/exec/63e7bcc6574a53c01e493079c51103ca6a8697eea14b821a098384b956224c8d/start HTTP/1.1\" 200 None\nconnectionpool.py          383 DEBUG    \"POST /v1.20/containers/0d8a59766345c5724aabd31a4aacb259b85cb2be2f9d21bf88a8277ff7369471/stop?t=1 HTTP/1.1\" 204 0\nconnectionpool.py          383 DEBUG    \"DELETE /v1.20/containers/0d8a59766345c5724aabd31a4aacb259b85cb2be2f9d21bf88a8277ff7369471?link=False&force=False&v=False HTTP/1.1\" 204 0\nThen I reran the exec suite commenting out self.assertEqual(next_size, len(line)) just to see where it gets:\n```\nplatform linux2 -- Python 2.7.10 -- py-1.4.30 -- pytest-2.6.4\nplugins: capturelog\ncollected 7 items \ntests/integration/exec_test.py .....F.\ntests/integration/exec_test.py:149: in test_exec_start_socket\n    self.assertEqual(data.decode('utf-8'), \"{0}\".format(line))\nE   AssertionError: u'yay, interactive exec!\\r\\n' != 'yay, interactive exec!'\n```\nAnd again just a single failing test:\n```\n$ py.test tests/integration/exec_test.py::ExecTest::test_exec_start_socket\nplatform linux2 -- Python 2.7.10 -- py-1.4.30 -- pytest-2.6.4\nplugins: capturelog\ncollected 8 items \ntests/integration/exec_test.py .\n```\nAnd now it works for me locally.\nEDIT: passes now even locally in dind.\nEDIT2: when I run the whole suite I get the error from CI, so it looks like that tests affect each other: does requests cache the connection?\n. Okay, so this is a python 3 only issue.\n. Done!\nIt was a race. Command in container didn't finish before tests tried to fetch results. Simple select([socket], [], []) solves it.\n@shin- PTAL\n. - got rid of the decorator\n- moved import select to top\n. @aanand thanks for the review. As I said previously, this code is copy&pasted from attach container integration test. So I find it pretty funny that you are commenting on a code which is already in.\nI'm assuming that I should make one helper function and use it in both codebases, right?\n. @aanand very good suggestions! code for tests looks a lot cleaner now, please take a look.\n. fixed the flake; rebased\n. > Apologies, I hadn't looked at the codebase in a while so I didn't realise it was copied.\nNo worries.\n\nI'd like to make one more suggestion, assuming it's possible - can the select call be moved to the top of the read_socket function?\n\nDone.\n. \\o/\ndockerpty now\n. I feel like that auto should be the default since this comes up so often. \n. Fair points.\nHow about improving the exception message then? Something like:\nYou have requested daemon API version 1.21 but it supports only 1.20.\nEither supply exact version to Client `Client(version=\"1.20\")` or let\nclient pick it automatically: `Client(version=\"auto\")`\n. It's not that hard to implement such thing yourself. I would actually open such request, providing a client without server overhead, at docker engine tracker.\n. In 1.12, there was a split in packaging. docker binary is a client only since then: https://github.com/docker/docker/pull/20639\n. rebased\n. rebased; ping @dnephin @shin- \n. found this little gem\n@dnephin removed code changes, added new document for 'common pitfalls; starting with the version mismatch issue, PTAL\n. since I'm not native speaker, I usually have hard times picking the right term; I really like FAQ, will change it to that one, thanks for suggestion\n. @dnephin updated\n. CI failure seems unrelated:\ndocker: Error response from daemon: Cannot link to a non running container: /dpy-dind-597 AS /dpy-dind2-597/docker.\ndocker: Error response from daemon: Cannot link to a non running container: /dpy-dind-597 AS /dpy-dind3-597/docker.\n. rebased\n. rebased\n. rebased\n@shin- can we merge this, it already has one lgtm from @dnephin \n. updated, @bfirsh PTAL\n. @shin- @bfirsh rebased\n. rebased\n. \\o/ thank you!\n. Would be nice to add 3.5 support :)\n. Fedora packager here. Unit tests for 1.6 pass just fine for us: https://kojipkgs.fedoraproject.org//packages/python-docker-py/1.6.0/2.fc24/data/logs/noarch/build.log\nAm trying 1.7 now.\nEDIT: 1.7.0rc2 passes too:\nhttps://kojipkgs.fedoraproject.org//packages/python-docker-py/1.7.0rc2/1.fc24/data/logs/noarch/build.log\n. (fedora packager here: what we do is that when dep is not available in required version, we patch docker-py, unpin requirements and pray -- running (integration) test suite may be a good way to figure out if it works)\n. minimal reproducer would be great\n. Looks like that engine did some changes. This is working for me:\n``` python\n!/usr/bin/python3\nimport docker\nd = docker.AutoVersionClient()\nrestore = d.create_container(\n    image='fedora:23',\n    command=[\"python3\", \"-m\", \"http.server\", \"8000\"],\n    tty=True  # this does the trick\n)\nd.start(container=restore)\nfor line in d.logs(container=restore, stream=True):\n    print(line, end=\"\")  # engine likely turns off line buffering and sends every character\n. `AutoVersionClient` uses latest API from engine. Yes, it has exactly the same interface as `Client`, it's just shortcut for `Client(version=\"auto\")`.\n. python\nfor line in d.exec_start(exec_id, stream=True):\n  print(line)\n  logger.debug(line)\n.\n$ pip3 freeze 2>/dev/null | egrep \"(requests=|urllib)\"\nrequests==2.9.1\nurllib3==1.13.1\n```\ndocker-py is git master (c3a66cc5999a5435b81769ac758d411d34c995c4)\n```\nClient:\n Version:      1.10.0\n API version:  1.22\n Go version:   go1.5.3\n Git commit:   590d5108\n Built:        Thu Feb  4 19:55:25 2016\n OS/Arch:      linux/amd64\nServer:\n Version:      1.10.0\n API version:  1.22\n Go version:   go1.5.3\n Git commit:   590d5108\n Built:        Thu Feb  4 19:55:25 2016\n OS/Arch:      linux/amd64\n```\nWhole stack trace\n```\nTraceback (most recent call last):\n  File \"/usr/lib/python3.5/site-packages/requests/packages/urllib3/response.py\", line 226, in _error_catcher\n    yield\n  File \"/usr/lib/python3.5/site-packages/requests/packages/urllib3/response.py\", line 486, in read_chunked\n    self._update_chunk_length()\n  File \"/usr/lib/python3.5/site-packages/requests/packages/urllib3/response.py\", line 432, in _update_chunk_length\n    line = self._fp.fp.readline()\n  File \"/usr/lib64/python3.5/socket.py\", line 575, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.5/site-packages/requests/models.py\", line 660, in generate\n    for chunk in self.raw.stream(chunk_size, decode_content=True):\n  File \"/usr/lib/python3.5/site-packages/requests/packages/urllib3/response.py\", line 340, in stream\n    for line in self.read_chunked(amt, decode_content=decode_content):\n  File \"/usr/lib/python3.5/site-packages/requests/packages/urllib3/response.py\", line 514, in read_chunked\n    self._original_response.close()\n  File \"/usr/lib64/python3.5/contextlib.py\", line 77, in exit\n    self.gen.throw(type, value, traceback)\n  File \"/usr/lib/python3.5/site-packages/requests/packages/urllib3/response.py\", line 231, in _error_catcher\n    raise ReadTimeoutError(self._pool, None, 'Read timed out.')\nrequests.packages.urllib3.exceptions.ReadTimeoutError: UnixHTTPConnectionPool(host='localhost', port=None): Read timed out.\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"./test.py\", line 10, in \n    for line in d.logs(container=restore, stream=True):\n  File \"/home/tt/g/docker-py/docker/client.py\", line 279, in _stream_raw_result\n    for out in response.iter_content(chunk_size=1, decode_unicode=True):\n  File \"/usr/lib/python3.5/site-packages/requests/utils.py\", line 363, in stream_decode_response_unicode\n    for chunk in iterator:\n  File \"/usr/lib/python3.5/site-packages/requests/models.py\", line 667, in generate\n    raise ConnectionError(e)\nrequests.exceptions.ConnectionError: UnixHTTPConnectionPool(host='localhost', port=None): Read timed out.\n```\n. This is about performance, right? (and I guess integrity, one can only have a single piece of code to handle streamed response) \n. > I feel like you shouldn't be initialising a docker.Client at the top level of a module if you need to wait for a daemon to start first?\nThat's exactly how we plan to solve that.\nI was just thinking that this feature could be beneficial for others. If you, as docker-py upstream, would like to keep this behavior, feel free to close.\n. feel free to close if you are not interested in this\n. I can't reproduce this. Does it also happen when doing exec with docker client?\nCan you post here, please, docker inspect foo1 and docker logs foo1; that could tell why the containers exited.\nAlso, looking at version of docker, it doesn't seem like a stable release: Version: 1.10.0-dev. Could you test with either 1.10.2 or 1.9.1?\n. so there won't be a 1.7.3 release for this issue, right?\n. finally the commit looks signed\n. ...which means this is a bug in engine, not docker-py; server should definitely not segfault when client sends a request\n. @xidianwlc seems that you've hit this: https://github.com/docker/docker/pull/20647\n. You need to pass stdin=True to exec_create, then get socket with exec_start(..., socket=True) and feed the data via that socket.\nCode in dockerpty could help you a bit: https://github.com/d11wtq/dockerpty/blob/81361cd90607a45298806db6968b45e75421ff2f/dockerpty/pty.py#L212\n. ~~Yep. docker cp -- client.copy()~~ See comment from @shin- below.\n. @zbyte64 could you please post a complete reproducer and your environment (ideally in a new issue)? it's hard to help with so few information\n. You're correct. dockerpty handles such situation like this: https://github.com/d11wtq/dockerpty/blob/f8d17d893c6758b7cc25825e99f6b02202632a97/dockerpty/io.py#L150\nIt would be very convenient if docker-py provided helper functions for this usecase.\n. docker-py uses python module tarfile to generate tarballs. It's possible there is an inconsistency between the implementations (docker-py, docker engine) - it could be interesting to try gnu tar. \n. Shouldn't comment when I'm tired, sorry about that.\nAfter reading http://www.gnu.org/software/tar/manual/html_node/Standard.html it seems that the additional flags of mode are suid, sgid and sticky bit:\n\nThe mode field provides nine bits specifying file permissions and three bits to specify the Set UID, Set GID, and Save Text (sticky) modes. Values for these bits are defined above. When special permissions are required to create a file with a given mode, and the user restoring files from the archive does not hold such permissions, the mode bit(s) specifying those special permissions are ignored. Modes which are not supported by the operating system restoring files from the archive will be ignored. Unsupported modes should be faked up when creating or updating an archive; e.g., the group permission could be copied from the other permission. \n\nLooks like that engine's API doesn't specify what tar format should be used https://docs.docker.com/engine/reference/api/docker_remote_api_v1.22/#build-image-from-a-dockerfile. After reading tarfile docs, it says that it's using gnu tar by default. How about the golang implementation engine is using - can it do different formats?\n. This seems to be hardcoded as the server API call doesn't take any parameters:\nhttps://docs.docker.com/engine/reference/api/docker_remote_api_v1.22/#get-container-stats-based-on-resource-usage\nhttps://github.com/docker/docker/blob/168b490062aed36bf95426d9d66dcf5703842016/daemon/stats_collector_unix.go#L60\nSince it's one second, it doesn't mean you have to obey, you can write the logic locally, something like:\npython\nwhile True:\n  time.sleep(5.0)\n  stats = d.stats(container, stream=False)\nif that won't work, this definitely should:\npython\nwhile True:\n  time.sleep(5.0)\n  stats = next(d.stats(container, stream=True))\n. That flag is deprecated:\nhttps://github.com/docker/docker-py/blob/81edb398ebf7ce5c7ef14aa0739de5329589aabe/docker/constants.py#L8\nYou need to set --insecure-registry on daemon.\n. Related: https://github.com/docker/docker-py/issues/983\nThat's how the API is designed. Feel free to open an issue on docker/docker if there's an improvement you have in mind. Here's the remote API call:\nhttps://docs.docker.com/engine/reference/api/docker_remote_api_v1.22/#extract-an-archive-of-files-or-folders-to-a-directory-in-a-container\n. dependency updates look like a pretty big change; some of them even jump several minor releases -- it could be very beneficial to run compose's test suite with the newer deps\n. This is awesome! I'm currently struggling with docker-py 1.8 in Fedora world since we don't have py2-ipaddress packaged.\n. This is an interesting proposal. I'm not sure if it fits to a project which is an API client, but can definitely say it would be usable. I personally often need to check engine's API and easiest way to do that is to open a python shell. Though that's definitely slower than running commands in plain shell.\n@shin- @dnephin @aanand do you guys think it would make sense to have such addition in docker-py?\n. full stacktrace, complete reproducer and a version of requests would be very helpful\n. the issue may also be in docker engine, so feel free to try with curl too\n. will fix tests after you confirm that this is the correct approach\n. ping @shin- \n. @shin- No problem. \nSo, it's possible that the issue may be fixed in engine https://bugzilla.redhat.com/show_bug.cgi?id=1329743 I found this in our bugzilla, where @runcom fixed it. \nWill update the patch, Joffrey, thanks for update. \n. @bacongobbler what version of OS and docker you are running? In my case it's Fedora docker (coming from projectatomic/docker) in version 1.9 and 1.11. @runcom suggested this issue is not present in vanilla docker/docker.\n. Given the fact that the issue was in our modified version of docker engine, I'm closing this. (CC @maxamillion) \n. Looks like that the default URL is wrong:\n```\n$ curl -v -XHEAD https://docker.io/v1/\n   Trying 52.4.72.58...\n Connected to docker.io (52.4.72.58) port 443 (#0)\n Initializing NSS with certpath: sql:/etc/pki/nssdb\n   CAfile: /etc/pki/tls/certs/ca-bundle.crt\n  CApath: none\n SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\n Server certificate:\n       subject: CN=.docker.io,OU=Domain Control Validated - RapidSSL(R),OU=See www.rapidssl.com/resources/cps (c)15,OU=GT98568428\n       start date: Mar 19 17:34:32 2015 GMT\n       expire date: Apr 21 01:51:52 2018 GMT\n       common name: .docker.io\n*       issuer: CN=RapidSSL SHA256 CA - G3,O=GeoTrust Inc.,C=US\n\nHEAD /v1/ HTTP/1.1\nUser-Agent: curl/7.40.0\nHost: docker.io\nAccept: /\n< HTTP/1.1 301 Moved Permanently\n< Content-length: 0\n< Location: https://www.docker.com/v1/\n< Connection: close\n<\n* Closing connection 0\n```\n\nEven docker info says index:\n$ docker info\n...\nRegistry: https://index.docker.io/v1/\n. works quite okay for me:\n``` python\nIn [1]: d = docker.AutoVersionClient()\nIn [3]: d.tag(\"fedora\", \"fedora-test\")\nOut[3]: True\nIn [4]: x = d.inspect_image(\"fedora-test\")\nIn [5]:\n```\nwhat versions you are using? this may be a regression in engine, I'm on these:\npython\nIn [5]: docker.version\nOut[5]: '1.7.2'\n``` bash\n$ docker version\nClient:\n Version:         1.9.1\n API version:     1.21\n Package version: docker-1.9.1-5.git64eb95e.fc22.x86_64\n Go version:      go1.5.3\n Git commit:      64eb95e/1.9.1\n Built:\n OS/Arch:         linux/amd64\nServer:\n Version:         1.9.1\n API version:     1.21\n Package version: docker-1.9.1-5.git64eb95e.fc22.x86_64\n Go version:      go1.5.3\n Git commit:      64eb95e/1.9.1\n Built:\n OS/Arch:         linux/amd64\n``\n. yup, repository is suppose to be without tag\n. This could be a very old version ofurllib3`. Share the version please:\n$ pip freeze | grep -e urllib3 -e requests\nIt could also be interesting to see if urllib3 (and requests) comes from distribution or from PyPI.\n. I agree that having the OO abstraction in docker-py would be great since that's what likely everyone is doing in their python projects using docker-py. (e.g. this is what I have in sen: common base class and object specific classes for container and image -- hence I would love to contribute some of the code back to this new API)\nBut please, make it backwards compatible. There's so many tools built on top of docker-py already. So I don't see why you would have to ~~remove~~ deprecate the already-used, stable and efficient low-level API.\n. docker-py doesn't have anything to do with this -- whole response is generated in docker engine, you should report at docker/docker.\nI have also experienced pretty inconsistent data in networking part, e.g. when you disable network namespace with --net=host, there's nothing in there.\n. @vklonghml please open a new issue\nimage size is definitely relevant; in order to save an image 700 MB big, the bandwidth needed for such scenario is 700 MB / 60 seconds = 11.6 MB/s -- on a system with high load, this may not be as simple as you think\n. build call supports fileobj so you can easily prepare the context yourself.\n. > But driver_opts is not working. What kinds of driver_opts can we use?\nThose should be available here: https://docs.docker.com/engine/admin/logging/overview/ \n. /facepalm sorry about the noise, I should read more carefully.\n. jenkins build seems to be broken or something\n. done (also fixed a test)\n. rebased\n. rebased\n. can we merge now since this has 2 lgtms already? rebasing is not the most fun stuff in the world\n. thanks Daniel\n. It really depends what you're trying to do as different solutions solve different issues.\nThis one is safest:\nclient = Client(version='1.20')\nThis one is latest & greatest (at the cost of that you may change code of your app on upgrades)\nclient = Client(version='auto')\n. Could it be an authentication issue? (similar as https://github.com/docker/docker-py/issues/1116)\nI can't really help without these:\na) debug logs of docker engine (run it with -l debug)\nb) error message from ECR (ideally the complete HTTP requests)\n. 80 > 79 is my favorite; tests should pass now, hopefully\n. rebased\n. Thanks @shin-\nRebased.\n. This is not an issue of docker-py, instead it's an issue of either python package management or distribution package management. tl;dr is that you shouldn't mix those two ecosystems.\nIf a python package requires package X in version Y, and X-Y breaks other packages, that's not a fault of the former python package. It doesn't know what versions are available on the system, nor does it know what packages in which versions other packages require. That's the responsibility of package manager.\ndocker-py maintainers identified range of versions in which their package is working. Your version is not in that range.\nSince the version of requests you have is pretty outdated. I would suggest installing docker-py in a new clean virtualenv or in a docker container (since newest requests break your host system).\n. There was a similar bugreport submitted recently with pretty much same symptoms: https://github.com/docker/docker-py/issues/1121\nThe solution was to refresh credentials, exactly the same thing as the error message is suggesting:\nYour Authorization Token has expired. Please run 'aws ecr get-login' to fetch a new one.\n. Please post complete HTTP communication, it's pretty much impossible to help with currently provided info.\n. Just a question about https://github.com/docker/docker-py/pull/1181: is https://github.com/shin-/dockerpy-creds going to be a standalone project and a new permanent dependency of docker-py? (linux distro packaging)\n. Looks like this may be related to https://github.com/docker/docker-py/issues/1195.\n. This is very interesting. We had an almost identical bug report recently in our bug tracker:\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1374265\nWhat I was able to figure out is that engine is likely blocked on getting stats from cgroups (for some reason) and hence it doesn't respond with anything).\nWould it be possible to send a http request via ncat to see what the response will be?\n```\n$ ncat -U /var/run/docker.sock\nGET /v1.22/containers/580dc1eaae69/stats?stream=0 HTTP/1.1\nHost: 127.0.0.1  # press  twice and wait for response\nHTTP/1.1 200 OK\nContent-Type: application/json\nServer: Docker/1.10.3 (linux)\nDate: Fri, 09 Sep 2016 09:57:20 GMT\nTransfer-Encoding: chunked\n1127\n{\"read\":\"2016-09-09T09:57:20.896712812Z\",\"precpu_stats\":{\"cpu_usage\":{\"total_usage\":981229\n```\nIn case of no response, we should likely reassign to docker/docker.\n. >  If I run a container from the command line I can get the stats in parallel no problem.\nThat's an interesting twist. So maybe some container configuration you specify messes up with cgroups so docker is not able to get stats. Let me see...\n. Reproduced!\nAnd even found the thing which breaks stats. When I removed 'network_disabled' : True, I was able to get stats once again.\n. @michaelbarton that's response to OP's first comment; when I took his container configuration I was hitting the issue; as soon as I removed the network_disabled entry; it started to work\n. So this is actually fixed in upstream: https://github.com/docker/docker/pull/25905\nThe problem is that container has disabled networking (SandboxID is set to \"\") and stat collector fails to pick networking stats (obviously) and won't publish any stats to channel, meaning server will never respond with any stats.\nTo workaround this issue: enable networking.\nTo fix this issue: wait for upstream to release a new version of docker (> 1.12.1).\n. Do you connect from a single thread (=using the same connection pool), or is the script being used from multiple processes?\n. And can you also post how you are running docker engine? Isn't it hidden behind a proxy by any chance?\n. I'm sorry but that code is not helpful at all:\n1. it's full of unrelated information\n2. it's actually completely unrelated\nYour exception in your original post states that connection to /build endpoint failed while code provided by you doesn't mention build in any way.\nPlease provide a minimal reproducer which we can copy and run on our machines.\n. > So it is no docker-py problem. Shall I open a ticket on the docker project site?\nDefinitely.\nI'm still really curious how is this even possible: with docker client you can build in parallel without any issues, so it may be possible that docker-py plays some role in this issue.\nThe way I would debug this: build a job that tries to access other URLs (e.g. /version); alternatively configure docker to listen on UNIX socket, not on a port (could this be the issue?); and finally: try parallel connection without jenkins (with curl, with docker-py). All these could narrow down the possibilities to the real cause.\n. Glad you solved it.\n. > docker for mac 1.12.0-rc3\nCould try with latest stable release?\n. @_@\nThanks for clarification. We live in a crazy world.\nSo I put a comment on top of the requirement in setup.py to clarify instead.\n. I like your rewrite. Updated.\n. rebased\n. \\o/\n. This was merged upstream 3 weeks ago: https://github.com/docker/docker/pull/22049 And I believe it will be experimental. So... Too soon.\n. The error message is response from docker daemon, not from the registry. If you start your docker daemon with --insecure-registry <hostname.with.port.to.the.registry> it should fix the issue (since apparently it's running on HTTP not on HTTPS).\n. Could you please curl -v https://<hostname:port>/v2/ the ECR endpoint?\nThis is what I get when using distribution:\n$ docker run --net=host registry:2\n...\n::1 - - [06/Oct/2016:06:57:59 +0000] \"GET /v2/ HTTP/1.1\" 200 2 \"\" \"curl/7.50.3\"\n```\n$ curl -v http://localhost:5000/v2/\n   Trying ::1...\n TCP_NODELAY set\n* Connected to localhost (::1) port 5000 (#0)\n\nGET /v2/ HTTP/1.1\nHost: localhost:5000\nUser-Agent: curl/7.50.3\nAccept: /\n< HTTP/1.1 200 OK\n< Content-Length: 2\n< Content-Type: application/json; charset=utf-8\n< Docker-Distribution-Api-Version: registry/2.0\n< X-Content-Type-Options: nosniff\n< Date: Thu, 06 Oct 2016 06:57:59 GMT\n<\n Curl_http_done: called premature == 0\n Connection #0 to host localhost left intact\n{}%\n```\n\nA little followup to @shin-'s request: providing the code which is failing would be much appreciated. \n. docker-py 1.2.3 is super ancient; please upgrade to latest version\n$ pip install -U docker-py\n. Why not let compose update to newer requests?\n. Could also please update documentation? https://github.com/rmb938/docker-py/blob/ac20dae89be25bdf9840640bebdf24e99e05a085/docs/api.md\n. @rmb938 :+1: \n. Needs rebase.\n. start, in its current form, doesn't accept host config, you should set it when creating the container. Please update your code.\n``` python\nIn [1]: import docker\nIn [2]: d = docker.AutoVersionClient()\nIn [3]: h = d.create_host_config(read_only=True)\nIn [4]: h\nOut[4]: {'NetworkMode': 'default', 'ReadonlyRootfs': True}\nIn [5]: c = d.create_container(\"fedora\", command=\"ls\", host_config=h)\nIn [6]: d.start(c)\nIn [7]: d.logs(c)\nOut[7]: b'bin\\nboot\\ndev\\netc\\nhome\\nlib\\nlib64...'\n```\n. This could be a race condition in engine.\nCould you please reproduce with docker client so we can be sure where the issue is?\n. good catch; fix incoming\n. I feel like there is a race: I always forget to sign my commits, so I force push signed commit a couple seconds after the PR is created -- I think github/gordon doesn't catch this. Anyway, will update so it's all green.\n. rebased\n. rebased. Can we have this merged? @shin- @bfirsh . rebased\n. This is how I'm doing the calculation in sen: https://github.com/TomasTomecek/sen/blob/master/sen/util.py#L158 (which is the exact copy of how docker client does it)\n. So... No backwards compatibility? I have also noticed that AutoVersionClient is gone.. Okay then.\n\nIn Fedora lands, we will likely keep both versions for some time (to give everyone time to migrate their code bases).. This is a very similar answer I provided on Red Hat bugzilla.\nUnfortunately ssl module in python 2.7.{9,10} is not on par with ssl in 3.5: it doesn't support serverAltName matching:\n\nhttps://hg.python.org/cpython/file/2.7/Lib/ssl.py#l249\nhttps://hg.python.org/cpython/file/3.5/Lib/ssl.py#l263\ncherry-picked code in backports.ssl_match_hostname\n\nMore info: https://bugzilla.redhat.com/show_bug.cgi?id=1229409#c9\nSo we can't do this yet :/. So the project will be renamed, is that correct? (I'm assuming we will need to do a rename in downstream distributions). > Am I crazy in thinking\nNo, you're not. But you have to bind-mount docker socket (if that's how you access docker daemon) inside the container. Then you're fine.\nThe error here means that /run/docker.sock doesn't exist.. Dockerfiles are a build time thing: no mounts are preserved in runtime.\nYou need to create and start the container with the mounts. Please see the API.. I'm using threading.Event for that: https://github.com/TomasTomecek/sen/blob/24dcbb63ca1dd9af2247f0cd25074dc41b27fc52/sen/tui/views/main.py#L63 https://github.com/TomasTomecek/sen/blob/24dcbb63ca1dd9af2247f0cd25074dc41b27fc52/sen/tui/views/main.py#L44. Would be nice if this was part of docker-py -- ability to shut down the generator.\nI agree that you have to wait for next event to pop up. I don't mind that in my application and certainly understand when this is not acceptable in your application.. Can you please post a minimal reproducer which we would run on our laptops? Also please post what is the content of the container.. Can you also reproduce using docker client?. Did you try to tinker switches? I would definitely start with:\nlogs = container.logs(stream=True, follow=True, stdout=True, stderr=True, tail='all')\nIt also depends what kind of application you are running in the container.\nAlso bear in mind that \n$ docker logs -f <container>\nis not an equivalent of\nlogs = container.logs(stream=True, follow=True, stdout=True, stderr=False, tail=0). @danqing since my comment is 6 months old, I forgot everything; I would need to check sources to answer, so please see the sources for more info. @cguethle I haven't investigated further.. Right now, it's impossible to do such thing in requirements.txt.\nYou could potentially pin to docker in requirements.txt, while in setup.py, you could check what version of docker-py is already installed -- and if there is none, pin to latest.\nAnyone has a better idea?. > whereas the version in Python is\n\nIn [3]: docker.version\nOut[3]: '1.10.3'\n\nThis is the version of library, not docker engine.\nWhen you import docker in IPython, can you print the path of imported package?\nIn [3]: docker.__file__\nOut[3]: '/usr/lib/python3.6/site-packages/docker/__init__.py'\nCan you also import ssl_match_hostname in IPython?\n```\nIn [4]: from backports.ssl_match_hostname import match_hostname\nIn [5]:\n```. I personally don't understand why this code should be part of docker-py, can you enlighten me? What's wrong with having it inside separate project?. I think this PR raises a more generic issue: what is the preferred way of dealing with plugins which are not inside upstream dockerd and need support in docker-compose. This PR suggests that it is to add the needed code which supports these plugins inside official python API client and docker-compose.\nMy worry is that if every vendor with their own dockerd plugin adds support to docker-py and docker-compose, we'll end up with code which will be hard to maintain & understand, potentially getting new dependencies which are only needed by users of the feature and finally complicating the API & maybe even cryptic bugs. This is the reason why I still think that either:\n\nthe code should part of dedicated project\ndocker-compose should have plugin system on its own\n\nBut hey, I'm no maintainer of this project, I'm just a contributor with an opinion on my own.. > @TomasTomecek any ideas for a plugin architecture that would be welcomed in the docker-py/compose projects?\n@andyneff As I said, I'm not maintainer of these projects. It's up to maintainers to figure out if and how they want to merge this code (it's been a month since last comment from a maintainer). I just expressed my personal concerns with the currently proposed PR.\nFiguring out plugin architecture for these projects is out of scope of my free time. Sorry.. Test failure means that the docker engine is not new enough. Shall I skip it?\nEdit: added @requires_api_version('1.25'). Janky is stuck :/. Thanks @shin-, now I understand it. Should be fixed.. @shin- is the overlord to ping in this repo. You should dig deeper:\nhttps://docker-py.readthedocs.io/en/stable/api.html#docker.api.container.ContainerApiMixin.inspect_container\nhttps://docker-py.readthedocs.io/en/stable/api.html#docker.api.image.ImageApiMixin.inspect_image. what do your containers look like?. What I usually ended up doing is that I copied exactly what docker cli is doing:\nhttps://github.com/TomasTomecek/sen/blob/67794e176e70fa77d01e2acae381b92e501c0e17/sen/util.py#L162\nhttps://github.com/TomasTomecek/sen/blob/67794e176e70fa77d01e2acae381b92e501c0e17/sen/docker_backend.py#L664\n. I'm not a maintainer of this project, my comments are pretty much just my opinions. For a proper code review you need to wait for @shin- . Would it make sense to cut 3.0.2 soon-ish?. From the gist of your code, it seems that you are actually using this method:\nhttps://github.com/docker/docker-py/blob/ba0e5332de2fb032b5b21e8e2835244152927c0f/docker/api/image.py#L308. Something similar happened to me (except that docker logs showed nothing either). I concluded it's a bug in journald logging driver and switched to json-file. https://bugzilla.redhat.com/show_bug.cgi?id=1535113. docker-py: 1.10.6 (available in $PYTHONPATH)\nThat is a really old version.\ndiff\n-# pip install docker-py\n+# pip install docker\nThe project was renamed on PyPI.. I would suggest contacting Azure support. As @shin- pointed out, your issue has nothing to do with this project since dockerd returns 500 as you can see in the log. Ideally, run dockerd with debug logs and check what's wrong.. You can also browse stackoverflow.com for similar questions. (Such as how to install pypi packages on windows machines.). Exit code should be in this metadata:\ndef inspect(self):\n    return self.client.api.exec_inspect(self.id). I wonder whether  function get_installed_distributions() was ever an official public API of pip.. AFAIK there is no way of doing that.\nHow about using the low level exec_create & exec_start API?\n```\nIn [1]: import docker\nIn [2]: d = docker.APIClient()\nIn [3]: c = d.create_container(\"fedora:28\", command=[\"sleep\", \"infinity\"])\nIn [4]: d.start(c)\nIn [5]: e1 = d.exec_create(c, [\"ls\", \"/\"])\nIn [6]: d.exec_start(e1)\nOut[6]: b'bin\\nboot\\ndev\\netc\\nhome\\nlib\\nlib64\\nlost+found\\nmedia\\nmnt\\nopt\\nproc\\nroot\\nrun\\nsbin\\nsrv\\nsys\\ntmp\\nusr\\nvar\\n'\nIn [7]: e2 = d.exec_create(c, [\"ls\", \"/root\"])\nIn [8]: print(d.exec_start(e2))\nb'anaconda-ks.cfg\\nanaconda-post.log\\noriginal-ks.cfg\\n'\n```\n. Why not? Do you have an example?. Does this happen on a fresh installation? Couldn't it be caused by an upgrade?. I suggest opening an issue in arch linux issue tracker then.. github.com/moby/moby\nWhat logging driver are you using? Would it be possible to analyze the traffic (tcpdump, strace) between your client code and the unix socket /tmp/docker_core1.sock when the improper behavior is occurring?\nThere was actually a similar bug in docker-py-1 when the response from docker daemon was processed incorrectly. It may be possible this is a regression.. Please provide more info: actual output, expected output, environment (operating system, where & how you installed docker).\nBased on your code, it is super-weird that you keep overriding the output variable. Shouldn't you raise at some point?. You provided serialized output from ansiple-playbook run: how is that related to docker-py?\nI would expect to get an output from the source code you showed us.. I'm sorry but I still have no idea what's wrong or what you are trying to do.. But docker-py should take care of that: https://github.com/docker/docker-py/blob/22b7b76142bd735c6be4f678dda8cf9d413e9f1c/docker/api/exec_api.py#L165\nhttps://github.com/docker/docker-py/blob/9b8e022fa1012cae381bb97823e30936877c57c3/docker/api/client.py#L334\nI would say it's a bug when such leftovers are left in place.. This is a cached property: cont.attrs[\"Name\"], you should reload the metadata..     I'm using SDK version 1.24\nThere is no such version. 1.10 is the last minor release of version 1.. Can you also reproduce with the latest PyPI release?. I suggest reporting at \"Docker for Mac\" issue tracker.. Does increasing the timeout value solve the problem? https://docker-py.readthedocs.io/en/stable/client.html#creating-a-client. The error message says it. It's 60. If your image is big, 600 could be a good candidate.. Did you try to increase the timeout? https://docker-py.readthedocs.io/en/stable/client.html#creating-a-client. https://pypi.org/project/docker/. This looks like a question for gitlab-runner project.\nNo such file or directory in this case usually means that the docker.sock is not present. So this indeed is probably a bug in gitlab-runner or a misconfiguration.. This is not a bug, to be honest. There were a ton of bugs posted about this specifically that it might be time to revisit the design decision.\nhttps://docker-py.readthedocs.io/en/stable/client.html?highlight=timeout#module-docker.client\nThe default timeout on the http connection is set to 60 seconds -- if the operation have not finished in those 60 seconds, you get the timeout error. So the workaround is actually a solution -- set it to some ridiculous number (e.g. 999999) and you won't be bothered.. This comes up so often that we could probably increase the default value: https://github.com/docker/docker-py/issues/2077. The issue is, that your database is not ready when you start executing the commands.\nYou should wait for it to become initialized, then start running commands.. We are doing this all the time and it works just fine.\nSince you haven't posted any details it's really hard to help you. What is your container environment? What user do you invoke the container as? What are versions of the software you are using?\nProof:\n```\n$ docker run -ti -v /var/run/docker.sock:/var/run/docker.sock fedora:28 bash\n[root@9028cd12db44 /]# pip3 install docker\n...\nSuccessfully installed certifi-2018.4.16 chardet-3.0.4 docker-3.4.1 docker-pycreds-0.3.0 idna-2.7 requests-2.19.1 urllib3-1.23 websocket-client-0.48.0\n[root@9028cd12db44 /]# python3\nPython 3.6.5 (default, Mar 29 2018, 18:20:46)\n[GCC 8.0.1 20180317 (Red Hat 8.0.1-0.19)] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport docker\nd = docker.APIClient()\nc = d.create_container(\"fedora:28\", \"ls\")\nc.start()\nTraceback (most recent call last):\n  File \"\", line 1, in \nAttributeError: 'dict' object has no attribute 'start'\nd.start(c)\nd.logs(c)\nb'bin\\nboot\\ndev\\netc\\nhome\\nlib\\nlib64\\nlost+found\\nmedia\\nmnt\\nopt\\nproc\\nroot\\nrun\\nsbin\\nsrv\\nsys\\ntmp\\nusr\\nvar\\n'\n``\n. Since you are getting \"permission denied\" it's clear that you are not allowed to write to the docker.sock from the container.\nWhat'sls -lha /var/run/docker.sockandid` in your container?\n\n\n\nEdit: mine is:\n[root@bffafe6c802d /]# id\nuid=0(root) gid=0(root) groups=0(root)\n[root@bffafe6c802d /]# ls -lha /var/run/docker.sock \nsrw-rw----. 1 root 1001 0 Jul 30 07:57 /var/run/docker.sock. Why not include the dockerfile in the context?. https://github.com/docker/docker-py/issues/2076#issuecomment-404757752. So try even bigger number then: 60000. Are you doing it over network or locally?. hm, so it seems that docker-py does the I/O inefficiently and this may indeed be a bug. @shin- That's actually pretty good point. It may be possible that @wekay102200's problem is that the machine starts to swap.. Users can do the same thing: write a proxy which will translate the user-agents to the expected one. #just-sayin. stream=True does the trick\nhttps://docker-py.readthedocs.io/en/stable/containers.html#docker.models.containers.Container.exec_run. Yes, you need to have permissions to access the docker.sock unix socket. Either be root or add your user to docker group.\n$ ll /var/run/docker.sock                                                                                                                                                                               \nsrw-rw---- 1 root docker 0 Oct 29 17:27 /var/run/docker.sock. https://docker-py.readthedocs.io/en/stable/api.html#docker.api.daemon.DaemonApiMixin.events. why not have that logic in your code?. I don't understand what's so unconventient about this:\npython\nfor im in image_list:\n  client.pull(im, tag=\"latest\")\nThe problem with adding such code to the library is... how do you handle errors or even parallelism? Such implementation could become pretty complex, that's why I think it's better to have the logic in an application, not in the library.. would it make sense to use compare_version here? so when 1.6 lands, one does not need to change the test again.\n. I think it could make sense to mention OS/distribution and whether docker-py comes from PyPI. \n. Makes sense to me. To be honest, I actually vote for socket=False since there is already an option for asking for stream. On top of it, you'll get this feature for free in future (for new similar API calls).\n. I copy-pasted from attach container test. Will fix.\n. @shin- waiting for your reply before I start rewriting. \n. Hm, since _get_raw_response_socket is used only in attach_socket, it doesn't make sense to use it anywhere else now. Not sure if it could be useable in future. Let me know if I should merge the functionality into exec_start directly or leave it as it is now.\n. typo\n. That's a pretty bold statement. Have you asked those 95% of users?\n. I was worried how 'Env': None gets serialized to json, API spec said it should be array.\nI trust you \ud83c\udf78 . Will this be a hard requirement if this PR gets merged?. shouldn't there be an assert here?. TypeError or ValueError would be better. NotImplementedError\nIn user defined base classes, abstract methods should raise this exception when they require \nderived classes to override the method, or while the class is being developed to indicate that \nthe real implementation still needs to be added.\n\n. makes sense. the description doesn't seem right. please update the comment, seems to be cut. ",
    "HaZiel9": "Hi! I've the same error : \n1: \nMy Post\ndef dni(request):\nurl = \"http://127.0.0.1:9200/api/check-api/\"\npayload = \"------WebKitFormBoundary7MA4YWxkTrZu0gW\\r\\nContent-Disposition: form-data; name=\\\"email\\\"\\r\\n\\r\\nadmin@admin.com\\r\\n------WebKitFormBoundary7MA4YWxkTrZu0gW\\r\\nContent-Disposition: form-data; name=\\\"password\\\"\\r\\n\\r\\n1234\\r\\n------WebKitFormBoundary7MA4YWxkTrZu0gW\\r\\nContent-Disposition: form-data; name=\\\"file\\\"\\r\\n\\r\\n{\\n\\\"numeroDossier\\\": \\\"GU-CE-GN00000204\\\",\\n\\\"numeroIdentificationFiscale\\\": \\\"031246\\\",\\n\\\"dateImmatriculation\\\":\\\"2018-04-06\\\",\\n\\\"numeroFormulaire\\\":\\\"0124/MB/DNI/CI/2018\\\",\\n\\\"nomFichierEnvoye\\\":\\\"nif_GU-CE-GN00000204.pdf\\\",\\n\\\"nomPrepose\\\":\\\"KEITA\\\",\\nprenomPrepose\\\":\\\"SALIF\\\"\\n}\\r\\n------WebKitFormBoundary7MA4YWxkTrZu0gW--\"\n\nplayload = {\n    \"Content-Disposition\": \"form-data\",  \n    \"email\": \"admin@admin.com\",\n    \"password\": password,\n    \"file\": {\n        \"numeroDossier\": \"GU-CE-GN00000204\",\n        \"numeroIdentificationFiscale\": \"031246\",\n        \"dateImmatriculation\":\"2018-04-06\",\n        \"numeroFormulaire\":\"0124/MB/DNI/CI/2018\",\n        \"nomFichierEnvoye\":\"nif_GU-CE-GN00000204.pdf\",\n        \"nomPrepose\":\"KEITA\",\n        \"prenomPrepose\":\"SALIF\"\n    }\n}\nheaders = {\n    'content-type': \"multipart/form-data\",\n    'Content-Type': \"application/x-www-form-urlencoded\",\n    'Cache-Control': \"no-cache\"\n}\nif request.method == 'POST':\n    response = requests.request(\"POST\", url, data=payload, headers=headers)\n    messages.success(request, _('Json envoy\u00e9 avec succes!'))\nreturn render(request, 'example.html')\n\n2 :\nerror\ndjango_1    |     raise ConnectionError(e, request=request)\ndjango_1    | requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=9200): Max retries exceeded with url: /api/check-api/ (Caused by NewConnectionError(': Failed to establish a new connection: [Errno 111] Connection refused',))\ndjango_1    | [30/May/2018 09:42:22] \"POST /contribuable/example-api HTTP/1.1\" 500 191335\nCould anyone HELP ME please!. ",
    "GaryLBaird": "Did you ever resolve this? I have the same problem and I am fairly new to Docker as well. I'm running ubuntu on hyper-v with docker and the ExposeVirtualizationExtensions set to true. And it seems to work for the most part. \nPowershell: Hyper-V Configuration command for Exposing Virtualization to the VM.\n\"Set-VMProcessor \"Ubuntu 18.04.1 LTS\" -ExposeVirtualizationExtensions $true\"\nTo me, this looks like an issue with port or routing issue. When I run curl I get Connection refused on the port.\ncurl -v 127.0.0.1:2376\n Rebuilt URL to: 127.0.0.1:2376/\n   Trying 127.0.0.1...\n TCP_NODELAY set\n connect to 127.0.0.1 port 2376 failed: Connection refused\n Failed to connect to 127.0.0.1 port 2376: Connection refused\n Closing connection 0\ncurl: (7) Failed to connect to 127.0.0.1 port 2376: Connection refused. ",
    "yograterol": "Done! :+1:  @shin- \n. I need this feature :D @hammerdr :+1: \n. ",
    "unclejack": "These are the two possible options and what each means:\nro: \"read only\"\nrw: \"read write\"\n. ",
    "lots0logs": "I have the same issue with Ubuntu 14.04.1, Kernel: 3.13.0-35\nI have been using subprocess.Popen to get the logs using the command line client: docker logs --follow which works perfectly \n. @shin- Please let me know if this is good. Once it is merged I will be able to submit a PR for #840 \nCheers!\n. @shin- \n. @shin- Who can I tag to get the second LGTM?\n. @shin- @dnephin \n. @dnephin @aanand @shin- :wink:\n. ",
    "michaelpj": ":+1:\nI have this problem with any of the commands that can return streaming output, not just logs - for example pull.\n. ",
    "kevinastone": ":thumbsup: \nunlike above, I'm able to stream push and pull just fine.\n. The problem is that _result tries to consumes the entire response:\nhttps://github.com/docker/docker-py/blob/master/docker/client.py#L91-L98\nit should use raw instead of content and consume the stream iteratively.\n. Here's my workaround.  (Sorry, it might be mangled a bit, my actual implementation used wrapper classes).\nThe actual fix should use _multiplexed_socket_stream_helper, but it wasn't consuming the raw response properly in my testing. \n```\ndef logs(self, container, stdout=True, stderr=True, stream=False, timestamps=False):\n    \"\"\"\n    Workaround for a blocking implementation in dockerpy\n    \"\"\"\n    params = {\n        'stderr': stderr and 1 or 0,\n        'stdout': stdout and 1 or 0,\n        'timestamps': timestamps and 1 or 0,\n        'follow': stream and 1 or 0\n    }\n    url = self._url(\"/containers/{container}/logs\".format(container=container))\n    res = self._get(url, params=params, stream=True)\n    res.raise_for_status()\n# result = self._client._multiplexed_socket_stream_helper(res)\nsock = self._get_raw_response_socket(res)\nsock.settimeout(None)\n\ndef stream_response():\n    while True:\n        header = res.raw.read(STREAM_HEADER_SIZE_BYTES)\n        if not header:\n            break\n        _, length = struct.unpack_from('>BxxxL', header)\n        data = res.raw.read(length)\n        if not data:\n            break\n        yield data\n\nresponse = stream_response()\nif stream:\n    return response\nelse:\n    return ''.join(iter(response))\n\n```\n. Great!\nJust to clarify, I needed v0.5 (was on v0.4) and you actually don't include the scheme, else you get:\ndocker.errors.InvalidRepository: Repository name cannot contain a scheme\n(I also needed insecure_registry=True for my private registry)\n. Should also be vanilla (docker over unix socket).  Here's a simple failing test case:\n```\nfrom docker.client import Client as DockerClient\nclient = DockerClient(base_url='http+unix://var/run/docker.sock')\nclient.ping()\n```\nResults in the following stacktrace:\n```\nAPIError                                  Traceback (most recent call last)\n in ()\n      3 # client = DockerClient(base_url='http+unix://var/run/docker.sock')\n      4 client = DockerClient()\n----> 5 client.ping()\n/usr/local/lib/python2.7/dist-packages/docker/client.pyc in ping(self)\n    790 \n    791     def ping(self):\n--> 792         return self._result(self._get(self._url('/_ping')))\n    793 \n    794     def port(self, container, private_port):\n/usr/local/lib/python2.7/dist-packages/docker/client.pyc in _result(self, response, json, binary)\n     96     def _result(self, response, json=False, binary=False):\n     97         assert not (json and binary)\n---> 98         self._raise_for_status(response)\n     99 \n    100         if json:\n/usr/local/lib/python2.7/dist-packages/docker/client.pyc in _raise_for_status(self, response, explanation)\n     92             response.raise_for_status()\n     93         except requests.exceptions.HTTPError as e:\n---> 94             raise errors.APIError(e, response, explanation=explanation)\n     95 \n     96     def _result(self, response, json=False, binary=False):\nAPIError: 404 Client Error: Not Found (\"client and server don't have same version (client : 1.16, server: 1.15)\")\n```\nMy Environment\n$ docker --version\nDocker version 1.3.2, build 39fa2fa\n$ pip freeze | grep docker-py\ndocker-py==0.7.0\n. Also confirms the same failure without a base_url (client = DockerClient())\n. Oh, now I see, why does the client and server version vary?  Does docker-py==0.7 require docker-1.4?\n. That makes sense.  I was hoping it used /version to determine API to talk.\n. If it helps anyone else, I wrote a quick handshake to find the right version of the API to talk:\n```\nfrom functools import cmp_to_key\nfrom docker.client import Client as DockerClient, DEFAULT_DOCKER_API_VERSION as MAX_CLIENT_DOCKER_API_VERSION\nfrom docker.utils import compare_version\nMINIMUM_API_VERSION = '1.14'\ndef get_api_version(*versions):\n    # compare_version is backwards\n    def cmp(a, b):\n        return -1 * compare_version(a, b)\n    return min(versions, key=cmp_to_key(cmp))\nversion_client = DockerClient(version=MINIMUM_API_VERSION)\nversion = get_api_version(MAX_CLIENT_DOCKER_API_VERSION, version_client.version()['ApiVersion'])\nclient = DockerClient(version=version)\n```\n. Yeah, you need to disable the timeout on the underlying socket.  I have this in my version of the code:\nsock = stream._fp.fp._sock\n        sock.settimeout(None)\n        self.stream = stream\n(I believe stream is response.raw)\n. ",
    "ilovenwd": "I have the same issue with Ubuntu 14.04.1, docker 1.3.1\n. by the way, the work round from here works:\nhttps://github.com/docker/docker-py/issues/300\n. ",
    "bo0ts": "Just experienced this issue with python 2.7.10 and docker-py 1.2.3.\nIs this still related to this bug?\n. ",
    "raimo": "Experiencing with docker-py 1.6.0 as well\n. ",
    "abevoelker": ":+1: This would definitely be really useful.\n. ",
    "dlemphers": "This is great! Cherry picked this until it gets rolled in, as there isn't any other way to excite the new restart feature remotely via the API. Thanks @shreyu86!! :+1: \n. ",
    "linlinlinlin": "OK, Thanks. \nSo, this is not an issue? Sorry! Because in document and merge log already introduced this feature.\nHope we can see next release soon.\n. Image: ubuntu:latest\nCreate container command:  docker run -it ubuntu bash\n. LGTM\n. Another question, whatever I set  dockercfg_path (str) or not.\nI didn't see any configure file at $HOME/.dockercfg or any path I set.\n. @shin Thanks for your information. \nI might misunderstand the description of API doc. It just use it but not save it, right? i.e. at /home/user/.docker/config.json\ndockercfg_path (str): Use a custom path for the .dockercfg file (default $HOME/.dockercfg)\nSo, if I restart my client process, I have to re-login again then I can do push image.\n. docker-py 1.8.0, docker 1.10.2\n``` python\nIn [1]: import docker\nIn [2]: c = docker.Client()\nIn [3]: c.login('joffrey', reauth=True)\nAPIError                                  Traceback (most recent call last)\n in ()\n----> 1 c.login('joffrey', reauth=True)\n/home/vagrant/.local/share/virtualenvs/aaa/lib/python2.7/site-packages/docker/api/daemon.pyc in login(self, username, password, email, registry, reauth, insecure_registry, dockercfg_path)\n     69         if response.status_code == 200:\n     70             self._auth_configs[registry] = req_data\n---> 71         return self._result(response, json=True)\n     72 \n     73     def ping(self):\n/home/vagrant/.local/share/virtualenvs/aaa/lib/python2.7/site-packages/docker/client.pyc in _result(self, response, json, binary)\n    156     def _result(self, response, json=False, binary=False):\n    157         assert not (json and binary)\n--> 158         self._raise_for_status(response)\n    159 \n    160         if json:\n/home/vagrant/.local/share/virtualenvs/aaa/lib/python2.7/site-packages/docker/client.pyc in _raise_for_status(self, response, explanation)\n    152             if e.response.status_code == 404:\n    153                 raise errors.NotFound(e, response, explanation=explanation)\n--> 154             raise errors.APIError(e, response, explanation=explanation)\n    155 \n    156     def _result(self, response, json=False, binary=False):\nAPIError: 500 Server Error: Internal Server Error (\"Unexpected status code [301] :\")\nIn [4]: docker.version\nOut[4]: '1.8.0'\n```\n``` python\nIn [2]: import docker\nIn [3]: c = docker.Client()\nIn [4]: c.login('joffrey', reauth=True, registry='https://index.docker.io/v1/')\nAPIError                                  Traceback (most recent call last)\n in ()\n----> 1 c.login('joffrey', reauth=True, registry='https://index.docker.io/v1/')\n/home/vagrant/.local/share/virtualenvs/aaa/lib/python2.7/site-packages/docker/api/daemon.pyc in login(self, username, password, email, registry, reauth, insecure_registry, dockercfg_path)\n     69         if response.status_code == 200:\n     70             self._auth_configs[registry] = req_data\n---> 71         return self._result(response, json=True)\n     72 \n     73     def ping(self):\n/home/vagrant/.local/share/virtualenvs/aaa/lib/python2.7/site-packages/docker/client.pyc in _result(self, response, json, binary)\n    156     def _result(self, response, json=False, binary=False):\n    157         assert not (json and binary)\n--> 158         self._raise_for_status(response)\n    159 \n    160         if json:\n/home/vagrant/.local/share/virtualenvs/aaa/lib/python2.7/site-packages/docker/client.pyc in _raise_for_status(self, response, explanation)\n    152             if e.response.status_code == 404:\n    153                 raise errors.NotFound(e, response, explanation=explanation)\n--> 154             raise errors.APIError(e, response, explanation=explanation)\n    155 \n    156     def _result(self, response, json=False, binary=False):\nAPIError: 500 Server Error: Internal Server Error (\"Registration: \"Missing password field\"\")\n``\n. OK, I agree with you. Thanks.\n. @TomasTomecek:\n Sorry, I found I put the tag with repository \ni.e.d.tag(\"busybox\", \"aaa/busybox:test\")`\nIf docker-py decided not to do this for client side. Just close this issue.\n. @shin Thanks for your support. Is there are any existing DriverOpts as an example we can use in Ubuntu environment? \nIs it possible add these features in 1.9.0 or 1.9.1?\n. Found another missing parameters:\nIn disconnect_container_from_network API.\nForce - Force the container to disconnect from a network\n. ",
    "jlamcanopy": "It is still does not work. When is the next release?\n. I'm trying to set mem_limit to 5g but it does not limit memory of the docker container. I inspect the container this is what I got from grep Memory:\n\"Memory\": 0,\n    \"MemorySwap\": 0,\n    \"Memory\": 0,\n    \"MemorySwap\": 0,\n. Hi @shin- \npip3 freeze | grep docker-py && python3 --version && docker version\ndocker-py==1.4.0\nPython 3.4.2\nClient version: 1.6.2\nClient API version: 1.18\nGo version (client): go1.3.3\nGit commit (client): 7c8fca2\nOS/Arch (client): linux/amd64\nServer version: 1.6.2\nServer API version: 1.18\nGo version (server): go1.3.3\nGit commit (server): 7c8fca2\nOS/Arch (server): linux/amd64\nFor create_container:\ncli.create_container(image='jupyter:latest', mem_limit='5g')\nInspecting the docker container will give the result above.\n. ",
    "harpingon": "I have same problem, I can't run your example because of\ndocker.errors.InvalidVersion: mem_limit has been moved to host_config in API version 1.19\nSome more investigation reveals this:\nAfter the initial c.create_container , the Memory parameter is there with my 2g\ndocker inspect garycowelle5sysbld140 | grep -i mem\n    \"Memory\": 2147483648,\n    \"MemorySwap\": 0,\n    \"CpusetMems\": \"\",\n    \"MemorySwappiness\": null,\nBut, after calling c.start , the configuration is gone:\ndocker inspect garycowelle5sysbld140 | grep -i mem\n       \"Memory\": 0,\n       \"MemorySwap\": 0,\n       \"CpusetMems\": \"\",\n       \"MemorySwappiness\": null,\nAfter stopping the container, configuration is still gone:\ndocker inspect garycowelle5sysbld140 | grep -i mem\n      \"Memory\": 0,\n      \"MemorySwap\": 0,\n      \"CpusetMems\": \"\",\n      \"MemorySwappiness\": null,\nCreate another container using docker-py:\n$ docker inspect fdde29acfea6 | grep -i mem\n     \"Memory\": 2147483648,\n     \"MemorySwap\": 0,\n     \"CpusetMems\": \"\",\n     \"MemorySwappiness\": null,\nBut this time, start it with docker command line:\n$ docker start fdde29acfea6\n    fdde29acfea6\n    $ docker inspect fdde29acfea6 | grep -i mem\n     \"Memory\": 2147483648,\n     \"MemorySwap\": 0,\n     \"CpusetMems\": \"\",\n     \"MemorySwappiness\": null,\nconfiguration remains.\nSo, seems to me that the docker-py start is clobbering some of the host config?\nSome versions for you:\n```\n  ('VERSION: ', {u'KernelVersion': u'3.10.0-229.14.1.el7.x86_64', u'Os': u'linux', u'BuildTime': u'Wed Oct  7 17:25:19 UTC 2015', u'ApiVersion': u'1.20', u'Version': u'1.8.2', u'GitCommit': u'0a8c2e3', u'Arch': u'amd64', u'GoVersion': u'go1.4.2'})\n$ pip freeze | grep docker\n docker-py==1.5.0\n$ docker --version\nDocker version 1.8.2, build 0a8c2e3\n$ uname -r\n3.10.0-229.14.1.el7.x86_64\n$ cat /etc/redhat-release\nRed Hat Enterprise Linux Server release 7.1 (Maipo)\n$ python --version\nPython 2.7.5\n```\nHope this helps\n. It's RHEL 7.1 \nLinux hostname 3.10.0-229.14.1.el7.x86_64\n. (sorry, posting my comment on this ticket also, as the referenced one status is closed)\nFurther investigation: I can't run your example because of\ndocker.errors.InvalidVersion: mem_limit has been moved to host_config in API version 1.19\nSome more investigation reveals this:\nAfter the initial c.create_container , the Memory parameter is there with my 2g\ndocker inspect garycowelle5sysbld140 | grep -i mem\n    \"Memory\": 2147483648,\n    \"MemorySwap\": 0,\n    \"CpusetMems\": \"\",\n    \"MemorySwappiness\": null,\nBut, after calling c.start , the configuration is gone:\ndocker inspect garycowelle5sysbld140 | grep -i mem\n       \"Memory\": 0,\n       \"MemorySwap\": 0,\n       \"CpusetMems\": \"\",\n       \"MemorySwappiness\": null,\nAfter stopping the container, configuration is still gone:\ndocker inspect garycowelle5sysbld140 | grep -i mem\n      \"Memory\": 0,\n      \"MemorySwap\": 0,\n      \"CpusetMems\": \"\",\n      \"MemorySwappiness\": null,\nCreate another container using docker-py:\n$ docker inspect fdde29acfea6 | grep -i mem\n     \"Memory\": 2147483648,\n     \"MemorySwap\": 0,\n     \"CpusetMems\": \"\",\n     \"MemorySwappiness\": null,\nBut this time, start it with docker command line:\n$ docker start fdde29acfea6\n    fdde29acfea6\n    $ docker inspect fdde29acfea6 | grep -i mem\n     \"Memory\": 2147483648,\n     \"MemorySwap\": 0,\n     \"CpusetMems\": \"\",\n     \"MemorySwappiness\": null,\nconfiguration remains.\nSo, seems to me that the docker-py start is clobbering some of the host config?\nSome versions for you:\n```\n  ('VERSION: ', {u'KernelVersion': u'3.10.0-229.14.1.el7.x86_64', u'Os': u'linux', u'BuildTime': u'Wed Oct  7 17:25:19 UTC 2015', u'ApiVersion': u'1.20', u'Version': u'1.8.2', u'GitCommit': u'0a8c2e3', u'Arch': u'amd64', u'GoVersion': u'go1.4.2'})\n$ pip freeze | grep docker\n docker-py==1.5.0\n$ docker --version\nDocker version 1.8.2, build 0a8c2e3\n$ uname -r\n3.10.0-229.14.1.el7.x86_64\n$ cat /etc/redhat-release\nRed Hat Enterprise Linux Server release 7.1 (Maipo)\n$ python --version\nPython 2.7.5\n```\nHope this helps\n. Yes, we're passing ports in start:\nc.start(syscontainer, port_bindings=pbind)\nAnd of course, I just tried without using port_bindings in the start call, and the memory limit has correctly remained.\nSo, I think there maybe a bug here, but for me, I think I can remove port_bindings from start.\n. Yes I see that now. I will just refrain from using anything other than container name in start.\nAPI moves fast :)\nThanks for your help. Closing this.\n. I've just tried that code for specifying alias, and I get:\nendpoint_config.aliases param is not supported in API versions < 1.22\nI'm running Docker 1.10.3 ...\n$ docker --version\nDocker version 1.10.3, build 20f81dd\nBut, with Swarm on tcp://localhost:4000\ndocker info\n$ docker info\nContainers: 14\n Running: 13\n Paused: 0\n Stopped: 1\nImages: 25\nServer Version: swarm/1.1.3\nI do also get the same error without swarm though ...\n$ docker -H tcp://localhost:2375 info\nContainers: 8\n Running: 7\n Paused: 0\n Stopped: 1\nImages: 102\nServer Version: 1.10.3\nSo, what is the problem , because I have 1.10.3, I should be on API 1.22, yes?\nCan see this with curl ...\ncurl http://localhost:2375/v1.22/info\n{\"ID\":\"XZQI:YF2Z:62DM:H7IF:JZJD:7SUE:VEC5:ON3M:2GJR:QMG4:SICL:I42Y\",\"Containers\":8,\"ContainersRunning\"\nDocker responds to API v1.22 requests\nI've probably missed something obvious, but, can't see what that might be so far\nRefreshed docker-py using pip today, before testing\n. Thank you.\n. ",
    "carlsverre": "bump?  This seems reasonable to me.\n. anyone against changing it to a setattr?\n. ",
    "groundeffect": "Done.\n. What kind of output were you expecting? Looking at the source, the Client.start method doesn't refturn anything. So you should be fine if no exception is being raised. Alternatively you could do:\npython\nmeta = c.inspect_container(my_container['Id'])\nif meta['State']['Running']:\n    print('{0} is running'.format(my_container['Id']))\n. Hm, I think that's because the command /bin/bash exits immediately. You should be able to see if and when it exited by running docker ps -a\n. ",
    "cmdelatorre": "FYI, the documentation is out-of-date here. The reference for create container does not specify the cpuset param.\n. ",
    "brejoc": "Oh, must have overlooked that. Sorry. But the container isn't running nevertheless. 'docker ps' shows no containers.\n. Thanks for the tip, @groundeffect. The container is indeed exiting with \"0\" right after the start. But shouldn't /bin/bash keep it running?\nBash\nbreuer@crusher ~> sudo docker ps -a\nCONTAINER ID        IMAGE                          COMMAND                CREATED             STATUS              PORTS                    NAMES\na43761ad5f82        ubuntu:14.04                   /bin/bash              2 minutes ago       Exit 0                                       condescending_wright\n. @ColinHuang Thanks! That was missing. Isn't this something worth mentioning in the README? Perhaps in a \"I'm new here section\"?\n. ",
    "ColinHuang": "You have to set tty=True, stdin_open=True, parameters when create a new container.\n. +1\n. Thanks. It works well after upgrade to the latest version.\n. Fixed test case for entrypoint new  behaviour.\n. Yes, only in python 2.\nDon't know how to make CI works for both python 2 and python 3. Need someone help. https://github.com/docker/docker-py/pull/774\n. ``` python\nimport docker\ndc = docker.Client()\nimage = 'busybox'\nhost_config = dict()\ncreate_args = dict()\nhost_config['binds'] = {\n    u'/home/test/\\u6e2c\\u8a66\\u6211\\u662f\\u4e2d\\u6587': {\n        u'bind': u'/a',\n        u'ro': False\n    }\n}\ncreate_args['host_config'] = docker.utils.create_host_config(\n    binds=host_config.get('binds', None))\nresult = dc.create_container(image, **create_args)\n```\nException:\n``` text\nTraceback (most recent call last):\n  File \"d.py\", line 36, in \n    binds=host_config.get('binds', None))\n  File \"/home/vagrant/.local/share/virtualenvs/0923/local/lib/python2.7/site-packages/docker/utils/utils.py\", line 461, in create_host_config\n    host_config['Binds'] = convert_volume_binds(binds)\n  File \"/home/vagrant/.local/share/virtualenvs/0923/local/lib/python2.7/site-packages/docker/utils/utils.py\", line 208, in convert_volume_binds\n    k, v['bind'], mode\nUnicodeEncodeError: 'ascii' codec can't encode characters in position 11-16: ordinal not in range(128)\n```\n. It works well for #787. Thanks!\n. ",
    "fkromer": "I am using tty=True and stdin_open=True ...\npython\nimport docker\nclient = docker.from_env()\nrcappdb = client.containers.run('rcapplication_database:latest', command='rcapplication_database_create /database new.db', tty=True, stdin_open=True, detach=True)\nrcappdb.exec_run(['rcapplication_database_devserver', '/database', 'new.db']) # is the container not running here for at least 3 seconds?\n(some Traceback)\nAPIError: 500 Server Error: Internal Server Error (\"Container 9707dbaed711af4fee65981416e22773d5f281d76666e32ca8fbdc6b3aeecc37 is not running\")\n... but I have still the problem that the container goes into exit after 3 seconds?\n\u276f docker ps -a\nCONTAINER ID        IMAGE                            COMMAND                  CREATED             STATUS                         PORTS               NAMES\n9707dbaed711        rcapplication_database:latest    \"rcapplication_dat...\"   4 seconds ago       Exited (0) 1 second ago                            brave_davinci. @shin- There are couple of issues which are about how to control cpu configuration of a container. I am not sure if I understood the configuration options properly. Are you open to a PR which adds an explanation about the different configuration options in docker-py? Would you review it or would someone else review it?. As far as I can remember PDF generation is a builtin feature of readthedocs. For the \"configuration in GitHub\" refer to e.g. tox (search for \"sphinx\"). I am not quite sure how a project had to be configured in the readthedocs dashboard exactly anymore.... Great, thanks a lot.. @AStotal I tried your solution but got NotFound: 404 Client Error: Not Found (\"network uplink not found\"). Can you imagine why?. Ah. Thought it's probably created implicitly. Do I have to create the network with subnet configuration (imap_pool)?. @shin- Thx a lot. In my case it should be sufficient to consider paramter changes (str) \u2013 Dockerfile instructions to apply while committing of Container.commit() of the running container.. > AFAIK there is no way of doing that.\n\nHow about using the low level exec_create & exec_start API?\n\nI hoped that I could get around the low level API :smirk: . I'd expect something like e1 = d.exec_run(c, [\"ls\", \"/\"]) instead of e1 = d.exec_create(c, [\"ls\", \"/\"]) but the docs about exec_create() clarified things (\"Sets up an exec instance in a running container.\"). Thx.. I am forced to use the low level API for all containers docker.api.client.APIClient and it is not possible to control other containers via docker.client.DockerClient, right?. @TomasTomecek I've not tried it so far. I thought someone have tried before? I can try it later in case.... @shin- In this case its not about the exec command. But hint about the since argument is very helpful, thx.. @TomasTomecek Thx for the fast response.\nFor reference:\n\nissue in gitlab-runner project\nquestion on stackoverflow.com. \n",
    "mminer": "If I understand it correctly, an image with <none> for its repository and tag is considered a \"dangling\" image. You can list dangling images by supplying filters to the images method.\npython\nclient.images(filters={'dangling': True})\n. ",
    "larsks": "I don't believe I was proposing ten different ways to specify port bindings.   You can decline a patch without being snippy about it.\n. Thanks, fixed that...\n. ",
    "twendt": "Did any of the tests fail for you?\nThey all went through OK for me. The only 2 tests failing are TestRestartingContainer and TestKillWithSignal, but they fail without my changes as well.\n. Sorry, I fixed the issues. Next time I know how to check them myself.\n. Did any of the tests fail for you? Apart from 1 or 2 tests near the end,\nwhich already failed without my patch, they all went OK for me.\nBut I can check again later on.\nAm 05.09.2014 14:03 schrieb \"Joffrey F\" notifications@github.com:\n\nHi, thanks for the contribution! Can you make sure the tests pass before I\nreview this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-py/pull/313#issuecomment-54616574.\n. \n",
    "dandroid88": "Bump, not exactly sure how i can help but I will with a bit of guidance.\n. bump\n. ",
    "ssssam": "I see that I've quite drastically broken the tests in this branch :) I'll fix that and resubmit.\n. I have opened https://github.com/docker/docker-py/issues/322 to keep track of this issue, sorry for the noise!\n. ",
    "kirankumar19": "Got it. Thanks mpetazzoni and shin. WIll make sure to go to appropriate mailing lists.\n. ",
    "rgbkrk": "I wrote a terrible way to do this when working with Tornado:\n``` python\nclass AsyncDockerClient():\n    '''Completely ridiculous wrapper for a Docker client that returns futures\n    on every single docker method called on it, configured with an executor.\n    If no executor is passed, it defaults to ThreadPoolExecutor(max_workers=2).\n    '''\n    def init(self, docker_client, executor=None):\n        if executor is None:\n            executor = ThreadPoolExecutor(max_workers=2)\n        self._docker_client = docker_client\n        self.executor = executor\ndef __getattr__(self, name):\n    '''Creates a function, based on docker_client.name that returns a\n    Future. If name is not a callable, returns the attribute directly.\n    '''\n    fn = getattr(self._docker_client, name)\n\n    # Make sure it really is a function first\n    if not callable(fn):\n        return fn\n\n    def method(*args, **kwargs):\n        return self.executor.submit(fn, *args, **kwargs)\n\n    return method\n\n```\nThe right way to do this would be to just extend your base class and change the underlying client.\n/cc @minrk\n. I've seen this several times too. Should Names be an empty list in that case?\n. Thank you, seeing that now. I'm most interested in getting this to work with swarm, using the same default setup so the env setup is the same across tools.\n. Thank you all! I look forward to using this.\n. ",
    "leonmax": "+1\n. ",
    "mastak": "+1\n. ",
    "jdavanne": "+1\n. ",
    "zbyte64": "Here is my approach using requests_futures:\n```\nfrom requests_futures.sessions import FuturesSession\nfrom docker import Client\nclass AsyncIOClient(FuturesSession, Client):\n    def request(self, *args, kwargs):\n        return FuturesSession.request(self, *args, kwargs).result()\n```\nReturning a future at the request level mucks up everything but at least this way I get co-operative threading. Edit: My understanding is that this is still blocking at the IO level.\n. I believe I am having the same issue (same error anyways). I uninstalled pyopenssl and reinstalled it:\npython -c 'import ssl; print ssl.OPENSSL_VERSION'\nOpenSSL 1.0.1f 6 Jan 2014\npython -V\nPython 2.7.11\nLast line of the stack trace:\nFile \"/home/jason/Repos/rethinkdb-factory/local/lib/python2.7/site-packages/requests/packages/urllib3/contrib/pyopenssl.py\", line 255, in ssl_wrap_socket\n    ctx = OpenSSL.SSL.Context(_openssl_versions[ssl_version])\nKeyError: 5\n. Ubuntu 14.04 LTS (with deadsnakes PPA so I get python 2.7.11 instead of 2.7.5)\npip freeze\nFabric==1.10.2\nPyYAML==3.11\nargparse==1.2.1\nbackports.ssl-match-hostname==3.5.0.1\nboto3==1.2.3\nbotocore==1.3.17\ncffi==1.5.0\ncryptography==1.2.2\ndocker-compose==1.5.2\ndocker-py==1.6.0\ndockerpty==0.3.4\ndocopt==0.6.2\ndocutils==0.12\necdsa==0.13\nenum34==1.1.2\nfalcon==0.3.0\nfunctools32==3.2.3-2\nfutures==3.0.3\ngunicorn==19.4.5\nidna==2.0\nipaddress==1.0.16\njmespath==0.9.0\njsonschema==2.5.1\nndg-httpsclient==0.4.0\nparamiko==1.16.0\npyOpenSSL==0.15.1\npyasn1==0.1.9\npycparser==2.14\npycrypto==2.6.1\npython-dateutil==2.4.2\npython-mimeparse==0.1.4\nrequests==2.7.0\nrethinkdb==2.2.0-3\nsix==1.10.0\ntexttable==0.8.4\nurllib3==1.14\nwebsocket-client==0.35.0\nwsgiref==0.1.2\n. It actually does. It wasn't clear to me that the container binded to a port because the port method didn't return any results. \n. Ya, so that actually doesn't work...\nIf I run this on docker I get the port binded to localhost even though I specified a different network:\ndocker run -d --net test7 -e RETHINKDB_JOIN_ADDR=10.0.3.2:29015 -p 28443 -v /etc/docker:/certs zbyte64/rethinkdb-tlsproxy\nBut if I tell docker-py publish_all_ports or ports=[28443] along with network_mode=test7 then no actual port binding happens. :-1: \n. Did more digging. If you specify the network id instead of the network name then docker will not bind your ports to the host. So if you want to use overlay with port binding then you must pass the network name to the network_mode. This behaviour does not seem to be documented anywhere....\n. Yes. But I think this recently changed with docker 1.10.1, it seems to be ignoring that all together.\nIn any event, it would be nice if the docs stated the canonical way to specify using an overlay network.\n. Can confirm this fixes #816 :)\npip install git+https://github.com/docker/docker-py.git@ssl_version_simpler#docker-py\n. When I try to write to the resulting socket I get: File or stream is not writable. I made sure to set stdin=True when specifying exec_create.\n. Sorry, let me clarify. To write to stdin one calls attach_socket with the appropriate flags:\nsocket = client.attach_socket(container_id, {\n    'stdin': 1,\n    'stream': 1,\n})\nBut the socket you get back cannot be written to. Instead you must use os.write to send data to stdin:\nos.write(socket.fileno(), b'echo hello world')\nThis usage seems awkward. But If you use TLS then you instead do:\nsocket.send(b'echo hello world')\n. Here is a gist of what I do to put a password file into a container:\nhttps://gist.github.com/zbyte64/6800eae10ce082bb78f0b7a2cca5cbc2\nHope it helps!\n. ",
    "cecton": "Hi everyone. I'm thinking of this too and wonder if it would be a good idea to have the Client class to use aiohttp's Session class instead of requests?\nThe idea is that requests doesn't handle async, so it would be nice to get async methods for every action. (Does that make make sense?)\n. @graingert Thx! I already saw it (too late) but I wanted something closer to docker-py and more official (because then Docker maintain it).\nBut my approach is bad too... I end up adapting all the code to put \"async\" and \"await\" keywords everywhere. This is definitely not what I wanted. I wanted some kind of wrapper on docker-py that would make the call asynchronous but now I kinda realize it's impossible or I totally missed something.\n.... actually.... I just realize now that it is under aio-libs so it's maintain by the devs of aiohttp. \ud83d\ude15 great lol. To whom is interested: I managed to wrap docker-py: https://github.com/tenforce/docker-py-aiohttp\nIt can certainly be ported to other asyncio libraries using the same principles. The core of the principles are located here: https://github.com/cecton/requests2aiohttp\nIn brief: I made a class that is meant to override requests.sessions.Session. All the HTTP calls are redirected to aiohttp and I have a custom Response class that wraps aiohttp's Response class. Its main purpose is to have a common API with requests.sessions.Session so the docker-py code can use it the same way.\nIt's not state of the art but I added integration tests for the same Docker versions tested by docker-py and it seems very stable.. I made a small attempt here to get docker-py asynchronous: https://github.com/tenforce/docker-py-aiohttp This is actually a wrapper around docker-py. I suppose it's not really what you want, especially since it is strongly bound to aiohttp and I think you want something more generic right?. @shin- sorry to bring you bad news, I think you broke the load_image. I just tried to load b'000' which obviously should fail and since the check of the status is not done, the call works but nothing is loaded. You should actually call _raise_for_status before returning with _stream_helper.\nI also noticed in _stream_helpers that the behavior is not consistent:\n *  If the response is chunked, the status code is not checked.\n *  If the response is not chunked, the status code is checked.\nYou may also want to check #1496 because the behavior is a bit more trickier if I recall.\n@shin- I can make a PR to fix all of this if you want.. @shin- thanks I confirm! I thought the status was varying because there was a _raise_for_status call. So apparently the _raise_for_status is completely useless. I don't see any 4xx possible error in the doc either: https://docs.docker.com/engine/api/v1.28/#operation/ImageLoad\n. The travis check is broken:\n$ python --version\nPython 3.5.3\n(...)\npy33 create: /home/travis/build/docker/docker-py/.tox/py33\nERROR: InterpreterNotFound: python3.3. ping @shin- (look also here https://github.com/docker/docker-py/pull/1695#discussion_r131831655 because I didn't remove the change on load_image yet). @shin- done!. Ah yeah actually I knew that, thanks!. Cool!. Ah but in #1667 it doesn't return the result, it only raises (which is the point of my PR). Either you merge #1667 first and I fix the conflict or you can update #1667 by replacing the self._raise_for_status by the self._result.. ",
    "graingert": "https://pypi.python.org/pypi/yieldfrom.requests is one option\nThe solution for all these blocking libraries, is to use async/await coroutines, but they will not be usable until 2020-01-01 when all currently supported Python versions without async/await are both EOL.\nIdeally requests, docker-py, etc etc will create an asyncio version of their libraries then import it into a legacy blocking version.\n. twisted now supports asyncio, and so you can create code that works with https://github.com/twisted/treq instead. This way there can be a blocking api that wraps the Twisted calls, and there can be a non-blocking api using Twisted or asyncio via Twisted on Python 3.\n. However it might be better to use the https://sans-io.readthedocs.io/ approach, then people can plug their own session in.\n. you could use pypi https://pypi.python.org/pypi/deferred just need to poke @hawkowl or @glyph to upload a fresh one\n. this way you can write a blocking request session wrapper that takes deferreds and the aiodocker-py library can replace it\n. @glyph well I'm really looking to have the async/await support here: https://github.com/twisted/twisted/pull/489\nalso wheels\n. and maybe get it off the launchpad and into Git(Hu|La)b.\n. ah I found it on https://launchpad.net/deferred\n. @cecton have a look at this: https://github.com/aio-libs/aiodocker. fixed in https://github.com/docker/docker-py/pull/1071\n. @felixonmars with a better approach to the flake8 errors\n. @dnephin any update on a merge?\n. This way someone can come and make an asyncio IOAdapter adapter to fix #327\n. This won't change too much of the implimentation of api classes, they will just need to add @defered.inlineCallbacks to their methods, and then instead of:\npython\nresp = self._request(...);\ndo:\npython\nresp = yield self._request(...);\n. @bfirsh is this something you'd like a PR on?\n. @shin- this issue is completely unrelated\n. @shin- is this something you guys would be interesting? It would take me a little bit of work up-front, so I'd like to know if this feature is something you would be interested in having.\n. The long_description seems to be invalid restructuredText\n. @shin- you should test your generated reStructuredText with https://pypi.python.org/pypi/readme\n. @shin- https://github.com/docker/docker-py/issues/1247\n. @lukasa ^. > @graingert So it looks like our patch to stop preparing non-HTTP-schemed URLs broke this. Specifically, the scheme is http+docker.\n\n@graingert I\u2019m not immediately certain that this is wrong. Given that http+docker may contain non-hostnames, IDNA-encoding would be wrong.\n\nFrom @lukasa via twitter\n. @shin- you might want to exclude 2.12.0-2.12.2 not sure though. in https://github.com/kennethreitz/requests/issues/3734 @lukasa recommends using \"http\" schemes rather than \"http+<something\" schemes.. @thomaschaaf neato. @shin- ping. @thomaschaaf can you update the ImageCollection docstring?. @thomaschaaf @MichaelPereira I've got a fork of this at https://github.com/docker/docker-py/pull/1417 with that change.. I'm not using 2.0. This does look like the issue . @shin- yeah that looks like my issue, do you know when this will be out for 1.x?. I found docker 2! It was hidden as docker rather than docker-py. rebased, and fixed version of https://github.com/docker/docker-py/pull/1383. If I put a debugger here, I can see that:\npython\n(Pdb) response\n<Response [101]>\n(Pdb) response.raw\n<requests.packages.urllib3.response.HTTPResponse object at 0x7f66cb884be0>\n(Pdb) response.raw._fp\n<docker.transport.unixconn.UnixHTTPResponse object at 0x7f66cb884ba8>\n(Pdb) response.raw._fp.fp\n<socket.SocketIO object at 0x7f66cb884c88>\n(Pdb) response.raw._fp.fp.raw\n*** AttributeError: 'SocketIO' object has no attribute 'raw'\n(Pdb) response.raw._fp.fp._sock\n<socket.socket fd=9, family=AddressFamily.AF_UNIX, type=2049, proto=0, raddr=/var/run/docker.sock>\n(Pdb) self.base_url\n'http+docker://localunixsocket'. I suspect the bug was introduced here: https://github.com/docker/docker-py/commit/1359eb1100d602c5115c07175724002d30a042ee. Function at fault https://github.com/docker/docker-py/blob/f238fe5554cca48bb9bb366a07bd219c090e445b/docker/api/client.py#L271-L280. I'm using python 3.6 connecting via 'http+docker://localunixsocket' on Ubuntu 17.10\n. @shin- can you make a test case for this?. I'd have thought that a simple integration test of, \"can I communicate with docker on py3\" would have caught this...?. @shin- I've added a ticket for that: https://github.com/docker/docker-py/issues/1803\n. all this does is allow users to enter their storage size in pounds (300lb) on python 2\n. They're still there. I just combined the 3K and 2 tests\n. ",
    "glyph": "If this would be useful we'd be quite happy to help out; just let us know!\nHowever, Deferred is a pretty slow-moving abstraction, so you're unlikely to need any new features; you can just pip install it and try it out as-is.\n. @graingert Oh!  Well in that case, yes, we should definitely do that :).\n. It's currently on GH as https://github.com/mikeal/deferred\n. (The plan is to move it into the Twisted org, of course.)\n. Yeah that was the original location.\n. I believe the problem here is simply an issue with openssl versions, and the apparent workaround of ndg-httpsclient just works if you happen to have built your cryptography wheel against the (old) system PyOpenSSL.  You can verify this by looking into your virtualenv like so:\notool -L $VIRTUAL_ENV/lib/python2.7/site-packages/cryptography/_Cryptography_cffi_* | grep libssl\nIf this shows you a /usr/local/lib/ then you are linked against homebrew; /usr/lib and you're linked against the system.\nI was able to get Python entirely out of the loop, like so:\n$ echo | /usr/bin/openssl s_client -verify 9 -cert \"${DOCKER_CERT_PATH}/cert.pem\" \\\n    -key \"${DOCKER_CERT_PATH}/key.pem\" -CAfile \"$DOCKER_CERT_PATH/ca.pem\" \\\n    -connect \"${DOCKER_HOST#tcp://}\" -servername boot2docker 2>&1 | grep Verify\n    Verify return code: 0 (ok)\n$ echo | /usr/local/bin/openssl s_client -verify 9 -cert \"${DOCKER_CERT_PATH}/cert.pem\" \\\n    -key \"${DOCKER_CERT_PATH}/key.pem\" -CAfile \"$DOCKER_CERT_PATH/ca.pem\" \\\n    -connect \"${DOCKER_HOST#tcp://}\" -servername boot2docker 2>&1 | grep Verify\n    Verify return code: 21 (unable to verify the first certificate)\nEven after years of practice, I still find OpenSSL's error messages completely inscrutable, so this needs more investigation, but it looks like the problem is something broken about how boot2docker is issuing its own certificates.  I think the issue might be that the server isn't setting the proper extended key usage for TLS web server authentication, but since the docker command line (and therefore the go TLS implementation) is happy with this certificate, I don't know what's different.\n. Thanks very much to @tdsmith for helping me figure this out - the root cause of this issue is almost certainly https://github.com/boot2docker/boot2docker/issues/808\n. I tried creating a machine with docker-machine rather than boot2docker and the certificates generated there seemed to be correct and work with more recent OpenSSL, so that is one possible workaround.\n. ",
    "oberstet": "FWIW, there is also txaio which would allow an implementation that runs on both Twisted and asyncio (user choice) ..\nDisclosure: I am affiliated with txaio. Thx, you are right! I got confused, had the wrong (old) version. With the current one (v3.5): works for me.. ",
    "zhaostu": "Thanks, that makes sense.\n. ",
    "guillaumededrie": "Hi @shin-, is it possible to review this PR?\nRegards,\nGuillaume\n. Sry @shin-, I did'nt know that \nHave a nice day.\nGuillaume\n. Well okay. Thx for the feedback.\n. ",
    "mpapierski": ":+1: \nI just noticed the same issue\n. Calling client.inspect_container(container) after wait() solves the problem.\nFor unknown reasons docker cli does wait (\"Block until container id stops, then returns the exit code\") and gets exit code using inspect instead.\n. ",
    "twllight": "I just noticed as well when using the tag parameter within Client.build that it will default to  for Repository and Tag, unless you provide a colon after your tag (ie: bigtest/funtimes will turn into   but bigtest/funtimes:latest will showup correctly. Just something I found.\n. I forgot to add my docker-py version:\ndocker==2.0.2\npython:\nPython 2.7.5\n. Ah I found the issue, HostConfig now requires an api version parameter sent to it. Sorry about the noise!. hey thanks for getting back so quick:\nsix==1.10.0\n. Sure!\ncont_create = docker.containers.create(image       = ship_opts['cont_image'],\n                                              hostname    = ship_opts['cont_hostname'],\n                                              ports       = [5520, 8181, 7088, 8088, 5020],\n                                              command     = ship_opts['cont_action'],\n                                              volumes     = ship_opts['cont_volumes'],\n                                              cpuset_cpus = ship_opts['cont_cpuset'],\n                                              labels      = ship_opts['cont_label'],\n                                              name        = ship_opts['cont_name'],\n                                              environment = ship_opts['cont_env'],\n                                              detach      = True,\n                                                 tty         = True)\nSorry about the jank formatting :(. Ahhh perfect! thank you for the explanation! I'll close . thanks for this @gtseres I did something pretty similar to what you have which is using the range function.  I think the work being done in #1072 was good its just awaiting merging?. ",
    "ssanderson": "This just bit me rather hard: I spent about 4 hours trying to figure out how I was creating phantom exited containers before I finally realized that rm was being sent as False to /build.  I ended up filing https://github.com/docker/docker-py/issues/401 before realizing what was happening.\n. 0.6.0 for docker-py and 1.3.1 for docker.\n. My initial thought was that this was some sort of race condition because the container was so simple, but I tried it with a more heavyweight build and saw the same truncation behavior.\n. This is occurring with docker 1.3.1 and docker-py 0.6.0.\n. Upon further inspection, I think this is a dupe of https://github.com/docker/docker-py/issues/332, (the particular confusion here was caused by the mismatch of default values for rm to build.  Closing.\n. I've started working on a separate project with the goal of providing a higher-level object-oriented API on top of the REST API implemented here: https://github.com/quantopian/dockorm.  It's currently built with deployment of IPython/Jupyter notebook servers in mind, but the library itself is pretty generic besides using IPython's traitlet system to provide configuration.  I plan on adding support for build and link hierarchies in the semi-near future.\n. @shin- I think this is fallout from https://github.com/docker/docker/pull/10298.  docker-py currently accepts mem_limit as a direct argument to create_container, but it looks like it should be moved and/or forwarded to create_host_config?\n. (I'm seeing this issue as well by the way)\n. ",
    "sidprak": "This fixes https://github.com/docker/docker-py/issues/325\n. That's a good point. Switched to if urllib_ver.\n. ",
    "CharlieEriksen": "Why does this check exist in the first place? If you specify the ssl_version parameter in the TLSConfig constructor, you'd expect it to be passed on always.\nIf I run this on OS X and Windows(Works on Ubuntu):\n``` python\nimport docker\ntls_config = docker.tls.TLSConfig(client_cert=('client-cert.pem', 'client-key.pem'), verify=False, ssl_version=\"TLSv1\")\nc = docker.Client(base_url='https://XXX:2376', tls=tls_config)\nprint c.info()\n```\nIt will NOT try to use TLSv1, and the handshake against the docker daemon will fail. But if I do like this:\n``` python\nimport docker\nimport requests.packages.urllib3 as urllib3\nurllib3.version = \"1.6\"\ntls_config = docker.tls.TLSConfig(client_cert=('client-cert.pem', 'client-key.pem'), verify=False, ssl_version=\"TLSv1\")\nc = docker.Client(base_url='https://XXX:2376', tls=tls_config)\nprint c.info()\n```\nIt will use TLSv1, and it will work. The fact that unless you override the urllib3 version explicitly, it won't use the ssl_version parameter without telling you, is counter-intuitive. This shouldn't be the case, IMO.\n. ",
    "leth": "It looks like these codepaths are untested and broken on python 3, even without my changes.\nAs an aisde: the reason why this test harness does not hang is that we close the connection after sending the response\n. @micahhausler Go for it! I was hoping for some input from the core devs because the cleanest solutions would probably involve API changes. I'm a bit busy at the moment as well, so not likely to get anywhere with it.\n. I've rebased, but my code is still broken in python3.\nIn python3 there's no way to inject the data back into the underlying buffer, and the code for this in python2 is pretty ugly anyway.\nIt'd be better to refactor this whole raw socket thing to return the original buffer objects; which would fix the garbage collection workaround too.\n. Sorry I should make it clear that I don't have time to do this right now, so please feel free to take this as a starting point and finish it off.\n. This issue will cause hangs most often when consuming from very short streams, i.e. short builds or log streams.\n. @sigmavirus24 the response is one of two custom stream protocols (see attach for one, the other is a JSON stream? I can't find any good docs for it. IIRC it's a line-based protocol where each message is a pair of lines, the first line is a message length, and the second is the JSON message)\nIt has no known content-length, and the remote side does not close the connection, so when reading it one needs to be careful not to attempt to read more data than is present, else we might block.\nThat said, I've not tried setting \"Connection: close\" on the request.\nAs I said above, I think this code needs a refactor, because fixing it to work as-is just keeps getting uglier.\nDo you have any suggestions as to how this might be done better?\n. @shin-: response.raw is a requests.packages.urllib3.response.HTTPResponse instance, and readline comes from io.IOBase so it won't be decoding the chunks for you.\nI've updated this MR with a version that works on 2.7\n. @sigmavirus24: This simplified version fails my early-response test on python 3.x. Do you think there could be something different in urllib3's buffer handling?\n. Okay, the good news is I have something that works on all python versions. Bad news is that it still pokes into the requests internals.\n@sigmavirus24: I couldn't find a nice way to consume each chunk separately without that digging, since http.client hides the transfer encoding.\n. I'll look at updating this tomorrow for the case where the chunk length was 1\nOn 15 December 2014 23:06:59 GMT+00:00, Joffrey F notifications@github.com wrote:\n\nLGTM so far. @leth and others watching this PR, is this in a satisfying\nstate to be merged and be an improvement over the way things are done\nin the current stable?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/docker/docker-py/pull/336#issuecomment-67083786\n\n\nSent from my Phone. Please excuse my brevity.\n. Looking at it again, I'm happy for the fixes for stream handling to be merged, but the raw socket attach stuff still needs some love.\nI'll rebase all of this, split them in two, and raise a new MR for the raw socket stuff.\nLet me know if you'd rather do things differently\nOn 15 December 2014 23:18:42 GMT+00:00, Joffrey F notifications@github.com wrote:\n\nSounds good, thank you!\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/docker/docker-py/pull/336#issuecomment-67085239\n\n\nSent from my Phone. Please excuse my brevity.\n. Ah. It's in the test I think. I start a thread to listen on a Unix socket, but it might not be have opened it in time perhaps. I tried to fix a race around there but clearly failed\nOn 18 December 2014 01:41:40 GMT+00:00, Joffrey F notifications@github.com wrote:\n\nI have implemented a simple retry-on-fail mechanism in the test itself\nfor now. I would like to determine whether the bug is in the test\nitself or the result of a race condition in the new streaming code.\nIt looks like a race condition of some sort and I can't reproduce it at\nall locally, but on Travis it seems to happen more frequently.\nI've held off on the 0.7 release until we can figure this out, whether\nthrough a fix or a rollback.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/docker/docker-py/pull/336#issuecomment-67429159\n\n\nSent from my Phone. Please excuse my brevity.\n. You can probably reproduce it by putting a sleep at the start of the thread method\nOn 18 December 2014 01:41:40 GMT+00:00, Joffrey F notifications@github.com wrote:\n\nI have implemented a simple retry-on-fail mechanism in the test itself\nfor now. I would like to determine whether the bug is in the test\nitself or the result of a race condition in the new streaming code.\nIt looks like a race condition of some sort and I can't reproduce it at\nall locally, but on Travis it seems to happen more frequently.\nI've held off on the 0.7 release until we can figure this out, whether\nthrough a fix or a rollback.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/docker/docker-py/pull/336#issuecomment-67429159\n\n\nSent from my Phone. Please excuse my brevity.\n. Looks like the same issue as #368 \n. I think you're seeing what I've described in #336\n. I'd say keep it open; my bug has kind of stalled.\n. With any luck this may have been resolved now!\n. NB: This doesn't fix #336, rather the tests added in #336 should be passing/closer to passing after this change.\n. Again this is another case of #336.\n. Nice :) I'm happy for you/anyone to take ownership of this issue; it is mostly here as a placeholder for things not addressed by #336.\n. Do whatever! I'm not a maintainer, I was just the first to get to the bottom of the issue ;-)\n. Anything which calls Client._get_raw_response_socket will still be affected. This includes: attach_socket, attach, execute and logs.\n. @shin-: ping ;)\n. Here's the docker daemon API docs for 'build': https://docs.docker.com/reference/api/docker_remote_api_v1.16/#build-an-image-from-dockerfile-via-stdin\n. Looks like it's an engine bug, I'll reopen over there!\nHere's the subprocess version:\n``` python\nimport time\nimport itertools\nfrom subprocess import check_output, CalledProcessError\ndef docker(*args):\n    return check_output(('docker', ) + args)\nfor i in itertools.count():\n    print(i, time.time())\n    container = docker(\n        'run', '--net=host', '-d', 'busybox:latest', 'sh', '-c', 'sleep 0.55'\n    ).strip()\n    try:\n        docker('exec', container, 'sh', '-c', 'killall sleep || true')\n    except CalledProcessError as e:\n        pass\n    docker('wait', container)\n    docker('rm', container)\n``\n. I need to test what happens if the chunk was of length 1; doeschunk_left` become 0? What happens then?\n. ",
    "sigmavirus24": "Hi! requests core developer here. One quick question: Why is this code digging into implementation details in urllib3? Specifically what necessity do you have to be using response.raw._fp.fp?\n. So I'm still at a loss for why it's necessary to access r.raw.fp._fp when r.raw supports read operations like a file-like object.\nThat said, what you're dealing with is a chunked transfer-encoding on the response side. Those are far less common so requests won't handle them for you. We could add it to the requests-toolbelt to take it off your back if you'd like me to. It would look something like:\n``` python\nfrom requests_toolbelt import iter_chunked_response\nr = requests.get('https://our-url.com')\nfor chunk in iter_chunked_response(r):\n    # do stuff with chunk\n```\nIt would just yield to you the actual data.\n. So I've looked into this and spoken with the other core developer of requests on the toolbelt issue that GitHub auto-linked to this issue. The real problem is httplib strips out the chunk headers (the numbers that indicate the length of the next chunk). With that said, when I last skimmed the code in the repo (and this PR) I don't see any use of that value other than to break the loop, which I'm not sure is necessary. I don't have a docker API to test against honestly (nor am I very familiar with it) but I've been testing with https://httpbin.org/stream/100 and each chunk should be separated by a single \\n after httplib processes it. The way to detect that the stream is finished (as best as I can tell) is to read until the next \"line\" is ''. I'll need to read up on the docker API more but I think this might work.\n. ",
    "minrk": "Awesome, I just tried to use pause/unpause and was surprised by their absence. Glad this is coming.\n. ",
    "SvenDowideit": "I wonder if instead, the docker-py docs should get merged into the docker API docs more, and we moved away from RTD, sphinx and rst to markdown in march this year.\n(I don't know what the decision process is, just pointing out that perhaps there is a decision to be made)\n@fredlf ?\n. @micahhausler I don't know the answer either :/ @shin- can you give a bit more context?\nOn my end, I just presume that it would be good to use the same documentation platform across everything we do.\nBut I also wouldn't like to lose details.\nTo add to the complexity, we're looking at swagger to generate API docs in Docker (assuming I get there today)\n. oh nice :) LGTM\n. nice! - LGTM - @shin-\n. it might just need a 2 step:\n- add the /etc/hosts entry\n- change the DOCKER_HOSTS to use boot2docker (i.e., export DOCKER_HOST=tcp://boot2docker:2376 )\n(untested, so i might have a typo)\n. ",
    "fredlf": "I flat out don't understand this change. I'll get @SvenDowideit to explain it to me so I can make an informed evaluation. Sorry.\n. Sorry, @micahhausler , we've all been heads down on the upcoming release. I'll try to chat with @SvenDowideit tonight.\n. Bah, sorry, forgot to write up the results of my discussions with Sven. Basically we agree that keeping things in .md is the right thing to do. The benefits of consistency and ease of use outweigh any benefit of switching we could think of.\nAs far as moving to the \"official\" API docs, maybe a good compromise would be to create a new reference page with links to other API clients. That would add convenience for devs without adding confusion about privileging one API client over another.\n. That's terrific, many thanks for doing it. LGTM!\nPing @jamtur01 @ostezer\n. @shin- Are you good with this? Want to be sure before merging.\n. Capitalize Docker, please.\n. ",
    "AgentO3": "Having a similar problem with python 2.7\nfatal: [ec2] => failed to parse: sudo: unable to resolve host ip-10-10-13-87\nsudo: unable to resolve host ip-10-10-13-87\nSUDO-SUCCESS-kefkggunwgxqydlrtskqnagxkxyturbw\nTraceback (most recent call last):\n  File \"/home/ubuntu/.ansible/tmp/ansible-tmp-1413477069.2-52467766880601/docker_image\", line 1598, in \n    main()\n  File \"/home/ubuntu/.ansible/tmp/ansible-tmp-1413477069.2-52467766880601/docker_image\", line 234, in main\n    image_id = manager.build()\n  File \"/home/ubuntu/.ansible/tmp/ansible-tmp-1413477069.2-52467766880601/docker_image\", line 144, in build\n    for chunk in stream:\n  File \"/usr/local/lib/python2.7/dist-packages/docker/client.py\", line 270, in _stream_helper\n    socket_fp = socket_obj(_sock=self._get_raw_response_socket(response))\n  File \"/usr/local/lib/python2.7/dist-packages/docker/client.py\", line 265, in _get_raw_response_socket\n    sock._response = response\nAttributeError: '_socket.socket' object has no attribute '_response'\nFATAL: all hosts have already failed -- aborting\n. ",
    "Hi-king": "Similar problem happened to me with python2.7 and local machine\nWith six 1.3 (written as dependency in setup.py), got following erors.\n```\n$ sudo pip install six==1.3\n$ python\n\n\n\nimport docker\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/lib/python2.7/site-packages/docker/init.py\", line 20, in \n    from .client import Client, AutoVersionClient # flake8: noqa\n  File \"/usr/lib/python2.7/site-packages/docker/client.py\", line 35, in \n    import websocket\n  File \"/usr/lib/python2.7/site-packages/websocket/init.py\", line 22, in \n    from ._core import \n  File \"/usr/lib/python2.7/site-packages/websocket/_core.py\", line 41, in \n    from ._url import \n  File \"/usr/lib/python2.7/site-packages/websocket/_url.py\", line 23, in \n    from six.moves.urllib.parse import urlparse\nImportError: No module named urllib.parse\n```\n\n\n\nWith six 1.4, works correctly.\n```\n$ sudo pip install six==1.4\n$ python\nPython 2.7.5 (default, May  5 2014, 09:13:10)\n[GCC 4.8.2 20140120 (Red Hat 4.8.2-16)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport docker\n```\n. \n\n\n",
    "gdw2": "Encountered this problem today.  Thanks for the workaround @Hi-king !\n. ",
    "nicolargo": "Also encountered on my system: CentOS 7 with Python 2.7.5. Thks @Hi-king !\n. ",
    "samalba": "@aanand can you review/comment #334? I'll take care of the merge if you're ok with it.\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. ",
    "mkoistinen": "No, it did not, the quotes were stripped out again. I ended up changing the command to be a script with some of the params that require quotes embedded there. But, someone is surely going to need this ability.\n. Haven't tried. Sorry, I found a solution and ran with it.\n. ",
    "ats0stv": "Facing the same issue. The below command in python runs smoothly.\n```check.py --coverage 'Hello World' --blabla something\nAll Raw Arguments\n['--coverage', 'Hello World', '--blabla', 'something']\nAll Args\nNamespace(blabla='something', coverage='Hello World')\nPrint Coverage\nHello World\nPrint blabla\nsomething\n```\nThe same script when I have it within the docker container and run using docker run, the text inside quotes is split into multiple vars of the list.\n```docker run --rm -i sample --coverage 'Hello World' --blabla something\nAll Raw Arguments\n['--coverage', 'Hello', 'World', '--blabla', 'sometext']\nusage: check.py [-h] [--coverage COVERAGE] [--blabla BLABLA]\ncheck.py: error: unrecognized arguments: World\nMy dockerfile definition is as follows\nFROM sample_base\nENTRYPOINT [\"/check.py\"]\nCMD [\"--help\"]\n```\nBreaking my head for the past couple of hours. Any help is much appreciated. ",
    "virtuald": "+1 on #360, lets me connect to boot2docker now\n. +1 on this\n. +1\n. ",
    "proppy": "ping? @samalba @shin- \n. PTAL\n. ping @shin- \n. /cc @shin- \n. @shin- they can always add themselves in follow up PRs, I'd rather not volunteer them :)\nCan we go ahead and merge this?\n. ",
    "garethr": "Just to say I've tried this out locally and, apart from the hardcoded container name, it's working for me. The only thing I hit was failing to realise that the cmd option had to be a list. ie. ls /bin would fail but ['ls',  '/bin'] would work. At the least this is probably worth documenting in the README. Apart from that minor niggle, good job. \n. I think this shouldn't be hardcoded to shell but rather be:\n'/containers/{0}/exec'.format(container))\nOr similar. Applying this locally allows exec to work.\n. ",
    "phensley": "Thanks for the feedback -- I was so busy using the exec feature, the pull request got a bit rushed.\n. ",
    "rwestgeest": "here's a workaround:\nhttp://blog.sequenceiq.com/blog/2014/10/17/boot2docker-tls-workaround/\nIt is not a fix, just a workaround enablign you to bypass tls\n. ",
    "RacingDawn": "\nhttp://blog.sequenceiq.com/blog/2014/10/17/boot2docker-tls-workaround/\n\nhello\uff0chow to bypass tls? the wed page is invalid.. > Does #360 help at all?\ni follow as #360. but the log is:\n\n. > See the stream format documentation.\nthank you, but i don't understand clearly. i used the exec_start to get the socket.\n\nin the exec_start function the code is:\n\nAs mentioned earlier, when i created docker client without tls, the docker_stream_stdout can be encoded 'utf-8', but it can't be encoded 'utf-8' when i creat docker client with tls.\n\n\nthe docs indicates the recv() return the bytes represent the object. so how i can resolve the bytes result?\n. > See the stream format documentation.\nthank you, i resolve it. I should use the stream_tmp to recv and sendall instead of the stream_tmp.socket when i create client with tls_config.\nstream_tmp = self.dockerClient.exec_start(execId[\"Id\"], socket=True, tty=True). ",
    "sgjesse": "Thanks!\n. ",
    "gijzelaerr": "sorry, that is 0.5.3\n. I was thinking the same, but then probably the error message should be a\nbit more descriptive about this. Now a naive person as myself has no idea\nwhat is going on.\n2014-10-22 18:50 GMT+02:00 Joffrey F notifications@github.com:\n\nHm, if I'm not mistaken Docker since 1.3.0 enforces SSL for its API. So\nyou probably need to provide a TLSConfig to your Client constructor. See the\nREADME\nhttps://github.com/docker/docker-py#connection-to-daemon-using-https\nfor more info.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-py/issues/369#issuecomment-60116652.\n\n\nGijs Molenaar\nhttp://pythonic.nl http://gijs.pythonic.nl\n. trying to use the certificates, but then I run into the same problems described in issue #365 \n. yes it does :)\n. ",
    "schnupperboy": "Thanks, you are right and everything works now. I specified the docker version instead of the docker api version.\nFor me it would have helped to name the parameter of the Client constructor api_version instead of just version, although the docker-py documentation says what version is wanted here.\n. ",
    "jamtur01": "LGTM\n. ",
    "vyivanov": "I also faced with this lack of informativity. It would be nice to have an option. +1\n. ",
    "speedplane": "I monkey patched docker/api/build.py and progress info via the class below. This class wraps a file object and will report how much of has been read. Once this class is defined, you can see progress by adding context = StreamPrinter(context) before we make the _post call.\n```\nclass StreamPrinter(object):\n    '''\n    Wrap a file object and print out read progress. Used in conjunction with\n    the \n    '''\n    def init(self, file):\n        self.read_so_far = 0\n        self.starttime = datetime.datetime.utcnow()\n        self.file = file\ndef __iter__(self, *args, **kwargs):\n    self.starttime = datetime.datetime.utcnow()\n    return self\n\ndef chk_print(self, fn):\n    old_megs = self.read_so_far / 1024 / 1024\n    out = fn()\n    self.read_so_far += len(out)\n    new_megs = self.read_so_far / 1024 / 1024\n    if old_megs != new_megs:\n        log.debug('Sent %s megabytes. Time elapsed: %s'%(new_megs,\n            datetime.datetime.utcnow() - self.starttime))\n    return out\n\ndef next(self):\n    return self.chk_print(fn = lambda: next(self.file))\n\ndef read(self, *args, **kwargs):\n    return self.chk_print(fn = lambda: self.file.read(*args, **kwargs))\n\n```\nThis is still a bit messy and I'm not sure that this is the best approach, which is why I didn't make a pull request. But thought others may find it useful.\n. Also, FYI, I caught this while experiencing pain with Docker on Google's managed VM.\nhttps://code.google.com/p/google-cloud-sdk/issues/detail?id=456\n. - I can remove total reads, I initially just used it to properly size the buffer to something reasonable, but it's probably not necessary anymore.\n- As it stands, this function does not rely on fseek or ftell, which would be required to implement the file size. Not sure we want to add that dependency.\n. ",
    "danielwhatmuff": "+1 . ",
    "StoneJia": "+1. ",
    "kuba": "FYI this has been reported in Debian as #767445 and IMO it is a python-docker packaging problem, which I explain in the comment.\n. FYI Debian Python Modules Packaging Team is currently working to fix that. It is python-requests that is broken, and will hopefully be fixed soon :)\n. ",
    "iTech-Developer": "I do not think it is python-docker packaging problem, because when I install docker-py with pip install docker-py on debian Wheezy/Jessie I get the exact same problem. \nModifying the requests library is not a solution for me because it can break other libraries. So I changed docker-py as follows:\nReplaced in /docker/unixconn/unixconn.py (usually under /usr/local/lib/python2.7/dist-packages):\ntry:\n    import requests.packages.urllib3.connectionpool as connectionpool\nexcept ImportError:\n    import urllib3.connectionpool as connectionpool\nWith just import urllib3.connectionpool as connectionpool\nAnd similarly I replaced in /docker/ssladapter/ssladapter.py:\ntry:\n   import requests.packages.urllib3 as urllib3\nexcept ImportError:\n   import urllib3\nWith import urllib3\nSame thing with from requests.packages.urllib3._collections import RecentlyUsedContainer\n. ",
    "inducer": "Appears fixed in Debian's python-requests version 2.4.3-4.\n. ",
    "voronaff": "Hi. It seems to me I have a similar problem with timeout.\nI try to commit a container\npython\nc = Client(base_url=\"tcp://%s:%s\" % (host, port), version=\"auto\", timeout=300)\nc.commit(container_Id, new_image_name, new_tag)\nbut I get error\nFile \"/usr/local/lib/python2.7/dist-packages/docker/client.py\", line 70, in __init__\n    self._version = self._retrieve_server_version()\n  File \"/usr/local/lib/python2.7/dist-packages/docker/client.py\", line 90, in _retrieve_server_version\n    'Error while fetching server API version: {0}'.format(e)\ndocker.errors.DockerException: Error while fetching server API version: a float is required\n. maybe...\npip freeze output:\nadium-theme-ubuntu==0.3.4\nansible==1.7.2\napt-xapian-index==0.46\nbackports.ssl-match-hostname==3.4.0.2\nboto==2.36.0\ncharacteristic==0.1.0\nchardet==2.2.1\ncm-api==9.0.0\ncolorama==0.3.1\ncommand-not-found==0.3\ndebtagshw==0.1\ndefer==1.0.6\ndirspec==13.10\ndocker-py==1.1.0\nduplicity==0.6.23\necdsa==0.13\nhtml5lib==0.999\nhttplib2==0.9\nidna==0.9\nJinja2==2.7.3\nlockfile==0.8\nlxml==3.3.6\nMarkupSafe==0.23\noauthlib==0.6.1\noneconf==0.3.7\nPAM==0.4.2\nparamiko==1.15.2\npexpect==3.2\nPillow==2.6.1\npiston-mini-client==0.7.5\npyasn1==0.1.7\npyasn1-modules==0.0.5\npycrypto==2.6.1\npycups==1.9.67\npygobject==3.14.0\npyOpenSSL==0.13.1\npyserial==2.6\npython-apt===0.9.3.10ubuntu1\npython-debian==0.1.22\npyxdg==0.25\nPyYAML==3.11\nreportlab==3.1.8\nrequests==2.6.0\nservice-identity==1.0.0\nsessioninstaller==0.0.0\nsix==1.9.0\nsoftware-center-aptd-plugins==0.0.0\nsystem-service==0.1.6\nTwisted-Core==14.0.2\nTwisted-Web==14.0.2\nunity-lens-photos==1.0\nurllib3==1.8.3\nvirtualenv==12.0.7\nwebsocket-client==0.29.0\nwheel==0.24.0\nxdiagnose==3.6.6\nzope.interface==4.1.1\n. Hi guys!\nDo you have any news about problem?\n. I tried and it didn't help\nFile \"/usr/local/lib/python2.7/dist-packages/docker/client.py\", line 70, in __init__\n    self._version = self._retrieve_server_version()\n  File \"/usr/local/lib/python2.7/dist-packages/docker/client.py\", line 90, in _retrieve_server_version\n    'Error while fetching server API version: {0}'.format(e)\ndocker.errors.DockerException: Error while fetching server API version: a float is required\nDo you have any thoughts?\n. I've found a solution to fix the problem\nthere is note in requests package\n:type timeout: float or tuple\nyou need to add float in _set_request_timeout method in client.py:\npython\nkwargs.setdefault('timeout', float(self.timeout))\n. @shin- , what do you think? is it a bug? are you going to fix the bug?\n. Sorry guys.\nIt was my mistake. I use argparse in my script and it returns string instead int. I needed to specify type requested variable.\n. ",
    "allonhadaya": "Thanks, @gijzelaerr!\n. ",
    "saemideluxe": "The docker CLI supports copy-from and copy-to a container. The current docs of docker-py mention only copy-from container. Are you planning to implement the copy-to feature in the near future? Currently I work with mounting a single file but copying would be much nicer.\n. :thumbsup:\n. ",
    "ewindisch": "@shin- yes! I've just updated my branch with changes for both tls.py and ssladapter.py.\n. so much for passing tests. Yes, it should be.\n. ",
    "fbacchella": "Done\n. ",
    "elliotwoods": "hey!\ndid this ever get implemented?\ni could really do with this now :)\nalso the issues don't directly discuss the feature of being equivalent to passing '-it' to the container creation step, so it's a bit of a shame that this is closed as duplicate. aha, using stdin_open = True, tty = True working here. my run command looks like this:\nclient.containers.run(imageName\n            , stdin_open = True\n            , tty = True\n            , name = \"hacnode\"\n            , detach = True\n            , volumes = {\n                '/root' : {\n                    'bind' : '/mnt',\n                    'mode' : 'rw'\n                }\n            }\n            , privileged = True\n            , restart_policy = {\n                \"MaximumRetryCount\": 0,\n                \"Name\": \"unless-stopped\"\n            }\n            , network_mode = 'host'\n        )\nyou just need to check those first lines about stdin_open and tty. please note that the container does not become interactive within the python terminal, it just means that i can attach to the container later (this is what i want). if you want to attach to the container, then i would suggest using a system command to docker to perform this for you. trying to do this from within python sounds very messy, as the remote terminal should essentially have full control of the local terminal, which would mean a lot of actions would have to be passed through python, and the docker executable would do a much more consistent job of this.. ",
    "ChrisTimperley": "@elliotwoods Would you mind showing an example of how you managed to open an interactive container?. Ah, I see; I'll stick with subprocess in that case. Thanks!. Any luck with this? I'm having the same issue.. ",
    "rapvelopment": "I am running it on a unix socket and what I get is below (the call to containers still returns an empty list):\necho -e \"GET /containers/json?all=1 HTTP/1.0\\r\\n\" | nc -U /var/run/docker.sock\nHTTP/1.0 200 OK\nContent-Type: application/json\nDate: Sat, 08 Nov 2014 13:58:33 GMT\n[{\"Command\":\"python launch_service.py -s bs\",\"Created\":1414981608,\"Id\":\"3e9dbb7fb9de0ef02e9a5dcc18f16d3ec8f6a243de3cd8662fca5c632b89c2ba\",\"Image\":\"troika:latest\",\"Names\":[\"/bs\"],\"Ports\":[],\"Status\":\"Up 14 minutes\"}\n,{\"Command\":\"python launch_service.py -s wordTranslator\",\"Created\":1414981607,\"Id\":\"07d0eeaacea7115d1e6b1328f0a43e7c3c78661f294cfb7d5cd89d8f54e42a49\",\"Image\":\"troika:latest\",\"Names\":[\"/wordTranslator\"],\"Ports\":[],\"Status\":\"Up 14 minutes\"}\n,{\"Command\":\"python launch_service.py -s dataService\",\"Created\":1414981605,\"Id\":\"05a999057bb1a9994820fb7d8f02eccb1f3c95ca340afe1fae4013f397785f1c\",\"Image\":\"troika:latest\",\"Names\":[\"/dataService\"],\"Ports\":[],\"Status\":\"Up 14 minutes\"}\n,{\"Command\":\"python launch_service.py -s serviceWatchdog\",\"Created\":1414981604,\"Id\":\"e49b87bbcad4e3524810760070daecef0b1fa272a99ed06b4d7932515f596154\",\"Image\":\"troika:latest\",\"Names\":[\"/serviceWatchdog\"],\"Ports\":[],\"Status\":\"Up 14 minutes\"}\n,{\"Command\":\"python launch_service.py -s time\",\"Created\":1414981603,\"Id\":\"dda87d3884b703068df2ab99b8acb940afe4c94d825fbe0b9180d9274d615f68\",\"Image\":\"troika:latest\",\"Names\":[\"/time\"],\"Ports\":[],\"Status\":\"Up 14 minutes\"}\n,{\"Command\":\"python launch_service.py --dtn\",\"Created\":1414981602,\"Id\":\"294128bb59018811b8124f0cead662fdf2d6b1709358a4d0a5f7c06f5b8b4125\",\"Image\":\"dtn:latest\",\"Names\":[\"/dtn\"],\"Ports\":[],\"Status\":\"Up 14 minutes\"}\n,{\"Command\":\"/bin/bash\",\"Created\":1414870964,\"Id\":\"17529f9149adf575e32938670cd3b1c0ef0eca3bfef46c439d80336a0dfcbfff\",\"Image\":\"acid-data-view:latest\",\"Names\":[\"/acid-data-view\"],\"Ports\":[],\"Status\":\"Exited (0) 6 days ago\"}\n,{\"Command\":\"/docker-entrypoint.sh postgres\",\"Created\":1414869401,\"Id\":\"b5661f1e9d6ee69aab3f10d904ff644e95f29921311fd0f196a1bb8e966657c0\",\"Image\":\"382086032ee4\",\"Names\":[\"/bs/postgres\",\"/serviceWatchdog/postgres\",\"/wordTranslator/postgres\",\"/dataService/postgres\",\"/time/postgres\",\"/troika-postgres\"],\"Ports\":[{\"IP\":\"127.0.0.1\",\"PrivatePort\":5432,\"PublicPort\":5432,\"Type\":\"tcp\"}],\"Status\":\"Up 14 minutes\"}\n,{\"Command\":\"true\",\"Created\":1414868931,\"Id\":\"d13aae88e4d13a0e2ec7eb15b8c3bb50f36fbb333520e719bd322f7cf74cfdcd\",\"Image\":\"acid-data:latest\",\"Names\":[\"/acid-data\"],\"Ports\":[],\"Status\":\"Exited (0) 6 days ago\"}\n. if I run \nimport docker\nc = docker.Client(base_url='unix://var/run/docker.sock', version='1.2', timeout=10)\nprint \"number of containers = \", len(c.containers())\nprint \"docker version is \", docker.version\nI get\nnumber of containers =  0\n docker version is  0.6.0\n(I upgraded docker-py last night to see if that might have fixed the problem).\nHowever, if I run \nimport docker\n  d = docker.Client()\n  print \"number of containers = \", len(d.containers())\nI get an error:\nprint \"number of containers = \", len(d.containers())\nFile \"/usr/local/lib/python2.7/dist-packages/docker/client.py\", line 508, in containers\n  res = self._result(self._get(u, params=params), True)\nFile \"/usr/local/lib/python2.7/dist-packages/docker/client.py\", line 93, in _result\n  self._raise_for_status(response)\nFile \"/usr/local/lib/python2.7/dist-packages/docker/client.py\", line 89, in _raise_for_status\n  raise errors.APIError(e, response, explanation=explanation)\ndocker.errors.APIError: 404 Client Error: Not Found (\"client and server don't have same version (client : 1.15, server: 1.14)\")\nNot sure if that helps you.\n. The issue is that I had not realized that there was a distinction between the docker version and the docker api version. Perhaps it might make sense to change \"The version of the API the client will use\" to \"The version of the API the client will use (not the version of docker)\" in the docs to emphasize the difference. Thanks.\n. ",
    "vladfr": "Not a bad idea, I will. Should I put a link to it in the main file?\nOn 12 Nov 2014 20:56, \"Joffrey F\" notifications@github.com wrote:\n\nHi, thank you for the submission! I think these instructions belong in\ntheir own file, if you can make this happen I'll be happy to merge!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-py/pull/394#issuecomment-62772745.\n. Oops, made a mistake, I'll fix and come back in a bit.\n. I saw that this is already done, thanks, sorry about the fuss.\n. \n",
    "mr337": "@ureyes84 \nWhen this happens are you having issues killing the container. I have encountered this and once this happens leaves Docker in a really slow state where Docker commands either hang or super slow.\nExample\n```\nubuntu@ip-10-171-68-239:/var/log/upstart$ time docker rm e26a081ce5ec\ne26a081ce5ec\nreal    0m44.090s\nuser    0m0.020s\nsys     0m0.001s\n```\nand that was with everything idling.\n. Finally found the root issue that was messing me up. We are running Docker on EC2 instances, without realizing it we was taxing the EBS volume by requesting too many writes and reads and it couldn't burst anymore without getting a bigger EBS volume. So in the end boiled back to overloading the host machine.\n. ",
    "weiwongfaye": "it happens to me today, there are 50 container start, and then the message appear. system became very slow.\n. ",
    "vitalyisaev2": "I face this issue regularly because we use docker-py for launching of dozens (and even hundreds) containers every day from our Jenkins. This is what I usually see:\nFile \"/usr/local/bin/package-create\", line 9, in <module>\n    load_entry_point('docker-buildscripts==0.2.39', 'console_scripts', 'package-create')()\n  File \"/usr/local/lib/python2.7/dist-packages/buildscripts/create.py\", line 154, in main\n    manager, runtime_config, aptly, proj, kwargs = build_params(options)\n  File \"/usr/local/lib/python2.7/dist-packages/buildscripts/create.py\", line 125, in build_params\n    assert(all(map(manager.image_exists, runtime_config['images'].values())))\n  File \"/usr/local/lib/python2.7/dist-packages/buildscripts/proxy/_docker.py\", line 22, in image_exists\n    return self.images(name=image_name)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/client.py\", line 640, in images\n    res = self._result(self._get(self._url(\"/images/json\"), params=params),\n  File \"/usr/local/lib/python2.7/dist-packages/docker/client.py\", line 106, in _get\n    return self.get(url, **self._set_request_timeout(kwargs))\n  File \"/usr/lib/python2.7/dist-packages/requests/sessions.py\", line 477, in get\n    return self.request('GET', url, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/requests/sessions.py\", line 465, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/usr/lib/python2.7/dist-packages/requests/sessions.py\", line 573, in send\n    r = adapter.send(request, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/requests/adapters.py\", line 433, in send\n    raise ReadTimeout(e, request=request)\nrequests.exceptions.ReadTimeout: UnixHTTPConnectionPool(host='localhost', port=None): Read timed out. (read timeout=60)\nDocker==1.6.0\ndocker-py==1.2.3\n. Our Jenkins builds dozens of images every day using the docker-py-powered scripts. It would be very convenient to have an exception thrown in docker.Client.build when something goes wrong.\n. Thanks for a prompt response! Unfortunately generator doesn't work for me. I started the mentioned process and launched two Ipython sessions simultaneously.\nGenerator blocks the program and prints nothing. I had to interrupt them after a 2 minutes:\n``` python\nIn [4]: for log in client.logs(container_id, stream=True): print log\n^C---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\n in ()\n----> 1 for log in client.logs(container_id, stream=True): print log\n/home/isaev/prog/paas-devel/venv_dockerized-packager/local/lib/python2.7/site-packages/docker/client.pyc in _multiplexed_response_stream_helper(self, response)\n    210             if not length:\n    211                 break\n--> 212             data = response.raw.read(length)\n    213             if not data:\n    214                 break\n/home/isaev/prog/paas-devel/venv_dockerized-packager/local/lib/python2.7/site-packages/requests/packages/urllib3/response.pyc in read(self, amt, decode_content, cache_content)\n    184                 else:\n    185                     cache_content = False\n--> 186                     data = self._fp.read(amt)\n    187                     if amt != 0 and not data:  # Platform-specific: Buggy versions of Python.\n    188                         # Close the connection when no data is returned\n/usr/lib/python2.7/httplib.pyc in read(self, amt)\n    547 \n    548         if self.chunked:\n--> 549             return self._read_chunked(amt)\n    550 \n    551         if amt is None:\n/usr/lib/python2.7/httplib.pyc in _read_chunked(self, amt)\n    589         while True:\n    590             if chunk_left is None:\n--> 591                 line = self.fp.readline(_MAXLINE + 1)\n    592                 if len(line) > _MAXLINE:\n    593                     raise LineTooLong(\"chunk size\")\n/usr/lib/python2.7/socket.pyc in readline(self, size)\n    474             while True:\n    475                 try:\n--> 476                     data = self._sock.recv(self._rbufsize)\n    477                 except error, e:\n    478                     if e.args[0] == EINTR:\nKeyboardInterrupt: \n```\nYou can think that container is dead or produces no information. Let's make sure that its output is not empty:\npython\nIn [13]: while True:\n   ....:     t = client.logs(container_id)\n   ....:     print len(t)\n   ....:     time.sleep(1)\n   ....:     \n2088481\n2088612\n2089018\n2089576\n2089940\n2090938\n2113254\n2114289\n2115393\n2117775\n2145325\n...\nI've no idea why the buffer is growing continuosly but I have no output from generator.\n. Hi Tomas, I use the most recent version of docker-py:\nbash\n$ pip freeze | grep 'docker\\|requests'\ndocker-py==1.0.1-dev\nrequests==2.4.3\nAnd I faced with the same issues with docker 1.0.0.\n. Thank you, Tomas, I will try to extract minimal working example of the our build system. It will take some time. \nBut first could you please clarify, if it's important to create container in detached or tty mode in order to get logs in the proper way. The following snippet doesn't contain enough information about the system, but probably I just have bad params, could you please check them:\n``` python\n    # Launching new container\n    create_params = {\n        \"name\": session,\n        \"image\": cfg['images'][\"buildenv\"],\n        \"volumes\": [cfg['package_exchange_dir']],\n        \"environment\": {\n            \"BRANCH\": \"MAILPAAS-42\",\n            \"BUILD_TARGET\": component,\n            \"PACKAGES_DIR\": session_dest\n        },\n        \"stdin_open\": True,\n        \"tty\": True,\n        #\"detach\": True,\n        \"command\": \"fetch\"\n    }\n    start_params = {\n        \"links\": [(cfg['containers']['reprepro'],)*2],\n        \"binds\": {\n            cfg['package_exchange_dir']:\n            {\n                'bind': cfg['package_exchange_dir'],\n                'ro': False,\n            }\n        }\n    }\nbuildenv_id = manager.client.create_container(**create_params)\nmanager.client.start(buildenv_id, **start_params)\nprint(\"Container '{}' has been started. Waiting...\".format(buildenv_id['Id']))\n\n# The program is hanging here... \nfor log in manager.client.logs(buildenv_id['Id'], stream=True):\n    print log\n\n```\n. Thanks, that's worked for me. I didn't see this in docs. Can I modify configs in order to make API respond faster? It seems like the buffer is too big, and I need to wait about a minute before the huge amount of logs will be printed. \n. The same error in my case. \n``` sh\nroot@44ca347ef5d4:/# pip install --user docker-py\nDownloading/unpacking docker-py\n  Downloading docker-py-1.1.0.tar.gz (42kB): 42kB downloaded\n  Running setup.py (path:/tmp/pip_build_root/docker-py/setup.py) egg_info for package docker-py\nDownloading/unpacking requests>=2.5.2 (from docker-py)\n  Downloading requests-2.5.3-py2.py3-none-any.whl (468kB): 468kB downloaded\nRequirement already satisfied (use --upgrade to upgrade): six>=1.3.0 in /usr/lib/python2.7/dist-packages (from docker-py)\nDownloading/unpacking websocket-client>=0.11.0 (from docker-py)\n  Downloading websocket_client-0.26.0.tar.gz (189kB): 189kB downloaded\n  Running setup.py (path:/tmp/pip_build_root/websocket-client/setup.py) egg_info for package websocket-client\nDownloading/unpacking backports.ssl-match-hostname (from websocket-client>=0.11.0->docker-py)\n  Downloading backports.ssl_match_hostname-3.4.0.2.tar.gz\n  Running setup.py (path:/tmp/pip_build_root/backports.ssl-match-hostname/setup.py) egg_info for package backports.ssl-match-hostname\nInstalling collected packages: docker-py, requests, websocket-client, backports.ssl-match-hostname\n  Running setup.py install for docker-py\nRunning setup.py install for websocket-client\n    changing mode of build/scripts-2.7/wsdump.py from 644 to 755\nchanging mode of /root/.local/bin/wsdump.py to 755\n\nRunning setup.py install for backports.ssl-match-hostname\nSuccessfully installed docker-py requests websocket-client backports.ssl-match-hostname\nCleaning up...\nroot@44ca347ef5d4:/# pip\nTraceback (most recent call last):\n  File \"/usr/bin/pip\", line 9, in \n    load_entry_point('pip==1.5.6', 'console_scripts', 'pip')()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 356, in load_entry_point\n    return get_distribution(dist).load_entry_point(group, name)\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2476, in load_entry_point\n    return ep.load()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2190, in load\n    ['name'])\n  File \"/usr/lib/python2.7/dist-packages/pip/init.py\", line 61, in \n    from pip.vcs import git, mercurial, subversion, bazaar  # noqa\n  File \"/usr/lib/python2.7/dist-packages/pip/vcs/mercurial.py\", line 9, in \n    from pip.download import path_to_url\n  File \"/usr/lib/python2.7/dist-packages/pip/download.py\", line 25, in \n    from requests.compat import IncompleteRead\nImportError: cannot import name IncompleteRead\n```\n. Thanks! \n. ",
    "jpetazzo": "Following up our Slack convo: if an API client doesn't specify a version (i.e. if it connects to the unversioned endpoint), the behavior will be less deterministic.\nLet's see what happens when Docker rolls out a new feature requiring an API change (the example I had in mind was \"links\"), you upgrade your client library, but you haven't upgraded your daemons yet (or, more realistically: not all of them). If you use an unversioned endpoint, you will still be able to start containers which don't use links (good!), but when you will try to start a container with links, it will fail if the daemon doesn't support them. It will be the job of the daemon to provide an explanatory error message (e.g. in that case, that there is an unknown parameter \"links\"). If you use a versioned endpoint (the current behavior), the client will immediately bail out at the first command, and you will immediately know that something is amiss.\nI liken this to a \"compile time\" vs \"run time\" check. Of course it's annoying when you get an error because your client lib is more recent than your server lib; but IMHO it's better than starting to work and suddenly break when you use a new feature.\nNow, from an usability point of view, if people want to be able to set the API version used by their clients without having to patch all their code, we could suggest the use of an environment variable (e.g. something like DOCKER_API_VERSION), to set a specific version or e.g. versionless, and encourage supporting this variable in all existing API libraries.\n. ",
    "gzur": "But how about allowing version to be set to 'auto' - in which case the API uses the newest version the daemon supports?\n. ",
    "michaelbarton": "This issue is somewhat old, I'd like to add a few points because I think API\nversioning is still relevant.\nI was working on some code that I hadn't worked on in a while and I immediately\ngot an error when I ran the tests for the first time without changing anything:\n``` python\n    def get_image_tags(docker_dict):\n        \"\"\"\n        Returns list of all Docker image names with and without tags.\n        E.g. includes both python and python:2.7\n        \"\"\"\n        return reduce(lambda acc, x: acc + [x, x.split(\":\")[0]],\n\n          docker_dict['RepoTags'], [])\n\nE       TypeError: reduce() arg 2 must support iteration\n```\n\nAs I'm familiar with Docker, I assumed that this might be because code I had\nwritten was trying to use an API field that had changed between versions and I\nlikely had a newer version of Docker installed. Looking through the docker-py\nissues lead me to this one and that I could use the following code to fix my\nproblem:\npython\ndef client():\n    args = docker.utils.kwargs_from_env(assert_hostname = False)\n    args['version'] = \"1.12\"\n    return docker.Client(**args)\nWe're working on [wrapping bioinformatics software in docker images][] and we\nhave a python CLI that uses the docker/docker-py library to manage this. I\nthink the solution we'll follow is to specify a specific version of the Docker\nAPI in this client and then ask that users have a least this version of Docker\ninstalled. This should prevent API errors such as the one above from occuring.\nThat's all I have to add. I don't have any specific requests, however I thought\nit worth adding my opinion should this conversation resurface again in future.\n. From reading the source code the default timeout for the docker-py client is 60 seconds. In #1009 it is described that the interval for stats collection is 1 second. Therefore should it be possible to collect 60 intervals of docker stats per minute or (as I assume might be the case) that the processing of the cgroup data that will push the time required to greater than one second and this could be causing the ReadTimeoutError?\n. I will close this issue as #1195 describes the problem much better.\n. I initially opened #1194  because I seemed to having a similar problem, but this github issue frames the problem much better than I did.\n. If I use {'network_disabled' : False} when using the create_container method this will fix this?\n. I think there is a chance of a race condition between checking whether a container is still running and then subsequently trying to get the Docker stats for it. An alternative might be to break if the container is no longer running, and just pass in the except block, I think otherwise there could be the chance that is there really is a network timeout error and you stop collecting metrics from the container when it is still running.\nI mention this because I spent all of yesterday struggling with the same problem.\nhttps://github.com/bioboxes/bioboxes-py/blob/master/biobox/cgroup.py\n. Thanks for both of your input here, this has been very useful in helping me\nupdate my code.\n. Would you mind updating this issue again when the underlying engine bug is resolved? Thank you.\n. Was the bug in the Docker engine causing this eventually resolved? Is there a linking issue?. ",
    "TimothyKlim": "\nCan you explain your thought process\n\nOur ansible with latest builds of docker and docker-py can't build docker images without nocache and cause some bugs. Option nocache didn't work, REST server just ignore it.\n. ",
    "tbronchain": "Thanks!\nIt seems like to disable TLS, the only solution is to change the configuration file within the VM (any other solution in mind?).\n-- Forget what I just said, it seems it's possible to disable TLS by editing the VM configuration file.\nI'll use the host file solution.\n. Actually after a lot of try yesterday, I still didn't manage to get it working. But I think it's a boot2docker issue rather than docker-py (reason why I closed this issue and opened another on in their repo - which didn't have been answered yet).\nThe problem is, boot2docker always require a TLS authentication, unless I add this DOCKER_TLS=no in a file within the VM (it doesn't work actually if you add it to the VM config file). This is really annoying because, in my case (I want to automate boot2docker deployments within a python software), well, I can't SSH the VM! (or it would be a very dirty hack). Using export DOCKER_TLS_VERIFY= then don't fix the issue (the boot2docker VM refuses the connection).\nI tried to add the entry to my host files and, surprisingly, it didn't work. It seems anyway like not being a reasonable fix for my needs.\nThanks for the help anyway!\n. Same problem here. I'm using docker-py to develop an automation tool,\nautomatically editing /etc/hosts file doesn't fit my needs.\nAlso, I may have multiple VMs running, and in this case, the hosts fix is\ninvalid.\nOn Wed, Jan 7, 2015 at 3:13 AM, Gregory Szorc notifications@github.com\nwrote:\n\nYes, updating /etc/hosts is a workaround. But not every user has\npermission to update /etc/hosts. This includes my use case, where I'm\nusing docker-py as part of testing automation and the Docker hostname is\nnot static. Requiring root or write privileges to /etc/hosts is a\nnon-starter.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-py/issues/406#issuecomment-68916062.\n. \n",
    "defermat": "@tbronchain did you try:\nexport DOCKER_TLS_VERIFY=\n. ",
    "anentropic": "@cglewis workaround works for me\n. assert_hostname=False seems to work without the /etc/hosts entry in recent boot2docker versions\n. sorry, I see now this a duplicate of https://github.com/docker/docker-py/issues/406\n. Looks like symptom of same issue here: https://github.com/docker/compose/issues/890\n. moved to https://github.com/docker/compose/issues/4569  ...seems more their issue. ",
    "indygreg": "Modifying /etc/hosts is not a workaround available to everybody.\nDisabling verification completely defeats the point.\nThe reason why assert_hostname=False isn't working is because of a bug in urllib3. See https://github.com/shazow/urllib3/issues/524.\nThis package is using urllib3 via requests. Downgrading requests to 2.4.3 will make assert_hostname=False work again (the first buggy version of urllib3 was included in requests 2.5.0). It will be safe to upgrade once urllib3 releases a fix and once requests imports that fix into their distribution. Alternatively, docker-py could change the import logic in docker.ssladapter.ssladapter to favor urllib3 over requests.packages.urllib3.\n. I spent a good chunk of time yesterday stepping through the debugger until I found some certificate verification code doing something it shouldn't have been with assert_hostname=False. I can tell you with confidence there is a bug in urllib3. The issue on their tracker (shazow/urllib3#524) all but confirms it. See also #424, where someone else recently encountered this same issue.\nOn my machine, I can guarantee that assert_hostname=False used to work but no longer works with modern versions of urllib3/requests.\nThere may be co-dependency on Python 2.7.9 to reproduce this bug. When I was stepping through the code, I believe urllib3 was interacting with parts of ssl that weren't introduced until 2.7.9.\n. Yes, updating /etc/hosts is a workaround. But not every user has permission to update /etc/hosts. This includes my use case, where I'm using docker-py as part of testing automation and the Docker hostname is not static. Requiring root or write privileges to /etc/hosts is a non-starter.\n. You can work around this in docker-py. Read https://github.com/docker/docker-py/issues/406#issuecomment-68078631 again.\nYou could also throw a big giant warning in the docs that urllib3/requests is busted so future souls don't have to search out this issue.\n. Also, this is purely a Python bug, not a boot2docker bug.\n. requirements.txt files are typically intended for \"complete environments.\" In many cases, docker-py is used as a library as part of a larger tool. As such, its requirements.txt are likely completely ignored. A better way to do version enforcement is with install_requires in setup.py. See https://packaging.python.org/en/latest/requirements.html. Please note that it is not a good practice to pin versions in install_requires (like it is in requirements.txt).\n. I was confounded by this issue as well. I spent several hours trying to grok what's going on and I'm convinced there are bugs or limitations in CPython, requests, or urllib3 at play here. I got as far as the C code behind Python's ssl package before I gave up in search of other solutions.\nInstead of downgrading OpenSSL versions (that should be a big security red flag), try installing the requests[security] or ndg-httpsclient package instead. This will pull in pyasn1, pyOpenSSL, cryptography and a bunch of other packages. tl;dr is ndg-httpsclient installs an HTTPS client that speaks to pyOpenSSL instead of the built-in ssl package and for whatever reason, doesn't exhibit the issues validating the peer certificate. This solution is working fine for me with Python 2.7.10, openssl 1.0.2c (Homebrew installed), docker-py 0.7.1, and requests[security] 2.7.0 on OS X 10.10.3.\n. ",
    "mcfiredrill": "Running into this issue via fig.\n. ",
    "mparker17": "Is it worth re-opening this issue until it's been fixed upstream?\nI can confirm that I'm getting the error on Mac OS/X 10.10.1 (14B25) \"Yosemite\", running Python 2.7.8, Boot2Docker-cli version: v1.4.1 (Git commit: 43241cb), fig 1.0.1, and VirtualBox 4.3.20 r96996.\n. ",
    "dzwicker": "using the /etc/hosts fix works. But make sure to use \nexport DOCKER_HOST=tcp://boot2docker:2376\nafter the call to\n$(boot2docker shellinit)\nas it reexport the DOCKER_HOST with the ip i think.\n. ",
    "willdurand": "Hi! It would be nice if you could tag a patch release :)\n. ",
    "pchico83": "+1, this would mitigate #419\n. ",
    "rmohr": "I forgot to mention that I am using python  3.3.2 on fedora 20\n. Thanks for pointing me to #336. I have tested everything against the new master with #336 merged and I still experience this when using Python 3. I have rebased my changes and an integration test is included.\nThe client.attach_socket method is returning a SocketIO object instead of the raw socket on Python 3, but I don't think that it will cause much problems, because everything relying on that method on Python 3 is broken anyway.\nYou can reproduce my problem quite easily by executing this:\n``` python\nimport docker\nclient = docker.Client()\nclient.pull('busybox:latest')\ncontainer = client.create_container(\n        image='busybox:latest',\n        command=\"/bin/sh\",\n        detach=False,\n        stdin_open=True,\n        tty=True,\n        )\nclient.start(container)\nsocket = client.attach_socket(container, ws=False)\nprint(socket.fileno())  # prints -1 on Python 3 without this pull request applied\n```\nHope this is of some help.\n. :+1: \n. In Python2 the socket stays open anyway. But it would be cool to get rid of the try-except block. I will look into it.\n. ",
    "dlorenc": "@proppy \nCool. I don't know much about auth, but let me know if I can help at all. We'd really like to see this supported before identity auth becomes the default in boot2docker.\n. @public Did you get anywhere on this?\n. ping\n@public, @bfirsh\nIs anyone looking at this?/\n. It's stalled in general, or just for docker-py? Is machine going to use TLS certs instead?\nIs there a discussion somewhere I can see?\n. Sounds good. I was mainly concerned about docker-py not working with docker machine. If docker machine is using TLS this should be fine.\n. Ping :)\n. Hey, any idea when the next release will be?\n. Hey,\nWe're using something like this to package up a docker context and stream it to another machine ourselves before running a build, and to do a few in-memory operations on the tar instead of writing it to disk. Getting this upstreamed will let us drop our \"fork\" of the tar function.\nIt's not a huge amount of code, but it would be nice to make sure we're using the exact same tar logic as docker-py over time.\n. ",
    "public": "Not quite yet. It's very much a prototype fresh from my keyboard right now. I should have it turned into something resembling an actual library in a few days. Would be good to have some feedback regarding getting it integrated with other things then though :-) \n. Everything is in place in https://github.com/pyca/cryptography for this to work now and I can make connections using libtrust identities.\nI still need to figure out how to verify the servers certificate properly. I am having some issues building a transport adapter for requests that can verify the connection before communicating with the server. If anyone has any experience with that it would be very helpful :) \n. ",
    "nir0s": "You can always install requests prior to installing docker-py and get the desired version.\n. So your request is not to allow you to use a different version of requests. It's that docker-py should support the latest version...\n. I've been using Sphinx for a long while now and it's great. I think docstrings make everything more easily understandable. You don't have to skip to the md files everytime you wanna understand what's going on within the code. Sphinx does require that you write RST (other than for the auto-generated API pages) unless you're willing to work with pandoc (I do).\nBottom line is, I'd definitely move from mkdocs to sphinx.\n. Some comments.\n- Need to add unit tests\n- Need to add integration tests\n- I have code in place to add a maximum number of stats messages to return in case you don't want to stream indefinitely. Thinking whether this is relevant or not.\n. I'll see if I get to the tests soon.. if not, i'd appreciate the help.\nRegarding the max number of messages.. I was just wondering if it would be useful. Don't wanna add useless code :)\n. Well then.. I added a unittest and am now using stream helper.\nI removed aggregation until I have a clearer view of how to implement this.\nThe reasoning behind it is that you might want to aggregate stats and return only then return them so that you're not blocked by the stream.\nObviously, my previous implementation was blocking. Comes to prove that you shouldn't work long hours :) makes your judgment fuzzy.\n. oops. my bad. will do.\n. ",
    "nickjoyce-wf": "@nir0s Yes, that is a possible workaround, but that doesn't necessarily mean that docker-py will support that version\n. @nir0s I think docker-py should support a minimum version of requests, something like requests >=2.2.1, <=3.0 (in the requirements.txt).\nThoughts?\n. Looks like that solves this issue - thanks!\n. ",
    "Julian": "FWIW I am +1 on this being a separate package, and I've already started assembling some helpers in my own app that I've used -- to add them to your list perhaps, temporary_image and temporary_container, which create images and containers and delete them in a contextmanager. Possibly by the time I'm done I'll have shoved them into a separate library myself anyhow.\n. Nice! I've got some as well, although it's a bit stuck on not having an\neasy way to write tests for it unfortunately, but maybe starting to put\nsome stuff in the same place would get this kick started, yeah.\nOn Jan 12, 2015 6:44 PM, \"Hamilton Turner\" notifications@github.com wrote:\n\n@Julian https://github.com/Julian great ideas! FWIW I've got some basic\ncode but it's a bit stalled, if anyone wants to take point on a PR for this\nI'm happy to deposit my current code into a gist in case it's useful\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-py/issues/417#issuecomment-69669831.\n. +1, import_image currently doesn't seem to work at all on file-like objects, large or otherwise.\n. Possibly, I'm still fighting through a couple of issues to get my actual app to work, but after that I'll have a look.\n. Awesome, thanks for the quick turnaround, appreciated!\n. stream=False actually doesn't work at all seemingly, it gets overridden by https://github.com/docker/docker-py/blob/master/docker/client.py#L438-L439\n. Oh. OK -- is the deprecation documented somewhere, I don't see it at http://docker-py.readthedocs.org/en/latest/api/?\n. Another commit here with some more stuff as we've worked on this.\n. Here's mine:\n\n```\n\u2299  ~/.local/share/virtualenvs/runner/bin/pypy --version\nPython 2.7.8 (10f1b29a2bd21f837090286174a9ca030b8680b2, Feb 05 2015, 17:51:14)\n[PyPy 2.5.0 with GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.56)]\n\u2299  ~/.local/share/virtualenvs/runner/bin/pypy -m pip freeze\nbackports.ssl-match-hostname==3.4.0.2\nbetterpath==0.2.2\nblessings==1.6\nbpython==0.15.dev85\ncffi==0.8.6\ncharacteristic==14.3.0\ncryptography==0.7.2\ncurtsies==0.2.3\ndocker-py==1.0.0\ndocker-runner==0.1.0.dev0\nenum34==1.0.4\ngreenlet==0.4.5\nmock==1.0.1\nndg-httpsclient==0.3.3\npudb==2015.2\npyasn1==0.1.7\nPygments==2.0.2\nq==2.5\nreadline==6.2.4.1\nrequests==2.4.3\nsix==1.9.0\ntoml==0.9.0\nTwisted==15.0.0\nurwid==1.3.0\nvcversioner==2.14.0.0\nwcwidth==0.1.4\nwebsocket-client==0.26.0\nwheel==0.24.0\nzope.interface==4.1.2\n```\n(I get the same flip flop of not working with a KeyError if pyOpenSSL is installed, and SSL verification failures if it is).\n. ",
    "thomasem": "I'm trying to wrap my head around whether this issue matches with the problem I'm having. Currently docker-py returns fairly painful-to-use responses from the API for a Python application. Even worse if you're writing against multiple versions of the Docker/Swarm API where things may have dramatically changed from version to version. Example would be how Docker Swarm going from API version 1.21 to 1.22 changed from DriverStatus to SystemStatus and broke a lot of integration.\nI would prefer being able to parse any version of the API that's supported by docker-py to a common, documented, format for consumers of docker-py. This would make it easier to integrate with a variety of versions of Docker that you may be talking to. This is especially useful for a provider of a Docker service.\nMy current solution is a parser module where you essentially do:\n``` python\nimport docker\nimport docker_parse\nclient = docker.Client(..., version='auto')\nresp = client.info()\nfriendly_info = docker_parse.parse(client.version(), \"info\", resp)\nSwarm case\nnodes = friendly_info.nodes\nimages_count = friendly_info.image_count\n```\nThoughts?\n. ",
    "cpuguy83": "This shouldn't be an issue on < 1.4\nIn 1.4 passing in HostConfig (ie, bind mounts, volumes-from, other host specific things) on start is deprecated... that said you can still pass in hostconfig on start.\n.. However, in 1.4 volumes are now created when at container creation instead of on container start... so if the Binds parameter is not passed in on create, the bind-mounts are not created.\n. I'm writing some tests for this right now on the docker side.\nBut I'm pretty sure that if a volume was passed into the create command (without specifying the binds in the HostConfig), if you then pass the Binds parameter on start, it will be ignored, since a volume would already be initialized for the given volume path.\n. I'm going to work on a fix on the Docker side since this was an unexpected breaking change.\nHowever, docker-py should definitely be updated to pass all container configuration on create.\n. No, just deprecated.  The issue is with the volume initialization.\nI'll have a PR up momentarily that fixes this on the Docker side.\n. Issue is detailed here: docker/docker#9628\n. And it's actually not deprecated, this was bad information.\n. \"whoop whoop\"\n. Just to copy over what I said in moby/moby, there should be nothing to change in docker-py (or any API client). If docker-py is broken because of a proposed change in moby, then the proposed change is incorrect.. ",
    "bercab": "\n.. However, in 1.4 volumes are now created when at container creation instead of on container start... \nso if the Binds parameter is not passed in on create, the bind-mounts are not created.\n\nAre you sure? , In my example, binds parameter is passed on start(), not on create_container(), and it works.\nAny way, seems clear that API does not work as stated on docs, at least with docker v1.4\n. Yes,  I agree, this is what happens in my test.\nBut also, I see that if you don't specify volumes nor binds to create(), and only pass binds to start(), it works (see /mnt/vol1 in example above)\nConfirmed, only happens in docker >=1.4 , I have modified my issue description, sorry.\n. ",
    "mlosev": "this issue also breaks ansible docker module (\n. ",
    "romanlevin": "Described in (much) more detail in #998.\n. Hey @shin- \nActually I am calling calling it with rm=True. That option (according to both API docs and docker-py docs) removes intermediate containers. What I get is a leftover image.\nAnd unless I'm missing something about docker-py error handling, there's no way to detect that the build failed or why.\n. > You would have to parse the stream and detect the line(s) that indicate failure (typically a JSON object containing an error key)\nAh, that's what I was looking for, thanks!\nI know the docs are pretty minimalist, but I think it would be helpful to have that noted there.\n. ",
    "hammerdr": "By end of the list, do you mean documentation, params hash, or method arguments? :)\n. Fixed!\n. ",
    "cacois": "My fault, thanks guys. I was confused by the example code in the docker-py readme:\nhttps://github.com/kiorky/docker-py#api\nwhich gives the following code:\ndocker.Client(base_url='unix://var/run/docker.sock', version=\"1.4\")\nMaybe the readme should be changed to reflect the current version, or omit the version parameter?\n. My fault again. Thanks for the help!\nOn Mon, Dec 15, 2014 at 7:07 PM, Joffrey F notifications@github.com wrote:\n\nThe fork you're linking to isn't up to date :( our docs\nhttp://docker-py.readthedocs.org/en/latest/api/ leave out the version\nparameter in the example as you suggest.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-py/issues/422#issuecomment-67090755.\n. \n",
    "sebgoa": "hum, not quiet. I added it, I can ping boot2docker but still no luck. Same error\n. fwiw:\nBoot2Docker version 1.3.2, build master : 495c19a - Mon Nov 24 20:40:58 UTC 2014\nDocker version 1.3.2, build 39fa2fa\n. yes that did it, thanks\n. ",
    "abanna": "Updates have been made.\n. ",
    "pranavs18": "Thanks for the quick fix dims :+1: \n. ",
    "dbbaskette": "Apologies.    I had 1.6 in my code and I kept reading it as 1.16.   That was the root cause\n. ",
    "objectified": "+1 for this patch. I've applied this patch in a project I'm working on, and it works great. Do keep in mind that the newline for every log line is not stripped by docker-py, so simply printing/logging the line will add an extra newline (unless you explicitly deal with it).\n. While working with this patch some more, I've discovered that this often leads to a read timeout when no data is being returned for a while (for example a long running Docker process).\nFile \"/Users/lbrouwer/dev/vdist/lib/python2.7/site-packages/docker/client.py\", line 323, in _multiplexed_response_stream_helper\n    header = response.raw.read(STREAM_HEADER_SIZE_BYTES)\n  File \"/Users/lbrouwer/dev/vdist/lib/python2.7/site-packages/requests/packages/urllib3/response.py\", line 210, in read\n    raise ReadTimeoutError(self._pool, None, 'Read timed out.')\nrequests.packages.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='boot2docker', port=2376): Read timed out.\n. Thanks, the updated patch seems to fix this problem. I ran a number of tests with running/stalling/running/stalling/etc. behavior inside a few different docker containers, and I could not reproduce the original problem.\n. Good point about Python 3 - I've just tested this functionality using Python 3, and one thing I noticed while doing so is that the yielded data is a byte object, not a string. I'm not sure if that's intentional, but since this functionality is used to get logging from a container, I'd assume a string type would make the most sense. It can be easily fixed by replacing:\nyield data\nwith:\nyield data.decode('utf-8')\nBut again, I don't know whether this is intentional.\n. @shin- Fair enough.\n@dims Any chance you could elaborate on the point raised by @shin about replacing the original method? It'd be great if this pull request could be merged.\n. Thanks @dims ! I've tested it with my project on 2.7 and 3.4 as well, and it appears to work nicely.\n. @shin- Seems your input is processed now. If there's anything else related to this that needs testing or anything else that needs to be fixed, could you let us know? If this PR could be merged, that would be great.\n. @shin- did you have a chance to run your tests yet?\nSorry for hunting this one, I need this pull request merged so that my own project will work (which I can then open source).\n. ",
    "omribahumi": "Having another look at the code, I think we should replace\npython\n        if stream:\n            return self._stream_helper(response)\n        else:\n            output = self._result(response)\n            srch = r'Successfully built ([0-9a-f]+)'\n            match = re.search(srch, output)\n            if not match:\n                return None, output\n            return match.group(1), output\n(from https://github.com/docker/docker-py/blob/master/docker/client.py#L472)\nWith\npython\n        if stream and response.raw._fp.chunked:\n            return self._stream_helper(response)\n        else:\n            output = self._result(response)\n            srch = r'Successfully built ([0-9a-f]+)'\n            match = re.search(srch, output)\n            if not match:\n                return None, output\n            return match.group(1), output\n. Client version: 1.4.1\nClient API version: 1.16\nGo version (client): go1.3.3\nGit commit (client): 5bc2ff8\nOS/Arch (client): darwin/amd64\nServer version: 1.4.1\nServer API version: 1.16\nGo version (server): go1.3.3\nGit commit (server): 5bc2ff8\n. Thanks\n. ",
    "davibe": "which docker version are you running docker-py against ?\n. ",
    "spinus": ":-( that's very very sad news. Thanks.\n. @dims hey, I found that they implemented this feature recently.\nhttps://github.com/docker/docker/issues/9432\nhttps://github.com/docker/docker/pull/9208\ngit tag --contains 90928eb1140fc0394e2a79d5e9a91dbc0f02484c\nv1.4.0\nv1.4.1\nis there any hope to see this in docker-py then?\n. @pixelbombs you don't have to break compatibility. You can return Stream object or str like now it is done and add attribute \"exit_code\". Like http://amoffat.github.io/sh/#exit-codes\n. As a workaround I am using client.attach(logs=True, stream=True) - works ok.\n. yey :-)\n. @shin- good stuff. Thank you for explanations! I didn't know that.\n. ",
    "athoune": "I use somme ugly monkey patch, for that :\n``` python\nclass ExecuteFuture(object):\ndef __init__(self, client, command_id, flow):\n    self.client = client\n    self.command_id = command_id\n    self.flow = flow\n\ndef __iter__(self):\n    return self.flow\n\ndef wait(self):\n    if six.PY3:\n        return bytes().join(\n            [x for x in self.client._multiplexed_buffer_helper(self.flow)]\n        )\n    else:\n        return str().join(\n            [x for x in self.client._multiplexed_buffer_helper(self.flow)]\n        )\n\ndef inspect(self):\n    url = self.client._url('/exec/{0}/json'.format(self.command_id))\n    res = self.client._get(url)\n    self.client._raise_for_status(res)\n    return res.json()\n\ndef execute_return(self, container, cmd, detach=False, stdout=True, stderr=True,\n            stream=False, tty=False):\n    if utils.compare_version('1.15', self._version) < 0:\n        raise errors.APIError('Exec is not supported in API < 1.15')\n    if isinstance(container, dict):\n        container = container.get('Id')\n    if isinstance(cmd, six.string_types):\n        cmd = shlex.split(str(cmd))\ndata = {\n    'Container': container,\n    'User': '',\n    'Privileged': False,\n    'Tty': tty,\n    'AttachStdin': False,\n    'AttachStdout': stdout,\n    'AttachStderr': stderr,\n    'Detach': detach,\n    'Cmd': cmd\n}\n\n# create the command\nurl = self._url('/containers/{0}/exec'.format(container))\nres = self._post_json(url, data=data)\nself._raise_for_status(res)\n\n# start the command\ncmd_id = res.json().get('Id')\nres = self._post_json(self._url('/exec/{0}/start'.format(cmd_id)),\n                    data=data, stream=stream)\nself._raise_for_status(res)\n\nreturn ExecuteFuture(self, cmd_id, res)\n\n```\nThe answer is iterable, and you can inspect the command. Is it nice enough for a pull request?\n. ",
    "pixelbombs": "@athoune I'm not sure using a specific class/object for an alternate version of execute is the best way to go, I think ideally you'd want it as an option in the original execute function or as an alternate execute function.\nThat being said, I cannot thank you enough for sharing your code with us, it helped me create my own version of execute that returns the exit code instead of the output.\nThank you thank you thank you thank you!\n. For what it's worth, this is my solution to the problem (and a huge shoutout to @athoune for his answer which helped me). Named docker_call after our favorite subprocess function, it does virtually the same thing as Client.execute except it returns the return code instead if stream=True it will print the output of stdout and stderr to the console:\n``` python\ndef docker_call(cmd, docker_client, container_name, smart_replace=True, verbose=True):\n    if isinstance(cmd, six.string_types):\n        cmd = shlex.split(str(cmd))\n    data = {\n        'Container': container_name,\n        'User': '',\n        'Privileged':False,\n        'Tty': False,\n        'AttachStdin': False,\n        'AttachStdout': verbose,\n        'AttachStderr': verbose,\n        'Detach': False,  #Might need this option in the future.\n        'Cmd': cmd\n    }\nurl = docker_client._url('/containers/{0}/exec'.format(container_name))\nres = docker_client._post_json(url, data=data)\ndocker_client._raise_for_status(res)\ncmd_id = res.json().get('Id')\nres = docker_client._post_json(docker_client._url('/exec/{0}/start'.format(cmd_id)), data=data, stream=verbose)\ndocker_client._raise_for_status(res)\nfor i in docker_client._multiplexed_buffer_helper(res):\n    logging.debug(i)\nurl = docker_client._url('/exec/{0}/json'.format(cmd_id))\nres = docker_client._get(url)\ndocker_client._raise_for_status(res)\nreturn res.json().get('ExitCode')\n\n```\n. @TomasTomecek Nice! I knew that exec syscall didn't understand shell syntax, as I highlighted in the original post, but it didn't occur to me to use the additional parameters in the regular shell call inside the execute statement (I didn't even figure it would behave the same way).  Thanks for the help. That's a nice way to get around the issue.\n. > exec syscall doesn't understand shell syntax\nThat issue, which is the same issue subprocess.call and subprocess.check_output have since, as you put it, they don't understand shell syntax.\nWhich, technically, isn't an issue. Just an issue for me ;D\n. ",
    "blackfader": "docker1.4.1\n. ",
    "lorin": "@dims If I do that, docker-py throws the following exception:\ndocker.errors.DockerException: 'volumes_from' parameter has no effect on create_container(). It has been moved to start()\n. @dims Upgrading Docker seems to have resolved the issue. This was my version before the upgrade:\n$ docker version\nClient version: 1.3.3\nClient API version: 1.15\nGo version (client): go1.4\nGit commit (client): d344625\nOS/Arch (client): darwin/amd64\nServer version: 1.4.0\nServer API version: 1.16\nGo version (server): go1.3.3\nGit commit (server): 4595d4f\nThis was the version after the upgrade:\nClient version: 1.4.1\nClient API version: 1.16\nGo version (client): go1.4\nGit commit (client): 5bc2ff8\nOS/Arch (client): darwin/amd64\nServer version: 1.4.1\nServer API version: 1.16\nGo version (server): go1.3.3\nGit commit (server): 5bc2ff8\nWorks for me now. Thanks for the the help.\n. ",
    "jessfraz": "I'm wiping the device now to see if that helps.\n. ya it's always that test\n. hmmm its working now that I cleared the device so closing until I see it again\n. ",
    "kkleidal": "Updated PR so that index.docker.io is still not allowed, but registry.hub.docker.com is.  #453 \n. PR #453 merged\n. Replaced by #453 \n. ",
    "naxhh": "Ok... just after creating the ticket I saw that I understand badly the binds...\nIt should be\npython\ncli.start(container=container.get('Id'), binds={\n    '/Users/nax/code/TESTS/phansible-ci/ansible': {\n        'bind': '/var/www',\n        'ro': False\n    }\n})\nSorry for that.\n. ",
    "nmistry": "I can confirm the same issue.   Running the following command seems to be a sufficient workaround, as i already had 1.0.1j_1 installed prior to upgrading to 1.0.2.\nbrew switch openssl 1.0.1j_1\n. ",
    "jiangz1024": "It seems that requests 2.4.3 can't handle openssl 1.0.2 correctly.\n. Hi shin,\nSet verify to True explicitly works.\n. ",
    "RichardBronosky": "@kiorky care to explain a little more? I have:\npip install -U pyOpenSSL ndg-httpsclient pyasn1\nRequirement already up-to-date: pyOpenSSL in /usr/local/lib/python2.7/site-packages\nRequirement already up-to-date: ndg-httpsclient in /usr/local/lib/python2.7/site-packages\nRequirement already up-to-date: pyasn1 in /usr/local/lib/python2.7/site-packages\nRequirement already up-to-date: cryptography>=0.2.1 in /usr/local/lib/python2.7/site-packages (from pyOpenSSL)\nRequirement already up-to-date: six>=1.5.2 in /usr/local/lib/python2.7/site-packages (from pyOpenSSL)\nRequirement already up-to-date: cffi>=0.8 in /usr/local/lib/python2.7/site-packages (from cryptography>=0.2.1->pyOpenSSL)\nRequirement already up-to-date: setuptools in /usr/local/lib/python2.7/site-packages (from cryptography>=0.2.1->pyOpenSSL)\nRequirement already up-to-date: enum34 in /Library/Python/2.7/site-packages (from cryptography>=0.2.1->pyOpenSSL)\nRequirement already up-to-date: pycparser in /Library/Python/2.7/site-packages (from cffi>=0.8->cryptography>=0.2.1->pyOpenSSL)\nAnd I get this:\n```\n\n\n\nfrom docker.client import Client\nfrom docker.utils import kwargs_from_env\nclient = Client(kwargs_from_env())\nprint client.version()\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/local/lib/python2.7/site-packages/docker/client.py\", line 905, in version\n    return self._result(self._get(self._url(\"/version\")), True)\n  File \"/usr/local/lib/python2.7/site-packages/docker/client.py\", line 82, in _get\n    return self.get(url, self._set_request_timeout(kwargs))\n  File \"/usr/local/lib/python2.7/site-packages/requests/sessions.py\", line 469, in get\n    return self.request('GET', url, kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/requests/sessions.py\", line 457, in request\n    resp = self.send(prep, send_kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/requests/sessions.py\", line 569, in send\n    r = adapter.send(request, **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/requests/adapters.py\", line 362, in send\n    timeout=timeout\n  File \"/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py\", line 516, in urlopen\n    body=body, headers=headers)\n  File \"/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py\", line 304, in _make_request\n    self._validate_conn(conn)\n  File \"/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py\", line 722, in _validate_conn\n    conn.connect()\n  File \"/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/connection.py\", line 229, in connect\n    ssl_version=resolved_ssl_version)\n  File \"/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/contrib/pyopenssl.py\", line 249, in ssl_wrap_socket\n    ctx = OpenSSL.SSL.Context(_openssl_versions[ssl_version])\nKeyError: 5\n```\n. pyOpenSSL is causing a lot of problems lately. Similar issue here https://github.com/aws/aws-cli/issues/1240#issuecomment-86213200 and https://github.com/boto/botocore/issues/494\n\n\n\nI try to keep it out of my system install and remove it from any virtualenvs that have a problem. In many cases (like boto) that fixes it.\n. ",
    "steveninmanCubs": "I'm running into this same issue on a newly imaged machine running Python with Beatbox site collection to connect to Salesforce.com\nWorks fine on all servers except the new one I'm on.  Any other thoughts as these additional components installed via PIP didn't resolve my issue either.\n. Shin,\nAssuming you are referring to urllib3 libraries within the open ssl python library?  Do you know which call this would need to be set on?  There is no explicit setting for this currently in my installation, so assuming would have to add it to some method call as an additional attribute.\n. Any clue how to set verify to True within this ssl python method (lowest level of my cert error)?\n```\ndef do_handshake(self, block=False):\n    \"\"\"Perform a TLS/SSL handshake.\"\"\"\n    self._check_connected()\n    timeout = self.gettimeout()\n    try:\n        if timeout == 0.0 and block:\n            self.settimeout(None)\n        self._sslobj.do_handshake()\n    finally:\n        self.settimeout(timeout)\nif self.context.check_hostname:\n    if not self.server_hostname:\n        raise ValueError(\"check_hostname needs server_hostname \"\n                         \"argument\")\n    match_hostname(self.getpeercert(), self.server_hostname)\n\n```\n. After searching and googling profusely about TLS connections, etc, I noticed this weekend that my cmd.exe window (which I run the PIP commands from) was launching under C:\\Windows\\System32> and all of my vms were launching within my personal user directory.\nSo I searched for my roaming cmd.exe and when I opened that, it was to my personal user directory.\nAfter doing an uninstall and install of beatbox using PIP in the new cmd.exe, all of my Salesforce calls via Beatbox work like they used to.\nNow I can't fully explain why this is the case, but something tells me that the certs for this admin location are probably mismatched to my personal certificates.  I'm up and running again.\n. ",
    "wardi": "@shin- setting verify=True does not work here, but for some reason running from a new virtualenv (built from the system python AFAICT) I don't trigger this problem at all.\n. sorry the virtualenv was a red herring, different versions of some dependencies seemed to fix the problem. I'll dig further\n. @shin- unfortunately I'm no longer able to reproduce the problem (this was on a colleague's OSX machine) but I couldn't figure out why. It's rather frustrating. \n. ",
    "cancan101": "I installed docker-py from master and am I still having this issue.\nimport ssl; print(ssl.OPENSSL_VERSION)\nOpenSSL 1.0.2 22 Jan 2015\n. client = docker.Client(**kwargs_from_env(assert_hostname=False))\nclient.version()\nOS X: Yosemite, Python 2.7.9 (brew)\ndocker version\nClient version: 1.5.0\nClient API version: 1.17\nGo version (client): go1.4.1\nGit commit (client): a8a31ef\nOS/Arch (client): darwin/amd64\nServer version: 1.5.0\nServer API version: 1.17\nGo version (server): go1.4.1\nGit commit (server): a8a31ef\n. requests==2.5.3\n. ",
    "dvestal": "@shin- I'm not sure if it's still of any help or not, but since I don't see a response from @cancan101 I wanted to confirm that, in my case, downgrading OpenSSL solves my connection.  I was getting the SSL verification error with an up-to-date homebrew install of python 2.7.9 and openssl 1.0.2a-1.\nI uninstalled openssl and python, installed openssl101 from the homebrew/versions tap, changed the dependency in the python.rb formula to be openssl101 instead of openssl.  After building from source everything works fine.\n. ",
    "zugaldia": "In case it helps, I can also confirm that brew switch openssl 1.0.1j solves the issue for me (I was running 1.0.2a-1). No need to uninstall Python.\n. ",
    "crlane": "@zugaldia 's solution worked for me also.\n. ",
    "mvd7793": "@aanand I'm having the same problem. Version output:\n\u279c  python -V\nPython 2.7.9 \n\u279c  python -c 'import ssl; print ssl.OPENSSL_VERSION'\nOpenSSL 1.0.2 22 Jan 2015\n@zugaldia fix worked for me as well. Let me know if there's anything else I can do to help debug. :)\n. ",
    "rstuckey": "@indygreg 's workaround was the only solution that worked for me without breaking pip.\n. ",
    "tdsmith": "Please note that unless you set CPPFLAGS and LDFLAGS carefully when you installed pyOpenSSL's dependency cryptography, you are likely to have built pyOpenSSL against OS X's system-installed OpenSSL 0.9.8. You can verify the version of openssl against which cryptography is built by running python -c 'import OpenSSL.SSL as SSL; print SSL.SSLeay_version(SSL.SSLEAY_VERSION)'.\n. ",
    "posita": "Needs SvenDowideit/generate_cert#10 and boot2docker/boot2docker#808. See https://github.com/docker/compose/issues/890#issuecomment-126854371 et seq. for relevant discussion.\n. FYI, PR with fix for underlying issue submitted as boot2docker/boot2docker#1029.\n. > Feel free to send a PR!\nDone (~~#602~~...er, I mean...#605). :blush: \n. @shin, I've moved the check to @check_resource as you suggest (and squashed commits). Tests pass. Are you sure this won't have detrimental side effects elsewhere?\n. BTW, if this PR looks good for merge as-is, I can propose another that moves code like the following into @check_resource (see my line note on docker/client.py above):\npy\n    if isinstance(..., dict):\n        ... = ....get('Id')\nI think those changes are beyond the scope of this PR though.\n. > I'd rather have several short, one-purpose decorators than a long, single, multi-purpose one.\nI would tend to agree with this, except that these behaviors already seem very closely coupled throughout client.py. It seems (at least so far), everywhere that @check_resource is used, the code in the OP is repeated (or if it isn't, it was intended to be). Just look at how many places I removed those lines from client.py.\n\nif we put it here the condition will also be unnecessarily evaluated in the case of image-oriented functions. Seems like a bad practice even though the overhead is negligible.\n\nPerhaps, but this is already the case. Well before I came along, @check_resource already pulled double duty between containers and images. Perhaps it makes more sense to have nearly identical, but separately named decorators for images and containers (e.g., @check_image and @check_container). I can do that, but that seems to be a more significant change than this was intended to be.\nPlus, the overhead may be comparable when stacking the same two decorators everywhere:\npy\n@check_container\n@check_id\ndef attach(self, container, ...):\n    ...\nI know that wasn't your original point though.\nAll I really meant to do here was an incremental improvement towards DRY without disturbing the status quo too much. Ultimately, it's your call whether such a thing is worth merging. Either way, the fact that the code in the OP shows up in so many places is probably indicative that something needs to be done.\n. FYI, the original error is being raised here (note the path to the standard library from requests):\n``` py\n(Pdb) bt\n  ...\n  /.../lib/python2.7/site-packages/docker/client.py(325)init()\n-> super(AutoVersionClient, self).init(args, kwargs)\n  /.../lib/python2.7/site-packages/docker/client.py(77)init()\n-> self._version = self._retrieve_server_version()\n  /.../lib/python2.7/site-packages/docker/client.py(89)_retrieve_server_version()\n-> return self.version(api_version=False)[\"ApiVersion\"]\n  /.../lib/python2.7/site-packages/docker/api/daemon.py(78)version()\n-> return self._result(self._get(url), json=True)\n  /.../lib/python2.7/site-packages/docker/client.py(110)_get()\n-> return self.get(url, self._set_request_timeout(kwargs))\n  /.../lib/python2.7/site-packages/requests/sessions.py(477)get()\n-> return self.request('GET', url, kwargs)\n  /.../lib/python2.7/site-packages/requests/sessions.py(465)request()\n-> resp = self.send(prep, send_kwargs)\n  /.../lib/python2.7/site-packages/requests/sessions.py(573)send()\n-> r = adapter.send(request, *kwargs)\n  /.../lib/python2.7/site-packages/requests/adapters.py(370)send()\n-> timeout=timeout\n  /.../lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py(544)urlopen()\n-> body=body, headers=headers)\n  /.../lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py(341)_make_request()\n-> self._validate_conn(conn)\n  /.../lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py(761)_validate_conn()\n-> conn.connect()\n  /.../lib/python2.7/site-packages/requests/packages/urllib3/connection.py(253)connect()\n-> match_hostname(cert, self.assert_hostname or hostname)\n\n/.../lib/python2.7/ssl.py(273)match_hostname()\n-> raise CertificateError(\"hostname %r \"\n(Pdb) dnsnames\n['localhost']\n(Pdb) hostname\n'10.211.55.34'\n(Pdb) cert\n{'subjectAltName': (('DNS', 'localhost'), ('IP Address', '10.211.55.34')), 'notBefore': u'Nov 28 05:52:00 2015 GMT', 'serialNumber': u'930745814BB1FC51B9785C5811FCAA2D', 'notAfter': 'Nov 12 05:52:00 2018 GMT', 'version': 3L, 'subject': ((('organizationName', u'xyz.test'),),), 'issuer': ((('organizationName', u'xyz'),),)}\n```\n\nI found these:\n- http://docs.python-requests.org/en/latest/community/faq/#what-are-hostname-doesn-t-match-errors\n- https://stackoverflow.com/questions/18578439/using-requests-with-tls-doesnt-give-sni-support/18579484#18579484\nBut of course those don't address the same issue. If I do a pip install 'requests[security]' I get a different error (see this https://github.com/docker/docker-py/issues/816#issuecomment-160264212).\n. FYI, this may result in docker-py raising a very confusing error:\npy\n...\n  File \"/.../lib/python2.7/site-packages/docker/client.py\", line 325, in __init__\n    super(AutoVersionClient, self).__init__(*args, **kwargs)\n  File \"/.../lib/python2.7/site-packages/docker/client.py\", line 77, in __init__\n    self._version = self._retrieve_server_version()\n  File \"/.../lib/python2.7/site-packages/docker/client.py\", line 92, in _retrieve_server_version\n    'Invalid response from docker daemon: key \"ApiVersion\"'\ndocker.errors.DockerException: Invalid response from docker daemon: key \"ApiVersion\" is missing.\nThis is because _retrieve_server_version is confusing the source of the KeyError:\npy\n    ...\n    def _retrieve_server_version(self):\n        try:\n            # self.version raises a KeyError from pyopenssl here ...\n            return self.version(api_version=False)[\"ApiVersion\"]\n        except KeyError:\n            # ... which is then misinterpreted as having come from\n            # the attempt to access the \"ApiVersion\" key here\n            raise errors.DockerException(\n                'Invalid response from docker daemon: key \"ApiVersion\"'\n                ' is missing.'\n            )\n        ...\nIt should probably be rewritten. Here's one suggestion:\npy\n    def _retrieve_server_version(self):\n        try:\n            res = self.version(api_version=False)\n        except Exception as e:\n            raise errors.DockerException(\n                'Error while fetching server API version: {0}'.format(e)\n            )\n        try:\n            return res[\"ApiVersion\"]\n        except KeyError:\n            raise errors.DockerException(\n                'Invalid response from docker daemon: key \"ApiVersion\"'\n                ' is missing.'\n            )\n. This if isinstance ... block wasn't part of the original issue, but is included to provide consistency with the inspect_container() method.\n. if isinstance ... block reproduced for inspect_image(). See note below.\n. This isn't required to fix #602, but the change is make this method consistent with inspect_container.\n@shin, incidentally, this also looks like a good candidate for inclusion in @check_resource. Similar checks are littered throughout client.py (but still missing from some methods). See my raise-on-inspect-empty-string-602-with-id-resolution-in-check-resource branch. (Tests pass.)\n. ",
    "jamespharaoh": "Ok well I found and fixed the #468 which was the cause of the error for me, so this doesn't affect me any more. From what you say, this would be a problem with docker really, correct? They should be returning an appropriate error code in the case of faliure... I can raise an issue there if so.\n. ok this is already reported in docker, but with no activity:\nhttps://github.com/docker/docker/issues/8844\n. ",
    "CleanCut": "@shin- If you or someone else would provide me with the steps to test #406, I would be happy to run through the procedures and report back.  (How to test #406 isn't obvious to me).  This issue is currently interrupting things in my department though, so I'd be happy to help out.\n. ",
    "GeyseR": "Hi!\nThis issue touches our company development process, so I've tried to reproduce issue #406 on my boot2docker mac os installation. But with no luck.\nMy installation:\n```\ngeyser$ boot2docker version\nBoot2Docker-cli version: v1.5.0\nGit commit: ccd9032\ngeyser$ boot2docker --vm=\"test-vm\" init\ngeyser$ boot2docker --vm=\"test-vm\" up\nWaiting for VM and Docker daemon to start...\n.o\nStarted.\nWriting /Users/geyser/.boot2docker/certs/test-vm/ca.pem\nWriting /Users/geyser/.boot2docker/certs/test-vm/cert.pem\nWriting /Users/geyser/.boot2docker/certs/test-vm/key.pem\nTo connect the Docker client to the Docker daemon, please set:\n    export DOCKER_HOST=tcp://192.168.59.103:2376\n    export DOCKER_CERT_PATH=/Users/geyser/.boot2docker/certs/test-vm\n    export DOCKER_TLS_VERIFY=1\ngeyser$ export DOCKER_HOST=tcp://192.168.59.103:2376\ngeyser$ export DOCKER_CERT_PATH=/Users/geyser/.boot2docker/certs/test-vm\ngeyser$ export DOCKER_TLS_VERIFY=1\ngeyser$ pip freeze\ndocker-py==1.0.0\nrequests==2.5.0\nsix==1.9.0\ngeyser$ python\nPython 3.4.2 (v3.4.2:ab2c023a9432, Oct  5 2014, 20:42:22)\n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport docker\nfrom docker.client import Client\nfrom docker.utils import kwargs_from_env\nclient = docker.Client(**kwargs_from_env())\n```\n\n\n\nSame results I have with Python 2/3, requests 2.5.0/2.5.1. Does anyone can confirm that this problem exists?\n. ",
    "dreamcat4": "Renamed ticket to encompass all of the new Docker 1.5 flags (not just --pid).\n. Thanks @shin- !\nDo we know yet for which minimum version of fig for this will be released ? Perhaps Docker 1.6 ?\n. ",
    "xilon-jul": "I'm going to have a look and make a try... maybe I got a bit confused by the documentation. I will close the issue if its already possible. Thank you.\n. Sorry, definitely got confused, this is already working. Closing the issue.\nSee example with : module version docker-py==0.7.1\nfrom docker import Client\nimport time\ndocker = Client(base_url='unix://var/run/docker.sock')\nresponse1 = docker.create_container('debian:wheezy', detach=True, volumes=['/new'],\n    command=\"/bin/bash\", name='mycontainer2')\ndocker.start(response1['Id'])\ntime.sleep(1)\nresponse = docker.create_container('debian:wheezy',\n    command=\"/bin/bash\",stdin_open=True)\ndocker.start(response['Id'], volumes_from='mycontainer2')\ntime.sleep(1)\nprint(docker.logs(response['Id']))\n. ",
    "ggtools": "Yes you're definitely right I adding this to the PR right now.\n. Oooops sorry I didn\"t see this PR before posting mine. My mistake\n. ",
    "abhat": "@ggtools and @shin- , this is of huge help. Should the docs/api.md document a dictionary of non-unicode strings if decode is None and the current version if True? If you agree, I will make the edit so that the documentation is more complete.\n. ",
    "hibooboo2": "@shin- Is there any chance you can take a look at this?\n. Thank you shin. \n. ",
    "br0ch0n": "I'm assuming this is because /containers/create in the remote api (which I believe docker-py is using) doesn't pull for you?  I'm fine working around it, but just out of curiosity, is there a reason it doesn't?  I'd think it would be the same as the 'docker create' command, which does pull for you.\n. OK, makes sense.  Thanks for clarifying.\n. ",
    "bobrik": "Is there a reason not to set DEFAULT_DOCKER_API_VERSION to auto?\n. ",
    "osallou": "Yes, the port is set in the ports section of the create_container:  [22]\nLe lun. 2 mars 2015 19:51, Joffrey F notifications@github.com a \u00e9crit :\n\nDo you declare the port you want to open in the create_container method?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-py/issues/513#issuecomment-76778470.\n. Here is an example program, where I expose port 22 and publish ports.\n\nfrom docker import Client\ndocker_client = Client(base_url='tcp://127.0.0.1:2376')\ncontainer = docker_client.create_container(image='centos:latest',\n                                            command='/bin/sleep 30',\n                                            cpu_shares=1,\n                                            mem_limit='1g',\n                                            ports=[22])\nprint 'ID: '+str(container.get('Id'))\nresponse = docker_client.start(container=container.get('Id'),\n                                    network_mode='host',\n                                    publish_all_ports=True\n                                    #port_bindings={22: None}\n                                    )\nprint str(response)\nInspect gives:\nIn Config:\n        \"ExposedPorts\": {\n            \"22/tcp\": {}\n        },\n....\n```\n\"HostConfig\": {\n    \"Binds\": null,\n    \"CapAdd\": null,\n    \"CapDrop\": null,\n    \"ContainerIDFile\": \"\",\n    \"Devices\": null,\n    \"Dns\": null,\n    \"DnsSearch\": null,\n    \"ExtraHosts\": null,\n    \"IpcMode\": \"\",\n    \"Links\": null,\n    \"LxcConf\": null,\n    \"NetworkMode\": \"host\",\n    \"PidMode\": \"\",\n    \"PortBindings\": null,\n    \"Privileged\": false,\n    \"PublishAllPorts\": true,\n    \"ReadonlyRootfs\": false,\n    \"RestartPolicy\": {\n        \"MaximumRetryCount\": 0,\n        \"Name\": \"\"\n    },\n    \"SecurityOpt\": null,\n    \"VolumesFrom\": null\n},\n\"NetworkSettings\": {\n    \"Bridge\": \"\",\n    \"Gateway\": \"\",\n    \"GlobalIPv6Address\": \"\",\n    \"GlobalIPv6PrefixLen\": 0,\n    \"IPAddress\": \"\",\n    \"IPPrefixLen\": 0,\n    \"IPv6Gateway\": \"\",\n    \"LinkLocalIPv6Address\": \"\",\n    \"LinkLocalIPv6PrefixLen\": 0,\n    \"MacAddress\": \"\",\n    \"PortMapping\": null,\n    \"Ports\": null\n},\n```\n. I just understood that default mode of docker is bridge and not host,  this explains that.\n. Seems to work now, don't understand.... but closing issue\n. nope... simply catching the error, that's why it disappeared from logs. Issue still present, sorry\n. what do you mean? (I am using Python 2 by the way)\nsetting\n'host_config': **conf\nWould be nice to have an example in documentation, like for volumes.\n. More info\n$ python --version\nPython 2.7.3\n$ pip --version\npip 1.4.1 from /home/ubuntu/virtualenv/python2.7/lib/python2.7/site-packages (python 2.7)\n. ok, I gonna force previous version to match available pip release in drone.io\nThanks\n. However, running docker service create... -p 8080:80 works for example.\nHow can i check what is sent to docker engine to see if it receices what is\nexpected?\nThe service is created and ran correctly except the missing ports.\nLe ven. 16 juin 2017 20:41, Joffrey F notifications@github.com a \u00e9crit :\n\nEndpointSpec is apparently properly set on the service object, so it's\nlikely you're running into an engine issue at that point.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/docker/docker-py/issues/1651#issuecomment-309103872,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA-gYhNaDXagHl8DjW77_CJR025SRloeks5sEsx0gaJpZM4N8gU6\n.\n. Looking at dockerd logs, port mapping info are indeed sent:\n\nJun 19 07:45:59 swarm dockerd[17148]: time=\"2017-06-19T07:45:59.481101505Z\" level=debug msg=\"Calling POST /v1.26/services/create\"\nJun 19 07:45:59 swarm dockerd[17148]: time=\"2017-06-19T07:45:59.481233796Z\" level=debug msg=\"form data: {\\\"EndpointSpec\\\":{\\\"Ports\\\":[{\\\"Protocol\\\":\\\"tcp\\\",\\\"PublishedPort\\\":10018,\\\"TargetPort\\\":22}]},\\\"TaskTemplate\\\":{\\\"ContainerSpec\\\":{\\\"Args\\\":null,\\\"Command\\\":[\\\"sleep\\\",\\\"120\\\"],\\\"Image\\\":\\\"debian\\\"},\\\"Resources\\\":{\\\"Reservations\\\":{\\\"NanoCPUs\\\":1e+09}},\\\"RestartPolicy\\\":{\\\"Condition\\\":\\\"none\\\",\\\"Delay\\\":0,\\\"MaxAttempts\\\":0,\\\"Window\\\":0}}}\"\n\nSo either it is engine issue, or api needs additional info about network (but raises no error). I created an issue on engine (https://github.com/docker/for-linux/issues/36)\n. Closing issue, problem was port mapping is not displayed the same way for containers and services.. pb occurs only with pip install\nif we clone docker-py repo and try to install with python setup.py install, we can see the error:\n(toto)root@swarm:~/toto/docker-py# python setup.py develop\nERROR: \"docker-py\" needs to be uninstalled before installing this package:\n\n. ",
    "istinspring": "So now it's 2017 and i have exactly same damn problem the TC got in 2015. Same issue.\nVersion 17.03.0-ce-mac2 (15654)\ndocker-py (1.10.6). ",
    "delikat": "Still seeing this as well.. ",
    "LK4D4": "@shin- Wow, thanks so much.\n. ",
    "kenden": "Thanks Tomas and shin.\n(I did a rebase on my fork, and this pull request got closed).\nBut I disagree: \nHaving an example putting the return to the function in a variable made me believe that there could be a return to the function that could be 'not None'. \nI had to look at the code to check that the function has no 'return' statement. \n. ",
    "aanm": "@aanand \nThanks for taking care of it.\n. ",
    "akadan47": "This occurs because ENV variable REQUESTS_CA_BUNDLE is not declared.\nhttps://github.com/kennethreitz/requests/blob/master/requests/sessions.py#L621\nbash\nexport REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt  # for Debian-based Linux\nBe reasonable to declare it in the ~/.profile.\n. ",
    "pmoosh": "I had an issue with private registry auth and ansible, here:\nhttps://github.com/docker/docker-py/pull/577\n. ",
    "moysesb": "@shin-  If you'd like having something concrete to look at while evaluating this proposal, there's an open PR in docker-compose with an implementation of the ideas presented here. Thanks!\n. @aanand \n1.  Scaled back the validation to be just on access rights, Dockerfile name check and tarball format. The docker go client does roughly that level of validation; it supports reading a compressed context from stdin and should implicitly reject a malformed tarball (not sure about whether it actually rejects it, but I'd consider it the right thing to do).\n2.  Added documentation at docs/context.md\n. Removed now. It would eventually be used for validating a Dockerfile thoroughly, but I discarded the idea.\n. ",
    "msabramo": "Yes. They were not interested in having it in requests because Unix sockets are not a thing in the HTTP standards. \nSee https://github.com/kennethreitz/requests/pull/2355\n. ",
    "yuval-k": "@aanand  - thanks for the feedback - fixed \n. ",
    "jameskyle": "Getting the same error here. binding /etc/localtime is very common.\n. Yeah, I was using ansible as well. But when I ran into the error I dug into ansibles module source and replicated it using docker-py directly.\n. ",
    "mkuzmin": "I've met the same issue using Ansible: https://github.com/ansible/ansible-modules-core/issues/1158\n. - Ubuntu 14.04.2\n- Docker version 1.6.0, build 4749651\n- docker-py 1.1.0\n. ```\n.\n\nRan 1 test in 0.133s\nOK\n```\nBut the testcase seems incomplete. The issue happens only when the mounted file exist in an image already, and needs to be overwritten by a copy from a host.\n. I've got\n```\nE\n======================================================================\nERROR: runTest (main.TestStartContainerWithFileBind)\n\nTraceback (most recent call last):\n  File \"./test_file_bind.py\", line 62, in runTest\n    img, cmd, volumes=volumes,\nNameError: global name 'img' is not defined\n\nRan 1 test in 0.006s\nFAILED (errors=1)\n```\n. I'm just Ansible user, and don't write anything by myself.\nHere is the code: https://github.com/ansible/ansible-modules-core/blob/devel/cloud/docker/docker.py#L1191\nBut I don't have enough Python knowledge to perform the change you requested. sorry.\n. ",
    "schmunk42": "@mkuzmin Same here, I updated my vagrant base box to 14.04.2 which was released just a day ago.\n. @shin- How do I run the test case? Would I need to get docker-py source on the broken system?\n. ",
    "Brett55": "if host_config was deprecated in 1.2.3, where do I pass in binds now?. thanks!\nOn Tue, Aug 22, 2017 at 12:40 PM, Joffrey F notifications@github.com\nwrote:\n\ndocker-py is the older, deprecated version of the library. :)\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/docker/docker-py/issues/1723#issuecomment-324115457,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGsE2R6vwr7Rzq3WIxlAX7lGdMSu3rCVks5sayCLgaJpZM4O-zQl\n.\n. self._container = self._docker_client.containers.run(image=self._image_id,\n                        command=self.run_paramaters.command,\n                        volumes=volumes,\n                        devices=devices,\n                        detach=True). @GordonTheTurtle done!. Did I do all the required steps for PR?. \n",
    "jfgreen": "Great stuff! Much appreciated.\n. ",
    "garrickp": "This does not appear to have been fixed, as the new ping_registry function is still tryign to hit a _ping endpoint which does not exist in v2.\n```\nvagrant@registry-vagrant:~$ curl -v http://localhost/v2/_ping\n Hostname was NOT found in DNS cache\n   Trying 127.0.0.1...\n* Connected to localhost (127.0.0.1) port 80 (#0)\n\nGET /v2/_ping HTTP/1.1\nUser-Agent: curl/7.35.0\nHost: localhost\nAccept: /\n< HTTP/1.1 404 Not Found\n< Content-Type: text/plain; charset=utf-8\n< Docker-Distribution-Api-Version: registry/2.0\n< Date: Thu, 23 Apr 2015 13:34:46 GMT\n< Content-Length: 19\n<\n404 page not found\n Connection #0 to host localhost left intact\nvagrant@registry-vagrant:~$ curl -v http://localhost/v2/\n Hostname was NOT found in DNS cache\n   Trying 127.0.0.1...\n Connected to localhost (127.0.0.1) port 80 (#0)\nGET /v2/ HTTP/1.1\nUser-Agent: curl/7.35.0\nHost: localhost\nAccept: /\n< HTTP/1.1 200 OK\n< Content-Length: 2\n< Content-Type: application/json; charset=utf-8\n< Docker-Distribution-Api-Version: registry/2.0\n< Date: Thu, 23 Apr 2015 13:34:52 GMT\n<\n* Connection #0 to host localhost left intact\n{}\n```\n. \n",
    "avdhoot": "This patch will not work if you have auth enabled registry 2.0. More info \nhttps://github.com/docker/docker/blob/v1.6.0/registry/auth.go#L342\n. ",
    "frewsxcv": "Relevant library: https://github.com/jgrowl/docker-machine-py\n. ",
    "MiLk": ":+1: \n. ",
    "fvozar": "Equivalent commands are\n$ docker run -d -i -t -w /tmp/tester/data -u tester -v /tmp -v /tmp/folder-with-files:/tmp/tester/data:rw --read-only my-image:latest /bin/bash\n$ docker exec -i <some_id> pwd\n/tmp/tester/data\n$ docker exec -i <some_id> whoami\ntester\n$ docker exec -i <some_id> ls -lha\ntotal 8.0K\ndrwxrwxr-x 2 tester tester 4.0K Apr 24 07:26 .\ndrwxr-xr-x 3 tester tester 4.0K Apr 24 07:27 ..\n-rw-rw-r-- 1 tester tester    0 Apr 24 07:26 Dockerfile\nBecause docker-py can't send stdin to running container, I have to run all exec commands via subprocess.Popen(), where stdin is supported. \nAs you see, via CLI is ls working as expected.\n. I figured out, that uid 1003 and gid 1004 belong to user in host system, who run whole application. Why docker didn't set ownership and permissions for user \"tester\" defined in Dockerfile? Is there connection between uid of real user and user in container, e.g. tester has uid 1000, but those files belong to user with 1003 and therefore I got permission denied?\n//EDIT:\nProblem was in permissions on original directory in host system. Despite read-write permissions (e.g. 775 on current dir) inside container, in host directory was permissions only 700 and that was causing whole problem. \nI am closing this issue at this point. Thanks.\n. ",
    "aidanhs": "Never trust the docker api docs until you've actually tested against the daemon. They're generally about right, but sometimes vary between \"a bit off\" to \"completely inaccurate\".\nCase in point, docs for 1.17 version (the version I have available) http://docs.docker.com/reference/api/docker_remote_api_v1.17/#search-images (same as 1.18).\nBut running against daemon:\n~ $ curl http://docker03:2375/v1.17/images/search?term=sshd\n[{\"description\":\"Dockerized SSH service, built on top of official Ubuntu images.\",\"is_official\":false,\"is_trusted\":true,\"name\":\"rastasheep/ubuntu-sshd\",\"star_count\":2}\n,{\"description\":\"Fedora docker file for ssh service which managed by supervisor\",\"is_official\":false,\"is_trusted\":true,\"name\":\"kumarpraveen/fedora-sshd\",\"star_count\":1}\n,{\"description\":\"\",\"is_official\":false,\"is_trusted\":true,\"name\":\"iliyan/docker-nginx-sshd\",\"star_count\":1}\n,{\"description\":\"\",\"is_official\":false,\"is_trusted\":true,\"name\":\"moul/sshd\",\"star_count\":1}\n[...]\nYou might want to raise an issue against docker itself :)\n. ",
    "blueyed": "Reported/forwarded it at: https://github.com/docker/docker/issues/16706.\n. @linlinlinlin @shin- \nI would appreciate your help/feedback on https://github.com/docker/docker/issues/16706 (Docker version etc), in case you've investigated into this issue before.\n. Fixed in docker: https://github.com/docker/docker/pull/16736\nShould be in 1.9 and possibly 1.8.3.\n. The ) should be at the end?!\n. s/an existing network/an existing Docker network/ ?\n. ",
    "mboersma": "+1\n. ",
    "smothiki": "Apologies for multiple commits. Ignored checking flake8 every time\n. @shin- I have added integration tests. Let me know if you want more test cases\n. ",
    "cmoro-deusto": "@shin- you are the man! Thanks!\n. ",
    "vpetersson": "Ah, good point. Yeah that kinda makes sense.\n. @shin- Let me know what you think.\n. @shin- When will the next version be posted to PyPi? \n. > This needs tests, and they should at least cover the cases covered\n\nby the Docker client's env file parser tests (although the \"too long file\" is less important).\n\nOk, let me take a look at that.\n\nThe Docker client performs very straightforward string splitting. Is there a compelling\nreason to use csv here, rather than just do the same?\n\nNone really other than I figured that it was better to use a built-in feature instead of re-inventing the wheel (even if a simple .split('=') is likely sufficient).\n\nit's not clear what happens when a variable is specified both in a file and in the environment key. \n\nIt works fine:\n```\nIn [4]: docker.utils.create_container_config('1.18', '_mongodb', 'foo',  environment={'john': 'doe', 'steve': 'smith'}, env_file='/Users/mvip/tmp/foo')['Env']\nOut[4]: \n[u'steve=smith',\n u'john=doe',\n 'BROKER_HOST=localhost',\n 'BROKER_PASSWORD=somePassword',\n 'BROKER_PORT=123']\nIn [5]: docker.utils.create_container_config('1.18', '_mongodb', 'foo',  environment='FOO=BAR', env_file='/Users/mvip/tmp/foo')['Env']                                                               \nOut[5]: \n['FOO=BAR',\n 'BROKER_HOST=localhost',\n 'BROKER_PASSWORD=somePassword',\n 'BROKER_PORT=123']\n```\nBut you're right, the environment file from disk would override the manual one. \n. > Sorry, I meant: what happens when the same variable (e.g. FOO) is specified in both\n\nplaces with a different value? Which one wins?\n\nYeah, in that case the environment file would override. Not sure if this makes sense or not, but it would be easy to add logic if that isn't the expected behavior. \n\nFor example, does it have special ways of dealing with quotes, or escaped delimiters?\nEven if those are useful things, we shouldn't deviate from the Docker client's behaviour.\n\nYeah, I was thinking about that too when I wrote it. I could not find any documentation for the escape character (and didn't find the code at first glance). The CSV library does have a feature for explicitly set this though so if you have the escape character, I can easily add that. \n. It looks like the entire 'create_container_config' function lacks testing. I'm not sure if I'm well suited to write a test for this entire function.\n. I see your point. Yeah, there are benefits with that, but for the use case I have in mind for this it would actually complicate things further, as you'd then have to re-implement the same stuff there (unless I'm missing something).\n. @aanand Let me know what you think.\n. Addressed the feedback, but still working on the tests. I'm having some issues there and have posted it on StackOverflow.\n. @aanand Tests added. Let me know what you think.\n. > The Docker client raises an error if there are bad lines in the file, so we should do that too.\nDone.\n\nIt'd be more useful if parse_env_file returned a dict, so that multiple env files can be\ncombined using the update() method on dict\n\nSure. Fixed.\n\nI'm not sure about the change to .gitignore, but that should definitely be done in a separate PR.\n\nYeah, that was an accidental commit. \n. Also, I'm not sure if I should commit the whole htmlcov folder as well.\n. > Could you add a test for the \"invalid line\" case?\nThere's already a test for that here or what do you mean?\n\nCould you change the error message wording to...\n\nDone.\n. Done like dinner!\n. @aanand can we land this? Would like to start working on the Ansible side of things :)\n. Nice! Yes, that's cleaner. Pushed.\n. ",
    "rhatdan": "@roldancer Opened issue here with docker-py\n. Looks good to me. So the next update to docker-py should fix this issue.\n. ",
    "jasinner": "Can we revisit this request? We can now enable TLS for encryption and auth on Docker TCP Socket, which makes it secure I believe. However atomic run doesn't work with Docker TCP Socket. For the details of how to configure the client see:\nhttps://docs.docker.com/engine/security/https/\nspecifically:\nexport DOCKER_HOST=tcp://$HOST:2376 DOCKER_TLS_VERIFY=1\n. ",
    "ctrlhxj": "Could some one help to review and possibly merge this pull requests. This is a bug fix. Also tested in our prod env as well\n. ",
    "sandkumaCode": "I have been trying to add syslog config to create_container but have not been able to.\nI am using \ndct = {'type': LogConfig.types.SYSLOG, 'config': {}}\n host_config_p=self._docker_client.create_host_config(log_config=dct)\n options.update(host_config_p)\nresult = yield self._docker_client.create_container(_options)\nhave also tried\nhost_config={'LogConfig': {'Type': 'syslog', 'Config': {}}}\noptions.update(host_config)\nresult = yield self._docker_client.start(container.id, _options)\nbut its not affecting still the default json remains\nAm using 1.4.0 docker-py\ndocker 1.7.1\nWill be great if you all can be let me know the correct way to enable syslog using the API above.\n. Thanks a lot with this change i have everything working.\nOne observation if we have options in create_container and if we follow up with start that has options too is the create_container option overridden?\nif i put configs in create_container and then do a start with just container id it works fine\nwhen i put options in start it seems the log_config is overwritten with the new options that i give.\nDo you advice to put all the options in create and do a start without any options? are there any specific options that we should pass only in start?\n. ",
    "garthy": "This is because logs doesn't return the byte headers so the value gets truncated by header bytes.\nRun docker run -t -i ubuntu  /bin/bash -c 'echo \"Hello this is a bug\"'\nThe stdout is docker logs <containerid>\nHello this is a bug\nThen run the code \nimport sys\nimport docker\ncontainer = sys.argv[1]\nklint = docker.Client(base_url='unix://var/run/docker.sock')\nprint \"LOGS:\",\nprint klint.logs(container,stdout=True, stderr=True, stream=False, timestamps=False)\nThis prints \nLOGS: is is a bug\nThis is with the following on Linux Mint\n$ docker version\nClient version: 1.6.2\nClient API version: 1.18\nGo version (client): go1.4.2\nGit commit (client): 7c8fca2\nOS/Arch (client): linux/amd64\nServer version: 1.6.2\nServer API version: 1.18\nGo version (server): go1.4.2\nGit commit (server): 7c8fca2\nOS/Arch (server): linux/amd64\n. OK this only seems to be the case when I create a container with\nstdin_open (bool): Keep STDIN open even if not attached\ntty (bool): Allocate a pseudo-TTY\nIf these are not set then the header bytes exist as part of the stream\n. from the Docker API docs..\nStream details:\nWhen using the TTY setting is enabled in POST /containers/create, the stream is the raw data from the process PTY and client's stdin. When the TTY is disabled, then the stream is multiplexed to separate stdout and stderr.\n. Yes.\n. ",
    "allan-simon": "\nOS:  ubuntu 12.04 \ndocker 1.4  (installed through the PPA)\ndocker-py the latest installed by pip  (1.2.1 )\nno env variable for DOCKER_HOST\n. my bad, my ansible tasks had installed package \"docker\"  which is totally not related to  lxc-docker and which was causing conflict of binary\n. \n",
    "mikexstudios": "Hey @shin-. I updated your PR with a unit test for this case. Your PR works for detecting trailing slashes on folders.\nHowever, I am concerned that we need a more general and robust way of handling .dockerignore files. The currently implementation fails on a lot of syntax defined here: https://github.com/docker/docker/blob/master/docs/sources/reference/builder.md#dockerignore-file\nI'll see if I can contribute to improving this in docker-py.\n. Hey @shin-. Thanks for the fix and sorry for the late reply. I will test your PR tomorrow and let you know.\n. Sorry, this is taking longer than expected since I want to test your PR rigorously. I'm writing a bunch of test cases, which should help.\n. Hey @shin-. I updated your PR with a unit test for this case. Your PR works for detecting trailing slashes on folders.\nHowever, I am concerned that we need a more general and robust way of handling .dockerignore files. The currently implementation fails on a lot of syntax defined here: https://github.com/docker/docker/blob/master/docs/sources/reference/builder.md#dockerignore-file\nI'll see if I can contribute to improving this in docker-py.\n. This PR supersedes #603 and #591.\n. ",
    "paulbellamy": "Excellent! Thanks so much. :)\n. That's great, thanks.\nIn particular, I was wondering if there is a way to hook up travis-ci to push to dockerhub on a successful, build. But I'm not sure what the integration there is like.\n. ",
    "longgeek": "Thank you for your reply, but I still have some questions.\ndocker-py Version:\nIn [12]: import docker\nIn [13]: docker.version\nOut[13]: '1.2.1'\nHelp create_container():\nIn [14]: help(docker.Client.create_container)\nHelp on method create_container in module docker.client:\ncreate_container(self, image, command=None, hostname=None, user=None, detach=False, stdin_open=False, tty=False, mem_limit=0, ports=None, environment=None, dns=None, volumes=None, volumes_from=None, network_disabled=False, name=None, entrypoint=None, cpu_shares=None, working_dir=None, domainname=None, memswap_limit=0, <code style=\"background: #cc6666;\">cpuset=None</code>, host_config=None, mac_address=None, labels=None) unbound docker.client.Client method\nUse cpuset:\n.create_container(cpuset=0, mem_limit=\"512m\", memswap_limit=-1, ....)\ninspect container content:\nroot@api:~# docker inspect 81\n[{\n    \"AppArmorProfile\": \"\",\n    \"Args\": [],\n    \"Config\": {\n        \"AttachStderr\": false,\n        \"AttachStdin\": false,\n        \"AttachStdout\": false,\n        \"Cmd\": [\n            \"bash\"\n        ],\n        \"CpuShares\": 0,\n        <code style=\"background: #cc6666;\">\"Cpuset\": \"0\",</code>   ------------------------------- cpuset is Valid! -------------------------------\n        \"Domainname\": \"\",\n        \"Entrypoint\": null,\n        \"Env\": [\n            \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n        ],\n        \"ExposedPorts\": {\n            \"22/tcp\": {},\n            \"80/tcp\": {}\n        },\n        \"Hostname\": \"81de5323a3c7\",\n        \"Image\": \"longgeek/ubuntu-14.04.1:base\",\n        \"Labels\": {},\n        \"MacAddress\": \"\",\n        <code style=\"background: #cc6666;\">\"Memory\": 536870912,\n        \"MemorySwap\": -1,</code>    ------------------------------- mem is Valid! -------------------------------\n        \"NetworkDisabled\": false,\n        \"OnBuild\": null,\n        \"OpenStdin\": true,\n        \"PortSpecs\": null,\n        \"StdinOnce\": false,\n        \"Tty\": true,\n        \"User\": \"\",\n        \"Volumes\": null,\n        \"WorkingDir\": \"\"\n    },\n    \"Created\": \"2015-05-07T15:52:16.550464035Z\",\n    \"Driver\": \"aufs\",\n    \"ExecDriver\": \"native-0.2\",\n    \"ExecIDs\": [\n        \"2abcb14a851b3c246c1c4d0651f9b2596c07acd3cb8c02b09dc9a3fb43fe51cd\"\n    ],\n    \"HostConfig\": {\n        \"Binds\": null,\n        \"CapAdd\": null,\n        \"CapDrop\": null,\n        \"CgroupParent\": \"\",\n        \"ContainerIDFile\": \"\",\n        \"CpuShares\": 0,\n        <code style=\"background: #cc6666;\">\"CpusetCpus\": \"\",</code>    ------------------------------- Didn't work? -------------------------------\n        \"Devices\": null,\n        \"Dns\": null,\n        \"DnsSearch\": null,\n        \"ExtraHosts\": null,\n        \"IpcMode\": \"\",\n        \"Links\": null,\n        \"LogConfig\": {\n            \"Config\": null,\n            \"Type\": \"json-file\"\n        },\n        \"LxcConf\": null,\n        <code style=\"background: #cc6666;\">\"Memory\": 0,\n        \"MemorySwap\": 0,</code>     ------------------------------- Didn't work? -------------------------------\n        \"NetworkMode\": \"\",\n        \"PidMode\": \"\",\n        \"PortBindings\": null,\n        \"Privileged\": false,\n        \"PublishAllPorts\": true,\n        \"ReadonlyRootfs\": false,\n        \"RestartPolicy\": {\n            \"MaximumRetryCount\": 0,\n            \"Name\": \"\"\n        },\n        \"SecurityOpt\": null,\n        \"Ulimits\": null,\n        \"VolumesFrom\": null\n    },\n    \"HostnamePath\": \"/var/lib/docker/containers/81de5323a3c7adff2d531f501e5b43706bf18077265bb4247ea2552fec18e982/hostname\",\n    \"HostsPath\": \"/var/lib/docker/containers/81de5323a3c7adff2d531f501e5b43706bf18077265bb4247ea2552fec18e982/hosts\",\n    \"Id\": \"81de5323a3c7adff2d531f501e5b43706bf18077265bb4247ea2552fec18e982\",\n    \"Image\": \"fbefd02fb923f18815e65df583f484ec9522b2d99e7ddd97d62fb93f0c7608c5\",\n    \"LogPath\": \"/var/lib/docker/containers/81de5323a3c7adff2d531f501e5b43706bf18077265bb4247ea2552fec18e982/81de5323a3c7adff2d531f501e5b43706bf18077265bb4247ea2552fec18e982-json.log\",\n    \"MountLabel\": \"\",\n    \"Name\": \"/c-153ik1jpw9cmby\",\n    \"NetworkSettings\": {\n        \"Bridge\": \"docker0\",\n        \"Gateway\": \"172.17.42.1\",\n        \"GlobalIPv6Address\": \"\",\n        \"GlobalIPv6PrefixLen\": 0,\n        \"IPAddress\": \"172.17.0.211\",\n        \"IPPrefixLen\": 16,\n        \"IPv6Gateway\": \"\",\n        \"LinkLocalIPv6Address\": \"fe80::42:acff:fe11:d3\",\n        \"LinkLocalIPv6PrefixLen\": 64,\n        \"MacAddress\": \"02:42:ac:11:00:d3\",\n        \"PortMapping\": null,\n        \"Ports\": {\n            \"22/tcp\": [\n                {\n                    \"HostIp\": \"0.0.0.0\",\n                    \"HostPort\": \"37491\"\n                }\n            ],\n            \"80/tcp\": [\n                {\n                    \"HostIp\": \"0.0.0.0\",\n                    \"HostPort\": \"37492\"\n                }\n            ]\n        }\n    },\n    \"Path\": \"bash\",\n    \"ProcessLabel\": \"\",\n    \"ResolvConfPath\": \"/var/lib/docker/containers/81de5323a3c7adff2d531f501e5b43706bf18077265bb4247ea2552fec18e982/resolv.conf\",\n    \"RestartCount\": 0,\n    \"State\": {\n        \"Dead\": false,\n        \"Error\": \"\",\n        \"ExitCode\": 0,\n        \"FinishedAt\": \"2015-05-07T17:42:12.139743371Z\",\n        \"OOMKilled\": false,\n        \"Paused\": false,\n        \"Pid\": 612,\n        \"Restarting\": false,\n        \"Running\": true,\n        \"StartedAt\": \"2015-05-07T17:55:53.788976399Z\"\n    },\n    \"Volumes\": {},\n    \"VolumesRW\": {}\n}\n]\nWhy mem and swap did not appear in the Hostconfig?\n. @shin- \nSee http://docs.docker.com/reference/api/docker_remote_api_v1.18/#create-a-container\nCpuset - The same as CpusetCpus, but deprecated, please don't use.\nCpusetCpus - String value containg the cgroups CpusetCpus to use.\n. My System:\nroot@api:~# lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 14.04.1 LTS\nRelease:        14.04\nCodename:       trusty\nDocker Info:\nroot@api:~# docker info\nContainers: 46\nImages: 25\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 117\n Dirperm1 Supported: false\nExecution Driver: native-0.2\nKernel Version: 3.13.0-44-generic\nOperating System: Ubuntu 14.04.1 LTS\nCPUs: 2\nTotal Memory: 1.955 GiB\nName: api\nID: 6YNH:I5E5:P5SO:4NT4:6CVI:X5LL:XD2N:OACO:ZI4V:3Z6V:QYGO:IWEE\nDocker Version:\nroot@api:~# docker version\nClient version: 1.6.0\nClient API version: 1.18\nGo version (client): go1.4.2\nGit commit (client): 4749651\nOS/Arch (client): linux/amd64\nServer version: 1.6.0\nServer API version: 1.18\nGo version (server): go1.4.2\nGit commit (server): 4749651\nOS/Arch (server): linux/amd64\nThe following is my operation\uff1a\n```\nIn [1]: from docker import Client\nIn [2]: c = Client(base_url=\"unix:///var/run/docker.sock\")                                                                                                                \nIn [3]: c.create_container('longgeek/ubuntu-14.04.1:base', '/bin/bash', mem_limit='512m', memswap_limit=-1)                                                             \nOut[3]: \n{u'Id': u'09fd71b4e9ba15a659db49f3b56748461c68a7aa5962371e9a95fa2a64553645',  u'Warnings': None}\nIn [4]: data = c.inspect_container('09fd')\nIn [5]: data['Config']['Memory']                                                                                                                                        \nOut[5]: 536870912\nIn [6]: data['HostConfig']['Memory']\nOut[6]: 536870912\nIn [7]: data['HostConfig']['MemorySwap']\nOut[7]: -1\nIn [8]: c.start('09fd', publish_all_ports=True)\nIn [9]: data = c.inspect_container('09fd')\nIn [10]: data['HostConfig']['Memory']\nOut[10]: 0\nIn [11]: data['HostConfig']['MemorySwap']\nOut[11]: 0\nIn [12]: data['Config']['Memory']\nOut[12]: 536870912\nIn [13]: data['Config']['MemorySwap']\nOut[13]: -1\n```\n. thanks @shin- :+1: \n```\ncontainer = c.create_container('longgeek/ubuntu-14.04.1:base', '/bin/bash', mem_limit='512m', memswap_limit=-1, cpuset=0, host_config=docker.utils.create_host_config(publish_all_ports=True))\nc.start(container)\n```\ncpuset=0 valid.\n\"HostConfig\": {\n    \"CpuShares\": 0,\n    \"CpusetCpus\": \"0\",\n    \"Memory\": 536870912,\n    \"MemorySwap\": -1,\n. ",
    "mnowster": "Minor typo to fix then it's a LGTM. \nThanks for your continued effort on this @delfick :fireworks: \n. From a design stand point I also don't think it's docker-py's responsibility to do the duplicated effort of maintaining the list of supported drivers. Separation of concerns, I view that as docker daemon's responsibility.\nThat's a good point @aanand about keeping the types for backwards compatibility, I'll update to include them.\n. Ok, I've rebased and responded to comments. \nAnything else or we good for merging? @shin- @aanand \n. I appreciate I'm not as familiar with this codebase, this was the only approach I could see how to do and saw that version checking was used elsewhere in the codebase. \nIs there an alternative approach I've missed? Do you have an idea how else this could be done?\nIf not, is this good enough for now as it's at least some help towards backwards compatibility for users which is an improvement on not having anything which gives the error of default not found for network_mode\n. @aanand this approach sounds good to me. I like the 2 step approach so we can display deprecation warnings then remove it, that'll be helpful to not screw over users who are using the direct approach.\nI think this will also help bring some clarity to the intent of some of the code. There's a lot in the utils file which isn't utility functions it's core logic, so to get to move this around would be a positive step in my view.\n. Closing as this has been resolved in https://github.com/docker/docker-py/pull/732\n. I'm really happy to see docker-py get a contributing guideline :+1: \n. LGTM\n. LGTM\n. @dnephin Good point. I made mistake of thinking it had been included in the 1.8.2 release.\n. @aanand I have updated to include extra tests.\n. The validation code that would have raised a ValueError has been removed, thus making this test redundant. The tests added in integration_tests cover the use case of invalid config types and check that we receive an error back from the API.\n. Yep, I think it is. I ran this test on it's own. I did also run make integration-test but get some time out errors which I believe aren't due to this test. Did you see the commit message that goes with this code change? It describes the why of doing this. By using json-file as the use case we can run this without needing to be on a machine with syslog installed.\n. This test is written in a convention that echoes that of docker-compose, https://github.com/docker/compose/blob/master/tests/unit/config_test.py, which is a good guide of how tests are to be structured and written.\nTest case names are easier to read without Test at the beginning, we know it's a test, so we can put it at the end. Naming our tests individually rather than runTest provides helpful feedback when a test fails or we get a stack trace error. This is a way of improving the code while I was there and a demonstration of what the tests can be refactored towards. \nConsistency for it's own sake forsaking that of improved code is not as beneficial.\n. It'd be good to refer to docker's contributing guidelines here, in the same way we do for docker-compose. https://github.com/docker/compose/blob/master/CONTRIBUTING.md#contributing-to-compose\n. Having commits be a series of logical well written commits is beneficial to talk about here. In compose we link to Government Digital Service(where I used to work) styleguide for commits, https://github.com/alphagov/styleguides/blob/master/git.md\n. Can we have a step that details how to set up a development environment to enable you to work on docker-py? It'll help ensure that our projects are accessible to people of all levels of experience.\n. I like this positive re-enforcement of questions being welcomed :smile_cat: \n. Re: Tests, it'd also be helpful to have an example command of how to run an individual test.\n. I think we want to put back in the c.inspect_image(BUSYBOX) after we've finished pulling it here.\n. I appreciate having a comment here is intended to be helpful, the vagueness of the description of what those factors are that's effecting the decision here isn't helping though. By removing the if six.PY3 statement it seems less clear to me now what one of those factors are.\nI prefer checking six.PY3 but I also know what the arguments are for doing it with hasattr. If you would like to keep the hasattr then it needs pulling out into a little function with some specific explanation. Something like _set_socket_timeout(socket) that can then include a doc string that explains what the circumstances are for when we have to set the timeout in different places. \n. Yeh, this is hard to test isn't it! While I'm not a fan of copying in the dockertpy code, I appreciate it's difficult to write a test without it.\nMinor point: please don't abbreviate variable names like nxt nxt_size, keep it fully expanded and descriptive, next next_size, readability is important. Thanks.\n. It's a pep8 style that we avoid abbreviated variable names for the same reason we avoid single letter variables in for loops. Unless there's a very specific reason/context to not have it, this is not one of those times.\nI appreciate the next conflicts with a reserved keyword, expanding it out to include more information is better than dropping a vowel. Thanks.\n. Yep, thanks for expanding that @shin- , apologies @delfick that I wasn't clearer.\n. Great. :+1: \nMinor: settimout's have a typo, missing the e.\n. ",
    "benwalther": "Note: the volume mount fails silently. I'd recommend a bug label; I should have mentioned that.\n. ",
    "ryanwalls": "I see that passing host config parameters in start() is now deprecated.  Closing this PR.\n. ",
    "aabdulwahed": "Kindly check out the code I am running:\n```\n    $ git clone https://github.com/aabdulwahed/cloudpots.git\n$ sudo pip install .\n\n$ cd agent/services\n\n$ sudo python manage.py\n\n```\nAlso, you'll find the code of calling docker-py in cloudpots/agent/client/container/ \n, pots dir where I am adding to container using dockerfile in cloudpots/pots/ and my docker file in cloudpots/docs/Dockerfile\n```\n # creating my potservice image\n$ sudo docker build -t potservice .\nstarting new container\n$ sudo docker run -p 4040:80 -it potservice\n```\n. Here is a sample of code to create and start containers container.py\n```\nimport simplejson as json\nfrom docker import Client\nDOCKER_BASEURL= 'unix://var/run/docker.sock'\nPORTS = [2200]\nclass ContainerEngine():\ndef __init__(self):\n        self.ports = [22]\n\ndef new_client(self,base_url=_DOCKER_BASEURL_):\n        return Client(base_url)\n\ndef create_container(self,client,\n                        image_id,\n                        command=None,\n                        mem_limit=None,\n                        cpu_shares=None,\n                        private_ports = []):\n\n        \"\"\"Initiate and Create Container\"\"\"\n        return client.create_container(image_id,\n                                        command,\n                                        detach=True,\n                                        ports = private_ports ,\n                                        mem_limit=mem_limit,\n                                        cpu_shares=cpu_shares)\n\ndef start_container(self,client, container_id, container_endpoints={}):\n        \"\"\"Start Container\"\"\"\n        return client.start(container_id, port_bindings = container_endpoints)\n\nc = ContainerEngine()\nclient = c.new_client()\nid = c.create_container(client, 'potservice', cpu_shares='1')\nc.start_container(client, id, container_endpoints={'80':'1616'})\n```\nand flask app pot.py and server.py that I created an Image for it.\n```\n   $ mkdir pots ; cd pots\n   $ vi pot.py\nimport os, sys, requests, json\nfrom flask import request\nfrom flask import Flask, session, render_template\nimport subprocess\npot = Flask(name,  static_url_path = \"/imgs\", static_folder = \"%s/imgs\"%(os.path.dirname(os.path.abspath(file))))\ngenerate random key for flask session\npot.secret_key = os.urandom(24)\nexecute command line\ndef run(command):\n    pipe = subprocess.PIPE\n    p = subprocess.Popen(command,stdout=pipe,stderr=pipe,shell=True)\n    return p.stdout.read()+\"\\n\"+p.stderr.read()\nmain page\n@pot.route('/')\ndef index():\n    return render_template('Hello World')\n@pot.route('/exec')\ndef execute():\n    command = \"bash /pots/bootstrap.sh\"\n    try:\n            if 'cmd' in request.args:\n                    command = request.args['cmd']\n            return json.dumps({'command': run(command)})\n    except:\n            return json.dumps({'ERROR':'expected command line'})\nif name == 'main':\n    pot.run()\n```\nand server.py \n```\nfrom pot import pot\nimport cherrypy\nfrom cherrypy.process.plugins import Daemonizer\nadd pot service in background\nDaemonizer(cherrypy.engine).subscribe()\nif name == 'main':\n# Mount the application\ncherrypy.tree.graft(pot, \"/\")\n\ncherrypy.server.unsubscribe()\n\n# Instantiate a new server object\nserver = cherrypy._cpserver.Server()\n\nserver.socket_host = \"0.0.0.0\"\nserver.socket_port = 80\nserver.thread_pool = 30\n\nserver.subscribe()\n\n\ncherrypy.engine.start()\ncherrypy.engine.block()\n\n```\nDockerfile\n```\n FROM ubuntu:14.04\nMAINTAINER Ahmed Abdullah ahmedaabdulwahed@gmail.com\n# install pre-requisites packages\nRUN apt-get update && apt-get install -y apache2 git python-pip curl \\\npython python-dev python-distribute tar git curl nano wget dialog \\\nnet-tools build-essential\n# add pot services to container\nADD /pots /usr/pots\nWORKDIR /usr/pots\nRUN pip install .\nEXPOSE 80\nCMD python server.py\n```\nsetup.py\nfrom setuptools import setup\nsetup(\n    name='cpot-manager',\n    version='0.1',\n    description='CLOUDPOTS provides creating container based clusters',\n    author='Ahmed Abdullah', author_email='info@cloudpots.com',\n    url='http://cloudpots.com/',\n    install_requires=['flask', 'requests','configobj', 'cherrypy']\n)\nregards,\nAhmed \n. @bfirsh No thanks for support!\n. ",
    "rojer": "let me provide additional clarification.\nthis bug is only triggered when registry 2.0 is used with token authentication directly with no proxy in front of it.\nbecause we've built a token authentication server (and are about to release it) there is no need for us to run the proxy.\n. ",
    "jesper": "Got the same issue - would be grateful if this was merged to master. \n. ",
    "newhook": "So when can get a new version of docker-py? This issue is causing some major pain for me.\n. Thanks!\n. ",
    "delcypher": "\nReturns the value -1 if the API responds without a StatusCode attribute.\n\nThis phrasing seems better to me.\n\nThis is interesting, because right now this exception will be returned any time the specified HTTP timeout is reached, it applies across the board, The fact that you seem to point it out as a Client.wait specific problem indicates that there may be a debate to be had about having this particular method behave differently, depending on whether we value being explicit over being consistent. Also it's a breaking change for people depending on the current behavior.\n\nYou don't necessarily have to break the interface. You could continue to raise the requests.exceptions.ReadTimeout exception but this needs to be explicitly documented and not changed. I personally don't like the fact you are leaking internal implementation details but it may be too late to change that.\n. @TomasTomecek requests and urllib3 claim to be thread safe. At a glance it looks like no additional state is maintained between method calls (although I could be wrong) in Client but I'm not very familiar with the code so I could be wrong.\n. This can probably be closed. #1583 implemented this and has already been merged into master.. @shin- You expressed interest in #1553 so I'd appreciate any help you can offer in getting this merged. It would also be great if a new release can be made so I can grab a version of docker-py that has the fixes in this PR.. ",
    "glogiotatidis": "btw pull parameter doesn't do anything with current released version (1.6) of docker due to https://github.com/docker/docker/issues/13631\n. See also https://github.com/docker/docker/issues/13631\n. Any updates on this? Since it's a bugfix and not a new feature it blocks a few things.\n. Luckily since this was broken anyway, setting it to False won't break anything. :)\n. Updated\n. ",
    "ilonajulczuk": "Hi, I have a similar issue, I would like to have a timeout parameter in exec_start.\n. ",
    "errordeveloper": ":+1: \n. Amazing!\nOn Fri, 19 Jun 2015 08:13 lukemarsden notifications@github.com wrote:\n\nwoohoo! Thanks @shin- https://github.com/shin- [image: :smile:]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-py/pull/628#issuecomment-113404117.\n. \n",
    "lukemarsden": "woohoo! Thanks @shin- :smile: \n. ",
    "dano": "@aanand The tests currently don't actually specify a tty setting - the changes I made add a tty=False setting to the mocked container object used in each test, just to keep the tests from failing because container['Config']['Tty'] is missing. I'll look into adding a test for the tty=True case in addition to just keeping the existing tests working.\n. Ugh, it looks like my attempt to squash my commits ended up screwing up the branch completely. I'll open a new request...\n. Ok, I've opened https://github.com/docker/docker-py/pull/669 to replace this. It addresses all the comments from @aanand above. Apologies for the hassle of the new request.\n. @shin- I share your concern - I just don't know what other option there is to determine if you're dealing with a TTY-enabled container or not. FWIW, it looks like the Docker CLI client uses the same technique (assuming I'm reading the source correctly).\n. ",
    "bcicen": "Closing in favor of #634\n. closing in favor of #638\n. ",
    "binarydud": "my bad, it is working now. Not sure what was going on. When it didn't work the first time and I couldn't see the previous versions on the pypi page, I assumed the versions were being removed.\n. ",
    "olsaki": "Sorry. The branch was lounging around for some time now and I forgot to rebase it properly. I will close this PR and open a new one after fixing that.\n. Can someone please review this?\n. @aanand @shin- ping. Any update on the idea of this in general? Please let me know if you think we should go ahead with this and I will continue to look into / work on the points mentioned above.\n. @dnephin good idea. Shouldn't be a problem to do that. Waiting for input from @aanand and @shin- to proceed.\n. ",
    "veggiemonk": "Hi everyone,\nThis fix breaks ansible docker module. Do you know it can be fixed? :cry: \nThanks in advance.\n. Thanks a lot! Will give it a try... :smile: \n. OK. It took me a while to figure out that it was on the server that the version needed to be pinned. \nThanks a lot for your help!\nCheers\n. ",
    "coderlol": "As it turned out, it was user error.  I used ansible docker module and had set docker_api_version to docker-py version \"1.2.3\".  That's incorrect.  \ndocker_api_version refers to docker api version ;), so docker_api_version: \"1.19\" fixed the problem.\n. ",
    "Melraidin": "@mpetazzoni Refactored a bit of that to avoid some duplication. Also allowed the provided config_path to override the default both for the old and new config locations.\n. On a side note it'd be nice if we could give some feedback to the caller on whether any authentication was sent. Without this docker-compose is left to simply say that a repo wasn't found with no indication that it could be a simple authentication issue.\n. @aanand I was aiming to follow the style later in the code for the legacy location where we're ignoring all exceptions but your point's totally valid. Will update.\n. ",
    "xavileon": "Setting detach=False did the trick. \n. ",
    "missedone": "right, Detached mode: run command in the background, so it's the expected behavior\nsee also: https://docs.docker.com/reference/commandline/exec/\n. ",
    "jasonamyers": "I'm also having this issue\n. I have this same issue with 1.4.0 of docker-py and 1.9.1 of docker-engine. It hits us when we try to push a large (>4gb) container.\n. ",
    "yank1": "+1\n. HI @aanand  , it has been fixed\n. ",
    "thieman": "The existence of this bug suggests a blindspot in integration testing, can we fix that?\n. Can we get this fix merged for 1.3.0 one way or the other? This is a serious problem for us.\n. I can confirm I'm seeing the same pattern on one of the two Macs I'm running this on currently. Not sure what difference might matter, but the one exhibiting the broken behavior is on 10.10 and the newer docker-py versions are working on my box with 10.11.\n. Seeing the same thing on my box exhibiting the problem\n/private/var/root/venv/lib/python2.7/site-packages/requests/packages/urllib3/util/ssl_.py:315: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#snimissingwarning.\n  SNIMissingWarning\n/private/var/root/venv/lib/python2.7/site-packages/requests/packages/urllib3/util/ssl_.py:120: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\n  InsecurePlatformWarning\nConnection error with protocol PROTOCOL_SSLv23: \"[Errno 1] _ssl.c:507: error:1407742E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert protocol version\"\n/private/var/root/venv/lib/python2.7/site-packages/requests/packages/urllib3/util/ssl_.py:120: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\n  InsecurePlatformWarning\nConnection error with protocol PROTOCOL_TLSv1: \"[Errno 1] _ssl.c:507: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed\"\nProtocol PROTOCOL_TLSv1_1 not found in SSL protocol list\nTraceback (most recent call last):\n  File \"<stdin>\", line 4, in <module>\nAttributeError: 'module' object has no attribute 'PROTOCOL_TLSv1_1'\nMight be a slight error in the script, on my box which works fine with the newer docker-py versions I get the following:\nConnection error with protocol PROTOCOL_SSLv23: \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:581)\"\nConnection error with protocol PROTOCOL_TLSv1: \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:581)\"\nConnection error with protocol PROTOCOL_TLSv1_1: \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:581)\"\nConnection error with protocol PROTOCOL_TLSv1_2: \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:581)\"\n. FWIW this is the code I run normally to get a client, this client works fine on my good box with 1.7.1+\n``` python\ndef get_docker_client():\n    \"\"\"Ripped off and slightly modified based on docker-py's\n    kwargs_from_env utility function.\"\"\"\n    env = get_docker_env() # this populates based off docker-machine env\n    host, cert_path, tls_verify = env['DOCKER_HOST'], env['DOCKER_CERT_PATH'], env['DOCKER_TLS_VERIFY']\nparams = {'base_url': host.replace('tcp://', 'https://'),\n          'timeout': None,\n          'version': 'auto'}\nif tls_verify and cert_path:\n    params['tls'] = docker.tls.TLSConfig(\n        client_cert=(os.path.join(cert_path, 'cert.pem'),\n                     os.path.join(cert_path, 'key.pem')),\n        ca_cert=os.path.join(cert_path, 'ca.pem'),\n        verify=True,\n        ssl_version=None,\n        assert_hostname=False)\nreturn docker.Client(**params)\n\n``\n. Trying to dump any info which might be relevant: my working box is using Python 2.7.9, non-working box is on 2.7.6. I think there were big changes to the SSL module in 2.7.9?\n. I realize nobody's said anything to the contrary yet but I agree, requiring 2.7.9+ would be a huge inconvenience. Hopefully we can restore functionality for 2.7.0+.\n. I can try that tomorrow and let you know if it works\n. I can confirm that both thessl_version=ssl.PROTOCOL_TLSv1` workaround and #971 work for me.\nThis is a major regression, right, if we think it's broken all versions < 2.7.9? What are the reasons to not get the fix out as 1.7.3?\n. Does anyone know whether the latest binary released by Compose works on pre-2.7.9 systems?\n. ",
    "schu": "+1\n. Thanks!\n. Done.\n. ",
    "rrbarrero": "+1\nAs \"workaround\", you can inject the file in the building process at the temp folder. I use this:\nsudo find /var/lib/docker/tmp/ -type d -name 'docker-build*' -exec cp $STARTFILE {} \\;\nin a shell script and run it with subprocess.\n. Ah! thanks.\n. ",
    "Jeffdude": "Okay thank you very much! version='auto' is a perfect option in this case. Glad that exists!\n. ",
    "estesp": "See docker/docker#14628 for my attempt to update Docker's Dockerfile to use the docker-py 1.3.0 release.\n. ",
    "novoxudonoser": "0\ndown vote\naccept\nOk, the problem was in:\nc=open('dockerfile.tar','r')\nshould be\nc=open('dockerfile.tar','rb')\n. ",
    "moutten": "By the way, thanks @shin-, your solution is much better.\n. The only issue that this causes, for my particular situation, is that docker-compose would still be broke since it registers the client API version as 1.18. I don't know really how to detect what version the docker server is expecting. I'm not sure that an API version was changed when this authentication header was changed.\n. This change was still necessary with 1.7.1. You'll see that the previous code was including a top level key of configs. Removing the top level key of configs works with 1.7.1. Based on Josh's comment, I'm not sure that was previously necessary. I haven't tried testing to see if removing the configs still works against older versions of docker (prior to 1.7).\n. I tested my setup using docker version 1.6.2, docker-compose 1.3.3, along with a patched version of docker-py, and I can verify it did work correctly with the change in this pull request. This looks like the cleanest way for docker-py to implement this change. It looks like I'm going to have to wait for docker-compose to change the supported API to 1.19.\n. ",
    "OriMenashe": "thanks guys !\n. ",
    "thaJeztah": "Looks like this needs a rebase\n. ping @shin- @aanand, thanks!\n. @shin- yeah, I thought about that, but I'm not a maintainer here, so :innocent: \nhappy to update though\n. Darn, wanted to ask on slack, but you weren't online; want me to remove Mazz in this PR?\n. ping @shin- PTAL\n. LOL, I'll revert that, will give a ping when done :smile:\n. @aanand @shin- updated again\n. Thanks!\n. ping @aanand @dnephin ptal :heart:\n. Also may be worth updating, because Ubuntu 15.04 is EOL'd and won't receive updates (including security updates)\n. Looks like https://github.com/docker/docker/issues/23596, and fixed by https://github.com/docker/docker/pull/23635, which will be in docker 1.12\n. Related: https://github.com/docker/docker/pull/27014, as there's a bug in the \"inspect\" output of services through the API\n. You'll have to either specify the API version to use, or set it to \"auto\"; https://docker-py.readthedocs.io/en/stable/client.html. ping @shin- ptal \ud83d\udc4d . ping @shin- @dnephin PTAL. I don't think this is related to docker-py; w.r.t. the command-line; the -e / --email option was deprecated for more than a year. The AWS team did make changes to adapt to this deprecation, but decided to default to the deprecated option, and require the user to set an additional --no-include-email to skip the deprecated option; see  https://github.com/aws/aws-cli/blob/9417311ddf284eaa02155aff850dd90b3e5d2c43/awscli/customizations/ecr.py#L49-L55\nDoes it work if you $(was ecr get-login --no-include-email)?. Yes, the email field is deprecated, but not removed; https://github.com/moby/moby/blob/b248de7e332b6e67b08a8981f68060e6ae629ccf/api/types/auth.go#L3-L22\nEven if it was, sending should not produce an error;\n\nThe API uses an open schema model, which means server may add extra properties to responses. Likewise, the server will ignore any extra query parameters and request body properties. When you write clients, you need to ignore additional properties in responses to ensure they do not break when talking to newer Docker daemons.\n\n```bash\ncurl -v \\\n  --unix-socket /var/run/docker.sock \\\n  \"http://localhost/v1.35/auth\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"password\":\"foobar\",\"serveraddress\":\"https://index.docker.io/v1/\",\"username\":\"thajeztah\",\"email\":\"foo@bar.com\"}'\n   Trying /var/run/docker.sock...\n Connected to localhost (/Users/sebastiaan/Library/Containers/com.dock) port 80 (#0)\n\nPOST /v1.35/auth HTTP/1.1\nHost: localhost\nUser-Agent: curl/7.54.0\nAccept: /\nContent-Type: application/json\nContent-Length: 108\n\nupload completely sent off: 108 out of 108 bytes\n< HTTP/1.1 200 OK\n< Api-Version: 1.35\n< Content-Length: 48\n< Content-Type: application/json\n< Date: Thu, 21 Dec 2017 15:52:35 GMT\n< Docker-Experimental: true\n< Ostype: linux\n< Server: Docker/17.12.0-ce-rc3 (linux)\n< \n{\"IdentityToken\":\"\",\"Status\":\"Login Succeeded\"}\nConnection #0 to host localhost left intact\n```\n\n\nOne thing I do notice, is that the 400 error coming from ECS is converted into a 500 error by the Docker API, which should be corrected. The stats endpoint in this case is a streaming endpoint, so the client has to stay connected to get the \"previous\" and \"current\" stats; The \"previous\" (precpu_stats) one will be reset if you disconnect, and open a new connection.\n(I have no experience with the docker-py SDK, but possibly this will help you further \ud83d\ude05). LOL, there's where my Python knowledge ends \ud83d\ude05 . Updated; now removed the test instead. Note that using API 1.25 will still return the container details; see https://github.com/moby/moby/pulls/30673\nThe list of containers was removed from the network list because it can be a heavy operation in larger setups (many containers, many networks), (inspecting a single network still returns attached containers). The format in the service-create API request needs to be [\"<ip-address> <hostname>\", \"<ip-address-2> <hostname 2>\"]\nFor example, here's the request that's sent by the docker cli for docker service create --name  foo --host some-host:1.1.1.1 nginx:alpine;\njson\n{\n  \"EndpointSpec\": {\n    \"Mode\": \"vip\"\n  },\n  \"Labels\": {},\n  \"Mode\": {\n    \"Replicated\": {}\n  },\n  \"Name\": \"foo\",\n  \"TaskTemplate\": {\n    \"ContainerSpec\": {\n      \"DNSConfig\": {},\n      \"Hosts\": [\n        \"1.1.1.1 some-host\"\n      ],\n      \"Image\": \"nginx:alpine@sha256:34aa80bb22c79235d466ccbbfa3659ff815100ed21eddb1543c6847292010c4d\"\n    },\n    \"ForceUpdate\": 0,\n    \"Placement\": {\n      \"Platforms\": [\n        {\n          \"Architecture\": \"amd64\",\n          \"OS\": \"linux\"\n        }\n      ]\n    },\n    \"Resources\": {\n      \"Limits\": {},\n      \"Reservations\": {}\n    }\n  }\n}\nAlso note that entries should not be sorted when creating  a service; the order in which extra hosts are provided should be kept (see https://github.com/docker/cli/pull/651)\nThe conversion that's used in the Docker CLI can be found here; https://github.com/docker/cli/blob/97b148b440f052679bd80cf2779801494c428afe/cli/compose/convert/service.go#L388-L399. ping @shin- \ud83e\udd17 . Thanks @shin- ! (Not enough experience with python though to review \ud83d\ude05). The endpoint you're describing is the Docker (engine) API, not the registry API; the registry does not expose that API. The volumes format is known to have some limitations, because it uses a colon as separator between options; does it work if you useMount option instead? https://docker-py.readthedocs.io/en/stable/api.html#docker.types.Mount\nLooks like you want to do a bind-mount, in which case you have to set type to \"bind\". I'll have to defer that one to @shin-, I'm not very familiar with the docker-py sdk (or python, for that matter) :blush:. If there's no TTY attached, the API uses a stream format that pretends each line with 8 bytes that can be used to (eg) determine if a line was printed on stdout or stderr; see https://docs.docker.com/engine/api/v1.37/#operation/ContainerAttach\n. > But docker-py should take care of that\nHm, looks like it should yes (sorry not too familiar with the docker-py codebase; my eye just fell on this ticket)\nTo be sure these random characters are indeed the 8-bytes of stream information; @chris3081 what happens if you start the container without TTY; do the random characters disappear? (In that case it would attach to the container output, and don't add the stream information). Is this expected? (https://jenkins.dockerproject.org/job/docker/job/docker-py/job/PR-2216/4/execution/node/25/log/)\n20:23:39 [docker_docker-py_PR-2216-VUUIZLMALJLRQO2LIHC36PEUWVSMCGAOFNW2SQKYNEQKZ7MZMHEA] Running shell script\n20:23:40 + docker pull dockerbuildbot/docker-py:py2-219c52141e3cd15db3348c5420220f640323499f\n20:23:41 Error response from daemon: manifest for dockerbuildbot/docker-py:py2-219c52141e3cd15db3348c5420220f640323499f not found. @chris-crone https://github.com/moby/moby/pull/38467 likely won't help in this case, because the daemon sent a 200 status before it detects the error; https://github.com/moby/moby/pull/38467 will make the client return the status code (instead of just a plain error).\nI can try if waiting to send the \"The push refers to repository\" message until after the initial error detection was done could allow us to send a proper status code (and prevent the 200 status from being sent before that).\nFWIW, I do think we still need the python sdk to detect errors if an errorDetail arrives; even though this initial error check could make it possible to send a 4xx status code, there may be situations where an error occurs further down the push (e.g. push is in progress, but the registry returns a failure). Due to this endpoint returning a stream, the status code cannot be updated at that point.\n\nAlso, as far as this specific error is concerned, the Docker Engine really should do some trivial validation and detect if a tag doesn't exist locally and send a 4xx HTTP response rather than attempt a push anyway.\n\nAlso see the above; in this case it looks like the validation is actually done before the actual push happens (but the initial message was already sent; I'll try and see if that particular case can be resolved). Typo s/includign/including/\n. I don't think this is the right approach; docker now supports logging driver plugins, which may (or may not) support reading. Hard-coding these options here will block that option.\nthe 500 error returned does not look right to me though, I think that should be fixed. Question is if different logic is needed based on the error; running with docker directly;\n```\n$ docker run -dit --log-driver=none --name foo busybox\n$ docker logs foo\nError response from daemon: configured logging driver does not support reading\n``. I wonder why it's using the logging endpoint, and not attaching to the container (which is whatdocker run` does). hm, perhaps this test should be removed altogether if this is testing that the name can be updated (let me know if it should). I'll update the PR, thanks!. Is this change needed? I think moby already sets the default if no name is specified; https://github.com/moby/moby/blob/47b3209f687c240dc2841bcfc147b2ffa341a7fe/daemon/cluster/swarm.go#L460-L464. Can we make the change in moby/moby so that the default is set there if omitted? It would still produce an error if any name other than \"default\" is given, but at least it won't be a breaking change for users of the API.\nTL;DR no changes in the docker-py tests should be needed, and if there are, it means we introduced a breaking change. I'm okay with adding a test to test that updating both with and without a name set works, but want to make sure that the current behaviour doesn't break (if name was intended to be a required parameter, we should've enforced that long ago; unfortunately we didn't, so we'll have to deal with that). ah, dang; let me update \ud83d\ude05 - I'm really bad at python. Updated; also happy to remove the new is_error if you think we should just inline it here. \ud83e\udd26\u200d\u2642\ufe0f updated again; thanks for bearing with me \ud83e\udd17 . ",
    "mpeuster": "Maybe it helps others who stumble across it: \nIt should be mode \"br\".\npython\nc = docker.Client()\nwith f = open('./fedora.tar', 'br'):\n    c.load_image(f)\n. ",
    "harrisonfeng": "This because password argument is not handled under reauth=True\n. ",
    "jhowardmsft": "@shin- @mpetazzoni @aanand - Guys, any chance of getting a review on this? It's needed as a pre-req for a docker PR which we need for the Windows daemon to support the upcoming TP3 docker release. Thanks :)\n. @shin- I have it passing again, but I'm not sure how to address the remainder of your comments. (Python is new to me entirely).\n. Oh that's much better! LGTM (but not verified against docker and my docker change yet).\n. Absolutely. Will try and get to it this evening.\n. @shin- Verified against my branch for PR 14530 in docker. LGTM :+1:  Thanks for following through :)\n. Hi @aanand  - The reason relates to https://github.com/docker/docker/pull/14530 and mentioned in https://github.com/docker/docker-py/pull/691. The daemon with 14530 will do network mode validation - previously it was done by the CLI. This is necessary as part of the port of the daemon to Windows where network modes are different between Linux and Windows. Hence, a more complete host config is necessary to be passed to the daemon. Strictly speaking, it probably should have been there before as many of the tests aren't passing a \"complete\" host config structure, but instead are passing the minimal set to allow the docker-py tests to pass. Hope that makes sense :)\n. @shin It looks like this PR is making Janky and Experimental fail on moby/moby master. Can you investigate? From the name of the commit here, and the failures in moby/moby Jenkins, and the timing of this being merged, this seems to line up. https://github.com/moby/moby/pull/33241#issuecomment-309933669 for more info.\ncc @crosbymichael . Just looked at v1.6.0 docker\\docker runconfig\\parse.go, func parseNetMode L445. (Sorry, can't work out how to get a link directly to a previous version). The valid cases are bridge, none, host and container. Looks like default didn't arrive until later. \n. Yup, you may well be right. I'd need to look at the flow more closely, but I believe there are cases where create_container_config is called without create_host_config. I'm not sure calling create_host_config from create_container_config is the right thing to do.\n. Using none is (IMO) cleaner as it pulls in \"less\" for the test on the daemon side (bearing in mind networking isn't needed for these tests). \n. Hmm, I'm not sure what I should do here in that case :)\n. Sure. I should have used none in the others lol. I'll update :)\n. Yes, I think you're right. Removing.\nEdit: Looks like it needed here. Reason being because of the nesting of create_container and create_host_config. The test will fail without it being present. \n. @shin- can correct me, I'm not a python person at all. I believe its just a shortcut to ensure that network_mode is always present when calling create_host_config. Therefore you don't need to have the code I originally had to add to the dictionary if it isn't present. \n. ",
    "icecrime": "Ping @shin-: can we move this forward please?\n. ",
    "rmb938": "Is there any updates on getting this branch merged or something like it implemented? @shin- @wallrj \n. Sounds good. Will start working on those.\n. @shin- Yup no problem. Sorry for the late reply I will get it done this weekend.\n. @shin- sorry for the delay. I got everything rebased. if it all looks good I can squash all the commits for you.\n. @shin- how does that look\n. Also are the docs more focused on github or ReadTheDocs? If ReadTheDocs using sphinx could help with organization.\nSee http://sphinx.readthedocs.org/en/latest/\n. Well more of adding categories so I can click a link and only see all the api's related to images, or all the apis related to containers, ect..\nThe current layout just seems like a lot of information all on one page without anyway for drilling down and seeing specific apis.\nSo an example would be:\nSomeone is relatively new to docker and knows how to use commands but now wants to build things on top of the api. Looks at the docker-py RTD and sees a whole page of API docs but they are just interested in API's to do with containers. So now they have to scroll and look or CTRL+F for key words to find certain things.\nThere is also very few examples for things. So if someone new to python or just doesn't want to look through the api page wanting to do something i.e creating a new container, there isn't something that they can just look at and easily understand. Yes there are examples for port bindings and using volumes but those examples could be to complex for someone new to python or wanting to just get up and running very quickly.\n. Yea a quickstart guide is what I was going for. \nAlso with sphinx at least the different API methods wouldn't need to be under different pages. I believe you just need to give it a different type of header and it will automatically group things in the left navbar. So under \"Client API\" there would be a container group or a images group and clicking that would scroll the page to that specific group.\n. Any updates on this? I recently ran into this issue.\n. @TomasTomecek yup no problem, how does that look?\n. @shin- Is it possible that we can get this merged sometime soon?\n. I've rebased this ~15 times since creating this pull request while other things have gotten merged and would like this to get merged as soon as possible. I have worked around not having this as best as I could but now it is a major blocker. @shin- I would appreciate if this can be merged and released relatively soon.\n. That makes sense, moving it now.\n. ",
    "abronan": "@danehans support for networking in docker-py is on the way, you can take a look at #729 :)\n. ",
    "danehans": "@abronan thanks for the follow-up. i'll keep an eye on #729 \n. ",
    "mrfuxi": "I would like to propose 3 options:\n1. escape whole path in _url\n   - Pro:\n     - transparent to existing code\n     - never to be forgotten\n   - Con:\n     - might need to be a bit less strict then if handling format argument (ie. '/'would be allowed because it would be in the path already)\n2. wrapper suggested by @aanand \n   - Pro:\n     - could be rather strict as most likely only images and container info will go there\n   - Con:\n     - has to be added every time, easy to forget/miss\n     - current code would need to be modified\n3. _url to accept string or tuple as first arg instead of just a string. Ie: self._url((\"/exec/{0}/json\", exec_id), versioned_api=True). Then format would happen inside _url. Cleanup could be more strict on formatting args and less strict for complete path.\n   - Pro:\n     - harder to forget (passing a string would still be allowed)\n     - could be rather strict as most likely only images and container info will go there\n   - Con:\n     - still possible to pass something dangerous\n     - current code would need to be modified\nI would still have a general question regarding: try to escape or just raise exception if something incorrect is found in path\n. ",
    "miketonks": "We could really use this feature, and the update to compose.\nAny chance someone could squash the commits or merge it anyway?\n. ",
    "goodwillcoding": "Are there any plans to address this?\n. ",
    "philpep": "The strange thing is that the docker native client do it:\nconsole\n$ id=$(docker run -d ubuntu tail -f /dev/null)\n$ docker exec $id ls -l /root /nonexistent 2>/tmp/stderr.txt; echo \"STDERR: \"; cat /tmp/stderr.txt\n/root:\ntotal 0\nSTDERR: \nls: cannot access /nonexistent: No such file or directory\nDocker logs during exec:\ntime=\"2015-08-12T10:47:32.499394595+02:00\" level=debug msg=\"Calling POST /containers/{name:.*}/exec\"\ntime=\"2015-08-12T10:47:32.499433254+02:00\" level=info msg=\"POST /v1.20/containers/92b53d25e274fa78a538678be432702a308246b4af7f10a1ff98a3acf5dd4a7e/exec\"\ntime=\"2015-08-12T10:47:32.499964918+02:00\" level=debug msg=\"Calling POST /exec/{name:.*}/start\"\ntime=\"2015-08-12T10:47:32.499988307+02:00\" level=info msg=\"POST /v1.20/exec/1c13e8b5ca4e3755dc3c776635328c14d2e43806e4caf4b480ad3daa6ac4d121/start\"\ntime=\"2015-08-12T10:47:32.500051966+02:00\" level=debug msg=\"starting exec command 1c13e8b5ca4e3755dc3c776635328c14d2e43806e4caf4b480ad3daa6ac4d121 in container 92b53d25e274fa78a538678be432702a308246b4af7f10a1ff98a3acf5dd4a7e\"\ntime=\"2015-08-12T10:47:32.500113023+02:00\" level=debug msg=\"attach: stdout: begin\"\ntime=\"2015-08-12T10:47:32.500136084+02:00\" level=debug msg=\"attach: stderr: begin\"\ntime=\"2015-08-12T10:47:32.631466826+02:00\" level=debug msg=\"Exec task in container 92b53d25e274fa78a538678be432702a308246b4af7f10a1ff98a3acf5dd4a7e exited with code 2\"\ntime=\"2015-08-12T10:47:32.631522181+02:00\" level=debug msg=\"attach: stdout: end\" \ntime=\"2015-08-12T10:47:32.631553231+02:00\" level=debug msg=\"attach: stderr: end\" \n2015/08/12 10:47:32 http: response.WriteHeader on hijacked connection\ntime=\"2015-08-12T10:47:32.631810063+02:00\" level=debug msg=\"Calling GET /exec/{id:.*}/json\"\ntime=\"2015-08-12T10:47:32.631823120+02:00\" level=info msg=\"GET /v1.20/exec/1c13e8b5ca4e3755dc3c776635328c14d2e43806e4caf4b480ad3daa6ac4d121/json\"\nI don't known how to inspect the complete POST and GET requests.\n. ",
    "sharkspeed": "had anyone fixed this ?. ",
    "Darnok99": "I'm also interested in getting that separated. Shell way of doing things is to put everything you want other people to see in stdout and all of the debugging in the stderr. With docker run we can only get either mixed output or each of streams in separate. ",
    "jeffrey4l": "From the docker http api, seems we can not separate the stdout and stderr from there.\nBut i curiosity that how \"docker\" client solution this issue? because it support this.. This is duplicated with #1321 . ",
    "jverhoeven": "Thanks for getting this in docker-py. Need it for maestro-ng support.\n. ",
    "jongiddy": "To avoid confusion with the other exec_* calls which do work with 1.15, it may be better to change the exception message to exec_inspect is not supported in API < 1.16.\n. ",
    "zubron": "You're right, it could definitely cause confusion. I've updated the error message.\n. ",
    "alunduil": "+1, also seeing this on docker-py-1.3.1.\n. No, I'm not sure of anything anymore.  I was seeing this behavior with docker-compose-1.3.3 (and 1.4.0), docker-py-1.3.1, and docker-1.7.1 but have a working docker-1.8.1 now.  Disregard my noise, it's probably nothing.\n. Piggybacking on this issue, the URL for docker-pycreds on PyPI points at (I assume) a private repository so it's not possible to get a complete tarball as an alternative either.\n. ",
    "cpburnz": "@aanand The syntax error is due to Python 3.0\u20133.2 not supporting explicit unicode literals which were re-added to Python 3.3 with PEP 0414.\n. @aanand The Python 3.2 syntax error has been fixed and pathspec 0.3.4 has been released.\n. ",
    "Kroderia": "Mistaken usage. Should use Client(local_addr).login(registry=remote_addr).\n. ",
    "bedwards": "added gelf too - https://docs.docker.com/reference/logging/overview/\n. ",
    "joelacrisp": "Ok thanks!\n. ",
    "iafilatov": "I think 5d95f24 specifies how the Devices key looks in the resulting host config dict, not what the param should look like. Inside create_host_config the param is passed to parse_devices (https://github.com/docker/docker-py/blob/master/docker/utils/utils.py#L369), which expects it to be a list of strings like you would use in docker run --device=.... \n. ",
    "mavenugo": "@aanand yes. the API has not landed into docker proper yet. It is still under experimental. We are waiting on a few fixes to be merged before moving the API/UX out of experimental.\n. the apis are being simplified and as @shin- indicated, there will be a /connect api with container as the resource of interest. sandbox is a backend detail and will not be exposed in the API.\nThis API that @aanand is using is based on experimental version and will be changed soon.\nI will add an update here once the docker/docker API/UX change PR is pushed.\n. ",
    "bboreham": "Getting a bunch of failures from the integration test now - doesn't it need to be updated to cope with this change?\n. Yeah, we moved to CircleCI for Docker-based tests.\n. Obsoleted by #747\n. @GordonTheTurtle it's signed now.\n. I'm not sure what your bot didn't like about my signature, but evidently re-signing it made it happy.\nHave also re-based against master.\n. @saifulhoque26 it seems to be tricky to code this for arbitrary test environments.\nPerhaps the test should be skipped if multiple interfaces are present?. ",
    "rade": "IIRC this isn't the first time the integration tests got broken. @shin- @aanand How can we prevent future occurrences of this? Could you set up some CI that runs the integration tests on PRs?\n. I believe the test was run against Docker 1.7.1. Could that be the issue, i.e. is the new test depending on some Docker 1.8+ functionality?\n. The test is run in the latest joffrey/docker-py container. The server is 1.8.2.\n. Is it possible that it might be picking a random port that is already taken?\nOne would think this would cause start to blow up, but perhaps it doesn't.\n. ",
    "choleraehyq": "Is there anybody working on this? I'd like to give a hand.\n. Great!\n. ",
    "odigity": "It was finished 11 days ago.  (See issue 14242 linked above.)\nI'm guessing it will get released in 1.9.0.\n. My bad.\n. ",
    "cholerae": "@odigity Oh, I meant code in docker-py.\n@TomasTomecek Thanks for reminding me.\n. ",
    "ibmalan": "How to solve this issue?any help will be great appreciate!\n. I have solved the issue.\n. you need to make sure the both names are same when you saving docker image on compute node.\n. ",
    "cheneydc": "@ibmalan Hi, I got the same porblem, could you tell me how you fix it?\n. @ibmalan oh, you're right, thanks for your help :)\n. ",
    "funkyfuture": "make test also passes.\n. of course one could explicitly formulate in the README that there's no guarantee that PyPy will be supported, but efforts are made (and thus it should be tested).\ni wouldn't anticipate any changes to a rest-adapter that still could break compatibility. you'll mainly extend the interfaces according to Docker's evolvement, right?\n. afaik, compatibility issues with PyPy are expected when using C-extensions and the inspect-module.\n. rebased on master and the tests still pass with PyPy, i added a config that allows them to fail on travis anyway.. to elaborate a little more.\nto get the labels to parse (within parse_labels above), the code called client.inspect_container, now it calls client.api.inspect_container.\nmy first approach was to mock like this: \npython\nmocker.patch('deck_chores.config.DockerClient.api.inspect_container',\n             return_value={'Image': '', 'Config': {'Labels': labels}})\nwhich yields this error:\n```\nthing = , comp = 'api'\nimport_path = 'deck_chores.config.DockerClient.api'\ndef _dot_lookup(thing, comp, import_path):\n    try:\n        return getattr(thing, comp)\n    except AttributeError:\n\n\n      __import__(import_path)\n\nE           ImportError: No module named 'deck_chores.config.DockerClient'; 'deck_chores.config' is not a package\n```\n\nindeed, that is a module (where DockerClient is imported), not a package. so, i don't get the error message, my guess is this is due to the fact that the api attribute is bound when the client is initialized.\nthe next try is this what follows the paradigm 'mock where it is used' that worked so far for me.\npython\nmocker.patch('docker.api.client.ContainerApiMixin.inspect_container',\n             return_value={'Image': '', 'Config': {'Labels': labels}})\nwhich evokes this when client.api.inspect_container is called:\n```\nself = , name = 'api'\ndef __getattr__(self, name):\n    s = [\"'DockerClient' object has no attribute '{}'\".format(name)]\n    # If a user calls a method on APIClient, they\n    if hasattr(APIClient, name):\n        s.append(\"In Docker SDK for Python 2.0, this method is now on the \"\n                 \"object APIClient. See the low-level API section of the \"\n                 \"documentation for more details.\")\n\n\n  raise AttributeError(' '.join(s))\n\nE       AttributeError: 'DockerClient' object has no attribute 'api'\n```\n\nno idea what's going on here.\n\nmy guess is that mocking a less complex assembled class would work. in my case that would need an image and a labels property on models.containers.Container and the latter also on models.images.Image. that seems feasible to me anyway. shall i go for it? (yep, other models would have labels too).. for two reasons i'd rather not want to do this:\n\ni'm using methods on a DockerClient instance anyway\ni couldn't use from_env to get an instance of an APIClient\n\nessentially i would rather use the 1.x than using a seemingly partly redundant api.\nregarding the last thoughts in my previous post: would there be a problem to add higher-level access to the mentioned properties on models? in my understanding these are missing anyway. but maybe there was a reason to not include them in the models.. imo, with the next breaking release Container.exec_run should return two values in that order: the exit code and the tty output, if stream is False.. @shin- do these features have a chance of getting merged somehow?. @dfoderick please close this issue.. some documentation would be helpful for reviewing and users.. there's no need to wrap the return values in brackets. also, i'd say that returning the exit code as first values seems more appropriate.. run2 is certainly not a descriptive name. what about a switch like stream that controls the return value?. i prefer the explicit over the implicit.. ",
    "eleweek": "\nYou should get a deprecation warning when passing arguments to start\n\nThere was a warning, and I guess I shouldn't have ignored it! :) \n\nand the current plan is to remove that ability entirely in the near future\n\nSounds great! In the meantime, maybe it'd be nice to retrieve current config from docker and check if it has any non-default settings? (And then e.g. raise an exception)\n. ",
    "toshitanian": "I also spent a few days on this issue :(\n. ",
    "rakesh-roshan": "With http url it worked. It is not working with an unix socket.\n. ",
    "grahamlyons": "Python 3 users will already be used to dealing with byte-decoding so documenting this would be the best approach.\nAltering the return types of these methods to indicate either str or bytes would be relatively simple:\nhttps://github.com/docker/docker-py/blob/6fe1c7f/docker/models/containers.py#L228-L229\nHow about changing those mentions to something like this:\nReturns:\n            (generator or str|bytes): Logs from the container.. I also see this problem with the latest docker-py.\nMinimal steps to reproduce, on CentOS 7 with docker-ce-17.03.1:\ndocker run --rm -it -v /var/run/docker.sock:/var/run/docker.sock:ro python:3 \\\npip install docker && \\\npython -c \"import docker; docker.from_env().containers.run('alpine', detach=True, command=['/bin/sh', '-c', 'sleep 20']).stop()\"\nResults in:\nrequests.exceptions.ReadTimeout: UnixHTTPConnectionPool(host='localhost', port=None): Read timed out. (read timeout=10)\nIt looks like the timeout value for the stop method might be being passed to requests.\nUpdate:\nThe logic in the stop method on the API takes the stop timeout and passes it as a parameter to the Docker API, then it sets the HTTP read timeout to that plus whatever timeout is set for the client, defaulting to 0:\nhttps://github.com/docker/docker-py/blob/dc2b24/docker/api/container.py#L1117\nThe initialisation of the client defaults that timeout to DEFAULT_TIMEOUT_SECONDS, which is 60:\nhttps://github.com/docker/docker-py/blob/dc2b24/docker/api/client.py#L87\nHowever, using the from_env method of the DockerClient class sets the timeout argument to None:\nhttps://github.com/docker/docker-py/blob/dc2b24/docker/client.py#L76\nThis means that the request times out at the same point that the container is forcefully stopped so we don't see the request return and the exception is raised from requests.. Looks like this can be closed.. Is the \"janky\" check configured correctly? It doesn't have a link to a Jenkins job and it hasn't completed on the last large handful of pull requests.. I'm not sure what the failure on Jenkins relates to; can anybody assist with that?. @shin-, is that the kind of change you envisaged? Let me know if not.. Awesome. Thanks, @shin-. In that instance the configuration is taken from the environment, as described here: https://docs.docker.com/engine/security/https/\ni.e. you can set DOCKER_TLS_VERIFY, DOCKER_HOST, and DOCKER_CERT_PATH and they'll be handled.\nIt's mentioned in the documentation - http://docker-py.readthedocs.io/en/latest/tls.html?highlight=configured+automatically+from_env() - but the specifics are in the main Docker docs, linked above.. I can't see what's gone wrong with AppVeyor, I'm afraid. Is Python 3.6 missing from there?\nEdit: Looks like it should be there: https://www.appveyor.com/docs/build-environment/#python. Well I'm fresh out of ideas.... ",
    "PierreF": "Added test.\nTest test using the fake API does not check returned value. I don't know how to do that (response that change depending on params).\nAnyway, integration test does check the output.\n. ",
    "devmapal": "pip freeze | grep docker-py && python --version && docker version\ndocker-py==0.7.2\nPython 2.7.6\nClient version: 1.0.1\nClient API version: 1.12\nGo version (client): go1.2.1\nGit commit (client): 990021a\nServer version: 1.0.1\nServer API version: 1.12\nGo version (server): go1.2.1\nGit commit (server): 990021a\n. Yes, I would love to upgrade docker, but for now I'm unfortunately stuck with the version from the official ubuntu 14.04 package repositories, which is 1.0.1\n. ",
    "tomwilkie": "And again: https://circleci.com/gh/weaveworks/weave/3038\n. ",
    "slavaboiko": "@dnephin Thanks for the tip, I've added the error in case the API doesn't support the flag.\n. ",
    "gotgenes": "@bfirsh Thanks for the update. I couldn't find any comments on #1186 regarding the API change directly addressing this issue. Would you be able to provide more information?. @bfirsh Great! Thanks for following up!. ",
    "themistymay": "Any interest in this at all? If not, I can close this and get it out of your queue.\n. Closing due to inactivity\n. ",
    "akalipetis": "Added tests, rebased with master and :gift: added an .editorconfig file to the repo.\n. ",
    "Davy1992": "This is really helpful, but can you guys merge it into the master branch pls?\n. @dnephin Thank you!\n. ",
    "dag24": "To clarify, I have redacted the registry, namespace and image_name from the above.\n. ",
    "alasdairnicol": "My initial patch was wrong, because create_host_config is a regular method, not a classmethod. I've fixed the patch and pushed it again.\n. Thanks for the quick review! I've squashed into a single commit and rebased.\n\nI'm not the biggest fan of calling the client instance cli because of the \"Command Line Interface\" acronym. \n\nSure, I understand. I chose cli because it appeared to be the most common variable name, not for any other reason.\n. ",
    "ZYNCMA": "It seems that they are deprecated arguments of create_container.\n```\nif compare_version('1.19', version) < 0:\n    if volume_driver is not None:\n        raise errors.InvalidVersion(\n            'Volume drivers were only introduced in API version 1.19'\n        ) \n    mem_limit = mem_limit if mem_limit is not None else 0\n    memswap_limit = memswap_limit if memswap_limit is not None else 0\nelse:\n    if mem_limit is not None:\n        raise errors.InvalidVersion(\n            'mem_limit has been moved to host_config in API version 1.19'\n        )   \nif memswap_limit is not None:\n    raise errors.InvalidVersion(\n        'memswap_limit has been moved to host_config in API '\n        'version 1.19'\n    )\n\n```\nCan we make a compatibility here instead of raising an error? Such as assign them in host_config and reset them to None?\n. Close issue as nobody cares about it.\n. ",
    "aebm": "Hi,\nIt would throw an decoding exception that is logged at the debug level  in load_config, ending with load_config returning a empty dict. When pulling the image you don't have auth and the error shown is that the image doesn't exists\n. Hi,\nI will close it to open it again to fix the tests\n. Hi this issue stills happens in\n```\ndocker==2.0.1\ndocker-pycreds==0.2.1\nPython 2.7.6\nClient:\n Version:      1.12.6\n API version:  1.24\n Go version:   go1.6.4\n Git commit:   78d1802\n Built:        Tue Jan 10 20:26:30 2017\n OS/Arch:      linux/amd64\nServer:\n Version:      1.12.6\n API version:  1.24\n Go version:   go1.6.4\n Git commit:   78d1802\n Built:        Tue Jan 10 20:26:30 2017\n OS/Arch:      linux/amd64\n```\nTo reproduce this issue you first need to pull have an image without tags. For example pulling an image by hash\nbash\ndocker pull busybox@sha256:${IMAGE_HASH}\nThen run this snippet \n```python\n!/usr/bin/env python\nimport docker\nclient = docker.from_env()\nimage_list = client.images.list(all=True)\nfor image in image_list:\n    print 'Id:', image.short_id\n    print ' Tags:', ','.join(image.tags)\n```\nYou get this exception\n```\nTraceback (most recent call last):\n  File \"./xxxx.py\", line 8, in \n    print ' Tags:', ','.join(image.tags)\n  File \"x/python2.7/site-packages/docker/models/images.py\", line 34, in tags\n    tag for tag in self.attrs.get('RepoTags', [])\nTypeError: 'NoneType' object is not iterable\n. ",
    "andyshinn": "The Compose example is helpful. Thanks! I think this is definitely something worth calling out in documentation so i'm not going to jump to close just yet.\n. There are actually a couple spots I will cherry pick. Along with the progress stream, https://github.com/docker/compose/blob/b6b9b002/compose/service.py#L716-L729 seems like a better example of calling cli.build in general.\n. ",
    "minskmaz": "This gets passed into create_container on the host_config key documented here:\nhttps://github.com/docker/docker-py/blob/master/docs/hostconfig.md\n. ",
    "leonty": "Hi @shin, I have a ready PR for this... just have stuck with the signing...\n. Done!\n. Done!\n. @shin what's the due date for the 1.6.0 milestone?\n. @dnephin voil\u00e0\n. @shin- no problem. But I used the existing tests as a reference and they don't clean anything after (all integration build tests)...\n. @shin- done. Though I'm not sure the squash was correct, have never done it before\n. Hi, is this accepeted? I'm about to push another PR for Ansible to use this functionality and just waiting to know for sure that this API will be in 1.6.\n. Done. But I don't know why the integration test has failed, for me locally it works:\n```\n\n\n\nimport docker, io\nc = docker.Client('127.0.0.1:4243')\nscript = io.BytesIO('\\n'.join([\n            'FROM scratch',\n            'ARG test',\n            'USER $test'\n        ]).encode('ascii'))\nc.build(fileobj=script, tag='buildargs', buildargs={'test': 'OK'}, stream=True)\ninfo = c.inspect_image('buildargs')\ninfo\n{'Os': 'linux', 'Parent': 'bc2b3187f9a63a6bc427c2b6a80675ec4885a4262d8f8ed7b04ba14c9b848ce2', 'RepoTags': ['buildargs:latest'], 'Config': {'Hostname': '4ee80013\n...\n```\n. Ok, thanks.\n. Great! Thanks again.\n. \n\n\n",
    "containscafeine": "Could you post the output of\npython --version\n. Well, taking a shot in the dark!\nCould you post the output of:\npython2.7 -c 'import sys; print(sys.path)'\nAnd in any one of those paths, does the directory docker exist?\nIn my case, it is located at /usr/lib/python2.7/site-packages/docker/\n. Hi @dustymabe,\nI was working on this for a couple of days, and I have written a small (and hacky) tool for this - shipy.\nHere is a quick PoC for the tool.\nIs this something that the docker-py community is interested in?\nIt would be very useful to have something like python -m docker.tool docker run -v /tmp/:/tmp/ busybox ping 8.8.8.8, so simply passing the docker commands to docker-py with no conversion on the user side, and getting better parsable output.\nIt would be great if I could get some feedback on the idea.\n. All, thanks for the feedback! :) \n@dustymabe definitely, on it!\n. ",
    "CatherineH": "Done, see original question\n. I uninstalled docker-py 1.6.0-dev, then re-installed the version from pip, and now it can import. I'm wondering if there's some setup step in the latest version that wasn't in pip. I'm going to make a copy of my machine and try again. \n. Alright, reverting back to the original state:\n$ python2 -c \"import docker\"\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nImportError: No module named docker\n$ python3 -c \"import docker\"\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nImportError: No module named 'docker'\nThe $PYTHONPATH variable is the same both when docker-py installed from pip as installed from this repo - it's empty.\n. To try to describe exactly my steps:\n- download and install vmware player \n- create a fresh ubuntu14.04 machine\n- install python-setuptools\n- clone pip from the github repo and install\n- sudo pip install docker-py\nThen, running the command \npython2.7 -c 'import sys; print(sys.path)'\nyields:\n['', '/usr/local/lib/python2.7/dist-packages/pip-8.0.0.dev0-py2.7.egg', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-i386-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages/PILcompat', '/usr/lib/python2.7/dist-packages/gtk-2.0', '/usr/lib/python2.7/dist-packages/ubuntu-sso-client']\nNo docker.\n. I installed pip from the github repo (this is because I believe the debian package python-pip to be too out of date). Then used pip to install docker-py, i.e.:\ngit clone https://github.com/pypa/pip\ncd pip\nsudo python setup.py install\nsudo pip install docker-py\n. Hmm. It is a pip vs. pip-dev issue. Repeating the same steps above with python-pip works.\nSo, in summary:\n- latest docker-py from github works\n- python-pip + version of docker-py in PyPI works\n- pip from github + version of docker-py in PyPI does not work.\n. I'm raising this issue with pip.\n. ",
    "dothebart": "What was missing for me is, that it seems to require the module on the target host; this doesn't come clear from the error  message. a local require docker thus worked flawless for me, fixing this required to run the pip command on the target host.\n. The host that I try to administrate using ansible. Sorry I missed that this is not primarily about ansible.\nAnsible loads this module on the source host, which I run it on with the configs, (which works) but also on the host it wants to provision. Thus - revalidate that before you run the docker commands you have this in your .yml-file:\n- pip name=docker-py\nas it installs this module on the target host.\n. ",
    "exarkun": "Thank you!\n. ",
    "staugur": "I've pushed it to hub docker, the base image is registry.saintic.com/base, or staugur/base.\n. I come from China, English is not very good, so may not fully understand what you mean.\nMy screenshots are https://saintic.top/img/right.png and https://saintic.top/img/volumes.png\n. You are welcome.\n. Container directory is /data/wwwroot, should be mounted on a host of/data/test.\nWhat do you mean variable cfs written backwards?\n. Oh,yes!\nYou are right!\nIn my python file, volume = '/data/wwwroot', cfs = ['/data/test:%s:rw' % volume], everything is OK.\nMy screenshots:\nhttps://saintic.top/img/change.png\n. ",
    "yoanisgil": "@shin- whether I go through swarm or docker I get the same error. However I've been trying to narrowing down a bit more and it turns out I do not have the same issue with Docker 1.8.3. So I think we should close this a file a new issue on the docker project.\n. ",
    "thomasboyt": "@aanand cool, added some docs/comments!\n. @aanand rebased :)\n. So, while before it was adding the parent directories of exceptions to the set of included paths, it wasn't adding back the contents of those directories. \nLet's say you have the following structure:\nfoo/\n  a.py\n  b.py\n  bar/\n    a.py\nand the following rules:\nfoo\n!foo/bar/a.py\nBefore, the final set of included paths would have looked like:\nfoo\nfoo/bar\nfoo/bar/a.py\nBecause the tar created in the build process adds each path non-recursively, this didn't actually include foo/a.py or foo/b.py, it just made empty directories for foo/ and foo/bar/.\nNow, the final set of included paths just looks like:\nfoo/bar/a.py\nEven though foo and foo/bar are no longer included in the set of paths, they'll still be created when foo/bar/a.py is added to the build context (which the updated integration test confirms, since I wanted to make sure it was creating the directories).\n. ",
    "sumitsahrawat": "\nThis PR requires changes to the docs and tests.\n\nYup, I thought so too. I'll take a look.\n\nThere is a much easier way to disable the network for a container by create it with net=\"none\"\n\nWe need the container to have networking, but prevent them from having generic internet access or inter-container access for tmpnb.\nUsing docker from the shell, we want to isolate containers as shown in https://github.com/docker/libnetwork/issues/735#issuecomment-157141350\n. Looking at https://docker-py.readthedocs.org, I found out that Client.networks, Client.create_network and other similar commands are present only in the changelog, not in the documentation.\nEDIT: I opened an issue for it, and will fix it soon (issue #869)\n. I'm not that experienced with Python and testing in Python. Do the tests\nneed any changes?\n. I'll take a look. Thanks for the guidance @aanand :+1: \n. All done!\n. @aanand Squashed to three. I reordered the commits so that the remove_volume fix appears before docs are added, but GitHub seems to think otherwise.\n. I wrote the docs by gazing at the code and the messaging spec.\nPing @aanand and @shin- for verification.\n. I wanted to make the remove_network function similar to the remove_volume function. Now, both return True if no exception is caused.\nhttps://github.com/docker/docker-py/blob/master/docker/api/volume.py#L32-L36\n. ",
    "ikreymer": "ok, thanks, I will look for a different way to do it..\n. hi, thanks for the follow up, I narrowed down my issue to the following.\nI am already using remove_container and that works great.\nBut, occasionally I have seen create_container throw an exception: APIError: 500 Server Error: Internal Server Error (\"Container created but refresh didn't report it back\") and the container exists but gets stuck in the 'created' state.\nWould it perhaps be possible to return the container id in this case so that it can still be removed with remove_container\n. ",
    "migueleliasweb": "Don't know if this helps but the ping method was moved from client.py:602 to daemon.py:73 and the internal method _url changed. But I couldn't find out why this fails...\n. Sir, you saved my day. Thank you a lot.\nI didnt knew about the version parameter when using the Client... sry\n. @TomasTomecek That's a great idea. This exception message would help many.\n+1\n. ",
    "prabhakar-pal": "For later versions of docker-py (2.2.1+), you would need to do this\nclient = docker.DockerClient(version='1.20'). For later versions of docker-py , you would need to do this\nclient = docker.DockerClient(version='1.20'). For later versions of docker-py (2.2.1+), you would need to do this\nclient = docker.DockerClient(version='1.22'). ",
    "smythp": "Thanks! This is a great help.\n. ",
    "prat0318": "ah thanks, @dnephin.\n. ",
    "yncxcw": "works fine for me,  thanks!\n. ",
    "5ARMALE": "Thanks for the reply,\nI didn't have time to check the engine code, but I just found issue: a layer 8 one (mine) :P\nI'm using argparse and configparse. I have defined some \"profiles\" in my config file and I call the rest from command line arguments (image, name, entrypoint, ports, etc.).\nThis is my code on the create_container part:\n...\ndockerhost = Client(base_url=cparser.get('environment', args.environment), timeout=240, version='1.19')\n...\ncpushares = cparser.get('cpu_profiles', args.cpu_shares)    \nhc_cpushares = {'lxc.cgroup.cpu.shares': cpushares}                                                                                                                                                    \n...\ndockerhost.create_container(image=args.image_path, name=args.name, entrypoint=args.entrypoint, command=args.cmd, ports=create_ports, cpu_shares=cpushares, host_config=dockerhost.create_host_config(port_bindings=hc_ports, binds=hc_volumes, mem_limit=cparser.get('ram_profiles', args.mem_limit), lxc_conf=hc_cpushares))\nAnd these are my sections defining the \"profiles\":\n```\n[cpu_profiles]\nmicro = 128\nsmall = 256\nmedium = 512\nbig = 1024\n[ram_profiles]\nmicro = 256m\nsmall = 512m\nmedium = 1024m\nbig = 2048m\n```\nSo the thing is... I should have used configparse's getint() instead of get() or just call int() before cparser.get. Did a couple of tests using lxc as execdriver and it works like a charm, so I'm closing this as \"It didn't happen\".\nI shouldn't have missed that one, sorry about wasting your time! :(\n. ",
    "jizhilong": "\nThese are conflicting requirements, you don't want to install one binary, but you want to create another one that will be installed?\n\nNot conflicting actually, since docker-py is already on our pip requirements list, and no extra overhead of installing another binary would be added  if there is a dockerpy cli tool within the docker-py repo.\n\nIt is possible to build a client-only binary from the current docker/docker repo.\n\nIt's possible, but not as simple and straightforward as a line of command pip install docker-py .\n. build client binary from docker/docker is not that complicated, I'll close this issue.\n. @TomasTomecek thanks for the sharing.\n. ",
    "dustymabe": "This feature request might be relevant: https://github.com/docker/docker-py/issues/1031\n. hey all. you can also use https://github.com/containscafeine/shipy that @containscafeine wrote. It is just a few python files that builds on docker-py and you can use it like: python shipy run --name batman -p 8080:8080 centos ping 8.8.8.8\n. @aanand @shin-, thanks for the feedback. \n@containscafeine should we clean up the code a bit and make this into a python module (maybe call it docker-py-cli or something like that) and start putting it out there for people to use and comment on? \n. hey all. you can also use https://github.com/containscafeine/shipy that @containscafeine wrote. It is just a few python files that builds on docker-py and you can use it like: python shipy run --name batman -p 8080:8080 centos ping 8.8.8.8\n. ",
    "kpavel": "Thanks for your review!\nFixed and rebased.\n. ",
    "tombee": "Is this PR just missing a test to check HttpHeaders are read from the ~/.docker/config.json file?\n@kpavel Are you able to add this test?  Would be great to get this PR merged.\n. Awesome work @kpavel! :+1: \n. Thanks @shin-!\n. Awesome work @shin-!\n. ",
    "GordonTheTurtle": "Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"autz\" git@github.com:kpavel/docker-py-1.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~2\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"patch-1\" git@github.com:speedplane/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~2\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"935\" git@github.com:mdaue/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~2\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Job: docker-py-PRs FAILED:\n``` console\n\n.2\nrootdir: /home/docker-py, inifile: pytest.ini\nplugins: cov\ncollected 107 items\ntests/integration/api_test.py ..........\ntests/integration/build_test.py ....F\ntests/integration/container_test.py ..................F................................\ntests/integration/exec_test.py .......\ntests/integration/image_test.py .........s\ntests/integration/network_test.py ..ss..ss...\ntests/integration/regression_test.py ......\ntests/integration/volume_test.py .......\n=================================== FAILURES ===================================\n____ BuildTest.test_build_with_dockerignore ______\ntests/integration/build_test.py:88: in test_build_with_dockerignore\n    for chunk in stream:\ndocker/client.py:230: in _stream_helper\n    yield self._result(response)\ndocker/client.py:150: in _result\n    self._raise_for_status(response)\ndocker/client.py:146: in _raise_for_status\n    raise errors.APIError(e, response, explanation=explanation)\nE   APIError: 500 Server Error: Internal Server Error (\"Cannot locate specif\n---\n```\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"readme-rst\" git@github.com:hobarrera/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"rst-for-pypi\" git@github.com:juanpabloaj/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:gferon/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:gferon/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:marshyski/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:vitalyisaev2/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~2\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"tls-fixes\" git@github.com:docker/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~2\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"tls-fixes\" git@github.com:docker/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~2\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:uggla/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~2\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"backports\" git@github.com:felixonmars/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~2\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"backports\" git@github.com:felixonmars/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~2\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"backports\" git@github.com:felixonmars/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~2\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"push-to-secure-registry-without-auth\" git@github.com:TomasTomecek/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"push-to-secure-registry-without-auth\" git@github.com:TomasTomecek/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"push-to-secure-registry-without-auth\" git@github.com:TomasTomecek/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"push-to-secure-registry-without-auth\" git@github.com:TomasTomecek/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:aiden0z/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~3\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:aiden0z/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~3\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"remove-network\" git@github.com:bboreham/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"remove-network\" git@github.com:bboreham/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"add_rest_of_dns_search_support\" git@github.com:romaimperator/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:ubante/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:ubante/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"patch-1\" git@github.com:garthy/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"support-py35\" git@github.com:graingert/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~3\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"readthedocs.io\" git@github.com:adamchainz/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:aiden0z/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~5\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:aiden0z/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~5\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"network_doc\" git@github.com:ColinHuang/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~3\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"network_doc\" git@github.com:ColinHuang/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~3\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:minzhang28/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:Faylixe/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"1105-support-more-api-options\" git@github.com:t-tran/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~3\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:mvdstam/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:mvdstam/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"env\" git@github.com:lsbardel/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:walkerlee/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"patch-1\" git@github.com:kanaka/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"patch-1\" git@github.com:kanaka/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"1.10.2-release\" git@github.com:dajose/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"addRestartPolicyUpdateSupport\" git@github.com:mferon/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~2\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:interesse/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"patch-1\" git@github.com:pacoxu/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:YorikSar/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:mrjj/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:mrjj/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"patch-1\" git@github.com:manics/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"master\" git@github.com:cglewis/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~2\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\nAmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"patch-1\" git@github.com:JoelJ/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmending updates the existing PR. You DO NOT need to open a new one.\n. Please sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\nconsole\n$ git clone -b \"patch-1\" git@github.com:JoelJ/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:bayazee/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:gferon/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:hw-zhangjiangyu/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842353691712\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:nkrabshuis/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842354412208\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"fix-container-run-doc\" git@github.com:gabrielpjordao/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"patch-1\" git@github.com:smiller171/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"add_cpuset_mems\" git@github.com:qq690388648/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842354460912\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"nvidia_docker\" git@github.com:andyneff/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842354470848\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:Yupeek/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/docker/docker/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"issue1567\" git@github.com:Niboo/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842361558592\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"refresh_credentials\" git@github.com:terminalmage/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"readtimeout_calling_container_stop\" git@github.com:grahamlyons/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"test-pull-latest\" git@github.com:bboreham/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"patch-1\" git@github.com:mattoberle/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"bump-2.5.0-dev\" git@github.com:docker/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842354499056\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:szczurmys/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:usbrandon/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"bugfix/run-container-with-syslog-driver\" git@github.com:bolshakov/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"add-remove-image-result\" git@github.com:cecton/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842354321112\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:seekintoo/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"services-mount-tmpfs\" git@github.com:knackworks/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"add_container_health_attribute\" git@github.com:tweakster/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"join-swarn-default-listen-address\" git@github.com:mbelang/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:swood/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"fix_docs\" git@github.com:Brett55/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"connect-with-mac\" git@github.com:hongbin/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842354570992\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"patch-1\" git@github.com:monperrus/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"feature-save-multiple-images\" git@github.com:ugomeda/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"attach-buf-fix\" git@github.com:cjh1/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"chardet-fix\" git@github.com:timvisee/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"2.6.1-release\" git@github.com:docker/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842354164496\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"add_verify_parameter_to_apiclient\" git@github.com:withsmilo/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:andreycizov/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842354330832\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"add_started_at_method\" git@github.com:stasfilin/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842354511488\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"name\" git@github.com:anshulpundir/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"patch-1\" git@github.com:asottile/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:qazbnm456/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"patch-1\" git@github.com:mccalluc/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"container_exec_run\" git@github.com:funkyfuture/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842354560608\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"1743-container-ports\" git@github.com:larkost/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:dementedhedgehog/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:BYU-PCCL/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842354175424\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"build_prune\" git@github.com:The-Loeki/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:levin-du/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"3.1.1-release\" git@github.com:docker/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842354469424\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"1981-different-drives\" git@github.com:kant2002/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"ushuz-patch-1\" git@github.com:ushuz/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:corpusops/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"use_six_moves\" git@github.com:asottile/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"patch-1\" git@github.com:EFanZh/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"2032\" git@github.com:srinivasreddy/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:lieryan/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842362546256\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"pywin32\" git@github.com:ofek/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"services_spec_list_syntax_error\" git@github.com:ChrsMark/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"bump-3.4.0\" git@github.com:docker/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842354380032\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"service-create-placement-preferences\" git@github.com:knackworks/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:NikolayMurha/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"allow_uid_integer_0\" git@github.com:asottile/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"fix-commit-changes-arg\" git@github.com:StoneJia/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"NikolayMurha-master\" git@github.com:docker/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842354291488\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"i2116\" git@github.com:adw1n/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:Matzz/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:little-dude/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"patch-1\" git@github.com:adamtheturtle/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"fix_deprecation\" git@github.com:little-dude/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"single-logger\" git@github.com:adaszko/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"adw1n-i2116\" git@github.com:docker/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842354173184\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"windows-exec-dbg\" git@github.com:little-dude/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842354472704\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"2200-swarm-default-addr-pool\" git@github.com:originalgremlin/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842354056040\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:little-dude/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"patch-1\" git@github.com:BoboTiG/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"patch-2\" git@github.com:BoboTiG/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:johnnydowns/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:eiffel-fl/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"release\" git@github.com:docker/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842354424400\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"2232-Adding_supported_options_to_container_run_documentation\" git@github.com:wvaske/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:Bob-Du/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"fix_api_ver_desc\" git@github.com:thombashi/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"fix/improve-ssh-error\" git@github.com:ssbarnea/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"patch-1\" git@github.com:p1100i/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"2200-swarm-default-addr-pool-reviewed\" git@github.com:bluikko/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842353560464\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"fix/ssh-channel-not-closed\" git@github.com:arcenik/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:philipxyc/docker-py.git somewhere\n$ cd somewhere\n$ git rebase -i HEAD~842358456128\neditor opens\nchange each 'pick' to 'edit'\nsave the file and quit\n$ git commit --amend -s --no-edit\n$ git rebase --continue # and repeat the amend for each commit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:lekster/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"patch-1\" git@github.com:heckad/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n.  AUTOMATED:POULE:DCO-EXPLANATION \nPlease sign your commits following these rules:\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#sign-your-work\nThe easiest way to do this is to amend the last commit:\n~~~console\n$ git clone -b \"master\" git@github.com:bieron/docker-py.git somewhere\n$ cd somewhere\n$ git commit --amend -s --no-edit\n$ git push -f\n~~~\nAmending updates the existing PR. You DO NOT need to open a new one.\n. ",
    "wenchma": "This PR and docker/docker#18928 depend on each other, so it will block each other. I workaround it that firstly we remove test_inspect_network temporarily, once docker/docker#18928 is merged, I can revert the remove and update the test.  wdyt ? @shin-\n. @shin- good idea, :+1:   already updated the PR to cover only a few specific values.\n. The related jenkins checks failure:\n=================================== FAILURES  ===================================\n08:03:52 __________________________ LinkTest.test_remove_link  ___________________________\n08:03:52 /docker-py/tests/integration/api_test.py:57: in test_remove_link\n08:03:52     self.client.start(container2_id)\n08:03:52 /docker-py/docker/utils/decorators.py:21: in wrapped\n08:03:52     return f(self, resource_id, *args, **kwargs)\n08:03:52 /docker-py/docker/api/container.py:356: in start\n08:03:52     self._raise_for_status(res)\n08:03:52 /docker-py/docker/client.py:142: in _raise_for_status\n08:03:52     raise errors.APIError(e, response, explanation=explanation)\n08:03:52 E   APIError: 500 Server Error: Internal Server Error (\"Cannot link to /elegant_lichterman,  as it does not belong to the same network with /boring_mccarthy\")\n. @shin- PTAL ,    because it blocks the jenkins jobs of https://github.com/docker/docker/pull/21115\n. just a rebase\n. ",
    "patrick-bark": "Awesome, thank you\n. Thank you again, that worked\n. ",
    "toli": "LGTM\nthanks for fixing it\n. ",
    "frol": "Hint: The best way, I could think of, to check for a list-like types might be isinstance(smth, collections.Container) and not isinstance(smth, six.string_types) and not isinstance(smth, dict) (this can be splitted and usually is already done in the code-base, so you don't need to check for not ... because you handle those cases before checking for collections.Container types).\n. > Conversion from any of these types to a list is trivial as well.\nIn the most cases (if not all), there is no reason for limiting a user by requiring the list type. The error messages are also misleading in some cases. For example, when I pass volumes to create_container as a tuple instead of a list, I get:\n``` Python\nTraceback (most recent call last):\n  File \"/usr/lib/python3.5/site-packages/docker/client.py\", line 138, in _raise_for_status\n    response.raise_for_status()\n  File \"/usr/lib/python3.5/site-packages/requests/models.py\", line 840, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http+docker://localunixsocket/v1.21/containers/create?name=ddots-test_solution-ddots-compiler-gpp\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/opt/ddots/testing_system/worker/docker_manager.py\", line 99, in init_container\n    container_reference = self._docker_client.create_container(**init_options)\n  File \"/usr/lib/python3.5/site-packages/docker/api/container.py\", line 117, in create_container\n    return self.create_container_from_config(config, name)\n  File \"/usr/lib/python3.5/site-packages/docker/api/container.py\", line 128, in create_container_from_config\n    return self._result(res, True)\n  File \"/usr/lib/python3.5/site-packages/docker/client.py\", line 146, in _result\n    self._raise_for_status(response)\n  File \"/usr/lib/python3.5/site-packages/docker/client.py\", line 142, in _raise_for_status\n    raise errors.APIError(e, response, explanation=explanation)\ndocker.errors.APIError: 500 Server Error: Internal Server Error (\"b'json: cannot unmarshal array into Go value of type map[string]struct {}'\")\n``\n. @shin- Please, review the PR.\n. @shin- @aanand Do you think it is worth to rebase this PR? I have been using it over a year now in production with no issues at all!. Givendemux` option is now implemented, there is no need in this PR anymore.. ",
    "ostrolucky": "adminer:\n    build: images/adminer\n    restart: always\n    ports:\n        - \"8012:80\"\nimages/adminer:\nFROM clue/adminer\n. Your output isn't from windows terminal as mine is. Also, you didn't show output of pwd. In my output it's clear that my home directory is with diacritic. In yours, it's not clear if that's home. I don't think so.\nI have access to windows pc like... once a month.\n. It isn't.\n```\nC:\\Users\\\u0160arlat\u00e1n                                                                  \n\u03bb docker-compose build                                                             \nBuilding adminer                                                                   \nERROR: Windows named pipe error: The system cannot find the file specified. (code: 2)\nC:\\Users\\\u0160arlat\u00e1n                                                                  \n\u03bb docker-compose --version                                                         \ndocker-compose version 1.23.0, build c8524dc1                                        \nC:\\Users\\\u0160arlat\u00e1n                                                                  \n\u03bb                                                                                  \n```\nReopen please, thx.. ```\nC:\\Users\\\u0160arlat\u00e1n>docker-compose version\ndocker-compose version 1.23.0, build c8524dc1\ndocker-py version: 3.5.0\nCPython version: 3.6.6\nOpenSSL version: OpenSSL 1.0.2o  27 Mar 2018\nC:\\Users\\\u0160arlat\u00e1n>docker-compose build\nERROR: Windows named pipe error: The system cannot find the file specified. (code: 2)\n```\n\nterminal program\nDoes it change anything if you run Compose from Powershell instead\n\nReproducible in any. Tried cmd.exe, powershell, ConEmu\n\nversion of Windows\n\nLast one I tried this is Windows 10 64bit, version 1803, build 17134.345, running in Virtualbox VM. I know docker won't work there, this is only to confirm this issue has been fixed.\n\nHow did you install Docker, docker-compose?\n\nI use chocolatey, install via choco install docker-compose\n\ndoes the binary package exhibit the same issue?\n\nyep\nC:\\Users\\\u0160arlat\u00e1n>docker-compose-Windows-x86_64.exe build\nERROR: Windows named pipe error: The system cannot find the file specified. (code: 2). Ah ok that makes sense. I'm not going to have access to windows machine for a while so I think we can close it until someone here will report this again.\nWhat might have been the issue anyway that that you are confident this is fixed?. ",
    "osaris": "After hours of tests I have finally found this thread which looks to be exactly my problem :\n```\nRapha\u00ebl@X1YOGA MINGW64 ~/Projets/docker/lamp\n$ docker-compose up\nBuilding lamp\nTraceback (most recent call last):\n  File \"\", line 3, in \n  File \"compose\\cli\\main.py\", line 56, in main\n  File \"compose\\cli\\docopt_command.py\", line 23, in sys_dispatch\n  File \"compose\\cli\\docopt_command.py\", line 26, in dispatch\n  File \"compose\\cli\\main.py\", line 191, in perform_command\n  File \"compose\\cli\\main.py\", line 657, in up\n  File \"compose\\project.py\", line 318, in up\n  File \"compose\\service.py\", line 351, in execute_convergence_plan\n  File \"compose\\service.py\", line 252, in create_container\n  File \"compose\\service.py\", line 275, in ensure_image_exists\n  File \"compose\\service.py\", line 679, in build\n  File \"site-packages\\docker\\api\\build.py\", line 42, in build\nTypeError: You must specify a directory to build in path\ndocker-compose returned -1\nRapha\u00ebl@X1YOGA MINGW64 ~/Projets/docker/lamp\n$ pwd\n/c/Users/Rapha\u00ebl/Projets/docker/lamp\nRapha\u00ebl@X1YOGA MINGW64 ~/Projets/docker/lamp\n$ docker-compose -v\ndocker-compose version 1.6.2, build e80fc83\n``\n. I changed my account folder toC:\\Raphael` and it works.\n. ",
    "keloyang": "why can't reopen?\n. I have a try and https://github.com/docker/docker-py/pull/988 is ok.\n. I have a try and https://github.com/docker/docker-py/issues/984 is fixed.thank you.\n. ",
    "morxa": "% docker images | grep hello\nhello                                                latest              6b40fe7724bd        11 weeks ago        15.6 MB\nregistry.com:2222/hello                              latest              6b40fe7724bd        11 weeks ago        15.6 MB\nregistry.com/hello                                   latest              6b40fe7724bd        11 weeks ago        15.6 MB\n``` python\nIn [1]: import docker\nIn [2]: docker_client = docker.Client()\nIn [3]: docker_client.images(name='registry.com:2222/hello')\nOut[3]: []\nIn [4]: docker_client.images(name='registry.com/hello')\nOut[4]: \n[{u'Created': 1446581252,\n  u'Id': u'6b40fe7724bd29107f6182ca2befec011cdf524b23ebc4c9a33591d6b7aea4ee',\n  u'Labels': None,\n  u'ParentId': u'2b4c55187a27a43f6c5aebab707e75cffee14f7c3cc02e4f74429992bd5f7db2',\n  u'RepoDigests': [],\n  u'RepoTags': [u'registry.com/hello:latest'],\n  u'Size': 0,\n  u'VirtualSize': 15602184}]\nIn [5]: docker.version\nOut[5]: '1.6.0'\n```\n. ",
    "awasilyev": "I check https://github.com/docker/docker-py/blob/master/docker/api/image.py#L150 before and did not see anything related to digest. Wrong place to check?\n. is it documented anywhere or could you please provide me with usage example?\n. ",
    "kadel": "@awasilyev As far as I know you need docker-py version 1.6.0 and up for this. \nThan you can use it just like this:\ndocker_client.pull('my.hub.int/image@sha256:82280d2495907dae5781870ed8d053436b68b947ce1d06aab36174bffbfd1159')\nI couldn't find any documentation about this and about 1.6.0 requirement by trail and error :-(\n. ",
    "dvenza": "I'm happy this works for you, for me it does not.\nEven if I call dict() on that list I reported earlier, I still get keys that are quite difficult to match, I have to cleanup the data. I am using a very standard OS/Storage driver combination, Ubuntu and the default storage driver, the difference is that I am querying the Swarm endpoint and not the Docker engine endpoint.\nSince everything is Docker standard in my install I expected the client API to \"just work\" and present a clean interface, but I guess I have to check with the Swarm guys.\n. Then you should state in the documentation that docker-py is not compatible with swarm.\nThis is what I get by doing dict() on driverstatus. The entries for each host have the same key and get overwritten. I am using the latest Swarm and Docker and docker-py versions.\nDo you really think this is ok?\n```\n\n\n\ndict(info[\"DriverStatus\"])\n len = {int} 21\n ' \u2514 Containers' (140416914380776) = {str} '6'\n ' \u2514 Error' (140416987361072) = {str} '(none)'\n ' \u2514 Labels' (140416987358960) = {str} 'executiondriver=native-0.2, kernelversion=3.19.0-25-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufs'\n ' \u2514 Reserved CPUs' (140416987212576) = {str} '0 / 33'\n ' \u2514 Reserved Memory' (140416987213024) = {str} '11 GiB / 132.1 GiB'\n ' \u2514 Status' (140416987359056) = {str} 'Healthy'\n ' \u2514 UpdatedAt' (140416914380568) = {str} '2016-03-02T12:25:21Z'\n '\\x08Filters' (140416909775472) = {str} 'health, port, dependency, affinity, constraint'\n '\\x08Nodes' (140416909930880) = {str} '10'\n '\\x08Role' (140416909933232) = {str} 'primary'\n '\\x08Strategy' (140416909776432) = {str} 'spread'\n 'bf12' (140416909932560) = {str} '192.168.45.3:2375'\n 'bf13' (140416909933456) = {str} '192.168.45.4:2375'\n 'bf14' (140416909703856) = {str} '192.168.45.5:2375'\n 'bf15' (140416909703912) = {str} '192.168.45.6:2375'\n 'bf16' (140416909704976) = {str} '192.168.45.7:2375'\n 'bf17' (140416909704808) = {str} '192.168.45.8:2375'\n 'bf18' (140416909703968) = {str} '192.168.45.9:2375'\n 'bf19' (140416914281840) = {str} '192.168.45.10:2375'\n 'bf20' (140416914282904) = {str} '192.168.45.11:2375'\n 'bf21' (140416914281504) = {str} '192.168.45.251:2375'\n```\n. Thank you for your link, very helpful.\nThere is already an open issue here: https://github.com/docker/swarm/issues/1214\n\n\n\nWhat I'm trying to point out here is that people try to use the Python API with Swarm and get back a data structure that is difficult to parse and this is not documented anywhere.\nAt least keep this bug open until Swarm is fixed.\n. Thanks!\nI use Swarm and use overlay networks, so I always pass a network name to the start_container call because I do not want to use the default docker network. How can I pass an alias there? From the command line I can pass aliases when I do docker run.\nShould I call connect_to_network passing a network that is already connected, but adding the aliases I need?\n. Thanks!\n. Did you find a way to do it? Thanks!\n. Thank you. If someone happens to get here looking for the same thing, you have to add an environment variable with the constraint as the name and None as the value.\nI was trying passing an empty string and I got a weird error about not enough container slots.\nOn 18 July 2016 21:09:34 CEST, Christoph Jansen notifications@github.com wrote:\n\nHi.\nIt's actually easy if you know that the -e flag of the docker CLI means\nEnvironment Variable. So you can look up how to set them with docker-py\nand use the same parameters as with the CLI. \nI'm on my phone and can't give you a Code example. \nHope this helps\n\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub:\nhttps://github.com/docker/docker-py/issues/1043#issuecomment-233427577\n. \n",
    "TJNII": "I vote this should be a general improvement across the library.  The pull() method behaves similarly:\nIn [24]: cli.pull('ubuntu:14.04')\nOut[24]: u'{\"status\":\"Pulling from library/ubuntu\",\"id\":\"14.04\"}\\r\\n{\"status\":\"Already exists\",\"progressDetail\":{},\"id\":\"8387d9ff0016\"}\\r\\n{\"status\":\"Already exists\",\"progressDetail\":{},\"id\":\"3b52deaaf0ed\"}\\r\\n{\"status\":\"Already exists\",\"progressDetail\":{},\"id\":\"4bd501fad6de\"}\\r\\n{\"status\":\"Already exists\",\"progressDetail\":{},\"id\":\"a3ed95caeb02\"}\\r\\n{\"status\":\"Digest: sha256:0844055d30c0cad5ac58097597a94640b0102f47d6fa972c94b7c129d87a44b7\"}\\r\\n{\"status\":\"Status: Image is up to date for ubuntu:14.04\"}\\r\\n'\nWhile I could do some magic to parse this, this output is clearly meant to be printed to the user and not parsed by a machine.  Compare this with the output of the docker-api rubygem comparable operation:\n=> #<Docker::Image:0x00000000d1f010\n @connection=#<Docker::Connection:0x000000018deef0 @options={}, @url=\"tcp://172.17.0.149:4444\">,\n @id=\"sha256:3876b81b5a81678926c601cd842040a69eb0456d295cd395e7a895a416cf7d91\",\n @info=\n  {\"RepoTags\"=>[\"ubuntu:14.04\"],\n   \"RepoDigests\"=>[],\n   \"Parent\"=>\"\",\n   \"Comment\"=>\"\",\n   \"Created\"=>\"2016-01-19T23:31:24.889720757Z\",\n   \"Container\"=>\"1351fd9fceb3cee2ceaa8fc342651f91ae84e445bfbb0d93f066dc34a36911c4\",\n   \"ContainerConfig\"=>\n    {\"Hostname\"=>\"dfc2eabdf236\",\n     \"Domainname\"=>\"\",\n     \"User\"=>\"\",\n     \"AttachStdin\"=>false,\n     \"AttachStdout\"=>false,\n     \"AttachStderr\"=>false,\n     \"Tty\"=>false,\n     \"OpenStdin\"=>false,\n     \"StdinOnce\"=>false,\n     \"Env\"=>nil,\n     \"Cmd\"=>[\"/bin/sh\", \"-c\", \"#(nop) CMD [\\\"/bin/bash\\\"]\"],\n     \"Image\"=>\"a4c5be5b6e596241b4530ade26294afa8d8a4a2b9163c30e36c87f879b0f5073\",\n     \"Volumes\"=>nil,\n     \"WorkingDir\"=>\"\",\n     \"Entrypoint\"=>nil,\n     \"OnBuild\"=>nil,\n     \"Labels\"=>nil},\n   \"DockerVersion\"=>\"1.8.3\",\n   \"Author\"=>\"\",\n   \"Config\"=>\n    {\"Hostname\"=>\"dfc2eabdf236\",\n     \"Domainname\"=>\"\",\n     \"User\"=>\"\",\n     \"AttachStdin\"=>false,\n     \"AttachStdout\"=>false,\n     \"AttachStderr\"=>false,\n     \"Tty\"=>false,\n     \"OpenStdin\"=>false,\n     \"StdinOnce\"=>false,\n     \"Env\"=>nil,\n     \"Cmd\"=>[\"/bin/bash\"],\n     \"Image\"=>\"a4c5be5b6e596241b4530ade26294afa8d8a4a2b9163c30e36c87f879b0f5073\",\n     \"Volumes\"=>nil,\n     \"WorkingDir\"=>\"\",\n     \"Entrypoint\"=>nil,\n     \"OnBuild\"=>nil,\n     \"Labels\"=>nil},\n   \"Architecture\"=>\"amd64\",\n   \"Os\"=>\"linux\",\n   \"Size\"=>187925225,\n   \"VirtualSize\"=>187925225,\n   \"GraphDriver\"=>{\"Name\"=>\"aufs\", \"Data\"=>nil},\n   \"id\"=>\"sha256:3876b81b5a81678926c601cd842040a69eb0456d295cd395e7a895a416cf7d91\"}>\nThe Ruby binding returns an object which is immediately usable.  Furthermore, compare the error handling of the two libraries:\ndocker-py:\nIn [25]: cli.pull('someimage/that:doesntexist')\nOut[25]: u'{\"status\":\"Pulling repository docker.io/someimage/that\"}\\r\\n{\"errorDetail\":{\"message\":\"Error: image someimage/that not found\"},\"error\":\"Error: image someimage/that not found\"}\\r\\n'\ndocker-api gem:\n[7] pry(main)> Docker::Image.create('fromImage' => 'someimage/that:doesntexist')\nDocker::Error::NotFoundError: No such image: someimage/that:doesntexist\nfrom /var/lib/gems/2.1.0/gems/docker-api-1.26.0/lib/docker/connection.rb:46:in `rescue in request'\nDocker-py returns a string of newline-delimited JSON strings that contain values intended for human consumption.  To use this I'll have to parse these strings searching for keys which I don't know.  The docker-api gem throws an exception which is immediately and easily handleable as an error.\nAs Docker-py is an API binding I expect the output of it's methods to be easily machine parsable, in my opinion that's the whole point of using the binding: to make coding against the API easy.  Returning data representations which must be massaged into a form which is machine parsable simply isn't good design for a binding.  Please use this issue to improve your return formats.  Thank you.\n. ",
    "etki": "Ow, that message about moving those parameters to start bugged me out and i didn't even think to check host config (and was afraid of creating container config because of volumes_from comma-concatenating). Thank you!\n. ",
    "zerthimon": "@dnephin yeah I know about virtualenv.\nI have a need to build a deb package for it, I've alreay done that, but I have to replace official libraries like requests and urllib3 and that broke some other software in the system.\n. @dnephin \nI need docker-py to be importable like a system module, a package built with dh-virtualenv still needs special handling (i have a software that needs docker-py, that I cannot modify)\nSo question remains, can I build and run docker-py with an official ubuntu requests package (2.2.1)\n. Thanks everyone for answers, this issue may be closed.\n. ",
    "darkermatter": "Hi Tomas: \npython\nconfig = machine.create_host_config(binds=['%s:/backup' % source], privileged=True)\nrestore = machine.create_container(\n    image='muse_backup',\n    command=['/restore.sh', directory, location],\n    entrypoint=entrypoint,\n    host_config=config,\n)\nlog('Restore container ID: ' + restore.get('Id'), name)\nmachine.start(container=restore)\nfor line in machine.logs(container=restore, stream=True):\n    log(line.decode(\"utf-8\").strip(), name)\n. Hi Tomas,\nI'll give that a try.\nWhat is AutoVersionClient? Does it take the same parameters as Client?\nThanks!\n. Thanks Tomas,\nI now have this working on Docker Engine 1.9.1 (I'll test 1.10.0 later). As your code comment mentioned, stream is spitting out every character without buffering. \npython\n machine.start(container=backup)\n line = ''\n for char in machine.logs(container=backup, stream=True):\n     if char == '\\n':\n         log(line, name)\n         line = ''\n     else:\n         line = line + char\n. Seems to be working fine on 1.10.0 too.\n. ",
    "messense": "Same issue when streaming logs.\nPython 3.5\ndocker-py==1.10.3\nrequests==2.10.0\nDocker version 1.12.1, build 23cf638\n. +1 for this.\n. ",
    "tlusk": "Same issue with 1.10.3, even with the tty=True trick from https://github.com/docker/docker-py/issues/919#issuecomment-181795059\n. ",
    "tedmiston": "I am still seeing this issue with the latest version of the Docker client (2.5.1).  Using tty=True plus the streaming fix by @darkermatter in https://github.com/docker/docker-py/issues/919#issuecomment-181815005 fixed it for us.. ",
    "mikedougherty": "The main difference between this and the docker CLI is that this infers TLS is enabled if either value is set. With all of env variables set as shown, I still have to specify docker --tls ... (same if I also provide a DOCKER_CERT_PATH). \n. rebased on master :+1: \n. Ah, looks like I didn't follow the pattern of overriding HOME for a test that uses the default cert path. working on a fix now.\n. Will update - that was the best I could do at the time, I like yours :)\n. If I understand correctly that would enable my use case, so I'd be fine with that. However it seems like it would be better to enable verification by default and require an explicit setting to turn it off (specifically, setting DOCKER_TLS_VERIFY to a falsy value).\nAlso you don't need the trailing or False, it would be more pythonic to write your snippet as:\npython\ntls_verify = os.environ.get('DOCKER_TLS_VERIFY')\nenable_tls = tls_verify or cert_path\nor perhaps wrap the values in bool such as bool(os.environ.get('DOCKER_TLS_VERIFY')) :)\n. i would recommend adding in the architecture to the label; we're starting to have different ones and they're not compatible (of course). i would recommendamd64 or x86_64 (which are synoynms in our ci system; if they're not actually please let me know :)). these images will need to be pushed since the other wrappedNode parts could be on other hosts. plus it's probably a good idea to push them anyway to reduce duplicated work.. also do we know why zfs specifically is no good? seems like a host configuration error.. or a docker bug?. need to escape the $ for the env var, like \\$. else groovy will complain. in groovy, in a double-quoted string, $ is meant for ${exprToInterpolate} like for ${dockerVersion}. similar here with $ escaping. i believe ${env.BUILD_NUMBER} would be synonymous if you don't want to do \\$ :). nothing wrong with this as is, but long term it's probably better written with 2 lists of pyVersion and dockerVersion and 2x for loops. also you can merge the two sh calls into one long string if you want.. kind of external to the Jenkinsfile syntax/semantics; but it could be an improvement to have a docker-compose file to manage the container inter-dependencies instead with --link; etc. \\ . i think the right side of this might be missing runTests(...). there should be some kind of cleanup script to remove the containers at the end. i would recommend a try { .. } finally { ... } with the two existing sh steps in the try, and the necessary docker stop / docker rm calls in the finally block.. ",
    "mountkin": "This change is breaking the docker integration tests.\nAccording https://github.com/docker/docker/blob/master/docker/common.go#L28 , if the environment variable DOCKER_TLS_VERIFY is an empty string, tls_verify is treated as False and TLS is not enabled.\nThis PR treated an empty DOCKER_TLS_VERIFY environment variable as if the user want to disable tls verify but enable tls.\nMeanwhile, docker integration test is passing an empty DOCKER_TLS_VERIFY environment variable  to the docker-py test https://github.com/docker/docker/blob/master/hack/make.sh#L242\nJenkins log: https://jenkins.dockerproject.org/job/Docker-PRs/25154/console\n. Oh there is already a PR to address the problem https://github.com/docker/docker-py/pull/988 so closing this one. \n. I think the block above should be changed to \ntls_verify = os.environ.get('DOCKER_TLS_VERIFY') or False\n    enable_tls = tls_verify or cert_path or False\nto keep align with the Docker CLI flags.\n. ",
    "christophlsa": "I had the same problem (requests = 2.3.0). Then I created a new virtual environment and installed the package(s) with the newest versions (requests = 2.9.1) and I had no problems anymore.\n. ",
    "ismaelfernandezscmspain": "I have the same problem, any answer about this?\nI have installed (requests = 2.9.1) and (docker-py =1.7.2)\n. If I try push to ECR from subprocess.call([docker push xxx]), I don't have problem, but if I use docker-py I receive this in the first time per day:\n['{\"status\":\"The push refers to a repository [xxx.dkr.ecr.eu-west-1.amazonaws.com/zipkin] (len: 1)\"}\\r\\n', '{\"status\":\"Image push failed\",\"progressDetail\":{},\"id\":\"306d7a77d309\"}', '{\"errorDetail\":{\"message\":\"Error parsing HTTP response: unexpected end of JSON input: \\\\\"\\\\\"\"},\"error\":\"Error parsing HTTP response: unexpected end of JSON input: \\\\\"\\\\\"\"}\\r\\n']\nI try all modes for connect with the client.\nHow can I fix this problem?\n. yes, today I have seen just this:\n2016-07-12 07:00:23,378 - docker-build - INFO - ['{\"status\":\"The push refers to a repository [xxx.dkr.ecr.eu-west-1.amazonaws.com/ms-if-index-search] (len: 1)\"}\\r\\n', '{\"status\":\"Preparing\",\"progressDetail\":{},\"id\":\"12444b3057c0\"}', '{\"errorDetail\":{\"message\":\"Error parsing HTTP response: invalid character \\'Y\\' looking for beginning of value: \\\\\"Your Authorization Token has expired. Please run \\'aws ecr get-login\\' to fetch a new one.\\\\\"\"},\"error\":\"Error parsing HTTP response: invalid character \\'Y\\' looking for beginning of value: \\\\\"Your Authorization Token has expired. Please run \\'aws ecr get-login\\' to fetch a new one.\\\\\"\"}\\r\\n']\nThanks a lot!\n. ",
    "francisjeanneau": "Not exactly.\nIn fact, I am executing untrusted code inside a container and I want to\nlimit that code execution time.\nIsn't my script stopping at c.start() and wait for container's result?\nOr am I completely lost and my script hangs on c.stop() while the code is\nexecuted inside the container? If yes, then your solution might just work!\nWill check that tomorrow and get back to you,\nThank you very much!\nLe 9 f\u00e9vr. 2016 15:04, \"Joffrey F\" notifications@github.com a \u00e9crit :\n\nIs this what you're looking for?\nimport dockerfrom requests.exceptions import ReadTimeout\nc = docker.Client()\nctnr = c.create_container('busybox', 'sleep 45')\nc.start(ctnr)try:\n  c.wait(ctnr, timeout=5)except ReadTimeout as e:\n  print('too slow!')\n  c.kill(ctnr)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-py/issues/933#issuecomment-182037419.\n. Mind = blown. Don't know how I could have missed that. Probably the 'docker run command' that had me suppose Client.start was doing the same thing.\n. Isn't that merge request adding the support for the 'tmpfs' parameter declaration only? I just cloned the project and the option is accepted, but it doesn't actually do anything. \n\nThanks again.\n. For example, using docker-py from project master branch, adding the following in host_config: \ntmpfs={ '/test' : '' }\ndoes not create the desired 'test' volume inside the container.\nUPDATE: Sorry, I think it's a Docker Swarm issue. With my local Docker deamon everything looks fine. However, when using my Swarm Master, the tmpfs flag does not seem to be provided to the deamon.\n. ",
    "mdaue": "Great! I squashed to one commit, leaving interesting comments in the notes. I set the minimum required API back to 1.21 where identified and compared with master before committing. I removed my addition to the unit test as it failed, possibly because it wasn't run with API 1.22. Alternatively, I can add that test again and perform the same version check.\nThanks!\n. Just in case it was missed, I did update the code base on the suggestions and squashed the commit.\n. Thanks - made changes.\n. I split the integration tests as per @dnephin suggestion. Also found that I had forgotten to change the arg to the function calls in the test (ip4 vs. ipv4_address) after making the change in network.py (fixed).\n. ",
    "seschwar": "Updated PR to remove seemingly superfluous branch.\n. Thanks for having picked this up, @christianbundy. My employer only had a very short lived interest in being able to build on Windows.\n. I did play around with the tarfile module in the Python interpreter on Linux and was just getting zeroed files in my tarball without it. However docker.Client.build() seems to work fine without it. I guess we can drop it then.\n. Yes, I am just following docker build's behaviour.\n. ",
    "janLo": "Can someone explain why the unittests are failling?\n. Ah. A typo. Should be fixed now.\n. Wtf, They ARE signed ... \n. ",
    "dimaspivak": "Any chance of getting this pulled in soon? It's a pretty innocuous change that is super useful for people who use a docker commit-based workflow.\n. Created a quick test to repro what I'm seeing. For this to work, run on a machine whose Docker daemon is listening both the Unix socket and 0.0.0.0:2375 (change port accordingly, if needed):\n```\nimport io\nimport tarfile\nimport docker\nCreate simple tarstream.\nfile_contents = 'Random text!'\ntarstream = io.BytesIO()\nwith tarfile.open(fileobj=tarstream, mode='w') as tarfile_:\n    encoded_file_contents = file_contents.encode()\n    tarinfo = tarfile.TarInfo('file.txt')\n    tarinfo.size = len(encoded_file_contents)\n    tarfile_.addfile(tarinfo, io.BytesIO(encoded_file_contents))\nWe're at the end of the tarstream, go back to the beginning.\ntarstream.seek(0)\nfor base_url in ('unix://var/run/docker.sock', '0.0.0.0:2375'):\n    print('Creating client with base_url of {0}'.format(base_url))\n    client = docker.Client(base_url=base_url)\n    test_container = client.create_container(image='alpine:latest', command='/bin/true')\n    print('Successfully created container with Id {0}'.format(test_container['Id']))\nprint('Trying to put archive (base_url={0})...'.format(base_url))\ntry:\n    client.put_archive(container=test_container['Id'], path='/', data=tarstream)\n    print(\"Didn't see exception while putting archive with base_url {0}!\".format(base_url))\nexcept Exception as e:\n    print('Saw exception {0}...'.format(e))\n\nWhen running, I get the following output:\nroot@ip-172-31-1-222:~# python3 put_archive_test.py \nCreating client with base_url of unix://var/run/docker.sock\nSuccessfully created container with Id 8dc5197db4bbe11a95afcfac7ca1fe7cc461731acf649d73867919b8ebd98d54\nTrying to put archive (base_url=unix://var/run/docker.sock)...\nSaw exception 400 Client Error: Bad Request (\"b'{\"message\":\"bad parameter: \\'path\\' cannot be empty\"}'\")...\nCreating client with base_url of 0.0.0.0:2375\nSuccessfully created container with Id ff5afb6f55fdfb4cc82d70f882aa7d7db74744f46e2d061b92c4c27e5ac22d69\nTrying to put archive (base_url=0.0.0.0:2375)...\nDidn't see exception while putting archive with base_url 0.0.0.0:2375!\n```. Bah, looks like this one is caused by #1321. Downgrading to requests 2.12.1 fixed things; debug logs showed that params weren't being added to the endpoint being hit in the case of the Unix socket, but were for the TCP one.\nBah. :). My concern is that, as it stands, duplicate networks get created (by default) whereas they're not created by default from the Docker CLI. But if that's the expected behavior, I can just update the docs here and rename the commit.. It might be a good idea to make this return a namedtuple (with output and exit_code) since then a user could do:\ncommand = container.exec_run(...)\nif command.exit_code == 0:\n  ...\ninstead of having to deal with indices.. ",
    "seguins": "I have added the backward compatibility and I have created a new unit test for it.\n. ",
    "intrepidlemon": "cleanup. ",
    "WhyNotHugo": "Looks like the bot is bugged and thinks the commits are still unsigned. :(\n. PyPi is the official source for python packages; is there some issue for explicitly wanting to not render the README there?\nAt a first glance, with a broken readme, the project looked like an unpolished library, which is definitely is not.\n. ",
    "yijia2413": "updated, thanks\n. ",
    "badc0re": "The index extension is still not supported in v2 ...\nhttps://github.com/docker/distribution/blob/731c43e3dbe30fb87b830bab3a46d940ed649a8a/ROADMAP.md\n. ",
    "benjixx": "@shin- Just tested again with docker-py 1.7.2 and unset DOCKER_TLS_VERIFY, so that docker environment variables were:\nDOCKER_HOST=tcp://192.168.156.137:2376\nDOCKER_MACHINE_NAME=dev2\nDOCKER_CERT_PATH=/Users/benjixx/.docker/machine/machines/dev2\nBut, I'm still seeing the same exception:\nDockerException: Error while fetching server API version: [Errno 1] _ssl.c:507: error:1407742E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert protocol version\n. I'm running latest Mac OS X 10.11.3 and have the problem, so I doubt it's directly related to the OS version.\n. @shin- Here's the requested output:\n/Users/benjixx/src/goodplay/.tox/py27-ansible20/lib/python2.7/site-packages/requests/packages/urllib3/util/ssl_.py:315: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#snimissingwarning.\n  SNIMissingWarning\n/Users/benjixx/src/goodplay/.tox/py27-ansible20/lib/python2.7/site-packages/requests/packages/urllib3/util/ssl_.py:120: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\n  InsecurePlatformWarning\nConnection error with protocol PROTOCOL_SSLv23: \"[Errno 1] _ssl.c:507: error:1407742E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert protocol version\"\n/Users/benjixx/src/goodplay/.tox/py27-ansible20/lib/python2.7/site-packages/requests/packages/urllib3/util/ssl_.py:120: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\n  InsecurePlatformWarning\nSuccessful connection with protocol PROTOCOL_TLSv1\nProtocol PROTOCOL_TLSv1_1 not found in SSL protocol list\nTraceback (most recent call last):\n  File \"dockersnippet.py\", line 15, in <module>\n    tls_config.ssl_version = getattr(ssl, proto)\nAttributeError: 'module' object has no attribute 'PROTOCOL_TLSv1_1'\n. My non-working box is running Python 2.7.6 as well. This should be the default Python version running on latest official Mac OS X, so would be great to have it working there as well.\n. BTW, looking at the commit log of the Python ssl module for Python 2.7.x it seems the module attribute PROTOCOL_TLSv1_1 and PROTOCOL_TLSv1_2 have been introduced at least after version 2.7.6 (according to the commit date).\n. @shin- Bugfix PR #971 is working fine for me. Thanks for getting this fixed so quickly.\nSo, is going to land in PyPI together with release 1.8.0 in mid of March? Or are you still considering this for an earlier 1.7.3 release?\n. @adamdecaf You should be able to install master branch via pip like so:\npip install git+https://github.com/docker/docker-py.git@master#egg=docker-py\n. @adamdecaf Could it be that docker-compose is installed in a separate environment? I can see different paths in your provided output: /Library/Python/2.7/site-packages and /usr/local/lib/python2.7/site-packages. Probably you have installed docker-compose via another method.\nI would suggest you try to create a new Python virtualenv together with the latest docker-compose and docker-py@master, and give this a try.\n. ",
    "tatsushid": "Hi, I had an issue with docker-compose originated in the problem discussed here and tried the fix #971. It works, thanks!\n. It may be a little different environment from @thieman mentioned but I tested the #971 patch with Python 2.7.5 on MacOS 10.9.5. I'm using docker-compose installed by Homebrew so I manually applied the patch to /usr/local/opt/docker-compose/libexec/vendor/lib/python2.7/site-packages/docker/tls.py. It works.\n. ",
    "adamdecaf": "What would be the steps to build and use the master branch for this fix? I'm currently unable to use docker-compose because of this. \n. @benjixx I'm not seeing that having done anything to have docker-compose see the updated version.\nDo you know of a timeline for docker-py and docker-compose releases fixes for this change? \n```\nadam@Planet-X -- ~:  pip install git+https://github.com/docker/docker-py.git@master#egg=docker-py\nRequirement already satisfied (use --upgrade to upgrade): docker-py from git+https://github.com/docker/docker-py.git@master#egg=docker-py in /usr/local/lib/python2.7/site-packages\nRequirement already satisfied (use --upgrade to upgrade): requests>=2.5.2 in /usr/local/lib/python2.7/site-packages (from docker-py)\nRequirement already satisfied (use --upgrade to upgrade): six>=1.4.0 in /usr/local/lib/python2.7/site-packages (from docker-py)\nRequirement already satisfied (use --upgrade to upgrade): websocket-client>=0.32.0 in /usr/local/lib/python2.7/site-packages (from docker-py)\nadam@Planet-X -- ~:  pip install --upgrade git+https://github.com/docker/docker-py.git@master#egg=docker-py\nCollecting docker-py from git+https://github.com/docker/docker-py.git@master#egg=docker-py\n  Cloning https://github.com/docker/docker-py.git (to master) to /private/var/folders/g2/6ccx3pj11rzcwwbwyqzs8n8w0000gn/T/pip-build-csDu5t/docker-py\nCollecting requests>=2.5.2 (from docker-py)\n  Using cached requests-2.9.1-py2.py3-none-any.whl\nCollecting six>=1.4.0 (from docker-py)\n  Downloading six-1.10.0-py2.py3-none-any.whl\nRequirement already up-to-date: websocket-client>=0.32.0 in /usr/local/lib/python2.7/site-packages (from docker-py)\nInstalling collected packages: requests, six, docker-py\n  Found existing installation: requests 2.7.0\n    Uninstalling requests-2.7.0:\n      Successfully uninstalled requests-2.7.0\n  Found existing installation: six 1.9.0\n    Uninstalling six-1.9.0:\n      Successfully uninstalled six-1.9.0\n  Found existing installation: docker-py 1.8.0.dev0\n    Uninstalling docker-py-1.8.0.dev0:\n      Successfully uninstalled docker-py-1.8.0.dev0\n  Running setup.py install for docker-py ... done\nSuccessfully installed docker-py-1.8.0.dev0 requests-2.9.1 six-1.10.0\nadam@Planet-X -- ~:  pip show docker-py\nMetadata-Version: 1.1\nName: docker-py\nVersion: 1.8.0.dev0\nSummary: Python client for Docker.\nHome-page: https://github.com/docker/docker-py/\nAuthor: UNKNOWN\nAuthor-email: UNKNOWN\nLicense: UNKNOWN\nLocation: /usr/local/lib/python2.7/site-packages\nRequires: requests, six, websocket-client\nClassifiers:\n  Development Status :: 4 - Beta\n  Environment :: Other Environment\n  Intended Audience :: Developers\n  Operating System :: OS Independent\n  Programming Language :: Python\n  Programming Language :: Python :: 2.6\n  Programming Language :: Python :: 2.7\n  Programming Language :: Python :: 3.3\n  Programming Language :: Python :: 3.4\n  Topic :: Utilities\n  License :: OSI Approved :: Apache Software License\nadam@Planet-X -- ~:  docker-compose version\ndocker-compose version 1.6.2, build unknown\ndocker-py version: 1.7.2\nCPython version: 2.7.5\nOpenSSL version: OpenSSL 0.9.8zg 14 July 2015\nadam@Planet-X -- ~:  big ps\n         run  docker-compose -f /Users/adam/src/banno/big/current-compose.yml ps from \"./src/banno/big\"\n/Library/Python/2.7/site-packages/requests-2.7.0-py2.7.egg/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\n  InsecurePlatformWarning\nERROR: SSL error: [Errno 1] _ssl.c:504: error:1407742E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert protocol version\n/Users/adam/src/banno/big/lib/big_helper.rb:9:in run!': 'docker-compose -f /Users/adam/src/banno/big/current-compose.yml ps' failed with exit code: pid 40596 exit 1 (RuntimeError)\n    from /Users/adam/src/banno/big/lib/big_helper.rb:153:inblock in docker_compose'\n    from /usr/local/lib/ruby/gems/2.3.0/gems/thor-0.19.1/lib/thor/actions.rb:184:in block in inside'\n    from /usr/local/Cellar/ruby/2.3.0/lib/ruby/2.3.0/fileutils.rb:128:inchdir'\n    from /usr/local/Cellar/ruby/2.3.0/lib/ruby/2.3.0/fileutils.rb:128:in cd'\n    from /usr/local/lib/ruby/gems/2.3.0/gems/thor-0.19.1/lib/thor/actions.rb:184:ininside'\n    from /Users/adam/src/banno/big/lib/big_helper.rb:148:in docker_compose'\n    from /Users/adam/bin/big:208:inps'\n    from /usr/local/lib/ruby/gems/2.3.0/gems/thor-0.19.1/lib/thor/command.rb:27:in run'\n    from /usr/local/lib/ruby/gems/2.3.0/gems/thor-0.19.1/lib/thor/invocation.rb:126:ininvoke_command'\n    from /usr/local/lib/ruby/gems/2.3.0/gems/thor-0.19.1/lib/thor.rb:359:in dispatch'\n    from /usr/local/lib/ruby/gems/2.3.0/gems/thor-0.19.1/lib/thor/base.rb:440:instart'\n    from /Users/adam/bin/big:329:in `'\n```\n. @benjixx An upgrade of the underlying python version (from an upgrade to 10.11) seems to have fixed the issue. I'm on python 2.7.10 now \n. ",
    "tgerbier": "Thanks for the good job.\nI concur, and I tried to find some help() ... help(docker.utils.LogConfig), but sadly I just get the dict type documentation.\nAs a first step, some docstring would be greatly appreciate ! \n```\nHelp on class LogConfig in module docker.utils.types:\nclass LogConfig(DictType)\n |  dict() -> new empty dictionary\n |  dict(mapping) -> new dictionary initialized from a mapping object's\n |      (key, value) pairs\n |  dict(iterable) -> new dictionary initialized as if via:\n |      d = {}\n |      for k, v in iterable:\n |          d[k] = v\n |  dict(**kwargs) -> new dictionary initialized with the name=value pairs\n |      in the keyword argument list.  For example:  dict(one=1, two=2)\n |\n |  Method resolution order:\n |      LogConfig\n |      DictType\n...\n```\n. ",
    "gferon": "Done!\n. TIL you should not pressed the \"Update branch\" button on a GitHub PR and do it manually when commits needs to be signed.\nI just rebased to master again.\n. ",
    "JesusTinoco": "@dnephin I think so, it seems that the API Rest provided by docker only set the ContainerIDFile and doesn't create the file. Checking the  api client used by docker seems that it creates that file as well. Am I wrong?\n. @dnephin any new update? \n. @dnephin thanks for the reply! Should I open an issue? or just wait for your review?\n. Do you think that this pull request could be interesting for docker-py? It would help to implement a solution for this issue: https://github.com/docker/compose/issues/940\n. ",
    "MateusAmin": "@JesusTinoco Thanks for the contribution! I was looking into doing this myself. If @dnephin thinks this needs anymore work I would be happy to help out if wanted.\nI took a brief look at the commit and based on my meager understanding of the code base everything looks good. I think the branch name does not follow the Docker contribution convention: XXXX-something where XXXX is the number of the issue.\n. ",
    "jamiehannaford": "@dnephin Apart from a rebase, does this PR need anything else? I would also like this feature.\n. ",
    "AndreLouisCaron": "I started working on a fix for docker/compose#940 and I hit a wall when I saw that the docker-py doesn't have an option to create the CID file.\nI was going to implement the change when I stumbled upon this existing PR.  Any chance we can have this fix included?\nThanks!. @shin- I think the basic idea was feature parity of docker-compose with docker run.  Currently, you can docker run -cidfile ..., but you can't do the same from a compose file.\nI personally don't mind whether it's implemented inside docker-py or internally in docker-compose, but why not solve the problem once for everyone in docker-py?. @shin- Just to make sure we're on the same page... the purpose of this change is to send the HostConfig/ContainerIDFile parameter in the ContainerCreate operation so that the Docker daemon can write the container ID inside the container before it boots (see ContainerCreate).\nHow would I accomplish something equivalent without changing docker-py?. @shin- OK maybe I need to take a step back.  I got here thinking I could use this to let the container know what its own ID is (see suggestions in this StackOverflow question: https://stackoverflow.com/q/20995351/313063).  The other suggestion seems to be to use $HOSTNAME, but that only gives a short ID.. ",
    "AlissonMMenezes": "Ok, thanks!! =)\n. ",
    "uggla": "Thanks @shin- for the prompt answer ! \nSorry the doc is not so clear on that point.\nSo I need to pass a tar file which include my files + the Dockerfile into fileobj and set custom_context=True.\nDo I understand well ? If yes, is it still possible to specify a specific Dockerfile name (ex : Dockerfile.debian)\n. ",
    "abtreece": "Also looking for some clarification on this. Trying to pass a Dockerfile with fileobj using docker-py version 1.10.6. It fails with a error: lstat testfile: no such file or directory but that file is definitely in the directory. I've tried to implement the custom_context param, but that errors with an unexpected EOF\nI'm trying to implement with a simple script similar to the fileobj script above.  Is this a bug or am i missing something in the documentation?. ",
    "aldenpeterson-wf": "It would also be helpful if you could simply print the full cli.build() results.\nInstead of your above, it would be useful if it could work like:\nprint cli.build(\n    path='.',\n    tag='ESSAI',\n    rm=True)\nIt is... not clear at all you need to read each line of the build output (your example is the only example I saw of this) in order to see your full docker build output.\nWhen I try just a print on the base cli.build() command the following obscure output is printed, since it does not return the build text:\n<generator object _stream_helper at 0x10fa295a0>\n. As I've been investigating this further, it seems like I can work around this by recreating the docker-py Client object itself (instead of reusing the same Client object for both calls). That makes me less convinced it is a bug with docker itself but more how docker-py is wrapping things.\nThe following does not have the problem:\n```\nfrom docker.client import Client\nfrom docker.utils import kwargs_from_env\ncli = Client(**kwargs_from_env())\nprint kwargs_from_env()\nname_one = 'exec_container'\ncli.create_container(**{'name': name_one, 'image': 'golang'})\ncli.start(name_one)\ncli.logs(name_one, stdout=True, stderr=True)\ncli2 = Client(**kwargs_from_env())\ne = cli2.exec_create(container=name_one, cmd='ls /usr/local/bin')\nprint cli2.exec_start(exec_id=e['Id'])\n```\nNotice I am using two separate clients in this instance. It seems that a single Client() instances is unable to handle both logs and exec_start() without failing.\nA workaround, though it feels pretty hacky, is to instantiate new Client() objects each time I need to use one.\n. @shin-  it appears closed here.\nThis is an open issue as is this one.\n. @shin-  I did not see that PR for Docker when I looked before. \nThis can probably be closed then, though hopefully anyone else that has this same problem can benefit from my efforts.\n. ",
    "apeace": "Thanks @shin- \nI can work on a patch for this, though I'm sure it will take me some time :) I have a couple questions for you:\n1. Are you sure this wouldn't be considered a bug? I believe (though have not yet tested) that this would also not work for public repositories if you use SSH syntax (git@github.com:user/repo.git). Essentially I think git SSH URLs will not work. Perhaps the bug should be clarified to say \"git SSH URLs\" instead of any \"git URL\"?\n2. If indeed this is not a bug, could you give me some guidance on how a docker-py client might enable this feature? For example, maybe a client like docker-compose might pass a flag (e.g. to this call)? Any guidance on that would be appreciated--I will incorporate it into my patch.\n. @shin- I'm not sure there is a bug at the engine level. If you read my last comment on docker/compose#2856 I think the problem is that the git clone is happening from inside the docker daemon. This means it doesn't have access to your SSH config.\nOne possible fix is docker/docker#6396, but I believe the docker binary has a simpler workaround that docker-py could use too.\nThe docker binary itself will perform the clone locally first (when you run docker build), before sending a build context to the docker daemon. What I'm suggesting is that docker-py should emulate this same behavior. Does that seem right to you?\nTherefore I am thinking the implementation in docker-py might look like this:\n- Create a util to clone a git repo and tar it like you said.\n- In BuildApiMixin.build, IF path begins with git@, then always clone/tar the repo locally, and provide it do the docker daemon.\nThis way clients (like docker-compose) don't have to change, and they get a fixed behavior that should always be breaking right now.\n. I believe @dnephin is right, the daemon is most likely running as a different user. I think that is why docker has the behavior of cloning locally instead of passing the remote param to the daemon.\n. @shin- sure. I should have time to work on a patch over the next couple weeks.\nJust to be clear, you're saying clients should have the choice to either:\n- Use docker.util.clone_repo (or something), and then pass with fileobj and custom_context\n- OR, pass path=git@github... and expect it will be passed to the docker daemon\nThank you again! :)\n. @somombo Apologies, I never ended up having the time to get to this--shouldn't have promised I would have! I still think it would be a great feature to have, but I'm no longer actively using docker-compose so I'm not sure I will ever find the time now :(\n. ",
    "somombo": "Hi @apeace  I recently ran into issues noted in #2856.  How is the progress going? Thanks so much for taking this on!\n. @shin- I think this issue should actually be labelled as a bug not a feature-request. Since\nThe docker-py documentation under client API build section indicatest that path can be a local path (to a directory containing a Dockerfile) or a remote URL.\nIn addition, the official compose file spec under the context section notes that, the context can either be:\n\" Either a path to a directory containing a Dockerfile, OR a url to a git repository.\"\nThe intent seems clear to me: There is neither any mention in that reference that using ssh git urls is an exceptional situation, nor that it an \"upcoming feature\". Thus from reading those references alone users should expect regular git urls to \"just work\". And if that expectation is not being met, then what we have is bug, not a missing feature.\nMy hope is that if this is escalated to being the bug that it is, it will be quashed more quickly. The last post on this thread was over half a year ago. \n. @apeace I agree that you should clarify the title of this thread as \"SSH git URLs\" instead of just \"git URL\", because https urls seems to be working fine.\n. ",
    "kwidholm-tm": "\nThe intent seems clear to me: There is neither any mention in that reference that using ssh git urls is an exceptional situation, nor that it an \"upcoming feature\". Thus from reading those references alone users should expect regular git urls to \"just work\". And if that expectation is not being met, then what we have is bug, not a missing feature.\nMy hope is that if this is escalated to being the bug that it is, it will be quashed more quickly. The last post on this thread was over half a year ago.\n\nJust to say that I followed the threads to this thread due to having read the documentation and having run into this exact issue\n$ docker-compose up\nBuilding https-proxy\nERROR: error fetching: fatal: cannot run ssh: No such file or directory\nfatal: unable to fork\n: exit status 128\nDocumentation states this should work:\nbuild: git@github.com:ORG_NAME/REPO.git\nIt does not. \nThe workarounds (github tokens + https) 1) only work for certain situations, and 2) still don't allow docker compose /docker py to work as documented.\nSo I agree with @somombo here. We can blame the engine or docker.py. It doesn't matter. In the end, the feature does not work as documented. \nSo perhaps the bug is in the documentation, and the documentation should be changed to reflect that building in docker compose via a private github repo over ssh is not supported. But I would imagine I would want to write feature parity between the two engines so they are interoperable.. ",
    "carlskii": "Thanks for the update - Now things are getting complicated. I've no idea where to start here. Any chance of a simple example ? \nAlternatively is there a better way to push data into a running container ? \n. Thats what I thought but has this not been deprecated ?\nClient.copy() is deprecated for API version >= 1.20, ' 'please use get_archive() instead',\n. @zbyte64 Could you provide a complete example here. I'm still struggling to get this to work. \nGoing back to my original question, I need to mimic this but using Python:\necho \"hello\" | docker exec -i $3 sh -c 'cat >/text.txt'\n. ",
    "monperrus": "This may be the complete example you're looking for: https://stackoverflow.com/questions/46521166/how-to-write-to-stdin-in-a-tls-enabled-docker-using-the-python-docker-api. Done. Thanks for the 100% perfect commands.\n. ",
    "whiteShtef": "@zbyte64 using \nos.write(socket.fileno(), b'echo hello world') doesn't work for me. All that ever gets passed to the containers stdin , for some to me unknown reason, is \\x01.\nI'm literally using a copypasta of your (above) code, and inside of the container there is a simple c program that prints whatever scanf(\"%s\",&s)captures. All that ever gets printed is \\x01.\nI also see that this is a relatively old issue, have you given any thoughts to fixing/implementing this in the docker-py API?. ",
    "colin-nolan": "I am also encountering this problem. It occurs when DOCKER_CERT_PATH and DOCKER_TLS_VERIFY are both set to empty string (i.e. the rather odd way of stating that the the connection to the Docker daemon should not be secure). This environment setup is not tested here: https://github.com/docker/docker-py/blob/387db11009f4b4f64a4f2c6fd64d3eeb01828585/tests/unit/utils_test.py#L159.\nThe issue is with the kwargs_from_env method in docker/utils/util.py. In release 1.7.0, a TLSConfig instance was only constructed if tls_verify and cert_path were True:\nhttps://github.com/docker/docker-py/blob/release-1.7.0/docker/utils/utils.py#L460\nThis is no longer the case in release 1.7.1 onwards:\nhttps://github.com/docker/docker-py/blob/1.7.1-release/docker/utils/utils.py#L485\n. I've just had to do the same awkward archiving of a file before passing the bytes through as the data.\nI also think that it would be nice if the Docker client could handle this as it's probably much more common that people will have a string to go in a file/file/directory of files, that they want to inject into a running container, than having a pre-made archive with them in. However, given the docker API, I do see why only the latter is currently supported.  \nDid you open a separate pull request for this feature @borismod?. ",
    "tbsf": "Created PR with this fix:\nhttps://github.com/docker/docker-py/pull/987\n. It looks like the 1.15 API introduces the NetworkMode option:\nhttps://docs.docker.com/engine/reference/api/docker_remote_api_v1.15/\nIn the Ansible docker module, there is nothing passed for version to the create_host_config option, so version defaults to 1.21 and causes network_mode to be set to default.\n. When docker.utils.create_host_config is called:\npython\n    if not version:\n        warnings.warn(\n            'docker.utils.create_host_config() is deprecated. Please use '\n            'Client.create_host_config() instead.'\n        )\n        version = constants.DEFAULT_DOCKER_API_VERSION\nsets version = '1.21'\npython\n    if network_mode:\n        host_config['NetworkMode'] = network_mode\n    elif network_mode is None and compare_version('1.19', version) > 0:\n        host_config['NetworkMode'] = 'default'\ncauses host_config['NetworkMode'] to be 'default'.\nParameters passed to create_host_config (as JSON):\n{\n \"publish_all_ports\": true, \n \"links\": null, \n \"lxc_conf\": null, \n \"network_mode\": null, \n \"binds\": null, \n \"port_bindings\": {\n  \"2888\": [\n   \"0.0.0.0\", \n   2888\n  ], \n  \"3888\": [\n   \"0.0.0.0\", \n   3888\n  ], \n  \"2181\": [\n   \"0.0.0.0\", \n   2181\n  ]\n }, \n \"privileged\": true\n}\nHostConfig returned from create_host_config (as JSON):\n{\n \"PortBindings\": {\n  \"3888/tcp\": [\n   {\n    \"HostPort\": \"3888\", \n    \"HostIp\": \"0.0.0.0\"\n   }\n  ], \n  \"2181/tcp\": [\n   {\n    \"HostPort\": \"2181\", \n    \"HostIp\": \"0.0.0.0\"\n   }\n  ], \n  \"2888/tcp\": [\n   {\n    \"HostPort\": \"2888\", \n    \"HostIp\": \"0.0.0.0\"\n   }\n  ]\n }, \n \"NetworkMode\": \"default\", \n \"Privileged\": true, \n \"PublishAllPorts\": true\n}\nThe version is used in the create_host_config function but not set.\n. Ok, I'll take it over there.\nOne question I have is, I only see in 1.21 and later documentation for Docker:\nNetworkMode - Sets the networking mode for the container. Supported standard values are: bridge, host, none, and container:<name|id>. Any other value is taken as a custom network\u2019s name to which this container should connect to.\nDoes this mean the value of \"default\" is taken as a custom network's name in 1.21+? Is this different behavior than in 1.19/1.20?\n. ",
    "adityamarella": "Here is the PR\nhttps://github.com/docker/docker-py/pull/1002\n. Thanks. sleep 1 fixes those errors for now. I will get my changes in and see if I can fix the tests in the right way. \n. Added the unit test.\n. ",
    "fixe": "@SamirTalwar we are having the same issue -- did you find any workaround?\n. ",
    "SamirTalwar": "@fixe: I'm afraid not. Either the server or docker-py will need to be updated to accommodate the variance in the tarball.\n. @TomasTomecek: That describes the fourth digit from the right:\n``` c\n/ Bits used in the mode field, values in octal.  /\ndefine TSUID    04000          / set UID on execution /\ndefine TSGID    02000          / set GID on execution /\ndefine TSVTX    01000          / reserved /\n                            /* file permissions */\n\ndefine TUREAD   00400          / read by owner /\ndefine TUWRITE  00200          / write by owner /\ndefine TUEXEC   00100          / execute/search by owner /\ndefine TGREAD   00040          / read by group /\ndefine TGWRITE  00020          / write by group /\ndefine TGEXEC   00010          / execute/search by group /\ndefine TOREAD   00004          / read by other /\ndefine TOWRITE  00002          / write by other /\ndefine TOEXEC   00001          / execute/search by other /\n```\nAll those #defines would imply that mode is four octal digits long, and therefore a null-terminated char[5], with the left-most digit being the set-UID/set-GID/\"reserved\" value. However, it's actually got a size of 7 (char mode[8]). That \"0\"/\"1\" discrepancy is in byte 1 (0-indexed), and the user/group/other flags are in bytes 4\u20136. I would expect the UID/GID/VTX flags to live in byte 3 in this situation, which is definitely a \"0\" in both cases, so that seems right.\nIn short, I really don't know what's going on, but I don't think it's as simple as that. The only thing I can think of is that three bytes of the mode are to be ignored for alignment purposes and that perhaps the server is incorrectly comparing the whole 7 characters instead of just the last 4.\nOf course, I don't know that this is what's causing the problem at all. It could be the username/group.\n. ",
    "deanrather": "Did anyone ever find a work-around for this?\n. ",
    "netproteus": "Is there any update on this?\n. ",
    "Larsjep": "I'm working on a fix for this problem. Unfortunately this requires a fix in golang and a docker daemon build with this fixed golang. See pullrequest #1582.. Anyone knows what this \"janky\" check is and why nothing happens with it ?. Hi @tomyan,\nI'm sorry but I'm also waiting for a response from the maintainers.\nThey might be busy with other stuff, because I can see several other pull requests waiting.\nBr\n  Lars . Unfortunately, the docker team decided to reintroduce the incorrect bits to avoid breaking the cache.\nThey said that they already planned to break the cache when introducing buildkit and then correct the bits at that point. But I haven't been able to get an answer on when buildkit would be implemented and that is 4 months ago :(.\nSee moby/moby#33935. ",
    "kilianc": "Is there a workaround while we wait for the patch?. Any updates on this?. ",
    "pokix": "I tried but it doesn't solve the problem\n. Yes, that was the issue. It is working now.\nThank you very much for your answers!\n. ",
    "raugturi": "Found my own answer: If you pass network_mode=NETNAME in as part of the host_config you don't need to connect it manually later.\n. ",
    "jefsayshi": "Thanks for the help. Here is the code that I got working.\n``` python\nfrom docker import Client\nfrom docker.utils import LogConfig\nc = Client()\nlog_cfg = LogConfig(type=LogConfig.types.GELF, config={ 'gelf-address': 'udp://:12201' })\nhost_cfg = c.create_host_config(log_config=log_cfg)\nc.create_container('busybox', 'echo Hello Graylog', host_config=host_cfg)\n```\n. ",
    "endophage": "Thank you! ssl_match_hostname.py plus tests LGTM. Can't speak to the requirements changes et al.\n. ",
    "FiloSottile": "This seems not to work for me on OS X.\nhttps://gist.github.com/FiloSottile/d308789cc7a8f1de8f36a127ecfbff19\nThe version is 1.8.0, the CLI works, the certificate has the IP on it, but it still raises requests.exceptions.SSLError: hostname '192.168.64.6' doesn't match 'localhost'\n. Interestingly, patching the function to print the certificate yields\n{'subjectAltName': [('DNS', 'localhost')], 'subject': ((('commonName', None),),)}\nso it's a parsing problem, not a matching one. I'll open a new issue.\n. Ok, at the moment I am failing to see how this ever worked, because match_hostname is called here https://github.com/shazow/urllib3/blob/8513dcb2/urllib3/connection.py#L305 on the result of getpeercert() here https://github.com/shazow/urllib3/blob/f7980a68/urllib3/contrib/pyopenssl.py#L270-L289 which only returns 'DNS' entries. \ud83d\ude15 \nEDIT: Oh, I see, that codepath that misses support for IPs is only enabled on inject_into_urllib3() https://github.com/shazow/urllib3/blob/f7980a68/urllib3/contrib/pyopenssl.py#L110\nI guess that now I have two things to do: figure out why and when it falls back, adn add IP detection to that urllib3 code.\n. Huh, interestingly requests always tries to run inject_into_urllib3() https://github.com/kennethreitz/requests/blob/190cddd4a/requests/init.py#L51-L56 and I suppose that works (and breaks us) when pyOpenSSL is installed.\nIndeed running extract_from_urllib3 fixes it.\n```\n\n\n\nimport requests\nrequests.packages.urllib3.contrib.pyopenssl.extract_from_urllib3()\nimport docker\ndocker.Client.from_env().ping()\nu'OK'\n```\n\n\n\nThis is interesting, an \"upgraded\" code path that has less features than the default. The relevant docs are here https://github.com/shazow/urllib3/blob/f7980a68a/docs/security.rst\nLooks like the best thing to do while waiting for my PR to urllib3 to be merged and then ported into request, would be to run extract_from_urllib3 when import ssl is available (Python > 2.7.8). Like:\ntry:\n    import ssl\nexcept ImportError:\n    pass\nelse:\n    import requests\n    requests.packages.urllib3.contrib.pyopenssl.extract_from_urllib3()\n. ",
    "MHBauer": "Looks more comprehensive. I see a filter for volume and isolation in the docker docs. Given windows isn't prime time yet, I think you could leave isolation off, but I'd like to see volume included.\n. ",
    "andornaut": "\nCan this be fixed by updating your setuptools package?\n\nYep, however, it'd be nice if docker-py continued to work with older versions of setuptools.\n. ",
    "nightvisi0n": "This should be fixed asap since it breaks travis-ci builds: https://travis-ci.org/jneureuther/dockgraph/jobs/131267651#L287\n. Thank you for clarifying this. Then i would suggest to close this issue? \n. ",
    "komuw": "after the merging of https://github.com/docker/docker-py/pull/1181 , I believe this issue ought to be closed now.. ",
    "pdbogen": "n.b., this is true on python 3 in docker-py 1.7.2\n. Alright. I guess that's not in 1.7.2 (or else it's broken). I don't know what the general release schedule is, but if a release incorporating that change isn't due for a while, perhaps releasing 1.7.3 is warranted, since the 1.7.2 docs represent it works and it in fact doesn't. But, given that a 'fix' so to speak is in master I'll go ahead and close this issue.\n. ",
    "mschnitzer": "OK, I found put_archive as an option, but I don't understand why it's better to specify a TAR archive? Don't want to pack a 429 Bytes file into an archive.\n. ",
    "borismod": "Hello gents, Let me join to this conversation and I share my feelings with you. I felt the same confusion when I had to archive a single file just in order to copy it into the archive. I am sure the above code snippet from gist can be added as an additional method on docker client. Do you want me to open a separate Issue/FR for this?. @colin-nolan no I haven't open a separate issue for this one.  . ",
    "felixonmars": "I can fix the flake8 errors here by adding some noqa, and will run compose's test suite soon.\nDo you have any idea for the E   docker.errors.APIError: 500 Server Error: Internal Server Error (\"b'Invalid signal: Signals.SIGKILL'\") failure in janky? I tried a bit but it's still strange :(\n. Thank you :)\n. Hmm, this is really weird. The IP Address test passes here with Python 2.7.11, and also passes on the Jenkins build, but failed on Travis.\n. Oops, nevermind. Turns out the ipaddress module is also needed. Updated and all should work fine now!\n. Looks like the Jenkins failure is not related to this PR.\n. According to https://github.com/felixonmars/docker-py/blob/backports/docker/ssladapter/ssladapter.py#L18 and https://bugs.python.org/issue23239, the IP address support wasn't backported to Python 3.3 and 3.4 (yet?), so when I use < 3.3 the tests failed.\n. Ah, I see. Thanks :)\n. ",
    "joshpurvis": "I needed this for swarm, so I implemented the changes in PR #1160.\n. I've pushed a change/rebased the branch to fix an issue with the InvalidVersion error on the earlier API versions. (I had the API version comparison reversed in create_container_config.)\nI also switched it from an error to a deprecation warning, as this mistake made me realize the old cpushare/cpuset unit tests would be broken when testing against newer API versions. (those that still pass these as kwargs into create_container: https://github.com/docker/docker-py/blob/7de30c3a9b75b12f26745c662f5d5bd3e1839021/tests/unit/container_test.py#L270)\nLet me know if the old unit tests should simply be removed.\n. ",
    "ibigbug": "I found that may be a requests issue.\nRequesting a docker host with requests directly also encounters the same issue.\nTo reproduce:\n``` python\nr = requests.get('http://DOCKER_HOST/containers/A_LONG_RUNNING_CONTAINER_SUCH_AS_NGINX/logs?stdout=1&stderr=1&follow=1', stream=True)\nwhile True:\n    sys.stdout.write(r.raw.read(1024))\n```\nTracestack:\n\u00a8 - - [15/Apr/2016:09:22:26 +0000] \"GET /Traceback (most recent call last):\n  File \"client.py\", line 29, in <module>\n    sys.stdout.write(r.raw.read(1024))\n  File \"/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/response.py\", line 311, in read\n    flush_decoder = True\n  File \"/usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py\", line 35, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/response.py\", line 244, in _error_catcher\n    raise ProtocolError('Connection broken: %r' % e, e)\nrequests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(859 bytes read)', IncompleteRead(859 bytes read))\nwith requests==2.9.1\nWith raw httplib to send this request is OK. I'm currently working into urllib3 to see if there were any issues with it.\n. Not seems to be a docker engine issue, curl or nodejs solution are both working correctly.\n. Debugging into https://github.com/python/cpython/blob/2.7/Lib/socket.py#L485, a break happend and httplib read an empty line, and https://github.com/python/cpython/blob/2.7/Lib/httplib.py#L637 closed the connection.\nDo you have any ideas about how can the socket blocking recv receive an empty byte?\n. ",
    "bacongobbler": "I've been hitting the same issue recently. I'll test over here and see if this fixes the issue for me :)\n. I can confirm that this change fixes the root issue. For what it's worth though, we have a TLS-disabled distribution without auth.\n. And to confirm, this is indeed the correct fix according to the daemon peeps: https://github.com/docker/docker/issues/10983#issuecomment-85892396\n. I'm on 1.9.1 as well, which is what's installed by default on kubernetes. I originally noticed the issue on redhat's bugzilla, which is what brought me here. Just looking for a quick n' dirty solution to my problems. ;)\nI'm going to try and verify this on 1.11.0 or 1.10.3 and get back to you. Thanks!\n. Since kubernetes just installs the default package from dnf, my assumption is projectatomic/docker on Fedora 23. I wasn't aware that Fedora ships a fork of Docker.\n. ",
    "runcom": "@bacongobbler I think the question was whether you're using vanilla docker or projectatomic/docker (the one shipped in RHEL/Fedora/CentOS)\n. So, the issue was in our projectatomic fork and it's now fixed. Expect dnf to pull the new pkg. \n. Ping someone, we fixed RT in docker and would like to have py support also. \nPatch LGTM fwiw. ",
    "maxamillion": ":+1: \n. ",
    "mikroskeem": "ETA? I want to remove subprocess.call(..pipework..)  and use native way to set IP addresses\n. Okay thanks! It worked :+1:\n. ",
    "Gnork": "Hi.\nIt's actually easy if you know that the -e flag of the docker CLI means Environment Variable. So you can look up how to set them with docker-py and use the same parameters as with the CLI. \nI'm on my phone and can't give you a Code example. \nHope this helps\n. Are you using a dict? If I remember correctly, I had problems with them as well. But you can instead use a list, so that you don't need this \"None Workaround\". \n. ",
    "netoisc": "Hi @Gnork , I had the same problem. I've used docker-py too, defining env-vars through a dict and in my case I solved this problem by setting the Key with constraint:node and the Value with =agent1. Now, when I run containers i can see the filter correctly within swarm constraints ej. \"Labels\": {\n                \"com.docker.swarm.constraints\": \"[\\\"node==agent1\\\"]\"\n. ",
    "raimonbosch": "Version: Docker version 1.10.3, build 20f81dd\nI tried to remove and add some containers and run \"docker ps\" and results are correct (also with watch -n 2 docker ps). So the problem must be in docker-py or in the way that it communicates with Docker API.\n. Seems related to my code. I will reopen if I find a concrete error on docker-py.\n. ",
    "romaimperator": "After reviewing my test with the Ansible module, it appears that passing the dns search param works with the existing version 1.8.1 of docker-py already so this pull request isn't required for the ansible module. I'm not sure if this would still be useful anyways though.\n. ",
    "vangie": "@shin- I have updated environment information.\n. root@ide-staging-01:~# pip freeze | grep -e urllib3 -e requests\nrequests==2.7.0\nurllib3==1.7.1\n. root@ide-staging-01:~# dpkg -l | grep urllib3\nii  python-urllib3                        1.7.1-1ubuntu3                        all          HTTP library with thread-safe connection pooling for Python\nii  python-urllib3-whl                    1.7.1-1ubuntu3                        all          HTTP library with thread-safe connection pooling\nroot@ide-staging-01:~# dpkg -l | grep request\nii  iputils-arping                        3:20121221-4ubuntu1.1                 amd64        Tool to send ICMP echo requests to an ARP address\nii  python-requests                       2.2.1-1ubuntu0.3                      all          elegant and simple HTTP library for Python, built for human beings\nii  python-requests-whl                   2.2.1-1ubuntu0.3                      all          elegant and simple HTTP library for Python, built for human beings\n. ",
    "DavidPineauScality": "Noticed the same issue on my side.\nI was setting up some Docker containers through ansible.\nSystem information:\nOS/Distrib: Linux / Ubuntu14.04\nPython version: 2.7\ndocker-py version: 1.8.1\ndocker version: 1.11.1\nAfter careful investigation, I noticed a few things:\n1. requests module was present, and looked like it matched the expectations of docker-py\n2. No module could be imported from within docker-py\n3. docker-py thus fallbacked onto urllib3 rather than requests.packages.urllib3\n4. urllib3 somehow did not provide the same API than my version of requests.packages.urllib3\nDigging more into it, I found out the following:\n1. https://github.com/kennethreitz/requests/blob/master/requests/packages/init.py#L14 : See the quote: requests.packages.urllib3 is not urllib3.\n   I think they attempt to imply that both should not be seen as equivalent. As such, the following lines were surprising to me, but from reading the comments on this PR, this may be intended for a recent urllib3 version: https://github.com/docker/docker-py/blob/master/docker/ssladapter/ssladapter.py#L13. Does it deserve an improvement ?\n2. Changing the ordering of the paths in PYTHONPATH seemed to fix the issue or resolution of requests.packages.urllib3, thus fixing docker-py\n   It looked like on the machine having this issue, some manual install/removal somehow changed the order of the paths in PYTHONPATH, so that /usr/local/lib/python2.7/dist-packages was coming later in the list than /usr/lib/python2.7/dist-packages.\n   The effect was that importing something within requests suche as requests.packages yielded a no such module error.\n   Inverting the order of the two paths previously quoted seemed to solve the issue (we did that globally through the file /etc/environment)\nI do not know how our machine came to this situation, and I couldn't find out more than this in the time I spent investigating. \n. @shin- Thanks for being reactive on this subject.\nI do not know which urllib3 version I was using, but it is probably older, as you pointed out.\nThat being said, I'll take anything that does not involve tweaking my environment variables to get requests.packages.urllib3 to be importable when it is present in the system's directories.\n. ",
    "mixmatch": "I'm getting this error with:\ndocker 1.11.1\ndocker-py 1.8.1\nurllib3 1.15.1\nWorks with:\ndocker 1.11.1\ndocker-py 1.7.2\nurllib3 1.15.1\n. ",
    "rpinohtyp": "this fixed it for me\n$ cat /etc/environment .bash_profile .profile | grep python2.7\nPYTHONPATH=\"/usr/local/lib/python2.7/dist-packages:/usr/lib/python2.7/dist-packages\"\nwith:\n$ lsb_release -a\nNo LSB modules are available.\nDistributor ID: LinuxMint\nDescription:    Linux Mint 17.3 Rosa\nRelease:    17.3\nCodename:   rosa\nthx @DavidPineauScality\n. ",
    "oryband": "This is still an issue, and practically breaks all our ansible playbooks on ubuntu 14.04\n. @shin- calling pip install urllib3==1.14 doesn't have any effect because the current OS installation overrides it.\n``` bash\n$ pip freeze\n...\nurllib3==1.7.1\n$ pip install urllib3==1.14\nDownloading/unpacking urllib3==1.14\n  Downloading urllib3-1.14-py2.py3-none-any.whl (89kB): 89kB downloaded\nInstalling collected packages: urllib3\n  Found existing installation: urllib3 1.7.1\n    Not uninstalling urllib3 at /usr/lib/python2.7/dist-packages, owned by OS\nSuccessfully installed urllib3\nCleaning up...\n$ pip freeze\n...\nurllib3==1.7.1\n``\n. @shin- ok, irm /usr/lib/python2.7/dist-packages/urllib3-1.7.1.egg-infoand then successfully installedurllib3==1.14and validated this viapip freeze`, but the problem still persists.\nI can confirm setting $PYTHONPATH like mentioned above fixes the problem.\n. ",
    "omegarus": "Only updating python-urllib3 package to 1.15.1 on Ubuntu 14.04 helped me to overcome this issue. \ndocker-py==1.9.0\ndocker version 1.11.2 api version 1.23\n. ",
    "ascheman": "Had the same problem with \n- ubuntu 14.04\n- docker 1.12.1\n- docker-py==1.8.1\n- urllib3==1.7.1\nSolved it by \npip install urllib3==1.14\nand\nexport PYTHONPATH=/usr/local/lib/python2.7/dist-packages:/usr/lib/python2.7/dist-packages\n. ",
    "VVhiteCoder": "this resolved my similar issue\napt-get install python-openssl. ",
    "c-nichols": "I solved this issue with the following:\nsudo pip install --upgrade pip\nUbuntu/Debian's default version of pip will alter sys.path and thus force requests and urllib3  down to  older versions.  Docker compose uses pip on startup to detect if old versions are installed.\nThis was actually on Bash on Ubuntu for Windows.. ",
    "eon01": "Also solved by :\npip install urllib3==1.14\nexport PYTHONPATH=/usr/local/lib/python2.7/dist-packages:/usr/lib/python2.7/dist-packages. ",
    "a568283992": "I solved by the code : sudo pip install --upgrade pip and \nthen use  pip install -U urllib3.. ",
    "NeoPolus": "Had this problem with docker-compose, but the solution proposed by @a568283992 worked for me:\nsudo pip install --upgrade pip && pip install -U urllib3\n. ",
    "yogeshVU": "solved using sudo easy_install -U pip &&\nsudo pip install docker-py. ",
    "oxtopus": "I encountered this issue, too (see https://github.com/docker/docker-py/issues/1526).  In my case, I'm using salt to configure the instance and something about how it attempts to install docker-py at https://github.com/saltstack-formulas/docker-formula/blob/0bff590b7bdd9568140c9693ca6e8b6fb4731408/docker/init.sls#L133-L155 results in both requests and urllib3 being out of date, so you must explicitly install the right version of dependencies manually or disable docker-py installation.. @aaosssa Ubuntu 14.04 LTS has outdated apt packages for both requests (2.2.1) and urllib3 (1.7.1).  In my case, I was using salt to configure an instance and the docker salt formula attempts to install docker-py.\nIn the following line, the requests import fails, resulting in an attempt to import urllib3 directly.  \npython\ntry:\n    import requests.packages.urllib3 as urllib3\nexcept ImportError:\n    import urllib3\nThen, the following line fails because it successfully imported older version of urllib3.\npython\nurllib3.connection.match_hostname = match_hostname\nI see your point re: avoiding adding dependency, but there is an attempt to import urllib3 directly, and the way it's being used only works with versions 1.8 or greater.  As long as there is a import urllib3 statement, a version should be specified.\nAnother reasonable approach (IMHO) is to not attempt to gracefully recover from the failed import and just let it fail.  That would also avoid adding a dependency.. It's also possible that the requests import passes, but for the reasons I described, it's outdated and results in the same failure.. Thanks for looking into this and I appreciate the consideration.  I understand it's a bit of a corner case, but the reason I brought it up is wrapping the requests import in a try...except ImportError block produces a bit of a red herring, resulting in some unnecessary confusion.\nIn my case, I'm not using docker-py, but the official docker formula attempts to install it by default.  At the very least, this suggestion is informed by the maxim Explicit is better than implicit.. @shin- What does it mean when it says \"This commit cannot be built\"?  The error at https://jenkins.dockerproject.org/job/docker/job/docker-py/job/PR-1527/2/console doesn't seem to be related to this change.  Please let me know if there's anything I can do to move this along.  Thanks.\n18:13:55 [py3.5_1.13.1] =================================== FAILURES ===================================\n18:13:55 [py3.5_1.13.1] ________________________ PluginTest.test_enable_plugin _________________________\n18:13:55 [py3.5_1.13.1] tests/integration/api_plugin_test.py:49: in test_enable_plugin\n18:13:55 [py3.5_1.13.1]     assert self.client.enable_plugin(SSHFS)\n18:13:55 [py3.5_1.13.1] docker/utils/decorators.py:35: in wrapper\n18:13:55 [py3.5_1.13.1]     return f(self, *args, **kwargs)\n18:13:55 [py3.5_1.13.1] docker/api/plugin.py:85: in enable_plugin\n18:13:55 [py3.5_1.13.1]     self._raise_for_status(res)\n18:13:55 [py3.5_1.13.1] docker/api/client.py:216: in _raise_for_status\n18:13:55 [py3.5_1.13.1]     raise create_api_error_from_http_exception(e)\n18:13:55 [py3.5_1.13.1] docker/errors.py:30: in create_api_error_from_http_exception\n18:13:55 [py3.5_1.13.1]     raise cls(e, response=response, explanation=explanation)\n18:13:55 [py3.5_1.13.1] E   docker.errors.APIError: 500 Server Error: Internal Server Error for url: http://docker:2375/v1.26/plugins/vieux/sshfs:latest/enable?timeout=0 (\"error setting up propagated mount dir: invalid argument\")\n18:13:55 [py3.5_1.13.1] === 1 failed, 237 passed, 4 skipped, 1 xfailed, 1 xpassed in 327.33 seconds ====\n18:13:55 [py3.5_1.13.1] Exception ignored in: <socket.socket fd=85, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.17.0.3', 34477), raddr=('172.17.0.2', 2375)>\n18:13:55 [py3.5_1.13.1] ResourceWarning: unclosed <socket.socket fd=85, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.17.0.3', 34477), raddr=('172.17.0.2', 2375)>. ",
    "darkn3rd": "Should not there be a requirement for the correct urllib3 version to be installed when pip install docker-compose?\nThis problem still occurs (using Vagrant imageubuntu/trusty64):\n```\n$ docker version\nClient:\n Version:      17.05.0-ce\n API version:  1.29\n Go version:   go1.7.5\n Git commit:   89658be\n Built:        Thu May  4 22:06:06 2017\n OS/Arch:      linux/amd64\nServer:\n Version:      17.05.0-ce\n API version:  1.29 (minimum version 1.12)\n Go version:   go1.7.5\n Git commit:   89658be\n Built:        Thu May  4 22:06:06 2017\n OS/Arch:      linux/amd64\n Experimental: false\n$ python --version\nPython 2.7.6\n$ pip freeze | grep -E 'urllib3|requests|docker'\ndocker==2.3.0\ndocker-compose==1.13.0\ndocker-pycreds==0.2.1\ndockerpty==0.4.1\nrequests==2.2.1\nurllib3==1.7.1\n```. I tried to upgrade, so removed it and tried to install.  Same result.\nUsing --no-use--wheel yields this:\n$ pip install --no-use-wheel docker.py\nDEPRECATION: --no-use-wheel is deprecated and will be removed in the future.  Please use --no-binary :all: instead.\nCollecting docker.py\n  Could not find a version that satisfies the requirement docker.py (from versions: )\nNo matching distribution found for docker.py\n. I have found that this also reproduces on Ubuntu 14.04 Trusty Tahr\n```\neasy_install pip\npip install docker.py\n...\nException:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/pip-8.1.2-py2.7.egg/pip/basecommand.py\", line 215, in main\n    status = self.run(options, args)\n  File \"/usr/local/lib/python2.7/dist-packages/pip-8.1.2-py2.7.egg/pip/commands/install.py\", line 317, in run\n    prefix=options.prefix_path,\n  File \"/usr/local/lib/python2.7/dist-packages/pip-8.1.2-py2.7.egg/pip/req/req_set.py\", line 742, in install\n    **kwargs\n  File \"/usr/local/lib/python2.7/dist-packages/pip-8.1.2-py2.7.egg/pip/req/req_install.py\", line 831, in install\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n  File \"/usr/local/lib/python2.7/dist-packages/pip-8.1.2-py2.7.egg/pip/req/req_install.py\", line 1032, in move_wheel_files\n    isolated=self.isolated,\n  File \"/usr/local/lib/python2.7/dist-packages/pip-8.1.2-py2.7.egg/pip/wheel.py\", line 348, in move_wheel_files\n    assert info_dir, \"%s .dist-info directory not found\" % req\nAssertionError: docker.py .dist-info directory not found\n...\npip install --no-use-wheel docker.py\nDEPRECATION: --no-use-wheel is deprecated and will be removed in the future.  Please use --no-binary :all: instead.\nCollecting docker.py\n  Could not find a version that satisfies the requirement docker.py (from versions: )\nNo matching distribution found for docker.py\n```\nFull Results including traceback: https://gist.github.com/darkn3rd/3528b5fd01687406ae5953a297780a9a\nAlso: \n```\npip freeze | grep docker-py && python --version && docker version\ndocker-py==1.10.3\ndocker-pycreds==0.2.1\nPython 2.7.6\nClient:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 05:22:43 2016\n OS/Arch:      linux/amd64\nServer:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 05:22:43 2016\n OS/Arch:      linux/amd64\n```\n. LOL. Duh. My bad.\n. ",
    "boltronics": "@darkn3rd Agreed. Sounds like this issue could be closed with a single line of code in setup.py.. ",
    "muisit": "After installation of docker-compose on Ubuntu 16.04, I got this attribute error on any interaction with docker-compose.\nFix for me was to update both urllib3 and the request module:\nsudo -H pip install -U urllib3\nsudo -H pip install -U requests\nOnly updating the urllib3 still left the 'unbundling' problem in the request module, which includes an older version of urllib3. ",
    "zealotous": "I had the same issue.\nIt was caused by ${site-packages}/requests/packages/urllib3 folder \nremained after upgrading from requests<2\n```\ninstall the most resent requests version\npip install --upgrade requests\ncheck if we have the problem\npython -c \"import requests.packages.urllib3 as u; u.connection\"\nprint rm command\npython -c \"import sys\nfrom os.path import exists, join, abspath\nfor p in sys.path:\n    module_path = abspath(join(p, 'requests'))\n    if exists(join(module_path, 'packages', 'urllib3')):\n        print('rm -rI \\'%s\\'' % module_path)\n\"\n```. ",
    "kevinfrommelt": "@dnephin I added a couple unit tests.  I think integration tests are going to be a bit trickier, as I'm not really sure how to replicate it without importing gevent and doing some patching.  Let me know what you think, I can try playing around with some integration tests if you think we need it.\n. ",
    "ubante": "Thanks for the quick review.\n. ",
    "danpalmer": "@shin- I don't think this is an issue with requests, it looks like docker-py assumes chunks will be at the JSON object boundary, and that something at the Docker API level has broken that previously-true assumption. Obviously there is no such guarantee from Requests, Docker (according to the docs), or HTTP. It looks like Docker is using \"almost\" line-delimited JSON, but with the difference that it sends CRLFs instead of just LFs.\nI think the best solution to this would be to continually receive chunks into a buffer, and then attempt to consume CRLF delimited JSON objects out of that buffer. docker-py should be able to handle receiving a chunk that contains multiple JSON objects, or a chunk that doesn't contain any valid JSON and must be concatenated with past or future blocks for the JSON to be valid.\n. Quick note, I don't think this is an enhancement (as I understand them), this is a bug fix for me.\n. After some investigation about ways to solve this, it looks like _stream_raw_result and _stream_raw_result_old would be the correct way of doing this. I don't have the context for why these aren't being used here, but it looks like they solve this problem exactly. Much better to delegate the chunking to Requests I think.\n. Any update on this being merged? The build looks green (although Janky hasn't updated correctly).\n. ",
    "stromnet": "Hi,\nseems like this issue describes a problem I'm seeing with docker-compose.\nWhen doing docker restart <container>, where the container is started by docker-compose which is tailing the log, the docker-compose CLI sometimes emits the following:\nCONTAINER exited with code 143\nException in thread Thread-5:\nTraceback (most recent call last):\n  File \"threading.py\", line 810, in __bootstrap_inner\n  File \"threading.py\", line 763, in run\n  File \"compose/cli/log_printer.py\", line 190, in watch_events\n  File \"compose/project.py\", line 343, in events\n  File \"site-packages/docker/client.py\", line 253, in _stream_helper\n  File \"json/__init__.py\", line 338, in loads\n  File \"json/decoder.py\", line 369, in decode\nValueError: Extra data: line 2 column 1 - line 3 column 1 (char 689 - 1382)\nThen it fails to output any more log data that container (which is now running, and also outputs proper logs if checked with docker logs).\nFrom the trace of it, I guess this is not an actual docker-compose issue but just wanted to report it.\nEnv: \n```\nMac OS 10.11.3\nDocker For Mac Version 1.12.0-a (build: 11213, ad6ab836187e4111082447b7c0a6a74d01929a5c)\ndocker-compose version 1.8.0, build f3628c7\nDocker version 1.12.0, build 8eab29e\n```\n. ",
    "robbiet480": "This is still an issue for me as well (I'm having this in Ansible's docker_image module and got here through ansible/ansible-modules-core#4116 and then #4116).\nThe fix proposed in #1081 works perfectly for me. I suggest that gets merged.\n. This PR fixes the issue for me. Can it be merged soon?\n. ",
    "pmarques": "I also would like to see this released, I'm having some trouble right now to work this around.... \ud83d\udc4d \n. Thanks @shin- there is any expected date to be released?\n. Ok, Thanks!\n. ",
    "crisron": "Hi shin, I am still seeing this issue with docker-py 1.10.6. What can I do about it?. ",
    "erhudy": "I am also still seeing this issue with 1.10.6. I am able to reproduce it reliably using Docker for Mac, both 1.13.0 and the 1.12.x version I had installed prior. It does not seem to occur with Docker 1.12.1 that I have installed on an Ubuntu OpenStack instance.. Just updated the package I'm working on to use the new APIClient in 2.0.0+ and am still seeing multiple JSON objects concatenated together with newlines using 2.1.0-dev, which I checked out after my comment above:\nSTRIPPED RESPONSE: '{\"status\":\"Pulling from library/ubuntu\",\"id\":\"16.04\"}\n{\"status\":\"Digest: sha256:71cd81252a3563a03ad8daee81047b62ab5d892ebbfbf71cf53415f29c130950\"}\n{\"status\":\"Status: Image is up to date for ubuntu:16.04\"}'\nERROR:kolla.image.build.base:Unknown error when building\nTraceback (most recent call last):\n  File \"/Users/erhudy/Documents/OpenStack/kolla/kolla/image/build.py\", line 428, in builder\n    stream = json.loads(response.strip().decode('utf-8'))\n  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/__init__.py\", line 339, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/decoder.py\", line 367, in decode\n    raise ValueError(errmsg(\"Extra data\", s, end, len(s)))\nValueError: Extra data: line 2 column 1 - line 3 column 58 (char 55 - 206). Okay, thanks for the clarification.. ",
    "lcarva": "Another observation is that import_image_from_url returns a string of multiple JSON objects which is not straight forward to parse:\n{\"status\":\"Importing\",\"p\nrogressDetail\":{\"current\":70080448,\"total\":72531870},\"progress\":\"[================================================\\u003e  ] 70.08 MB/72.53 MB\"}{\"status\":\"Importing\",\"progressDetail\":{\"current\":70605952,\"total\":72531870},\"progress\":\"[=====\n===========================================\\u003e  ] 70.61 MB/72.53 MB\"}{\"status\":\"Importing\",\"progressDetail\":{\"current\":71131456,\"total\":72531870},\"progress\":\"[=================================================\\u003e ] 71.13 MB/72.53 MB\"\n}{\"status\":\"Importing\",\"progressDetail\":{\"current\":71656960,\"total\":72531870},\"progress\":\"[=================================================\\u003e ] 71.66 MB/72.53 MB\"}{\"status\":\"Importing\",\"progressDetail\":{\"current\":72185040,\"total\":725\n31870},\"progress\":\"[=================================================\\u003e ] 72.19 MB/72.53 MB\"}{\"status\":\"Importing\",\"progressDetail\":{\"current\":72531870,\"total\":72531870},\"progress\":\"[==================================================\\\nu003e] 72.53 MB/72.53 MB\"}{\"status\":\"9f84bd8ef2f1bd2bda6c4e6e48dea4f726ed76269f87beca5bba9cd2ea69157f\"}\nSeems that build call handles this scenario by searching output and only returning built image ID: https://github.com/docker/docker-py/blob/81edb398ebf7ce5c7ef14aa0739de5329589aabe/docker/api/build.py#L106\n. ",
    "jamesongithub": "Hi @shin-,\nCan you explain how login works? With respect to pulling/pushing after it's called. Does the client maintain some state and you have to reuse it?\nTrying to use docker-compose via cli (since it's not yet in docker-py) after calling this and doesn't seem to authenticate when pulling.\n. will this be in next release on pypi?. when will be merged?. @swagh1983 how does it work for registry and tag in it? whats the format?. ok working now. seems like container object should be ref. 18:37:17     raise docker.errors.DockerException(error.get('message'))\n18:37:17 docker.errors.DockerException: COPY failed: stat /mnt/docker/17.06.0~ce-0/tmp/docker-builder014221688/backend/package.json: no such file or directory\n18:37:17 Build step 'Execute shell' marked build as failure\npartial .dockerignore\n```\n/.*/\n/coverage/\n/logs/\n/node_modules/\n**/test/\n/.*\n/.crt\n/*.md\n/.log\n/*.pyc\n/keys.json\n.py\n.sh\njscs\n```. @shin-  occurs in all versions past 2.0.2. \ndoes looks like the issue @mikz raised.\n. ",
    "mbroomfield-r7": "why couldn't docker-py's login function simply have a persist parameter, which, on success, writes the authentication out to `$HOME/.docker/config.json? Seems simple enough.\n. I definitely just have a way old version of this library installed. Apologies. . ",
    "tomaszzielinski": "Here is the link to the docs: \nhttps://docs.docker.com/engine/reference/api/docker_remote_api_v1.24/#/retrieving-information-about-files-and-folders-in-a-container.\nWhat's interesting is that Docker Remote API page doesn't mention that HEAD works for /containers/(id)/archive.. ",
    "yunzhu-li": "Request review on this change \ud83d\ude00 ~! @dnephin @shin-\n. ",
    "HusterWan": "sorry, my bad! but in the docker-py document,  it does not mention the cgroup_parent param in host-config definition. \n. ",
    "waynr": "@shin- I agree that this should be documented, I'd be happy to submit a PR if you point me in the right direction.. @shin- is there any particular reason that the cgroup_parent option isn't available for the \"high-level API\" Container object (https://docker-py.readthedocs.io/en/stable/api.html#module-docker.api.container)? I am now finding that in order to specify a --cgroup-parent on the underlying container I have to rewrite code I had already written with Container in the style of what you show in your code snippet above.\nAside from the obvious fact that it's annoying and counterintuitive to have to rewrite my code to take advantage of features not available on the recommended high level API, the main reason I find this objectionable is that the ApiClient.create_containers and docker.containers.run have misleadingly similar lists of parameters that take different values--in the former you are required to define a list of volumes in create_containers as well as a more complex data structure in a separate call to create_host_config:\n```python\n        binds = {\n            m2_repo_dir: {\n                'bind': m2_repo_dir,\n                'mode': 'rw',\n            },\n            runhistorian_dir: {\n                'bind': runhistorian_container_dir,\n                'mode': 'rw',\n            }\n        }\n    if os.path.isfile('/.dockerenv'):\n        host_config = self.api_client.create_host_config(\n            cgroup_parent=self.__get_current_cgroup(),\n            binds=binds,\n        )\n    else:\n        host_config = self.api_client.create_host_config(\n            binds=binds,\n        )\n\n    container_id = self.api_client.create_container(\n        'jrh:latest', mvn_cmd,\n        host_config=host_config,\n        detach=True,\n        environment={\n            'JAVA_HOME': java_home,\n        },\n        volumes=[volume['bind'] for volume in list(binds.values())]\n    )\n    self.__jenkins_container = self.client.containers.get(container_id)\n    self.__jenkins_container.start()\n    self.__jenkins_container.exec_run(\n        mvn_cmd,\n        detach=True,\n    )\n\n```\nvs\n```python\n        cgroup_parent = None\n        if os.path.isfile('/.dockerenv'):\n            cgroup_parent = self.__get_current_cgroup(),\n    self.__jenkins_container = self.client.containers.run(\n        'jrh:latest', mvn_cmd,\n        detach=True,\n        cgroup_parent=cgroup_parent,\n        environment={\n            'JAVA_HOME': java_home,\n        },\n        volumes={\n            m2_repo_dir: m2_repo_dir,\n            runhistorian_dir: runhistorian_container_dir,\n        }\n    )\n\n```\nIf you're open to accepting a PR to add the cgroup_parent option to DockerClient.containers.run and willing to provide some guidance I'd be glad to contribute.. Nevermind my previous comment, after browsing through the source working on a different problem I see that there is an undocumented cgroup_parent parameter available for docker.models.containers.create_container.. I've verified the same problem is showing up when I pin docker==2.2.0 in my test requirements file.. @shin- hmm, I don't see Container.reload() documented in https://docker-py.readthedocs.io/en/stable/containers.html\nAlso, if the values are cached--and it's not clear to me what the value of a cached status is since the reason ~~someone would~~ I want to check it more than once is to see if it changed since the last time--that fact should probably be documented.. @shin- sure, that makes sense--thanks for updating the docs. I wonder if it would make sense to create a subclass of Container for the cache (or non-cache) use case. (just thinking out loud). ",
    "jgiannuzzi": "This should also fix #1040 and #1041.\n. I am preparing a pull request for docker-compose to add support for creating internal networks.\nIn order to successfully run the test suite, I need to make it depend on the latest git version of docker-py.\nWhat would be the best way to submit that pull request? Depend on docker-py==1.9.0, even though it has not yet been released?\n. Thanks @shin-!\nSubmitted PR https://github.com/docker/compose/pull/3488\n. Creating an internal network requires API version 1.22\n. Checking whether a network is internal requires API version 1.23\n. ",
    "robnagler": "Keep to the Docker API naming makes sense. Thanks for the answer.\n. ",
    "tsturzl": "I'm having a similar issue pushing images when using a remote docker api HTTPConnectionPool(host='*.*.*.*', port=4243): Read timed out.(host redacted). It's almost as if the timeout is timing out if the push takes longer than the set timeout.. ",
    "WheresWardy": "Is there any update on this?\n. ",
    "wukai-gd": "@shin- ok, it works for me. \n. ",
    "yoyoidea": "It didn't have the trailing slash in my base_url . You can direct request \nhttp://10.9.24.32:6732/v1.22/containers/create,  it will return 404, you can try it.\nbut request http://10.9.24.32:6732/containers/create, It works. \nMaybe you can remove the version flag?\n. This question already solve\uff0c I made a mistake.  Thx for your help.\n. ",
    "BlueShells": "it works ,but also need all=True\n. ",
    "farassadek": "Nevermind guys I found the issue. \nIt is a config option in dockerswarm\n. ",
    "tescalada": "I was also running into issue #1059. This fixes it for me but does not seem like a good solution.\nI was also unable to update the unit tests to show the bug. For some reason it works fine in testing.\nHere is the test I was using:\n```\nfrom io import BytesIO\nfrom docker import Client\ndockerfile = '''\nFROM alpine:latest\nRUN apk add --update python-dev\nCMD [\"/bin/sh\"]\n'''\nf = BytesIO(dockerfile.encode('utf-8'))\ncli = Client()\nresponse = cli.build(\n    fileobj=f,\n    rm=True,\n    tag='yourname/volume',\n    stream=True,\n    decode=True,\n    nocache=True,\n)\nfor line in response:\n    print line\n```\n. ",
    "pahaz": "I have the same problem and I think we must split chunks before decode. Otherwise, we will have the problem with the glued raw data.\nNow, I use similar patch in my code (Python 3.x only):\ndef _events(client, filters=None):\n    events = client.events(filters=filters)\n    for event in events:\n        raw_event = event = event.decode('utf-8').strip('\\n')\n        try:\n            if '\\n' in event:\n                # multiple events!\n                for x in event.split('\\n'):\n                    yield json.loads(x)\n            else:\n                yield json.loads(event)\n        except json.JSONDecodeError:\n            warn('bad event json: %r' % raw_event)\n. ",
    "ghostsquad": "@bfirsh - I saw your demo at DockerCon, pretty cool. I'd love to help carry the torch. I'm working on a hack for the Hackathon inspired by your work.\nhttps://github.com/ghostsquad/docker-py/tree/feature/1085-add-swarm-mgmt\n. ",
    "tuxity": "Any ETA for that awesome feature ?\nI saw the milestone 1.10.0 but don't know if it's a question of days, weeks or months \n. ",
    "FrenchBen": "I believe the dependency was in compose. Probably better use-case to have @aanand test it. \n. ",
    "duplessisaa": "Hi guys, same exact error here...\nHow do I solve it ?\n. ",
    "hrobertson": "Is there some record of design discussions regarding this that I can read?\n. ",
    "guillaumedsde": "this would be useful, I'm using docker-py for validation testing which involves pulling image from a docker registry :). ",
    "nballenger": "This would be useful. Right now I'm having to run a subprocess call to log out of an ECR registry, because otherwise my credential store is attempting to use an expired temporary auth token from the last push I did, and ignoring the newly acquired token I'm passing to docker_client.login, despite passing reauth=True. If logout is verboten from the python SDK, maybe it'd be good if the login method actually paid attention to reauth?. ",
    "bdecampe": "Never mind it's totally my bad I should read Client doc more carefully, I adjust my timeout\n. ",
    "vklonghml": "@kZk93 how did you resolve this problem, any details, thks.\n. ",
    "darxtrix": "Thanks !\n. Thanks for replying ! Looks promising :)\n. ",
    "mferon": "Hi,\nHere is a pull request about it : #1158 \n. Any news for this PR ?\n@mvdstam could you fix pep8 issues (lines too long) and sign your commit please ?\n. Thank you for your review ;)\nI updated the branch to fix what you said.\n. Why 4 for default value ?\nThe default value seems to be 0 :\n```\n$  docker update --restart on-failure:4 266515885ce7\n266515885ce7\n$  docker inspect --format '{{json .HostConfig.RestartPolicy}}' 266515885ce7\n{\"Name\":\"on-failure\",\"MaximumRetryCount\":4}\n$ docker update --restart on-failure 266515885ce7\n266515885ce7\n$ docker inspect --format '{{json .HostConfig.RestartPolicy}}' 266515885ce7\n{\"Name\":\"on-failure\",\"MaximumRetryCount\":0}\n$  docker update --restart on-failure:4 266515885ce7\n266515885ce7\n$  docker inspect --format '{{json .HostConfig.RestartPolicy}}' 266515885ce7\n{\"Name\":\"on-failure\",\"MaximumRetryCount\":4}\n$  docker update --restart always 266515885ce7\n266515885ce7\n$  docker inspect --format '{{json .HostConfig.RestartPolicy}}' 266515885ce7\n{\"Name\":\"always\",\"MaximumRetryCount\":0}\n```\n. ",
    "saraswat2385": "Hi @shin- \nI am getting following error\ndocker.errors.APIError: 500 Server Error: Internal Server Error (\"device or resource busy\") \nwhile starting container ....\nany idea ?????\n. @shin- Thanks For help \n. ",
    "srikalyan": "@GordonTheTurtle, it is done. Do you know what is the normal ETA in closing a PR?\n. @shin- ,Thank you for the quick review. I made the changes you have suggested. @GordonTheTurtle seems not be an intelligent bot to reply to my questions :p.\n. @shin-, added doc and squashed commit. Please let me know if you need anything else and thank you for helping me out really appreciate it.\n. Done.\n. ",
    "soulne4ny": "Just to note https://pypi.python.org/pypi/pathspec/.\nThere is evidence, that it handles ** as well \u2013 http://stackoverflow.com/questions/10048667/gitignore-style-fnmatch#answer-22090594. @lelit, I'm just wondering if pathspec differs from what is needed?\nIs it restricted to bring third-party dependencies to the library?. I haven't expected anything like ^ vs ! difference. Yes, then .gitignore and pathspec comply man 7 glob and Go\u2019s filepath.Match made it different.\nCruel world :(\n. ",
    "lelit": "Since a couple of days I'm using the following monkey patch that apparently implements the same\nsemantic of Docker's .dockerignore. I don't know whether it's good/portable enough, but it\nworks quite well and it even comes with unittests.\nShould you judge it positively, I'm willing to properly adapt it and open a PR.\n```\n!/usr/bin/env python3\n\n\"\"\"\nThis is quick&dirty attempt to workaround the non-conformant handling of\n.dockerignore files of docker-py.\nPython's fnmatch() uses a slightly different syntax, so it can't be used.\nThis tries to implement a behaviour closer to Docker CLI by translating the\nfile patterns (see http://golang.org/pkg/path/filepath#Match) into regexps.\n\"\"\"\nimport os, re, unittest\nfrom docker.utils import utils\ndef pattern_to_regexp(pattern):\n    i = 0\n    n = len(pattern)\nregexp = ''\n\n# Simplify by explicitly anchoring the pattern at root: accordingly to the\n# docker's refmanual, \u201c/foo/bar and foo/bar both exclude a file or\n# directory named bar in the foo subdirectory\u201d\n\nif pattern[0] == '!':\n    if pattern[1] != '/':\n        regexp += '/'\nelif pattern[0] != '/':\n    regexp += '/'\n\nwhile i < n:\n    c = pattern[i]\n    i += 1\n    if c == '\\\\':\n        if i < n:\n            regexp += re.escape(pattern[i])\n            i += 1\n        else:\n            raise ValueError(\"Bad escape at index %d: %s\" % (i, pattern))\n    elif c == '*':\n        if i < n and pattern[i] == '*':\n            regexp = '.*'\n            i += 1\n        else:\n            regexp = regexp + '[^/]*'\n    elif c == '?':\n        regexp = regexp + '[^/]'\n    elif c == '[':\n        regexp += c\n        openpos = i-1\n        while i < n:\n            c = pattern[i]\n            i += 1\n            regexp += c\n            if c == ']':\n                break\n        else:\n            raise ValueError(\"Unclosed '[' at index %d: %s\"\n                             % (openpos, pattern))\n    else:\n        regexp = regexp + re.escape(c)\n\nreturn re.compile('^' + regexp + '$', re.DOTALL)\n\ndef dockerignore_patterns_as_regexps(patterns):\n    repatterns = []\n    for pattern in patterns:\n        pattern = pattern.strip()\n        if not pattern or pattern.startswith('#'):\n            continue\n        negated = False\n        if pattern.startswith('!'):\n            negated = True\n            pattern = pattern[1:]\n        repatterns.append((negated, pattern_to_regexp(pattern)))\n    return repatterns\ndef valid(path, patterns):\n    # Simplify by assuming all paths start with a slash,\n    # that is are absolute within the root\nif not path.startswith('/'):\n    path = '/' + path\n\nvalid = True\nfor negated, rx in patterns:\n    if negated:\n        if rx.match(path):\n            valid = True\n    else:\n        if rx.match(path):\n            valid = False\nreturn valid\n\ndef valid_paths(root, patterns, dockerfile=None,\n                _exists=os.path.exists,\n                _islink=os.path.islink,\n                _join=os.path.join,\n                _readlink=os.readlink,\n                _relpath=os.path.relpath,\n                _walk=os.walk):\n    repatterns = dockerignore_patterns_as_regexps(patterns)\nif dockerfile is None:\n    dockerfile = 'Dockerfile'\n\n# Be sure that the Dockerfile is always listed, even when\n# it's filtered out by the patterns\n\ndockerfile = _join(root, dockerfile)\nif _exists(dockerfile):\n    dockerfile = _relpath(dockerfile, root)\n    if not valid(dockerfile, repatterns):\n        yield dockerfile\n\nfor current_dir, subdirs, files in _walk(root):\n    current_dir = _relpath(current_dir, root)\n\n    if current_dir == '.':\n        current_dir = ''\n\n    subdirs[:] = [d for d in subdirs\n                  if valid(_join(current_dir, d), repatterns)]\n\n    if files or subdirs:\n        for file in files:\n            path = _join(current_dir, file)\n            if valid(path, repatterns):\n                yield path\n\n        # Since os.walk won't follow (by default) symlinks,\n        # loop over directories accepting symlinks to valid paths\n\n        for dir in subdirs:\n            path = _join(current_dir, dir)\n            fpath = _join(root, path)\n            if _islink(fpath) and valid(path, repatterns):\n                linkto = _relpath(_readlink(fpath), root)\n                if linkto == '.':\n                    linkto = ''\n                if valid(linkto, repatterns):\n                    yield path\n    else:\n        # The directory is empty\n        if valid(current_dir, repatterns):\n             yield current_dir\n\nclass TestPatternToRegexp(unittest.TestCase):\n    cases = (\n        ('A',\n         ('/A',),\n         ('/B',)),\n        ('/A',\n         ('/A',),\n         ('/B',)),\n        ('?/.c',\n         ('/a/x.c', '/x/abcz.c', '/a/.c'),\n         ('/.c', '/ab/.c', '/a/.d', '/a//.d')),\n        ('[^a-c]/?.c',\n         ('/d/a.c', '/A/..c'),\n         ('/a/b.c', '/xy/1.c')),\n        ('*/.git',\n         ('/.git', '/A/.git'),\n         ('/x.git', '/a/b.git')),\n    )\ndef test_pattern_to_regexp(self):\n    for pattern, should_match, should_not_match in self.cases:\n        rep = pattern_to_regexp(pattern)\n        for sm in should_match:\n            self.assertTrue(rep.match(sm),\n                            \"%r should match %r\" % (pattern, sm))\n        for snm in should_not_match:\n            self.assertFalse(rep.match(snm),\n                             \"%r should match %r\" % (pattern, snm))\n\nerrors = (\n    ('A\\\\', r'Bad escape at index 2:'),\n    ('[A-B', r\"Unclosed '\\[' at index 0:\"),\n)\n\ndef test_bad_patterns(self):\n    for pattern, msgrx in self.errors:\n        with self.assertRaisesRegex(ValueError, msgrx):\n            pattern_to_regexp(pattern)\n\nclass TestDockerignorePatternsAsRegexps(unittest.TestCase):\n    cases = (\n        (('A', '!B'),\n         ((False, r'^/A$'),\n          (True, r'^/B$'))),\n    )\ndef test_dockerignore_patterns_as_regexps(self):\n    for dips, erxs in self.cases:\n        rxs = dockerignore_patterns_as_regexps(dips)\n        self.assertEqual(rxs, [(flag, re.compile(rx, re.DOTALL))\n                               for flag, rx in erxs])\n\nclass TestValid(unittest.TestCase):\n    cases = (\n        ('/app/.tox',\n         ('', '!app', 'app/.tox'),\n         False),\n        ('/app/config.tox',\n         ('', '!app', 'app/.tox'),\n         True),\n        ('/foo',\n         ('', '!app', 'app/.tox'),\n         False),\n        ('/foo/bar/.git',\n         ('!foo', '*/.git'),\n         False),\n    )\ndef test_valid(self):\n    for path, dips, is_valid in self.cases:\n        rxs = dockerignore_patterns_as_regexps(dips)\n        self.assertEqual(valid(path, rxs), is_valid,\n                         \"%r against %r\" % (path, dips))\n\nclass TestValidPaths(unittest.TestCase):\n    mock_fs = (\n        ('/', (('a', 'b', 'c', 'd', '.git'), ('a.txt', 'b.py'))),\n        ('/a', (('.tox',), ('readme.txt',))),\n        ('/a/.tox', (('dist', 'log', 'py35'), ())),\n        ('/a/.tox/dist', ((), ())),\n        ('/a/.tox/log', ((), ('tox-0.log'))),\n        ('/a/.tox/py35', (('bin',), ('pip-selfcheck.json',))),\n        ('/a/.tox/py35/bin', ((), ('pip',))),\n        ('/b', ((), ('foo.c',))),\n        ('/c', ((), ())),\n        ('/d', (('symlinkto:../c',), ())),\n        ('/.git', (('objects',), ('HEAD',))),\n        ('/.git/objects', (('pack',), ())),\n        ('/.git/objects/pack', ((), ('pack-1.idx', 'pack-1.pack'))),\n    )\ndef mock_walk(self, root):\n    mock_fs = dict(self.mock_fs)\n    queue = ['/']\n    while queue:\n        dir = queue.pop(0)\n        dirs = list(mock_fs[dir][0])\n        files = list(mock_fs[dir][1])\n        yield dir, dirs, files\n        queue = [os.path.join(dir, d) for d in dirs\n                 if not self.mock_islink(d)] + queue\n\ndef test_mock_walk(self):\n    self.assertEqual(list(self.mock_walk('/')),\n                     [(dir, list(dirs), list(files))\n                      for dir, (dirs, files) in self.mock_fs])\n\ndef mock_islink(self, path):\n    return 'symlinkto:' in path\n\ndef mock_readlink(self, path):\n    assert self.mock_islink(path)\n    head, _, tail = path.partition('/symlinkto:')\n    return os.path.normpath(os.path.join(head, tail))\n\ncases = (\n    (('*', '!a', '**/.git', '**/.tox'),\n     ('a/readme.txt',)),\n    (('**/.git', '**/.tox'),\n     ('a.txt', 'b.py', 'a/readme.txt', 'b/foo.c', 'c', 'd/symlinkto:../c')),\n    (('*', '![c-d]'),\n     ('c', 'd/symlinkto:../c')),\n    (('*', '!d'),\n     ()),\n)\n\ndef test_valid_paths(self):\n    for dips, expected in self.cases:\n        self.assertEqual(list(valid_paths('/', dips,\n                                          _walk=self.mock_walk,\n                                          _islink=self.mock_islink,\n                                          _readlink=self.mock_readlink)),\n                         list(expected))\n\ndef show_context():\n    if os.path.exists('.dockerignore'):\n        with open('.dockerignore', 'r') as f:\n            patterns = f.read().splitlines()\n    else:\n        patterns = []\n    for path in valid_paths(os.getcwd(), patterns):\n        print(path)\ndef main():\n    import sys\n    if len(sys.argv) > 1 and sys.argv[1] == 'test':\n        unittest.main(argv=sys.argv[:1], verbosity=2)\n    elif len(sys.argv) > 1 and sys.argv[1] == 'context':\n        show_context()\n    else:\n        utils.exclude_paths = valid_paths\n    from compose.cli.main import main\n\n    main()\n\nif name == 'main':\n    main()\n``\n. From a quick inspection, it seems that it does not comply with the docker specs, **as is**. For example AFAICT its[a-z][wants](https://github.com/cpburnz/python-path-specification/blob/master/pathspec/patterns/gitwildmatch.py#L204) a _bang_!instead of a _caret_^` to negate the pattern, as docker does (actually the underlying Go\u2019s filepath.Match).\nIt surely could be extended, implementing docker's peculiarities into particular patterns.dockerignorematch. Maybe it's the way to go.. Sorry to bother you again, but it seems that the solution introduced by #1455 is wrong, in particular because the utils.build.exclude_paths() function does not respect the order of items in the .dockerignore file (it separates the patterns in two, include_patterns and exclude_patterns using a set difference): .dockerignore doc explicitly state that \u201cthe placement of ! exception rules influences the behavior: the last line of the .dockerignore that matches a particular file determines whether it is included or excluded\u201d and that implies the patterns must be considered in the original order.\nConsider the following script:\n```python\nimport os\nimport shutil\nfrom docker.utils.build import exclude_paths\ndef create_file(fpath, content='foo'):\n    parent = os.path.split(fpath)[0]\n    if not os.path.isdir(parent):\n        os.makedirs(parent)\n    with open(fpath, 'w') as f:\n        f.write(content)\ndef read_dockerignore(basedir):\n    dockerignore = os.path.join(basedir, '.dockerignore')\n    exclude = None\n    if os.path.exists(dockerignore):\n        with open(dockerignore, 'r') as f:\n            exclude = list(filter(bool, f.read().splitlines()))\n    return exclude or []\ndef create_hierarchy(basedir):\n    create_file(os.path.join(basedir, 'Dockerfile'))\n    create_file(os.path.join(basedir, 'src', 'main.py'))\n    create_file(os.path.join(basedir, 'src', '.tox', 'foo', 'bar'))\n    create_file(os.path.join(basedir, '.dockerignore'),\n                '\\n'.join([\"\",\n                           \".dockerignore\",\n                           \"!src\",\n                           \"*/.tox\"]))\nhere = os.getcwd()\ntry:\n    basedir = os.path.join(here, 'test')\n    create_hierarchy(basedir)\n    for p in sorted(exclude_paths(basedir, read_dockerignore(basedir))):\n        print(p)\nfinally:\n    shutil.rmtree(basedir)\n```\nIt outputs the following:\n.dockerignore\nDockerfile\nsrc\nsrc/.tox\nsrc/.tox/foo\nsrc/.tox/foo/bar\nsrc/main.py\nwhere you can see it includes the src/.tox subdirectory even if it's explicitly listed as an ignore pattern.\nMy valid_paths() function (see the script above) emits:\nDockerfile\nsrc/main.py\nAm I doing something wrong, or is the issue still present?. I hit this too, in a similar scenario, except that the CI pipeline runs on top of Python 3.6 normal image, not Alpine.. ",
    "g0t4": "I'm using docker-compose version 1.13.0, build 1719ceb8 on both a Mac and Windows machine (Linux containers mode), I have a .dockerignore with two entires that are problematic:\n**/bin/\n**/obj/\nAt the root of this particular build context, I have a bin and an obj folder. docker build excludes everything just fine, docker-compose build works on the Mac to exclude these folders but not on my Windows machine. I assume this is related to the issue above but I am uncertain because the milestone for this issue is tagged as 2.1.0 and I know from testing manually that docker-py's exclude_path has the glitch on windows up to version 2.2.1 but not in 2.3.0. I would assume the milestone is just outdated here but I'm also perplexed as to why the issue doesn't exist on my Mac then, though I'm wondering if somehow the code prior to 2.3.0 somehow worked on non-Windows platforms?\nFurther compounding my uncertainty, in the release notes for docker-compose 1.11.2 is a note: \n\nFixed an issue where recursive wildcard patterns ** were not being\nrecognized in .dockerignore files.\n- https://github.com/docker/compose/releases/tag/1.11.2\n\nThis seems to suggest the issue was fixed a while ago. Or at least partially so. \nAnyways is this problem related or is this a different problem? If different, should I open a new issue here on the docker-py repo or on the docker-compose repo to make sure it is resolved and people are aware that it is fixed in a future release of docker-compose? \nUpdate: I've confirmed the issue on my Windows machine to be with both Windows and Linux Containers and that makes intuitive sense as this is a client concern. . @shin- backslashes don't work with docker-compose build in my use case, backslashes work with docker build - FYI I tested with Docker for Windows in Linux Container mode. Interestingly, I have other excludes in my .dockerignore that do work with docker-compose + Docker for Windows.\nworking:\n```\ndoes exclude Dockerfile in root of build context\n*/Dockerfile \ndoes exclude .dockerignore in root of build context\n*/.dockerignore\nnot working:\ndoesn't exclude bin and obj in root of build context, does exclude bin and obj in nested folders\n/bin/ \n/obj/\ndoesn't exclude docker-compose.yml in root of build context\n*/docker-compose.yml \n```. ",
    "jmichalicek": "I'd love to.  I looked into it a bit when writing this, but didn't immediately see a clean way to do it with where the current mocks occur.  Let me take another look at it today with my head clear and brain fresh, though.  I probably just missed something obvious.\n. There we go, pretty obvious coming at it fresh.  Test cases are in place.\n. Sorry about that, that was just keeping the name exact with the method's leading underscore.  I'll change it first thing in the morning, rebase, and push it up.\n. ",
    "Faylixe": "I removed the sentence. Just let the buildargs parameter description in the list.\n. ",
    "dgageot": "@bfirsh sure...\n. @bfirsh should be better now\n. Closing in favor of https://github.com/docker/docker-py/pull/1126\n. ",
    "AkihiroSuda": "Seems need to apply some fixes:\ne.g.\nhttps://github.com/docker/engine-api/commit/14c70cc34bfee8f1335137b77cc8d496d0169bf8\nhttps://github.com/docker/engine-api/commit/2d2ff7c7d6ee8bc7408e7c2440a4da1cf309c198\n. Thanks a lot for working on this, could you also support specifying username@ ?\nhttps://github.com/docker/cli/blob/master/cli/connhelper/ssh/ssh_test.go. > The current known blocker for this is we need a Python implementation of the fssync gRPC service (a service which runs on the session but on the client side) and that service is distinctly non-trivial to implement.\nIn addition to fssync, there are auth, content, secrets, and sshforward client-side services.\nAs @mipearson  commented in https://github.com/moby/buildkit/issues/685 , docker-py should just invoke docker build CLI.\n. Do we need a helper binary other than existing docker CLI?. Can we refuse parsed.password, parsed.path, parsed.fragment, parsed.query explicitly?\n. ",
    "atakada": "Is there anyone still working on it? I found some parameter names to be updated, but I'll be happy if there are any branches I can try.\nhttps://github.com/atakada/docker-py/tree/service-support-fix\n. Thanks for comments&updates! Unfortunately I'm working for other stuff, but will come back.\n. ",
    "mattarchie": "I see you've submitted a pull request which seems to be just about ready for merging. I'm glad to see that this will be supported soon.\nIn the meantime (or if someone needs to use an older version), I got around this issue by using update_container, which does support both of those. My code uses the same function for creating and updating a container, so if my create flag is set, then I create the container before updating it, otherwise, I update the existing container. Depending on your code, this approach could actually offer an improvement by allowing the same code to be reused for both create and update cases.\n. Note: Sorry for the length of my issue and this comment, but I wanted to give as much detail as possible. Any help resolving this would be much appreciated - maybe I'm just missing something... In the meantime, I will just build the networking_config dictionary myself given I now know the format.\nOkay, I just realized that this doesn't work either. I'm not sure if it's an issue with passing things in as a dictionary or with the way the kwargs are passed onto the utils.create_endpoint_config method, but I'm finding that the ipv4_address is showing up as a link, rather than in the IPAMConfig and then docker is auto-assigning an IPAddress within the network range.\nI built a test functions to see how the _args, *_kwargs thing works and it seems like the way you have args and kwargs should work... And I now see why I am getting the ip address as a link using None,{'ipv4_address':'10.10.10.10'} given it's being accepted as an arg instead. I tried getting around this by passing in an extra None arg prior to the dict as a work around, but this gives an error on the number of inputs: TypeError: create_endpoint_config() takes at most 3 arguments (4 given).\nGiven my tests, I'm very confused why I'm getting the error from container.create_endpoint_config for passing in ipv4_address='10.10.10.10' as a kwarg.\nTest Function\ndef func(*args,**kwargs):\n     print \"args:\", args\n     print \"kwargs:\", kwargs\n     if (not args or args[0] is None) and not 'arg' in kwargs:\n             kwargs['arg'] = 0\n     elif 'arg' in kwargs:\n             return\n     elif args[0]==2:\n             return\n     else:\n             argl = list(args)\n             argl[0] += 1\n             args = tuple(argl)\n     func(*args,**kwargs)\nInput and Output from Tests\nIn: func(None,key='val')\nOut:\nargs: (None,)\nkwargs: {'key': 'val'}\nargs: (None,)\nkwargs: {'key': 'val', 'arg': 0}\nIn: func(0,key='val')\nOut:\nargs: (0,)\nkwargs: {'key': 'val'}\nargs: (1,)\nkwargs: {'key': 'val'}\nargs: (2,)\nkwargs: {'key': 'val'}\nIn: func(key='val')\nOut:\nargs: ()\nkwargs: {'key': 'val'}\nargs: ()\nkwargs: {'key': 'val', 'arg': 0}\nIn: func(0,{'key':'val'})\nOut:\nargs: (0, {'key': 'val'})\nkwargs: {}\nargs: (1, {'key': 'val'})\nkwargs: {}\nargs: (2, {'key': 'val'})\nkwargs: {}\n. oh... I did make sure to do a pip install -U docker, but I never even thought that the \"latest\" version of the docs would point to a release candidate version. Is there anyway to add a \"dev\" or \"rc\" version to the docs and keep \"latest\" as whichever the newest release is? I think that would definitely be helpful going forward.\nI am currently using 'docker_macvlan': { 'IPAMConfig': {'IPv4Address': '10.10.10.14'} } for the endpoint_config dict and that is working. But if the IPAMConfig layer isn't needed, that would definitely make it cleaner, so I'll take a look at that.\nThanks for the response @shin- !\n. Oh, I see what you mean. That makes sense - stable vs. latest. I guess I'd just assumed I would the links I followed would give me the stable version and that there would be some kind of banner possibly to warn if you were on latest, just in case.\nThank you for the assistance, I'll close this ticket now. Looking forward to the new version release.\n. ",
    "EtienneDufresne": "\ud83d\udc4d \n. ",
    "Knetic": "Rebased to handle merge conflict\n. Is there a date when this might get merged? Not sure if there's like a \"merge day\" where PRs get pulled in, and I should have this rebased and ready to go. I don't remember to check the rebase status that often.\n. ",
    "kmala": "@aanand can you look into this?\n. I had added tests.Can you look into this again\n. @dnephin rebased\n. in which release can i expect this to be in?\n. Thanks!! Any tentative date for the release?\n. ",
    "himslm01": "I found this issue and worked around it in my code - then I searched github and found this issue.\nI do this in my code, as a work around. I don't know how brittle it is, but it's working for now.\n```\nfrom docker import Client\nimport json\ncli = Client(base_url='unix://var/run/docker.sock')\ngenerator = cli.build(..., stream=True, decode=False, ...)\nfor block in generator:\nprint 'block::' + block\nfor line in block.splitlines():\n\nprint 'line::' + line\n    try:\n        data = json.loads(line)\n        ...\n\n```\n. ",
    "jtakkala": "@aanand fixed and squashed. The Jenkins build is failing on disk space, but I ran unit-tests locally and all passed.\n. Ok, fixed. Hope it's good now!\n\nOn Aug 1, 2016, at 20:59, Joffrey F notifications@github.com wrote:\nThanks! I have just one request regarding the addition to the Client.start method. Rest is LGTM.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "gtseres": "@shin- would you need help with this? Except for the snippet above, some tests should be added and the docs updated, right? If so, I can give this a try.. You 'll have to use the low-level API for that:\npython\nimport docker\nclient = docker.APIClient(base_url='unix://var/run/docker.sock')\nport_data = client.inspect_container(<container_id>)['NetworkSettings']['Ports']. What version of the module are you using? Could you please share the following?\npython\ndocker.version\ndocker.__path__\nMy guess is that you have a version where from_env() is not implemented.. Could you please share some code to reproduce this?. Please also make sure that you attach the information mentioned here. I don't think that this is supported, but you could do something like the following to construct the dict of ports. Obviously, you can skip the host_port_range parts if you want the container ports to be mapped to random host ports. Hope that helps.\n```python\ncontainer_port_range = '2222-2232'\nhost_port_range = '3322-3332'\ncstart, cend = map(int, container_port_range.split('-'))\ncrange = range(cstart, cend)\nhstart, hend = map(int, host_port_range.split('-'))\nhrange = range(hstart, hend)\nassert crange == hrange\nports = {'{}/tcp'.format(c): h for c, h in zip(crange, hrange)}\n\n\n\nprint ports\n{'2231/tcp': 3331, '2225/tcp': 3325, '2222/tcp': 3322, '2223/tcp': 3323, '2229/tcp': 3329, '2226/tcp': 3326, '2227/tcp': 3327, '2224/tcp': 3324, '2230/tcp': 3330, '2228/tcp': 3328}\n```. @shin- Please let me know if you would find such a feature useful, and I 'll try to help.. @twllight Yeah, I guess so. Thanks for the finding.. \n\n\n",
    "thomas-melville": "Hi all,\nAny movement on this? This would be really useful to me.. ",
    "EFanZh": "I have implemented saving multiple images in PR #2050, which is based on PR #1767 but with test failures fixed.. ",
    "maguowei": "requests  v2.11.1 has released. please support it.\nhttp://docs.python-requests.org/en/latest/community/updates/#id1\n. ",
    "Dagur": "Sure. The Docker version is 1.7.1, build 786b29d/1.7.1  (the latest on CentOS 6.8).\nI was using docker-py 1.2.2 and got this error after I upgraded it to 1.9 (rolling back to 1.2.2 fixed the problem).\nI'm not super familiar with the code but I think this is the relevant bit:\n```\nhost_config = docker.utils.create_host_config(port_bindings={8082: ('0.0.0.0', 59000 + self.id)})\ncontainer_id = dc.create_container(\n    'company/product:v' + self.version.name,\n    ports=[8082],\n    name=self.container_name(),\n    environment=dict(\n        PRODUCT_ENV=settings.PRODUCT_ENV,\n        PRODUCT_TENANT_ID=self.id,\n        PRODUCT_TENANT=self.slug,\n        DATABASE_URL=db_url,\n        ELASTIC_URL=el_string,\n        PRODUCT_LANGUAGE=self.language.code\n    ),\n    host_config=host_config\n)\ndc.start(container_id['Id'])\n```\n. I did some tests again and now everything works fine with 1.9. I'm not sure what happened but I think it's safe to close this. Sorry for wasting your time\n. ",
    "antoineco": "@Dagur @shin- I would reopen that, I'm seeing the same thing happening using docker-py==1.9.0 and ansible==2.1.1.0 when running the following Ansible task:\nyaml\n- name: start containers\n  local_action:\n    module: docker\n    docker_url: tcp://host01.example.com:2375\n    docker_api_version: \"1.19\"\n    image: nginx:1.11\n    state: reloaded\n    pull: always\n    name: \"nginx-{{ item }}\"\n    restart_policy: always\n  with_sequence: start=1 end=5 format=\"%02d\"\nTASK [ocr : start containers] **********************************************\nfailed: [host01.example.com -> localhost] (item=01) =>\njson\n{ \"changed\": true,\n  \"failed\": true,\n  \"item\": \"01\",\n  \"msg\": \"Docker API Error: Cannot start container 2c3b5d84f6929aa2b9402b7d4711befd76a8c6433613543d9034493015821413: error locating network with name default: network default not found\"\n}\nThe host is running Docker 1.7.1 (API version 1.19)\nIt works if I roll back to `docker-py 1.3.1``\n\nThe code of the Ansible module is here: docker.py\n. ",
    "jayenashar": "i don't know what local_action is but adding net: bridge to my docker task worked for me!\n. ",
    "walkerlee": "Fixes #1165 \n. ",
    "tallandtree": "I agree partly, but there seems no reason for docker-py to upgrade requests to the latest version. As far as I can see (and test), it works with the installed (debian jessie default package) version of requests in /usr/lib/python2.7/dist-packages.\nBut you are correct that requirements of docker-py is minimal 2.5.2. So, maybe not everything works as it should be.\nMain issue is indeed with incompatibility of requests 2.11.1 and installed (default debian jessie) pip version. But then it would make sense to also upgrade pip.\nFor now, I've solved it by also upgrading pip.\n. ",
    "jess-lawrence-axomic": "Hey, as I mentioned before I was using the decoded token that aws ecr get-login was generating, but to no avail.\nI have however managed to get it working in a production environment. These are the commands that I'm using:\nThe Production system is all within an AWS VPC. In order to test locally I:\nssh -L 2375:10.0.4.192:2375 ....\nAnd set ADDR according to the env.\nimport os\nfrom config import config\nos.environ['AWS_ACCESS_KEY_ID'] = config['aws_key_id']\nos.environ['AWS_SECRET_ACCESS_KEY'] = config['aws_key_secret']\nimport boto3\necr = boto3.client('ecr', region_name='us-east-1')\nimport base64\nauth_code = base64.b64decode(ecr.get_authorization_token()['authorizationData'][0]['authorizationToken']).split(':')[1]\nfrom docker import Client\nclient = Client(base_url = ADDR)\nclient.login(username='AWS', password=auth_code, registry='https://xxxxx.dkr.ecr.us-east-1.amazonaws.com')\nWhen performing locally I get a weird response back:\n{'username': u'AWS', 'password': u'xxxxxx', 'email': None, 'serveraddress': u'https://xxxxx.dkr.ecr.us-east-1.amazonaws.com'}\nWhen doing it within the VPC I get back:\n{u'Status': u'Login Succeeded'}\nBoth docker-py versions are 1.9.0. The only other difference I can think of is different timezones (BST vs UTC). Any ideas, it would be nice to be able to test locally going forward.\n. It looks like somewhere in the login process the local version is finding what it thinks is an auth and so the login uses the cached result. Adding reauth=True to the login call fixes it. Sorry for the rabbit hole Tomas.\nDEBUG:docker.auth.auth:Looking for auth entry for 'xxx.dkr.ecr.us-east-1.amazonaws.com'\n  DEBUG:docker.auth.auth:Found u'https://xxx.dkr.ecr.us-east-1.amazonaws.com'\n. ",
    "lie-nielsen": "I found it helpful to delete the Amazon login credentials from ~/.docker/config.json as well. The token is only good for 12 hours.. ",
    "kleptog": "There are various patches already merged for this I believe, see for example #1127 and #1147.\n. @sheirys Unfortunately, the API only supports a single container at a time. The way the docker stats does it is by starting connection per container and combining the results.. The Network configuration was moved to the TaskSpec structure in https://github.com/docker/engine-api/pull/375 , which was Aug 2016. It would probably be a good idea if docker-py also did this, that would mean we conform to the actual current API.. If you use the new format, then you can change whatever you like. If you use the old format, then yes it break on any changes.\nISTM there is no reason to use the old format except for backward compatibility.. I'm really glad this issue is getting some attention.. I'm getting this issue too. Not sure exactly why it's triggering, but I do notice there are two load_config functions:\n/usr/local/lib/python2.7/dist-packages/docker/auth/auth.py:def load_config(config_path=None):\n/usr/local/lib/python2.7/dist-packages/docker/auth.py:def load_config(config_path=None, config_dict=None):\nLooking at the names of the files and directories I wonder if there is an importing order difference that causes it for some people to import the wrong module?\nAlso, this is the output of pip freeze:\ndocker==3.4.1\ndocker-compose==1.22.0\ndocker-py==1.10.6\nApparently docker is the replacement for docker-py, but does not conflict with it, so they simply install over each other. To fix this you have to uninstall both of them, then install just the right one. (And hope other pypi modules don't pull the old module).. ",
    "approxit": "Any plans for release date docker-py with Docker Swarm mode support from 1.12?\n. Ow, I get it now... Totally misleading. It looks like every parameter need to be checked.\nIn TaskTemplate placement parameter suffers same situation.\n500 Server Error: Internal Server Error (\"{\"message\":\"json: cannot unmarshal array into Go value of type swarm.Placement\"}\")\nDocker go sources defines Placement like dict of one key with array of string.\nTemporary walkaround: \npython\nClient.create_service(\n    task_template=TaskTeplate(\n        placement={\n            'Constraints': ['node.hostname==some_hostname']\n        }\n    )\n)\n. @TomasTomecek Thanks for response.\nYes, threading.Event WOULD do the job, but browsing your code I think you're getting to point I wrote about:\n\nFetch events live without time limits. Generator is blocking until some event pops out.\n\nSo yes, you are using theading.Event, but it will stop that thread as soon as new docker.event arrives and you will return to your code that can check threading.Event. Not earlier.\nBrowsing even more I've finally found this stackoverflow question which answer would work. A little dirty hack... @shin- any chances to make it a little more official?. In version 2 of docker-py we have attach_socket in container methods. Socket-like object would be a just right for this issue with events.\n@shin-, any chances to make events_socket method?. ",
    "ashishjain14": "Thanks @shin-  will try the master branch and get back.\n. ",
    "xeor": "Based on the comment about stacks being implemented on the cli side. Will this issue then ever get resolved? Or is subprocess.run() the best way to do this for now?. ",
    "shoxxdj": "Any update about this feature ? . ",
    "craigt1212": "Any news regarding this issue?. ",
    "Sarniak1991": "Any plans regarding stack commands?. ",
    "matthewmrichter": "It's not very confidence inspiring for the future of Swarm if this does not get added... ",
    "rnickle": "I just built a whole flow using the docker-py SDK, and I was expecting to implement stack deploy and was shocked when I found the support wasn't there.. ",
    "ferm10n": "It sounds like all the underlying actions docker stack ... commands perform are supported by docker-py (service creation and managing tasks). Maybe all we'd need to do is implement whatever logic is in the docker-cli and do yaml parsing here too? (can anyone point me to where that is located?)\n(or uh, maybe we could just call subprocess internally :sweat_smile:). ",
    "fw0037": "docker -v\nDocker version 1.11.2, build b9f10c9\npython\nPython 2.7.6 (default, Mar 22 2014, 22:59:56) \n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n. Inside the container. I would running the command \"/bin/echo 'test1 teip'>>/test.txt\" to add  'test1 teip'  into the file /test.txt  inside the container with using docker CLI.  But I didn't find 'test1 teip'  exists in file test.txt.\n. Fine. Thanks. \n.  python\nPython 2.7.6 (default, Mar 22 2014, 22:59:56) \n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\ndocker version\nClient:\n Version:      1.11.2\n API version:  1.23\n Go version:   go1.5.4\n Git commit:   b9f10c9\n Built:        Wed Jun  1 21:20:08 2016\n OS/Arch:      linux/amd64\nServer:\n Version:      1.11.2\n API version:  1.23\n Go version:   go1.5.4\n Git commit:   b9f10c9\n Built:        Wed Jun  1 21:20:08 2016\n OS/Arch:      linux/amd64\n. I am referring to the docs to do, but the results of the implementation of the line when the results obtained with the command is inconsistent, I guess it is not brought under control container port by the file DockerFile .\n. Yes, I found the ports list is incorrect. And now I get the results I want.\n. root@SZX1000137723:/opt/bin# docker run --shm-size 8G iquoter_s_webphere8.5/redhat6.5_jdk1.7 df -h\nFilesystem     1K-blocks     Used Available Use% Mounted on\nrootfs         515929528 24237548 465477632   5% /\nnone           515929528 24237548 465477632   5% /\ntmpfs           16463404        0  16463404   0% /dev\ntmpfs           16463404        0  16463404   0% /sys/fs/cgroup\n/dev/xvde1     515929528 24237548 465477632   5% /etc/resolv.conf\n/dev/xvde1     515929528 24237548 465477632   5% /etc/hostname\n/dev/xvde1     515929528 24237548 465477632   5% /etc/hosts\nshm              8388608        0   8388608   0% /dev/shm\ntmpfs           16463404        0  16463404   0% /proc/kcore\ntmpfs           16463404        0  16463404   0% /proc/latency_stats\ntmpfs           16463404        0  16463404   0% /proc/timer_stats\ntmpfs           16463404        0  16463404   0% /proc/sched_debug\nroot@SZX1000137723:/opt/bin# \nbut when I create a new container with image  iquoter_s_webphere8.5/redhat6.5_jdk1.7, the container's shm size is already 64m.\nroot@SZX1000137723:~# docker exec -it 42633bf55408 /bin/sh\nsh-4.1# df -h\nFilesystem      Size  Used Avail Use% Mounted on\nrootfs          493G   26G  442G   6% /\nnone            493G   26G  442G   6% /\ntmpfs            16G     0   16G   0% /dev\ntmpfs            16G     0   16G   0% /sys/fs/cgroup\n/dev/xvde1      493G   26G  442G   6% /etc/resolv.conf\n/dev/xvde1      493G   26G  442G   6% /etc/hostname\n/dev/xvde1      493G   26G  442G   6% /etc/hosts\nshm              64M     0   64M   0% /dev/shm\ntmpfs            16G     0   16G   0% /proc/kcore\ntmpfs            16G     0   16G   0% /proc/latency_stats\ntmpfs            16G     0   16G   0% /proc/timer_stats\ntmpfs            16G     0   16G   0% /proc/sched_debug\nsh-4.1# \n. I found if I remove other fields when start the docker container with docker-py, it will be available. it's like below:\nfrom docker import Client\ncli = Client(base_url='tcp://10.21.127.24:2375')\nc = cli.create_container(image=\"oracle_11g_r1/redhat6.5_jdk1.7\", stdin_open=True, tty=True, command=\"/bin/bash\", name=\"Common_Data_1.0.1.3\",ports=[1521], host_config=cli.create_host_config(privileged=True,shm_size =\"8g\",port_bindings={\"1521/tcp\":(\"0.0.0.0\",8011)})) \nc = cli.start(container='Common_Data_1.0.1.3')\n. the issue had been fixed. thanks.\n. ",
    "crosbymichael": "Whatever tech is the best works for me as long as it produces xunit output ;)\nSome background, we are working on getting this for jenkins to help the docker integration suite run in parallel and since the docker-py tests run during this it will have to produce the expected output of the rest of the test results.\n. @shin- perfect thanks!\n. ",
    "dbdd4us": "LGTM (IMNAM) \n. ",
    "bartgrantham": "This also occurs on Ubuntu 16.04.  Here's my code, slightly edited for brevity.  Maybe I'm just doing it wrong and someone can correct me:\n```\n    c = Client(base_url='unix://var/run/docker.sock', version='auto')\ncontainer_name = str(uuid.uuid4())\nvolumes=['/mnt/container_local']\nhost_config = c.create_host_config(\n    binds={\n        container_local_mnt: {\n            'bind': volumes[0],\n            'mode': 'rw'\n        }\n    })\n\ncontainer_kwargs = {\n    'image'    : manifest['image'],  # just a docker image name...\n    'name'     : container_name,\n    'hostname' : container_name,\n    'command'  : [command],  # the program I'm trying to get stats on\n    'entrypoint' : 'sh -c',\n    'volumes' : volumes,\n    'working_dir' : volumes[0],\n    'host_config' : host_config,\n    'detach'   : True,\n    'network_disabled' : True,\n    'environment' : {},\n}\n\ncontainer = c.create_container(**container_kwargs)\nc.start(container=container['Id'])\n\n# just want to see it printed out for now...\nwhile container['Id'] in [i['Id'] for i in c.containers()]:\n    # vvv this will always timeout and throw an exception unless the container exits first vvv\n    print c.stats(container['Id'], stream=False)\n    # ^^^ this will always timeout and throw an exception unless the container exits first ^^^\n    time.sleep(2)\n\nexit_code = c.wait(container=container)\n\n```\n. Yup, that bug report looks like exactly what I'm seeing, except I'm also seeing it on docker 1.12.1 on Ubuntu.  Trying the ncat direct http test now, it appears to be hanging indefinitely (I'm assuming it'll return when the container finishes).\nI'd like to point out that while this might be a bug in docker, it is triggered by some behavior in docker-py.  If I run a container from the command line I can get the stats in parallel no problem.  Is it possible this happens because the docker-py client holds open the connection to the socket (ie. doesn't close/reopen between function calls) and this doesn't play nice with how docker stats expects to be able to stream data back to the client?\nCome to think of it, I'm going to test exactly this by closing the client after starting the container, using a fresh client until the stats runs out, and opening a third one for everything after that.\n. Doing {'network_disabled' : False} did not fix it for me, neither did explicitly closing the docker-py client and reconnecting before/after trying to get stats.  I am beginning to think that Tomas is on the right track that there's something about the way the container is started that prevents docker from collecting the stats.\n. Wait!  {'network_disabled' : False} did fix my problem!  But it was masked by another important problem that might be related the source of the bug.  In my code I iterate until the container disappears from the list of running containers:\nwhile container['Id'] in [i['Id'] for i in c.containers()]:\n        print c.stats(container['Id'], stream=False)\n        time.sleep(2)\nWith {'network_disabled' : False} that loops correctly until the container is no longer running.  Then, and I suspect there's a race condition here, what should be the final call to stats() hangs.  My solution is to set the client timeout to something smaller and less annoying like 15 seconds and do this:\nwhile container['Id'] in [i['Id'] for i in c.containers()]:\n        try:\n            stats = c.stats(container['Id'], stream=False)\n        except:\n            break\n        print stats\n        time.sleep(2)\n. @michaelbarton: yeah, definitely in my code there's a race condition but I'd expect a graceful failure (ie. stats of 0).  I was thinking that I was actually racing against a bug in docker where trying to get stats of a stopped container hung.  Which according to @TomasTomecek might be the case!\nFor anyone else tripping over this while we wait for docker to get fixed, I'm using the generator, streaming version of statssince it's a bit cleaner (still have to catch the final timeout):\n```\nc = Client(base_url='unix://var/run/docker.sock', version='auto', timeout=15)\n... create/start your container WITH NETWORKING ENABLED ...\ntry:\n    for stat in c.stats(container['Id']):\n        print stat\nexcept Exception as e:\n    pass  # Gross.\n... collect up container exit code, stdout, stderr, etc.\n```\n. @michaelbarton: Oh hey!  You're in bioinformatics, too!  Birds of a feather suffer through frustrating docker bugs together... :)\n. ",
    "rcoh": "This should reproduce it:\n- hosts: my_host\n  roles:\n     - angstwad.docker_ubuntu\n- hosts: my_host\n  tasks:\n    - name: login\n      docker_login:\n        username: foo\n        password: bar\n        email: a@a.com\n. Temporary fix\nDowngrade to 1.9.0:\nroles:\n    - { role: angstwad.docker_ubuntu, pip_version_docker_py: 1.9.0 }\n. whoops sorry I accidentally created this twice. Closing this one, see the other issue above.\n. (Not really your fault, also filing the same issue against ansible)\n. For people who found this via google:\nTemporary fix\nDowngrade to 1.9.0:\nroles:\n    - { role: angstwad.docker_ubuntu, pip_version_docker_py: 1.9.0 }\n. ",
    "lubronzhan": "+1\n. ",
    "jcali": "+1\n. ",
    "kkarimi": "Hi @shin- I get this with latest ansible and docker-py 1.10.1 running on OSX (think it's ok in linux):\n\nFailed to import docker-py - No module named requests.exceptions.\n\nAny ideas? Thanks\n. ",
    "netoneko": "It gets even worse, because in current version of docker-pycreds if you install it manually it will update six to required version, but if you install it as part as pip install docker-compose==1.7.0 it fails to update six and therefore docker-compose command stops working.\n. Also, if you run pip install --upgrade docker-compose==1.8.0 it works, if there was no --upgrade, it does not work.\n. Seems like the issue comes from this commit (by @shin-): https://github.com/shin-/compose/commit/3f08efb32822d2a762f1ba81ecd0a5b9ee563208\nThe main problem is that now clean installation trough pip that used to work for half a year is broken and that might impact production systems for many people around the world.\n. Here is the log of Amazon CloudFormation attempting to install docker-compose:\n2016-09-11 05:49:40,236 P2570 [INFO] ============================================================\n2016-09-11 05:49:40,236 P2570 [INFO] Command 01_install_docker_compose\n2016-09-11 05:49:44,219 P2570 [INFO] -----------------------Command Output-----------------------\n2016-09-11 05:49:44,219 P2570 [INFO]    You are using pip version 6.1.1, however version 8.1.2 is available.\n2016-09-11 05:49:44,219 P2570 [INFO]    You should consider upgrading via the 'pip install --upgrade pip' command.\n2016-09-11 05:49:44,219 P2570 [INFO]    Collecting docker-compose==1.7.0\n2016-09-11 05:49:44,219 P2570 [INFO]      Downloading docker-compose-1.7.0.tar.gz (141kB)\n2016-09-11 05:49:44,219 P2570 [INFO]    Collecting cached-property<2,>=1.2.0 (from docker-compose==1.7.0)\n2016-09-11 05:49:44,219 P2570 [INFO]      Downloading cached_property-1.3.0-py2.py3-none-any.whl\n2016-09-11 05:49:44,219 P2570 [INFO]    Collecting docopt<0.7,>=0.6.1 (from docker-compose==1.7.0)\n2016-09-11 05:49:44,219 P2570 [INFO]      Downloading docopt-0.6.2.tar.gz\n2016-09-11 05:49:44,219 P2570 [INFO]    Requirement already satisfied (use --upgrade to upgrade): PyYAML<4,>=3.10 in /usr/lib64/python2.7/dist-packages (from docker-compose==1.7.0)\n2016-09-11 05:49:44,220 P2570 [INFO]    Collecting requests<2.8,>=2.6.1 (from docker-compose==1.7.0)\n2016-09-11 05:49:44,220 P2570 [INFO]      Downloading requests-2.7.0-py2.py3-none-any.whl (470kB)\n2016-09-11 05:49:44,220 P2570 [INFO]    Collecting texttable<0.9,>=0.8.1 (from docker-compose==1.7.0)\n2016-09-11 05:49:44,220 P2570 [INFO]      Downloading texttable-0.8.4.tar.gz\n2016-09-11 05:49:44,220 P2570 [INFO]    Collecting websocket-client<1.0,>=0.32.0 (from docker-compose==1.7.0)\n2016-09-11 05:49:44,220 P2570 [INFO]      Downloading websocket_client-0.37.0.tar.gz (194kB)\n2016-09-11 05:49:44,220 P2570 [INFO]    Collecting docker-py<2,>1.7.2 (from docker-compose==1.7.0)\n2016-09-11 05:49:44,220 P2570 [INFO]      Downloading docker_py-1.10.1-py2.py3-none-any.whl (48kB)\n2016-09-11 05:49:44,220 P2570 [INFO]    Collecting dockerpty<0.5,>=0.4.1 (from docker-compose==1.7.0)\n2016-09-11 05:49:44,220 P2570 [INFO]      Downloading dockerpty-0.4.1.tar.gz\n2016-09-11 05:49:44,220 P2570 [INFO]    Requirement already satisfied (use --upgrade to upgrade): six<2,>=1.3.0 in /usr/lib/python2.7/dist-packages (from docker-compose==1.7.0)\n2016-09-11 05:49:44,220 P2570 [INFO]    Collecting jsonschema<3,>=2.5.1 (from docker-compose==1.7.0)\n2016-09-11 05:49:44,220 P2570 [INFO]      Downloading jsonschema-2.5.1-py2.py3-none-any.whl\n2016-09-11 05:49:44,220 P2570 [INFO]    Collecting enum34<2,>=1.0.4 (from docker-compose==1.7.0)\n2016-09-11 05:49:44,220 P2570 [INFO]      Downloading enum34-1.1.6-py2-none-any.whl\n2016-09-11 05:49:44,221 P2570 [INFO]    Collecting ipaddress>=1.0.16 (from docker-py<2,>1.7.2->docker-compose==1.7.0)\n2016-09-11 05:49:44,221 P2570 [INFO]      Downloading ipaddress-1.0.17-py2-none-any.whl\n2016-09-11 05:49:44,221 P2570 [INFO]    Collecting docker-pycreds>=0.2.0 (from docker-py<2,>1.7.2->docker-compose==1.7.0)\n2016-09-11 05:49:44,221 P2570 [INFO]      Downloading docker_pycreds-0.2.0-py2.py3-none-any.whl\n2016-09-11 05:49:44,221 P2570 [INFO]    Collecting backports.ssl-match-hostname>=3.5 (from docker-py<2,>1.7.2->docker-compose==1.7.0)\n2016-09-11 05:49:44,221 P2570 [INFO]      Downloading backports.ssl_match_hostname-3.5.0.1.tar.gz\n2016-09-11 05:49:44,221 P2570 [INFO]    Collecting functools32 (from jsonschema<3,>=2.5.1->docker-compose==1.7.0)\n2016-09-11 05:49:44,221 P2570 [INFO]      Downloading functools32-3.2.3-2.zip\n2016-09-11 05:49:44,221 P2570 [INFO]    Installing collected packages: cached-property, docopt, requests, texttable, websocket-client, ipaddress, docker-pycreds, backports.ssl-match-hostname, docker-py, dockerpty, functools32, jsonschema, enum34, docker-compose\n2016-09-11 05:49:44,221 P2570 [INFO]      Running setup.py install for docopt\n2016-09-11 05:49:44,221 P2570 [INFO]      Found existing installation: requests 1.2.3\n2016-09-11 05:49:44,221 P2570 [INFO]        Uninstalling requests-1.2.3:\n2016-09-11 05:49:44,221 P2570 [INFO]          Successfully uninstalled requests-1.2.3\n2016-09-11 05:49:44,221 P2570 [INFO]      Running setup.py install for texttable\n2016-09-11 05:49:44,222 P2570 [INFO]      Running setup.py install for websocket-client\n2016-09-11 05:49:44,222 P2570 [INFO]      Found existing installation: backports.ssl-match-hostname 3.4.0.2\n2016-09-11 05:49:44,222 P2570 [INFO]        Uninstalling backports.ssl-match-hostname-3.4.0.2:\n2016-09-11 05:49:44,222 P2570 [INFO]          Successfully uninstalled backports.ssl-match-hostname-3.4.0.2\n2016-09-11 05:49:44,222 P2570 [INFO]      Running setup.py install for backports.ssl-match-hostname\n2016-09-11 05:49:44,222 P2570 [INFO]      Running setup.py install for dockerpty\n2016-09-11 05:49:44,222 P2570 [INFO]      Running setup.py install for functools32\n2016-09-11 05:49:44,222 P2570 [INFO]      Running setup.py install for docker-compose\n2016-09-11 05:49:44,222 P2570 [INFO]    Successfully installed backports.ssl-match-hostname-3.5.0.1 cached-property-1.3.0 docker-compose-1.7.0 docker-py-1.10.1 docker-pycreds-0.2.0 dockerpty-0.4.1 docopt-0.6.2 enum34-1.1.6 functools32-3.2.3.post2 ipaddress-1.0.17 jsonschema-2.5.1 requests-2.7.0 texttable-0.8.4 websocket-client-0.37.0\n2016-09-11 05:49:44,222 P2570 [INFO] ------------------------------------------------------------\n2016-09-11 05:49:44,222 P2570 [INFO] Completed successfully.\n2016-09-11 05:49:44,223 P2570 [INFO] ============================================================\n2016-09-11 05:49:44,223 P2570 [INFO] Command 02_run_docker_pull\n2016-09-11 05:49:44,320 P2570 [INFO] -----------------------Command Output-----------------------\n2016-09-11 05:49:44,320 P2570 [INFO]    Traceback (most recent call last):\n2016-09-11 05:49:44,320 P2570 [INFO]      File \"/usr/local/bin/docker-compose\", line 5, in <module>\n2016-09-11 05:49:44,321 P2570 [INFO]        from pkg_resources import load_entry_point\n2016-09-11 05:49:44,321 P2570 [INFO]      File \"/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 3020, in <module>\n2016-09-11 05:49:44,321 P2570 [INFO]        working_set = WorkingSet._build_master()\n2016-09-11 05:49:44,321 P2570 [INFO]      File \"/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 616, in _build_master\n2016-09-11 05:49:44,321 P2570 [INFO]        return cls._build_from_requirements(__requires__)\n2016-09-11 05:49:44,321 P2570 [INFO]      File \"/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 629, in _build_from_requirements\n2016-09-11 05:49:44,321 P2570 [INFO]        dists = ws.resolve(reqs, Environment())\n2016-09-11 05:49:44,321 P2570 [INFO]      File \"/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 812, in resolve\n2016-09-11 05:49:44,321 P2570 [INFO]        raise VersionConflict(dist, req).with_context(dependent_req)\n2016-09-11 05:49:44,321 P2570 [INFO]    pkg_resources.ContextualVersionConflict: (six 1.8.0 (/usr/lib/python2.7/dist-packages), Requirement.parse('six>=1.10.0'), set(['docker-pycreds']))\n2016-09-11 05:49:44,321 P2570 [INFO] ------------------------------------------------------------\n2016-09-11 05:49:44,321 P2570 [ERROR] Exited with error code 1\nIf we check docker-compose manually, we'll get:\n$ /usr/local/bin/docker-compose -v\nTraceback (most recent call last):\n  File \"/usr/local/bin/docker-compose\", line 5, in <module>\n    from pkg_resources import load_entry_point\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 3020, in <module>\n    working_set = WorkingSet._build_master()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 616, in _build_master\n    return cls._build_from_requirements(__requires__)\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 629, in _build_from_requirements\n    dists = ws.resolve(reqs, Environment())\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 812, in resolve\n    raise VersionConflict(dist, req).with_context(dependent_req)\npkg_resources.ContextualVersionConflict: (six 1.8.0 (/usr/lib/python2.7/dist-packages), Requirement.parse('six>=1.10.0'), set(['docker-pycreds']))\nHere is docker-pycreds issue in action:\npip install docker-pycreds==0.2.0\nYou are using pip version 6.1.1, however version 8.1.2 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nRequirement already satisfied (use --upgrade to upgrade): docker-pycreds==0.2.0 in /usr/local/lib/python2.7/site-packages\nCollecting six>=1.10.0 (from docker-pycreds==0.2.0)\n  Downloading six-1.10.0-py2.py3-none-any.whl\nInstalling collected packages: six\n  Found existing installation: six 1.8.0\n    Uninstalling six-1.8.0:\n      Successfully uninstalled six-1.8.0\nSuccessfully installed six-1.10.0\nChecking again:\n$ /usr/local/bin/docker-compose -v\ndocker-compose version 1.7.0, build 0d7bf73\n. @shin- pleasure doing business with you! Everything works great.\n. ",
    "dajose": "thanks :)\nFor the milestone you set, I understand this goes live until release 1.11.0. is that correct?\n. thanks shin-  :)\n. Hi @PierreF, actually I am not on the admin team for this project :S \nI also would like that release sooner :P\n. ",
    "PierreR": "Thanks for the PR !\nIf I am not mistaken, this is currently blocking the upgrade of docker-compose from 1.8 to 1.8.1. from src (pypi) ; at least on nix/nixos (nixpkgs master) where requests has already been updated to 2.11.1 .\n@shin- Is there a possibility to get this fix sooner in a 1.10.4 release ?\n. Sorry wrong ping (I have just updated my comment ;-)\n. ",
    "GinoHereIam": "Hi Tomas,\nit's running from multiple processes. \n. Hi Tomas,\nno no, there are no proxies. Everything happens in same network.\nthe method looks like this:\n``` python\ndef init(self, docker_env, base_url=\"\", logginglvl=logging.info, timeout=3000):\nself.base_url = base_url\nself.timeout = timeout\nself.dockerenv = docker_env\nlogging.basicConfig(level=logginglvl)\n\n# Pre-defined variables\nself.binds = None\nself.host_config = None\nself.volumes = None\nself.container = None\nself.dockerfile = None\nself.template = None\nself.scripttemplate = None\nself.script = None\nself.image = None\nself.stream = False\n\nif self.base_url:\n    self.cli = Client(base_url=self.base_url, timeout=self.timeout)\nelse:\n    raise DockifyException\n\ndef CreateAndStart(self, image=\"tag/build_image:latest\", stream=True):\n    self.image = image\n    self.stream = stream\n\n    # Host specific parameters\n    self.volumes = ['/ssd', '/opt', '/nfs']\n    self.binds = ['/ssd:/ssd:rw', '/opt/:/opt:rw', '/nfs:/nfs:rw']\n    Dockify.LOGGER.info(\"Create container!\")\n    self.container = self.cli.create_container(\n        image=self.image, tty=True, volumes=self.volumes,\n        host_config=self.cli.create_host_config(binds=self.binds))\n    Dockify.LOGGER.info(\"Start container: %s\", self.container)\n\n    try:\n        self.cli.start(container=self.container.get('Id'))\n        if self.stream is True:\n            res = [l for l in self.cli.logs(\n                self.container, follow=True, stdout=True,\n                stderr=True, stream=True)]\n            Dockify.LOGGER.info(\"\".join(res))\n        else:\n            Dockify.LOGGER.info(\"\".join(self.cli.logs(\n                self.container, follow=True, stdout=True,\n                stderr=True, stream=False)))\n        ret = self.cli.wait(self.container.get('Id'))\n        sys.exit(ret)\n\n    except (TypeError, RuntimeError, NameError) as ex:\n        raise ex\n\n```\n. @TomasTomecek you've got new information for me? Could you figure out the issue? \nThanks\nGino\n. The actual problem is that I have 3 Jobs which are trying to access the same URL as mentioned at same time:\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=4243): Max retries exceeded with url: /v1.22/build?q=False&pull=False&t=myimage&nocache=False&forcerm=False&rm=False\nAll 3 jobs take the same image to build a container as I have shown the code above. I don't build a new image, I take the one which is already built. That's why I am confused it's unrelated information for you.\nAs it looks for me the endpoint doesn't take more than one TCP connection. Which is pretty bad for us.\nA minimal producer would be, you set up a Jenkins with the matrix plugin. In the matrix configuration are at least 2 configurations with same job. This job is what I do above with the Python. Each configuration run this job and try to a container with same image.\nMy set up:\nJenkins  =>          Another host / Docker Service   =>  Back to Jenkins\n|  Job 1 &&  Job 2  |  => /build?q=False?t=myimage ...   => 1. Job Pass  | 2. Job Fail \nSo it is no docker-py problem. Shall I open a ticket on the docker project site?\n. So ... this look pretty good:\ncurl http://hostname:4243/v1.22/version;  curl http://hostname:4243/v1.22/version\n{\"Version\":\"1.12.1\",\"ApiVersion\":\"1.24\",\"GitCommit\":\"23cf638\",\"GoVersion\":\"go1.6.3\",\"Os\":\"linux\",\"Arch\":\"amd64\",\"KernelVersion\":\"4.4.0-36-generic\",\"BuildTime\":\"**2016-08-18T05:33:38.359262265+00:00**\"}\n{\"Version\":\"1.12.1\",\"ApiVersion\":\"1.24\",\"GitCommit\":\"23cf638\",\"GoVersion\":\"go1.6.3\",\"Os\":\"linux\",\"Arch\":\"amd64\",\"KernelVersion\":\"4.4.0-36-generic\",\"BuildTime\":\"**2016-08-18T05:33:38.359262265+00:00**\"}\nThe BuildTime is same. Might it be a docker-py problem if I can't trigger multiple jobs at same time then?\nI've got another idea to test it more to my use case. I'll will call curl/urllib after the rejection to see if a GET will be rejected to or it might be a POST issue.\nUPDATE:\n@TomasTomecek \nI caught my exception and proceeded a GET with curl. It worked. So it might be a POST issue as I supposed. Did you try two synchronous POSTcalls with docker-py? I'll try POST with curl and will give you another feedback. BTW, I forgot to say that I don't build container by identically images. Even if I change the build tag, it doesn't work. So no matter what I build after one connection it always fail for me. \nUPDATE 2:\nI have a running build. Started with docker-py. When I want to access via POST or GETit works without connection issues. Can you please investigate it? So multiple connections via Curl works pretty fine.\n10:40:14 requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=4243): Max retries exceeded with url:\nCan you tell me what is the maximum of retries you set to the package request? Default it is declared None\nDjekuji for your help! :)\n. :D found the issue ... the daemon was wrong configured on one slave but other one was fine. I hate systemd. Thanks for help!\n. Thank you very response! :) \nSo technically I could stream the response to a tarball? . ",
    "bubaflub": "Also including @ryan0x44 who helped me with this and @shin- who we've chatted with briefly about this.\n. Great, I'll fix things up and get the tests to pass.\n. I've rebased off of master and fixed the merge conflict.  I've made the changes you requested and now all of make test passes locally.  I also appeased pep8.  Slightly related -- I used autopep8 to fix up those nits.  Highly recommended.\n. ",
    "rickmendes": "@shin\nI have virtualenv and virtualenvwrapper installed and I just re-did my laptop Friday to purge old virtual environments and my global space.\nSo I have 1 virtualenv with docker-py in it, global site-packages toggled off for that and no docker-py  in my system environment.\n. @shin\nThis is from the virtualenv I am seeing the failures in.\npip freeze\n-e git+https://github.com/ansible/ansible.git@07a76bece15d2568d7ea76e77266190652a0beec#egg=ansible\nbackports.ssl-match-hostname==3.5.0.1\ncffi==1.8.2\ncryptography==1.5\ndocker-py==1.10.3\ndocker-pycreds==0.2.1\nenum34==1.1.6\nidna==2.1\nipaddress==1.0.17\nJinja2==2.8\nMarkupSafe==0.23\nparamiko==2.0.2\npy2-ipaddress==3.4.1\npyasn1==0.1.9\npycparser==2.14\npycrypto==2.6.1\nPyYAML==3.12\nrequests==2.10.0\nsix==1.10.0\nwebsocket-client==0.37.0\n. Docker version 1.12.1, build 6f9534c\nI am contemplating doing a fresh install of Python to avoid the El Capitan issues.\n. Not a problem. I can do that.\n. Error\nfatal: [test-image-3]: FAILED! => {\n    \"changed\": false,\n    \"failed\": true,\n    \"invocation\": {\n        \"module_args\": {\n            \"api_version\": null,\n            \"archive_path\": null,\n            \"buildargs\": null,\n            \"cacert_path\": null,\n            \"cert_path\": null,\n            \"container_limits\": null,\n            \"debug\": false,\n            \"docker_host\": null,\n            \"dockerfile\": null,\n            \"filter_logger\": false,\n            \"force\": false,\n            \"http_timeout\": null,\n            \"key_path\": null,\n            \"load_path\": null,\n            \"name\": \"test-image-3\",\n            \"nocache\": \"False\",\n            \"path\": null,\n            \"pull\": false,\n            \"push\": true,\n            \"repository\": \"127.0.0.1:5000/test-image-3\",\n            \"rm\": true,\n            \"ssl_version\": null,\n            \"state\": \"present\",\n            \"tag\": \"role-test\",\n            \"timeout\": null,\n            \"tls\": null,\n            \"tls_hostname\": null,\n            \"tls_verify\": null,\n            \"use_tls\": \"no\"\n        },\n        \"module_name\": \"ilmn_docker_image\"\n    },\n    \"msg\": \"Error pushing image 127.0.0.1:5000/test-image-3: Extra data: line 2 column 1 - line 3 column 1 (char 64 - 128)\"\npackages\npip freeze\n-e git+https://github.com/ansible/ansible.git@07a76bece15d2568d7ea76e77266190652a0beec#egg=ansible\nbackports.ssl-match-hostname==3.5.0.1\ncffi==1.8.2\ncryptography==1.5\n-e git+https://github.com/docker/docker-py.git@6b7a828400f46ea81374bc5764d8aa81bf38f6f7#egg=docker_py\ndocker-pycreds==0.2.1\nenum34==1.1.6\nidna==2.1\nipaddress==1.0.17\nJinja2==2.8\nMarkupSafe==0.23\nparamiko==2.0.2\npy2-ipaddress==3.4.1\npyasn1==0.1.9\npycparser==2.14\npycrypto==2.6.1\nPyYAML==3.12\nrequests==2.10.0\nsix==1.10.0\nwebsocket-client==0.37.0\n. Installed with\npip install -e 'git+https://github.com/docker/docker-py.git@6b7a828400f46ea81374bc5764d8aa81bf38f6f7#egg=docker-py'\n. OK. Let me know if you want me to do some more testing. Thanks.\n. No problem. I will file a bug a against their module tonight. I work with them a lot so I should get a response. Thanks for digging.\n. ",
    "mikeplavsky": "Duplicate! Sorry! \nhttps://github.com/docker/docker-py/issues/1212 \n. ",
    "nathannis": "Hey guys, any thoughts on this?\n. This is my first contribution to this project,.  Please pardon my questions.\nThe tests all passed locally.  I tried to follow the style of all existing code.  I have a consumer that is functioning properly when docker-py is installed from my source.  Please let me know when this is included in a release posted to pypi so that I can switch my application from using the source copy back to using pip install.\nIf you have any thoughts or suggestions, please let me know.\nThanks,\nNathan\n. Hey guys, \n  I am currently running off my fork, but would like to get back to running off of a version of docker-py from pypi.  Is there anything I can do to get this included in a future release?\n. Thanks!\n. Should this be\nif endpoint_spec is not None:\n. ",
    "christianbundy": "Thanks for the quick review @dnephin. I amended my commit to use a 2.6-friendly try...except block around an open() call. For what it's worth, I've also used the 'rb' mode for opening the file, since the docs say that's important for Windows. It looks like the test are all passing now, please let me know if you notice anything else.\n. @seschwar Thanks for getting it started! Our team supports Windows so this is super helpful for us.\n. Looks great, thank you! Excited to test this in Compose.\n. On it!\n. Done.\n. ",
    "lllama": "Unfortunately this doesn't seem to work for me. If I patch docker/api/service.py to use EndpointSpec instead of Endpoint then the other Ports example will work, however.\n. ",
    "zhourunlai": "so now use endpoint_spec  instead of  endpoint_config(low-level api support), right?\nclient.create_service(task_template, endpoint_spec={\n  'Ports': [\n    { 'Protocol': 'tcp', 'PublishedPort': 8080, 'TargetPort': 80 },\n  ]\n}). ",
    "ashish-bhangale": "Hey,\nThanks for your reply! Yes, the image was available locally. I was facing this problem because of my host machine weird behaviour, anyway, it is working now. \n. ",
    "arkkanoid": "I tried your code snippet running on ubuntu 14.04 and in a virtual machine with the same OS.\nIn both cases, after run the code the pid_limit is 0.\ndocker-py v1.10.3\ndocker client v1.12.1\nPython 3.5.2\n. ",
    "CGNonofr": "According to the documentation (https://docs.docker.com/engine/reference/commandline/create/), pidsLimit only works with a kernel >= 4.3.\n. ",
    "shawnbutts": "i need this like yesterday. :)  \nany idea when 1.11.0 might be released?  anyway this could be added to 1.10.x?\nthanks\n. ",
    "JoelJ": "Any idea why this hasn't been merged yet? Anything we can do to help get this released? This is a pretty big issue and makes using docker-py completely useless for using Services :/\n. @shawnbutts and anyone else who wants to get this to work while we wait: I've done a gross thing to make this work for me now. I really didn't want to fork this repo just to fix things. So I've resorted to monkey patching. This appears to work for me:\n``` python\ndef monkey_patch_service():\n    import six\n    import docker.api.service\n    from docker import errors\n    from docker.auth import auth\n    import warnings\ndef create_service(\n        self, task_template, name=None, labels=None, mode=None,\n        update_config=None, networks=None, endpoint_config=None,\n        endpoint_spec=None\n):\n    if endpoint_config is not None:\n        warnings.warn(\n            'endpoint_config has been renamed to endpoint_spec.',\n            DeprecationWarning\n        )\n        endpoint_spec = endpoint_config\n\n    url = self._url('/services/create')\n    headers = {}\n    image = task_template.get('ContainerSpec', {}).get('Image', None)\n    if image is None:\n        raise errors.DockerException(\n            'Missing mandatory Image key in ContainerSpec'\n        )\n    registry, repo_name = auth.resolve_repository_name(image)\n    auth_header = auth.get_config_header(self, registry)\n    if auth_header:\n        headers['X-Registry-Auth'] = auth_header\n    data = {\n        'Name': name,\n        'Labels': labels,\n        'TaskTemplate': task_template,\n        'Mode': mode,\n        'UpdateConfig': update_config,\n        'Networks': convert_service_networks(networks),\n        'EndpointSpec': endpoint_spec\n    }\n    return self._result(\n        self._post_json(url, data=data, headers=headers), True\n    )\n\ndef update_service(self, service, version, task_template=None, name=None,\n                   labels=None, mode=None, update_config=None,\n                   networks=None, endpoint_config=None,\n                   endpoint_spec=None):\n\n    if endpoint_config is not None:\n        warnings.warn(\n            'endpoint_config has been renamed to endpoint_spec.',\n            DeprecationWarning\n        )\n        endpoint_spec = endpoint_config\n\n    url = self._url('/services/{0}/update', service)\n    data = {}\n    headers = {}\n    if name is not None:\n        data['Name'] = name\n    if labels is not None:\n        data['Labels'] = labels\n    if mode is not None:\n        data['Mode'] = mode\n    if task_template is not None:\n        image = task_template.get('ContainerSpec', {}).get('Image', None)\n        if image is not None:\n            registry, repo_name = auth.resolve_repository_name(image)\n            auth_header = auth.get_config_header(self, registry)\n            if auth_header:\n                headers['X-Registry-Auth'] = auth_header\n        data['TaskTemplate'] = task_template\n    if update_config is not None:\n        data['UpdateConfig'] = update_config\n    if networks is not None:\n        data['Networks'] = convert_service_networks(networks)\n    if endpoint_spec is not None:\n        data['EndpointSpec'] = endpoint_spec\n\n    resp = self._post_json(\n        url, data=data, params={'version': version}, headers=headers\n    )\n    self._raise_for_status(resp)\n    return True\n\ndef convert_service_networks(networks):\n    if not networks:\n        return networks\n    if not isinstance(networks, list):\n        raise TypeError('networks parameter must be a list.')\n\n    result = []\n    for n in networks:\n        if isinstance(n, six.string_types):\n            n = {'Target': n}\n        result.append(n)\n    return result\n\ndocker.api.service.ServiceApiMixin.create_service = create_service\ndocker.api.service.ServiceApiMixin.update_service = update_service\n\n```\nJust call the monkey_patch_service method and it should fix things for you.\n. Closing because I just noticed this is a duplicate of PR: #1239 \n. ",
    "user171": "I also need support for checkpoint. Do you have an update? I could not find anything in the docs. Thanks.. ",
    "yurijvolkov": "Any update?. ",
    "YorikSar": "@shin- Do you plan to release 1.10.4 with this fix soon?\n. Oh, I see, you're in the process. Sorry for bothering you.\n. ",
    "tardyp": "Hi @shin- Can we have a 1.10.4 for this fix? this would avoid me to fix my requirement.txt to docker-py==1.10.2\n. ",
    "iverberk": "@TomasTomecek thanks for your response! The AWS ECR registry is definitely not running on http (I tried that and it don't want to use it over http anyway) so setting that daemon option is not going to fix it I think. Also, I believe the response (HTTP error 40x) is actually coming from the registry (maybe proxied through the daemon). AWS might be using a custom implementation. I hope you can take another look at this. Thanks!\n. @TomasTomecek I get the following output:\n```\ncurl -v https://123456789012.dkr.ecr.eu-west-1.amazonaws.com/v2/                                                                                                          +4282 9:04\n   Trying 54.77.217.207...\n Connected to 123456789012.dkr.ecr.eu-west-1.amazonaws.com (54.77.217.207) port 443 (#0)\n TLS 1.2 connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\n Server certificate: .dkr.ecr.eu-west-1.amazonaws.com\n Server certificate: Symantec Class 3 Secure Server CA - G4\n* Server certificate: VeriSign Class 3 Public Primary Certification Authority - G5\n\nGET /v2/ HTTP/1.1\nHost: 123456789012.dkr.ecr.eu-west-1.amazonaws.com\nUser-Agent: curl/7.43.0\nAccept: /\n< HTTP/1.1 401 Unauthorized\n< Content-Type: text/plain; charset=utf-8\n< Date: Thu, 06 Oct 2016 07:04:41 GMT\n< Docker-Distribution-Api-Version: registry/2.0\n< Www-Authenticate: Basic realm=\"https://123456789012.dkr.ecr.eu-west-1.amazonaws.com/\",service=\"ecr.amazonaws.com\"\n< Content-Length: 16\n< Connection: keep-alive\n<\nNot Authorizied\n* Connection #0 to host 123456789012.dkr.ecr.eu-west-1.amazonaws.com left intact\n```\n\nI've put a workaround in the library to not ping if it starts with my URL id and then it works no problem. The distribution allows non-authorized requests to that endpoint, Amazon ECR does not. The docker-py version 1.2.3. Thanks for looking!\n. Very sorry about this. Apparently I have this ancient version on my local dev environment and was under the impression that it was latest (ran a pip install but it was already there it seems and I didn't notice). A fresh install on the server shows that this case has been accounted for. Sorry for the noise and thanks for looking.\n. ",
    "friism": "@shin- can you perhaps add a test for this? I'm also happy to help you test\n. @shin- I can only test when this gets into compose, let me know\n. ",
    "HuanhuanSunMSFT": "$Env:DOCKER_HOST = \"localhost:2375\" fixes this issue though\nI'll close https://github.com/docker/compose/issues/4012 since it's partially resolved and also a duplicate of https://github.com/docker/compose/issues/3923\n. ",
    "MTRNord": "I think that is what I did search :) thanks.\n. ",
    "kshenk1": "\ud83d\udc4d  Just ran into this myself.. I'm also in favor of seeing this functionality as config usage is going to be essential to several services I'm currently shaking deployment operations out for.. Looks like this was addressed in 2.6.0?\nhttps://github.com/docker/docker-py/releases/tag/2.6.0\nhttps://docker-py.readthedocs.io/en/stable/api.html#docker.types.ConfigReference. ",
    "manics": "@shin- I've tried to modify the code as suggested (separate branch): https://github.com/manics/docker-py/commit/bf1e41f70f9b6059a07218a528b52413e5fd047f\nHowever this fails because adding from ..utils.utils import format_environment to docker/types/services.py introduces a circular dependency with docker/utils/__init__.py:\nfrom ..types import LogConfig, Ulimit\nfrom ..types import SwarmExternalCA, SwarmSpec\nDo you have any advice on how best to deal with this?\n. @shin- Thanks, I've rebased, removed the previous commit and updated the code.\n. ",
    "christopherobin": "Indeed I hadn't realized that bindings were part of HostConfig, sorry my bad, nothing to see here\nThanks\n. ",
    "harshagsv": "Thanks.\n. ",
    "qasim9641": "Hi All I am trying to Scale up the no of containers in my service but I am getting these errors. Can someone help me what I am missing in the update command?\n\n\n\nservice.update(mode=replica_mode)\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/home/qraza/.local/lib/python2.7/site-packages/docker/models/services.py\", line 77, in update\n    create_kwargs\n  File \"/home/qraza/.local/lib/python2.7/site-packages/docker/utils/decorators.py\", line 34, in wrapper\n    return f(self, *args, kwargs)\n  File \"/home/qraza/.local/lib/python2.7/site-packages/docker/utils/decorators.py\", line 19, in wrapped\n    return f(self, resource_id, args, *kwargs)\n  File \"/home/qraza/.local/lib/python2.7/site-packages/docker/api/service.py\", line 350, in update_service\n    self._raise_for_status(resp)\n  File \"/home/qraza/.local/lib/python2.7/site-packages/docker/api/client.py\", line 222, in _raise_for_status\n    raise create_api_error_from_http_exception(e)\n  File \"/home/qraza/.local/lib/python2.7/site-packages/docker/errors.py\", line 31, in create_api_error_from_http_exception\n    raise cls(e, response=response, explanation=explanation)\ndocker.errors.APIError: 500 Server Error: Internal Server Error (\"rpc error: code = 12 desc = renaming services is not supported\")\n\n\n\nLet me also paste my code here\nimport docker\nStart the docker client\nclient=docker.from_env()\nAs we already Created the Swarm cluster so need to initialize the cluster here\ntotal_containers=5 # \nreplica_mode = docker.types.ServiceMode('replicated', replicas=total_containers)\ntestservice=client.services.create(image='alpine',command='ping docker.com',name='testservicev6',mode=replica_mode)\ntotal_containers=10\nreplica_mode = docker.types.ServiceMode('replicated', replicas=total_containers)\ntestservice.update(mode=replica_mode). Did anyone solve this issue ?. ",
    "alshabib": "Seems like the documentation is incorrect. \n'links' should be a dict where the key is the container name and the value is the alias\n. ",
    "bodnarbm": "I should be able to add some unit tests for the should_check_directory function this evening.\n. I added some unit tests for the new should_check_directory function in docker/utils/utils.py\nThe overall behavior of processing .dockerignore when creating the tar should still be covered by the integration tests added in #863.\n. ",
    "zebpalmer": "In docker years, this (untouched) case is ancient... but it is the highest google result for docker swarm sherpa (as I found searching for unrelated issue). So I'll drop a comment here for anyone having a similar problem. I suggest this case be closed \nI'm more surprised you weren't getting random results with direct http, than the fact you were with docker-py. Given the service command shown, I would expect the behavior you experienced with docker-py as that request is handled through the swarm routing mesh.\nTo mitigate this behavior, you need to expose sherpa on the host rather than via the routing mesh.  Swap out the -p 4550:4550 with --publish mode=host,target=4550,published=4550 and it works like a charm. \nMore info: https://docs.docker.com/engine/swarm/services/#publish-ports. ",
    "Stef3478": "Thank you, this code worked fine!\nCheers\n. ",
    "mwgamble": "It's also possible to do this with labels:\ncli.containers(filter={\"label\": [\"label1=value1\", \"label2=value2\"]})\n. ",
    "hensing": "Still a problem in version 2.4.2 (observed on python:3.6.1-alpine). ",
    "alexmedanchuk": "Same issue\nFor me issue was in incorrect version of docker-py. Compose v.1.9 requires docker-py==1.10.6 but for some reason version of docker-py was 1.9.0\n. ",
    "boskiv": "docker-py where ? on localhost or remote host ?\non remote host i have \nroot@docker-01:~# docker-compose version\ndocker-compose version 1.9.0, build 2585387\ndocker-py version: 1.10.6\nCPython version: 2.7.6\nOpenSSL version: OpenSSL 1.0.1f 6 Jan 2014\non jenkins:\n(d41d8cd9) jenkins@jenkins:~/jobs/personal-area/workspace$ docker-compose version\ndocker-compose version 1.9.0, build 2585387\ndocker-py version: 1.10.6\nCPython version: 2.7.6\nOpenSSL version: OpenSSL 1.0.1f 6 Jan 2014. ",
    "orodbhen": "Turns out this was a red herring caused by my error logging framework. Closing.. I'm still seeing this in 3.4.1:\nOS: CentOS 7.5\nDocker: 18.03\nAs before, it does not occur with the docker CLI, only the Python API.  \nIn my case it occurs when running container.wait(). It originates with a socket.timeout exception in the urllib3 package, which sets off a cascade of other exceptions:\n\nsocket.timeout\nurllib3.exceptions.ReadTimeoutError\nrequests.exceptions.ConnectionError: UnixHTTPConnectionPool\n\nIt seems that requests is raising the wrong exception: requests.exceptions.ConnectionError instead of requests.exceptions.ReadTimeout. Shouldn't these exceptions be handled in the Docker API and converted to a public exception, rather than relying on the exception type from an underlying package?. Please fix this. Very confusing. . I already made this comment on #1374, but this seems to be a better place for it. \nI'm still seeing this in 3.4.1:\nOS: CentOS 7.5\nDocker: 18.03\nAs before, it does not occur with the docker CLI, only the Python API.  \nIn my case it occurs when running container.wait(). It originates with a socket.timeout exception in the urllib3 package, which sets off a cascade of other exceptions:\n\nsocket.timeout\nurllib3.exceptions.ReadTimeoutError\nrequests.exceptions.ConnectionError: UnixHTTPConnectionPool\n\nIt seems that requests is raising the wrong exception: requests.exceptions.ConnectionError instead of requests.exceptions.ReadTimeout. Shouldn't these exceptions be handled in the Docker API and converted to a public exception, rather than relying on the exception type from an underlying package?. ",
    "robottwo": "Second for this feature.. ",
    "dserodio": "Shouldn't the MAINTAINER instruction be replaced with a corresponding LABEL maintainer instruction as mentioned in docker/docker#25466 ?. ",
    "kinghuang": "Ah, yes. #1305 makes more sense. I'll close this PR. Thanks!. ",
    "glensc": "Yep, #1314 solves as well.. is there plan to work on unpinning, is it reported to requests project (if it's requests bug)?\n. hmm, #1204 just removes the pin. so it's actually ok to use requests-2.11 without any other code changes?\nmay i recall that the pin was added in 95d9306d2a1fd22dffb12a0548abf2d2f744ed9d :\n\nExclude requests 2.11 from setup.py to work around unicode bug\n. thanks for clarification!\n. \n",
    "tangyouze": "I found in the requirements.txt, it specify a requests version, however in setup.py it doesn't. So I use\npip install docker-py -U\n causes an incorrect requests version installed. ",
    "Lukasa": "Relevant Requests issue is kennethreitz/requests#3734. Temporary workaround is to pin to v2.12.1.. I think for the way docker-py uses Requests 2.12.{0,1} are safe: docker-py uses a hardcoded ACSII hostname that shouldn't be at any risk of breaking.. ",
    "agherzan": "Confirm I hit the same issue. Upgrading to 2.12.3 fixed the problem. When are we going to have this change in pip packages? I mean a new version there that would fix this issue.. Please update here as soon as it happens. Thanks @shin- \n. @westsouthnight you already have an workaround: use requests  2.12.3. ",
    "westsouthnight": "Please help, no one workaround not works. ",
    "yaakov-berkovitch": "docker-py version is 2.0.0-dev.\nI extracted the call in a very simple code, and it occurs again.\nFollowing the code that fails:\n```\nimport os,sys\nimport docker\nfrom docker import APIClient\nclient = APIClient(base_url='unix://var/run/docker.sock')\nnewNetwork = client.create_network('my_network', driver=\"overlay\")\nprint('Network created ' + str(newNetwork))\nTraceback (most recent call last):\n  File \"docker-create-service.py\", line 7, in \n    newNetwork = client.create_network('my_network', driver=\"overlay\")\n  File \"/home/paas/tools/paas-orchestrator/ThirdPartyPackages/docker/utils/decorators.py\", line 35, in wrapper\n    return f(self, args, *kwargs)\n  File \"/home/paas/tools/paas-orchestrator/ThirdPartyPackages/docker/api/network.py\", line 122, in create_network\n    return self._result(res, json=True)\n  File \"/home/paas/tools/paas-orchestrator/ThirdPartyPackages/docker/api/client.py\", line 214, in _result\n    self._raise_for_status(response)\n  File \"/home/paas/tools/paas-orchestrator/ThirdPartyPackages/docker/api/client.py\", line 210, in _raise_for_status\n    raise create_api_error_from_http_exception(e)\n  File \"/home/paas/tools/paas-orchestrator/ThirdPartyPackages/docker/errors.py\", line 28, in create_api_error_from_http_exception\n    raise cls(e, response=response, explanation=explanation)\ndocker.errors.APIError: 500 Server Error: Internal Server Error for url: http+docker://localunixsocket/v1.24/networks/create (\"rpc error: code = 3 desc = driver name: if driver is specified name is required\")\n```\nThanks.\n. i closed it by error...so i reopened it.. This issue can be closed because it was related to docker 1.12 experimental. Bug in the REST API.. Sorry for the post, found how to overcome the problem.\nInstead of giving a list of ::mode, I created a Mount object and the give it when instantiates a ContainerSpec object.\nYaakov. ",
    "huikang": "I think I found the root cause is the recent released pip package requests 2.12.2. After rolling back to 2.12.1, everything works fine.. ",
    "dvigueras": "Update: the issue here is the same issue present in #1321 \ndocker-py is no working with requests-2.12.2, but it works fine with requests-2.12.1. \nTemporary workaround: pip uninstall requests && pip install requests==2.12.1\nClosing this issue as it has nothing to do with docker-py. ",
    "ianseyer": "@TomasTomecek \ncould I get some guidance on what that would look like from docker-py?\nI imagine it's something akin to docker_client = Client('/var/run/docker.sock')?\nOr would it be from the Dockerfile of the container doing the spawning?\nEDIT: It's from the Dockerfile Volumes:/var/run/docker.sock:/var/run/docker.sock. ",
    "ori-n": "I tried adding this mount but it did not help - still getting the same error. I did not edit the Dockerfile, what I did was to mount the container /var/run dir to the same host dir, but it did not help.... ",
    "dineshputchala": "other command with pip as also facing same issue , I will check this issue in pip forum !\nthanks !. ",
    "bugchecker": "just enough\ndiff --git a/docker/models/images.py b/docker/models/images.py\nindex 32068e6..21139ae 100644\n--- a/docker/models/images.py\n+++ b/docker/models/images.py\n@@ -31,7 +31,7 @@ def tags(self):\n         The image's tags.\n         \"\"\"\n         return [\n-            tag for tag in self.attrs.get('RepoTags', [])\n+            tag for tag in (self.attrs.get('RepoTags') or [])\n             if tag != '<none>:<none>'\n         ]. ",
    "sean-abbott": "Any chance of this fix getting back-ported to the 1.10.x branch? Pretty much all of tools I use that use this library are still on that branch.... ",
    "cstrzadala": "Thanks @shin-, that worked a treat. When I got sometime, I'll do a PR to add this into the project.. ",
    "realityone": "Actually docker-py already provide the streaming way to save an image.\npython\nimage = cli.images.get(\"fedora:latest\")\nresp = image.save()\nf = open('/tmp/fedora-latest.tar', 'w')\nfor chunk in resp.stream():\n    f.write(chunk)\nf.close()\nI updated the docstring for this https://github.com/docker/docker-py/pull/1377.. @inge5t0r \nI remember the image.save() just send the request and will return a raw response which has not read yet.\nIf you mean this request will block sometime, it may be caused by docker daemon, docker daemon is packing the image.. The image save example in docker-py is iterating the urllib3.response.HTTPResponse directly, which is inherited from io.IOBase. In this way, every chunk is separated by \\x0a, so the chunk size is not guaranteed. This will take several time to read the whole response.\nI prefer using resp.stream(amt=***) to iterate the response stream, specify the chunk size manually, this will save a lot of time to read the response.. Hi, @david-drinn \nAfter digging further, I found that in docker-py 3.0.*, the image.save() is using _stream_raw_result to iterate the response. The chunk size in _stream_raw_result method is 1. This will take a lot of time to read the whole response.\nHi @shin- ,\nMaybe we should not using _stream_raw_result in image.save(), or the _stream_raw_result method should support customizing the chunk size.\nWhich one do you prefer, I will create an PR to fix this issue.. Thanks for updating! (\u00b4\u03c9\uff40)\nAll codes LGTM. \nThe only thing i'm concerned about is the comment at line https://github.com/docker/docker-py/pull/1906/files#diff-b5bb91e2b94da86ea05c1fe77a6ff432R354 should be updated now.\nWe not only use this method to attach to a TTY but also used in more case now.. ",
    "inge5t0r": "@realityone \nThis doesn't have the desired effect. The entire image is still read into memory with resp = image.save().. ",
    "david-drinn": "In docker-py 3.0.0, image.save() returns a generator object:\npython\nIn [73]: image.save()\nOut[73]: <generator object APIClient._stream_raw_result at 0x7fc8292d0780>\nIf I use this as the 3.0.0 docs imply (I say imply, since the example uses cli.get_image() and not image.save()), it takes several minutes to write my 30 MB docker image to tarball file:\n```python\nsave_gen = image.save()\nwith open('/tmp/img.tgz', 'wb') as img_f:\n    for chunk in save_gen:\n        img_f.write(chunk)\n```\nThe workaround is to use the same @shin- code snippet from above, which takes less than a second to write the same image to tarball file:\n```python\nurl = client.api._url('/images/{0}/get', image.id)\nresp = client.api._get(url, stream=True)\nwith open('/tmp/img.tgz', 'wb') as img_f:\n    for chunk in resp.iter_content(chunk_size=None):\n        img_f.write(chunk)\n```\nI could use some clarity here on whether I'm using image.save() incorrectly.\nNote: I'm using python3.. docker-py 3.0.0 I think must give a different object type from image.save() than previous versions, since it doesn't have the stream() attribute.\n```\nIn [96]: resp = image.save()\nIn [97]: resp.stream\nAttributeError                            Traceback (most recent call last)\n in ()\n----> 1 resp.stream\nAttributeError: 'generator' object has no attribute 'stream'\n```. I ran into this issue on 3.0.0. Can this be merged into 3.0.x?. Docker recommends putting data created by and used by the container that you want to be persistent on a docker volume. If things ended there I would agree that it doesn't warrant a property.\nUnfortunately, it's very easy to forego this recommendation, and instead bind in a host directory, and put persistent data created by and used by the container there. Take, for example, https://github.com/JensErat/docker-seafile, which recommends starting the container with /srv/seafile bind mounted to store its binaries:\nbash\ndocker run -t -i \\\n  -p 10001:10001 \\\n  -p 12001:12001 \\\n  -p 8000:8000 \\\n  -p 8080:8080 \\\n  -p 8082:8082 \\\n  -v /srv/seafile:/opt/seafile \\\n  jenserat/seafile -- /bin/bash\nIf you're going to write generic scripts to backup container volume data, as I was :], \"bind\" type volumes need to be checked, in addition to the docker volumes you can list via client.volumes.list().. ",
    "goldmann": "I agree with @realityone, setting chunk size to 1 is a bad idea IMHO. We deal with images that weight few GB. This is a regression for me and I've opened: #1905 to track this specific issue.. ",
    "renatoriccio": "Same problem here on Ubuntu Server when trying to pull an image.\nHere more information:\n```bash\ndocker==2.0.0\ndocker-py==1.10.6\ndocker-pycreds==0.2.1\nPython 2.7.12\nClient:\n Version:      1.12.4\n API version:  1.24\n Go version:   go1.6.4\n Git commit:   1564f02\n Built:        Tue Dec 13 00:08:34 2016\n OS/Arch:      linux/amd64\nServer:\n Version:      1.12.4\n API version:  1.24\n Go version:   go1.6.4\n Git commit:   1564f02\n Built:        Tue Dec 13 00:08:34 2016\n OS/Arch:      linux/amd64\nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=16.04\nDISTRIB_CODENAME=xenial\nDISTRIB_DESCRIPTION=\"Ubuntu 16.04.1 LTS\"\n```\nHere a snippet to reproduce the issue on my system:\n```python\n\n\n\nimport logging\nimport docker\nlogging.basicConfig(level=logging.DEBUG)\ncli = docker.from_env(assert_hostname=False)\nDEBUG:docker.auth.auth:Trying paths: ['/home/mkcroiser/.docker/config.json', '/home/mkcroiser/.dockercfg']\nDEBUG:docker.auth.auth:No config file found\ncli.images.pull(\"busybox\")\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/home/mkcroiser/.local/lib/python2.7/site-packages/docker/models/images.py\", line 256, in pull\n    self.client.api.pull(name, **kwargs)\n  File \"/home/mkcroiser/.local/lib/python2.7/site-packages/docker/api/image.py\", line 333, in pull\n    header = auth.get_config_header(self, registry)\nAttributeError: 'module' object has no attribute 'get_config_header'\n```\n. Hi @shin- , thanks a lot for your help it works again now!. \n\n\n",
    "xyalan": "I have same problem, @shin- Thanks.. ",
    "RonnyPfannschmidt": "@shin- i can replicate the issue.\ndocker-py was installed, pip install docker would use the wheel and not catch the error\n$ pip install docker-py docker\nwill succeed, afterwards i get \n```\nE               AttributeError: 'module' object has no attribute 'get_config_header'\n.../site-packages/docker/api/image.py:358: AttributeError\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nentering PDB >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n.../site-packages/docker/api/image.py(358)pull()\n-> header = auth.get_config_header(self, registry)\n(Pdb) auth\n\n(Pdb) auth.auth\n\n(Pdb) \n```\nits only fixable by uninstalling both packages and reinstalling docker\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nin order to protect users i suggest to move the check from setup.py to docker/__init__.py\nin modern times setup.py is not executed at all since pip uses wheels if possible and aggressively caches wheels locally as well. check for a docker folder in your site packages after uninstalling everything, if its still there its the cause\npart of this issue is partial metadata corruptio. @IoTPlay docker-compose depends on docker, not on docker-py - so installing docker-py beforehand will ensure a broken install in all cases,\nplease skip docker-py. @IoTPlay im glad my suggestion helped, please report this to ansible itself, perhaps linking this issue\nnote that there may be a unintended legacy requirement (changed apis between docker and docker-py)\nin any case it would help if that mismatch was documented in a more discoverable fashion. ",
    "adiroiban": "So, I fixed this by removing the leftover files after an upgrade \nrm -rf YOUR-PYTHON/site-packages/docker/auth/\nI have upgraded from 1.6 to 2.4.2\n\nLong story here\nI got this error when I try to run a container for an image which does not exists.\nhere is the test_docker.py code\nimport docker\nclient = docker.DockerClient(\n    base_url='tcp://localhost:2375',\n    version='1.24',\n    timeout=2,\n    tls=False,\n    )\nclient.containers.run(\n    image='no-such-image:exp',\n    detach=True\n    )\nand here is the output\n$ ./build-ubuntu1604-x64/bin/python test_docker.py \nTraceback (most recent call last):\n  File \"test_docker.py\", line 10, in <module>\n    detach=True\n  File \"/home/adi/chevah/server/build-ubuntu1604-x64/lib/python2.7/site-packages/docker/models/containers.py\", line 699, in run\n    self.client.images.pull(image)\n  File \"/home/adi/chevah/server/build-ubuntu1604-x64/lib/python2.7/site-packages/docker/models/images.py\", line 273, in pull\n    self.client.api.pull(name, tag=tag, **kwargs)\n  File \"/home/adi/chevah/server/build-ubuntu1604-x64/lib/python2.7/site-packages/docker/api/image.py\", line 358, in pull\n    header = auth.get_config_header(self, registry)\nAttributeError: 'module' object has no attribute 'get_config_header'\nI done some debugging and i got\n(Pdb) auth.__file__\n(Pdb) '/home/adi/chevah/server/build-ubuntu1604-x64/lib/python2.7/site-packages/docker/auth/__init__.pyc'\ndir(auth)\n(Pdb) ['INDEX_NAME', 'INDEX_URL', '__builtins__', '__doc__', '__file__', '__name__', '__package__', '__path__', 'auth', 'encode_header', 'load_config', 'resolve_authconfig', 'resolve_repository_name']\nWhile checking the upstream code for release 2.4.2 https://github.com/docker/docker-py/tree/2.4.2-release/docker  I saw that there is no auth package... only a module.\nSo, it looks like the trouble is from a bad upgrade.\nManual cleanup and all is good :)\n\nMy versions\n```\ndocker==2.4.2\ndocker-py==1.10.6\ndocker-pycreds==0.2.1\nPython 2.7.12\nClient:\n Version:      1.12.6\n API version:  1.24\n Go version:   go1.6.2\n Git commit:   78d1802\n Built:        Tue Jan 31 23:35:14 2017\n OS/Arch:      linux/amd64\nServer:\n Version:      17.06.0-ce\n API version:  1.30\n Go version:   go1.8.3\n Git commit:   02c1d87\n Built:        Fri Jun 23 21:19:04 2017\n OS/Arch:      linux/amd64\n```\n. ",
    "IoTPlay": "Hi, is this still an open bug since Dec 2016? I have same issues, I logged this issue, ansible/ansible#34851, but ansible closed it and referred my back to this issue. I have no clue how to move forward, is docker still working on this issue? Or is it an ansible issue?. Since this is one of the few that is still open, and since my issue on Ansible have been closed and pointed here, my details:\nAnsible on Mac - the Controller, RPi3 with Stretch is the 'Managed Host', pip versions are:\n$ pip freeze\n| Mac - The Ansible Controller                             |\n|---------------------------------------------- |                        \naltgraph==0.10.2\nansible==2.4.2.0\nasn1crypto==0.24.0\nbackports.ssl-match-hostname==3.5.0.1\nbcrypt==3.1.4\nbdist-mpkg==0.5.0\nbonjour-py==0.3\ncached-property==1.3.1\ncertifi==2017.11.5\ncffi==1.11.4\nchardet==3.0.4\ncryptography==2.1.4\ndocopt==0.6.2\nenum34==1.1.6\nfunctools32==3.2.3.post2\nidna==2.6\nipaddress==1.0.19\nJinja2==2.10\njsonschema==2.6.0\nmacholib==1.5.1\nMarkupSafe==1.0\nmatplotlib==1.3.1\nmodulegraph==0.10.4\nnumpy==1.8.0rc1\nparamiko==2.4.0\npy2app==0.7.3\npyasn1==0.4.2\npycparser==2.18\npycrypto==2.6.1\nPyNaCl==1.2.1\npyobjc-core==2.5.1\npyobjc-framework-Accounts==2.5.1\npyobjc-framework-AddressBook==2.5.1\npyobjc-framework-AppleScriptKit==2.5.1\npyobjc-framework-AppleScriptObjC==2.5.1\npyobjc-framework-Automator==2.5.1\npyobjc-framework-CFNetwork==2.5.1\npyobjc-framework-Cocoa==2.5.1\npyobjc-framework-Collaboration==2.5.1\npyobjc-framework-CoreData==2.5.1\npyobjc-framework-CoreLocation==2.5.1\npyobjc-framework-CoreText==2.5.1\npyobjc-framework-DictionaryServices==2.5.1\npyobjc-framework-EventKit==2.5.1\npyobjc-framework-ExceptionHandling==2.5.1\npyobjc-framework-FSEvents==2.5.1\npyobjc-framework-InputMethodKit==2.5.1\npyobjc-framework-InstallerPlugins==2.5.1\npyobjc-framework-InstantMessage==2.5.1\npyobjc-framework-LatentSemanticMapping==2.5.1\npyobjc-framework-LaunchServices==2.5.1\npyobjc-framework-Message==2.5.1\npyobjc-framework-OpenDirectory==2.5.1\npyobjc-framework-PreferencePanes==2.5.1\npyobjc-framework-PubSub==2.5.1\npyobjc-framework-QTKit==2.5.1\npyobjc-framework-Quartz==2.5.1\npyobjc-framework-ScreenSaver==2.5.1\npyobjc-framework-ScriptingBridge==2.5.1\npyobjc-framework-SearchKit==2.5.1\npyobjc-framework-ServiceManagement==2.5.1\npyobjc-framework-Social==2.5.1\npyobjc-framework-SyncServices==2.5.1\npyobjc-framework-SystemConfiguration==2.5.1\npyobjc-framework-WebKit==2.5.1\npyOpenSSL==0.13.1\npyparsing==2.0.1\npython-dateutil==1.5\npytz==2013.7\nPyYAML==3.12\nrequests==2.18.4\nscipy==0.13.0b1\nsix==1.4.1\nurllib3==1.22\nwebsocket-client==0.46.0\nxattr==0.6.4\nzope.interface==4.1.1\n| RPi - the Managed Host.    |\n|--------------------------- |\nbackports.ssl-match-hostname==3.5.0.1\ncached-property==1.3.1\ncertifi==2017.11.5\nchardet==3.0.4\ncryptography==1.7.1\ndocker==2.7.0\ndocker-pycreds==0.2.1\ndocopt==0.6.2\nenum34==1.1.6\nfunctools32==3.2.3.post2\nidna==2.6\nipaddress==1.0.19\njsonschema==2.6.0\nkeyring==10.1\nkeyrings.alt==1.3\npyasn1==0.1.9\npycrypto==2.6.1\npygobject==3.22.0\npyxdg==0.25\nPyYAML==3.12\nrequests==2.18.4\nRPi.GPIO==0.6.3\nSecretStorage==2.3.1\nsix==1.11.0\ntexttable==0.9.1\nurllib3==1.22\nwebsocket-client==0.46.0\ndocker on the Managed Host:\ndocker --version\nDocker version 18.01.0-ce, build 03596f5\nAnsible on the Mac Controller:\nansible --version\nansible 2.4.2.0\nansible python module location = /Library/Python/2.7/site-packages/ansible\npython version = 2.7.10 (default, Jul 15 2017, 17:16:57) [GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.31)]\nPython on the RPi\npython --version\nPython 2.7.13\nWith the above setup, docker_image works, but docker_container does not work.\nThis is after I removed docker-py from the RPi, cleaned up after .../site-packages/auth, \nI also tried downgrading as per other suggestions:\npip uninstall docker docker-py ; pip install docker==2.0.1\nBut, sadly cannot get docker_container to work as well, still getting:\n\"Failed to import docker-py - No module named requests.exceptions. Try `pip install docker-py`\"\nBut if I pip install docker-py on the 'managed host' or remove it and install pip install docker this does not work.\nWhere to from here?\n. According to https://github.com/ansible/ansible/issues/20492, for Failed to import docker-py for docker_container module, now also tried one of @jpriron 's suggestions, also not working:\npip uninstall docker\npip uninstall docker-py\npip uninstall docker-compose\npip install docker-compose==1.9.0\nJust note, these suggestions dates back a year....\nAn further down, @jpiron suggests, unlike others, to uninstall docker and install docker-py again...\npip uninstall docker\npip install --ignore-installed docker-py\nSadly, ... still the same error.\nFurtherdown, @Appleman suggest: (on the 'managed host')\npip uninstall docker\npip uninstall docker-py\npip install docker\nBut sadly, same error.\nFurtherdown, @Appleman suggests to run the playbook with Python3, have no idea how to do this, will study it now...\nAnd I solved it by run playbook under python3.\nFurtherdown, @antoinetran clarifies, docker for python3, docker-py for python2.7, so back to the drawing board. \npip uninstall docker docker-py docker-compose\npip install docker-compose\nNope, still not working...., and versions are:\n$ pip freeze | grep docker\ndocker==2.7.0\ndocker-compose==1.18.0\ndocker-pycreds==0.2.1\ndockerpty==0.4.1\nWhat now?????. Thank you Ronny, should I then just delete this docker folder - will that\nbe enough ?\nI checked, pip uninstalled all docker* modules, none left in the folder\n~/.local/lib/python2.7/site-packages. Dear all, I resolved my controller on Mac, Target on Pi with a test playbook.\nI created a test bench from Mac to RPi with docker_login, docker_image, docker_container and docker_service. See it here:\nhttps://github.com/IoTPlay/docker-iotplay-standards/blob/master/10_Test_docker-with-ansible.yml\nAll my tests pasts, after recreated a Pi from scratch, instructions in the above link as well. I used the standard pi user, and not a user I created myself through ansible. I will not have to go check what I missed out for this created user.\nThe instructions in the above link, installs the stock standard python programs on the TARGET (pi) machine, not using sudo.\n$  sudo apt install python-pip\n$  pip install docker-py\n$  pip install docker-compose\nApologies for the 'noise' from me in this issue, maybe this can assist others.\nJ\u00e9an.. Ronny, your comment above is the most important statement of this -and ansible's multitude of issues logged. I wish they will update their software to not ask for the installation of docker-py, it cost me 3 weeks of investigations.\nplease skip docker-py\nThank you for your assistance.. ",
    "Howardbrooks1": "The Long path tool is the very best program for error, unlock solution.and it is very useful tool.. ",
    "numberz004": "For those on Windows 10 v1607 and later, the group policy value has been moved to Computer Configuration > Administrative Templates > System > Filesystem > Enable Win32 long paths. ",
    "fallphenix": "Run docker-compose with Docker Quickstart Terminal. It will solve your error. ",
    "sanchitbh": "Trying to run docker-compose and I keep getting this error:\nc:\\tmp>docker-compose up\nBuilding web\nTraceback (most recent call last):\n  File \"docker-compose\", line 6, in <module>\n  File \"compose\\cli\\main.py\", line 68, in main\n  File \"compose\\cli\\main.py\", line 121, in perform_command\n  File \"compose\\cli\\main.py\", line 938, in up\n  File \"compose\\project.py\", line 430, in up\n  File \"compose\\service.py\", line 317, in ensure_image_exists\n  File \"compose\\service.py\", line 918, in build\n  File \"site-packages\\docker\\api\\build.py\", line 148, in build\n  File \"site-packages\\docker\\utils\\build.py\", line 14, in tar\n  File \"site-packages\\docker\\utils\\utils.py\", line 100, in create_archive\n  File \"tarfile.py\", line 1881, in gettarinfo\nWindowsError: [Error 3] The system cannot find the path specified: u'c:\\\\tmp\\\\var\\\\node_modules\\\\axiom\\\\node_modules\\\\npm\\\\node_modules\\\\pacote\\\\node_modules\\\\make-fetch-happen\\\\node_modules\\\\http-proxy-agent\\\\node_modules\\\\agent-base\\\\node_modules\\\\es6-promisify\\\\node_modules\\\\es6-promise\\\\lib\\\\es6-promise\\\\promise\\\\all.js'\nFailed to execute script docker-compose\nVersions:\n```\nc:\\tmp>docker -v\nDocker version 17.09.0-ce, build afdb6d4\nc:\\tmp>docker-compose -v\ndocker-compose version 1.16.1, build 6d1ac219\n```\nI guess this is because of MAX_PATH limit on Windows for which I've tried enabling the Enable Win32 long paths but I still keeping getting this error. \nAny ideas on how I can fix this? In the meantime is there an option to tell docker-compose to ignore this file and run the rest of it anyway?. ",
    "mbiebusch": "@sanchitbh Could you fix or workaround the problem in the meantime?. ",
    "mmalek06": "Guys, has anything changed? It's been more than a year and the problem still persists.... Well, actually that's not enough. I did it, but still the gpedit window says that in order for this to work, the app has to be manifested. In other words it has to tell windows it wants to use this feature. Docker is not manifested, therefore the issue persists.. ",
    "Puneeth-n": "I am getting it on my Jenkins server running on ubuntu.\n```\n+ make -C python_lib build\nmake: Entering directory `/var/lib/jenkins/workspace/PR-3284/python_lib'\nBuilding ct-python-lib\nTraceback (most recent call last):\nFile \"bin/docker-compose\", line 6, in \nFile \"compose/cli/main.py\", line 68, in main\nFile \"compose/cli/main.py\", line 121, in perform_command\nFile \"compose/cli/main.py\", line 249, in build\nFile \"compose/project.py\", line 343, in build\nFile \"compose/service.py\", line 918, in build\nFile \"site-packages/docker/api/build.py\", line 148, in build\nFile \"site-packages/docker/utils/build.py\", line 14, in tar\nFile \"site-packages/docker/utils/utils.py\", line 100, in create_archive\nFile \"tarfile.py\", line 1881, in gettarinfo\nOSError: [Errno 2] No such file or directory: '/var/lib/jenkins/workspace/PR-3284/terraform/services@tmp/durable-a9021a03'\nFailed to execute script docker-compose\nmake: *** [build] Error 255\nmake: Leaving directory `/var/lib/jenkins/workspace/PR-3284/python_lib'\nscript returned exit code 2\n```. ",
    "robertlamberson": "Still an issue with docker-compose on windows 10 build 1803.  Group policy Enable win32 long paths and registry setting long paths both enabled.   Please provide a work around \n[8180] Failed to execute script docker-compose\nTraceback (most recent call last):\n  File \"docker-compose\", line 6, in \n  File \"compose\\cli\\main.py\", line 71, in main\n  File \"compose\\cli\\main.py\", line 127, in perform_command\n  File \"compose\\cli\\main.py\", line 282, in build\n  File \"compose\\project.py\", line 373, in build\n  File \"compose\\service.py\", line 1027, in build\n  File \"site-packages\\docker\\api\\build.py\", line 154, in build\n  File \"site-packages\\docker\\utils\\build.py\", line 31, in tar\n  File \"site-packages\\docker\\utils\\build.py\", line 162, in create_archive\n  File \"tarfile.py\", line 1802, in gettarinfo\nFileNotFoundError: [WinError 3] The system cannot find the path specified: . Shin, I confirmed the work around of installing compose via pip and removing the earlier installed docker-compose resolved my build failures.  Thanks . ",
    "BrunoZell": "adding node_modules to your .dockerignore can solve this issue too possibly. ",
    "james-knott": "dockerpty does not work anymore. Any other way to connect interactively with python?. I was able to get it to work for my application. Thank you. ",
    "rycus86": "@approxit this should now be done in https://github.com/docker/docker-py/pull/1965, thanks to @shin- . Hi @shin- ,\nSorry, had some trouble with the tests but should be OK now hopefully.\nOne thing I'm unsure of: this change technically enables specifying networks on both the TaskTemplate and on the API client call. Should I check this in the _check_api_features?\nIn all honesty, if this PR goes through, I'd like to have a look at the Service.update method to allow only specifying attributes we want to change and just read the rest from the previous spec - if you think that's a good idea. If yes, I could just do this check there along with some others perhaps.\nThanks!. This change became part of https://github.com/docker/docker-py/pull/1807, please have a look at that one instead.\nThanks!. Thank you very much for the review @shin- !. Thank you for looking at it!\nI see your point, it might be a bit painful to keep these in sync with the API changes. I have a couple of projects using docker-py and this would have come in handy I thought. :)\nOne alternative I could think of would be making it easier to add/maintain the properties using some Python-magic class modification so you'd only have a mapping of some sort to update.\nI'm happy to code it up to see how that would work, otherwise I'm OK to get this closed - it's more of a convenience for users than something they'd actually need.. Thanks for having a look at this!\nI've moved the new flag to the model only and left the API client unchanged. I only added it there because of https://github.com/docker/docker-py/issues/1775#issuecomment-338863313 but it makes sense to leave the API client as it was.\n@thaJeztah thanks for letting me know!\nIt looks like the behavior has changed in API 1.28, added a check for it and won't inspect the services if the client is on a lower version.. Hi @shin- ,\nSorry I haven't done reviews on GitHub yet, is there a link/button somewhere to update that I've made the changes that were requested?\nThank you!. Right, sorry, forgot about Thanksgiving! :)\nEnjoy your break!. Thanks a lot for the suggestions!\nSure, I can add the iterator methods to the new class, we'll still need to keep a reference of the returned object though, so it would be something like:\n```python\nevents = client.events(cancellable=True)\nfor event in events:  # we're blocked here\n  print event\nthen cancel with:\nevents.close()\n```\nAlso, for logs and attach and others, it makes complete sense, I could make this a generic cancellable iterator so we could plug it in more places.. I think I've got some unrelated test failures on Jenkins. :(\n22:39:32 [py3.6_17.06.2-ce] tests/integration/api_container_test.py::AttachContainerTest::test_attach_no_stream FAILED. OK, I think I've got all the changes you requested (with the exception of close).\nIf all looks good, would you want me to add the CancellableStream in the other places you mentioned as part of this PR?. Hi @shin- \nLet me know what you think of adding the pytest-timeout module, I think the tests have failed because one of them had to os._exit(1) on timeout.\nHaven't really worked out how to make shutting the socket down more reliable on 2.7, though seem 10 out of 10 on 3.x\nThanks!. No worries! :)\nThanks for coming back to it!. I think I've got some unrelated test failures on Jenkins:\n22:45:48 [py3.6_17.12.0-ce] tests/integration/api_swarm_test.py::SwarmTest::test_init_swarm_with_ca_config FAILED. Thanks again, @shin- !\nI have renamed the method to force_update and changed the docstring to be the same (or very similar to) what the engine uses as help strings.\nI've also snuck in another change: the force_update parameter on the service model's update method now accepts a bool, and if it is True then it does the counter increment logic. I thought it makes more sense to have it there, so you can just say service.update(force_update=True, **other_kwargs) and it's more inline with the docker service update --force flag being a boolean flag.\nIt would be great if you could give the change another look because of these extras. Thank you!. Sure, it's done now, thanks a lot!. Awesome, thanks!\nI'll have a quick look at it in the evening.. I've run it on Python 2.7 and 3.6 a good few times and couldn't make it fail.\nThanks a lot again for taking this forward!\nLGTM. I think you meant this, right?\npython\ninspect_defaults = True\nif utils.version_lt(self._version, '1.29'):\n  inspect_defaults = None. No, unfortunately, it doesn't interrupt the blocking socket read.\nFrom what I could find out, it looks like we need the socket.shutdown(SHUT_RDWR) call to do that fo us.. This seems to help in Python 2.7 somewhat, but for container.attach(..) the test is still a bit flaky...\nIt fails around once in every 10-20 runs, sometimes it's more often though. :/. To avoid this, we could bring in the pytest-timeout module, if that sounds better?. This test can potentially cause some problems, seems flaky on Python 2.7 (fine on 3.5).. ",
    "Mr-Linus": "I hope that developers could design a static events dict attrs for us to use.. ",
    "saifulhoque26": "In model/swarm.py, the client.api.init_swarm(**init_kwargs) method is being called without advertise_addr. That's where the problem is. If there is multiple interface advertise_addr  (or interface) should be given.\ninit_kwargs = {}\nprint kwargs\n    for arg in ['advertise_addr', 'listen_addr', 'force_new_cluster']:\n        print arg\n        if arg in kwargs:\n            print arg\n            init_kwargs[arg] = kwargs[arg]\n            print kwargs[arg]\n            del kwargs[arg]\ninit_kwargs['swarm_spec'] = SwarmSpec(**kwargs)\nprint init_kwargs\nself.client.api.init_swarm(**init_kwargs)\nself.reload()\nI solved it locally, by just adding 2 parameters:\nself.client.api.init_swarm(advertise_addr, listen_addr, **init_kwargs). ",
    "qazbnm456": "@shin- Sure!. @shin- Ok, I will add a test later. \ud83d\ude03 . @bfirsh Should be okay now. :). @narenst Actually it's there in docker-py 2.0.0 and 2.0.1.\nScreenshot from docker-py (should be renamed to docker) inspect_container low level API:\n. @shin- Changed it, thanks!. @feliperuhland Thank you. \ud83d\ude04 . I've built a project mimicking the behavior of Docker CLI with docker-py, and you may check out how I handle these commands: qazbnm456/tsaotun#tsaotun/lib/Docker/Container.\nI think you should focus on exec related commands which I also take advantage of d11wtq/dockerpty.. Good catch, thanks!. Nice, I'll update it.. ",
    "pipozzz": "services = self.client.services.list(filters={'name': 'test'})\nand getting all services instead of service with name test. ",
    "LordNewt": "Apologies, the reauth is working.. ",
    "narenst": "@JordanSussman I'm running into the same issue, did you happen to figure out the cause?. Thanks @adeslade. Turns out I had a previous installation of docker-py 1.10.6 installed in the same virtualenv. It was not enough to just uninstall it, I had to recreate my env. It worked after that.. Thanks @shin- and @qazbnm456 for the help. I ended up using docker 1.10.6. Below is the snippet:\n```\nIn [9]: from docker import Client\nIn [10]: client = Client()\nIn [11]: client.inspect_container(container=\"7f1aa27309769655e581ef5c9af88a3b514eee62390df0297148fd7c910ac865\")['State']\nOut[11]:\n{'Dead': False,\n 'Error': '',\n 'ExitCode': 0,\n 'FinishedAt': '2017-01-02T22:51:27.150992603Z',\n 'OOMKilled': False,\n 'Paused': False,\n 'Pid': 22764,\n 'Restarting': False,\n 'Running': True,\n 'StartedAt': '2017-01-03T00:02:48.612091619Z',\n 'Status': 'running'}\n```. ",
    "adeslade": "@narenst Looks like there was a file named docker.py which was being imported instead of the docker library.. ",
    "aftabnaveed": "For me the main fail was the same as  the main docker module. Renaming it worked for me.. ",
    "dzimine": "Hey @GordonTheTurtle the robot, commit is signed and amended, as requested. . Hi folks, happy new year; \nhow can I remove 'dco-signed' label now that the commit is signed? \nAnything more I have to do to have this reviewed? . Sorry I missed the memo. Will update in a separate pull-request (I want to add a new service mount syntax support parser so I'll fix the tests once I get to it). Thanks! . ~~If you're OK with the idea, say so; I'll complete PR + address any comments.~~\nReady to go.. Sorry, was distracted, will take a look. . ",
    "nicoladj77": "I've got the same issue, docker installed with an installer from their website. ",
    "SaintNazaire": "Same problem here.\nConfiguration:\n\nWindows 10\nAnaconda 2-4.4.0\nDocker toolbox install, version 17.04.0-ce, build 4845c56. \n",
    "ivanjacobs": "problem persists.\ninstalled with Docker Toolbox. ",
    "pommetjehorlepiep": "Hack to get around the problem:\ncatch requests.exceptions.ReadTimeout exception and call container.kill()\nIf you don't call kill() container still seen as running. Broad steps on how to reproduce\n\nInitial image:  Debian 8 Jessie. Used debootstrap to create an image, imported the tar file into docker\nUsing apt-get install apt-cacher-ng in a container using the initial image\nCommit the container with apt-cacher-ng as new image (repo:tag: apt-cacher-ng:base)\nCreate a container using image apt-cacher-ng:base\n\nclient = docker,from_env()\nclient.containers.create(\n  image='apt-cacher-ng:base'\n  ,command='/usr/local/bin/runner' \n  ,detach=True\n  ,environment=['ADMIN_PWD=password']\n ,hostname='apt-cacher-ng'\n  ,labels=['apt-cacher-ng']\n  ,name='apt-cacher-ng'\n  ,network_mode='bridge'\n  ,ports={'3142/tcp' : 3142 }\n  ,tty=False\n  ,user='apt-cacher-ng:apt-cacher-ng'\n  ,volumes={ '/docker/images/apt-cacher-ng/containers/apt-cacher-ng' : { 'bind': '/mnt/scripts', 'mode':   'rw' }}\n  )\n/usr/local/bin/runner:  A shell script which loads some library functions and then calls a script in /mnt/scripts which runs apt-cacher-ng in the foreground using exec\n\nOnce container created, start container using docker.from_env().containers.container.start()\nCheck container up and running\nNow try to stop the running container\nFind the running  container using containers.find('apt-cacher-ng')\nOnce found, do ..containers.container.stop()\nException is thrown after 10 seconds. Using docker stop apt-cacher-ng works fine. No problems here.. \n\n\n",
    "cjh1": "I am seeing this as well. @shin- I can confirm that this also resolves me issue.. @shin- Any idea when the next release will be? Thanks. . Awesome, thanks for helping me get this in.. Sure, I agree, I will test out your changes.. This is a much nicer approach, thanks.. ",
    "AnthonyDeroche": "Hello, I'm experiencing the same issue with docker-py 2.3.0.\nHave you planned to have a look at this soon? I dont know if the docker cli is using the REST API as well, but I do not encounter the problem with the cli.\nRegards,\nAnthony. ",
    "ah19": "I have run into a similar problem with myself -- try running container.reload(). I think this is the same problem that I was about to open an issue for -- the documentation for running a container does not mention that Container.attrs is cached (specifically in the detach=True example). The Container.reload method is not mentioned in the documentation (nor is wait by the way), is this an oversight or should it not be called?\nmy code for running a command in two docker images ended up looking like this:\n```python\ndef run_comparison_cmd(old_image, new_image, command, block=True):\n    print(\"Running command '{}' in containers {}, {}\".format(command, old_image, new_image))\n    client = docker.from_env()\n    old = client.containers.run(old_image, command, detach=True)\n    new = client.containers.run(new_image, command, detach=True)\nif block:\n    print(\"Containers started, waiting for them to finish\")\n    old.wait()\n    new.wait()\n\n    #attributes are cached, update them\n    old.reload()\n    new.reload()\n\nreturn old, new\n\n```\nshould the wait method on Container be updated to perform a reload before returning?. ",
    "lgilz": "I think this is a serious Issue as Container.create returns a container-object. And I'm especially interested in its ExitCode (attrs['State]['ExitCode'], But this always remains 0, even if the container is exited with a non-zero ExitCode. \nEven its status remains unchanged ('created') instead of ('exited').\nI think, that the container-object you get back from containers.create() is actually nothing else than the response of the docker-api and does not follow the changing state of it's container. With the package, its like calling an api and you have to call client().containers.get(container.id) again, if you want to have updated information\nHowever, this should definitely be mentioned in the Documentation more clearly - as the description of a container-object is completely misleading!. I had the same problem on my ubuntu 17.04. \nI solved it by installing via apt get\nsudo pip uninstall docker docker-compose\nsudo apt-get update\nsudo apt-get upgrade\nsudo apt-get install docker docker-compose. ",
    "felicitychou": "I meet this problem too. With the help of @ah19 , I solve it.  \nAt the same time, I agree with @lgilz. . ",
    "cdaringe": "man, this was painful.  thanks all.  . actually, reload doesn't get the 'HostConfig' up-to-date.  i still do not see my exposed ports. ",
    "mygoda": "@shin- . @shin- ok  i already use in your method\u3002thanks\u3002i have another question. just i want to scale service 1 to 5. but i can found in docker-py, i use update_service method can not do yet\u3002what can i do\u3002. ",
    "wannabesrevenge": "Thanks!. ",
    "RobbieClarken": "I believe there is also a bug with named volumes not being supported by DockerClient.containers.run:\n```\n\n\n\nprint(docker.version)\n2.0.1\nclient.containers.run('alpine', 'ls', volumes={'my-named-volume': {'bind': '/data', 'mode': 'ro'}})\nTraceback (most recent call last):\n[...]\ndocker.errors.APIError: 500 Server Error: Internal Server Error for url: http+docker://localunixsocket/v1.24/containers/create\n(\"Invalid volume spec \"my-named-volume\": Invalid volume destination path: 'my-named-volume' mount path must beabsolute.\")\n```\n\n\n\n(The volume had already been created with docker volume create --name my-named-volume.) . ",
    "milk4candy": "I had the same problem that @RobbieClarken  mentioned above. And I solved it by doing a tiny modification on the line 882 in docker/models/containers.py.(on branch 2.0.2-release)\nThe original line of codes is\ncreate_kwargs['volumes'] = [v.split(':')[0] for v in binds]\nAnd I change it as below:\ncreate_kwargs['volumes'] = [v.split(':')[1] for v in binds]\nWhich I believe might be a bug to fix because the definition of volume parameter for docker engine api is\n\nAn object mapping mount point paths inside the container to empty objects.\n\nHence, the volume parameter should be mapping to v.split(':')[1] instead v.split(':')[0] because former one means mount point paths inside the container and later one means host path or volume name.. ",
    "bestksw": "@milk4candy It works on docker-py version 2.0.2! Thanks a lot.\n. ",
    "ariaviran": "I also encountered this issue.\nIt appears there is no high-level API for this. ",
    "roidelapluie": "Opened PR #1495 . I am not. Feel free to move forward :). ",
    "mitar": "Hm, but how to get exit code if do want a stream? It seems, from reading the code, that currently you can only get exit  code without a stream?. ",
    "thomaschaaf": "I'm going to try and create a pr for this now.. Should be done :) @shin- . Is there a problem with my addtion?. ",
    "MichaelPereira": "@thomaschaaf To save you the search, this is where it should be added: https://github.com/docker/docker-py/blob/bb665257ed10239e9846c94151a154e044f22681/docker/models/images.py#L111-L142. ",
    "matthanley": "From the Docker API code, the workaround is to pass a dict:\nmode={\"global\": {}}. ",
    "gpolaert": "Could someone help me to understand?. ",
    "danqing": "Run into a similar problem. @TomasTomecek can you explain the difference between the two, and also what's the difference between stream and follow? What happens if you specify one or both? Why does stream not follow? Does follow automatically stream? What is returned when both are specified?. Yeah the source is pretty confusing too :( I've hacked a solution, but it looks like the logging part of this library still needs some significant improvements to work reliably.. This seems to be the function. I don't see how stream plays any role here. It delegates to the low-level API but whether stream or not doesn't seem to affect how this method itself behaves. It intercepts the stream and does not return until the stream is over (effectively making it non-stream), iiuc.. anyone here? if this is an official project then the lack of support (or response) is a bit astonishing.. So instead of \"always true\" it's actually \"always false\", correct?. Thanks!. So I struggle to get the relation between TTY and stream/follow. When I turn off TTY, I don't seem to get anything (it's stuck forever) - not sure if that's the correct behavior or not.\nIt seems to me that if I have stream=True but follow=False, the iterator still waits for future results, behaving like \"follow\". That is, I found whether I follow or not the behavior seems to be the same.\nI couldn't come up with a minimal example that hangs, but I do have one additional piece of info that may be helpful: if I do a while loop first that's not streaming, waiting for the first bit of log to show up, and only then jump out of the loop and start streaming, then it will work correctly.\nI don't know how docker manages the underlying file, but there's a chance that streaming didn't find the file/it didn't exist until the first line of log is generated.. For the hanging logs, it seems that if no new logs come out in a minute or two it will start to hang and not recover. Logs need to constantly come out in order for streaming to work.. any suggestions as of what to do when stream hangs?. @shin- apparently the issue is that the connection times out after a minute or so. Can we have custom timeouts for these APIs? When there's no log produced for a few minutes the connection becomes broken and no logs can go through.. Friendly ping @shin- . Ping again @shin- . nano_cpu doesn't seem to be mentioned anywhere on the page.. Also, maybe I didn't find it, but is it possible to \"run container on any 2 of the X CPUs\"?. What does \"counterpart\" mean? And cpus (cpu-period, cpu-quota) only mentions 1 CPU scenario. What happens when there are more CPUs?\nAnd is there an option to assign CPU numbers, not specific CPUs? Say I have two containers, each wanting 2 cores. Do I need to assign specific cores (1,2; 3,4) to them, or is there a way to tell them to use 2 cores each?\nThanks!. ",
    "aboutlo": "I have the very same problem, I rolled back to 1.10.5 to fix the issue. thank you @shin- for the explanation. \nDoes it mean we can use docker-compose without docker-py? \nCurrently, I'm installing docker via an ansible role. And as you can see there is an open issue there as well https://github.com/angstwad/docker.ubuntu/issues/135\nSo if we could rid off docker-py it could be simpler to have a proper fix. \n. I double check here: https://github.com/docker/compose/blob/master/requirements.txt\ndocker-py doesn't seem to me a docker-compose dependency. \nSo I'm not getting how to install the last versions of docker docker-compose docker-py into a linux box. . ",
    "serialdoom": "apparently my proposed fix doesnt quite work as its breaking docker-py.\ninstead of @aboutlo sulution, downgrating docker-compose to 1.9.0 also fixes the problem. ",
    "james-stephenson": "For those coming here because they are investigating this issue as a result of using ansible, you can no longer install docker-compose via pip and also use the ansible docker module together, which requires docker-py.\nOur workaround was to not use the ansible docker module (unfortunately), and instead use docker directly.. The specific case in which this failed was just due to pulling docker images onto the machine during our AMI generation.  This use case is rather specific to how we're using ansible.  ansible hadn't supported a docker pull module in earlier versions, and if I recall correctly there were also some issues with ansible's Docker module incorrectly comparing versions lexically rather than numerically (I would have to dig that PR up to remember why).\nNeedless to say, we've had a few issues here and there with ansible's docker support, so we had the docker pull code ready to go as a remediation from previous PRs.  It ends up being no more complex than using the docker module, so for us it doesn't actually add any tech debt despite the fact that it may sound radical.\nWe are entertaining the idea of using the docker-compose binary, but it's actually easier to manage the installation of docker-compose using pip right now in our ansible configuration.  Hope that helps clarify our use case and why we went this route for the time being.. ",
    "isaacweathers": "Old but fixed for this issue:\n1.  pip install --upgrade pip\n2.  pip install -e 'git+https://github.com/shin-/compose.git@1.6.2#egg=docker-compose'\n3. pip uninstall docker-py\n4. sudo pip install docker-py\nThis seemed to work for me on Mac. . ",
    "dj-wasabi": "@dzimine Can you take a look at the merge issues? I really would like to mount directories with my services.. @shin- Thanks! I've got it working now. . ",
    "appendjeff": "@shin- I appreciate the quick response. From the documentation, I saw stream as a kwarg.\n\nstream (bool) \u2013 Deprecated for API version > 1.8 (always True). Return a blocking generator you can iterate over to retrieve build output as it happens\n\nAlthough deprecated, it seems like an always True value would always return a generator that I can iterate over. I ultimately want to stream the logs over a socket as the image is building. Is there a better way to do this than my sample code?. My mistake, the low-level build method does exactly what I want. Thanks for the clarification.. @shin- can you review this tiny change?. ",
    "is-unix": "I recently ran into this issue, however the reason was not because the docs were wrong, but because I had previously installed docker-py, I'm assuming as a pre-requisite of docker-compose and it was conflicting with the newer docker pip package. I discovered this after I ran into further issues when pushing the image (see this issue)\nPrior to removing docker-py, the following code would throw the error TypeError: 'Image' object is not iterable. Now that it has been removed and replaced with the docker pip package, all is well.\n```python\nimport docker\nclient = docker.from_env()\ni,log = client.images.build(path=cwd)\nfor line in log:\n    print line\n```. ",
    "leroygr": "Same issue for me with version 2.0.1. ",
    "chrisv2": "updated the PR with a much simpler solution (which happens to work, actually). okay, that breaks PortsTest.test_split_port_invalid:\n```\n=================================== FAILURES ===================================\n____ PortsTest.test_split_port_invalid _____\ntests/unit/utils_test.py:558: in test_split_port_invalid\nlambda: split_port(\"0.0.0.0:1000:2000:tcp\"))\n\nE   AssertionError: ValueError not raised\n========== 1 failed, 451 passed, 3 skipped, 1 xfailed in 2.82 seconds ==========\nERROR: InvocationError: '/home/travis/build/docker/docker-py/.tox/py27/bin/py.test --cov=docker tests/unit'\n```\nI think we should change the test in this case, as it tests for an odd corner case which would be somewhat difficult to satisfy, with no apparent advantage. Opinions?. @lenovouser We can't pass v6 IP in square bracket ([x:y::z]) notation to docker (while the engine accepts it, it still listens on 0.0.0.0). So we had to remove them in our code, which would of course be possible, but lead to Python API using a different notation than docker itself, which seems a bad idea to me.\nI still believe that the unit test should be changed. . ",
    "lenovouser": "Maybe use the [1234:1234:1234:1234::]:22:22 format?. Any progress?. Hello? \ud83d\ude1e . @chrisv2 I understand and don't mind that in particular. I rather want this to be merged so that I can use docker-compose with IPv6 \ud83d\ude06 . Can this be merged?. \ud83d\ude1e @shin- do you know who needs to review this? I really need https://github.com/docker/compose/issues/2663. @SethMichaelLarson there are questions to be resolved by the maintainers before any further changes can be made. @chrisv2 explicitly stated that:\n\nI think we should change the test in this case, as it tests for an odd corner case which would be somewhat difficult to satisfy, with no apparent advantage. Opinions?. @SethMichaelLarson I understand what you mean and I would do that if there was something to change and then create a PR. But the info if or what to change needs to come from the maintainers. I can't read peoples minds - my guess is that they just haven't even seen this PR and/or that it got lost in all the others.. \n",
    "sethmlarson": "@lenovouser Remember that most people in Open Source are volunteers and we need to respect their time. There are failing tests that haven't been fixed in the CI. If you want to expedite the merge of this change perhaps you can open your own PR with this change.. I'm aware that that is the test being mentioned. I'm suggesting that it may be more productive for you to open your own PR with the changes rather than pinging the same issue over and over if the change is really important or time-sensitive to you.. This is where the failure is: https://ci.appveyor.com/project/shin-/docker-py/build/master-319#L2382\nPerhaps there should be a python -m pip install -U pip in appveyor.yml somewhere?. Well in the .travis.yml we only ask for Python 3.5 so that's why there's no 3.3 available to us. That should be changed.\nIn regards to AppVeyor could you instead do python -m pip install -U pip instead of just pip install -U pip. Might be defaulting to upgrading only Python 2.7s pip.. ",
    "kaiyou": "After trying to wrap my head around the commits and why the tests fail, it appears to me that split_port and to_port_range could (should ?) be written differently. Generating a port range should somehow be at a lower level than deciding if ports in that range are TCP or UDP.\nAs I understand it, this is only used when creating containers, wouldn't it be simpler and safer to use regular expressions in that case?. Sorry about commit signatures, will fix that asap.. ",
    "ZelphirKaltstahl": "Will this still be done? Is there a reason this has been closed? Maybe some policy, which prevents upgrading it on PyPI?. @jocelynthode Thanks! I initially searched on PyPI for docker-py immediately before going to the GIT repository.. ",
    "jocelynthode": "@ZelphirKaltstahl : It was a mistake on my part. the pypy repository for docker-py changed as they now call it \"docker\" so you have to use the docker repo on pypy and install \"docker\" from pip to get the 2.0 version as it is writtent in the README.md. Here is the repo.\nI think there should still be a redirection of some sort on the docker-py repository of pypy to point to the new version.. ",
    "bayazee": "I think the failure is not related to my changes.. ",
    "elacheche": "Thanks fr the reply\nWhy when I use the list() method  it returns only the containers IDs, if we compare it with docker ps:\n\ndocker ps --quiet=false returns id + name + other info\ndocker ps --quiet=true returns id only\n\nThe quiet arg in here is always False, so why it returns only the ID, also why if quiet: in there, is there other methods that use that and can change it to True? because the list() method can't do that.. . Thank you.. My bad, didn't understand well what was written in the docs... ",
    "vmindru": "@GordonTheTurtle  what is the dco/no label? . I think Jenkins failure is a bit strange, give both the error message and the type of changes this patch brings. \nI guess this Jenkins tests  will simply fail with docker=1.12.0  \nVM  . Hold on :) one would have no idea that he can pass an iterable object instead of simple bytes. One would have to check the source code find out you use requests in the background and figure out that he can pass an iterable object, maybe you folks give it a second thought before you close this?\n. ",
    "ns-cweber": "I came here specifically to figure out why put_archive() claims that it only supports bytes when it apparently supports iterable objects too? I specifically thought it was curious since the get_archive() docs say it returns a stream of tar data.. ",
    "niels-bom-md": "Ah hell yeah that was confusing \ud83d\ude15.\nI contacted the PyPI package's maintainer, let's hope he responds :-)\nhttps://twitter.com/niels_bom/status/850361509800800256. ",
    "felixfontein": "@shin- I don't see any such remark on the docker-py pypi page. Maybe it got lost somewhen?. Actually, this fixes APIClient.inspect_node, which has been unusable from 1.10.0 on until 2.4.0 (when this fix has been first included) because it always errored with image or container param is undefined.. ",
    "mathiasimmer": "Was affected by this bug/unexpected usage today but @shin-'s suggestion worked.\n````\nimport docker\nclient = docker.from_env()\nnetwork = client.networks.create(\"mynetwork\")\nExpected to work but does not\nc1 = client.containers.run('alpine', 'echo hello', detach=True, name=\"c1\", networks=[network.name])\nc2 = client.containers.run('alpine', 'echo hello', detach=True, name=\"c2\", networks=[network.name])\nDoes work\nc1 = client.containers.run('alpine', 'echo hello', detach=True, name=\"c1\", network_mode=network.name)\nc2 = client.containers.run('alpine', 'echo hello', detach=True, name=\"c2\", network_mode=network.name)\nnetwork.reload()\nprint network.containers\n````. ",
    "Tjorriemorrie": "Why is this closed? How is networks supposed to be used? The documentation should be updated for both params, as this is not clear in any way.. ",
    "abdelrahmanhosny": "So, ['NetworkSettings']['Ports'] is what is generated when using the docker inspect, and yes, it shows the port binding.\nHere is the container.attrs:\n```\n{\n   u'ExecIDs':None,\n   u'State':{\n      u'Status':u'created',\n      u'Pid':0,\n      u'OOMKilled':False,\n      u'Dead':False,\n      u'Paused':False,\n      u'Running':False,\n      u'FinishedAt':      u'0001-01-01T00:00:00      Z',\n      u'Restarting':False,\n      u'Error':u'',\n      u'StartedAt':      u'0001-01-01T00:00:00      Z',\n      u'ExitCode':0\n   },\n   u'Config':{\n      u'Tty':False,\n      u'Cmd':None,\n      u'Volumes':None,\n      u'Domainname':u'',\n      u'WorkingDir':u'',\n      u'Image':      u'algorun/tophat:2.0.0',\n      u'Hostname':u'07a0e7a22175',\n      u'StdinOnce':False,\n      u'ArgsEscaped':True,\n      u'Labels':{  \n  },\n  u'AttachStdin':False,\n  u'User':u'',\n  u'Env':[  \n     u'PATH=/home/algorithm/src/tophat:         /home/algorithm/src/bowtie2:         /usr/local/sbin:         /usr/local/bin:         /usr/sbin:         /usr/bin:         /sbin:/bin',\n     u'CODE_HOME=/home/algorithm'\n  ],\n  u'ExposedPorts':{  \n     u'8765/tcp':{\n\n     }\n  },\n  u'OnBuild':None,\n  u'AttachStderr':False,\n  u'Entrypoint':[  \n     u'/usr/bin/nodejs',\n     u'/home/algorithm/Server.js'\n  ],\n  u'AttachStdout':False,\n  u'OpenStdin':False\n\n},\n   u'ResolvConfPath':u'',\n   u'HostsPath':u'',\n   u'Args':[\n      u'/home/algorithm/Server.js'\n   ],\n   u'Driver':u'aufs',\n   u'Path':u'/usr/bin/nodejs',\n   u'HostnamePath':u'',\n   u'RestartCount':0,\n   u'Name':u'/tender_chandrasekhar',\n   u'Created':   u'2017-02-06T21:22:21.834394115   Z',\n   u'GraphDriver':{\n      u'Data':None,\n      u'Name':u'aufs'\n   },\n   u'Mounts':[  \n],\n   u'ProcessLabel':u'',\n   u'NetworkSettings':{\n      u'Bridge':u'',\n      u'Networks':{\n         u'bridge':{\n            u'NetworkID':u'',\n            u'MacAddress':u'',\n            u'GlobalIPv6PrefixLen':0,\n            u'Links':None,\n            u'GlobalIPv6Address':u'',\n            u'IPv6Gateway':u'',\n            u'IPAMConfig':None,\n            u'EndpointID':u'',\n            u'IPPrefixLen':0,\n            u'IPAddress':u'',\n            u'Gateway':u'',\n            u'Aliases':None\n         }\n      },\n      u'SecondaryIPv6Addresses':None,\n      u'LinkLocalIPv6Address':u'',\n      u'HairpinMode':False,\n      u'IPv6Gateway':u'',\n      u'SecondaryIPAddresses':None,\n      u'SandboxID':u'',\n      u'MacAddress':u'',\n      u'GlobalIPv6Address':u'',\n      u'Gateway':u'',\n      u'LinkLocalIPv6PrefixLen':0,\n      u'EndpointID':u'',\n      u'SandboxKey':u'',\n      u'GlobalIPv6PrefixLen':0,\n      u'IPPrefixLen':0,\n      u'IPAddress':u'',\n      u'Ports':None\n   },\n   u'AppArmorProfile':u'',\n   u'Image':   u'sha256:b2c98afd2250321437e6195f581c3a468995b5728be459ac7c5d46e54d3f1b2a',\n   u'LogPath':u'',\n   u'HostConfig':{\n      u'CpuPeriod':0,\n      u'MemorySwappiness':-1,\n      u'ContainerIDFile':u'',\n      u'KernelMemory':0,\n      u'Memory':134217728,\n      u'CpuQuota':0,\n      u'UsernsMode':u'',\n      u'AutoRemove':False,\n      u'BlkioDeviceReadIOps':None,\n      u'Dns':None,\n      u'ExtraHosts':None,\n      u'PidsLimit':0,\n      u'DnsSearch':None,\n      u'Privileged':False,\n      u'IOMaximumIOps':0,\n      u'CpuPercent':0,\n      u'NanoCpus':0,\n      u'Ulimits':None,\n      u'CpusetCpus':u'',\n      u'DiskQuota':0,\n      u'CgroupParent':u'',\n      u'BlkioWeight':0,\n      u'RestartPolicy':{\n         u'MaximumRetryCount':0,\n         u'Name':u''\n      },\n      u'OomScoreAdj':0,\n      u'BlkioDeviceReadBps':None,\n      u'VolumeDriver':u'',\n      u'ReadonlyRootfs':False,\n      u'CpuShares':0,\n      u'PublishAllPorts':False,\n      u'MemoryReservation':0,\n      u'BlkioWeightDevice':None,\n      u'ConsoleSize':[\n         0,\n         0\n      ],\n      u'NetworkMode':u'default',\n      u'BlkioDeviceWriteBps':None,\n      u'Isolation':u'',\n      u'GroupAdd':None,\n      u'CpuRealtimeRuntime':0,\n      u'Devices':None,\n      u'BlkioDeviceWriteIOps':None,\n      u'Binds':None,\n      u'CpusetMems':u'',\n      u'Cgroup':u'',\n      u'UTSMode':u'',\n      u'PidMode':u'',\n      u'Runtime':u'runc',\n      u'VolumesFrom':None,\n      u'CapDrop':None,\n      u'DnsOptions':None,\n      u'ShmSize':67108864,\n      u'Links':None,\n      u'CpuRealtimePeriod':0,\n      u'IpcMode':u'',\n      u'PortBindings':{\n         u'8000/tcp':[\n            {\n               u'HostPort':u'',\n               u'HostIp':u''\n            }\n         ]\n      },\n      u'SecurityOpt':None,\n      u'CapAdd':None,\n      u'CpuCount':0,\n      u'MemorySwap':268435456,\n      u'OomKillDisable':False,\n      u'LogConfig':{\n         u'Config':{  \n     },\n     u'Type':u'json-file'\n  },\n  u'IOMaximumBandwidth':0\n\n},\n   u'Id':u'07a0e7a221753a48b8e8c34ac7ad774b31e506008c96211230cebfb71131038e',\n   u'MountLabel':u''\n}\n```\nAs you can see, there is no ['NetworkSettings']['Ports']. I tried it and it didn't update. Should there be some lag time between the client.containers.run() line and the container.reload()?. ",
    "mccalluc": "I had the same initial problem, but immediately calling reload() does seem to fix it for me:\nu'Ports': {u'443/tcp': None,\n                                 u'80/tcp': [{u'HostIp': u'0.0.0.0',\n                                              u'HostPort': u'32769'}]},\nI'm running with docker==2.1.0. If I do reproduce it I'll comment here.. The difference in behavior can be isolated to the pull:\n```python\n    def test_hello_world_sdk_with_cli_pull(self):\n        client = docker.from_env()\n        call(['docker', 'pull', 'ubuntu'])\n        output = client.containers.run(\"ubuntu\", \"echo hello world\")\n        self.assertEqual(output, 'hello world\\n')\ndef test_hello_world_sdk_with_sdk_pull(self):\n    client = docker.from_env()\n    client.images.pull('ubuntu')\n    output = client.containers.run(\"ubuntu\", \"echo hello world\")\n    self.assertEqual(output, 'hello world\\n')\n\n\n$ docker rmi ubuntu\nUntagged: ubuntu:latest\n$ python -m unittest -v docker_engine_app.tests.DockerTests.test_hello_world_sdk_with_cli_pull\ntest_hello_world_sdk_with_cli_pull (docker_engine_app.tests.DockerTests) ... Using default tag: latest\nlatest: Pulling from library/ubuntu\nDigest: sha256:dd7808d8792c9841d0b460122f1acf0a2dd1f56404f8d1e56298048885e45535\nStatus: Downloaded newer image for ubuntu:latest\nok\n\nRan 1 test in 2.188s\nOK\nversus\n$ docker rmi ubuntu\nUntagged: ubuntu:latest\n$ python -m unittest -v docker_engine_app.tests.DockerTests.test_hello_world_sdk_with_sdk_pull\ntest_hello_world_sdk_with_sdk_pull (docker_engine_app.tests.DockerTests) ... ok\n\nRan 1 test in 63.027s\nOK\n``. Thanks! Yes: Changingclient.run('ubuntu', 'echo hello world')toclient.run('ubuntu:latest', 'echo hello world')` fixes this, even without an explicit pull. The difference in behavior surprises me coming from the command line, but it's probably just me.. Sorry: Let me try again and include the signature.. ",
    "josepainumkal": "Actually it was not throwing any exception earlier.  Also, for the network I am not sure what IP should be given for the attribute \"ipv4_address\". See below:\ndocker network inspect my-overlay\n[\n   {\n        \"Name\": \"my-overlay\",\n        \"Id\": \"f30ced0d013977e5a4bda9cd27c3b6e53abbcf85d4aeaeeb53f41fe4d4daeefe\",\n        \"Created\": \"2017-02-03T16:09:03.131635218-08:00\",\n        \"Scope\": \"global\",\n        \"Driver\": \"overlay\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": {},\n            \"Config\": [\n                {\n                    \"Subnet\": \"192.168.3.0/24\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Attachable\": false,\n        \"Options\": {},\n        \"Labels\": {}\n    }\n]\nThere is no specific IP address I could find for my-overlay network. Any way I fixed the issue for my scenario by  commenting the  line \"ipv4_address=overlay_network_ipv4Address\"\nIt works !!!\n. ",
    "sdorgancs": "Glad this help :). ",
    "D3f0": "I have a similar problem with docker-py (running on Docker for Mac, but also tried it on Ubuntu 16.04). Basically dockerpy won't show the bindings docker provide when expose_all_ports=True is used as run parameter.\nI took a look at https://github.com/docker/docker-py/issues/513 and read this guide and did some tests. \nIn order to summarize what I did, I wrote this short notebook: https://gist.github.com/D3f0/e5913d5d3ab8fe7c594fd9d1e6743a9a\nPorts are exposed but docker-py won't show them.\n. I found that there's a way to get the assigned ports from Python when you use expose_all_ports=True, there's a way to call inspect and get a dict from docker-py:\npython\ncontainer = client.containers.run(image=\"some_image_with_expose\", publish_all_ports=True, detach=True)\nclient.api.inspect_container(container.id)[\"NetworkSettings\"][\"Ports\"]\nthis will returns something like:\npython\n{'3000/tcp': [{'HostIp': '0.0.0.0', 'HostPort': '32780'}]}\nA not recommended dict comprehension  I used in my environment:\n```python\n{dock_port.split('/')[0]: host_port[0]['HostPort'] for dock_port, host_port in exposed.items()}\n-> {'3000': '32780'}\n```. ",
    "swood": "Hello.\nI just changed one row in api/client.py and it fix this problem.\nhttps://github.com/swood/docker-py/commit/528c1da44d3e117359ba665f020ad42153a679fc\nCould you tell me, is it hack or right way to fix the problem?. ",
    "JasonGiedymin": "Rebase plz! @shin- review comments?. ",
    "asierra01": "it looks that's what I have c:\\python27\\lib\\site-packages\\pypiwin32-219-py2.7-win-amd64.egg\\win32pipe.pyd. Let me do more research. I notice you guys do make test, let me see if the test cover something like \n....\n         isinstance(command_output,docker.transport.npipesocket.NpipeSocket) == True:\n           print command_output.gettimeout()\n           command_output.settimeout(0)\n....\n. ",
    "philtay": "Duplicate of https://github.com/docker/docker-py/issues/1397. ",
    "diego-treitos": "You are right @philtay. I couldn't find any reference to this problem but this is clearly a duplicate. I am closing it.. ",
    "tmad": "I have fixed issues that you pointed out.\nI have also added support for init_path parameter, as suggested.. Fixed, this was unnecessary.. You are right, changed. Done. My bad.. I've changed the test, could you verify now?. Sure, fixed. ",
    "nmaas87": "@shin- Thank you, that would be good \ud83d\udc4d . ",
    "scbunn": "def connect(self, container):\n        \"\"\"\n        Connect a container to this network.\n        Args:\n            container (str): Container to connect to this network, as either\n                an ID, name, or :py:class:`~docker.models.containers.Container`\n                object.\n            aliases (:py:class:`list`): A list of aliases for this endpoint.\n                Names in that list can be used within the network to reach the\n                container. Defaults to ``None``.\n            links (:py:class:`list`): A list of links for this endpoint.\n                Containers declared in this list will be linkedto this\n                container. Defaults to ``None``.\n            ipv4_address (str): The IP address of this container on the\n                network, using the IPv4 protocol. Defaults to ``None``.\n            ipv6_address (str): The IP address of this container on the\n                network, using the IPv6 protocol. Defaults to ``None``.\n            link_local_ips (:py:class:`list`): A list of link-local (IPv4/IPv6)\n                addresses.\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        if isinstance(container, Container):\n            container = container.id\n        return self.client.api.connect_container_to_network(container, self.id)\nAm I missing something or do none of the arguments in the documentation exist?. I worked my way to an example that works.\n```\n!/usr/bin/env python\nimport docker\nclient = docker.from_env()\nnetwork_config = client.api.create_networking_config({\n    'salt': client.api.create_endpoint_config(\n        aliases=['foo', 'bar', 'baz']\n    )\n})\nhost_config = client.api.create_host_config(\n    cap_add=[\"SYS_ADMIN\"],\n)\ncontainer = client.api.create_container(\n    \"foo/salt/master\",\n    networking_config=network_config,\n    host_config=host_config,\n    detach=True\n)\nnetwork = client.networks.create(name=\"salt\", driver=\"bridge\")\nsm = client.containers.get(container.get('Id'))\nsm.start()\nsm.kill()\nsm.remove()\nnetwork.remove()\n```\nBut I think this using subprocess is going to be faster are more readable. \n. ",
    "cguethle": "@TomasTomecek Did you get anywhere with this? I am actually seeing this same problem running in a container for something totally different. Trying to requests.put a file to a rest interface on another server.. ",
    "mikz": "I can report that **/.* matches any file with a dot. Which is wrong and different from the official docker client.. ",
    "Anvil": "You're welcome. :]. @shin- thanks !. ",
    "pradeepinmar": "Perfect, that fixed it, thanks!. ",
    "nsteinmetz": "I would agree but noticed that for old version of docker (like the one in Debian), I think it's still the case. You can't have both.\nWith recent versions, I agree you can use both.. Hi,\nThere are other occurences of \"auto_remove\" accross the code (being code or doc about the code).\nIs this PR enough to solve the issue ?. Indeed, auto_remove in not in one of the two lists (https://github.com/docker/docker-py/blob/master/docker/models/containers.py#L778-L850) and so the error is raised as this keywork is not managed by one of the configuration filters above (https://github.com/docker/docker-py/blob/2.1.0/docker/models/containers.py#L853-L879). Hi @shin- \nOups indeed, I search one issues but not on PR - my bad.\nSo now my point is #1477 \nThanks,\nNicolas. ",
    "ztlpn": "Workaround for this and #1579 (checked with docker-py=2.2.1 and docker API v.1.27)\npython\nclient = docker.from_env()\nimage = client.images.get('hello-world')\nhost_config = client.api.create_host_config(auto_remove=True)\ncontainer_id = client.api.create_container(image.id, detach=True, host_config=host_config)['Id']\ncontainer = client.containers.get(container_id)\ncontainer.start(). ",
    "adamlavie": "Well that does make sense, but very saddening. Thanks for your quick response and for the referral. \nI'd offer a PR to make sure the client uses http/https but not sure what the right fix would be since I've noticed this PR: https://github.com/docker/docker-py/pull/1322/files\nThanks again,\nAdam.. ",
    "achimnol": "@shin- I wonder if being unable to use websockets is a fundamental limitation of Unix sockets or not. Is it?\nOn the other hand, I'd like to see any example that writes stdin / reads stdout & stderr of a container using docker-py (like the docker attach command). I tried attach_socket with 'stdin': True in params dict but the returned socket is still non-writable.. ",
    "JanKoppe": "My bad, should have read better. It's already there!. ",
    "SeanBE": "I'm running into the same problem. . ",
    "lisapeters": "I have also hit this issue. . ",
    "yonilevy": "I'm running into this as well.. ",
    "ppizzo": "Hi, many thanks for your answer. My Docker version is as follows:\n```\nClient:\n Version:      1.13.1-cs1\n API version:  1.25 (downgraded from 1.26)\n Go version:   go1.7.5\n Git commit:   8709b81\n Built:        Thu Feb  9 02:53:15 2017\n OS/Arch:      linux/amd64\nServer:\n Version:      ucp/2.1.0\n API version:  1.26 (minimum version 1.12)\n Go version:   go1.7.4\n Git commit:   009d87a\n Built:        Thu Feb  9 00:47:21 UTC 2017\n OS/Arch:      linux/amd64\n Experimental: false\n```\nI think I'm using the latest stable version because I installed it with pip3 install docker on a Python 3 container based on the latest Alpine stable release.\nI've also tried to run run the script on the physical engine (i.e. not in a container) and the result is still the same: I get all images, even the intermediate layers not tagged which are skipped by default by docker images.. Any progress on this? In the meanwhile I've upgraded to 1.13.1-cs2 but still no luck. There's no way to remove intermediate levels from APIs.\n```\nClient:\n Version:      1.13.1-cs2\n API version:  1.25 (downgraded from 1.26)\n Go version:   go1.7.5\n Git commit:   ad32da7\n Built:        Thu Feb 23 16:38:18 2017\n OS/Arch:      linux/amd64\nServer:\n Version:      ucp/2.1.0\n API version:  1.26 (minimum version 1.12)\n Go version:   go1.7.4\n Git commit:   009d87a\n Built:        Thu Feb  9 00:47:21 UTC 2017\n OS/Arch:      linux/amd64\n Experimental: false\n```. Based on the output you posted, the results look identical: 5 containers with Python API, 5 container with native Docker client. Also the IDs are the same.\nSo, what's the problem?. @jameyang, now I understand: you were talking about the number of digits.\nThat's just an abbreviated representation. Actually internal IDs are even more longer than 10 or 12 digits. If you inspect the containers you should be able to see the complete IDs (64 digits if I remember well). ",
    "AndrewYang2046": "@ppizzo, hi.   This is docker-py get container id is 10 number. Actually the container id is 12 number.\n  === 4ba3e59b9fed. @ppizzo , sorry , i don't say clear .  that's it.\nthis is my code:\nimport docker\ndocker_host = 'tcp://172.28.250.59:1234'\nclient = docker.DockerClient(base_url=docker_host)\nprint client.containers.list()\nthis is output:\n\nbut, actually container id is this:\n\ndocker-py get id two fewer than it actually was\n. @ppizzo , hi, i found the reason. \nthis is file:  docker\\models\\resource.py\n@property\ndef short_id(self):\n    \"\"\"\n    The ID of the object, truncated to 10 characters.\n    \"\"\"\n    return self.id[:10]\n\nedit it : \n@property\ndef short_id(self):\n    \"\"\"\n    The ID of the object, truncated to 10 characters.\n    \"\"\"\n    return self.id[:12]\n\nnow is ok.\n. ",
    "boaz0": "I didn't know that.\nI thought it should be there because when I run this Dockerfile, it only pip-installs the modules from test-requirements.txt.\nThanks for the clarification. I am closing this PR.\n:man_dancing: . ",
    "hnq90": "@roidelapluie Are you still working on this PR? If not, I will continue your work.. ",
    "ymc-dabe": "I doubt that this makes it into docker-py before moby/moby#14080 is resolved, but nevertheless I added support in PR #1937 for it. It especially is useful if one uses a patched docker, that has support for build time volumes (e.g. Project Atomic or balena - I think Fedora and RHEL have patched-in support as well).. ",
    "mcfletch": "I'm seeing this error in Python 2.7 on Ubuntu 17.04 when docker is installed with pip install --user docker. There the \"backports\" module reports as being:\n\n\n\n\nbackports\n\n\n\n\n\nwith the path of that pointing to /usr/lib/python2.7/dist-packages/backports while the backports that includes the code you are expecting is in ~/.local/lib/python2.7/site-packages/backports\nThis seems like a fundamental problem with the backports module/package idea when used with --user mode installation of docker and its dependencies. Possibly backports could be fixed to allow for this type of usage, but I can confirm that docker does import if you install it into a virtualenv instead of doing --user installation.. ",
    "xander-ye": "I have the same problem just now. After I just updatting docker ,everything becomes OK. . ",
    "fchevitarese": "I've got the same problem and ran sudo pip install --upgrade docker and it worked :) \n. ",
    "xiamencloud": "I've got the same problem now. \nOS: macOS 10.12.3\nPython Version\uff1aPython 2.7.13 (default, Jun 26 2017, 20:06:48)\nDocker Version: Docker version 17.09.0-ce, build afdb6d4\n$ pip show docker \nName: docker\nVersion: 2.5.0\nSummary: A Python library for the Docker Engine API.\nHome-page: https://github.com/docker/docker-py\nAuthor: Joffrey F\nAuthor-email: joffrey@docker.com\nLicense: Apache License 2.0\nLocation: /usr/local/lib/python2.7/site-packages\nRequires: docker-pycreds, requests, backports.ssl-match-hostname, ipaddress, six, websocket-client\n$ python\nPython 2.7.13 (default, Jun 26 2017, 20:06:48) \n[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport docker\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/local/lib/python2.7/site-packages/docker/init.py\", line 2, in \n    from .api import APIClient\n  File \"/usr/local/lib/python2.7/site-packages/docker/api/init.py\", line 2, in \n    from .client import APIClient\n  File \"/usr/local/lib/python2.7/site-packages/docker/api/client.py\", line 11, in \n    from .build import BuildApiMixin\n  File \"/usr/local/lib/python2.7/site-packages/docker/api/build.py\", line 9, in \n    from .. import utils\n  File \"/usr/local/lib/python2.7/site-packages/docker/utils/init.py\", line 2, in \n    from .build import tar, exclude_paths\n  File \"/usr/local/lib/python2.7/site-packages/docker/utils/build.py\", line 5, in \n    from .utils import create_archive\n  File \"/usr/local/lib/python2.7/site-packages/docker/utils/utils.py\", line 18, in \n    from .. import tls\n  File \"/usr/local/lib/python2.7/site-packages/docker/tls.py\", line 5, in \n    from .transport import SSLAdapter\n  File \"/usr/local/lib/python2.7/site-packages/docker/transport/init.py\", line 3, in \n    from .ssladapter import SSLAdapter\n  File \"/usr/local/lib/python2.7/site-packages/docker/transport/ssladapter.py\", line 21, in \n    from backports.ssl_match_hostname import match_hostname\nImportError: No module named ssl_match_hostname. \n\n\n",
    "feliperuhland": "Hi @fjhaveri. Have you tried pip install --upgrade docker? How is your pip list?. Hi @Anagha5. I think these problems are not related. It may be best to open a new issue with the steps for reproducing the problem and the versions of the applications and systems.. Hi @shin-. Can I work on this? Thanks.. @tobiasfielitz @shin- I am already working on this! . Hi @remy-phelipot @erezool.\n@shin- is right. Unfortunately, the Docker Engine API has this behavior. The method returns before knowing if there was an error due to authorization problem. It may be possible to use some alternatives:\n\nVerify that credentials are present (Does not guarantee successful login) https://github.com/moby/moby/issues/15466;\nLog in before sending the image;\nUse the subprocess module;\nDownload a private image (very small) before sending the image to ensure successful access.. Hi @remy-phelipot \n\nI understand your point of view, but I do not know if the purpose of the library is to act this way. Anyway, I made a quick change (without testing) of the possible check_for_errors flag. Can you test? https://github.com/feliperuhland/docker-py/tree/issue-1772-docker-push-unauthorized\nYou should pass the argument check_for_errors=True. \nAnd just to make it clear, I do not know if this idea will be included in the code.. Hi @andreycizov. Maybe you can use the container.id to send to the queue and use the docker-py client to get the container object back, like cli.containers.get(<id>).. Hi, @StephenPCG. I usually use remove=True and docker-py remove the container for me, after getting the log. The auto_remove flag enable auto-removal of the container on daemon side when the container\u2019s process exits and I think it will be harder to get the log.. Hello, I tried to reproduce the exception, but I did not succeed.\ndocker (2.6.1)\ndocker-pycreds (0.2.1)\nrequests (2.18.4)\n```\nClient:\n Version:      17.09.0-ce\n API version:  1.32\n Go version:   go1.8.3\n Git commit:   afdb6d4\n Built:        Tue Sep 26 22:39:28 2017\n OS/Arch:      linux/amd64\nServer:\n Version:      17.09.0-ce\n API version:  1.32 (minimum version 1.12)\n Go version:   go1.8.3\n Git commit:   afdb6d4\n Built:        Tue Sep 26 22:45:38 2017\n OS/Arch:      linux/amd64\n Experimental: false\n```\n```\nPython 2.7.13 (default, Apr 20 2017, 12:13:37) \n[GCC 6.3.0] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport docker\nclient = docker.from_env()\nservice = client.services.create(\"redis:4.0.1 - alpine\", name='redis')\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/lib/python2.7/site-packages/docker/models/services.py\", line 177, in create\n    service_id = self.client.api.create_service(create_kwargs)\n  File \"/usr/lib/python2.7/site-packages/docker/utils/decorators.py\", line 34, in wrapper\n    return f(self, *args, kwargs)\n  File \"/usr/lib/python2.7/site-packages/docker/api/service.py\", line 134, in create_service\n    self._post_json(url, data=data, headers=headers), True\n  File \"/usr/lib/python2.7/site-packages/docker/api/client.py\", line 228, in _result\n    self._raise_for_status(response)\n  File \"/usr/lib/python2.7/site-packages/docker/api/client.py\", line 224, in _raise_for_status\n    raise create_api_error_from_http_exception(e)\n  File \"/usr/lib/python2.7/site-packages/docker/errors.py\", line 31, in create_api_error_from_http_exception\n    raise cls(e, response=response, explanation=explanation)\ndocker.errors.APIError: 400 Client Error: Bad Request (\"rpc error: code = InvalidArgument desc = ContainerSpec: \"redis:4.0.1 - alpine\" is not a valid repository/tag\")\n```\n\n\n\nMaybe if we know more about variable e and/or e.response.\n. @mbovo can you try running this code in Python console?\n```python\n\n\n\nimport docker\nclient = docker.from_env()\nclient.services.create(\"redis:4.0.1 - alpine\", name='redis')\n`\nAnd see if it raises the exception.. @shin- I think we can close this.. Hi, I believe there was a network problem here:docker.errors.APIError: 500 Server Error: Internal Server Error (\"Get https://registry-1.docker.io/v2/library/hello-world/manifests/latest: Get https://auth.docker.io/token?scope=repository%3Alibrary%2Fhello-world%3Apull&service=registry.docker.io: net/http: TLS handshake timeout\"). Hi, @qazbnm456. The build failed because the line is too long:docker/api/container.py:841:80: E501 line too long (85 > 79 characters)``.. Hi, @qazbnm456. I think you must sign the commit.. @shin- You're right. I just submitted a validation for it. If you think that the exception message isn't good, please tell me. Thanks.  . > Could not update commit status. Message: Server returned HTTP response code: 201, message: 'Created' for URL: https://api.github.com/repos/docker/docker-py/statuses/0e0a8526801d207bfdf2e9011ab1348525437a79\n\n\n\nI think it's a network problem.. Hi @johannespostler. Seems about right.. Hi @shin-. I think you just need to fix a test:\n```\ndiff --git a/tests/unit/models_containers_test.py b/tests/unit/models_containers_test.py\nindex a479e83..95295a9 100644\n--- a/tests/unit/models_containers_test.py\n+++ b/tests/unit/models_containers_test.py\n@@ -225,7 +225,7 @@ class ContainerCollectionTest(unittest.TestCase):\n         container = client.containers.run('alpine', 'sleep 300', detach=True)\n     assert container.id == FAKE_CONTAINER_ID\n\n\nclient.api.pull.assert_called_with('alpine', tag=None)\n\nclient.api.pull.assert_called_with('alpine', platform=None, tag=None)\ndef test_run_with_error(self):\n     client = make_fake_client()\n```\n\n\nJenkins:\n=================================== FAILURES ===================================\n____________________ ContainerCollectionTest.test_run_pull _____________________\ntests/unit/models_containers_test.py:228: in test_run_pull\n    client.api.pull.assert_called_with('alpine', tag=None)\n/opt/python/3.5.4/lib/python3.5/unittest/mock.py:794: in assert_called_with\n    raise AssertionError(_error_message()) from cause\nE   AssertionError: Expected call: pull('alpine', tag=None)\nE   Actual call: pull('alpine', platform=None, tag=None)\n========== 1 failed, 500 passed, 4 skipped, 1 xfailed in 4.73 seconds ==========. Hi @dnovvak, Have you tried any of the suggestions here: https://github.com/docker/docker-credential-helpers/issues/24?. @dperny 5 test with python2.7 failed because AttributeError: 'module' object has no attribute 'PROTOCOL_TLSv1_2'. Hi @greenmaid. You should use https://docker-py.readthedocs.io/en/stable/api.html#docker.types.EndpointSpec. Regards.. Hi @shin-, maybe put out = None before the if not kwargs.get('auto_remove') looks better. What do you think?. Thanks @shin-. I already changed.. It looks like the default value is replicated.. The conditional is wrong.. In fact, the conditional was very wrong. It returned an empty dictionary and did not raise the exception.\n  . Hi @funkyfuture. It looks like the else block is unnecessary because of the return. You can use something like:\n```\nif socket or stream:\n    return ExecResult(None, exec_output)\nreturn ExecResult(\n    self.client.api.exec_inspect(resp['Id'])['ExitCode'],\n    exec_output)\n```\nRegards.. I think it looks cleaner, but it's a personal matter. Your code, your decision. Greetings.. ",
    "KedemBarak": "I'm facing a similar error with Docker python image - I'm using Windows machine with latest version of Docker Edge - i get this error:\nImportError: No module named XXXXXX\n. ",
    "michaeldayCA": "I'm getting this when I try to do a pip install --user docker-compose:\n'''\n$ pip install --user docker-compose\nCollecting docker-compose\n  Using cached docker_compose-1.19.0-py2.py3-none-any.whl\nCollecting websocket-client<1.0,>=0.32.0 (from docker-compose)\n  Using cached websocket_client-0.46.0-py2.py3-none-any.whl\nCollecting backports.ssl-match-hostname>=3.5; python_version < \"3.5\" (from docker-compose)\n  Using cached backports.ssl_match_hostname-3.5.0.1.tar.gz\nCollecting PyYAML<4,>=3.10 (from docker-compose)\n  Using cached PyYAML-3.12.tar.gz\nCollecting dockerpty<0.5,>=0.4.1 (from docker-compose)\n  Using cached dockerpty-0.4.1.tar.gz\nCollecting ipaddress>=1.0.16; python_version < \"3.3\" (from docker-compose)\n  Using cached ipaddress-1.0.19.tar.gz\nCollecting docopt<0.7,>=0.6.1 (from docker-compose)\n  Using cached docopt-0.6.2.tar.gz\nCollecting enum34<2,>=1.0.4; python_version < \"3.4\" (from docker-compose)\n  Using cached enum34-1.1.6-py2-none-any.whl\nCollecting docker<3.0,>=2.7.0 (from docker-compose)\n  Using cached docker-2.7.0-py2.py3-none-any.whl\nCollecting requests!=2.11.0,!=2.12.2,!=2.18.0,<2.19,>=2.6.1 (from docker-compose)\n  Using cached requests-2.18.4-py2.py3-none-any.whl\nCollecting texttable<0.10,>=0.9.0 (from docker-compose)\n  Using cached texttable-0.9.1.tar.gz\nCollecting cached-property<2,>=1.2.0 (from docker-compose)\n  Using cached cached_property-1.3.1-py2.py3-none-any.whl\nCollecting jsonschema<3,>=2.5.1 (from docker-compose)\n  Using cached jsonschema-2.6.0-py2.py3-none-any.whl\nCollecting six<2,>=1.3.0 (from docker-compose)\n  Using cached six-1.11.0-py2.py3-none-any.whl\nCollecting docker-pycreds>=0.2.1 (from docker<3.0,>=2.7.0->docker-compose)\n  Using cached docker_pycreds-0.2.1-py2.py3-none-any.whl\nCollecting certifi>=2017.4.17 (from requests!=2.11.0,!=2.12.2,!=2.18.0,<2.19,>=2.6.1->docker-compose)\n  Using cached certifi-2018.1.18-py2.py3-none-any.whl\nCollecting chardet<3.1.0,>=3.0.2 (from requests!=2.11.0,!=2.12.2,!=2.18.0,<2.19,>=2.6.1->docker-compose)\n  Using cached chardet-3.0.4-py2.py3-none-any.whl\nCollecting idna<2.7,>=2.5 (from requests!=2.11.0,!=2.12.2,!=2.18.0,<2.19,>=2.6.1->docker-compose)\n  Using cached idna-2.6-py2.py3-none-any.whl\nCollecting urllib3<1.23,>=1.21.1 (from requests!=2.11.0,!=2.12.2,!=2.18.0,<2.19,>=2.6.1->docker-compose)\n  Using cached urllib3-1.22-py2.py3-none-any.whl\nCollecting functools32; python_version == \"2.7\" (from jsonschema<3,>=2.5.1->docker-compose)\n  Using cached functools32-3.2.3-2.zip\nInstalling collected packages: six, websocket-client, backports.ssl-match-hostname, PyYAML, dockerpty, ipaddress, docopt, enum34, docker-pycreds, certifi, chardet, idna, urllib3, requests, docker, texttable, cached-property, functools32, jsonschema, docker-compose\n  Running setup.py install for backports.ssl-match-hostname ... error\n    Complete output from command /usr/bin/python2 -u -c \"import setuptools, tokenize;file='/tmp/pip-build-WGvE7j/backports.ssl-match-hostname/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record /tmp/pip-QUZVUK-record/install-record.txt --single-version-externally-managed --compile --user --prefix=:\n    running install\n    running build\n    running build_py\n    creating build\n    creating build/lib\n    creating build/lib/backports\n    copying backports/init.py -> build/lib/backports\n    creating build/lib/backports/ssl_match_hostname\n    copying backports/ssl_match_hostname/init.py -> build/lib/backports/ssl_match_hostname\n    running install_lib\n    creating /home/myusername/.local/lib/python2.7/site-packages/backports\n    copying build/lib/backports/init.py -> /home/myusername/.local/lib/python2.7/site-packages/backports\n    creating /home/myusername/.local/lib/python2.7/site-packages/backports/ssl_match_hostname\n    copying build/lib/backports/ssl_match_hostname/init.py -> /home/myusername/.local/lib/python2.7/site-packages/backports/ssl_match_hostname\n    byte-compiling /home/myusername/.local/lib/python2.7/site-packages/backports/init.py to init.pyc\n    byte-compiling /home/myusername/.local/lib/python2.7/site-packages/backports/ssl_match_hostname/init.py to init.pyc\n    running install_egg_info\n    running egg_info\n    creating backports.ssl_match_hostname.egg-info\n    writing backports.ssl_match_hostname.egg-info/PKG-INFO\n    writing top-level names to backports.ssl_match_hostname.egg-info/top_level.txt\n    writing dependency_links to backports.ssl_match_hostname.egg-info/dependency_links.txt\n    writing manifest file 'backports.ssl_match_hostname.egg-info/SOURCES.txt'\n    warning: manifest_maker: standard file '-c' not found\nreading manifest file 'backports.ssl_match_hostname.egg-info/SOURCES.txt'\nwriting manifest file 'backports.ssl_match_hostname.egg-info/SOURCES.txt'\nCopying backports.ssl_match_hostname.egg-info to /home/myusername/.local/lib/python2.7/site-packages/backports.ssl_match_hostname-3.5.0.1-py2.7.egg-info\nrunning install_scripts\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/tmp/pip-build-WGvE7j/backports.ssl-match-hostname/setup.py\", line 33, in <module>\n    packages=['backports', 'backports.ssl_match_hostname'],\n  File \"/usr/lib64/python2.7/distutils/core.py\", line 152, in setup\n    dist.run_commands()\n  File \"/usr/lib64/python2.7/distutils/dist.py\", line 953, in run_commands\n    self.run_command(cmd)\n  File \"/usr/lib64/python2.7/distutils/dist.py\", line 972, in run_command\n    cmd_obj.run()\n  File \"/usr/lib/python2.7/site-packages/setuptools/command/install.py\", line 53, in run\n    return _install.run(self)\n  File \"/usr/lib64/python2.7/distutils/command/install.py\", line 575, in run\n    self.run_command(cmd_name)\n  File \"/usr/lib64/python2.7/distutils/cmd.py\", line 326, in run_command\n    self.distribution.run_command(command)\n  File \"/usr/lib64/python2.7/distutils/dist.py\", line 972, in run_command\n    cmd_obj.run()\n  File \"/usr/lib/python2.7/site-packages/setuptools/command/install_scripts.py\", line 15, in run\n    from setuptools.command.easy_install import get_script_args\n  File \"/usr/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 49, in <module>\n    from setuptools.package_index import PackageIndex\n  File \"/usr/lib/python2.7/site-packages/setuptools/package_index.py\", line 14, in <module>\n    from setuptools import ssl_support\n  File \"/usr/lib/python2.7/site-packages/setuptools/ssl_support.py\", line 85, in <module>\n    from backports.ssl_match_hostname import CertificateError, match_hostname\nImportError: No module named ssl_match_hostname\n\n----------------------------------------\n\nCommand \"/usr/bin/python2 -u -c \"import setuptools, tokenize;file='/tmp/pip-build-WGvE7j/backports.ssl-match-hostname/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record /tmp/pip-QUZVUK-record/install-record.txt --single-version-externally-managed --compile --user --prefix=\" failed with error code 1 in /tmp/pip-build-WGvE7j/backports.ssl-match-hostname/\n'''. ",
    "mulmschneider": "The following workaround worked for me (sudo pip install --upgrade docker didn't):\nsudo cp -r ~/.local/lib/python2.7/site-packages/backports/ssl_match_hostname/ /usr/lib/python2.7/dist-packages/backports. ",
    "jean-christophe-manciot": "On Ubuntu 17.10, a workaround is to uninstall docker python module with pip and install python-docker with apt:\n```\npip uninstall docker\n...\napt install python-docker\n...\npython2 -c 'import docker; print(docker.version)'\n2.5.1\n. On Ubuntu 17.10, a much better workaround is to use a **virtualenv**.\nvirtualenv awx-build-virtualenv\nRunning virtualenv with interpreter /usr/bin/python2\nNew python executable in awx-build-virtualenv/bin/python2\nAlso creating executable in awx-build-virtualenv/bin/python\nInstalling setuptools, pkg_resources, pip, wheel...done.\nsource awx-build-virtualenv/bin/activate\n(awx-build-virtualenv) # pip install docker\nCollecting docker\n  Using cached docker-3.1.1-py2.py3-none-any.whl\nCollecting backports.ssl-match-hostname>=3.5; python_version < \"3.5\" (from docker)\n...\n(awx-build-virtualenv) # python2 -c 'import docker; print(docker.version)'\n3.1.1\n. On Ubuntu 17.10, **outside a virtualenv**, it is possible to import docker pip module with **downgraded requests** pip module (thanks to this [thread](https://github.com/requests/requests/issues/4160)):\npip freeze | grep requests\nrequests==2.18.1\npip freeze | grep docker\ndocker==2.7.0\ndocker-compose==1.19.0\ndocker-pycreds==0.2.2\ndockerpty==0.4.1\npython2 -c 'import docker; print(docker.version)'\n2.7.0\n. Same issue with all docker module releases between2.7.0and3.1.1```. I have not tested earlier versions.. ",
    "Admin9705": "We are running into the same issue: https://plexguide.com/threads/cant-install-any-containers.1062/#post-5984\n3 people reporting it via ansible and there is no good solution. you have to like reinstall your box for all the problem to go away. ",
    "raiene": "I had the same problem running docker-compose commands (ImportError: No module named ssl_match_hostname). After reinstall docker and docker-compose with apt the docker-compose commands are working, but I still can't import docker with \npython -c 'import docker'. ",
    "rbelem": "I fixed the issue by removing all my ~/.local/lib/python{2,3}, but maybe it could be fixed by running touch ~/.local/lib/python2/dist-packages/backports/__init___.py.. ",
    "jouellnyc": "@jean-christophe-manciot - You were a relentless beast on this. I've been there :0 - Thanks for all the effort! This fixed me on Ubuntu 18.04 as well. . ",
    "et304383": "None of the given workarounds worked for me.  How can be fix this?. I discovered I had a bad version of backports.functools-lru-cache sitting in my .local folder conflicting with the globally installed pip packages.\nSo uninstalling that seemed to help.. ",
    "msholty-fd": "I just had this fun issue while setting up my Ubuntu 18.x machine. docker-compose was working for me, then this morning I tried to do some additional installation of some libraries, specifically using python3.6 and suddenly my docker-compose and docker wasn't working.\nI eventually figured out that the version of backports that python2 was trying to import was here:\n/usr/lib/python2.7/dist-packages/backports\nEven though I had another version of it here:\n/usr/local/lib/python2.7/dist-packages/backports\nSo I ran: \ncp -r /usr/local/lib/python2.7/dist-packages/backports/ssl_match_hostname/ /usr/lib/python2.7/dist-packages/backports\nAnd that fixed it and docker-compose works again.. ",
    "danielfaust": "@msholty-fd I'm having the same issue with a clean install of Ubuntu 18.04.\nWhat could cause this?. ",
    "Nick1211": "Run bash command is a good solution :\nmv ~/.local ~/.local.bak\n. ",
    "ruCyberPoison": "I'm confronting the same issue and tried to fix it during 12 hours but no fixes found in Ubuntu 18.xx ,\nAny one have some suggestions ?\n@Nick1211 when you are talking abou ~/.local you mean /local/ ? \nbecause in my /root/.local it's empty \ud83d\ude15 \n@jean-christophe-manciot How you have fixed this issue on Ubuntu 18.10 ? I'm under a vps and i have this issue also, so if you have any suggestion to fix this issue everything is welcome. \nI Have tried to reintal with pip and unistall the pip docker and install it again with aptitude but nothing, the issue still here.\nAs i have reported here also, https://github.com/Admin9705/PlexGuide.com-The-Awesome-Plex-Server/issues/725\nRgds. ",
    "nischay30": "$ sudo pip uninstall backports.ssl_match_hostname\nand then install the docker-py with\n$ sudo pip install docker-py. ",
    "ssbarnea": "@nischay30 something is clearly not right about your recommandation because docker-py is the old/outdated name of the pip package. We are supposed to use just simple docker.\ndocker-py had last release back in 2016, is not maintained .\nAfter doing more testing I was able to find a way to avoid this problem by patching requirments with:\n```\nbackports.ssl-match-hostname>=3.7.1; python_version=='2.7'  # PSF\n^ workaround for https://github.com/docker/docker-py/issues/1502\n``` \nIf you miss to add this line, you may endup with a broken docker container on those systems that happen to have an older versions of backports.ssl-match-hostname.\nNote: I am not sure which is the first version that avoids this bug but the latest one that I used worked.\n. It proved to be caused by paramiko https://github.com/paramiko/paramiko/issues/1383 -- in case someone else encounters it we could maybe add a runtime warning if the ssh:// url does not contain a user. Missing user does not mean that ~/.ssh/config one will be used, just mean current user.\ndocker will user the one from ssh config but docker-compose or docker-py would not!. @shin- Yes I encountered that but I do not remember it now. I would rather throw the original error back instead of creating a new one that is crippled of important information. That code is wrong as it makes some dangereus assumptions.. ",
    "M0nsieurChat": "Ran into the very same problem with a freshly installed Ubuntu 18.10\nEdit: what worked for us is to use python3\n```\nsudo apt-get install python3\nEventually use this with Ansible if you're going to use python module docker with a playbook :\nansible_python_interpreter: /usr/bin/python3\n```. ",
    "n1b0r": "In the case that every parameters should be given to update() I'm getting stopped by:\n```\n\n\n\n[x['Target'] for x in service.attrs['Spec']['Networks']]\n[u'k9gcb5znnv60ah5k9iq23n40j']\nservice.update(networks=[x['Target'] for x in service.attrs['Spec']['Networks']])\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/local/lib/python2.7/dist-packages/docker/models/services.py\", line 77, in update\n    create_kwargs\n  File \"/usr/local/lib/python2.7/dist-packages/docker/utils/decorators.py\", line 35, in wrapper\n    return f(self, *args, kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/utils/decorators.py\", line 21, in wrapped\n    return f(self, resource_id, args, *kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/api/service.py\", line 282, in update_service\n    self._raise_for_status(resp)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/api/client.py\", line 216, in _raise_for_status\n    raise create_api_error_from_http_exception(e)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/errors.py\", line 30, in create_api_error_from_http_exception\n    raise cls(e, response=response, explanation=explanation)\ndocker.errors.APIError: 500 Server Error: Internal Server Error for url: http+docker://localunixsocket/v1.25/services/lmwvz3uvbb09mb95e8jbfjwqj/update?version=173358 (\"rpc error: code = 2 desc = changing network in service is not supported\")\n\n\n\n```. ",
    "rogergithub3": "Hi,\nI have the same issue, I'm trying to scale the service with update service's method. This works for me:\nimport docker\nscale = 10\ncli = docker.DockerClient(base_url='unix://var/run/docker.sock', version='1.25')\nservice = cli.services.list()[0]\nsecret = cli.secrets().list()[0]\nmode = docker.types.services.ServiceMode(\"replicated\", int(scale))\nservice.update(name=service.attrs['Spec']['Name'], networks=service.attrs['Spec']['Networks']). Thank you, if I try your code it's working, but I'm using docker sdk for python (https://docker-py.readthedocs.io/en/stable/).\nMy code:\nclient.login(username=username, password=password, registry=host)\nclient.services.create(\"image\")\nI want to pass to create() the parameter \"--with-registry-auth\" that's a option parameter in docker doc (https://docs.docker.com/engine/reference/commandline/service_create/#options)\nWhen I create service, how can I put --with-registry-auth parameter?\n. Great. Thank you.. ",
    "nkprince007": "I have a similar issue too. Indefinite hang of container.. ",
    "seifertm": "I ran into the same problem. This is related to #983.\n. ",
    "jstangroome": "If container run/create supported the StdinOnce parameters to the API, closing the socket should be enough.\nhttps://docs.docker.com/engine/api/v1.35/#operation/ContainerCreate. ",
    "iamanikeev": "Hi! Any chance this would be merged soon? Also, there are utils for making a context from git repo, but they are not used in the build method itself. I'll give this a shot today, see if it helps to fix #980 . ",
    "jasonmichalski": "@iamanikeev  I didn't directly integrate into the build() command itself based on the preferences expressed in #980, though it would be trivial to make that change if that was the desired behavior. \nI wasn't sure if there was still interest in adding this feature, so I haven't really tracked whether or not this works in most recent versions of docker-py.\nIf the project maintainers would like to see this feature added, I'd be happy to take another look and make any updates based on their recommendations.. ",
    "SergeyPirogov": "Well it will be great to mention this somewhere it the Readme. . Oh yes, sorry and thank you for your help.. I want to use newer api, but if it's not possible I am ok to use APIClien. ",
    "tjerkw": "I still see docker pull in bash running much faster than the python code. I guess its multithreaded?. ",
    "Amtime": "I didn't run any container, what a stupid boy, sorry about that. ",
    "thisismsreddy": "That's works like charm Thank you.. ",
    "sergei-ivanov": "Sorry if I am hijacking the topic here, but I think the problem we are seeing is related to this issue.\nWe have a .dockerignore file for a Maven project, where we are ignoring everything except for the files that we are copying into the image. We use explicit file paths in exclusions now, something like this:\n*\n!docker/Dockerfile\n!docker/run.sh\n!app1/target/*.jar\n!app1/src/main/resources/*.properties\n!app2/stub/target/*.jar\n!app2/stub/src/main/resources/*.properties\nAnd we tried to replace them with **, but that did not work at all, because the files are not added to the build context:\n*\n!docker/Dockerfile\n!docker/run.sh\n!**/target/*.jar\n!**/src/main/resources/*.properties\nResult:\nStep 6/19 : COPY app1/target/app1-0.0.1-SNAPSHOT-spring-boot.jar /usr/local/app1/app1-spring-boot.jar\nlstat app1/target/app1-0.0.1-SNAPSHOT-spring-boot.jar: no such file or directory. ",
    "johannespostler": "I can confirm this in the following setup:\nDocker version 17.09.0-ce\nPython 3.6.3\ndocker-py 2.7.0. ",
    "ornoone": "it seem this behavior is intended from the docker api (https://github.com/docker/swarmkit/issues/1394).\nyou must provide to your update all the parameters to preserve the old definition.. I have wrote a PR to add a Service.update_preserve for this (https://github.com/docker/docker-py/pull/1561). since no test seem to work for no apparent reason, and noone in the project seem to care about this, I copied my code in my solution, and it work in production. . ",
    "Nils-K": "OK, the reasoning behind that makes sense.\nHowever, the Python API doesn't provide a clean/easy way to set all required attributes making it very complicated to update a service (I currently have to divert to the Low-Level API).\nAre there plans to expand the Python API to either implement update() as per my initial suggestion, that is copying the entire existing service definition and apply the differences?\nOr, alternative expand the Service Object so that we can completely reconstruct the object ourselves? Currently one is only able to set all required parameters/attributes through the LLA.\n. ",
    "lachmanfrantisek": "Thank you for the correction.. ",
    "alex-dr": "Is something up with Janky PR tests? It seems like all recent PR's to the project are stuck waiting for this build to be scheduled. ",
    "aaossa": "Why does importing urllib3 from requests fails? Finding out this would avoid adding a dependency . Yes, but urllib3 has been a package from requests for a long time, so having just:\npython\nimport requests.packages.urllib3 as urllib3\nshouldn't fail. Also, docker-py requires requests==2.11.1, which should include urllib3. That's why I said that the import shouldn't fail. I think that avoid import urllib3, as you said, is the way to go.\nPS: Ups, missclick made me post this before I could finish writing it. ",
    "edmBernard": "I try to use APIClient : \npython\nclient = docker.from_env()\nserv = client.services.list()[0]\ndocker.APIClient.update_service(serv.id, version=serv.version+1, mode=docker.types.ServiceMode(\"replicated\", replicas=3))\nand I got this error : \nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python3.4/dist-packages/docker/utils/decorators.py\", line 29, in wrapper\n    if utils.version_lt(self._version, version):\nAttributeError: 'str' object has no attribute '_version'. I got it to work : \npython\nclient = docker.from_env()                                 \nserv = client.services.list()[0]\nserv.update(name=serv.name, command=\"replicas\", args=[\"2\"])\nthx for your help. ",
    "mxylin": "No, forcing API version to 1.20 doesn't make it work. (details later)\nCurrently, Docker Engine for Linux is not affected by this issue because it handles backward compatibility for VolumeDriver. (https://github.com/docker/docker/blob/master/runconfig/config_unix.go)\nThat is, when I want to create a container with a named volume and specify a volume driver using Docker SDK for Python:\n it works1 if DOCKER_HOST points to a Docker Engine \n it doesn't work2 if DOCKER_HOST points to a Docker Swarm (standalone) manager \nTo illustrate:\n\nThis works1:\n\ncreate container\n                with volume-driver\n    docker-py  --------------------> Docker Engine\n\nThis doesn't work2 because VolumeDriver got filtered/ignored at the Docker Swarm part:\n\ncreate container\n                with volume-driver\n    docker-py  --------------------> Docker Swarm ------> Docker Engine\n1: works: a new named volume is created using the specified volume driver.\n2: doesn't work: a new named volume is created using local driver.\nTest details of enforcing API version to 1.20\nTesting with docker-py/2.2.1, swarm/1.2.6\n```python\n\n\n\nimport docker\nclient = docker.from_env(version='1.20')\n/usr/local/lib/python2.7/dist-packages/docker/api/client.py:155: UserWarning: The minimum API version supported is 1.21, but you are using version 1.20. It is recommended you either upgrade Docker Engine or use an older version of Docker SDK for Python.\n  'Python.'.format(MINIMUM_DOCKER_API_VERSION, self._version)\nclient.version()\n{\n u'KernelVersion': u'4.4.0-66-generic',\n u'Os': u'linux',\n u'BuildTime': u'date -u',\n u'ApiVersion': u'1.22',\n u'Version': u'swarm/1.2.6',\n u'GitCommit': u'git rev-parse --short HEAD',\n u'Arch': u'amd64',\n u'GoVersion': u'go1.7.1'\n}\ncontainer = client.containers.create(image='ubuntu', command='uname', name='ubuntu-test', volume_driver='convoy', volumes={'testvol': {'bind': '/mnt/vol', 'mode': 'rw'}})\n```\n\n\n\nrequest dump of client.containers.create():\n```http\nPOST /v1.20/containers/create?name=ubuntu-test HTTP/1.1\nHost: localhost:2375\nAccept: /\nAccept-Encoding: gzip, deflate\nConnection: keep-alive\nContent-Length: 314\nContent-Type: application/json\nUser-Agent: docker-sdk-python/2.2.1\n{\n    \"Tty\": false,\n    \"NetworkDisabled\": false,\n    \"Image\": \"ubuntu\",\n    \"Cmd\": [\n        \"uname\"\n    ],\n    \"StdinOnce\": false,\n    \"HostConfig\": {\n        \"Binds\": [\n            \"testvol:/mnt/vol:rw\"\n        ],\n        \"NetworkMode\": \"default\"\n    },\n    \"AttachStdin\": false,\n    \"Volumes\": {\n        \"/mnt/vol\": {}\n    },\n    \"VolumeDriver\": \"convoy\",\n    \"AttachStderr\": true,\n    \"AttachStdout\": true,\n    \"OpenStdin\": false\n}\n```\nresult:\n$ docker volume ls\nDRIVER              VOLUME NAME\nlocal               testvol\nTesting with docker-py/1.10.6, swarm/1.2.6\n```python\n\n\n\nimport docker\nclient = docker.from_env(version='1.20')\nclient.version()\n{\n u'KernelVersion': u'4.4.0-66-generic',\n u'Os': u'linux',\n u'BuildTime': u'date -u',\n u'ApiVersion': u'1.22',\n u'Version': u'swarm/1.2.6',\n u'GitCommit': u'git rev-parse --short HEAD',\n u'Arch': u'amd64',\n u'GoVersion': u'go1.7.1'\n}\ncontainer = client.create_container(image='ubuntu', command='uname', name='ubuntu-test', volume_driver='convoy', volumes=['/mnt/vol'],host_config=client.create_host_config(binds=['testvol:/mnt/vol']))\n```\n\n\n\nrequest dump of client.create_container()\n```http\nPOST /v1.20/containers/create?name=ubuntu-test HTTP/1.1\nHost: localhost:2375\nAccept: /\nAccept-Encoding: gzip, deflate\nConnection: keep-alive\nContent-Length: 311\nContent-Type: application/json\nUser-Agent: docker-py/1.10.6\n{\n    \"Tty\": false,\n    \"NetworkDisabled\": false,\n    \"Image\": \"ubuntu\",\n    \"Cmd\": [\n        \"uname\"\n    ],\n    \"StdinOnce\": false,\n    \"HostConfig\": {\n        \"Binds\": [\n            \"testvol:/mnt/vol\"\n        ],\n        \"NetworkMode\": \"default\"\n    },\n    \"AttachStdin\": false,\n    \"Volumes\": {\n        \"/mnt/vol\": {}\n    },\n    \"VolumeDriver\": \"convoy\",\n    \"AttachStderr\": true,\n    \"AttachStdout\": true,\n    \"OpenStdin\": false\n}\n```\nresult:\n$ docker volume ls\nDRIVER              VOLUME NAME\nlocal               testvol\nReference\nThe volume_driver support was added to docker-py in docker/docker-py#628\n. Hi @shin- thanks for the fix. I've tested the master branch and here's some results (when creating a container on swarm/1.2.6 with a named volume and a specified volume driver)\n\nSuccess, if using low-level APIClient.\nContainer ubuntu-test is created, a new named volume testvol is created using the specified volume driver convoy\n\npython\nimport docker\nclient = docker.APIClient(base_url='tcp://swarm:2375',version='1.24')\ncontainer_id = client.create_container(\nimage='ubuntu', \ncommand='uname', \nname='ubuntu-test', \nvolumes=['/mnt/vol1'], \nhost_config=client.create_host_config(binds=['testvol:/mnt/vol1'],volume_driver='convoy')\n)\n\nFail, if using high-level DockerClient.\nContainer ubuntu-test is created, a new named volume testvol is created using the local driver, this is an unexpected result.\n\npython\nimport docker\nclient = docker.DockerClient(base_url='tcp://swarm:2375',version='1.24')\ncontainer = client.containers.create(\nimage='ubuntu', \ncommand='uname', \nname='ubuntu-test', \nvolume_driver='convoy', \nvolumes={'testvol': {'bind': '/mnt/vol', 'mode': 'rw'}}\n)\nFor the issue regarding DockerClinet, I see volume_driver is currently defined in the RUN_CREATE_KWARGS list (https://github.com/docker/docker-py/blob/master/docker/models/containers.py#L793). I guess moving it to the RUN_HOST_CONFIG_KWARGS might solve the issue, but I'm not quite sure yet how this affects backward compatibility for API versions 1.19 and 1.20.\nThanks!\n. ",
    "illera88": "Nice! Thank you for the pull requests.\nHope someone that has write access can approve it.  (@shin-)\nCheers. @shin- is there anything wrong with this pull request?. I think this is not a duplicated. The problem is that both parameters can't be used simultaneously not that the auto_remove parameter is actually remove.\nWhat is the duplicate issue?. ",
    "annatisch": "@shin- We would really be interested in this fix - is it likely to be merged soon? :). ",
    "Alphasite": "Not to pester you, but its been a couple of months, has there been any progress made in merging this? Its quite a nuisance to work around for complicated run commands.. ",
    "sheirys": "Is it possible to get stats() from multiple containers like in docker stats without delay? E.g.:\n```\n\n\n\nclient.containers.list()\n[, , , ]\nfor container in client.containers.list():\n...     stats = container.stats(stream=False)\n...     print(stats)\nwill print `stats()` with delay for each container. It would be nice to have something like:\nclient.containers.stats([, ], stream=False)\n```\nIs there anything similar?. \n\n\n",
    "shabbirkagalwala": "I just wanted to ask, so can this be done using python like by using multi threading or a different module? Or is this not possible at all?. Thank you guys @Chuseuiti @shin- ! I will let you know how it goes and possible post my final code here. Thank you again!. @shin- Thank you! Just a small correction, it is \"filters\" instead of \"filter\" took a while to get that figured out!. @TomasTomecek Its an apache-php container with a few additional components, which i start using docker run. Also I can see CPU % when i enter the docker stats command.\n\n. @thaJeztah Thank you! That makes it clear to why everything is zero. As far as I understand if I stream stats it will reset always. So now the question is how to keep the client connected to get the \"previous\" and \"current\" stats so that \"precpu_stats\" do not reset? . I am not sure if this is the right way to do this, but I got it working this way. \ndef calculateCPUPercentUnix(v):\n    cpuPercent = 0.0\n    previousCPU = v['precpu_stats']['cpu_usage']['total_usage']\n    cpuDelta = v['cpu_stats']['cpu_usage']['total_usage'] - previousCPU\n    previousSystem = v['precpu_stats']['system_cpu_usage']\n    systemDelta = v['cpu_stats']['system_cpu_usage'] - previousSystem\n    if systemDelta > 0.0 and cpuDelta > 0.0:\n        cpuPercent = (cpuDelta / systemDelta) * len(v['cpu_stats']['cpu_usage']['percpu_usage']) * 100\n        return \"{0:.2f}\".format(cpuPercent)\n    else:\n        return cpuPercent\n\n\nwhile True:\n    cont = client.containers.list(filters={'status':'running'})\n    for v in cont: \n         stats = client.api.stats(v.id,stream=True,decode=True)\n         while True:\n              for i in stats:\n                  try:\n                       cpuPercent = calculateCPUPercentUnix(i)\n                  except KeyError:\n                       continue\n                  print(cpuPercent)\n              break\n         break.\n",
    "Chuseuiti": "You can do this in python with the following code, just simply combine it with the multiprocessing python library and call each stats on a different process.\nimport docker\nclient=docker.from_env()\nclient_lowlevel = docker.APIClient(base_url='unix://var/run/docker.sock')\nclient1_stats=client_lowlevel.stats(container=\"server1\",decode=True, stream=False)\nclient2_stats=client_lowlevel.stats(container=\"server2\",decode=True, stream=False)\nGet Outlook for Androidhttps://aka.ms/ghei36\n\nFrom: Joffrey F notifications@github.com\nSent: Friday, January 26, 2018 7:16:36 PM\nTo: docker/docker-py\nCc: Chuseuiti; Author\nSubject: Re: [docker/docker-py] docker stats equivalent (#1546)\nIt's possible, and yes you'll probably need threading.\nYou can refer to the CLI code to figure out how it's done there ( https://github.com/docker/cli/blob/master/cli/command/container/stats.go ) - it's in Go, but uses the same API.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/docker/docker-py/issues/1546#issuecomment-360940585, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AF8yeBRBelIEQz5KgWTtfMowSwSRvkq7ks5tOmrkgaJpZM4Mzr67.\n. This is a full example, I didnt test it but it should do what you were mentioning. Parallel retrieval of the stats\n```\nimport docker\nimport multiprocessing as mp\noutput = mp.Queue()\ndef stats(server):\n    client=docker.from_env()\n    client_lowlevel = docker.APIClient(base_url='unix://var/run/docker.sock')\n    client_stats=client_lowlevel.stats(container=server,decode=True, stream=False)\n    output.put(client_stats)\nprocesses =[ mp.Process(target=stats, args=(server)) for server in ['server1','server2']]\nRun processes\nfor p in processes:\n    p.start()\nExit the completed processes\nfor p in processes:\n    p.join()\n```\nEdit. You could always put that code in a function to obtain the execution that you were mentioning\n```\nclient.containers.stats([, ], stream=False)\n```\n. ",
    "joebloggs12": "I'm having a similar issue - I try and pull an image using the following:\nimport docker\nclient = docker.from_env()\nclient.images.pull('bsidescbrctf/2017-crypto-docuprotect', tag='1')\nRaises the following:\ndocker.errors.ImageNotFound: 404 Client Error: Not Found (\"no such image: bsidescbrctf/2017-crypto-docuprotect: No such image: bsidescbrctf/2017-crypto-docuprotect:latest\")\nNot sure why the 1 tag is being replaced by latest.\nI have found a way around it by doing:\ntest = client.images.list('bsidescbrctf/2017-crypto-docuprotect')\nactual_image = test[0]\nBut I would prefer to be able to pull the actual image itself.. Is there a way to pass the tag without using the tag kwarg? Or perhaps some way of stopping the Build Error from being thrown?. ",
    "duanshiqiang": "@shin- Thanks for the workaround, but to be able to pull the actual image itself is better.. ",
    "antoineVerlant": "Yes, a long running container (odoo container), so I should have logs. I have logs when I do the docker logs command manually.\nIn my case it's not a problem.. @dacameron Thanks for your answer.\nIn fact it looks like that the network.connect(container) works now, I should have did mistake while testing.\nBut I finally use your \nfor network_name in instance.parse_network():\n            client.api.connect_container_to_network(container.name, network_name)\nWich is better for my code base. Thanks a lot.\nBut like you said, the client.containers.run(networks=[network_name]) still doesn't work.\n. Sry this is my first Pull Request ever, not use to it. Is there anything else I can do for you ?. Any news ?. Hey again, Thank for your answer.\nShoud I sign the 2 first commit too ?\nI admit I don't realy got how those test work and why it failed. Can you help me with this.\nTo use the debugger I used these test in my own project:\n   def test_create_service_with_volume_mount(self):\n        vol_name = 'banane'\n        print vol_name\n        container_spec = docker.types.ContainerSpec(\n            'nib:oo', ['ls'],\n            mounts=[\n                docker.types.Mount(target='/test', source=vol_name)\n            ]\n        )\n        # self.tmp_volumes.append(vol_name)\n        task_tmpl = docker.types.TaskTemplate(container_spec)\n        name = 'pomme'\n        client = docker.APIClient(base_url='unix://var/run/docker.sock')\n        svc_id = client.create_service(task_tmpl, name=name)\n        svc_info = client.inspect_service(svc_id)\n        print 'ContainerSpec' in svc_info['Spec']['TaskTemplate']\n        cspec = svc_info['Spec']['TaskTemplate']['ContainerSpec']\n        print 'Mounts' in cspec\n        print len(cspec['Mounts']) == 1\n        mount = cspec['Mounts'][0]\n        print mount['Target'] == '/test'\n        print mount['Source'] == vol_name\n        print mount['Type'] == 'volume'\nEvery print is true (except the first one which is banane).\nSo the test should work right ?\nIn the original code you don't test if the mount element is already parsed into a dictionary but should I change my else into an elif isintance ?\nThanks for your answer and time.. Here it is.\nThanks for your support. Thx, i will use this for now. Here is the best way I found. But it still not perfect as long at it maybe have more than one added image while the image is loaded\ndef load_image(self, node, application_image_version):\n        \"\"\"\n        Load an image for a saved_image tar file\n        :param node: node to connect\n        :param application_image_version: application image where to\n         get saved_image\n        :return: true for image loaded\n        \"\"\"\n        client = self.connect_node(node)\n        # Work around so I can have the image build\n        # Issue: https://github.com/docker/docker-py/issues/1623\n        # Get les images before load\n        images_list = client.images.list()\n        client.images.load(application_image_version.get_saved_image())\n        # Get les images after load so I can do difference to isolate image\n        images_list2 = client.images.list()\n        result_image_list = []\n        for image in images_list2:\n            if image not in images_list:\n                result_image_list.append(image)\n        if 1 < len(result_image_list) or 0 == len(result_image_list):\n            image = self.get_image_from_application_image_version(\n                node,application_image_version)\n        else:\n            image = result_image_list[0]\n            self.tag_image(application_image_version, image)\n        return bool(image). ",
    "sanimej": "@shin- verbose option was introduced in v1.28. I will add a version check and an error message similar to what is done for other optional flags. Thanks.. @shin- Added the version check. PTAL.. ",
    "andyneff": "@shin- I've cleaned it up how I think you were suggesting it. I tried a Mixin at first, but everything was static or needed the container api, so it ended up being moved to either the NvidiaAPIClient class or docker.utils.nvidia. When you get the chance, can you look through it and let me know if anything is unacceptable?\nI didn't look at an integration test yet because I didn't know if any of the CI systems have GPU support. Is there something you want to do about that?. @shin- Any idea why the travis-ci says it failed? When I look at the Details it says the Exit code should be zero.. @TomasTomecek I am trying to get nvidia support into docker compose. It started with the docker/compose#4657 issue.\ndocker-compose is a popular tool that I would like to use. Unfortunately I deal with GPU work (Cuda, opengl, and opencl). But there is no way for me to use docker-compose in its current state.\nA work around of \"Just add the devices and mounts\" myself in the docker compose yaml will only \"work on my computer\", as enough varies from computer to computer (version numbers, which particular devices get mounted in, etc...) For this reason I feel it is a very useful ability to leverage some of what nvidia-docker has created for us.\nSince create_container_config has to add a few things to accomplish mounting in the correct devices and volumes, I was following @shin- 's advice of creating a NvidiaAPIClient. One of the the important things to note is that APIClient is unaffected by these changes. But now other apps such as docker-compose can use the NvidiaAPIClient which will automatically set up nvidia drivers for nvidia enabled docker images (where the label com.nvidia.volumes.needed is nvidia_driver). @TomasTomecek any ideas for a plugin architecture that would be welcomed in the docker-py/compose projects?\nI believe what I've done can be converted to:\n\nA mixin for the APIClient in docker-py\nA way to add my own error handling to handle_connection_errors in compose.cli.errors\nRunning my tests. It would be nice to have a way of to run both my test and the existing test in the current test harness. Of course I would be running it on my own computer or my own CI system to test my plugin, not expecting the main docker-py/compose projects to run tests for plugins.\n\nI'm sure there are other endpoints that would be useful in a plugin system, but these three were sufficient for my case.. No long necessary with nvidia-docker2. @petergerten I'm not 100% sure on the docker-py call (as I use docker-compose which uses docker-py to do this) but I believe when you call create_container, you need to specify runtime=\"nvidia\"\nThe new nvidia-docker2 adds a runtime to docker daemon, so using the nvidia-docker2 plugin is as simple as specifying the nvidia runtime. For example, using the docker-cli:\ndocker run -it --rm nvidia/cuda nvidia-smi\nFails\nBut\ndocker run --runtime=nvidia -it --rm nvidia/cuda nvidia-smi\nSucceeds!\nIf this docker-cli test doesn't work for you, then I believe you don't have nvidia-docker2 installed successfully.\n\nBackground\nThe nvidia-docker cli used to be a shim that sets up the nvidia specific settings for you (volumes, devices, etc...) if and only if a label existed. This is what this PR was originally reproducing. Now the nvidia-docker2 cli is a script that just adds the --runtime=nvidia flag for you. This new runtime is a modified version of the docker default runtime, that will add the nvidia specific volumes, devices, etc if and only if an environment variable is set.\nSo because runtime is part of the docker, docker-py, and docker-compose API, there is no need for this PR.. ### My Environemnt\nbash\npipenv install docker==3.3.0\npipenv run python\nRunning Docker 18.03.1-ce on Fedora 27.\n\n```python\nimport docker\nc=docker.from_env()\nFailes\nc.containers.run('nvidia/cuda', command='nvidia-smi')\nSucceeds\nc.containers.run('nvidia/cuda', command='nvidia-smi', runtime='nvidia')\n``. @petergerten If this does not work, can you please verify the contents of/etc/docker/daemon.json??? . @petergerten, can you give me the output ofnvidia-docker version` (no dashes)?\n\nIt looks like you might be using nvidia-docker version 1, not version 2. This would explain why nvidia-docker run -it --rm nvidia/cuda nvidia-smi works, however I'm surprised nvidia-docker run --runtime=nvidia -it --rm nvidia/cuda nvidia-smi does work, but I'll just toss that up to luck.\nOn Fedora and Ubuntu, only the stable (not edge) releases (every 3 months, so 18.03, 18.06, etc...) have nvidia-container-runtime releases precompiled. So in order to use nvidia-docker2, you can only use these stable releases. The package manager takes care of that for you.\nHere's is an arch package for nvidia-docker2 https://aur.archlinux.org/packages/nvidia-docker2/\n~~I've never tried it on arch, but I'm guessing...~~\n\n~~Install /etc/docker/daemon.json~~\n~~Install /usr/bin/nvidia-docker~~\n~~Now for the tricky part. Install nvidia-container-runtime. I have never attempted this myself, and haven't the faintest clue how, but I'm afraid you'll have to build this from source on arch, unless someone else has.~~\n\n~~If you decide you want to go down this road, I offer no guarantee this even works on arch~~\n~~Every time you update the docker daemon, you will have to rebuild the nvidia-container-runtime to match the runc.~~. @petergerten That's great! It's possible the arch package maker does release for ever Edge build, which would be awesome. But if that is not the case, I think you need to be careful if it's really the \"docker 18.03 nvidia-container-runtime\" running in docker 18.04. This may work perfectly, or may not. Just keep that in mind if anything crazy starts happening. (The only way I know of to check this is to ask the package manage what version of nvidia-container-runtime is installed, in my case it says 2.0.0-1.docker18.03.1). @joebloggs12 To work around it, I tagged the version after building in a separate command. But it look like #1581 will solve this problem once and for all when it is released.. That is up for debate. But this requirement is only for the nvidia support. All other use of docker-py will not require this of course. This is captured in the nvidia_docker_compatible function. So I'm open to suggestions to changing the logic of that function. Right now it checks three things\n\nIs this linux. Nvidia-docker only works on non-virtual machine instances of linux dockers. So this is limited to Linux hosts only\nIs the nvidia-driver installed. If you aren't using an nvidia card or the nvidia driver (nouveau for example) then there is use for this either\nIs the nvidia-docker executable found. This would indicate the \"plugin\" is installed\n\nThe nvidia-docker plugin does two things.\n\nCreate a special nvidia-docker mount called nvidia_driver_{version_number}. This includes all the .so files and binaries necessary to use nvidia/cuda in a docker. This is not a simple process and is all coded up in go in the nvidia-docker project. Due to the complexity and superior knowledge nvidia will have and the chance that they will be updating this, I choose to not even consider reproducing this.\nMounting the necessary devices and a special nvidia driver library mount into the container.\n\nI'm reproducing the second of these two in python. The first is accomplished by calling nvidia-docker on a dummy container to trigger it in create_nvidia_driver. This is why I choose to make nvidia-docker a requirement for using the NvidiaAPIClient.. ",
    "scott-vsi": "This looks promising. When is it going to be available? . ",
    "petergerten": "@andyneff I still don't understand what the solution would be to start a nvidia-docker container with docker-py. Could you provide a simple example?. @andyneff thanks.\nI know how to use the docker cli but currently try to use docker-py to start an nvidia runtime container.\nI indeed tried to use runtime=\"nvidia\" as well as several variations of it but always get\ndocker.errors.APIError: 400 Client Error: Bad Request (\"Unknown runtime specified nvidia\")\n. @andyneff \nRunning Docker 18.04.0-ce on Arch.\nOk, this might not be docker-py related:\nnvidia-docker run --runtime=nvidia -it --rm nvidia/cuda nvidia-smi\nworks, but\ndocker run --runtime=nvidia -it --rm nvidia/cuda nvidia-smi\ndoes not work. (Same \"Unknown runtime\" error)\n[pg@x240 ~]$ docker --version\nDocker version 18.04.0-ce, build 3d479c0af6\n[pg@x240 ~]$ nvidia-docker --version\nDocker version 18.04.0-ce, build 3d479c0af6\nI do not have a /etc/docker/daemon.json file on my system.. @andyneff \nthanks a lot! I got it working now. It was my fault - I was indeed still using nvidia-docker version 1. But the --version  output confused me. version without dashes showed that I was using v1.\nI switched to nvidia-docker2 from arch AUR and now have it working (even with the non stable release):\n```\nnvidia-docker version\nNVIDIA Docker: 2.0.3\nClient:\n Version:   18.04.0-ce\n API version:   1.37\n Go version:    go1.10.1\n Git commit:    3d479c0af6\n Built: Tue Apr 17 22:46:17 2018\n OS/Arch:   linux/amd64\n Experimental:  false\n Orchestrator:  swarm\nServer:\n Engine:\n  Version:  18.04.0-ce\n  API version:  1.37 (minimum version 1.12)\n  Go version:   go1.10.1\n  Git commit:   3d479c0af6\n  Built:    Tue Apr 17 22:46:48 2018\n  OS/Arch:  linux/amd64\n  Experimental: false\n```\n. ",
    "fmobus": "Hi,\nDo you have any updates on this pull request?. ",
    "dacameron": "You could try \npython\nclient.api.connect_container_to_network(container.name, network_name)\nwhich is what I did to get around this bug. I can't get \npython\nclient.containers.run(networks=[network_name])\nto work either.. ",
    "anirudh-chhangani": "can anyone from the core team acknowledge the issue, any timeline when this might get fixed.. ",
    "cyli": "@shin How would you feel about a tls option in the setup.py?  So people would have to pin or pip install docker[tls]?. (I'm not sure what is causing the windows failure). @shin- Ah ok, thank you!  Will update.. @shin- I believe there a wheels for windows:  https://pypi.org/project/cryptography/1.8.1/#files . @shin- Looks like we were using an older version of tox/virtualenv in appveyor, which installed an older version of pip, which for some reason did not find a matching wheel.. > Do we need those changes to requirements.txt?\nI wasn't sure if this was in reference to the appveyor failure or a more general question about the extra packages in the requirements.txt in this particular PR.  If the latter, I have mainly been following the advice in https://caremad.io/posts/2013/07/setup-vs-requirement/ and https://packaging.python.org/discussions/install-requires-vs-requirements/#requirements-files, where the setup.py has the dependencies required to install the package, generally, and requirements.txt pins the exact versions of both dependencies and dependencies of dependencies, to make sure that CI is repeatable.\nHappy to dump that in something else, like requirements.dev.txt or something like that though if you'd prefer.. No worries, thank you!. ah, good point, thanks!. ",
    "tommy2049": "after reading the docs, i realized client = docker.from_env(version='1.24') did the trick for me. I should stop being so lazy and read more.. ",
    "yueyongyue": "I also found the. ",
    "andrewreece": "Agreed - in particular, the force argument in Network.disconnect() would be useful to have available.\nFor example, sometimes a container is actually already gone, but it is still listed with a docker network inspect.  In that case, from the command line the -f flag is needed, otherwise it returns an error, \"No such container\".  The same problem occurs in this Python implementation when force is unavailable.. ",
    "AlexeyRokhin": "@shin- \nDone. Please check.\nShould I do anything with your review?. Preparation for https://github.com/docker/compose/issues/4582. @shin- \nCould you please clarify how to do this? I've never did rebase.... Closing in favor of PR 1619. @shin- \nIs it possible to include this fix into docker-py 2.3.1 in order to release it before compose 1.14?\nWithout this fix compose will throw error when nano_cpus >2^32 (cpus >= ~2.2).. ",
    "lyager": "I've added a pull-reguest for the issue here: https://github.com/docker/docker-py/pull/1573. @hussam-almarzoq Could you maybe provide a codesnippet?. What I did using the above patch was (without using the underlaying API):\nBuild the containers needed: client.build \nCreate the containers: client.create\nAttach containers to network, using aliases: network.connect(container, aliases=[])\nAnd start the container: container.start()\nNot a solution to you API request, I agree, but maybe as a workaround?\n. @hussam-almarzoq Haven't used swarm services much. Sorry. ",
    "hussam-almarzoq": "@lyager But this does not help with services .. @lyager The APIClient has a method called create_container. This method has a parameter named networking_config where I can attach the container to networks and add aliases. This parameter is not available to the high level containers api nor the swarm services api even in low level ones.. @lyager Works good enough with containers. But what about swarm services ?. ",
    "overmike": "Is there any update on this issue?\nto whom may interest, I circumvent by using the engine api networkattachment format\ndocker_client.services.create(....., networks=[{'Target': 'data', 'Aliases': ['mysql']}]). We are using a private registry with token auth setup. I can install the plugin by docker cli but fail to use docker-py to install.\nfrom the code, seems like only \"pull_plugin\" add the \"X-Registry-Auth\" header but not in \"plugin_privileges\". Does this mean either (1) the token auth service should allow unauth plugin privilege query, or (2) docker-py should include auth header when query plugin privilege\n\n\n\nd.plugins.install(remote_name='redacted.host/volume')\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/local/lib/python2.7/site-packages/docker/models/plugins.py\", line 182, in install\n    privileges = self.client.api.plugin_privileges(remote_name)\n  File \"/usr/local/lib/python2.7/site-packages/docker/utils/decorators.py\", line 34, in wrapper\n    return f(self, args, *kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/docker/api/plugin.py\", line 171, in plugin_privileges\n    return self._result(self._get(url, params=params), True)\n  File \"/usr/local/lib/python2.7/site-packages/docker/api/client.py\", line 228, in _result\n    self._raise_for_status(response)\n  File \"/usr/local/lib/python2.7/site-packages/docker/api/client.py\", line 224, in _raise_for_status\n    raise create_api_error_from_http_exception(e)\n  File \"/usr/local/lib/python2.7/site-packages/docker/errors.py\", line 31, in create_api_error_from_http_exception\n    raise cls(e, response=response, explanation=explanation)\ndocker.errors.APIError: 500 Server Error: Internal Server Error (\"Get http://redacted.host/v2/volume/manifests/latest: unauthorized: authentication required\")\n\n\n\n$ docker plugin install redacted.host/volume\nPlugin \"redacted.host/volume\" is requesting the following privileges:\n - network: [host]\n - mount: [/dev]\n - mount: [/var/lib/docker]\n - allow-all-devices: [true]\n - capabilities: [CAP_SYS_ADMIN]\nDo you grant the above permissions? [y/N]\n..... May I know when is the tentative 3.4.0 release date?. ",
    "robkooper": "I added the following code to work around this issue for now:\nmode = docker.types.ServiceMode(\"replicated\", int(count))\nif int(count) == 0:\n    mode.get('replicated')['Replicas'] = 0. ",
    "lindycoder": "right, will be fixed by #1545 \nshould i close this?. This seems addressed better in #1545. I've just got this new error on the same condition with 2.4.1\nValueError: Invalid port \"_ServicePort(target=8000, published=8001, protocol=None, mode=None, external_ip=None)\", should be [[remote_ip:]remote_port[-remote_port]:]port[/protocol]\nEDIT:\nPip freeze\nbackports.ssl-match-hostname==3.5.0.1\ncached-property==1.3.0\ncertifi==2017.4.17\nchardet==3.0.4\ncolorama==0.3.9\ndocker==2.4.1\ndocker-compose==1.14.0\ndocker-pycreds==0.2.1\ndockerpty==0.4.1\ndocopt==0.6.2\nidna==2.5\njsonschema==2.6.0\nPyYAML==3.12\nrequests==2.18.1\nsix==1.10.0\ntexttable==0.8.8\nurllib3==1.21.1\nwebsocket-client==0.44.0\nFull trace back \nTraceback (most recent call last):\n  File \"/tmp/myvenv/bin/docker-compose\", line 11, in <module>\n    sys.exit(main())\n  File \"/tmp/myvenv/lib/python3.4/site-packages/compose/cli/main.py\", line 68, in main\n    command()\n  File \"/tmp/myvenv/lib/python3.4/site-packages/compose/cli/main.py\", line 118, in perform_command\n    handler(command, command_options)\n  File \"/tmp/myvenv/lib/python3.4/site-packages/compose/cli/main.py\", line 926, in up\n    scale_override=parse_scale_args(options['--scale']),\n  File \"/tmp/myvenv/lib/python3.4/site-packages/compose/project.py\", line 424, in up\n    get_deps\n  File \"/tmp/myvenv/lib/python3.4/site-packages/compose/parallel.py\", line 69, in parallel_execute\n    raise error_to_reraise\n  File \"/tmp/myvenv/lib/python3.4/site-packages/compose/parallel.py\", line 167, in producer\n    result = func(obj)\n  File \"/tmp/myvenv/lib/python3.4/site-packages/compose/project.py\", line 410, in do\n    rescale=rescale\n  File \"/tmp/myvenv/lib/python3.4/site-packages/compose/service.py\", line 460, in execute_convergence_plan\n    self.show_scale_warnings(scale)\n  File \"/tmp/myvenv/lib/python3.4/site-packages/compose/service.py\", line 205, in show_scale_warnings\n    if self.specifies_host_port() and desired_num > 1:\n  File \"/tmp/myvenv/lib/python3.4/site-packages/compose/service.py\", line 983, in specifies_host_port\n    return any(has_host_port(binding) for binding in self.options.get('ports', []))\n  File \"/tmp/myvenv/lib/python3.4/site-packages/compose/service.py\", line 983, in <genexpr>\n    return any(has_host_port(binding) for binding in self.options.get('ports', []))\n  File \"/tmp/myvenv/lib/python3.4/site-packages/compose/service.py\", line 963, in has_host_port\n    _, external_bindings = split_port(binding)\n  File \"/tmp/myvenv/lib/python3.4/site-packages/docker/utils/ports.py\", line 60, in split_port\n    _raise_invalid_port(port)\n  File \"/tmp/myvenv/lib/python3.4/site-packages/docker/utils/ports.py\", line 43, in _raise_invalid_port\n    'port[/protocol]' % port)\nValueError: Invalid port \"ServicePort(target=8000, published=8001, protocol=None, mode=None, external_ip=None)\", should be [[remote_ip:]remote_port[-remote_port]:]port[/protocol]. Seems to be working, thank you!. ",
    "aaronthebaron": "@shin- let me know if these changes fit the bill. Thanks.. ",
    "tomyan": "Hi @Larsjep \nThanks for working on a fix for this. Any idea when it's likely to be merged?\nThanks\nTom. ",
    "edmorley": "I may be misreading, but it looks like moby/moby#33935 has re-added the very bits that were intentionally removed in golang/go#20150, in an attempt to fix the test failures (due to cache hashes now being different) seen in moby/moby#33892?. ",
    "oskarpearson": "Hi @shin- and @Larsjep\nDocker CE 17.12.0 has just been released, and includes \"Bump Go to 1.9.2\" for both moby and the cli. See the Runtime section of https://github.com/docker/docker-ce/releases/tag/v17.12.0-ce\nIs there a way I can test this patch out?\nThanks for your help!. ",
    "r631269": "looking forward to this. is it stuck indefinitely, or is there still hope?. ",
    "doonhammer": "I upgraded/installed setup tools into my virtualenv and the problem went away.  The version of setup tools installed is 35.0.2.py2.7. ",
    "terminalmage": "@shin- That's a good idea. Would this be best added to the DaemonApiMixin, or to the APIClient itself?. Since the _auth_configs are initially created in the APIClient, it's probably best to locate it there.. Hmm, maybe not. It looks like, from a design perspective, the functionality in the APIClient is largely from the mixins, there are only a couple public functions in APIClient proper.. @shin- I've removed the earlier commit and added the function to the DaemonApiMixin. Let me know if you think other changes need to be made.. @shin- Thanks for the comments, I believe I have addressed them all.. :+1: . Fair enough, though it does make comparisons slightly more complicated. For our use case we are managing a named container with a desired configuration by using the create_container function in the APIClient to create a temporary container, and then comparing the Config and HostConfig in the existing and temp container for differences, replacing the existing container with the temp if any were discovered. If a container was created outside of docker-py, you end up with one container having an empty list for a given value and None for the other.\nHowever:\n\nI can see how trying to match the CLI defaults can be a pain in the butt, as any changes to the defaults in Docker itself will cause docker-py to drift from Docker. Playing whack-a-mole like this is no fun.\nIt can be worked around easily by just doing a boolean comparison when one side is None. bool([]) is the same thing as bool(None), and the passing of any DNS options would keep them from being set to None in the created container anyway.\n\nGiven the above, and since this isn't considered incorrect behavior, I'll go ahead and close this. I appreciate the feedback!. @shin- How does one perform the same reload when using the APIClient? I'm seeing something similar when running docker.client.APIClient.networks(). Each network's containers key is an empty dict.. @shin- It looks, like you said, that hitting the endpoint for a specific ID (via docker.client.APIClient.inspect_network()) will get you the details for the containers.\nJust adding a friendly reminder here to give the APIClient a little love if/when this enhancement is implemented.. I've also tried client.prune_networks(filters={'label': 'foo!=bar'}) even though it didn't seem to match the engine docs, and that also didn't work.. Oh, duh. I was thinking of code re-use and obviously did not test this part of the code. Good catch. Since I'm moving this to the APIClient I'll just revert these two changes in the login func.. ",
    "lshahar": "found issue at my code, not longer relevant\n. ",
    "BrendanDrewZippy": "More info:\npython --version\nPython 3.6.1\ninstalled via homebrew, pip freeze shows\nappdirs==1.4.3\ndocker==2.2.1\ndocker-pycreds==0.2.1\ngitdb2==2.0.0\nGitPython==2.1.3\npackaging==16.8\npyparsing==2.2.0\nrequests==2.14.0\nsix==1.10.0\nsmmap2==2.0.1\ntqdm==4.11.2\nvirtualenv==15.1.0\nwebsocket-client==0.40.0. D'oh. Sorry about that -- should have caught that. Thanks!. ",
    "medzila": "Hello shin-\nThanks a lot. Just a little correction for others that have the same question:\n```python\nfrom docker import APIClient\nfrom docker.types import Mount, ContainerSpec, TaskTemplate, DriverConfig\ncli = APIClient()\ndriver_cfg = DriverConfig(name=None, options={\n  'o': 'addr=10.x.x.x',\n  'device': ':/docker/pgtest'\n  'type': 'nfs4'\n})\nm = Mount(type='volume', target='/var/lib/postgresql/data', driver_config=driver_cfg, source=None)\nc_spec = ContainerSpec('medzilla/postgres', mounts=[m])\ntask_tmpl = TaskTemplate(c_spec)\ncli.create_service(task_tmpl, name='pgtest')\n```\nKind regards,\nMedzila.. This should work I guess.\nMount(type='volume', source='my-volume', target='/path/in/container', labels={'color': 'red', 'shape': 'round'}). ",
    "alfred-landrum": "Closing, issue is in the engine:\nhttps://github.com/moby/moby/issues/33407\n. ",
    "stevvooe": "How about status_code >= 400? That will ensure that it is resilient to changes. . LGTM (not a maintainer). ",
    "allencloud": "@shin- @stevvooe \nThanks for your feedback. Currently I tried to modify the comparison to be >=400 to take into legacy api into consideration.\nAnd Yeah, actually I understand the consequence of changing status code, while I think we still need to pay more attention on locking the APIs and be very careful.\n. ",
    "bdeluca": "I found, 1602, but the low level api doesn't appear to have all the things I need to add.. ",
    "Dids": "Here's a quote from the file above:\nrm (bool): Remove intermediate containers. The ``docker build``\n                command now defaults to ``--rm=true``, but we have kept the old\n                default of `False` to preserve backward compatibility\nDo you still need to maintain compatibility? If so, there needs to be a better way to do so, instead of breaking compatibility with new defaults.. The issue is that it's not removing intermediate containers by default, since it's not using the default value. To me, it'd make more sense to use default docker build values everywhere, even if it means dropping support for older Docker versions. I'll work around this for now, I guess.. ",
    "asottile": "Hitting this as well, is there a good reason for the inconsistency with cli docker build?. ugh right, github editor doesn't sign commits. very very minor, noticed this while debugging another issue. @shin- does this look ok?. ",
    "haizaar": "May be the Return section the docs should clarify that image/logs are returned after build is done. Currently it mentions word \"generator\" and for those of us who migrate from older lower-level API (docker.from_env().api.build(...)) there is a place for confusion.. Can anyone please update whether there are any plans to have this implemented soon?. Thanks for aligning expectations :). Hit this one as well - need to show logs as they come. Ended up using low level API: from_env().api.build(). > Request for build secret support: #2230\nDid you mean to link to my original ticket? :). ",
    "biran0079": "It is fixed in master.. I see. Thanks for explanation!. ",
    "datwiz": "added integration test to tests/integration/model_images_test.py to check that lines in the build output stream are correctly handled and the last success message is used to extract the new image id.. oops - failed style check - thought I ran that before the commit - fixing.... ",
    "madhuri-rai07": "Hi, This is my first commit to docker repo. Please let me know if I need to change anything. Thanks!. @shin- Hi Can you help merge this? Thanks.. @shin- Thank you for reviewing. I have updated the patch.. ",
    "hongbin": "LGTM. @shin- Done. @shin- when you said \"I'm not able to set a different MAC address for the network\", could you provide the exact reproducing steps? I will follow up with that.. ",
    "guedressel": "~~I ran into the same error. Tough I'm using an older version of docker-py (1.8.1) it reads like the handling of the repositories  is limited.~~\n~~Since we activated Googles gcr.io repositories the list in \\~/.docker/config.json was quite long (>10) what lead to this issue.~~\nNever mind my comment. It turned out that I had some bad mixtures of package dependencies on my system. After creating a fresh pipenv and reinstalling docker-py everything is up and running again...\n. ",
    "BBerastegui": "Just in case someone ends here like I ended:\npip install requests==2.11.1. ",
    "scriptonist": "Thanks, @shin- This workaround works perfectly.But I used another hacky way to implement the same.\nThanks for the information.Will come handy in near future.. ",
    "mindcurv-nandaraj": "container.exec_run(['sh', '-c', 'echo 5 | python test.py']) in this command where to specify the container name? . ",
    "edouardtheron": "@shin- Your workaround works great, it just solved a problem I have been facing for a few hours now, so thank you. \nHowever, I don't understand why it is needed to prefix the required command with sh -c... If you have time to explain it would be appreciated, thanks again!\nEDIT\nI think I found the explanation here: https://askubuntu.com/a/230482. ",
    "faheel": "@mindcurv-nandaraj \npython3\nclient = docker.from_env()\ncontainer = client.containers.get('enter container ID here')\ncontainer.exec_run(...). ",
    "maaydin": "@shin- work around makes no sense :(\nException v2.3.0:\nException in thread Thread-1:\nTraceback (most recent call last):\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py\", line 810, in __bootstrap_inner\n    self.run()\n  File \"__init__.py\", line 63, in run\n    exec_start= llclient.exec_start(detach=False, tty=False, stream=True, socket=False)\n  File \"/Library/Python/2.7/site-packages/docker/utils/decorators.py\", line 35, in wrapper\n    return f(self, *args, **kwargs)\n  File \"/Library/Python/2.7/site-packages/docker/utils/decorators.py\", line 19, in wrapped\n    'Resource ID was not provided'\nNullResource: Resource ID was not provided. ",
    "zero57": "@shin- Any chance on gettting this merged? Here's a minimal working example of the error occurring:\nCreate a local docker-machine: docker-machine create -d virtualbox testmachine\nThen run:\n```\nimport docker\nimport multiprocessing as mp\nimport re\nimport subprocess\ndef foo(client):\n    pass\nif name == 'main':\n    cmd = ['docker-machine', 'config', 'testmachine']\n    p = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, _ = p.communicate()\n    output = stdout.decode('utf-8')\n    regex = \"\"\"(--tlsverify\\n)?--tlscacert=\"(.+)\"\\n--tlscert=\"(.+)\"\\n--tlskey=\"(.+)\"\\n-H=(.+)\"\"\"\n    match = re.match(regex, output.strip())\n    tlsverify, tlscacert, tlscert, tlskey, host = match.group(1, 2, 3, 4, 5)\n    tlsverify = bool(tlsverify)\nif tlsverify:\n    base_url = host.replace('tcp://', 'https://')\n    tls = docker.tls.TLSConfig(client_cert=(tlscert, tlskey),\n                               ca_cert=tlscacert,\n                               verify=True)\n\n\n    client = docker.DockerClient(base_url=base_url, tls=tls)\n\n    pool = mp.Pool()\n    job = pool.apply_async(foo, args=[client])\n    job.wait()\n\n```\nThe error:\nProcess ForkPoolWorker-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n    task = get()\n  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 345, in get\n    return _ForkingPickler.loads(res)\n  File \"/tmp/testlocal/lib/python3.6/site-packages/requests/adapters.py\", line 142, in __setstate__\n    block=self._pool_block)\n  File \"/tmp/testlocal/lib/python3.6/site-packages/docker/transport/ssladapter.py\", line 40, in init_poolmanager\n    'assert_hostname': self.assert_hostname,\nAttributeError: 'SSLAdapter' object has no attribute 'assert_hostname'. @shin- Yup, also rebased on top of latest upstream/master.. ",
    "unitymind": "Builds fails due flake8. @shin- Hmm.. seems like Docker bug, because...\njoin_swarm(remote_addrs, join_token) (with remain two default None) will never went through and never ends up successful, but always fires through 400 Client Error: Bad Request (\"invalid ListenAddr \"\": invalid empty address\").\nPR is invalid, but issue still actual - default listen_addr=None is not work\njoin_swarm(remote_addrs, join_token, None, advertise_addr) - will not work too.\nAs workaround - we should set default to listen_addr='0.0.0.0:2377'\n. ",
    "mattoberle": "I found that this block of code from urllib3 was the failure point in my case:\n``python\n    def _get_timeout(self, timeout):\n        \"\"\" Helper that always returns a :class:urllib3.util.Timeout` \"\"\"\n        if timeout is _Default:\n            return self.timeout.clone()\n    if isinstance(timeout, Timeout):\n        return timeout.clone()\n    else:\n        # User passed us an int/float. This is for backwards compatibility,\n        # can be removed later\n        return Timeout.from_float(timeout)\n\n```\ntimeout was an instance of urllib3.util.timeout.Timeout\nTimeout was an instance of requests.packages.urllib3.util.timeout.Timeout\nWhen the isinstance(timeout, Timeout) check fails the urllib3.util.timeout.Timeout object is passed as the connection argument to requests.packages.urllib3.util.timeout.Timeout.from_float.\nIn the docker-py repository this import business seems to be an integral part of the issue:\ntry:\n    import requests.packages.urllib3 as urllib3\nexcept ImportError:\n    import urllib3\nCommenting out the try/except and only using import urllib3 prevented the error from occurring.\nIt looks like a recent commit to the requests library breaks compatibility with this commit:\nhttps://github.com/requests/requests/commit/168109f13c135d719a0be21ce220534e4b657aed\nIn the meantime I've found that pinning requests to version 1.17.3 side-steps the issue.. Here is the issue in the requests project regarding the problem:\nhttps://github.com/requests/requests/issues/4160\nPR #1646 places the version on the list of incompatible versions.. ",
    "sbenderli": "Looks like requests released twice today: 2.18.0 was problematic, but 2.18.1 seems not to have this issue\nhttps://github.com/requests/requests/commits/master\nedit: We don't have set requirements for the requests module, so we get whatever docker-py brings in. Previously (prior to 10am today) requests 2.17.3 was coming along, and things were ok. After 10am, we started getting 2.18.0 while we saw the errors, and finally around 1pm 2.18.1 came along and fixed the issue.. Mystery explained, thanks @mattoberle . ",
    "ChrsMark": "@shin- could you plz review?. @shin-, comments addressed, also rebased on top of the latest upstream/master. Hey guys,\nI could not reproduce this behavior though.\n```\n\n\n\nimport docker\nd = docker.from_env()\n[n.name for n in d.networks.list()]\n[u'none', u'host', u'apptest_default', u'ansibleplayground_default', u'bridge']\n[n.containers for n in d.networks.list()]\n[[], [], [], [], []]\n`pip freeze | grep docker && python --version && docker version`\n-e git+https://github.com/ChrsMark/docker-py.git@bb148380e1ea92e26c8e4dc783dd18b8a39506c0#egg=docker\ndocker-pycreds==0.2.1\nPython 2.7.6\nClient:\n Version:      17.03.1-ce\n API version:  1.27\n Go version:   go1.7.5\n Git commit:   c6d412e\n Built:        Mon Mar 27 17:10:36 2017\n OS/Arch:      linux/amd64\n\n\n\nServer:\n Version:      17.03.1-ce\n API version:  1.27 (minimum version 1.12)\n Go version:   go1.7.5\n Git commit:   c6d412e\n Built:        Mon Mar 27 17:10:36 2017\n OS/Arch:      linux/amd64\n Experimental: false\n``` . Cannot reproduce:\n- Docker version 1.13.1\n```\n\n\n\nimport docker\ncli = docker.from_env()\ncli.services.list(filters={'name': 'redis'})\n[]\ncli.services.list(filters={'name': 'redis1'})\n[]\ndocker.version\n'3.4.0-dev'\n```\n. @shin-, integration tests failed apparently, any ideas plz?\n\n\n\nedit: amending the commit to trigger the tests again. Tests passed.\n@shin- could you review please?. ",
    "matthewtberry": "@shin- connect_container_to_network() and disconnect_container_from_network() check 'image' as resource ID.  Shouldn't these check 'container' instead?. @shin- Yeah I can do both!  Thanks.. ",
    "rahulrajaram": "I think the problem is with the file, docker-2.6.1-py2.py3-none-any.whl, listed here.\nUnzipping docker-2.6.1-py2.py3-none-any.whl shows that it does not contain a setup.py file, which is where the check for docker-py is defined. This looks like a problem in my opinion.\nI have two questions following the above observations:\n\nIs it necessary to publish docker-2.6.1-py2.py3-none-any.whl?\nIs there a way that one can prefer downloading docker-2.6.1.tar.gz instead of docker-2.6.1-py2.py3-none-any.whl through pip install ...?. Thanks for your response @shin- . \n",
    "LuisPiedra": "I have started to work in a pull request, but I have noticed that maybe most of network drivers does not honour the configuration provided for MAC address, so integration tests are failing.\nI have filled an issue also in https://github.com/docker/libnetwork/issues/1811. ",
    "darkl0rd": "Docker version: 17.05.0-ce\nDocker Python version: 2.3.0. From swarmkit code: https://github.com/docker/swarmkit/blob/master/manager/controlapi/service.go\nline 755-763:\n           // It's not okay to update Service.Spec.Networks on its own.\n    // However, if Service.Spec.Task.Networks is also being\n    // updated, that's okay (for example when migrating from the\n    // deprecated Spec.Networks field to Spec.Task.Networks).\n    if (len(request.Spec.Networks) != 0 || len(service.Spec.Networks) != 0) &&\n        !reflect.DeepEqual(request.Spec.Networks, service.Spec.Networks) &&\n        reflect.DeepEqual(request.Spec.Task.Networks, service.Spec.Task.Networks) {\n        return grpc.Errorf(codes.Unimplemented, errNetworkUpdateNotSupported.Error())\n\n}. Further investigation confirm that the spec defined by docker itself differs from what the Docker Python API does.\nServices created/updated by the Docker CLI tools result in having networks under Spec/TaskTemplate/Networks, whereas updating a Service through the Docker Python API still results in defining it as Spec/Networks.\n. Any update on this one?. Keep in mind that the actual behavior is not as lenient as the commit message makes you believe.\nIf you create a Service using the new TaskSpec structure and then update (well, re-create really) the service using docker-py it will just blatantly fail -- see OP.. I'm trying to point out that this is what the docker-py API currently breaks.\n\nCreate a Docker Service using the docker CLI, this will be created in the new format.\nThen attempt to update this service using docker-py (service.update()) - this will work the first time, but \"re-create\" the service using the old format.\nAttempting to update the service again will now fail.\n. \n",
    "ColdHeat": "@darkl0rd were you ever able to work around this or did you end up subprocess'ing the docker CLI?. ",
    "mdantonio": "I experienced a similar problem with docker-compose after upgrading docker-py to 2.4\nUsing both docker-compose 1.14 and 1.13 and the following configuration:\n```\nversion: '3'\nservices:\n  myservice:\n    image: myimage\n    ports:\n      - \"80:80\"\n```\nI obtain the following error:\n```\ndocker-compose up\nERROR: for myservice expected string or bytes-like object\nTraceback (most recent call last):\n  File \"/usr/local/bin/docker-compose\", line 11, in \n    sys.exit(main())\n  File \"/usr/local/lib/python3.5/dist-packages/compose/cli/main.py\", line 68, in main\n    command()\n  File \"/usr/local/lib/python3.5/dist-packages/compose/cli/main.py\", line 118, in perform_command\n    handler(command, command_options)\n  File \"/usr/local/lib/python3.5/dist-packages/compose/cli/main.py\", line 926, in up\n    scale_override=parse_scale_args(options['--scale']),\n  File \"/usr/local/lib/python3.5/dist-packages/compose/project.py\", line 424, in up\n    get_deps\n  File \"/usr/local/lib/python3.5/dist-packages/compose/parallel.py\", line 69, in parallel_execute\n    raise error_to_reraise\n  File \"/usr/local/lib/python3.5/dist-packages/compose/parallel.py\", line 167, in producer\n    result = func(obj)\n  File \"/usr/local/lib/python3.5/dist-packages/compose/project.py\", line 410, in do\n    rescale=rescale\n  File \"/usr/local/lib/python3.5/dist-packages/compose/service.py\", line 460, in execute_convergence_plan\n    self.show_scale_warnings(scale)\n  File \"/usr/local/lib/python3.5/dist-packages/compose/service.py\", line 205, in show_scale_warnings\n    if self.specifies_host_port() and desired_num > 1:\n  File \"/usr/local/lib/python3.5/dist-packages/compose/service.py\", line 983, in specifies_host_port\n    return any(has_host_port(binding) for binding in self.options.get('ports', []))\n  File \"/usr/local/lib/python3.5/dist-packages/compose/service.py\", line 983, in \n    return any(has_host_port(binding) for binding in self.options.get('ports', []))\n  File \"/usr/local/lib/python3.5/dist-packages/compose/service.py\", line 963, in has_host_port\n    _, external_bindings = split_port(binding)\n  File \"/usr/local/lib/python3.5/dist-packages/docker/utils/ports.py\", line 57, in split_port\n    match = PORT_SPEC.match(port)\nTypeError: expected string or bytes-like object\n```\nUsing compose 1.12 it still works\nI added a couple of prints in split_port to confirm that compose calls the split_port function by passing a non-string argument:\ndef split_port(port):\n    print(port)\n    print(type(port))\n    match = PORT_SPEC.match(port)\nAnd this is the output:\n```\n80:80\n\n80:80\n\nServicePort(target=80, published=80, protocol=None, mode=None, external_ip=None)\n\nERROR: for myservice  expected string or bytes-like object\nTraceback (most recent call last):\n[...] (previous stack)\n  File \"/usr/local/lib/python3.5/dist-packages/docker/utils/ports.py\", line 59, in split_port\n    match = PORT_SPEC.match(port)\nTypeError: expected string or bytes-like object\n```\nI think that the problem was introduced with this commit: 5dd91cd4aaa2e7cd8dde1dd316d53cab25ef9b78\nPrior of this commit the cast to string of the input port (parts = str(port).split(':')) was robust enough to withstand this call from compose\n. ",
    "aellwein": "Same error here. I used \npip install docker-compose docker==2.3.0\nto avoid the error with docker-compose \"ports\" notations.. ",
    "digitaldavenyc": "This is causing a pretty big issue on Circle CI right now. ",
    "gregflynn": "If you're on Arch and this bit you:\nsudo pacman -U /var/cache/pacman/pkg/python-docker-2.3.0-1-any.pkg.tar.xz\nassuming 2.3 is still in your pacman cache, confirmed the downgrade fixed it for me locally. ",
    "zikphil": "Yes same here. ",
    "sono-bfio": "My work around:\n```\npip uninstall docker\npip uninstall docker-compose\npip install docker==2.3.0\npip install docker-compose==1.14.0\n```\nReferencing the docker-compose issue as well:\n* https://github.com/docker/compose/issues/4972\n. ",
    "treemore": "if you use localhost:7001:7001 ,the problem will be again ,however 127.0.0.1:7001:7001 is ok.\ndocker-compose version 1.15.0, build e12f3b9\ndocker-py version: 2.4.2\nCPython version: 2.7.10\n. ",
    "ajomadlabs": "@shin- @impredicative Can I take up this issue. @shin- Is this issue taken up by someone, if not can I take this . ",
    "mrosales": "Sorry, shouldn't have included the ecr bit in the issue. That was just how I was first made aware of the deprecation. I was collecting the URL and token from the ecr python sdk, and then using docker-py sdk to call docker login (not including an email field). \nThe code segment worked before upgrading to docker 17.0.6 yesterday, and then it stopped working. Calling the command line docker login with the token and URL from the python ECR command works fine. \nThis was the call I made. Using the same param values here in the command line worked fine, but failed in docker-py\ndocker.login(\n                username=\"AWS\",\n                password=auth_data['authorizationToken'],\n                registry=auth_data['proxyEndpoint']\n               )\nThe error is:\ndocker.errors.APIError: 500 Server Error: Internal Server Error (\"login attempt to https://<AWS_ID>.dkr.ecr.<REGION>.amazonaws.com/v2/ failed with status: 400 Bad Request\"). My suspicion is that the problem is here. The docker-py login command to log into a registry still passes an email parameter no matter what. This parameter was removed 17.0.6 and this is now an API error. \nI'm, guessing that the expected behavior is that when no email parameter value is specified, the parameter will be dropped from the login request body.. ",
    "abuckenheimer": "any update here? The only way I can get around this right now is to login via the shell in a subprocess and then instantiate the APIClient. ",
    "timwah": "I had the same issue until I realized that the tokens are base64 encoded - RTFM fail :(.\n```\nsession = boto3.Session()\necr = session.client('ecr')\nlogin = ecr.get_authorization_token()\nb64token = login['authorizationData'][0]['authorizationToken'].encode('utf-8')\nusername, password = base64.b64decode(b64token).decode('utf-8').split(':')\nregistry = login['authorizationData'][0]['proxyEndpoint']\nclient = docker.from_env()\nclient.login(username, password, registry=registry)\n```. ",
    "Dvelezs94": "thanks @timwah that was it!. ",
    "branflake2267": "The workaround I used to login to aws docker repo. \n\nRun aws ecr get-login \nRun the output of get-login. (Remove the flag -e none in the comand). Look for successful login output.\nRun cat ~/.docker/config.json to verify. . \n",
    "Juanlu001": "Yeah, definitely pip 7.1.2 is quite old (August 2015).. Comparing https://travis-ci.org/docker/docker-py/jobs/249976694 and https://travis-ci.org/docker/docker-py/jobs/255634259, it seems that Ubuntu Precise has Python 3.3 but Trusty doesn't.\nAnd regarding Windows, I don't have much experience with tox so I can't help.. ",
    "ymoses": "Of course. Thanks!. This isn't a bug, I simply misunderstood the API:\nhttps://github.com/moby/moby/issues/34121. ",
    "Anagha5": "Hi, \nI'm facing error with docker while using it with Azure ML Workbench. Unable to pull the image. Below are the error details. Please let me know how to go about the issue \nError :\n{'Error': MlCliError({'Error': 'Unable to start container', 'Response Content': APIError(HTTPError('500 Server Error: Internal Server Error for url: http+docker://localnpipe/v1.30/images/create?tag=1&fromImage=mlcrpacr95813916189d.azurecr.io%2Faicimage',),)},), 'Azure-cli-ml Version': '0.1.0a27.post3'}\n. ",
    "chaseDream1314": "A new error has emerged after fixing the error above. The 500 server error can be solved by switching to linux container. It is easy to switch windows container to linux container by right clicking on the docker icon. My error are as follow:\n(if you want to see more detailed information about the error, there is a url:\nhttps://social.msdn.microsoft.com/Forums/en-US/5ff623a0-2768-43e1-84e8-81657d60c985/to-deploy-the-train-model-of-image-classification-cntk-with-docker-locally?forum=MachineLearning\n) \n[Local mode] Running docker container.\n[Local mode] Pulling the image from mlcrpacr8fb197558338.azurecr.io/imgclassapi1:18. This may take a few minutes, depending on your connection speed...\n[Local mode] Pulling...............................................\nContainer port: 32770\n[Local mode] Waiting for container to initialize...{\n    \"Azure-cli-ml Version\": \"0.1.0b2.post2\",\n    \"Error\": \"Error, container failed to initialize. Please run 'az ml service logs realtime -i imgclassapi1' to determine the cause.\". I just want to say why it is so hard to start a container? I am eager to deploy the rest API using azyre workbench, however, It's difficulty is amazing!\n. ",
    "leroix": "In this case no, I'm testing from within a docker container.\nI've verified by checking the value of client._auth_configs.. ",
    "FromZeus": "Yep, sure. ",
    "lokimwenga": "I figured out the problem...it seems that when this stream option is set to \"True\", the first streaming response comes through with nil values, but then changes on subsequent responses to show all values under \"precpu_stats\". . ",
    "bolshakov": "@shin- Done, review please. I'm not sure if using any \"special\" message is a good idea. . I decided to check how it would be rendered with None and found it a bit unfriendly:\nCommand 'echo Hi' in image 'e9aa60c60128' returned non-zero exit status 42: None\n\nFor me it is not clear what does None mean in this message. . Good catch. @shin- what should I do -- remove it or keep it?. I agree that hardcoding logging drivers is not the best option, however Docker API doesn't provide information about logging driver abilities. Do you think catching docker.errors.APIError is better approach?\npython\ntry:\n    out = container.logs(stdout=stdout, stderr=stderr)\nexcept docker.errors.APIError:\n    out = None\nFor me the only disadvantage is that we can distinguish different types of api errors only by error message.. I consider client.containers.run as replacement for docker run command. Docker run command does not fail if you run it with non readable driver. \nIf you need different logic based on the error you are always able to run two commands:\npython\ncontainer = client.containers.run(\"busybox\", detach=True, log_config=dict(type='none'))\ncontainer.logs()\nTraceback (most recent call last):\n.....\ndocker.errors.APIError: 500 Server Error: Internal Server Error (\"configured logging driver does not support reading\")\nThis pull request does not change logs() method behaviour, it affect only run method. Currently it is impossible to run attached container with non-readable logging driver.. Folks, I'm pretty sure there is always the better way doing something. The point of this PR was to overcome the pain of using not readable logging driver. But I'm wonder how we come to discussing why it was written this way in the past?\nThis is my first contribution in python project. Moreover this is the first time I'm programming python at all. I'm does not feel anouth courage to rewrite the whole method at this point.. ",
    "robertknox": "I think I am:\nI have rewritten it to make things more obvious:\ndockerAPIClient = docker.APIClient\n        networking_config = dockerAPIClient.create_networking_config({\n           'network1': dockerAPIClient.create_endpoint_config(\n                ipv4_address=ip\n           )\n        })\nipv4_address=ip\nTypeError: unbound method create_endpoint_config() must be called with APIClient instance as first argument (got nothing instead)\nI very much appreciate you help.\n. Thanks very much for taking the time to answer. \nBut things are still not working. Now docker-py is complaining about the name: \"networking_config\"\nTypeError: run() got an unexpected keyword argument 'networking_config'\ndockerAPIClient = docker.APIClient()\n        networking_config = dockerAPIClient.create_networking_config({\n           'network1': dockerAPIClient.create_endpoint_config(\n                ipv4_address=\"172.27.153.11\"\n          )\n        })\n    options = {\n        \"networking_config\": networking_config,\n        \"detach\": True,\n        # \"remove\": True,\n        \"name\": \"mbussim_\" + ip,\n        \"cap_add\": \"NET_ADMIN\",\n        \"network\": \"br0\",\n        \"volumes\": {self.instance_dir_src_dir: {'bind': '/src', 'mode': 'rw'}}\n    }\n\n    self.dockcon = self.dockerclient.containers.run(self.dimg, **options)\n\n. I tried this:\nand got:docker.errors.APIError: 500 Server Error: Internal Server Error (\"Container cannot be connected to multiple networks with one of the networks in private (none) mode\")\nThanks again for taking the time to respond.\n. I left out the \"network_mode\" option I did get this to work thanks to you.\n\noptions = {\n        \"detach\": True,\n        \"name\": \"mbussim_\" + ip,\n        \"cap_add\": \"NET_ADMIN\",\n        \"volumes\": {self.instance_dir_src_dir: {'bind': '/src', 'mode': 'rw'}}\n    }\nself.dockcon = self.dockerclient.containers.run(self.dimg, **options)\nnetwork = self.dockerclient.networks.get('br0')\nnetwork.connect(self.dockcon, ipv4_address='172.27.153.11')\n\nI could then ping 172.27.153.11.\n\nI can't tell you how appreciative I am. Thanks very much. But I am a little frustrated as well.\nThis seems to be documented no where, and deviates from the original Docker usage considerably.\nThanks again.\n-r\n. It seems that right after  this call: network.connect(self.dockcon, ipv4_address='172.27.153.11')\nI get this error.\n\ndocker.errors.APIError: 500 Server Error: Internal Server Error (\"service endpoint with name mbussim_172.27.153.5 already exists\")\nI am wondering if I need to flush out the endpoint service name?\nHow would I do that?\n. It looks like docker is trying to start another container?\nI took out the \"name\" option and tried again and got the same error:\ndocker.errors.APIError: 500 Server Error: Internal Server Error (\"service endpoint with name mystifying_hugle already exists\")\nmystifying_hugle was of course the auto generated name that came into being at the time the docker container was created: at :\noptions = {\n            \"detach\": True,\n            \"cap_add\": \"NET_ADMIN\",\n            \"entrypoint\": \"/app/run_static_ip.sh\",\n            \"network\": \"br0\",\n            \"volumes\": {self.instance_dir_src_dir: {'bind': '/app', 'mode': 'rw'}}\n        }\n    self.dockcon = self.dockerclient.containers.run(self.dimg, **options)\n    network = self.dockerclient.networks.get('br0')\n    network.connect(self.dockcon, ipv4_address='172.27.153.11')\n\nAll we are tying to do is assign the ip number ( 172.27.153.11 ) to the container.\n. I tried both approaches. They both work.\nI can't help but think simply passing in the ip address in run options would be the easy way to do this.\nWhy should we have to connect and disconnect and connect just to give the container an ip?\nOn the command line docker accepts an \"--ip\" option. Why should docker-py be different?\nRegardless,\nThanks again for taking the time to respond. I can move forward.\n-r\n. ",
    "davidCarlos": "I'm having the same problem. In a long log message, part of it is not getting printed to stdout.. ",
    "guyisra": "using docker logs doesn't reproduce the issue, only using docker-py. version 1.10.6. ",
    "keis": "What I'm seeing at docker==2.4.2 is output chunked at the 16k boundary. So a longer message is yielded out of logs(stream=True) as multiple 16k messages.\nCan this be tweaked?\nIs it safe to assume 16k chunks can be rebuffered by the consumer?\nedit: this 16k chunk size seems to be what is given by the daemon in _multiplexed_response_stream_helper. ",
    "GuangTianLi": "I found the exec_run if use TTY, The docker socket\u2018s protocol is different from attach\uff0c why?. ",
    "mathewtrivett": "Can answer my own question.  Apologies.  You just include the arguments in the command string, e.g.\nclient.containers.run(image, command = ['osrm-extract -p /opt/car.lua /data/berlin-latest.osm.pbf']. ",
    "AndreasBackx": "Thank you!. ",
    "tylerjones4508": "I also tried client.services.get(service_id) and client.services.list(**kwargs) and got the same result.\nroot@tjones:~# docker service ls\nID                  NAME                MODE                REPLICAS            IMAGE               PORTS\n4j7snwo1d5ln        TEST                replicated          1/1                 nginx:latest        \nroot@tjones:~# python \nPython 2.7.12 (default, Nov 19 2016, 06:48:10) \n[GCC 5.4.0 20160609] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import docker\n>>> client = docker.from_env()\n>>> client.services.get('4j7snwo1d5ln')\nTraceback (most recent call last):\n   File \"<stdin>\", line 1, in <module>\nAttributeError: 'function' object has no attribute 'get'\n>>> \n>>> \n>>> \n>>> client.services.list()\nTraceback (most recent call last):\n   File \"<stdin>\", line 1, in <module>\nAttributeError: 'function' object has no attribute 'list'\n>>>\n\n. ",
    "daytonpid": "I would also like to reference #1571 and #1769.\nDear maintainers, 3 Issues is a bit much spanning over 6 months :-1: for a critical usability bug.\nNot to mention 2 seperate pull requests.. @shin- \nThank you very much for this, was running into a wall here too. Now we wait :). I would also like to reference #1571 and #1712 which explains the issue.\nDear maintainers, 3 Issues is a bit much spanning over 6 months :-1: for a critical usability bug.\n@shin- @shin- @shin- @shin- @shin- @shin- @shin- . ",
    "jonathanchristison": "I reduced the reproducer further the exec command executed previous is irrelevant \n```\nimport docker\nimport pdb\nclass Test(docker.APIClient):\n    def init(self, containerid=\"867cb24efc3490357904b10e56e40e5621ab18157a9cd2665380a94a46db4759\",\n            port=0, tag=None, name=None,\n            archived=False, image=None, cport=4873):\n        self.containerid = containerid\n        super(Test, self).init(base_url='unix://var/run/docker.sock')\n        at=self.attach(self.containerid, logs=True)\nt = Test()\n. I'm guessing there is something with my setup because even basic usage fails with the same error -\nimport docker\nimport pdb\ncid = \"867cb24efc3490357904b10e56e40e5621ab18157a9cd2665380a94a46db4759\"\nclass Test(docker.APIClient):\n    def init(self, containerid=cid):\n        self.containerid = containerid\n        super(Test, self).init(base_url='unix://var/run/docker.sock')\n        at=self.attach(self.containerid)\nSimple test\nclient = docker.from_env()\nca = client.containers.get(cid)\nca.attach()\nWith inheritance\nt = Test()\n``\n . @shin- thanks for the suggestions, downgrading unfortunately does not work and the docker container being attached to seems not to be relevant, it happens with both latest alpine and centos images, started withdocker run -it alpine`  \n```\nimport docker\ncid=\"2412ca14994c4e28eaa2a9afe8608e151a03d1e383ba831502d4a07450eb8028\" #centos\nprint docker.version\nSimple test\nclient = docker.from_env()\nca = client.containers.get(cid)\nca.attach()\n```\nOutput\npython reproducer.py \n2.4.2\nTraceback (most recent call last):\n  File \"reproducer.py\", line 9, in <module>\n    ca.attach()\n  File \"/home/jonny/Src/JBoss/jboss-prod/npmvirt/lib/python2.7/site-packages/docker/models/containers.py\", line 70, in attach\n    return self.client.api.attach(self.id, **kwargs)\n  File \"/home/jonny/Src/JBoss/jboss-prod/npmvirt/lib/python2.7/site-packages/docker/utils/decorators.py\", line 19, in wrapped\n    return f(self, resource_id, *args, **kwargs)\n  File \"/home/jonny/Src/JBoss/jboss-prod/npmvirt/lib/python2.7/site-packages/docker/api/container.py\", line 55, in attach\n    return self._read_from_socket(response, stream)\n  File \"/home/jonny/Src/JBoss/jboss-prod/npmvirt/lib/python2.7/site-packages/docker/api/client.py\", line 366, in _read_from_socket\n    socket = self._get_raw_response_socket(response)\n  File \"/home/jonny/Src/JBoss/jboss-prod/npmvirt/lib/python2.7/site-packages/docker/api/client.py\", line 278, in _get_raw_response_socket\n    sock = response.raw._fp.fp._sock\nDocker inspect output\ndocker inspect 2412ca14994c4e28eaa2a9afe8608e151a03d1e383ba831502d4a07450eb8028\n[\n    {\n        \"Id\": \"2412ca14994c4e28eaa2a9afe8608e151a03d1e383ba831502d4a07450eb8028\",\n        \"Created\": \"2017-08-19T19:18:03.704144478Z\",\n        \"Path\": \"/bin/bash\",\n        \"Args\": [],\n        \"State\": {\n            \"Status\": \"running\",\n            \"Running\": true,\n            \"Paused\": false,\n            \"Restarting\": false,\n            \"OOMKilled\": false,\n            \"Dead\": false,\n            \"Pid\": 29442,\n            \"ExitCode\": 0,\n            \"Error\": \"\",\n            \"StartedAt\": \"2017-08-19T19:18:04.350601839Z\",\n            \"FinishedAt\": \"0001-01-01T00:00:00Z\"\n        },\n        \"Image\": \"sha256:328edcd84f1bbf868bc88e4ae37afe421ef19be71890f59b4b2d8ba48414b84d\",\n        \"ResolvConfPath\": \"/var/lib/docker/containers/2412ca14994c4e28eaa2a9afe8608e151a03d1e383ba831502d4a07450eb8028/resolv.conf\",\n        \"HostnamePath\": \"/var/lib/docker/containers/2412ca14994c4e28eaa2a9afe8608e151a03d1e383ba831502d4a07450eb8028/hostname\",\n        \"HostsPath\": \"/var/lib/docker/containers/2412ca14994c4e28eaa2a9afe8608e151a03d1e383ba831502d4a07450eb8028/hosts\",\n        \"LogPath\": \"/var/lib/docker/containers/2412ca14994c4e28eaa2a9afe8608e151a03d1e383ba831502d4a07450eb8028/2412ca14994c4e28eaa2a9afe8608e151a03d1e383ba831502d4a07450eb8028-json.log\",\n        \"Name\": \"/competent_poitras\",\n        \"RestartCount\": 0,\n        \"Driver\": \"devicemapper\",\n        \"MountLabel\": \"\",\n        \"ProcessLabel\": \"\",\n        \"AppArmorProfile\": \"\",\n        \"ExecIDs\": null,\n        \"HostConfig\": {\n            \"Binds\": null,\n            \"ContainerIDFile\": \"\",\n            \"LogConfig\": {\n                \"Type\": \"json-file\",\n                \"Config\": {}\n            },\n            \"NetworkMode\": \"default\",\n            \"PortBindings\": {},\n            \"RestartPolicy\": {\n                \"Name\": \"no\",\n                \"MaximumRetryCount\": 0\n            },\n            \"AutoRemove\": false,\n            \"VolumeDriver\": \"\",\n            \"VolumesFrom\": null,\n            \"CapAdd\": null,\n            \"CapDrop\": null,\n            \"Dns\": [],\n            \"DnsOptions\": [],\n            \"DnsSearch\": [],\n            \"ExtraHosts\": null,\n            \"GroupAdd\": null,\n            \"IpcMode\": \"\",\n            \"Cgroup\": \"\",\n            \"Links\": null,\n            \"OomScoreAdj\": 0,\n            \"PidMode\": \"\",\n            \"Privileged\": false,\n            \"PublishAllPorts\": false,\n            \"ReadonlyRootfs\": false,\n            \"SecurityOpt\": null,\n            \"UTSMode\": \"\",\n            \"UsernsMode\": \"\",\n            \"ShmSize\": 67108864,\n            \"Runtime\": \"runc\",\n            \"ConsoleSize\": [\n                0,\n                0\n            ],\n            \"Isolation\": \"\",\n            \"CpuShares\": 0,\n            \"Memory\": 0,\n            \"NanoCpus\": 0,\n            \"CgroupParent\": \"\",\n            \"BlkioWeight\": 0,\n            \"BlkioWeightDevice\": null,\n            \"BlkioDeviceReadBps\": null,\n            \"BlkioDeviceWriteBps\": null,\n            \"BlkioDeviceReadIOps\": null,\n            \"BlkioDeviceWriteIOps\": null,\n            \"CpuPeriod\": 0,\n            \"CpuQuota\": 0,\n            \"CpuRealtimePeriod\": 0,\n            \"CpuRealtimeRuntime\": 0,\n            \"CpusetCpus\": \"\",\n            \"CpusetMems\": \"\",\n            \"Devices\": [],\n            \"DeviceCgroupRules\": null,\n            \"DiskQuota\": 0,\n            \"KernelMemory\": 0,\n            \"MemoryReservation\": 0,\n            \"MemorySwap\": 0,\n            \"MemorySwappiness\": -1,\n            \"OomKillDisable\": false,\n            \"PidsLimit\": 0,\n            \"Ulimits\": null,\n            \"CpuCount\": 0,\n            \"CpuPercent\": 0,\n            \"IOMaximumIOps\": 0,\n            \"IOMaximumBandwidth\": 0\n        },\n        \"GraphDriver\": {\n            \"Data\": {\n                \"DeviceId\": \"1354\",\n                \"DeviceName\": \"docker-253:1-67446110-80ebf34456fa7c34607f8d8f741dcc46394dd496ec866f3afb568148892d25f5\",\n                \"DeviceSize\": \"10737418240\"\n            },\n            \"Name\": \"devicemapper\"\n        },\n        \"Mounts\": [],\n        \"Config\": {\n            \"Hostname\": \"2412ca14994c\",\n            \"Domainname\": \"\",\n            \"User\": \"\",\n            \"AttachStdin\": true,\n            \"AttachStdout\": true,\n            \"AttachStderr\": true,\n            \"Tty\": true,\n            \"OpenStdin\": true,\n            \"StdinOnce\": true,\n            \"Env\": [\n                \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n            ],\n            \"Cmd\": [\n                \"/bin/bash\"\n            ],\n            \"ArgsEscaped\": true,\n            \"Image\": \"centos\",\n            \"Volumes\": null,\n            \"WorkingDir\": \"\",\n            \"Entrypoint\": null,\n            \"OnBuild\": null,\n            \"Labels\": {\n                \"build-date\": \"20170801\",\n                \"license\": \"GPLv2\",\n                \"name\": \"CentOS Base Image\",\n                \"vendor\": \"CentOS\"\n            }\n        },\n        \"NetworkSettings\": {\n            \"Bridge\": \"\",\n            \"SandboxID\": \"5986a62f03576171df6d479cacc90c1ff111f01c0915fa362fd2b4e2bf775660\",\n            \"HairpinMode\": false,\n            \"LinkLocalIPv6Address\": \"\",\n            \"LinkLocalIPv6PrefixLen\": 0,\n            \"Ports\": {},\n            \"SandboxKey\": \"/var/run/docker/netns/5986a62f0357\",\n            \"SecondaryIPAddresses\": null,\n            \"SecondaryIPv6Addresses\": null,\n            \"EndpointID\": \"e90d66213d6fa33582c95678a3c39063707e042802221481ead7fc25a85ac194\",\n            \"Gateway\": \"172.17.0.1\",\n            \"GlobalIPv6Address\": \"\",\n            \"GlobalIPv6PrefixLen\": 0,\n            \"IPAddress\": \"172.17.0.4\",\n            \"IPPrefixLen\": 16,\n            \"IPv6Gateway\": \"\",\n            \"MacAddress\": \"02:42:ac:11:00:04\",\n            \"Networks\": {\n                \"bridge\": {\n                    \"IPAMConfig\": null,\n                    \"Links\": null,\n                    \"Aliases\": null,\n                    \"NetworkID\": \"504ebd5a2b07b7a869b9dc3e36c6092a1721238d9a7bca8f274ac5963df8cce2\",\n                    \"EndpointID\": \"e90d66213d6fa33582c95678a3c39063707e042802221481ead7fc25a85ac194\",\n                    \"Gateway\": \"172.17.0.1\",\n                    \"IPAddress\": \"172.17.0.4\",\n                    \"IPPrefixLen\": 16,\n                    \"IPv6Gateway\": \"\",\n                    \"GlobalIPv6Address\": \"\",\n                    \"GlobalIPv6PrefixLen\": 0,\n                    \"MacAddress\": \"02:42:ac:11:00:04\",\n                    \"DriverOpts\": null\n                }\n            }\n        }\n    }\n]\nI'll do a bit more digging on Monday and also try and reproduce this on another system, its likely to be something odd with my setup if you've not seen this before. . I'm able to reproduce this on another system with an older version of docker, docker-py & python (RHEL7), so maybe the problem has been there for a while but no one has used attach with python 2.7 and docker-py >=2.0.0 ? \n```\npython --version && docker version\nPython 2.7.5\nClient:\n Version:         1.10.3\n API version:     1.22\n Package version: docker-common-1.10.3-46.el7.10.x86_64\n Go version:      go1.6.2\n Git commit:      2a93377-unsupported\n Built:           Fri Jul 29 13:45:25 2016\n OS/Arch:         linux/amd64\nServer:\n Version:         1.10.3\n API version:     1.22\n Package version: docker-common-1.10.3-46.el7.10.x86_64\n Go version:      go1.6.2\n Git commit:      2a93377-unsupported\n Built:           Fri Jul 29 13:45:25 2016\n OS/Arch:         linux/amd64\npython -c \"import docker; print docker.version\"\n2.0.0\n``. Ok i think I figured why this is happening, I call attach withoutstream=True` which means the requests request object will not create the raw socket object which response.raw._fp.fp._sock expects - \nsee https://github.com/requests/requests/blob/205755834d34a8a6ecf2b0b5b2e9c3e6a7f4e4b6/requests/models.py#L602\nSo maybe def attach needs to change to stream=True as default and _read_from_socket and _get_raw_response_socket altered to handle request objects without the raw socket data. ",
    "vitkhab": "\ud83d\udc4d. ",
    "mbelang": "Build failed, don't know why, I didn't touch that part of the code.\nHere is the error:\n20:33:19 [py2.7_1.13.1] =================================== FAILURES ===================================\n20:33:19 [py2.7_1.13.1] ________________________ PluginTest.test_enable_plugin _________________________\n20:33:19 [py2.7_1.13.1] tests/integration/api_plugin_test.py:49: in test_enable_plugin\n20:33:19 [py2.7_1.13.1]     assert self.client.enable_plugin(SSHFS)\n20:33:19 [py2.7_1.13.1] docker/utils/decorators.py:34: in wrapper\n20:33:19 [py2.7_1.13.1]     return f(self, *args, **kwargs)\n20:33:19 [py2.7_1.13.1] docker/api/plugin.py:85: in enable_plugin\n20:33:19 [py2.7_1.13.1]     self._raise_for_status(res)\n20:33:19 [py2.7_1.13.1] docker/api/client.py:222: in _raise_for_status\n20:33:19 [py2.7_1.13.1]     raise create_api_error_from_http_exception(e)\n20:33:19 [py2.7_1.13.1] docker/errors.py:31: in create_api_error_from_http_exception\n20:33:19 [py2.7_1.13.1]     raise cls(e, response=response, explanation=explanation)\n20:33:19 [py2.7_1.13.1] E   APIError: 500 Server Error: Internal Server Error (\"error setting up propagated mount dir: invalid argument\")\n20:33:19 [py2.7_1.13.1] === 1 failed, 250 passed, 10 skipped, 2 xfailed, 1 xpassed in 328.52 seconds ===\nIf somebody could help me understand the nature of the problem, it would be appreciated.\n. Hum I guess a test is/was flaky, rerunning without any change: All green!. no worries :). ",
    "pstrzelczak": "Exposing test case\nhttps://gist.github.com/pstrzelczak/d4fe49b42afa887d613ab5230f7ea368#file-docker-py-1732-py\nOutput on Centos 7.3 with Docker version 1.13.1, build b303bf6/1.13.1:\n(docker-venv)[strzelczak@xxx ~]$ python docker-py-1732.py \nTraceback (most recent call last):\n  File \"docker-py-1732.py\", line 12, in <module>\n    assert output == \"hello\\n\", \"Not hello? '%s'\" % output\nAssertionError: Not hello? ''\n(docker-venv)[strzelczak@xxx ~]$\nTest case rewritten using docker REST API and requests_unixsocket library (no checks though)\nhttps://gist.github.com/pstrzelczak/0fa2b0866a8311bbbba9c44a8054021b#file-docker-py-1732-requests-py\n. strace of exposing test case on Centos 7.3 with Docker version 1.13.1, build b303bf6/1.13.1:\n(docker-venv)[strzelczak@xxx ~]$ strace -f -e trace=network -s1024 python docker-py-1732.py\nsocket(PF_INET6, SOCK_STREAM, IPPROTO_IP) = 12\nbind(12, {sa_family=AF_INET6, sin6_port=htons(0), inet_pton(AF_INET6, \"::1\", &sin6_addr), sin6_flowinfo=0, sin6_scope_id=0}, 28) = 0\nProcess 21432 attached\nProcess 21433 attached\n[pid 21433] +++ exited with 0 +++\n[pid 21432] --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=21433, si_status=0, si_utime=0, si_stime=0} ---\n[pid 21432] +++ exited with 0 +++\n--- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=21432, si_status=0, si_utime=0, si_stime=0} ---\nProcess 21434 attached\nProcess 21435 attached\n[pid 21435] +++ exited with 0 +++\n[pid 21434] --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=21435, si_status=0, si_utime=0, si_stime=0} ---\n[pid 21434] +++ exited with 0 +++\n--- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=21434, si_status=0, si_utime=0, si_stime=0} ---\nProcess 21436 attached\nProcess 21437 attached\n[pid 21437] +++ exited with 0 +++\n[pid 21436] --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=21437, si_status=0, si_utime=0, si_stime=0} ---\n[pid 21436] +++ exited with 0 +++\n--- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=21436, si_status=0, si_utime=0, si_stime=0} ---\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 3\nconnect(3, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(3, \"GET /version HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: docker-sdk-python/2.5.1\\r\\n\\r\\n\", 148, 0, NULL, 0) = 148\nrecvfrom(3, \"HTTP/1.1 200 OK\\r\\nApi-Version: 1.26\\r\\nContent-Type: application/json\\r\\nDocker-Experimental: false\\r\\nServer: Docker/1.13.1 (linux)\\r\\nDate: Wed, 30 Aug 2017 12:25:38 GMT\\r\\nContent-Length: 263\\r\\n\\r\\n{\\\"Version\\\":\\\"1.13.1\\\",\\\"ApiVersion\\\":\\\"1.26\\\",\\\"MinAPIVersion\\\":\\\"1.12\\\",\\\"GitCommit\\\":\\\"b303bf6/1.13.1\\\",\\\"GoVersion\\\":\\\"go1.7.4\\\",\\\"Os\\\":\\\"linux\\\",\\\"Arch\\\":\\\"amd64\\\",\\\"KernelVersion\\\":\\\"3.10.0-514.26.2.el7.x86_64\\\",\\\"BuildTime\\\":\\\"2017-07-03T15:57:33.186109768+00:00\\\",\\\"PkgVersion\\\":\\\"<unknown>\\\"}\\n\", 8192, 0, NULL, NULL) = 450\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 4\nconnect(4, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(4, \"POST /v1.26/images/create?tag=latest&fromImage=busybox HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: docker-sdk-python/2.5.1\\r\\nContent-Length: 0\\r\\n\\r\\n\", 209, 0, NULL, 0) = 209\nrecvfrom(4, \"HTTP/1.1 200 OK\\r\\nApi-Version: 1.26\\r\\nContent-Type: application/json\\r\\nDocker-Experimental: false\\r\\nServer: Docker/1.13.1 (linux)\\r\\nDate: Wed, 30 Aug 2017 12:25:38 GMT\\r\\nTransfer-Encoding: chunked\\r\\n\\r\\n47\\r\\n{\\\"status\\\":\\\"Trying to pull repository docker.io/library/busybox ... \\\"}\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 271\nrecvfrom(4, \"43\\r\\n{\\\"status\\\":\\\"Pulling from docker.io/library/busybox\\\",\\\"id\\\":\\\"latest\\\"}\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 73\nrecvfrom(4, \"5e\\r\\n{\\\"status\\\":\\\"Digest: sha256:b82b5740006c1ab823596d2c07f081084ecdb32fd258072707b99f52a3cb8692\\\"}\\r\\n\\r\\n47\\r\\n{\\\"status\\\":\\\"Status: Image is up to date for docker.io/busybox:latest\\\"}\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 177\nrecvfrom(4, \"0\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 5\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 5\nconnect(5, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(5, \"POST /v1.26/containers/create HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: docker-sdk-python/2.5.1\\r\\nContent-Type: application/json\\r\\nContent-Length: 190\\r\\n\\r\\n{\\\"Tty\\\": false, \\\"NetworkDisabled\\\": false, \\\"Image\\\": \\\"busybox:latest\\\", \\\"Cmd\\\": [\\\"cat\\\"], \\\"StdinOnce\\\": false, \\\"AttachStdin\\\": false, \\\"AttachStderr\\\": false, \\\"AttachStdout\\\": false, \\\"OpenStdin\\\": true}\", 408, 0, NULL, 0) = 408\nrecvfrom(5, \"HTTP/1.1 201 Created\\r\\nApi-Version: 1.26\\r\\nContent-Type: application/json\\r\\nDocker-Experimental: false\\r\\nServer: Docker/1.13.1 (linux)\\r\\nDate: Wed, 30 Aug 2017 12:25:40 GMT\\r\\nContent-Length: 90\\r\\n\\r\\n{\\\"Id\\\":\\\"e7eec1c49bbbff357f202bf4d80d4205fa8f59fe042baf3faa218067d3bb745f\\\",\\\"Warnings\\\":null}\\n\", 8192, 0, NULL, NULL) = 281\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 6\nconnect(6, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(6, \"POST /v1.26/containers/e7eec1c49bbbff357f202bf4d80d4205fa8f59fe042baf3faa218067d3bb745f/start HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: docker-sdk-python/2.5.1\\r\\nContent-Length: 0\\r\\n\\r\\n\", 248, 0, NULL, 0) = 248\nrecvfrom(6, \"HTTP/1.1 204 No Content\\r\\nApi-Version: 1.26\\r\\nDocker-Experimental: false\\r\\nServer: Docker/1.13.1 (linux)\\r\\nDate: Wed, 30 Aug 2017 12:25:40 GMT\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 142\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 7\nconnect(7, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(7, \"POST /v1.26/containers/e7eec1c49bbbff357f202bf4d80d4205fa8f59fe042baf3faa218067d3bb745f/exec HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: docker-sdk-python/2.5.1\\r\\nContent-Type: application/json\\r\\nContent-Length: 220\\r\\n\\r\\n{\\\"Tty\\\": false, \\\"Container\\\": \\\"e7eec1c49bbbff357f202bf4d80d4205fa8f59fe042baf3faa218067d3bb745f\\\", \\\"Cmd\\\": [\\\"echo\\\", \\\"hello\\\"], \\\"AttachStdin\\\": false, \\\"User\\\": \\\"\\\", \\\"AttachStderr\\\": true, \\\"Privileged\\\": false, \\\"AttachStdout\\\": true}\", 501, 0, NULL, 0) = 501\nrecvfrom(7, \"HTTP/1.1 201 Created\\r\\nApi-Version: 1.26\\r\\nContent-Type: application/json\\r\\nDocker-Experimental: false\\r\\nServer: Docker/1.13.1 (linux)\\r\\nDate: Wed, 30 Aug 2017 12:25:40 GMT\\r\\nContent-Length: 74\\r\\n\\r\\n{\\\"Id\\\":\\\"40cfec7ac80a6903c3aee28a3a5a7832f7c71c0717e3f607b1f30813cde5df7f\\\"}\\n\", 8192, 0, NULL, NULL) = 265\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 8\nconnect(8, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(8, \"POST /v1.26/exec/40cfec7ac80a6903c3aee28a3a5a7832f7c71c0717e3f607b1f30813cde5df7f/start HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: Upgrade\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: docker-sdk-python/2.5.1\\r\\nUpgrade: tcp\\r\\nContent-Type: application/json\\r\\nContent-Length: 31\\r\\n\\r\\n{\\\"Tty\\\": false, \\\"Detach\\\": false}\", 317, 0, NULL, 0) = 317\nrecvfrom(8, \"HTTP/1.1 101 UPGRADED\\r\\nContent-Type: application/vnd.docker.raw-stream\\r\\nConnection: Upgrade\\r\\nUpgrade: tcp\\r\\n\", 8192, 0, NULL, NULL) = 107\nrecvfrom(8, \"Api-Version: 1.26\\r\\nDocker-Experimental: false\\r\\nServer: Docker/1.13.1 (linux)\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 80\nrecvfrom(8, \"\\1\\0\\0\\0\\0\\0\\0\\0\", 8, 0, NULL, NULL) = 8\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 8\nconnect(8, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(8, \"DELETE /v1.26/containers/e7eec1c49bbbff357f202bf4d80d4205fa8f59fe042baf3faa218067d3bb745f?force=True&link=False&v=False HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: docker-sdk-python/2.5.1\\r\\nContent-Length: 0\\r\\n\\r\\n\", 274, 0, NULL, 0) = 274\nrecvfrom(8, \"HTTP/1.1 204 No Content\\r\\nApi-Version: 1.26\\r\\nDocker-Experimental: false\\r\\nServer: Docker/1.13.1 (linux)\\r\\nDate: Wed, 30 Aug 2017 12:25:40 GMT\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 142\nTraceback (most recent call last):\n  File \"docker-py-1732.py\", line 12, in <module>\n    assert output == \"hello\\n\", \"Not hello? '%s'\" % output\nAssertionError: Not hello? ''\n+++ exited with 1 +++\nstrace of exposing test case on Linux Mint 18.3 and Docker version 17.05.0-ce, build 89658be:\n(docker-venv) strzelczak@yyy /home/strzelczak $ strace -f -e trace=network -s1024 python docker-py-1732.py\nsocket(PF_INET6, SOCK_STREAM, IPPROTO_IP) = 12\nbind(12, {sa_family=AF_INET6, sin6_port=htons(0), inet_pton(AF_INET6, \"::1\", &sin6_addr), sin6_flowinfo=0, sin6_scope_id=0}, 28) = 0\nstrace: Process 23712 attached\nstrace: Process 23713 attached\n[pid 23713] +++ exited with 0 +++\n[pid 23712] --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=23713, si_uid=20001, si_status=0, si_utime=0, si_stime=0} ---\n[pid 23712] +++ exited with 0 +++\n--- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=23712, si_uid=20001, si_status=0, si_utime=0, si_stime=0} ---\nstrace: Process 23714 attached\nstrace: Process 23715 attached\n[pid 23715] +++ exited with 0 +++\n[pid 23714] --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=23715, si_uid=20001, si_status=0, si_utime=0, si_stime=0} ---\n[pid 23714] +++ exited with 0 +++\n--- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=23714, si_uid=20001, si_status=0, si_utime=0, si_stime=0} ---\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 3\nconnect(3, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(3, \"GET /version HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: docker-sdk-python/2.5.1\\r\\n\\r\\n\", 148, 0, NULL, 0) = 148\nrecvfrom(3, \"HTTP/1.1 200 OK\\r\\nApi-Version: 1.29\\r\\nContent-Type: application/json\\r\\nDocker-Experimental: false\\r\\nOstype: linux\\r\\nServer: Docker/17.05.0-ce (linux)\\r\\nDate: Wed, 30 Aug 2017 12:26:38 GMT\\r\\nContent-Length: 225\\r\\n\\r\\n{\\\"Version\\\":\\\"17.05.0-ce\\\",\\\"ApiVersion\\\":\\\"1.29\\\",\\\"MinAPIVersion\\\":\\\"1.12\\\",\\\"GitCommit\\\":\\\"89658be\\\",\\\"GoVersion\\\":\\\"go1.7.5\\\",\\\"Os\\\":\\\"linux\\\",\\\"Arch\\\":\\\"amd64\\\",\\\"KernelVersion\\\":\\\"4.8.0-53-generic\\\",\\\"BuildTime\\\":\\\"2017-05-04T22:06:06.693142599+00:00\\\"}\\n\", 8192, 0, NULL, NULL) = 431\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 4\nconnect(4, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(4, \"POST /v1.29/images/create?tag=latest&fromImage=busybox HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: docker-sdk-python/2.5.1\\r\\nContent-Length: 0\\r\\n\\r\\n\", 209, 0, NULL, 0) = 209\nrecvfrom(4, \"HTTP/1.1 200 OK\\r\\nApi-Version: 1.29\\r\\nContent-Type: application/json\\r\\nDocker-Experimental: false\\r\\nOstype: linux\\r\\nServer: Docker/17.05.0-ce (linux)\\r\\nDate: Wed, 30 Aug 2017 12:26:40 GMT\\r\\nTransfer-Encoding: chunked\\r\\n\\r\\n39\\r\\n{\\\"status\\\":\\\"Pulling from library/busybox\\\",\\\"id\\\":\\\"latest\\\"}\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 276\nrecvfrom(4, \"5e\\r\\n{\\\"status\\\":\\\"Digest: sha256:b82b5740006c1ab823596d2c07f081084ecdb32fd258072707b99f52a3cb8692\\\"}\\r\\n\\r\\n3d\\r\\n{\\\"status\\\":\\\"Status: Image is up to date for busybox:latest\\\"}\\r\\n\\r\\n0\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 172\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 5\nconnect(5, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(5, \"POST /v1.29/containers/create HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: docker-sdk-python/2.5.1\\r\\nContent-Type: application/json\\r\\nContent-Length: 190\\r\\n\\r\\n{\\\"Tty\\\": false, \\\"NetworkDisabled\\\": false, \\\"Image\\\": \\\"busybox:latest\\\", \\\"Cmd\\\": [\\\"cat\\\"], \\\"StdinOnce\\\": false, \\\"AttachStdin\\\": false, \\\"AttachStderr\\\": false, \\\"AttachStdout\\\": false, \\\"OpenStdin\\\": true}\", 408, 0, NULL, 0) = 408\nrecvfrom(5, \"HTTP/1.1 201 Created\\r\\nApi-Version: 1.29\\r\\nContent-Type: application/json\\r\\nDocker-Experimental: false\\r\\nOstype: linux\\r\\nServer: Docker/17.05.0-ce (linux)\\r\\nDate: Wed, 30 Aug 2017 12:26:40 GMT\\r\\nContent-Length: 90\\r\\n\\r\\n{\\\"Id\\\":\\\"891fcd673641b13a8ef587f9a7bd190f48b91e4173f83378f94efaa7cec7ad5f\\\",\\\"Warnings\\\":null}\\n\", 8192, 0, NULL, NULL) = 300\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 6\nconnect(6, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(6, \"POST /v1.29/containers/891fcd673641b13a8ef587f9a7bd190f48b91e4173f83378f94efaa7cec7ad5f/start HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: docker-sdk-python/2.5.1\\r\\nContent-Length: 0\\r\\n\\r\\n\", 248, 0, NULL, 0) = 248\nrecvfrom(6, \"HTTP/1.1 204 No Content\\r\\nApi-Version: 1.29\\r\\nDocker-Experimental: false\\r\\nOstype: linux\\r\\nServer: Docker/17.05.0-ce (linux)\\r\\nDate: Wed, 30 Aug 2017 12:26:41 GMT\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 161\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 7\nconnect(7, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(7, \"POST /v1.29/containers/891fcd673641b13a8ef587f9a7bd190f48b91e4173f83378f94efaa7cec7ad5f/exec HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: docker-sdk-python/2.5.1\\r\\nContent-Type: application/json\\r\\nContent-Length: 220\\r\\n\\r\\n{\\\"Tty\\\": false, \\\"Container\\\": \\\"891fcd673641b13a8ef587f9a7bd190f48b91e4173f83378f94efaa7cec7ad5f\\\", \\\"Cmd\\\": [\\\"echo\\\", \\\"hello\\\"], \\\"AttachStdin\\\": false, \\\"User\\\": \\\"\\\", \\\"AttachStderr\\\": true, \\\"Privileged\\\": false, \\\"AttachStdout\\\": true}\", 501, 0, NULL, 0) = 501\nrecvfrom(7, \"HTTP/1.1 201 Created\\r\\nApi-Version: 1.29\\r\\nContent-Type: application/json\\r\\nDocker-Experimental: false\\r\\nOstype: linux\\r\\nServer: Docker/17.05.0-ce (linux)\\r\\nDate: Wed, 30 Aug 2017 12:26:41 GMT\\r\\nContent-Length: 74\\r\\n\\r\\n{\\\"Id\\\":\\\"ae108562938bdd0233a1d4b89b1099740a2aca4c470fb10419e315b07d989755\\\"}\\n\", 8192, 0, NULL, NULL) = 284\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 8\nconnect(8, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(8, \"POST /v1.29/exec/ae108562938bdd0233a1d4b89b1099740a2aca4c470fb10419e315b07d989755/start HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: Upgrade\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: docker-sdk-python/2.5.1\\r\\nUpgrade: tcp\\r\\nContent-Type: application/json\\r\\nContent-Length: 31\\r\\n\\r\\n{\\\"Tty\\\": false, \\\"Detach\\\": false}\", 317, 0, NULL, 0) = 317\nrecvfrom(8, \"HTTP/1.1 101 UPGRADED\\r\\nContent-Type: application/vnd.docker.raw-stream\\r\\nConnection: Upgrade\\r\\nUpgrade: tcp\\r\\nApi-Version: 1.29\\r\\nDocker-Experimental: false\\r\\nOstype: linux\\r\\nServer: Docker/17.05.0-ce (linux)\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 206\nrecvfrom(8, \"\\1\\0\\0\\0\\0\\0\\0\\6\", 8, 0, NULL, NULL) = 8\nrecvfrom(8, \"hello\\n\", 6, 0, NULL, NULL) = 6\nrecvfrom(8, \"\", 8, 0, NULL, NULL)       = 0\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 8\nconnect(8, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(8, \"DELETE /v1.29/containers/891fcd673641b13a8ef587f9a7bd190f48b91e4173f83378f94efaa7cec7ad5f?force=True&link=False&v=False HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: docker-sdk-python/2.5.1\\r\\nContent-Length: 0\\r\\n\\r\\n\", 274, 0, NULL, 0) = 274\nrecvfrom(8, \"HTTP/1.1 204 No Content\\r\\nApi-Version: 1.29\\r\\nDocker-Experimental: false\\r\\nOstype: linux\\r\\nServer: Docker/17.05.0-ce (linux)\\r\\nDate: Wed, 30 Aug 2017 12:26:41 GMT\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 161\n+++ exited with 0 +++\n. The key thing seems to be interpretation of the http body in reply to /v1.26/exec/40cfec7ac80a6903c3aee28a3a5a7832f7c71c0717e3f607b1f30813cde5df7f/start :+1: docker-py code is fooled that there is nothing else in the reply because '\\1\\0\\0\\0\\0\\0\\0\\0' is interpreted as frame size by the following code:\n```\ndef read_exactly(socket, n):\n    \"\"\"\n    Reads exactly n bytes from socket\n    Raises SocketError if there isn't enough data\n    \"\"\"\n    data = six.binary_type()\n    while len(data) < n:\n        next_data = read(socket, n - len(data))\n        if not next_data:\n            raise SocketError(\"Unexpected EOF\")\n        data += next_data\n    return data\ndef next_frame_size(socket):\n    \"\"\"\n    Returns the size of the next frame of data waiting to be read from socket,\n    according to the protocol defined here:\nhttps://docs.docker.com/engine/reference/api/docker_remote_api_v1.24/#/attach-to-a-container\n\"\"\"\ntry:\n    data = read_exactly(socket, 8)\nexcept SocketError:\n    return 0\n\n_, actual = struct.unpack('>BxxxL', data)\nreturn actual\n\n```. My rewriten test case proves this: there is something more to read after '\\1\\0\\0\\0\\0\\0\\0\\0':\nstrace of rewritten test case on Centos 7.3 with Docker version 1.13.1, build b303bf6/1.13.1:\n(docker-venv)[strzelczak@xxx ~]$ strace -f -e trace=network -s1024 python docker-py-1732-requests.py \nsocket(PF_INET6, SOCK_STREAM, IPPROTO_IP) = 10\nbind(10, {sa_family=AF_INET6, sin6_port=htons(0), inet_pton(AF_INET6, \"::1\", &sin6_addr), sin6_flowinfo=0, sin6_scope_id=0}, 28) = 0\nProcess 1164 attached\nProcess 1165 attached\n[pid  1165] +++ exited with 0 +++\n[pid  1164] --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=1165, si_status=0, si_utime=0, si_stime=0} ---\n[pid  1164] +++ exited with 0 +++\n--- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=1164, si_status=0, si_utime=0, si_stime=0} ---\nProcess 1166 attached\nProcess 1167 attached\n[pid  1167] +++ exited with 0 +++\n[pid  1166] --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=1167, si_status=0, si_utime=0, si_stime=0} ---\n[pid  1166] +++ exited with 0 +++\n--- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=1166, si_status=0, si_utime=0, si_stime=0} ---\nProcess 1199 attached\nProcess 1200 attached\n[pid  1200] +++ exited with 0 +++\n[pid  1199] --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=1200, si_status=0, si_utime=0, si_stime=0} ---\n[pid  1199] +++ exited with 0 +++\n--- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=1199, si_status=0, si_utime=0, si_stime=0} ---\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 3\nconnect(3, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(3, \"GET /version HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: python-requests/2.18.4\\r\\n\\r\\n\", 147, 0, NULL, 0) = 147\nrecvfrom(3, \"HTTP/1.1 200 OK\\r\\nApi-Version: 1.26\\r\\nContent-Type: application/json\\r\\nDocker-Experimental: false\\r\\nServer: Docker/1.13.1 (linux)\\r\\nDate: Wed, 30 Aug 2017 12:33:18 GMT\\r\\nContent-Length: 263\\r\\n\\r\\n{\\\"Version\\\":\\\"1.13.1\\\",\\\"ApiVersion\\\":\\\"1.26\\\",\\\"MinAPIVersion\\\":\\\"1.12\\\",\\\"GitCommit\\\":\\\"b303bf6/1.13.1\\\",\\\"GoVersion\\\":\\\"go1.7.4\\\",\\\"Os\\\":\\\"linux\\\",\\\"Arch\\\":\\\"amd64\\\",\\\"KernelVersion\\\":\\\"3.10.0-514.26.2.el7.x86_64\\\",\\\"BuildTime\\\":\\\"2017-07-03T15:57:33.186109768+00:00\\\",\\\"PkgVersion\\\":\\\"<unknown>\\\"}\\n\", 8192, 0, NULL, NULL) = 450\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 3\nconnect(3, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(3, \"POST /v1.26/images/create?tag=latest&fromImage=busybox HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: python-requests/2.18.4\\r\\nContent-Length: 0\\r\\n\\r\\n\", 208, 0, NULL, 0) = 208\nrecvfrom(3, \"HTTP/1.1 200 OK\\r\\nApi-Version: 1.26\\r\\nContent-Type: application/json\\r\\nDocker-Experimental: false\\r\\nServer: Docker/1.13.1 (linux)\\r\\nDate: Wed, 30 Aug 2017 12:33:18 GMT\\r\\nTransfer-Encoding: chunked\\r\\n\\r\\n47\\r\\n{\\\"status\\\":\\\"Trying to pull repository docker.io/library/busybox ... \\\"}\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 271\nrecvfrom(3, \"43\\r\\n{\\\"status\\\":\\\"Pulling from docker.io/library/busybox\\\",\\\"id\\\":\\\"latest\\\"}\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 73\nrecvfrom(3, \"5e\\r\\n{\\\"status\\\":\\\"Digest: sha256:b82b5740006c1ab823596d2c07f081084ecdb32fd258072707b99f52a3cb8692\\\"}\\r\\n\\r\\n47\\r\\n{\\\"status\\\":\\\"Status: Image is up to date for docker.io/busybox:latest\\\"}\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 177\nrecvfrom(3, \"0\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 5\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 3\nconnect(3, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(3, \"POST /v1.26/containers/create HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: python-requests/2.18.4\\r\\nContent-Length: 190\\r\\nContent-Type: application/json\\r\\n\\r\\n{\\\"Tty\\\": false, \\\"AttachStderr\\\": false, \\\"AttachStdin\\\": false, \\\"NetworkDisabled\\\": false, \\\"Image\\\": \\\"busybox:latest\\\", \\\"Cmd\\\": [\\\"cat\\\"], \\\"StdinOnce\\\": false, \\\"AttachStdout\\\": false, \\\"OpenStdin\\\": true}\", 407, 0, NULL, 0) = 407\nrecvfrom(3, \"HTTP/1.1 201 Created\\r\\nApi-Version: 1.26\\r\\nContent-Type: application/json\\r\\nDocker-Experimental: false\\r\\nServer: Docker/1.13.1 (linux)\\r\\nDate: Wed, 30 Aug 2017 12:33:20 GMT\\r\\nContent-Length: 90\\r\\n\\r\\n{\\\"Id\\\":\\\"087ee71e89ad322294ab8ccf910b0628dc6074491a5f286c73898252b6b6cb55\\\",\\\"Warnings\\\":null}\\n\", 8192, 0, NULL, NULL) = 281\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 3\nconnect(3, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(3, \"POST /v1.26/containers/087ee71e89ad322294ab8ccf910b0628dc6074491a5f286c73898252b6b6cb55/start HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: python-requests/2.18.4\\r\\nContent-Length: 0\\r\\n\\r\\n\", 247, 0, NULL, 0) = 247\nrecvfrom(3, \"HTTP/1.1 204 No Content\\r\\nApi-Version: 1.26\\r\\nDocker-Experimental: false\\r\\nServer: Docker/1.13.1 (linux)\\r\\nDate: Wed, 30 Aug 2017 12:33:20 GMT\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 142\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 3\nconnect(3, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(3, \"POST /v1.26/containers/087ee71e89ad322294ab8ccf910b0628dc6074491a5f286c73898252b6b6cb55/exec HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: python-requests/2.18.4\\r\\nContent-Length: 220\\r\\nContent-Type: application/json\\r\\n\\r\\n{\\\"Tty\\\": false, \\\"AttachStderr\\\": true, \\\"Container\\\": \\\"087ee71e89ad322294ab8ccf910b0628dc6074491a5f286c73898252b6b6cb55\\\", \\\"User\\\": \\\"\\\", \\\"AttachStdin\\\": false, \\\"Cmd\\\": [\\\"echo\\\", \\\"hello\\\"], \\\"Privileged\\\": false, \\\"AttachStdout\\\": true}\", 500, 0, NULL, 0) = 500\nrecvfrom(3, \"HTTP/1.1 201 Created\\r\\nApi-Version: 1.26\\r\\nContent-Type: application/json\\r\\nDocker-Experimental: false\\r\\nServer: Docker/1.13.1 (linux)\\r\\nDate: Wed, 30 Aug 2017 12:33:20 GMT\\r\\nContent-Length: 74\\r\\n\\r\\n{\\\"Id\\\":\\\"d83f217ce1a3dc4cc63dd4e68d6b5c0f7e7c8fdd35e5f54d1bda4358181e0c05\\\"}\\n\", 8192, 0, NULL, NULL) = 265\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 3\nconnect(3, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(3, \"POST /v1.26/exec/d83f217ce1a3dc4cc63dd4e68d6b5c0f7e7c8fdd35e5f54d1bda4358181e0c05/start HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: python-requests/2.18.4\\r\\nContent-Length: 31\\r\\nContent-Type: application/json\\r\\n\\r\\n{\\\"Tty\\\": false, \\\"Detach\\\": false}\", 305, 0, NULL, 0) = 305\nrecvfrom(3, \"HTTP/1.1 200 OK\\r\\nContent-Type: application/vnd.docker.raw-stream\\r\\n\", 8192, 0, NULL, NULL) = 66\nrecvfrom(3, \"Api-Version: 1.26\\r\\nDocker-Experimental: false\\r\\nServer: Docker/1.13.1 (linux)\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 80\nrecvfrom(3, \"\\1\\0\\0\\0\\0\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\", 10240, 0, NULL, NULL) = 16\nrecvfrom(3, \"\\1\\0\\0\\0\\0\\0\\0\\6hello\\n\", 10224, 0, NULL, NULL) = 14\nrecvfrom(3, \"\", 10210, 0, NULL, NULL)   = 0\nrecvfrom(3, \"\", 10240, 0, NULL, NULL)   = 0\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 3\nconnect(3, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(3, \"GET /v1.26/exec/d83f217ce1a3dc4cc63dd4e68d6b5c0f7e7c8fdd35e5f54d1bda4358181e0c05/json HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: python-requests/2.18.4\\r\\n\\r\\n\", 220, 0, NULL, 0) = 220\nrecvfrom(3, \"HTTP/1.1 200 OK\\r\\nApi-Version: 1.26\\r\\nContent-Type: application/json\\r\\nDocker-Experimental: false\\r\\nServer: Docker/1.13.1 (linux)\\r\\nDate: Wed, 30 Aug 2017 12:33:20 GMT\\r\\nContent-Length: 374\\r\\n\\r\\n{\\\"ID\\\":\\\"d83f217ce1a3dc4cc63dd4e68d6b5c0f7e7c8fdd35e5f54d1bda4358181e0c05\\\",\\\"Running\\\":false,\\\"ExitCode\\\":0,\\\"ProcessConfig\\\":{\\\"tty\\\":false,\\\"entrypoint\\\":\\\"echo\\\",\\\"arguments\\\":[\\\"hello\\\"],\\\"privileged\\\":false},\\\"OpenStdin\\\":false,\\\"OpenStderr\\\":true,\\\"OpenStdout\\\":true,\\\"CanRemove\\\":false,\\\"ContainerID\\\":\\\"087ee71e89ad322294ab8ccf910b0628dc6074491a5f286c73898252b6b6cb55\\\",\\\"DetachKeys\\\":\\\"\\\",\\\"Pid\\\":1334}\\n\", 8192, 0, NULL, NULL) = 561\n{u'OpenStderr': True, u'OpenStdout': True, u'ContainerID': u'087ee71e89ad322294ab8ccf910b0628dc6074491a5f286c73898252b6b6cb55', u'DetachKeys': u'', u'Pid': 1334, u'CanRemove': False, u'Running': False, u'ProcessConfig': {u'tty': False, u'entrypoint': u'echo', u'arguments': [u'hello'], u'privileged': False}, u'ExitCode': 0, u'ID': u'd83f217ce1a3dc4cc63dd4e68d6b5c0f7e7c8fdd35e5f54d1bda4358181e0c05', u'OpenStdin': False}\nsocket(PF_LOCAL, SOCK_STREAM, 0)        = 3\nconnect(3, {sa_family=AF_LOCAL, sun_path=\"/var/run/docker.sock\"}, 22) = 0\nsendto(3, \"DELETE /v1.26/containers/087ee71e89ad322294ab8ccf910b0628dc6074491a5f286c73898252b6b6cb55?force=True&link=False&v=False HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: keep-alive\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nUser-Agent: python-requests/2.18.4\\r\\nContent-Length: 0\\r\\n\\r\\n\", 273, 0, NULL, 0) = 273\nrecvfrom(3, \"HTTP/1.1 204 No Content\\r\\nApi-Version: 1.26\\r\\nDocker-Experimental: false\\r\\nServer: Docker/1.13.1 (linux)\\r\\nDate: Wed, 30 Aug 2017 12:33:20 GMT\\r\\n\\r\\n\", 8192, 0, NULL, NULL) = 142\n+++ exited with 0 +++\n(docker-venv)[strzelczak@xxx ~]$\n. A bit related to #1697 . When tty=True is used in exec_start a different protocol is used and the problem does not occur.. Yes, I am using latest 2.5.1. Sorry not to include this in a original bug report.. ",
    "tossmilestone": "Why the integration tests always failed with the error:\n=================================== ERRORS ====================================\n03:55:19 [py3.5_1.13.1] __________________ ERROR at setup of ServiceTest.test_create ___________________\n03:55:19 [py3.5_1.13.1] tests/integration/models_services_test.py:15: in setUpClass\n03:55:19 [py3.5_1.13.1]     client.swarm.init('eth0', listen_addr=helpers.swarm_listen_addr())\n03:55:19 [py3.5_1.13.1] docker/models/swarm.py:96: in init\n03:55:19 [py3.5_1.13.1]     self.client.api.init_swarm(**init_kwargs)\n03:55:19 [py3.5_1.13.1] docker/utils/decorators.py:34: in wrapper\n03:55:19 [py3.5_1.13.1]     return f(self, *args, **kwargs)\n03:55:19 [py3.5_1.13.1] docker/api/swarm.py:102: in init_swarm\n03:55:19 [py3.5_1.13.1]     self._raise_for_status(response)\n03:55:19 [py3.5_1.13.1] docker/api/client.py:222: in _raise_for_status\n03:55:19 [py3.5_1.13.1]     raise create_api_error_from_http_exception(e)\n03:55:19 [py3.5_1.13.1] docker/errors.py:31: in create_api_error_from_http_exception\n03:55:19 [py3.5_1.13.1]     raise cls(e, response=response, explanation=explanation)\n03:55:19 [py3.5_1.13.1] E   docker.errors.APIError: 500 Server Error: Internal Server Error (\"interface eth0 has more than one IPv6 address (2001:db8:1::242:ac11:2 and fe80::42:acff:fe11:2)\")\n@shin- please help to resolve it. Thanks!. Updated, @shin- PTAL, thanks!. Closed since there is already a PL #1783 that added this feature. . ",
    "tobiasfielitz": "Second this! I can't find it in the API though.. @shin- thanks for pointing me at this, that should do (until @feliperuhland or anyone else has written a function for it).\nThanks for the great work guys!. Seems to happen when self.version is an old version. I have to run\nself.version = api_client.inspect_service(self.docker_service_name)['Version']['Index'] (right) before scaling.. ",
    "TheFriendlyCoder": "Hmmm - could you not create a dev branch from the same revision as the last 1.x release of the project, and simply change the default deployment folder from 'docker' to, say, 'docker_py'? Then you could release a new version of the docker-py package with this fix. This change should be largely if not totally transparent to users of the docker-py library, and would avoid the conflict with the newer v2 package.. Hmmm - now that I think of it, this would break imports since the package name is still 'docker' :(\nSo maybe this isn't as easy to fix as I had hoped. :(. Maybe someone could add a notice to the docker-py package on PyPI warning people that they shouldn't use that version of the library any more and that it conflicts with the newer version of the library. Then at least there'd be an incentive, albeit a small one, for people to adopt the newer version.. ",
    "njthykkathu": "@shin- Docker version I use in 2.1.0. Would it be possible if you could give me an example? . ",
    "markzz": "docker.__path__ showed the issue and I resolved it. Closed.. ",
    "treuherz": "Brilliant, I'd missed that. Since it directly mimics part of the docker CLI, is there a reason it's not in the high-level API?. Great, hadn't caught that before. Does this #1375 not still make that unreliable, though?\nEDIT:  I see Container has a reload() method for that purpose. Documentation on attrs is pretty lacking, though. Thanks for pointing it out.. ",
    "kyjsqd222": "@shin- that works! thank you.. ",
    "lightninglu10": "This issue wasn't caused by Docker. It was actually an issue with Django channels as it has a 60s timeout, but the build command can take longer than that. Resolved by setting the http_timeout option (run python manage.py runserver --help to see it).. ",
    "Mraoul": "I'm running Docker 17.03.2, which might be the reason why this is not working as intended, since it supports up to api 1.27. I spun up a box (ubuntu 16.04) and installed 17.06.2 (the latest, afaik) and it works as expected. But, here's what happens when using 17.03.2 (verified on a different box):\n```python\n\n\n\nd = docker.APIClient()\npprint(d.version())\n{u'ApiVersion': u'1.27',\n u'Arch': u'amd64',\n u'BuildTime': u'2017-06-27T03:35:14.394387481+00:00',\n u'GitCommit': u'f5ec1e2',\n u'GoVersion': u'go1.7.5',\n u'KernelVersion': u'4.4.0-96-generic',\n u'MinAPIVersion': u'1.12',\n u'Os': u'linux',\n u'Version': u'17.03.2-ce'}\nd.images(name='alpine')\n[{u'Created': 1505313146, u'Labels': None, u'VirtualSize': 3965955, u'SharedSize': -1, u'ParentId': u'', u'Size': 3965955, u'RepoDigests': [u'alpine@sha256:f006ecbb824d87947d0b51ab8488634bf69fe4094959d935c0c103f4820a417d'], u'Id': u'sha256:76da55c8019d7a47c347c0dceb7a6591144d232a7dd616242a367b8bed18ecbc', u'Containers': -1, u'RepoTags': [u'alpine:latest']}, {u'Created': 1505297656, u'Labels': None, u'VirtualSize': 1129289, u'SharedSize': -1, u'ParentId': u'', u'Size': 1129289, u'RepoDigests': [u'busybox@sha256:99ccecf3da28a93c063d5dddcdf69aeed44826d0db219aabc3d5178d47649dfa'], u'Id': u'sha256:54511612f1c4d97e93430fc3d5dc2f05dfbe8fb7e6259b7351deeca95eaf2971', u'Containers': -1, u'RepoTags': [u'busybox:latest']}]\nd = docker.APIClient(version='1.24')\nd.images(name='alpine')\n[{u'Created': 1505313146, u'Labels': None, u'VirtualSize': 3965955, u'SharedSize': -1, u'ParentId': u'', u'Size': 3965955, u'RepoDigests': [u'alpine@sha256:f006ecbb824d87947d0b51ab8488634bf69fe4094959d935c0c103f4820a417d'], u'Id': u'sha256:76da55c8019d7a47c347c0dceb7a6591144d232a7dd616242a367b8bed18ecbc', u'Containers': -1, u'RepoTags': [u'alpine:latest']}]\n```\n\n\n\nI'm guessing this is related to 17.03.2 only supporting up to 1.27 and the library trying to default to 1.30? I ran some further tests and it will work with any version that is supported:\n```python\n\n\n\nd = docker.APIClient(version='1.26')\nd.images(name='alpine')\n[{u'Created': 1505313146, u'Labels': None, u'VirtualSize': 3965955, u'SharedSize': -1, u'ParentId': u'', u'Size': 3965955, u'RepoDigests': [u'alpine@sha256:f006ecbb824d87947d0b51ab8488634bf69fe4094959d935c0c103f4820a417d'], u'Id': u'sha256:76da55c8019d7a47c347c0dceb7a6591144d232a7dd616242a367b8bed18ecbc', u'Containers': -1, u'RepoTags': [u'alpine:latest']}]\nd = docker.APIClient(version='1.27')\nd.images(name='alpine')\n[{u'Created': 1505313146, u'Labels': None, u'VirtualSize': 3965955, u'SharedSize': -1, u'ParentId': u'', u'Size': 3965955, u'RepoDigests': [u'alpine@sha256:f006ecbb824d87947d0b51ab8488634bf69fe4094959d935c0c103f4820a417d'], u'Id': u'sha256:76da55c8019d7a47c347c0dceb7a6591144d232a7dd616242a367b8bed18ecbc', u'Containers': -1, u'RepoTags': [u'alpine:latest']}]\nd = docker.APIClient(version='1.28')\nd.images(name='alpine')\n[{u'Created': 1505313146, u'Labels': None, u'VirtualSize': 3965955, u'SharedSize': -1, u'ParentId': u'', u'Size': 3965955, u'RepoDigests': [u'alpine@sha256:f006ecbb824d87947d0b51ab8488634bf69fe4094959d935c0c103f4820a417d'], u'Id': u'sha256:76da55c8019d7a47c347c0dceb7a6591144d232a7dd616242a367b8bed18ecbc', u'Containers': -1, u'RepoTags': [u'alpine:latest']}, {u'Created': 1505297656, u'Labels': None, u'VirtualSize': 1129289, u'SharedSize': -1, u'ParentId': u'', u'Size': 1129289, u'RepoDigests': [u'busybox@sha256:99ccecf3da28a93c063d5dddcdf69aeed44826d0db219aabc3d5178d47649dfa'], u'Id': u'sha256:54511612f1c4d97e93430fc3d5dc2f05dfbe8fb7e6259b7351deeca95eaf2971', u'Containers': -1, u'RepoTags': [u'busybox:latest']}]\n\n\n\n```\nAlthough, it's kinda weird the 'filter' param still works even though I don't see it anywhere in the API reference anymore. I don't know if that's a bug or a 'feature.'\nSo, to further test, I modified the DEFAULT_DOCKER_VERSION in constants.py from '1.30' to '1.27' and it works against 17.03.2 ...\n```python\n\n\n\nd = docker.APIClient()\npprint(d.version())\n{u'ApiVersion': u'1.27',\n u'Arch': u'amd64',\n u'BuildTime': u'2017-06-27T03:35:14.394387481+00:00',\n u'GitCommit': u'f5ec1e2',\n u'GoVersion': u'go1.7.5',\n u'KernelVersion': u'4.4.0-96-generic',\n u'MinAPIVersion': u'1.12',\n u'Os': u'linux',\n u'Version': u'17.03.2-ce'}\nd.images(name='alpine')\n[{u'Created': 1505313146, u'Labels': None, u'VirtualSize': 3965955, u'SharedSize': -1, u'ParentId': u'', u'Size': 3965955, u'RepoDigests': [u'alpine@sha256:f006ecbb824d87947d0b51ab8488634bf69fe4094959d935c0c103f4820a417d'], u'Id': u'sha256:76da55c8019d7a47c347c0dceb7a6591144d232a7dd616242a367b8bed18ecbc', u'Containers': -1, u'RepoTags': [u'alpine:latest']}]\n```. \n\n\n",
    "KevinTHU": "it is not right, and this may cause socket leak, some linux operation may be effected, and even docker-py exec itself, because python docker exec use python select(), and if socket leak number is larger than 1024, this select() method may failed!. @shin- PTAL, this may cause python using docker exec cmd (not -it) to having a socket leak, and after leak socket number larger than 1024, the docker-py will raise exception.. @ravjanga it is a docker-py problem, and you have no way to close the response, unless you make the method params socket=True, but you have to do a lot of things for response socket process. So I think it is better to solve this problem in docker-py code.. ",
    "ravjanga": "I am hitting similar issue and my CLOSE_WAIT FDs keep on increasing over time. Any solution/workaround?\nI did a proper close on the APIClient session. This didnt help. To my surprise, even after closing the APIClient Session i can still do docker operations.. An update  on this. This happens only when are interacting with docker over TLS. W/o TLS, we dont see any socket FD leaks. Still investigating the issue.. There is a socket leak (not being closed properly) during exec_create/exec_start. This is happening only for TLS, but not for plain. Couldnt find anything obvious in the code as the close is being called fine. . Thanks for sharing the workaround. We have also tried the same workaround and it worked fine. We were not sure of the side effects of this fix, hence might go non TLS route for now. \nHave you encountered any other issues with the workaround?. ",
    "brunograz": "Just noticed that there is a PR open on this.. ",
    "mikeage": "Traceback (most recent call last):\n  File \"/Users/mikemi/analyze_base.py\", line 71, in <module>\n    main()\n  File \"/Users/mikemi/analyze_base.py\", line 37, in main\n    client.images.pull(base)\n  File \"/usr/local/lib/python2.7/site-packages/docker/models/images.py\", line 277, in pull\n    return self.get('{0}:{1}'.format(name, tag) if tag else name)\n  File \"/usr/local/lib/python2.7/site-packages/docker/models/images.py\", line 204, in get\n    return self.prepare_model(self.client.api.inspect_image(name))\n  File \"/usr/local/lib/python2.7/site-packages/docker/utils/decorators.py\", line 19, in wrapped\n    return f(self, resource_id, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/docker/api/image.py\", line 262, in inspect_image\n    self._get(self._url(\"/images/{0}/json\", image)), True\n  File \"/usr/local/lib/python2.7/site-packages/docker/api/client.py\", line 226, in _result\n    self._raise_for_status(response)\n  File \"/usr/local/lib/python2.7/site-packages/docker/api/client.py\", line 222, in _raise_for_status\n    raise create_api_error_from_http_exception(e)\n  File \"/usr/local/lib/python2.7/site-packages/docker/errors.py\", line 31, in create_api_error_from_http_exception\n    raise cls(e, response=response, explanation=explanation)\ndocker.errors.ImageNotFound: 404 Client Error: Not Found (\"no such image: dockerhub.internal-company-server.com/foo: No such image: dockerhub.internal-company-server.com/foo:latest\"). ",
    "bingzhang": "Thank you. And may I know how to set disk limit for all containers of a service, please? . Thank you. I will close this one.. ",
    "diocas": "In the documentation, they say that it's similar to cpu-period + cpu-quota. But they recommend using the cpus flag on version 1.13 and up (whatever the system). They don't say not to use it in any case, that's why I was asking why docker-py was blocking this option for other systems.\nThanks. ",
    "ivansenic": "Any chance this PR or #1712 gets integrated soon.. This is huge usability bug, community offered 2 PRs and are willing to improve them further, but there is no reaction from the maintainers.. Wtf?. ",
    "egelmex": "Are there any plans to match feature parity of docker cp? In my use case I am generating a single files that I wish to copy onto containers.. ",
    "artgoldberg": "One problem with tar archives is that the historical tar formats do not support encodings, so they're not compatible with unicode filenames. Do you support the POSIX.1-2001 pax format which resolves this issue?. @egelmex:\nThere are a couple of ways to work around the absence of cp in the docker API.\n1. Simply issue a 'docker cp ...' shell command.\n2. Reuse some of the Docker API code in docker-py/docker/utils/build.py and docker-py/tests/integration/api_container_test.py to build a utility that takes a path and builds a temporary tar archive and then puts it in your container.\nArthur. ",
    "smheidrich": "Related: #1027 (if one of them should be closed due to being a duplicate, I would close #1027 because #1771 here is more general).\nAlso, is there a PR implementing a convenience method for this by now?. @shin-, fair enough, thanks for the example code from the integration tests! :+1: . ",
    "remy-phelipot": "Well, I'll try to convince you then :)\n\nThis is not an explicit behavior (no exception has been raised, so everything is all right isn't it?). In consequence, if the developer is not a Docker Engine API expert (like me :) ), it's clearly unexpected. \nThis is not very well documented, it only indicates that the function returns The output from the server. It might be good idea to precise errors can be embedded in the output message.\nIt's not the behavior of docker-cli. If a docker push fails due to unauthorized access, the command line returns an error exit code (1). It's therefore safer to use pythonsubprocess.check_call(['docker', 'push', 'centos']) than docker-py API.\nIf it's not the job of docker-py, it's the job of the caller. But in the future Docker Engine responses can change, who knows.. Callers shall in consequence be sure to support all Docker Engine versions. I thought it was the goal of docker-py to solve this kind of problems.\n\nFor your information, this error is extracted from the JSON message by the HTTP client of docker-cli:\ndocker/cli/cli/command/plugin/push.go:\n```go\nfunc newPushCommand(dockerCli command.Cli) *cobra.Command {\n    ...\nreturn jsonmessage.DisplayJSONMessagesToStream(responseBody, dockerCli.Out(), nil)\n\n}\n```\nmoby/moby/client/request.go:\ngo\nfunc (jm *JSONMessage) Display(out io.Writer, termInfo termInfo) error {\n    if jm.Error != nil {\n        if jm.Error.Code == 401 {\n            return fmt.Errorf(\"authentication is required\")\n        }\n        return jm.Error\n}\nI think it might be a good improvement to implement this in docker-py.. > Verify that credentials are present\nAs you said, there is no guarantee\n\nLog in before sending the image\n\nWhat about unsecured registries? Does it work?\n\nUse the subprocess module\n\nIt should work, but it is overkill in my point of view.\n\nDownload a private image (very small) before sending the image to ensure successful access.\n\nIf you are off-line with a private registry, this operation fails.\nWhy not implement the same behaviour as docker-cli? We have to parse the response message, detect if an error message has been given by the API and raise it if necessary (and we have to do it for every REST requests, not only the push of an image). Something like:\npython\nresponse_msg = xxxx  # REST API request (Push or something else)\nresponse_dict = json.loads(response_msg)\nif 'error' in response_dict and response_dict['error']:\n     raise docker.errors.APIError(response_dict['error'])\nIt's simple and shall cover all cases,and one more time, it is the actual docker-cli behaviour, so it's not an ugly fix ;-). ",
    "erezool": "+1. ",
    "theo-walton": "yeah went from using subprocess to docker sdk and now back to subprocess because I need this feature.... ",
    "neilconway": "@shin- Yep, seems like that works:\n```\n$ python\nPython 3.5.2 (default, Nov 17 2016, 17:05:23)\n[GCC 5.4.0 20160609] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport docker\nd = docker.from_env()\nnetworks = d.networks.list()\n[n.name for n in networks]\n['bridge', 'host', 'none', 'pedlintegrationtests_default']\n[n.containers for n in networks]\n[[], [], [], []]\ntarget = networks[3]\ntarget.containers\n[]\ntarget.reload()\ntarget.containers\n[, ]\n[c.id for c in target.containers]\n['b82bb19d44f18577ff7629e2bf9f350f3c3c581640e2a238ae710d84cb51d2ca', 'a6256ad04ff04f2ae4ab7d5e67398824a1eb7eba7baf6622519049e67629545c']\n$ docker network inspect pedlintegrationtests_default | jq '.[0].Containers | length'\n2\n``. Right, makes sense. FWIW, the current behavior is very confusing, because silently returning[]makes it hard to distinguish this situation (incomplete object) from \"complete object but network has no connected containers\". e.g., ifcontainerswasNone(or there was acontainers()` method that threw an exception in this situation), that would at least make it more obvious.. \n\n\n",
    "pizzop": "Thanks @shin- . That's also my feeling. Looking in the SDK source code I noticed that API calls are quite straightforward, It should not be a bug on client side. Docker client's behaviour is confirming that thought.\nI will contact the Docker support.\nClosing for now and thanks again for the quick reply.. ",
    "dimorinny": "About docs: no problems, I will generate it.\nAbout quotes: I found, that non consistent quotes are using on this project and  I decide to change that in all files, that I changed. If it is confused you, I can rollback this changes.. About docs again: Hm, I didn't get it. What should I do for updating docs?\nOn this pull request I changed only the internal low level API for creating network (https://docker-py.readthedocs.io/en/stable/api.html#networks) and I changed it on doc string. This is not enough for updating docs?. ",
    "baldoalessandro": "Sure, no problem! \nI didn't add the mode key because I wasn't sure if it needs some kind of version check in the code.\nIn particular docker.api.services is defined like:\npython\n@utils.minimum_version('1.24')\ndef services(self, filters=None):\n     # [ ... ]\nbut the mode key in service filter is only supported from 1.29. Should I add some version check like:\npython\nif utils.version_lt(version, '1.29'):\n  raise errors.InvalidVersion(\n    'mode filter is not supported in API version < 1.29'\n  )\nThanks for the project by the way! \ud83d\udc4d . Oh right, understood. I've added the mode key\nedit : sorry I've skipped the line length check, it should be good now. ",
    "timvisee": "I've signed the commit now. The PR has been updated.. Thank you for looking into it.\nHmm, maybe a temporary fix could be to require the proper chardet version explicitly in docker-py. Although, I don't think such a thing is recommended. . ",
    "RW83": "Same error message but on Python 3.4, Ubuntu 14.04, http+docker://localunixsocket here.\nERROR: for   'SocketIO' object has no attribute 'raw'\nTraceback (most recent call last):\n  File \"/usr/local/bin/docker-compose\", line 11, in \n    sys.exit(main())\n  File \"/usr/local/lib/python3.4/dist-packages/compose/cli/main.py\", line 68, in main. ",
    "wraydulany": "I've also seen this with python 3.4 on Ubuntu 14.04, connected...however calls to 'exec_run' connect.\nCame up in an automated build on travis.\nFile \"myscript.py\", line 209, in my_function\n    container.exec_run(['/load_data.bash', 'thing'])\n  File \"/home/travis/.local/lib/python3.4/site-packages/docker/models/containers.py\", line 165, in exec_run\n    resp['Id'], detach=detach, tty=tty, stream=stream, socket=socket\n  File \"/home/travis/.local/lib/python3.4/site-packages/docker/utils/decorators.py\", line 34, in wrapper\n    return f(self, *args, kwargs)\n  File \"/home/travis/.local/lib/python3.4/site-packages/docker/utils/decorators.py\", line 19, in wrapped\n    return f(self, resource_id, *args, kwargs)\n  File \"/home/travis/.local/lib/python3.4/site-packages/docker/api/exec_api.py\", line 159, in exec_start\n    return self._read_from_socket(res, stream, tty)\n  File \"/home/travis/.local/lib/python3.4/site-packages/docker/api/client.py\", line 368, in _read_from_socket\n    socket = self._get_raw_response_socket(response)\n  File \"/home/travis/.local/lib/python3.4/site-packages/docker/api/client.py\", line 276, in _get_raw_response_socket\n    sock = response.raw._fp.fp.raw\nAttributeError: 'SocketIO' object has no attribute 'raw'. ",
    "andreycizov": "All of the docker-py API is based around accessing the methods associated with the objects. How do I use the Container object down the stream in an another process, then? That goes with saying that there are ways to implement that using the docker-py API, but it's not documented.\nAnother point: urllib3 provides __setstate__ and __getstate__ for HTTPAdapter that is used to de/serialize the ConnectionPool used for accessing the API. So serialization does work if you use the S/HTTP, but not if using UnixAdapter.\n```python\nimport docker\nfrom multiprocessing import Manager, Pool\nThis will work with https:// but not with unix://\nADDR = ''\ndef container_list_down_the_line(queue):\n    cli = docker.DockerClient(base_url=ADDR)\nfor c in cli.containers.list():\n     queue.put(c)\n\ndef execute_in_subprocess(queue):\n    while True:\n        c = queue.get()\n        c.remove()\nmanager = Manager()\nqueue = manager.Queue()\nwith Pool(2) as pool:\n    res_1 = pool.apply_async(\n                container_list_down_the_line,\n                (\n                    queue,\n                )\n            )\n    res_2 = pool.apply_async(\n                execute_in_subprocess,\n                (\n                    queue,\n                )\n            )\n    res_1.get()\n    res_2.get()\n```. @stasfilin Sorry - didn't get the fact that all of the commits I made need to be signed.. The maintainers of this project object to this pull request as they assume the docker-py objects should not be serializable: https://github.com/docker/docker-py/issues/1809. @MatthiasLohr I have to admit that I completely disagree with their logic as the only reason we can't do that is the implementation of the UNIX socket adapter. Which is easily fixable (given this pull request).\nLet me be clear about that: what's the point of supplying me with the deserialized Container objects in the library, if I still need to copy them to another Container object in my code? You're making us do the job twice instead. \nYou could have made the API much simpler by just allowing us to work directly with the dictionaries. But since you preferred to go the way of abstracting them from us - then why do you then actively try to stop us from doing things?\n@feliperuhland Given your reply in #1809 I again disagree with the answer since my specific use case was loading a list of objects from the API in bulk, and then passing them down the queue for processing. Docker-py forced me to simply copy the attributes of these objects, which is not optimal.. I had updated my docker to the latest version and this still hangs for the same reason.\nServer:\n Engine:\n  Version:          18.06.0-ce\n  API version:      1.38 (minimum version 1.12)\n  Go version:       go1.10.3\n  Git commit:       0ffa825\n  Built:            Wed Jul 18 19:07:56 2018\n  OS/Arch:          linux/amd64\n  Experimental:     false. ",
    "withsmilo": "@shin- : Great! Thank you!. Add 'rollback' to errors.InvalidArgument message.. ",
    "thiagofigueiro": "+1\nMinimal steps to reproduce using example from manual:\n```\nIn [1]: import docker\nIn [2]: client = docker.from_env()\nIn [3]: client.containers.run('alpine:2.7', 'echo hello world')\nOut[3]: b'hello world\\n'\nIn [4]: client.containers.run('alpine:2.7', 'echo hello world', auto_remove=True)\n\nHTTPError                                 Traceback (most recent call last)\n(...)\n~/.pyenv/versions/3.6.3/lib/python3.6/site-packages/docker/errors.py in create_api_error_from_http_exception(e)\n     29         else:\n     30             cls = NotFound\n---> 31     raise cls(e, response=response, explanation=explanation)\n     32\n     33\nNotFound: 404 Client Error: Not Found (\"No such container: 6151f0219d2dbb90df7288f30f1221e817fd9d3c63afdbadf2ad2a87ba6e7e45\")\n```\n. ",
    "StephenPCG": "This problem is not totally resolved by #1836 . I can still reproduce this issue with current master code.\n```\nIn [1]: import docker\nIn [2]: client = docker.from_env()\nIn [3]: client.containers.run('alpine:latest', 'echo hello world', auto_remove=True)\n...\n~/.venvs/docker/lib/python3.6/site-packages/docker/errors.py in create_api_error_from_http_exception(e)\n     29         else:\n     30             cls = NotFound\n---> 31     raise cls(e, response=response, explanation=explanation)\n     32\n     33\nNotFound: 404 Client Error: Not Found (\"No such container: 2e5677d940399606886ad6ce9303d458c27b6c251c33d624f0660640b9489547\")\n```\nI think there are still two problems with the code when auto_remove is True:\n```python\ndocker/models/containers.py\n725    container.start()\n726\n727    if detach:\n728     return container\n729\n730 logging_driver = container.attrs['HostConfig']['LogConfig']['Type']\n731\n732 out = None\n733 if logging_driver == 'json-file' or logging_driver == 'journald':\n        # the container may have already been removed if it exited too fast,\n        # just like my above example, cause logs() to fail with NotFound.\n734     out = container.logs(\n735         stdout=stdout, stderr=stderr, stream=True, follow=True\n736     )\n737\n738 exit_status = container.wait()\n739 if exit_status != 0:\n        # after wait(), container was already removed, thus this logs()\n        # call will certainly fail with NotFound.\n740     out = container.logs(stdout=False, stderr=True)\n`. @feliperuhland @shin- Thank you for pointing that out! I didn't realize there is aremoveparam and the difference betweenauto_remove``. That solves my problem.. ",
    "stasfilin": "@andreycizov Hi, you need to update commit message here: https://github.com/docker/docker-py/pull/1814/commits/e9c6c86e4dee33408ad2a8cec9ac93f14d663b6b. ",
    "MatthiasLohr": "What about this pull request? I have the same problem.. ",
    "mhank": "Hi, I'm currently an undergraduate student taking a course on virtualization. For our final project, my group of two is required to find open-source issues to work on in our last few weeks. This issue sounded manageable for the time frame and our skill level. Is anybody already working on this and does this sound reasonable?\nThank you. Currently running into an issue in which one of the tests is failing because the updated format of the extra hosts does not match that expected by Docker run's --add-host flag. Not sure how to address this, as it seems that it would require modifying parts of Docker/its docs, unless we're missing/misunderstanding something. Would you mind giving us some guidance @shin- ?. @shin- fixed!. ",
    "stsaid": "Yes, this should not be too complicated. Go ahead!\nHowever, first an thorough analysis should be done, to discover whether our assumptions hold true.\nDocker commenting on this would be nice also.. Nice, that you have been putting effort into this! \n@shin-: What's your point on this?. Great, thanks!. ",
    "Datamance": "Having this exact same issue with 3.6.0 release -- @shin- and @mowreyw did y'all ever find a resolution to this? I'm thinking the problem right now is that my build context is ginormous (like above 5.3G at the moment - trying to test some things), but wanted to be sure I wasn't missing anything obvious. Nope, just tested it without the crazy build context.\nthe dockerd logs from the os x console don't help much either -\n```\ndefault 18:51:18.157715 -0500   com.docker.driver.amd64-linux   proxy >> POST /v1.35/build?t=ember_build%3Aa427f62d9b0a76340c0c2a0f30055525b2543e8a_b291e1bb3653808b32b32eb9c84675043d79abe3&q=False&nocache=False&rm=False&forcerm=False&pull=True&buildargs=%7B%22player_addons_concat_sha%22%3A+%22a427f62d9b0a76340c0c2a0f30055525b2543e8a_b291e1bb3653808b32b32eb9c84675043d79abe3%22%7D&cachefrom=%22ember_build%22\ndefault 18:51:18.661741 -0500   com.docker.driver.amd64-linux   error forwarding client's request to Docker: write unix ->vms/0/00000003.00000948: write: broken pipe. Here's my `docker version`:\nClient: Docker Engine - Community\n Version:           18.09.0\n API version:       1.39\n Go version:        go1.10.4\n Git commit:        4d60db4\n Built:             Wed Nov  7 00:47:43 2018\n OS/Arch:           darwin/amd64\n Experimental:      false\nServer: Docker Engine - Community\n Engine:\n  Version:          18.09.0\n  API version:      1.39 (minimum version 1.12)\n  Go version:       go1.10.4\n  Git commit:       4d60db4\n  Built:            Wed Nov  7 00:55:00 2018\n  OS/Arch:          linux/amd64\n  Experimental:     true\n```. @statiksof what was the resolution to this?. ",
    "STRML": "\ud83d\udc4d  Please merge. This cost us a bunch of time today. Imagine my surprise when I finally find the bug... and the first comment on it (#4623) is my own. \ud83d\ude44 . ",
    "mtsmfm": "Here is a URL for my patched docker-compose:\nhttps://bintray.com/mtsmfm/docker/compose#files. @shin- Could you review?. > Should we add a detach_keys param to start, attach and exec_start?\nThe following is my opinion:\n\ud83d\udc4d  : implemented\n\ud83e\udd14  : not implemented and I'm not sure it should be implemented\n\ud83d\udc4e  : not implemented and I think it shouldn't be implemented\n| method | read config.json | add detach_keys keyword argument | note |\n| ------- | -------- | ------------- | ------ |\n| start | \ud83e\udd14  | \ud83d\udc4e    | it doesn't accept params |\n| attach | \ud83e\udd14  | \ud83d\udc4e   | it doesn't return socket |\n| attach_socket | \ud83d\udc4d  | \ud83e\udd14  (implemented as params argument)  | it's better to pass via params than to add keyword arg because of current interface |\n| exec_start | \ud83d\udc4d  | \ud83d\udc4d  | |\n. > Is there a way we can test the detachKeys feature programmatically?\nI found a way to test and it works on my machine.\nhttps://github.com/docker/docker-py/pull/1826/files#diff-51581c0c732dcf484a455168fe36c731R1212\nBut I'm not sure it works well on CI and I noticed integration test doesn't run on travis.\nHow can I find the results of integration test?. > Can we move the config.py file into the utils/ subfolder?\n\u2705 . @shin- I hope I finished all todos!\nCould you review again?\nSomehow jenkins doesn't start build for this change yet though(probably it searches PRs once a day?).. I noticed jenkins still failed \ud83d\ude47 \nhttps://jenkins.dockerproject.org/job/docker/job/docker-py/job/PR-1826/3/console. @shin- Hmm, integration test works well on my machine...\nI want to debug on Jenkins.\nCan I log in?. I found assert_socket_closed_with_keys doesn't work well with dockerd booted by dind.\n\nHowever, I was able to trigger a socket.error (Broken pipe) using this code snippet:\n\nI can't reproduce.\nI guess at that time you didn't use dind.. I think the test works well if we use bind-mounting docker socket instead of dind.\nI'm looking for a way with dind.... ",
    "mbovo": "Maybe with a clearer title and text the issue won't be ignored for 10 days..... Hi @shin-  thanks for your reply, requests version is 2.18.4. I've tried also with 2.18.0 and 2.17.0 but the error still persists.\nAt the time I'm testing fix #1828 with no changes:\n```\nvenv/lib/python2.7/site-packages/docker/models/services.py:177: in create\n    service_id = self.client.api.create_service(create_kwargs)\nvenv/lib/python2.7/site-packages/docker/utils/decorators.py:34: in wrapper\n    return f(self, *args, kwargs)\nvenv/lib/python2.7/site-packages/docker/api/service.py:134: in create_service\n    self._post_json(url, data=data, headers=headers), True\nvenv/lib/python2.7/site-packages/docker/api/client.py:228: in _result\n    self._raise_for_status(response)\nvenv/lib/python2.7/site-packages/docker/api/client.py:224: in _raise_for_status\n    raise create_api_error_from_http_exception(e)\n\ne = HTTPError(u'500 Server Error: Internal Server Error for url: http+docker://localunixsocket/v1.30/services/create',)\ndef create_api_error_from_http_exception(e):\n    \"\"\"\n    Create a suitable APIError from requests.exceptions.HTTPError.\n    \"\"\"\n    response = e.response\n    try:\n        explanation = response.json()['message']\n    except ValueError:\n        explanation = (response.content or '').strip()\n    cls = APIError\n    if response.status_code == 404:\n        if explanation and ('No such image' in str(explanation) or\n                            'not found: does not exist or no pull access'\n                            in str(explanation) or\n                            'repository does not exist' in str(explanation)):\n            cls = ImageNotFound\n        else:\n            cls = NotFound\n\n\n  raise cls(e, response=response, explanation=explanation)\n\nE       TypeError: 'exceptions.ValueError' object is not callable\n\nvenv/lib/python2.7/site-packages/docker/errors.py:31: TypeError\n. @shin-  The code involved (simplified but still failing) is a function served as api rest using Flask\ndef deploy(self, data):\n        client = docker.from_env()\n        service = client.services.create(\"redis:4.0.1 - alpine\", name='redis')\n        return json.dumps(service.attrs)\n```\n(yes the image name has blank space just to trigger the error)\nFull trace:\n```\n(venv) pytest ./tests/test_api.py \n=============================================================================== test session starts ================================================================================\nplatform darwin -- Python 2.7.13, pytest-3.3.1, py-1.5.2, pluggy-0.6.0\nrootdir: /Users/mbovo/fworks/devops/gindrop, inifile: setup.cfg\nplugins: xdist-1.20.1, pep8-1.0.6, forked-0.2, cov-2.5.1\ncollected 8 items                                                                                                                                                                  \ntests/test_api.py .......F                                                                                                                                                   [100%]\n===================================================================================== FAILURES =====================================================================================\n_____________ test_deploy ________________\ndef test_deploy():\n\n    filename = os.path.join(os.path.dirname(__file__), 'files', 'deploy.yml')\n    with file(filename) as f:\n        deploy = f.read()\n    data = dict(\n        file=(io.BytesIO(deploy), \"deploy.yml\"),\n    )\n\n\n  response = client.put('/deploy/deploy_pytest', content_type='multipart/form-data', data=data)\n\n\ntests/test_api.py:77: \n\nvenv/lib/python2.7/site-packages/werkzeug/test.py:811: in put\n    return self.open(args, kw)\nvenv/lib/python2.7/site-packages/flask/testing.py:127: in open\n    follow_redirects=follow_redirects)\nvenv/lib/python2.7/site-packages/werkzeug/test.py:764: in open\n    response = self.run_wsgi_app(environ, buffered=buffered)\nvenv/lib/python2.7/site-packages/werkzeug/test.py:677: in run_wsgi_app\n    rv = run_wsgi_app(self.application, environ, buffered=buffered)\nvenv/lib/python2.7/site-packages/werkzeug/test.py:884: in run_wsgi_app\n    app_rv = app(environ, start_response)\nvenv/lib/python2.7/site-packages/flask/app.py:1997: in call\n    return self.wsgi_app(environ, start_response)\nvenv/lib/python2.7/site-packages/flask/app.py:1985: in wsgi_app\n    response = self.handle_exception(e)\nvenv/lib/python2.7/site-packages/flask/app.py:1540: in handle_exception\n    reraise(exc_type, exc_value, tb)\nvenv/lib/python2.7/site-packages/flask/app.py:1982: in wsgi_app\n    response = self.full_dispatch_request()\nvenv/lib/python2.7/site-packages/flask/app.py:1614: in full_dispatch_request\n    rv = self.handle_user_exception(e)\nvenv/lib/python2.7/site-packages/flask/app.py:1517: in handle_user_exception\n    reraise(exc_type, exc_value, tb)\nvenv/lib/python2.7/site-packages/flask/app.py:1612: in full_dispatch_request\n    rv = self.dispatch_request()\nvenv/lib/python2.7/site-packages/flask/app.py:1598: in dispatch_request\n    return self.view_functionsrule.endpoint\ngindrop/api.py:186: in do_deploy\n    jdata = manager.deploy(data)\ngindrop/swarm.py:78: in deploy\n    service = client.services.create(\"redis:4.0.1 - alpine\",name='redis')\nvenv/lib/python2.7/site-packages/docker/models/services.py:177: in create\n    service_id = self.client.api.create_service(create_kwargs)\nvenv/lib/python2.7/site-packages/docker/utils/decorators.py:34: in wrapper\n    return f(self, args, **kwargs)\nvenv/lib/python2.7/site-packages/docker/api/service.py:134: in create_service\n    self._post_json(url, data=data, headers=headers), True\nvenv/lib/python2.7/site-packages/docker/api/client.py:228: in _result\n    self._raise_for_status(response)\nvenv/lib/python2.7/site-packages/docker/api/client.py:224: in _raise_for_status\n    raise create_api_error_from_http_exception(e)\n\ne = HTTPError(u'400 Client Error: Bad Request for url: http+docker://localunixsocket/v1.30/services/create',)\ndef create_api_error_from_http_exception(e):\n    \"\"\"\n    Create a suitable APIError from requests.exceptions.HTTPError.\n    \"\"\"\n    response = e.response\n    try:\n        explanation = response.json()['message']\n    except ValueError:\n        explanation = (response.content or '').strip()\n    cls = APIError\n    if response.status_code == 404:\n        if explanation and ('No such image' in str(explanation) or\n                            'not found: does not exist or no pull access'\n                            in str(explanation) or\n                            'repository does not exist' in str(explanation)):\n            cls = ImageNotFound\n        else:\n            cls = NotFound\n\n\n  raise cls(e, response=response, explanation=explanation)\n\nE       TypeError: 'exceptions.ValueError' object is not callable\n\nvenv/lib/python2.7/site-packages/docker/errors.py:31: TypeError\n------------------------------------------------------------------------------- Captured stderr call -------------------------------------------------------------------------------\n2017-12-18 12:01:05,665 | 11994 |[MainThread] |     DEBUG | docker.auth:find_config_file() | Trying paths: ['/Users/mbovo/.docker/config.json', '/Users/mbovo/.dockercfg']\n2017-12-18 12:01:05,666 | 11994 |[MainThread] |     DEBUG | docker.auth:find_config_file() | Found file at path: /Users/mbovo/.docker/config.json\n2017-12-18 12:01:05,666 | 11994 |[MainThread] |     DEBUG | docker.auth:load_config() | Found 'auths' section\n2017-12-18 12:01:05,666 | 11994 |[MainThread] |     DEBUG | docker.auth:parse_auth() | Found entry (registry=u'docker.facilitylive.int', username=u'mbovo')\n2017-12-18 12:01:05,666 | 11994 |[MainThread] |     DEBUG | docker.auth:load_config() | Found 'HttpHeaders' section\n2017-12-18 12:01:05,666 | 11994 |[MainThread] |     DEBUG | docker.auth:get_config_header() | Looking for auth config\n2017-12-18 12:01:05,666 | 11994 |[MainThread] |     DEBUG | docker.auth:resolve_authconfig() | Looking for auth entry for 'docker.io'\n2017-12-18 12:01:05,666 | 11994 |[MainThread] |     DEBUG | docker.auth:resolve_authconfig() | No entry found\n2017-12-18 12:01:05,666 | 11994 |[MainThread] |     DEBUG | docker.auth:get_config_header() | No auth config found\n2017-12-18 12:01:05,671 | 11994 |[MainThread] |     DEBUG | urllib3.connectionpool:_make_request() | http://localhost:None \"POST /v1.30/services/create HTTP/1.1\" 400 125\n-------------------------------------------------------------------------------- Captured log call ---------------------------------------------------------------------------------\nauth.py                    234 DEBUG    Trying paths: ['/Users/mbovo/.docker/config.json', '/Users/mbovo/.dockercfg']\nauth.py                    238 DEBUG    Found file at path: /Users/mbovo/.docker/config.json\nauth.py                    282 DEBUG    Found 'auths' section\nauth.py                    214 DEBUG    Found entry (registry=u'docker.facilitylive.int', username=u'mbovo')\nauth.py                    285 DEBUG    Found 'HttpHeaders' section\nauth.py                     44 DEBUG    Looking for auth config\nauth.py                    102 DEBUG    Looking for auth entry for 'docker.io'\nauth.py                    113 DEBUG    No entry found\nauth.py                     59 DEBUG    No auth config found\nconnectionpool.py          396 DEBUG    http://localhost:None \"POST /v1.30/services/create HTTP/1.1\" 400 125\n======================================================================== 1 failed, 7 passed in 0.55 seconds ========================================================================\n(venv) docker images\nREPOSITORY                              TAG                 IMAGE ID            CREATED             SIZE\nsonatype/nexus3                         latest              ee8a3baf060c        2 weeks ago         474MB\nredis                                   4.0.1-alpine        d8b7cc942736        3 months ago        27.5MB\ndocker.facilitylive.int/custom_centos   7.4.1708            1f2cdc055c5f        3 months ago        269MB\n(venv) docker services\ndocker: 'services' is not a docker command.\nSee 'docker --help'\n(venv) docker service ls\nID                  NAME                MODE                REPLICAS            IMAGE               PORTS\n(venv) \n```\nThe expected result is failure with something more clear about what's going wrong on docker engine side but the only thing retrieved is the TypeError: 'exceptions.ValueError' object is not callable\nThanks in advance. @feliperuhland sorry for the delay, \nfrom the python console all is working as expected:\n```\nPython 2.7.13 (default, Jul 18 2017, 09:16:53) \n[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport docker\nclient = docker.from_env()\nclient.services.create('redis:4.0.1 - alpine', name='redis')\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/Users/mbovo/fworks/devops/gindrop/venv/lib/python2.7/site-packages/docker/models/services.py\", line 177, in create\n    service_id = self.client.api.create_service(create_kwargs)\n  File \"/Users/mbovo/fworks/devops/gindrop/venv/lib/python2.7/site-packages/docker/utils/decorators.py\", line 34, in wrapper\n    return f(self, *args, kwargs)\n  File \"/Users/mbovo/fworks/devops/gindrop/venv/lib/python2.7/site-packages/docker/api/service.py\", line 134, in create_service\n    self._post_json(url, data=data, headers=headers), True\n  File \"/Users/mbovo/fworks/devops/gindrop/venv/lib/python2.7/site-packages/docker/api/client.py\", line 228, in _result\n    self._raise_for_status(response)\n  File \"/Users/mbovo/fworks/devops/gindrop/venv/lib/python2.7/site-packages/docker/api/client.py\", line 224, in _raise_for_status\n    raise create_api_error_from_http_exception(e)\n  File \"/Users/mbovo/fworks/devops/gindrop/venv/lib/python2.7/site-packages/docker/errors.py\", line 31, in create_api_error_from_http_exception\n    raise cls(e, response=response, explanation=explanation)\ndocker.errors.APIError: 400 Client Error: Bad Request (\"rpc error: code = InvalidArgument desc = ContainerSpec: \"redis:4.0.1 - alpine\" is not a valid repository/tag\")\n```\nSo it seems not a library bug \ud83d\udc4d \nUsing the lib wrapped around Flask is working too.\nI'm investigating further... the problem is limited to pytest + Flask test_client:\n\n\n\n```\napp = Flask(name)\nappclient = app.test_client()\nresponse = appclient.put('/deploy/deploy_test', content_type='multipart/form-data', data=data)\n``\nreturns the well knownE       TypeError: 'exceptions.ValueError' object is not callable`\nSeems the library docker/errors.py:22 \ncls = APIError\ncan't found the correct APIError class when using pytest + Flask test_client()\nThanks again for your support!\n. ",
    "pkit": "Spurious test failures??. Done. ",
    "anshulpundir": "@shin- @dperny @andrewhsu for review. >IMHOIRL, instead of changing the test, we should change the code so that it inserts the default name.\nWhich code ? @dperny . It was designed to accept any value allowed to be changed. But this is where we're at now. @shin- \nI did consider autofill in swarmkit, but I wanted to avoid hidden behavior. Explicit is always better IMO. Also the contract with swarmkit is read-modify-write, so spec name shouldn't really be changed @dperny . >Defaults are filled in everywhere else in the API (e.g. creating a volume without specifying a driver), I wouldn't call that \"hidden behavior\". Since there's nothing to actually configure here, it's just a frustrating \"gotcha\" that serves no practical purpose.\nSetting defaults is OK when doing create(), during update, I prefer more explicit behavior. \nThe need to fill in defaults in this case only arises because of the use of gRPC, which doesn't specify if a field was set in the message. For example, thrift does have mechanism to say which fields were explicitly set.\n. >One of the main reasons for running the SDK tests on moby/moby in the first place was to be able to catch and address breaking changes. I don't feel that updating the tests to match the new, arbitrary constraints really accomplishes that.\nIf clients don't do read-modify-write on swarm objects, it's incorrect usage and should be a breaking change. There's nothing wrong with breaking to enforce correct usage.. You're right. Thanks for pointing this out! @thaJeztah \nThe call to init_swarm() below will sanitize it and add the name if its not added.\nI didn't recall this and made the change everywhere. I will remove it and only keep it only for test_update_swarm() where init_swarm() is not called.. Should be we doing an init_swarm() here before calling update() ? @thaJeztah . Right, I guess I meant doing init_swarm() with a name specified. But I guess it wouldn't matter. Keeping this change and reverting others.. I'm OK with setting the default name in the client code. \nThe correct semantics for updating swarmkit objects is read-modify-write. Clients should not be writing cluster spec without reading it first i.e. the spec on the client side should be exactly the same as the spec in swarmkit.\n. Name is not a required param. What is required is that you don't change it, basically always read it before writing.\nRead-modify-write is not really a new thing and I believe has always been the semantic. If clients already do that, then this should not be breaking. \nThe problem in the failing test is that it tries to update the spec without reading it first.\nSeparately, the change to add name if it's not set on ClusterInit() should also be moved from moby/moby to swarmkit.. ",
    "dperny": "IMHOIRL, instead of changing the test, we should change the code so that it inserts the default name.. @shin- you're not wrong.. @anshulpundir the create_swarm_spec method should be changed to autofill the name with default. Really, we should alter swarmkit to automatically fill in that parameter on cluster spec create... . oh, thanks. i'm not sure why this didn't work in my PR, i'm glad someone with more python chops is working on it. i've closed my PR in favor of this one.. ",
    "Fiser12": "I have this same problem, for the moment I will have to pull that version. ",
    "dimaryaz": "I'm seeing the same bug - but I'm using docker 1.13.1. It hangs every time.. ",
    "agxs": "Hmm that doesn't seem to match what we're seeing, although maybe I'm just misunderstanding the magic of the client :)\nOur application does quite a lot of API requests to create services (Jupyterhub which creates a new docker service for each Notebook), and we're trying to improve performance as sometimes Notebooks take a long time to start.\nIf we use say local.registry.url:5000/foo/bar as the image name when calling self.docker('create_service', task_tmpl, name=self.service_name, networks=networks, labels=labels) then we see requests at the registry end from the workers. But if we change the image name to local.registry.url:5000/foo/bar@sha256:7dbcb0e8c0d96d04449ddf32c4053c07ecfa2bfce79a69ce137e337c4ae40ad4 then the requests to the registry go away. Ideally we'd like to not have to specify the hash of the image.\nIf it wasn't clear, this is docker running in swarm mode.. ",
    "lhenry7": "Hi, I find this new method does not work consistently. I got varying results, 1st time I called it with 1 task already running, service.scale(2) it worked like 'service update -force' and updated the running task and started a 2nd one.  But after that, service.scale(3) just didn't work at all, didn't start any new tasks or update existing ones.\nI find \n    mode = docker.types.ServiceMode('replicated', replicas=running_tasks + 1)\n    service.update(mode=mode, fetch_current_spec=True)\nworks like 'service scale', so I 'll continue doing that for now. . ",
    "umardraz": "Hello\nI have just did that\npip uninstall docker-pypip install docker\nThen it start working    On Thursday, December 28, 2017, 12:22:02 AM GMT+5, Joffrey F notifications@github.com wrote:  \nCan you provide the necessary information as explained here? Thank you!\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n    . ",
    "dfroger": "This should work:\npip install https://github.com/docker/docker-py/archive/1.10.6.tar.gz. The comand above is ok for me, I'm closing this (short) issue.... great, thanks!. ",
    "kairichard": "Hi @shin- \nThanks for clarifying - it did help. \nOnce I get around to having something packaged up, I will post back into this thread.\n. @circa10a no sadly, I did not make progress as there simply was not enough need.\n@shin- feel free to close.. ",
    "circa10a": "@kairichard You ever make any progress on this?. That was it. Thanks!. ",
    "kocsob": "And links parameter for user defined network can be provided somehow to the high level API for client.containers.create(..) or it works only with low level create_container(...)?\nThe documentation shows something similar in create parameters, why I am confused.. ",
    "pavdmyt": "@shin- I have updated the PR. Please take a look.\nNow instead of catching requests.exceptions.ConnectionError it makes GET /containers/(id or name)/archive to check non-existent path case. According to Docker API the response object in success case (i.e. path exists inside container) contains HTTP 200 status code and X-Docker-Container-Path-Stat header is set. If path doesn't exist inside a container the response object simply has 404 status code and mentioned header is not set. I have found out that checking just for proper status code is enough.\nPassing response object from GET /containers/(id or name)/archive to _raise_for_status method allows to reproduce the exact errors.NotFound instance as it would be produced in scenario when transferring a small file (when everything works fine). \nI've also experimented with HEAD /containers/(id or name)/archive which could be a more \"light-weight\" implementation. The HEAD request has the same properties as GET in terms of status codes and X-Docker-Container-Path-Stat header behavior. However, passing the resulting response to _raise_for_status method doesn't allows to instantiate errors.NotFound instance with detalized error message like this:\ndocker.errors.NotFound: 404 Client Error: Not Found (\"Could not find the file /foo in container ad1ca507a883de7dcea3a915bca9170cd6e1d3890fe1cec996618910f85781af\")\nInstead it returns this:\ndocker.errors.NotFound: 404 Client Error: Not Found\nThat's why I've decided to stick with GET.. Agree. We need more fine-grained control here. I have some ideas how to track the exact behavior described in #1808. Will experiment a bit and come up with improvements.. ",
    "dnovvak": "Thank you @feliperuhland for hint.\nI just removed \"credsStore\": \"wincred\" from ~/.docker/config.json as suggested here  and successfully deployed docker service with valid registry authentication:\n```\nDEBUG: Looking for auth entry for 'XXX.amazonaws.com'\nDEBUG: No entry found\nDEBUG: http://localhost:None \"POST /v1.35/auth HTTP/1.1\" 200 48\nDEBUG: {u'IdentityToken': u'', u'Status': u'Login Succeeded'}\nDEBUG: Looking for auth config\nDEBUG: Looking for auth entry for 'XXX.amazonaws.com'\nDEBUG: Found auth config\nDEBUG: http://localhost:None \"POST /v1.35/services/create HTTP/1.1\" 201 35\nDEBUG: http://localhost:None \"GET /v1.35/services/twclgt7acjuh3tz8uv717g1po HTTP/1.1\" 200 None\nThis is a good workaround for the issue but there is one more interesting thing - `docker login` command from cli works fine with `wincred`:\n\ndocker login XXX.amazonaws.com\nUsername: AWS\nPassword:\nLogin Succeeded\nAfter that I can create service with docker py successfully:\nDEBUG: Looking for auth config\nDEBUG: Using credentials store \"wincred\"\nDEBUG: Looking for auth entry for 'XXX.amazonaws.com'\nDEBUG: Found auth config\nDEBUG: http://localhost:None \"POST /v1.35/services/create HTTP/1.1\" 201 35\nDEBUG: http://localhost:None \"GET /v1.35/services/dvuywu1p5yktskprp9zgrn691 HTTP/1.1\" 200 None\n```\n\nSo in general there is something wrong with saving credentials to wincred I guess. I am not sure whether it is docker or docker py responsibility.. Fix verified successfully. Thank you.\n```\nDEBUG: Using credentials store \"wincred\"\nDEBUG: Looking for auth entry for 'XXX.amazonaws.com'\nDEBUG: No entry found\nDEBUG: No entry in credstore - fetching from auth dict\nDEBUG: Looking for auth entry for 'XXX.amazonaws.com'\nDEBUG: No entry found\nDEBUG: http://localhost:None \"POST /v1.36/auth HTTP/1.1\" 200 48\nDEBUG: {u'IdentityToken': u'', u'Status': u'Login Succeeded'}\nDEBUG: Looking for auth config\nDEBUG: Using credentials store \"wincred\"\nDEBUG: Looking for auth entry for 'XXX.amazonaws.com'\nDEBUG: No entry found\nDEBUG: No entry in credstore - fetching from auth dict\nDEBUG: Looking for auth entry for 'XXX.amazonaws.com'\nDEBUG: Found auth config\nDEBUG: http://localhost:None \"POST /v1.36/services/create HTTP/1.1\" 201 35\nDEBUG: http://localhost:None \"GET /v1.36/services/sd2r0u0zynlvr30qb7qna68ac HTTP/1.1\" 200 None\n```. ",
    "mrohera": "I am not sure why you closed this issue but this currently impacts Python 2,7 as well. Sorry if my message was not clear. I have edited it for clarity.. Perfect thank you for your message, this clears things up.. ",
    "mwilliammyers": "Thanks!\nIf I wanted to implement this, would I to need to update create_service() and create() ? \nIs this even available in the Docker Engine HTTP API (which I assume is what this package uses) yet?. Yeah I am interested in implementing this. I'll open a PR soon.. So I see a few options for implementing this:\n1. Most explicit:\npython\n  generic_reservations=[\n     docker.types.DescreteGenericResource(kind='SSD', value=1),\n     docker.types.NamedGenericResource(kind='GPU', value='UUID1')\n  ]\n  pros:\n\nputs the burden of figuring things out like value='1' on the user\ndockerd will give an error for e.g.: docker.types.NamedGenericResource(kind='GPU', value=1)\n\ncons:\n\ncumbersome and awkward for the user\n\n2. Determine NamedResourceSpec and DiscreteResourceSpec automatically with an isinstance() check:\npython\n  generic_reservations=[\n     docker.types.GenericResourceReservation(kind='SSD', value=3),\n     docker.types.GenericResourceReservation(kind='GPU', value='UUID1')\n  ]\npros:\n\na little less awkward than option 1\n\ncons:\n\nstill cumbersome and awkward for the user, doesn't gain ease of use\n\n3. Use a list of dict and determine NamedResourceSpec and DiscreteResourceSpec automatically with an isinstance() check:\npython\n  generic_reservations = [{'SSD': 3}, {'GPU': 'UUID1'}]\npros:\n\na lot simpler/cleaner for the user\n\ncons:\n\nless explicit, have to use isinstance() and all str values will be a NamedResourceSpec and all int values will be DiscreteResourceSpec.\nCould lead to silent errors...\n\n4. Use a list of str and determine NamedResourceSpec and DiscreteResourceSpec automatically with an isinstance() check:\npython\n  generic_reservations = ['SSD=3', 'GPU=UUID1']\npros:\n\nmatches the docker CLI\n\ncons:\n\nNot very pythonic\nA little awkward\nCould lead to silent errors...\n\nAnd for all cases, use generic_reservations like:\npython\n client.services.create(resources=docker.types.Resources(generic_reservations=generic_reservations))\nWhat are your thoughts? Or does it not matter? There seems to be some variation of each throughout the existing API...\nI have them all pretty much coded up so it doesn't a lot to me but I am partial to either 1 or 3. Yeah, should I add the integration tests in: tests/integration/models_resources_test.py and/or tests/integration/api_service_test.py?. @shin- I implemented your suggestions. Let me know if you want me to change anything else or if you would like me to squash my commits.. @shin- Done. \nI improved my integration test, but couldn't get the @pytest.mark.parametrize decorator to work from within the ServiceTest(BaseAPIIntegrationTest) class so I had to do a workaround to support testing dict, list and bad inputs.. Thanks! Sorry I didn't break the parsing out into a function and thanks for adding the API tag, I couldn't figure out exactly what version that was added in.... yeah, for sure.. So just allow:\n[{'DiscreteResourceSpec': {'Kind': 'gpu', 'Value': 1}}, ...]\nI was wondering if we wanted to expose that API to users...\nbecause I am building a CLI app that mimics the docker CLI so they are coming in like:\ngpu=1\nbut I don't think we should use that at the API level?. ok! Ill change it to match the docker API. Is L215 not doing that? As a sanity check I printed out res_template['Resources']['Reservations'] and it contained the generic resources...\nYeah I wasn't sure if we wanted an separate test function.... ",
    "greenmaid": "Hello @feliperuhland , thanks for your reply !\nIn fact, EndpointSpec only support \"vip\" and \"dnsrr\" modes. I'm looking for the \"host\" mode. IMHO it is even not the same kind of 'mode' we are speaking about.... If i understand well, \"vip\"/\"dnsrr\" mode refers to internal swarm load-balancing mode, the \"host\" mode i'm looking for refers to external port publishing...\nA more precise link from docker documentation: https://docs.docker.com/engine/reference/commandline/service_create/#publish-service-ports-externally-to-the-swarm--p-publish\nRegards\n. ",
    "larkost": "Just in case... this was signed 19 days ago.. @TomasTomecek Could you please respond to my responses? This PR is getting a bit stale.. No, the int() will already throw an error with a reasonably good message if it is not an integer, and that will show up nicely as a failure. And the actual port returned (on the host) is going to be effectively random in this case.. I don't think so. This is just me being forward-looking. If someone changed the test in the future to have multiple ports, then it would go down this branch. But for the moment it should just report that this has not been implemented (hopefully prompting that person to expand it).. ",
    "backtrackbaba": "Hey @shin- any update on this PR?. ",
    "veyalla": "@shin Can you please take a look at this one? This is a blocker for us.. ",
    "wekay102200": "@shin- thanks very much !!\nI use the docker-py package at begin,but My colleague use docker package.\nNow I understand the difference between them,I will user docker package and discard docker-py later.. the detail error information is below:\nTraceback (most recent call last):\n  File \"test_core.py\", line 51, in \n    load_image(\"/home/ubuntu/wgs_image_gatk3.7.tar\",\"\")\n  File \"test_core.py\", line 29, in load_image\n    ret = client.load_image(f.read())\n  File \"/usr/local/lib/python2.7/dist-packages/docker/api/image.py\", line 140, in load_image\n    res = self._post(self._url(\"/images/load\"), data=data)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/utils/decorators.py\", line 47, in inner\n    return f(self, *args, kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/client.py\", line 135, in _post\n    return self.post(url, self._set_request_timeout(kwargs))\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/requests/sessions.py\", line 559, in post\n    return self.request('POST', url, data=data, json=json, kwargs)\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/requests/sessions.py\", line 512, in request\n    resp = self.send(prep, send_kwargs)\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/requests/sessions.py\", line 622, in send\n    r = adapter.send(request, kwargs)\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/requests/adapters.py\", line 526, in send\n    raise ReadTimeout(e, request=request)\nrequests.exceptions.ReadTimeout: UnixHTTPConnectionPool(host='localhost', port=None): Read timed out. (read timeout=60). But when I use docker load \n    load_image(\"/home/ubuntu/wgs_image_gatk3.7.tar\",\"\")\n  File \"test_core.py\", line 29, in load_image\n    ret = client.load_image(f.read())\n  File \"/usr/local/lib/python2.7/dist-packages/docker/api/image.py\", line 140, in load_image\n    res = self._post(self._url(\"/images/load\"), data=data)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/utils/decorators.py\", line 47, in inner\n    return f(self, *args, kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/client.py\", line 135, in _post\n    return self.post(url, self._set_request_timeout(kwargs))\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/requests/sessions.py\", line 559, in post\n    return self.request('POST', url, data=data, json=json, kwargs)\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/requests/sessions.py\", line 512, in request\n    resp = self.send(prep, send_kwargs)\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/requests/sessions.py\", line 622, in send\n    r = adapter.send(request, kwargs)\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/requests/adapters.py\", line 526, in send\n    raise ReadTimeout(e, request=request)\nrequests.exceptions.ReadTimeout: UnixHTTPConnectionPool(host='localhost', port=None): Read timed out. (read timeout=600). @TomasTomecek i do it locally.when i use docker load xxxx.tar command ,it only spend less then 1 minute to load 2GB image.but when i use docker api to load,it occur the timeout error above.. \n\n. ",
    "harlowja": "Yup, that uses login method (though I didn't include it here); maybe thats an issue?. Sure, will try.. ",
    "hongkunyoo": "This is my environment settings\npip freeze | grep docker && python --version && docker version\n\ndocker==3.0.0\ndocker-pycreds==0.2.1\nPython 3.6.1 :: Continuum Analytics, Inc.\nClient:\n Version:      17.05.0-ce\n API version:  1.29\n Go version:   go1.7.5\n Git commit:   89658be\n Built:        Thu May  4 22:10:54 2017\n OS/Arch:      linux/amd64\nServer:\n Version:      17.05.0-ce\n API version:  1.29 (minimum version 1.12)\n Go version:   go1.7.5\n Git commit:   89658be\n Built:        Thu May  4 22:10:54 2017\n OS/Arch:      linux/amd64\n Experimental: false\n\n\nlsb_release -a\n\nDistributor ID: Ubuntu\nDescription:    Ubuntu 16.04.3 LTS\nRelease:        16.04\nCodename:       xenial\n\n\nuname -a\n\nLinux ryan 4.4.0-101-generic #124-Ubuntu SMP Fri Nov 10 18:29:59 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\n\n\nAlso, I got the same error when calling pull function without auth_config parameter even though I've used the same client object.\nclient.login(user, pwd, registry=my_ecr_registry)\nclient.images.pull('000.dkr.ecr.us-west-2.amazonaws.com/name')\n\nTraceback (most recent call last):\n  File \"/home/user_name/miniconda3/lib/python3.6/site-packages/docker/api/client.py\", line 223, in _raise_for_status\n    response.raise_for_status()\n  File \"/home/user_name/miniconda3/lib/python3.6/site-packages/requests/models.py\", line 928, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: http+docker://localunixsocket/v1.35/images/create?tag=latest&fromImage=0000.dkr.ecr.us-west-2.amazonaws.com%2Fname\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/home/user_name/miniconda3/lib/python3.6/site-packages/docker/models/images.py\", line 309, in pull\n    self.client.api.pull(repository, tag=tag, **kwargs)\n  File \"/home/user_name/miniconda3/lib/python3.6/site-packages/docker/api/image.py\", line 374, in pull\n    self._raise_for_status(response)\n  File \"/home/user_name/miniconda3/lib/python3.6/site-packages/docker/api/client.py\", line 225, in _raise_for_status\n    raise create_api_error_from_http_exception(e)\n  File \"/home/user_name/miniconda3/lib/python3.6/site-packages/docker/errors.py\", line 31, in create_api_error_from_http_exception\n    raise cls(e, response=response, explanation=explanation)\ndocker.errors.ImageNotFound: 404 Client Error: Not Found (\"repository 000.dkr.ecr.us-west-2.amazonaws.com/name not found: does not exist or no pull access\")\n. I've acquired the password through aws boto API and it worked.\nIs there a difference between acquiring the password through aws ecr get-login on console manually and acquiring through boto API? (I used the password getting from aws ecr get-login within 1hr)\nSorry to bother you.\n\nboto_client = boto3.client('ecr')\nauth = boto_client.get_authorization_token(registryIds=['00'])\nuser = 'AWS'\npwd = auth['authorizationData'][0]['authorizationToken']\nclient = docker.from_env()\nclient.login(username=user, password=pwd, registry='https://00.dkr.ecr.us-west-2.amazonaws.com')\nimage = client.images.pull(repo). ",
    "pvizeli": "LGTM. That should solve all our problems with v3. What is the release circle of this patch?. I think also that need go into a hotfix. Actual it make every image/file handling unusable. ",
    "RoxanaTapia": "Also works for me. ",
    "mefyl": "I don't really like the idea of squashing: I don't see what it brings to the table as you can always review the diffs for the whole merge, but I think it removes a lot. As in, readibility, and for instance I added two optional \"optimization\" commits that can easily be blamed back, and reverted individually while conserving the semantics if those optimisations cause problem.\nNow, I don't mean to come in the project on my high horses and impose my ways, if you prefer I'll squash for now, but that's a discussion I'd like to have at some point.. If you give me the .dockerignore and the culprit file, I can assess whether it's included by the current code or not, and whether that's a bug or a feature.. The official documentation about this is rather elusive. It does however stipulate that absolute paths are equivalent to relative paths, hence /../foo should be equivalent to ../foo. It also stipulates that paths are run through Go's filepath.Clean, which explicitely makes this equivalence: \nEliminate .. elements that begin a rooted path:\nthat is, replace \"/..\" by \"/\" at the beginning of a path\nFrom this, I would argue that yes, the specification prompts for ../foo to be equivalent to foo.\nBut as you smartly suggested, I tested with Docker 17.09.1-ce, and ../foo does not ignore a foo file at the root - I suppose it won't ignore anything ever.\nHowever, brace yourself, if you use /../foo, it does ignore a foo file at the root, which violates the statements that absolute and relative paths are equivalent. Long story short: it's a mess.\nIt might be more reasonable to mimic the current engine behavior, although not following the spec could seem worse. The spec would probably need some updating if we choose to mimic the engine.. Nah, there's a for loop in walk reusing the patterns. As for the intermediate list inside reverse, sadly python2's reverse expects a plain collection -_-. The previous implementation did not handle ** in exceptions, and it didn't look easy with this approach while preserving the optimization to not explore everything recursively when we're sure a directory is entirely excluded. Also, the current implementation looks much shorter and simpler to me, leveraging the stdandard fnmatch - provided I actually use the standard fnmatch, granted ;). I agree that even any .. in a .dockerignore file doesn't make much sense to me. Especially since anything/.. will be dropped, even if anything does not exist as a directory, so I really don't see the point. IMO we should even deprecate and then reject them.. Nitpicking, but any(e[0] == path for e in extra_files) won't build the whole list in memory. Maybe even building a set(e[0] for e in extra_files) before to avoid iterating too many times.. ",
    "stephanbochet": "I am running into the same problem.  I had to downgrade to docker-py2.7.  Currently using docker  Version 17.12.0-ce-mac49 \nI fixed it by removing python and python packages and re-installing python.  Installing the legacy docker-py package appears to cause issues when installed along the latest docker package.. Yes, different versions caused an issue on my machine.  I had to remove python and re-install it.. ",
    "pycampers": "I am getting the same error in a pipenv virtualenv\n$ docker --version          \nDocker version 17.12.1-ce, build 7390fc6\nunder Python 3.6.4,\n$ pipenv graph           \nclick==6.7\ncrayons==0.1.2\n  - colorama [required: Any, installed: 0.3.9]\ndocker==3.1.2\n  - docker-pycreds [required: >=0.2.2, installed: 0.2.2]\n    - six [required: >=1.4.0, installed: 1.11.0]\n  - requests [required: !=2.18.0,>=2.14.2, installed: 2.18.4]\n    - certifi [required: >=2017.4.17, installed: 2018.1.18]\n    - chardet [required: <3.1.0,>=3.0.2, installed: 3.0.4]\n    - idna [required: >=2.5,<2.7, installed: 2.6]\n    - urllib3 [required: >=1.21.1,<1.23, installed: 1.22]\n  - six [required: >=1.4.0, installed: 1.11.0]\n  - websocket-client [required: >=0.32.0, installed: 0.47.0]\n    - six [required: Any, installed: 1.11.0]\nGitPython==2.1.8\n  - gitdb2 [required: >=2.0.0, installed: 2.0.3]\n    - smmap2 [required: >=2.0.0, installed: 2.0.3]\ngiturlparse==0.9\nhalo==0.0.12\n  - backports.shutil-get-terminal-size [required: ==1.0.0, installed: 1.0.0]\n  - colorama [required: ==0.3.9, installed: 0.3.9]\n  - cursor [required: ==1.2.0, installed: 1.2.0]\n  - log-symbols [required: ==0.0.11, installed: 0.0.11]\n    - colorama [required: ==0.3.9, installed: 0.3.9]\n    - enum34 [required: ==1.1.6, installed: 1.1.6]\n  - six [required: ==1.11.0, installed: 1.11.0]\n  - spinners [required: ==0.0.19, installed: 0.0.19]\n    - enum34 [required: ==1.1.6, installed: 1.1.6]\n  - termcolor [required: ==1.1.0, installed: 1.1.0]\nipython==6.2.1\n  - decorator [required: Any, installed: 4.2.1]\n  - jedi [required: >=0.10, installed: 0.11.1]\n    - parso [required: ==0.1.1, installed: 0.1.1]\n  - pexpect [required: Any, installed: 4.4.0]\n    - ptyprocess [required: >=0.5, installed: 0.5.2]\n  - pickleshare [required: Any, installed: 0.7.4]\n  - prompt-toolkit [required: <2.0.0,>=1.0.4, installed: 1.0.15]\n    - six [required: >=1.9.0, installed: 1.11.0]\n    - wcwidth [required: Any, installed: 0.1.7]\n  - pygments [required: Any, installed: 2.2.0]\n  - setuptools [required: >=18.5, installed: 39.0.1]\n  - simplegeneric [required: >0.8, installed: 0.8.1]\n  - traitlets [required: >=4.2, installed: 4.3.2]\n    - decorator [required: Any, installed: 4.2.1]\n    - ipython-genutils [required: Any, installed: 0.2.0]\n    - six [required: Any, installed: 1.11.0]\nnumpy==1.14.2\nTraceback\nTraceback (most recent call last):\n  File \"dockapt.py\", line 12, in <module>\n    docker_client = docker.from_env()\n  File \"/home/dev/.local/share/virtualenvs/dockapt-LVPjMQXP/lib/python3.6/site-packages/docker/client.py\", line 81, in from_env\n    **kwargs_from_env(**kwargs))\n  File \"/home/dev/.local/share/virtualenvs/dockapt-LVPjMQXP/lib/python3.6/site-packages/docker/client.py\", line 38, in __init__\n    self.api = APIClient(*args, **kwargs)\n  File \"/home/dev/.local/share/virtualenvs/dockapt-LVPjMQXP/lib/python3.6/site-packages/docker/api/client.py\", line 110, in __init__\n    config_dict=self._general_configs\nTypeError: load_config() got an unexpected keyword argument 'config_dict'\nHere are my Pipfile and Pipfile.lock \ndockapt.zip\nUPDATE:\nDowngrading to docker==2.7.0 using pipenv install \"docker<3\" solves the issue\n. ",
    "w7089": "Have the same issue still with docker 3.3.0\n\nTraceback (most recent call last):\n  File \"/net/netstore2/vol/paas_clusters_vol/cluster336/PAAS/paas-config/1.1.290/ServicePostDeploy/mariadb\", line 77, in \n    docker_client = docker.from_env()\n  File \"/usr/lib/python2.7/site-packages/docker/client.py\", line 81, in from_env\n    kwargs_from_env(kwargs))\n  File \"/usr/lib/python2.7/site-packages/docker/client.py\", line 38, in init\n    self.api = APIClient(args, *kwargs)\n  File \"/usr/lib/python2.7/site-packages/docker/api/client.py\", line 110, in init\n    config_dict=self._general_configs\nTypeError: load_config() got an unexpected keyword argument 'config_dict'. @shin- removed dockerand docker-py packages, reinstalled docker and docker-py packages and getting the error still:\ndocker_client = docker.from_env()\n\nFile \"/usr/lib/python2.7/site-packages/docker/client.py\", line 81, in from_env\n    kwargs_from_env(kwargs))\n  File \"/usr/lib/python2.7/site-packages/docker/client.py\", line 38, in init\n    self.api = APIClient(args, *kwargs)\n  File \"/usr/lib/python2.7/site-packages/docker/api/client.py\", line 110, in init\n    config_dict=self._general_configs\nTypeError: load_config() got an unexpected keyword argument 'config_dict'\n\npython version is \n\nPython 2.7.5 (default, Aug  4 2017, 00:39:18)\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux2\n\n\nOn other system, your suggestion helped:\n\nPython 2.7.5 (default, Nov 20 2015, 02:00:19) \n[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport docker\nc = docker.from_env()\n. i'm confused sorry, what should be installed then? just docker?\ndocker-py docs offers to run:\n\n\n\n\npip install docker\nimport docker\nwhat is docker-py, then?\n. ",
    "hushenbeg": "@shin-  i am using python 3.6 version and trying to import the docker \nand i got the\nTypeError: load_config() got an unexpected keyword argument 'config_dict'\nplease tell me whats happing exactly inside the code \n. ",
    "geoffreywiseman": "I'm getting the sense that if I'm trying to use the docker module with Python2, I need to install \"docker<3\", but that's not covered by the module documentation anywhere that I can see? The README suggests to just install 'docker', and the pypi docs suggest 'docker' is compatible with Python2, but if I do that, I end up at this error.. ",
    "The-Loeki": "there you go... Although I do have to say I fail to see reasoning behind the hard demand for it; after all the only reason the key is trustworthy is because I said so in GH, so if my account was compromised so would the trust be.\n. @GordonTheTurtle I've added the GPG sig as per your request, but your dco-signed is apparently stuck in 'Waiting for status to be reported' or something?. Guess I should've read a bit better ;) sorry about that. ",
    "Tabrizian": "I think it is because of this line in the dependencies\nhttps://github.com/docker/docker-py/blob/8b246db271a85d6541dc458838627e89c683e42f/requirements.txt#L15. No, I'm using CPython.\n```\n\n\n\nimport platform\nplatform.python_implementation()\n'CPython'\n```. Yes. Could you please say why this installation fails? I'm just wondering about it.. \nMay I start working on it?\n\n\n\nOn Feb 26, 2018, at 11:20 PM, Joffrey F notifications@github.com<mailto:notifications@github.com> wrote:\nThe pypiwin32 package has a 220 releasehttps://pypi.python.org/pypi/pypiwin32/220 that's compiled for Python 3.6 and a 219 releasehttps://pypi.python.org/pypi/pypiwin32/219 that works with 2.7-3.5. The newer 223 release may work with 3.7, but I haven't had time to validate it yet.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/docker/docker-py/issues/1926#issuecomment-368627180, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AJoxVwiAhh8HKuS-PBY8yVAarSk5SeHVks5tYwrmgaJpZM4SSa5k.\n. ",
    "zhangxuehong": "When I run \u201c\u201dpip install docker\u201d on windows cmd(administror), i got the error, PermissionError: [Errno 13] Permission denied: 'd:\\develope\\python35\\Lib\\site-packages\\pypiwin32_system32\\pythoncom35.dll'\ni use python 3.5. ",
    "montao": "Still no python 3.7 support? The awsebcli seems to depend on docker-py. I get this error message with python 3.7:\n```\nCould not find a version that satisfies the requirement pypiwin32==220; sys_platform == \"win32\" and python_version >= \"3.6\" (from docker<4.0,>=3.3.0->docker-compose<1.22.0,>=1.21.2->awsebcli) (from versions: 219, 223)\nNo matching distribution found for pypiwin32==220; sys_platform == \"win32\" and python_version >= \"3.6\" (from docker<4.0,>=3.3.0->docker-compose<1.22.0,>=1.21.2->awsebcli)\n```. ",
    "jdstraye": "Yes, I have the same problem, trying to install aws-sam-cli. I just upgraded to pip 18.0 and when I tried again to \"pip install aws-sam-cli,\" a lot of wheels were downloaded/updated. I was hopeful, but I ran into the same \"requirements not met\" problem. \nI would love for this to be fixed, but I also have work to do. It seems I can proceed by reverting to Python 3.6.6. When do you expect this compatibility issue to be fixed?\n@shin- , you mentioned that the 223 release may work. I notice that both docker and aws-sam-cli have the requirement pypiwin32==220 in the error reports. How can we try 223 to see if it works? Hopefully, this is not as basic as it sounds. If they were specified in a local requirements.txt I could do it, but these requirements seem to be coming from elsewhere.. ",
    "jongio": "@shin- Do you have rough ETA for 3.5.0?  Thanks!. ",
    "DarthJahus": "Best ETA ever!\nC:\\Python37\\Scripts\\pip.exe install docker\n...\nSuccessfully installed docker-3.5.0 docker-pycreds-0.3.0\nThanks, @shin-!. ",
    "mnottale": "Fixes docker-compose issue https://github.com/docker/compose/issues/3419. Yes I did using tcpdump. I can confirm no DNS queries with this patch (and one per docker connection without). Fixes #1929 . Fixes #1934 . ",
    "ywang412": "Thanks! . ",
    "c4tz": "Thank you for the fast response! :)\ncat ~/.dockercfg\n{\n  \"https://eu.gcr.io\": {\n    \"email\": \"not@val.id\",\n    \"auth\": \"XYZ\"\n  },\n  \"https://staging-k8s.gcr.io\": {\n    \"email\": \"not@val.id\",\n    \"auth\": \"XYZ\"\n  },\n  \"https://gcr.io\": {\n    \"email\": \"not@val.id\",\n    \"auth\": \"XYZ\"\n  },\n  \"https://us.gcr.io\": {\n    \"email\": \"not@val.id\",\n    \"auth\": \"XYZ\"\n  },\n  \"https://k8s.gcr.io\": {\n    \"email\": \"not@val.id\",\n    \"auth\": \"XYZ\"\n  },\n  \"https://asia.gcr.io\": {\n    \"email\": \"not@val.id\",\n    \"auth\": \"XYZ\"\n  }\n}\ndocker-compose pull\nPulling test (eu.gcr.io/some/private/image:latest)...\nERROR: pull access denied for eu.gcr.io/some/private/image, repository does not exist or may require 'docker login'\nAgain, this will only happen, if there is no ~/.docker/config.json created before by any other process (like docker itself, e.g.). ",
    "cinjon": "Hmm, I think I must be doing something silly. If I go to ~/repo and run docker build -t foo/bar . -f path/to/directory/DockerFile, then everything works no problem. If I instead do it in a python environment with \nos.chdir('~/repo') \nclient.images.build(path=\"path/to/directory\", tag='foo/bar', quiet=False, rm=True)\nThen it fails at a RUN command where I use pip to install packages from a requirements.txt.. Aye, thanks. That works.. Ok, I figured this out. It looks like I needed to explicitly run .tag(\"foo/bar\").. ",
    "VillanCh": "The similar situation I met, sometimes the containers.list blocks all my operations! OMG. Awosome! You are right, thanks for your advice, the low-level api client.api.containers(all=True) is Similar to the docker ps command . It looks like never frozen.\nMaybe, is there the same situation as client.images.list? I guess.. ",
    "reverland": "@shin- \nThe similar situation I met, it just blocks like frozen.. ",
    "yugokato": "client.api.containers(all=True) worked fine as shin- suggested. But it would be nice if the option to create Container objects with limited information without blocking is available.. ",
    "ansemjo": "I have just hit that same error when running docker-compose. Actually, I had a different one before that but I did not take note what it was specifically. It was obvious however that the culprit was a version mismatch of python-requests.\nI proceeded to reinstall python-requests through my package manager to get the latest version. When I ran docker-compose afterwards I got the error message above.\nCompletely purging all requests-related files, i.e.\n```\nrm -rf /usr/lib/python3.6/site-packages/requests*\n``\nand then reinstalling that package again fixed it for me. [Others](https://github.com/coursera-dl/coursera-dl/issues/611) seem to suggest that downgradingrequeststo2.10.0` helps.. ",
    "socketpair": "so, what is the minimal version of Docker where RUN --mount=.... works ?. ",
    "nbrady-techempower": "Running into the same issue, there are actually prunes for everything:\nclient.containers.prune()\nclient.images.prune()\nclient.networks.prune()\nclient.volumes.prune()\nNote that you're just calling it on images.. ",
    "robsco-git": "There also seems to be a discrepancy between using the Python API and the Docker command line for me.. ",
    "rodrigodlima": "For me, the client.images.prune() don't works too. For example, I have one image nginx:1.12.2 that is not in use for any container, when I run the client.images.prune(), this image is not removed.\n```Python 2.7.5 (default, Aug  4 2017, 00:39:18)\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport docker\nclient = docker.from_env()\nclient.images.list()\n[, , ]\nclient.images.prune()\n{u'SpaceReclaimed': 0, u'ImagesDeleted': None}\n````\n[root@localhost ~]# docker image prune -a -f\nDeleted Images:\nuntagged: docker.io/nginx:1.12.2. \n\n\n",
    "rsjethani": "I get the same behavior as @rodrigodlima has with server api version 1.37. yes that worked! Thanks. Do we have 'until' filter available?. Ahh yes 'until' filter also works. But its not mentioned in the docs. ",
    "levin-du": "ping @shin- \ud83e\udd17  thks. ",
    "HariSekhon": "@shin- Thanks very much, that's what I thought.\nIt might be worth documenting that field in the docker-py documentation for what client.info(). ",
    "caniciotekno": "Resolved. Problems with pip cache in my environment. Sorry.. ",
    "sanathkr": "Love it! Instead of creating the stream inside attach(), how about letting the user pass the streams? Easier to unit test and users can pass any fancy stream they want - even sys.stdout/sys.stderr if they choose to:\n```\ndef attach(**kwargs):\n\"\"\"\nIf both stdout_stream and stderr_stream are not specified, then this method will return an iterator of output strings containing the contents of both stdout and stderr.\n:param bool stdout: existing argument\n:param bool stderr: existing argument\n:param bool stream: existing argument\n:param bool logs: existing argument\n:param io.BaseIO stdout_stream: Optional stream where the stdout from Docker container is written to.\n:param io.BaseIO stderr_stream: Optional stream where the stderr from Docker container is written to.\n\"\"\"\n```. ",
    "holiman": "I would really love to have this. Any progress? (for my usecase, on docker_exec ). ",
    "little-dude": "@holiman I've the same use case than yours. I've opened a PR for that if you want to try it out. . I'd like to have that. Are you planning to finish this up @Matzz / @onzo-mateuszzakarczemny ? Otherwise can I pick it up?. @shin- do you mind taking a quick look at this? I'll have some time this week to fix the tests (and add some for this feature). If you have any comment I'll handle them as well.. Tests should pass now. I'd like to add a few other tests for demux=True though, because _read_from_socket is a bit more complex now. Fwiw I've been using that for the past two weeks and it works fine for me.. @shin- any chance of getting this reviewed? It's quite demoralizing to not get any feedback.... Thank you! Rebased. ~~I'm not on my dev machine right now though, so I haven't re-run the tests locally. Let's see what CI has to say about this.~~\nEdit: it looks ok :). @shin- CI passed. What do you think? As I said I can add additional tests, but I'd like to know you like the implementation first to avoid extra work.. Thanks a lot for the review! I'll address the comments and add a test or two.\n\nDesign-wise, I don't think there's ever a case where the data we receive from the server is in the STDIN channel.\n\nYeah I was confused by this as well, this is a weird API.. Addressed the comment (except the one about the docstring), and rebased on top of https://github.com/docker/docker-py/pull/2178 for now to prevent some pytest failures.\nI still need to add a test for the new API.. Rebased, comments addressed! I added a bunch of unit tests (and found a bug!). I'll look at the integration tests to see if there's something I can easily add there.. I found a couple places where I could add tests. I'll update the PR in a few hours.. @shin- I think this is ready (provided that CI passes).. Thanks a lot for your reviews @shin- !. Not sure why Jenkins is failing. Is it because of those INTERNALERROR?. Ah yeah. Looks like it. The last green job tells us which tests trigger the error:\n19:56:27 [py2.7_17.06.2-ce] tests/integration/api_container_test.py::LogsTest::test_logs_streaming_and_follow_and_cancel\n19:56:27 [py2.7_17.06.2-ce]   /usr/local/lib/python2.7/site-packages/pytest_timeout.py:178: RemovedInPytest4Warning: MarkInfo objects are deprecated as they contain merged marks which are hard to deal with correctly.\n19:56:27 [py2.7_17.06.2-ce]   Please use node.get_closest_marker(name) or node.iter_markers(name).\n19:56:27 [py2.7_17.06.2-ce]   Docs: https://docs.pytest.org/en/latest/mark.html#updating-code. Seems that it's due to pytest-timeout:\npython\n    @pytest.mark.timeout(5)\n    @pytest.mark.skipif(os.environ.get('DOCKER_HOST', '').startswith('ssh://'),\n                        reason='No cancellable streams over SSH')\n    def test_attach_stream_and_cancel(self):\n        # .... Seems like bumping pytest-timeout does the trick. Tests should be passing now.. The integration tests don't seem to run on appveyor :(. working on a fix for this.. @funkyfuture I can write something, I'm not sure where it should go, since these changes don't expose anything new. It just ensures that if you have a proxy defined in your client's config.json, the corresponding environment variables are set when building a container, creating an exec instance, and creating a container. This is more a \"bug fix\" than a new feature in the sense that it implements the documented behavior.\nEdit: I edited my first message to mention that this is a fix for https://github.com/docker/docker-py/issues/2199.. @shin- when/if this is merged, would you mind making a release? I'd really like to have a release with proxy support and exec streams demux from https://github.com/docker/docker-py/pull/2150. @shin- this should be ready for reviewing now :). @shin- This is ready for another round :). Awesome, thank you very much @shin-, and sorry for not pushing that past the finish line.. Definitely.. done. done. I got rid of socket_raw_iter(), and added a tty argument to iter_frames().. I'm not sure where to put it though. I can just copy paste if for attach(), or add a reference to it.. This is a bit unfortunate. To be consistent, this should probably be:\npython\nassert exec_log == (b'hello out\\r\\nhello err\\r\\n', None)\nBut on the other hand, tty=True and demux=True does not really make sense anyway so I doubt it's going to be an issue in practice.. Note to self: remove stdin. remove stdin. We could but strictly speaking I don't think it's really invalid. I don't see a use case for it, but I'd like not to restrict the API. I propose we fix the consistency issue instead. Here is a possible fix: https://github.com/docker/docker-py/pull/2150/commits/f80eecc0a3dc0be580b1a48259938ca98de1598d. I don't mind reverting, but I think it can be useful because when a test fail we often get this kind of message:\n16:18:05 [py2.7_18.09.1-rc1] =================================== FAILURES ===================================\n16:18:05 [py2.7_18.09.1-rc1] _______________________ ExecTest.test_execute_proxy_env ________________________\n16:18:05 [py2.7_18.09.1-rc1] tests/integration/api_exec_test.py:28: in test_execute_proxy_env\n16:18:05 [py2.7_18.09.1-rc1]     assert exec_log == '''FTP_PROXY=a\n16:18:05 [py2.7_18.09.1-rc1] E   AssertionError: assert 'FTP_PROXY=a\\n...b\\nno_proxy=d\\n' == 'FTP_PROXY=a\\n...c\\nno_proxy=d'\n16:18:05 [py2.7_18.09.1-rc1] E       FTP_PROXY=a\n16:18:05 [py2.7_18.09.1-rc1] E       HTTPS_PROXY=b\n16:18:05 [py2.7_18.09.1-rc1] E       HTTP_PROXY=c\n16:18:05 [py2.7_18.09.1-rc1] E       NO_PROXY=d\n16:18:05 [py2.7_18.09.1-rc1] E       ftp_proxy=a\n16:18:05 [py2.7_18.09.1-rc1] E     + https_proxy=b\n16:18:05 [py2.7_18.09.1-rc1] E       http_proxy=c...\n16:18:05 [py2.7_18.09.1-rc1] E     \n16:18:05 [py2.7_18.09.1-rc1] E     ...Full output truncated (5 lines hidden), use '-vv' to show\n16:18:05 [py2.7_18.09.1-rc1] =========================== short test summary info ============================\nThen I have to edit the Makefile to add -vv and re-run the tests.. Much nicer indeed, thanks.. I assume you're talking about the 'default' key here? I couldn't find what other keys could be present, it does not seem to be documented.. No. It's actually unrelated with the of the PR but I thought this was dead code since the client does not have a _cfg attribute.. Sure.. I think I got this in 2fc5a36. ",
    "DxCx": "Well, it had much much more files then just those few..\nI just gave an example of the error i was getting since i couldnt really share what i am building :/\nIll try to see if i can reproduce that with other sources, however, it looks like it is related to context creation on client side (since server/engine did not update)\nIs there any changes around this area on the new version?. Not intentionally :)\nIll try importing both packages and see if the functions around here returns different values.\nThanks that was very helpful, hope to provide with more info soon :). Just finished quick review of the patch, i bet thats the root cause for the breaking change in having.\nIll write a small python script to invoke my exculde_patterns om my project with both versions and report back with the findings :)\nHelped me a ton! Thanks!. hey @shin- thanks for the fix!\ni've just verified updating to 3.1.4 and it works =). ",
    "teohhanhui": "Is there any chance of this making it into docker-compose 1.20 so as not to cause havoc for potentially many people? (We at https://github.com/api-platform/api-platform are definitely affected.). ",
    "BenDoan": "Thanks for the response!\nUsing sparse mode should avoid this issue as well, so I don't think it makes too much sense to add an ignore_removed option.\nI'm not sure I agree that this a breaking change. I can't think of any legitimate case where a user would be relying on 404s thrown while list() is iterating, and the bug makes containers.list() useless for some users. \nThat being said, I definitely understand wanting to be cautious about breaking existing users.. ",
    "crazychl": "the other ports all map correctly ! only the 2375 crash!. Is it possible that DockerClient will map port to random port in the case of insufficient  memory or CPU resources?. ",
    "mpasternacki": "I am experiencing this as well. This started occuring after upgrading Docker from docker-engine 17.05 to docker-ce 18.03 on Ubuntu 16.04 LTS.. I can see some entries in the backtrace that seem to be related to reading chunked response; might this be related to https://github.com/requests/requests/issues/4402 ?. ",
    "marpetr": "Looks like this release is totally broken - #1968. . Looks like the public wheel package was built incorrectly. There are two conflicting definitions:\n```\nLink copied from https://pypi.python.org/pypi/docker\n$ wget https://pypi.python.org/packages/41/e9/a41d58413cdedf7de0faf98f8681cd1810611a9a3826891fc05df165a2ff/docker-3.1.2-py2.py3-none-any.whl#md5=502ebdf82fee5c0f337ff9ed8ab5f0f6\n$ tar xf docker-3.1.2-py2.py3-none-any.whl\n$ grep -R 'def load_config' docker\ndocker/auth.py:def load_config(config_path=None, config_dict=None):\ndocker/auth/auth.py:def load_config(config_path=None):. Works now. Thanks for the quick response!. ",
    "mmariani": "removing the \"auth\" directory and leaving auth.py worked for me. works for me at least, thanks. ",
    "xingxing": "This bug is shown again in 3.1.4?\nDocker version 17.12.0-ce, build c97c6d6\n\u279c  ~se_docker git:(master) \u2717 docker-compose up -d\nTraceback (most recent call last):\n  File \"/usr/local/bin/docker-compose\", line 11, in <module>\n    sys.exit(main())\n  File \"/usr/local/lib/python2.7/site-packages/compose/cli/main.py\", line 71, in main\n    command()\n  File \"/usr/local/lib/python2.7/site-packages/compose/cli/main.py\", line 124, in perform_command\n    project = project_from_options('.', options)\n  File \"/usr/local/lib/python2.7/site-packages/compose/cli/command.py\", line 41, in project_from_options\n    compatibility=options.get('--compatibility'),\n  File \"/usr/local/lib/python2.7/site-packages/compose/cli/command.py\", line 121, in get_project\n    host=host, environment=environment\n  File \"/usr/local/lib/python2.7/site-packages/compose/cli/command.py\", line 92, in get_client\n    environment=environment, tls_version=get_tls_version(environment)\n  File \"/usr/local/lib/python2.7/site-packages/compose/cli/docker_client.py\", line 120, in docker_client\n    client = APIClient(**kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/docker/api/client.py\", line 110, in __init__\n    config_dict=self._general_configs\nTypeError: load_config() got an unexpected keyword argument 'config_dict'. ",
    "ezh": "Confirmed 3.1.4 has the same problem. Downgrade to 3.1.3 is solved the issue. . ",
    "ice-92": "I tried downgrading using pip install docker==3.1.3, but I'm still getting the same error message. Any other suggestions?\nI'm pretty new at this.. Still getting the error when I use docker-compose and python -c 'import docker; docker.from_env()'.\n[psudo@dedibox apache]$ docker-compose build --pull\nTraceback (most recent call last):\n  File \"/usr/bin/docker-compose\", line 11, in <module>\n    sys.exit(main())\n  File \"/usr/lib/python2.7/site-packages/compose/cli/main.py\", line 71, in main\n    command()\n  File \"/usr/lib/python2.7/site-packages/compose/cli/main.py\", line 124, in perform_command\n    project = project_from_options('.', options)\n  File \"/usr/lib/python2.7/site-packages/compose/cli/command.py\", line 41, in project_from_options\n    compatibility=options.get('--compatibility'),\n  File \"/usr/lib/python2.7/site-packages/compose/cli/command.py\", line 121, in get_project\n    host=host, environment=environment\n  File \"/usr/lib/python2.7/site-packages/compose/cli/command.py\", line 92, in get_client\n    environment=environment, tls_version=get_tls_version(environment)\n  File \"/usr/lib/python2.7/site-packages/compose/cli/docker_client.py\", line 120, in docker_client\n    client = APIClient(**kwargs)\n  File \"/usr/lib/python2.7/site-packages/docker/api/client.py\", line 110, in __init__\n    config_dict=self._general_configs\nTypeError: load_config() got an unexpected keyword argument 'config_dict'\n[ice-92@dedibox apache]$  \n[ice-92@dedibox apache]$ python -c 'import docker; docker.from_env()'\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/usr/lib/python2.7/site-packages/docker/client.py\", line 81, in from_env\n    **kwargs_from_env(**kwargs))\n  File \"/usr/lib/python2.7/site-packages/docker/client.py\", line 38, in __init__\n    self.api = APIClient(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/docker/api/client.py\", line 110, in __init__\n    config_dict=self._general_configs\nTypeError: load_config() got an unexpected keyword argument 'config_dict'\n[ice-92@dedibox apache]$\nNot really sure what to do. I just need to get docker-compose to work. \nUsing docker 3.1.4, docker-py 1.10.6 and docker-compose 1.20.1. Does it matter what version of pip I use? My distro's version is 8.1.2 and I did upgrade it to 9.0.3 earlier using pip's upgrade feature, is it possible that caused any conflicts? \n. Alright, I somehow fixed it by doing\npip uninstall docker-py\npip uninstall docker-compose\npip install --upgrade --force-reinstall --no-cache-dir docker-compose . ",
    "wjhill": "Hi -- I'm seeing the same error with a fresh install this evening, that started with 3.1.4:\n```\n$ pip list | grep docker\ndocker (3.1.4) [etc...]\n$ python -c 'import docker; docker.from_env()'\n[Traceback...]\nTypeError: load_config() got an unexpected keyword argument 'config_dict'\n```\nthen found this page, downgraded to 3.1.3 per above, and got the same error again:\n[pip uninstall command...]\n  Successfully uninstalled docker-3.1.4\n$ sudo pip install --no-cache-dir docker==3.1.3 \nCollecting docker==3.1.3\n  Downloading docker-3.1.3-py2.py3-none-any.whl (121kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 122kB 603kB/s\nRequirement already satisfied[etc...]\nSuccessfully installed docker-3.1.3\n$ python -c 'import docker; docker.from_env()'\n[Traceback...]\nTypeError: load_config() got an unexpected keyword argument 'config_dict'\nI can wipe/reinstall (I'm writing an Ansible script for this machine) if needed to help diagnose, also open to me having fat fingered something, here. :)\n. Removing docker-py allowed the \"[...]docker.from_env()\" command to run successfully with 3.1.4 installed.\nI see where the Ansible Role I'm using installs docker-py; I'll put in an Issue with them about this, and point to this discussion, once I test locally.\nThanks!. @ice-92 try removing docker-py, per what @shin- told me to do up above.  I just confirmed that having only the latest docker (without the -py) Python package fixed my issue, hope it helps you too!. @ice-92  I got you. In fact, I just looked and confirmed that, at first, I got the same error after I removed it, too. \nI thought it was just me...but I fixed it with pip uninstall docker, then running pip install --no-cache-dir docker again. After that, I ran OK.\nApologies, I assumed it was just a fluke on my system, but if that works for you it might just be how it has to be done in general.. ",
    "make-it-git": "Same issue for me.\nI had to downgrade python-docker to version 3.1.4.1 and it works.. ",
    "kant2002": "/cc: @shin- It is working. Thanks. And change is much cleaner then mine \ud83d\udc4d . ",
    "shalak": "@shin- Hi! I also lost some time not knowing that I need to call the reload() to get the current status. However, once the container is stopped (or failed to start the deamon, i.e. it's exited already) - the reload() raises docker.errors.NotFound: 404 Client Error: Not Found (\"No such container: bfd99c790547f27c766c66a29a35c8ef65f431a6baa24004a4e2d0df2b5800d6\") - is there a \"nice\" way to check the status of possibly already stopped container?. ",
    "KOPACb": "Yawww! i missed it out, thank you!. ",
    "avirshup": "Maybe I'm misunderstanding, but I'd consider the interface of the return type as a fundamental part of the API contract, not an implementation detail. So, it sounds like Iterable[bytes] is what we should expect for the stream object - would there be any objection to explicitly stating this in the documentation?. Great, will do. ",
    "NiklasRosenstein": "I found a workaround. Maybe this or similar functionality could be added to the ExitResult returned by Container.exec_run()?\n```python\ntools.py\nimport sys\ndef container_exec(container, cmd, stdout=True, stderr=True, stdin=False,\n                   tty=False, privileged=False, user='', detach=False,\n                   stream=False, socket=False, environment=None, workdir=None):\n  \"\"\"\n  An enhanced version of #docker.Container.exec_run() which returns an object\n  that can be properly inspected for the status of the executed commands.\n  \"\"\"\nexec_id = container.client.api.exec_create(\n    container.id, cmd, stdout=stdout, stderr=stderr, stdin=stdin, tty=tty,\n    privileged=privileged, user=user, environment=environment,\n    workdir=workdir)['Id']\noutput = container.client.api.exec_start(\n    exec_id, detach=detach, tty=tty, stream=stream, socket=socket)\nreturn ContainerExec(container.client, exec_id, output)\nclass ContainerExec(object):\ndef init(self, client, id, output):\n    self.client = client\n    self.id = id\n    self.output = output\ndef inspect(self):\n    return self.client.api.exec_inspect(self.id)\ndef poll(self):\n    return self.inspect()['ExitCode']\ndef communicate(self, line_prefix=b''):\n    for data in self.output:\n      if not data: continue\n      offset = 0\n      while offset < len(data):\n        sys.stdout.buffer.write(line_prefix)\n        nl = data.find(b'\\n', offset)\n        if nl >= 0:\n          slice = data[offset:nl+1]\n          offset = nl+1\n        else:\n          slice = data[offset:]\n          offset += len(slice)\n        sys.stdout.buffer.write(slice)\n      sys.stdout.flush()\n    while self.poll() is None:\n      raise RuntimeError('Hm could that really happen?')\n    return self.poll()\n```\nUsage example\npython\n  result = tools.container_exec(container, cmd, stream=True, **kwargs)\n  res = result.communicate(line_prefix=b'--> ')\n  if res != 0:\n    error('exit code {!r}'.format(res)). ",
    "TTimo": "Looks like you have to use the low level API then. No way to get at the id using exec_run.. ",
    "FRidh": "Note this in effect breaks moto and thereby the testing of pandas.. No, it was not.\nhttps://github.com/pypa/pip/issues/5154#issuecomment-378197307\nAlthough by not prefixing it with an underscore it never was that obvious. Until now then.. Probably pkg_resources.working_set is best used here.. Though not mentioned in PR or commit, a fix was merged as part of https://github.com/docker/docker-py/pull/1990.\n@shin- can we get a 3.2.2?. ",
    "adamtheturtle": "https://github.com/docker/docker-py/commit/1d6f8ecf9277ef1c77f3d530efaafd39323cc8e7#diff-2eeaed663bd0d25b7e608891384b7298 fixes this issue but is not yet released.. This was silly, I did the following and it worked:\nintersphinx_mapping = {\n    'docker': ('http://docker-py.readthedocs.io/en/stable', None),\n}. ",
    "niebelung": "Thanks! Looks like it works on both environments.. ",
    "dmccrevan": "@shin- I tried client.ping() but go the same error, is there a feature in this SDK that checks this or should I try to use some other way?\n. @confiq @shin- \nI ended up just using some of the os package's built in features to figure this out.. ",
    "confiq": "What's wrong with try: except DockerException?. ",
    "zhangyanwei": "I met same error, is there a workaround ?. ",
    "chenlei1998": "@zhangyanwei \nrewrite read function in utils\\socket.py \n```\ndef read(socket, n=4096):\n    \"\"\"\n    Reads at most n bytes from socket\n    \"\"\"\nrecoverable_errors = (errno.EINTR, errno.EDEADLK, errno.EWOULDBLOCK)\n\nif six.PY3 and not isinstance(socket, NpipeSocket):\n    select.select([socket], [], [])\n\ntry:\n    if hasattr(socket, 'recv'):\n        return socket.recv(n)\n    # fix OSError\n    try:\n        return os.read(socket.fileno(), n)\n    except OSError:\n        return socket.read(n)\nexcept EnvironmentError as e:\n    if e.errno not in recoverable_errors:\n        raise\n\n```. ",
    "amancevice": "This might be a longer-term solution (in case pip changes again):\n```python\nimport pkg_resources\ntry:\n    pkg_resources.get_distribution('docker-py')\n    print(\n        'ERROR: \"docker-py\" needs to be uninstalled before installing this'\n        ' package:\\npip uninstall docker-py', file=sys.stderr\n    )\n    sys.exit(1)\nexcept pkg_resources.DistributionNotFound:\n    pass\n```. ",
    "attex": "i think you do not have to put these strings in quotes. Just write command=some_command\nhow does your Dockerfile look like?\n. ",
    "beldhia": "thanks for reply @attex, I modified the code something like this\n```python\ndef start_reciever(ip,im,port,volume,proto,secured=False):\n    file_name = 'proto' + '-' + str(port) + '.txt'\n    recv_url = 'tcp://'  + ip + ':2375'\n    p1 = str(port)\n    p = {}\n    p[port] = port\n    vol =['/home/tli:/opt/tm_recievers']\n#connect to the Docker host\nc =docker.DockerClient(base_url=recv_url)\n\ncmd = '\\'--receiver {0} --port {1} -vvv --logfile /opt/tm_recievers/{2}\\''.format(proto,port,file_name)\n\nprint('starting the container')\nprint(im)\nprint(p)\nprint(cmd)\nprint(vol)\noutput=(c.containers.run(image=im,ports=p,tty=True,command=cmd,volumes=vol,detach=True))\nprint(output)\n\ncalling the function with variables\nstart_reciever('1.1.1.1','image:latest',5004,'/home/test','http')\n```\nI am not getting any error but i don't see the container running on the host . It returns me \n. thanks @attex , my issue is resolved by directory passing the variables now.\n I have another requirement where in i need to pass the argument '-ports' as {portnumber:portnumer/udp} , but when  i am trying to pass it like this {50004:'50004/udp'} it is getting errored, as -ports expects dictionary.\nAny suggestion how to get this working.. ",
    "eljrax": "Gotcha, thanks for confirming! . ",
    "AStotal": "Container's entrypoint is iperf, so I cant run container and then connect the network.\nIs there way to run container with IP at once instead of create and connect network to it?. I found solution with a few inter steps\nsrv = doc.containers.create('astotal/iperf3', '-s', detach=True, name='iperf_server')\ndoc.networks.get(\"uplink\").connect(srv, ipv4_address=\"10.195.10.11\")\nsrv.start()\nThanks a lot!. @fkromer Yes you do. E.g:\n```\nimport docker\ndoc = docker.from_env()\nipam_pool = docker.types.IPAMPool(\n    subnet='10.2.0.0/24',\n    iprange='10.2.0.0/24',\n    gateway='10.2.0.2',\n    aux_addresses={\n        'host_system': '10.2.0.1',\n    }\n)\nipam_config = docker.types.IPAMConfig(\n        pool_configs=[ipam_pool])\nuplink\ndoc.networks.create('uplink',\n                    driver='macvlan',\n                    options={'parent': 'eth2.3200'},\n                    ipam=ipam_config,\n                    )\n```. ",
    "greenpau": "It looks like the documentation is out-of-date. The class is docker.Client, not docker.DockerClient:\ncat /usr/lib/python2.7/site-packages/docker/client.py | grep class\nclass Client(. @shin- , I am using Python 2. The docker library erred with ImportError: No module named docker. Similar to the issue https://github.com/docker/docker-py/issues/844\n. > You probably have some path conflicts.\n@shin- , I had to manually delete /usr/lib/python2.7/site-packages/docker and install again.. ",
    "alex-codefresh": "Thanks for the response\nThat makes sense. However, I have narrowed it down to a few lines of code and if the issue is not with the API call, then I don't understand why this throws the error:\n```\nimport win32pipe\ndef connect(address):\n        win32pipe.WaitNamedPipe(address, 10000)\nconnect('\\\\.\\pipe\\docker_engine')\n```\nIf I specify any other named pipe, it works, but with with the mounted one it always throws this error. I double checked that the address of npipe is correct. Could you give me some insights on this?\nI remind you the way I run the container is:\ndocker run --rm  -v //./pipe/docker_engine://./pipe/docker_engine microsoft/windowsservercore:1709\n. ",
    "atomictrout": "I'm seeing the same results with microsoft/windowsservercore:1803. The docker cli works fine (docker info) from within the windows container when the named pipe is mounted like this:\n\ndocker run -v //./pipe/docker_engine://./pipe/docker_engine ...\n\nHowever, trying to access the named pipe in the container from python appears to be an issue.\n  File \"C:\\Python\\lib\\site-packages\\docker\\transport\\npipesocket.py\", line 50, in connect\n    win32pipe.WaitNamedPipe(address, self._timeout)\npywintypes.error: (2, 'WaitNamedPipe', 'The system cannot find the file specified.'). . ",
    "andyliu08": "@shin- Thanks for your comments, I have raised an issue on pipenv. https://github.com/pypa/pipenv/issues/2355. @etienne-napoleone Sorry, still can not work.\n. ",
    "etienne-napoleone": "@andyliu08 Have you got it working? I have the same problem here. Why not import your json object as a dictionary and simply expand it?\n```python\nimport json\nwith open('container.json') as f:\n    container = json.load(f)\ncontainer = {\n'image': 'alpine:latest',\n'command': 'echo hello world'\n}\nclient.containers.run(**container)\n```. ",
    "douglasmiranda": "Master branch is working.. ",
    "remorses": "I get the same error for client.configs.create\n. ",
    "ikit": "Yes, it's what I did: waiting in another thread, It works like a charm :+1: .. ",
    "dzervas": "I think so - but test it again\nI installed it through pacman on Arch linux. ",
    "Vacant0mens": "@shin- It seems that pywin32==220 is not available with Python3.7. There is pypiwin32==219, pypiwin32==223, and pywin32==223. Is there a way to get around this without using a virtualenv?\nPS ~\\> python.exe --version\nPython 3.7.0\nPS ~\\> python.exe -m pip install pypiwin32==220\nCollecting pypiwin32==220\nCould not find a version that satisfies the requirement pypiwin32==220 (from versions: 219, 223)\nNo matching distribution found for pypiwin32==220\nPS ~\\> python.exe -m pip install pywin32==220\nCollecting pywin32==220\nCould not find a version that satisfies the requirement pywin32==220 (from versions: 223)\nNo matching distribution found for pywin32==220. Looks like pywin32==224 is available for 3.7 as of a couple weeks ago. Is this version compatible with docker-py?. ",
    "guillaume0811": "Ok I understand.\nThank you. ",
    "kavefish": "I consulted the documentation and tried exactly what you suggested prior to submitting the bug report because it results in exactly the same error. . The same error occurs if the colon appears in the bind mount path for the container, e.g. \npython\nimport docker\nclient = docker.from_env()\nvols={'/somedir': {'bind': '/test:path'}}\nclient.containers.create(\"ubuntu:latest\", volumes=vols). @thaJeztah, thanks very much for the suggestion. \nYes, I do want a bind mount. I thought I was specifying a bind mount by using the dictionary notation which specifies \"bind\" in the dictionary. \nIs the recommendation that the low-level API is required to accomplish a bind mount with a colon in the path?   . ",
    "LanDeQuHuXi": "It is actually in the container object as well, which can be found at container.attrs[\"NetworkSettings\"][\"IPAddress\"]\nAnd I actually found the ip of the container at container.attrs[\"NetworkSettings\"][\"Networks\"][\"{project_name}_default\"][\"IPAddress\"], but I'm using docker-compose, which may cause the difference. ",
    "ecemlago": "Hi,\nWe have the same issue running the latest version of docker in Windows 10.\n$ docker version\nClient: Docker Engine - Community\n Version:           18.09.0\n API version:       1.39\n Go version:        go1.10.4\n Git commit:        4d60db4\n Built:             Wed Nov  7 00:47:51 2018\n OS/Arch:           windows/amd64\n Experimental:      false\nServer: Docker Engine - Community\n Engine:\n  Version:          18.09.0\n  API version:      1.39 (minimum version 1.12)\n  Go version:       go1.10.4\n  Git commit:       4d60db4\n  Built:            Wed Nov  7 00:55:00 2018\n  OS/Arch:          linux/amd64\n  Experimental:     false\n. ",
    "Tioborto": "Hello, \nThe actual output : https://gist.github.com/Tioborto/28b8f511065cd50cc1965eff64ca77a9\nExpected output : All stack of Ansible run\nEnvironment : \n- Redhat 7\n- Install docker like it's explain on docker official tutorial\nYeah, I totally agree with you. Here is the new code : \n```python\n        # Creation of container\n        try:\n            conteneur = self.client.create_container(ansible_api_config.ansible_docker_image, command=ansible_command_line, working_dir=\"/opt/dab_projects\", volumes=['/root/.ssh', '/opt/projects', '/usr/lib/python2.7', '/etc/ansible', '/root/.ara'], host_config=self.client.create_host_config(binds={ '/root/.ssh': {'bind': '/root/.ssh', 'mode': 'rw'}, '/opt/projects': {'bind': '/opt/projects', 'mode': 'rw'}, '/usr/lib/python2.7': { 'bind': '/usr/lib/python2.7', 'mode': 'ro'}, '/etc/ansible': {'bind':'/etc/ansible', 'mode': 'ro'}, '/root/.ara':{'bind':'/root/.ara', 'mode': 'rw'} }))\n        except docker.errors.APIError as api_error:\n            app.logger.info(api_error)\n            raise\n    # Container start\n    try:\n        self.client.start(conteneur.get('Id'))\n    except docker.errors.APIError as api_error:\n        app.logger.info(api_error)\n        raise\n\n    # Wait container to get output\n    try:\n        status_code = self.client.wait(conteneur.get('Id'))\n        app.logger.info(\"Status code : %s\" % status_code)\n    except docker.errors.APIError as api_error:\n        app.logger.info(api_error)\n        raise\n    except requests.exceptions.ReadTimeout as read_timeout:\n        app.logger.info(read_timeout)\n        raise\n\n    # Get logs\n    try:\n        output = self.client.logs(conteneur.get('Id'))\n        app.logger.info(\"Output : %s\" % output)\n    except docker.errors.APIError as api_error:\n        app.logger.info(api_error)\n        raise\n\n    # Remove container\n    try:\n        self.client.remove_container(container=conteneur.get('Id'), v=True)\n    except docker.errors.APIError as api_error:\n        app.logger.info(api_error)\n        raise\n\nIt is in the log recovery output that I display.. Why is it connected to docker-py? \nSince if I run my Ansible playbook inside a container without using docker-py, I get the full run-time log.\nOf course I'm not saying it's a mistake but would you have any idea to get the log not truncated?. I'm just trying to run a playbook inside a container and retrieve its log.. Up. Anybody else ? @shin-   ?. No, I haven't tried. Using the \"attach\" method? . @shin-  : I've tried this code but i don't get all logs.\nDocker import\nimport docker\nclient = docker.APIClient()\nansible_command_line = \"ansible-playbook /opt/projects/project/master/deploy.yml -i /opt/projects/project/inventories/project_1527859174.54.tmp\"\ntry:\n    conteneur = client.create_container(\"ansible-centos7\", command=ansible_command_line, working_dir=\"/opt/projects\", volumes=['/root/.ssh', '/opt/projects', '/usr/lib/python2.7', '/etc/ansible', '/root/.ara'], host_config=client.create_host_config(binds={ '/root/.ssh': {'bind': '/root/.ssh', 'mode': 'rw'}, '/opt/projects': {'bind': '/opt/projects', 'mode': 'rw'}, '/usr/lib/python2.7': { 'bind': '/usr/lib/python2.7', 'mode': 'ro'}, '/etc/ansible': {'bind':'/etc/ansible', 'mode': 'ro'}, '/root/.ara':{'bind':'/root/.ara', 'mode': 'rw'} }))\n    print(conteneur)\nexcept docker.errors.APIError as api_error:\n    print(api_error)\n    raise\ntry:\n    result = client.start(conteneur.get('Id'))\n    print(result)\nexcept docker.errors.APIError as api_error:\n    print(api_error)\n    raise\ntry:\n    output = client.logs(conteneur.get('Id'), stream=True)\n    for i in output:\n        print(i)\nexcept docker.errors.APIError as api_error:\n    app.logger.info(api_error)\n    raise\n```\nWhen i run the following command, all works fine.\ndocker run -it -v /root/.ssh:/root/.ssh -v /opt/projects:/opt/projects -v /etc/ansible:/etc/ansible ansible-centos7 ansible-playbook /opt/projects/project/master/deploy.yml -i /opt/projects/project/inventories/project_1527859174.54.tmp. For your information, it was just the docker driver. \nI set it on json-file with 5M as length and it's work fine. \nI can close this issue. \nThanks for all. . ",
    "chris3081": "Thanks @thaJeztah, so based on that I'm guessing I need to write a handler in my code to determine that or is this something that should be handled in the exec_run code?. Good point @thaJeztah I've done some quick testing... I only see the random artifact when streaming is on and tty is off. \nSettings:\nstream=False\nscoket=False\ntty=False\nOutput:  No artifact, output displays correctly with no formatting/colours\nSettings:\nstream=False\nscoket=False\ntty=True\nOutput: No artifact, output displays correctly with ansi formatting/colours\nSettings:\nstream=True\nscoket=True\ntty=False\nOutput: Artifact is present,  output shows Artifact on first line with directory, the rest of the output is correct with no formatting/colours\nSettings:\nstream=True\nscoket=True\ntty=True\nOutput: No Artifact, output displays correctly with ansi formatting/colours. Is there a way I can stream the output but have the decoding done for me?. your right @shin-  my bad, I appear to have misunderstood the documentation, I made the assumption that it was required due to it crashing out in windows without it being enabled. I've since found that that is due to issue #1997 . ",
    "1aimbot": "usage error, input parameter should be data, not filename.\nhttp://docker-py.readthedocs.io/en/stable/images.html\nOpen file and pass in file handle. ",
    "ramkrishnan8994": "@sureshsankaran Did you find a solution?\nI have tried two things:\nclient.images.load(tarurl)\nand\nclient.images.build(fileobj = tarurl,custom_context = True)\nBoth of them do notwork. Following are the errors:\ndocker.errors.ImageLoadError: Error processing tar file(exit status 1): unexpected EOF\nand\ndocker.errors.APIError: 500 Server Error: Internal Server Error (\"unexpected EOF\")\n. ",
    "houziyu": "Hi, thanks for your advice. I just tried to test the method you mentioned in version v2.7.0 and found that the result was not satisfactory. I added print breakpoint before log_str to help me determine whether it was stuck in log_str. That's true. The test results are as follows:\nlog_str = b''.join(chunk for chunk in log_init[0]).decode('utf-8')\nlog_size=11.1MB   and time_consuming=2.47min  and CPU single core full load\nlog_str = str(log_init[0].data, encoding=\"utf-8\")\nlog_size=11.4MB   and time_consuming=Less than 1 SEC  and CPU utilization is less than 10%\nSo, I don't know what to do now, can you give me some more advice? Thank you very much!!. ",
    "rkonkul": "I worked around this by using forward slashes in certain places. It is a pain to know when to which. I guess that is standard for docker on windows.. ",
    "mrakitin": "Thanks for the clarification, @shin-! I tried the suggested option, and it works great:\npy\nIn [7]: print(client.containers.run(\"mrakitin/debian-with-miniconda\", command=\"sh -c 'echo \\\"Hello World!\\\"; echo \\\"Another echo!\\\"'\", remove=True, stdout=True, st\n   ...: derr=True).decode())\nHello World!\nAnother echo!\nClosing as resolved \ud83d\ude42 . ",
    "xiaods": "@shin- hi , i am confuse the package name: the Pypi have two version:\nhttps://pypi.org/project/docker-py/\nhttps://pypi.org/project/docker/\nbecause my ansible dependency is docker-py, so i have install it, but it not working. i am currently dig the root cause.. @shin got it.  thanks for your commets. ",
    "lzfxxx": "Find solution https://github.com/TomasTomecek/sen/blob/master/sen/util.py#L158. ",
    "WellingtonLucas": "Thank's, 1.24 was my old API version. I checked my code and my SDK version is 2.1.0.. My API version is 1.26.\nMy problem is, the container's status is changed to exited, without instruction, and I expected a status \"paused\".  . ",
    "bcnewlin": "I neglected to include that I am using docker-machine to run these tests on MacOS 10.13.5 with Virtualbox Version 5.2.12 r122591 (Qt5.6.3). I have upgraded my boot2docker image to the latest but the errors remain.\nSince reporting this issue I have installed Docker for Mac and verified that the errors do not occur. However, I am seeing other erroneous behavior with Docker for Mac that makes it an unsuitable replacement for docker-machine at this time.. The error does not occur when using Docker for Mac, so I don't know how I could report it there. I considered that it may be a Docker Machine issue, but I decided to open it here as I believed that docker-py should be properly handling EINPROGRESS responses on attempts to open sockets to the Docker daemon.. ",
    "stefanseefeld": "Here is a complete traceback. I'm getting this reliably, on different machines.\nInterestingly enough, the actual docker commit operation seems to have succeeded.\n```\nTraceback (most recent call last):\n  File \"/home/administrator/.local/lib/python3.5/site-packages/urllib3/connectionpool.py\", line 384, in _make_request\n    six.raise_from(e, None)\n  File \"\", line 2, in raise_from\n  File \"/home/administrator/.local/lib/python3.5/site-packages/urllib3/connectionpool.py\", line 380, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/usr/lib/python3.5/http/client.py\", line 1197, in getresponse\n    response.begin()\n  File \"/usr/lib/python3.5/http/client.py\", line 297, in begin\n    version, status, reason = self._read_status()\n  File \"/usr/lib/python3.5/http/client.py\", line 258, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/usr/lib/python3.5/socket.py\", line 575, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/home/administrator/.local/lib/python3.5/site-packages/requests/adapters.py\", line 445, in send\n    timeout=timeout\n  File \"/home/administrator/.local/lib/python3.5/site-packages/urllib3/connectionpool.py\", line 638, in urlopen\n    _stacktrace=sys.exc_info()[2])\n  File \"/home/administrator/.local/lib/python3.5/site-packages/urllib3/util/retry.py\", line 367, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"/home/administrator/.local/lib/python3.5/site-packages/urllib3/packages/six.py\", line 686, in reraise\n    raise value\n  File \"/home/administrator/.local/lib/python3.5/site-packages/urllib3/connectionpool.py\", line 600, in urlopen\n    chunked=chunked)\n  File \"/home/administrator/.local/lib/python3.5/site-packages/urllib3/connectionpool.py\", line 386, in _make_request\n    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n  File \"/home/administrator/.local/lib/python3.5/site-packages/urllib3/connectionpool.py\", line 306, in _raise_timeout\n    raise ReadTimeoutError(self, url, \"Read timed out. (read timeout=%s)\" % timeout_value)\nurllib3.exceptions.ReadTimeoutError: UnixHTTPConnectionPool(host='localhost', port=None): Read timed out. (read timeout=60)\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"./ci.py\", line 288, in \n    sys.exit(0 if main(sys.argv) else 1)\n  File \"./ci.py\", line 281, in main\n    c.commit('ci')\n  File \"./ci.py\", line 188, in commit\n    self._container.commit(repository=repository, tag=tag)\n  File \"/home/administrator/.local/lib/python3.5/site-packages/docker/models/containers.py\", line 124, in commit\n    kwargs)\n  File \"/home/administrator/.local/lib/python3.5/site-packages/docker/utils/decorators.py\", line 19, in wrapped\n    return f(self, resource_id, *args, kwargs)\n  File \"/home/administrator/.local/lib/python3.5/site-packages/docker/api/container.py\", line 143, in commit\n    self._post_json(u, data=conf, params=params), json=True\n  File \"/home/administrator/.local/lib/python3.5/site-packages/docker/api/client.py\", line 257, in _post_json\n    return self._post(url, data=json.dumps(data2), kwargs)\n  File \"/home/administrator/.local/lib/python3.5/site-packages/docker/utils/decorators.py\", line 46, in inner\n    return f(self, *args, kwargs)\n  File \"/home/administrator/.local/lib/python3.5/site-packages/docker/api/client.py\", line 194, in _post\n    return self.post(url, self._set_request_timeout(kwargs))\n  File \"/home/administrator/.local/lib/python3.5/site-packages/requests/sessions.py\", line 559, in post\n    return self.request('POST', url, data=data, json=json, kwargs)\n  File \"/home/administrator/.local/lib/python3.5/site-packages/requests/sessions.py\", line 512, in request\n    resp = self.send(prep, send_kwargs)\n  File \"/home/administrator/.local/lib/python3.5/site-packages/requests/sessions.py\", line 622, in send\n    r = adapter.send(request, kwargs)\n  File \"/home/administrator/.local/lib/python3.5/site-packages/requests/adapters.py\", line 526, in send\n    raise ReadTimeout(e, request=request)\nrequests.exceptions.ReadTimeout: UnixHTTPConnectionPool(host='localhost', port=None): Read timed out. (read timeout=60)\n```. I will give it a try. What is the default value ?. ",
    "felixsiju": "Any Suggestions?. @TomasTomecek  Tried it and that worked \nThanks\nclosed the issue. Find some old reference for this issue. Closing  it\nThanks\nshin- commented on 25 Oct 2017\nYes, just f.write the data you receive into a new file.. Closing the issue. After enabling \"experimental\": true it worked\n{\n    \"experimental\": true\n}\n. ",
    "luk": "Hi, \nThanks for fixing!\n. ",
    "devnore": "\nShould still work the same as it used to:\n```python\nimport docker\nc = docker.APIClient()\nstream = c.build('.', dockerfile='Dockerfile.run', rm=True)\nfor chunk in stream:\n    print(chunk)\n```\n\nThis will do the building and when done display the log. Is there a way to display the output as it happens?\n. ",
    "ofek": "Can we try to incorporate this? https://github.com/docker/docker-py/pull/2047. Nice! Though I meant could we try to make use of their new official packages?\nofficial: https://pypi.org/project/pywin32/#files\ndeprecated: https://pypi.org/project/pypiwin32/#files. ",
    "pishchalnikov": "@TomasTomecek thank you!. ",
    "mojovski": "same here. Expecting to receive output generator and the logs in realtime. getting socket.timeout: timed out instead.\ncode:\nexict_code, output=container.exec_run(cmd, stream=True)\nfor line in output:\n    print(line, end='')\n\n. ",
    "douglasjreynolds": "I am experiencing the same issue...\nFile \"/usr/local/lib/python2.7/dist-packages/docker/models/images.py\", line 86, in save\n    return self.client.api.get_image(self.id, chunk_size)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/utils/decorators.py\", line 19, in wrapped\n    return f(self, resource_id, *args, kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/api/image.py\", line 40, in get_image\n    res = self._get(self._url(\"/images/{0}/get\", image), stream=True)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/utils/decorators.py\", line 46, in inner\n    return f(self, *args, kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/api/client.py\", line 198, in _get\n    return self.get(url, self._set_request_timeout(kwargs))\n  File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 480, in get\n    return self.request('GET', url, kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, send_kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/requests/adapters.py\", line 449, in send\n    raise ReadTimeout(e, request=request)\nrequests.exceptions.ReadTimeout: UnixHTTPConnectionPool(host='localhost', port=None): Read timed out. (read timeout=60). I don't know if this should be a docker-py issue or a docker issue.  The reason for the timeout is that docker is exporting the tar file to disk before streaming the result back to the API.  This is obvious when you look at /var/lib/docker/tmp/docker-export-XXXXX when exporting a large image.  \nIn theory, docker should be able to stream the layers through the web server so that the client can just read the stream while the container is being exported.\n. ",
    "robertomlsoares": "Thanks for the investigation, @douglasjreynolds!\nThat's too bad, I was trying to avoid creating temporary files by streaming everything.... ",
    "ScottBrenner": "Nice, thank you!. ",
    "gaborbernat": "Noe though that library has not been updated to dockerpy 3.0, so it's not really an out of box great solution.. ",
    "EvanBaum1": "This was attempted, with no change. . This is being run on a secure MapR 6.0.1 cluster. The run file is as follows:\ndocker run --privileged -it -v /tmp/ticket:/tmp/mapr_ticket:ro -v /var/run/docker.sock:/var/run/docker.sock:ro --cap-add SYS_ADMIN --cap-add SYS_RESOURCE --device /dev/fuse --rm --env-file env_file.env --network data --name service_service service\nThe code is running on:\nPython 3.6.4 (default, May 11 2018, 14:19:50)\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nthe Dockerfile is as follows:\n`\nFROM pacc_kafka\nRUN mkdir /app || true\nWORKDIR /app\nCOPY requirements.txt /app/requirements.txt\nCOPY requirements_sog.txt /app/requirements_sog.txt\nRUN pip3 install -r requirements.txt && pip3 install -r requirements_sog.txt && rm /app/requirements.txt && rm /app/requirements_sog.txt\nCOPY main.py /app/main.py\nCOPY Service.py /app/Service.py\nCOPY config.yml /app/config.yml\nCOPY consumerConfig.txt /app/consumerConfig.txt\nCOPY consumerService/ /app/consumerService/\nCMD cd /app && export LD_LIBRARY_PATH=/opt/mapr/lib && ln -s /opt/mapr/lib/libMapRClient.so /opt/mapr/lib/libMapRClient_c.so | true && LD_PRELOAD=/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so python3 /app/main.py #command line inputs here if required\n`\nI am able to run the line that says\nself.client = docker.from_env()\nHowever it errors when I try to do anything to the self.client. It will also error out when \ndocker.from_env().info()\nIs called.\nThe error I posted is the result of this call.. This is the permissions, where the 'docker' user contains the user 'mapr' which is where I'm running from.\nsrw-rw---- 1 root docker 0 Aug  2 15:01 /var/run/docker.sock\nI'm not sure which you mean by id in my container. \nEdit: I see. Here is the id\nuid=5000(mapr) gid=5000(mapr) groups=5000(mapr),0(root),993(docker),5002(shadow). Figured out the issue. The MapR PACC wasn't able to access things that weren't owned by the user, regardless of permissions.. ",
    "MahdiZareie": "Thank you @shin- works great . ",
    "majkrzak": "In my opinion perfect solution will be to pass context as context and Dockerfile as dockerfile then it might be distinguished if context is an directory path, URL, or tar fileobject same with dockerfile but little harder.\nclient.images.build(context='.', dockerfile='Dockerfile.local')\nclient.images.build(context='http://remote.uri', dockerfile=''' FROM ubuntu\nfacny docker file\n''')\nclient.images.build(context=open('adas'), dockerfile=open('dasdasd')). OK, here is even better fix. It handles multiline `dockerfile` as content of Dockerfile. It should also let us embed Dockerfile in docker-compose.yml\n\ndiff --git a/docker/api/build.py b/docker/api/build.py\nindex 0486dce..41649b7 100644\n--- a/docker/api/build.py\n+++ b/docker/api/build.py\n@@ -336,6 +336,12 @@ def process_dockerfile(dockerfile, path):\n    if not dockerfile:\n        return (None, None)\n\n+    if '\\n' in dockerfile:\n+        return (\n+            '.dockerfile.{0:x}'.format(random.getrandbits(160)),\n+            dockerfile\n+        )\n+\n    abs_dockerfile = dockerfile\n    if not os.path.isabs(dockerfile):\n        abs_dockerfile = os.path.join(path, dockerfile)\n\n. If dockerfile it's not part of context for some reason, it have to be affected to be dockerized. I know that it can be passed as absolute path, but if dockerfile is ephemeral it requires saving is somewhere is filesystem which is little more complicated. Generally it py lib lacs functionality of cli, and arguments could be simplified a bit.. ",
    "vincenthcui": "Thanks for quick response. And I think I don't express my meanings clearly :(\nWhat I want is to get meanings from every line of those response, instead of how to format json data.\nFor examples, how many kinds of status need to be process correctly, or how many fields in those lines.\nThank you!. ",
    "astrocaribe": "@vincenthcui This may be too late for you, but here is an example of how i use the pull statuses:\n```python\ndef extract_progress_info(line):\n    \"\"\"\n    Extract progress information from a Docker Pull progress object\n    :param line: Docker pull progress object (Json)\n    :return: Transoformed progress object (Json)\n    \"\"\"\n    # Extract pieces of interest\n    pid = line['id'] if 'id' in line else \"\"\n    status = line['status'] if 'status' in line else \"\"\n    progress_detail = line['progressDetail'] if 'progressDetail' in line else {}\n    complete_statuses = ['Download complete', 'Pull complete', 'Already exists', 'Layer already exists', 'Pushed']\npercent = 0\nif status in complete_statuses:\n    percent = 100\nelif progress_detail != {}:\n    current = line['progressDetail']['current']\n    total = line['progressDetail']['total']\n    percent = round((current / total) * 100)\n\n# Create progress object\nprogress = {'id': pid, 'status': status, 'percent': percent}\n\nreturn progress\n\n```\nThe input to this function is decoded as @shin- alluded above.. ",
    "ividal": "Thanks for the info. Actually, I think by the time we get to the run/start part, we already have a problem. The \"pull\" happens too fast, which makes me think it's retrieving an error page from our registry and not really pulling a docker image (bear in mind we use proxies - just to make things interesting).\nMy question for you: is there any chance that the docker python API tries to retrieve images via a different port than \"docker pull\" from the command line?\nThanks again!. ",
    "OlivierAlbertini": "I get the same error if I remove docker.io. ",
    "knorx": "Hey, of course - why should it be reproducible on your side ... that would make it easy to solve. \ud83d\ude06 See output on my side below. So what is the difference here? The funny thing is, if I change the command run on the container from cat to e.g. a hash with sha224sum, the error does not appear on my side as well.\nI had the feeling that it has sth. to do with the binary symbols of the archive being printed. The reason for this assumption is that another clue was: When I do cat on a small binary like e.g. /bin/hostname, I also did not see the error. It drives me crazy. I once managed to debug into the containers.py on line 789 where the output is returned. The response object there had the empty byte string in it, while docker logs on command line returned the correct output like here:\nR:\\tmp\\docktest>docker logs fervent_keldysh\nPK\u0004\u0014     3~\u000e\ufffd<g\u000e   \u000e   \f   network_fileNetworkContentPK\u0001\u0014 \u0014     3~\u000e\ufffd<g\u000e   \u000e   \f           \ufffd\u0001    network_filePK\u0005\u0006    \u0001\nError-Test on my side:\n```\nR:\\tmp\\docktest>docker build -t docker-py .\nSending build context to Docker daemon  4.608kB\nStep 1/2 : FROM ubuntu:16.04\n ---> 7aa3602ab41e\nStep 2/2 : COPY archive.zip /\n ---> Using cache\n ---> a5fc1fb1b8a3\nSuccessfully built a5fc1fb1b8a3\nSuccessfully tagged docker-py:latest\nSECURITY WARNING: You are building a Docker image from Windows against a non-Windows Docker host. All files and directories added to build context will have '-rwxr-xr-x' permissions. It is recommended to double check and reset permissions for sensitive files and directories.\nR:\\tmp\\docktest>python errorTest.py\nF\n======================================================================\nFAIL: testLogs (main.LogReturnTest)\n\nTraceback (most recent call last):\n  File \"errorTest.py\", line 14, in testLogs\n    self.assertNotEqual(b'', output, msg=\"iteration #{}\".format(i))\nAssertionError: b'' == b'' : iteration #1\n\nRan 1 test in 1.100s\nFAILED (failures=1)\nR:\\tmp\\docktest>python errorTest.py\nF\n======================================================================\nFAIL: testLogs (main.LogReturnTest)\n\nTraceback (most recent call last):\n  File \"errorTest.py\", line 14, in testLogs\n    self.assertNotEqual(b'', output, msg=\"iteration #{}\".format(i))\nAssertionError: b'' == b'' : iteration #2\n\nRan 1 test in 2.559s\nFAILED (failures=1)\nR:\\tmp\\docktest>python errorTest.py\nF\n======================================================================\nFAIL: testLogs (main.LogReturnTest)\n\nTraceback (most recent call last):\n  File \"errorTest.py\", line 14, in testLogs\n    self.assertNotEqual(b'', output, msg=\"iteration #{}\".format(i))\nAssertionError: b'' == b'' : iteration #4\n\nRan 1 test in 5.614s\nFAILED (failures=1)\n```. ",
    "kolyshkin": "\ntests\\integration\\api_container_test.py:876:80: E501 line too long (87 > 79 characters)\n\nfixed. patch updated based on review comments. OK the suggestion to gather container logs after container stop won't work in case auto_remove=True, so this has to be reverted. Perhaps we just need to do out.Close() once the container is stopped?. I gave up on the idea of changing the behavior for ContainerLogs(follow=true), as it is not backward-compatible; thus closing this PR.. I guess it's easier to call container.logs after container is stopped. What about this:\n```diff\n--- a/docker/models/containers.py\n+++ b/docker/models/containers.py\n@@ -777,16 +777,15 @@ class ContainerCollection(Collection):\n         logging_driver = container.attrs['HostConfig']['LogConfig']['Type']\n     out = None\n\n\nif logging_driver == 'json-file' or logging_driver == 'journald':\nout = container.logs(\nstdout=stdout, stderr=stderr, stream=True, follow=False\n\n)\n exit_status = container.wait()['StatusCode']\n if exit_status != 0:\n\n\nout = None\n             if not kwargs.get('auto_remove'):\n                 out = container.logs(stdout=False, stderr=True)\nelse:\nif logging_driver == 'json-file' or logging_driver == 'journald':\nout = container.logs(\nstdout=stdout, stderr=stderr, stream=True, follow=False\n) if remove:\n     container.remove()\n\n``. One thing I am not sure is how it works (and worked before) in caseremove=True.. Yeah,container.logs()after container stop won't work in case ofauto_remove=True`.\n\n\nWe can use container.logs with follow=True in a separate thread and kill it once the container is stopped. The problem is I am not quite sure how to implement it in Python :(. Or maybe we just need to do out.Close() in proper places.... ",
    "adw1n": "The docstring for chunk_size parameter is wrong:\n\nThe number of bytes returned by each iteration of the generator.`\n\nIt should just say the same thing as https://urllib3.readthedocs.io/en/latest/reference/#urllib3.response.HTTPResponse.stream\n\nThe generator will return up to that much data per iteration, but may return less. \n\nThe 32768 limit is caused by server returning responses in chunks of 32768 size (0x8000):\n8000\\r\\n\nsome data....\\r\\n\nurllib3 yields data from only one chunk at a time (https://github.com/urllib3/urllib3/blob/1.23/urllib3/response.py#L594), hence the \"wrap-around\" behavior.. @shin- \n\nForcing stream=False seems hamfisted.\n\nI only suggested forcing it in docker.models.images.pull, because I don't see any benefits to using stream=True there. The only benefit to using stream=True is if you fear that the pull log can be very big (like >1MB) and your app will run out of memory.I doubt that can happen.\nI do not force it in docker.api.image.pull\n\nmaybe we simply need a warning in the DockerClient documentation\n\nI agree about adding the documentation (I've done it in docker.api.image.pull), although code in docker.models.images.pull definitelly needs to be changed, otherwise self.client.api.pull(repository, tag=tag, stream=True) will be called by the library code without consuming the generator.\nMy suggestions for docker.models.images.pull:\n1. force stream=False (kwargs.pop('stream', None) or kwargs['stream']=False) + optionaly log a warning that stream=False doesn't make sense here\n2. consume the generator if stream=True (best choice if the pull log can be big):\n++        if kwargs.get('stream'):\n++            for _ in self.client.api.pull(repository, tag=tag, **kwargs): pass\n++        else:\n++            self.client.api.pull(repository, tag=tag, **kwargs)\n--        self.client.api.pull(repository, tag=tag, **kwargs)\nor test if result of pull is a generator and consume it depending on that\n3. \n++ list(self.client.api.pull(repository, tag=tag, **kwargs))\n-- self.client.api.pull(repository, tag=tag, **kwargs). @shin- ping. How did you decide on the +10 part? IMO it's not enough, look here: https://github.com/moby/moby/blob/master/daemon/container_operations_unix.go#L341 When SIGTERM fails to kill the container dockerd will send a SIGKILL and sleep for additional 10 seconds. When you consider whatever additional overhead that there might be, we can still get a ReadTimeout.. I doubt that that the pull log can be big enough to cause memory issues.. ",
    "davclark": "This seems like an easy fix and is catastrophic for anyone doing programmatic builds on Windows. Is there any reason it's not fixed (@LazarusX  provides one fix in their git repo).\nI'm guessing this may be due to the continued situation with confusion around paths on Docker for Windows? I can say fairly definitively that there is simply no right answer at this point, and I haven't seen any uptake on core devs (Docker, Docker Compose...) on documenting a right answer.\nSo, I think it's OK to just pick something for now. I would propose that at a minimum, native windows paths (such as would be generated by os.path.abspath('.')) should \"just work.\"\nI'm happy to submit a PR if the core devs here are happy with @LazarusX' fix (and I'm happy if someone else does also).. ",
    "askorski-fc": "Thanks for the quick answer, it works perfectly.\nI suggest to mention the reload method in the attrs descriptions to avoid further questions like this one.. ",
    "Immortalin": "Or to rephrase, what's the best way to dump logs to a browser asynchronously without blocking flask?\nE.g. many visitors to a site viewing logs for many different docker services. I tried quite a few ways of implementing things but they all required me putting a pause()  timeout while calling service.logs/container.logs generators to give a chance for other requests otherwise the whole thing would block. I am wondering if there is any other way that don't require pausing. E.g. if I have a 10 000 line log file I want to dump the whole thing to browser first before letting it run its course of periodic checking. Right now with e.g. pause(0.1 seconds) dumps only ~10 lines a second, less if multiple users are querying multiple logs.\nE.g. in aiohttp, not flask:\n```Python\nasync def logging(request):\n    d = \"\"\"\n        \n\n\n                var evtSource = new EventSource(\"/logs\");\n                evtSource.onmessage = function(e) {\n                    document.getElementById('response').innerText = e.data\n                }\n            \nResponse from server:\n\n\n\n    \"\"\"\n    return Response(text=d, content_type='text/html')\nimport docker\nclient = docker.from_env()\nasync def logs(request):\nservice = client.services.get('x6gk7uigridu')\nloop = request.app.loop\ndata = service.logs(follow=True, stdout=True, stderr=True)\nasync with sse_response(request) as resp:\n    while True:\n\n        await resp.send(next(data).decode('utf-8'))\n        await asyncio.sleep(0.1, loop=loop) # Without this function this will block, with this function it still doesn't scale well with multiple users\nreturn resp\n\n```. ",
    "georgijd": "Hi @ravjanga ,\nI'm experiencing the same behavior and I think the line that's causing this is https://github.com/docker/docker-py/blob/master/docker/api/client.py#L292. I patched my api client with a version of that method that doesn't keep the response reference and it's now releasing the sockets. I'm not sure if that's a viable solution for all use cases, but it works for me. . Hi @ravjanga, \nI haven't had any problems since I've changed it. However, my use case is very simple:\npython\n...\nc = cli.api.exec_create(container=container_id, cmd=\"df -lh\")\nstdout = cli.api.exec_start(c[\"Id\"])\n...\nI'm not using the detach, tty or stream parameters and that workaround might prevent the client from working correctly if any of these parameters are passed. I haven't tried it. Maybe someone could check if it works for all use-cases.. ",
    "Kazade": "Typically solved it immediately after posting this. The solution was:\nclient.containers.run(\n        \"kazade/dreamcast-sdk\",\n        \"bash --login\",\n        detach=True,\n        volumes={\n            abspath(\".\"): {\n                \"bind\": \"/simulant-project\",\n                \"mode\": \"Z\"\n            }\n        },\n        tty=True,\n        stdin_open=True,\n        name=DOCKER_CONTAINER_NAME,\n        working_dir=working_dir\n    ). ",
    "onzo-mateuszzakarczemny": "\nI'd like to have that. Are you planning to finish this up @Matzz / @onzo-mateuszzakarczemny ? Otherwise can I pick it up?\n\nHi. Sorry I forgot about this PR. I had some issues with make it working and did some workaround instead of finishing that.\nFeel free to pick it if you want. . ",
    "shivam05011996": "I was thinking to use containers.log(), but I didn't find any parameter to write to a specific file, or if I could even separate the STDOUT and STDERR. ",
    "py2010": "hello everyone,\nAPI: GET https://ip:2375/v1.30/images/get?names=Image_Name\nhttps://github.com/docker/docker-py/blob/master/docker/api/image.py\n        res = self._get(self._url(\"/images/{0}/get\", image), stream=True)\n...........\n        res = self._get(self._url(\"/images/get?names={0}\", image), stream=True)\nalso OK.. ",
    "MatheusRV": "\nYou can't pass a list of commands to run, only a single command.\n\nAccording to the documentation, I can. But even running just one command I'm getting the same error.\nhttps://docker-py.readthedocs.io/en/stable/containers.html\ncontainer = self.server.containers.run(\n                \"matheusrv/ssgberk.test.%s\" % test.name,\n                \"/bin/bash -c 'echo hello'\",\n                name=name,\n                network=self.benchmarker.config.network,\n                network_mode=self.benchmarker.config.network_mode,\n                stderr=True,\n                detach=True,\n                init=True,\n                extra_hosts=extra_hosts,\n                privileged=True,\n                ulimits=ulimit,\n                sysctls=sysctl,\n                remove=True,\n                log_config={'type': None})\nSame error. I've tried it in several ways and still the error remains. I already checked if the file /bin/bash is presente in client O.S., and it was.. ",
    "maciejjaskowski": "Turned out it was bug on my side.. ",
    "Karhide": "Library version 3.5.0. I'll give your suggestion a try, thanks!\nEdit: Can confirm, deleting credsStore from the config made this work.. So to follow on, is there a way I can make this not happen? I'd like any users of my code to not have to go through the same thing.. ",
    "new-guy": "I'm running into this issue as well.  I can't auth into ECR for one of my builds, and the build command doesn't accept auth_config, so I'm boned :(. ",
    "sigma65535": "I run the code in root ,it is ok .\nclosed. ",
    "Jie-Yuan": "how . ",
    "minamijoyo": "@shin- Thanks !!! . ",
    "pmb0": "Hello, what do I have to do to be able to use this function? When running I get the exception Install paramiko package to enable ssh:// support. pip install paramiko does not help.\n```sh\n$ DOCKER_HOST=ssh:// docker-compose up -d\nTraceback (most recent call last):\n  File \"site-packages/docker/api/client.py\", line 151, in init\nNameError: name 'SSHAdapter' is not defined\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"docker-compose\", line 6, in \n  File \"compose/cli/main.py\", line 71, in main\n  File \"compose/cli/main.py\", line 124, in perform_command\n  File \"compose/cli/command.py\", line 42, in project_from_options\n  File \"compose/cli/command.py\", line 123, in get_project\n  File \"compose/cli/command.py\", line 94, in get_client\n  File \"compose/cli/docker_client.py\", line 127, in docker_client\n  File \"site-packages/docker/api/client.py\", line 156, in init\ndocker.errors.DockerException: Install paramiko package to enable ssh:// support\n[45766] Failed to execute script docker-compose\n```. Ah, thank you!. ",
    "technoe": "Still broken. Traceback (most recent call last):\n  File \"site-packages/docker/api/client.py\", line 151, in init\nNameError: name 'SSHAdapter' is not defined\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"bin/docker-compose\", line 6, in \n  File \"compose/cli/main.py\", line 71, in main\n  File \"compose/cli/main.py\", line 124, in perform_command\n  File \"compose/cli/command.py\", line 42, in project_from_options\n  File \"compose/cli/command.py\", line 123, in get_project\n  File \"compose/cli/command.py\", line 94, in get_client\n  File \"compose/cli/docker_client.py\", line 127, in docker_client\n  File \"site-packages/docker/api/client.py\", line 156, in init\ndocker.errors.DockerException: Install paramiko package to enable ssh:// support\n[23282] Failed to execute script docker-compose. If only it were that easy. Paramiko is installed on the server and client. . ",
    "pll": "Hmm, The Salt guys seem to feel that this is the correct place, since they just wrap this library and call it when pulling docker containers.\nIf I were using this library, and wanted to use a different command than docker, how would I do so? Does this library facilitate over-riding the command called for docker?. Oh, I see. Thanks.. ",
    "chris-crone": "@haizaar I've created an issue to track BuildKit support here. I'll close this specific issue and reference it from there.. There is some work being done on the engine to provide richer errors so that they're easier to manage. Not exactly this but related: https://github.com/moby/moby/pull/38467. Hi @wonderchang! Thanks for the suggestion!\nI think that this is too high-level and specific for the SDK. If one needs to check the health of a container; I'd expect that this would be done via an inspect and some logic in the application code. This would match the pattern that we have in our Go client.. @wonderchang The health of a container is volatile unlike the static properties you listed (id, and name). The SDK would have to either keep the property up to date asynchronously or update it on each call. Neither of those are really good solutions.\nI think it's best to push this kind of work to the end developer so that it's visible to them.. Looking a bit more, some of the other properties are also volatile (like status and label). @shin- Is there a specific reason we don't have a health property?. The current known blocker for this is we need a Python implementation of the fssync gRPC service (a service which runs on the session but on the client side) and that service is distinctly non-trivial to implement.. Request for build secret support: https://github.com/docker/docker-py/issues/2174. Fixed, thanks @haizaar . PR merged, closing.. Should add new parameters at the end of the list so that callers of init_swarm that use ordered (i.e.: unnamed) parameters continue to work as expected.\ne.g.: An existing function could call:\npython\ns.init_swarm('192.168.0.1', '0.0.0.0:2377', True)\nWhich would now break.. Same comment here for parameters.. nit: Might be good to use the default as the example.. nit: Extra \\n. ",
    "getor": "reinstalled docker to 2.1.0, issues is gone.. ",
    "sebpiq": "I understand the idea between this division, it's just that I feels silly to duplicate the high-level API source code in order to parse my output from the low-level API (I want to get the image id). Maybe extracting the parsing from the high-level API and making it available as some sort of utils would be useful then ( output event -> parsed event<Error|ImageId|Log> ) ?\n. Ah nevermind, I don't actually need to parse, I can just use client.images.get(name) no need for image id. thanks for the great library by the way :) loving it. ",
    "hbhanawat": "I was using client.services.create API to which I cannot pass Placement object .  Should I be using apiClient. create_service?. I tried creating the placement object like this. \ncassandra1EnvList = envVarsList + ['CASSANDRA_BROADCAST_ADDRESS=' + self.cassandra1service,'CASSANDRA_SEEDS='+ self.cassandra1service + \",\" + self.cassandra2service, 'CASSANDRA_LOG_DIR=/var/log/cassandra']\npreferences1 = [\"spread=node.labels.launcher\"]\ncassandra1Binding = [cassandraConfig.dataFolder1 + \":\" + \"/var/lib/cassandra\", cassandraConfig.logFolder1 + \":\" + \"/var/log/cassandra\"]\nmode = ServiceMode(\"replicated\", 1)\nrestartPolicy = RestartPolicy(\"on-failure\")\ncs = ContainerSpec(image=image, args=None, env=cassandra1EnvList, labels=None, mounts=cassandra1Binding)\nplacement=Placement(constraints=None,preferences=preferences1)\ntt = TaskTemplate(cs, resources=None, restart_policy=restartPolicy, placement=placement, networks=[cassandraConfig.networkName])\napiClient.create_service(task_template=tt, name=self.cassandra1service, labels=None, networks=[cassandraConfig.networkName], endpoint_spec=None, mode=mode)\nThis code failed with the previous error. \nI also tried with preferences1=[{'spread': 'node.labels.launcher'}]. This failed with another error. \nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/drive/repos/spark-jobserver/deployment/fluirpy/startup/cassandra.py\", line 25, in <module>\n    cassandraLauncher.startCassandraService(de, config)\n  File \"plugins/cassandra.py\", line 42, in startCassandraService\n    apiClient.create_service(task_template=tt, name=self.cassandra1service, labels=None, networks=[cassandraConfig.networkName], endpoint_spec=None, mode=mode)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/utils/decorators.py\", line 34, in wrapper\n    return f(self, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/api/service.py\", line 185, in create_service\n    self._post_json(url, data=data, headers=headers), True\n  File \"/usr/local/lib/python2.7/dist-packages/docker/api/client.py\", line 235, in _result\n    self._raise_for_status(response)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/api/client.py\", line 231, in _raise_for_status\n    raise create_api_error_from_http_exception(e)\n  File \"/usr/local/lib/python2.7/dist-packages/docker/errors.py\", line 31, in create_api_error_from_http_exception\n    raise cls(e, response=response, explanation=explanation)\ndocker.errors.APIError: 500 Server Error: Internal Server Error (\"json: cannot unmarshal string into Go struct field PlacementPreference.Spread of type swarm.SpreadOver\")\nCan you help me with an example of how to create the preference object? \n. Ok. It worked when, in the above example, preferences was set as \npreferences1 = [{'SpreadOver': 'node.labels.launcher'}]\nTook me considerable time to figure this out. Keeping this ticket open so that it can be documented. . ",
    "Koc": "Wow, nice. Thanx. ",
    "originalgremlin": "PR in review: https://github.com/docker/docker-py/pull/2201. Adding the arguments to swarm init was relatively straightforward.  I do wonder how the maintainers will feel about adding these arguments at all: other parts of the docker-py swarm API rely on a docker engine API version of 1.24, whereas these features are from a much more recent docker engine API of 1.39.\nIf any maintainers have ideas on how best to handle modernization of the swarm part of this repo I would be happy to help round out the codebase and truly \"let you do anything the docker command does\".. @shin- That makes sense.  Thanks for pointing out the examples for how such things are handled.\nCode updated and pushed.  Has been tested a fair amount in my local environment.  Did my best to stick with the project style though you may well have suggestions for things that I missed.. @shin- Should be ready to go now.  All tests pass and all protocol followed to the best of my ability.  Let me know if there's any other polishing I should do.. @shin- Squeaky wheel ping.  Anything missing or needing improvement in order to get this PR merged?. ",
    "Addvilz": "Simple example, given the dict, is there a standard way to transform that dict to, let's say, output in stdout? Kind of like docker own output.. ",
    "yyminmax": "Well, It works for me.  Thanks a lot.. ",
    "yichinzhu": "I'm using version 2.5.1, it fixed after upgrading to 3.0.0. Thank you.. ",
    "sohail535": "Thanks @shin- It works. Just a humble suggestion, may add this to documentation?\nClosing this.. ",
    "eiffel-fl": "You are welcome! I did what you asked!\nThe make test ran more test but some failures were met:\n```\n============================================================== FAILURES ===============================================================\n______ AttachContainerTest.test_attach_stream_and_cancel ______\ntests/integration/api_container_test.py:1275: in test_attach_stream_and_cancel\n    for line in output:\ndocker/types/daemon.py:32: in next\n    return next(self.stream)\ndocker/api/client.py:404: in \n    gen = (data for (, data) in gen)\ndocker/utils/socket.py:83: in \n    return ((STDOUT, frame) for frame in frames_iter_tty(socket))\ndocker/utils/socket.py:115: in frames_iter_tty\n    result = read(socket)\ndocker/utils/socket.py:31: in read\n    select.select([socket], [], [])\nE   Failed: Timeout >5.0s\n______ ImageCollectionTest.test_list_with_repository ______\ndocker/api/client.py:256: in _raise_for_status\n    response.raise_for_status()\n/usr/local/lib/python3.6/site-packages/requests/models.py:940: in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nE   requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://docker:2375/v1.35/images/create?tag=latest&fromImage=alpine\nDuring handling of the above exception, another exception occurred:\ntests/integration/models_images_test.py:64: in test_list_with_repository\n    image = client.images.pull('alpine:latest')\ndocker/models/images.py:437: in pull\n    repository, tag=tag, stream=True, kwargs\ndocker/api/image.py:400: in pull\n    self._raise_for_status(response)\ndocker/api/client.py:258: in _raise_for_status\n    raise create_api_error_from_http_exception(e)\ndocker/errors.py:31: in create_api_error_from_http_exception\n    raise cls(e, response=response, explanation=explanation)\nE   docker.errors.APIError: 500 Server Error: Internal Server Error (\"Get https://registry-1.docker.io/v2/: EOF\")\n======================================================= short test summary info =======================================================\nXFAIL tests/integration/api_container_test.py::CreateContainerTest::test_create_with_cpu_rt_options\n  CONFIG_RT_GROUP_SCHED isn't enabled\nXFAIL tests/integration/api_container_test.py::CreateContainerTest::test_create_with_init_path\n  init-path removed in 17.05.0\nXFAIL tests/integration/api_swarm_test.py::SwarmTest::test_init_swarm_with_log_driver\n  This doesn't seem to be taken into account by the engine\nSKIP [1] tests/integration/api_image_test.py:290: Doesn't work inside a container - FIXME\nSKIP [1] /src/tests/integration/api_swarm_test.py:31: Test stalls the engine on 1.12.0\n=============================== 2 failed, 348 passed, 2 skipped, 3 xfailed, 3 xpassed in 486.94 seconds ===============================\nMakefile:74\u00a0: la recette pour la cible \u00ab\u00a0integration-dind-ssl\u00a0\u00bb a \u00e9chou\u00e9e\nmake: * [integration-dind-ssl] Erreur 1\n```\nThe make docs succeeded but 2 warnings were met.. I forgot to add my local changes before amend my commit...\nIt should be good now!. ",
    "CyberTheReape": "Yes i know  . But i cant write other repo.  \u0130 did write sh , shadowbox is opening.. but watchtower container not opening with sh.\n\u0130 am using outline manager vpn server. Now i want forward my vps port to docker container. \u0130 will forward ports in container to my home computer.\nEXAMPLE;\nVps port and ip\u300bcontainer \u300bvpn \u300bhome\n\u0130 am using ipv6 ip address in my home. \u0130 cant open port in my modem. \u0130 did install vpn server for it. My vpn server working now. But i want use my vps ports with vpn ip address. There were 2 separate containers because I installed vpn server with docker.help me now please.if i can do this job i will feed 40 poor people.i don't have much information about docker yet.I need the help of professional craftsmen like you.handsome boy waiting for answer.\n@shin-. ",
    "Mattrack": "I'm experiencing the same issue. I was able to implement my own inspect_distribution method using an HTTP client, but it'd be much more reliable if I could use the Docker API consistently in my project.\n@shin- I saw the fix you made for this issue in #2227 , and I was wondering if by any chance this fix could be part of a next minor/patch update, instead of 4.0?\nThank you! \ud83d\udc4d :)\nEDIT: For now I'm installing docker-py using pip install git+git://github.com/docker/docker-py.git@b6f6e7270ef1acfe7398b99b575d22d0d37ae8bf \ud83d\udc4d . ",
    "regan-karlewicz": "Any timeline estimate to release the fix for this bug? Thanks!. ",
    "paulcdejean": "Working with programmerq on irc we've discovered that when you do a docker push of a non existent tag, that the docker http API gives a 200 return code.\n```\n\n2019/01/11 20:29:03.992355  length=91 from=0 to=90\nGET /ping HTTP/1.1\\r\nHost: 127.0.0.1:5566\\r\nUser-Agent: Docker-Client/18.06.1-ce (linux)\\r\n\\r\n< 2019/01/11 20:29:03.992762  length=215 from=0 to=214\nHTTP/1.1 200 OK\\r\nApi-Version: 1.38\\r\nDocker-Experimental: false\\r\nOstype: linux\\r\nServer: Docker/18.06.1-ce (linux)\\r\nDate: Fri, 11 Jan 2019 20:29:03 GMT\\r\nContent-Length: 2\\r\nContent-Type: text/plain; charset=utf-8\\r\n\\r\nOK> 2019/01/11 20:29:04.064731  length=232 from=91 to=322\nPOST /v1.38/images/485757098324.dkr.ecr.us-east-1.amazonaws.com/ssh_proxy/push?tag= HTTP/1.1\\r\nHost: 127.0.0.1:5566\\r\nUser-Agent: Docker-Client/18.06.1-ce (linux)\\r\nContent-Length: 0\\r\nContent-Type: text/plain\\r\nX-Registry-Auth: censored=\\r\n\\r\n< 2019/01/11 20:29:04.208823  length=569 from=215 to=783\nHTTP/1.1 200 OK\\r\nApi-Version: 1.38\\r\nContent-Type: application/json\\r\nDocker-Experimental: false\\r\nOstype: linux\\r\nServer: Docker/18.06.1-ce (linux)\\r\nDate: Fri, 11 Jan 2019 20:29:04 GMT\\r\nTransfer-Encoding: chunked\\r\n\\r\n65\\r\n{\"status\":\"The push refers to repository [485757098324.dkr.ecr.us-east-1.amazonaws.com/ssh_proxy]\"}\\r\n\\r\nf3\\r\n{\"errorDetail\":{\"message\":\"An image does not exist locally with the tag: 485757098324.dkr.ecr.us-east-1.amazonaws.com/ssh\nproxy\"},\"error\":\"An image does not exist locally with the tag: 485757098324.dkr.ecr.us-east-1.amazonaws.com/ssh_proxy\"}\\r\n\\r\n< 2019/01/11 20:29:04.210393  length=5 from=784 to=788\n0\\r\n\\r\n```. Well as is we basically need to write a docker push wrapper function that attempts the push, parses the output. Then throws an exception if there's an error key in any of the json dictionary lines of the output.\n\nThat type of logic could be brought into the SDK as a stopgap before something more elegant is implemented. If performance is a concern, then it could be implemented as an alternative push function even?\nAs is people are either writing this sort of wrapper function, avoiding using the SDK, or just crossing their fingers and hoping none of their docker pushes ever error out.. I'll add a code example of this sort of wrapper function soon (once I write it, probably before the end of the day), as is I'm using the finger crossing method).. Here's the wrapper I wrote to workaround this issue.\n```python3\nclass RemoteError(docker.errors.DockerException):\n    \"\"\"\n    Raised if a remote docker repository returns an error. Usually due to a failed push or pull.\n    \"\"\"\n    def init(self, error_dict):\n        self.msg = error_dict['error']\n        self.reason = error_dict['errorDetail']['message']\ndef push_wrapper(self, repository, kwargs):\n    push_output = self.client.api.push(repository, kwargs)\n    parsed_push_output = []\n    digest = ''\n    for line in push_output.splitlines():\n        parsed_line = json.loads(line)\n        if 'error' in parsed_line:\n            raise RemoteError(parsed_line)\n        if 'aux' in parsed_line:\n            if 'Digest' in parsed_line['aux']:\n                digest = parsed_line['aux']['Digest']\n    parsed_push_output.append(parsed_line)\nreturn parsed_push_output, digest\n\ndocker.models.images.ImageCollection.push_wrapper = push_wrapper\n```\nIt's not super tested due to many push errors being difficult to reproduce (the rate limit one is one I've been struggling with with amazon ECR in particular).. ",
    "wonderchang": "Sure, but how about the health property? Like other essential properties id, name, and status.. @chris-crone Thank you for answering!\nIn my point of view, when you start to implement Container object, you are not just a SDK at all. It becomes more high-level abstractions and has more detail encapsulations. And to be Pythonic, it is worth to wrap them for high-level usage because the client code will looks very simple and readable.. > most containers do not use healthchecks at all.\nWhy? People tends to check from outside the container?. ",
    "tonistiigi": "My recommendation is to create a helper binary that compose can call as a quick practical solution. Note that fssync is optional for BuildKit, the Docker API can still receive tarballs fine and build them with BuildKit. It's less optimal but you still get the other benefits. But if you want secrets/ssh support then you do need either a helper or reimplement the grpc provider.. There are some things that might be interesting for compose that can be done in Go library and not in CLI atm. Like doing multiple requests in parallel with a shared session.. ",
    "mipearson": "Is there a possible future where most docker-compose things are done via a helper binary rather than via docker-py?\nI seem to be having a very bad run of hitting docker-py specific issues lately :(. ",
    "sh7dm": "Related: #953. @TomasTomecek I think it will be much convenient to have this in library. ",
    "Tset-Noitamotua": "Actual\nGiven I have not pulled mongo image manually (e.g. with docker cli)\nWhen I execute the code below\nThen ALL available mongo images will be downloaded\n\n\nExpected\nGiven I don't specify an image tag to the `run` command\nWhen I execute the code below\nThen only the mongo:latest image is downloaded\n\nCode Example\n```python\nimport docker\nclient = docker.from_env()\ndef run_mongodb_container():\n    \"\"\"run a mondodb container in background\"\"\"\n    container = client.containers.run(\"mongo\", name=\"MongoDB\",\n                                        ports={'27017/tcp': 27017},\n                                        detach=True)\n    container = client.containers.get(\"MongoDB\")\n    logs = container.logs()\n    return logs\n```\n. ",
    "wvaske": "See PR https://github.com/docker/docker-py/pull/2233. ",
    "ulyssessouza": "Thank you for the PR @hannseman !\nLGTM. @jeanbritz Can you post the output of a whereis docker in the target machine?\ndocker-compose expects the docker cli to be in the $PATH of the login shell (note that it's not the interactive shell)\nFor example, if you have it as /usr/bin/docker (and of course /usr/bin is in your $PATH) it should be good.. Hello @heckad ! Thank you for the PR.\nI agree with you that container_name_or_id would make it more explicit and if that wasn't a breaking change, I would approve but since it's not the case and the upside is minor, I have to decline.\nAnd we still have the documentation to make it explicit.. nit: Remove empty line. Linter says that before a class it needs 2 empty lines. typo: orriginal -> original. Not sure if changing the message is necessary here.. Any special reason not to use the default parameter value in line 20?\nLooks like a buildargs={} in line 20 would be cleaner.. nit: Use Oxford comma to be clear please.\nLike: output: (generator, bytes, or tuple):. wouldn't it be a tuple with two elements of type byte: stdout and stderr ?. Since you are in a test and you have the input and the expected output, IMHO looks more reasonable to assert your expected path like:\npython\nassert target_host is None\nassert int(actual_port['HostPort']) > 0\nFor the other types, you can can add other tests that does essentially the same but with different structures, like list and tuple.. I think that you are mixing 2 concerns when using the term generator iterator. They are 2 different things. Just using generator should be fine.\nMy proposal would be:\n(generator of bytes or bytes)\n. Same as a above. And you had a little typo here generatoran -> generator. Ahhh, OK!. No problem with the new line. The content is more important. ",
    "Royco1": "how come this bug is not even looked at? . This might have come out ruder than I meant it, I apologise.. thanks for\nthe clarification\nOn Wed, Jan 30, 2019, 7:56 PM Joffrey F <notifications@github.com wrote:\n\nhow come this bug is not even looked at?\nMaintainers have lives too. But also, it helps to include as much\ninformation as possible\nhttps://github.com/docker/docker-py/blob/master/CONTRIBUTING.md#reporting-issues\nwhen reporting an issue.\nIn this particular case, this is not a bug; you simply need to call\nreload()\nhttps://docker-py.readthedocs.io/en/stable/networks.html#docker.models.networks.Network.reload\non the network object to see the updated list of containers.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/docker/docker-py/issues/2235#issuecomment-459043766,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AWj4AYflja_6IAobVPi_G9PHUEWSbFqCks5vIdzkgaJpZM4aJkZd\n.\n. \n",
    "ackerleytng": "This produces empty lists\n[n.containers for n in client.networks.list()]\nThis does work, but perhaps I'm missing something - requiring a reload() doesn't seem like expected behavior\n```\n\n\n\nfor n in [n for n in client.networks.list()]:\n...     n.reload()\n...     n.containers\n... \n[]\n[, , , , ]\n[]\n[]\n```\n. \n\n\n",
    "Sanderhuisman": "Thank you for the response, it seems indeed that its a limitation of Docker itself. I will ask further on the Docker forum.. ",
    "scot5": "Thanks for the quick response.\nHowever this is, if I'm not mistaken, the equivalent of --publish not --expose. I do not wish to publish my ports to my host, I just want to expose it so my nginx-proxy container can access it.. ",
    "rumpl": "Why not :). Thing is, compose doesn\u2019t yet have anything for different transports, would be a shame to add it there now. \nA flag would be better I think and maybe some env var?. Also, as I said in the first post, for some reason num_pools should be 9, not 10, some dependency of docker-py is not counting as everyone else. It doesn't seem so...\n@jeanbritz, could you try by setting pool_connections to 10 for example?. ",
    "ijc": "I wonder -- could it be counting the underlying network connection as one, then allowing 9 additional sessions on top (essentially meaning the first sessions counts as two)?. Is it possible that your container is only producing output on stderr? Only stdin is included in the result of run by default. You can pass stderr=True to change this. By contrast containers.log (and docker log) include stderr by default.\n```python\n\n\n\nimport docker\nclient = docker.from_env()\nclient.containers.run('alpine', ['/bin/sh', '-c', 'echo hello world'])\n'hello world\\n'\nclient.containers.run('alpine', ['/bin/sh', '-c', 'echo hello world 1>&2'])\n''\nclient.containers.run('alpine', ['/bin/sh', '-c', 'echo hello world 1>&2'], stderr=True)\n'hello world\\n'\n```\n\n\n\nThis is documented in the docs for run, although it's a bit buried in the midst of the long list of kwargs\nFrom https://docker-py.readthedocs.io/en/stable/containers.html:\n\n\nstdout (bool) \u2013 Return logs from STDOUT when detach=False. Default: True.\nstderr (bool) \u2013 Return logs from STDERR when detach=False. Default: False\n\n\nI'm going to speculatively close the issue since I think that's most likely the answer, please do reopen if that doesn't help though.. I'm afraid your reproduction steps are not complete enough to be able to help (you haven't said how your redis was deployed, not which version nor how it is setup).\nI do know that there is no get command in the redis:latest container I just pushed.\nYour example dockerList.exec_run(\"get hi\")) is trying to run get as a new command inside the container. You need to pass socket=True to your first exec_run and then interact with the resulting socket option in order to interact with the redis-cli.\nI hope that is enough to get you going. I'm going to close this issue, since this is not really a support forum. Please consult the docs (https://docker-py.readthedocs.io/en/stable/) carefully.. ",
    "silvin-lubecki": "Hello @mhrabovcin , thank you for filling this issue.\nAs far as I understand your issue, you have an error one of your test case.\nCan you fill the issue with more context? For example looking at the code you pasted, we can't see how the cli object was initialized.\nThank you!. ",
    "mhrabovcin": "The initialization of the client: https://github.com/dcos/bouncer/blob/4beab0338e6859508d58f254d52942cd7bdd9fbd/tests/containers/containerbase.py#L33-L55\nThe code that is interacting with library: https://github.com/dcos/bouncer/blob/4beab0338e6859508d58f254d52942cd7bdd9fbd/tests/containers/cockroach.py. ",
    "simonferquel": "@rumpl or @ulyssessouza  PTAL. @ulyssessouza @rumpl any of you want to tackle this ?\n~~@thaJeztah would it require a new version of compose schema ?~~ nvm, I thought I was triaging docker/compose :/. @rumpl is this related to #2246 or #2248 ?. ",
    "arcenik": "Hi,\nI was abble to reproduce the 10th connection problem using only docker-py on a host with 20 running containers with the following script:\n```python\n! /usr/bin/env python\nfrom docker import APIClient\nimport sys\nimport logging\nlogging.basicConfig()\nlogging.getLogger(\"paramiko\").setLevel(logging.DEBUG)\nclient = APIClient(base_url='ssh://192.168.x,y)\nlist = client.containers()\nfor c in list:\n    i = (client.inspect_container(c['Id']))\n    print(\"Inspect returned: {id}\".format(id=i['Id']))\n    print(\"#\"*200)\n    sys.stdout.flush()\nprint()\nprint(\"#\"*200)\nprint(\"test finished\")\n```\nI've also made a PR with a fix: https://github.com/docker/docker-py/pull/2262\n. ",
    "jeanbritz": "Hi @simonferquel, I have check to see if there are any related issues before I posted, but could not find anything similar. The only related issue I could find is this But that is when you use a Docker Swarm. I want to know if it is possible to get this to work on a Docker Daemon?. Hi @rumpl,\nI have set the pool_connections to 10 in my example code, but it does not make a difference. It is also hard-coded in the docker-py SDK to 25.\nI've read here which have done something similar. I have not written test code using the library mentioned there, because it is quite old.\nThey mentioned that they use socat to forward the SSH tunnel connection to the Docker Unix socket (unix:///var/run/docker.sock) \nIs this a feasible way of getting this to work?. ",
    "christianbur": "if i check the available methods of SocketIO i get the following \ncall:\nprint ([method_name for method_name in dir(worker_socket) if callable(getattr(worker_socket, method_name))])\noutput:\n['__class__', '__del__', '__delattr__', '__dir__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', \n'__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__ne__', '__new__', \n'__next__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', \n'_checkClosed', '_checkReadable', '_checkSeekable', '_checkWritable', 'close', 'fileno', 'flush', 'isatty', \n'read', 'readable', 'readall', 'readinto', 'readline', 'readlines', 'seek', 'seekable', 'tell', 'truncate', \n'writable', 'write', 'writelines']\ni checked if the stream is writable, unfortunately i get False \nprint (worker_socket.writable()) -> False\nto me, it sounds like IO, but that's for \"implementation of I/O streams\" and not a socket ??. so the above example with python3\n```\nimport docker\nclient = docker.DockerClient(base_url='unix://var/run/docker.sock', version='auto')\ncontainer = client.containers.run(\"gliderlabs/alpine\", command= \"sleep 100\", detach = True)\nsocket = container.exec_run(cmd=\"sh\", stdin=True, socket = True)\nprint(socket)\nsocket.output._sock.send(b\"ls\\n\")\nprint socket\nsocket.sendall(b\"ls\\n\")\na read block after a send\ntry:\n    unknown_byte=socket.output._sock.recv(1)\n    while 1:\n        # note that os.read does not work\n        # because it does not TLS-decrypt\n        # but returns the low-level encrypted data\n        # one must use \"socket.recv\" instead\n        data = socket.output._sock.recv(16384)\n        if not data: break\n        print(data.decode('utf8'))\nexcept so.timeout: pass\nsocket.output._sock.send(b\"exit\\n\")\n```\nmethodes for \"socket.output._sock\"\n['__class__', '__del__', '__delattr__', '__dir__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_accept', '_check_sendfile_params', '_decref_socketios', '_real_close', '_sendfile_use_send', '_sendfile_use_sendfile', 'accept', 'bind', 'close', 'connect', 'connect_ex', 'detach', 'dup', 'fileno', 'get_inheritable', 'getpeername', 'getsockname', 'getsockopt', 'gettimeout', 'listen', 'makefile', 'recv', 'recv_into', 'recvfrom', 'recvfrom_into', 'recvmsg', 'recvmsg_into', 'send', 'sendall', 'sendfile', 'sendmsg', 'sendmsg_afalg', 'sendto', 'set_inheritable', 'setblocking', 'setsockopt', 'settimeout', 'shutdown']. ",
    "jrcichra": "Having the exact same issue, did someone introduce Socket.IO into this instead of a python socket?. Here's what I came up with after a day of trying to figure this out:\n```python\nimport docker, tarfile\nfrom io import BytesIO\nclient = docker.APIClient()\ncreate container\ncontainer = client.create_container(\n    'ubuntu',\n    stdin_open = True,\n    # environment=[\"PS1=#\"],\n    command    = 'bash')\nclient.start(container)\nattach stdin to container and send data\ns = client.attach_socket(container, params={'stdin': 1, 'stream': 1,'stdout':1,'stderr':1})\nwhile True:\n    original_text_to_send = input(\"$\") + '\\n'\n    if(original_text_to_send == \"exit\\n\"):\n        s.close()\n        break\n    else:\n        s._sock.send(original_text_to_send.encode('utf-8'))\n        msg = s._sock.recv(1024)\n        print(msg)\n        print(len(msg))\n        print('==================')\n        print(msg.decode()[8:])\nprint(\"We're done here\")\nclient.stop(container)\nclient.wait(container)\nclient.remove_container(container)\n```\nThis creates an ubuntu container that starts up bash. It then attaches stdin, stdout, stderr, in a stream form. I made a little while loop to handle commands. I noticed the first 8 bytes are something special, maybe header data for SocketIO? Not sure. The formatted output strips off those 8 bytes.. ",
    "aBradleyUno": "Response does nothing on 101 status code, does it?. ",
    "mobileben": "One other thing to note here. I mentioned that if I get login and then log into docker prior to running the script it works. I noticed it also recreates the config.json file, whereas calling it from python does not. If config.json does exist, the entry is not updated when run from python.\nIf I run login as a subprocess, config.json does get modified. This behavior seems rather strange.. I need to do a little more testing here, but if I run docker login as a subprocess. (which defeats the purpose of using docker-py), it works. \nIt seems that the problem comes from when config.json contains an entry for the registry with a stale auth token. \nIf config.json does not exist or if config.json exists but doesn't have an entry for the registry, it will work properly. Note this is independent of whether or not the reauth parameter is passed in with login.. Okay, I ran some experiments the last few days. Had to deal with the 12 hour AWS ECR ticket so it took a little longer to do.\nIt does seem that there is an issue with docker-py.\nBased on my findings, I can either use boto3 or run a sub-process calling the command line to aws ecr. However the only permutation that seems to work with the following steps.\n\nuse a sub-process to perform the docker login. This will result in the config.json file being updated (not sure if this has any relevance at all or not).\nCreate the docker client via docker_client = docker.from_env(). I have found doing this prior to the sub-process results in it not working properly (unless you have an already valid config.json\nThen call docker_client.login(username=username, password=password, registry=registry_url)\n\nWhether or not this is expected or not or if I'm doing something wrong, I don't know. This is what I've  come up with as steps that work.. ",
    "jcsirot": "@p1100i thank you for your contribution. Thank you for your report @damienrj \nPlease could you provided a short code snippet triggering the issue.. @arcenik Thank you for your PR. However we are currently working on a patch globally improving the SSH support, including a fix for #2248. ",
    "damienrj": "@jcsirot Sure\nDockerfile:\nFROM python:2.7.15-slim-jessie\nRUN touch script.py\nRUN echo \"raise Exception('test')\" >> script.py\nENTRYPOINT [ \"python\" ]\nCode:\npython\nimport docker\nclient = docker.from_env()\nclient.containers.run('test:latest', command='script.py', tty=True, stderr=True, stdout=True)\nOnly see the ContainerError\n```\nIn [19]: import docker\n    ...: client = docker.from_env()\n    ...: client.containers.run('test:latest', command='script.py', tty=True, stderr=True, stdout=True)\n    ...:\n\nContainerError                            Traceback (most recent call last)\n in ()\n      1 import docker\n      2 client = docker.from_env()\n----> 3 client.containers.run('test:latest', command='script.py', tty=True, stderr=True, stdout=True)\n/usr/local/lib/python2.7/site-packages/docker/models/containers.pyc in run(self, image, command, stdout, stderr, remove, **kwargs)\n    812         if exit_status != 0:\n    813             raise ContainerError(\n--> 814                 container, exit_status, command, image, out\n    815             )\n    816\nContainerError: Command 'script.py' in image 'test:latest' returned non-zero exit status 1:\n```\npython\nimport docker\nclient = docker.from_env()\nclient.containers.run('test:latest', command='script.py', tty=False, stderr=True, stdout=True)\nCan see the Exception: test\n```\nIn [21]: import docker\n    ...: client = docker.from_env()\n    ...: client.containers.run('test:latest', command='script.py', tty=False, stderr=True, stdout=True)\n    ...:\n\nContainerError                            Traceback (most recent call last)\n in ()\n      1 import docker\n      2 client = docker.from_env()\n----> 3 client.containers.run('test:latest', command='script.py', tty=False, stderr=True, stdout=True)\n/usr/local/lib/python2.7/site-packages/docker/models/containers.pyc in run(self, image, command, stdout, stderr, remove, **kwargs)\n    812         if exit_status != 0:\n    813             raise ContainerError(\n--> 814                 container, exit_status, command, image, out\n    815             )\n    816\nContainerError: Command 'script.py' in image 'test:latest' returned non-zero exit status 1: Traceback (most recent call last):\n  File \"script.py\", line 1, in \n    raise Exception('test')\nException: test\n```\n. ",
    "philipxyc": "It's my great honor, Thank you, too!. Okay, Thank you for point this out! I've changed it.. Sure, that was exactly what I wrote originally. But since we have to follow PEP8 standard, if written in this way, a new line is needed for the last word, like this:\n\nPersonally, I thought it made the code not that pretty, so I changed this sentence into a shorter expression with the same meaning. But it turns out that the new expression is more ambiguous, if  the situation in the screenshot above doesn't matter for you, I am glade to change the expression into this way.. Okay! I will change it. ",
    "gnthibault": "Thanks to a SO user, I discovered that the problem had more to do with the documentation:\nhttps://docker-py.readthedocs.io/en/stable/api.html#docker.types.Ulimit\nBoth hard and soft are optional, but using both of them alone yield an error (a different error though).\nBut setting both option to the same value yielded the expected result.\nIt would be nice to have a better documentation on this point, much like the command line interface:\nhttps://docs.docker.com/engine/reference/commandline/run/#set-ulimits-in-container---ulimit\nThank you. ",
    "xiaochenluo": "Multiprocess may be the cause. The problem is repeated today. This job used multiprocess too, no pytorch.\n. ",
    "hannseman": "\nWe can ship the change with 4.0, and the API change won't be an issue in that case.\n\nSounds good!\n@shin- and @ulyssessouza thanks for the review and merge.. ",
    "heckad": "A quick review showed that in other places it will be exactly container_id, but here container_id can also be a name of container. Perhaps need to rename docstring on 301 line in containers.py. Also, when you write the code, a hint appears with the names of the variables and it showed container_id. I went to Google and spent 5 minutes for finding a method that returns a container by name, but if the variable name would be container_name_or_id I would not waste time. The second principle of python is Explicit is better than implicit., becouse I made this pull request. Thank you.. So, merge or close? If we merge, it will be better to read.. What does it break?. Just out of curiosity, why do so?. ",
    "axiqia": "@shin- Thanks for your rely. Bellow is library verison.\n$ pip list | grep docker \ndocker           3.6.0   \ndocker-pycreds   0.4.0. ",
    "m1keil": "I believe this is mechanism for \"batching\" stats. \nBut I think it's a bit problematic since (I think) it's blocking? \n. ",
    "ianblenke": "Heh, yep, the tests caught it. Looks like I also did the split wrong, and have some whitespace before/after parenthesis that the tests are complaining about. Trying to fix those now as well. Thanks for the catch!\n. ",
    "stevenewey": "Added.\n. You're right, it's not necessary, I'll remove it.\n. ",
    "mohitsoni": "@shin- Good to know. Can you please tell how cgroup_parent and other host config arguments be passed ?\n. ",
    "kanzure": "probably better to use a timestamp here instead of \"current docker logic\"\n. live mutation of the os.walk output variable dirs is probably non-obvious and deserves a comment\n. ",
    "madjam002": "@aanand Should I make it so you just pass in the raw { Test, Interval, Timeout, Retries } object then?\nThe consumer could then be responsible for converting the duration string into a duration if necessary, and docker-compose could do the conversion from the cleaner { command, interval, timeout, retries } object then.\n. Cool, I'll do that then, thanks \ud83d\ude03 \n. host_config_version_error only appears to be used inside create_host_config, this is error is thrown in create_container_config?\n. I've updated the PR with some of the changes mentioned, I'm just in the process of making the subsequent changes in docker compose and writing unit tests.\n. ",
    "justincormack": "MAINTAINER is deprecated... best to remove from examples.\n. ",
    "huzhengchuan": "Thank you very much, I will update the Docstring and integration tests.. ",
    "Revolution1": "@utils.minimum_version('1.25'). and you may add some @requires_api_version('1.25') to your test. 'LayersSize' in client.version() ????. ",
    "emillynge": "typo :). ",
    "moshez": "This should be \n{py27,py33,py34,py35}-windows: windows\nand ditto for the other relevant lines.\nIf you're asking whether I feel bad already for giving you this advice: yes but I have no better advice :(. you probably want to put an install command here too, because otherwise things will break :(\n{py27,...}-{darwin,linux}: pip install {opts} {packages}. ",
    "greut": "No commit reviews / reviews then commit? This test doesn't look like any of the others.. ",
    "mtszb": "Yep, that name is more sensible. The updated pull request uses the uts_mode attribute name as suggested.. I have included a simple integration test, test_create_with_uts_mode, similar in style to the other container API tests already present.. Hmm... as far as I am aware of, it seems that the UTSMode attribute was already present when the HostConfig structure itself was moved to within the Container API type; that's why I did not include a version check for that attribute.\nThe actual version when this attribute was introduced seems to be around 1.7, but in that version the HostConfig structure was in runconfig.. ",
    "wk8": "Comes in handy to just run a test or two using -k (https://docs.pytest.org/en/latest/example/markers.html#using-k-expr-to-select-tests-based-on-their-name). "
}
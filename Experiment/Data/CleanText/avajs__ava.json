{
    "sindresorhus": "https://github.com/sindresorhus/ava/issues/31\n. :+1:\n. I don't want TAP output as default as it's quite limiting and annoying to work with. I definitely want to support it, but the default reporter should be something nice and simple. My thought was to have a hard-coded reporter and a TAP option, but no reporter option, as TAP has a gadzillion reporters and it might be a good idea not to reinvent the wheel. Thoughts?\n. Having to depend on the TAP output for our default reporter limits what we can do. I honestly see no benefit of doing that. We can easily support TAP too, but I would prefer our default reporter is more tightly coupled so we can make it really good. So good most people won't need another reporter.\n. Yup, let's build what we need first and make it stable. I've honestly never needed another reporter. We can add bloat like that later on like you said.\n. I can't remember why I didn't implement it. But tape doesn't either. So must be a reason. If we do decide to implement it, it should be really high. I'm so annoyed of having to up the limit in Mocha for everything. The most important goal of ava IMHO is speed and good defaults.\n. Let's punt on this for now until there's an actual need.\n. @jenslind It's supposed to skip the assertion, but right now it tries to skip the whole test, which is clearly wrong.\nGoing forward:\n- [ ] Change t.skip to only create a skipped over assertion: https://github.com/substack/tape#tskipmsg\n- [ ] Add ava.skip to skip over a test.\n. Yeah, I'm not totally sold on its merits either, but here's the reasoning I've heard:\n- Less diff noise.\n- It still shows up in the output, but as skipped. So you don't forget to unskip it.\n- https://github.com/substack/tape/issues/41\nAs for skipped assertions; they're better than commenting out as you won't have to change the t.plan() count when you want to activate the assertion.\nHappy to hear arguments either way.\n. I like .skip(), .warning(), .todo(). I think warning should just be an option to the normal test methods though. So it would work with both test() and test.serial().\nExample: test('foo, {warning: 'haalp'}, t => {});\nNot sold on .when(), .browser(), .node(). I feel those would be better as just if statements around the test. I can't think of situation I could have needed those. Browser/node is also premature as we don't even support browser usage yet. Consistent test suite is also a good argument against these.\n\n\n.critical()\n\nWe should rather provide a fail-fast flag for people that want to stop on the first failure (Please discuss in https://github.com/sindresorhus/ava/issues/48). You can use test.serial and put the critical tests first.\n\nWhat you described about .critical(), is exactly the same as .before() (it's already implemented).\n\n.before is meant for initialization, not for actual testing. See above.\n. I'm going to improve the before/after docs soon.\n. Status update: test.skip() was just implemented. You can follow test.only() status in #132. We'll defer the other modifiers to later as we have more important things to focus on at the moment. Contributions are of course always welcome if you want something to be done sooner ;)\n. I think we can use ANSI escape codes to wipe out the previously logged \u2714 [anonymous] line if only one test.\n. Yup, see the actual commit.\n. @mdibaiee I think it's related to this: https://github.com/sindresorhus/ava/blob/ff5eebba5d06ea22a1ed1ca5f76f33386bee8381/test/test.js#L245-L247\n. @mdibaiee Did you have a chance to look into it? No worries if not :)\n. Added a test for this, which seems to pass, so might have been fixed after the really old currently published version.\n. Go ahead ;)\n. @alubbe That will come when we add TAP support #27. You can then use any TAP-reporter.\n. @vdemedes :heart: Exactly what I was thinking.\n. An easy win and people will ask for it. Also makes it automagically work with async function when those arrive in ES2016.\n. I was thinking ES2015 support by default. Can't think of any reason not to want ES2015 when there's no setup cost.\n. No, we still need to support handling unplanned assert in async.\nhttps://github.com/sindresorhus/ava/blob/373e88d432748de75cf5e1b1f9aa44aa49daf37d/test/test.js#L245\nTape handles this by waiting for the callback and then checking those asserts too. We just exit right away when the assertion count matches planned.\n. Awesome! Thanks for working on this :)\nWould you mind adding something about it to the readme too?\n. Why did you remove the test? Can you fix the merge conflict?\n. Can you add back the test? We need a test to make sure it's working ok.\n. Looks good. Thank you @andrepolischuk :)\n. :beers: \n. Maybe open an issue on Browserify about shimming setImmediate?\n. I'll give it a week, but I doubt anything will happen there... \nI guess I can add https://github.com/sindresorhus/set-immediate-shim if that is the only thing blocking it from being used in the browser.\n. @Qix- It's faster than setTimeout.\nhttp://www.nczonline.net/blog/2013/07/09/the-case-for-setimmediate/\n. Browserify should already shim process, so sounds like either a Browserify bug or an issue with your build setup.\n. https://github.com/defunctzombie/node-process/issues/35\nI guess we can make squeak use console.log/console.error instead in the browser. PR welcome there.\n. Yeah. I guess we could replace the logger.js file with a custom logger-browser.js using the Browserify replace feature.\nAnd process.exit = window.close;. https://github.com/Jam3/hihat#basic-examples\n\nThe process will stay open until you call window.close() from the client code.\n. Happy to recommend hihat in the readme when we get this working :)\n. > What it would be better is to patch squeak (or use another library)\n\nAgreed.\n\nso that it falls back to console and still maintains all the right logging levels.\n\nHow would it do that? Prefix the console output with logging level?\n. Shouldn't be hard to patch squeak to use console.log when used in the browser though. I think that would be the best and less hacky solution.\n. I have no idea anymore what it would take to make it run in the browser. Browser support comes with an immense amount of overhead, both in edge-cases and user support. I want for us to finish the core of AVA before committing to something like this. We already have more than enough overhead with Babel.\nThat being said, I don't see any reason we can't discuss potential solutions, and anyone is free to look into how to achieve this. It might be as simple as replacing lib/fork.js with a shim for the browser that just communicates directly with lib/child-worker.js (browserify supports file overrides for the browser), or it might require a lot more. I know this isn't what people were hoping for, but we have to prioritize.\n. I have no idea anymore what it would take to make it run in the browser. Browser support comes with an immense amount of overhead, both in edge-cases and user support. I want for us to finish the core of AVA before committing to something like this. We already have more than enough overhead with Babel.\nThat being said, I don't see any reason we can't discuss potential solutions, and anyone is free to look into how to achieve this. It might be as simple as replacing lib/fork.js with a shim for the browser that just communicates directly with lib/child-worker.js (browserify supports file overrides for the browser), or it might require a lot more. I know this isn't what people were hoping for, but we have to prioritize.\n. @phuu The test runner lib/runner.js is decoupled and can be used independently, see its test and how it's used in index.js.\n. @phuu The test runner lib/runner.js is decoupled and can be used independently, see its test and how it's used in index.js.\n. Done. Yes you can. Updated the readme about it: https://github.com/sindresorhus/ava/commit/3deba5189186ce73ef6a7bc18c4044f713c63eed\nIt won't work with planned assertions though. For that, we'll need an AVA method that hooks the assertion methods in the assertion module you supply so that it can do the assert counting. Shouldn't be that hard, but not top priority.\n. How to do what? With assert counting is not possible yet as outlined above. Without assert counting is documented in the readme as already mentioned.\n. Please read the discussion in https://github.com/sindresorhus/ava/pull/49 before commenting.\n\nWe should support any assertion library, but have built-in \"adapters\" for the most popular ones. By \"builtin adapter\", I mean users can just pass the instance (e.g. chai) and have it just work.\n. People can already use their favorite assertion module with AVA. It just won't work with planned assertions. That being said, I almost never use planned assertions anymore, now that we have async/await.\n. People can already use their favorite assertion module with AVA. It just won't work with planned assertions. That being said, I almost never use planned assertions anymore, now that we have async/await.\n. :dancer: \n. It was closed by a mistake. Planned for 0.8.0.\n. This will probably be easier when #1 is implemented. AVA will then run tests in separate sub-processes.\n. No: https://github.com/bcoe/nyc\n. We want AVA to work with it, but I haven't tested. Not a top priority right now as we're focusing on finishing the API and performance. Happy to receive some help debugging/PR/etc, though.\n. @Qix- We're now forking each test file. Any recommendation on how we should handle this?\n. @Qix- We're not using threads (there are no threads in JS), but subprocesses.\n. fork in the Node.js sense is just a spawned Node.js process with an IPC channel. So it's just a normal child process with normal process signals. I think the best solution here is to just do nothing. If a test file has a SIGSEGV it will just fail the test file, not all the test files.\n. @Qix- What would you like to keep?\nCurrently it looks like:\nAssertionError: false === true\n    at Test.(anonymous function) [as assert] (/Users/sindresorhus/dev/ava/lib/test.js:49:14)\n    at Test.fn (/Users/sindresorhus/dev/acosh/test.js:8:4)\n    at Test.run (/Users/sindresorhus/dev/ava/lib/test.js:78:18)\n    at Runner.<anonymous> (/Users/sindresorhus/dev/ava/lib/runner.js:39:8)\n    at Immediate.immediate._onImmediate (timers.js:418:16)\n    at processImmediate [as _immediateCallback] (timers.js:367:17)\nWith this PR, it looks like:\nfalse === true\n    at Test.fn (/Users/sindresorhus/dev/acosh/test.js:8:4)\nAll it hides is useless stack info coming from AVA and Node.js core libs.\n. Can you provide some code that result in a \"full stack\"? I definitely agree it should not remove that.\n. @mdibaiee No need. I squash one merge. Easier to see individual changes the current way.\n. Looks good to me now :)\n@Qix- ?\n. Minimal vs verbose is a hard one, but from experience with debugging a lot of failed assertions I have to agree with @Qix-. I often have to dig deeper than just the test file. I'm willing to sacrifice visual beauty in favor of practicality here. We should preserve any stack info referencing user code as @Qix- has outlined above. I'm allergic to options and I'd rather go with a good default. We can always change it in the future if it turns out to be the wrong choice.\n@mdibaiee Would you be willing to update your PR even though you seem to strongly disagree?\n\nError.stackTraceLimit = Infinity;\n\n:+1: \n. > but since the introduction of error-ex we might run into exceptions with non-standard lines that start with something other than at; we shouldn't filter those out either.\n:+1: I've seen other error modules add stuff too. Better to be on the safe side.\n. > Use @Qix- 's RegExp?\nYes\n. Landed! Thanks for the PR @mdibaiee. Keep'em coming ;)\nAnd thanks for the review @Qix-.\n. Was thinking about doing something like this, but instead only show the time when it passes some kind of threshold, as the number is really only useful for identifying slow tests. Thoughts?\n. If so, what kind of threshold makes sense? Not interested in adding an option to change it, so it needs to cover most use-cases.\n. Looks very good @vdemedes! Can you add a test?\n. Here's how it looks:\n\n. Excellent work @vdemedes. Keep'em coming! ;)\nMore than happy to receive addition suggestion on how AVA can be better in form of issues or PRs.\n. I don't understand how this is related to a --debug flag? Right now it doesn't use any child-processes, but just require's the test files.\nAs for sharing config with sub-processes, that's better discussed in #1. But tl;dr, child_process.fork and message passing.\n. It could just read argv: process.argv.indexOf('--debug') !== -1. But more likely the CLI will pass it as an option to the lib/test.js interface.\n. @Qix- Why? It will make no difference if you've already hard-coded your paths with __dirname.\n. > But would this confuse people who just learned require(./) and fs.readFile(./) as the path are relative to different base?\nThat's not correct. If you start the node script from the file location they're the same. The point is that running a test is kinda like doing node test.js from the test file location.\n. @jamestalmage I agree. In hindsight, it was a bad idea with good intentions. Too bad it didn't show its downsides until much later. I've personally been bitten by this on multiple occasions too and seen some problems with it in the ecosystem. I think we should reconsider. I've never seen anyone other than me use it anyways (I've looked at a gadzillion open source AVA test files).\nSome known problems this feature caused:\n- https://github.com/sindresorhus/ava/issues/175\n- https://github.com/istarkov/babel-plugin-webpack-loaders/issues/41\n- https://github.com/sindresorhus/ava/pull/531 (had to absolutify paths from NODE_PATH becuase of the cwd change)\n- https://github.com/sindresorhus/ava/issues/707 (would resolve this issue)\n\n@vdemedes @novemberborn @sotojuan @jedrichards @istarkov @alexbooker @MarkHerhold @KidkArolis\n. Ok, we're going to do this.\n. We might need some codemods for this change: https://github.com/jamestalmage/ava-codemods/issues/19\n. It's blocked on having a migrate script: https://github.com/avajs/ava/issues/32#issuecomment-218998130 The change itself is simple.\n. Yeah, let's just do the change. \nIt's hard to do a codemod for this correctly anyway, and I don't think it's important enough to force us to continue having surprising behavior for new users.\n@avajs/core ?\n. @wmertens https://twitter.com/slicknet/status/782274190451671040\n. I don't think there should be. The public interface should be as simple and config-less a possible.\nThe flags are something we will handle internally, not in the public API. It will be passed to the Runner which will handle the changes, like making everything serial for the --debug flag.\n. Yes, we can use something like has-flag.\n. Built-in Babel support is about to land in https://github.com/sindresorhus/ava/pull/23. You'll then be able to use ES2015 and JSX without doing anything extra. You can of course supply your own Babel version if wanted.\nHowever, I'm not interested in providing support for custom compilers. AVA is opinionated.\n. Babel support landed in AVA 0.2.0.\n. Babel support landed in AVA 0.2.0.\n. Looks superb to me. Can you fix the merge conflict? :D\n. Excellent! Thanks again @vdemedes for this awesome contribution :)\n\n. What does that entail? Can you show an example test of how it would look? What changes would need to be made to AVA?\n. @vdemedes Still not sure what's needed to do in AVA. Would AVA have to have co as a dependency? With the recent builtin ES2015 support, wouldn't generators just work out of the box? AVA already support promises, which is what co returns. Sorry for the noobishness, but I've never used generators.\n. Hmm, wouldn't it be better to enable the async/await Babel transformer? async/await is the future of async programming, not hacked up generators.\n. Alright. PR welcome :)\n. @Qix- Yes, see: https://github.com/sindresorhus/ava/issues/45#issuecomment-139216416\n. > Use mz module when promises are needed for core modules (e.g. fs)\nYou can just use pify for that. See pify.all().\n. You can also remove pinkie-promise from the tests and devDependency and use Bluebird there too.\n. The code base is so much cleaner with promises. Excellent work @vdemedes :)\nGonna give it a more thorough review tomorrow. Need sleep now.\n. I tried running this on pageres and got an error:\nUnhandled rejection TypeError: Cannot read property 'testCount' of undefined\n    at exit (/Users/sindresorhus/dev/ava/index.js:52:11)\n    at tryCatcher (/Users/sindresorhus/dev/ava/node_modules/bluebird/js/main/util.js:26:23)\n    at Promise._settlePromiseFromHandler (/Users/sindresorhus/dev/ava/node_modules/bluebird/js/main/promise.js:503:31)\n    at Promise._settlePromiseAt (/Users/sindresorhus/dev/ava/node_modules/bluebird/js/main/promise.js:577:18)\n    at Promise._settlePromises (/Users/sindresorhus/dev/ava/node_modules/bluebird/js/main/promise.js:693:14)\n    at Async._drainQueue (/Users/sindresorhus/dev/ava/node_modules/bluebird/js/main/async.js:123:16)\n    at Async._drainQueues (/Users/sindresorhus/dev/ava/node_modules/bluebird/js/main/async.js:133:10)\n    at Immediate.Async.drainQueues [as _onImmediate] (/Users/sindresorhus/dev/ava/node_modules/bluebird/js/main/async.js:15:14)\n    at processImmediate [as _immediateCallback] (timers.js:371:17)\nIt seems the stats and results arguments in the exit function are undefined.\n(Can you add a regression test for this when fixed?)\n. Using exec is fine.\n. Yeah, let's use each-async for now. Add a TODO about improving it though.\n. Awesome! I'm really happy about this change. Super work @vdemedes :D\nWould you happen to be interested in joining the project? I think you have a lot of good to bring to AVA.\n\n. The thinking is that you should only write your tests in ES2015. It's time. We finally have a good transpiler (Babel). You can of course write your tests in ES5, but I want to optimize for the future, not the past. Providing this out of the box without requiring the user to do anything is one of the things that is going to set AVA apart.\nI agree Babel is a large dependency, but that's less of an issue for devDependencies. I'm also going to work on getting down the size of the Babel package. Help welcome there ;)\nI see that the Babel package currently includes a lot of junk you don't need in Node. The package is 13.6 MB. I think with some tweaks we could get it down many MBs. Removing browser specific stuff brings it down to 9.8 MB (Maybe Babel should have a Node specific package without those?). There's also some potential for better deduping. I'll open a Babel issue about that when I have some more time to look into it.\n. Babel 6 is out and it's smaller, and it will be even smaller with https://github.com/babel/babel/pull/2659/files and https://github.com/babel/babel/issues/2635. Closing this, but I'm going to continue working on bringing down the dependency size.\n. > And I'd also change import test from 'ava' to var test = require('ava') to make things completely straightforward. I think, not everyone is familiar with ES6 module system.\nIf they're not familiar with the module system they're probably not familiar with arrow functions either. I'd rather go all in with the ES2015 syntax than awkwardly something in the middle.\n. Bluebird is faster than native (i know...) and includes some useful utilities. Also no point in including multiple promise libs. I'm using pinkie-promise for reusable modules, but in larger things like AVA bluebird is useful, same as using lodash is more useful than the native, if you can afford the added dependency bloat.\n. @floatdrop I'm btw slowly migrating my modules over to promises using pinkie-promise ;)\nhttps://www.npmjs.com/browse/depended/pinkie-promise\n. Nice catch.\n. Since we're already depending on Bluebird, I guess we could use the bluebirdCoroutines transformer.\n. I'd like to keep this open for a while so people can see it. I think power-assert is really cool and I actually considered including it in AVA early on. The reasons I didn't were the compile step (which is not a problem now that we include Babel by default) and the result, while being very informative, is a bit noisy. foo === bar is a lot clearer and faster to grasp. Is there any way to simplify the output?\n. Instead of:\n# test.js:26\n  t.ok(a.name === 'bar')\n       | |    |         \n       | |    false     \n       | \"foo\"          \n       Object{name:\"foo\"}\n  --- [string] 'bar'\n  +++ [string] a.name\n  @@ -1,3 +1,3 @@\n  -bar\nI think I would want something like:\n# test.js:26\n  a.name === 'bar'\n     |\n     |\n   \"foo\"\n. @twada Any updates? Would really like to use power-assert to improve AVAs assert output.\n. @twada Yes, looks very good :) I'm fine with this for now. Would be awesome if you would integrate it into this PR and fix the merge conflict.\n. @twada Can you review?\n. @twada Can you explain what the modifyMessageOnRethrow option does? Should we enable it?\n. Landed. Woop woop. Great work @uiureo. And thanks for reviewing @twada :)\n. Yup, we should add a tip about using nyc, which wraps Istanbul with childprocess support.\n. Looks good to me now. Awesome work on this @vdemedes!\n\n. What flag name do you prefer? bikeshedding\n. I don't really care what other test runners do. AVA exists because of those. I want something succinct as flag name.\n. I don't really care what other test runners do. AVA exists because of those. I want something succinct as flag name.\n. @Qix- Most assert libs doesn't have a .plan(). That's usually part of the test runner. What asked here is how to mixin should methods, since they're chained on the native prototype of objects/string/etc. The native assert module works here, since it's only methods on an object. But it doesn't work for more advanced cases, like should. Maybe we should also have an adapter interface of some kind that can handle more advanced cases. How would that look like though?\n. @Qix- Oh, I didn't realize. Yeah, that makes things easier. But there still might be other assertion libs where the easy way doesn't work. Maybe we should add support for the most popular assertion lib, so that you can just pass in the main export, and we'll handle whatever's nessecary to decorate it.\n. The intention was to decorate the assert methods with an assertion counter, not mix them into the test context. I guess we could expose a t.countAssert() (better name?) or something too.\n. > should(should) is redundant. What else would you pass to should()?\nIt's to be explicit. If we only have ava.setupAssert, we would have to detect should. Is that possible? If yes, how? Having only one setup method would indeed be nicer.\n\nIf you take that out, then you have to somehow enforce should be installed before calling ava.should(), unless you package all of them together, which... don't. :)\n\nNot sure I understand what you mean, but this is only for mixing in the assert counter into the assert lib, not the other way. After calling ava.setupAssert(should), you would just call should normally, not through AVA.\n\nI want to get this moving. What's the minium thing we can do first? Maybe ignore the should/except stuff and just expose a generic ava.countAssert() and ava.setupAssert().\n. Sorry @jenslind. Seems we aren't going to be able to reach a good solution for this right now. Closing for now, but happy to open it again when we have a decision on how to handle custom assertion libs.\nLet's continue the discussion in #25.\n. :+1: We already know which files are the test files, so we can just blacklist anything that is not passed into AVA.\n. :+1: \n. I still think this is better fixed in squeak: https://github.com/sindresorhus/ava/issues/24#issuecomment-140687957\n. // @mattdesl\n. @mattdesl Good point. So what should we do here?\n. @kevva said he would make squeak browserifyable and this PR needs to be updated to only include the exit change.\n. @kevva said he would make squeak browserifyable and this PR needs to be updated to only include the exit change.\n. Might be more, though, as we recently introduced parallism, which forks the processes. Not sure whether browserify handles that or if we need to do some special casing. Help welcome. Our main focus is elsewhere.\n. Might be more, though, as we recently introduced parallism, which forks the processes. Not sure whether browserify handles that or if we need to do some special casing. Help welcome. Our main focus is elsewhere.\n. Closing in favor of #97.\n. I'm allergic to options, but I'm happy to make the normalizing smarter. Any other naming patterns you can think of?\n. @Qix- That won't look very pretty for test titles with spaces, which most of mine are.\ntest('unicorn rainbow cake') => foo.bar.unicorn rainbow.cake\nProbably better to use a non-common symbol, so not to be confused when the user has a test like: test('foo - bar') too.\nHow about foo \u203a bar \u203a unicorn rainbow cake?\n. @Qix- Yup, we can use figures.pointerSmall.\n. Tests are failing.\nCan you add a section to the readme with usage example? Like (and below): https://github.com/sindresorhus/ava#es2015-support\n. Oh, and add it here: https://github.com/sindresorhus/ava#why-ava Below \"Promise support\"\n. Awesome!\n. Maybe we should enable async/await, but document that it's still experimental?\n. You also need to enable the async/await transformer: http://babeljs.io/docs/advanced/transformers/other/bluebird-coroutines/\n. @vdemedes Any thoughts on how to best test to ensure that this is working correctly? Meaning it only transpiles the tested files and nothing more.\n. By whitelist I meant only.\n. > Does passing a string to only, instead of a pattern, works?\nIt should in theory as the prop is being arrayified, but we need a test for the behavior anyways.\n. > what about for projects that are using babel to transpile the rest of their app (e.g. by running their code with babel-node), wouldn't this force babel to only transpile the test files?\nWe already limit that in master. This PR only changes it to only ignore the path explicitly instead of using a ignore pattern. You would just do whatever you with your existing test framework that doesn't support ES2015 builtin, not sure what that is?\n. Shouldn't be very hard. Not sure we want this, though. I've honestly never needed it. You could achieve the same thing with multiple files (which would also be beneficial for parallelism). Do you have a real scenario you could link to where this was useful?\n. I still feel it's an anti-pattern and nobody has convinced me otherwise (hint hint). Having tests in separate files instead of nesting has the benefit of cleaner test files and parallelism.\n@vdemedes @kevva Thoughts?\n. Probably related to https://github.com/sindresorhus/ava/issues/15.\n. The prefixing doesn't seem to work when specifying a direct path:\n```\n$ ava test/foo/http.js\n\u2714 test title\n```\nExpected:\n```\n$ ava test/foo/http.js\n\u2714 foo \u203a http \u203a test title\n```\n. Oh right. That makes sense. If only one file, it should not show the path.\n. Super work @vdemedes. I really like this feature! :)\n. Super work @vdemedes. I really like this feature! :)\n. \n. \n. Can you enable this test: https://github.com/sindresorhus/ava/blob/5dc0873b74d59a3f3cc33a36020c78cb305153aa/test/test.js#L262 and add some more?\n. :+1: :tada: \n. This is now PR welcome. Should use https://github.com/paulmillr/chokidar.\n@tomazzaman has indicated in #458 that he's interested in working on this.\n. @vdemedes I'm afraid of that too, but I'm thinking we add it undocumented, dogfood it, and only document it when we feel it's stable enough and we're ready to take on the addition support burden. And many people have told me watch is a big reason for them to stay with Mocha, which makes me think watch is part of providing a great experience.\n. @tomazzaman Thanks. The more usage feedback we get on issues, the easier it is for us to make informed choices :) Feel free to comment opinions on any other issue. You might also like https://github.com/sindresorhus/ava/issues/115.\n. @tomazzaman I didn't mean you should do it. Just that you might find the idea interesting ;)\n. This needs to be taken into account: https://github.com/babel/babel/pull/3307\n. :+1: This looks very good! Pretty much exactly how I would have done it :)\n. :+1: This looks very good! Pretty much exactly how I would have done it :)\n. > There is one difference though, if beforeEach hook fails, tests don't stop (unlike before).\nWhat's the reasoning for that? Since the test depend on the beforeEach hook, and if that fails silently, the test might end in a false positive.\n. > There is one difference though, if beforeEach hook fails, tests don't stop (unlike before).\nWhat's the reasoning for that? Since the test depend on the beforeEach hook, and if that fails silently, the test might end in a false positive.\n. Yeah, I would personally want it to fail. Failing hooks should be the same as a failing tests. Hooks are really just implicit function calls.\nImagine:\njs\ntest(t => {\n  before();\n  t.pass();\n  after();\n});\n. Yeah, I would personally want it to fail. Failing hooks should be the same as a failing tests. Hooks are really just implicit function calls.\nImagine:\njs\ntest(t => {\n  before();\n  t.pass();\n  after();\n});\n. One tiny inline comment and needs docs, otherwise ready to be merged.\n. One tiny inline comment and needs docs, otherwise ready to be merged.\n. Landed. Woohooo! The feature set is really coming together now.\n\n. Landed. Woohooo! The feature set is really coming together now.\n\n. When we're ready to do another release. Probably in a few days. As with every other OSS project, the readme is master, if you want to see docs for a specific release go to the git tag or the npm page.\n. > For t.throws() to catch your error, it needs to check if promise rejects and that's not supported at the moment.\nCould be useful, though. Let's leave this as a feature request?\n. > For t.throws() to catch your error, it needs to check if promise rejects and that's not supported at the moment.\nCould be useful, though. Let's leave this as a feature request?\n. In got, we have to repeat t.fail('Exception is not thrown'); in a lot of tests, which is pretty sub-optimal.\n. Anyone wanna try implementing this?\n. This is just inherited from the Node.js assert module.\nCan you bring this up on the Node.js issue tracker first?\n\nThat assert method is pretty irrelvant with promises though.\n. @alebelcor If you have any good suggestions for a better name we might consider adding an alias, but I don't really care much tbh, as the future is promises and async functions which makes this assert method moot.\n. Related: https://github.com/nodejs/node/issues/3398\n. @mralexgray This is completely irrelevant to the issue, which is concerning ES2016 async functions, not async in general. Please don't use unproductive words like \"epic fail\". We have a passing test for t.plan with an async assertion. It, however, seems like your success function is never called. Happy to be proven wrong with a failing test, though.\n. We're using Bluebird, yes, but that's an implementation that shouldn't be dependent upon.\n. Looks good to me. Thanks @floatdrop :)\n@vdemedes You? Anything else we should test in the fork module?\n. We're not and tbh it's annoying that we'll probably have too. Why can't the system just pool the load for us instead of we doing it manually? I hate computers so much... Let's experiement with what kind of number is optimal. I'm thinking (require('os').cpus().length || 1) * 2;.\n. We should also look into why forking is so slow. Maybe there's something we can optimize there.\n. Node.js isn't inherently slow to spin up, it's requiring things that are slow. Babel caches the compilation, so that should only be an issue on the a cold load.\n. We could use https://github.com/jaguard/time-require to measure if any of our require's are slowing down the startup. Anyone interested in looking into this?\n. I ran time-require on a forked test file and here are the results:\n#  module                           time  %\n 1  has-color (../...lor/index.js)   20ms  \u2587 2%\n 2  chalk (../node...alk/index.js)   26ms  \u2587 3%\n 3  ./util.js (../...main/util.js)   12ms  \u2587 1%\n 4  bluebird (../n.../bluebird.js)  107ms  \u2587\u2587\u2587\u2587 11%\n 5  esprima (../no...a/esprima.js)   78ms  \u2587\u2587\u2587 8%\n 6  array-foreach...ach/index.js)    15ms  \u2587 1%\n 7  escallmatch (....tch/index.js)  109ms  \u2587\u2587\u2587\u2587 11%\n 8  ./lib/decorato...decorator.js)  113ms  \u2587\u2587\u2587\u2587 11%\n 9  define-propert...ies/index.js)   12ms  \u2587 1%\n10  empower (../no...wer/index.js)  127ms  \u2587\u2587\u2587\u2587 13%\n11  ./strategies (...trategies.js)   14ms  \u2587 1%\n12  stringifier (....ier/index.js)   18ms  \u2587 2%\n13  esprima (../no...a/esprima.js)   53ms  \u2587\u2587 5%\n14  ./traverse (...../traverse.js)   59ms  \u2587\u2587 6%\n15  ./javascript/d...ompressed.js)   29ms  \u2587 3%\n16  googlediff (.....iff/index.js)   29ms  \u2587 3%\n17  ./udiff (../no...lib/udiff.js)   31ms  \u2587 3%\n18  ./lib/create (...ib/create.js)  119ms  \u2587\u2587\u2587\u2587 12%\n19  power-assert-f...ter/index.js)  120ms  \u2587\u2587\u2587\u2587 12%\n20  ./assert (../n...ib/assert.js)  289ms  \u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 29%\n21  ./test (../nod.../lib/test.js)  292ms  \u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 29%\n22  ./lib/runner (...ib/runner.js)  400ms  \u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 40%\n23  https (https)                    27ms  \u2587 3%\n24  ./lib/_stream_..._readable.js)   17ms  \u2587 2%\n25  readable-strea.../readable.js)   26ms  \u2587 3%\n26  once (../node_...once/once.js)   13ms  \u2587 1%\n27  end-of-stream...eam/index.js)    15ms  \u2587 1%\n28  duplexify (../...ify/index.js)   44ms  \u2587\u2587 4%\n29  ./_stream_read...eam_readable)   22ms  \u2587 2%\n30  ./lib/_stream_...am_duplex.js)   23ms  \u2587 2%\n31  readable-strea.../readable.js)   35ms  \u2587\u2587 3%\n32  read-all-strea...eam/index.js)   39ms  \u2587\u2587 4%\n33  node-status-co...des/index.js)   18ms  \u2587 2%\n34  ../ (../index.js)               361ms  \u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 36%\nTotal require(): 327\nTotal time: 1s\n1 second is way too much for just requiring dependencies. We should look into optimizing that somehow.\n. We could also cache the \"require resolving\", so every test files doesn't have to do that.\nMaybe by using https://github.com/bahmutov/cache-require-paths by @bahmutov.\n. Node.js 5.2.0 is also ~45% faster to startup! :) https://twitter.com/rvagg/status/674775383967858688\n. > I think the biggest improvement will come from moving Babel to the main thread.\nDefinitely. Was just looking for other minor wins.\n. Agree. Ignore that.\n. // @silvenon @floatdrop @SamVerschueren @kevva \n. Looks good. I'll add some docs tomorrow.\n. Oh, damn... Although, in the next version of AVA we're spawing each test file in it's own process, so we'll have to explicitly include it then. Try using AVA master as dependency for now. Example: https://github.com/sindresorhus/got/blob/f4cf6c7b44d047d505ed1fdf3a4e361b743bdfc2/package.json#L64\n. @LinusU Maybe just use tape for testing that module then.\n. Superb!\n\n. Looks good :)\n. @blesh Can you open an issue/PR on async-done?\n. @blesh Not really. We just went with adding support for it directly in AVA.\n. You should explicitly depend on Babel in your own code. The AVA Babel thing is just for tests.\n. Sounds like a bug.\n. Super! :)\n. That gives me:\n```\n  ...................................................................................................../Users/sindresorhus/dev/ava/test/test.js:763\n    throw new Error('blah');\n    ^\nError: blah\n    at Test. (/Users/sindresorhus/dev/ava/test/test.js:763:8)\n    at Test.bound [as _cb] (/Users/sindresorhus/dev/ava/node_modules/tape/lib/test.js:61:32)\n    at Test.run (/Users/sindresorhus/dev/ava/node_modules/tape/lib/test.js:77:10)\n    at Test.bound [as run] (/Users/sindresorhus/dev/ava/node_modules/tape/lib/test.js:61:32)\n    at Immediate.next [as _onImmediate] (/Users/sindresorhus/dev/ava/node_modules/tape/lib/results.js:70:15)\n    at processImmediate [as _immediateCallback] (timers.js:368:17)\n```\nI'm honestly not sure what you're trying to prove with that, though. As we're using tape to test AVA. So that has nothing to do with AVA.\n\nAs for the second comment, that's probably because we limit where Babel would compile.\n\nI'm still unsure what the point of this issue is?\n. Using CLI flags will make it not co-operate with the editor plugins and other tools.\n. I don't really see the need for this tbh. Hopefully ESLint will soon be switching to Acorn, which will give us better ES2015+ compat by default.\n. You have to use nyc as AVA spawns the test files.\n\na code coverage tool built on istanbul that works for applications that spawn subprocesses. - nyc\n\n\nI've been meaning to document this, but I wanted to ensure it works correctly with nyc before doing so.\n. Oh, I just noticed you are using nyc. Do note that got is using master: https://github.com/sindresorhus/got/blob/f4cf6c7b44d047d505ed1fdf3a4e361b743bdfc2/package.json#L64 Try that.\nWe plan on doing another release in a few days.\n. Keeping this open so I don't forget to document it after the new release.\n. @davidchase Can you share something I can test?\n. @davidchase nyc supports the Istanbul config. Like: https://github.com/vesln/todo/blob/master/.istanbul.yml (I couldn't find the actual docs for that).\n. @davidchase Open an issue on nyc. I'm sure they'll happy to receive feedback on what needs improving ;)\n. Sorry about that. This was fixed in AVA 0.3.0. It now only transpiles the test file and not required dependencies.\n. test(t => {\n    const foo = 'foo';\n    t.ok(foo.indexOf('bar') !== -1);\n    t.end();\n});\nt.ok(foo.indexOf('bar') !== -1)\n           |\n           -1\n@twada Any way we could show the value of foo here too? I think that could be useful.\n@vdemedes What do you think?\n. @twada \nCurrently this:\ntest(t => {\n    const b = 'bar';\n    const c = 'baz';\n    t.is(b, c);\n    t.end();\n});\nOutputs:\nt.is(b, c)\n       |  | \n       |  \"baz\"\n       \"bar\"\nIs there any way to have it output the old simple output when using direct inputs? Meaning no expressions.\nOld simple output:\n'bar' === 'baz'\nThat output is faster to look at.\n. @twada @uiureo I've documented the enhanced asserts. Would appriciate your review. Happy to change anything. https://github.com/sindresorhus/ava/commit/89219ce6d64e1b4117c94a712dc96e8e8a37e661?short_path=0730bb7#diff-0730bb7c2e8f9ea2438b52e419dd86c9 :)\n. I know I'm asking a lot, but I would like the following not to use power-assert, since it's just direct input.\njs\ntest(t => {\n    const b = 'bar';\n    const c = 'baz';\n    t.is(b, c);\n    t.end();\n});\nBut this should use power-assert since it's an expression and could benefit from the power-assert output.\njs\ntest(t => {\n    const b = 'bar';\n    const c = 'baz';\n    const d = 'faz';\n    t.is(b + d, c);\n    t.end();\n});\n. It wouldn't nessecarily have to be turned off, could maybe just change the renderer to not show diagrams when Identifier or Literal is passed. This is just a nitpick/nicetohave, though.\n. @SamVerschueren Looks like https://github.com/sindresorhus/ava/issues/113.\n. Yup, can be closed in favor of https://github.com/twada/power-assert-renderers/issues/3.\n. @schnittstabil The programmatic API is meant for running the tests, not be used in the test file itself.\n. @timoxley Yes, we intend to, but shouldn't be default as it will (and did) cause subtle bugs.\nFollow: https://github.com/sindresorhus/ava/issues/111\n. :+1: \n. Users can just use $ time ava.\n. Awesome! Glad we could get this in :)\n. Confirmed.\n. The actual stored props should be used, but unsure why they're not. Maybe because we overwrite the stack in https://github.com/sindresorhus/ava/blob/04b3db5d96d6530d6b03c5f8f4c915dea16cd125/lib/test.js#L159.\n. Does the caching still work with this solution? Since Babel is relatively slow, the caching is very important.\n. @floatdrop Maybe you could either extract this into a separate module we can depend on or do a PR on Babel to expose it? \n. Doesn't look like co has much activity, so I don't think we should wait on that tbh.\n. Why do we even depend on co again? Can't Babel+Regenerator do this for us?\n. > Why don't we use @floatdrop's fixed fork for now, until they merge his fix into master?\n:+1: \n. > okay, but need to do some performance tests - I wonder, how fast cache file will fill up and parsing time eat all the benefit.\n@floatdrop I'm ok with merging this now if you commit to looking into caching later? Compilation is a lot larger overhead than just reading in a cached file from disk. I do think we can do caching better than Babel itself, though. Mainly using separate cache files for each test file (Babel uses one large file) and a LRU cache.\n. > I'm started to use ava in some work related projects\n:heart_eyes: \n. This looks good to me. @vdemedes Can you take a final look?\n. Thanks again @floatdrop :)\n. > before/after can have titles and display them only when they exist. If before/after hook does not have a title, don't display it.\nWhy? In what cases is that needed or useful?\n\nIf any of the supported hook methods fail, display it like usual: x [anonymous]: error.\n\nIf a beforeEach/afterEach hook fails, we could show something like: beforeEach for 'test title' failed\n. > I was thinking, that having titles for beforeEach/afterEach will pollute the output, but titles for before/after won't do much noise. That's the only reason ;)\nI figured, but I'm more interested in use-cases for when titles would be useful in before/after? If there are good use-cases, we might want to document that too.\n. You could also look at the line number and just go to it in the test file, though.\n. How is not the line number the quickest way? With the line number you can just go directly to that line using your editor shortcut. With the title you have to search for the correct hook.\n. Keep title support for all hooks, but only show it on failure. I realized it can also be beneficial to have titles on them for code readability in the test files if you have many hooks. For the beforeEach/afterEach also show the connected test. (Note to self: This behavior also needs to be documented).\n@schnittstabil Not a big fan of nested-error-stacks. It creates unparsable stack traces. We can still include the test title in the beforeEach/afterEach error.\n. Perfect.\n. @schnittstabil Good catch. That totally slipped my mind.\n@vdemedes Maybe we should use a property on t for holding the context instead then? t.context or t.data?\n. Oh, right. I guess all we have to do is revert https://github.com/sindresorhus/ava/pull/106/files#diff-dd2e881af3311f834199c7b515cff0c7R109 then.\nMaybe we should make t.context non-writable, so people don't accidentally do:\njs\ntest.beforeEach(t => {\n    t.context = 'foo';\n    t.end();\n});\n. Yeah, I think it would be nicer if the user could use the context for whatever they wanted, it being an object or something else.\n. Pageres master with AVA 0.3.0 works fine for me on OS X and Ubuntu. Can you check whether you have frozen phantomjs processes? Can you try running with $ ava --serial?\n. That's because AVA no longer transpiles required files. That was a nasty bug. It's already fixed in Pageres. Just pull down latest master.\n. See https://github.com/sindresorhus/ava/issues/9.\n. // @vdemedes @schnittstabil\n. @schnittstabil I didn't put it there, but I assume it's so that even tests without hooks can have a context, for consistency. @vdemedes Correct?\n. Implicitly transpiling dependencies in 0.2.0 was a bug that was later fixed 0.3.0. Transpiling dependencies by default is both surprising and slow. I'm open to considering an option for enabling this, but it will not be by default.\n. That won't work as we spawn the test files. Just use AVA 0.2.0 until we've figured out how to deal with this. Alternatively use a fork where you've removed this line.\n. What should the option be named?\n. @floatdrop Any thoughts as to how we should solve this now that #103 has landed?\n. We could set the execPath, when we fork a test file, to the babel-node executable instead of vanilla node.\n. @theaqua This issue is still open, so no. @floatdrop has already provided a workaround if you can't wait.\nTo move this issue forward, come up with a good name for a CLI flag to enable it.\n. @sparty02 Sure, PR welcome.\n. Not that I'm aware of.\n. Not a big fan of aliases tbh. It's a good idea in theory, but they complicate the simplicity. Not in code, but in documentation and overhead for users, as they have to pick the aliases they prefer. AVA is inherently opinionated. This also makes it easier for contributors. AVA is always the same no matter which project you contribute too.\nWhen we implement the possibility to use any assertion lib https://github.com/sindresorhus/ava/pull/49, you could just use the core assertion lib, right?\n\n// @vdemedes @kevva @arthurvr @SamVerschueren\n. Another argument against. The Node.js core team have repeatedly said the assert module is locked and won't change. That it is mainly for internal use and users should prefer userland assertion libs. In the future we might want to change things which might not be fully compatible with assert and it would be confusing to have the same method names then.\nhttps://github.com/nodejs/node/pull/2315#issuecomment-130062336\n. > and now you're storing something in the working directory in order to keep that data. People will have to migrate to this model and include something in their gitignore.\nWe would store it in ~/.cache/ava/project-name-from-package-json, not in the project directory.\n. We would use an LRU cache, so it wouldn't expunge it right away when switching branches.\n. Wallaby.js somehow did it, though.\n\n. > Clearly this feature shouldn't be preset. I would imagine a --quick CLI flag, or similar, --help may note its inadequacies\u2026\n:+1: \nYes, the intention is to combine it with some kind of watch mode. That's where it would shine. Being able to code and see the test results almost live.\n. I think we should defer this for a couple of weeks until Babel 6 settles a bit. I see a lot of issues on the Babel issue tracker that might affect us.\n. This was resolved in v0.9.0.\n. @sebmck Oh cool. Are you ok with exposing the cache functionality? We're not using the require hook, so we need to be able to access the cache functionality from the Babel API.\n. Yay! Been missing this :)\n. Lgtm.\n@vdemedes ?\n. @jamestalmage Good find btw. I've been experiencing this problem too.\n@SamVerschueren This might be related to our nyc problems in Pageres.\n. Yes, it was modeled after node-tap. You can use the --fail-fast flag if you want it to bailout after the first failure.\nI'm open to supporting showing all assertion failures. I just didn't see the point when I first made AVA. There has been some times that I could have used this since then, though. Note to self: No matter what we go for we should at least more clearly document the behavior.\n@vdemedes What do you think?\n. I'm ok with switching to showing the first assertion instead of the last, but I'm still undecided about showing multiple asserts.\n. @Qix- Totally agree. Not sure why I did this initially.\n. @vdemedes See: https://github.com/sindresorhus/ava/pull/124#discussion_r43855517\n. > It looks like the only option is the then(success, err) one. \nI'm fine with using this pattern. It's IMHO only an anti-pattern when you use it thinking it's a shortcut for .catch(). I'm all for simplifying it as the current implementation is a bit hard to follow and verbose.\n. > Not sure if it is a downside, but it's not standard assert stuff which might confuse people.\nIt's not. We only use the core assert so not to duplicate code. If anything can be done better, we'll do it. Also the core Node.js people have said assert is done and won't change anymore.\n\n:+1: from me.\n. @Qix- In their defense it was a mistake to expose it as it was only really meant to be used to test core, from what I've read.\nAlso why ideally almost everything should be in user-land so it can be individually versioned and evolved.\n. :+1: Thanks Sam :)\n. @SamVerschueren Hopefully today. Just want to get in the two open PRs. Any help reviewing https://github.com/sindresorhus/ava/pull/120 welcome ;)\n. More tests are always welcome. Can't get enough. Nom nom.\n. Looks good :)\n. Everything can be overridden, though. We'll have to save references to every single global with properties if we're going to be able to totally fix this. Crazy idea, maybe we should?\n. Do you have a local Babel install? If so, what version?\n// @twada\n. @TrySound Not a very useful title. Please try harder the next time ;)\n. https://github.com/sindresorhus/ava/milestones/0.4.0\n. https://github.com/sindresorhus/ava/releases/tag/v0.4.0\n. It might work if you depend on the tagged git version of Babel 5 locally as AVA will use that one instead of the builtin one. I use that trick in XO to be able to lint itself: https://github.com/sindresorhus/xo/blob/dd233cc40a1984a793000f260062c520767a685b/package.json#L78\nIf that doesn't work, I guess we could add a flag, but I was hoping we didn't have to, as there's no other reason I can think of to disable Babel.\n. It might work if you depend on the tagged git version of Babel 5 locally as AVA will use that one instead of the builtin one. I use that trick in XO to be able to lint itself: https://github.com/sindresorhus/xo/blob/dd233cc40a1984a793000f260062c520767a685b/package.json#L78\nIf that doesn't work, I guess we could add a flag, but I was hoping we didn't have to, as there's no other reason I can think of to disable Babel.\n. > I am looking to use AVA to test a node ^4.2.0 app that uses the built in ES6 features without babel.\nNode.js 4.2.0 is missing a lot of ES2015 features. Also nice being able to use ES2016 features like async functions. Disabling Babel will not only disable Babel, but also the enhanced asserts. I plan on looking into creating Node.js version specific presets, so it only transpiles what's not supported natively. This became a lot easier to achieve with Babel 6.\n. @jamestalmage Yeah, only an idea yet. Feel free to take it on. Let's continue this in https://github.com/sindresorhus/ava/issues/148.\n. @callumlocke We're not just using Babel for the syntax. Babel enables power-assert, detecting incorrect use of t.throws (very common mistake), and more. And we plan on using it for many more things in the future.\n. I don't understand the question. JSPM support in what form? Can you elaborate?\n. I don't understand the question. JSPM support in what form? Can you elaborate?\n. We just run the test file through Babel.js. Whatever Babel supports, we support.\nThis might help: https://babeljs.io/docs/setup/#jspm ;)\n. We just run the test file through Babel.js. Whatever Babel supports, we support.\nThis might help: https://babeljs.io/docs/setup/#jspm ;)\n. The best solution is probably to bundle the JSPM code and test that. We have Babel support and you can do a lot with that. We're not going to add native support for JSPM.\n. A recipe for using JSPM with AVA would be most welcome! :D\n. :+1: We plan to add this. Help welcome as we have a lot of other things to deal with right now.\n. Not sure about this. You only save 3 characters. I'll think about it.\n\nIn hindsight test() should have been purely sync and async would be activated by using an async function. That would have been so much cleaner.\n. @vdemedes Actually, a benefit with it is that it has clearer intent, and I've many times forgotten the t.end()...\n. :+1: \n. This is for anyone looking to contribute :heart:\nWe could definitely use some help! At the moment, what we could really use are good opinions on the various open issues. We're trying to figure out the fundamentals as fast as possible and more opinions help us make the right choices.\nAs for code contributions, I've added a good for beginner label, but we don't really have many issues yet that are very easy. If you're ready to dig in, we do have some that are not too hard: #128 #102 #84.\nI'm gonna keep this issue open for a while as this applies to anyone looking to contribute. Feel free to ask here about anything. We would be happy to tutor anyone and answer questions about the code base. No contribution is too minor. Documentation improvements are the best contributions we get, as we all know, docs can always be better \u2192 https://github.com/sindresorhus/ava/issues/160.\nWe have a Gitter chat. Come and lurk! ;)\nIf you have any questions regarding contributing, ask us here or in the chat.\n. @vdemedes I'd like to keep this open for visibility for anyone looking to contribute.\n. Closing this now that we have comprehensive contributing guidelines :)\n. https://github.com/sindresorhus/ava/releases/tag/v0.4.0\n. @rmg As far as I know there's no way to actually fix this (would be so happy to be proven wrong), as we would never know whether an async planned assertion will happen at some point. Currently, we end the test in the next event loop when all planned assertions are fulfilled.\nI removed the test in https://github.com/sindresorhus/ava/commit/7a3f4444997099cd571118508f08783bce8a5526 when I realized it's not really possible. I had this idea of checking process._getActiveHandles().length === 1, but I don't think that'll work when tests run concurrently and there might be a lot of other async things going on in AVA itself.\nHow do you think we could further improve the docs about this? Would you be interested in doing a documentation pull request?\n. Cool! Makes sense. PR welcome.\n. I would prefer not to change the name of .throws. A lot of things will differ from the core assert module. Even more when we evolve further. I think we should rather fix the issues mentioned here and document clearly that the API is not core assert compatible.\nAs for regexTest, that unfortunate, but honestly, do we even need it now that we have power-assert? t.true(/foobar/.test(foo)); would be just as good with the excellent output of power-assert.\n. > As for regexTest, that unfortunate, but honestly, do we even need it now that we have power-assert?\nThoughts on deprecating regexTest?\n. @vdemedes But the current one has the wrong argument order. I suggest we deprecate regexTest and add regex with the correct argument order then. Unless anyone has a better idea.\njs\nt.regex('oh ok', /ok/);\n. > Or make Promise local to regenerator with function wrapping.\nThis sounds like the best solution, but I would say we go with the fastest solution now. We can do it properly later. This issue is blocking AVA 0.4.0, so would be nice to get it fixed as fast as possible.\n. > core.js polyfills\nAre we including this now? If not, that's a regression too.\n. > all polyfills now excluded from tests, because they affect tested code\nWith Babel 6 we can use https://github.com/babel/babel/tree/master/packages/babel-plugin-transform-runtime, right?\n\nExternalise references to helpers and builtins, automatically polyfilling your code without polluting globals\n. Actually, in the Babel 5 docs, it says that the runtime plugin does not pollute globals:\nBasically, you can use built-ins such as Promise, Set, Symbol etc as well use all the Babel features that require a polyfill seamlessly, without global pollution, making it extremely suitable for libraries.\n\nhttps://github.com/babel/babel.github.io/blob/862b43db93e48762671267034a50c30c00e433e2/docs/usage/runtime.md\n. > install babel-runtime as peer-dependency\nAre you sure you meant peerDependency? They don't install anything, but rather only warns the user to manually install it. I don't think we'd want that.\n. Agreed.\n. Ok, I've clarified it in the readme, but the reality is that most people just use the default interface. The TAP part is regarding node-tap/tape, not Mocha. Feel free to do a PR if you think anything else could be said better or something. I'm not trying to bash on Mocha. Just my opinion on it. I was a happy user for a long time.\n. Please do. I would like to see Mocha improve too. AVA is not for everyone. Most will continue using Mocha. I'm just trying to move testing forward for everyone :)\n. Awesome work @jamestalmage! I use await inline a lot.\n. Thanks @uiureo :)\n. Wouldn't live feature testing be slow? I had a similar idea of doing the feature testing for every major Node.js version before publishing.\n. :+1: That's exactly what I was thinking.\n. @novemberborn I'm a little bit sceptical of feature testing. New features might be buggy or slow. I would be more comfortable with a preset that choose plugins based on vetted testing on various Node.js versions. Maybe babel-preset-env when it gets more mature.\n. We don't support Babel 6 yet: https://github.com/sindresorhus/ava/issues/117\n. Couldn't it just read the runtime from https://github.com/facebook/regenerator ?\n. I don't think this is compatible with npm@3, though. As it could put babel-runtime on any level of the dependency tree, to flatten it.\n. You also need to bump require-from-string to latest version to ensure npm never uses an older version.\n. Awesome! Thanks for working on this and resolving it so quickly @floatdrop :)\nContinues in: https://github.com/sindresorhus/ava/issues/152\n. > It is possible to wrap the existing babel runtime plugins (both v5 and v6) and manipulate the require paths they put in the source.\n:+1: \n. I think we should upgrade to Babel 6 first, though, before we spend to much time making it work with Babel 5, which is going to be replaced.\n. Yup, but I'm unsure why. That test should pass, but it's rejecting somehow.\n. Oh wow. I would honestly never have thought of this. Thanks again @floatdrop :)\n. Why did you close?\n. I'm open to it, as it would be very nice to be able to use ES2015 stuff in test helper files too. I think a whitelist would be better so we could still make use of the ES2015 import semantics.\n. @jamestalmage I think defaulting the whitelist to test/_* is a good idea, not so sure about the whole test directory, though. I would definitely not want my fixtures implicitly transpiled.\n. @sparty02 Pending enhancement. Definitely something we want to support, but haven't had the chance yet. See https://github.com/sindresorhus/ava/issues/111. Any thoughts there much appreciated.\n. @jamestalmage I'm warming up to the idea, but would like to hear some more people's thought about it in   https://github.com/sindresorhus/xo/pull/32.\n. No matter which direction we go, I think it's important we find a way to get rid of the mismatch problems once and for all.\n. Thanks @Qix- :)\n. > Are you open to github-pages documentation, or do you want to keep it all in the README?\nI would prefer to keep it in the readme for now. I plan on having a website for AVA, but not until after 1.0.0. I want to focus on AVA itself for now. Marketing can be prioritized later.\n\nhttp://documentation.js.org/ looks interesting. (particularly this demo)\n\nIndeed. There's also https://readme.io/ (free for open-source).\n\nAlso, you probably want to reserve ava at https://dns.js.org/ (that would get you https://ava.js.org)\n\nI already have https://ava.li.\n. Agreed, documentation.js looks really good.\nNot a big fan of JSDoc. If I wanted type-hints I would go with Facebook Flow.\n. @jamestalmage I opened a separate issue for a future website: https://github.com/sindresorhus/ava/issues/162\n. @sotojuan I'm all for that, but would prefer to keep it succinct and rather link to a good blog post or two with more comprehensive explanation.\n. We're now also looking for people interested in translating the docs: https://github.com/sindresorhus/ava-docs\n. We're now also looking for people interested in translating the docs: https://github.com/sindresorhus/ava-docs\n. Would be nice to embed the Gitter chat on the website: https://sidecar.gitter.im\n. @vdemedes How would it work exactly? Not sure I understand the purpose.\n. @vdemedes How would it work exactly? Not sure I understand the purpose.\n. Oh yeah, that would be cool. We'll have browser support by the time we do a website anyways.\n. Oh yeah, that would be cool. We'll have browser support by the time we do a website anyways.\n. @Qix- Just to give the user a feel of how nice and simple the test syntax is and what the output will look like.\n. @Qix- Just to give the user a feel of how nice and simple the test syntax is and what the output will look like.\n. Never tested AVA on Windows, but we're in the process of setting up automated Windows testing.\nLooks like we'll need to escape the path.sep.\n. // @vdemedes \n. Generally looks good :)\nWill need docs and tests are currently failing.\n. > I think that using forEach is probably your safest bet. It returns a promise, so you wouldn't have to make a change.\n:+1: \n. Off topic to this PR, but kinda weird that you have to call Observable.forEach(function () {}); to convert it to a promise. I would have preferred something like Promise.resolve(observable).\n. @BarryThePenguin Can you rebase from master and fix the merge conflict? This looks good to me.\n@vdemedes @jamestalmage Anything final you'd like to point out?\n. > Feel free to cancel build 168\nDone.\n. Landed! Thank you @BarryThePenguin :)\n. I've added some docs for it https://github.com/sindresorhus/ava/commit/e57eff059f73e53f1c8db1af096dc53e2608a322, but I don't really know how Observables work, so any improvements welcome! For example, can you await an Observable in an async function?\n// @thejameskyle\n. > We've discussed it at length and implicit conversion to promise for use by await is controversial.\nThat's too bad... I was hoping await could be used directly by any async primitive. Do you happen to have a link to that discussion?\nIn that case, observable.toPromise() is definitely needed. The forEach-to-convert-to-promise thing is super weird.\n. Ignore the Appveyor failure, it was already failing.\n. @TrySound I've commented this before, but please try harder with your titles ;)\n. https://github.com/sindresorhus/ava/releases/tag/v0.4.2\n. Not sure it's worth the time writing one. I've tried it with awesome and some other projects and after hundreds of pull requests, not even one person bothered to read it. I think the problem is that people that bothers to read a contribution guidelines are usually the kind of people that doesn't need to in the first place. That goes the other way too.\nRelevant:\nhttps://twitter.com/sindresorhus/status/655792500590923776\nhttps://twitter.com/sindresorhus/status/620278511127953408\n\nbut I think it's also important that the basics of git clone, npm test etc. are there\n\nWhy? I don't think every project should cover how to do generic and basic things like that.\n\nLet's see what others think, but I'm generally pessimistic of the usefulness of a contributing.md.\n. https://github.com/RichardLitt/docs/blob/master/amending-a-commit-guide.md\n. Good idea!\n. @markthethomas You don't have to amend. You can just push new commits too. I usually squash commits and cleanup afterwards anyways ;)\n. Open source is definitely the best way to learn to use Git ;)\n. Btw, noticed that your created the PR from master. First rule of PRs is to never do it from master, since that makes it hard to create multiple PRs. Create a new branch for each PR you do. Master is for syncing with upstream.\n. No, you can finish this one first.\nSince you can't change the branch of an existing PR.\n. No worries :)\n. You're dealing with computers. It's never easy :p\n. AVA doesn't transpile your code, only your test file.\nYou probably want https://github.com/sindresorhus/ava/issues/111.\n. Ok, so I think the conclusion is; set a default timeout if process.env.CI (I would go with 60s instead of 30s just to be safe), and add ability to add a timeout, but no default.\n. :+1: to globally and --timeout.\n. @vdemedes You're assigned to this. Still interested or should I unassign you?\n. Agreed. I've already seen this, but forgot to open an issue.\n. Sure, sounds like an easy win.\n. Looks good to me. @vdemedes @jamestalmage ?\n. That's a cool idea. I like it.\n. That's a cool idea. I like it.\n. @jamestalmage https://github.com/sindresorhus/got has a lot of tests and uses a local server for most, so the tests are pretty fast.\n. @jamestalmage https://github.com/sindresorhus/got has a lot of tests and uses a local server for most, so the tests are pretty fast.\n. For what purpose? What would it do?\n. For what purpose? What would it do?\n. I don't think I would want to have a big tree view, but seeing the test result in the status bar and clicking it taking me to the failing test could be mildly useful. I think an editor plugin will be much more useful when we add support for incremental testing https://github.com/sindresorhus/ava/issues/115, as we could then show the test result almost instantly on save and even inline in the test file. We should wait until after 1.0.0 regardless.\n. I don't think I would want to have a big tree view, but seeing the test result in the status bar and clicking it taking me to the failing test could be mildly useful. I think an editor plugin will be much more useful when we add support for incremental testing https://github.com/sindresorhus/ava/issues/115, as we could then show the test result almost instantly on save and even inline in the test file. We should wait until after 1.0.0 regardless.\n. @varemenos That looks amazing! Much more in line with what I would like to see.\n@jacobmendoza Would you be interested in modularizing it out into more reusable components? I imagine a lot of test runners could benefit from your work on this. This really shows the power of the editor just being JS and CSS. Would be cool to have something like Atom Linter, but for test runners.\n. I haven't really dug into the code yet. Was just my first thought to make it easier to reuse things in a potential AVA plugin. I just got an idea though. If you would add support for TAP potentially any test runner could be made compatible with it. AVA plans to add support for TAP (#27) and most existing test runners support it. What do you think?\n. AVA now has TAP output with the --tap flag.\nContinued in https://github.com/sindresorhus/atom-ava/issues/3.\n\nAlso see https://github.com/sindresorhus/atom-ava/issues/2.\n. I think that's a bad idea. Transpiling on postinstall is a huge anti-pattern. Lots of CoffeeScript projects do that and I absolutely hate it. As @vdemedes mentioned you would risk your code suddenly breaking on every Babel update. You can no longer guarantee consistent install. You should really never use postinstall for anything. Package installs should be predictable. Even npm has said postinstall was on of their biggest mistakes.\nI'm not totally against writing AVA in ES2015, but it should be a prepublish step, and I don't think we should use our resources on that right now. We can rewrite in ES2015 at a later time when AVA is more stable. Rewriting something when it's actively developed is a very popular doom fallacy. Even though it's very tempting.\n\nyou need to to create an additional commit that includes the transpiled dist folder\n\nOnly if you commit dist. The common pattern is to transpile in the prepublish npm run script hook. Like: https://github.com/sindresorhus/pageres/blob/28dd9b3004c05470202f02e0ac0d0ee148a63768/package.json#L17\n. > You do need to commit the transpiled dist if you want to temporarily rely on your hotfix\nNot really. You can just publish it as a scoped package under your username namespace.\nhttps://docs.npmjs.com/getting-started/scoped-packages\n. > It might be worth giving this a shot. If you still run into problems, could you point me at a repo I can test against?\n@sparty02 Did that solve your problem? If no, would you be able to provide something that reproduces it?\n. You can already use any custom assertion. The only minus is that assertion counting doesn't work. We plan to add support for adding custom assertions. See #49.\n. :+1: \n. :+1: Agreed. Anything to make debugging easier. Just pick whichever reporter you like.\n. And I dig the module name ;)\n. Closing in favor of https://github.com/sindresorhus/ava/pull/352.\n. Looks good! Commented some nitpicks inline.\n. // @floatdrop @jamestalmage \n. Ace :)\n. \n@vdemedes being productive today :D\n. Clean code is happy code :D\n. You're on :fire:!\n. @sohamkamani Awesome! Keep 'em coming :D\n. Thanks for report. We'll look into it.\n\n\nprocess.chdir(__dirname);\n\nYou don't need this. AVA does this for you.\n\nAnd btw, you should take advantage of async functions ;)\n``` js\nimport test from 'ava';\nimport { rollup } from 'rollup';\ntest('should load from memory', async t => {\n    const bundle = await rollup({\n        entry: '../src/index.js',\n        plugins: []\n    });\nconst result = bundle.generate({\n    format: 'cjs'\n});\n\nconsole.log(result.code);\n\n});\n```\n. Maybe we should throw an error if it's used incorrectly?\n. @vdemedes You're saying that as if most users actually read the docs :p\n. @vdemedes I disagree. It's an easy win. I think software should be human proof. Having useful fail-safe checks is part of that. Now everyone that does the mistake has to waste time Googling it and stuff. Instead we could just directly point out their error.\n. Agreed. Rejecting without an error makes it impossible to see where it happened.\n. Fails on Node 0.10 :/\nhttps://ci.appveyor.com/project/sindresorhus/ava/build/96/job/3yun8im3pcpp3o8w\n. Alright\n. :+1: We currently have a lot of open PRs with changes to the tests. We should probably wait until those land.\n. According to Coveralls there's no change in the coverage:\n\ncoverage/coveralls \u2014 Coverage remained the same at 95.866%\n. :+1: Alright. Thanks :)\n. Awesome @lijunle :) Both @vdemedes and I are traveling, so we probably won't be able to review this until monday.\n\n// @jamestalmage \n. @jamestalmage Any improvements welcome in a follow-up pull request ;)\n. :+1: Good catch :)\n. Why combine the unhandledRejection/uncaughtException stuff with the rest? Seems it could be a separate PR? Or is there a coupling with the other fixes?\n. // @vdemedes @floatdrop \n. Looks good to me. Let's get this merged asap. Just waiting for @vdemedes to review.\n. Landed! Superb work tracking down all these issues @jamestalmage :)\nI would honestly have just given up and removed AppVeyor :p\n. > Now you tell me!! :wink:\nAutomated Windows testing is especially important for a test runner, though, so glad we have it, regardless of what I feel about Windows.\n. @joshuajabbour Ideally, yes, but Babel 6 broke all the things. Babel 6 was also super broken when released with a gadzillion regressions, hence why it has taken us some time to move to it.\n. @novemberborn Yes, and with the Babel 6 change we're not going to use the users Babel, but instead just tell users to use npm@3 if they want to enforce a version as npm will deduplicate it for the user.\n. @novemberborn Yes, and with the Babel 6 change we're not going to use the users Babel, but instead just tell users to use npm@3 if they want to enforce a version as npm will deduplicate it for the user.\n. > Which is effectively where we are with Babel 5 at the moment. Babel 6 is incompatible and shouldn't be used. In the future a user won't be able to force Babel 7 either.\nThat's the whole point. The user would then be able to use Babel 5 themselves, even if we only support Babel 6+, or inverse.\n\nMy understanding of #292 is that AVA provides certain transformations of the test source. If different transformations are required the best way to achieve that is by swapping out AVA's transform process entirely.\n\nYes, but the above will work for most.\n. > Which is effectively where we are with Babel 5 at the moment. Babel 6 is incompatible and shouldn't be used. In the future a user won't be able to force Babel 7 either.\nThat's the whole point. The user would then be able to use Babel 5 themselves, even if we only support Babel 6+, or inverse.\n\nMy understanding of #292 is that AVA provides certain transformations of the test source. If different transformations are required the best way to achieve that is by swapping out AVA's transform process entirely.\n\nYes, but the above will work for most.\n. I put fixes at the end, and if I squash merge a pull request I put the pull request issue number first.\nSquash merge commit for this would be something like:\nClose PR #208: Properly handle empty results from test files - fixes #198\n. :+1: \n. > When running AVA against empty files or files that do not require ava\nWe're going to fix it, but I have to ask, why are you doing that?\n. > Babel 6 has crazy long bootup.\nOh, that sucks... You might want to open an issue on Babel about it. You can use time-require to see what's slowing it down.\n. Cool, I guess it's fine then :)\n. I don't have anything against using domains. It's fine if you use it correctly and it will not go away for a long time. I do, however, wonder if it's worth doing this. Promises and async functions are the future and we already handle those nicely. This really only applies to async exceptions in callback-style APIs, right?\nFrom what I can remember, async listener is the intended domains replacement, but it's much more low-level and basically meant to be something you could build a domains implementation on in user-land, and many other use-cases too. Some use-cases: https://www.npmjs.com/browse/depended/async-listener\n. @vdemedes ?\n. Yeah, I just don't want us to waste too much time on legacy use-cases.\n\nsetTimeout\n\nhttps://github.com/sindresorhus/delay\n\nevent-emitters\n\nhttps://github.com/zenparsing/es-observable (I know, I know :p) \n. @jamestalmage Read some weeks ago on an issue that it was close to landing. That's all I know.\nWe can continue discussing this, but let's defer actually doing it until later. There are lot of other more important issues to attend to.\n. Relevant read: https://github.com/trevnorris/node/blob/domain-postmortem/doc/topics/domain-postmortem.md\n. > which is a quite common practice really\nNever seen it before. Can you link to something that proves your point?\n\nIt might even make sense to include support for the whateverSpec.js variation.\n\n:-1: \n\nAnd where do you keep these *.spec.js files? In a test folder? In root?\n. > I personally keep them in the test/ folder, yes.\nOk, so it seems it's for marking something as a test when you have both the test and the actual code in the same directory. Not saying we won't accept this PR, but you are not doing this, so you really don't need the .spec postfix in your case.\n. Relevant: https://github.com/babel/babel/pull/3094\n. > Blocking issue is: babel/babel#2954\nIs this still blocking? Would you mind updating the link? I have no idea what issue on the new issue tracker it is.\n. They were fed up with GitHub being non-responsive on improving the issue system and they needed something more powerful.\nIf it helps, the issue title were: (0 , _typeof3.default) is not a function\n. Another take on fixing the issue => https://github.com/babel/babel/pull/3142\n. Another take on fixing the issue => https://github.com/babel/babel/pull/3142\n. I believe this is also blocking => https://github.com/babel/babel/pull/3139\n. I believe this is also blocking => https://github.com/babel/babel/pull/3139\n. Oh ok, that's great. So as soon as the Babel issue is fixed we can upgrade then.\n. Oh ok, that's great. So as soon as the Babel issue is fixed we can upgrade then.\n. From a quick skim this looks like the same as https://github.com/sindresorhus/ava/issues/62 just with a different name.\nI'm not a big fan of this kind of complication to be honest. If you think you need nested tests or groups, you're probably better off splitting it up into multiple files (and get the benefit of parallelism) or simplifying your tests.\nI'll keep this open for discussion, but even if we are going to add this, it's not going to happen right now. I want to focus our efforts on making a really good minimal core first.\n. @novemberborn I'm not saying splitting up would work for every use-case and it does sound like yours is a difficult one. I just try very hard not to introduce new features until I'm absolutely certain it cannot be solved in a less complicated way. We'll still consider this, but we have a lot on our plates, and maintaining something like this with the level of code churn we currently have is not something I want to take on right now. Happy to continue discussing how it would look like, though.\n. Thanks for all the use-cases. The more practical use-cases we get, the easier it is to evaluate a possible feature and how to implement it.\n. You need to add it to files in package.json.\n. @jamestalmage Oh ok. Then let's keep it internal like this for now until a need arises.\n. @jamestalmage Can you add a short example on how to use this to https://github.com/sindresorhus/ava/blob/master/maintaining.md ?\n@vdemedes Thoughts?\n. @vdemedes It's currently not included in the npm package as it's not listed in the files field in package.json. I would rather have it in the repo than an open PR.\n. :+1: \n. Closing in favor of https://github.com/sindresorhus/ava/issues/242.\n. Yes, definitely.\n. Yeah, not sure what I was thinking there.\n. The main focus for AVA right now is testing Node.js things. We do intend to support browser use-cases too, but that takes the backseat for now.\nWould exposing the Node.js binary --require flag while disabling our Babel hook solve your problem?\nSo you could for example do: $ ava --require=babel-core/register --require=jsx/register\n@vdemedes Thoughts?\n. Can't we just use another non-broken reporter? Or fix the tap-spec one.\n. @vdemedes I'm fine with switching to tap. It's more actively developed and generally better.\ntape tl;dr, tape was initially made to add browser support to tap. tap also had a long period of inactivity, but after npm decided to beef up their test suite it has gotten a lot of <3.\n. I'm fine with 150s. That way it doesn't hang the CI forever if we have some kind of timeout issue.\n. I prefer the full flag with =, for readability: tap --timeout=150.\n. Why? I don't it's our responsibility to test whether pinkie works or not.\n. // @jamestalmage \n. Are you using latest AVA version? We fixed that some time ago.\n. @jamestalmage Node.js 0.10 seems to be failing again on Windows :(\nhttps://ci.appveyor.com/project/sindresorhus/ava/build/job/vrwhv5konnjgylcc\n. @jamestalmage Yeah, my suspicion too. Might be relevant: https://github.com/nodejs/node/issues/3670\n. \n. Still failing. I've had enough, let's just remove Node.js 0.10 from AppVeyor. It's already tested on Linux. ?\n. > Do you know anybody working on node that could push that along?\n@silverwind\n\nThis will impact our ability to provide TAP output on Node 0.10 / Windows.\n\nYeah, let's just document it. I honestly couldn't care less about an ancient Node.js version on Windows. Most people test on Travis anyways. I think we've wasted more than enough time dealing with this.\n. There's https://github.com/cowboy/node-exit, but process.on('exit') won't work correctly when using it.\n. \n@vdemedes @jamestalmage Feel free to add more debug instances where it makes sense ;)\n. I like where this is going :)\nChaining is an important step as it will let you skip a test without losing the type of test. Right now you can't have a skipped test that is serial. You have to remove the serial part, which loses some important information.\n. @jamestalmage Reviewed. This looks very good. Could you update the readme about the methods being chainable?\n. @jamestalmage Reviewed. This looks very good. Could you update the readme about the methods being chainable?\n. :+1: Landed. Superb work on this one @jamestalmage :)\n\n. @Qix- test.async() is there for callback-based APIs which aren't going away anytime soon...\n. > Does t.end() exist in the sync version? Might be easier to upgrade, but add confusion later.\nWe can expose it there as a noop function.\n\nMaybe test.callback might be a better name?\n\nEven though the main use-case is for callback-style APIs, it doesn't mean that's the only. I'd rather name it for what it is.\n\nI seems like async is still possible in the regular version.\n\nNo, it will finish at the end of the tick. So async operations will be ignored.\n. > I would be a little confused by t.end being a noop. I'd rather have it not exist to make sure I didn't make a mistake.\nYeah, good point. I agree that's better.\n\nI guess we disagree on what is async. I consider this async.\n\nOh, I misunderstood you. Yes, async is possible with test() by using an async function.\n. > Let's take that further:\n@jamestalmage :+1:!\n. @jamestalmage I agree t.plan in \"implied async\" should not auto-end the test. I already decided that, but forgot to write it down. With those we actually have the opportunity to catch \"more than planned assertions async\", which we previously couldn't (can you add a test for that in your commit?).\nAs for \"declared async\" (test.async), I'm not sure. I would like to change it, both because it's better and for consistency with \"implied async\", but not sure if it's worth it seeing as it's legacy anyways and lots of people would assume that's how it works. Happy to discuss it, though (hint hint, people express your thoughts). Would be a nice change.\n. @Qix- Already answered in https://github.com/sindresorhus/ava/issues/244#issuecomment-158411625.\n. > What should happen if they are using \"declared async\" (test.async), and they return a Promise?\nThat should not be allowed. They should instead use test(). We should detect it and throw an helpful error message.\n\nOK, so when using callbacks, auto-ending with plan does have a use. Coordinating ending after two or more callbacks who will be called in indeterminate order:\n\nI think we should drop auto-ending. Not even an option for it. I want to keep AVA simple and consistent. Instead, we should document how to count callbacks and t.end() when reached and recommend instead just splitting up the test into multiple, which is better anyways.\n. Could you share the test file?\n\n@jamestalmage Any way we could show the stack of the uncaught exception?\nWe should also hide our own stack on Error: Never got test results from as it has no purpose and is only noise.\n. @TrySound I cloned your repo, npm installed, and ran npm test on Node.js 0.12 and worked fine for me. Weird. Can you try rm -rf node_modules && npm cache clean && npm install && npm test?\n. @TrySound Try upgrading the global AVA version?\n. It's not the same. We depend on https://github.com/jonpacker/destroy-circular/pull/1.\nWe plan to fix it here instead: https://github.com/sindresorhus/serialize-error/issues/2\n. Can you give the PR a proper title and use ES2015 over ES6?\n. @sparty02 Nah, I'll squash merge myself when it's done.\n. Thank you @sparty02 and @BarryThePenguin :)\n. > Can you start assigning me tasks you want me to own so I don't forget about them?\nWill do from now on. You can also self-assign yourself tasks, on the right-hand side (Assignee).\n. It's intentionally not recursive, as a good default and simplicity. If you need it to be recursive just specify the appropriate glob pattern. Like: $ ava test/**.\n. We don't add options lightly and we try very hard to keep AVA simple. You can solve this with a glob pattern. We'll fix the output somehow :)\n. Agreed, that's what I was thinking too.\n. @jamestalmage @vdemedes Thoughts on making it recursive by default? I'm warming up to the idea.\n. Alright. Let's go for this.\n. @ariporad Both\n. :+1: from me. @vdemedes ?\n. > (Ordered from best to worst workaround IMHO)\n1 is how I already use it and how it was intended. We should just document it better.\n. I think we have a resolution in https://github.com/sindresorhus/ava/issues/244, unless there's anything else that's still undecided? Could you rebase this PR?\n. Looks good to me, except for a few inline comments :)\n@vdemedes ?\n. @jamestalmage No worries at all. Enjoy Thanksgiving :D\n. Done\n. :+1: Excellent. I'm super excited about this!\n\n. Looks good :+1: \n. > I do not know how expensive it is to load up source-map-support in each forked process like this.\n\nSince all errors are serialized and transmitted over IPC before logging, It may be possible to defer the stack-trace transform until it has been transmitted back to the main thread.\n\nLet's do some benchmarking before we start investigating this. Might not make a difference.\n. Sure, since we don't have power-assert with tap.\n. I don't think we ever made a decision about t.skip() in #9. I guess it's about time we do. I was initially against it, but I find myself often having to comment out single test assertions when testing and manually updating the t.plan count. So I'm leaning towards :+1:. @vdemedes ?\n. I don't think we ever made a decision about t.skip() in #9. I guess it's about time we do. I was initially against it, but I find myself often having to comment out single test assertions when testing and manually updating the t.plan count. So I'm leaning towards :+1:. @vdemedes ?\n. @vdemedes Ah, forgot, but seeing how simple the implementation is and the fact I've had a need for it recently, I'm :+1:.\n@jamestalmage Can you rebase?\n. Totally slipped my mind when :+1:, but this needs some docs.\n. @jamestalmage Looks good except for some minor nitpick :) Would you mind adding the assertion failure behavior to the docs so it's explicit how it works?\nWhen you reference tickets in the the title, can you do so in the description too? It makes it easier to click through. The issue references in the title aren't clickable.\n@vdemedes ?\n. > Errr...\nAh yes. I should have mentioned. I added the issue references to the PR description for you here.\n. Can you fix the merge conflict?\n. @jamestalmage You are so productive! :D\n. @sotojuan With how this is currently implemented, yes, but it would work if you instead just always pushed it as the last item to the files array before passing it to globby.\n. @sotojuan That's what I said ;)\n. Right, yeah.\n. Landed. Very nice work @sotojuan ;)\n\n. Landed. Very nice work @sotojuan ;)\n\n. Not sure we should have a verbose modifier. I don't see why would want to augment just some tests. I'd rather have a --verbose flag for all tests.\n\nPerhaps only mode should also just imply verbose mode (you should have way less noise with a single test).\n\nI think I like the idea of only triggering verbose mode for those tests.\n. Been thinking hard about this one today. I've changed my mind about implicit verbose when using test.only(). That's just too magic. As for a --verbose flag, I guess that could be useful, but I've honestly never needed it. I think we should punt it for now, but leave this open for additional comments. We can make a decisions later.\n. Been thinking hard about this one today. I've changed my mind about implicit verbose when using test.only(). That's just too magic. As for a --verbose flag, I guess that could be useful, but I've honestly never needed it. I think we should punt it for now, but leave this open for additional comments. We can make a decisions later.\n. I will let @jamestalmage comment first, but I would close this too.\n. Sure, why not, although I've never needed this, we should make debugging failing tests as easy as possible.\n. Sure, why not, although I've never needed this, we should make debugging failing tests as easy as possible.\n. @MaffooBristol https://twitter.com/slicknet/status/782274190451671040 :). Looks good to me.\n. Looks good. @novemberborn Would be cool if you could do a followup pull request with :arrow_up: :)\n. Looks good. @novemberborn Would be cool if you could do a followup pull request with :arrow_up: :)\n. Wonder if the vm module could help in this situation.\n. Wonder if the vm module could help in this situation.\n. @ariporad The problem with such polyfill is that it mutates the global Array prototype, so it would influence not just your test code, but the code being tested.\n. @novemberborn Maybe we should document it somewhere? It can be surprising behavior.\n. :+1: \n. :+1: \n. Good idea! But I think this would be better as a separate project. Something like a ES2015 tester project that both we and Babel could consume.\n. Good idea! But I think this would be better as a separate project. Something like a ES2015 tester project that both we and Babel could consume.\n. There's actually https://github.com/kangax/compat-table which already does this. Should probably just reuse that one instead.\n. There's actually https://github.com/kangax/compat-table which already does this. Should probably just reuse that one instead.\n. I think we should put this on hold until we have upgraded to Babel 6.\n. > But I think this would be better as a separate project. Something like a ES2015 tester project that both we and Babel could consume.\nClosing for the above reason. @jamestalmage If you're no longer interested in doing this and think it's still relevant, can you open an issue?\n. > We should resolve with an failure result, that will simply be summarized in the output. We should save rejection for unexpected/unrecoverable problems.\n:+1: \n. Not sure we ever got an agreement on this:\n\nWould exposing the Node.js binary --require flag while disabling our Babel hook solve your problem\n- https://github.com/sindresorhus/ava/issues/229#issuecomment-157397442\n\n@jamestalmage ^\n. Thank you @jokeyrhyme! :) \ud83d\udca5\ud83d\udc4c\nKeep the quality contributions coming. We really appreciate your help.\n. @ariporad We're planning a new release when https://github.com/sindresorhus/ava/pull/326 lands.\n. @ariporad Actually, I did a release now. https://github.com/sindresorhus/ava/releases/tag/v0.8.0\n. :+1: when test is added.\n. > #300 has an alternate solution (allow circular comparison).\nThis is #300 :p\n. > #300 has an alternate solution (allow circular comparison).\nThis is #300 :p\n. I will always favor solutions that makes things just work over throwing, as long as it's not too magic :)\n. I will always favor solutions that makes things just work over throwing, as long as it's not too magic :)\n. > The only solution was to make them enumerable. \nWhy do they need to be enumerable?\n. > t._capt, t._expr are not enumerable, \n@jamestalmage I remember @twada saying it might be possible to remove them at some point. Is there any power-assert issue I can follow about that?\n. > t._capt, t._expr are not enumerable, \n@jamestalmage I remember @twada saying it might be possible to remove them at some point. Is there any power-assert issue I can follow about that?\n. Looks good. Thank you @mcmathja :)\n. Looks good. Thank you @mcmathja :)\n. > Is there any reason not to do this?\nWe now have two different ways to do the same thing, which I'm not a super big fan of.\n\nWould it have been less painful if we had named it t.ctx? Or is it that you can't use the helper names directly? Do you have an realistic example of when the verboseness is painful? I usually only use context for a few things, and the minor verboseness doesn't really bother me much.\n. > Is there any reason not to do this?\nWe now have two different ways to do the same thing, which I'm not a super big fan of.\n\nWould it have been less painful if we had named it t.ctx? Or is it that you can't use the helper names directly? Do you have an realistic example of when the verboseness is painful? I usually only use context for a few things, and the minor verboseness doesn't really bother me much.\n. Yes. It's more consistent and less magicky this way. With the current way it's clearer as you set the context the same way as you use it. With destructuring it's not really that much more verbose anyways. Readability and consistency over LOCs any day.\n. We will do Browserify and Webpack support at some point, but we're not there yet. See #24.\n. We will do Browserify and Webpack support at some point, but we're not there yet. See #24.\n. So what's the resolution on this? Anything we can do with the docs to make the behavior clearer?\n. So what's the resolution on this? Anything we can do with the docs to make the behavior clearer?\n. You can use any assert module with AVA, including HTTP ones. The only downside is that planned assertions won't yet work with it (#25). We don't plan on having something like that built-in. There's already a lot of good modules for that. From a quick look it seems Supertest and Chakram should work fine with AVA. Frisby seems tied to Jasmine.\nSince this is such a common use-case, I think we should have a recipe for how to do this with AVA. Help welcome :)\n. LGTM\n. :+1: Looks good :)\nShouldn't we remove the CLI tests that are now tested in the API instead?\n. @sotojuan Doesn't really matter what the separator is, so I would go with:\njs\n/test \\w fixture \\w one-pass-one-fail \\w this is a failing test/\n. https://www.google.no/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=fix%20merge%20conflict\n. https://www.google.no/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=fix%20merge%20conflict\n. There's a conflict because of https://github.com/sindresorhus/ava/commit/386416846bdb7b8aae84e1b0d8cf77da0bb31fa4.\n. There's a conflict because of https://github.com/sindresorhus/ava/commit/386416846bdb7b8aae84e1b0d8cf77da0bb31fa4.\n. LGTM\n. @vdemedes No need. There were no conflicts.\n. @sotojuan Thanks again for this awesome contribution! \ud83d\udc4c So nice not having all those CLI tests anymore :) Keep the great pull requests coming.\n\n. @vdemedes Yes, it was missing one LGTM.\n. Thanks for working on this @novemberborn. Generally looks good to me.\n. @novemberborn XO ignores fixtures by default.\n. @jamestalmage @vdemedes Looks good to you?\n. Can you explain your changes? We're not interested in changing the paths like that, which I also really doubt is the solution to anything. Would like to hear your thinking behind it though.\n. @vdemedes Sure, but now all the places we use those things have to do an additional argument parsing. Even worse when we want to send arrays and more complex data structures down to the child process, like with the --require PR. Would be much nicer if anywhere we wanted to use the info from the parent process we could just JSON.parse(process.argv[2]).\n. See: https://github.com/sindresorhus/ava/pull/319#discussion_r47169248\n. See: https://github.com/sindresorhus/ava/pull/319#discussion_r47169248\n. @jokeyrhyme It's for example used here: https://github.com/jokeyrhyme/ava/blob/af4e70377a2b0143005c5fad52913821fcbdfbeb/api.js#L150\nI would like to remove it though, as I feel it's pretty dirty to have arbitrary options mixed into this. @vdemedes You ok with that? Any other places it's used?\n. @jokeyrhyme It's for example used here: https://github.com/jokeyrhyme/ava/blob/af4e70377a2b0143005c5fad52913821fcbdfbeb/api.js#L150\nI would like to remove it though, as I feel it's pretty dirty to have arbitrary options mixed into this. @vdemedes You ok with that? Any other places it's used?\n. Very nice work @jokeyrhyme :) Can you squash it down to one commit?\n. Very nice work @jokeyrhyme :) Can you squash it down to one commit?\n. LGTM\n. LGTM\n. Thank you! :D\n. Hmm, not sure about this. Now that we have power-assert support it doesn't really make sense to add a bunch more assertion methods as you can just use t.true(). In this case it would be t.true(foo.includes('bar'));, although [].includes is currently not working, but will be fixed.\n. Hmm, not sure about this. Now that we have power-assert support it doesn't really make sense to add a bunch more assertion methods as you can just use t.true(). In this case it would be t.true(foo.includes('bar'));, although [].includes is currently not working, but will be fixed.\n. js\nt.contains(foo, 'bar');\n// vs\nt.ok(foo.includes('bar'));\n// @twada @uiureo\n. Ok, updated. I went with just removing the example as it didn't give much.\n. @jamestalmage Not sure. @bradrydzewski ?\n. > It's a nice information to show.\nWe don't add things on the merit of it being \"nice to have\". Does it have any actual use-case for every test run? I think knowing the test time can be useful once in awhile to make sure your tests aren't getting too slow, but then you can just do $ time ava. I don't want to see the time every time I run a test and I don't really see how it's useful every time either.\n. > In my opinion some users want to see the total execution time\nWhy?\n. LGTM\n. Great work @sotojuan :) :cake: \n. Looks very promising! :) :cake: \n\n\nDecide on assertion output\n\nWhat specifically needs to be decided on?\n. > We need to decide what title do we display for assertions\n'yes' === 'no' and the power-assert output when t.ok()/t.notOk(). We should also show the assert message when provided.\n. Can we defer the assert title complications for later and focus on just getting some TAP output in this PR? We can just go for naive assert titles for now.\n. Alright. I've opened an issue for it so we don't forget. https://github.com/sindresorhus/ava/issues/341\n. Alright. I've opened an issue for it so we don't forget. https://github.com/sindresorhus/ava/issues/341\n. Can you add some tests?\n. And add tap as a keyword to package.json.\n. :+1: \n. LGTM\n. >  Support fail-fast mode via Bail out!\n:arrow_up: Can you open an issue for this?\n. > (and probably separate into own module)\nI'm surprised there's no compliance suite any TAP producer could use. Would be so useful (hint hint). Something like https://github.com/promises-aplus/promises-tests\n. \nSo much tap! :dancer: \n. :+1: LGTM when the dependency is updated to :arrow_up: and tests are passing.\n@twada Does the changes look good to you?\n. Landed! :)\n\n. LGTM\n. LGTM\nWe can land this now and rather simplify it when a new hook-std release is out.\n. Thank you @sotojuan :cake: :tada: \n. > Is there something special about async-to-generator that we feel we must handle it specially now before we \"do it right\"\nI don't know how that plugin works now, but we need to polyfill generator support on Node.js 0.10. I don't really have a strong opinion of how that's solved though.\n. @jamestalmage We only want to include regenerator on Node.js 0.10 though, as it's a lot slower than native generators.\n. @jamestalmage We only want to include regenerator on Node.js 0.10 though, as it's a lot slower than native generators.\n. There's something mentioning blacklist here: https://github.com/babel/babel/blob/59f5e7f218ffa788e6e3f320a3e7e0cbe6d50894/packages/babel-plugin-transform-runtime/src/index.js#L14-L22\nOr maybe we could just do a PR to babel-plugin-transform-runtime to only add the regenerator include when needed (based on has-generator).\nI would prefer we didn't have to maintain our own preset.\n. There's something mentioning blacklist here: https://github.com/babel/babel/blob/59f5e7f218ffa788e6e3f320a3e7e0cbe6d50894/packages/babel-plugin-transform-runtime/src/index.js#L14-L22\nOr maybe we could just do a PR to babel-plugin-transform-runtime to only add the regenerator include when needed (based on has-generator).\nI would prefer we didn't have to maintain our own preset.\n. > I think it would need to be an config param to babel-preset-es2015 (can you even pass config params to presets?)\nI read somewhere on the Babel issue tracker they would be open to that.\n. > I think it would need to be an config param to babel-preset-es2015 (can you even pass config params to presets?)\nI read somewhere on the Babel issue tracker they would be open to that.\n. A lot of stuff are blocked by Babel 6 support. I'm trying to think what's the minimum we can do to land this? Maybe we can just have regenerator even for newer Node.js version and open a ticket about submitting an option to babel-preset-es2015?\n. Done: https://github.com/sindresorhus/ava/issues/348\n@jamestalmage Anything else before we can merge this?\n. > Or we just rip off the band-aid and go for Babel 6.\n:+1: I'm not too concerned with that as Babel 5 is not supported anymore and people will have to upgrade sooner than later and they can just stay on AVA 0.8.0 in the meantime.\n. Landed. Yay! :)\n. Yeah. Let's just nest. I think the code is clearer this way too.\n. @Lokua Use our gitter chat for support. Are you sure you're on the latest AVA version locally (and globally if you run AVA from the command line with $ ava)?\n. It can wait until the next release.\n. > Seems like it would be more convenient if the API supported the way you originally had it. And honestly, as I read the code, it should have worked that way.\n@jamestalmage I don't really follow, but if you think anything should be improved with the API, can you open an issue about it?\n. LGTM\n. >  the fork function logs the ENOENT error to the console.\nIt definitely shouldn't do that. Might be an exception leak somewhere that aren't caught in any promises.\n. >  the fork function logs the ENOENT error to the console.\nIt definitely shouldn't do that. Might be an exception leak somewhere that aren't caught in any promises.\n. > What do you think about moving this logic into empower-core? Auto populating event.computedMessage or similar?\n:+1: The less custom code in AVA dealing with this the better :)\n. > What do you think about moving this logic into empower-core? Auto populating event.computedMessage or similar?\n:+1: The less custom code in AVA dealing with this the better :)\n. > What do you think about moving this logic into empower-core? Auto populating event.computedMessage or similar?\n\nMaybe we can offer a standard way to map wrapOnlyPatterns to messages?\n\n@twada :arrow_up: Any thoughts?\n. @jamestalmage Can this be resolved now that empower-core is updated?\n. Sorry about pinging you all over the place @jamestalmage, but trying to clean up some long-running PR/issues. What's the next step with this PR? Not saying it should be fixed right now. Just would be useful to know the path forward.\n. Thanks for the update. Solid plan.\n\nFor example, our errors currently have expected and actual properties, but the tap reporters prefer found and wanted\n\nTape uses actual/expect, so I always assumed that was correct. Doesn't really matter either way. Better to be as compatible with node-tap output as possible.\n\nI would love for that to also end up being code shared with node-tap, but that won't be as straightforward as clean-yaml-object or stack-utils were (since it's not directly extracted from node-tap in the first place).\n\nAwesome that we can share some code with node-tap. Improving those libs will improve both test runners. Hopefully we can share even more code in the future.\n\nLet us know if there's anything @vdemedes and I can help with. Even if it's just feedback on any of those modules.\n. It's confusing. The v14 draft has examples with both wanted/found and expected/got and expect/got. \n// @isaacs \n. What Node.js version are you on?\nDebugging is broken in Node.js 5.2.\n. What Node.js version are you on?\nDebugging is broken in Node.js 5.2.\n. We definitely want to support using the debugger, but not really sure from the top of my head what's preventing it. We'll look into it. Thanks for reporting @hberntsen :)\n@jamestalmage might have a better idea.\n\npossibly due to the code transformations via babel.\n\nI thought https://github.com/sindresorhus/ava/blob/df48ce69b841c8b5fb1f522aa435d3a676876078/lib/babel.js#L26-L37 did that for us. // @novemberborn\n\noptions['execArgv'] = ['--debug-brk=5860'];\n\nAny downside adding this to AVA?\n. We definitely want to support using the debugger, but not really sure from the top of my head what's preventing it. We'll look into it. Thanks for reporting @hberntsen :)\n@jamestalmage might have a better idea.\n\npossibly due to the code transformations via babel.\n\nI thought https://github.com/sindresorhus/ava/blob/df48ce69b841c8b5fb1f522aa435d3a676876078/lib/babel.js#L26-L37 did that for us. // @novemberborn\n\noptions['execArgv'] = ['--debug-brk=5860'];\n\nAny downside adding this to AVA?\n. > I think this is separate from remapping coverage/sourcemaps.\nChrome DevToools use source maps to adjusts breakpoints too, so I just assumed this would do that too.\n\nCan we detect if the debugger is in use and enforce serial execution of the test files?\n\nMaybe, but I think it would be better to be explicit about it. I think it would be worth introducing a --debug flag that activates --serial and other things related to debugging. Thoughts?\n. > I think this is separate from remapping coverage/sourcemaps.\nChrome DevToools use source maps to adjusts breakpoints too, so I just assumed this would do that too.\n\nCan we detect if the debugger is in use and enforce serial execution of the test files?\n\nMaybe, but I think it would be better to be explicit about it. I think it would be worth introducing a --debug flag that activates --serial and other things related to debugging. Thoughts?\n. A test would be nice.\n. A test would be nice.\n. LGTM\n. LGTM\n. Test failing on Windows: https://ci.appveyor.com/project/sindresorhus/ava/build/609\n. Yeah, seems so. Tweaked it a little bit and landed. Thanks @ariporad :)\n. We appreciate you wanting to help out :D\nSmall focused pull requests are easier to merge than large ones. For larger changes, open an issue or discuss in the Gitter chat beforehand. More here: https://github.com/sindresorhus/ava/blob/master/contributing.md#submitting-a-pull-request\n\nand is there a volume I should stay below to avoid overloading you?\n\n@jamestalmage and @vdemedes can land pull requests too. Don't worry about that.\n. > Open a PR as soon as you have done the minimum amount of work to demonstrate your idea (just prefix the title with [WIP], and describe what you still have to complete, so people know not to make \"the tests are failing\" comments). In general - get your stuff in front of the community for feedback ASAP.\n@jamestalmage Good advice. Can you add this to contributing.md?\n. It's kinda already covered in https://github.com/sindresorhus/ava/blob/df48ce69b841c8b5fb1f522aa435d3a676876078/test/api.js#L241-L252, but I guess it wouldn't hurt to have a CLI integration test for it. LGTM.\n. @thedev This isn't about having regenerator as a dependency. We just don't want the runtime overhead of using regenerator when it's not needed for newer Node.js versions which supports generators.\n. @thejameskyle That's a good point. I just considered our case of live transpiling. Would you be open to add an option to the preset for toggling regenerator? We really don't want to have to maintain our own presets.\n. @jamestalmage del-cli now supports Node.js 0.10.\n. Confirmed.\n```\n~/dev/ava master*\n\u276f time ava example.js\n\u2714 wow\n1 test passed\nreal    3.46s\nuser    3.21s\nsys 0.32s\n``\n. If I runnpm dedupe` and then try again, it's faster.\n```\n\u276f time ava example.js\n\u2714 wow\n1 test passed\nreal    1.40s\nuser    1.30s\nsys 0.14s\n``\n. Babel 6 is a lot more modular, so that might be where all the overhead comes from. Using npm@3 would also be faster (since it dedupes by default).\n. This can be closed now, right?\n. I like thenode_modules/.cache` convention.\n. // @floatdrop\n. LGTM. @jamestalmage ?\n\nTried it on got (6.0.0 branch). First uncached run 17 sec, and second cached run 7 sec.\n. > The cache is currently saved in ./node_modules/ava/node_modules/.cache/ava, whereas I think it should be in ./node_modules/.cache/ava.\nNot sure about this. It would mean when the user upgrades to a new AVA version, that might change how the cache works, the existing cache is still kept around. Should at least then be ./node_modules/.cache/ava-0.8.0.\n\nHowever, there is a chance they are using a global install, and do not have a package.json anywhere. I think we just disable caching in that situation\n\n:+1: \n\nWe should probably salt the hash with babel.version\n\n:+1: Actually, we should make the salt based on the versions of the various Babel related packages. Upgrades to any of them should invalidate the cache.\n\nWhy don't we just fallback to xdg-basedir (with os-tmpdir as a backup)? It is a stable approach.\n\nWe now have multiple ways the cache can go wrong and when in xdg-basedir it's never emptied so cache errors will live forever unless the user does something manually to solve it, which I really don't want.\n\nPerhaps, when we can't resolve their node modules folder, we do put it with the global AVA install and just provide a --clear-cache flag.\n\nI don't see the point of a flag if we only put the cache in ./node_modules/.cache. A flag comes with a lot of hidden overhead.\n. > I was going to suggest we include the AVA version in the salt.\n:+1: AVA version in the salt would be even better.\n\nI think we are going to find that difficult and costly. Babel doesn't do that in it's own cache implementation.\n\nOk, lets not do it then.\n. :+1: \n. It's pretty unclear what kind of error it is now, though:\n``` js\nvar test = require('./');\ntest('wow', async t => {\n    throw new Error('unicorn');\n});\n```\n```\n\u2716 wow unicorn\n1 test failed\n\nwow\n  Error: unicorn\n    at _callee$ (/Users/sindresorhus/dev/ava/example.js:4:8)\n...\n```\n\nIt just logs unicorn where you usually have true === false, etc.\nWe should somehow make it clear something in the test threw/rejected.\n. :+1: \n. LGTM\n. :+1: \n. @sotojuan Yes, and needs to be documented.\n. Excellent.\n. > Should have waited for a bit more consensus on this PR!\nAh, yes, but it looked pretty clear cut. Will give it some more time in the future.\n. Lol, that's true :p\n. Looks good! Thank you @sotojuan. Merry xmas! :santa: :christmas_tree: \n. You should not depend on master as it's a moving target. Instead depend on the latest commit:\n\"ava\": \"sindresorhus/ava#65ae07c\",\n. @vdemedes Lets just keep it open until we have a new release so we don't get a bunch of duplicate issues when people encounter this.\n. Closing as a new release is out. https://github.com/sindresorhus/ava/releases/tag/v0.9.0\n. I think we should keep the old output in a --verbose mode. (And --debug triggers verbose mode).\n. :heart: Looks really good. I much prefer this default output.\n. When all tests are finished and it logs the last 68 passed that is moved one line up since the test title is no longer there. Would be nice if 68 passed was always in the same place. Just log an empty line above it on the final call.\n. Good move adding the test title btw.\n. LGTM\n@jamestalmage ?\n. > In that case, we should create our own internal error class and use it where we explicitly want to suppress the stack trace.\n:+1: \n. Should be done in a follow-up: https://github.com/sindresorhus/ava/issues/368\n. :heart_eyes_cat: \n\n. > delay doesn't like node@<4.0.0\nIt's node@<0.12, and just put global.Promise = require('bluebird'); at the top.\n. @ariporad delay@1.3.0 is now out. Can you update tests to use it? Other than that, this looks good to land.\n. Landed. Really dig this fix @ariporad :)\n. :-1: The benefit is minor as Travis is fast already and I like to test with all the latest sub-dependencies for each run. Imagine if a sub-sub-sub-dependency breaks in a patch release. I want Travis to be able to catch that.\n. The slowest part about AppVeyor is that your builds are always queued for a long time before starting. AppVeyor probably deprioritize free plans.\n. LGTM\n. This cleans it up nicely :)\n. Good catch. Thanks :)\nMerry xmas \ud83c\udf84\ud83e\udd84\n. :+1: \n. I like the idea, but I'm a bit wary of it. What if the native implementation has bugs and we want to use Babel for that? What is the runtime overhead of doing a bunch of feature detections? And https://github.com/hax/babel-features doesn't look very actively maintained nor used much https://www.npmjs.com/package/babel-features.\nI just don't want this to cause us a lot of extra overhead with support issues, etc.\n\n// @hax\n. > I think there's a little overhead, but not a whole lot. I'm not sure how we'd go about it any other way, \nWe could create a preset for each Node.js major version  \u2192https://github.com/sindresorhus/ava/issues/148. Albeit that won't work when we eventually add browser support \u2192 https://github.com/sindresorhus/ava/issues/24.\n\nAnd I guess we'd just assume that V8 isn't going to ship something horrifically broken.\n\nThey do all the time. That's the price for rapid evolution.\n\nThe overhead is going to down once we move babel to the main thread.\n\nThat's a good point.\n. There's also the case of many of the ES2015 features being a lot slower than the transpiled ones.\n. 1. The biggest overhead in installing is installing Babel, but that's also AVA's greatest strength. The ability to use ES2015+ syntax and features while still supporting old Node.js versions. Bundling everything into one file would make startup a LOT slower as the whole file would have to be parsed on startup. Keep in mind that you don't install AVA that often and npm caches the dependencies, so this shouldn't be that much of an issue.\n2. I would like to see some numbers on that claim, but the biggest performance gain using AVA is not for tiny test files, but larger ones. Tests doing a lot of IO will have the biggest gain. Although, we have worked hard on performance lately and the next AVA version has caching, which will help a lot on the startup time. The version after that will improve even more. The launch overhead is mostly caused by transpiling the test files with Babel.\n3. Unclear what you're missing here? We output detailed failing assertions and the stack. We don't yet do diffing, but you can achieve that by using a 3-party reporter using the new $ ava --tap option (in the next version).\n4. No browser support yet. \u2192 https://github.com/sindresorhus/ava/issues/24\n. @ariporad Seems cleaner to use an error subclass as it can be detected as such. Normal errors are expected to have a .stack property.\n. @novemberborn There are some things that can be done even if the worst offender, Babel and fsevents, are out of our control. Many packages include junk like tests or examples directory, etc. I did a PR marathon a few years ago for Yeoman on 100+ different packages adding a \"files\" property to their package.json to reduce the total disk size of the dependency tree. That helped a lot. Pretty much have this still open for when I have time to do the same here.\n. > Actually, I think it would be better if Babel did that (all the Babel 6 plugins should depend on a differently named module that exports babel-runtime@5). That way everyone benefits.\n:+1:\n. @jokeyrhyme That's unfortunately not an option for so many reasons. Some of the best features of AVA require Babel, so I don't see why anyone would want to drop it for some minor space savings. The biggest Babel size issue is about to get fixed anyways https://github.com/babel/babel/pull/3438.\nIf you want to help out with this, I'd suggest you find packages in the dependency tree that includes junk like test/examples/etc folders and ask them (or better yet do a PR) to use the files property in package.json.\n. Here's a list of packages we depend on that doesn't have a files entry or .npmignore. Please help us reduce the size of AVA by submitting PRs to those packages adding a files property in package.json.\nTip: $ npm i -g npm-home and then just write nh package-name to go to a specific package.\n- [x] acorn-jsx@3.0.1 - master has .npmignore\n- [x] amdefine@1.0.0 - master has .npmignore\n- [x] argparse@0.1.16 - master has files entry in package.json\n- [ ] array-map@0.0.0 - https://github.com/substack/array-map/pull/6\n- [ ] array-reduce@0.0.0 - https://github.com/substack/array-reduce/pull/4\n- [x] assert-plus@0.1.5 - master has .npmignore\n- [x] assert-plus@0.2.0 \n- [x] assert-plus@1.0.0 \n- [x] async@0.2.10 - master has .npmignore\n- [x] async@0.9.2\n- [x] aws-sign2@0.5.0 - no extra files\n- [x] aws-sign2@0.6.0\n- [ ] babel-core@6.8.0\n- [x] block-stream@0.0.8 - master has .npmignore\n- [ ] caseless@0.11.0 - https://github.com/request/caseless/pull/26\n- [ ] caseless@0.6.0\n- [ ] combined-stream@0.0.7\n- [ ] combined-stream@1.0.5\n- [ ] commondir@1.0.1 - https://github.com/substack/node-commondir/pull/6\n- [ ] concat-map@0.0.1 - https://github.com/substack/node-concat-map/pull/5\n- [x] core-js@1.2.6 - has .npmignore\n- [x] core-js@2.3.0\n- [ ] core-util-is@1.0.2 - https://github.com/isaacs/core-util-is/pull/13\n- [x] dashdash@1.13.0 - master has .npmignore\n- [ ] dashdash@1.13.1\n- [ ] deep-equal@0.1.2 https://github.com/substack/node-deep-equal/pull/39\n- [ ] deep-equal@1.0.1\n- [ ] defined@0.0.0 - https://github.com/substack/defined/pull/3\n- [ ] eastasianwidth@0.1.1 - https://github.com/komagata/eastasianwidth/pull/3\n- [x] escope@3.6.0 - master has .npmignore\n- [x] esrecurse@4.1.0 - master has .npmignore\n- [x] estraverse@1.9.3 - master has .npmignore\n- [ ] estraverse@4.1.1\n- [ ] estraverse@4.2.0\n- [ ] events-to-array@1.0.2 - https://github.com/isaacs/events-to-array/pull/1\n- [x] extsprintf@1.0.2  - master has .npmignore\n- [ ] forever-agent@0.5.2 - https://github.com/request/forever-agent/pull/35\n- [ ] forever-agent@0.6.1\n- [x] form-data@0.1.4  - master has .npmignore\n- [ ] form-data@1.0.0-rc4\n- [ ] formatio@1.1.1 - https://github.com/busterjs/formatio/pull/8\n- [ ] googlediff@0.1.0 - https://github.com/shimondoodkin/googlediff/pull/6\n- [ ] inflight@1.0.4 - https://github.com/npm/inflight/pull/2\n- [ ] inherits@2.0.1 - https://github.com/isaacs/inherits/pull/21\n- [ ] is-buffer@1.1.3 - Author refuses to fix: https://github.com/feross/is-buffer/pull/12\n- [ ] is-typedarray@1.0.0 - https://github.com/hughsk/is-typedarray/pull/2\n- [x] isarray@0.0.1 - https://github.com/juliangruber/isarray/pull/9\n- [x] js-yaml@3.0.1 - has files entry in package.json\n- [ ] json-schema@0.2.2 - https://github.com/kriszyp/json-schema/pull/68\n- [ ] jsonify@0.0.0 - https://github.com/substack/jsonify/pull/6\n- [ ] jsonpointer@2.0.0 - https://github.com/janl/node-jsonpointer/pull/26\n- [x] jsprim@1.2.2  - master has .npmignore\n- [x] mime@1.2.11 - master has .npmignore\n- [ ] minimist@0.0.8 - PR already filed\n- [ ] minimist@1.2.0\n- [ ] mkdirp@0.5.1 - https://github.com/substack/node-mkdirp/pull/104\n- [ ] mute-stream@0.0.5 - https://github.com/npm/mute-stream/pull/8\n- [ ] mute-stream@0.0.6\n- [x] nan@2.3.3 - master has .npmignore\n- [x] node-pre-gyp@0.6.25 - master has .npmignore\n- [ ] npmlog@2.0.3 - https://github.com/npm/npmlog/pull/32\n- [x] oauth-sign@0.4.0 - https://github.com/request/oauth-sign/pull/21\n- [ ] oauth-sign@0.8.1\n- [ ] optimist@0.6.1 - https://github.com/substack/node-optimist/pull/142\n- [x] path-is-inside@1.0.1 - has .npmignore\n- [ ] private@0.1.6 - https://github.com/benjamn/private/pull/11\n- [x] process-nextick-args@1.0.6 - MERGED: https://github.com/calvinmetcalf/process-nextick-args/pull/9\n- [ ] pseudomap@1.0.2 - https://github.com/isaacs/pseudomap/pull/5\n- [x] regenerate@1.2.1 - master has .npmignore\n- [ ] resolve@1.1.7 - https://github.com/substack/node-resolve/pull/99\n- [ ] resumer@0.0.0 - https://github.com/substack/resumer/pull/3\n- [ ] rx-lite@3.1.2\n- [ ] rx@4.1.0\n- [x] slide@1.1.6 - does not contain extraneous files\n- [x] spdx-correct@1.0.2 - master has .npmignore\n- [x] spdx-exceptions@1.0.4 - no extraneous files\n- [x] spdx-expression-parse@1.0.2 - master has .npmignore\n- [ ] symbol@0.2.1 - https://github.com/seanmonstar/symbol/pull/1\n- [x] table@3.7.8  - master has .npmignore\n- [ ] text-table@0.2.0 - https://github.com/substack/text-table/pull/8\n- [ ] through@2.3.8 - https://github.com/dominictarr/through/pull/42\n- [x] time-require@0.1.2  - master has .npmignore\n- [ ] tmatch@2.0.1 - https://github.com/tapjs/tmatch/pull/1\n- [x] tunnel-agent@0.4.2 - https://github.com/request/tunnel-agent/pull/18\n- [x] tv4@1.2.7 - master has .npmignore\n- [ ] typedarray@0.0.6 - https://github.com/substack/typedarray/pull/13\n- [x] uid-number@0.0.6 - no extraneous files\n- [ ] uid2@0.0.3\n- [ ] util-deprecate@1.0.2\n- [ ] validate-npm-package-license@3.0.1  - master has .npmignore\n- [ ] window-size@0.1.0 - master has files entry package.json\n- [ ] wordwrap@0.0.3 - https://github.com/substack/node-wordwrap/pull/13\n- [ ] wordwrap@1.0.0\n- [x] wrappy@1.0.1 - https://github.com/npm/wrappy/pull/2\n\nThe list was generated with:\n\u276f package-config-checker | grep \u2716 | sed 's/^ *\u2716/- [ ]/g'\n. LGTM\n. LGTM\n. LGTM\n. No, the stack could still be useful. We rather check the type when outputting the error.\n. No, the stack could still be useful. We rather check the type when outputting the error.\n. @ariporad At least one other maintainer need to review it.\nNext time try to make your PR focused. This should probably have been two separate PRs.\n. 1. Any suggestions for what it should be named?\n2. Yeah, let's just do that by default and put the coverage in a separate run script and update .travis.yml\n3. https://github.com/sindresorhus/ava/issues/70\n. 1. Any suggestions for what it should be named?\n2. Yeah, let's just do that by default and put the coverage in a separate run script and update .travis.yml\n3. https://github.com/sindresorhus/ava/issues/70\n. > child.js?\n:+1: \n\nCan I go ahead and implement #70? It seems no one has done so.\n\nI'm not sure I want to deal with that right now. We have enough Babel related overhead and issues. Watching comes with its own headaches. The watch functionality in Node.js core is unreliable and the various watcher libs have edge-cases and downsides, like requiring native addons, which is a no go for us. We've added a lot of features lately, and while I realize it's more fun adding features than fixing bugs, I think we should prioritize the latter right now. My top priority is stabilizing the core and APIs, fixing bugs, and improving performance.\n. > child.js?\n:+1: \n\nCan I go ahead and implement #70? It seems no one has done so.\n\nI'm not sure I want to deal with that right now. We have enough Babel related overhead and issues. Watching comes with its own headaches. The watch functionality in Node.js core is unreliable and the various watcher libs have edge-cases and downsides, like requiring native addons, which is a no go for us. We've added a lot of features lately, and while I realize it's more fun adding features than fixing bugs, I think we should prioritize the latter right now. My top priority is stabilizing the core and APIs, fixing bugs, and improving performance.\n. > Should I implement a coverage-less test option\nActually, we already have test-win for that. We could just rename it into something more generic. (and update appveyor.yml)\n\nnpm run watch too?\n\n:+1: from me. @jamestalmage ?\n. > Should I implement a coverage-less test option\nActually, we already have test-win for that. We could just rename it into something more generic. (and update appveyor.yml)\n\nnpm run watch too?\n\n:+1: from me. @jamestalmage ?\n. @jamestalmage Do you have any preference on having coverage by default when running npm test? I never use it personally so wouldn't mind moving it into test:cov, but don't have any strong opinion about it.\n. @ariporad It's a good idea, but too annoying in reality. Nobody reads the contributing guidelines, so you end up having to tell people (often multiple times) how to format the commit. It creates annoyance for both the contributor and maintainer with minimal gain. I've seen this so many times in the wild. As for semantic-release, I'm not convinced.\nThat being said, I would be happy to improve on our commit messages, but don't want to enforce it on contributors.\n. :+1: test-worker.js\n. Could be more succinct. Something like, but better:\n\nAssertion count of 4 does not match planned of 5\n. Could be more succinct. Something like, but better:\nAssertion count of 4 does not match planned of 5\n. @jamestalmage I would still want to know which assertion and test that triggers the \"incorrect plan count error\".\n. Tests are failing.\n. Tests are failing.\n. @ariporad Sorry, missed this one. Can you rebase on master and fix the merge conflict?\nDo AvaError's need .operators, .actuals or .expecteds?\n\nNo\n. So, what should we do about this PR?\n. Would like to see this land, but closing for now to clean up inactive PRs. @ariporad Feel free to do a new one if you ever have time ;)\n. > I'm not sure if I'm allowed to do this\nYou're allowed to ask about anything. That applies to anyone lurking on this issue tracker.\n\nOr just use the assign feature\n\nI added you to the repo so you can assign yourself stuff and handle issues, but don't commit directly to master ;) We should be better at using the assign feature. Feel free to go through the issue tracker and add labels where appropriate.\n\nWould it be possible to get a blocked label\n\nJust added it https://github.com/sindresorhus/ava/labels/blocked.\n. Any other feedback to our process welcome :)\n. > Also, should do you prefer internal branches or forks?\nUse your fork for now.\n. > and a link back to the original discussion.\nIn the format Fixes #234234, so the original issue is closed when the PR is merged.\n. LGTM when squashed.\n. Good stuff @ariporad :)\n. LGTM\n. @ariporad \u2192 https://github.com/sindresorhus/ava/commit/437e88f203791f9828b0d746e4c36e192079197f\n. LGTM\n// @ben-eb\n. > Instead of separate errors and tests arrays, should it just be tests? A test could have a status attached (passed / failed), and all assertions with their respective results. Might be better than just standardizing errors as described in #365\n:+1: Can you open an issue?\n. You can follow https://github.com/sindresorhus/ava/issues/24 regarding browser support.\n. LGTM\n. ./node_modules/.bin/tap => tap?\n. > Node.js stable releases are even numbers, so 5.3.0 is not a stable release.\nNot exactly true. All major versions are stable, it's just the even ones are LTS releases. https://nodejs.org/en/blog/community/node-v5/\n. Nice! :+1: \n. Nice! :+1: \n. Woo, nice improvement on cold start! :+1: \nGenerally looks good to me. Could use a test though.\n@vdemedes ?\n. I don't think we should run all tests with and without. Just need some integration tests to ensure it's working correctly. What you've proposed sounds fine.\n. Probably related to https://github.com/sindresorhus/ava/pull/360. You no longer need to await t.throws.\n. Probably related to https://github.com/sindresorhus/ava/pull/360. You no longer need to await t.throws.\n. I noticed this too. This is because of the animated reporting. Although, logging like that isn't very useful when you have more than one tests as the logging is synchronous while the test is not, so the logging could be outputted with a different test. I think a solution here is to handle the logging so that it's outputted with the correct test in verbose mode and with the default minimal reporter we could either show all logging at the end or during the testing by pushing the animated output down (meaning calling logUpdate.done()).\nFor now you can use the --verbose flag.\n// @vdemedes \n. Yes, 3 is what I was thinking.\n\nFor the mini reporter, only output logs with failed tests\n\nI think we should still log it with the mini reporter as it's still useful to quickly print out some values.\n. Yes, we could, but I have a feeling that comes with a lot of edge-cases, so we should first figure out t.log.\n. > The output of the mini reporter offers no context for log statements in passing tests anyways.\nUsually I just console log in one place so I don't really need a context, but ok with using --verbose when I do that.\n. Yup, already suggested something like that initially, using logUpdate.done(). Would be nice.\n. I don't think it's worth dropping Bluebird. Long stack traces are very useful, especially when it comes to testing.\n50ms is a long time for just require & parsing though. Might be worth looking more closely into why it takes so long. Maybe there's something we could do to improve it or at least open an issue on the Bluebird issue tracker about it.\n. > 82  bluebird (node_modules.../release/bluebird.js)  45ms  \u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 85%\nI was thinking using https://github.com/s-a/iron-node flame graphs to see what actually slows down the loading. Maybe something in Bluebird could be made lazy.\n. @jamestalmage It's already tested in Babel I assume, but sure, if you want.\n. @jamestalmage Looks good to me. Just merge when you've fixed the merge conflict ;)\n. > Thoughts on a --no-babel option\nFor what purpose exactly?\n. > I think messing with the module path is a bad idea\n:+1: \n. @jamestalmage Anything blocking merging this other than the merge conflict?\n. @jamestalmage Any chance you could rebase this? This PR have been open forever and would be nice to get it resolved :)\n. LGTM\n. > We could then wrap babel-runtime to reduce installed package size too.\n@novemberborn Could you open an issue about this?\n. I think this should be in a docs/recipes/endpoint-testing.md linked to from the readme. The readme is already pretty long.\n. Thank you @mattkrick :)\n. Thanks, but I think the current is fine.\n. Good idea!\n. Pitfalls:\n- Using global variables and forgetting that tests run concurrently.\n- console.log not logging where the test result is printed (I hope we can fix https://github.com/sindresorhus/ava/issues/392, but we should document until we do)\n. > What if we made this a wiki page and linked to it from the README?\nNo. I want to have quality control on the docs. I don't want random strangers to be able to just freely modify the docs. Also: https://plus.google.com/+sindresorhus/posts/QSS2du26Mg4\n\nThat way others could contribute?\n\nThey already can with a pull request.\n. \ud83d\udc4d \n\ncommon_pitfalls.md => common-pitfalls.md\n. Another pitfall I see often is trying to do an async operation in a normal test and not understand why it's not finishing.\nForgetting to return the promise:\njs\ntest(t => {\n  fetch().then(data => {\n    t.is(data, 'foo');\n  });\n});\nOr should have used a callback test:\njs\ntest(t => {\n  fetch((err, data) => {\n    t.is(data, 'bar');\n  });\n});\n. Now documented here: https://github.com/avajs/ava/blob/master/docs/common-pitfalls.md\nGonna keep this issue open for discovery reasons.\n. I've seen people trying to pass CLI flags to their tests: $ ava --user-flag This doesn't work as we don't pass flags to the test processes. Instead, users should use environment variables.\n. @jfmengels Good idea! Can you add it?\n. @sotojuan Have you encountered anyone getting confused by those?\n. Can you copy paste the conversation for context? Like, what was confusing about it.\n. @sotojuan Yeah, definitely. If anything is unclear, we should make it clearer. Wanna do a PR improving it?\n. @cameronroe http://stackoverflow.com/questions/tagged/ava would be a better place to ask this. Include some more info too, like AVA and Webpack config.. @davewasmer Open a new issue. I think we simply need to extend https://github.com/avajs/ava/blob/master/docs/common-pitfalls.md#ava-and-connected-client-limits. @cameronroe You didn't include the information I recommended. Just a guess, but you need to use https://github.com/istarkov/babel-plugin-webpack-loaders or precompile your component before testing.. :+1: I do that with XO too. I would prefer only a section in package.json. Makes things simple.\nhttps://github.com/sindresorhus/pkg-conf\n. > I'd also like to add a \"files\" section\n:+1: \n\nand have it be \"config.ava\", vs. \"ava \"\n\n:-1: That's a convention nobody uses. Top-level is simpler and it will never conflict with anything anyways.\n. Hah, opening an issue like this was already on my todo list. :+1: :+1: \nIs this maybe something the power-assert reporter should handle?\n. From Gitter, for posterity:\n\njamestalmage\n@ben-eb power-asserts BinaryExpressionRenderer will output pretty diffs. So it would just be a matter of adding that in enhance.js.\nben-eb\n@jamestalmage here? https://github.com/sindresorhus/ava/blob/master/lib/enhance-assert.js#L47-L52\njamestalmage 01:05\n@ben-eb See the discussion in that PR. We don\u2019t want BinaryExpressionRenderer to always show a diff. It\u2019s too verbose.\nben-eb 01:06\ni wouldn't either, no\njamestalmage 01:06\nyep - that\u2019s where.\nI think the difficulty in fixing this is figuring out how to decide when to enable the BinaryExpressionRenderer.\nben-eb 01:07\nso, is it possible to set a length at where diffs would render?\nright\njamestalmage 01:08\nIt\u2019s not possible in BinaryExpressionRenderer itself, but if you make a good PR that is general use (i.e. not exclusively targeted towards AVA). @twada will accept it. Our other option is to just wrap the renderer and delegate to it only if certain conditions are met.\nI\u2019m not sure if it\u2019s as simple as string.length. It may be. Or you may need to examine the AST that is available in powerAssertContext that gets passed to the renderer.\nben-eb 01:10\nwhat about this option? https://www.npmjs.com/package/power-assert-formatter#optionslinediffthreshold\njamestalmage 01:12\nYep - that looks key. Note that we don\u2019t use that module. But build a formatter from individual renderers.\nerr - I\u2019m wrong on that.\nben-eb 01:13\nwas going to say it's required in that file :smile:\njamestalmage 01:14\nWe only use two of these five renderers\nThis may be doable with existing power-assert options.\nben-eb 01:18\nthink my only issue is that my string diffs don't have newlines, so I would want to be able to test against the length instead of the number of lines :smile:\nben-eb 01:40\ni didn't read that documentation properly, says it supports character level for lineDiffThreshold \nben-eb 03:17\nsorry james, i'm completely lost with this.\ni don't even know how i can break AVA's default assertions so that i can always show a diff\n. @creeperyang It's being worked on in https://github.com/avajs/ava/pull/1154.. Looks good :D\n. > Gotta say, didn't think this was only going to end up being 4 LOC. Small reusable modules FTW!\n\n. @ariporad What about it?\n. I would prefer to just add support for Map/Set in deeper. is-equal brings in 2.3 MB of dependencies.\n. Closing in favor of https://github.com/othiym23/node-deeper/issues/2.\n. LGTM :)\n. LGTM\n. I naively assume setting it in the terminal would be the best solution as it would fix it everywhere, not just in this project, but would be cool if someone knowledgeable about this would comment here (or email James).\n. Can you add a test? So we can ensure it doesn't regress.\nShould be added around here: https://github.com/sindresorhus/ava/blob/7b764ea3be2d0ad1d894d015feac8b38b7e071ce/test/promise.js#L179\n. Thank you @therealklanni :)\n. @jamestalmage Why not use log-update?\n``` js\nvar logUpdate = require('log-update');\nvar text = '';\nvar i = 0;\nsetInterval(function () {\n    text += Math.random().toString(36).replace(/[^a-z]+/g, '').substr(0, 5);\n}, 150);\nsetInterval(function () {\n    logUpdate(text + '\\n\\n  ' + i++ + ' passed');\n}, 50);\n```\n\n. @jamestalmage Awesome! All the visual tests passed perfectly now on Terminal.app OS X 10.11.2 (El Capitan).\n. Marvelous!\n\n. Should we open a new issue about matching console.log output to the relevant test, discussed in https://github.com/sindresorhus/ava/issues/392?\n. Yay! LGTM :)\n. We decided in https://github.com/sindresorhus/ava/issues/171 not to do a default timeout as there's no good default that fits all, instead we're going to add ability to set timeout.\n. \ud83d\ude01\nLooks kinda weird to have it double here:\nfalse == true\nAssertionError: false == true\nMaybe we can remove the AssertionError?\n. > We would just need to rework the reporters so it's their responsibility to print the message.\n:+1: Can you open a new issue about that? It's unrelated to this PR, just came to my mind when looking at your screenshots.\n. :+1: \n. @vdemedes Go ahead ;)\n. That's not needed. We handle that for you.\n. > I'm strongly against this one as it's very irritating when obvious & simple things (like console.log()) don't work as expected.\n:+1: I also want them visible by default.\n\nProvide a t.log method.\n\nDo we really need this if we manage to route console calls correctly?\n\nThe problem with domains is this:\n\nDomains are deprecated, but so much depend on it that they're not going to remove it anytime soon (source: Node.js core people). They don't even have an alternative yet. And the alternative is going to be powerful enough to enable building a user-land implementation of domains. More here: https://github.com/sindresorhus/ava/issues/214#issuecomment-157402230.\n. > Am I crazy or we could use cluster module for that?\nI'm not really familiar with the cluster module.\n. > I actually thought about running a worker per test.\nI considered that a year ago, but decided not to open an issue about it as it would come with a lot of overhead. Might be something worth exploring at some point as totally isolated tests would be so nice.\n. > You are probably right. I was more thinking this for t.log.\nThat's kinda what the message arguments in assertions are meant for, but I guess it could be useful for generic debug stuff when it fails. Not sure .log is the best name for something like that, though. Maybe t.debug(), which is shown when a test fail or in debug mode?\n. > Could be cool as a configurable option. Turn it off for faster testing on the local dev machine. On for your CI build so you ensure you are writing truly isolated tests.\nYeah, can you open an issue? I don't think we should prioritize it right now, but can have an issue to at least discuss and so we don't forget.\n. > Could be cool as a configurable option. Turn it off for faster testing on the local dev machine. On for your CI build so you ensure you are writing truly isolated tests.\nYeah, can you open an issue? I don't think we should prioritize it right now, but can have an issue to at least discuss and so we don't forget.\n. Paste your above comment into that issue so not to derail this discussion ;)\n. Paste your above comment into that issue so not to derail this discussion ;)\n. > With this proposal, the above will give you a nice, seemingly synchronous log:\nI don't really see that as much of a problem.\n. > Fresh global state for each test. This would be a big help for polyfill developers.\nThis would be huge, not only for polyfill dev, but anything that modifies global state.\n\nIt will almost certainly introduce a performance penalty.\n\nSometimes it's worth it. For example in https://github.com/sindresorhus/get-stdin I had to split my tests into two files as they somehow conflicted with each other.\n\nIt will be really hard to replicate when it comes to implementing browser support.\n\nThat won't be a goal either. We'll just run it normally there. Running files in parallel will also be gone when run in the browser.\n. > Fresh global state for each test. This would be a big help for polyfill developers.\nThis would be huge, not only for polyfill dev, but anything that modifies global state.\n\nIt will almost certainly introduce a performance penalty.\n\nSometimes it's worth it. For example in https://github.com/sindresorhus/get-stdin I had to split my tests into two files as they somehow conflicted with each other.\n\nIt will be really hard to replicate when it comes to implementing browser support.\n\nThat won't be a goal either. We'll just run it normally there. Running files in parallel will also be gone when run in the browser.\n. Oh, didn't realize that was possible. Make sure the mention that in https://github.com/sindresorhus/ava/issues/24 ;)\n. Oh, didn't realize that was possible. Make sure the mention that in https://github.com/sindresorhus/ava/issues/24 ;)\n. @ariporad Test hooks would still work as we would execute the file as normal, but only run a specific test (with hooks) instead of all.\n. @ariporad Test hooks would still work as we would execute the file as normal, but only run a specific test (with hooks) instead of all.\n. Ah, that's true.\n. Ah, that's true.\n. I'd rather look into solutions that don't require adding two new methods with very similar semantics.\n. I'd rather look into solutions that don't require adding two new methods with very similar semantics.\n. @ariporad Not at all. I just want us to explore other options before adding more API surface. Adding new methods and options are easy. Keeping things minimal while powerful is very hard.\n\n. @ariporad I know, but when rereading my previous comment I sounded a bit dictatorial, so just wanted to clarify ;)\n. I see both up and downsides. This issue is just to explore it. It's very likely we'll never do this.\n. Sure, but I don't see this being something we prioritize for 1.0.0. It's a nice to have, but not essential at all.\n. Sure, but I don't see this being something we prioritize for 1.0.0. It's a nice to have, but not essential at all.\n. Note to self: Update screenshot (https://github.com/sindresorhus/ava/pull/428) when this is done.\n. @naptowncode Correct.\n. @sotojuan What does the TAP spec say about it? When there are no skipped tests should it not be outputted or outputted with 0?\n. // @vdemedes \n. @ghpabs That is invalid. Use let if you want to reassign the variable. Nothing to do with Babel. \n. @ghpabs That is invalid. Use let if you want to reassign the variable. Nothing to do with Babel. \n. It's true that it works in sloppy mode (which the REPL is running in), but you should really not depend on that. It's the wrong use of const and it's not future proof. It will not work in strict mode or ES2015 modules, which implicitly run in strict mode.\n```\n\u276f node -e \"'use strict';const foo = 7;foo = 15;console.log(foo);\"\n'use strict';const foo = 7;foo = 15;console.log(foo);\n                               ^\nTypeError: Assignment to constant variable.\n```\nThe whole point of const is creating an immutable binding. If you want const with mutable binding it's pretty much exactly let.\n\nNot derailing this issue. We'll consider you request. Just wanted to point it out to you.\n. It's true that it works in sloppy mode (which the REPL is running in), but you should really not depend on that. It's the wrong use of const and it's not future proof. It will not work in strict mode or ES2015 modules, which implicitly run in strict mode.\n```\n\u276f node -e \"'use strict';const foo = 7;foo = 15;console.log(foo);\"\n'use strict';const foo = 7;foo = 15;console.log(foo);\n                               ^\nTypeError: Assignment to constant variable.\n```\nThe whole point of const is creating an immutable binding. If you want const with mutable binding it's pretty much exactly let.\n\nNot derailing this issue. We'll consider you request. Just wanted to point it out to you.\n. Yes, let's close it in favor of https://github.com/sindresorhus/ava/issues/448#issuecomment-190286370, which should handle that.\n. Applied the workaround and released 0.9.2. Sorry about the delay, I've been mostly afk today.\n. @forresst @AlbertoFuente You can find ready made translations here: http://contributor-covenant.org/version/1/3/0/es/code_of_conduct.md http://contributor-covenant.org/version/1/3/0/fr/code_of_conduct.md Just replace [INSERER ADRESSE EMAIL] with sindresorhus@gmail.com.\n. Yup, I just wanted everyone to be aware.\n. \ud83c\udf89\n. @jamestalmage Nah, we can mention it when we add a --debug flag.\n. :-1: I don't want to create another silo of reporters. We have two great builtin reporters that should work for most use-cases. For anything else, create a tap reporter.\n. We could consider including more AVA specific info in the YAML data block in the TAP output.. How about defaulting to the verbose reporter?\n. @vdemedes I meant defaulting to the verbose reporter on a CI, not normally. I love the mini reporter as default in my own terminal.\n. Tap output should IMHO be an explicit choice through --tap. I'd prefer isCI ? verbose : mini.\n\nava --tap | spec \n\nLet's add some short-flags then at least: ava -t | spec ;)\n. > I think I will prefer our reporters in most cases\nDitto. And if there's something you don't like about the built-in reporters, you're in a very good position to do something about it ;)\n. Glad we could simplify this logic out into a module :)\nAPI looks good. I would probably not have done the spread option as it's moot with ES2015 and would cause a lot of boilerplate without for now, but doesn't matter.\n. Yeah, that's exactly why I left out --fail-fast. I think it's better written out in full than a shortflag tbh.\n. Should be:\njs\ndefault: conf,\nalias: {\n  t: 'tap',\n  v: 'verbose',\n  r: 'require',\n  s: 'serial'\n}\nNo tests needed. Need to update help output and the help output in the readme though.\n. :shipit:\n. If a user uses Bluebird with long stack traces in their app code and something there throws in a test would we get long stack traces from that or does the whole chain need to use Bluebird to get the long stack traces? Long stack traces for the test themselves aren't that useful, but from the app code it definitely is.\n\nIt would be great if bluebird had a means of exposing a bare-minimum Promise implementation that was still super-fast and supported debug-ability (like maybe a superclass of bluebird).\n\nIndeed, but if you read the Bluebird source you'll see how coupled everything is, probably in the name of optimal performance. Wonder if we somehow could hide the Bluebird specific methods to the user.\n. Nice! :)\n. > Just tried the ES2015 version without the reporters, it needs at least one otherwise it throws, so I'll leave in the text reporter for now.\nCan you report that as a bug to nyc? It should support it as the default is text reporter:\n-r, --reporter  coverage reporter(s) to use                  [default: \"text\"]\n. Can you not hard wrap the text?\n. Add it to the recipe list in the readme: https://github.com/sindresorhus/ava#recipes\n. Awesome! This will be helpful for a lot of users. Thanks @ben-eb :)\n. > I usually inline the text and lcov reporters:\nThat adds bloat to your package.json with limited gain. The more boilerplate we add to package.json, the less likely it is people will bother using it. \"People\" here being me too.\n\nbcoe/nyc#137 is part of the reason why. nyc report can fail if anything happens to the cache between running the tests and generating reports.\n\nThat should just be fixed in nyc.\n\nI prefer after_script to after_success because it's still helpful to have coverage info when reviewing WIP PR's that might still have a few failing tests (or if the project is plagued by a few finicky tests).\n\nYou can still have that locally, but I don't think failing PRs should add noise to the Coveralls history.\n\nIn this case however, I wouldn't use it. Instead I would add it to AVA's config in package.json:\n\n:+1: \n. @vdemedes Thoughts?\n. This looks interesting: https://twitter.com/bruderstein/status/718113603484823553\n\nPlugin for unexpected to allow for testing the full virtual DOM, and against the shallow renderer\n. I like the idea in general, but I'll need to think about how to handle the specifics.\nmaybe we automatically append power-assert for them?\n\n:+1: Would be too bad if people lost out on this one and can't think of any bad side-effect of doing it.\n\n// @ariporad  @JaKXz @MoOx @kentcdodds @sotojuan @SamVerschueren @kevva \n. @kentcdodds I would generally agree, but think of power-assert more like an internal plugin. It doesn't affect the functionality of the code in any way. Babel is only one of many ways to add power-assert. We use Babel since it's already parsing the source, so we can take advantage of that. It's completely different from us choosing es2015 as a preset, which does affect the code.\n. @develar There are still some unanswered questions:\n\nThere are a couple questions that need to be answered in order to proceed. Specifically, can we change which env Babel uses on a per file basis? It might be as easy as setting process.env.BABEL_ENV before each call to babel.transform(..)\n. @develar There are still some unanswered questions:\nThere are a couple questions that need to be answered in order to proceed. Specifically, can we change which env Babel uses on a per file basis? It might be as easy as setting process.env.BABEL_ENV before each call to babel.transform(..)\n. @talexand You can put your Babel config in package.json under the babel field. We would of course support both.\n. @talexand You can put your Babel config in package.json under the babel field. We would of course support both.\n. @spudly :+1: That's some very good cents. I like it. \n\n@jamestalmage @vdemedes You good with this?\n. @spudly :+1: That's some very good cents. I like it. \n@jamestalmage @vdemedes You good with this?\n. @spudly Yes, that's resolved with your solution.\n\nIs this something you guys need help implementing?\n\nYes. That definitely make it happen faster ;)\n\nreact won't be included on default \".avarc\" ? if so, why? To not have the babel profile as a dependency?\n\n@axyz No, because not everyone need React and adding it for everyone adds unnecessary bloat and slower performance because of added transforms.\n. @jamestalmage :+1: That would indeed make it very flexible and nice.\n. I think it should also be possible to disable Babel completely with: \"babel\": false, per #424.\n. Reopening this as we still need to implement babel: false and the extends feature.\n. New AVA version is out with customizable Babel config.\n. :-1: I don't see how this is any improvement. It's both harder to read and more code.\nAnd incorrect. It would have to be:\n``` js\nimport test from 'ava';\nimport fn from './';\ntest('title', t =>\n  return fn('foo').then(data => t.ok(data))\n).then(() =>\n  return test('title2', t =>\n    return fn('bar').then(data => t.ok(data))\n  )\n);\n```\nI think the serial tests are already plenty explicit about their dependencies. They're named serial tests and run serially in the order you specify them.\n. > Do you mean the lack of a return? Arrow expressions automatically return, so return is not needed.\nAh, didn't notice that, but doing braceless arrow function for anything non-inline isn't very readable.\n\nTo me it's clearer which tests rely on other tests and makes more sense than a long string of serial tests which aren't necessarily connected.\n\nIdeally your tests should be atomic and not have a dependency on other tests. test.serial is mostly there to fill a small niche when you have to modify some global state or something can't handle the concurrency.\nI would help if you presented a realistic scenario where your proposal would be useful.\n. Generally looks good. Commented some minor nitpick.\n. :shipit: \n. > One goal is to support addressable test indexes which could be used implement #421.\nIs there any benefit to this other than for #421? #421 should really not have any impact on how we structure the code. It's just an early discussion that could easily be abandoned.\n. Alright\n. Would be cool if someone familiar with this could write a recipe (/docs/recipes).\n. > Not sure about this, as it's out of AVA's scope in my opinion. \nI agree.\n\nA basic continuous runner that keeps the process open for instant feedback. I can turn this into a recipe as well.\n\nI think it would be better as a pull request for https://github.com/sindresorhus/ava/issues/70.\n. @tomazzaman \\o/ :)\n. @jamestalmage How do you think we should we handle this? The verbose reporter already checks if there's a .stack before beautifying: https://github.com/sindresorhus/ava/blob/3fdb6fc1368b0ce6dc91c9fb408e98f7fbed5d32/lib/reporters/verbose.js#L48, but the mini reporter does not: https://github.com/sindresorhus/ava/blob/3fdb6fc1368b0ce6dc91c9fb408e98f7fbed5d32/lib/reporters/mini.js#L107 Should the reporters really have to do these checks? I kinda feel the API itself should guarantee err is always an error.\n. @jamestalmage did a superb job adding a pretty benchmark in https://github.com/sindresorhus/ava/pull/496. It's easy to add additional things to benchmark, so suggestions welcome.\n. Instructions here: https://github.com/sindresorhus/ava/commit/c22eabd717451dbeaeeff1520f399f5a5fd85764\n. :+1: Looks good! (\u1d54\u1d25\u1d54)\nToo bad https://github.com/jamestalmage/ava/commit/4e23943eb9a54fb74ada303f3bfed81d7aade933 was slow. It was more readable.\n. > though not one I am really concerned about\nMe neither.\n. (\u3064\u25d5\u0c6a\u25d5)\u3064\u2501\u2606\uff9f.*\uff65\uff61\uff9f\n. Can you add it to the readme: https://github.com/sindresorhus/ava#recipes\n. @tomazzaman No, you never need to resubmit the PR. Just push additional commits. See this guide: https://github.com/RichardLitt/docs/blob/master/amending-a-commit-guide.md\n. @tomazzaman Would you be able to finish this? :)\n. Closing for lack of activity. This is pretty moot now anyways as we finally have a solution for the Babel mess https://github.com/sindresorhus/ava/pull/573.\n. Sounds like a bug. It should only show the .only() test.\n. @sotojuan I manually reran the Node.js 0.10 test. Passes now. Chalk it up to flakiness...\n. LGTM\n. Closing in favor of #502.\nSorry @tomazzaman, but that one is a lot more comprehensive.\n. :clap: From a quick skim this looks very promising, although a lot to take in. I like the improved separation of concerns and performance improvements, but not totally happy about the added verboseness. Would be nice if we could follow up later with ways to simplify it. I'll try to do a more thorough review tomorrow.\n\n@vdemedes @ariporad @sotojuan @novemberborn I could use some help reviewing this :)\n\nTips for reviewing: Review the commits individually and use https://github.com/sindresorhus/ava/pull/466/files?w=1 to review the complete diff ignoring whitespace changes (for a more readable diff).\n. > I have some style/cleanup-related suggestions, but I can follow up with a PR myself ;)\nYes, let's do style stuff later. (I did a few myself, but realized it's better not to dilute the review with nitpicks).\n. > Once merged, we should all dogfood this on existing projects before release\n:+1: I've already tried it on got and pageres and seems to work good.\n\nmaybe deploy with a next tag for a while as well\n\n:+1: \n. Landed! Awesome work @jamestalmage :)\n\n. Excellent. Thank you :)\n. :+1: :+1: :+1: \n. :+1: This also catches the case of forgetting to remove an .only().\n@therealklanni Interested in doing a PR?\n. > Should the use of .only() cause a non-zero exit code? Or the ability to configure it to exit with a non-zero exit code in that case?\nI like the idea, but need to think about possible cases where you don't want it to fail. I'm also not very interested if it has to be configurable. Trying to keep config to a minimum.\n\nWe could probably create a linter rule that handled warning about .only() usage\n\nAlready planned regardless: https://github.com/sindresorhus/eslint-plugin-ava/issues/5\n. > It might be interesting to offer a --disallow-only flag that could be used on your CI server to ensure .only didn't make it into master. Better yet, automatically detect and disallow using is-ci (with --allow-only to override?).\n:-1: That's better covered by the no-only-test ESLint rule.\n. > I think we had a similar discussion regarding t.end for non-cb tests and decided it was a better user experience to build in the error.\nThe difference is that was 100% predictable and detectable. This will require a flag, which you know I'm very conservative about. Automagically doing it on CI is too magic and people might want to allow .only on CI to quickly test something on more machines for example.\n. This should also include handling dependency mocking.\nFrom Gitter:\n\nany best practice or suggestion on how to handle external dependencies while keeping tests running in parallel? Currently I'm using babel-plugin-rewire and on each test I call a function that overwrite the previous rewire. However the imported module is of course global and I may end up overwriting a rewire while a test is running in parallel that is not good... What exactly happens when ava spawns a new process for each test? If the imports are going to be resolved each time, maybe this is not a problem, otherwise I'd need a solution (maybe using require inside a function that will return the mocked dependency...) any experience on similar situations?\n. @isaacs Seems your PR https://github.com/TestAnything/testanything.github.io/pull/36 has been open for a while. What would it take to get it merged? Anything we can help with?\n. > I like this idea too.\n\n:+1: \n\n@therealklanni Done. Sorry about the delay. I've been traveling.\n. Can you quickly mention how to use this in maintaining.md?\n. Can you add a .iron-node.js config file? With these to true: https://github.com/s-a/iron-node/blob/85198516e90bb9e03d45efe0bd361ad4a4fe467e/.iron-node.js#L11-L12 \n. Tried it now. What's the workflow for profiling? The tests run on startup and finish before I manage to do anything. Is the recommended practise to put a debugger; statement at the top of the test file?\n. Awesome. Excited to have an easy way to perf test AVA.\n. @s-a I added a short section in https://github.com/sindresorhus/ava/blob/master/maintaining.md#profiling. Was hoping @jamestalmage would fill it out with his workflow.\n. Relevant: https://github.com/Jam3/devtool#iron-node\n@mattdesl Is there anything devtools is worse at than iron-node for our needs (profiling CPU usage)?\n. @jamestalmage console.profile('foo') and console.profileEnd('foo').\n. I agree we should have something like this, but I'd like to discuss how we can innovate on this instead of just copying the Mocha option. I also have a feeling if we add this, people will follow up and demand --fgrep (only run tests containing ) and then --invert (inverts --grep and --fgrep matches). I don't want more than one flag for this.\nMaybe we could use globs instead?\n$ ava --match='fo*' --match='!bar'\nRegex might work well too. Just putting it out there.\n. Can you think of any use-cases where a glob might not work?\n. And to be clear, !bar is a negated match, so we wouldn't need another flag for that.\n. > I can think of many times where mocha's grep was too limited to the point where I don't even use it so pretty much anything (especially globs) would be a vast improvement.\nWould be awesome if you could provide some examples. That would help make a decision.\n. @matthewbauer There's no \"for now\". Anything we add we'll have to support for a long time, and as I originally said, not interested in more than one flag for matching tests. I think we should figure out how to do it correctly. Not just a quick fix :)\n\nbecause RegEx/glob matching is fairly expensive when done on large test suites\n\nNo. The regex and globbing are still just matching against strings. You'd have to have hundreds of thousands of tests for it to matter at all.\n. I don't think we should combine test paths and titles.\nBeen thinking about it and maybe even using globs is overdoing it a bit. Globs have a lot of archaic rules and is actually really complex and really meant for paths. Maybe we could do our own simple replacers? * will match anything and ! at the start negates the match. Do we really anything more? The common use-case for non-exact matching is cases like foo - 1 foo - 2 foo - 3, and you could do foo - *.\n. @jamestalmage My thinking is that some characters require escaping in a regex, so this would be simpler. And foo - * would really be ^foo - .*$.\n. > I don't see how to do that with what you are suggesting.\n$ ava --match !*@slow. ! negates it. * is wildcard. It matches from the start to the end of the string. Equivalent to --match !/^.*@slow$/.\n\nI really don't want 1, but I would be ok with 2 too.\n. @SamVerschueren It seems @vdemedes would like it even simpler and drop *.\n@vdemedes It's a custom matcher, true, but an extremely simple one. The benefit with my proposal is that it's a lot more flexible while only being marginally harder to grok. With your proposal it would be impossible to match an exact title, start or end of a title, or a specific location in the title.\nSome examples of uses of my proposal:\n- Exact title: foo\n- Start of title: fo*\n- End of title: *oo\n- Middle of title: f*o\n- Anywhere (like your proposal): *o*\nNot being able to match the exact title is a big loss. It means it won't be possible to use the matching to programmatically to run specific tests. I imagine tools could use this. Like an editor plugin running the test your cursor is in.\n. Regardless of the outcome here, I made a simple module for wildcard matching as I can see it being useful in other cases too. Could use some feedback on it: https://github.com/sindresorhus/matcher/issues/1\n. Alright. Seems we have an agreement :)\n@matthewbauer Can you update your pull request to use matcher?\n. https://github.com/sindresorhus/ava/pull/477 went stale, so this is now PR welcome again.\n. @matthewbauer Yes * and ! should be possible. And it should be possible to use it multiple times, e.g. --match='*oo' --match='!foo', so it would be better to use matcher() against the test collection as matches() handles the filtering correctly for you.\n. :+1: I usually prefer that too as it also has clearer intent.\n. @jamestalmage Let's wait until after the weekend. Would like @vdemedes feedback on #476.\n. @matthewbauer Can you also fix the merge conflict?\nSorry about the back and forth. Glad we finally have a resolution on this.\n. Should it be possible to use it multiple times? I'm thinking --match='*oo' --match='!foo'.\nWe also need to document the syntax somewhere, the * wildcard and ! negation.\n. @matthewbauer Just a friendly ping :)\n. Closing for lack of activity. Would be happy to reopen if this get's any updates, but I want to give other users the opportunity to work on this.\n. @Bearlock This issue is for master, not the published release.\n. Nah, if I accept this we'll have to bump it for every version. We'll bump it to 1.0.0 when we get there. Thanks though.\n. @jamestalmage I think that's overdoing it. I don't see the point. It will be moot anyways when we do 1.0.0. And it's just an example. Not meant to be copy-pasted.\n. @JasonRitchie Of course. Go ahead :). Yeah, doesn't make sense to have deeper stack trace for this as you only care about where the plan was defined.\n. :+1: \n. :+1:\n. Does it have any non-neglibable perf impact? The result event is a nice convenience.\n. Alright. Then I'm :+1: \n. LGTM\n. LGTM\n. Thank you @sotojuan :)\n. @jamestalmage Can you open an issue on devtool about this? It's weird that setImmediate is required.\n. Very good! Can you add it to the Recipe section in the readme?\n. LGTM\n. When there's no test title, instead of outputting an empty line, it should output nothing.\nRight now it outputs the following:\n```\n\u276f ava --verbose\n1 test passed\n```\nNotice that there are 3 empty lines above and only 2 below.\nInstead it should only output 2 empty lines above:\n```\n\u276f ava --verbose\n1 test passed\n``\n. I would go with1`.\n. Looks superb! Such pretty tables :)\nI think it would be useful with a quick script to automate gathering data from master and the last commit and then run the tests. This would be the most common operation, so would be good to automate it, but we can do that later.\n. \n. LGTM. Just squash and merge when Windows tests are passing.\n. It looks like the stack traces on Windows have backward-slashes. I thought stack-utils converted all paths to forward slashes?\n. It's weird, because it always works the second time when restarted.\n. @sotojuan Yeah. Opened this because @jamestalmage mentioned serial might be broken on master.\n. Already done: https://github.com/sindresorhus/ava/commit/c22eabd717451dbeaeeff1520f399f5a5fd85764 ;)\n. This looks very promising. I'm happy about how non-intrusive this is. Great work @novemberborn :)\n\nPutting patterns in a CLI flag is tricky though given that the ava invocation is supposed to end with a pattern. Perhaps we could use --watch-pattern=\"\" for this case. The pattern should behave the same as the files pattern.\n\nHow about ava test/*.js --watch=src/*.js --watch=!src/bundle.js?\n\nLike in nodemon you can type rs\\n to rerun all tests. This is useful when you made a change that was not picked up.\n\nDo you know why rs\\n was chosen? Just r would be faster to type.\n. > Does it mean watch only test/foo.js and run the default test setup? Or only run test/foo.js and watch all the normal files?\nThat's why we define types here, to make it non-ambigious. Also why I much prefer using --key=val instead of --key val, as it will never be ambiguous.\n. Easier to explain with code:\n``` js\nvar minimist = require('minimist');\nconsole.log(minimist(['ava', '--watch', 'src/.js', 'test/.js'], {string: ['watch']}));\n/\n{\n    _: ['ava', 'test/.js'],\n    watch: 'src/.js'\n}\n/\n```\nhttp://requirebin.com/?gist=0b900158776c2a2a4100\n. Ok, good point. Was just trying to not have to introduce yet another flag... --sources sounds good.\n. > We can leave it as a feature request/improvement, so that this PR gets in faster ;) Don't want that amount of hard work to just wait around!\n:+1: Would like this too, but can be added later.\n. > I'd much rather it required glob patterns that matched files and not directories.\nIt's for humans. Most users don't really grok glob patterns completely and it's nice for them to be able to just do ava dir instead of ava dir/**/*. I don't think we should sacrifice this convenience.\n. > This means the key in package.json is also source, which is different from files, but consistent with require. Thoughts?\n:+1: \n. \u2728Amazing work on this @novemberborn. It turned out really good! :beers: \n\n. > and open an issue regarding cleaning up the internals\n@novemberborn @jamestalmage Can either of you open issues for follow-up things to discuss?\n. @forresst Yes, but undocumented for now as we want to make sure it's good before we get a gadzillion support issues for it.\n. :+1: Generally looks good :)\n. You missed https://github.com/sindresorhus/ava/pull/503/files#r51689060 ;)\n. Thanks for fixing so quickly :)\n. Looks good, but not sure of its use-case when it's just a rendering of the readme?\n. > Documentations accessible via a web application with a navigation can help some users.\nWe already have a Table of Contents menu in the readme.\n\nAnd AVA can be more popular with it.\n\nNot really a concern right now.\n\nNone of the arguments are very convincing.\n@vdemedes @jamestalmage What do you think about adding this?\n. :+1: This has irked me too. t.doesNotThrow was initially used to be compatible with Node.js core assert, but we no longer care about that, so I think t.notThrows would be a good change. We can keep an undocumented alias to t.doesNotThrow for backwards-compatibility.\n. You also need to update the reference in lib/enhance-assert.js, and in test/test.js:455.\n. I don't see your changes. I think you forgot to push.\n\n(unless that's not a problem).\n\nThat's not a problem.\n. Looks good. Nice work @sotojuan :)\n. > It's the last line of the clean function.\nIt should return an empty string there.\n. > There is actually a test that to check if it returns null if it are all internals.\nDefinitely not what I would expect when the method name is .clean().\n\nWill create an issue and do a PR.\n\n:+1: \n. @novemberborn I assume this can be closed now then?\n. I would also at the end mention some of the benefits of using beforeEach. From #481:\n\nStill, there are benefits of beforeEach too. They're not mutually exclusive. beforeEach has benefits like supporting non-promise asynchronicity. Better output when a hook fails. Void hook usage doesn't require a call in each test. And more.\n\nThere might be more I haven't thought of.\n. @jarrettmeyer ping :)\n. Closing for lack of response by OP.\n. Closing for lack of response by OP.\n. Yes, we need to resolve this soon. We need more feedback on #448 ;)\n. As James pointed out, it could just be a warning.\n. That will only benefit people using ESLint and our plugin, though. If we can do things directly in AVA without any negative side-effects, I think we should.\n. Agreed. I'm just opening issues when I have ideas. It's not like we have to implement everything now. This is btw what the blocked (suggestions for a better title for it?) label is for. I do think this should land before 1.0.0, though, as it's a breaking change.\n. > Add a --werror flag to the cli. Stolen from gcc, the --werror flag treats warnings as errors.\n:-1: This will already be handled by: https://github.com/sindresorhus/eslint-plugin-ava/issues/7\n. @novemberborn Yeah, let's just go with the ESLint plugin for this.\n. @novemberborn Yeah, let's just go with the ESLint plugin for this.\n. Cleanup code should be in the after/afterEach hooks, so why not run them when tests are cancelled?\n. > Every concurrent test in the file is launched in a single turn of the event loop, once you have kicked off the \"run the concurrent tests\" phase, there is no turning back.\nWhat I was thinking is that even though we have started the concurrent tests we can still choose to ignore their result and pretty much unref / mark them as aborted, and ignore. Like an unref'd/detached child process, it still runs to completion, but the parent process don't care and have no communication with it anymore.\n. @novemberborn Yes, but the idea is that the after/afterEach will run right away when it's unref'd.\n. We're intentionally only using package.json for simplicity and consistency.\n\nI'm not a huge fan of keeping config in my package.json\n\nWhy? We are not big fans of having an endless amount of meta files in our repos. It's IMHO nice to have one place for all config. This is becoming more and more common, with both AVA, ESLint, XO, nyc, standard, etc, tools doing it.\n\nThis is also pretty common practice with other tooling frameworks\n\nCommon practice doesn't necessarily mean it's a good one.\n. @mattecapu Sorry, we're not interested in having multiple ways to define config.\n. @smithamax Yup, that's already planned: https://github.com/sindresorhus/eslint-plugin-ava/issues/5 ;)\n. Closing as the ESLint plugin is very mature now.\n. The current behavior is there for a reason: https://github.com/sindresorhus/ava/issues/32 https://github.com/sindresorhus/ava/issues/175\n. This is already documented in the CLI help and in the readme, though: https://github.com/sindresorhus/ava#cli\n. First-timer experience is very important information. I appreciate you sharing it. I'm just not a big fan of duplicate information. Do you really need to know all that when getting started? I'm just thinking out loud. Would it be enough if the docs told you to name the test file test.js and put it in root? Then everything would just work. You can later learn the test file naming conventions and other things.\n. Cool. You also seem to have included some unrelated changes. \n. Thank you :) Do let us know if there's anything else that could be improved with the docs. We really do want AVA to be easy to start using for anyone.\n. It would be weird if t.throws is plural while the inverse method t.notThrow is singular. Compatibility with node-tap is not a concern as we already differ in too many ways. Changing the node-tap usage of t.doesNotThrow to t.notThrow is just unnecessary code churn.\n. Thanks for the PR, but as commented in https://github.com/sindresorhus/ava/issues/528#issuecomment-181673612, this is not something we're interested in :)\n. > Errors are not always instances of AssertionError, so we definitely should not just remove the first line of err.stack.\n@vdemedes But the message is always the first line of the stack no matter what kind of error.\n. @naptowncode This looks good to me. Can you fix the merge conflict?\n. Thank you @naptowncode :)\n. Ugh, I really dread this change. Not the PR itself, but the fact of having to support NODE_PATH, which is deprecated and awful in so many ways...\nWouldn't it be better to just pass all environment variables from the parent to the worker? That way we wouldn't have to special-case future environment variable needs.\n. js\nif (process.env.NODE_PATH) {\n    process.env.NODE_PATH = process.env.NODE_PATH\n        .split(path.delimiter)\n        .map(function (p) {\n            return path.resolve(p);\n        })\n        .join(path.delimiter);\n}\nThen?\n. Yes, or even better, just pass this directly to the env option when forking the worker, without overwriting the current process.env.NODE_PATH.\n. Yup, and keep in mind that the whole existing process.env needs to be passed to the fork thing to preserve existing behavior, so use object-assign to copy the object and then modify the NODE_PATH key.\n. @ingro You resolve the merge conflict. (No need to open a new PR)\n. I like that idea. Ideally the CLI should just be a tiny wrapper around the API, with CLI only stuff.\n. :+1: Sounds good. I would still want minimal output in non-watch mode.\n. LGTM\n. Thank you @sotojuan! Good work as usual.\n. > hzhu/dev/test/node_modules/power-assert-formatter/node_modules/acorn/dist/acorn.js:945\n@twada Any chance power-assert has something to do with this issue?\n. @BarryThePenguin Yup.\n@twada Thanks for the quick response. That did it for me.\n. :+1: Generally looks good to me, but would like @jamestalmage to take a look before merging.\nCould you share some benchmark results?\n. // @vdemedes \n. :+1: \n. Generally looks good, but would like @jamestalmage to review here too, as I'm not that familiar with this part of the code.\nMaybe @novemberborn would be interested in reviewing too.\n. Does this have any perf side-effects? Maybe worth running the benchmark?\n. I honestly think it's a negligible perf regression. We should optimize for what matters instead. Whether we have a 1ms or 2ms overhead per test doesn't matter much when the test itself takes 100ms to execute. I think we need a more realistic benchmark so we don't end up bikeshedding over numbers that doesn't really give us any perceived perf improvements for the common-case.\n. I honestly think it's a negligible perf regression. We should optimize for what matters instead. Whether we have a 1ms or 2ms overhead per test doesn't matter much when the test itself takes 100ms to execute. I think we need a more realistic benchmark so we don't end up bikeshedding over numbers that doesn't really give us any perceived perf improvements for the common-case.\n. @vdemedes Can you resolve the inline feedback so this can be merged?\n. @novemberborn I'll try to take a look tomorrow. Been busy IRL and James is off traveling :)\n. The approach generally looks good.\n\nThis tracks dependencies per test.\n\nBy test, I assume you mean test file?\n. The approach generally looks good.\n\nThis tracks dependencies per test.\n\nBy test, I assume you mean test file?\n. Alright. I've been testing this now in many different projects and seems to be working good.\n. > My only concern with the require hook is that it is easily broken by bad actors.\nIs there anything we could do to prevent this?\n\nSomething like babel-detective might be more robust (but maybe wouldn't catch as thorough a dependency graph)\n\nI think an actual dependency graph is a lot better than what a static analysis could do.\n. Merging. We can follow-up with any improvements/requests. Really good stuff @novemberborn :)\n\n. > Maybe instead of a conventional hook we wrap Module.prototype.compile? That is a no-no for conventional hooks (not really extendable), but since our hook doesn't manipulate the args (it just tracks files) I don't think it would be a problem\nI like this idea. Could even be a separate module we consume. @novemberborn hint hint ;)\n\n@novemberborn Also feel free to extract anything else into modules where it makes sense.\n. > I don't think that would work for native and JSON dependencies?\nYup, that's true, since they don't call ._compile, but those are also rarely overriden, so a combination of ._compile + hooks for JSON/native could work.\nhttps://github.com/nodejs/node/blob/0bea78682a0530cc51d3dfeab504352034a9fc02/lib/module.js#L423-L445\n. See https://github.com/sindresorhus/ava/issues/474 for the planned solution. I think @therealklanni is working on it. Any feedback on the referenced issue thread more than welcome :)\n. LGTM. @vdemedes ?\n. Agreed. This is a bug.\n. Why would you use that? It's more verbose than t.same().\n. We'll consider it, but this honestly feels like just using a new language feature for the sake of it, rather than actually being beneficial. It also IMHO makes tests look more noisy and inconsistent.\n\nAnd while this test is trivial I like how it makes immediately clear what assertions I'm using.\n\nI don't see any benefit of knowing that up front.\n. This is what I get on master:\n``` js\n\u276f ava\n1 passed\n```\n``` js\n\u276f ava --verbose\n1 test failed\n```\nExit code 1 on both, so at least it would fail, even if confusingly.\nSeems to be two issues here:\n- The default reporter reports false passing.\n- None outputs that an error was thrown.\nThis is only a problem with errors without a message.\n\nWith:\ntest(() => {\n  throw new Error('foo');\n});\n```\n\u276f ava          \n1 failed\n\n[anonymous]\n  failed with \"foo\"\n  Error: foo\n    Test.fn (test.js:4:9)\n```\n. Honestly not exactly sure where to look without taking a deeper look. It looks like the the error is correctly handled, but doesn't reach the reporters somehow:\n\n```\n\u276f DEBUG=ava ava\nava ipc stats:\n{ testCount: 1, file: 'test.js' } +0ms\n  ava ipc test:\n{ duration: 0, title: '[anonymous]', error: { name: 'Error', stack: 'Error\\n    Test.fn (test.js:4:9)\\n    Test._run (/Users/sindresorhus/dev/ava/lib/test.js:96:14)\\n    Test.run (/Users/sindresorhus/dev/ava/lib/test.js:135:17)\\n    Sequence._run (/Users/sindresorhus/dev/ava/lib/sequence.js:40:30)\\n    Sequence.run (/Users/sindresorhus/dev/ava/lib/sequence.js:23:14)\\n    Concurrent.run (/Users/sindresorhus/dev/ava/lib/concurrent.js:42:25)\\n    Sequence._run (/Users/sindresorhus/dev/ava/lib/sequence.js:40:30)\\n    Sequence.run (/Users/sindresorhus/dev/ava/lib/sequence.js:23:14)\\n    Sequence._run (/Users/sindresorhus/dev/ava/lib/sequence.js:40:30)\\n    Sequence.run (/Users/sindresorhus/dev/ava/lib/sequence.js:23:14)\\n    Runner.run (/Users/sindresorhus/dev/ava/lib/runner.js:99:81)\\n    process. (/Users/sindresorhus/dev/ava/index.js:84:10)\\n    emitOne (events.js:77:13)\\n    process.emit (events.js:169:7)\\n    process. (/Users/sindresorhus/dev/ava/lib/test-worker.js:113:10)\\n    emitTwo (events.js:87:13)\\n    process.emit (events.js:172:7)\\n    handleMessage (internal/child_process.js:689:10)\\n    Pipe.chan  [anonymous]\n1 passed\n  ava ipc results:\n```\nNotice how the result event is empty.\nWild guess, around here somewhere: https://github.com/sindresorhus/ava/blob/e28e24ceb8ca2647c38f3e53d9880a5ea32dacde/api.js#L214\n\n@novemberborn @vdemedes Thoughts?\n. Looks good :)\nAny chance you could handle the below case too, which causes the same false-positive:\njs\ntest(() => {\n  throw '';\n});\nI know it's a big anti-pattern, but it should not cause false-positives no matter how bad it is.\n. @jokeyrhyme I don't think it's directly related, but yeah, we should clean up our error handling as proposed in that issue.\n. Can't we just wrap thrown non-errors in an error? Something like this:\njs\nif (!(err instanceof Error)) {\n  err = new Error('Non-error thrown with value: ' + err);\n}\nMaybe also include the stack and test title from the test it was thrown for some context.\nI don't think it's a good idea to require unwrapping and the reporters should not have to care about something like this.\nPrior art: https://github.com/sindresorhus/loud-rejection/blob/b2676e557331991264def77a37ae8b94cab49f55/index.js#L9-L13\n\n\nSince AVA runs in strict mode this line throws if test.error is a literal.\n\nGood thing. I didn't even know that was possible in sloppy-mode.\n. > It's kinda useless though if it comes back with Non-error thrown with value: [Object object]. Hence my inspect() suggestion.\nOk, got it now. Yeah, we should could use inspect.\n. Thank you @novemberborn. Very happy we got this sorted out :)\n. LGTM. @vdemedes @jamestalmage ?\n. LGTM. @vdemedes @jamestalmage ?\n. @naptowncode Tests are failing on Travis it seems: https://travis-ci.org/sindresorhus/ava/builds/112949149\n. Good catch! Thank you. Keep'em coming ;)\n. > Nested Unit Tests: An Anti-Pattern - https://twitter.com/kentcdodds/status/735502601119137793\n// @kentcdodds \n. Yes, from the AVA readme:\n\nWhen using npm test, you can pass positional arguments directly npm test test2.js, but flags needs to be passed like npm test -- --verbose.\n\nSo it has to be npm test -- -s (Really wish npm test would just pass them through... Anyone wanna open a npm ticket about that?)\n. @SamVerschueren On a side note, the above would be great as a recipe. Hint hint ;)\n. @kentcdodds Can you elaborate on your use-case? They're not using the same spy. The hook is run before each test and with a unique t.context for each test.\n. LGTM\n. Thanks @kentcdodds. Let us know if there's anything else that could be improved with the docs. It's sometimes hard for us to see the obstacles as we're so used to this stuff.\n. :+1: Would you like trying to do a pull request?\n. Thank you @Carnubak :D\n. Thank you @Carnubak :D\n. Can you also update the docs?\nhttps://github.com/sindresorhus/ava#testskiptitle-body\nhttps://github.com/sindresorhus/ava#skip-tests\n. This allows omitting test function for all tests, not just .skip, @vdemedes are we ok with that?\n. Looks pretty good. The docs needs to be updated and some inline feedback resolved.\n. LGTM\n. > Happy to squash/rebase to make the commit history a little more sane\n:+1: \n. Still some things to iron out I think.\njs\nimport test from 'ava';\ntest.todo('foo');\ntest.todo('bar');\nGives no output with the default reporter:\n```\n~/dev\n\u276f ava          \n```\nAnd with --verbose:\n```\n~/dev\n\u276f ava --verbose\n\nfoo\nbar\n\n2 tests failed\n```\nI don't think todo tests should be failing.\n\njs\nimport test from 'ava';\ntest('foo', t => t.pass());\ntest.todo('bar');\nOutputs it's passing, but no mention of the 1 todo and the test run still exit with exit code 1 even though the output indicates differently:\n```\n~/dev\n\u276f ava\n1 passed\n```\n. The TAP description doesn't explicitly say they should fail either, though, just that they shouldn't succeed like a normal test. I think we should rather enable users to fail todo tests in the linting step, not in AVA. Todo tests should be like TODO comments, informative, but not affecting.\n. @BarryThePenguin Perfect. Exactly what I was thinking :)\nLGTM\n. Landed. Thank you for tirelessly working on this one @BarryThePenguin. It turned out really good.\n\n. Thanks :)\n. I'm getting the same issue with AVA 0.12.\n```\nbefore each\nbefore each\nbefore each\n3 passed\n  3 exceptions\n\n\nUncaught Exception\n  Error: 789\n    test.js:21:14\n\n\nUncaught Exception\n  Error: 456\n    Immediate._onImmediate (test.js:15:14)\n\n\nUncaught Exception\n  Error: 123\n    [object Object]._onTimeout (test.js:9:14)\n```\n. I'm getting the same issue with AVA 0.12.\n\n\n```\nbefore each\nbefore each\nbefore each\n3 passed\n  3 exceptions\n\n\nUncaught Exception\n  Error: 789\n    test.js:21:14\n\n\nUncaught Exception\n  Error: 456\n    Immediate._onImmediate (test.js:15:14)\n\n\nUncaught Exception\n  Error: 123\n    [object Object]._onTimeout (test.js:9:14)\n```\n. > var test = require('ava').test;\n\n\nBtw, can be just var test = require('ava');\n. > var test = require('ava').test;\nBtw, can be just var test = require('ava');\n. Looking at your first example with fresh eyes, it's obvious it wouldn't work as you're trying to use async inside a sync test. Sync tests end in the current tick. If you want to use callbacks, use test.cb().\nAs for your second example. This is what I got with latest AVA:\n```\n4 exceptions\n\n\nUncaught Exception\nError: 789\n  test.js:34:17\n\n\nUncaught Exception\nError: 456\n  Immediate._onImmediate (test.js:25:18)\n\n\nUncaught Exception\nError: 123\n  test.js:7:14\n  FSReqWrap.cb [as oncomplete] (fs.js:212:19)\n\n\nTest results were not received from test.js\n```\n\n\nIt shows the correct line for all but setTimeout.\nHowever, I would say you're using it incorrectly. You shouldn't throw asynchronously inside a promise. That's what reject() is for (or promisify the setTimeout function - see delay). If you want to use async tests, use promise compatible semantics. Otherwise, use test.cb().\nMight help if you could provide a realistic scenario where this is a problem.\n. Looking at your first example with fresh eyes, it's obvious it wouldn't work as you're trying to use async inside a sync test. Sync tests end in the current tick. If you want to use callbacks, use test.cb().\nAs for your second example. This is what I got with latest AVA:\n```\n4 exceptions\n\n\nUncaught Exception\nError: 789\n  test.js:34:17\n\n\nUncaught Exception\nError: 456\n  Immediate._onImmediate (test.js:25:18)\n\n\nUncaught Exception\nError: 123\n  test.js:7:14\n  FSReqWrap.cb [as oncomplete] (fs.js:212:19)\n\n\nTest results were not received from test.js\n```\n\n\nIt shows the correct line for all but setTimeout.\nHowever, I would say you're using it incorrectly. You shouldn't throw asynchronously inside a promise. That's what reject() is for (or promisify the setTimeout function - see delay). If you want to use async tests, use promise compatible semantics. Otherwise, use test.cb().\nMight help if you could provide a realistic scenario where this is a problem.\n. Yeah. We'll keep this open, but I doubt we'll do anything until AsyncWrap lands in Node.js.\nYou could do what I do and just manually promisify all callback API's. It makes the tests so much nicer as a bonus. See: pify or Bluebird.promisify.\n. Just wrap callback APIs in pify and you will have a much better experience in general.\n. I don't think anyone on the team uses TypeScript, so would be hard to keep up to date, but if anyone from the community is willing to maintain it, I'm :+1:.\n. I don't think anyone on the team uses TypeScript, so would be hard to keep up to date, but if anyone from the community is willing to maintain it, I'm :+1:.\n. :+1: \n. // @SamVerschueren \n. Does this handle test being chainable? E.g. test.skip.cb().\n. You also need to explicitly include the file in the files field in package.json.\n. @SamVerschueren That won't happen, but even if it did, we would just add it to package.json then. Same reason index.js is not in package.json. It's automatically resolved.\n. @SamVerschueren :+1: \n. Looks good. Thank you @ivogabe :)\n. Good start :) This will need tests and docs too.\n. Yay! Nice work @spudly :dancer: \n\n. Thank you! Keep'em coming :)\n. @opensrcken Please read and adhere to the contributing guidelines :)\nGive the PR a proper title and don't change the indentation.\n. > Or were you referring to something else?\nYou changed the indentation from tabs to spaces.\n. > import {test} from 'ava';\nThat should be import test from 'ava';\n. @novemberborn We recommend users install it globally so they can use the local AVA directly by typing AVA. Local because that's how Node.js works. Why --init? Well, it's a nice way to get started. They would have to npm install anyways, and init adds to package.json run script too. npm should cache dependencies, so installing it locally after doing it globally should be faster than the first time. The slow npm speed really pains me as users always blame the project, not npm, while it's almost totally out of our control. Alternative npm clients have shown it's possible to speed it up considerably without any server changes. Anyhow. What we can do is to show the npm progress bar in --init during install: https://github.com/sindresorhus/ava-init/issues/2\n. > I really want to have ava only once, it has 300 dependencies and it takes a long time to install in each project.\nUse a better npm client. https://github.com/alexanderGugel/ied is known to be faster.\n. > And I guess it's neccessary for ava to have 300 deps but still I think for me it is unacceptable to have it in each project ...\nYou should read up on why it's like this with Node.js. Nothing specifically to do with this project. Having local unique dependencies in each project gives you a lot of benefits too. If all dependencies are global, every project would potentially conflict dependency-wise.\n. @novemberborn Yup, that's a better workflow. Unfortunately most don't do that.\n. > this will be a breaking change.\nTrue, although I really doubt (hope) nobody depends on this behavior.\n. Thanks @kasperlewau :)\n. :+1:\n. Would be really useful to use why-is-node-running to also show why tests are still pending.\n. Thank you @bachstatter. Good stuff :)\n. Can you also add a link to the readme?\n. @SamVerschueren Add it here: https://github.com/sindresorhus/ava#recipes\n. > tips for squashing when you had to sync with the remote in between?\nDon't sync with git pull, but with git pull --rebase.\n. But don't worry. I'll squash when I merge.\n. Landed. I'd like to keep it simple as is. TypeScript will (should) know how to use the TSC options. Not the correct place to teach it here.\nThanks Sam!\n. get-stream@2 is out.\n. I don't think it makes sense to match on implicit titles. It should IMHO only match on what people write out in their test file.\n. @novemberborn Does this still apply https://github.com/sindresorhus/ava/pull/592#discussion_r54730961 ?\n. @novemberborn Is the exclusive change something you want to do in this PR or can it be a follow-up?\n. > if you have a .only test and use --match it will only be applied to .only tests. I think that's a bug.\nAgreed.\n. > Not having the power to trigger a re-run had me shouting in agony\nJust amend the latest commit without any new changes and force push and you trigger a rerun ;)\n. LGTM2. Landed.\nSuperb work on this @kasperlewau, and thanks for persevering through our extensive review. This turned out really good! :)\n\n. > I wonder if .only should be sticky so that when changes are detected the watcher still only executes .only tests. But then if you remove the .only it starts executing all tests again. Similarly adding a .only switches back into the sticky mode.\n:+1: That's what I would expect.\n. Yes, we discussed this a while ago and agreed it would be very useful, but not a priority for 1.0.0.\nhttps://github.com/bahmutov/rocha is a good inspiration.\n\n\"I thought it was a pretty neat idea. I really like that he saves the execution order when tests fail. That way things stay reproduceable until fixed.\" - @jamestalmage about the above.\n. > We could consider running test files in a random order as well, though they're already somewhat random because each test file runs in its own process\n\nI think we should still shuffle the glob-resolved array of files just to be sure.. Superb work @forresst, keeping the translations up to date :)\n. Just some minor inline nits, but generally looks very good :)\n. > although the right indentation varies by code block which is annoying.\nBlame npm. They're the ones requiring 2 space indent.\n\nTiming wise perhaps best to land #592 and #573 first.\n\n:+1: \n. @novemberborn #573 was landed. Can you fix the merge conflict and squash merge this to master?\n. @novemberborn Looks good. Just merge now. We can follow up later with improvements if anyone has comments. This changes a lot of things, so would be good to get it merged asap.\n. Thanks for leveling up the docs @novemberborn :)\n. Now the test title and the total count (71 passed) is vertically unaligned. They should be aligned.\n. While you're at it, can you put a linebreak above the test title to give it some breathing-room?\n. One more minor tweak. Move the spinner one char to the right without moving anything else. It's a bit too far from the test title and a bit too much crammed into the left edge.\n\u280b  test title => \u280b test title\n. I'm good with A, but will let @vdemedes chime in.\n. The default reporter looks so beautiful now \ud83d\ude0d. Thanks James :)\n. LGTM\n. Thanks @sotojuan :)\n. LGTM\n. You can force chalk with chalk.enabled = true;.\n. >  support r + Enter to rerun all tests, in addition to rs + Enter\nI meant just r. No Enter. What really is the point of having to press Enter?\n\nresolve whether source negation patterns should disable all default ignored directories, whether those default directories can be overriden, etc (this may need to be a follow-up issue)\n\nYeah, let's not dilute this PR with that. Can you open a new issue?\n. > Happy to open a ticket though as a future improvement?\n:+1:\n\nI'm inclined to keep rs + Enter for those users with nodemon muscle memory (or who may use AVA and nodemon in the same project).\n\nSure, that's fine, but r+enter should be the documented one.\n. > Right now they're both documented, with r + Enter being first.\nI don't see the point of documenting both. Your argument is that you have rs + Enter as muscle memory. I don't see why we would document rs to new users without that muscle memory and create overhead for them by making them have to choose which one to pick.\n. > How would users realize rs also works? Other than accidentally. Maybe that's good enough though?\nIsn't that the point? If they have to read the docs, they can just as easily use the shorter r+Enter version.\n. > You'd have to read the docs to even find out about r+enter though.\nExactly. Unless it's muscle memory, users have to read the docs anyways, so there's really no reason to document both behaviors, when r is shorter. I consider rs a graceful handling of a mistyping, not something we'd want to officially support. IMHO, we should only have one way to do something.\n. :+1: \n. Looks good :)\n. > AVA currently only transpiles the tests you ask it to run. It will not transpile modules you import from outside of the test. - https://github.com/sindresorhus/ava#transpiling-imported-modules\nThis will be possible in the next version as soon as https://github.com/sindresorhus/ava/pull/573 lands ;)\n. \ud83d\udc4d Sounds good.\n. LGTM\n. Tip for reviewing this, append ?w=1 to the diff URL to ignore whitespace changes: https://github.com/sindresorhus/ava/pull/613/files?w=1\n. Nice! Thanks for working on this @develar :)\n. Can you add some tests?\n. > fork now returns object with custom methods (on, run) and promise field. We don't add custom methods to promise anymore.\nWhat's the benefit of this?\n. @novemberborn I don't think we should invent our own glob patterns.\n\nCurrently if you use a negation pattern in --source it stops ignoring those default directories.\n\nWhy?\n\nI'm failing to see the problem here. Maybe provide an example?\n. // @twada \n. > t.same behaves not the same way as assert.deepEqual\nIt's not the same as assert.deepEqual. It's assert.deepStrictEqual.\n. Try https://github.com/sindresorhus/sort-keys instead ;)\n. Let's continue this in https://github.com/sindresorhus/ava/issues/628.\n. > plus it's not necessarily a settled feature\nIt is now \ud83d\ude0e\n\nthoughts on merging config by default?\n\nI would be ok with that, as long as we have very clear docs about the behavior, so there's very little chance of confusion.\n. > Note that if you edit a file without exclusive tests, AVA will report 0 tests passed. Let me know if this needs a more helpful message.\nCan you show an example on when this would happen exactly? More helpful messages are always welcome.\n. > Maybe it should report 0 exclusive tests found, but that would entail modifying the various reporters.\n\nIt'd also help if I added debug output so you can see the tests are being run in exclusive mode.\n\nAgreed. Mind opening issues for this? Can be follow-up improvements.\n. Looks good. Land it ;)\n. LGTM\n. @sotojuan Looks pretty good. Can you fix the merge conflict?\n. :+1: \nJust ignore AppVeyor. It sometimes completely fails to run for no reason...\n. > Just a thought, but what if we showed both the current test, and the last failed test. Or maybe just the counts and the last failed test (Since we intend to print pending tests on SIGINT - the last passed test isn't that valuable anymore). What failed is usually the most important info anyways. - @jamestalmage \n. I was going to say wrapping, but it looks very ugly, so let's go with truncation. Can use \u2026 character to indicate it's truncated. I made a module for this: https://github.com/sindresorhus/cli-truncate\n. :+1: Do you have a suggestion on how we could solve this?\n. Maybe, if it does the same except for the constructor checking. We previously tried with is-equal, but the package is ridiculously large.\n. > I see no reason not to emulate tap here, with same and strictSame just as described by\nI don't see the reason to have both to be honest. Same reason we don't have is and strictIs.\n\nbuffertools only offers a significant performance benefit when used with large Buffers, and with versions of Node earlier than Node 0.12, which includes buffer.equals(), which is undoubtedly the fastest method available.\n\n:-1: We definitely don't want to include more native dependencies for such a minor to none gain.\n. I'm having a hard time thinking of when I would ever want it to check the equality of the constructors of objects. And I would honestly prefer one method, not two. It's very easy to add another method, but it adds a lot of overhead for users. Now they must think hard about which method to use in which cases and know about the subtle differences between them.\n. > If they explicitly define an extension just trust the user will add the require hook themselves (perhaps a helpful message if they don't).\n:+1: \n. @novemberborn The global CLI just hands it directly off to the local AVA if available.\n. \ud83d\udc4c\ud83d\ude0e\ud83c\udf89\n. Thanks. Good catch :)\n. Shouldn't this be implemented in the API? timeout can be useful for other consumers too, like the grunt/gulp plugins.\n. Also discovered a problem with the timeout handling and its test being faulty:\njs\nimport test from 'ava';\ntest.cb(t => {\n    setTimeout(t.end, 10000);\n});\n$ ava --timeout=1s\nThis will output the error message after 10 seconds because of the setTimeout, not after 1s of inactivity. If you remove the setTimeout it will never exit. Can you also add a test where it never exits by itself?\n. LGTM\n. @vdemedes https://github.com/sindresorhus/ava/pull/654#discussion_r56964317\n. @novemberborn @jamestalmage :shipit:\u2753\n. @jamestalmage Explained in https://github.com/sindresorhus/ava/pull/654#issuecomment-205262984\n. Awesome\u203c\ufe0f Primo work @vdemedes\n\n. @kutyel Can you fix the merge conflict?\n. @kutyel Can you fix the merge conflict?\n. > Do people actually use those?\nNever used either personally.\n\nIt would be really nice if npm didn't print that huge mess of log statements when a script fails. So annoying to scroll up past that.\n\nCouldn't agree more! The npm CLI generally has pretty awful UX.\nRelated issue: https://github.com/npm/npm/issues/8821 (Please comment there!)\n\n\"this isn't a problem with npm\" language significantly reduces their support requests.\n\nThe irony is that usually it is a problem with npm even when they output that. So the usual flow is a user opening an issue on one of my repos, I have to waste time triaging, and just send them back to the npm issue tracker...\n. Ok, let's revert to R+Enter. I don't think people will need it that often anyways. Our dependency tracking is pretty good.\n. Ok, let's revert to R+Enter. I don't think people will need it that often anyways. Our dependency tracking is pretty good.\n. > I just think fixing it in babel would be easier and better for the community at large.\nAgreed. Let's keep this open so we don't forget to follow-up with Babel. We can discuss doing this if they don't fix it in the short-term.\n. Looks like https://github.com/babel/babel/pull/3438 will solve our babel-runtime problem.\n. Looks like https://github.com/babel/babel/pull/3438 will solve our babel-runtime problem.\n. I think our output could be better than just undefined undefined undefined though.\n. > I guess I can live with t.throwsAny\nI can't.\n. > Whether t.throws() should accept non-errors is yet another discussion\nIt shouldn't. If you want to do that you can easily try/catch yourself.\n. > but people will create issues for that as to why it throws an error regarding literals\nSounds like we just need a good error message.\n. LGTM2\n. We have intentionally not added any aliases (there are a lot of them in tape for example). It might seem nice at first, but it causes a lot of overhead for the user and inconsistency. Now a team needs to decide which alias to use and in what situations, and enforce that consistently. It goes against the AVA mantra (stolen from Python) of having preferably one way to do something. Instead, I would recommend taking advantage of power-assert and just do t.ok(result == null);. That's even clearer than the word truthy, which many don't really know what means exactly anyways.\n@vdemedes @novemberborn @jamestalmage Thoughts?\n. I think ok and truthy are equally bad. truthy is also longer to type and can easily be confused with true when skimming assertions. I added ok initially since tape had it and it's useful for just checking that a property exists t.ok(obj.prop) or that a function returns something t.ok(fn()).\n. > ... also if the recommended is always going to be to lean on powerassert\nt.true(condition) is what you want in most cases.\n. > Not wanting to introduce aliases, I'd support a s/same/deepEqual change.\nYeah. I kinda regret that one now. It sounded clear when we added it. t.same means it's the same structure, while t.is, means it is the same thing. I do wonder if deepEqual is clearer because you've heard it before, rather than actually being clearer.\n\nfalsy/truthy is pretty well defined. There are Wikipedia entries on both easily found by googling. Googling ok is not helpful. So I prefer those names if these assertions stick around.\n\nBut if neither is clear enough by the code itself, it's doesn't really matter if one is more Google-able. The documentation would cover either.\n. > I'd be sad to see t.same() go :( deepEqual is way longer\nUse the editor snippets. They autocomplete ;)\n. Can you open an issue on https://github.com/npm/marky-markdown?\n. It's clearly explained in the documentation: https://github.com/sindresorhus/ava#es2015-support\n(Translation: https://github.com/sindresorhus/ava-docs/pull/17/files#diff-ffcaea08d53883913f740e672af4c9daR539)\nAVA comes with Babel bundled which transpiles your test files down to ES5, including import into require calls.\n. @novemberborn Any reason we can't just overwrite the output like we do in the mini-reporter?\n. Timestamp would indeed be good for both default and verbose mode.\n. https://github.com/csabapalfi/tap-growl ?\n. > I could be wrong - it seems the reporters could figure this out from the plan line, but that is likely pretty brittle\nIt's possible with tap-finished, but every TAP reporter would have to support this, which is not realistic.\n\nThe final option would be \"Sorry, no TAP output in watch mode\". Personally, I am totally fine with this option, since I prefer our own reporters.\n\n:+1: Let's just ignore the --tap flag in watch mode and document the behavior. Using our reporters, we can optimize the output in watch mode, like for example adding timestamps. TAP is for normal test runs.\n. :+1: Sounds good.\n. I think this is a very cool idea, and would hopefully encourage more people to submit failing tests instead of issues. Not sure about .see, maybe .issue or .reference?\n. I think this is a very cool idea, and would hopefully encourage more people to submit failing tests instead of issues. Not sure about .see, maybe .issue or .reference?\n. I'm good with .see.\n. I'm good with .see.\n. Yeah... I've restarted the builds.\n. Sounds nice, but I think it's outside the scope of AVA core. Would be better as a recipe or something.\nI just added an additional binary to anybar-cli that makes it so you can just do:\nconsole\n$ anybar-exit ava\nAnd have AnyBar turn green/red depending on the exit code.\nDo you think that would be a good compromise? Also looking for feedback on how the binary works.\nhttps://github.com/sindresorhus/anybar-cli/commit/747a9562d8e5c99701f6f92382444ea4f03a3f8f\n. I honestly think a plugin system is way too early and not something we should focus on at the moment. When we open up that floodgate there's no way back. I think IMHO it's rather something we should explore post-1.0.0.\n. > We need a \"future\" label for that kind of stuff.\n:+1: \u2192 https://github.com/sindresorhus/ava/labels/future\n. Yes, this is fixed in master. We'll do a release soon. For now you can do $ npm i -D sindresorhus/ava#f2c0709 to get latest master.\n. LGTM\n. I would honestly prefer we handled most cases nicely by default. Adding options is last resort.\nCan you link to the power-assert options docs?\n\n@twada Any way we could handle this case nicely automatically?\n. Can you also add what was thrown to the message, the type and value. Might be helpful.\n. > Not sure if necessary as it isn't being printed in the console?\nNot really necessary as they're only useful in an AssertionError.\n. Thanks for starting the conversation @kentcdodds :)\n\nWe'll definitely need a codemod if we're going with this.\n. Thanks for starting the conversation @kentcdodds :)\n\nWe'll definitely need a codemod if we're going with this.\n. Not to start a bikeshedding, but is there any other word we could use that might be even clearer?\n. Not to start a bikeshedding, but is there any other word we could use that might be even clearer?\n. I'm good with deepEqual.\n\nWe should really have a codemod for this change before releasing it. Anyone interested in trying to make one (I especially encourage community members to try)? Resources: https://github.com/sindresorhus/ava/issues/644\n. See https://github.com/jamestalmage/ava-codemods.\n. @kentcdodds We now have a codemod, so this could potentially go into the next release. Would you be able to finish this up and fix the failing tests?\n. Yay for better UX. Thanks for nudging us towards this and doing the work @kentcdodds :)\n. Yes. We're going with that. PR super appriciated :)\n. Oh, right, duh. I thought the enhancement caused it...\n. Oh, right, duh. I thought the enhancement caused it...\n. The current command is correct. npm test swallows flags, so we use the -- to indicate it should ignore the following flags.\n. http://unix.stackexchange.com/questions/11376/what-does-double-dash-mean-also-known-as-bare-double-dash\n. No idea what went wrong there, but it's available in the latest commit, so probably just a glitch.\nhttps://coveralls.io/builds/5643290/source?filename=cli.js\n. :+1: To improving the error message here.\nI wonder if this should be an optional option in resolve-cwd to throw with a nice error message?\n. LGTM\n. > Shouldn't it throw if it can't be resolved?\nThat's annoying for most use-cases when you just want to check if something exists and fallback if not. Our use-case is different as we're exposing it directly to the end-user.\n\nWilling to do a PR to another library to make this work.\n\nHappy to merge a PR on resolve-cwd that  adds such option. PR should be on resolve-from and then exposed in resolve-cwd. Should be disabled by default though.\n. I like the idea of being as graceful a possible. One concern though. Look at it the other way around. What if you have a lot of tests and you use one of them incorrectly. Now it will run all the tests before letting you know you have an issue and then you'll have to rerun the whole test file again. Just a thought.\n. Is there anything that actually needs this? If so, I think we should limit the scope to what's actually needed, and drop anything that might-be-useful.\n. > config.isTest('/some/path');\n\nconfig.isSource('/some/path');\nconfig.isHelper('/some/path');\n\nThis doesn't look config related though. More like util.\n. :+1: Alright \n. \n. :-1: That makes it harder to read in my opinion.\n. Thanks @sotojuan :)\n. LGTM\n. @Siilwyn If you had said Node.js 8, I would have agreed. The main reason we added Babel in the first place was to get async/await (which is part of Node.js 8) early.. > I don't understand the use nor the interest of Babel when writing tests for nodejs.\nBabel was added to AVA long before Node.js 4 and before ES2015 was fully implemented in Node.js. There's usually a reason things are like they are.\n\nSince 4.x, nodejs widely supports JavaScript.\n\nNode.js has always supported JavaScript, but I assume you meant ES2015. The thing is that JS is constantly evolving and there will always be features that are not yet implemented in V8. There are lots of interesting proposals for future JS features: https://github.com/tc39/proposals So Babel will always be needed if you want the absolute latest features.\n\nThe tooling around ava is still very poor (no atom / visual code integration, no debugging)\n\nHelp improve it then. I don't use either Atom or VS Code.\n\nI may understand the need for you guys, as ava developers (for async/await) but, why forcing the tests to be run with Babel? May you consider removing Babel for node >= 8.0 \u2014 now that async/await is part of it?\n\nYes, we're open to it. It's just waiting for someone to implement it: https://github.com/avajs/ava/issues/1556. @jamestalmage Assigned.\n(Sidenote: We should use the \"assignment\" feature more)\n. The mobile version is completely useless. It misses so many features I don't even bother to use it anymore. Better to use the desktop version on mobile and zoom in and out.\n. LGTM\n. LGTM\n. Seems you need to update some test assertions too.\n. No worries @dcousineau :)\n. I think we should defer this to after 1.0.0. We already have a lot on our plates and this is non-essential to our core features and use-cases.\n. That's something the reporter should fix. We can't possible correctly escape our titles for every possible scenario.\n. It seems we also incorrectly include some ansi escape codes in the title ESC[90mESC[2m. We should use strip-ansi on the title in the tap reporter. @sotojuan Wanna do a PR for that?\n(The actual issue is still that the reporter doesn't escape the title though)\n. @7373Lacym Go ahead :)\nLet us know if you have any questions. We're also in https://gitter.im/sindresorhus/ava\n. Oh wow. You're fast! Excellent work as always @forresst :)\n. > IMHO we could remove the duplicate, invalid and wontfix labels. We've basically never used them.\nDone.\n\nI would suggest we set the assigned label even when a collaborator is assigned. That way they don't need to look two places.\n\n:+1: \n. LGTM when the nitpick inline feedback is resolved.\n. @jamestalmage :shipit:?\n. > Please hold off on merging until a new release of has-flag is out.\nIt's out. Bump it to 2.0.0.\n. Super-duper \ud83c\udf89\n. @novemberborn No need to do PRs for minor stuff like this. Just commit directly :)\n. I like it a lot!\nOne thing that might make it clearer is to add one more newline between persisted results.\nThis looks a bit cramped:\n\n. @novemberborn Perfect. Much clearer separation now.\n. Tests are failing.\n. @novemberborn :shipit: \n. Marvelous. :shipit: \n. > \"AVA always includes a few internal plugins regardless of configuration, but they should not impact the behavior of hour code\", and then link to the Notes section of the babelrc recipe.\n:+1: \n. \ud83c\udf7e\u2728\n. @novemberborn It's not very common to link to the exact same thing multiple times though.\n. \ud83e\udd84\n. @jamestalmage Should we have some tests for the benchmarks?\n. @jamestalmage Should we have some tests for the benchmarks?\n. > Isn't it better to mention in README.md about this leak?\nNo point. We'll do a release soon. People encountering this can easily find this issue by searching.\n\nShould this issue be closed, because when installing via npm it is not yet fixed?\n\nIssues are closed when they're fixed, not released. This is true for most open source projects.\n. Yay :)\n. Nice work @adriantoine. This looks very promising :)\n// @MoOx @kentcdodds @istarkov\n. > Also, as a native french speaker, I'll be happy to make the translation to French once this is merged.\n@forresst FYI ;)\n. I think it's worth mentioning.\n. I think it's worth mentioning.\n. Alright. Let's merge! Superb work on this one @adriantoine. I think a lot of people will benefit from the recipe :)\n. \n. @shinzui I doubt it's the exact same issue as we have unit tests proving it's working. Go ask in our https://gitter.im/avajs/ava and share some code ;). What command did you use to run AVA?\nInstead of adding an option we'll just fix the auto-detection.\n. I guess we could pass all flags after a double-dash to node, like $ ava -- --harmony_destructuring.\n. > Also, Node.js 6 will be released on the 26th and then there won't be a need to run node with flags anymore...\nNo. There will always be new features behind flags.\n. :shipit: \n. :shipit: \n. Hard to give feedback without seeing how it's going to be used, but I like the idea in general.\nAre we still doing this?\nMaybe @novemberborn can share some thoughts on it.\n. I like the idea in general, but this has me a bit worried. execSync is very slow.\n. Might be useful if packageHash supports accepting an array of package paths to hash. So we could just do:\njs\npackageHash.sync([\n  require.resolve('../package.json'),\n  require.resolve('babel-core/package.json'),\n  require.resolve('babel-plugin-espower/package.json')\n]);\nMaybe even some an option with extra text to hash:\njs\nvar salt = packageHash.sync([\n  require.resolve('../package.json'),\n  require.resolve('babel-core/package.json'),\n  require.resolve('babel-plugin-espower/package.json')\n], {\n  extra: JSON.stringify(this.babelConfig)\n});\n. Might be useful if packageHash supports accepting an array of package paths to hash. So we could just do:\njs\npackageHash.sync([\n  require.resolve('../package.json'),\n  require.resolve('babel-core/package.json'),\n  require.resolve('babel-plugin-espower/package.json')\n]);\nMaybe even some an option with extra text to hash:\njs\nvar salt = packageHash.sync([\n  require.resolve('../package.json'),\n  require.resolve('babel-core/package.json'),\n  require.resolve('babel-plugin-espower/package.json')\n], {\n  extra: JSON.stringify(this.babelConfig)\n});\n. > Are you mixing concerns at that point?\nYeah, probably not a good suggestion. Just wanted to simplify by removing the double hashing.\n. > Are you mixing concerns at that point?\nYeah, probably not a good suggestion. Just wanted to simplify by removing the double hashing.\n. \ud83d\udc4d \n. > Hooks at top level don't feel right..\nWhy?\n. > Hooks at top level don't feel right..\nWhy?\n. :+1: I got hit by the bad output of a RuleTester failure the other day too.\n. @danilosampaio Go ahead :)\nJoin AVA chat if you have any questions.\n. > Your first example will hang forever.\n@jamestalmage It won't. Sync tests end in the same tick. But yeah, needs to use test.cb(). Any way we could catch this mistake in eslint-plugin-ava? I think we'll see more of this as people move from tap/tape.\n. > Your first example will hang forever.\n@jamestalmage It won't. Sync tests end in the same tick. But yeah, needs to use test.cb(). Any way we could catch this mistake in eslint-plugin-ava? I think we'll see more of this as people move from tap/tape.\n. > should fail\nNo it shouldn't. In that case you're incorrectly using an async callback in a sync test. The test ends before the callback is executed.\n\nand tried:\n\nWhat's the problem here?\n. What's left for this to land?\n. Good catch :)\n. Thanks :)\n. Thank you @kevmannn :)\n. It's weird choice having the enhanced assertion as a reporter. It's not. It's a helper.\n. \ud83d\udc4d \nlib/reporters/helper/ \u2192 lib/reporters/helpers/\n. > I'm not sure if it's the best solution, but adding equals to lib/assert.js, we get the output fixed. \nHuh? Adding random things isn't a solution. We already have t.is() for the same.\n\nclarifying: i'm using your example, but calling t.equal instead of assert.equal\n\nI've read through this whole discussion multiple times and I'm still confused about most things here including this clarification. If you can clearly explain what the issue is that's blocking this PR we might be able to help ;)\n. No worries :)\n. I like the idea. Any potential drawbacks?\nAnd we're probably never going to use domains anyways.\n. What do you propose instead?\n. @atomless Would you be interested in doing a pull request?\n. @atomless I think both should be covered then. jsdom is the lowest primitive. Zombie is a nicer API on top of it. Then users can choose which abstraction they like.\n. @atomless \ud83d\udc4d The jsdom init should be in a beforeEach hook and use t.context for passing and using the window. That way the tests will be atomic and doesn't have to be serial. Care to submit a pull request?\n. LGTM\n. > This works great, but this has a caveat of not being able to override options when running tests from the command line:\nYour example is invalid. It should be ava --tap | tap-foo-reporter and --verbose is mutually exclusive with the --tap option, so passing --verbose when using --tap does nothing. Can you share an actual use-case?\n. > I'm having Travis errors installng buffertools on Node 4 and 5, but when I install it on a local machine it works.\nJust drop the buffertools stuff from the code. Newer Node.js versions have buffer.equals, and for older we don't care. Would bet no one have buffertools installed anyways.\n. let is not available in Node.js 0.10-0.12. https://github.com/sotojuan/not-so-shallow/blob/b97d551d30f40401fd196ccf69afb74f155fcdac/index.js#L3\nYou also need to test on older Node.js versions in your module.\n. Looks good! Splendid work @sotojuan :)\n\n. Yes. I've noticed this too. Just hadn't gotten around to reporting it yet.\n. Could you share such test? Just curious what makes it hang.\n. You're using t.throws incorrectly? Is that intentional? What exactly made it hang?\n. > The only reason I can gather that makes that particular test to hang is the incorrect usage of t.throws()\nToo bad this happened now. If you'd used master (not that you should have), AVA would have warned you about it: https://github.com/sindresorhus/ava/commit/3201b1b4ff80ff75f0e1c288ca7da22f92c9b814\n. AVA adheres to the official npm. We shouldn't have to adapt to every weird edge-case of unofficial solutions. It's a very common pattern to have config just in package.json. I'm not very interested in complicating the purity of that.\n. > There is no such thing as the official NPM (there is no spec for what the API, there is no spec defining the structure of the package.json [apart from the supported properties]).\nBy that I mean, what works with official npm is what we'll follow.\n. @gajus We always consider user's needs, but you're just one user with a very edge-casy need. If we adapted to every edge-case, AVA would be bloated and hard to use. Creating great simple products requires saying no to thousand things.\n\nWe stopped using ava for this and several other reasons.\n\nI would appreciate if you would elaborate on what other things stopped you from using AVA :)\n. I like the proposal and have had similar needs in the past. Let's keep this open for a while to gather feedback. We should be very careful about introducing new API's.\n. @jamestalmage Can you also include an example not using Observables? Want to tweet about this issue, but most aren't familiar with Observables.\n. @jfmengels Any specific suggestions would help ;)\n. Very good improvement. Thanks James.\nI manually merged and fixed @jfmengels comment.\n. I like the idea, but I think it be better if ava --report opened the new issue page with the issue body prefilled with that info?\nExample: https://github.com/sindresorhus/caprine/blob/2acdd358eafdc42b45b532c90e87ba5465a30de7/menu.js#L53-L62\n. I agree with James long-term, but right now I think we should go with the easy solution.\n. Yup\n. Should we have a better default than Infinity? From experience, sometimes it's more efficient with 10 in concurrency than Infinity, since systems are bad at handling that much concurrency and get overloaded. We should maybe do some test runs and find some magic number as default.\n. @dcousineau Last time I did concurrency profiling, I found the following to be optimal: \njs\nMath.max((os.cpus().length || 1) * 2, 2)\nFrom: https://github.com/sindresorhus/grunt-concurrent/blob/9618f67c01e15ca447db5a36bd03bff88e40ce9d/tasks/concurrent.js#L13\n. Tests are failing\n. LGTM\n. \ud83c\udf89 Woo! Thanks for your perseverance on this @dcousineau :)\n. Hmm. Thinking out loud. Are we attacking this from the wrong side? Maybe we should color the test title in the reporters that needs it, not remove it here?\n. @sotojuan Can you add Fixes #00 (replace 00 with issue number) to the PR description so the relevant issue is closed when this is merged.\n. Oh, right, then include just the issue number in the description. The more references the easier it is to navigate this at a later point.\n. I don't think there's much benefit of testing the title alone. stripAnsi is already tested and all we do is apply it. We should rather test the whole tap output to ensure there are no ANSI escape codes in any of the outputted information, not just the title.\nhttps://github.com/chalk/has-ansi\n. No worries. Looks good. \ud83d\udc4d\nNow go back to your studies. That's more important ;)\n. No worries. Looks good. \ud83d\udc4d\nNow go back to your studies. That's more important ;)\n. See: https://github.com/strongloop/fsevents/pull/128\nWe're using chokidar for file watching, which uses the native dependency fsevents for more consistent behavior and better performance.\n. I like how it simplifies event testing, but I honestly very rarely test events. And wouldn't this be better solved with Observables?\n. > EventEmitters emit multiple named events. Observables only emit one. In a sense, EventEmitters are essentially a bag of Observables with a common completion.\nI'm aware, but I was thinking just simply converting each event to an Observable and then have all the power of Observables for testing.\n\nWould you if it were easier to do?\n\nBadly phrased. I meant I rarely have the need to, as I almost never use events.\n. // @twada \n. Move it out of the inline assertion for now:\n``` js\nimport test from 'ava'\ntest('fail', t => {\n  const obj = {a: 1}\n  const obj2 = {...obj}\nt.true(obj == obj2)\n})\n```\n. Alright. Let's skip this then.\n. Really wish we had something like this: https://github.com/sindresorhus/xo/issues/103\n. Really wish we had something like this: https://github.com/sindresorhus/xo/issues/103\n. Let's have it in both for now.\n@kentcdodds Can you submit to https://github.com/sindresorhus/awesome-ava too?\n. See: https://github.com/sindresorhus/ava/issues/474\n. Nice work @cgcgbcbc :)\nLGTM\n. Hey @fenos. Could you post this question on Stack Overflow instead and tag it with ava? We like to answer, but if we answer here it will just disappear and nobody else will benefit. Make sure to comment the link to the question here.\nhttps://stackoverflow.com/questions/tagged/ava\n. I've not experienced this personally. You say \"at random\", but approximately how often? Can you try npm cache clean && rm -rf node_modules && npm install && npm test? Babel just released a new version which improves require performance considerably. Could you also try to see if you can reproduce with Node.js 6? That would help us track it down. Thanks :)\n. Yeah, most of our perf problems are Babel related. Ok, changing the test invalidates the cache for that file, but it should never take this long to recompile. Does indeed sound like it's trying to transpile the whole node_modules directory by mistake or something. Would you be able to share a demo repo with the issue? If not, at least the AVA and Babel (if there) config in package.json?\n. Closing for lack of response.\n. LGTM\n. \ud83d\udc4d \n. > I'm of the opinion that using after / afterEach for cleanup is a bad plan.\nWhy? I've always done cleanups in the after hooks too.\n\nI think you should have a conditional cleanup in a before / beforeEach instead.\n\nIt doesn't sound very atomic to have to clean up after the previous test in the before hooks.\n. That's a good point, but definitely needs good docs on that as it can be surprising behavior.\nI guess what I'd like is a hook that is run both before and after, so I can ensure it's cleaned up no matter what. Not a big problem for me though, as I've learned to use temp files for fixtures instead of putting them in the current directory, and they don't need cleaning up.\n. Can you give the PR a proper title?\n. It might also be restore-cursor not being able to catch the error. (It's using the exit-hook module for that).\n. I don't think you meant to commit ava-0.14.0.tgz.\n. \ud83d\udc4d \n. \ud83c\udf70\n. See: https://github.com/sindresorhus/ava/issues/695\n. :shipit: \n. Yup, only the link.\n. LGTM\n. Really great work on this @cgcgbcbc \ud83d\udc4c Thank you!\n. @gluons Most of it is Babel, which is unfortunately out of our control. Try https://github.com/yarnpkg/yarn instead of npm. It's a lot faster.. Yeah, it should already work, but trying it out now it doesn't. We'll look into it. We're probably just not passing the flag down to the spawned test file.\n\nAlso, please open up an issue on https://github.com/chalk/supports-color with the output of running node -p 'process.env' (ensure you scrub any sensitive info) in TeamCity so we can try to disable colors automatically for you.\n. @vdemedes No worries. The beginner badge is for fixing the --no-color flag in AVA.\n. I also noticed that we need to handle __tests__ so it's not shown in the output.\n. \ud83d\udc4d \n. We also need to add test.failing. Anything else that needs updating?\n. // @ivogabe\n. @ivogabe @SamVerschueren Could either of you add test.failing?\nhttps://github.com/avajs/ava#failing-tests\n. > should we prevent skipping hooks?\nNo\n\nalso should we allow only on hooks?\n\nNo\n. I think we should leave the deprecation in for the next 0.15.0 release and then remove it afterwards. There's no rush.\n. > I'm fine with leaving doesNotThrow in. Do we want to write a codemod for it?\nNah. It's been warning for a while, so most users should have fixed it.\n\nPS: Shouldn't this rather be in the 0.16 milestone if we want to leave it in 0.15?\n\nFixed.\n. // @twada \n. Thanks :)\n. Ouch, glad you caught this. Changes looks good :)\n. It's not released yet. The readme is for master (this is the case for most projects on GitHub). See https://github.com/avajs/ava/tree/v0.14.0 or https://www.npmjs.com/package/ava for docs for the latest version. We plan on doing a new release in a day or so.\n. \ud83d\udc4d \n. Shouldn't we log no matter what the code is?\nPeople should not be calling process.exit() in tests at all.\n. I also think we should simplify the event transmission.\n. > I'm not sure if this is possible?\nIt is. Order doesn't matter. The modifiers just add options to the test. Imagine test.cb.serial as just test({type: 'cb', serial: true}).\n. @novemberborn Node.js also strips the BOM when you require something. I think we need it to match that behavior and ensure we never fail because of a BOM in a file.\n. @jamestalmage We can use skipped tests.\n. We also need this for a forthcoming Flow type definition: https://github.com/avajs/ava/pull/1007\n. @jamestalmage I don't think that will help much. You ignore the cache directory when adding AVA to a project and then forget. Later you experience a cache bug or another team member experiences it, and neither realizes it's a cache issue. Instead, I think we should do a good job as possible to hash correctly, even Babel config chains, and when it's absolutely not possible, print a warning. We could maybe also improve the documentation about clearing the cache on issues (even in the issue_template.md) and add a --clear-cache flag.\n. @jamestalmage I don't think that will help much. You ignore the cache directory when adding AVA to a project and then forget. Later you experience a cache bug or another team member experiences it, and neither realizes it's a cache issue. Instead, I think we should do a good job as possible to hash correctly, even Babel config chains, and when it's absolutely not possible, print a warning. We could maybe also improve the documentation about clearing the cache on issues (even in the issue_template.md) and add a --clear-cache flag.\n. @jescalan rm -rf node_modules/.cache. @SamVerschueren @ivogabe :shipit:?\n. @develar Doesn't https://github.com/avajs/ava/pull/929 supersede this? Or are both needed? It's a bit unclear how they relate.\n. This looks good to me.\n\nNote to self: Open issue about removing this eventually when the Node.js version we target has the fix (https://github.com/nodejs/node/pull/5025).\n. Landed. Finally :)\nThanks for doing the hard work here @develar and persevering through this long review.\n. :shipit: \n. @twada Is there any way power-assert can gracefully handle failures like this? Instead of hard failing, it would just not decorate the output, and include a message on why. That way it's still usable.\n. I fixed the merge conflicts and landed this to unblock other work. Great work James :)\n. @jamestalmage Is this PR blocked by https://github.com/babel/babel/pull/3564?\n. I've commented on some nitpick, but generally looks great!\n. Oh wow. That translation arrived fast! :)\n. @jrajav What breaks? Please open an issue about it.\n. @GramParallelo Yes, go ahead. We're happy to handhold you through it :)\nJump into https://gitter.im/avajs/ava if you have any questions.\n. Yes\n. > not sure why the stacks got longer from when I started):\nSee: https://github.com/tapjs/stack-utils/issues/14#issuecomment-238026000\n. Looking good :)\nThe test output should be vertically aligned on the left with 2 failed.\n. @sotojuan I think you forgot to push some commits.\nThis is what I'm seeing:\n\nIt's not aligned and the stack should not be red.\n. Also notice from the original issue:\n\nAnd I changed failed with \"foo\" to Error: foo to make it clear it failed with an error and what type of error. Could be a ReferenceError for example which would be lost in the previous output.\n\nCurrently, if I throw a TypeError in the test, it will still show Error: foo, but it should show TypeError: foo.\n. And the merge conflict needs fixing ;)\n. Using the code in the original issue, this is what I'm seeing:\n\nNotice how the stack trace is indented consistently, and undefined shows up twice.\n\nAlso think we might need to indent the stack trace two spaces instead of 1. Right now it looks accidentally unaligned.\n. Hmm, must have been me doing something weird.\nAnyways, tried it fresh now, and with:\n``` js\nimport test from 'ava';\nconst fn = () => 'foo';\ntest('unicorn', t => {\n    t.is(fn(), 'bar');\n});\n```\nI get:\n\nNotice how the foo is printed twice and the stack is indented way too much.\n. @vdemedes Already answered: https://github.com/avajs/ava/pull/951#issuecomment-238353808\n. Super work @sotojuan :)\n. \ud83d\udc4d \n. Looks good. Thank you @MK-61 :)\n. Thanks for starting this.\nMaybe link to https://medium.com/@benlesh/hot-vs-cold-observables-f8094ed53339 for people not knowing the difference.\nCan you also add it to https://github.com/avajs/ava#recipes\n\nAva doesn't support most of the observables created in web development.\n\nIs there anything AVA could do to better support hot observables?\n\nDo you know anyone that could review this?\n// @blesh @SamVerschueren \n. Cool. I'm going to close this for now to clean up the pull request queue, but more than happy to reopen when you're ready for another review :) (Also make sure to address the inline feedback then).\n. > I had to switch to tape due to the lack of feedback on which test was failing\nHow is tape any better for this? Just don't return the observable and use test.cb and you have the exact same behavior as tape.\n. Thanks for the feedback @marcusnielsen. Appreciate you taking the time to write this out and totally understandable.\n\nThe recursive imports made it easier to find the Socket.io bug since we could comment out whole parts of the file-tree until the bug went away.\n\nI think using different globs could have helped here, like only testing a sub-directory: $ ava test/foo/bar.\n\n2nd, we dealt with a non-closing Socket.io server which meant that about 3 tests were not completing.\n\nThat's indeed annoying. Has happened to me too. I have some ideas for using https://github.com/mafintosh/why-is-node-running to show why it's pending, but in the short term, we plan on showing pending tests: https://github.com/avajs/ava/issues/583\n\nI'd love to see Ava succeed, but I'd love to get a more slimmed experience where transpilation isn't a part of the tool.\n\nhttps://github.com/avajs/ava/issues/709\n. Fixed by https://github.com/avajs/ava/commit/1dd6d5b9127a6850d9a03c242e677ea359d67053.\n. Super first PR. Thanks @asafigan! Keep 'em coming ;)\n\n. @jamestalmage Agreed. I've updated the issue text.\n. @jonathandelgado It has the downside of making test.only() only work per file and not globally. See caveat in https://github.com/avajs/ava/pull/791.\n@novemberborn Do you still think we shouldn't do this? What is your current opinion?. > This would give us the experience of exclusive tests while not having to load all test files in advance (which then gives us other scheduling benefits).\nIt's a cool idea, but I think it's also too clever and not worth all the work and potential downsides. I'd rather spend energy on making it easier to filter down tests.. Just wanted to say that we appreciate the pull request :) This will have to wait for a little bit as we would like to get https://github.com/avajs/ava/pull/880 landed first.\n. @dananichev #880 is not happening now after all, so we can go forward with this :)\n. Nice work @dananichev :)\n. Added some more nitpick feedback inline. Otherwise looks good :)\n. The Windows build is failing: https://ci.appveyor.com/project/ava/ava/build/170\n. \ud83c\udf89 Looks great now. Thank you @dananichev :)\n. Not much we can do unless you follow the instructions here and provide some more info.\n. // @ivogabe @SamVerschueren \n. Is this good to merge now?\n. test('2 + 2 === 4', macro, {4: '2 + 2'}); should definitely be supported.\n. Alright, let's go with ...args: any[] then. We can improve it when https://github.com/Microsoft/TypeScript/issues/5453 is fixed.\n\nbut I think that it might be better to define macros based on partial application/currying like this: (using the same example)\n\n\ud83d\udc4d Could be added to the TypeScript recipe.\n. Thank you @ChristianMurphy for this excellent contribution, and to @ivogabe and @SamVerschueren for reviewing :)\n. Do you mean the source of AVA itself or the source of your tests?\n. No plans to do that. Creating a test runner is already hard and adding a layer of transpilation on top is not worth it. We'll adopt more ES2015 syntax when we can drop support for Node.js 0.10 and 0.12.\n. Thanks @ChristianMurphy and @ivogabe :)\n. > Also, @sindresorhus, do we have rules around when error messages end in a period? Looking at the deletions we haven't been consistent about this before, and perhaps we don't care, but wanted to raise it regardless.\nSee: https://github.com/sindresorhus/eslint-plugin-unicorn/issues/26\n. @asafigan Any chance you would be able to address @novemberborn's feedback so this can be merged?\n. ping @asafigan \n. > So, I need to rewrite the tests in test/runner.js.\nYes, so they're not tied to the internal implementation. Just make them like the existing ones you already have there: https://github.com/avajs/ava/pull/980/files#diff-a0b87ab613d07bba77930609ec360b3dR323\n\nWhat about reformatting the error messages? I couldn't tell if there was a discussion with that.\n\nWhat you have now is fine.\n. Thank you @asafigan. Excited to finally land this :). Looks like a Babel bug or caveat indeed.\n@Timyipstudio Could you maybe try asking about it in the Babel support channels?\nSidenote: If you want to check if something throws, use t.throws() instead.\n. @jfmengels Yup: https://github.com/avajs/ava/issues/148 \n. Related Node.js issue: https://github.com/nodejs/node/issues/6624\n. @novemberborn would be the best one to review this.\n. @ltk Friendly ping :)\n. Closing for lack of response.\n. > I guess that Flow would also need a generator script, since the parameter type depends on the chain\nThat really depends on how good the Flow type system is. It might have ways to handle those cases without the TS amount of boilerplate. Would be good to have someone familiar with Flow to comment.\n\nHowever, can't Flow read TS definition files?\n\nNot yet, and it wouldn't be optimal as Flow is stricter than TypeScript.\n. I appreciate everyones input. We really want to have the best possible type definition for both TypeScript and Flow. Let's keep this thread Flow related and continue the TypeScript definition discussion in https://github.com/avajs/ava/issues/1005.\n. Same answer as https://github.com/avajs/ava/issues/989#issuecomment-251148859.\n. @novemberborn It can be useful, especially until we do https://github.com/avajs/ava/pull/929. No idea why it's currently breaking though.\n. > not sure if the whole combinedTickcallback etc should appear though....\nIt should not, but that's an unrelated issue. I've reported it here: https://github.com/tapjs/stack-utils/issues/14\n. // @twada \n. It might just be that Node.js 4 is slower. Did you try increasing the timeout? To maybe 20s just to test.\nAlso make sure to test with the latest AVA 0.16.0.\n. I would rather bet on some race issue in your test or code that causes t.end() not to be called in one of the tests. If t.end() is not called the test run will timeout. I would recommend using t.plan(), especially in tests like https://github.com/kripod/wsx/blob/84b892acf83713461784dbf9522760b7909a71e0/test.js#L21-L44\n. @kripod It won't solve your issue, but it will help pinpoint where it is.\n. Needs to be fixed in https://github.com/sotojuan/not-so-shallow.\n// @sotojuan \n. @forresst Yes, useful in case we at some point change to another implementation.\n. \ud83d\udc4c\n. @platy11 Yup, looks good.\n. Yes, and add it to https://github.com/avajs/ava-codemods/blob/master/codemods.json (add a 0.17.0 version)\n. Yes, looks good. Also needs to be added to the readme: https://github.com/avajs/ava-codemods#supported-codemods\n. The code in the readme is valid JavaScript, so that's either a problem with the type definition or TypeScript.\n. // @ivogabe \n. > Sorry, forget to mention \u2014 nodejs/node#5025 only about --debug-port=0, but currently no issue about --inspect \u2014 please see nodejs/node#5025 (comment) \"it should be possible to extend this to --inspect after this PR lands.\" Currently, --inspect should be used to achieve the best user debug experience (yeah, nodejs 7 is required). - @develar\n@develar Could you open an issue on Node.js about the --inspect flag?\n. @develar Do you know anyone that could be interested in doing this?\n. Excellent first contribution! \ud83c\udf89\n\n. Keep 'em coming ;)\nYou might also be interested in https://github.com/avajs/ava/issues/850.\n. Thanks :)\n. \ud83c\udf8a\ud83c\udf89\n. Remove this test: https://github.com/avajs/ava/blob/28bb0d5cb2907282d61e224acdf621ce3c1abdfd/test/assert.js#L362-L378\n. Cool. Thank you :) Keep 'em coming \ud83c\udf7b\n. @sotojuan No, we're using another more advanced module in AVA. Should be very easy to fix in stack-utils though. Just need to support / in the stack path.\n. // @ivogabe \n. Agreed. In hindsight, it was not worth all the bloat, but it wasn't clear at the time how verbose it would turn out. I don't think anyone would do test.serial.serial anyway and we could indeed have a ESLint rule for it.\n\nwe should create a AVA TSLint rule as well.\n\nI think we should wait on typescript-eslint-parser. We don't want to recreate all of our rules in another linter.\n. @novemberborn I agree. Would be nice to limit it at runtime too. Maybe something that could be added to option-chain? Like mutually exclusive options or the inverse.\n. By default, AVA shows minimal output on success, but verbose failures. We feel there's no point in showing a lot of verbose info when everything worked. If you really want verbose output, run AVA with the verbose flag: $ ava --verbose.\n. @kennetpostigo That's the verbose output. Try $ ava --verbose and you'll see.\n. Ah ok. Yes, that screenshot is misleading. I've removed it.\n. Awesome!\nYou also need to add this file to the files array in package.json\n. Sidenote: Someone really needs to submit Flow syntax highlighting to the syntax highlighter GitHub uses.\n. @thejameskyle Do you know anyone familiar with Flow that could review?\n. @thejameskyle Is this good to merge?\n. Yay \\o/ Thank you @thejameskyle :)\n. Thank you @prigara :)\n. It's fixed in master (https://github.com/avajs/ava/commit/c268c8dfce325010f84dd4097ba8696c5f3a47c1) and will be available in the next release: https://github.com/avajs/ava/blob/master/docs/recipes/debugging-with-webstorm.md\n. @jasongin I think that fix is only for WebStorm. For a CLI fix, see https://github.com/avajs/ava/pull/929.\n. I'm not familiar with Webpack, but it's not something AVA itself should be concerned about. You're using Webpack to add non-standard functionality (aliasing) to require, so it's up to you, or rather Webpack to make it compatible. I guess you could compile your code using Webpack and test the compiled code. There are also Babel plugins you could use, like babel-plugin-webpack-loaders or babel-resolve-relative-module.\nCan anyone with Webpack experience comment here?\nWe should really have a recipe for using Webpack with AVA.\n\n// @istarkov\n. We would be happy to answer your questions, but this is an issue tracker for actual issues. Ask on Stack Overflow instead and use the ava tag. Make sure to post the link to your question here for posterity :)\n. > or is there a way to use AVA with Karma?\nWe're working on it, but it's not yet ready.\n. Looks great! \ud83d\udc4c\n. Good catch. Thanks Zeke :)\n. Makes sense to me. I've run into some problems with power-assert in the past too.\nThat doesn't mean you shouldn't report issues regarding power-assert, though. @xjamundx Can you open a separate issue about that?\n. > If also in the CLI, what should the flag be named? --no-power-assert?\n\ud83d\udc4d \n. @rhubarbselleven Interested in submitting a recipe?\n. @jfmengels Tests pass on 6.4.0, but not on the just released 6.5.0. So it seems something changed there that broke some assumptions in the tests. Unrelated to your changes though, as it breaks in master too.\n. Looks great! Congrats on your first pull request as a core team member :D\n\n. Thank you! :)\n. I wouldn't consider it a bug in fn-name. It's just reading the .name property off the function. It seems the V8 version in Node.js 6.5.0 got better at inferring the function name.\n. Awesome :)\n. @fearphage Why do you need this? What problem does it solve? Why not just run $ time ava when you want to know the total test run time?\n. > It shows you the total duration of a test run.\nThat's the solution, not the problem. What will you do with the knowledge of the time it takes to run the test suite? Why do you need it on every run? I'm just trying to figure out why this is essential.\n. > I don't fully understand what you're asking. I will do the same thing I do with the knowledge of the time it takes to run individual tests. That's currently a feature in ava today right now. Do you have a problem with that functionality as well? What's the difference between individual test times and suite run times?\nThat feature is there to expose slow tests. We only show it when the tests crosses a set threshold, which currently is 100ms. That's different from showing the total time. You can't do much about the total time unless you know the time of the individual tests.\n\nI have a few questions for you. Why would you not want to see how long your tests take to run? What is it detracting from your code, tests, and/or life?\n\nBecause everything we add to the output creates noise and distracts from the other information. We are very conservative about adding things. The default AVA output is intentionally minimal. But we're happy to consider adding things if they have practical usefulness. Convince us ;)\n. > I don't see individual test time in my verbose output.\nOnly for slow tests. >100ms.\n. I'm open to adding the test time to the verbose and TAP reporter, but I'm hesitant to add it to the mini-reporter. The mini-reporter is meant to be as simple as possible, and I personally don't see much value in the test time. And I agree with @novemberborn, no point in having it in watch mode.\n. > What about adding an option for it? It's inconvenient to have to leave watch mode just to see how long the tests are taking.\n@novemberborn already commented why showing total test time in watch mode is not useful.\n\nWhat's the point of printing the timestamp in watch mode? Is it so you can see that things are changing at a glance? If that is the sole purpose, could this replace the timestamp? It's more useful in terms of testing than showing the time.\n\nYes, it's to see if something has changed, and no, total test time could not replace it for the same reason as above and the fact that the test time could be the same as previous and you would have to remember the previous test time to be sure it's different.\n. > Unless I'm missing it, @novemberborn only said s/he didn't find it useful.\nNo, he said why it's objectively useless:\n\nespecially since the number of test files run can vary greatly.\n\n\n\nThis is a possibility. The lack of precision in the timing adds to that factor. Hitting the same millisecond is definitely possible. The same nanosecond is less probable.\n\nThis has nothing to do with precision. Even if we used nanosecond precision we would only show a prettified output to the user. Let's say a test takes 1.4243242 seconds to run the first time and 1.45435353 the second time, we would only show 1.4 second for each.\n. > I have a use for this \"objectively useless\" thing.\nEither you're not understanding how it's useless or you have a use-case you haven't being able to explain, but I feel this discussion is going nowhere. I would recommend focusing on what we're willing to do and not push this anymore unless you can actually come up with some convincing arguments not already exhausted ;)\n\nI had it setup for 3 decimal places so the output in your examples would be 1.424s and 1.454s respectively I believe.\n\npretty-ms converts it to 1.4, which is what we want.\n. > pretty-ms in its current configuration increases the likelihood of repeated output. You cited repetitious output as a problem. You also said this has nothing to do with precision. However as you increase the precision, repeated output becomes less likely. So it directly addresses one of the problems that you cited. I don't understand why you're saying it has nothing to do with this.\nAlready said this, but I guess I can repeat myself again, we want it to be 1.4, not 1.424s. I do realize more precise output would make one of my arguments less relevant, but that's irrelevant as it's not something we want.\n\nReally?! Since I already explained the use case a few weeks ago, I guess it's the former in this false dichotomy. Are you going to elaborate on the uselessness of my use case? Help me understand how useless it really is.\n\nYou want the total test run time so you can use the deltas to see if it became faster or slower, but watch mode only runs tests that changed or had their source change, so the delta is not constant, making it useless for this use-case, as the time will vary greatly depending on what tests/sources changed.\n. > Does ava support external reporters currently?\n$ ava --tap and you can use any TAP-reporter, but it does not apply during watch mode, neither will it ever.\n. Closing for lack of response from OP.\n. @sotojuan Could you add a test?\n. AppVeyor error:\n\nError: Cannot find module 'C:projectsavaindex.js\n\nMy first thought; Where are the path separators?\n. Sounds good.\nThis would also be a good opportunity to document the package.json options separately instead of just referring to the CLI flags and show a JSON config dump.\n. Alternative proposal.\nMake it t.throws(fn, matchers, [message]);, where matchers is an object of different matchers that all has to match. Matchers can be string or regex.\nThis could easily be an addition to supporting string/regex/constructor in the second argument, for backwards compatibility.\nFor example:\njs\nt.throws(() => foo(), {\n    constructor: RangeError,\n    message: /unicorn pasta is/\n});\nCompared to the current way:\njs\nconst err = t.throws(() => foo(), /unicorn pasta is/);\nt.true(err instanceof RangeError);\nPossible matchers constructor, message. Could also consider supporting name for when you don't have easy access to the constructor function. Maybe also stack if you want to ensure something is part of it, though I don't think I've ever needed that.\nBenefits of this is that it makes everything explicit. No more wondering for users what t.throws(() => foo(), 'Unicorn') matches against - Is it the name, type, or message. There's also benefit in being able to specify all the constraints directly instead of in separate calls later, like potential for a better diff.\n\nWe should also have much better validation logic to prevent mistakes. For example, using is-error-constructor on the constructor matcher.\n. > It would still return the error for extra assertions, right?\nYes\n\nJust support only constructors in the second argument, since that's the only type that is non-ambiguous for what it is matching. Delegate everything else to t.is(), t.regex(), etc.\n\nIt creates verbose code for common cases though. The most common case is wanting to ensure the error is the correct type and has the correct message. I think we should optimize for that.. I had the need the other day to match against a custom error property. Should we support that too?. @novemberborn I agree, but what other flags would we remove?\n\nOn the other hand the --source flag may have to stay\n\nWhy?\n. > I assume we treat any t.true expression in the test file as if it was a power assertion?\nYes\n\nI don't think we can scale that to every imported module (recursively!), especially if we'd want to cover macros distributed via npm.\n\nCorrect, mostly because we don't yet transpile imported files. Maybe we could come up with a naming convention for macros so we would know to transpile and power-assert'ify them, kinda like helper files \u2192 #720.\n\nPerhaps we could add an ava/macro helper which compiles the received function source?\n\nCan you provide an example of how it would be used?\n\nFor now, we should at least find a way to be able to power-assert macros defined in the same file. That's the most common use-case. @twada Any ideas?\n. > We could assume helper files contain macros? Intercept the require calls and transpile them separately.\n\ud83d\udc4d \n\nIn other words we can get the function source and transpile it on the fly, then evaluate using the vm module. Perhaps with some caching.\n\nThat could work, but I would very much prefer something that just works, like now, than having to have a separate macro function.\n. @novemberborn I agree that for distributable macros, ava/macro would make sense.\n. Dropping support for those versions would make a lot of our code simpler and enable us to upgrade many dependencies that now require Node.js 4. Although test runners are unfortunately usually the last projects to drop support for older versions... If it were up to me, I would drop support today, but I know people depend on this in production, so that's not what we're going to do. I like the idea of maintaining an LTS version of AVA though, which only gets critical bug fixes.\n. @novemberborn I'm open to that. Let's see what the rest of @avajs/core thinks about it.\n. Proposed plan:\n- Release 0.17.0 as last actively maintained version with Node.js 0.10/0.12 support. That means it might get some bug fix releases, but no new features.\n- Release 0.18.0 requires Node.js 4\n- Drop total support for 0.10/0.12 at the end of the year.\n. Yes\n. Yes. We can close this. The version in master targets Node.js 4.. This is fixed in master and will be part of the next release. https://github.com/avajs/ava/commit/8816faf6bee675213c5bfe32f628a0a683708484\n. @lukechilds Are you committed to keeping this module up to date with jsdom?\n. Good idea. Should be in addition to the React one.\n. Thanks :)\n. I assume it does strict value equality checks?\nDoes it handle circular references?\nI noticed it doesn't handle Node.js buffers? Would you able to add that?\nSee https://nodejs.org/api/buffer.html#buffer_buf_equals_otherbuffer\nWould you be willing to ensure lodash.isEqual handles these issues: https://github.com/sotojuan/not-so-shallow/issues ?\n\n\nIt is an equivalence method so somethings may be seen as equiv that aren't like 1 and Object(1) (legacy rules).\n\nThat's fine. They're not equal.\n\nThe behavior can be customized with lodash.isEqualWith.\n\nMy goal is not having to do that. Would be better if we all agreed what deep equality entails.\n. > Node buffers fall under the typed array support.\nI would be explicit about that in the docs. While Node.js buffers inherit from TypedArray, they're not entirely the same thing and not everyone knows buffers inherit from TypedArray.\n. @vdemedes It's already part of a Lodash release, so we can just use isEqual. We should bring in the tests from https://github.com/sotojuan/not-so-shallow/tree/master/test just to make sure we didn't break anything. Wanna do a PR?\n. Is this still an issue? I was unable to reproduce by cloning the repo and running the same environment as you. If yes, can you try upgrading to Node.js 6.7.0, npm cache clean && rm -rf node_modules && npm install && npm test and see if it still happens?\n. @mmkal It's seems like what you need is the path of the project root directory (where package.json resides)? If so, in https://github.com/avajs/ava/issues/32 we plan on changing process.cwd() to always be the project root path (where the tests are run from - package.json). When that's changed, you could simply use process.cwd().\n. I think that's specific to your tests. I use AVA with many projects and never experienced any problems with that. Did you figure it out?\n. If I were to guess, copying the node_modules folder in so many processes exhausted the memory limit. See: https://docs.travis-ci.com/user/common-build-problems/#My-build-script-is-killed-without-any-error\n. @diegohaz Does it also fail if you limit the concurrency to 5?\nIn https://github.com/avajs/ava/issues/966, we're looking into setting a default concurrency. Right now it's unlimited.\n. Would be good if @jamestalmage could review, as he's the one that wrote most of the code in api.js.\n. Alright. Landed. Good stuff @vdemedes :)\n. Yeah, that would be useful for our Atom and Sublime plugins too. I think we could add some custom metadata to the TAP output, like filepath, line, column. Actually, we should really include that info for all tests.\n. > From what I have read about the TAP standard these additions we are talking about are outside of the specification, so we can call them and format them however we want?\nYes. They should be added as data in the YAML-block. There we are free to add whatever data we want.\n\nSo we could use your filepath, line and colum as a starting point, adding this to the output in both error and pass? https://github.com/avajs/ava/blob/master/lib/reporters/tap.js#L35:L51\n\n\ud83d\udc4d \n\nDo we actually have access to that information when we are in the reporter though? As far as I know thats not in test.metadata or anything else?\n\nOnly for errors currently, but would be useful to have it in the metadata.. Do you run $ node with the --preserve-symlink flag? That's the only time that message should show or if you're using an old v6 version.\n. Sounds like a very bad idea. It's a flag for a reason. You should only use it when you control the whole environment.\n. I don't remember why. See: https://github.com/avajs/ava/issues/814. @jamestalmage would be the best person to comment on this.\nWe're not going to add an option, but if there's any way we could automatically support it without too much code I'm open to it.\n. > any interest in such a feature, if it should come up as a PR? (and is even feasible, haven't dug too deep into the worker/process stuff)\nNo, it's not something we want to explore right now.\nRelevant issue where we should have this discussion: https://github.com/avajs/ava/issues/421\n. What exactly does ./teapot contain?\n. I looked into this. Turns out you can't extend a native ES6 class with a ES5 type \"class\". Since your test is transpiled while ./teaport is not, the test has a transpiled ES5 type class while ./teapot has a real ES6 class.\nSee:\n- http://stackoverflow.com/questions/32981988/class-constructors-cannot-be-invoked-without-new/32982156\n- https://github.com/babel/babel/pull/3582 (fix)\n. Yes, AVA transpiles with Babel by default. See: https://github.com/avajs/ava/blob/master/docs/recipes/babelrc.md\nWhat Node.js versions do you run your test with?\n. You could use this config then in your package.json:\njson\n{\n  \"ava\": {\n    \"babel\": {\n      \"presets\": [\"es2015-node6\"],\n      \"plugins\": [\"transform-async-to-generator\"]\n    }\n  }\n}\n$ npm install --save-dev babel-preset-es2015-node6\n. @novemberborn Does the caching take into account process.version?\n. @novemberborn Done. I went with only salting per major Node.js version. Don't want the cache to expire just because the user did a security upgrade.\n. Thanks for reviewing :)\n. @ntwcklng We always appreciate help, but @vdemedes is currently doing a big rewrite of the output, so this would conflict. I would ask you to wait until that work is done or try another issue :)\n. Only partly. The current output looks like:\n```\n\u276f ava\n1 failed\nfoo\n  test.js:24\n23:\n   24:   throw new Error('unicorn');\n   25: });\nError: unicorn\nTest.fn (test.js:24:8)\n```\nAs you can see, the test file and line are shown twice. I think we could simplify it to not show the stack trace if it's only the test function. @avajs/core Thoughts?. @LasaleFamine Yes :). @LasaleFamine Yes, the mini and verbose reporter, but not tap.. You also need to update this paragraph in the readme:\n\nTest files are run from their current directory, so process.cwd() is always the same as __dirname. You can just use relative paths instead of doing path.join(__dirname, 'relative/path').\n. @alathon Fixed in https://github.com/avajs/ava/commit/6165a233c5f35bdf3538103f4a6cdcac4a560d6d.\n. Thanks @alathon :)\n. @novemberborn This fixes #32, right?\n. Update readme.md is not a very good or descriptive commit title ;)\n. > We could end up deleting a dependency that we still use, yes, but we have a similar problem with deduped dependencies.\n\nThe latest XO prevents this through the import/no-unresolved rule.\n. I'm still sceptical to caching the node_modules folder, but I'm open to trying it out and see how it works out. As long as we use np when publishing, we're certain we'll at least never publish a broken version. And yes, let's do pruning.\n. Nice cleanup @vdemedes :)\n. @vdemedes What's your use-case for transpiling fixtures? I'm leaning towards @novemberborn's argument.\n. Code looks good to me, but needs to be added to the readme.. Yay! This one is going to be super useful \ud83d\udc83 \nThanks for finishing this up Vadim \ud83d\udc4c. @tonyeung Using AVA with TypeScript and using our type definition are different things. The former we have a recipe for. The latter should just work. I'm not really sure what you're asking for here?\n. I don't use TypeScript, so no idea, but maybe @SamVerschueren that wrote the recipe could chime in.\n. > The reason for upgrading, rather than leaving xo pinned at 0.16.0 is that when I'm editing using atom-linter-xo I still get all the errors from 0.17.0. This is annoying to me and possibly confusing to contributors.\nhttps://github.com/sindresorhus/atom-linter-xo/issues/41\n. @novemberborn Looks good. Can you fix the merge conflict and merge this?\n. I think you linked to the wrong issue. Can you correct that?\n. \ud83d\ude4c\u2764\ufe0f\n. // @twada \n. @twada I would definitely want to see the array value with the SuccinctRenderer. Just seeing -1 isn't very useful.\n@jyboudreau The verbose one is too verbose. We try to strike a balance. In this case, ours was too minimal.\n. Duplicate of https://github.com/avajs/ava/pull/480.\n. // @lukechilds \n. Not at the moment, but your needs can be solved with the existing methods:\njs\nt.true(object instanceof clazz);\nt.is(array.length, length);\nt.true(array.every(x => [item, item].includes(x)));\nt.true(string.includes(substring));\nYou can already use any external assertion library. The only caveat is that t.plan() doesn't work.\n. @capaj This is about tests, not test files.\n\nBTW I think ava should by default limit the number of concurrent tests to the number of available procesor cores on the current machine. That way optimal performance is achieved on my experience.\n\nWe might do that for test files (see open issue about it somewhere), but it does not make sense for tests.\n. Thanks :)\n. @thinkimlazy Why did you close?\n. @thinkimlazy You never have to open a new PR. That just creates noise in the issue tracker. You can always reset the state of an existing PR and start over if you really messed up.\n. No worries :)\n. Duplicate of https://github.com/avajs/ava/issues/148. I'd like to see it mature more first.\n. Ah yes\n. t.throws is already too overloaded. We're not gonna make it even harder to use.\nWe're planning to simplify it over in https://github.com/avajs/ava/issues/1047. Could use your thoughts there ;)\n. 1. is correct as it's just ===.\n2. is just === for every property in the compared object.\nThis is just a quirk in JS. I don't think it's our job to fix that.\nSame result with:\n```\n\u276f node\n\nassert.deepEqual(NaN, NaN)\nAssertionError: NaN deepEqual NaN\n```\n\nThe correct way of checking for NaN is using:\njs\nt.true(Number.isNaN(NaN));\n. @SergioMorchon This issue is old. t.is() handles NaN now. t.deepEqual() does not. Open a new issue about handling NaN in t.deepEqual().. It also needs to pass the --no-color flag down to the forked process. If I add console.log(require('chalk').red('colorzzzz')); in a test file, it will still log colors even with $ ava --no-colors. I would expect that to prevent all colors.\n. Not for me:\n```\n\u276f ava\n1 failed\nno colors here\nt.false(chalk.enabled)\n                |\n                true\nTest.fn (test.js:5:5)\ninternal/child_process.js:721:12\n\n``\n. Closing for lack of activity and to free it up for someone else interested in working on this.. @novemberbornprocess` is requirable:\njs\nrequire('process');\n. And if all tests in a file are serial, you can import just the serial test modifier:\n``` js\nimport {serial as test} from 'ava';\n// this is a serial test\ntest(t => {});\n``\n.before` hooks always run before the tests.\n. We have a short guide, but it's not currently working \u2192 https://github.com/avajs/ava/issues/988. We plan on supporting the built-in Node.js devtools inspector at some point \u2192 https://github.com/avajs/ava/pull/929.\n. Great. I'll keep this open for discovery until I have time to add something more visible.\n. @jsatk No one is currently working on this. We would welcome a pull request :)\nSee https://github.com/avajs/ava/pull/929 for the previous attempt. Especially https://github.com/avajs/ava/pull/929#issuecomment-248280229.. @tony-kerz That command only works on one file, not a directory.. @Qix- Yup, that's the plan. https://github.com/avajs/ava/pull/929 started work on it, but was never finished.. Thanks for picking this up @zixia :)\nYour proposal looks pretty good.\nSome random feedback:\n- It should be possible to set --ext multiple times.\n- .js files should always be included.\n- TypeScript should not be a dependency of AVA, but rather in the user's package.json. If it's not, we should throw a friendly error message. We bundle Babel because we depend on it in AVA for other things too.\n- Setting extension automatically works fine when you specify exact files, but most users specify globs, so not sure whether it's worth the inconsistency of only working in some cases.\n- I think --extension/-e CLI flag and extensions option name would be better.\n- Make sure you handle caching correctly.\n. I was planning to do it today actually, but didn't have much computer time. I've been a bit strapped for time lately. Will do a new release tomorrow.\n. Done: https://github.com/avajs/ava/releases/tag/v0.17.0\n. @masaeedu Nothing is stopping you from using the master branch if you need to: $ npm i avajs/ava. We do releases when it makes sense.. Yes, a pull request would be lovely \ud83d\ude01\nTitle suggestion: Debugging tests in Chrome DevTools (filename: debugging-tests.md)\nCould be a recipe: https://github.com/avajs/ava/tree/master/docs/recipes\n(Make sure to also add it to https://github.com/avajs/ava#recipes)\n. \ud83d\udc4d Makes sense to me. Lots of users need this.\n\nContext: https://facebook.github.io/jest/blog/2016/07/27/jest-14.html\n. > Produce filename and snapshot path dynamically\nI would look at how Jest handles this.\n\nEnhance reporting\n\n@vdemedes is working on a new output which includes diffing, so this could simply use t.is() comparison of the snapshot, and diffing will be supported soon.\n. > Builds are failing because jest-snapshot is using const in build files which isn't compatible with node 0.12. Is there any chance of upgrading node on travis?\nJust ignore that for now. We're dropping Node.js 0.12 support very soon.. > Does that mean we need to wait for the upgrade to be made?\nNo. I just upgraded a big part of the code base, but made sure not to touch anything changed by this pull request. Feel free to start using ES2015 features (for your changes only). Otherwise I'll do it after merge, no problem.. @lithin This looks very good :) I'll play with it some more tomorrow and do a more thorough review.\nDoes snapshot testing only work with JSX or can you use it with other things too?\nYou also need to add an entry to the Assertion section: https://github.com/avajs/ava#assertions\nCan you fix the lint issues reported by Travis: https://travis-ci.org/avajs/ava/jobs/177985206 ?. I was thinking something like this:\n```js\nx._snapshot = function (tree, optionalMessage, match, snapshotStateGetter) {\n    // actual implementation here\n};\nx.snapshot = function (tree, optionalMessage) {\n    x._snapshot(tree, optionalMessage);\n};\n```\nThat way the user (which uses x.snapshot) can never accidentally supply those internal arguments, and we can still easily test it using x._snapshot.. I noticed if create a snapshot, rename the test, and run ava -u, the previous test is still in the snapshot file. Is this intentional?\n\nI'm not getting the colorized diff output like you have:\n\n\nThe generated output should also be indented so it's vertically aligned with the test title in the output. You can use indent-string for this.\n. The indentation is not correct. As commented earlier, it should be vertically aligned with the test title. So you need to add 2 space indent. It seems the first line in the output is already indented by the assertion output, so we just indent the following text. Like this:\n```js\nvar message = 'Please check your code or --update-snapshots\\n\\n';\nif (optionalMessage) {\n    message += indentString(optionalMessage, 2);\n}\nif (typeof result.message === 'function') {\n    message += indentString(result.message(), 2);\n}\nstate.save();\ntest(result.pass, create(result, false, 'snapshot', message, x.snap));\n```. > Any ideas what this might be caused by?\nYes. The error is colored red by our reporter. I'm more confused by how you actually got a diff color there in the first place. But we can ignore it for this pull request, as we're very close to a rewrite of the reporter output, where this can be handled.. > I haven't seen anywhere that the test would appear twice after updating the snapshot. It could happen if you've changed the name of the test though?\nYes, that's what I wrote. I renamed the test and then ran ava -u. Shouldn't that remove old tests?. > Jest identifies tests by their name - if you rename a test, it's as if you were running a new one. It has no way of telling if a test of another name should be identified with the renamed one. In other words, I don't think it can remove old tests from snapshots - it has to be done manually.\nYes, that's desirable between runs, but when running $ ava --update-snapshots, I expected it to update the snapshot to the reflect the current state of the tests file, removing non-existing/renamed tests. Is there any benefit to keeping non-existing tests around after updating the snapshot? I don't really see it, as we already have git, but I might be missing something fundamental about how it works.. @lithin Alright. I'll open an issue on Jest when I have some time :) This looks ready to me, but the tests are failing on Windows: https://ci.appveyor.com/project/ava/ava/build/1.0.462/job/jnx6609ubg58gfcq Mind taking a look?. Yay! Super excited about this. Thank you so much for working hard on this @lithin. We really appreciate it. \u2728\ud83c\udf89\ud83e\udd84\ud83d\ude80\ud83c\udf38. @thejameskyle halp\n. Thank you :)\n. You have your parens wrong:\ndiff\n-t.throws(Promise.reject(new Error('foo'), 'bar'));\n+t.throws(Promise.reject(new Error('foo')), 'bar');\n. Thanks for reporting. It's a documentation bug. It should say that it returns a promise for the rejection reason. I'll fix that up shortly.\nYou need to await it:\njs\ntest('rejection reason is foo', async t => {\n    const error = await t.throws(Promise.reject(new Error('foo')));\n    t.is(error.message, 'foo');\n});\nThat being said. We're planning a better interface for the throws assertion: https://github.com/avajs/ava/issues/1047 (feedback welcome)\n. I don't think you pushed your changes.\nMerge conflict also needs fixing.. https://github.com/avajs/ava/pull/1122#discussion_r89947713. // @thinkimlazy. Closing as this is working now.. What error occured in your promise?\nThe only way the above error can be hit is if the message of the error exists but is not a string.. :+1:. You need to use the --timeout CLI flag.. @gajus We considered having a global default, but we realized there's no good default that works for everyone. Instead, we plan to show when we think a test is hanging: https://github.com/avajs/ava/issues/583 (Help welcome btw). See: https://github.com/avajs/ava/pull/1104. @fankee There's already an open pull request for this unfortunately: https://github.com/avajs/ava/issues/1130. Very good. Thank you @anoff. Keep the PRs coming \ud83d\ude0e\ud83d\udc4d. But then you wouldn't see the results until after they're all done. I don't think that's a very good UX.. > CLI-flags can be removed all the way now since i am not going to put ava options in 2 different places.\nYes, that's the motivation. Having it in package.json makes it easier for external tools, like editor plugins, to use the same config.. Duplicate of https://github.com/avajs/ava/pull/1111.. Actually. Let's just do it. Thank you @anoff :). I agree we should make it clear, maybe instead by showing the number of pending tests that didn't run?. @tjbenton I would recommend using our ESLint plugin, which would have caught that and many other things.. @ThomasBem Go for it! \ud83d\ude04. Looks good to me too. Nice work @rnkdev :). Yes, that's pretty bad UX. For this assertion we're using the built-in assert module, which obviously isn't very good. We plan on rewriting the throws/notThrows assertions: https://github.com/avajs/ava/issues/1047. Can you comment on what's left?. I noticed $ ava and $ ava --verbose has some inconsistent output. We should really try to unify the output and share logic between them. The output below 1 failed and 1 test failed [02:03:57] should be identical:\n\n. For t.fail(), I think we should make an exception and not show the Actual/Expected output. It makes no sense and the code excerpt is clear enough.\n\n. > I squash commits right away, so there wasn't a quick way to track progress.\nCan you not squash? It makes it impossible to review what changed. Just add commits and we'll squash at the end when merging.. @vadimdemedes Note this issue when using this PR: https://github.com/avajs/ava/issues/1176 (I've confirmed it). @vadimdemedes For reference, currently they output:\n```\n\u276f ava\n1 passed\n  1 exception\nUncaught Exception\n  function fooFn() {}\n```\n```\n\u276f ava\n1 passed\n  1 exception\nUncaught Exception\n  () => {}\n```\nI don't think we can do much better than that, as we have no information when people throw non-errors. We could maybe better indicate that it's a non-error. For example:\n```\n\u276f ava\n1 passed\n  1 exception\nUncaught Exception\n  Threw non-error: function fooFn() {}\n```. @vadimdemedes It's not entirely clear what you're asking. What error message? Why is it not helpful?. > Regarding https://github.com/avajs/ava/pull/1154/files#diff-9c6c3cd6ffcef4f57ac6e0c793b31b85R337, I don't think there's a need anymore to generate error message for t.deepEqual(). They aren't helpful either way. That's why I want to remove those NOTEs and keep the tests without asserting error messages, are you guys ok with this?\n\ud83d\udc4d No need now that we have good diffing.\n\nI mean, it's not a blocker, but it's probably easier for you to organize then for somebody who comes along later.\n\nRegardless of the outcome, it could be a separate PR, not here.\n\nDocument jsxEqual & jsxNotEqual (Added by Sindre)\n\n@vadimdemedes What's left other than this?. > Aside from documentation, nothing, I think. I'd like to take another look on output of t.true()/t.false(), but it can be postponed until later. I need to limit the number of statements in the output and see how we can prevent big arrays/objects displayed in full.\nYes. Should be postponed. We need to land this PR asap. Can you open a new issue about it so we don't forget?\n\nAs for docs, if anyone has some free time, I'd really really appreciate the help on updating the readme and documenting t.jsxEqual().\n\nSure. Happy to :). Snapshot testing seems to be broken:\n```js\nimport test from 'ava';\ntest('foo', t => {\n    t.snapshot({foo: 'bar'});\n});\n```\nI run AVA and then change 'bar' to 'bar2', I get:\n```\n  1 failed\nfoo\n  test.js:4\n3: test('foo', t => {\n   4:   t.snapshot({foo: 'bar2'});\n   5:\nActual:\nObject {\n  foo: \"bar2\",\n}\n\nExpected:\nundefined\n\nPlease check your code or --update-snapshots\nTest.fn (test.js:4:4)\n```\nExpected should not be undefined.. t.deepEqual() doesn't display very useful output:\n```js\nimport test from 'ava';\ntest('foo', t => {\n    t.deepEqual({\n        foo: {\n            bar: 3\n        }\n    }, {\n        foo: {\n            bar: 4\n        }\n    });\n});\n```\nResults in:\n```\n  1 failed\nfoo\n  test.js:4\n3: test('foo', t => {\n   4:   t.deepEqual({\n   5:     foo: {\n{foo:{bar:3}}\n  => Object {\n    foo: Object {\n      bar: 3,\n    },\n  }\n{bar:3}\n  => Object {\n    bar: 3,\n  }\nsdfs\nTest.fn (test.js:4:4)\n```\nI don't see the change to 4 anywhere there.. > Is this a source maps issue? I've always assumed that babel-core/register causes conflicts in source-map-support. We'll be rolling our own solution with RFC 001 so perhaps this isn't worth worrying about.\n@vadimdemedes Can you open an issue about this and mention that this will be handled by RFC1?. This is absolutely amazing @vadimdemedes! So good in fact, I almost want test failures now \ud83d\ude1d\n\n. The next AVA version will be lighter on Babel, as we'll require a lot less Babel plugins: https://github.com/avajs/ava/commit/5158ac8bb0d0a79acde12c1bf6dfc1bbf32fd71e\nApart from that, you could cache the node_modules folder and Yarn cache: https://github.com/avajs/ava/blob/415cd2fa0ec4bd2db585cffd8d9ba372e6f05a05/.travis.yml#L6-L9\nThis would be better asked on Stack Overflow though ;). Already answered on Gitter. In the future we would appreciate it if people kept the issue tracker reserved for actual issues. We have Gitter and Stack Overflow for support.. > But how will TS compilation be made? Will AVA have to ship with the TypeScript build tools? Or can we expect it as an optional peer dependency or something?\nWe will not bundle TypeScript. Users will have to add TypeScript as a dependency. We'll throw a nice error if they forgot to add the dependency. (It will also not be a package.json peerDependency as it's not strictly a dependency, but more like an optional peer dependency).. :shipit: . Looks great. Thanks for this excellent contribution @ThomasBem :)\n@novemberborn Can you open that follow-up issue you talked about?. The common mistake is not including the correct Babel plugin if you don't use the AVA defaults.. The common mistake is not including the correct Babel plugin if you don't use the AVA defaults.. It's not released yet. Like most repos on GitHub, the readme reflect latest master. If you want a specific release, see the correct git tag or npm page.. It's not released yet. Like most repos on GitHub, the readme reflect latest master. If you want a specific release, see the correct git tag or npm page.. Thanks for helping us out with this one @leebyron. We really appreciate it. No one on the team has much experience with Flow.. babel-preset-env is a good idea, but I'm not sure it's mature enough yet. Also the main reason we recently switched to eslint-plugin-es2015-node4 (https://github.com/avajs/ava/commit/5158ac8bb0d0a79acde12c1bf6dfc1bbf32fd71e) by default was to get rid of lots of heavy Babel plugins to reduce our install time. This plugin brings them back.. I would recommend opening an issue on babel-preset-env about doing something about all the dependencies. Maybe do a lightweight version of it that only targets Node.js 4 and higher.. If you just want to know the time of a test, put console.time() in the beginning and console.timeEnd() at the end. If you want to compare test run time between Mocha and AVA, only the total test run time is useful as AVA runs tests concurrently.. \ud83d\udc4d Makes sense to me.\n@leebyron Any opinion?. This looks good now, but needs a test.. Closing for lack of activity.. Sounds like an easy win. I don't like how big its dependency tree is though. It brings with it 2.5MB of mostly unused dependencies.... Hm, weird. This is what I'm getting with your exact code and environment:\n```\n1 failed\nfoo\n  Missing expected exception..\n    Test.t [as fn] (test.js:4:4)\n``. Yes, I've confirmed it's an issue with the magic-assert PR.. Thank you @jarlehansen. This is excellent \ud83d\udc4c\ud83d\ude04. You have to ensure you build the TypeScript code with source maps sonycis able to translate the offsets.. Most likely Babel, yeah.. Alright. I'm convinced too. What @forresst proposed above sounds good to me.. Right. It will work in the next release though, as AVA now transpiles helper files too. So I think we can just keep this issue open until the next release and not do anything.. New release is out: https://github.com/avajs/ava/releases/tag/v0.18.0 :). Confirmed. Seems we do not account for helpers defined in therequire` config.. Oh wow. Our French coverage is amazing thanks to you @forresst :). Yeah. We need a custom preset.. Not entirely clear what you're suggesting with \"short time\". Can you elaborate or show an example? How is showing what file changed useful?. There are some lint issues you need to fix. See Travis.. Some tests are failing: https://travis-ci.org/avajs/ava/jobs/193048227. No, only use syntax supported on Node.js 4.. @novemberborn I tried this PR on some of my projects, like Got, Pageres, etc, and seems to be working well.. @tylerjharden Yes, it removes the warning. It's good to go when it's done.. @novemberborn Don't you need to publish the extracted modules first and update package.json? Right now they reference Git commits.. It's approved. :shipit: :shipit: :shipit: :shipit: . \n. > why do you think ava --no-color should disable colors not coming from AVA itself?\nBecause that's how it would work normally. That's how I designed Chalk. AVA is special since we spawn child processes, but the expected behavior should be preserved, by passing down the flag. --no-color should apply to everything.. > We've previously argued against forwarding command line arguments to the workers though.\nYes, and we're not gonna start doing it with arbitrary flags, but I would say it's worth making an exception for this one as it matches expected outcome.. @ThomasBem When you push new changes, can you push them as additional commits instead of squashing? Makes it easier to see what changed. We squash on merge anyways.. @Qix- FORCE_COLOR is for users that can't pass flags. We control the child process, so we can pass flags. I don't really see the problem with that or any benefit in using FORCE_COLOR?. > The parent AVA process is passed --colors, etc. Then it sets the environment variable FORCE_COLOR for child processes to force them to match whatever color level the main (parent) process is running at.\nI still don't see how that's any easier. It's the same amount of work to pass down --color/--no-color based on those flags being set for AVA, as it's passing FORCE_COLOR=0/FORCE_COLOR=1 based on AVA receiving --color/--no-color. The benefit of passing the flags is that we still respect FORCE_COLOR being passed to AVA by the user.\n\nNo need to update flags in AVA in the future if supports-color changes how it handles flags. This is the perfect use case.\n\nI don't do what-ifs. We work with how it works now. In the future, the environment variable might be renamed, who knows.. Great work @ThomasBem! Another high-quality pull request :). AVA only transpiles your test files, not your source. If you want AVA to also handle source files, read this guide: https://github.com/avajs/ava/blob/b24495d5f495abee798589c7c0b91202f4207064/docs/recipes/babelrc.md#transpiling-sources. > Perhaps we could even remove \"statements\" output for t.true/t.false until it's more production ready and just show actual/expected.\n\ud83d\udc4d I think we should do that for now and work on improving the output before turning it on again.. I'm a big \ud83d\udc4d on requiring unique test titles. I think that's a great change.\nI would also be ok with requiring a test title for multiple tests, but I kinda like the ability to not have a title when you only have one test though. I use this a lot in my tiny modules. Example. Although, I'm happy to be convinced otherwise.. > that seems like an unnecessary exception. More work on our side for little gain on the user's side.\n\ud83d\udc4d Many good arguments for why a test title makes sense here, so sure.. Note to self: We need to adjust the test-title ESLint rule accordingly.. This is because we use Babel, and it replaces globals with polyfills. I'm pretty sure this is fixed in master. Can you try it? \u2192 $ npm i avajs/ava.\nDon't know the context, but I would recommend against doing instance checking for Promise. Most promise libraries do not inherit from Promise (even Bluebird). The correct way of checking if something is a promise is: https://github.com/sindresorhus/p-is-promise/blob/4333abad2f82fb0b33fbdd962ec25d7e3f0e0672/index.js#L5-L8. That's good to know. I'll keep this issue open until we've done a new release (Hopefully soon).. New release is out: https://github.com/avajs/ava/releases/tag/v0.18.0 :). Exactly how you would transpile with Babel normally, just that you instead point to the transpiled files instead of the source files in your test file:\nimport app from './source/app'; \u2192 import app from './transpiled/app';.\nThis is an issue tracker, so I would ask you to rather use our support channels: https://github.com/avajs/ava#support :). Do you have any ava config in package.json, like babel-register?. Is this Webpack v1 or v2? I think it should be Webpack v2 and be specified as such.. Note that I added a commit to this PR: https://github.com/avajs/ava/pull/1212/commits/4fe61974af3c4b074c084053ac199034f6c46197. You also need to add this to the recipes list: https://github.com/avajs/ava#recipes. ping @danny-andrews . > Is this Webpack v1 or v2? I think it should be Webpack v2 and be specified as such.\n@danny-andrews \u2b06\ufe0f. @TheLarkInn Could you take a look?. @danny-andrews Good stuff. Thanks :). @danny-andrews Good stuff. Thanks :). There are more places that needs updating (Use Ack or The Silver Searcher):\n``\n~/dev/ava master\n\u276f ag '\\--source'\ndocs/recipes/watch-mode.md\n70:You can configure patterns for the source files in the [avasection of yourpackage.json] file, using thesourcekey. This is the recommended way, though you could also use the [--sourceCLI flag].\n74:If your tests write to disk they may trigger the watcher to rerun your tests. If this occurs you will need to use the--sourceflag.\n116:[--source` CLI flag]: https://github.com/avajs/ava#cli\nreadme.md\n164:    --source, -S            Pattern to match source files so tests can be re-run (Can be repeated)\ntest/cli.js\n241:    const child = execCli(['--verbose', '--watch', '--source=source.js', 'test-*.js'], {dirname: 'fixture/watcher/with-dependencies'}, err => {\n```. Nice work @LasaleFamine. Thanks for contributing :). Can you elaborate on why you need that? What OS are you using?. > I'd like to set pollInterval to 1000ms.\nI'm looking for the reason for why you need this?. \ud83c\udf89. I added a couple of todo items to the PR description.. I just realized the benefit of doing like Jest is with a JS snapshot file is that they can have template literals with multiple line, so they can have good git diffing when updating snapshots.. > Is the only issue one where a test file is removed but we don't clean up the snapshot file?\nNot just removing a test file. Imagine the user removes a single test. There will now be a stale entry in the snapshot file. I would expect AVA to remove the stale test entry in the snapshot when I run --update-snapshots.\nThis is made harder by t.skip()/.only() though. Here's my discussion with Vadim about it. I'm too lazy to summarize.\n\n\n\nAnd does Jest handle this?\n\nYes. \ud83d\udc4d Good idea! A pull request would be lovely :). > > I'd just go for a set of accepted types to be pretty-formatted in the \"statements\" output and push a fix. We can always make it more complex later ;)\n\n\ud83d\udc4d\n\n\ud83d\udc4d . > Why should it actually? Isn't it more useful if you get the Promise's value, so that you can make more assertions on it? I don't see the value in having it always return undefined.\nI think that would be surprising and inconsistent. This is assertion about errors, not the value. Also, you already have the promise, so what's really the benefit of having its returned. Promises are only resolved once anyways, so you can just do await promise and get the already resolved value without any wait.. I would rather say:\njs\nconst prom = getProm();\nawait t.notThrows(prom);\nt.is(typeof (await prom), 'string');\nvs\njs\nconst val = await t.notThrows(getProm());\nt.is(typeof val, 'string');\nI think the former, while a tiny bit more verbose, is much clearer.. Approximately how many files/directories and how deep? Do you specify any arguments to $ ava or any AVA config in package.json?\nCould you try removing https://github.com/avajs/ava/blob/b886c5b25ae54430f908c49f9932d8f998c41702/lib/ava-files.js#L141-L143 and see if it helps?\nCould you try to narrow down to which patterns are causing this? \nI find it strange that just globbing for files would cause this. AVA must be reading in too much data somewhere.. > The folder is already in .gitignore. Would it make sense to automatically exclude those in .gitignore in addition to node_modules?\nYes, that should be done regardless. Could you open a separate issue for that?. Just to add some description, this issue is about ignoring files matching glob patterns defined in .gitignore (not ignoring the .gitignore file itself). Ideally, the functionality would be added to globby as an option so it could benefit many other projects, but we would also consider adding it here if it's too hard to generalize.. Looks great! Thank you @LasaleFamine :). // @leebyron . See: https://github.com/avajs/ava/issues/222. I think we should just show the difference and not try to invent a new meaning like \"how to fix the problem\". To make it clearer, we could do what Mocha does and clearly label what the symbols mean:\n\n. Are you using latest AVA? I thought this was fixed in https://github.com/avajs/ava/commit/0603edcbfbcee4cca7cf3a150d8899ef80b66840.. Hah, I noticed that and thought it was a bug in the latest version of node-tap.. Let's merge this now. The slowness is painful. We can improve it further if @leebyron has any additional suggestions.. I can confirm it correctly crashes for me too when following the above steps.. > Seems like supports-color reports false on Travis anyway.... do you have a hint how to solve this? I'm already mocking process.stdout.isTTY\nIt's not enough with just process.stdout.isTTY. You also have to indicate color support, see the relevant detection code: https://github.com/chalk/supports-color/blob/8400d98ade32b2adffd50902c06d9e725a5c6588/index.js You could for example set process.env.TERM = 'xterm256';. AVA 0.18.2:\n```js\nimport chalk from 'chalk';\nimport test from 'ava';\ntest(t => {\n    t.true(chalk.enabled);\n});\n```\n1 passed\nSeems to already work fine?. Sorry about the super slow reply. I agree with the second option, good choice. Thank you for looking into this and your extensive detective work :). Please see https://github.com/avajs/ava/pull/1243#discussion_r100118036.. > ```js\n\n// After\ntest('my passing test', (t, ctx) => { // context == ctx, sut would also be a convenient parameter >name.\n   const data = ctx.desiredData;\n});\n```\n\nThis is not possible as it would conflict with test macros.. I you only need one thing in the context, you can assign it directly to t.context:\n```js\ntest.beforeEach(t => {\n    t.context = () => {\n        // do something\n    };\n});\ntest('my passing test', t => {\n    t.context();\n});\n```\nIf you need multiple things, you can take advantage of destructuring:\njs\ntest('my passing test', t => {\n    const {data1, data2, data3} = t.context;\n});. @7373Lacym The issue is about ignoring files matching glob patterns defined in .gitignore, not ignoring the .gitignore file. Should have been clearer. I've added some more text to the issue now: https://github.com/avajs/ava/issues/1229. I think inline in the code is a good place to document this. We also have https://github.com/avajs/ava/blob/master/maintaining.md for internal notes.. I don't really see how that is any more readable than:\njs\ntest(t => {\n    t.deepEqual([1, 2], [1, 2]);\n});\nIt actually get's very verbose when you use multiple assertions, which is common:\njs\ntest(({ deepEqual, is, throws }) => {\n    deepEqual([1, 2], [1, 2]);\n    is(1, 1);\n    throws(() => foo());\n});\nLet's see what the other team members think.\nMaybe you can elaborate on why you want it this way.. It's very easy to implement. We already bind t.end(). The problem is that if we support it, people will expect it to be documented, and I'm not sure it's a pattern we want to recommend.. @vadimdemedes Regardless, I don't think magic assert be shown on non-assertion errors.. Pretty sure all of them are err.name === 'AssertionError. You could maybe check for actual, expected properties too.. > I suppose we could open this as a feature request?\n\ud83d\udc4d . \ud83d\udec0\ud83d\ude0e. Good list of TAP reporters here: https://github.com/sindresorhus/awesome-tap#javascript\nI've created a request for a good HTML reporter for TAP: https://github.com/sindresorhus/module-requests/issues/84. Generally looks good. Will give it a more thorough read later today. I've ran it on a bunch of projects of mine and the performance seems to be the same as master, which is good.. Sorry about the delay. I did a thorough review now and nothing more to point out. Excellent work as always Mark.. > I'm not sure if tools for that already exist, @sindresorhus?\nNot that I'm aware of.. Excellent :). @caesarsol Not necessarily the commits directly, but a way to achieve the same results in a way that the maintainers of pretty-format accepts. Probably an option, but I haven't really looked into what will be needed. And then use the upstream pretty-format.. \ud83d\udc4d . @novemberborn Even if that turns out to be the case, we should at least try to upstream as much as possible so it's easier to keep the fork current.. > Long story short, this means errors have the expected shape in the reporters and we don't have to keep checking if certain properties are strings etc. @avajs/core thoughts?\n\ud83d\udc4d . Woot! Very good work @jakwuh :). As a user, I would expect them to not run. Same with beforeEach and afterEach.. 1. You can already do this with: --match=\"!*something*\"\n2. You can do this with globbing: ava **/*foo*.js. @shofel Can you fix the merge conflict? Do you know anyone in the Xstream/Most community we could mention here? I don't really have that much experience with Observables.. Nice work @yatharthk \ud83d\udc4c. This should be documented somewhere, probably here: https://github.com/avajs/ava/blob/master/docs/recipes/typescript.md. This should be documented somewhere, probably here: https://github.com/avajs/ava/blob/master/docs/recipes/typescript.md. Thank you @mmkal. This is a great improvement! :). @mmkal No, releases are manually published. Was planning to do a new one next week (It's a lot of work to manually test and write release notes).. Looks good :)\nI wish TAP had some kind of testsuite we could run our implementation at to check for conformance.. Looks good :)\nI wish TAP had some kind of testsuite we could run our implementation at to check for conformance.. \ud83c\udf89\ud83c\udf89\ud83c\udf89. \ud83d\udc4c\u2728\ud83c\udf08. \ud83d\udc4c\u2728\ud83c\udf08. You're doing async work in a sync test. Per the documentation, if you want to do async work, you either need to return a promise from the test, use an async function (recommended) or generator function as the test body, or use test.cb().. You're doing async work in a sync test. Per the documentation, if you want to do async work, you either need to return a promise from the test, use an async function (recommended) or generator function as the test body, or use test.cb().. By the time you call t.fail() AVA has already considered the test function done, since all the synchronous code has run.. By the time you call t.fail() AVA has already considered the test function done, since all the synchronous code has run.. Thank you :). Don't change the indentation and describe what you fixed and why.. > Is it preferred to use tabs?\nYes, that's what we use.. \ud83d\ude4c Great as always.. I think the regex could still be looser. I decided to extract it into a separate module as it can be useful for other projects too: https://github.com/sindresorhus/extract-stack\n@neoeno Can you update to use that module and remove lib/extract-stack.js and its tests?. > t.throws() and t.notThrows(), when passed a promise or observable, no longer reject the returned promise with the AsssertionError. Instead the promise is fulfilled with undefined. This behavior is consistent with when a function is passed, and with other assertions which also do not throw when they fail.\nI don't get it.\njs\ntest(async t => {\n    const err = await t.throws(Promise.reject(new Error('foo')));\n    console.log(err.message);\n});\nThe console.log here will never be reached as t.throws throws:\njs\ntest(async t => {\n    const err = await t.throws(Promise.resolve(new Error('foo')));\n    console.log(err.message);\n});\nWhat will change?. I see. I use the pattern of assigning t.throws to err a lot, so having to guard err would be inconvenient. Maybe we should hold on off this until we've done https://github.com/avajs/ava/issues/1047, so users doesn't have to change their code twice?. > Concurrent, Sequence and Test have been refactored and are (hopefully) easier to understand.\nSignificantly more readable, indeed.. Mark, you're on \ud83d\udd25!. Got it. My problem with this change is that it breaks a popular pattern that we also document:\njs\ntest('async', async t => {\n  const err = await t.throws(Promise.resolve());\n  t.is(err.message, 'foo');\n})\nWill result in:\n[TypeError: Cannot read property 'message' of undefined]\nWhich is not very helpful for the user.\n\nDo you remember why and when we stopped throwing on assertion failures? Seems like that would have resolved all of this, as no code would run after a failed assertion. What is even the benefit of assertions not throwing?\nAnother workaround would be to just return an empty object until we implement https://github.com/avajs/ava/issues/1047. I just don't want to force the user to change their code twice.. @novemberborn Amazing. I'm good with it now. :shipit:  :shipit:  :shipit: . This looks very good. I almost couldn't tell the difference with our diff. Usable now is better than perfect at some point.. I'm also concerned about the performance implications of this.\n\n\nfiles patterns that match files cause those files to be treated as tests, not helpers\n\n@novemberborn How is this different from how it already works? Or do you mean the ability to do $ ava _foo.js and have it run as a test instead of being a helper?. Inheriting comes with some downsides though. We would not be able to consume or transform the output at all.\nAnother solution would be to add an option to not spawn AVA test files as child-processes. Pretty much what ava/profile.js does now. I also need this for testing Electron apps, as their tester needs to inject some globals. Having such option would also fix our React/etc performance issues. Thoughts?\n\nRelevant: https://github.com/sindresorhus/np/issues/136. > But we don't do either of those things for either the tap or verbose reporters.\nRight now we don't, but we might need it to be able to place console output together with the correct test when running concurrently, although there might be other workarounds. Another issue is that it will create different environments for the verbose and default reporter. So a test might fail with the default reporter, but pass with the verbose reporter.\n\nWe should make that an option regardless. Not sure it would improve performance all that much.\n\nI'm pretty confident it will, but we won't know for sure until we try.. > Another solution would be to add an option to not spawn AVA test files as child-processes. Pretty much what ava/profile.js does now. I also need this for testing Electron apps, as their tester needs to inject some globals. Having such option would also fix our React/etc performance issues. Thoughts?\n@novemberborn Any thoughts on this? I would really like to see this happen.. > I can see that, though it would have to be a global option (apply to all files). I think we'd need more refactoring to make this work, and perhaps run the test files in a new script context rather than directly in the global.\nYes, should be a global option and I was also thinking of it using the vm module.. @jamestalmage Can you open a separate proposal (issue) about it? Another solution could be to have config overrides like in XO.. I split the \"same process\" discussion into a new issue: https://github.com/avajs/ava/issues/1332. \ud83d\udc4d \nI've opened an issue about better integration tests: https://github.com/avajs/ava/issues/1324. Answered in https://twitter.com/sindresorhus/status/846418632989028353. For further questions, use Stack Overflow or our Gitter chat.. I'm tempted to say we require t.throws() to be awaited. I (and many users I've talked to) find it surprising that t.throws() works with a Promise in a \"sync\" test. Would be better to have a clear separation and only allow t.throws() with a promise in an async test (test(async t =>{})).. I already always do:\njs\ntest(async t => {\n  await t.throws(\u2026);\n});\nInstead of:\njs\ntest(t => {\n t.throws(\u2026);\n});\nThe former is clearer by being explicitly async. It's easier to add stuff after the throws assertion and get expected order of execution. You can get the error without having to change things around. Simpler to document.. Why do you have to press r a lot? That sounds like a bug is misconfiguration.\nWhat kind of verbose information are you missing? As far as I know we show all the information we have.\nAlso try the latest AVA version to see if you can still reproduce your issues.. > The verbose reporter would have a duplicate entry for the test but perhaps that's OK.\nThat's ok.. > because require('vm') doesn't spawn a separate process (...or does it?)\nIt does not. That's the whole point of this issue, to not spawn.. > perhaps we should change the default value of the color option if we can detect colors are not supported?\n\ud83d\udc4d . Can we detect Promise/Observable being passed to t.throws/t.notThrows when using sync test test(t => {}); and disallow it?. But can't we throw when the test returns then? We could have t.throws add a flag to the test when used with a promise, then right after the test implementation is run, in the same tick, check the flag. If a test finishes in the same tick and doesn't return a promise, it's synchronous. This is not essential for this PR though. I just assume it will be a common issue.. This is actually already caught by another check (nice!), but would be useful to have a better message for it.\n```js\nimport test from 'ava';\nconst foo = () => Promise.resolve('foo');\ntest(t => {\n    t.throws(foo());\n});\n```\n```\n  1 failed\n[anonymous]\nTest finished, but an assertion is still pending\n```. Yay. I'm very happy about this change.. @novemberborn We could if it's not too hard. I would also be good with just failing the test saying something like:\n\nWe detected t.throws or t.notThrows being used with a promise/observable, but the test is synchronous. Either use an async function or return a promise.\n\nThat's at least clearer than:\nTest finished, but an assertion is still pending. Yes. This should be fixed by https://github.com/avajs/ava/commit/e45695197c8ba5d0ec7df7a7fe38f00f9a5f5fb0.. @medikoo The import/export syntax: https://github.com/avajs/babel-preset-stage-4/blob/master/plugins/8.json We also use Babel in AVA itself to detect incorrect usage of t.throws(). And there will soon be more: https://github.com/avajs/babel-preset-stage-4/issues/7 JS is constantly evolving and there will always be features not even available in the latest engines.. // @leebyron . This looks very promising!\n\n\nPerhaps controversially, Kathryn serializes to a binary format.\n\nCan you elaborate on why this is needed? I can see the benefit of having two files, as we can make the readable one even more readable. I'm just curious why binary.. Just some nitpick:\nObject {\n        foo: Object {\n    -     bar: Date 2017-04-19T06:58:10.047Z,\n    +     bar: Date 2017-04-19T06:58:30.166Z,\n        },\n      }\nSince we control the output now. I don't like the trailing commas. And I think we should drop the type for Object and Array as their type is already known with {} and []. I also think we could make the date output nicer: 2017-04-19T06:58:30.166Z => 2017-04-19 06:58:30 166ms Z.\n\nI think the readable snapshot should be the main one, so test.js.readable.snap => test.js.snap and we could do another one: test.js.binary.snap for the binary one.. Can you document the binary format? Why not use something like Protocol Buffers?\nMy biggest concerns with a binary format is debuggability and it not being diffable in git, so each snapshot update will take the whole size of the snapshot. This can have big impact of projects with lots of large snapshots.. > I can look into that. There isn't a lot to the current encoding though (at least in AVA).\nWith Protocol Buffers we don't really have to care about the binary part. We just define the schema and automatically get a encoder/decoder. Instead of how we have lots of custom code to encode/decode now.\n\nWe could make an exception for true Objects and Arrays but I'm not convinced it's worthwhile.\n\nTrue Objects and Arrays are the most common output, and simplifying that simplifies the 95%. I think it's very much worth it.\n\nI don't think there is much value in the serialization being readable. Even if it's JSON it wouldn't be pretty formatted, and a single line diff is just as useless as a binary diff. If we do pretty format it would tempt people to make changes, and that's likely to break the snapshot. With the binary format we discourage all that and we get to use compression. I think that strikes the right balance.\n\nGood point. I'm warming up to the idea.\n\nFor the last property you mean? It'd be a bit more work to track which item / property / map entry is the last one, flag it, and then prevent the comma. Though it would be possible.\n\nYes. That's how JS is usually presented, like with util.inspect(). It's also how most JS is written. Having a trailing comma is distracting and makes the output more noisy. Same reason I'd like the Object/Array type names gone.. > Lastly there is a commit that switches to protocol buffers.\nHow do you like it? Do you think it's worth using or do you prefer the custom binary handling?\n\nIf we go for this, I think we should just publish vendor/protobufjs/minimal.js as a module instead of vendoring.. > What do you think, given the diff?\nI'm gonna say it's up to you. I'm slightly in favor of Protocol Buffers, but it does add some overhead in tooling and I'm not seeing as much use for it as I had hoped, and you're right that the manual binary handling is not that advanced. For example, I would have thought that Protocol Buffers would handle the whole binary thing, so I'm curious why you're adding a header and version manually: https://github.com/avajs/ava/blob/1695c449960d6d3f86bd385754b0afcd46333696/lib/snapshot.js#L91-L92 ? Manual binary handling have the downside of boilerplate code and high chance of off-by-one errors.. > On the other hand it seems strange that AVA would actually write files without being told to do so.\nIt is being told so though, kinda. The user is explicitly writing a t.snapshot() assertion. I don't think it's very user-friendly to require a --update-snapshot every time the user creates a new t.snapshot(). That command is meant only for updating existing snapshots.. I tried latest now with my existing snapshot and got:\n```\n  [anonymous]\n  /Users/sindresorhus/dev/private/ava-playground/test.js:4\n3: test(t => {\n   4:   t.snapshot({foo: {\n   5:     bar: new Date()\nError thrown in test\nError:\nError {\n    message: 'Snapshot version is v7937, can only handle v1',\n  }\n```\nAh never mind, probably because the protocol buffer change.\n\nSidenote: I also think the error output could be better here. We're saying Error 3 times. Ideally it would be:\nError thrown in test:\nSnapshot version is v7937, can only handle v1. I think Contains 1 snapshot from 1 test. See test.js.snap for the actual snapshot. should be one separate lines so it will diff better. Only the first part is dynamic.. See `test.js.snap` for the actual snapshot.\nYou can't really see the snapshot in that file, so I would rather say:\n\nThe actual snapshot is in test.js.snap.\n\nOr something similar.. I like the idea of using Markdown for the readable version.\nThis is how I would design the report:\n```\nSnapshot report for test.js\nContains 1 snapshot from 1 test.\nThe actual snapshot is saved in test.js.snap.\nGenerated by AVA. \nTest title\nSnapshot 1\n```\nObject {\n  foo: Object {\n    bar: Date 2017-04-19T16:11:13.136Z {},\n  },\n}\n```\n```\nIgnore the \\ of course.. > The header is so that people can see what generated the file. The version so that eventually, older AVA versions can detect a newer snapshot and not even try to decode it. Currently it's compressed and then within that there's the encoded index. If we change the compression then older AVA versions would just crash. If we change how the version is encoded inside the decompressed binary blob (easy to accidentally do with protobufs) then again older AVA versions would crash. Hence leaving it outside, which makes it easier to guarantee we never (accidentally) change it.\nGood points. Thanks for elaborating.\n\nAlso, a big use case for protobufs is when you need to share data between different programs. You can write a definition once and then generate parsers / generators in different languages. Here it's just AVA reading its own output.\n\nBeen thinking about this some more and I think you're right. Not enough benefit of using Protocol Buffers, since we are the only writer and consumer, and we only really need a couple of fields. Let's go with your custom implementation. In the future, I would like to see this extracted out into a reusable module, but I think it's easier to have it mature here for now.. > Based on this tweet added t.snapshot(expected, {id: 'my snapshot'}) which creates a snapshot that is identified by my snapshot, rather than the test title and t.snapshot() invocation count. This also means you can compare against the same snapshot in different tests in the same file\nGood idea!. Latest changes looks and works great.. Yay! I'm super excited about this. \ud83d\ude4c\n. Sorry for being unclear, by message, I meant explanation, not the message argument.\n\nI think what you're saying is that we should always print something like \"Expected promise to be rejected, but it was resolved instead.\"\n\nI would also include the value for completeness:\n\nExpected promise to be rejected, but it was resolved with \"foo\" instead.. I'm a little bit sceptical of differentiating between -0 and +0, but I don't think it happens that often in real code anyways, and a testing framework should in general be very strict. Let's try it out and we can always revert if it's a big annoyance to users.. Thank you @alexrussell :). @novemberborn I would just add safe-buffer. I agree it's dumb, but not worth wasting time on issues about this.. Looks good. I did some minor cleanup.\n\nThis needs to be added to the Recipes section in the main readme: https://github.com/avajs/ava#recipes. ping @CImrie :). npm doesn't preserve indentation, so wouldn't help much. Most people use npm install --save which makes npm modify the package.json. I would be willing to add this if npm added supported for preserving the existing indentation, but not worth the overhead when it doesn't.. Ah, I didn't know that. I think it makes more sense to just preserve the existing indentation though, than specifically read .editorconfig. Could just get the indentation with detect-indent. Pull request welcome.. @yatharthk Good idea, let's add it to write-pkg.. For reference, npm@5 will also preserve the indentation, so this will be much needed soon :). > Would it not be better to add an auto option to write-json-file?\n\ud83d\udc4d . Same without Sinon actually:\n```js\nimport fs from 'fs';\nimport test from 'ava';\ntest(t => {\n    const _ = fs.readFileSync;\n    fs.readFileSync = () => 'foo';\n    const isFoo = require('.');\n    fs.readFileSync = _;\n    t.true(isFoo);\n});\n```\n```\n  2 exceptions\nUncaught Exception\n  SyntaxError: Unexpected token L in JSON at position 0\n    SyntaxError: Unexpected token L in JSON at position 0\n        JSON.parse ()\n        new SourceMapConsumer (node_modules/source-map/lib/source-map-consumer.js:17:22)\n        mapSourcePosition (node_modules/source-map-support/source-map-support.js:171:14)\n        wrapCallSite (node_modules/source-map-support/source-map-support.js:338:20)\n        node_modules/source-map-support/source-map-support.js:373:26\n        Function.prepareStackTrace (node_modules/source-map-support/source-map-support.js:372:24)\n\u2716 Test results were not received from test.js\n``. @OmgImAlexis Not necessarily. Doesn't look like that error is coming from AVA or a dependent.. Thanks for doing the recipe. \ud83d\ude4c. Duplicate of https://github.com/avajs/ava/issues/1047. I agree it's currently sub-optimal. You should be able to assert the different specifics of an error in one assertion. See my proposal https://github.com/avajs/ava/issues/1047#issuecomment-261483195 and add your thoughts there.. With https://github.com/avajs/ava/pull/1341, we'll usesnapshotsonly if you usetests, otherwisesnapshots`.. Simpler test case:\n```js\nimport test from 'ava';\ntest(async t => {\n    await t.throws(async () => {\n        throw new Error();\n    });\n});\n```. @sdd No it's the same as:\n```js\nimport test from 'ava';\nconst promise = () => Promise.reject(new TypeError('\ud83e\udd84'));\ntest('rejects', async t => {\n    const error = await t.throws(promise);\n    t.is(error.message, '\ud83e\udd84');\n});\n```\nNote the arrow function.. > That said I think we should extend t.throws() and t.notThrows() so that if the function returns a promise without throwing, the assertion is applied to that promise instead.\n\ud83d\udc4d . Looks good. Thank you @yatharthk :). @novemberborn Thanks for the tips. Should be good now.. > (Possibly we could consider changing the value to always be in seconds?)\n\ud83d\udc4d . @revelt Why?. No, I mean, why not just finish it? You only had a couple of super minor changes left. I'm curious, as I often see people just not finishing almost done PRs and I still don't understand why.. > It was people winning against people, not a family winning together against imaginary \"Monopoly\".\nBut open-source is \"a family winning together\"! \ud83e\udd17\n\nAs opposed to formal \"requests\" and rubbing in to original poster's ego\n\nI find it \ud83d\ude14 that you consider a PR review \"rubbing it in\". Rather look at it as an opportunity to learn or see other ways to solve problems. Your PR was actually correct, it was just not according to our preferences and required some minor tweaks, which you couldn't have known, and it's as expected. Very few, or close to no PRs are merged without changes.\nI could have done the changes myself of course. It would have taken much less time. But I value pull request reviews, both as a submitter and as a reviewer. I often learn things as a reviewer on PRs. I suggest some change and the submitter elaborates on why the existing way is better. I don't always request the best changes, but by requesting something, I start a conversation about it.\nI would recommend leaving out \"ego\" and \"emotions\" when doing open-source. Yes, I know it's hard.. > seen this mistake repeated in too many repos\nIt's not a mistake. With your change, we're now relying on where the cli.js file is placed in the package. That could change. Although not likely since we control it.\nWindows supports executing without extension as long as the extension is in %PATHEXT%, which cmd is, so this sounds more like a WebStorm issue.. See: https://youtrack.jetbrains.com/issue/WI-18109\n// @develar. \u2764\ufe0f . We already do the first thing, but I agree, the rest would be very useful too.. Is there an open VSCode issue about the buggy breakpoints?. Agreed. This is a duplicate of https://github.com/avajs/ava/issues/1207 though.\nI would recommend using our ESLint plugin, which has a rule for this already: https://github.com/avajs/eslint-plugin-ava/blob/master/docs/rules/no-identical-title.md. > IIRC I wasn't a fan of passing --color, but you said it was convention ;-)\nI know. It is convention though, but turns out to be annoying in this context.\n\nI'm not sure process.argv should be relied upon inside tests.\n\nOh it shouldn't, but I don't think we should clutter it either.\n\nHow about we pass whether to use colors or not in the argv[2] JSON string and handle it accordingly before requiring the test file?. > If you mean that for use cases under our control we use options.color, yes.\nYes, I mean our auto-detection.\n\nIt could be argued though that if the user explicitly passes --color and --no-color flags, due to the convention, we should still forward those.\n\nAgreed.. @kevva Yeah, basically, we want to pass on the auto-detect color support to the child worker, which always detects it as no color support, without clobbering process.argv for the test file context.. https://github.com/sindresorhus/float-equal. This can be solved by modules or external assertions. Maybe Chai has something for this.. > Should we splice out the options?\n\ud83d\udc4d \n\nI took #1393 (comment) to read us having consensus on only forwarding --color and --no-color. Previously we've argued that users should use environment variables to pass runtime values to their tests. These flags don't sit well with that though.\n\nYes, we should forward them if they're specified by the user. It's an exception since the flags apply to both us and other modules. ava --color -- --color would be super weird. I do not want that.\n\nOTOH that would make it possible to do ava --color -- --no-color.\n\nI don't see why you would want to do that. Colors is a binary thing. Either you want them or you don't. I don't think we should complicate it more.. @kevva You need to handle the color option in the worker, by setting the Chalk singleton .enabled property to the same value as the .enabled property of Chalk in the CLI process. So the automatic color support is brought into the child worker, which defaults to no colors always because it's not a TTY.. We have a recipe for using AVA with TypeScript: https://github.com/avajs/ava/blob/master/docs/recipes/typescript.md\nQuestions like these are better asked on Stack Overflow :). Object spread is still just a proposal: https://github.com/tc39/proposal-object-rest-spread. @niftylettuce Yes, see: https://github.com/avajs/babel-preset-stage-4/issues/7 Help welcome.. I think we're trying to solve this problem in the wrong place. Better to fix it in update-notifier so it's fixed for everyone. See: https://github.com/yeoman/update-notifier/issues/112. @tdeschryver Go ahead \ud83d\ude03. Thanks @tdeschryver :). Thank you @tdeschryver :). https://github.com/sindresorhus/xo/pull/228#issuecomment-307655803. The feature makes sense to me :) @avajs/core Thoughts?\nIt also needs to be documented.. > happy with the regexes?\n\ud83d\udc4d . See https://github.com/istanbuljs/nyc#including-files. Sounds like a lot of headache for very little gain.. Just make it slower for AppVeyor. AppVeyor is notoriously slow.. @novemberborn Anything else? Travis is failing on FRESH_DEPS=true, but so is master, so I don't think that's relevant.. Use the npm issue tracker for npm issues.. Can you check if any other test runner like Mocha, Jest, QUnit, etc, sets this? Any prior art?. We can keep it open to see what the rest of the team and anyone else thinks.. > but I wonder if the default value ('test') should be configurable in the package.json#ava object.\nNeh, not until someone asks for it and provides a good use-case I would say.\n\n@sindresorhus what's your preference on this?\n\nI'm \ud83d\udc4d  as long as we have a warning in the docs about its dangers. You should not use it extensively in your code, as you're then no longer testing the actual code paths, but rather the ones made for the tests.. I was first thinking a diff:\n```\n  Threw unexpected exception:\n\nRangeError {\nTypeError {\n    message: 'Uh oh',\n  }\n```\n\nBut I think your suggestion would be better.. You're right. We need to document this better.\nIn short:\n\nfiles - Your test files. You can define them there instead of doing $ ava file1.js file2.js ...\nsource - Source (non-test) files not detected automatically in watch mode. See https://github.com/avajs/ava/blob/master/docs/recipes/watch-mode.md#source-files-and-test-files\nmatch - Not really useful in the package.json config, but can be useful on the command-line to filter down tests by test title. Let's say you only want to run the tests which names start with uni, you could do $ ava --match='uni*'.. Not really related to AVA, but you need to put ESLint in your test script. In package.json:\n\njson\n\"scripts\": {\n    \"test\": \"eslint . && ava\"\n},\nNow it will lint when you run npm test and in your CI.\nIf you need to run it on git push, see https://github.com/typicode/husky. I guess it's caused by https://github.com/sindresorhus/cli-truncate/commit/f379a0ea694135dd58e437f8131102af8dd20c0f. > Not sure if we should explicitly bump any dependencies?\nNah. You need to upgrade npm. Your npm version is ancient.. This is also the behavior of the built-in assert:\n```js\nconst assert = require('assert');\nclass Foo {}\nit('foo', () => {\n    const f = new Foo();\n    f.foo = 10;\n    assert.deepStrictEqual(f, {foo: 10});\n});\n```\n```\n  1) sd\n0 passing (10ms)\n  1 failing\n1)  foo:\n  AssertionError [ERR_ASSERTION]: Foo { foo: 10 } deepStrictEqual { foo: 10 }\n  + expected - actual\n\n```\n(With Mocha)\nWhat you seem to want is something like t.deepLooseEqual() and that we don't currently support.. @novemberborn What do you think about having a loose structural matcher? Like node-tap's t.match(): http://www.node-tap.org/asserts/. > Does match detect extra properties?\nNo. You're right. Must have been a copy-paste mistake. Thanks :). https://stackoverflow.com/questions/tagged/ava or https://gitter.im/avajs/ava. > Give that we do it for process.stdout it seems like a bug that we don't for process.stderr.\nYeah, probably just an oversight.\n\nCan process.stdout.isTTY be false while process.stderr.isTTY is true?\n\nYes, users could redirect individual streams. For example, to only redirect stderr to a file:\nava 2> file\n\nCan they have different column and row values?\n\nNo, I don't think so.. @unional I don't think I understand your question. The use-case doesn't matter. It's expected behavior of a process to handle non-TTY mode.. Yes, you're right. We need to get it added to both the TypeScript and Flow type definitions.. Great. Can you add a test to https://github.com/avajs/ava/tree/master/test/flow-types ? Should just showing usage of the different ways of using t.snapshot so we can make sure it works and doesn't regress.. This looks like a problem with npm. I've seen this many times now. Try upgrading to the latest npm and reinstall.. I initially wanted just u, but I no longer remember why we went with u + Enter.. // @nowells In case you would be interested in working on this. No worries if not though.. Upgrade npm. Older 5.x versions are super buggy and install incorrect sub-dependencies.. @fruch Please read the previous discussion about this: https://github.com/avajs/ava/pull/1038. Adding more options is rarely a good solution. Maybe we should just use __helpers__ for helpers if you use the naming convention __tests__ for tests, like we do with snapshots.. Would be cool if someone could submit a recipe for this: https://github.com/avajs/ava/tree/master/docs/recipes. That's not currently possible. Can you elaborate on your use-case?. Relevant Travis issue: https://github.com/travis-ci/travis-ci/issues/4696\nI think we should only cap it to 4 when process.env.CI exists. (Maybe even lower? CIs can be pretty slow) When not on a CI, we could cap it at maybe 6?. Alright, so let's cap it at 2 for CI and 8 for normal.\n@novemberborn Thoughts?. > but thought maybe that's too much for one line?\nIt looks fine. That's how I would have written it.. Can you also mention the capping behavior in the readme?. > Technically this should be threads not cores. Do you want me to change that?\nNo, it's CPU cores: \n\nThe os.cpus() method returns an array of objects containing information about each CPU/core installed. - https://nodejs.org/api/os.html#os_os_cpus\n. I see. Alright, update it to threads.. Node.js issue: https://github.com/nodejs/node/issues/16279. Let's go with logical cores per Node.js issue discussion and make logical cores link to https://superuser.com/questions/1105654/logical-vs-physical-cpu-performance. > but I don't understand why we'd need to cap this at all for non-CI?\n\nFor cases where Docker is used locally since it reports the incorrect count. And, from experience, anything more than 4-5 doesn't improve the performance. But yes, it's pretty arbitrary, so not sure whether it makes sense.. Ok, let's drop it. We can add it back if it's an actual proven problem.. \nIt shouldn't reprint the message when I press r+Enter.. Yeah, I'd like to see that too. Now that Node.js 8 has native async/await, there's no longer a need for me personally to transpile. The main reason we chose to do transpilation by default initially was to get async/await.. @avajs/core Thoughts?. > Say we have a \"babel': false option. Should that also imply \"powerAssert\": false?\nYes, unless we can figure out a way to run it efficiently at runtime on a failure.\n\nShould there be an easy way to opt back in to it?\n\nI don't see how you could opt into the throws helper again without having Babel, and then using babel: false is moot.. > Say we have a \"babel': false option. Should that also imply \"powerAssert\": false?\nYes, unless we can figure out a way to run it efficiently at runtime on a failure.\n\nShould there be an easy way to opt back in to it?\n\nI don't see how you could opt into the throws helper again without having Babel, and then using babel: false is moot.. You could place your hooks in a separate file that has a function that accepts the test instance and creates the hooks, and import and initiate that in each file.. WIth:\njs\nthrow 1;\nI would print:\nThrew non-error: 1\nYOU SHOULD NOT THROW NON-ERRORS. GO THINK ABOUT WHAT YOU DID.\nJokes aside, I'm serious about including the first sentence in some form. People should learn. The fact that it's even allowed in JS is very JS, but also very annoying.. Ok. Just thinking out loud, there must be a reason all other major test frameworks provide this built-in:\n\nhttps://mochajs.org/#test-level\nhttp://facebook.github.io/jest/docs/en/jest-object.html#jestsettimeouttimeout\n\nMaybe it would make more sense to have it built-in if it worked a little bit smarter, like our existing --timeout flag. For t.timeout(5000), instead of just naively timing out after 5 seconds, it would instead timeout 5 seconds after the last activity in the test. An activity could be defined as any method of t being called, like t.true or t.log.\nAnother benefit of t.timeout() is that it makes the intent clear. It clearly points out that this test should not take longer than 5 seconds. We could have a linter rule to enforce placing it at the top for readability reasons. With p-timeout you would have to use it with the library call itself, which might be in the middle of the test if it requires some other setup.\nWe could also have timed out tests retried once, just in case they were flaky.. @dflupu You need to submit your PR to IssueHunt for me to release the bounty: https://issuehunt.io/repos/26820798/issues/1565. Great work @codeslikejaggars \ud83d\udc4c\u2728\ud83c\udf89. > Right, but I'm thinking we'll detect when people have v7 installed and use that instead. Then a little while after v7 has come out and the wider ecosystem has had a chance to adjust we can switch to run v7 by default (and perhaps detect and use v6 when installed).\nSeems like a lot of extra work and complexity for little gain. Can't we just upgrade to Babel 7 in a new AVA version and people that can't upgrade to Babel 7 yet can just wait on upgrading AVA?. @unional Would https://github.com/avajs/ava/issues/1332 help you? If so, that one is \"PR welcome\". If not, can you elaborate on what you need instead of how? Maybe we can figure out a better way to achieve it together :). @unional Agreed. That's how it should work. Feel free to explore this.. Ah, you're right... I didn't realize Proxy would proxy all properties of an array. I was just thinking the elements. Should be fixed now: https://github.com/sindresorhus/negative-array/commit/7263ff43c93a0cc817410bb84ad37b312a777cbe\n\nand all Concordance can do is to throw an error when it thinks it's dealing with a list but length is not a number.\n\nThat would have been helpful.. Closing as this is my mistake.. I've been wanting this too, but should definitely be a new object.\n\nor we require the context that's assigned in test.before() to be an actual object and we do Object.create(context) to create a new reference for each test.\n\n\ud83d\udc4d . \ud83d\udc4d . Can you please elaborate? I don\u2019t fully understand what you\u2019re requesting. Also explain why you need it.. @motss Would you be able to submit a recipe?\nSee: https://github.com/avajs/ava/tree/master/docs/recipes. @irahulcse You mean how to contribute? See our comprehensive contribution guide: https://github.com/avajs/ava/blob/master/contributing.md. Thanks for starting this @motss. I've tweaked the text and simplified parts of it a little bit.\nCan you also add it to the AVA readme: https://github.com/avajs/ava#recipes. What do we do about this PR now that #1618 is merged?. I can reproduce this with Mocha too, so this would be better posted on the nyc issue tracker.. \ud83d\udc4d Makes sense. . > We'll support any major version that is supported by Node.js itself, see nodejs/Release#release-schedule1\nWe need to specify whether this means supporting the oldest Node.js maintenance LTS release or active LTS release.. You can get the test title from t.title, per docs: https://github.com/avajs/ava#t. AVA removes internal stack frames coming from Node.js, but it shouldn't remove any user frames. Can you try replacing this chunk in AVA with module.exports = stack => stack; and share what you get?. So that means that there are really no more stack frames captured.. Closing as I can reproduce this without AVA:\n```js\nconst puppeteer = require('puppeteer');\n(async t => {\n    const browser = await puppeteer.launch();\n    const page = await browser.newPage();\n    await page.goto(https://github.com/avajs/ava);\n    await page.waitForSelector(#this-doesnt-exist, {timeout: 1000});\n})().catch(console.error);\n```\n\u276f node index.js\nError: waiting failed: timeout 1000ms exceeded\n    at Timeout.WaitTask._timeoutTimer.setTimeout (/Users/sindresorhus/Downloads/t1/node_modules/puppeteer/lib/FrameManager.js:593:58)\n    at ontimeout (timers.js:478:11)\n    at tryOnTimeout (timers.js:302:5)\n    at Timer.listOnTimeout (timers.js:262:5)\nOpen an issue on Puppeteer about this instead. God jul :). \"Minimal and fast\" refers to the API. The main offender in our dependency tree is Babel, and it's hard to do anything about that. Maybe Babel 7 will improve the situation. We'll see when it's out.\nWe've done everything we possibly can to keep the size down. You could help out by making sure none of our dependencies include unnecessary files, and if they do, submit PRs to use the \"files\" property in package.json.. Duplicate of #369 . chokidar, used for our watch mode, also takes up 8 MB.\nWithout Babel, Chokidar, and Emcore (For power-assert), AVA would only take up 5 MB, and I'm confident I could reduce it to 4 MB.\nBut in the end, making a great developer experience is more important than some megabytes. Although I truly wish we could reduce it more.. I want remove the bundled Babel and just let users add it as a dependency if they need it instead, but that will require #1556.. Thanks :). Duplicate of #520. There's is an API, but it's not officially supported and might break at any time. const api = require('ava/api'). Example: https://github.com/avajs/ava/blob/c1faf95b9bbaa1981013e98576fc58a4d6217020/test/api.js#L34-L40\n  . The ESLint rules have saved me many many times. I do understand not everyone will or can use that though, so I'm open to doing something in AVA. It's a very common mistake to make.\nMaybe we could also have an option to fail if .only() is used in CI.\n@novemberborn Thoughts?. > Would folks want to log more than one argument in that case?\nYes, most are used to console.log where this is possible, and it's very useful too.. I agree, it's way overdue. There are things we really wanted to fix before 1.0.0, like #1047, but we just didn't have enough time. I think it makes sense to do it after the Babel 7 release (which will be out soon) We don't want to do 2.0.0 a couple of weeks after 1.0.0.. Uninstalling is done by npm, so this would be better opened there: https://github.com/npm/npm. Can you give the PR a proper title?. Thanks, but this is not needed. Semver handles this for us. Read more here: https://nodesource.com/blog/semver-tilde-and-caret You just need to reinstall AVA. The package-lock.json doesn't apply when you install AVA from npm.. @novemberborn What's the point of semver then, if we have to manually bump for each patch release of dependencies? Generally, you only manually bump major, and minor if you take advantage of a new feature, and let patch releases just bubble up. This is at least how I, and most people, have been practicing semver.. And what about all the other dependencies with semver ranges that are not forcing the latest patch release? We're probably missing out on a lot of fixes in those, by this logic.\nI'm not again this PR, I'm just genuinely curious about how we should handle things :). > What do you think about still running the tests in a separate worker process, but reusing that process?\nI think reusing could be useful for multi-process too, but this PR should focus on only the same process. The use-cases are things that need to be run in the same process as the AVA CLI, for example, debugging or Electron. So I'd rather have multi-process mode reuse processes and let this one be without any process forking. If users want reuse of a single process, they can just set concurrency to 1 in multi-process mode.. I don't know if it currently works or not, but we should support negating the ignore pattern to unignore default ignore patterns: \"files\": [\"!foo/node_modules/**\"]. Can't we just run it through util.format()?. For support questions, use https://gitter.im/avajs/ava instead.. > I can't quite figure out what those would be though.\nMe neither. We'll just have to see.. > I can't quite figure out what those would be though.\nMe neither. We'll just have to see.. \ud83d\udc4d. \ud83d\udc4d. Good to see this being cleaned up before the 1.0 release.. Can you open an issue on eslint-plugin-ava about making a rule for this?. profile.js was only meant as a stop-gap solution before we could introduce a better permanent solution, so I would go with the latter.. profile.js was only meant as a stop-gap solution before we could introduce a better permanent solution, so I would go with the latter.. @forresst Thanks for your excellent maintenance of the French translation \ud83d\ude4c. Inquirer can easily be mocked, which would be a better way to test it. Alternatively, spawn a separate file and check the output.. js\nconst beforeAfter = () => { \u2026 };\ntest.before(beforeAfter);\ntest.after(beforeAfter);. You have to use double-quotes on Windows. That's not really something to do with AVA, but rather how the command-line works on Windows.. @novemberborn I like it. It's simple and can enable a lot of interesting use-cases. \ud83d\udc4d from me.. I've looked through each commit and everything looks good \ud83d\udc4c. > See #1425 (comment) though.\nI agree. Way too much of an edge-case to care about.. Yay! I've wanted this many times.. Yay! I've wanted this many times.. > t.throws(fn, {of: SyntaxError}) // err instanceof SyntaxError\nI don't think of is clear enough. Maybe name it instanceOf or constructor?\n\nFrom the original issue:\n\nWe should also have much better validation logic to prevent mistakes. For example, using is-error-constructor on the constructor matcher.\n\nCould you do this?. > t.throws(fn, {of: SyntaxError}) // err instanceof SyntaxError\nI don't think of is clear enough. Maybe name it instanceOf or constructor?\n\nFrom the original issue:\n\nWe should also have much better validation logic to prevent mistakes. For example, using is-error-constructor on the constructor matcher.\n\nCould you do this?. What do you think about dropping support for t.throws(() => {}, TypeError);. It's not that commonly used and can now be done with t.throws(() => {}, {of: TypeError}); instead, which is only slightly longer. Just checking the type of the error is usually an anti-pattern, as it's too generic of a check.. What do you think about dropping support for t.throws(() => {}, TypeError);. It's not that commonly used and can now be done with t.throws(() => {}, {of: TypeError}); instead, which is only slightly longer. Just checking the type of the error is usually an anti-pattern, as it's too generic of a check.. > I like the brevity of of but I've also had some doubts about it. instanceOf seems most appropriate, though the capital O is cumbersome to type. constructor would work but it doesn't quite signal how we run the assertion. instanceOf then?\nI like the brevity too, but I feel we're gonna get the same complaints as we got with t.same(), which is now t.deepEqual(). Out of constructor and instanceOf, I would pick the latter, even though the Of is ugly and annoying to type.\n\nThat depends how tightly you want to couple your test to any current error message. E.g. when validating input type checks, looking for TypeError can be sufficient.\n\nBut then you don't know whether the TypeError is coming directly from your function or some other function you're calling. I'm fine with leaving it, just thought I would mention how it can cause subtle issues.\n\nSimilarly if you're testing an API with custom errors then often the error class is the relevant bit and the message may be static.\n\nThat's when I would use it, yeah, but it's very rare that I create custom errors.. > I like the brevity of of but I've also had some doubts about it. instanceOf seems most appropriate, though the capital O is cumbersome to type. constructor would work but it doesn't quite signal how we run the assertion. instanceOf then?\nI like the brevity too, but I feel we're gonna get the same complaints as we got with t.same(), which is now t.deepEqual(). Out of constructor and instanceOf, I would pick the latter, even though the Of is ugly and annoying to type.\n\nThat depends how tightly you want to couple your test to any current error message. E.g. when validating input type checks, looking for TypeError can be sufficient.\n\nBut then you don't know whether the TypeError is coming directly from your function or some other function you're calling. I'm fine with leaving it, just thought I would mention how it can cause subtle issues.\n\nSimilarly if you're testing an API with custom errors then often the error class is the relevant bit and the message may be static.\n\nThat's when I would use it, yeah, but it's very rare that I create custom errors.. > I'm not sure it's necessary. We always check that the value is an error, so if it's not then you'd never get to the instanceof check. is-error checks the string tag, which works across realms, whereas is-error-constructor does not, so that wouldn't be consistent either.\nGood point. I didn't think about that when commenting back then.. > I'm not sure it's necessary. We always check that the value is an error, so if it's not then you'd never get to the instanceof check. is-error checks the string tag, which works across realms, whereas is-error-constructor does not, so that wouldn't be consistent either.\nGood point. I didn't think about that when commenting back then.. I've been playing with this PR and seems to work perfectly.. I've been playing with this PR and seems to work perfectly.. \ud83d\udc4d to code, \ud83d\udc4eto errno.. \ud83d\udc4d to code, \ud83d\udc4eto errno.. Most likely related to jsdom (used by browser-env). See: https://github.com/jsdom/jsdom/blob/master/lib/old-api.md#external-resources You may have to allow remote image explicitly.\nAlso, next time please include links to any other places you have asked the question, or preferably only ask it in one place. https://stackoverflow.com/questions/48840541/ava-testing-image-loading-with-promise-promise-returned-by-test-never-resolved. I'm fine with this as long as we display what tests are blocking AVA from finishing on its own.. > And then we may have to let users configure that\u2026\nConfigure the time, or whether to exit right away or wait to drain?. For inspiration, here's how Jest solved it:\n\nhttps://facebook.github.io/jest/docs/en/cli.html#detectopenhandles. It was useful early on when callbacks were the main async thing and we didn't have as good assertion output as we have today. It doesn't make much sense today, so I agree we should remove it.. The changes so far look good to me. I've been running this PR (using ava -v) on lots of projects of mine and nothing has failed yet.. I've run this PR on a bunch of my projects and it seems to be working perfectly.. You have to use the latest beta to get that feature: https://github.com/avajs/ava/releases/tag/v1.0.0-beta.2\nThe readme documents master and not the latest stable version. Always check the docs on npm or the specific Git tag. This is how most projects on GitHub operate.. \ud83d\udc4d . @Seiyial Still interested in finishing this? :). Ping @Seiyial :). The PR needs a more descriptive title.. The PR needs a more descriptive title.. > Was there a particular reason to only let assertions take a single message argument?\nSimplicity. Having variadic parameters makes the assertions hard to read. We could consider supporting an array as an argument or having a t.format() method you pass to it.. Patterns prefixed with ! are ignore patterns in the files config. We should better document this though.\nhttps://github.com/avajs/ava#configuration. Can you give the PR a proper title that explains what it fixes?. @mfainshtein2 Still interested in finishing this? :). Closing for lack of response.. I think the benchmark could be useful if we have performance-related commits or PRs. I would prefer to just fix them.. The PR needs a more descriptive title.. > My guess is that the backticks are used for visual effect rather than semantics.\nYup. > It may be worth considering not printing non-AVA output unless --verbose is set. #1849 (comment)\n\ud83d\udc4d Seems fine. We have t.log() now for stuff that should actually be shown.. > Are you saying that if we drop interoperability with Flow v0.69 we can support .then(null, func)? I'd rather be more fully compatible with how people write JavaScript than support older Flow versions.\nMe too\n\nIt does make me wonder whether we should publish type defs in separate packages so we can break them without having to bump AVA versions. @sindresorhus thoughts on that?\n\nI don't think we should do that. I really like how it just works right now. If we need to make breaking type definition changes later on, we can either wait on the next major version (we're gonna make those more often going forward) or decide it's worth a small breakage.. Good catch. Thank you :). I think we've already discussed this before, but I couldn't find a ticket.\n\n\nI'm not sure whether this should apply to .skip() as well. Perhaps it should?\n\nI think it should. Skipping a test might be temporary and you don't want to lose the snapshot. Imagine you use .skip(), run ava -u, then remove .skip() but also modify the test accidentally, you want the snapshot to catch that.. \ud83d\udc4d I'm all for simplifying this.. @hallettj Any ideas?. As you can see, it's deep down in the dependency-tree and not something we control. I would recommend trying to handle this further down in the dependency-tree.. It feels weird having to add/change APIs because of Flow/TS limitations.... > tl;dr: I think that the simplest option is to split throws into multiple assertions: throws for functions that should throw a synchronous error, rejects for functions that should return a rejected promise, \nI've considered this before, but with async/await the naming becomes unnatural, as you usually use the throw keyword in async functions instead of Promise.reject(), so it doesn't really seem like it's rejecting.. > Similarly, would it be a terrible bother if our typings were stricter on promises?\nI would be fine with this if we used a Object.prototype.toString.call(promise) (this would allow libs to set Symbol.toStringTag) type check and not instanceof.\n\nIf you're using observables or generators, how would you feel if we'd remove first-class support?\n\nI don't use either, but I've seen a lot of people use the observable support in their tests implementation. I'm fine with removing observable support from t.throws though. It always felt out of place. And I think we should definitely remove support for generator functions in the test implementation. It's really just a leftover from the early days when generators were a popular alternative to async/await.. > It would still work with functions that throw synchronously, however the t.throws() & t.notThrows() assertions must now be awaited on. This is a breaking change.\nThis would be really awkward for synchronous tests.\nI think we should keep t.throws() synchronous, but remove promise support and introduce a t.throwsAsync() for that instead. Test implementations are synchronous by default, and I think t.throws() should be that too. It's also what people would expect coming from other assertion APIs.\n\nI also want to consider typing the return value as Promise / Error respectively. \n\n\ud83d\udc4d . > is there a case for a t.rejects() that fails if passed a function that throws synchronously? Rather than t.throwsAsync().\nNot sure I understand. Fail in what sense? Fail the test or process.exit() kinda fail? And how is it different from t.throwsAsync()?. The reason I prefer the naming throwsAsync over rejects is that most of the time I use async functions and there you throw too.. js\nawait t.rejects(() => { throw new Error() }) // fails test\nWhat's the use-case for supporting this? I really think if we split up the methods we should not allow synchronous stuff in t.throwsAsync nor asynchronous stuff in t.throws.. @novemberborn Ah, I totally misunderstood you. Yeah, then we agree.. Top-level await is not relevant to the recipe and those changes should removed.. As for the rest of the changes, @jdalton would you be able to review?. This was fixed in 1.0.0-beta.2: https://github.com/avajs/ava/releases/tag/v1.0.0-beta.2. @niftylettuce What AVA version and Node.js version? Can you try the latest AVA beta?. @xtx1130 \"me too\" comments are not useful unless you provide additional information ;). Sounds similar to https://github.com/istanbuljs/nyc/issues/619. Sorry, not going to happen. We intentionally don't have any aliases.\nI would recommend using eslint-plugin-ava which will catch incorrect usage live in your editor. We could even auto-fix it: https://github.com/avajs/eslint-plugin-ava/issues/190\nSee https://github.com/avajs/ava/issues/114#issuecomment-152706993 and https://developer.mozilla.org/en-US/docs/Glossary/Falsy. I don't think the future direction of AVA is power-assert-first, simply because I don't think power-assert is good enough in the general case. When I first created AVA, I thought power-assert was amazing, but after having used it through the years I've found it less helpful than the explicit assertions in the majority of cases.\nI would personally prefer having a separate assertion method for power-assert and remove it from the others.. Regardless whether it's possible to use t() or not, I don't think it's a good idea. I prefer t.assert, unless we can come up with a better more explicit name.. Only the 1.0.0 beta versions have the throws matcher: https://github.com/avajs/ava/releases. This also needs some docs.. I don't really see the problem here. AVA has had built-in ESM support for years, so AVA users are used to using it. Extending it to the config makes sense in that way.. @vjpr What is the exact problem/error?. > Looking at the esm options we could enable the cjs.vars option. That would expose __dirname, __filename and require to the ava.config.js file\nLet's go with this.. I for one would love to have a simple stubbing solution built-in. I find libraries like Sinon, while great, a bit too much and hard to use. The downside is that it's difficult to maintain a good stubbing library. There are lots of edge-cases to handle and I don't think we have the resources to handle that. We definitely should improve the extensibility of AVA though, making it super easy to integrate an external stubbing library.. Duplicate of #1668. Ping @ashimagarwal . I'm still against this. While it's possible to reject with a non-error, it's not a good practice. Supporting this would mean a worse experience for the common-case. It would complicate the TS/Flow type definitions, where we would like them to return an Error. And it would be inconsistent as it would not work with t.throws(fn, {instanceOf: MyNonErrorClass}), so we would have to document where it doesn't work.\nI'm happy to be convinced otherwise with use-cases and examples.\nFor reference, this has been in AVA for half a year and you're the first one to complain, so it can't be such a big issue.. Async testing is inherently unpredictable. The correct way to test time-related things is to either use test.serial or use some timer mocking library (#1827).\n. Ensure you're on the latest AVA 1.0.0 bet version and that you're following: https://github.com/avajs/ava#using-avaconfigjs The config file is a JS file, not JSON.. Can you add a test here and here?. Your indentation is off. Make sure you use tab-indentation.. @bigslycat Ping :). Ping @bigslycat :). Closing for lack of response. Happy to reopen if you ever get to this ;). From the docs:\n\nDirectories are recursed, with all *.js files being treated as test files. Directories named fixtures, helpers and node_modules are always ignored. So are files starting with _ which allows you to place helpers in the same directory as your test files.\n\nSo you need to either put the setup directory inside a directory named helpers or explicitly ignore that folder by specifying the globs to your test files in the files option or explicitly ignore that directory.. \ud83d\udc4d Yes, please.. > do you still think we should make t.throws synchronous, and add t.throwsAsync?\nYes, I think the change makes sense regardless, as it will make it more explicit and prevent mistakes.. > Given that this is less a type definition than a wrapper implementation, should we perhaps publish this as @ava/bucklescript?\n\ud83d\udc4d I think this would be better as a separate package.. @novemberborn I intentionally didn't so not to cause conflicts for https://github.com/avajs/ava/pull/1776 and other PRs. We can enable those rules after 1.0.0 when there are less activity and chance of conflicts.. @jamiebuilds What information is missing? We could embed arbitrary data in TAP if something is missing.. t.throwsAsync is not yet released.\nYou should read the readme from the latest version: https://github.com/avajs/ava/tree/v1.0.0-beta.6#readme. > The build failures seem unrelated to my changes and rather seem to have been introduced in 8663028. Please correct me if I'm wrong and I will happily fix.\nYup. Already fixed.. Can you also update the TypeScript and Flow type definitions?\nhttps://github.com/avajs/ava/blob/master/index.d.ts\nhttps://github.com/avajs/ava/blob/master/index.js.flow. I don't think it's worth breaking pretty much everyone's tests just because some users don't understand how it works. The solution here is better docs.\nI do agree the esm module is a better than transpiling the syntax with Babel, but I don't think it's worth this big of a breakage.. > Configure AVA's Babel pipeline to convert to CJS, which introduces users to the Babel pipeline and lets us more clearly signal how to configure source compilation\nI was hoping AVA would rather move to a model where it either transpiles everything (or the specified files to be transpiled) or nothing. That way there's no confusion.. I could be \ud83d\udc4d for this change if we made the breakage worth it:\n\nDisable transpilation by default unless there's a babel config.\n    I want to remove the performance hit of Babel for users that don't use it and improve stack traces.\nRemove Babel as a direct dependency.\n    We would use the Babel version used by the project if it's in the range supported by AVA.\nDon't transpile ESM by default when Babel is used.\nWhen Babel is used, transpile everything by default, not just tests.\n\nMaybe this needs to wait for AVA v2, but I would still like to do all of this in one go to reduce churn for users.. > If we can deal with the partial test failures I may just change this PR to update to the next esm release, without changing the helper code.\n\ud83d\udc4d . @novemberborn \ud83d\udc4d . Thanks for contributing, @sh7dm \ud83c\udf89. This is already clearly documented at the top of the readme:\n\nThis documentation covers the 1.0 beta releases.. The default AVA output is not guaranteed to be stable. It's meant for humans. Check out the --tap flag instead, which generates machine-readable output.. Done. Done. Does this handle babel.config.js? I'm asking since that file has different semantics from the old babelrc.. You could also use p-event for this:\n\n```js\nimport test from 'ava';\nimport pEvent from 'p-event';\ntest('2 callbacks', async t => {\n    const alice = new Alice();\n    await pEvent(alice, 'connect');\nconst bob = new Bob();\nawait pEvent(bob, 'connect');\n\nt.pass();\n\n});\n``. Thanks for your awesome work on keeping things translated, @forresst \ud83d\ude4c. I think we should list the docs files directly in the readme instead of just linking to the file browser. That one less click and makes it easier to Ctrl+F.. Turns out the logic incaller-callsite` is pretty buggy in recent Node.js versions, although it used to work perfectly. That's what happens when you accidentally rely on implementation details...\nI have done a new version of clear-module and import-fresh which both now works with your test case.. Duplicate of #1841. > I think postcss should throw an Error that actually extends Error, however this type of situation might happens within many library and that make it harder to test code that uses those with ava@1.0.1.\nSo the end-result is that AVA encourages postcss to make CssSyntaxError an actual Error. That sounds like a win to me.. > #1841 was closed, so I opened this one. Does it means this won't be addressed?\nWe can continue to discuss it here. If we decide that it's worth doing something about it, we'll reopen this issue.\n\nAny workaround to suggest when your code depends on a library on which you don't have control that throws non Error object?\n\nFix the library? I realize it's not always that simple, but I also don't want to enable bad patterns in AVA and complicate the assertion method and typing definitions for such an edge-case anti-pattern that can actually be fixed in the offending library.. // @ai. I like the idea (and naming) of t.cancel(). I can see that being useful for many situations.. Any idea why Travis is failing? It's passing for me locally.... @dflupu Thank you!\nYou need to submit the PR URL to IssueHunt to claim the bounty ;). @dflupu You need to submit the PR URL to IssueHunt to claim the bounty: https://issuehunt.io/repos/26820798/issues/583. I agree the tagline be a bit more positive.. > TypeScript 3.0 introduces a new type called unknown that does exactly that. Much like any, any value is assignable to unknown; however, unlike any, you cannot access any properties on values with the type unknown, nor can you call/construct them. Furthermore, values of type unknown can only be assigned to unknown or any.\nSo for the assertion parameters, it will work the same as any. It's mostly just for consistency, however, had AVA been written in TS, we could have benefitted from the strictness when handling the assertion parameters internally.\nFor return values, it forces the user to cast it to a proper value, which is good.. Actually, looking at the type definition in more detail now, I see that there are no any being returned. I was sure there was. Maybe there was before 1.0.. > I still think any is more self-documenting for those calling our assertions than unknown would be.\nHow so?. And unknown means that too. But doesn't matter.. @vjpr You can test this PR with:\nnpm install --save-dev 'avajs/ava#344b49c371f9354e3b96ac3a92721912f0c3255b'. Well, would be nice if they actually did what they should. It's not hard to shim them. Can use https://github.com/sindresorhus/ansi-escapes for everything except for .getWindowSize(), which can just return [80, 24].. Please don't include .DS_Store.. @be5invis It's not clear what 1000 means in your example. And what does resources('memory') actually do? Is memory the key? Would be nice if you could elaborate a bit more about the proposed API.. But what is 1000 and who enforces/controls its usage? How would AVA enforce this? AVA could easily control concurrency, but not memory usage and other things.. I think a better and less fragile solution is for your code to wait for the database to be ready (detect it somehow).. cwd: path.resolve(__dirname, '..')\n. execFile('../cli.js', ['es2015.js'], {\n. ES2015\n. This check is moot and can be removed.\n. Don't change this. XO should test all JS files.\n. I think we should try to load the local babel before using the built-in one. So the user can provide their own.\nYou can use https://github.com/sindresorhus/resolve-from to achieve this.\n. js\nlog.success(title + ' ' + chalk.dim('(' + hrtime(duration) + ')'));\n. Just remembered that dim doesn't work everywhere. Can you make it chalk.gray.dim?\n. And the below ' ' should be here instead.\n. } catch (err) {\n. Use babel-core instead. It doesn't include the Babel CLI junk. You'll need to update the require statements.\n. t.ok(); => t.pass();\n. Leftover\n. This should also be babel-core.\n. Actually, should probably do both, in case users use plain babel.\nrequire(resolveFrom('.', 'babel-core/register') || resolveFrom('.', 'babel/register'));\n. Relevant: https://github.com/sindresorhus/ava/issues/38\n// @vdemedes \n. Yeah, promisification should be a separate PR.\n. Actually. I think it's worth using https://github.com/petkaantonov/bluebird here for performance reasons.\n. Just use Promise.all()\n. https://github.com/petkaantonov/bluebird/blob/master/API.md#eachfunction-iterator---promise\n. ?\n. I can't really think of a better way right now, but this feels weird.\n. Why?\n. Should we reject with an error with a error message here?\n. Yup, I should have been clearer.\n. Ok. Can you at least add a TODO in there so it's not forgotten that it needs improving?\n. Agreed. Just use it inline below:\njs\nlog.writelpad(chalk.red(beautifulStack(result.error.stack)));\n. This inner function can be DRYed up as the Runner.prototype.concurrent has the exact same function content.\n. This should be a private method, meaning prefixed with _: Runner.prototype._addTestResult\n. Yeah, would be better if Test._setAssertModule were explicitly called below.\n. I was thinking that only the CLI would print to stdout. The forked processes would message status updates through the message channel. That way the parallelism is contained to the API and the CLI only calls the API normally, without any knowledge of the parallelism.\n. ?\n. It should be bi-directional: http://stackoverflow.com/a/10394457/64949\n. k\n. This file needs to be added to files in package.json.\n. The below should be in a separate function.\n. Or better yet, just move it to /lib\n. Instead of running it directly here, how about we just return the file, and use [].map here: https://github.com/vdemedes/ava/blob/2651df6749af48ee985bad9b92d107d4770ea0eb/cli.js#L125 We can then run everything from here https://github.com/vdemedes/ava/blob/2651df6749af48ee985bad9b92d107d4770ea0eb/cli.js#L128 instead and use Promise.all for the execution instead of a global files counter.\n. For ease, this can just be fs.statSync.\n. I think all the fork logic should be outside cli.js, in index.js or maybe a separate run.js/fork.js file or something. Separation of concerns and all. This way the CLI could just use a promise interface and get back the result when done without having to know at all that there's forking going on behind the scenes.\n. This will also make the fork logic testable, which it should be.\n. It can both return a promise and emit events. Events are emitted on the object, not promise.\n. Only cli.js should write to stdout/stderr. This could maybe also be attached on the returned promise. Unless you can come up with a better solution.\n. This should also not log. Should happen in the CLI.\n. This needs to take the exit code into account and resolve/reject based on that. And shouldn't the resolved promise contain the result?\n. I think we should first resolve all paths before we do anything. That would make the code cleaner and less coupled. Then with that array of files, run fork (fork(file).on('message', test)) on them here.\n. So one function (handlePaths()?) that globs the files, filters out js files (like run does now), stats.isDirectory() (like run does now), convert paths to absolute, then returns them. This function should be used here.\n. Instead of a message event, the result should be as the resolved promise.\n. These should be returned from the API as some kind of stats, not here.\n. Actually, all information should just be in message events, not stdout/stderr.\n. Can you add a TODO here so we don't forget?\n. Disregard my second comment. My first still stand though. Only cli.js should write to stdout/stderr.\n. Hmm, I think we're way past supporting node test.js now, with parallelism and ES2015. Unless you can come up with a good way to support both cases. Maybe detect that it's executed directly and log output. You can detect direct execution with: require.main === module.\n. It should get the final stats (passed/failed tests, etc), where we write 1 test passed successfully.\n. Oh, yeah, my mistake. Never mind.\n. How will this work with hihat? According to its docs:\n\nThe process will stay open until you call window.close()\n\nWouldn't it be better to call window.close here if in the browser. Or even better, keep it as before, and have hihat shim process.exit => https://github.com/Jam3/hihat/issues/27\n. let => const\n. let => const\n. let => const\n. let => const\n. just put it inline\n. inline\n. this is duplicated in 3 places now, can you make it a helper function?\n. I think async/await should be a separate section below this one.\n. ### Async function support\n. ### Generator function support\n. ```\n\nAVA also supports async functions (async/await) with no configuration required.\n```\n\nNote the link change.\n. I was wrong, you can use arrow function with async. Can you add an additional example using arrow functions? https://tc39.github.io/ecmascript-asyncawait/#async-arrows\n. and an additional tests for async arrow function.\n. test ( => test(\n. Place this at the top of the file.\n. And name it execCli\n. log.error('Couldn\\'t find any files to test\\n'); instead of the extract log.write() call.\n. I prefer to be explicit.\n. Can you make the number a const: TIMEOUT_UPPER_LIMIT or something.\n. var separator = ' ' + chalk.dim(figures.pointerSmall) + ' ';\n. Can you put these at the top of the file or at least above where they're used?\n. Can you add an err check here too? To make sure the fail is propagated.\n. Why is actual and expected undefined? Shouldn't we follow the sync method: https://github.com/sindresorhus/core-assert/blob/494c1eaea60090d4d4a187f03de9c2f4051b6f9b/index.js#L321\nAs for msg, see: https://github.com/sindresorhus/core-assert/blob/494c1eaea60090d4d4a187f03de9c2f4051b6f9b/index.js#L317-L318\nMaybe it would be easier to just use assert.throws(fn, err, msg); here.\n. var promise => var fn\n. Would be cleaner to pass this as a command-line option when forking the process.\n. Use !== -1 instead of >= 0\n. Instead of doing a replace here just put the figures directly into the indexOf check.\n. Also needs to be added to the readme: https://github.com/sindresorhus/ava#cli\n. The options here need to be put in a variable and used in line 18 too.\n. assign => objectAssign\n. This should probably be in the assert.js file.\n. What are these? Add a comment inline in the code.\n. @twada Can this be published on npm? I don't want to depend on a git repo.\n. ?\n. Why can't this be done at require time in https://github.com/sindresorhus/ava/pull/46/files#diff-c56ba0fef1bb510acefb4680eb31e9c6R21?\n. No need to return since the return value is not used.\n. Looks like we'll have to add patterns for all the non-standard assert methods: https://github.com/power-assert-js/empower#optionspatterns\n@twada Is this correct?\n. Yeah, I don't see why we wouldn't want to decorate all of them, even though t.ok/notOk are the ones with the most gain.\n\n(Is t.assert public API?)\n\nNo, it's only there for legacy reasons.\n. No need for true.\n. How is this testing that it's working? It's only the normals output.\nAnd should be:\njs\nt.not(stderr.indexOf('t.ok(a === \\'bar\\')'), -1);\n. wrong indent\n. @twada Is there anything you can do in power-assert to make this unnecessary?\n. This list is duplicated. Maybe move it out into a separate file and require it in both places.\n. Maybe move the whole function enhanceAssert(assert) { function into a file called enhance-assert.js and also export the list. Then that file could be both used here and in lib/assert.js.\n. > Run tests serially\n. Instead of doing this for every option I think we should just JSON.stringify the options and pass them as the first argument.\n. > AVA runs tests concurrently by default, which is suboptimal when you need to debug something. Instead, run tests serially with the --serial option:\n. Use const\nconst only means no new assignments, it's not immutable.\n. wrong indent\n. k\n. Runtime should still be included, even if hasGenerators is true.\n. \"co\": \"floatdrop/co#343xsv324\",\nuse the shorthand format and use a specific commit hash (first 6 chars of the hash is enough), otherwise the npm cache will become stale.\n. lol, that was a fake hash, i don't know which hash you want to use.\n. Why are you using promises here? cp.kill() is a synchronous operation. Just do a [].forEach().\n. Oh, right. Hard to read diffs on mobile...\n. ``` js\nif (typeof err === 'string') {\n  err = function (e) {\n    return e.message === err;\n  };\n}\nassert.throws(fn, err, msg);\n```\n. Yup\n. Why? I don't want to manually require a Promise polyfill in all my tests.\n. I guess there's really no good solution for this...\n. I usually prefer to import the full module.\njs\nvar path = require('path');\n. This won't work with npm@3 as it might put regenerator at the top-level.\n. core.js (which is the runtime polyfill) already includes a small promise polyfill. Wouldn't it be better to use that? Bluebird is huge and slow to eval.\n. Yup, it's only ours.\n. :+1: \n. Why?\n. @jamestalmage I've added Windows testing with AppVeyor and this assertion fails: https://ci.appveyor.com/project/sindresorhus/ava/build/2/job/w7efao2if7n2g2q9 Could you take a look?\n. Use the ## header style instead of underline.\n. and Table of Contents\n. * \u2192 -\n. Can you place the TOC below the below intro?\n. Use - instead of * for the items\n. Don't remove this.\n. You can just use childProcess.execFile(process.execPath, ['../cli.js'].concat(args), {cwd: __dirname}, cb); instead of which.\n. Yeah, I added it as a quick hack because the 1 test passed message wouldn't output without it when running multiple tests and I couldn't reproduce it in a test. If it works fine now you can just remove it, things have changed so much since then.\n. Not unless it interferes with them. @vdemedes ?\n. if (arguments.length >= 1 && err) {?\n. Might we worth extracting all the Observable related test into a separate test file, since there are so many now.\n. Yeah, but that should be done in a separate PR.\n- test.js\n- promise.js\n- observable.js\n. Not for this PR, but what do you think about pushing and object here instead of all the arguments? We could just reuse the object in this.results.push.\n. no comma needed here and extraneous space at the end.\n. no comma needed\n. .then(t.end);?\n. .then(t.end);?\n. .then(t.end);?\n. Can you make sure it follows XDG?\nhttps://github.com/sindresorhus/xdg-basedir#cache\n. Why? Maybe add a comment.\n. Makes sense.\nWhy can't it just do that on every call for expired ttls?\n. @floatdrop What would new Test.hook(title, cb) be? Just a wrapper function that sets the internal this.type?\n. Does it do that every time cache.clean() is called? If yes, it might be worth only doing it once in awhile and checking with a file the last time it was done.\n. :+1: Once a week or less sounds sensible. It doesn't generate that much junk.\n. But this is tape, not AVA.\n. Yeah, we can look into this later. It could be a subclass too, new Hook(), but I'm not a big fan of that either. Maybe using Symbols as constants. ava.HOOK_TYPE = Symbol('hook') or something. But meh.\n. Oh, nvm then. Sorry about the noise.\n. Probably clearer to just check if the file was executed directly: require.main === module\n. var path = relative('.', process.argv[1]);\n. using the AVA CLI\n. Actually:\n\nTest files must be run with the AVA CLI:\n. Oh, nvm. I'm sleepy...\n. double ::\n. and maybe add chalk.dim('$') and chalk.cyan to the command? just to be fancy.\n. I was thinking the full command:\n\nchalk.dim('$') + ' ' + chalk.cyan('ava ' + path)\nTo indicate what they can copy-paste.\n. Perfect. Just ava in cyan looks better, but this is clearer.\n. I would prefer error for consistency.\n. Why is test.assertError left in here? It's already part of props.\n. This should use t.not().\n. Just fyi, tape supports t.is as an alias, so we could use that. Probably better as a separate PR though.\n. I feel it's different. This is more like passing around data than variables. Think of it as a table of data and the keys are the headers. At least how it's like in my mind. Also none of the other keys are abbreviated. Just looks wrong.\n. Unrelated, but we can probably remove this now, right? Because of https://github.com/sindresorhus/ava/commit/bb1304c5be056689c39211d89c673f2f732e4c4a\n. Is cyan really the best color for skipped? Just asking. It should at least be .dim too.\n. Ok, let's try it out like that for now.\n. > Also, in some ways this is a feature - you can use all the fancy sugar bluebird adds in your test\nNot a big fan of implicitly exposing Bluebird to users. If they need the extra functionality they can require it themselves.\n. @jamestalmage Can you open an issue about it over at core.js? Would be more convenient to use that one, otherwise we'll need to figure out a way to manually alias it to pinkie.\n. > If the Babel polyfill doesn't support unhandledRejection\nI'm just going off your comment :p\n. setImmediate?\n. console.error('No tests found in ' + testPath + '. Make sure to import `ava` at the top of your test file.');\n. error: new Error('No tests for ' + file),\n. reject(new Error('Never got test results from ' + file));\n. Wut, another one? :p\n. Agh, so much hacks to work around Windows and AppVeyor flakiness...\n. This is weird. Why not just wait on ps.on('exit')?\n. exceptions \u2192 exception\n. 0 can be dropped as it's the same as none\n. Differently how?\n. What do we need it for?\n. For TODO's I prefer to include the username so we know who added it without having to git blame.\nTODO(jamestalmage):\n. :+1: \n. @vdemedes I think the intention is to only do it when we run AVA's tests on AppVeyor, not when the user run their tests there.\n. I don't think it matters to be honest. AppVeyor is sooooo slow regardless. But we can try with 10 seconds to see if it still works.\n. @jamestalmage Probably because it's annoying to always have to wait for AppVeyor before merging a PR.\n. Haha :p\nSounds like me this weekend. I was bumping dependencies on a gadzillion modules on a super slow connection. $ david update, wait 10 minutes, commit, push, npm publish, wait 5 minutes, then next module...\n. I'll do it.\n. I prefer to have builtin requires grouped first.\n. We know the reason, AppVeyor is just super bad and flaky :p\n. Can you just import path instead and use methods on it?\n. 'ava-' \u2192 /^ava-/ just to be sure we don't strip any future message with ava- in the middle. I know, edge case, but this gives clearer intent too.\n. Is this really needed? Do we really pass a custom process anywhere? And even if we do, wouldn't it be better to just have it as the third argument instead? For simplicity.\n. Call it filepath or pth.\n. needs a space after :\n. Oh, ok, nvm then.\n. Promise.pending()? Couldn't find this anywhere in the Bluebird docs: http://bluebirdjs.com/docs/api-reference.html\n. I prefer an empty return if it actually doesn't return anything. It's more explicit and has a better intent.\njs\nthis.exit();\nreturn;\n. Why promise.promise?\n. Same as https://github.com/sindresorhus/ava/pull/240/files#r45325062\n. Wonder if we could use https://github.com/sindresorhus/immediate-promise and have .exit return a promise. Related to: https://github.com/sindresorhus/ava/pull/240/files#r45325153\n. Place this below 'ava'. No point in wasting time transpiling AVA.\n. Can you put a subheading above here. ####\n. If you move it down it will not have an effect on the below import which is AVA. AVA is pretty large, so it will definitely slow it down if it needs to be transpiled too.\n. I think we can use '!**/node_modules', to match node_modules at any level and not having to do the filtering below.\n. Not sure why promise is mentioned here. It's not really relevant. Should be more like:\n\nfails with the first assertError\n\nSem with the below.\n. I usually prefer early returns over nesting when possible:\n``` js\nif (this.assertError !== undefined) {\n    return;\n}\nif (err === undefined) {\n    err = 'undefined';\n}\nthis.assertError = err;\n``\n. super minor, but for consistencyfalsie\u2192falsy. Just put the filter function inline.\n.their=>there. Can you place it with the other relative requires?\n. Wish we hadpower-asserthere so we didn't need theduration + ' < 1000'`.\n. Why not just this here instead?\njs\nif (!this.metadata.async) {\n    throw new Error('t.end is not supported in this context. To use t.end as a callback, you must explicitly declare the test asynchronous via `test.async(testName, fn)` ');\n}\n. We should rather use async functions in this example.\n. Any reason to allow promises in test.async? Wouldn't it be better to contain it to test()?\n. Can you add something about that it's also useful to make sure not too many assertions were run?\n. Make this a subheading\n. Can't we just prevent it? Throw an error if it's used with legacy mode?\n. // @vdemedes \n. Yes, but I see no reason you wouldn't rather use 'test()' with an async function in that case. Or even 'test' with a normal function and return a promise. I want to enforce best practices whenever possible.\nUnless I'm missing something?\n. Yes\nAnd error message should tell the user to use \"test()\" instead.\n. Does it matter? It shouldn't be used with test() anyways. So I don't see how it makes a difference.\n. Ok, never mind. Got it now. Sorry about the noise.\n. Yup\n. Ok\n. Can you open an issue with the contents of this comment? I'm not sure to be honest.\n. Been thinking about changing resolve-from to just return null when not found instead of throwing. Thoughts?\n. Can you not hard-wrap?\n. Maybe this is something we could have in a future --verbose mode or just add it as a debug()?\n. You can't process.exit() here as ava-init won't have a time to finish. Just return and XO will pass when https://github.com/sindresorhus/ava/pull/277/files#diff-b9cfc7f2cdf78a7f4b91a753d10865a2R132 lands.\n. Maybe new Api(files, opts)?\n. @jamestalmage We already do this in https://github.com/sindresorhus/ava/blob/e2e45151dc5ac249d49a7cd94baca8c70f9ed2ce/cli.js#L57\n. return Promise.reject(new Error('Couldn\\'t find any files to test'));\n. add an empty line above this\n. You don't have to check if debug is enabled. That's already done by debug\n. Typo\n. This this is moot.\n. js\n[\n    'assertCount',\n    'title',\n    'end',\n    '_capt',\n    '_expr'\n]\n. Maybe make this _publicApi? I don't see why anyone would need it.\n. My OCD, can you put the self statement before api?\n. I think we should drop using the the user's local Babel version. Instead we should just document that to use npm@3 or npm deduplicate. The user can then just enforce any Babel 6 version and we will use the same since our dependency is loose.\n. Why is the generator check no longer needed?\n. Use caret ^ for the versions.\n. http://babeljs.io/docs/plugins/transform-async-to-generator/\n. Oh, I didn't realize async functions went to stage3. That's great! :)\n. @jamestalmage Yeah, was only commenting what I want the final PR to look like. We can wait on doing it until this PR is ready.\n. Put it below serial\n. options => opts\n. Or just use a loose regex.\n. I noticed this the other day too. Can you open an issue about improving it?\n. semicolon\n. semicolon\n. semicolon\n. semicolon\n. Just use arrify on line 69 and drop this whole thing.\n. Sidenote: I think it's time we simplify argument passing to the forked process. Too much code now just proxying CLI args.\n@jamestalmage @vdemedes I propose we just JSON.stringify() the data we want to pass instead of using CLI args. Thoughts?\n. semicolon\n. semicolon\n. '  --require    Module to preload (Can be repeated)',\n. @jokeyrhyme No, we use use child_process.fork which is different. We could probably use it in the CLI tests, though.\n. @jamestalmage Yeah, just XO that ignores it because it's fixtures. https://github.com/sindresorhus/xo/blob/0a5060c717876f77d9bcc9b8b453ba14eec332f1/index.js#L20 Starting to question that.\n. https://github.com/sindresorhus/ava/pull/296#discussion_r47035400\n. Ok, opened an issue: https://github.com/sindresorhus/ava/issues/318\n. > I thought we only ignored tests/fixtures because it might have some esnext stuff in it, while production might not?\nYes, that's why XO ignores them, because fixtures might not follow code style or anything.\n\nIt would be nice if XO had an easy way to multiple configs (and a way to specify which got applied would depend on glob patterns).\n\nYup, I've been planning this for a long time, just haven't had the time to implement it yet.\nhttps://github.com/sindresorhus/xo/issues/22\n. put an empty line above this one\n. I think we should do this.options = options in the Api constructor so we can just pass the whole options object here instead of explicitly specify each option.\n. The point of this change is not having to do this.\n. I think the first argument should be the file and second should be opts.\n. I don't think you meant to include this file. It was already removed in https://github.com/sindresorhus/ava/commit/386416846bdb7b8aae84e1b0d8cf77da0bb31fa4.\n. > stack traces for exceptions are corrected using a source map file\n. This test was removed in https://github.com/sindresorhus/ava/commit/386416846bdb7b8aae84e1b0d8cf77da0bb31fa4.\n. This should use t.is, t.same is for deep testing.\nSame with all the other ones.\n. You can't just remove this. The options are used directly on this in the code and needs to be changed to use this.options.x.\n. It's weird that babel exports the options. I think we should just get them directly here too. JSON.parse(process.argv[2]);\n. This is outdated\n. var objectAssign = require('object-assign');\n. js\nthis.options = opts || {};\n. No, meant that exports.opts._sorted is outdated with the latest PR changes.\n. I think opts.require is already guaranteed to be an array, so this check is moot.\nhttps://github.com/sindresorhus/ava/pull/296/files#diff-2cce40143051e25f811b56c79d619bf5R73\n. Can you make these api.js tests instead? Spawning CLI for each test is slow and we now have an API we can test directly instead.\n. This needs to be updated in the readme.md too.\n. I already fixed this in https://github.com/sindresorhus/ava/commit/3fdf44d1acbe8c6f14eb3865352ce49812c4a956, sorry. Can you rebase to fix the merge conflict?\n. Instead of #1, #2, etc, it would be better if they just said specifically what they were testing differently than the others.\n. No, I'm advocating dedupe so the user is able to set an explicit Babel 5 version that we will also use. Since we depend on a loose Babel 5 version, the user can set an exact version and it will be deduped to the user's choice.\n. @novemberborn That's not really the point of that example. It could use either, but by using an exact version of Babel 5, I'm illustrating my above point of it being deduped.\n. Good feedback. I'll try to make that clearer. Any suggestions?\n. Oh, never mind then. I read it wrong.\n. Don't remove\n. Good point. Fixed.\n. Fixed\n. Sounds like the above should be a code comment ;)\n. :+1: \n. nitpick, but equal => strictEqual\n. stage-2 includes stage-3\n. - 'npm i -g npm@latest'\n. You can use \\\\ in require and --require, but it only works on Windows. So it won't be cross-platform. We could normalize internally, but do we really want to implement it differently than $ node --require?\n. @jamestalmage Oh, ok. I misunderstood. Agree with that.\n@ariporad Can you update?\n. Don't use bash as it's not valid shell script with the $.\n. 3rd-party reporters => TAP reporter and linkify it to https://github.com/sindresorhus/awesome-tap#reporters\n. I think something like this would look nicer:\njs\nconsole.log([\n    '# ' + err.message,\n    format('not ok %d - %s', ++i, err.message),\n    '  ---',\n    '    name: ' + err.name,\n    '    at: ' + getSourceFromStack(err.stack, 1),\n    '  ...'\n].join('\\n'));\n. Same as https://github.com/sindresorhus/ava/pull/326/files#r48151151\n. Would be more fun to illustrate with the nyan reporter, right? You also need to show installing the reporter. Otherwise we'll end up with support issues on how it doesn't work...\n. Yes, but the highlighting is very random.\n. Actually, drop the install instruction. We can't know if they want to use it locally or globally. It has to be globally installed if used like the above, or locally if used in a npm run script.\n. > Should we include a screenshot or that'd be too much?\nSure! Here you go. Make sure to use <img> and half its width so it becomes retina.\n\n. double comma\n. Move this above fork.\n. Use https://github.com/sindresorhus/del-cli to make it work on Windwos too.\n. Both of these are ignored by default in nyc 5.\n. @jamestalmage Re trash-cli, that's a bug https://github.com/sindresorhus/trash/pull/43, but no point in having the overhead of moving it to the trash, not something you'd ever care about restoring.\n. assign => objectAssign\n. Just curious, is there any difference in using .update() multiple times rather than just concatenating the strings together before passing them to .update()?\n. Doesn't look like this is used anywhere.\n. you need make => you need to make\n. PR => pull request\n. Don't remove this part starting with _. We still support that.\n. We still want this.\n. Can you add logUpdate.stderr(); here to hide the cursor as early as possible. Currently it doesn't hide until the tests actually starts running. Should include a comment on why it's there.\n. I think the fixtures should be placed in a super folder with a descriptive name. Like test/fixture/ignored-dirs/fixtures/test.js. Right now it looks like it's for something else. Same with the below.\n. Yes\n. This should be a dev dependency.\n. I would make it a private instance method (prefixed with _). The function checks if assertion count matches plan count. I think the name could be clearer.\n. remove this empty line\n. noCache => cache. I don't like double negatives.\n. This could use https://github.com/sindresorhus/md5-hex now.\n. This would be clearer as if (!(test.error && test.error.message)) {\n. What's the reasoning behind this change? Why did we previously only show the stack for non-errors?\n. Ok, I thought so. So why do we not want to show the stack for Error? Why is it special-cased?\n. Ok, good to know.\n. https://github.com/substack/node-commondir\n. This also matches test.js, so the above can be removed.\n. Same with the below.\n. Actually, the change to this array is wrong.\nIt should match test.js and test-*.js in root, and any JS file recursively in test folder as long as it's not prefixed with an underscore.\n. This could be explained better. Not clear what it means.\n. Not sure what it's testing from the test description.\n. Excessive newlines\n. Use ES2015 syntax like the other test fixtures.\n. Use ES2015 syntax like the other test fixtures.\n. Not totally happy with this, but something like:\n\nBy default, AVA run tests named test.js or starting with test- in root and any tests recursively in the test directory.\n\nYou also need to update the globbing pattern in the CLI help output (and the identical output in the readme).\n. @ariporad Good point, should include that too.\n. Why change this file? Looks unrelated to the intent of this pull request.\n. Can you remove that change? And the below file change too.\n. :+1: \n. @jamestalmage I know it's a breaking change, but it's what people except (#249). Ideally, no one should have to specify anything and it should just work. By recursing test by default, but ignoring the common helper and fixture dir, we can have it just work with convention. What kind of error are you afraid of?\n\nI find it far more confusing that there are .js globs for the base-dir, but we will recurse the test folder indefinitely.\n\nEven though we explicitly specify the default glob patterns?\n. This should be in the other PR.\n. There's a good reason the inconsistency, though. It's only safe to recurse the test folder. We could of course only recurse the test folder, but most tiny modules (like most of mine) have a single test.js file in root or a couple of test-*.js, so it's useful to support that too.\n\n**/test.js\n\nThis would match https://github.com/sindresorhus/ava/blob/master/lib/test.js which confirms my above argument.\n\n**/*.spec.js **/*.test.js\n\nI'd prefer to wait until someone actually asks for it. I feel it's better to keep your tests separate from the code in a dedicated test folder.\n. > why isn't it safe to recurse the entire directory?\nI just answered that: \"This would match https://github.com/sindresorhus/ava/blob/master/lib/test.js\" Which is not a test file.\n\nI have a really cool system, I can explain more if you like\n\nPlease do :)\n. > Then you use app-module-path for my-app, and then your tests look like this:\nJust fyi. Your links never work as you use relative links.\n\nThanks for elaborating. I've never seen this use-case before and doesn't seem like something we should support by default. You could easily support it with a simple glob pattern. Your suggested default glob pattern of **/*.test.js wouldn't fit your use-case of webapp.test.e2e.js anyways.\n\nIf you feel strongly about it can you move this into a separate issue? It's out of scope of this PR and I don't want that discussion to block this change.\n. moot now. remove.\n. moot now. remove.\n. Maybe:\njs\nmessage: 'Promise rejected with \"' + err + '\"',\n?\n. Make sure to include the babelrc: false thing here.\n. 'utf8' is moot as it's the default\n. ?\n. Should probably salt with the Babel plugins too: https://github.com/sindresorhus/ava/pull/390/files#diff-b72e3e25f31d6b3c3e5f40b5991dc5f6L157\n. Can you put this at the bottom? I'm going for sorting by importance.\n. Ok, just drop it.\n. @jamestalmage Relevant https://phabricator.babeljs.io/T6709#67067\n. I prefer being explicit and use new for classes. ES2015 seems to agree with me.\n. Link to the GitHub repo, not npm page.\n. What does ES7 syntax mean? Be more specific.\n. Then I would write that instead. ES7 doesn't mean anything, it's ES2016, but async functions are not yet part of it.\n. pkgConf already returns an empty object when not found.\n. Maybe mention the benefit of specifying config here instead of the CLI is that it will be used when you run it with $ ava.\n. Should be camelCased\n. Yeah, I think it would be useful if meow did that for us.\n. Or you could just use objectAssign on cli.flags instead of using the minimist default thing.\n. @jamestalmage New version of meow is out. Just bump to 3.7.0.\n. npm apparently adds this now, but it's not needed as github is the default for shorthands.\n. Maybe use https://github.com/sindresorhus/indent-string here.\n. Why is the first line not indented? And why do we need to manually indent it? Shouldn't stack-utils do that for us?\n. Can you normalize the paths in stack traces to forward slash? That way they're consistent on all systems and with the added benefit of simpler regex here.\n. > I guess we could add an indent option to stack-utils.\n:+1: Can you open an issue on stack-utils so we don't forget to simplify this?\n. I don't like the standard either. Let's go with --serial, -s which looks nicer.\n. @jamestalmage That's only true when all flags have shortflags, which isn't the case here or pretty much ever.\n. ES6 => ES2015\n. npm install nyc --save-dev\n. Can you put this before ES6.\n. Can you use package.json instead of babelrc?\nhttps://babeljs.io/docs/usage/babelrc/#use-via-package-json\n. I generally prefer separating the coveralls stuff from the local test running. Meaning I don't see the point in having --reporter=lcov here.\n. So should look like:\n\"test\": \"nyc ava\"\n. and in .travis.yml, you can do node_modules/.bin/nyc report --reporter=text-lcov\n. Should be after_success: https://github.com/sindresorhus/pageres/blob/aa4bf7997a14ab4f37f0150323dd46718268b2d4/.travis.yml#L7\n. --reporter=html\n. Should probably add that the repo needs to be activated in the Coveralls web UI.\n. node_modules/.bin/nyc => ./node_modules/.bin/nyc\n. Yes, I meant, for consistency, use = between key and value.\n. Should end in a .\n. Babels => Babel's\n. babel => Babel\n. Don't indent\n. Don't indent\n. nyc => NYC to be consistent with the usage further down.\n. https://coveralls.io/ => https://coveralls.io\n. :+1: \n. :+1: \n. ?\n. Should be TypeError to match ES2015 classes and message: Class constructor Concurrent cannot be invoked without 'new'\n. e => err\n. Same as https://github.com/sindresorhus/ava/pull/466/files#r50755945\n. I would quote only to make it clear that we're talking about the only test type.\n. ?\n. :ghost: \n. Can you open an issue on iron-node about this?\n. Add these to ./lib/globals?\n. > Only run tests matching a pattern\n. I don't think the API should support both regex and string. It should be converted from a string to regex in the CLI.\n. Yeah, I know. Just thought it would be cleaner to keep all overridden globals in our globals file.\n. Why not just assign it directly to PublicApi.prototype.plan?\n. Why not just put all this logic in the internal API?\n. Is planStack ever not defined?\n. js\nPublicApi.prototype.plan = function plan(ct) {\n. :ghost: \n. We could just remove it, but this works too.\n. :+1: \n. > Only run tests with matching title\n. Should also use path.join()\n. :+1: \n. This should be commented out. We decided to not yet expose this. https://github.com/sindresorhus/ava/issues/70#issuecomment-173746486\n. :+1: \n. Why? And setImmediate instead?\n. Can you linkify both tools?\n. Maybe also mention how to show the flamegraph?\n. # Assertion planning?\n. auto-end?\n. ```\nPoor uses of t.plan\n.\nGood uses of t.plan\n``\n. Here, \n. lets\n. errors => error\n. I assume this should becallbackB?\n. const\n. arrow function\n. Not sure about including this. The difference now is completely negligible, even with thousands of tests.\n. I don't think we should mention it as it complicates it for the user. Now the user has to consider whether their use-case is worth usingt.plan. Same as too many options in an API; Don't give the user too much choice, or they'll get choice paralysis.\n. Why do you need to coerce these to booleans? If it's needed it should happen at the source where they're gotten/defined.\n. add an empty line belowt.plan(1);. Same here. Can you coerce it to a boolean at the source?\n. Why in the next tick?\n. The flag docs also needs to be reflected in the CLI help output in the readme.\n. Maybe we should useutil.deprecate` on this one? // @jamestalmage @vdemedes \n. Alright. The message could say:\n\nt.doesNotThrow is renamed to t.notThrows. The old name still works, but will be removed in AVA 1.0.0. Update your references.\n. :+1: Should be done regardless. Previously it wasn't a real error.\n. ```\n\nSetup functions vs. beforeEach\n. This comment is pretty moot as the file name already outlines what this does.\n. `tests.tests` looks kinda weird. Maybe the test collection should have a `.size` getter property (like a `Map`) or something?\n. `t.pass();`?\n. This message is a bit confusing and I don't see the point of it. There's already a warning when `chokidar` is not installed. I don't see the point of commenting that it is installed.\n. `opts.nodePaths.length > 0` is not needed as it's handled by `concat`\n. Just replace `dirname` and `env` with an options object.\n. Use `path.join()` instead\n. `module.exports = function () {`\n. `module.exports = function () {`\n. js\nvar nodePaths = process.env.NODE_PATH ? process.env.NODE_PATH\n    .split(path.delimiter)\n    .map(function (p) {\n        return path.resolve(p);\n    }) : [];\n. js\nif (!Array.isArray(tests)) {\n  throw new TypeError('Expected an array of tests');\n}\n``\n. Oh, nvm, I read it aspath.sep`...\n. Yes, but I'm only interested if Chokidar is not installed. Otherwise IMHO it's just noise.\nIf no message, I know it worked.\n. :+1: \n. I'm surprised the linter didn't catch this. Would have thought ESLint would have rule for this.\n. Ugh, discovered this too late. This makes no sense. Here you overwrite the modified NODE_PATH with the original one. This means the tests are also invalid. Tests still pass when the .map is commented out.\n// @ingro @novemberborn \n. Yup\n. This error message is unclear. What is the purpose of it?\n. Oh ok. That makes sense.\n. I think it would be a better approach to send it all at once, as you've suggested.\n. :+1: \n. object-assign?\n. Not sure what making it explicit would entail, but I always prefer explicit.\n. > When you set module to commonjs,\nMaybe mention where this is set? It can easily be confused as being set in the AVA config.\n. ES6 => ES2015\n. Wrong indentation here.\n. Wrong indent.\n. throw new TypeError('Expected a function. Use `test.todo()` for tests without a function.');\n. throw new TypeError('`todo` tests require a title');\n. semicolons please\n. semicolons\n. $ npm install --save-dev jsdom\n. Fixed\n. test(async t => {\n. $ npm install --save-dev tsc\n. > '  --match, -m      Only run tests with matching title (Can be repeated)',\n. !*foo* needs to be quoted in ZSH at least.\n. So does foo*, so probably better to quote them all?\n. Is there really any point in guarding this? clearInterval gracefully accepts anything.\n. Can you wrap the spinner char in chalk.dim()?\n. It's also a bit weird that self.spinnerChar() + self.currentTest is duplicated in MiniReporter#test too.\n. spinnerIndex would be a better name. When it says spinnerFrame I thought it was the actual frame.\n. @jamestalmage It's not supported on all Terminals, but it dims the spinner to be less intruding. Could maybe also do chalk.gray.dim() to support all terminals.\n. Wouldn't you need to do this for skipped and todo tests results too?\n. Yes, with master:\n\n. You now have the exact same error defined twice. Put it in a variable and reuse it.\n. Instead of adding a new error message, why don't we just remove opts.skipped && from here? Unless I'm missing something? // @novemberborn \n. This should be placed above the relative requires (on line 17).\n. Can't you just use the https://github.com/sindresorhus/has-flag dependency here? (we already depend on it)\n. single-quotes\n. I don't understand why you need the position? These are flag arguments, not positional arguments, and can be in any order.\n. I think memoization would be a premature optimization for this as execArv usually contains zero or one elements.\n. Just use Array#some here.\n. This also doesn't handle the case where the user would do --debug-brk 213 instead of --debug-brk=213.\n. Why do we manually count these in the reporter? Shouldn't it be provided by the API?\n. No point. The test is not run anyways, so it finishes faster than a human could see.\n. > If a human can not see todo and skip tests, shouldn't we also remove color from there?\nThat's what this PR does.\n\nBut wait, if the next test is slow, user theoretically can actually see skip/todo tests. So I guess we should keep the prefix and color.\n\nHow about we just not show them all together? So if a slow test is running, the last actual test is shown, not todo/skip.\n. I think the second argument would be better as an optional options object with an option called forceHasExclusive. \nhttp://ariya.ofilabs.com/2011/08/hall-of-api-shame-boolean-trap.html\n. Put an empty line above here.\n. js\npromise = api.run(specificFiles, exclusiveFiles.length !== this.exclusiveTests.length);\n. I don't totally get why we're splicing in here at a specific position. Maybe a comment would help.\n. throws return the error thrown though: https://github.com/sindresorhus/ava#throwsfunctionpromise-error-message\n. I don't think this check is needed. cliTruncate handles that for you anyways.\n. :+1: \n. >= 0 \u2192 !== -1\n. > A global timeout can be set via the --timeout option.\n. > AVA resets a timer after each test, forcing tests to quit if no new test results were received within the specified timeout.\n. Use = not =>$ ava --timeout=100 # 100 milliseconds\n. js\nprocess.exit(2);\n?\n. > Control-C is fatal error signal 2, (130 = 128 + 2, see above)\nhttp://tldp.org/LDP/abs/html/exitcodes.html\nI've seen 2 used many times for SIGINT overrides.\n. Would maybe be clearer to use something like: https://github.com/sindresorhus/is-obj\n. Should have an empty newline above return err;\n. Why change it from an arrow function to a function?\n. :+1: \n. > completed within the last\n. Should be above relative require calls.\nWish we had a ESLint rule for this. (https://github.com/sindresorhus/module-requests/issues/62)\n. @novemberborn Yes, https://github.com/jfmengels/eslint-plugin-import-order/blob/master/docs/rules/import-order.md, which will soon be included in XO.\n. I would put this at the top, since it's the default.\n. This is not really needed, and people will just copy-paste this. So I would leave it out.\n. I think it's fine to just add those like that and we can expand on them later.\n. We should mention the codemods here or at least a link to docs that mention it.\n. You could use util.deprecate.\n. Node.js core uses util.deprecate. So I would trust it.\n. Actually, these should just be t.true.\n. t.pass();\n. t.fail();\n. t.pass();\n. t.true()\n. t.true()\n. I thought the convention is that __tests__ can be anywhere? Hence why it's wrapped in underscores.\n. > By default, AVA transpiles your tests (and only your tests) using the es2015 and stage-2 Babel presets.\n. You need to manually linkify #577\n. Or maybe just linkify open issue. The number doesn't have much value.\n. Ava => AVA\n. Maybe include some babel config here to show the source of what's merged for clarity?\n. I think I meant line 64.\n. I think keeping the links inline, while a bit more messy, is more user-friendly as you don't have to go to the bottom to hunt down a link and then forget where you left off.\n. I mostly read raw markdown documents and having to hunt for matching links at the bottom gets annoying fast.\n. Linkify babel-preset-react and linkify sample project configuration instead of showing the full URL.\n. configuration => config\n. Needs to be more descriptive.\n. Let's see first => Let's first see\nava => AVA\n. npm i => npm install\n. import {shallow} from 'enzyme';\n. remove empty line\n. Use t.true and missing a )\n. t.true\n. environment, to do so, => environment. To do so,\n. react => React\n. Don't put in plain links, linkify some text instead.\n. ava => AVA\n. Inconsistent quote usage. You used double-quotes earlier.\n. Can you linkify power-assert doesn't handle JSX correctly to the relevant power-assert issue?\n. need extra setup => need any extra setup\n. First install enzyme: should be on a new line.\n. This doesn't really have much value. It's just a longer description of the title. I would drop it or come up with something else.\n. Can you use https://github.com/sindresorhus/repeating? We already depend on modules using it, so it would mean no extra dependency.\n. The npm search sucks... Try http://node-modules.com.\n. .bower_components => bower_components\n. Maybe we should put in theavasection of yourpackage.jsonfile and mention it's the recommended way. Having the config in package.json will be important for future editor plugins and other integrations as they won't be able to read the config from the npm run script.\n. > Or maybe we should allow undefined in md5-hash\nWhat would the behavior be?\n. > Not sure it's a good idea.\nHashing is not the place you want implicit behavior.\n. this.runVector > 0 is the best option here. I would use !== 0 when it can also be negative.\n. Maybe show an example of the config needed directly here so lazy people can just copy-paste something that works? And then link to the recipe for people wanting to read more.\n. https://github.com/airbnb/enzyme/ => https://github.com/airbnb/enzyme\n. Maybe linkify shallow component rendering? Not everyone might know what it means exactly.\n. $ npm install --save-dev enzyme react-addons-test-utils react-dom\n. And\n. , if you want => . If you want\n. > Check out the browser testing recipe on how to do so.\n. with Enzyme => with Enzyme,\n. you can have a look at => have a look at\n. with AVA, to get more information about => with AVA. For more information about\n. please have a look at => have a look at the\n. $ npm install --save-dev jsx-test-helpers\n. And\n. with AVA, to see => with AVA. To see\n. please have a look => have a look\n. * => -\n. I'd like to see some more structural tests.\n. Sure, that, and maybe some tests from https://github.com/sotojuan/not-so-shallow Just to make sure we never regress, even if we were to replace your lib with something else any time in the future (hopefully we won't though).\n. This is not really necessary as we already tell them to reproduce it with the latest version.\n. Good point.\n. Since you're not hiding these with HTML comments anymore we can change them back to checkboxes;\n- => - [ ]\n. locations => locations:\n. our => Our\n. Maybe add a HTML comment to tell them to remove this part when done.\n. ERROR AND STACK TRACE\nWe just got a bug report without the actual error: \nava/lib/serialize-error.js:11\n        target.stack = beautifyStack(source.stack);\n. Configuration => Config\n. We should automatically include this in the --report command\n. This too, re --report\n. I would include these in the above Environment snippet though.\n. \ud83d\udc4d \n. This is the same as the above.\n. I wonder if I should just make the slash module do this check? https://github.com/sindresorhus/slash/blob/master/index.js\n. No point in naming the function really. Modern engines can infer it.\n. I got that. I've seen this done before too, so was just curios if it makes sense to move the platform check into the slash module?\n. Don't add this. It should be in your own global gitignore, not here.\n. Would be nice to reduce the nesting below here \u2b07\ufe0f \n. I think concurrency would be a better flag name than pool-size. It's the most common name for it that I've seen. For example: http://bluebirdjs.com/docs/api/promise.map.html#map-option-concurrency\n. I personally don't remember us getting any feedback on the watch trial. It was pretty much only us on the team using it. I think we should document it as (EXPERIMENTAL). That way we're free to do whatever we want with it.\n. I think this should be a getter that returns the object. It might be an edge case, but if the consumer modifies the object, later results will also be modified. I might be missing something though and this might be a non-issue, but thought I would bring it up.\n. Use two spaces to separate the flag and description columns\n. Why are we using a callback cb here instead of a promise?\n. Maybe this?\n\nMaximum number of test files running at the same time (EXPERIMENTAL)\n. Can you also sync this output into the readme? Just copy paste node cli.js --help.\n. Just put this inline in the reporter.test call.\n. Use chalk to colorize it. Makes it easier if we ever want to change the style.\n. Should have been clearer. I meant instead of the ANSI escape codes.\n\njs\n'test ' + chalk.gray.dim('\u203a') + ' my test')\n. The outer chalk.gray.dim( can be removed now.\n. This is intentionally left out as it just duplicates our readme and repo description.\n. Sure\n. It's kinda weird only having a . after the first sentence.\n. We might want an ESLint rule for this, either way.\n. https://github.com/sindresorhus/eslint-plugin-xo/issues/26\n. @jamestalmage Good thing we decided not to pass the context as the second argument \ud83d\ude1d\n. if (providedTitle) {\n. var => const\n. Should we maybe mention why it's beneficial to use test macros over hand-made loops/etc? (To make static analysis possible === better perf).\n. You need to escape the value here.\nhttps://github.com/sindresorhus/escape-string-regexp\n. This needs to be above relative imports, so on line 9.\n. Use caret ^\n. Nah, we have enough meta files. \n. This would have been the perfect place to use ES2015 Set if we could...\n. **Note**: If the `--fail-fast` flag is specified, AVA will stop after the first test failure and the `.always` hook will **not** run.\n. Should be underscored as it's not something we'd like people to use.\n. js\nconst base = fs.readFileSync(path.join(__dirname, 'base.d.ts'), 'utf8');\n. @jamestalmage I this correct? I thought any of the modifiers could be the function.\n. @jamestalmage Doesn't matter. It's for internal use only.\n. Haven't tried, but I don't think it will work. The reason you can use binaries in dependencies is that when npm install binaries it creates a \"shim\" on Windows that makes it behave like Unix.\n. Typescript => TypeScript\n. Drop of configuration\n. I would add a note that even if you see warnings about optional dependencies failing, it will still work fine. People have tendency to just blindlessly open issues when they see a warning of any kind.\n. Also doesn't need a linebreak. Just continue on line 62.\n. warnings about optional dependencies failing, => warnings about optional dependencies failing during install,\n. > AVA uses is-ci to decide whether it's in a CI environment using these environment variables.\n. I think we should be clear that it's those services that are being bad and not AVA. They shouldn't hang AVA just because it's concurrency greedy.\n. @jamestalmage No, they should just handle it with throttling, but very few do...\n. Sure. I just want to make it clear they're working around the service, not AVA, when they follow this.\n. Same with arrow functions, let, etc, used here. Would be much easier to just require('babel/register') this script.\n. The above 3 options can just be put as defaults in the objectAssign call.\n. Yes, we should recommend promisifying, but still show both solutions.\n. Put this at the top. Better for it to be defined before it's used.\n. Is there anything we can request from Node.js to make this easier?\n. Why? What is it for? I can't see it being called here at all.\nNode => Node.js\n. Great! This should have been done regardless of browser support :)\n. ES6 => ES2015\n. json\n\"presets\": [\"es2015\", \"stage-2\"]\n. jspm => JSPM\n. Loader => loader\n. NPM => npm\n. if => If\n. No need for empty line here.\n. Don't align the from.\n. js\nfunction fn() {\n. single-quotes, tab indent, and semicolons\napplies to all the code examples\n. Packages => packages\n. Don't do unrelated changes.\n. Kinda funny that the issue number is 404 :p\n. comment on => comment in\n. I opened an issue about this: https://github.com/avajs/eslint-plugin-ava/issues/122\nWe should really try to detect this at runtime and in our ESLint plugin too.\n. Actually, we could just use babel-node here instead of node. We just need to add babel-cli as a devDependency. Then we could drop this boilerplate.\nhttps://babeljs.io/docs/usage/cli/#babel-node\n\nThis will also fix running it on Node.js 0.10 which is currently failing and would let you remove the polyfills in https://github.com/avajs/ava/pull/884/files#diff-148e5d484548912a06d9d0dd7cd4f891R19\n. I would name it:\n\nTesting hot observables\n. Ava => AVA\n. Clarify here what #take() is.\n. > Installing RxJS 5\n. Use semi-colons, tab indentation.\n. Ava => ava\n. tests => test\n. > Tap reporter\n. Mention that it's the default reporter.\n. Mention that it can be selected with the --verbose flag.\n. Needs to be added to the Table of Contents.\n. Also need to update the link in \"Why AVA?\" then.\n. I would use isRunning instead.\n. hasStarted?\n. Let's go with that then.\n. Can you add a heading # Experimental Features?\n. concurrency:5 => \"concurrency\": 5\n\nto make it copy-pasteable.\n. Can you linkify the text here to the relevant issue?\n. I know it's linked below, but would be useful to linkify it here too.\n. precompile:true => \"precompile\": true\n. Worth adding a note in the Babel recipe about this and linking here?\n. typescript => TypeScript\n. > Support max two arguments, and mark them optional (?) - this means that the amount of arguments isn't checked; a macro with 2 arguments can be run without arguments, and the other way around\nI think this would be the best solution. At least it would then typecheck the first two arguments. We should open an issue to improve this when https://github.com/Microsoft/TypeScript/issues/5453 is fixed.\n. Why should it return full stack?\n. Should also test to make sure it correctly strips off moot stack info.\n. w/o => without\n. w/o => without\nand drop the should\n\nsuccessfully initializes without any options provided\n. I think sinon handles overriding console.error and restoring.\n. Description needs to be clearer about when it should return empty string. Drop the should prefix.\n. should return => returns\n. w/o => without\n. w/o => without\n. Using process.chdir is almost always a bad idea. It changes the CWD globally and can cause race issues and all kinds of problems. See: https://github.com/avajs/ava/pull/985#discussion_r73789258\n. Should maybe switch to https://github.com/sindresorhus/resolve-from here and pass in the custom cwd.\n. Name it cwd?\n. This should use indexOf instead. You're actually not using a regex in the search() method.\n. I don't think this is needed. You're already restoring it above.\n. Probably need to switch to stub here too.\n. This should be moved before the t.end() call.\n. Same in other places.\n. >= 0 => !== -1\n. I think /ava/cli.js:27:11 should be ava/cli.js:27:11\n. js\nvar stubModulePath = path.join(__dirname, 'fixture/empty');\n. > runs the profiler and throws an error when invoked without files to run\n. js\nt.ok(/Specify a test file/.test(err.message));\n\nAlso note, no need for the g flag.\n. Yes, that's probably what it usually is printed as.\n. > node_modules/ava/cli.js:27:11\nThis\n. Can you add a comment referencing https://github.com/nodejs/node/issues/6624 for posterity. Otherwise someone will come by and try to simplify the code without understanding what it's solving.\n. There's no point in having the g flag here.\n. Fixed it for you when merging.\n. Is there any plan to have this builtin in Flow? Kinda weird having to define it manually in every type definition.\n. @develar Unrelated, but would be nice if the user could just specify the binary name, like ava, instead of the full path, like they can in npm run script. Pretty easy to do. You can see how it's done here: https://github.com/sindresorhus/npm-run-path/blob/master/index.js\n. There are a bunch of t.end(); that needs to be removed. Like line 391: https://github.com/avajs/ava/pull/1014/files#diff-adeeb9de91f4da36bd570d73ef0d19a1R391\n. > Disable Power Assert\n. Double-negatives are usually bad, better to make it powerAssert: cli.flags.powerAssert !== false,\n. Might be worth making it an options object instead of arguments.\n. Yeah, the CLI part should be kept as-is. --no is the best way to handle negative flags.\n. > It will help you use AVA correctly and avoid some common pitfalls.\n. What if the user names their test or function fn?\n. No point in using process.hrtime(). Date.now() is more than accurate enough for this purpose.\n. Would be better as a simple ternary:\njs\nvar pkgDir = filepath === null ? process.cwd() : path.dirname(filepath);\n. I don't really see how this improves anything though. It was already clear what the options are for.\n. I don't see the point of extracting this either. It has the same title as the option name, so I don't see how this makes it any clearer.\n. js\noptions = options || {};\n. What change made this necessary? Maybe worth using onetime here?\n. Can you move this to the the top? Better to have it defined before use.\n. If base is unclear, I think we instead should rename it in RunStatus.\n. This could just be pretty-ms instead.\n. That's a very verbose module name. Why not simply browser-env? Node is already implied.\n. Should use import, not require.\n. Use import\n. node => Node.js\nfront end => front-end\nloads of => lots of\n. It's pretty easy. You just bump major on the existing one and deprecate with a message saying it's been renamed. We bump major first so the deprecation notice doesn't annoy existing users until they bump a version. I would personally do it as that module is going to be used a lot and browser-environment would look nicer in the import statement, but your choice ;)\n. Actually, https://github.com/CamShaft/browser-env is deprecated, so you could just request the module name.\nhttps://docs.npmjs.com/misc/disputes\n. @lukechilds Not very relevant in this situation, as it's a top-level module, so maybe not necessary, but when your module is deep in a dependency tree, people will start opening issues on every module that depends on it somewhere in the dependency tree about updating, which is very annoying for the maintainers and a lot of wasted time. Users think warnings are critical.\n. You also need to remove the no-so-shallow dependency here.\n. js\nvar avaPath = require.resolve('../');\n. dir => cwd\n. It needs to handle a possibly rejected promise too.\n. \ud83d\udc4d Ah true. Forgot node-tap got support for promises.\n. Drop the following: To override this behavior and run tests from a *different* folder, you can set theresolveTestsFromoption to a relative path, which will be used instead of the package file directory.\nIt doesn't make sense here as it's for the API, not the consumer. I don't think we want it documented at all as the API is not stable.\n. You can't use object literal shorthand. We still support older Node.js versions.\n. Instead of nesting the below in a function, maybe we should split it out into a new file and just have this file as a conditional loader CLI?\njs\nif (localCLI && path.relative(localCLI, __filename) !== '') {\n    debug('Using local install of AVA');\n    require(localCLI); // eslint-disable-line import/no-dynamic-require\n} else {\n    require('./cli');\n}\nAnd rename this file to _cli.js or something.\n. Actually, just noticed. This does nothing. It should either be executed as a function or imported as loud-rejection/register.\n. They're different as you changed from tabs to spaces. They should remain spaces.\n. I'll fix it on merge, but try not to do unrelated changes when doing a PR ;)\nYou could have uncommitted that part btw. Would be easiest to use a GUI git app for that. I use GitUp.app.\n. Yes, in case of rejected promises, since we don't manually catch them all. This is easier.\n. Why use .map if you're not actually going to map anything?\n. This is incorrect. It should be:\njs\ncli.flags.color === false\n. I think chalk.enabled = false should be enough.\n. Actually. None of this should be needed as chalk already supports the --no-color flag.\n. What actually needs to be done is to pass the flag down to the forked process.\n. \"color\": false\n. snap => snapshot\n. Don't hard-wrap\n. Should be no space before console.\n. > AVA runs tests in child processes, so to debug tests, you need to do this workaround:\n. Put it on the previous line.\n. Remove space\n. Don't titleize\n. * => -\n. The description are unaligned now. Same with the one in the readme.. Put a 'use strict'; at the top, above this.. react => React. (t) => t. js\n. it'll => it will. you can use => you can use the. `x.get` => `exports.get` and drop the `module.exports`. Fix the issue instead..js\nimport {test} from 'ava';\n```\nAnd use semi-colons.. Don't do unrelated changes.. Why?. Extention => Extension\ntypescript => TypeScript. \ud83d\udc10 . AVA error: `--extensions=ts` require TypeScript to be installed.. typo in comment. It should throw an useful error here.. This is an internal API. We can just make passing the extensions argument required and drop this if-statement.. As we discussed in https://github.com/avajs/ava-files/pull/9#issuecomment-261962080, the option should override js.. Test file extension, for example `ts` for TypeScript (Can be repeated). It should throw an useful error here.. We should not expose these internal arguments match and snapshotStateGetter to the outside. Instead you could expose an internal base method.\n```js\nx._snapshot = function (tree, match, snapshotStateGetter) {\n...\nx.snapshot = function (tree) {\n``. It should also have an optionalmessageargument like the other assertions.. It should also include how to handle the snapshots. Should they be committed or gitignored.. Yeah, that's weird, I don't get that failure locally either. Just fix it though.. Should be added to the belowNON_ENHANCED_PATTERNSas this enhancement replaces the output, which we don't want here.. The message argument should be in addition to the default output, not replace it. Probably best to prepend beforemessagePrefix.. It should say that Yarn is a better alternative to npm.. It should not replacenpm`, but be a recommended alternative.\nLike:\nWe recommend using Yarn bla bla.\nyarn install\nAlternatively with npm:\nnpm install. Don't do unrelated changes.. Same as above.. yarn => Yarn. Don't hard-wrap.. using [Yarn] => [using Yarn]. alternatively => Alternatively. I agree it could be shortened, but not like that.\nShould be:\nTest file extension (e.g. `ts`) (Can be repeated). @vdemedes Agreed. I usually don't use switch unless there are lots of same if's.. Use console.error\nAVA error: => AVA:\nSame with the below code.. --extension ' => --extension='. Needs a test to ensure it support reading user TS config file.. Needs a watcher test for TypeScript.. Just export this directly.. Doesn't ts-node already do this?. Remove comment.. You can use String#repeat() now.. Mistake?. Use this.options.failFast instead and you don't have to pass it on https://github.com/avajs/ava/pull/1160/files#diff-f8849ae9f0614ba9464fdd14c2d98dc0R163 as it's already passed in the API constructor.. it's => its. expandn => expands. It's not immediately clear that ignore and only refers to Babel options.. \ud83d\udc4d . So does this mean that compilation of sources are now true by default without any config?. Instead of wrapping every msg, you can just add the validation to the create function.. It should fail if msg is anything other than a string, not gracefully handle it.. Good :). No, that's a common issue when using template strings like that. You either need to just remove the indent, which looks weird, or use something like https://github.com/sindresorhus/redent In this case, I don't think it's worth it.. 1. Message should always be a string. Would be better to invert the check and put the throw in the if-statement. Also: assertion => Assertion. I simply meant:\n```js\nif (msg !== undefined && typeof msg !== 'string') {\n    throw ...\n}\nreturn ...\n``. Shouldn't this benotJsxEqual?. \ud83d\udc4d . I agree about it looking uglier, but better to be consistent and always havenotfirst. In a perfect world we would use snakecase in JS.... You forgot to change this back ;). Put'use strict';above here.. @novemberborn Do you remember why we have laziness here? There must be a reason. Maybe we should preserve the laziness?. Yes, go with for-of whenever possible.. @novemberborn By default, if the user doesn't specify either--coloror--no-color`, it's not set. \nIf we add the following, it will be true if not specified, but then we wouldn't know when it's not specified and should be auto-detected. So I wouldn't do this:\njs\nconst cli = meow(``, {\n    boolean: ['color'],\n    default: {\n        color: true\n    }\n});. I would go with color instead of disableColors though. Double-negatives are not very good.. You have a typo. You're also importing the same thing twice.. @vadimdemedes What's the intention here? Upstream those changes? We cannot depend on a git branch, so we'll either have to upstream or you need to publish your fork to npm.. When looking into documenting these, I realized they're exactly the same as t.deepEqual(). So what is the point of these?. Why?. Can you remove these then? And in the other places.. Can you include the link in the comment. Usually comments are great for explaining \"why\".. Ok. I like that idea. Can you remove the jsxEqual stuff for now and open a new issue about jsxEqual? We can add it back when we have a better story for it. It was pretty unrelated to this pull request anyways.. I agree it could be more succinct, but I like how it includes the reason for why it's not available. Users don't really like being told something and not knowing \"why\".. Feel free to improve it :). > Watch mode is not available in CI, as it prevents AVA from terminating.\n\ud83d\udc4d . This is incorrect. You're no longer testing the error message. The third argument is just an assert failure message.\nYou could do:\njs\nt.throws(() => {\n    runner.todo('todo', () => {});\n}, new TypeError('`todo` tests are not allowed to have an implementation. Use `test.skip()` for tests with an implementation.'));. Add a link to this recipe in the AVA readme at https://github.com/avajs/ava#transpiling-imported-modules. Result => result. Result => result. Why are we setting this both here and in the constructor? Wouldn't it make more sense to only set it in the constructor?\nSomething like this:\njs\nchalk.enabled = this.options.color;\nfor (const key of Object.keys(colors)) {\n    colors[key].enabled = this.options.color;\n}. let => const. @Qix- This is needed because we assign shortcuts to the colors in lib/colors.js (context https://github.com/avajs/ava/pull/1104#issue-187486491) Any way we could make the above loop not nessecary? Maybe if we made .enabled property on the prototype?. @ThomasBem Ignore this ;) Unrelated to this PR, but rather to chalk.. > but it caused this test to fail because chalk and colors did not seem to work as intended, https://github.com/avajs/ava/blob/master/test/cli.js#L73\nI think that's as expected as we used to force Chalk in child processes and we no longer do. I guess we can just remove the colors.error() call from that test.. See: https://github.com/avajs/ava/pull/1198#discussion_r98352347. I'll add the esnext: true option, which enforces const, when https://github.com/avajs/ava/pull/1154 lands (hopefully in the next few days), as it's the last piece of non-ES2015ified code.. Oh, I didn't notice this removes it from the actual code. In https://github.com/avajs/ava/pull/1198#discussion_r98352347, I was talking about removing the chalk call from the test, not elsewhere.\n\nIs this because execCli spawns a new process and the chalk.enabled and colors.enabled stuff is not passed on?\n\nNot exactly. Colors used to be forced \"on\", now it's auto-detected and colors are automatically off when spawned.. It accepts an optional message too, like all the other asserts.. Ditto. \ud83d\udc4d . '  ' => '\\t' :trollface: . I would really prefer not introducing more options. Can we make it __snapshots__ if inside a __tests__ directory, otherwise snapshots?. Super nitpick, but would prefer err.name here just for consistency with the other props and more future proof if we decide to change the error type or something.. I wonder, should we put this logic in serializeError so it handles reporting about unserializable exceptions everywhere?. Would look better to use async function here and throw instead. Same with the below fixture.. js\nt.true(chalk.enabled);. Why should it have complained?. I think we need to handle quotes.\nFor example:\nmessage: 'hello' this is not valid YAML\nWouldn't it be better to just use a real YAML stringifier? Like js-yaml. ### => ####. Should it really wait until all tests are finished before warning about this? I would think would call for an immediate error even if the user didn't pass --fail-fast.. I just tried and you're right. It is the existing behavior. I still think user mistakes like this should fail the test right away no matter what flags the user used though, but that's a separate discussion.. These fixtures are transpiled with Babel, so you can drop the 'use strict'; and use import.. Why not just a for-of loop instead of extracting the iterator?. Time to use an object for the arguments?. This feature, while useful, is going to hit a lot of people. Any chance we could add support for assertions throwing AssertionError?\nI know we have https://github.com/avajs/ava/issues/1094, but that's more about integrating with t and t.plan(). I sometimes use assertions helper, and would like to use them in AVA without having to do anything extra.. \u2764\ufe0f This commit is an amazing improvement! I remember trying to achieve something like this in AVA 0.0.2, but I could not get it working. Probably because beforeExit didn't exist then.. Could we maybe have it on by default, but disable it if the user imports any of the popular assertion libs? That way we can have a good default, but also support people using external assertion libs without them having to configure something.. Me neither. Maybe not worth it. We can discuss in a new issue after this is merged. Let's go with on by default for now.. I would have gone with incorrect instead of improper, but not important.. This is exactly what Promise#try is useful for.. I bit unrelated, but I usually only use inline arrow functions when I actually make use of the return value. This makes it clear whether something is returned or not. What do you think of such approach?. We need to change this to AVA Snapshot v1. We don't want to mislead people with being fully compatible with Jest and it's also confusing for AVA users that it doesn't say AVA.\nWe might need to open an issue on Jest about getting an option for this, but for now I guess we can just read the file in, modify it, and write it out again.. @novemberborn Can you open an issue on Jest to make it configurable? They seem open to making it more generic when needed.\nI'm happy to land this as is for now, but long-term we'll want our own naming and versioning.. Too bad, would have been nice to reuse that code.\n// @cpojer. I really don't think this should be ok. Many people do the mistake of using setTimeout inside a sync test and it silently passing. If users need to do this they should either use an async function or test.cb().. Ah, ok.. You can also push an empty commit or force push, to trigger a rebuild.. > avoid OS-specific issues\nWhat kind of issues?. Vue.js. Vue.js. Remove trailing commas. Make it clear that this is ./test/helpers/setup.js. Use tab indentation. Would be cool if someone made all this a module, so you could just do require('ava-vue-setup'); and be done.. So let's go with 16 bit from the start then? Then we don't have to document such limitation and never have to worry about it.. Nitpick: Use template literals. The snapshot version should be defined in the schema too.. I was thinking so we could warn about different versions, but seems like the best practise is not to version them, so I guess having it in the Snapshot field makes more sense: http://stackoverflow.com/questions/8519381/how-does-protocol-buffer-handle-versioning. This is not the correct way to check. AVA can be run with either ava --concurrency=1 or ava --concurrency 1.. the maximum number of test files to run with. => the maximum number of test files to run at once.. No, that wouldn't work if the user had concurrency defined in package.json.\nThe correct check would be:\njs\nif (cli.flags.concurrency === '') {\nAs it would be an empty string if the value is not defined, as we already define concurrency to be a string in the meow config.. This is unrelated to what you're trying to show.. standard && too.\nAnd put the reporter option as package.json config instead CLI flags. (See the nyc docs).. ./ => .. bash => console. Watch => watch. It's not used by .travis.yml. Read the above .travis.yml code.. Incorrect indentation here.. @blake-newman Any updates?. Cool :). You can't use default arguments. We still support Node.js 4.. I would extract {runOnlyExclusive: false, updateSnapshots: false} into a variable and use it in all these asserts. Would make it easier to add additional options in the future.. From the Vue docs:\n\nNew in 2.1.0+: returns a Promise if no callback is provided and Promise is supported in the execution environment. - https://vuejs.org/v2/api/#Vue-nextTick\n\nSo you can just use an async function for the test and await it here.. The .then() here is moot. You're already await'ing it. Just get value back the async/await way.. ES6 => ES2015. > should the indentation apply on those new lines?\nYes. > Could you add a comment explaining the logic here? Like, why use 2 on single-core machines?\nWill do. It's to make it still concurrent on VMs that report only one CPU: https://github.com/sindresorhus/grunt-concurrent/commit/ef076acedd2bfcac84f9ce038d84af884b12d841 Does that make sense? It might not, I dunno. Edge-case anyway, so can easily remove it.\n\nWhen would os.cpus() return an empty array?\n\nSome years ago someone reported an issue about it returning an empty array for a VM or something. I can't find the issue now, so let's just remove it and see.. \ud83d\udc4d Fixed. Actually, I'm just going to remove it. Too edge-casey and magic.. You're not assigning this to anything.. It's just a way to signify that there should be more content than just this in the file. Any better suggestion?. Use a for-of loop instead of forEach.. babel => Babel. Can you fix the incorrect indentation?. Promise => a Promise. Promise => a Promise. or `function` that return rejected `promise`.\n\u2b07\ufe0f \nor `function` returns a rejected `promise`.. This sentence needs to be better worded.. return => returns. ComonJS => CommonJS. import {* as ava} from 'ava' => import * as ava from 'ava'. You don't need to prefix the file path with ./. It works without too.. must be an instance => must be an instance of the constructor. Since we're being strict, can we only support null? Doesn't make sense to specify an empty object.. Typo. Incorrect indentation here and it other places.. Only things related to the project should be in the project gitignore. The rest should be in your own global gitignore. \nhttps://gist.github.com/subfuzion/db7f57fff2fb6998a16c. Don't do unrelated changes.. This last */ is not aligned.. Incorrect indentation and missing a }. @novemberborn Didn't we discuss and conclude that there's no point in supporting module.exports?. I think think // eslint-disable-line no-global-assign is needed.. That's a good point. I don't care strongly. Just thought I'd mention it.. \ud83d\udc4d . No need to use os.EOL. Windows also supports \\n for console output. Even console.log in Node.js uses \\n: https://github.com/nodejs/node/blob/34bd9f318a164d6b2ee949793146b85b3e152d5f/lib/console.js#L141-L148. Using .toString('utf8') on chunks is an anti-pattern, as the chunk might be split on a Unicode code point, resulting in an invalid character. The correct way is to do setEncoding('utf8') on the stream. That is also more efficient.. Can you use a more descriptive name thanevt?. Can you make0x0Aa local constant, so it's clear what it is?. With latest Ora, you can checkthis.spinner.isSpinninginstead ofthis.spinner.id: https://github.com/sindresorhus/ora/commit/3df8d0d962b3af9aa5c269ed9ac3d21734b34d32. Not relevant to this PR, but we still wanted to type it to return anErrorinstead ofany, right? Per https://github.com/avajs/ava/issues/1794. ```js\nprocess.stdout.getColorDepth = () => options.tty.colorDepth;\n```. Agreed. Done.. Couldn't the sorting here be part ofchunkdtoo? Maybe as an opt-in option.. If *what* is present?. ?. Maybe also include the contents ofmyFuncsomewhere as it's not that clear whereTypeErrorcomes from.. You can use the// @ts-ignorecomment to ignore the below line.. Btw, Git now supports settinglf` for all text files (since 2016, but I didn't realize until recently):\n* text=auto eol=lf. I don't really understand the question. We already enforce LF on checkout for JS, so this will only affect other text files in the repo. I see no downside of this, and it's what I have been using in my repos for a while now.\n* text=auto eol=lf will replace line 1-4.. Nitpick, but I think the sentence should still end in a . even though there's an emoji.\nsuggestion\nTesting can be a slog. AVA's here to help you get it done. Its small API, detailed error output, embrace of new language features and process isolation let you write tests more effectively. So you can ship more awesome code. \ud83d\ude80. I think we should also mention that it's a JavaScript test runner. Imagine someone stumbling upon this project knowing nothing about AVA.. This is almost perfect. Maybe:\n\nTesting can be a slog. AVA helps you get it done.\n\nMaybe we could also add something about AVA making it more fun / faster / less painful to do tests.. I had never heard about the word slog before. Is there any other more common word we could use?\n\nWriting tests can be boring. AVA helps you get it done.\nTesting can be a bore. AVA helps you get it done.\nTesting can be a pain. AVA helps you get it done.\nTesting can be a drag. AVA helps you get it done.\n\nIf not, slog is fine. Just thought I'd bring it up.. small API => small and intuitive API ? Or maybe: concise API. > Testing can be a slog. AVA helps you get it done so you can spend more time shipping awesome code and cuddling with your pet.. This is for the next XO version. See: https://github.com/xojs/eslint-config-xo/pull/52. This should also document why one would want this and what problem it solves.. We only document Babel 7.. This is a fragile way to test for a node_modules directory. It will also match paths like foo-node_modules-bar.. ",
    "kevva": "VVV is sexy.\n. Renamed since error reporting etc will happen in a reporter. Let's convert the output to be TAP compliant first.\n. Hm, yes I agree that it shouldn't be used as default (for the user), but maybe just under the hood. My thought was having it always write TAP output but use a default reporter, looking like the one we use now, if nothing else is specified.\n. Alright, that makes it a little easier too :). Let's focus on getting everything else to work nicely first and add TAP support later on.\n. Yeah, can be really high or disabled by default.\n. Wish this could be in .true() and .false() but it might be hard to have a second optional argument with msg being a string too.\nBut I'm +1 for this. We can change the name later on.\n. Sort of closed in https://github.com/kevva/claim/commit/7e2270a63e130aaa9e38443a91532a8ab28a070d.\n. Good idea. Guess we need to add a method in runner like .runSerial() or something. And then rewrite some of index.js so we can run the correct run method.\n. Not a big fan of promises in general, but due to it's popularity this might be a good idea :+1:.\n. Fixed in https://github.com/sindresorhus/ava/commit/ae4c2667632b730fc016cd4dc18a2ab3c29dafef?\n. Yeah, if you look at the stack we produce we could mostly (don't know if that's the case in throws and doesNotThrow) just remove the first line in it to get the correct line and file.\nBut yeah, we could go the tape way too.\n. Yeah, I like option two too because that's really what is expected. Using the project root would confuse me.\n. Yeah, I like option two too because that's really what is expected. Using the project root would confuse me.\n. Really exciting stuff.\n. I'd leave it as an option too, although I'm usually hitting ctrl +c whenever I see a fail test locally if there are time consuming tests (like in pageres). But most of the times, the tests are finished before that.\nAlso, +1 on fail-fast or something like it that makes it's intention clear. bail has never been that obvious to me.\n. I'd leave it as an option too, although I'm usually hitting ctrl +c whenever I see a fail test locally if there are time consuming tests (like in pageres). But most of the times, the tests are finished before that.\nAlso, +1 on fail-fast or something like it that makes it's intention clear. bail has never been that obvious to me.\n. Browser support in squeak is fixed in https://github.com/kevva/squeak/commit/fd7333e343b1ff5f73cd14736ccc049c45dead3b.\n. > ... this PR needs to be updated to only include the exit change.\nSo what should we decide on? Just check for process.exit and do nothing if it doesn't exist?\n. Aren't we setting some kind of limit of forks? We should probably not create more forks than there are CPUs.\n. I'll try work on this tomorrow. Definitely needed.\n. Not so sure I agree with this. You'd still have to compile your files with Babel afterwards so why not setup a npm script to compile them before tests? See this for an example, and then require the compiled code in your tests.\n. Yeah, it seems like we're running afterEach after every after too. I don't know if that's intended @sindresorhus @vdemedes or should we change it?\n. FWIW, these are the lines causing it https://github.com/sindresorhus/ava/blob/master/lib/runner.js#L87-L89. Not sure what's the nicest way to solve this. I guess we could pass an option to _runTest or something.\n. Don't we have this already?\n. I usually use nock for testing API endpoints/HTTP requests. Looks something like https://github.com/kevva/download/blob/master/test.js#L13-L23.\n. No color or yellow/orange imo.\n. > Sorry I deleted my comment asking about the color because in the tests for the mini reporter it uses cyan for skipped.. Though I think for the actual reporting it should be yellow or no color as you say!\nYup,  but I think yellow makes more sense depending on how we want to treat skipped tests as.\n. Nice work @sotojuan. And I agree with the yellow color since that's more \"warningy\", i.e. tests shouldn't really be skipped for no reason.\nlgtm :+1:\n. :+1: \n. This is super nice.\n. :+1: I believe everyone agrees with this.\n. Matching them would be pretty easy since you'd just have to check their buffers. Comparing/diffing them is another beast.\n. @adriantoine, yup. Preferably :).\n. > I think @gajus\u2019s situation qualifies as \u201clegitimately preventing\u201d things getting done.\nHaving a npm server that throws 400 on unknown fields is hardly a legitimate reason for introducing a .avarc file imo.\n. > See #637, #520...\nNone of those issues contains a problem which is preventing users from getting things done (which makes this one an edge-case) and they are both answered why we're not having a .avarc file.\n\nThis is not going to get implemented until someone can demonstrate an actual need.\n\nUsers preference is not an actual need. Unless npm suddenly starts preventing users from defining custom fields in their package.json (which they never will), I don't see a need for adding a .rc file ever.\n. > .eslintrc, .babelrc, .jscsrc, web.config, cosmic.config, webpack.config, rollup.config and the list goes on\nThat's also a point for not having one. I feel like all those dotfiles will only pollute your project.\n. I didn't say it needed to be giant. I guess I don't use as many tools (or as much configuration) as you are. If I compile using babel I'll only need a few presets and for linting I'm using xo which I rarely need to configure.\nI respect that you're using other tools and it's fine if there are rc files for them, but ava doesn't come with a ton of config and in most cases you only need to override some of the options.\n. Not sure I follow you here. How could they get the wrong mock?\n. If I understand you correctly, the .persist() method in nock should fix this? I just added it to download. That'll make it totally non-dependent of the tests running in a particular order etc. That way you don't need to run beforeEach too.\n. The response is always the same when using nock though since no actual operations are made AFAIK. That changes if you're using a real server for mocking though.\n. So you want the endpoints to sort of depend on each other to replicate a real use case (like getting your friends, add one and then get them again)? Then yeah, that'll be hard, but I don't know if that's how it's expected to work. Isn't the point just to test that your endpoints returns the expected data, not to test the real \"flow\" of your API.\n. > - listFriends returns correct response if person has no friends\n\n\nlistFriends handles one friends\nlistFriends handles two friends\nlistFriends handles more than 100 friends (paging limit)\nlistFriends handles a 401 (unauthorized) code by asking for credentials\n\n\nAlright, that clear things up :). I was just assuming that you wanted one typical response from your endpoint, not testing all the possible responses, but I agree that's a good use case.\n. Maybe we should take this into account in ava-files though?\n. I think we should add it to the globs instead. It's probably less prone to fail.\n. > If nothing else, we can just check for Node versions...\nIf the purpose is to fix the test, I would just check the Node version in the test and assert accordingly.\n. I think this was added in V8 5.1.39. These links might be worth reading:\nhttp://www.2ality.com/2015/09/function-names-es6.html\nhttps://bugs.chromium.org/p/v8/issues/detail?id=3699\nhttps://bugs.chromium.org/p/chromium/issues/detail?id=588803\n. Yup, sounds good to me. Especially since this is a feature in ES2015 and not something we want to circumvent.\n. So from what I could gather from #1198, we pass on --color/--no-color to the worker to enable/disable for example chalk.red('unicorn') in the test files. If we skip it and passes on whatever flags the user passes on, chalk will be disabled in the test files (because it's not a TTY) unless the user explicitly runs ava with --color.\nSo, do we want colored text in test files to always display as colored?. > Yeah, basically, we want to pass on the auto-detect color support to the child worker, which always detects it as no color support, without clobbering process.argv for the test file context.\nWe already do that, I think. The only function --color/--no-color currently fills is passing the auto detection into the actual tests (not the reporter) which I don't think we should do. If the user wants to log stuff in their tests using chalk, just run ava with the --color flag.. This has been discussed in length in multiple issues already, e.g, https://github.com/avajs/ava/issues/520.. @novemberborn, I'm going to address your comments soon :).. Ok, so I've changed the PR to only forward the color/no-color flag if it's set, and I'm also setting chalk.enabled in the worker.\nShould we include support for different levels as in https://github.com/avajs/ava/pull/1455 too?. Some tests are failing now because they relied upon checking the ava options forwarded as arguments before.. > My take, if --color is set, it should be forwarded in exactly the same way. If it's not set, it shouldn't be forwarded.\nReason I didn't implement it that way is because it would've required some sort of command line arguments parsing, and it wouldn't look too good using meow in there. I extracted the command line parsing out of meow though, and created an option that I think would fix this use case https://github.com/kevva/argvments#any.. You should use throw new Error(string) in your code. Read more about the Error object here https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Global_Objects/Error#Throwing_a_generic_error.. I'm aware of that, but it's a bad practice.\n\nAnd/or should strings thrown be caught by .throws ?\n\nThere has been some discussion around it in https://github.com/avajs/ava/issues/661 and a new proposal for throws in https://github.com/avajs/ava/issues/1047.\nWill close this as a duplicate.. Maybe some of these issues are of any help https://github.com/avajs/ava/issues/1079 https://github.com/avajs/ava/issues/1109?. Looks like an issue with babel-plugin-lodash. This comment describes the same issue https://github.com/lodash/babel-plugin-lodash/issues/188#issuecomment-322873062.. Thanks \ud83d\udcaf . LGTM, thanks!. @danny-andrews, yes, if you add something like this to your ava config:\njson\n\"babel\": {\n    \"testOptions\": {\n        \"configFile\": \"./babel.config.js\"\n    }\n} . @novemberborn, wouldn't just providing configFile: true in defaultOptions work? Then it'd fallback to Babels own behaviour. And yeah, needs some tests too.. Don't think it's specific to @sindresorhus local environment. Passes locally for me too.. Yes, it does. http://www.appveyor.com/docs/environment-variables\n. I don't see the difference.\n``` js\nconf.files = 'foo.js';\ncli.input.concat(conf.files);\n//=> ['foo.js']\narrify(conf.files);\n//=> ['foo.js']\n``\n. Maybe you meant thatava just-testing.jsalways should take precedence if you want to quick test something and not bother having to updatepackage.json:P. Then I agree.\n. Nitpick, but an empty line between these (after the title and the text),\n. Add empty line.\n. Add empty line.\n. I don't think that's possible afaik.\n. One linebreak too much?\n.JavaScript=>js. I preferimport {fn} from 'module':without spaces.\n. Same here, without spaces is nicer imo.\n.JavaScript=>js.import {noop, renderJSX, JSX} from 'jsx-test-helpers':.const actual = renderJSX();(notice removal of the space in closing tag).\n.{'Click Me'}. t.pass()}/>.const wrapper = shallow();.const wrapper = shallow();.t.true(wrapper.contains());.const wrapper = shallow( t.pass()}/>.. Add a linebreak between line70and71.\n.if (this.runVector)orif (this.runVector !== 0)imo :).\n. Remove this blank line :)..chalkisn't used anywhere.. Yeah, I did this just to make [this](https://github.com/avajs/ava/blob/master/test/fork.js#L144) test pass since it doesn't run the CLI.. To forward them *exactly* like they were received requires some sort of parsing. We can forward--no-colorinstead of--color=false` if it was specified though.. Oh, escaped my eyes.. ",
    "vadimdemedes": "Working on this, PR very soon ;)\n. @Qix- I mostly agree with you on the points you mentioned, but I think there should be at least ava.skip() method for one simple reason. To avoid constant comment/uncomment loop when you need some test to temporarily not be executed. .skip() method should not be a seen as a way to make failing test suite to succeed, it should be seen only as a convenience method.\nAVA should still display the skipped test, but make it loud and clear, that this test was skipped.\nActually, your conversation gave me an idea on how we could implement one more unusual feature in AVA. Code example is worth a thousand words, so here it is:\n``` js\nvar test = require('ava');\ntest('regular test', function (t) {\n  t.end();\n});\ntest.warning('text of the warning', 'failing test', function (t) {\n  t.true(false);\n  t.end();\n});\ntest.skip('skipped test', function (t) {\n  t.end();\n});\ntest.todo('test to implement soon', function (t) {\n  t.end();\n});\n```\nOutput:\n\nI call that thing a test modifier. It modifies when a test is executed and whether it should be executed at all. I have these modifiers in mind:\nskip\nSkip a test completely:\njs\ntest.skip('some test', fn);\nwarning\nExecute a test, but also display a custom warning message on the side:\njs\ntest.warning('this test has some weird shit going on', 'some test', fn);\nwhen\nExecute a test, when testFn returns true:\njs\ntest.when(testFn, 'some test', fn);\nbrowser\nExecute a test only in browser environment. Useful for libraries, that support both node and browser, but also need to test some specific cases only in browsers.\njs\ntest.browser('some test', fn);\nnode\nOpposite of .browser():\njs\ntest.node('some test', fn);\ntodo\nMention, that this test needs to be implemented. Useful when you come up with a some condition you need to test, but have no time for it right now. Test is not executed, but its title is displayed in \"TODO\" section at the end of AVA's output (see screenshot above).\njs\ntest.todo('test to be implemented', fn);\nLet me know what you guys think!\n. I know this is something very new and unusual, but so is AVA! It needs to be different, otherwise it will end up with no major advantages over tap/tape (aside concurrent execution). I am not pushing on it, let's discuss!\nI am thinking, that this will make tests more verbose, and as a result, more clear and understandable.\n. @tunnckoCore the idea behind test modifiers - is to modify something related to a test. Like when it executes or if it executes at all. If those modifiers are inside t, we won't be able to influence test execution. It would also introduce inconsistency in the API. What's inside a test function, should only be related to the test body, not its description.\n. @MadcapJake +1 for .unless(). What you described about .critical(), is exactly the same as .before() (it's already implemented). \n@Qix- .when() is the same as .node() or .browser(), with a predefined testFn. Last two are basically presets. What do you think about ava.test.skip()? If you liked .todo(), then you also agree with .skip() (they are identical). \n. @MadcapJake I think .before() tests, that don't block, should just be \"regular\" tests :D .before() is meant to be used for preparation, so if a preparation fails, a test suite fails too.\n@Qix- @sindresorhus Ok, let's skip conditional tests for a while. If someone will show a real example when they're needed, we'll review them again, but with an actual project.\n@sindresorhus I see the point of .critical(), could be useful, agree.\nSo, let's proceed with .skip(), .warning() and .todo().\n. @MadcapJake No no, the tests are atomic. But the .before() preparation does not have to do anything with testing functionality, it is just a preparation step. \nHere's a real-world .before() use-case. I was writing a etcd client. There's also an etcd server. To test my client, the server must be running. Before my tests, I use .before() to start a Docker container with etcd server inside. I also use .after() to stop this Docker container.\n. Yeah, reporters are an important feature to implement. But I think we're not \"there\" yet, there are some issues that have a much higher priority.\nBut once those are resolved, I am 100% on board with implementing custom reporters.\n. @sindresorhus do you still want to work on this? I'd like to assign this to myself :)\n. What do you think about this?\n\n. @tomekwi You're welcome!\nI see what the problem is, will try to not forget and fix it!\n. So, here's my results and thoughts regarding browser support implementation (#887).\nOverview\nTo run tests in the browser, AVA provides --browser flag, which:\n1. Precompiles and assembles test environment (AVA itself) and tests using browserify\n2. Starts an HTTP server, index page of which runs tests and displays results\nTests in browser\nAs all of you know, AVA runs test files in an individual, isolated environment (a new node process) with ability to configure concurrency. In the browser, AVA offers the exact same functionality by taking advantage of Web Workers. For each new test file, AVA spawns a new Worker. Communication between main thread and workers happens via standard methods provided by browser: onmessage and postMessage. Those are basically identical to process.on('message') and process.send in Node.js.\nWatch mode\nWatch mode will be supported by leveraging SSE (Server-Sent Events) or WebSockets. AVA process, that's running in the console, will watch for changes (same as now), only with --browser flag it will assemble a new bundle of tests and notify browser to download & run new tests.\nBabel\nAVA's Babel-related functionality remains in place. Files are pre-compiled when building a bundle, so babel is not included in the bundle and served to a client.\nUnder the hood\nAbove mentioned PR tries to re-use the more code as possible from AVA core. These new files needed to be added:\n- browser-index.js - the same index.js, but uses Web Worker APIs\n- browser-fork.js - the same fork.js, but spawns Web Workers instead of node processes\n- browser-test-worker.js - the same test-worker.js, but does not use require hooks\nThere's also a browser.js, which serves as cli.js, but for the browser. It reads configuration, talks to API and listens for events from the AVA server.\n. We should also implement #709 to disable Babel, in case users want to test cross-browser compatibility. Not entirely, but only remove ES6/7 presets. Our plugins, like power-assert, should remain untouched.\n. Yeah, I forgot about DOM in Web Workers. I think I need to switch that PR to iframes, unfortunately.\nAlthough with iframes debugging of tests would be complicated, because we'd remove the iframe after tests finished and its output wouldn't be visible in the \"main\" page.\n. Coming Soon!\n\n. TAP support is ready, waiting for #279.\n. Added code to display test duration only when a threshold (100ms) is reached. If you want, you can easily change it here: https://github.com/sindresorhus/ava/commit/2791234f08d8d474ae7558522e366b253a741443#diff-168726dbe96b3ce427e7fedce31bb0bcR31\n. @sindresorhus of course, will add a test for sure\n. @sindresorhus Added a test for measuring duration, let me know what you think!\n. Thank you @sindresorhus, will definitely contribute more!\n. Currently, ava does not have a single config store, so we have to come up with some way to share configuration between scripts. Right now, I can see 2 ways to implement this, without major code changes:\n1. ava --debug sets environment variable, so that other scripts (files) see it: process.env.AVA_DEBUG = true. Also, it will be possible to enable debug mode by AVA_DEBUG=true ava.\n2. Have a config.js with module.exports = {} and set config values there (all scripts that require('./config') will get the same object).\nWhat do you think?\n. Config would also be handy for #27 \n. @Qix- Sorry, did not understand your question\n. Yes, child processes will get modified process.env object (node.js docs say so).\n. @sindresorhus I was talking about the ways to indicate, that user specified --debug flag and wants tests to run serially. For example, as for now, how would lib/test.js know, that cli.js has debug = true?\n. :+1: Also tired of join(__dirname, 'fixtures', 'shit')\n. Yeah, looks good to me too, except Runner.prototype.run.\n. @sindresorhus Yeah, sure, already on it ;)\n. @sindresorhus You're welcome! I am really enjoying contributing to ava and got, incredible stuff. Switched almost all my shit to them from request and mocha.\nP.S. That gif is amazing :D\n. Sorry, should've clarified things more. No major changes are required to make this work and the API would remain exactly the same.\nExample test with generators:\n``` js\nconst test = require('ava');\nconst fs = require('mz/fs'); // promisified fs\ntest('generator support', function * (t) {\n  let content = yield fs.readFile('/path/to/some/file', 'utf-8');\nt.same(content, 'expected content of the file');\n});\n```\nTo support generators, co module would be used, because it's the most mature and simple. I've been using it for almost a year now and never encountered any issues.\nAlso, because Ava already supports promises, I expect generator support to be very easy to add, because co returns a simple promise to run a generator function:\n``` js\nconst co = require('co');\nfunction * exampleTest () {\n  // body of generator function\n}\n// to run a generator, simply pass it to co\nco(exampleTest).then(function () {\n // generator function completed execution\n}).catch(function (err) {\n  // generator function threw an error\n});\n```\n. I believe I will have some free time to build a PR today\n. As a workaround, I currently do the following:\n``` js\nconst ava = require('ava');\nconst co = require('co');\ntest ('generator fn', function * (t) {\n  // wow\n});\nfunction test (title, fn) {\n  ava(function () {\n    return co(fn.apply(null, arguments));\n  });\n}\n```\n. @sindresorhus \nWhat's needed to do in AVA?\nAVA would need to detect if a test is a generator function using is-generator-function and if it is, convert it to a promise using co. And that's it, since AVA already supports promises.\nWould AVA have to have co as a dependency?\nYes, co would need to be added as a dependency.\nWith the recent builtin ES2015 support, wouldn't generators just work out of the box?\nNope, they wouldn't and here's why ;) Generators are just functions that can be run step-by-step:\n``` js\nfunction * generator () {\n  yield 1; // blocks until .next() is called\n  yield 2; // blocks until .next() is called\n  yield 3; // blocks until .next() is called\n// all done\n}\nlet fn = generator();\nfn.next().value; // 1\nfn.next().value; // 2\nfn.next().value; // 3\nfn.next().value; // undefined, function completed\n```\nThis functionality in its \"plain\" form does not give us many benefits, but the brilliant co library uses generators to allow writing asynchronous code in a synchronous style. Normally, you would pass generators to co, but it can also accept promises:\n``` js\nconst co = require('co');\nfunction * generatorFn () {\n  return 'some value';\n}\nfunction promiseFn () {\n  return Promise.resolve('some value');\n}\n// yield does not work outside of generators\nco(function * () {\n  let first = yield generatorFn(); // 'some value'\n  let second = yield promiseFn(); // 'some value'\n});\n```\nIn practice, generators allow me to avoid code like this:\n``` js\nUser.findOne({ email: 'test@test.com' })\n  .then(function (user) {\n    let name = user.get('name');\nconsole.log('My name is', name);\n\n})\n  .catch(function (err) {\n    // error\n  });\n```\nand write code like this:\n``` js\ntry {\n  let user = yield User.findOne({ email: 'test@test.com' });\n  let name = user.get('name');\nconsole.log('My name is', name);\n} catch (err) {\n  // error\n}\n```\n. The thing is, nobody uses async/await yet and god knows when it will land in node.js. But generators are widely used today and there are many packages for them. There is also a wonderful Koa framework, specifically for apps that use generators.\nAlso, it will not add any overhead to AVA, because the implementation would be fairly trivial. I will send a PR later today to demonstrate this, so you can see the end result ;)\n. @tunnckoCore I am not completely sure what is all this complexity for, if generator support could be added in a few lines:\njs\nif (isGenerator(fn)) {\n  fn = co(fn);\n}\n. @MadcapJake yield with co is basically the same as async/await:\n``` js\nfunction promiseFn () {\n  return Promise.resolve('some value');\n}\nasync function asyncFn () {\n  return Promise.resolve('some value');\n}\nyield promiseFn(); // 'some value'\nawait asyncFn(); // 'some value'\n```\nYou would use generators in a test just like you are using promises or anything else now, nothing is changing really:\n``` js\ntest ('content of the file', function * (t) {\n  let content = yield fs.readFile('/path/to/file', 'utf-8');\nt.same(content, 'expected file content');\n});\n``\n. And we also need to disableregeneratortransform inbabelto not convert generators into some ES5 crap.\n. Don't know why, but I knew you would say that, haha\n. @sindresorhus this PR is ready for you to review, let me know what you think ;)\n. @sindresorhus Right, good catch, missed that\n. Thank you @sindresorhus ;)\n. @sindresorhus Trying to figure out the right way to testindex.js. It is a bit problematic, since it exposes only functions to add tests, so I can't test it withoutchild_process.exec. What do you think?\n. @sindresorhus Pushed critical fixes and updated es6 test to useindex.js, instead oflib/test.js` directly. Also, pageres tests succeed now:\n\nBut now we have another problem. There's no continuous output, meaning that all output is displayed at the end, when all tests finish. This happens, because Promise.all (https://github.com/sindresorhus/ava/pull/39/files#diff-b3b53682a18f203ac8d29b0e277cad26R56) resolves only when all promises are resolved.\n. First thing that I came up with to fix that is to use each-async again, instead of Promise.all. Maybe you have some other solution in mind?\n. @sindresorhus Oh my God, I know how to fix this, never mind ;)\n. I came up with a much better solution and without use of each-async ;) Just a few seconds and you'll see the commit!\n. Here's that change: https://github.com/sindresorhus/ava/pull/39/files#diff-b3b53682a18f203ac8d29b0e277cad26R52.\nThe logic here is to output test results immediately, as they are available. But, use Promise.all to return from Runner#concurrent, when all the promises are resolved (tests are completed).\n. @sindresorhus @Qix- Thank you, guys!\nYes, I am definitely interested in joining AVA! I really like it and would love to contribute more!\n. Looks good. I'd only change the text example to a test with some no-brainer assertion, not t.pass():\njs\ntest(t => {\n  t.same([1, 2], [1, 2]);\n  t.end();\n});\nAnd I'd also change import test from 'ava' to var test = require('ava') to make things completely straightforward. I think, not everyone is familiar with ES6 module system.\n. Oh yes, this is awesome. I don't think this feature would have any disadvantages. \n. In that case, we need to support generators (#37). I can work on this!\n. @Qix- Great catch!\n. @sindresorhus @Qix- @kevva Any thoughts on this PR?\n. @sindresorhus Implemented IPC with process.send.\n. For some unknown reason tests fail on Travis with node 4.0.0. On my laptop, they run just fine with the latest node:\n\n. Pushed a change, which now shows output from forked processes. Before, all output (console.log, process.stdout.write) was hidden, when tests are executed via ava command. Now developers won't be confused, why their console.logs are not showing up.\n. Yes, this would be a nice option to have.\nI would leave this as an option, because AVA is a tool that enforces writing atomic tests. So, one failing test does not mean that all the other ones will fail too. But as an option, it is going to be useful for sure.\n. @jenslind First of all, thanks for PR! How would you implement a support for should interface?\njs\na.should.equal(b);\nshould.not.exist(c);\n. @jenslind In my opinion, t.should, t.expect, t.assert is really not a desired solution for supporting custom assert modules. It shouldn't assign those modules to a test context. All it has to do is to patch them with an assert count for t.plan() to work.\n@sindresorhus what do you think?\n. I'd implement something like the following.\nTo use should:\n``` js\nvar should = chai.should();\nvar ava = require('ava');\nava.should(should);\n```\nTo use expect:\n``` js\nvar expect = require('chai').expect;\nvar ava = require('ava');\nava.expect(expect);\n```\nor any other module, which does not require custom magic like should or expect does:\n``` js\nvar assert = require('assert');\nvar ava = require('ava');\nava.assert(assert);\n```\nTo avoid duplication, we could prefix those methods with setup:\njs\nava.setupShould(should);\nava.setupExpect(expect);\nava.setupAssert(assert);\n. Good one!\n. Agree with @mattdesl, on client-side process.exit() should do nothing or send an exit event, but definitely not close a window.\n. @mattdesl this happens, because we want to cover a case when test is being executed directly.\n$ node test-something.js\nSo if only CLI would use process.exit(), in the above case process would exit always with zero (success).\n. Weird, because they all pass in a clean Docker container with node 0.12:\n\n. All tests are passing now after travis rebuild.\n. Added a test for async/await, this works too!\n. It's enabled by default in babel and thanks to generators support it works right away! I wouldn't say it's experimental and unstable, because babel just converts async/await to generator functions and they are definitely stable.\n. All done.\n. Well, if we are executing babel per file, we should just set file name to only property.\nwhitelist property is not for files, it is a list of transform to include.\n. @sindresorhus Does passing a string to only, instead of a pattern, works?\n. I considered using Bluebird.coroutine.addYieldHandler() at first. We would need to detect type of input and return another coroutine or promise afterwards. So I decided to ditch it and go with battle-tested co.\n. Honestly, I don't think it's a good idea. It does not look good, it will cause bugs and it does not make much sense to use test as an alternative to describe in mocha.\n. @Qix- all good points, I should not submit PRs after 10pm. Will review this again tomorrow, thanks!\n. Added changes to babel.js to detect if generators are supported. If they are supported, babel's gonna use asyncToGenerator transform and blacklist regenerator transform. Before that, tests on 0.10 and 0.12 failed, because they don't support generators.\n. @sindresorhus This PR is related to issue #53, which is \"Prefix test results with filename when running multiple files\". So this output is expected. When running a single test file, I don't think we need prefixes, since direct path is specified.\n. https://github.com/sindresorhus/ava/blob/5dc0873b74d59a3f3cc33a36020c78cb305153aa/test/test.js#L262 this test is not related to this PR (this PR won't fix this problem).\n. Good point, I think we should switch. @sindresorhus what do you think?\n. @sindresorhus Are you sure AVA is ready for this feature? We have lots of perf/babel issues on the table to solve. Watch feature is not that important like providing great experience (especially first experience) to the users.\n. @sindresorhus Ok, sounds like a plan ;)\n. I am now thinking that these hooks must run serially, just like before() and after().\n. I am now thinking that these hooks must run serially, just like before() and after().\n. Closing this (until fixed) as I discovered more issues.\n. Closing this (until fixed) as I discovered more issues.\n. I will add info to readme when this PR is complete (by \"complete\" I mean discussed and everything is agreed) ;)\n. I will add info to readme when this PR is complete (by \"complete\" I mean discussed and everything is agreed) ;)\n. Glad you like it!\nWhat's the reasoning for that? Since the test depend on the beforeEach hook, and if that fails silently, the test might end in a false positives.\nJust did not implement that yet, maybe I should include it in this PR ;)\n. Glad you like it!\nWhat's the reasoning for that? Since the test depend on the beforeEach hook, and if that fails silently, the test might end in a false positives.\nJust did not implement that yet, maybe I should include it in this PR ;)\n. Updated to fail when beforeEach hook fails ;)\n. Updated to fail when beforeEach hook fails ;)\n. Added info to readme\n. Added info to readme\n. throws() expects a function you are executing to throw. This will not work here, because async/await is converted to generator functions, thanks to asyncToGenerator babel transform. So, your example is transformed to this:\njs\ntest('throws error', function * (t) {\n  t.throws(yield fn(), Error);\n});\nFor t.throws() to catch your error, it needs to check if promise rejects and that's not supported at the moment.\n. throws() expects a function you are executing to throw. This will not work here, because async/await is converted to generator functions, thanks to asyncToGenerator babel transform. So, your example is transformed to this:\njs\ntest('throws error', function * (t) {\n  t.throws(yield fn(), Error);\n});\nFor t.throws() to catch your error, it needs to check if promise rejects and that's not supported at the moment.\n. @sindresorhus Yep, let's!\n. @sindresorhus Yep, let's!\n. Submitting a PR in a minute ;)\n. I think it's worth explaining why it does not work. When using promises in AVA, it is advertised that t.plan() is not needed, because test finishes when promise resolves/rejects. So that's exactly what happens here, async functions are converted into promises by babel. So when a promise resolves, the test is done here and it does not wait for data event.\n. Looks good to me too! Thanks @floatdrop ;)\n. I checked and babel takes the most time to require. I'm planning to migrate AVA to Babel 6 (#117), will see if there are any time wins.\n. PR updated\n. \n. @sindresorhus Updates pushed\n. Thanks!\n. You shouldn't pass regex as a second argument to t.throws(), second arg is error class. \n. Oh, my bad then, never mind. I will look into this issue. \n. I agree with @kevva. \nI don't think this PR will get merged. In the related issue, @sindresorhus said that you shouldn't rely on built-in babel, it is just there for tests. Therefore, people disagree on this feature. \n. @paazmaya Thanks for reporting this!\n. This is fucking amazing.\n@sindresorhus, agree on the simple output, looks way better!\n. @twada is babel-plugin-espower compatible with babel 6?\n. Cool!\n. @sindresorhus should we leave this as a feature request? Or advise to use:\n$ time ava\n. @kevva No, we only report individual test time, when it reaches certain threshold. We don't display total test time atm.\n. @floatdrop in order to support the stuff that \"generator community\" is used to, co is required. Babel's built-in implementation is very limited.\n. @floatdrop generators are only being used in node 0.11, iojs and node v4.x projects, so we don't have to worry about that.\n. @floatdrop Hmm, did not understand the point. I am just saying that nobody uses generators on the old node.js versions.\n. Because co implements some very nice features, which babel's regenerator does not implement. Babel's implementation only makes Promises yieldable, but nothing else.\nWhy don't we use @floatdrop's fixed fork for now, until they merge his fix into master?\n. Good to go! \ud83d\udc4d\n. I'd handle this issue like this:\n1. beforeEach/afterEach can't have titles, never display them\n2. before/after can have titles and display them only when they exist. If before/after hook does not have a title, don't display it.\nIf any of the supported hook methods fail, display it like usual: x [anonymous]: error.\n. If a beforeEach/afterEach hook fails, we could show something like: beforeEach for 'test title' failed\n\ud83d\udc4d\nWhy? In what cases is that needed or useful?\nI was thinking, that having titles for beforeEach/afterEach will pollute the output, but titles for before/after won't do much noise. That's the only reason ;)\n. I can think only of one use-case: I have many before/after hooks and having a title helps me find quicker which one failed. We can't give them friendly titles, like for beforeEach: beforeEach for \"something\" failed.\n. hahaha, I know that, that's why I wrote \"find quicker\" :D\n. I'm fine with either options, you just asked for a use-case and here it is :) Let's see what else might come up in this thread.\n. So what's the status on this?\n. I think it's good to go!\n. @sindresorhus Yep, correct!\n. Sure, we can close it for now.\n. Sure, we can close it for now.\n. This is the way I solved it in my side-project. Run tests under istanbul, check how coverage changes between tests and create a mapping of test -> depending file line. After the initial test run, which creates a mapping for all tests, I see what lines in my project have changed. And then I run only tests that \"touch\" those lines.\n. AVA 0.3.0 removed support for node test.js, now you have to use ava CLI to run your tests: ava test.js. Let me know if it works.\n. @Ivan-Feofanov Could you please open a new issue with a more detailed description?\n. So, basically, this PR could be cleaned up by:\n1. Attaching kill() on the returned Promise, not the results object\n2. Don't resolve here - https://github.com/jamestalmage/ava/blob/jt-kill-forked-processes/lib/fork.js#L39, but resolve/reject based on the exit code:\nps.on('exit', function (code) {\n  // resolve/reject\n});\ninside https://github.com/jamestalmage/ava/blob/jt-kill-forked-processes/lib/fork.js#L36\n. So, basically, this PR could be cleaned up by:\n1. Attaching kill() on the returned Promise, not the results object\n2. Don't resolve here - https://github.com/jamestalmage/ava/blob/jt-kill-forked-processes/lib/fork.js#L39, but resolve/reject based on the exit code:\nps.on('exit', function (code) {\n  // resolve/reject\n});\ninside https://github.com/jamestalmage/ava/blob/jt-kill-forked-processes/lib/fork.js#L36\n. I don't understand your point. I am just suggesting to resolve/reject based on exit, not results. The forked processes will exit anyway, we are just detecting it a bit differently.\n. I don't understand your point. I am just suggesting to resolve/reject based on exit, not results. The forked processes will exit anyway, we are just detecting it a bit differently.\n. I think I will merge this PR and pick it up from then, I still have some tips to clean up the code.\nThank you for your work!\n. I think I will merge this PR and pick it up from then, I still have some tips to clean up the code.\nThank you for your work!\n. Honestly, I'm not sure about this one. I agree with @Qix-, it is a bit weird, that failed assertion does not throw. I realize it would be handy in some cases, but it also has its downsides.\nImagine I have a test with Promise A, 5 assertions, then another Promise B inside and 5 more assertions. If it does not fail early, the whole code will be executed and I'll get a ton of errors just for one test. And where I'll start looking to fixing it? Right, from the first failed assertion. \n. @jamestalmage I liked your idea on this:\n\nIt might be cool to print all the assertions with dots and x's:\n\nBut, there's one reason it is just not possible with AVA - Parallelism. AVA executes all tests in parallel, so there's no way we can modify previous output, because other tests might've displayed in the meantime.\n. @jamestalmage that is correct, my oversight ;)\n. I was just thinking about displaying the assert progress live.\n. Could you update it to:\njs\nreturn fn\n  .catch()\n  .then()\ninstead of shortcut form?\n. @sindresorhus this applies if we have .then() and .catch() in that order.\n.then(yes, no) is a shortcut for .catch(no).then(yes).\n. @schnittstabil https://github.com/petkaantonov/bluebird/wiki/Promise-anti-patterns#the-thensuccess-fail-anti-pattern\n. My suggestion is to convert this:\njs\n.then(function () {\n            x.throws(function () {}, err, msg);\n            }, function (fnErr) {\n                x.throws(function () {\n                    throw fnErr;\n                }, err, msg);\n            });\nto this:\njs\n.catch(function (fnErr) {\n    x.throws(function () {\n        throw fnErr;\n    }, err, msg);\n})\n.then(function () {\n    x.throws(function () {}, err, msg);\n})\n. Ok, let's merge this and we'll see what we can do later.\n. lgtm\n@sindresorhus?\n. @SamVerschueren Could you please create a minimal reproducible example and publish it as a gist?\n. @SamVerschueren Could you please create a minimal reproducible example and publish it as a gist?\n. Awesome, thank you.\n. Awesome, thank you.\n. test.only does not exist at the moment. We're having a long discussion on test modifiers at the moment - #9.\n. test.only does not exist at the moment. We're having a long discussion on test modifiers at the moment - #9.\n. Closing via #204.\n. I think this will introduce more garbage in the lib/test.js and confusion about what to do (test.sync or t.plan or t.end) among AVA starters.\n.sync is 5 characters, t.end(); is 7 characters. So it's not really a shorthand, imo.\n. Why do we need to wait for the process to exit? results event is being sent after all tests are done.\n. Why do we need to wait for the process to exit? results event is being sent after all tests are done.\n. Ok, I'll push an update, thanks for the tip!\n. Ok, I'll push an update, thanks for the tip!\n. I assume this is safe to close since #136 is in master?\n. I assume this is safe to close since #136 is in master?\n. I think this test is kind of complicating it, since we already have a test for this. What situation does this test checks?\n. Just not understanding the purpose completely. Could you please explain?\n. Ok, let's see what @sindresorhus thinks.\n. In my opinion, AVA shouldn't exit with failure, if there were no tests in a file. There was no failure, so why indicate the failure?\n. I think just a friendly warning message, that there were no tests defined would be good.\n. > The child process should always send a results message though right? Even if it had zero tests?\nYes, and according to index.js it should work now that way. Need to confirm this.\n\nMy point is that if the process exits without sending the results message, then something happened that prevented ava from sending it. That is out of the ordinary and should be an error.\n\nThat is correct, an error should be displayed in that case. But this issue is about files with zero tests, so it's unrelated here.\n. Copy-pasted your code and ran with ava#master:\n\nAre you using master branch or installing via npm?\n. There were some changes in the way we set context after the release (I think), so as I see, this issue is resolved in master.\n. This is happening, because when first t.pass() is executed, AVA exits the test and treats it as finished. We had a TODO test for this case before, but removed it because we could not find a good & stable solution for this one.\n. I agree with @rmg, t.end(4) does not really address this issue. \nThere's simply no way to detect, that more assertions are coming. We could just go mocha-way and display an error, if more assertions are happened after ending the test. That's the best we can do here.\n. Glad you like AVA!\nCould you please provide some examples, what kind of events are you interested in?\n. Sounds reasonable to me, I think it'd be great to have this anyway.\n@sindresorhus @kevva what do you think?\n. Could you please attach some code to both of these?\nI just tested the first one and it works as expected, even when I don't set error argument:\n``` js\nimport test from '.';\ntest(t => {\n        t.throws(function () {\n            throw new Error('damn it');\n        });\n    t.throws(Promise.reject(new Error('damn it')));\n\n    t.end();\n\n});\n``\n. Oh no, let's not change the name ofthrows().rejectslooks like it supports only promises andraises` is from Ruby.\nAll we have to do, is to fix & stabilize issues pointed out in the first comment.\n. @sindresorhus I'd leave it, but rename to just t.regex. To me it looks a bit more convenient (less nesting of statements):\njs\nt.regex(/ok/, 'oh ok');\nt.true(/ok/.test('oh ok'));\n. @sindresorhus oh, in that case, definitely, regexTest should be deprecated and correct regex should be implemented.\n. What gulp-ava version are you using?\n. Just tried gulp-ava@0.2.0 with an official example (https://github.com/sindresorhus/gulp-ava) and it works. I assume you linked some future version of AVA into your directory.\n. I just realized we are in ava repository, not gulp-ava. So, this is unclear why it's failing now. Could you please prepare a set of steps to reproduce your error and open this issue over at https://github.com/sindresorhus/gulp-ava? Thanks.\n. Closing this issue as it's already implemented.\n. How about a live code playground?\n. How about a live code playground?\n. @sindresorhus We'd have a text editor on the left side and live output (result of running ava) on the right side. That way we could demonstrate our nice stuff, like super dope assertions.\n. @sindresorhus We'd have a text editor on the left side and live output (result of running ava) on the right side. That way we could demonstrate our nice stuff, like super dope assertions.\n. Although we'd need AVA to have support for browsers.\n. Although we'd need AVA to have support for browsers.\n. @TrySound what versions on node were you using?\n. @TrySound and yesterday...?\n. Everything looks good to me, very straightforward, thanks @BarryThePenguin!\n. Tests are failing, could you take a look?\n. I just realized something. Why don't we just let forks exit themselves, without interaction with a parent process?\njs\nprocess.send(results);\nprocess.exit();\n. @markthethomas AVA is supposed to be fast, but that has nothing to do with user's tests. 500 ms is too small imho, I think we'd better choose tap's default timeout - 30 seconds.\n. > Rather it should be an idle timeout, e.g. we're still waiting for at least 1 test and it's been 30 seconds since the last test completed.\nIt is actually a very good idea, @novemberborn!\n. Still interested!\n. Still interested!\n. I think this information should not belong in the AVA's Readme. Project's readme is about project, not about explaining concepts. What I would do here, is to link to that stack overflow answer, instead of copying it.\n. Could you please post a few lines of your code?\nRelated: #32 \n. @markthethomas gave a good answer on this one, I think it's safe to close.\n@mattfysh If you have some questions regarding AVA, feel free to chat in our gitter room.\n. I like this idea, thank you for submitting it. @sindresorhus what do you think?\n. Except suggestions in comments, everything is good.\n. Hey @timoxley, could you please fix this suggestion (https://github.com/sindresorhus/ava/pull/181#discussion-diff-44397790), so that we can merge it in? Thanks!\n. Closed via https://github.com/sindresorhus/ava/commit/403a28c8cebf9a8ff736aaaf2c4cb79ca30c5a47\nThanks @timoxley!\n. Really good idea! Well done @jamestalmage \n. Really good idea! Well done @jamestalmage \n. That's actually a nice idea! Although I think it should be postponed until we are more close to 1.0 release.\n. That's actually a nice idea! Although I think it should be postponed until we are more close to 1.0 release.\n. Babel changes constantly, we can't publish source code and hope that after npm install code will be successfully transpiled and work fine. Also, when we want to test AVA on 0.10 and 0.12, we'll need to transpile that code before running the tests.\nThe most important concern is: what benefits could writing the code in ES6 bring to AVA?\n. It'd also be good to do the same for npm test too, those dots don't really give an idea of what's going on.\n. @sindresorhus PR updated ;)\n. \n. \n. Couldn't agree more!\n. I was, I was! Now my day is complete :D \n. t.end is not meant to be used with generators, promise and async functions, so I don't really see a problem.\n// @sindresorhus \n. Not sure about this, I think we have this clearly written in the docs, that users don't need t.end() with promises.\n. @sindresorhus lol yeah, but I think it's easier to point them to the docs, than to put \"guidance\" or \"tutorials\" in the source code :)\n. Everything looks good so far, I pointed out some things in the diff. One question I have in mind, should only tests be executed serially on in parallel? Btw, mocha allows only one only test, so maybe we should also do that? And it would solve the serial vs parallel question too.\n. > Mocha allow several only modifiers. For example, if there are 10 test cases, and 3 of them are marked as only, those 3 test cases will be run.\nI guess I was running some out-dated version then.\n. @lijunle Could you please rebase your PR (to pick up latest fixes for windows tests) and address comments in the diff? Happy to merge this ASAP.\n. Yeah, the comment is not really needed and this - https://github.com/sindresorhus/ava/pull/204/files#r45035490\n. The refactoring and cleanup work is ahead, so don't worry about this 1 line ;)\n. @lijunle Thank you!\n. As for me, I don't think it's worth doing it at the moment. As @sindresorhus stated, it only applies to \"async\" errors/exceptions. For now, we have more important stuff to focus on. Maybe in the future, who knows.\n. I am now wondering if it's not just easier to rename .spec suffix from file names. That way we won't tailor AVA to every possible use case out there.\n. But for now, @sindresorhus and I think that this is an easy fix'n'win, so I guess we'll just merge this PR ;) \n. @jankuca could you please fix the comments in this PR?\n. @jankuca Never mind, I made the changes myself ;)\n. I think it has some potential as a feature request. Something like describe in mocha. At the moment, we'd like to merge currently open PRs and do a complete cleanup/refactoring, and then continue with feature development.\nIf you'd be interested in making a PR, that'd be great. I don't recommend you to do that now, because after the rewrite, your PR will have a lot of merge conflicts.\n@sindresorhus what do you think?\n. Here's a recap of all proposed APIs, I think it'll make it easier to pick a winner and continue the discussion.\n1. test.group() with nesting by @novemberborn\n``` js\ntest.beforeEach(t => {\n  t.context.forAllTests = true;\n});\ntest.group('some nested tests', () => {\n  test.beforeEach(t => {\n    t.context.forGroupTests = true;\n  });\ntest(t => {\n    t.true(t.context.forAllTests);\n    t.true(t.context.forGroupTests);\n  });\n});\ntest(t => {\n  t.true(t.context.forAllTests);\n  t.falsy(t.context.forGroupTests);\n});\n```\nPros:\n- API is easy to learn/use and is already familiar to most people\nCons:\n- Nesting - tests are harder to find (see https://github.com/avajs/ava/issues/222#issuecomment-160516588)\n2. Groups with no nesting and ability to mix together by @jamestalmage\n``` js\nvar bluebird = test.group('bluebird');\nvar pinkie = test.group('pinkie');\nbluebird.beforeEach(t => t.context.Promise = require('bluebird'));\npinkie.beforeEach(t => t.context.Promise = require('pinkie'));\nvar both = test.group(bluebird, pinkie);\nboth.beforeEach(...);\nboth.test(...); // common test\nbluebird.test(...); // bluebird specific test\n```\nPros:\n- No nesting, continues to maintain \"AVA style\"\n- Flexibility and hooks reuse\n3. Groups with nesting and overriden test function by @matthewbauer\n``` js\ntest.group(test => {\n  test.beforeEach('before test a but not test b', () => {});\ntest('test a', () => {});\n});\ntest('test b', () => {});\n```\nSimilar to nr. 1, except test is overriden inside each group (which may be a good thing actually, for an easier implementation).\n4. .before() and .after() test modifiers by @spudly\n``` js\nconst setup = t => {\n  t.context.realCwd = process.cwd;\n  process.cwd = () => '/fake/path';\n};\nconst teardown = t => {\n  process.cwd = t.context.realCwd;\n};\ntest(t => {\n  t.is(getCwd(), '/fake/path');\n}).before(setup).after(teardown);\n```\nPros:\n- Explicitness\n- Probably least complex implementation\n- Minimum of the new API introduced\nCons\n- Need to repeat .before() and .after() for each test. But, with those modifiers implemented, it may be very easy to implement something like t.group(), which applies these hooks to all nested tests. It would even make it possible to implement as a separate module, outside of AVA core.\n5. test.fork() without nesting by @novemberborn\n``` js\ntest.beforeEach(t => {\n  t.context.forAllTests = true\n});\nconst refined = test.fork().beforeEach(t => {\n  t.context.refinedTestsOnly = true;\n});\ntest(t => {\n  t.true(t.context.forAllTests);\n  t.false(t.context.refinedTestsOnly);\n});\nrefined.test(t => {\n  t.true(t.context.forAllTests);\n  t.true(t.context.refinedTestsOnly);\n});\n```\nPretty much similar to nr. 2, but with .fork() instead of .group() (I find latter more understandable) and without mixing of groups (less complexity -> less bugs and maintenance - good thing).\n\nNow that we have proposals side by side, hope it makes it easier for everyone to choose what they prefer.\n. @floatdrop could you please confirm?\n. I did not have a need for something like that for now, but if you find it useful, why not. But still, I don't think it should be included on npm. Perhaps leave it as an open PR?\n. Oh wait, I have an idea. Why don't we use debug in AVA? It will allow for an extremely easy debugging and a simple way to enable it. Plus, we won't need to ship/include one more file.\n$ DEBUG=ava ava test.js\n$ DEBUG=ava:fork ava test.js\n$ DEBUG=ava:* ava test.js\nAnd it's been more or less a standard way to enable logs for a certain module (check debug readme).\n. js\nfork.on('message', function (message) {\n  // log message\n});\n?\n. It will capture everything that a forked process sends via process.send(), won't it?\n. Nope, but somehow maintaining.md ended up in your PR. Rebase gone wrong?\n. Anyway, I think this PR should be closed in favor of using debug utility, so don't bother fixing it.\n. I like this change, @sindresorhus what do you think?\n. > Would exposing the Node.js binary --require flag while disabling our Babel hook solve your problem?\nPerfect solution for this issue. I have nothing against this, should be clean to implement ;)\n. Please, try to write better titles and describe your PRs properly. There's no rush and any bonuses over creating PRs fast. From this title and lack of description, it seems that this PR demonstrates some issue, stating that AVA should fail, but it's not.\nIt would be great, if we did not have to look at changed files to get a sense of what is this PR about. Thanks.\n. And I see that this PR contains the same code as #231, is it a duplicate?\n. Could you post some example test code to reproduce this?\n. The output on Travis is a bit crappy - https://travis-ci.org/sindresorhus/ava/jobs/91561792. Although faucet is really cool.\n. Regarding this tap-spec issue #231, is there a way I can reproduce this? I wrote a failing test and tap-spec exited correctly with 1 exit code.\n. Damn, still can't reproduce. My test file:\n``` js\nvar test = require('tap').test;\ntest(function (t) {\n    t.end();\n});\ntest(function (t) {\n    var obj = {};\n    obj.child.value = 1;\n});\ntest(function (t) {\n    t.end();\n});\n``\n. Ok, got it, I was usingtapinstead oftape. Maybe we should just usetap` in AVA tests? I can't find the difference between those too.\n// @sindresorhus \n. Great, so let's just use tap and tap-spec reporter.\n. Instead of:\njs\nvar tap = require('tap');\nvar test = tap.test;\nyou can just:\njs\nvar test = require('tap').test;\n. $ tap -t 0 test/*.js should solve the timeout issue.\n. tap -t150 is not a good way to specify a value. It should be tap -t 150.\n. Yeah, global timeout should be there, was just pointing out a better way to set an argument value.\n. Sure, that works too!\n. Thanks!\n. @jamestalmage thanks for the tip!\n. No no, I want to clean up everything, all files and tests, then move on to refactoring.\n. Unfortunately, code interferes between files, so I can't guarantee I can leave unaffected runner.js. I was hoping to do it all in one wave, so that I don't wait for anyone else (that is why we cleaned up our PR queue) and after that, any suggestions and contributions are welcome ;)\n. I don't really understand the point of Hook class, since it returns a Test instance in test method. @jamestalmage Could you please explain?\nOtherwise, I really like the idea behind this PR.\n. I don't really understand the point of Hook class, since it returns a Test instance in test method. @jamestalmage Could you please explain?\nOtherwise, I really like the idea behind this PR.\n. @jamestalmage makes sense!\n. @sindresorhus +1!\n. I like this suggestion, we could simplify paths and remove the need for globs.\n. What if we just made them non-enumerable? So that we won't need to use this.options.assertCount instead of this.assertCount inside lib/test.js itself.\n. Oh, I get it now. delegates usage would be really good, love this module.\n. Oh, I get it now. delegates usage would be really good, love this module.\n. \u2705from me.\n. Fixed by #331.\n. @sindresorhus we discussed this a few weeks ago and agreed to support only test-wide skip(), not per assertion.\nAlthough, I see that implementation does not add much code, so I guess we can merge it in.\n. @sindresorhus we discussed this a few weeks ago and agreed to support only test-wide skip(), not per assertion.\nAlthough, I see that implementation does not add much code, so I guess we can merge it in.\n. As for me, I can't find the use-case for a verbose mode in my mind. If you could help me with that, that'd be great :) If I have 10 assertions with 1st assertion causing the rest to fail, why do we need to see the other 9? Are there any examples in some projects or other test runners, where this functionality is needed/implemented?\n. > tap/tape shows you every assertion failure.\nThis shouldn't really bother us, since AVA is not trying to replicate tap/tape, but trying to stand out.\n\nThough sometimes additional information can be helpful\n\nIn that case, verbose mode should definitely be optional and dead-clear to enable. I'm not a fan of enabling verbose mode automatically for test.only().\n. > tap/tape shows you every assertion failure.\nThis shouldn't really bother us, since AVA is not trying to replicate tap/tape, but trying to stand out.\n\nThough sometimes additional information can be helpful\n\nIn that case, verbose mode should definitely be optional and dead-clear to enable. I'm not a fan of enabling verbose mode automatically for test.only().\n. As previously, no matter how hard I try I don't see a point in continuing test execution, when test had already failed. And it will for sure confuse users, when tests continue to run after failed assertions. I think we should close this.\n. Oooh, I am not sure about this one. Our test suite will grow tremendously, if we test every ES6 feature. That's definitely up to babel project to test these. We just need to fix our babel issues and get it right.\n. Oooh, I am not sure about this one. Our test suite will grow tremendously, if we test every ES6 feature. That's definitely up to babel project to test these. We just need to fix our babel issues and get it right.\n. @sindresorhus +1\n. @sindresorhus +1\n. To be honest, I am not a fan of this kind of API. As for me, I'd rather write t.end() in every non-promise test instead of t.endIf(condition).\n. @jamestalmage I appreciate your suggestions, but I'd like to ask you not to modify WIP-kind of PRs. I already had a fix in development, but I had to cancel all my changes to merge yours in. Or at least communicate that with PR author. Thanks.\n. @jamestalmage Good suggestion about errors!\n. I'd really like to merge this now and add TAP support for the next release. I could add/convert tests in a separate PR. @sindresorhus @jamestalmage what do you think?\n. \n. I like it! That's how it should've been done in the first place ;) Thank you for this PR!\n. // @jamestalmage \n. @jamestalmage You are right! Reverting these now.\n. @sotojuan Converting 'child_process stuff to api' - right, but with a new API we can and should test more cases and have more precise tests.\n. @sotojuan Converting 'child_process stuff to api' - right, but with a new API we can and should test more cases and have more precise tests.\n. @sohamkamani I don't have them in my mind at the moment (1am), but I'm sure you will get some ideas during CLI -> API conversion :) Anyway, let's start with what we have right now. Sounds good?\n. @sohamkamani I don't have them in my mind at the moment (1am), but I'm sure you will get some ideas during CLI -> API conversion :) Anyway, let's start with what we have right now. Sounds good?\n. @sotojuan replied to your PR there ;)\n. All code examples in this thread are missing *, which actually marks a generator function, otherwise it's not.\njs\ntest('something', function * (t) {\n  yield requestAsync(...);\n});\n@alubbe let me know if that works!\n. @jamestalmage oh, what a mess...\n. > This keeps process.stdout pristine and will not interfere with the pending TAP output.\nI like this one, although option 1 seems \"more correct\".\n. > This keeps process.stdout pristine and will not interfere with the pending TAP output.\nI like this one, although option 1 seems \"more correct\".\n. Yes, beforeEach and afterEach work just like you expected, no worries ;) You are seeing such output, because AVA runs tests in parallel. It means, that Test 2 does not wait for Test 1 to complete, but both tests run at the same time.\n. Yes, beforeEach and afterEach work just like you expected, no worries ;) You are seeing such output, because AVA runs tests in parallel. It means, that Test 2 does not wait for Test 1 to complete, but both tests run at the same time.\n. AVA both runs test files and tests concurrently :)\n\nSince the tests get run concurrently how would you recommend patching global objects for tests?\n\nI'd recommend use before() and after() methods, here's the docs for them.\n\nso that I could make Promises synchronous in my tests\n\nI may not understand your goal correctly, but I advise using async/await in your tests in order to write \"synchronous looking\" code.\nLet me know if you have any more questions ;)\n. AVA both runs test files and tests concurrently :)\n\nSince the tests get run concurrently how would you recommend patching global objects for tests?\n\nI'd recommend use before() and after() methods, here's the docs for them.\n\nso that I could make Promises synchronous in my tests\n\nI may not understand your goal correctly, but I advise using async/await in your tests in order to write \"synchronous looking\" code.\nLet me know if you have any more questions ;)\n. \n. \n. LGTM\n. Thank you for your work, @jokeyrhyme!\n\n. Added test ;)\n. I also don't understand why would anyone want destructuring.\n. > Still, I think given the potential confusion, we should still guard against it.\nFair point.\n. > Still, I think given the potential confusion, we should still guard against it.\nFair point.\n. @mcmathja Great work! Clean & simple, I like it. Thank you for your contribution and welcome to the AVA project!\n@sindresorhus all good here, so that we can merge?\n. @mcmathja Great work! Clean & simple, I like it. Thank you for your contribution and welcome to the AVA project!\n@sindresorhus all good here, so that we can merge?\n. I'm not a fan of this to be honest. It does not really save any characters:\njs\ntest(function (t, context) {\n  context.key = 'value';\n});\nvs\njs\ntest(function (t) {\n  t.context.key = 'value';\n});\nThe latter actually has less characters.\n. I'm not a fan of this to be honest. It does not really save any characters:\njs\ntest(function (t, context) {\n  context.key = 'value';\n});\nvs\njs\ntest(function (t) {\n  t.context.key = 'value';\n});\nThe latter actually has less characters.\n. So where we are on this? In my opinion, shortening variable names (like c) is a bad practice, because only the original author can understand what those variables are for. It's only useful in JS1k competitions.\nI'd stick with t.context or at least t.ctx, although \"full\" t.context looks simpler and better to me.\nAnd passing context as a second argument results in 2 places where context can be accessed (what @sindresorhus said). Context is not a widely used feature, so we may use 2nd argument for something more common (e.g. maybe like done in mocha).\n@jamestalmage there are workarounds for your issue/suggestion that don't involve modifying AVA:\n1. var c = t.context at the beginning of the test\n2. t.c = t.context in the beforeEach hook\n. It is, indeed, crazy :D\n. Hmm, what if I have multiple beforeEach hooks?\n. Hmm, what if I have multiple beforeEach hooks?\n. Sorry, I don't think \"waterfall context\" would be a useful feature in AVA. Current context implementation is safe to use, because context is shared only between beforeEach/afterEach hooks (which run sequentially) and a single test. It means, that each test has its own context. If you need a global context, it'd be better to use a variable in the global scope, instead of passing it for each test.\nLet me know if I understood something the wrong way. If you have a new idea, perhaps it's worth opening another issue, so that we don't mix things up here.\nOriginally, this issue was for providing a shorthand access to a context and I think we agreed, that having 2 different ways to do 1 thing would not be beneficial for us in the long term. I assume, this issue can be closed now?\n. Sorry, I don't think \"waterfall context\" would be a useful feature in AVA. Current context implementation is safe to use, because context is shared only between beforeEach/afterEach hooks (which run sequentially) and a single test. It means, that each test has its own context. If you need a global context, it'd be better to use a variable in the global scope, instead of passing it for each test.\nLet me know if I understood something the wrong way. If you have a new idea, perhaps it's worth opening another issue, so that we don't mix things up here.\nOriginally, this issue was for providing a shorthand access to a context and I think we agreed, that having 2 different ways to do 1 thing would not be beneficial for us in the long term. I assume, this issue can be closed now?\n. Oh, so return value from each hook adds to the arguments of the test function? Still, my opinion is the same. Also, I'm not a fan of dynamic function arguments. This would eliminate the possibility for us to provide something else in 2nd argument in the future.\n. @ariporad honestly, I just don't see what real-world benefits this would bring. As @jamestalmage pointed out, it would lead to mess in the tests. We already have context support, but this feels like a magical layer on top of it. And magic is hard to explain in the docs and maintain in the code. If you feel strongly about your idea, feel free to provide real project examples, where this feature would help. Maybe we could come up with another solution together ;)\n. @ariporad why getting rid of t.context would be a best thing?\n. > We now have two different ways to do the same thing, which I'm not a super big fan of.\nI think we initially agreed that it probably won't happen, as we already provide context via t.context. \n. The thing is, we don't want to pass context as a second argument. I think, we can use second argument for something more significant and widely used, t.end (done) for example. \nTo sum up, we have context support via t.context property. Offering a 2nd way to use it (do the same thing) is not optimal. @sindresorhus offered to rename t.context into t.ctx, but honestly I'd rather not do that either. Because ctx needs explanation, context does not. I'd rather keep things simple & clear, than short & weird.\n. LGTM\n. @novemberborn Thank you!\n. @sotojuan Great, I like this PR! Thank you for your contribution!\n. @sindresorhus fair point. @sotojuan could you please remove the old CLI tests you replaced with your API tests?\n. @sindresorhus fair point. @sotojuan could you please remove the old CLI tests you replaced with your API tests?\n. @sotojuan and the git rebase master is also needed to merge this ;)\n. @sotojuan and the git rebase master is also needed to merge this ;)\n. LGTM\n. @sotojuan new changes got merged into master branch, so this PR needs rebase one more time.\n. @sindresorhus Oh, github's message then was not unclear. I wonder if it's related to lgtm.co.\n. @sotojuan Thank you for your work!\n. At the moment, AVA can not be run in browser by default. We are definitely looking to support that in future, so I think I can surely say we should expect this feature to land by 1.0.\nClosing it in favor (and as a duplicate) of #24.\n. Yep!\n. Or we could just pass process.argv.slice(2) to the fork process?\n. Either way is good for me ;)\n. @sindresorhus This (https://github.com/jokeyrhyme/ava/blob/af4e70377a2b0143005c5fad52913821fcbdfbeb/api.js#L150) is required for serial execution of forks (#282).\n. @jokeyrhyme Oh, my bad. Everything is fine then!\n. LGTM\n. @jokeyrhyme Ron, thank you for your valuable contribution to AVA, we really appreciate it! We'd love to see your new PRs soon!\n\n. LGTM\n. @brunoqueiros If users will need to use a flag (for example, --show-time), they might as well just:\nbash\n$ time ava\nwhich will also show total execution time.\n. LGTM\n. @sotojuan Thank you for a quick fix!\n. AVA's default output won't change, this is only needed for TAP output. But yes, it is a step forward towards verbose mode.\n. Nope, this issue does not make such assumptions, it's only about tracking assertions. Perhaps we should discuss this after TAP support is landed in master.\n. I can only submit a PR, but I wouldn't want to merge it to master without solving this issue.\n. Leonardo says thank you!\n\n. > Decide on assertion output\n\nWhat specifically needs to be decided on?\n\nSorry for not making that clear. We need to decide what title do we display for assertions:\ntest something\n  \u2705 'yes' === 'no'\n  or\n  \u2705 t.is()\n  or\n  \u2705 t.is('yes', 'no')\nSo, test have titles and assertion have titles too, so we need to decide on the latter.\n. @jamestalmage So I tried to use latest changes to assertions and turned out it's not actually usable for tracking assertions. I inserted console.log(event) at the beginning of onAssertionEvent and it shows no useful information (in case there was no error):\njs\n{ originalMessage: undefined,\n  enhanced: true,\n  args: [ true, undefined ],\n  assertionThrew: false,\n  returnValue: undefined }\nevent.powerAssertContext is undefined.\nThe assertion is a simple t.true(true). Am I missing something?\n. > It should still be possible to build something close to the original source using args.\nI don't think this will have good consequences, we need a stable solution. Otherwise we'll end up with a babel-like stream of issues.\n. Ok, let's postpone it for now, I'll come up with something simpler. I'm off tomorrow, so I'll post updates the day after.\n@jamestalmage will take a look, thanks.\n. Ok, let's postpone it for now, I'll come up with something simpler. I'm off tomorrow, so I'll post updates the day after.\n@jamestalmage will take a look, thanks.\n. PR updated with additions to Readme. Now it can be merged and we'll handle assertion output later.\n. @sindresorhus \n\nCan you add some tests?\n\nI will!\n. I'm thinking if we should return generated output from lib/tap.js, instead of console.log()ing it from there. Would also be easier to test TAP output.\n. PR updated according to all suggestions:\n- Improve readme and add a screenshot\n- Return TAP output, instead of console.log()ing it directly from lib/tap.js\n- Add tests for TAP\n- Add \"tap\" keyword to package.json\n- Clean up lib/tap.js\n. @sindresorhus \n\nSupport fail-fast mode via Bail out!\n:arrow_up: Can you open an issue for this?\n\nI figured there's no need for this one, as we end the tests \"safely\". And not all reporters handle this stuff.\n@jamestalmage Try to use tap-* reporters, not reporters for mocha, as it had some out-dated TAP output.\n. > I'm surprised there's no compliance suite any TAP producer could use. Would be so useful (hint hint). \n@sindresorhus good idea!\n. @sindresorhus @jamestalmage guys, need you to LGTM one more time. It does not want to merge it otherwise.\n. @sindresorhus @jamestalmage Never mind, merged!\n. \n. LGTM\n. @sotojuan Tip: check out git reflog and git reset commands.\n. I like this idea \ud83d\udc4d!\n. Oh wow, something incredible happened, AppVeyor passed, but Travis failed.\n. @jamestalmage I think it was because of lgtm.co.\n. Great stuff @jamestalmage!\n. LGTM\n. @sotojuan Could you please squash 2 commits into one?\n. @sotojuan What's wrong with the commit message? It is totally fine.\n. Thank you @sotojuan, keep those PRs coming!\n. > I think it would be worth introducing a --debug flag that activates --serial and other things related to debugging\n+1!\n. There already was a test for it, but not precise enough. Updated ;)\n. There already was a test for it, but not precise enough. Updated ;)\n. LGTM. Thanks @sotojuan!\n. Rebase went wrong here.\n. Then we are stuck with initial delay on every test run. Not good.\n. I think the solution to this is to cache babel's output. There already was some development in that direction in #189, but left incomplete.\nMy proposal is to cache transformed code based by md5 of original source code. That way, we don't even need to require('babel'), when test files don't change. When md5 changes, load babel and re-transpile source code.\nThis should improve performance.\n. Seems like #349 will take good amount of time. I can implement cache now and send a quick PR, so that we can do a release with babel 6 within next few days.\n. Yep, #352 got it covered.\n. Compared to master branch (see results in #351), now test runs take ~0.7s, given the same test files.\n. Good suggestion! Update is coming.\n. @sindresorhus @jamestalmage PR updated to store cache in node_modules/.cache/ava. Do you have other things in mind or we can merge it in?\n. Why don't we just fallback to xdg-basedir (with os-tmpdir as a backup)? It is a stable approach.\nRegarding cache, when do you think users would want to clear it?\n. So, are we good on this? Would really like to get it in before the release, so that users don't experience bad performance.\n. How about:\n\u2716 wow failed with \"unicorn\"\n. Updated output:\n\n. @gajus Why would you want to disable it?. Could you squash commits into one?\n. \n. I assume it's safe to close this?\n. Good point. But we can only show the last test title, we don't have an information regarding currently running test.\n. > We could maybe pass up a nextTest property in serial mode\nI think this is a bit overkill.\n. I put this on hold until #363 gets merged.\n. Feedback welcome!\n. > Just log an empty line above it on the final call.\nJust tried it, looks really bad:\n\n. > Good move adding the test title btw.\nCredits for the idea go to @jamestalmage ;)\n. Good suggestion!\n. LGTM, great stuff!\n. I haven't thought about that yet, but every item in errors should have the same set of properties, so that we can easily output the list of errors at once. So, whether it's a failed test or uncaught exception (which does not have a test assigned to it), the error object should have the same props.\n. The fix for stack traces is already in master ;)\n. The fix for stack traces is already in master ;)\n. Ok, turns out it's not a bug. It is simply a mini reporter, which does not display error messages. Mini reporter displays only stacks (err.stack).\n. It is also a lack of API documentation actually. Related to #365. Mini reporter was displaying errors at the end of output using api.tests property, but it should've used api.errors property. Messages in errors are enhanced, unlike in tests.\n. When I see child.js I am asking myself a question \"child of what?\". In my opinion, it is just as unclear as babel.js. How about forked-process.js, worker.js, test-worker.js?\n. @ariporad subprocess.js works too. Let's see what other guys think.\n. @jamestalmage a bit long, I think.\n. @jamestalmage++\n. Looks good to me!\n@ariporad could you squash your commits into one?\n. Thank you!\n. @ariporad Could you describe what this PR improves/fixes and provide an example of before/after output?\n. Great, thanks!\n@sindresorhus @jamestalmage \n. @jamestalmage we also need to store rejections and exceptions, which are not attached to any test and don't have test title, etc. So storing rejections and exceptions in tests doesn't sound right.\n. fn.withThrow() throws before even getting to t.throws: t.throws(fn.withThrow()). So t.throws() is not executed at all. The correct code would be:\njs\nt.throws(function () {\n  fn.withThrow();\n});\nor:\njs\nt.throws(fn.withThrow);\n. ~~The bug is related to regenerator~~. Try surrounding https://github.com/jamestalmage/loud-rejection/commit/432b4fb3657ec4af407f4098ee80ff727e1c0d13#diff-1dd241c4cd3fd1dd89c570cee98b79ddR176 with try..catch and console.log(err). It outputs 1 (number).\n. And it outputs 1, because of this -> https://github.com/jamestalmage/loud-rejection/blob/432b4fb3657ec4af407f4098ee80ff727e1c0d13/test.js#L17. It rejects/resolves with exit code, not with Error.\n. Actually, this thing was caused by https://github.com/sindresorhus/ava/commit/945dbea40a6a500a24d54ebcc7e10dcd75647fc7#diff-dd2e881af3311f834199c7b515cff0c7L128 (#354).\nSo, promises need to reject with Errors.\n. nvm install stable installs latest stable Node.js version:\n\n. Node.js stable releases are even numbers, so 5.3.0 is not a stable release.\nI just talked to @sindresorhus and we agreed to test for 4.x too as it is LTS release. I'll accept all your PRs now, thanks ;)\n. @sindresorhus correcting myself, Node.js stable releases are even minor numbers. \n. Oh ok, I should update myself on release cycles.\n. @ntwb Thank you!\n. @ntwb Thank you!\n. I really dig the perf improvement! I commented on a few things, but never mind. We can do a cleanup later, I'd rather have this PR merged ASAP.\n. I think we should just hide the output from the mini reporter. I've encountered many TAP/mocha reporters, that just don't show console.log()s and that's it. With our animated beauty, there is simply no good way to display console.log() output.\n\nI doubt I'd use t.log very much.\n\nHighly agree with @ecowden, I don't think anyone would use t.log.\n. @jamestalmage++\n. Great suggestion, I'm on board! \n. @paulyoung Good trick!\n. Damn it, was about to send the same PR! :D\n\n. Then it's totally my oversight, no worries ;)\n. Love it!\n. If mini reporter uses yellow for skipped number, we should do the same for the verbose reporter as well (it's cyan there).\n. Thank you!\n. Do we need visual tests? As I understand, those are supposed to be executed and completed manually.\n. It's just I'm sure those tests will be forgotten often. Why can't we have automatic tests for comparing output?\n. Great research, @jamestalmage!\n. Hell Yeah!\n\n. Love love love this!\n. Probably adding those when debugging is enabled would be useful:\ndiff\n+       /[\\\\\\/]ava[\\\\\\/](?:lib[\\\\\\/])?[\\w-]+\\.js:[0-9]+:[0-9]+\\)?$/,\n+       /[\\\\\\/]node_modules[\\\\\\/](?:bluebird|empower-core)[\\\\\\/]/\n. Or that :)\n. I think this feature should go into killer-feature list.\n\n. > Just write your tests in ES2015. No extra setup needed.\nThis is actually true, you don't need to import babel-core/register to write tests in ES2015. I assume your code was crashing, because you wrote other code (outside of tests) in ES2015, but using Node.js' version that does not support it.\n. > only display the logs when the test failed\nI'm strongly against this one as it's very irritating when obvious & simple things (like console.log()) don't work as expected.\nDomains looks like a good solution for our issues. We could also group exceptions & rejections, not just console.log()s. It was mentioned before in some issue too.\nThe problem with domains is this:\n\nBut if a replacement will be provided, that shouldn't be an issue to just migrate to a new API.\n. Am I crazy or we could use cluster module for that? It allows to listen to error events when worker (test) fails. Also we could attach to message events and:\njs\nconsole.log = function () {\n  process.send(...);\n};\n. > they're not going to remove domains anytime soon \nIn that case, let's move on with domains on board.\n. @jamestalmage I actually thought about running a worker per test. I just realized it's completely insane.\n. Mind wonders when you're desperate :D\n. Actually, I don't think it's surprising, considering that we promote parallelism & concurrency all over the place.\nHowever, as a solution to this issue, I see implementing t.log() method, but instead of forcing developers to use it, add a babel plugin that transforms console.log() inside test() to t.log(). Bam, simple and it works.\nWhat do you think?\n. How about t.log() instead? Shorter and simpler.. Have no idea how I missed this thread, just stumbled upon it.\nI think it's a bad idea. Not only it will dramatically slow down tests, but also introduce a lot of mess in the AVA core. Quick example that will require \"custom trickery\" - serial tests. They won't work after this change. In my opinion, isolation per file is more than enough. New node process + babel witchcraft per each tests is a huge overhead.\n. I'm pretty sure it is an insane idea at the moment, so we should not even try to do it now :)\n. I'm pretty sure it is an insane idea at the moment, so we should not even try to do it now :)\n. I observed that TAP parsers used in many reporters don't always follow the spec or there are many interpretations of what TAP output should look like. I'd suggest including # skip only if necessary, but quick manual check with always-on # skip output won't hurt.\n. I'd just check in some of the most popular reporters and see if they have problems with # skip 0 always present.\n. @sotojuan If tap also follows this rule, let's stick to it then.\n. Nope, not at the moment. I was also wondering whether we should have an ability to turn off babel. I think we should leave it as a feature request.\n. Nope, not at the moment. I was also wondering whether we should have an ability to turn off babel. I think we should leave it as a feature request.\n. Good catch, it should be updated. Thanks!\n. LGTM!\n. We appreciate your efforts, @SimonDegraeve, but we have to reject this PR. That's the exact reason why we worked on TAP support. So that users would be able to customize output with a dozen of already available TAP reporters.\nYou can check out the list of the most popular TAP reporters here: https://github.com/sindresorhus/awesome-tap#reporters\n. Thanks @kasperisager! And great commit message btw ;)\n. If we're gonna make verbose reporter a default one, we can also remove mini reporter. It will not fit into our no-custom-reporters strategy and might as well be transformed into a TAP reporter.\n. @sindresorhus Oh, in that case - ;)\n. I think we should simplify our lives and just use is-ci.\n. We should also remember to always prioritize --tap over isCI check. So that users who enabled TAP output won't end up with verbose reporter on CI.\n. LGTM! @sindresorhus ?\n. Landed in master.\n. Thank you @sotojuan! Closed via https://github.com/sindresorhus/ava/commit/5d36a8045d5f124b63b0b287091f8efc0ef732a4\n. I like this recipe, have nothing major to add or correct. The only thing I noted is that I usually prefix bash examples with $, but it's not that important.\n. @jamestalmage As for me, I also prefer having only nyc && ava in npm test. I generate lcov and pipe it to coveralls right away. The same stuff as in AVA's config. It looks clean and simple. \n. @danielhusar No, not yet, can't give any ETA on this unfortunately :(\n. @danielhusar No, not yet, can't give any ETA on this unfortunately :(\n. I like it! Thanks @spudly!\n. I like it! Thanks @spudly!\n. I'm not a fan of this change, tbh. AVA's rules promote writing independent tests, which only benefits developers and this change sounds like a step in a opposite direction. As a \"bonus\", it would break things in AVA core, like reporting number of tests beforehand. Serial tests already solve this issue, so this would also result in a duplicate functionality too.\n. LGTM\n. Running a test file directly via node and omitting ava won't work. We removed this capability looong time ago. AVA's CLI must be used to run the tests.\n. First of all, love reading this: I'm happily using ava :)\nI never done this before, so would be great if you pointed out examples tests somewhere else maybe. How others do it, what tools are being used. Hope to be able to help you after that!\n. JSX is not yet supported in AVA's tests.\nAs a workaround, you can separate <FieldError> component in a separate file, which you can compile yourself via babel. Then, import the already compiled component and do something like:\njs\nrenderer.render(React.createElement(FieldError, props));\n. Would be great to describe this workaround in a \"Testing with React\" recipe (#446).\n@tomazzaman Do you have some free time to contribute a recipe for React? No worries if not ;)\n. No probs, you'll love it! So, here's what you'll need to do:\n1. Check out our Contributing guide, especially \"Submitting a pull request\" part.\n2. Fork the ava repository to your account.\n3. Clone tomazzaman/ava repository.\n4. Create a new git branch with a short descriptive name of your work (e.g. react-recipe).\n5. Create a new markdown file for your React recipe in docs/recipes (e.g. docs/recipes/react.md).\n6. Write the recipe. You can check out recipes submitted by other contributors for inspiration - https://github.com/sindresorhus/ava/pull/444 and https://github.com/sindresorhus/ava/pull/402\n7. After you review your recipe and think it's ready to go, commit & push this branch to your repository\n8. Now go to https://github.com/tomazzaman/ava and you'll see GitHub offering to send a pull request of your branch. Here's the official guide on sending pull requests - https://help.github.com/articles/using-pull-requests/.\nLet me know if you need any help ;)\n. @tomazzaman Could you send me a draft I can read?\n@benji6 I don't think we had this discussion among core team members, but as for me personally I would like to add native JSX support in future. At the moment, feel free to use the workaround described in this thread.\n. @tomazzaman Not sure about this, as it's out of AVA's scope in my opinion. There are many ways to do integration testing, and I don't feel that we can have one universal recipe for it. Let's see what others think anyway ;) /cc @sindresorhus @jamestalmage \n. Results look promising, let's get it in!\n. I think this recipe should focus strictly on React + AVA, without explaining version history, react code, etc. Examples of that:\n- https://github.com/sindresorhus/ava/pull/462/files#diff-2cb79c7fb78b66228297358846395c3aR3\n- https://github.com/sindresorhus/ava/pull/462/files#diff-2cb79c7fb78b66228297358846395c3aR21\n- https://github.com/sindresorhus/ava/pull/462/files#diff-2cb79c7fb78b66228297358846395c3aR50\nCould you please put more explanatory text in the recipe? I also noticed a few code suggestions, they should be listed below.\nThanks for your contribution and looking forward to merge this into master!\n. Massive applause for huge amount of work, @jamestalmage.\nI have some style/cleanup-related suggestions, but I can follow up with a PR myself ;)\n. \ud83d\udc4f\ud83d\udc4f\ud83d\udc4f\n. Thank you for taking the time & effort to dig into AVA and report your thoughts back to us, we really appreciate that.\nBelow are my comments on your suggestions/statements.\n\nI would drop babel support almost entirely\n\nAt the moment, I don't see that happening, as ES2015 support in tests out-of-the-box is one of they key selling points of AVA. Also, rich assertions rely on babel too. So removing babel would also mean removing one of the coolest features.\n\nIf babel was necessary, then a babel: true key would be added to the configuration\n\nI was actually thinking to do the opposite (#424). Be able to turn off AVA's babel, so that user is free to set it up however he wants. Imho it would satisfy both sides.\n\nin AVA's configuration, I'd have a sources key (glob) which would make AVA transpile sources (and cache them) before running any tests.\n\nThis essentially is related to the previous answer. However, I'd like to note that we always transpile and cache test files before running any tests.\n\nRequiring the runtime with every forked process adds roughly 300ms to execution time, regardless whether any files were changed from the previous run or not\n\nIt's not true anymore, since we added babel caching to AVA. If there's a cached version of a file, we don't even require babel at all.\nHope I clarified a few things, let me know if you've got any more questions or suggestions.\n. Yes, exactly, AVA transpiles & caches only test files, not source files. \n. The rest of the tests should not be in the output at all. .skip() has nothing to do with .only().\n. The rest of the tests should not be in the output at all. .skip() has nothing to do with .only().\n. Yeah, I think I used tap or tape when building TAP support into AVA.\n. I think it's definitely a valid request and has its right to be in the core. And .finally() looks good too. But unfortunately .finally() for \"each\" does not: finallyEach(), meh.\n. What if we make this a default behavior?\n. I like this idea too.\n\nNot sure the first one makes sense.\n\nNope, it does not :)\n. I'd prefer to not provide in-house regex-like matcher and only implement these for the sake of simplicity and ease of maintenance:\n1. pony will match every test, where its title has pony\n2. !pony will match every test, where its title does not have pony\nWe won't even need to construct any regexps in runtime for that functionality.\nI think, if we put custom matcher or glob into AVA, we'll end up with more issues coming in. There will always be test titles, where our custom match rules or glob will fail. This won't happen if we stick to the basics.\nTo sum up, here are the excerpts from the beginnings of this thread, to highlight that the real feature request is actually pretty simple.\n@matthewbauer:\n\nI mean to be honest for my use case all that's needed is just to search for a string literal in the titles.\n\n@wilmoore:\n\nconstructing a valid RegExp in the shell feels awkward.\n. So #477 is a perfect solution for this issue. It only needs to add negation support (!pony) and done!\n. @sindresorhus Looks good & simple, exactly what we need!\n. Yeah, something is wrong with 0.10 build, I just had an issue with it too. Restarted yours.\n. @sotojuan return null did not work?\n. PR updated, implemented @jamestalmage's suggestions.\n. Oh what the hell :D\n. Ok, I figured it out thanks to iron-node. The fix is amazingly simple - remove this:\n\njs\nBluebird.longStackTraces();\nand execution time drops to 2.65s (even faster than master).\nIt's appeared in this PR, because there's more nesting of promises (no more Promise.each) than in master branch.\n. > The current debug strategy (DEBUG=ava), is a bit annoying because we also spit out ipc and time-require stuff that is helpful for contributors, but not really for users. I guess we could still use the debug module like two different flags: (DEBUG=ava-internals vs just DEBUG=ava).\nThere is no need for:\njs\nrequire('debug')('ava-internals')\nWe can do:\njs\nrequire('debug')('ava:internals')\nand\n$ DEBUG=ava:internals\nto display only internal logs. Same goes for ipc and time-require:\njs\nrequire('debug')('ava:ipc') // DEBUG=ava:ipc\nrequire('debug')('ava:require') // DEBUG=ava:require\nTo show all logs from all sources:\n$ DEBUG=ava ava\nor\n$ DEBUG=ava:* ava\nCheck out docs on using wildcards for debug module - https://www.npmjs.com/package/debug#wildcards.\n. Ok, since we all agree on this, merging this PR so that I can move on to next ones :)\n. > Just call it time-require\nAgree\n. Re https://github.com/sindresorhus/ava/pull/502/files#diff-4fdccd335c2e54ebb16284a24389ce67R294, I don't think we should wait for the previous tests to finish, if new ones are in a queue. If developer made a code update, he obviously does not care whether tests fail/pass before that last update. He wants to see the immediate test log after his change, right?\nImpressive work, @novemberborn!\n. I meant, we could cancel the \"old\" test run (so that we no longer get results/callbacks/etc from it) and start a new one along with a new output.\n. We can leave it as a feature request/improvement, so that this PR gets in faster ;) Don't want that amount of hard work to just wait around!\n. Yep!\n. Yep!\n. I agree with @sindresorhus point, it's a pretty wrapper for the exactly same content. I don't see how it would help AVA become more popular.\nMoreover, we have a website planned when we reach near-1.0 release (#162).\n@zckrs We have to pass on this PR for now, but thank you for your work and effort to contribute to AVA! Little tip, to avoid these kinds of situations, where work does not get merged, I recommend opening an issue first and see what other people think about your idea.\n. Thanks Sam!\n. I wouldn't focus on a \"stylistic\" side of manually doing setup/teardown. I'd suggest focusing on actual benefits it gives to developers, which you actually mentioned at the end:\n\n\nSetup functions allow for easier modifications between test runs.\n\n\nIf the recipe focuses on a code style, it's not really a recipe, it's just an opinion/preference. Recipes are meant to be official, ready-to-go solutions for questions/problems, that often occur while using AVA.\nEverything else looks good, thanks @jarrettmeyer, looking forward to checking out and merging your updated recipe!\n. Yes, the supporting text. I think the code examples are fine. But I'd focus reader's attention to the point, that using manual setup/teardown functions gives you more flexibility. AVA's before/after hooks can't be turned off for certain tests, so here's exactly where \"manual work\" pays off.\nThe point of style, looking better and having straightforward tests is secondary, but also worth mentioning.\n. And @jamestalmage pointed out another advantage:\n\nThe real advantage of the setup function approach is that you could have more than one, and call them based on what you need in a particular test.\n. Thanks @sotojuan!\n. I will follow up with another PR in coming days regarding this issue.\n. Yep this is the one ;)\n. Yep this is the one ;)\n. While this is an interesting idea, I'm wondering how it will affect user experience. Are we just going to show an error, instead of running perfectly fine tests?\n. Ok, in that case I think the best place for this feature is to be in that ava linter we talked about. \n. In my opinion, we should postpone or completely ignore that feature for now. It kind of feels lately, that we add more bloat to AVA, instead of fixing/improving monsters (perf, babel, no browser support - off the top of my head) we have at the moment.\n. > Cleanup code should be in the after/afterEach hooks, so why not run them when tests are cancelled?\n\nExactly what should be done. I imagine sending \"abort\" event to forked process, which would prevent execution of next steps and run the after hooks.\n. @jamestalmage I think you misunderstood my idea. \nSay we have long-running tests, does not matter how much. Inagine each of them takes at least a second. When in watch mode, after I make a change, I don't need to see the test results from the old test run, I want to see new tests immediately starting. So we need to abort & clean up the previous tests, in order to start new ones asap. \n. What I meant in terms of implementation is much simpler and does not need to kill the process, add one more config option and other mess. Simply notify forked process, that no more tests should be run. Let the currently running tests and their hooks finish to properly clean up everything, but don't run the next concurrent/serial test.\n. James, you are right about concurrent tests, can't believe I actually suggested that. \nAs for serial tests, example of such test suite is Ghost blogging software, almost all their tests rely on database state, so they have to clean it before each new test. Their test suite takes 11m to run. If we wouldn't abort old tests on new code update, watch mode becomes useless in that case.\n. @sindresorhus yes, I think this is the best we can do in that situation, great idea!\n. Thanks @allensb!\n. I don't think we need this, as it's quite obvious that it should be installed from npm and is not a core Node.js module.\n@sindresorhus @jamestalmage \n. But we are not using node-tap, so we don't have a t.notThrow method, but we have t.notThrows (from #508).\n. Oh my, a lot has changed while I was absent. Sorry for providing out-dated info guys. \n. @allensb @rclanan Ok, tap is only being used for AVA's tests, not in the AVA itself. So, yes, my comment in the beginning is valid, we are not compliant with tap and have a different API.\n\nwe don't have a t.notThrow method, but we have t.notThrows\n. See my comment in #528.\n\nAlso, little advice, to avoid working on things that won't get merged or accepted, I recommend discussing it in an issue first and see if people actually want this.\n. Errors are not always instances of AssertionError, so we definitely should not just remove the first line of err.stack.\n. I think this is a good suggestion and we should do it.\n. There's an issue for it already - #115. Closing this in favor of the original one.\n. LGTM!\n. \n. Sure, will do and post the results.\n. PR updated, ping!\n. @sindresorhus All done, good to go ;)\n. I'm a bit busy this weekend, I'll take a look during the start of the week. Thanks!\n. LGTM from me, will clean up later myself ;)\n. LGTM from me, will clean up later myself ;)\n. @novemberborn it's no big deal, few things I noticed:\n1) I never use .reduce()\n2) https://github.com/sindresorhus/ava/pull/544/files#diff-b72e3e25f31d6b3c3e5f40b5991dc5f6R94 raises question \"old what?\", var name needs to be explicit\n3) I'd replace arrayUnion with merge.\nMost of it and others are stylistic changes, so don't worry ;)\n. @novemberborn it's no big deal, few things I noticed:\n1) I never use .reduce()\n2) https://github.com/sindresorhus/ava/pull/544/files#diff-b72e3e25f31d6b3c3e5f40b5991dc5f6R94 raises question \"old what?\", var name needs to be explicit\n3) I'd replace arrayUnion with merge.\nMost of it and others are stylistic changes, so don't worry ;)\n. I meant just to change the var name, not the module :) Never mind, it's totally my preferences, not forcing them.\n. Cool, interesting idea @novemberborn! I don't think we need to squash commits here, just merge as it is.\n. Thanks @sotojuan!\n. Looks good to me, thanks @novemberborn!\n@sindresorhus ?\n. \n. LGTM from me! @naptowncode could you please resolve the merge conflict? Big thanks for your work, this is an important change!\n. Thank you so much, @naptowncode!\n. Thanks, would you like to do a pull request?\n. Thank you for your contribution, I commented in a few places ;) Looking forward to merge it.\n. Thanks for this PR, @BarryThePenguin!\n. @sindresorhus Very good point. I guess we came back to the problems discussed in #9, test.todo() makes more sense in that case. But for now, maybe we should restrict function-less tests only to .skip.\n. I like it, LGTM from me!\n@sindresorhus @novemberborn @jamestalmage ?\n. Looks fine to me as well, thanks @BarryThePenguin!\n. I don't think we can catch errors on next tick, because at that time test function has completed. And we also can't know where this uncaught error came from. To sum up, I'm not sure we can do anything about it, without using domains.\n. I don't think we can catch errors on next tick, because at that time test function has completed. And we also can't know where this uncaught error came from. To sum up, I'm not sure we can do anything about it, without using domains.\n. Thank you, @sotojuan, keep the good PRs coming!\n. Unfortunately, AVA is not yet supported in browsers, see #24. We hope to get there around 1.0.0, which shouldn't be that far away :)\n. Hey @spudly, could you rebase from master to resolve a conflict? Thanks for the great work!\n. Great, I think it's good to go!\n@sindresorhus @novemberborn @jamestalmage ?\n. I guess \"outside\" commits shouldn't be in this PR, should they?\n. \n. @sotojuan Seems like, try and see :)\n. No no, this kind of top-level output from CLI should be written without reporters, as those are fatal errors. \n. To me, returning object with reason property is not logical. If t.throws() throws, everything is good and execution is continued, there's no \"failure reason\", because nothing failed. Why don't we just return error from t.throws()?\n. Oh great, so it does return thrown Error from t.throws(), right? No rush, thank you!\n. Thanks a lot!\n. Am I confused or we already do this via #390?\n. Yeah, but we don't transpile source files at all now. So we may start with that.\n. I'm afraid it's a bad practice to use global modules in your projects. Each project has to be complete, with its own dependencies. Plus, when you'll want to test it on CI, you will have to install AVA locally anyway. So I wouldn't recommend going that path.\n. Yes, we are aware about the slow installation of AVA, but we actually need all those dependencies to provide a great experience, which users expect from AVA.\nWe will definitely try to minimize dependencies, but given that our biggest dependency is babel, I'm not sure we can do much in that area.\nGlad you like AVA and let us know if any other suggestions come up ;)\n. ava --init works, but very long, because npm install takes a long time.\n. I assume we can close this now.\n. I remember we had some back and forth with this decision, but atm I'd also like to merge this and strictly expect Error objects.\n. Great idea!\n. We had some users ask for this one previously. I like it, thank you for your work @bachstatter! If you could address fixes/suggestions, it'd be good to merge ;)\n. Neat, let's merge and update sindresorhus/get-stream first and get this in right away ;)\n. I :heart: it! Simple code which does its job. Thank you!\nThere's one little thing though, could you please add some minimal docs to the readme along with a few examples?\n. Yep, in cli section ;) Sure, no pressure, great job!\n. All versions failed, so it must be something :)\n. Try pulling latest changes (#556 just got merged in). \n. @novemberborn It can't be moved to Runner, because we display full test titles only when there are multiple files and only CLI knows how many test files we have. And plus, it would be out of Runner's scope imho.\n. @novemberborn Oh I see, I mixed that up. In that case, @kasperlewau's logic seems very reasonable:\n\nTests without a title will not run if any match pattern is supplied.\nit'd feel weird to see tests without a title running when I've asked for a specific set of titles.\n\nAnd still title manipulation feels out of runner's scope, so I'd leave it in test collection.\n. Ok, I see your point. What about merging this PR as it is and opening a new issue to extend this functionality? I see some refactoring work involved here.\n. > This will flush out any interdependencies per test.\nI believe it is already being done, as we start all tests at the same time and running them in parallel.\nIn a way, order of running/output is random, because it depends on the execution speed of the test.\n. If @novemberborn made all requested changes, is it good to go or anyone else has some more suggestions?\n. I'm also ok with A (assuming all other counters will be updated too). \n. I think we shouldn't do it, because those libraries/components should be stable on their own. They should not depend on other code passing these arguments correctly.\n\nAllowing defaults throughout the code makes it hard to reason about expected values.\n\nWe can improve our checking of value types, using functions like Array.isArray(), so that it's more clear what we're expecting.\n. I think this is good to go, thanks @novemberborn!\n. Closing due to lack of response from the author of PR.\n. PR looks good @novemberborn, but I feel like we really have to come up with a replacement for exclusive-related names. hasExclusive, trackExclusivity, updateExclusivity sound very weird and unclear. At least to me. Let's hear what others think.\n. So, we agreed to accept .jsx extensions in our API?\n. So, we agreed to accept .jsx extensions in our API?\n. I agree that we shouldn't expose any babel-related options in our CLI.\n- --presets\n- --plugins\n- --ignore\n- --only\nThese look like they might be needed for AVA itself at some point.\n. Thanks @mattkrick!\n. I don't think it fits AVA's atomic tests philosophy. One failed test should not stop or cause all the other ones to fail.\nAs for your example use case, I think it can and probably should be treated as a set up step. \n. I just tried running AVA \"remotely\", outside project folder, and it worked fine. Are you sure you imported ava in test1.js? Also, repositories/x/y.js is still a relative path :)\n. I've tried absolute paths and it works that way too. Could you please try using AVA's master branch instead of the version on npm? Here's the commands you need to run:\n$ npm i -g sindresorhus/ava\nand inside project folder:\n$ npm rm ava --save-dev\n$ npm i sindresorhus/ava\n. No worries, @jacobmendoza, it's too bad I can't reproduce this. Perhaps someone else might help with this.\n. Love this idea! Would be super cool if someone could post links to great medium-sized projects using tap/tape/mocha to test these codemods against :)\n. Love this idea! Would be super cool if someone could post links to great medium-sized projects using tap/tape/mocha to test these codemods against :)\n. Would also be nice to convert mocha's:\njs\ndescribe('group', () => {\n  it('test');\n});\ninto:\njs\ntest('group - test');\n. @spudly Of course, just dumping thoughts ;) Thanks, definitely helpful!\n. I'm sure there might be a better error message, so feedback is welcome!\n. PR updated!\n. PR updated!\n. AVA has Babel built-in, which allows writing tests using latest JS language features without any configuration on your side. In other words, you just create a test.js (for example), write tests in ES6/7 and simply $ ava test.js. Try it on practice, it's easier to understand it that way ;)\n. I appreciate your time & effort that went into thinking this approach through, but I must say I'm a bit skeptical about having to navigate the test results. Interactive test results would complicate the UX, imo.\n. I appreciate your time & effort that went into thinking this approach through, but I must say I'm a bit skeptical about having to navigate the test results. Interactive test results would complicate the UX, imo.\n. I've got an idea, will post a demo soon. \n. If an error is encountered, it gets displayed immediately and live test counter moves down. That way user does not need to wait for tests to complete to see the errors & their stacks.\n\n. @novemberborn totally forgot about this one, will submit my fix as a PR soon.\n. > Shouldn't this be implemented in the API? timeout can be useful for other consumers too, like the grunt/gulp plugins.\nHm, probably, I'll push an update.\n. > Shouldn't this be implemented in the API? timeout can be useful for other consumers too, like the grunt/gulp plugins.\nHm, probably, I'll push an update.\n. > Clearing and restarting timers can be expensive.\nCould you provide any links backing this info?\n. > Clearing and restarting timers can be expensive.\nCould you provide any links backing this info?\n. @sindresorhus Sure, will rebase tomorrow.\nWill try to fix it, good catch ;)\n. @novemberborn Could you please explain the behavior and code behind that timeout-no-clear branch?\n. Anyone has ideas on how to fix this - https://github.com/sindresorhus/ava/pull/654#issuecomment-199748315? The only way I see is process.exit().\n\nThis will output the error message after 10 seconds because of the setTimeout, not after 1s of inactivity. If you remove the setTimeout it will never exit. Can you also add a test where it never exits by itself?\n\n@sindresorhus expect that it won't exit and .kill() it to finish the test?\n. @novemberborn in timeout-no-clear, won't timeout be different from what user specified with --timeout? Update: never mind, I missed your comment:\n\nit doesn't reschedule with a decreased interval\n\nSo we would need to calculate those intervals and take them into account. Sounds a bit over-complex for such a simple thing.\nI pushed a new technique, ridiculously simple - debouncing. Instead of restarting timer after every test, this function is debounced with 200ms interval, so that it's not fired often. I believe it should improve things a lot.\nIt is simple and understandable, no black magic involved. Let me know what you think ;)\n. > Even with debounce there is an issue with this when someone has a long timeout set and the tests finish. There is no way to cancel the scheduled timer.\nThis is not a problem, because we explicitly call process.exit() when the tests finish.\n. > It will be a problem with the watcher. If we can't get cancel support upstream we'd need to track some state in the debouncer callback so it doesn't abort tests from newer test runs.\nIn that case we have 3 choices:\n1. Disable timeouts and merge\n2. Wait for debounce module to support cancelling\n3. Find alternative module that supports cancelling already\nI'm for 3.\n. debounce in lodash has a cancel() method, which is perfect for us - https://lodash.com/docs#debounce.\n. PR updated.\nI re-implemented stats calculation in Runner, because it returned zero passCount, failCount, etc when timeout was reached and forks were killed. Now Runner returns current stats (including completed tests):\n\nI squashed all the timeout-related commits and left runner-related commit \"outside\". I'd like to keep it that way to maintain clear history.\n. Updated ;)\n. > Was the change needed to get all results in case of a timeout?\nYes, exactly! When test worker needed to exit before tests completed, all stats were zero. So I implemented \"on-demand\" calculation of stats.\n@sindresorhus Thanks :)\n\n. @novemberborn Yes, definitely, I guess I forgot to remove it myself.\n. I guess we should rollback this PR and just rerun via r + Enter. Will be easier for everyone and additional \"Enter\" is not big a deal.\n. I'm for reverting, one less thing to maintain. Didn't think such a simple PR would have such complications afterwards :)\n. I'm for reverting, one less thing to maintain. Didn't think such a simple PR would have such complications afterwards :)\n. Personally, I'm fine with ok and notOk, although I do understand that truthy/falsy would probably be quicker to understand for newcomers.\nAs I see, our opinions on this are separated, so would be good to get more input from AVA users.\n/cc @sotojuan @twada @floatdrop @ariporad @kasperlewau @BarryThePenguin @jokeyrhyme @naptowncode @spudly \n. I'd be sad to see t.same() go :( deepEqual is way longer, but if that's clearer to AVA users, let's do it.\n@novemberborn now I'm convinced we should migrate to truthy/falsy too (https://github.com/sindresorhus/ava/pull/663#issuecomment-202339809).\n. Mongoose uses %20 instead of _ as separator, so this should work too and is prettier.\n. I'm all for .failing() and .see()!\n. I'm all for .failing() and .see()!\n. @dcousineau Could you please add a test to ensure this won't happen in future? Would like to merge it asap ;) Thanks!\n. I think solutions/suggestions from both @novemberborn and @sindresorhus are valid. Also, big +1 on plugin system, @novemberborn!\n. Sure, I think it's the right thing to do, there's too much stuff on our hands at the moment. We need a \"future\" label for that kind of stuff.\n. LGTM ;)\n. Here's an idea: what if t.same becomes t.is? Couldn't be clearer and shorter to type. And t.is gets removed in favor of t.true(a === b). \nI use t.is only for primitive values and when I need to check that references are equal. t.same also handles primitive values, so the only benefit of t.is is checking reference equality.\nWhat do you think?\n. Here's an idea: what if t.same becomes t.is? Couldn't be clearer and shorter to type. And t.is gets removed in favor of t.true(a === b). \nI use t.is only for primitive values and when I need to check that references are equal. t.same also handles primitive values, so the only benefit of t.is is checking reference equality.\nWhat do you think?\n. PR updated.\n. @sindresorhus @jamestalmage @novemberborn :shipit:\u2753\n. AVA still has to be compatible with Node.js v4 and it also uses Babel for power-assert and t.throws() helper.. LGTM!\n. Closing as a duplicate of #709.\n. > /Users/jamestalmage/dev/ava/test/fixture/improper-t-throws.js (4:10)\nPerhaps displaying relative path would be better and would align with our \"clean stack traces\" feature. \n. I know it is an overkill, but would be incredibly cool to have syntax highlight in errors (not just this one).\n. Super awesome!\n. Thanks for reporting this!\n. What happens with the following set up in that case?\npackage.json:\njs\n{\n  \"scripts\": {\n    \"test\": \"ava -- --harmony_destructuring\"\n}\nterminal:\n$ npm test -- specific-test.js\n. That's not something we want though. Although this case should not occur often, so we might just roll with it.\n. That's not something we want though. Although this case should not occur often, so we might just roll with it.\n. I don't think we'd want to have that method in AVA core, as it belongs to a very very specific use case. I'd suggest either using the helper you came up with or using child_process.exec and see if the process crashes with an error.\nThanks for taking the time to suggest a new idea, but I don't think it fits in AVA. I'll let other people speak up, but it's -1 from me.\n. Not sure we want an option for it, as we would need to handle output piping and spawning tap-* process ourselves.\nLet's see what others think.\n. LGTM\n. Tests are failing.\n. > In my project I cannot depend on any fields being present in a package.json\nCould you explain a case where it's not possible?\n. Good points!\nAs for async-task-pool, we can use Bluebird's concurrency parameters instead of a new module, plus I'm not a fan of its API (function returning a function).\n\nTests we don't need to recompile should be launched first.\n\nThis one can be accomplished soonest of all, like this idea.\n. > Then grab the source for the failing test and include it as well as the stack trace\nIn that case tests could take up a lot of space in an issue. I suggest creating an anonymous gist with the contents of a test file. Authorization (api key) is not required for it.\n. I think the prompt is a better solution. Babel trickery will bring complexity and more bugs, as a result. \n. > Where did it go? Did you change your mind? I think it's a fair point.\nDid not understand what you mean by that.\n\nWe may be implementing some of that Babel trickery anyways.\n\nWe may, but we'd rather spend efforts on AVA itself now, as there are lots of issues on the table.\n. @novemberborn I think fail-fast should quit asap. If we allow cleaning up or running after hooks, other parallel tests will continue to execute. I agree with you!\n. For an unknown reason, the cause of this weird bug is assigning XMLHttpRequest to global. Test code to confirm:\n``` js\nimport test from 'ava'\nimport {jsdom} from 'jsdom'\nglobal.document = jsdom('')\nglobal.window = document.defaultView\nObject\n    .keys(document.defaultView)\n    .filter(property => typeof global[property] === 'undefined')\n    .filter(property => property !== 'XMLHttpRequest')\n    .forEach(property => global[property] = document.defaultView[property]);\ntest('hey', t => {\n    t.is(true, false);\n});\n```\n\n. More information: error occurs only inside test() block. Code to confirm:\n``` js\nimport test from 'ava';\nimport {jsdom} from 'jsdom';\nglobal.document = jsdom('');\nglobal.window = document.defaultView;\nconsole.log(new TypeError().stack);\nObject\n    .keys(document.defaultView)\n    .filter(property => typeof global[property] === 'undefined')\n    // .filter(property => property !== 'XMLHttpRequest')\n    .forEach(property => global[property] = document.defaultView[property]);\nconsole.log(new TypeError().stack);\ntest('hey', t => {\n    // no error if next line is commented out\n    console.log(new TypeError().stack);\n});\n```\n. Traced it down to this failing test case:\n``` js\nimport test from 'ava';\nimport {jsdom} from 'jsdom';\nglobal.document = jsdom('');\nglobal.window = document.defaultView;\nglobal.XMLHttpRequest = document.defaultView.XMLHttpRequest;\ntest('hey', t => {\n    t.is(false, true);\n});\n```\n\nIf you comment out global.window or global.XMLHttpRequest, it works fine:\n\n. @nfcampos Thank you so much for your work as well! I spent half a morning and couldn't trace it down to the cause, this is a big help!\n. I'm not sure we still understand the problem you are suggesting to solve. Could you explain the issue you are running to explicitly?\n. Thanks a lot, @nfcampos!\n. Submitted a PR to fix this - https://github.com/chalk/supports-color/pull/39.\n. Oh damn, did not see the \"good for beginner\" badge... Sorry!\n. I like the idea of t.like() to do partial comparisons, but t.match sounds complicated to me and would need \"good\" explanation to users.\n. I did not mean the name t.match sounds complicated, but rather proposed functionality.\nI liked the idea behind t.like to not compare objects 1:1, but only a subset of first-level keys. See the example in the first post.\n. > Using a temp dir doesn't make inspecting the fixture contents much easier. You have to figure out where the temp dir for that test was (usually just by adding a logging statement), then track it down on the filesystem (and it's usually at some weird and lengthy path).\nI disagree. I don't see how these 2 steps are complicated. There's no need to figure out the path, because console.log (as you said) displays it. And you don't need to track anything down as well, because you copy that path and cd into it, no matter how weird and lengthy it is.\nWIth .ava-temp-dirs, the flow does not change actually. Let's assume AVA displays tmp path on test failure. Then you have to copy it and cd, just like before.\nThere's another downside with .ava-temp-dirs. When building a Docker image, it will get added to it, unless ignored in .dockerignore. Users might not be aware it exists and gets added, because Docker is silent about the files it copies.\n. > You wouldn't need to display the temp path. You would just cd into ./.ava-temp-dirs after a failed test\nIn that case, this process becomes one step shorter by not adding console.log(). I don't think it's worth building .ava-temp-dirs feature to skip this tiny step.\n\nI don't get the .dockerignore argument. you're going to notice pretty quickly, because you likely won't have the files added to .gitignore.\n\nTalking specifically about Docker, not git:\n\nWhen building a Docker image, it will get added to it, unless ignored in .dockerignore. Users might not be aware it exists and gets added, because Docker is silent about the files it copies.\n. Fix for Docker argument is to put .ava-temp-dirs inside node_modules/ava, like .cache/ava.\n. Good point, although notRegex reads like not a regex in my mind.\n. Good point, although notRegex reads like not a regex in my mind.\n. > I really liked the Logger facade. It makes testing a lot easier.\n\nI wouldn't say tests changed much and it became \"a lot\" harder to test reporters. It's just as easy to test their output.\n\nWhat does doing this get us?\n\nLess code in the AVA repository, especially unneeded code. As I mentioned in the first post, Logger did not do anything useful, except checking for method existence and falsy return value. It's an unnecessary layer. Reporters, that listen to API events are much more \"javascript\", than previous objective-c delegates-like code.\n\nI was thinking of some functionality we should actually move IN to logger. Specifically the explicitPrefixes option we use to manipulate whether we prefix titles with the file name.\n\nThis sounds more like a helper function, rather than a method belonging to Logger - a class, that displays output.\n. I meant less modules.\n. While working on browser support, I discovered another argument in favor of this PR: reporters can have different output destinations. Not all reporters just write to stderr.\nWith browser support, there will be a reporter, that renders DOM elements. If we keep a centralized Logger, it will have to take care of detecting browser environment and rendering DOM elements.\nThis PR definitely brings more flexibility and makes reporting future-proof. \n. Does not make sense to me, because it's basically a wrapper over emitter.on. Reporters can just use .on() to listen to events they expect, why provide a function to do the exact same task? As I mentioned before, tests did not become any harder after this PR's changes.\n. I'd really like this PR to go in. @jamestalmage is -1 on this, @novemberborn is +1.\n@sindresorhus @sotojuan we'd love your feedback on this.\n. @jamestalmage I don't see the point in going forward, when we don't know whether this is going in or not.\n. PR updated with a refactored mini reporter and passing tests for it. Code comments on this PR were fixed as well. Watch mode also works, but tests weren't adapted to the latest changes.\n@novemberborn I'm going on vacation today, so I won't be able to take care of watcher tests. Could you please pick this up from now on?\n. I agree, I think our \"browser\" setup code should be extended with assigning default properties to window.\nBy the way, I think you might want to check out https://github.com/avajs/ava/issues/822. Never mind the issue reported, but rather code examples it contains. It does exactly the same thing - assign stuff to window, but without an additional dependency (less stuff for user to install during the browser recipe).\nAbsolutely, we'd love a PR to fix that example!\n. Interesting point, I like it. I'm only wondering whether it would mess up with people's expectations that everything should be working out-of-the-box. That is sort of how AVA works, it assumes good defaults and lets developers customize them only when necessary. \nLet's hear what other people think ;) \nP.S. I'm going on vacation today, so I'll probably be completely offline until 20th July. Sorry for not being able to continue this discussion!\nP.P.S. Thank you for such detailed posts, explanations and code examples, I wish more people explained their thoughts & arguments like that! \ud83d\udc4d \n. I pushed browser support work I've done, so that you can get a glance.\nTo test it out:\n1. Generate tests.\n$ ava --browser\n1. Start http server to serve current directory (I use serve):\n$ serve\n1. Open http://localhost:3000 in your browser.\n2. Open console.\n. Implementation and usage details: https://github.com/avajs/ava/issues/24#issuecomment-222367426.\n. > I don't think we should try to run the API in the browser.\nWhy? I had no issue with doing that.\n. > We have efforts underway to bring watcher into the API\nIs there a discussion/issue/pr that I missed?\n\nIt's not so much a matter of whether or not it's possible, but rather if it is a good idea. It's definitely adding complexity.\n\nI don't see how it adds more complexity. I'm not modifying API in any way, it's good to work in the browser 1:1, unchanged. As I mentioned previously, I was trying to reuse as much as possible. I don't want to rewrite AVA core for browsers. Instead, the plan was to replace node-dependent parts with browser-compatible ones. What makes it a bad idea? Do you have specific arguments against doing this?\n. I'm struggling to see how this PR simplifies things. send function was already simple and straightforward. Now we need to use send-to-parent function when we need to send data to a parent process, and wrap-send + function it returns to send data in a same way, but to a child process?\nThis looks more like making things complex. What was the reason behind this PR, what issue does it fix?\nUpdate: If you are worried about consistency, we can compromise by using send(process, 'some-event') in the child processes to send event to a parent process.\n. @jamestalmage We got away from my original question with this discussion in the diff. Could you please address it and explain?\n. Better, but I still don't see the value. Now we have to maintain the same message structure in two places to avoid bugs: https://github.com/avajs/ava/pull/896/files#diff-a48173105195b7a6481a3ad85b45144eR57 and https://github.com/avajs/ava/pull/896/files#diff-0e75bb55e5299c7ac7e253725935a6c4R18.\nI think we should just resort to enforcing send(where, what) everywhere:\n- in the fork.js: send(ps, event)\n- in the test-worker.js: send(process, event)\n. I still don't see the value in these changes. I'll try to describe why step by step.\n1. One message structure, two functions.\nPreviously, we called send('event name', { key: value }) and we knew the message is going to be in the \"ava format\". To change it, we edited lib/send.js and done. Now we have 2 functions in 2 files that do exact same thing and we need to edit 2 files to change the message structure (try not to forget about that one).\n2. One task, two functions.\nThis basically repeats the above point. For one task (sending data to another process) we now have 2 functions in 2 different places: send in lib/fork.js and send in lib/send-to-parent.\n3.\n\nThe old send had a weird overload allowing you to pass in the ps object.\n\nWhy is it weird? It looks logical to me: send(childPs, 'event name', data), which means send this child process an event with that data.\n4. IMO, fork guard does not belong in lib/send-to-parent.\nTo test lib/send-to-parent we have to set a fake function to process.send to pass the fork guard. I think the fork guard belongs to lib/test-worker, where it was residing previously.\n\nTo address the new PR description:\n\nWe already had special wrapping send when the communication direction was parent to child, so I just inlined a bit of logic in fork.js (basically a 1 liner).\n\nCould easily be:\njs\nps.send = function (event, data) {\n  if (!exiting) {\n    send(ps, event, data);\n  }\n};\nwhich is just as clear (if not more) and avoids duplication.\n@jamestalmage I'm arguing against PR's changes, but I've never heard an original intention or purpose of this PR. To bring up an unanswered older question: What was the reason behind this PR, what issue does it fix?\n. Hmm, wasn't .always supposed to serve this exact same purpose? To always run the clean up task, regardless of previous failure? Perhaps it's worth fixing the existing modifier, instead of introducing a new one. How would we explain the difference between the two to the user?\n. I think --no-cleanup may be a wrong way to approach this issue. I agree with this statement of @sholladay: \n\nI don't see the logic in skipping .afterEach when a test fails.\n\nIf it would be totally up to me, I'd consider removing .always and making it a default behavior for .afterEach.\n. Yeah... Looks like there's no win-win solution at the moment for this.. I don't think I'm on board with this addition as well, because it does not give any benefit or useful information to the user.\nAs for \"shoddy testing techniques\", not sure testing framework would help you discover those. How about this:\njs\nfor (let i = 0; i < 100; i++) {\n  t.true(true);\n}\nwould show that this test executed 100 assertions.\nAs for me, I usually look at the overall quality of the module, including taking a peek at its source code. I rarely (if not never) look at its code coverage or test results.\n. > in the more likely good-faith case, it is an (easily collected?) metric that could be used as an anecdotal indicator of coverage, and i would use it as such.\nWe try to not add nice-to-have features, that don't help/solve the real pain. Sure, it may be easy to add, but that does not mean it should be done. Sometimes we just have to say no, unfortunately.\nFeel free to propose your other ideas in new issues! ;)\n. As for me, I'm opposed to t.flaky assertions in favor of test.flaky for the exact same reasons by @novemberborn and @sindresorhus :\n\nUsers should be encouraged to determine exactly what is making their test flaky.\nI don't buy the argument for flaky assertion. IMHO if you have flaky assertions mixed with other working assertions, you should just move them into a separate test. I would also consider having flaky tests an anti-pattern. It should only be temporary until you have time to make it non-flaky.\n. @novemberborn Yeah, but your reason fits my favorness of test.flaky as well \ud83d\ude06 \n. I just realized something. We could skip adding test.flaky in favor of using \"retry\" modules, like https://github.com/zeit/async-retry:\n\njs\ntest('retry me', retry(async () => {\n  // retry the test, if it throws\n});\nI like this solution, because that way we avoid adding more stuff to the API and we let user handle their unique use cases.\nWhat do you think?. Not sure this will bring any performance benefits for us, because our writes don't happen that often.\n. @jamestalmage we are still not settled on #896, would be cool to finalize discussion on that one before including it here.\n. @michaelgilley I believe Jest does isolation via vm module, which indeed provides a sandbox for the tests. While it's an interesting solution, we are aiming for 100% isolation using forked node processes. As mentioned up the thread, @novemberborn has alternative speed-up implementation in the labs and there's also this one by @jamestalmage. Performance is one of our top concerns, we are constantly looking on how to speed your tests up.\n. @sotojuan Good stuff!\n. Shouldn't we erase stack trace lines with next_tick.js? Also, looks like the newline is missing at the end of output.\n. Closing due to lack of response from issue author.\n. Thanks for bring this up, @jgkim!\n. I agree, something like a minute ago would be faster to understand when was the last run. Thanks for the suggestion, @tusharmath, let's see what other people think!\n. @novemberborn Good point :)\n. That would involve rewriting all the previous output, which is a nightmare.\n. Not sure I understand the goal of exporting tests and including them in another file. Couldn't you place your tests along with before hooks in MainTest.js?\n. I meant place tests and hooks in the same file.\n. Good work, @asafigan! Sorry for not taking a look sooner. Let's hear one more LGTM and merge it in.\n@sindresorhus @jamestalmage @novemberborn @sotojuan @jfmengels looks good to you?\n. Hey @asafigan, any chance you have some free time to complete this? :)\n. @asafigan No worries, thanks!\n. @asafigan Thanks for finishing this up! I'm a little busy now, I'll try to review ASAP, unless someone from the team beats me to it.\n. > The problem is with the use of !== to compare localCLI and __filename.\nSwapping !== with != wouldn't solve the problem, as the case of strings affects the result anyway.\n@sindresorhus @jamestalmage @sotojuan @jfmengels Would a simple .toLowerCase() be a good fix for this case?\n. @novemberborn I meant to lowercase both variables: localCLI and __filename.\n. Although yes, path.relative would be better.\njs\npath.relative(localCLI, __filename) !== ''\n. Let's see how test files look between different AVA versions:\n1. switch to 0.14.0\n2. rm -rf node_modules/.cache/ava\n3. npm test -- test/server/core/crypto.js (to test that specific file, that's causing trouble)\n4. save the contents of node_modules/.cache/ava/*.js file in a gist\n5. switch to latest AVA (0.16.0)\n6. rm -rf node_modules/.cache/ava\n7. npm test -- test/server/core/crypto.js\n8. save the contents of node_modules/.cache/ava/*.js file in a gist\n9. post the gists here\n. Could you paste them without diff (as separate files/gists)?\n. PR updated.\n. I just noticed this is a 1000th pull request! Congrats everyone!\n. AVA does not have a \"public\" concept of reporters, they are only for internal use and each of them has a specific use case. If you are interested in creating a custom output, create a TAP reporter and execute AVA with --tap flag.\nAs mentioned in the original issue and #676, we might introduce a plugin system, but that definitely won't happen in the near future.\nSad to see your time wasted, that's why I encourage you to discuss upcoming pull requests beforehand in the dedicated issue, to prevent stuff like this. Thank you for understanding ;)\n. I think we should proxy CLI flags to the node child processes, so that users could easily access them. I doubt it will do it any harm to us.\nTip: https://github.com/sindresorhus/has-flag.\n. AVA does not show which test timed out, because AVA does not track timeouts per test. What we do is, we reset one global timer every time the test finishes. So what AVA implements is actually a global idle timer. When no new tests have completed during a specified time frame, AVA fails with a timeout message.\nPlease check out the docs regarding timeouts: https://github.com/avajs/ava#global-timeout.\nAlthough, I agree that this feature would be useful. Related: #583.\n. How is this different from t.afterEach.always, except that t.finally is for specific test only?\n. Not sure I want to see this in the core, as there's a solution for this problem already in there - .afterEach.always. This looks like an API sugar to me. Let's see what other people think.\n\nI strongly advise you to check out contribution guidelines before submitting pull requests. We don't want to waste people's time by rejecting pull requests, that haven't been discussed before or just don't fit in. That's why it's always best to discuss your changes/ideas in the issue first, so we can all agree on the solution before doing any actual work.\n. Also passed locally on clean node_modules without a problem.\n. I absolutely support your proposal!\nAnswers to your questions:\n\nI don't think it should be allowed. ESLint's docs on no-throw-literal rule explain it best:\n\n\nIt is considered good practice to only throw the Error object itself or an object using the Error object as base objects for user-defined exceptions. The fundamental benefit of Error objects is that they automatically keep track of where they were built and originated.\n\nServices like BugSnag, Sentry, Rollbar, etc will provide zero information, if code throws strings, instead of Error objects.\n\nI think they should be supported, with behavior exactly like you outlined in your first code example:\n\njs\nt.throws(fn, 'string') // Throws an Error (or subclass thereof), whose .message === 'string'\nt.throws(fn, /regexp/) // Throws an Error (or subclass thereof), whose /regexp/.test(err.message) === true\nI use this way constantly and find it very convenient.\n\n\nI think this would also be a nice thing to have, but not insisting on this one. Let's hear from others whether they use this way or not.\n\n\nDidn't completely understand the question, mind providing a quick example?\n. I think --require and --source are the only ones that can safely go away. All the other ones are quite useful, don't think there's more room for removal.\n. I'm excited about LTS + stable plan as well. Big +1 from me. \n\n\nQuestion is, when should we stop maintaining LTS for 0.10/0.12? It shouldn't be too long, because it will drag us down more and throttle development of the main branch.\n. Totally on board!\n. > I think this would allow us to do Babel transpilation in the test workers themselves\nThis would be just awesome!\n. @sindresorhus  So I assume, until then we could use lodash.isEqualWith with an additional equality check just for buffers? It's pretty solid. Or do you want to keep searching for the module with the satisfying defaults?\n. @sindresorhus Yes, definitely!\n. Perhaps you could pass this path as an environment variable to your tests:\n$ TESTS_BASE='....' ava\nand in your tests:\njs\nconst base = process.env.TESTS_BASE;\n. Actually, this solution is straightforward and clear. process.env is a \"standard\" way to access environment variables in Node.js apps. AVA proxies all the environment variables you pass to its test processes, so it's totally fine to use that.\n\nIs there any reason the options object shouldn't be exposed?\n\nWe try not to add alternatives/aliases or functionality, that already exists in some other form. If we would, AVA could quickly become hard to maintain. That's why we are so picky what goes in the core and what does not. Each change has to be supported in future versions, so we try to avoid mistakes. Hope you understand ;)\nLet me know if the process.env solution works for you.\n. Closing due to inactivity.\n@diegohaz feel free to reply back whenever you have time and we'll reopen the issue ;). PR is ready for review!\n. @sindresorhus addressed all your feedback, thanks!\n. Thanks guys!\n. If the thing that's worrying you is writing .serial for each test, you could use --serial flag, which makes all your tests run serially. As people on SO said, it complicates things that you stub the same dependency in multiple tests, so I guess --serial is the only workaround.\n. I'm sure many people use --require babel-register or similar, I think deprecation message pointing to configuration in package.json is a must have.\n. \n. > This compiles the helpers as if they were test files, right? That seems correct to me.\nYep, exactly.\n\nLet's see where that gets us before doing anything else for macros.\n\nYes for sure, that is unrelated to this PR.\n. > Specifically transpiling fixtures seems fraught.\nCould you elaborate on this? I don't quite understand the reasoning.\n. My thinking was that everything that resides under test/ belongs to AVA, so it should take care of those files and provide its features (power-assert, ES6, etc) to them as well.\nHowever, @novemberborn's reasoning about fixtures relation to tests does make sense to me now. I will correct this PR and the one in ava-files as well.\n. @Snugug only files with names starting with _ and located in helpers/ folders get transpiled in this PR. In the issue you mentioned the files are located in macros/ folder, right?. Added a small note to readme: https://github.com/avajs/ava/pull/1078/files#diff-0730bb7c2e8f9ea2438b52e419dd86c9R738.. LGTM!\n. @sotojuan Good point, totally agree.\nI think we should make t extendable with custom methods (assertions). Was thinking to create a separate module for t.jsxEqual(), would be nice to make this possible.\n. @odigity section \"Custom Assertions\" in readme is actually related to using 3rd-party assertion libraries, like expect or chai, not extending t with custom assertions. I understand the confusion, I think we have to rename the title to something like \"Using 3rd-party assertion libraries\".. Good job explaining the what & why!\n. I'm not a fan of these kinds of comparisons. I think AVA users should be attracted to it by its features, not by mentioning how bad alternatives are. No product is perfect, that is a fact. AVA has its pros & cons as well and it's perfectly normal. Every testing tool loses to each other in some way.\n. I think we sorted this out, closing.\n. Closing due to inactivity.\n@pocesar feel free to reply back whenever you have time and we'll reopen the issue ;). Hey @thinkimlazy, happy holidays! Will you have some free time soon to finish this up?. Hey! You can use .serial() test modifier to run selected tests serially:\njs\ntest.serial('requires to run serially', t => {\n  // ...\n});\n. > It would be great to have one modifier for many tests of for one file.\nSee @sindresorhus's solution above on how to accomplish this: https://github.com/avajs/ava/issues/1107#issuecomment-260247033.\n. Duplicate of https://github.com/avajs/ava/pull/480.\n. Really well done, @lithin! Nicely organized and clean PR, thanks a lot for working on this!. This is amazing, thank you, Anna!\n\nP.S. I post this gif only on really cool stuff :). Since the lodash.isEqual is fixed now, we just need to update the dependency. Thanks for reporting!. @jdalton Thanks for the note! We'll keep watching and update when it's ready. . Hey @zixia, thanks for all this hard work you've done!\nI was reviewing the code, but found this file https://github.com/avajs/ava/pull/1122/files#diff-f9bf36e65f4c439ede43fea80500f031 to have a lot of comments. Is it still work in progress?. @novemberborn Agree, a check wouldn't hurt.. Thank you, Sam :). I don't think this is going to be helpful, since it's basically what mini reporter does. The default output is as minimal as it could get, I think.\n\npeople are usually only interested when tests fail\n\nMini reporter only shows failed tests and exceptions too.\n\nwhen using with gulp, output is unnecessarily verbose (takes 5 lines instead of gulps regular 2)\n\nI don't use gulp anymore, so I don't know what you mean here. Could you paste a screenshot?. I think this change would better belong to gulp-ava or maybe even a layer on top of it. The idea is to hide all output if exit code is zero (all tests are passing). What do you think?. > Can you comment on what's left?\nFrom what I can remember, mostly tests. I also wanted to check if t.jsxEquals works equally great with Preact.\n\nThe output below 1 failed and 1 test failed [02:03:57] should be identical\n\nI think this can be solved right now outside of this PR, as this change would be unrelated.\n\nFor t.fail(), I think we should make an exception and not show the Actual/Expected output. It makes no sense and the code excerpt is clear enough.\n\nSure, good catch.. Added a todo list, so that you guys know what's going on. I squash commits right away, so there wasn't a quick way to track progress.. Good point. . What output should we display for these tests? https://github.com/avajs/ava/blob/master/test/fixture/throw-named-function.js https://github.com/avajs/ava/blob/master/test/fixture/throw-anonymous-function.js. Oh my, big milestone for magic-assert - first green CI build!\n\n. Before I forget, I better note this here: when this is released, users will need to update their snapshots. This is required, because this PR changes the way snapshots are stored. Previously, snapshots stored the complete JSX tree, now they store JSON representation of that tree. Thanks to that, AVA can render beautiful diffs between your actual JSX tree and the one stored in the snapshot.. Regarding https://github.com/avajs/ava/pull/1154/files#diff-9c6c3cd6ffcef4f57ac6e0c793b31b85R337, I don't think there's a need anymore to generate error message for t.deepEqual(). They aren't helpful either way. That's why I want to remove those NOTEs and keep the tests without asserting error messages, are you guys ok with this?. @novemberborn Agree. Could we do it in a separate beginner-friendly PR instead?. @sindresorhus I'm referring to err.message that gets auto-generated by AssertionError when t.deepEqual() fails:\njs\nt.deepEqual(actual, expected);\n// if no message is provided as a 3rd argument,\n// a default message will be auto-generated and will look like:\n// actual === expected\nSee https://github.com/avajs/ava/pull/1154/files#diff-9c6c3cd6ffcef4f57ac6e0c793b31b85L340 for how we tested it before (note 2nd argument of t.throws()).. >> I wonder if some of the new files in lib can be placed in a subdirectory instead? lib is becoming quite hard to scan.\n\n@vadimdemedes I mean, it's not a blocker, but it's probably easier for you to organize then for somebody who comes along later.\n\n@novemberborn Agree, but I just want to get it out ASAP. I'd like to postpone this to another PR (can assign this to myself) and focus on blocking issues for this PR, due to lack of a lot of free time.. @jfmengels good suggestion! I need to try that out with this PR and see if it works already. It might work, since we use char by char diffing for strings.\n@sindresorhus Aside from documentation, nothing, I think. I'd like to take another look on output of t.true()/t.false(), but it can be postponed until later. I need to limit the number of statements in the output and see how we can prevent big arrays/objects displayed in full.\nAs for docs, if anyone has some free time, I'd really really appreciate the help on updating the readme and documenting t.jsxEqual().. > Yes. Should be postponed. We need to land this PR asap. Can you open a new issue about it so we don't forget?\nSure, in a minute!\n\nSure. Happy to :)\n\nThanks a lot!. > t.deepEqual() doesn't display very useful output:\nHm, this is the output for t.true/t.false. Weird how it got displayed for t.deepEqual(), will check that.. Updated PR description with new todo items.\n@sindresorhus I removed \"Document t.jsxEqual\" todo added by you, since we're removing it.. Ok, I figured out why t.deepEqual() was missing the expected \"Difference\" output and was showing statements instead. In https://github.com/avajs/babel-preset-transform-test-files t.deepEqual() is still contained in enhanced patterns config, that's why power assert context is created for them. If it's removed from @ava/babel-preset-transform-test-files, output goes back to normal. Will create a new branch there and fix it.. Attention everyone, magic-assert just crossed off all points from the todo list and is very short from landing in master! I would very appreciate, if you tested it out in some real-world projects and report any issues here. Thank you!. @jfmengels Great catch, I'll disable assertion output for t.throws. And yes, dots were there before, will create a beginner-friendly issue. . @novemberborn @jfmengels PR updated, thanks! No more output for t.throws/t.doesNotThrow.. Added a new todo:\n\nFix code excerpt extraction from non-test files when babel-register is involved (line number doesn't match the actual one). > Is this a source maps issue?\n\n@novemberborn Yes, I think so, because it points to a compiled file, not the original.\nCool, I'll open an issue, so that we don't forget about this. Until then, I'll add a change, that prevents AVA from crashing, when there's no code excerpt available.. Done, AVA doesn't crash anymore when code excerpt points to the wrong location and is broken.. Thank you, thank you, thank you! I'm sooo excited! Can't wait for the first bug report :D\n. @ccorcos I agree, that it may be a little confusing. Are there any alternative \"outputs\" for diffs? I was following git diff's output.. I think we'll probably stick with the default \"-\" for \"missing\" and \"+\" for \"extra\" signs, like in git diff. We can improve it after some real world usage of magic assert (if needed).. Great work, @novemberborn! Thorough and to the point!\nI wonder if we could come up with a better name for babelrc: true option, since it pulls config not only from .babelrc, but from babel prop in package.json.. Closing as a duplicate of https://github.com/avajs/ava/issues/1124.. Closing as a duplicate of https://github.com/avajs/ava/issues/1124.. Thank you, @jarlehansen!. Hmm, good point. Would be great if we had a universal render-to-string package, that would detect React/Preact/Inferno/etc vnodes and render them to string.. Then t.jsxEqual() would be universal as well.. On the other hand though, t.jsxEqual(), diffs and syntax highlighting in magic assert are tied to React. Unfortunately we can't be \"easily\" universal at the moment. \nHere are the possible routes we can go:\n\nCreate packages to serialize library-dependent JSX trees into JSON, bundle them and support most popular frameworks out of the box\n\nThis way would be the most comfortable from the user's perspective, but the most time consuming and expensive in terms of maintenance for us.\n\nRemove React support and let the user handle JSX assertions\n\nLeast preferable way for both users and AVA, despite being the easiest solution. For diffs users would use libraries like chai-jsx.\n\nLeave t.jsxEquals() and make it accept JSON tree, like in t.snapshot()\n\nThis would make t.jsxEquals() compatible with every possible JSX-based library. I think this is best way to go. Developers can create similar libraries to react-test-renderer (converts JSX to JSON), which would allow AVA users to write tests with these libraries.\nA shortcut could make it simpler to transform JSX for assertions:\n```jsx\nimport renderer from 'react-test-renderer';\nconst render = tree => renderer.create(tree).toJSON();\ntest('jsx', t => {\n  t.jsxEqual(render(), render());\n});\n``. I don't think it can be reliably implemented, because we'd have to detect which library is imported (react/preact/inferno/etc) and import accordingreact-test-renderer` alternative (and I don't think there are any right now).\nIn my opinion, we could do just fine with the third option for a while. Minimal maintenance, universal compatibility.. So steroids (babel plugins to transform JSX into JSON) are basically left for community to implement? Sure, why not? They can freely use any babel plugins in configuration.. Closing this, since the issue is now resolved.. I also agree with @mightyiam and @novemberborn. We shouldn't silently ignore CLI flag and instead display a help message instead. Plus, in that specific case, we're encouraging the incorrect use of -w.. \u2705 Readme updated\n\u2705 Lint-free code\n\u2705 Test added\nLove PRs like this one! Thanks @forresst!. Ah, excuse me for quick merge, I read the refd issue and thought you all agreed on the message.. > Why did we remove the old formatter code? To get consistency in how we were displaying objects etc?\nYeah, for consistency. Otherwise it'd be pretty weird to see completely different looking output for different assertions.\n\nWould it be an appropriate short-term fix to just show the last property, and hope it's not too massive?\n\nTotally agree. Perhaps we could even remove \"statements\" output for t.true/t.false until it's more production ready and just show actual/expected. What do you think?\n\nLonger term it might still be useful to show each property in the MemberExpression, but perhaps just their type? I don't think we can do that with pretty-format though.\n\nOutput would still be pretty long, not sure we'd want to show each property. I'd go for a proposal in #1205.. Also a possibility, but I'm pretty sure it wouldn't solve the problem with the long output.. > That doesn't leave us with much power-assert output does it?\nYes, since \"statements\" output is the only thing using it.. Nope, there wasn't any specific behavior inside t.jsxEqual. I just thought it may be easier for 3rd-party transforms to detect what needs to be transformed, if we had t.jsxEqual.. @jfmengels I thought to add it, just to clearly indicate for 3rd-party babel plugins what JSX needs to be transformed.\n@novemberborn Actually, this issue can be closed, since goal is to not have restrictive preferences and be open & universal to all JSX-based libraries. I should create a new issue describing my thoughts and a possible route we can take.\n\nBut I'm not convinced those plugins need to latch onto t.jsqEqual calls. They can just detect the JSX AST nodes.\n\nI don't have experience in that area, so I can't argue with this. If it's just easy as detecting t.jsxEqual, I'm fine with having just t.deepEqual.\nOne thought: would this confuse users who'd use t.is with JSX too?. Nope, but I'm wondering if users will assume t.is is for shallow compare (JSX) and t.deepEqual for deep JSX.. Yes, also true.. New developments on this issue. It's now centered around t.snapshot() instead of testing equality of JSX. The idea is to have separate and optional babel plugins, which would transform t.snapshot(<Hello/>) into t.snapshot(reactTestRenderer.create(<Hello/>).toJSON()). Those plugins would only be responsible for injecting the renderer and placing JSX into reactTestRender.create(JSX).toJSON() code and putting it inside t.snapshot() args.\nRenderers:\n\nreact-test-renderer for React\npreact-render-to-json for Preact\n\nUser should be free to choose any test renderer they desire, but in case we already have a babel plugin for it, they can just go with that.. Thank you for a detailed bug report, I just reproduced it and looking into it.. Hm yes, this makes the issue gone indeed. I've been debugging this issue inside jest-snapshot, and the problem lies in saving snapshot file. For some reason, on the second run, it reserializes/reencodes the last item in the snapshot.. Found this issue, seems to be related, from what I've observed - https://github.com/facebook/jest/issues/2478.\nIf you keep a snapshot open in the editor, you'll see how it gets modified on the 2nd run.. Just opened https://github.com/avajs/ava/pull/1223 - a dead-simple snapshot implementation, I hope it will eliminate such bugs and you can use t.snapshot with almost any kind of input ;). By the way, I would really appreciate it, if you could try this branch on your project and see if it solves existing/introduces new issues, if you've got some free time. No worries if not, though!. This is a good catch, will fix. Thank you a lot for taking the time to test it out!. > @vadimdemedes I assume magic assert doesn't care about key order? I suppose the snapshot comparison does, which seems fair.\nNope, it doesn't. t.deepEqual() doesn't care about key order, so you won't simply get an error, so it won't even \"get to\" magic-assert.. Could you post your failing test?. I released a few minor fixes to the underlying dependency (@ava/pretty-format), so make sure to run npm update before rerunning your tests. You should now see assertion output.. Great, thanks for reporting, it got resolved thanks to you!. Hey @lusentis, I don't think it can be considered a bug, since d is still contained in that object, we can't skip it because it's undefined. If you try the following, it will also error:\njs\nt.deepEqual({a: 1, b: undefined}, {a: 1});. Right, it is an unexpected behavior for sure. This is happening, because we use JSON.stringify to serialize snapshot data and it skips undefined values.. @lusentis Thanks for trying out this PR and reporting the issue!. A quick fix would be to JSON.parse(JSON.stringify(actual)), when comparing to expected value stored in the snapshot. Not sure about the drawbacks of this though. Otherwise, we'd have to manually serialize incompatible values like undefined and Function using a replacer parameter.. We could use pretty-format to serialize values into snapshots, but then we wouldn't get a highlighted diff (when t.snapshot fails). I think it'd look weird, if only t.snapshot didn't support diff highlighting, while other assertions did.. No, not @ava/babel package, but ava/babel.js file. This file would simply serve as a preset, which would include @ava/stage-4 and @ava/transform-test-files. Think of it as a handy shortcut, nothing more.\n. > I think we should wait and see how the new presets get usedI think we should wait and see how the new presets get used\nCould you explain that more? I didn't get the point.. Sure. You can easily cause a crash if one of the statements contain (or is) a reference to a global scope (global). There's a ton of stuff in there and it doesn't make sense to print that.\nSame goes with all the libraries. Unless it's a plain object or array, we can't pretty print it. Even if we could, why would we? The output needs to be meaningful and compact. So in the above example, t.true(some.spy.calledOnce) it's enough to print that some.spy.calledOnce is false.. Sure, Map and Set are ok. Talking about instances of classes/libraries. For example, you wouldn't want to pretty print an instance of Mongoose model, right?. I think implementing this might be overly complicated and could take a while. This bug is quite annoying actually, I've seen it at least 6 times during last 2 days (yes, I've set up a counter \ud83d\ude04). I'd just go for a set of accepted types to be pretty-formatted in the \"statements\" output and push a fix. We can always make it more complex later ;). Totally makes sense! We should avoid extracting everything from node_modules and .gitignore. .gitignore requires slightly more work, but ignoring node_modules should be pretty easy.. I wouldn't want t.deepEqual to check for the order of keys. As for t.keyOrder, this feels like a \"nice-to-have\" feature, which generally we are against from. If someone absolutely has to check for the order of keys: t.deepEqual(Object.keys(obj)).. Hmm, I will look into this, thanks for reporting!. Not sure I'd want such \"rich\" output. I think \"-\" and \"+\" would be enough, we just need to display removed/added values after all.\nI'm a big fan of the proposal by @jfmengels. Jeroen suggested we treat a diff like a set of guidelines to fix the error. So \"+\" would mean \"add this\" and \"-\" - remove this. So instead of \"actual\"/\"expected\" we tell how to fix this problem. I don't think I've seen this before, it feels like an interesting approach to take.. Yep, agree.\nBut we'll still need relative path to fix #1231 (skip code excerpts and magic-assert for errors coming from node_modules).. I think it's a duplicate of https://github.com/avajs/ava/issues/1231, since the fix will be the same for both issues. The plan is to skip code excerpts and magic-assert output for all errors coming from node_modules directory.. Oh, right, that's a good point. Will add a check to detect the assertion error. Should be not so strict though, to support chai, expect, etc.. This is nice! I like the new isWithinProject and isDependency props.\nBuilds are failing though. Otherwise, \ud83d\udc4d . @novemberborn Thanks for a detailed writeup, very helpful!\nI think I'd want to go with a new solution to snapshot testing and try to find a better way to serialize snapshot values. We need to look for a library that handles serializing/deserializing of data and which also includes types that JSON ignores. Any tips are welcome!\nI think I've seen some time ago something like an extended JSON serializer, which handled most of its limitations. Need to look for it.\nThe reasons why I'd go without jest-snapshot are:\n\nIt brings unnecessary and unused dependencies, such as jest-diff and jest-matcher-utils. We aren't using jest-snapshot's error message, so the diffs provided are useless for us.\nWe can't pretty-print or diff the values and display the same rich output other assertions provide.\nIt's not as open as I'd wish it could be for other JSX-based libraries, like Preact, Inferno or others. See #1254.\n\n\nUnfortunately, because it uses JSON serialization it limits the kinds of object structures we can safely snapshot:\nno support for custom classes: instances are reduced to plain objects\n\nI don't think there's any possibility for that to work with any serialization/deserialization method, is there? We just can't \"recreate\" non-native (Map, Set, etc - language features) instances.\nAs for key order, I'm not even sure we'd want to check that. In what cases key order of objects/props (in React components) affects the output/functionality? I can't think of any at this moment, because I've never encountered such a problem.. Thanks for submitting such an easy-win PR, love it! Obviously, I'm all for perf optimization, especially with such a little change. However, this module seems to be unmaintained. The last commit is from 2015, despite having open issues and PRs. \nPerhaps you are aware of some alternatives?\nAlso, can we swap the usage of 'diff' to the other library, so that we don't have 2 dependencies that basically do the same thing?. Well, if there are no viable and just-as-fast diff libraries out there, we can go with this one at least for string diffs, I think. It's better to use this library, than to have 20m of waiting time.. This one doesn't have tests, so it's not a good candidate. Ok, so I see there aren't that many diff libs out there, I think we may just go with the one you used in the PR. I will test how it works and what output it provides, thank you!. I agree, we should change the string color from red to white. Thanks for reporting this!. The red color for strings is a default value set in @ava/pretty-format module (at the end of index.js in default props for 'theme' prop, if I'm not mistaken).\nNow that I think of it, it'd be inconsistent if strings in other types of output (comparing objects, for example) were red, but white in string diff?. How about the following:\n\nBefore generating a string diff, strip the red color from the string using strip-ansi module, which we already depend on.\nGenerate a string diff, colorize actual/expected chars (red/green background) and use a red color for unchanged values here https://github.com/avajs/ava/blob/master/lib/format-assert-error.js#L59.\n\nThat way the red color won't interfere with the highlighting of actual/expected values.. > If you're ok with hardcoding the red colour in format-assert-error.js then that's all that's needed. \nYeah, hardcoding is rarely a great solution. You can hardcode it for now and I'll take care of it later by creating a \"theme.js\" with colors passed to pretty-format.\n\nNo need to strip any ANSI codes out first.\n\nWouldn't it end up with duplicate red color ansi codes?. Haha, no worries, I totally forgot we already have a colors.js as well, so no need for a theme.js!. Tests are failing, but other than that - works great, just tested it!. Thank you, good fix!. Damn it, Mark is on a row!\n\nRather than printing Actual/Expected, vary those labels by the assertion, depending on whether the assertion was improperly used or failed. This creates more helpful output\n\nBig fan of this change, removes the need to guess what output means.\n. @novemberborn I think I tried to make a regex that'd be as simple as possible and match \"standard\" stack lines. The point of that was to get only stack from err.stack.. Mark, how can I buy you a beer?. @novemberborn \n\n@vadimdemedes \ud83e\udd23 whenever we meet in person I suppose. Or there's services like https://honestbrew.co.uk/ \ud83c\udf7b \ud83d\ude1c\n\nDM me your address and which beers you like (I'm not an expert in this area).. . I'm so looking forward to this change! . @lukechilds Do you think this would make the extending easier? https://github.com/avajs/ava/issues/1225\nava/babel (babel.js in root) would always export the current Babel setup, therefore situations like the following shouldn't occur:\n\nif in the future AVA decides to use a different default preset I'm gonna be stuck on stage-4 or get an error if that file gets deleted.. Yes, only with that example:\n\njson\n\"ava\": {\n  \"babel\": {\n    \"plugins\": [\"transform-react-jsx\"],\n    \"presets\": [\"ava\"]\n  }\n}\nwe'd need to have babel-preset-ava pkg, which is also a possibility, but not that different from what we have now. Only a different, even though nicer, name. ava/babel seems to follow the established convention of bundling pkg-related babel presets/plugins at pkg/babel.. As for @novemberborn's concern about using AVA's presets in .babelrc, it would still be possible with ava/babel, if anyone wants to do that (I never do). At least one popular pkg example which uses this \"convention\" is glamor.. Does Jest \"explain\" the - and + in each diff or once per output?. I don't think it should be in the core, since it's easy to do what you're suggesting without AVA's involvement:\n```js\nconst keys = obj => Object.keys(obj || {}).sort()\nt.deepEqual(keys(response), ['foo', 'bar'].sort())\n```\nIf we would add it to AVA, it'd most probably have a long name like t.deepEqualKeys and we'd have to add a t.not* method for it as well. This doesn't sound like something we would want, I think.. I don't think there's a standard way for it. Perhaps @dlumma could use TAP comments for it. See https://github.com/avajs/ava#tlogmessage.. > Heh, \"pretty much\" implies there might be some behavioral changes?\nI don't think so, since I haven't modified AVA's tests and they pass \ud83d\ude09 . Interesting idea, I think it would help with correct console.log() handling.. > I think rather than mimicking the rather low-level fork(), we should abstract the \"pool\" and how it communicates with the API and run-status.\nThat doesn't sound like an easier way to implement this right now to me :) This PR isn't that complex, is it? What improvements specifically do you have in mind?. But this PR doesn't change events, it just switches process.on and process.send with adapters. Refactoring that you have in mind could be done afterwards with the same outcome imo.. > I'd really like this to be a first class implementation, rather than a mimicking of how fork and test-worker interact. Cause I've been looking a lot at that code lately and I still can't load it all into my head.\nDefinitely agree with this. Ok then, we can put this PR on hold for now, I'm super busy lately anyway.. @novemberborn Of course ;). Could AVA only output some error to tell a user that a certain test suite was still running instead of waiting for all the I/O to finish? If all the tests completed, but something is still running, that test suite should be considered failing right away.. Pushed an update.\n. Sure, going to add that now.\n. Pushed an update.\n. Sometimes I forget I write docs for developers, haha\n. Will commit your version soon, thank you ;)\n. thanks, will change ;)\n. Yeah, I thought about converting this.serial and this.concurrent to return promises, but decided postpone that. Wanted to hear feedback on a PR and thought that you might not like too much changes at once, haha\n. I could do that after this PR gets merged (to avoid merge conflicts in commit history), no probs\n. Forgot to remove that\n. Yeah, it does. My goal was to switch to promises with a minimum code changed.\n. Do you prefer .then(this.concurrent.bind(this, concurrent))?\n. There was no error message before, so I did not insert it too. Also, there wouldn't be a meaningful message, only smth like \"Error happened during tests\" and that's it. The goal of this line is just to prevent execution of the following tests.\n. Oh, right, I am just checking notes from the PR, not from the files themselves.\nBecause previously, when error happened, this.end executed and that's it. So that's exactly what I did here. In case of an error, just exit.\n. Yeah, sure\n. Yeah, thought of that too\n. Just wanted to get a quick feedback on the PR as a whole\n. Why have 2 functions for a one task? You can just leave it as setAssertModule.\n. What is a self-executing function for?\n. I also thought to use some kind of IPC, but turned out that you can send messages only parent -> fork way, but not the opposite. So in a PR, fork process detects if it is a fork (by checking process.env.AVA_FORK) and outputs passed tests to stdout and failed ones to stderr. If you could recommend a better way of exchanging data between processes, that would be great! I am not a huge fan of the current method either.\n. For logger to access a runner object, there would need to be some kind of initialization to get a reference to it. For example:\n``` js\nvar Runner = require('./lib/runner');\nvar log = require('./lib/logger');\nvar runner = new Runner();\nlog.init(runner);\n```\nSo I decided to skip it for now.\n. Oops, I missed that there are Runner and Test there, sorry. I would leave setAssertModule on Runner and when you create a new instance of Test, assign it there - https://github.com/sindresorhus/ava/blob/master/lib/runner.js#L34.\n. You can do it in the constructor of a Test.\n. Damn, missed this somehow. Will convert it to .send() later.\n. In that case we need to know why it fails :)\n. That's totally fine, it is not a resource-expensive task.\n. Functions inside functions is a really no-go, please separate it out.\n. title and fn are more common properties, while assertModule is absolutely optional. I think it needs to be the last one in the argument list.\n. So you are saying fork.js should return a Promise? But it also needs to emit test event for each test.\n. How can it both return a promise and emit events? I had this in mind:\n``` js\nfunction fork (file) {\n  return new Promise(function (resolve, reject) {\n    var ps = child_process.fork(file);\n// need also to emit test events somehow\n\nps.on('close', function (code) {\n  if (code > 0) return reject();\n\n  resolve();\n});\n\n});\n}\n```\nAs a workaround, we could accept test listener:\njs\nfunction fork (file, test) {\n  ...\n  ps.on('data', test);\n  ...\n}\n. js\nps.stdout.pipe(process.stdout);\nps.stderr.pipe(process.stderr);\nThis is not for sending data from forked processes. I am sending data via process.send(). This is for piping output from forked processes, so that console.log()s won't be hidden, when executed via CLI.\n. But then, if tests are executed directly, via node test.js, there won't be any output.\n. I decided to skip reject(), because there was no logic for that previously. There was just an exit() function, that outputs number of passed/failed tests. And what kind of result you are thinking of?\n. Good point.\n. Don't understand. This function returns a promise. It also emits message event on completed/failed test, like we agreed earlier. Returned promise resolves, when all tests are completed.\n. We would still need to sum all results and display all errors at the end.\n. No issues here, since it is in cli.js. This is an entrypoint file.\n. If test wouldn't wait for pending assertion, a.duration would be (or close to) zero. So if a.duration is more than 1234, it means test was waiting.\n. I am not doing new Test(title, cb) here, because we need multiple instances of that test for *each hooks. So instead I am storing title and function and creating Test instances later.\n. Executing beforeEach and afterEach hooks serially.\n. Created these helpers to DRY up.\n. Sure, np\n. Ooh, this will not look good... I propose to postpone this, until we have more options, what do you say?\n. I think it would be better to attach kill() method to a returned Promise, like on() - https://github.com/jamestalmage/ava/blob/jt-kill-forked-processes/lib/fork.js#L66.\nThat way, we won't have weird stuff in test results.\n. I don't really understand the need for killedPromise. Why don't we listen to exit event inside return Promise? https://github.com/jamestalmage/ava/blob/jt-kill-forked-processes/lib/fork.js#L36\n. Then we better store references to forks, rather than injecting a function in result object.\n\nNot sure how this would create \"weird stuff\" in test results.\n\nFunction in an object with test results is weird stuff.\n. I think I will revert this one, because it seems unrelated to this PR. I think this question deserves its own space. \n. I will revert it, just for the sake of keeping things clear.\n. Those are error messages, they are displayed when assertion fails.\n. Haha, race condition :D\n. process.stdout.write(''); was there before me, @sindresorhus should have a reason for it.\n. These descriptions still look like test descriptions. Those messages are error messages, not assert descriptions. imho smth like this would be better: t.ok(cleanupCompleted, 'clean up did not complete').\nWould be great, if you improved those ;)\n. how about test waited for a pending setTimeout or test should've exited without waiting for a timer?\n. Nah, last one is crap, first one is simple & good. What do you think?\n. To avoid conflicts with t.plan(), this.ifError(err) could be replaced with:\n``` js\nthis.assertError = new assert.AssertionError({\n  actual: err,\n  message: 'Callback called with an error \u2192 ' + err,\n  operator: 'callback'\n});\nthis.exit();\n```\nThis is the same stuff we do when promise rejects.\n. Absolutely, was going to do that here at first, but then thought about a new PR :D \n. #181 is not merged yet, so it will call t.end() with a global context (t.end() is not binded yet).\n. imho new Test.hook(title, cb) does not look good. In such case, Test.hook is a class (function), which has to be uppercase.\n. Oh damn it.\n. Just tried it, does not work, because fork resolves with test results: t.end(results).\n. No probs ;) runner.then(t.end) works though!\n. But index.js is never required directly.\n. No matter if I use node test.js or ava test.js, require.main === module is always false.\n. damn it\n. chalk.grey.dim('$') + ' ' + chalk.cyan('ava') + ' ' + path?\n. \n. \n. Haha, I changed it to err for consistency too :D \n. To conform to errback style. However, since it's the event, not a callback, I can skip it.\n. Yep, as part of that whole cleanup process.\n. Yeah, spotted this crap, will remove later in cleanup PRs ;)\n. I am afraid .dim will make it less noticeable, and the purpose of .skip() is to temporarily disable it. So if we keep it bright, it will stay in developer's focus, that something needs fixing.\n. can just be:\nps.send({\n  name: command,\n  data: data\n});\n. AppVeyor doesn't set env variables like Travis to identify itself?\n. Yeah, there's 3 of process.stdout.write('') now for no explained reason.\n. how about just: timeout 30 && npm install?\n. Great, in that case we don't need our own AVA_APPVEYOR.\n. @sindresorhus Hmm, don't those fixes also needed for user's tests?\n. Can we make this timeout shorter?\n. to avoid the if condition, you could just do this:\njs\n.replace(/\\.spec/, '')\n.replace(/test\\-/g, '')\n. > Why do you want it shorter?\nSo that tests are executed sooner.\n. it can be just return self.concurrent(tests.only), since tests.only defaults to [].\n. I think it's perfectly fine, I wouldn't call it a hack.\n. yeah, did that before, but we need path here - https://github.com/sindresorhus/ava/pull/236/files#diff-a48173105195b7a6481a3ad85b45144eR13\nWhen we will support #229, babel variable name won't be a good choice.\n. will update the comment ;)\n. Np ;) I just order stuff by module name length in my projects.\n. Ok!\n. We pass messages in lib/fork.js. Yeah, I can move it to 3rd argument.\n. Turns out it will conflict with the following:\njs\nsend('teardown', ps);\nwhere events don't have any data.\n. Good one!\n. Fully agree!\n. @BarryThePenguin we need to return a promise by the end of function and resolve/reject it later, so we can't use these right away.\n. Np ;)\n. because Promise.pending() returns something called a resolver, it's not an actual promise, it's not thenable. To get a promise from resolver, we need to get it from promise property:\n``` js\nvar resolver = Promise.pending();\nvar promise = resolver.promise;\npromise.then(...);\n``\n. The ~~main~~ only reason for usingPromise.pending()is to have less nesting/indentation in this function.\n. well promises are async, that's whytest.asyncallows them, right?\n. I don't think we need that warning, it gives no helpful information to the test output. Plus, it will not look good.\n. Copied fromlib/test.js.\n. Also curious about this one.\n. Instead of(\u203a|\u00bb), you should use:figures.pointerSmall(and requirefiguresmodule at the top).\n. Logger should not calculate total time, it should only write output.\n.lib/globals.jsand itsnow()method should be used instead ofnew Date().\n. This functionality should be tested by using API (api.js), without executing AVA's CLI.\n. Need to remove this.\n. NeedAssertclass, because assertions need to be tracked per test.\n. Parses stack trace and extracts original function name, file name and line.\n. Need to collect output first and then display it all at once to avoid collisions with other concurrentconsole.log()s.\n. Comments need to be removed.\n. Comments need to be removed.\n. Workaround to avoid modifying each test in this file. Will need to create new instance ofAssert` in every test.\n. Is there a simpler word to replace \"triage\"? Had no idea what it meant until now.\nPerhaps \"improve issues\" or something along those lines?\n. \"Triaging\" still left over there in the 2nd sentence.\n. True, I just commented it all here to have some kind of todo list ;)\n. > Do we have to make assert a class? Can't we just track assertions here?\nI'm open to suggestions!\n\nDemanding the assert class track assertions is going to make it harder to support custom assertions down the road.\n\nIt will, see my comment below.\n\nIs the issue simply that it is hard to know which argument is the message here?\n\nThe issue lies in AVA's feature - concurrency. As you know, tests run concurrently, so we have to track which assertions happened during that particular test. That's why Assert class is needed, because that way we can safely get a list of test's assertions.\n. Will errors here also have powerAssertContext?\n. What is additionalMethods option for? Couldn't find it in empower's docs.\n. Given your answer below I understood what is onAssertionEvent for, but I still don't understand what would the answer to my question be.\nHere, if assertion returns promise and that promise rejects, we just _setAssertError(), but without powerAssertContext like above (lines 237-242).\nOr (correct me if I'm wrong) if \"in that promise\" there is a failed assertion, onAssertionEvent will be triggered again with event.type === 'error'. In that case, we don't need a .catch() block on that promise, don't we?\n. Yes, only throws and doesNotThrow might return promises.\n. Was using it, because sometimes it highlights executable name and symbols like |. Will remove it, no probs ;)\n. Should we include a screenshot or that'd be too much?\n. Should be fixtures/test.js, without node_modules\n. Should be helpers/test.js, without node_modules\n. fixture/fixtures/test.js => fixtures/test.js\n. fixture/helpers/test.js => helpers/test.js\n. Ah wait, it's fine.\n. So, the cache is being stored in node_modules/.cache/ava, but later down this file versions from ava & babel-* packages are being included in the cache key.\nThat way, when users rm -rf node_modules/.cache/ava, they can be sure they removed all the cache. And they won't need to get the AVA version to remove ava-0.8.0 (for example).\n. Previous code was wrong.\nerr instanceof Error is always true, because TypeError, SyntaxError, etc all inherit from Error. So when there was a TypeError, we only showed err.message without stack.\n. And with that change, we show only err.message only in case of throw new Error() (e.g. https://github.com/sindresorhus/ava/blob/master/lib/fork.js#L50).\n. Because we use Error for our internal messages (like the in the comment above), which don't need a stack and just informational. \nErrors that need stack trace are most of the time (I think) TypeError, ParseError, etc.\n. I'd use Test worker exited with ...\n. Cool!\n. I'd use isCacheEnabled, something more verbose & logical given the boolean variable value.\n. Maybe split this into multiple lines? Does not look clean when everything is on one.\n. Or that, yeah\n. How about:\njs\ncli.input.concat(conf.files || [])\n. True\n. @kevva Yeah, I think what @jamestalmage meant, was if we have:\npackage.json\njs\n\"ava\": {\n  \"files\": [\"a.js\", \"b.js\"]\n}\nand we want to test just b.js:\n$ ava b.js\n. lib/reporters was intended to contain files of reporters only. lib/colors.js would be just fine as well\n. I think testing if fnErr instanceof Error would be better here.\n. Could you comment the code in this function?\n. Unclear variable name.\n. Same https://github.com/sindresorhus/ava/pull/415/files#r49449825\n. No need for github: prefix, it will work the same without it.\n. See https://github.com/tapjs/stack-utils/issues/2#issuecomment-169176569, if I'm not mistaken\n. Yep, fixing, thanks!\n. Yeah we do - https://github.com/sindresorhus/ava/blob/master/index.js#L42\n. I'd reverse flags (--serial, -s), so that these lines are consistent with the rest of the output.\n. The standard is short flag first, but it does not look very good to me. Of course this thing is way too small to argue over, so if guys prefer the other way, fine by me ;)\n. Not necessary for the demo\n. test('title', t => { instead of assert\n. t.is() instead of same()\n. Had to insert this rule, because it does not like https://github.com/sindresorhus/ava/blob/cleanup-sequence/lib/sequence.js#L42:\njs\nfor (...) {\n  return fn().then(function () {\n    // this anonymous function is a problem for linter\n  });\n}\n. I always try to avoid functions in functions. I will need to add another _ method.\n. Clear & informative, I like it!\n. Very good point.\n. No need to use is-windows, since all of its logic is 1 line (https://github.com/jonschlinkert/is-windows/blob/master/index.js#L20):\njs\nprocess.platform === 'win32'\n. Needed to make .catch(err, handler) work. Without this change, Bluebird did not detect AvaError class as a class inherited from Error and assumed it was handler.\n. @sindresorhus Yeah, that's what I was thinking too.\n. Sequence#run() and Concurrent#run() behavior is on purpose, @jamestalmage says it affects performance to create promises when they are not required.\n. Agree!\n. Looks way better!\n. Right, this is better.\n. Agree.\n. I think this would be cleaner:\n``` js\nvar reporter;\nif (cli.flags.tap) {\n  reporter = new TapReporter();\n}\nvar logger = new Logger(reporter);\n``\n. I'd prefer to assignapi` to reporter here:\njs\nthis.reporter.api = this.api\n. See https://github.com/sindresorhus/ava/pull/564/files#r53278132\n. Shouldn't it be \"set up\", not \"setup\"?\n. This error will never be shown to a user, it's just for us. Did not want to leave AvaError without a message. It's for us, like a code comment, to indicate that this error causes execution to bail. \n. I think having function noop() {} at the top of the file and using it here instead of anonymous function inline would be better.\n. Nah, || is enough here.\n. Yes, Logger passes api to reporters. You can set api for reporters in cli.js too, I think.\n. I don't think you need reporter.api = api if you are passing it to tapReporter().\n. why did \"at\" have to be removed? Because stack here is cleaned up by stack-utils?\n. I meant, previously we were removing at from the \"standard\" stack trace:\njs\nError: damn it\n  at Test.fn (file.js:4:1)\nBut now the replacing regex does not have it.\n. And you removed at from the stack trace in a test: https://github.com/sindresorhus/ava/pull/570/files#diff-373bdde32276db897f685ce9d49e3b2cR63. But all \"standard\" stack traces have at at the beginning. That's why I was wondering whether we work with an already cleaned up stack here (by stack-utils).\n. Yeah, stack is already cleaned up there, that's why at is no longer in it.\n. @jamestalmage insisted on not creating Promises when they're not needed, because it slows down test suites with thousands of tests. I think we need to bring this discussion up again, as the code looks like (excuse me) shit, when we need to return variable async/sync result from Sequence/Concurrent#run(). \n. This looks a bit ugly, if condition is cleaner. \n. We don't have a hook type, but beforeEach, afterEach, before and after. So it's quicker to check for type !== 'test' than checking for all four of these.\n. Yes.\n. \", if it is a feature request\"\n. If I'm not mistaken, there's no such directive as # TODO in TAP spec. For TAP, we could treat todo tests just like skip and output them the same way.\n. > I'm not even sure whether they should be counted as tests now!\nI agree with that. But for the sake of simplicity, we can just mark them as skipped tests internally, to prevent any kind of execution.\n. Extra \".\" here\n. \"Ava\" -> \"AVA\"\n. \"AVA does not support running tests in browsers yet.\" - maybe\n. \"window\" -> window\n. \"Some libraries require window object to be defined.\"\n. \"React.js\" -> \"React\"\n. \"This recipe works\"\n. Instead of first/second/Nth step, better provide a meaningful title that indicates what this step is about. For example, \"Install jsdom\"\n. Missing dot.\n. Ava -> AVA\n. Set up jsdom\n. I think this part is out of scope of this recipe.\n. Configure all tests to set up jsdom, or something like that\n. Enjoy!\n. Missing dot\n. node.js -> Node.js\n. folder called \"helpers\"\n. requires a setup file (no \"our\")\n. Yeah, '.throws() returns the rejection reason of promise' is a correct version indeed.\n. In that case, great! :)\n. \n. Agree, that should be changed.\n. If a human can not see todo and skip tests, shouldn't we also remove color from there?\n. But wait, if the next test is slow, user theoretically can actually see skip/todo tests. So I guess we should keep the prefix and color. \nAlso, what about changing prefixes to these?\n- if todo test, prefix = To-do:\n- if skip test, prefix = Skipped:\n. > That's what this PR does.\nOh my God, what an embarrassment, I got confused with the new \"Files\" view and read the wrong column in diff...\n\n\nHow about we just not show them all together? So if a slow test is running, the last actual test is shown, not todo/skip.\n\nEven better.\n. Exlusivity -> exclusivity\n. index would look way better\n. Could you explain why is array of N (self.fileCount) null elements needed?\n. Could you please add a comment to this?\n. That way it will fail when babel config is not even present. It should only check and display this error message, when babel config is there and has invalid value.\n. > It won't, it has a default value of \"default\".\nOh right! Updating PR to handle this case.\n. ms can't be added, because https://github.com/sindresorhus/ava/pull/654/files#diff-0730bb7c2e8f9ea2438b52e419dd86c9R689\n. Can just be typeof err !== 'object'\n. Missing newline\n. Why throwing a function here?\n. Fixed, thx!\n. It works without .js extension as well, why add it?\n. Missing newline here\n. Perhaps use t.throws() here?\n. The point of the latest update is to create less timers. That's why debounce should be on _restartTimer().\n\nAlso, you shouldn't use debounce on a prototype method, as it will debounce across every instance and only call on the most recent.\n\nAgree.\n. Yeah\n. You could use anonymous function.\n. ES6 alert! Fails on 0.12 and 0.10.\n. I order them by variable length (descending).\n. I'd prefer to rename init to _init (indicate that it's private) instead, as it's clear what containing code is about.\n. Moved back to previous order.\n. lodash.debounce contains only debounce() function, while lodash module contains all of them.\n. I think it would be cleaner to remove var convertSourceMap instead of a shortcut.\n. I'm wondering, why {'Click Me'} is between {' and '}?\n. \n. I think colors are the issue here.\n. Did not see your comment when I posted mine ;)\n. True, should be removed.\n. Thanks!\n. Because API expects resolved paths, findTestFilesSync needs to be stubbed to return \"input\" files without changes, in order to easily check them in tests. Otherwise, findTestFilesSync would return empty array.\n. Async and sync versions of handlePath need to be split out into functions to DRY it up.\n. Those tests are not deleted, they moved to ava-files.js tests, which covers these cases.\n. I don't agree. Removal of one argument does not make things any simpler in this case. I'd prefer to leave it to make it more clear. It speaks for itself, \"send ps a teardown event\". \nWhile send('teardown') leaves me (at least) wondering where is this getting sent, what is the context.\nIf you are worried about consistency, we can compromise by using send(process, 'some-event') in the child processes to send event to a parent process.\n. We'd better move it to its own utility file. wrap-send.js does not sound like a good place for it either. I'm requiring a module to wrap a function, but under the hood it checks if the process was forked?\n. Could you elaborate? I wouldn't call a line, which consists of variable declaration and a require statement, a complication.\n. https://github.com/avajs/ava/pull/896#issuecomment-223197857\n. The argument stays the same (just another file name, I chose the wrong one). I meant own module for a check if the process is forked.\n. Each behavior remains verified, because there are all those should-be-ignored directories in the fixture, that AvaFiles tests use. If at least one directory was not ignored, test would fail.\n. can just be exports.on\n. Missing newline.\n. Sure.\n. I think just the message is wrong and should be fixed.\n\nNot sure why failing tests should not be skipped/only-ed by the way.\n\nI agree, failing tests should be allowed to skip and only.\n. The point of this is not the name (I named it findOptions, because options is already defined there), but extracting options to reduce nesting. I can move it back, not insisting on this one.\n. Unfortunately, I forgot... I remember it had something to do with this https://github.com/avajs/ava/blob/ea25e802bce67b195d5f0fc331291bb02f16b866/api.js#L305. Sure, I'll use onetime.\n. Yes, it'd rename it to basePath, but in a separate PR dedicated to cleaning up RunStatus.\n. Sure.\n. I removed this completely and tests ran successfully, I guess there's no need for it.\n. Oh, I thought I removed it.\n. Done.\n. I think you should just return the promise. You don't test the output of the command, so no need for manual then/catch. Return a promise and test will fail, if the promise rejects.\n. Can simplify this:\njs\nNumber(process.version.match(/^v(\\d+)/)[1])\ninto:\njs\nNumber(process.version.slice(1)[0])\n. Good catch.\n. Fixed.\n. I was using it as a way to async iterate the items. Is there another bluebird method that does that?\n. Tests are being run from the root, so AVA finds and precompiles helpers and fixtures from the test folder (e.g. test/fixture/ignored-dirs/helpers/test.js).\nThat's why I change the cwd to make it look like the tests are being executed from that dir. I think it's actually what each test should do.\n. > This is a byproduct of the way power-assert works.\nI'd rather say that \"This is a requirement by power-assert.\".\n. Thanks for the tip!\n. 'snap' => 'snapshot'\nx.snap => x.snapshot. I'd skip the npm script and give an example with ava binary:\n$ ava --update-snapshots. Can be on the same line:\njs\nvar state = {save: saveSpy};. Can also be on the same line as well.. Can be on the same line too.. 'use strict'; is missing here.. Extra change.. Comment should be removed.. I'm not a fan of switch statements, simple if here could do just as well. @sindresorhus ?. I think example here is extra. Or it could at least be shortened to:\nTest file extension (e.g. `coffee`, `ts`). extensions might be a better name.. Comment should be removed.. Comment should be removed.. This is a fork with support for colorizing the output, from what I can see the latest release doesn't have smth similar.. Ah, need to check that.. Why not just err?. => .concat(expectedStack.split('\\n')). Remove this and see comment below.. could be:\njs\nerrors: [\n  {title: 'failed one', error: err},\n  {title: 'failed two'}\n]. Tried that, but didn't work out, because the resulting string contains code's indentation:\nActual:\n    ...\n    Expected:\n    ...\nAm I doing smth wrong here?. React is needed by react-element-to-jsx-string module. I could try submitting a PR (if possible) to avoid the dependency on React.. I was changing it back and forth, probably should be that way indeed. I didn't like capitalization of jsx is different in jsxEqual and notJsxEqual.. Cool, will do!. I will publish it as @ava/pretty-format and resubmit my syntax highlighting proposal to facebook/jest (original at https://github.com/thejameskyle/pretty-format/issues/57).. Will move my fork over to our org too.. I don't think there's any now.\nI do have one idea for these, but not sure if that can be implemented to be stable enough. We (or community) could write a babel plugin for react/preact/inferno, which detects t.jsxEqual and convert:\njsx\nt.jsxEqual(<This/>, <That/>);\ninto:\njsx\nt.jsxEqual(renderer.create(<This/>).toJSON(), renderer.create(<That/>).toJSON());\nto save the user from typing that manually or creating shortcuts (as mentioned in #1175).. This issue is described in https://github.com/developit/preact-render-to-string/issues/16, where user tried to save arbitrary string (not JSX) as a snapshot.. Sure, now that we agreed it's not needed, I will remove all the NOTEs.. Sure, will do.. Yeah, agree. . Haha yeah, it took me a while to find out about it too.. I agree, I'd prefer snapshots too.. For tests.. > Perhaps export both Snapshot and getSnapshot separately (rather than exporting the class as the main?)\nThis doesn't seem to change anything. What is the benefit of this?. Hm, but the thing is, both getSnapshot() and Snapshot aren't public. t.snapshot is an access point for these, users won't ever need to access the above mentioned stuff.. Could you move it above chalk?. you sometimes => sometimes you?. Agree, will clarify that.. Done.. Commands should start with $ and flags should come after install.. Could you also update other PRs to address this?. ",
    "arthurvr": "I guess https://github.com/sindresorhus/ava/commit/72de995171bc7a477ebc287746779a693d7f2590 fixed this?\n. :+1: \n. > Yeah, I like option two too because that's really what is expected. Using the project root would confuse me.\nMe too. :+1: \n. I like fail-fast.\n. I like fail-fast.\n. > Or even shim process.exit()? It fires the exit event (process.on('exit', ...)) so that might even be the ideal way anyway to keep full compatibility.\nTo me that sounds the most reasonable way to me. Though I guess this is kind of a rare case anyways.\n. before(function (t) { (spacing)\n. ",
    "Qix-": "I'd assume so.\n. What's the advantage of using ava.skip? Why not just comment it out? I suppose I'm missing the point.\n. I suppose I'm more of an orthodox \"no useless crap in Git\" kind of person. If you take it out, it's clear what happens in the diff, and you can put it back in whenever you need to thanks to a thing called Git. It's the same reason why I don't commit commented out code to begin with.\nThat's just me, though.\n\nHowever, t.skip() or something to that effect, in the event a test doesn't need to be run (i.e. optional dependency not being present, etc.) would make more sense - but still no sense.\nThe big reason behind my conclusion on this is Travis. If you have t.skip() in any sort of conditional, that introduces the chance that your test and Travis' test are going to be checking for inconsistent things.\nThis is kind of the point of tests - consistently looking for literally the exact same thing across all systems, every time, to spot inconsistencies with the tested code.\n\nI'd say the argument for any sort of skip functionality is weak, at best. I'm more or less decided on ava.skip, less on t.skip, so I'd love to see use cases for the latter (or former, too, I suppose).\n. Idk, I still feel strongly against it. That's what TODO's are for. Though I see your point.\nIf there is a means by which you convey to a team \"Hey, here is a test that should work, but doesn't currently - TDD that thing up!\", then skip is a misnomer. It should be indicated, clearly, that that is the intent behind the test being, for all intents and purposes, commented out.\nUnfortunately, there isn't a great way to check whether or not .skip() is being called in an appropriate manner (i.e. at the beginning of the test), other than to remove t.skip() and only keep ava.skip(). That would be the most sensible commonground solution.\n\nThough just making it clear, I think the whole skip a test thing is begging for fragmentation, inconsistencies across systems, and just general confusion.\n. I like that, except for .when() and .unless() for the same reasons as conditional t.skip().\n. I agree with the idea that if you're going to have .skip(), then there is a great case for .todo() and other types of alerts for documenting why this test is being skipped. All or nothing, I suppose.\nI'm still skeptical about conditional testing, though. Browser vs. Node I can understand, but when it comes down to test vs. test, I could see faulty PRs mistakenly being merged because, let's face it, maintainers rarely actually check the output of each test. Why? Because all test frameworks adhere to a the idea that tests are consistent across all platforms, always, and that if a test fails then the whole process will fail.\nThis kind of introduces the need to check each test run on a CI platform to ensure all the tests you really need to pass, passed. Having conditional tests will make it hard to see if the code being submitted actually passed all the necessary tests.\nAn argument against .browser() - which CI platform supports that? Travis doesn't support browser testing. I see the need for it, but it's going to make CI a very needlessly complicated thing, and the whole conditional test thing introduces a very real and high risk of headache and faulty or insecure code being introduced into a system - so much so that I wouldn't trust tests written with AVA, personally.\n\ntl;dr I like the idea of todo tests/having verbose \"This is why it's skipped\" messages, but any conditional enabling of tests is sure to cause problems.\n. Doesn't AVA enumerate tests before running them?\n. :+1:\n. If I'm understanding this correctly, the way I've seen it done before is even if t.end() has been called and the test passed, an assertion called after that point will re-show the test as failed. You don't need to know the point of last assertion, and you will never know thanks to the halting problem.\n. > A funny idea hit my head! Inspecting the test body's code, searching for .assert and similar method calls :laughing:\nGoing to assume that was a joke :sos: \n\nIf things are blocking and async functions are still at play, node won't terminate. You don't have to wait for anything unless it's a faulty test.\n. Because node won't terminate unless they're done executing.\n. Isn't it just a fancy way of doing setTimeout(fn, 0) or Window.requestAnimationFrame?\n. Another cool tool, quixote. Thanks for the link :dancer: \n. Shimming process.stdout would be simple, assuming somewhere along the way you write a newline.\n. :dancer: :tangerine: :rabbit: \n. That'll prevent coverage tests from working.\n. :+1: Starred. Istanbul is what I use and it doesn't; figured that was a global thing.\n. No idea how signals work with threads. Id have to do some research.\n. Well right. I'd assume each fork would have to register its own signals. I've never done that before, but I'm pretty sure it's analogous to how it works on the native level. And then there's windows...\n. Fair enough. It's an edge case but maybe when the core is a little more stable in terms of the other forking issues that are open, perhaps we can reconsider this. :+1:\n. Without looking too deeply at the code, is there an option to show full stacks? This is one thing about Mocha I actually hate; a lot of the time I'm writing two modules that complement each other concurrently, and the stack traces never show the other's stack entries unless I use --full-trace.\n. I'd have to see a full stack. If it's just removing lines caused by the tests then that should be just fine.\n. Well more specifically with Mocha in general, it removes any stack entries that have node_modules in them. I suppose it makes sense, but at the same time it's somewhat futile and time-wasting because, in the case of errors with suppressed node_modules stack lines, the error message doesn't normally correspond with the actual error site at the \"top\" of the trace, causing confusion.\nJust wanting to make sure this isn't the case.\n. Not quite, but that is true. For instance,\n``` javascript\n// mocha\n// Note, `my-module' is not part of the source tree, but it's still mine\n// in this example. Maybe a sister/plugin module, etc.\nvar myModule = require('my-module');\nit('should xyz', function () {\n  myModule(); // throws\n});\n```\nstack:\nError: some sort of error\n    at function (test.js:8:4)\n    at <mocha stuff>\nIt doesn't show where inside the module my-module the error was thrown. I have to pass --full-trace in order to get that information.\n. Then assertions won't show up correctly. :tangerine: \n. I understand the desire for clean stack traces. I just heavily question the usefulness of this particular approach.\n\nTape's stack errors are even more useless than Mocha's if all they're showing is the deepest user-script call site of the error. Many times, the parts of the stack that make the error's cause immediately clear are the parts above (deeper) the deepest user script. For example:\nTypeError: Cannot convert object to primitive value\n    at String (unknown source)\n    at util.js:38:25\n    at String.replace (native)\n    at Object.<anonymous> (util.js:35:23)\n    at Object.<anonymous> (console.js:25:36)\n    at EventEmitter.<anonymous> (/project/src/routines/debug/boot.js:16:21)\nWithout looking at boot.js in the above project, I can safely assume looking at the affected line (boot.js:16) by itself is probably not clear about what exactly is going on. Take a look.\n``` javascript\n'use strict';\nvar path = require('path');\nvar message = 'TypeError: Cannot convert object to primitive value';\nvar stack = [\n  '    at String (unknown source)',\n  '    at util.js:38:25',\n  '    at String.replace (native)',\n  '    at Object. (util.js:35:23)',\n  '    at Object. (console.js:25:36)',\n  '    at EventEmitter. (' + __filename + ':16:21)',\n].join('\\n');\nvar dir = path.dirname(module.filename);\nconsole.log('DIR', dir);\nvar split = (stack || '').split('\\n');\nvar related = split.filter(function (line) {\n    return line.indexOf(dir) > -1;\n});\nvar beautiful = message + '\\n' + related.join('\\n');\nconsole.log(beautiful);\n```\nwhich outputs\n$ node test.js\nDIR /private/tmp/avatest\nTypeError: Cannot convert object to primitive value\n    at EventEmitter.<anonymous> (/private/tmp/avatest/test.js:16:21)\nI'd have no idea what was going on, making the stack trace absolutely useless - hence why I asked if there was a way to show full traces.\n\nIf the intent here is to only show relevant stack lines, so that we exclude:\n- Lines that are part of Node.js proper\n- Lines that belong to the AVA module\nthen why not filter on lines that have absolute paths, starting at the first found? The logic here is that, beginning at the deepest source file (going upward through the stack), we filter out anything that isn't a user script. We also show anything deeper than the deepest user script.\nThis is an approach I've used elsewhere for clean error printing and I've never run into problems with not seeing enough information.\njavascript\n/\\((?:[\\\\\\/](?:(?!node_modules[\\\\\\/]ava[\\\\\\/])[^:\\\\\\/])+)+:\\d+:\\d+\\)/\nThe above regex looks for all () groups with absolute paths inside of them, along with line numbers and characters (i.e. (/a/b/c/d.js:10:5)). It excludes those that have node_modules/ava in them.\nExample:\n``` javascript\n'use strict';\nvar path = require('path');\nvar message = 'TypeError: Cannot convert object to primitive value';\nvar stack = [\n  '    at String (unknown source)',\n  '    at util.js:38:25',\n  '    at String.replace (native)',\n  '    at Object. (util.js:35:23)',\n  '    at Object. (console.js:25:36)',\n  '    at EventEmitter. (' + __filename + ':16:21)',\n  '    at Object. (console.js:25:36)',\n  '    at Object. (console.js:25:36)',\n  '    at Object. (console.js:25:36)',\n  '    at Object. (console.js:25:36)',\n  '    at EventEmitter. (' + __filename + ':16:21)',\n  '    at EventEmitter. (' + __filename + ':16:21)',\n  '    at Object. (console.js:25:36)',\n  '    at Object. (console.js:25:36)',\n  '    at Object. (console.js:25:36)'\n].join('\\n');\nvar reg = /((?:\\\\/+)+:\\d+:\\d+)/;\nfunction beautifulStack(stack) {\n  var found = false;\n  return stack.split('\\n').filter(function (line) {\n    var relevant = reg.test(line);\n    found = found || relevant;\n    return !found || relevant;\n  }).join('\\n');\n}\nconsole.log(message + '\\n' + beautifulStack(stack));\n```\nwhich yields\n$ node test.js\nTypeError: Cannot convert object to primitive value\n    at String (unknown source)\n    at util.js:38:25\n    at String.replace (native)\n    at Object.<anonymous> (util.js:35:23)\n    at Object.<anonymous> (console.js:25:36)\n    at EventEmitter.<anonymous> (/private/tmp/avatest/test.js:16:21)\n    at EventEmitter.<anonymous> (/private/tmp/avatest/test.js:16:21)\n    at EventEmitter.<anonymous> (/private/tmp/avatest/test.js:16:21)\nFor @sindresorhus's stack example, this yields\n$ node test.js\nAssertionError: false === true\n    at Test.(anonymous function) [as assert] (/Users/sindresorhus/dev/ava/lib/test.js:49:14)\n    at Test.fn (/Users/sindresorhus/dev/acosh/test.js:8:4)\n    at Test.run (/Users/sindresorhus/dev/ava/lib/test.js:78:18)\n    at Runner.<anonymous> (/Users/sindresorhus/dev/ava/lib/runner.js:39:8)\nwhich, if we replace dev/ava with node_modules/ava (since it's clear it's his dev environment), we actually get:\n$ node test.js\nAssertionError: false === true\n    at Test.(anonymous function) [as assert] (/Users/sindresorhus/node_modules/ava/lib/test.js:49:14)\n    at Test.fn (/Users/sindresorhus/dev/acosh/test.js:8:4)\n\nAnother example, this stacktrace:\nError: `foo` has been removed in favorof `bar`\n    at function() (/home/ray/dev/test/error.js:53:3)\n    at b() (domain.js:183:18)\n    at Domain.run() (domain.js:123:23)\n    at function() (/home/ray/dev/test/error.js:52:3)\n    at Module._compile() (module.js:456:26)\n    at Module._extensions..js() (module.js:474:10)\n    at Module.load() (module.js:356:32)\n    at Module._load() (module.js:312:12)\n    at Module.runMain() (module.js:497:10)\ngets reduced down to\n$ node test.js\nError: `foo` has been removed in favorof `bar`\n    at function() (/home/ray/dev/test/error.js:53:3)\n    at function() (/home/ray/dev/test/error.js:52:3)\nThat seems much more appropriate to me.\n\nThe point is that reducing the amount of information in errors is going to cause more headache than it solves. It should be intelligent, not \"beautiful\", as there is beauty in intelligence itself.\nIf it improves the signal-to-noise ratio on callsites, then great - I'm all for it. If it removes pieces of information relevant to debugging my code, it's 100% absolutely useless.\n. @mdibaiee No, it's an example stack trace I found on SO. My point is is that you don't need to look at the code to know that the first 5 frames are important for debugging the error.\n. @mdibaiee aren't tests a vital part of debugging code?\n. @mdibaiee Because a first glance view of all of the relevant parts of an error is important. Removing the internals of the deepest user-call will remove the relevant parts of an error.\n. Which is what I requested to begin with; however\n$ grep -r \"full-trace\" *\n$\nYou don't have the option in there.\n\n@mdibaiee add it to this PR.\n. I'd also recommend setting infinity length stack traces before filtering.\njavascript\nError.stackTraceLimit = Infinity;\n. One last thing (I know I'm persistent) but since the introduction of modules like error-ex we might run into exceptions with non-standard lines that start with something other than at; we shouldn't filter those out either.\njavascript\n// in filter function\nreturn str.substr(4, 6) !== 'at' || <other regex stuff>;\nshould suffice. Could probably be worked into the regex, but you get the idea. :+1:\n\nFor clarification, I'm referring to the fact one could add a line that said\nError: `foo` has been removed in favorof `bar`\n    in file /a/b/c/d/some-file.json                         <--------\n    at function() (/home/ray/dev/test/error.js:53:3)\n    at b() (domain.js:183:18)\n    at Domain.run() (domain.js:123:23)\n    at function() (/home/ray/dev/test/error.js:52:3)\n    at Module._compile() (module.js:456:26)\n    at Module._extensions..js() (module.js:474:10)\n    at Module.load() (module.js:356:32)\n    at Module._load() (module.js:312:12)\n    at Module.runMain() (module.js:497:10)\n. No problem! Thanks @mdibaiee :dancer: :+1: \n. :+1: Neat. Code looks good to me.\nCould there be a way to optionally specify that I want to always show at least millisecond output?\n. That's how mocha does it, and it makes sense. I don't care if a test took 200us. I do care if it took 200ms.\n. :+1:\n. Do you mean for child processes? Or just for the AVA scripts in general?\n. I re-read your comment; I think see what you mean now.\nI'd have to test, but I'd suspect that ad-hoc environment variables added to process.env would be passed down to child processes too, which means that's a plausible way of storing configuration so child AVA processes will be aware of configuration. Unless that's not how @sindresorhus plans on implementing child processes.\n. It sounds good, but it might make migrating from Mocha a bit of a pain. New projects will benefit from it; ones coming from Mocha won't.\n. True. Those that haven't should be anyway.\n. Other than the two things I saw (I'm sure @sindresorhus will see something else), it looks good to me. Seems like a clean implementation.\n. Keep them coming @vdemedes :)\n. ## \nIf we're going to add support for this, we should definitely include async/await. If we want to be realistic about it, can't we also support coroutines? I would personally prefer the standard way, and we're already depending on babel.\n\n\n. :+1:\n. :+1: for @vdemedes on the project. Lots of great stuff there.\n. :+1:\n. It'd be nice if this translated into some way to hook into Ava to allow it to use any arbitrary assertion library, including support for t.plan().\n\nSide note, power-assert looks badass.\n. @uiureo why did you close? :o\n. :+1: sounds good\n. To be honest, the binary renderer is the part that seems noisy to me. The diagram is what makes it super useful.\nThanks for the information @twada :D Very tempting to switch from should. I'll play with it a bit more.\n. I remember saying something about it before, but we just need to make it completely obvious you'll need to re-check your coverage tools, e.g. istanbul. I think @sindresorhus had an answer for that a while back. Most coverage tools don't work with forked scripts.\n. Totally on @sindresorhus for this one, I still haven't looked too closely at the ava code to know whether or not this is good. :dancer: \n. fail-fast means more to me as a programmer. I'd have no idea what bail meant.\n. fail-fast means more to me as a programmer. I'd have no idea what bail meant.\n. :O Yay. I don't know enough about the code base to see whether or not this is correct; @vdemedes and/or @sindresorhus will look it over, but this looks exciting.\n. should in and of itself doesn't have .plan(). It's on their radar, but it doesn't as of now (unfortunately). The question to be asked is what to do when .plan() is called. Doing nothing isn't ideal.\n. should is actually just the Should constructor returned by require('should'), and a binding of it is added to Object.prototype. Doing whatever wrapping you have for Should.prototype should work, and propagate to the Object.prototype invocations, if that's what you mean.\n. Remember - not all modules report successful asserts, which is something required for t.plan() to work. For instance, this was the problem with should - it doesn't, by default, report successful asserts.\n. That seems messy for two reasons:\n1. should(should) is redundant. What else would you pass to should()?\n2. If you take that out, then you have to somehow enforce should be installed before calling ava.should(), unless you package all of them together, which... don't. :)\n. Hmm. Perhaps ava.on('exit', ...), where ava detects if it's in node and registers the default handler of process.exit()? Let test implementers decide; we can add sensible defaults as necessary.\n. Or even shim process.exit()? It fires the exit event (process.on('exit', ...)) so that might even be the ideal way anyway to keep full compatibility.\n. It is, but it's still probably the correct way. Just a thought.\n. I personally like dot notation.\nWith the test name/function being testQux:\n- test/test.js or test.js => testQux (special cases)\n- test/foo.js => foo.testQux\n- test/foo/bar.js => foo.bar.testQux\netc.\n. foo \u203a bar \u203a unicorn rainbow cake is great; just make sure to substitute the character on windows since it doesn't support unicode. :+1:\n. With the nature of AVA as I understand, I can't imagine this taking much work.\n. :+1: for multiple files anyway. I think it's intended to be synonymous with Mocha's describe(), which would definitely be better with multiple files IMO.\n. Doesn't Ava have a timeout?\n. Haha no problem; common path stuff.\n. Lgtm :+1:  @sindresorhus will have to be the ultimate decider.\n. @vdemedes while I agree that that is sound behavior, I'm with @sindresorhus on it; consistency is important.\n. That gif is life... :baby:\n. That gif is life... :baby:\n. Can you write a quick test case @alebelcor?\n. Nice tool, I'll add that to my toolbox. :dancer: \n. Isn't the overhead of forking the actual forking bit? Like the allocation the OS does to fork processes is what the overhead really brings, unless it's also doing babel compilation for each fork...\n. > Why can't the system just pool the load for us instead of we doing it manually?\nBecause that would put a lot of scheduling engineers out of jobs :dancer: /s\n\nAgreed with @jamestalmage. Concatenation is a fix for something that isn't broken. I wish it were that easy.\n. So have we actually profiled AVA to determine that babel is indeed the culprit? Or is it speculation?\nSomething else to consider - forking for every test at once is going to cause problems. Not sure if that's what's happening but that causes hella context switches between the child processes if it is.\n. Perfect, fair enough @jamestalmage :)\n. You shouldn't have to. You should only have to do require('babel/register') at the top of your entry point script.\n. \"node_modules/utils\"\nWhy are you excluding just node_modules/utils? Usually you exclude all of node_modules.\n. Then yes, this is a bug within nyc. :hamburger: \n. This is badass. This might actually make me switch to Ava. :dancer: \n. Usually \"reserved keyword\" error messages are a sign either a library is outdated or your node installation is outdated. :+1:\n. Did you try rm -rf node_modules and npm i?\nWhat version of Node are you using? Something tells me Babel was removed from compiling tests.\n. Yep, that's my guess.\n. --transpile-transitive would be the longest command line flag I've seen in a while.\n. Per GNU specs on arguments, -tt would be supplying the short-form argument -t twice. Doesn't make sense. --transpile-all would make the most sense whilst still being correct. --compile-all isn't technically correct but would still probably be easier for people to type.\n. @jamestalmage very good points. However, one of the things to consider is the viability of backwards compatibility in Node. NPM thrives under the idea of progressive code reform. Node proper is no exception. I think one of the motivations behind AVA is that it is, indeed, very progressive and puts into low-priority the considerations it makes towards older versions of Node.\n. @madbence you don't have to write your lib in ES2015. The fact AVA allows you to write your tests in ES2015 is just a functional gain.\n. @vdemedes that's clever, but still not theoretically sound. We're still dealing with the halting problem here; we have no idea if the program will run differently given different inputs. Keep in mind time is an input.\nThis means that even though a small code change might only affect one or two tests according to the last run's data, the inputs might change (or the way inputs are interpreted might change) to which it will actually affect many tests - which, given the isolate data (the profiling/coverage data), do not get run. This will almost definitely introduce regressions.\nAs well, this will require a command line flag for re-running all tests. Removing a cache file or something isn't really user friendly, and now you're storing something in the working directory in order to keep that data. People will have to migrate to this model and include something in their gitignore.\n\nSide note: istanbul uses V8's profiler, which can dramatically slow things down. Keep that in mind, too.\n. What if you change branches in the project?\n. \n. \n. sigh and not even a history to prove that I'm not a potato. Thanks Github.\nBDFLAbuse #PotatoLivesMatter\n. @schnittstabil not quite sure what you mean there.\n. But by even introducing the possibility of skipping tests that could have been affected by change, you're introducing a chance of introducing regressions into your code base. Further, it becomes the test suite's fault - not something a test suite should do. Ever.\nIt's a sweet idea though.\n. +1 for watch mode. That's a great idea, @schnittstabil. However, I think the functionality should be initial run of watch-mode runs them all, and then subsequent triggers as it's watching just does the affected tests. THAT would be cool.\n. This is strange. BDD/TDD style assertions should throw if the assertion fails. Can you comment out t.ok(b === c); and see if it still throws?\nAlso, you're missing t.end().\n. This is a bug within t.ok(), then...\n. I agree that showing all failed assertions would be nice, instead of just dying on the first one.\n. Showing the last assertion makes absolutely zero sense.\njavascript\nconst ctx = new Context('test', 'hello\\n new\\n  again\\n back\\n sustain\\nroot');\nt.ok(ctx.scope.spaces === 0);\nt.ok(ctx.inIndent === true);\nt.ok(ctx.skip(5));\nt.ok(ctx.inIndent === false);\nt.ok(ctx.peek() === '\\n');\nt.ok(ctx.read() === '\\n');\nt.ok(ctx.inIndent === true);\nt.ok(ctx.peek() === ' ');\nt.ok(ctx.scope.spaces === 0);\nt.ok(ctx.read() === ' ');\nt.ok(ctx.inIndent === true);\nt.ok(ctx.scope.spaces === 1);\nt.end();\nIn the above, the last assertion is the only one that shows up, leading me to initially believe everything leading up to it was ok. Low and behold, I commented it out and the one before it (ctx.inIndent === true) also failed the assertion. Now I have to comment out every single assertion until I find the one that is actually the problem.\nIncredibly counter-intuitive.\n. should does this and to be honest it's the only way I could see reliably testing for expected exceptions. Expecting an exception but not specifying which one introduced a few regressions before I realized I needed to specify the message.\n. > Also the core Node.js people have said assert is done and won't change anymore.\nOne of my problems with the Node team. /offtopic\n. That makes sense.\n. I don't think a live editor will help since this is a test framework. What would you have to test? Make sure to attach everything to analytics to see what's actually being used. :+1:\n. I don't think a live editor will help since this is a test framework. What would you have to test? Make sure to attach everything to analytics to see what's actually being used. :+1:\n. I approve, but I would drop the legacy support and release as a major/minor. Why not go all-the-way?\nThe use of the async keyword keeps things clean and seems much more natural to me, personally. test() now remains an all-in-one method (of sorts) that can still show explicit sync/async tests.\n:+1:\n. Oh I see, nevermind. :+1:\n. For the record, t.plan() is useful to catch loop bound errors, such as when you expect a loop to run X amount of times with Y amount of assertions (t.plan(X + Y)), and the loop runs too long/short.\nAnother, probably more relevant use of planned assertions are double-callback invocations. If a callback is called multiple times, the easiest and cleanest way to detect multiple assertions is with t.plan().\nIt's not \"useless\", it's just high maintenance.\n. If you're referring to test.async(t => vs test(async t =>, why not test.callback(t =>? Gets the point across and no longer makes the former two ambiguous.\n. I think it should be recursive by default.\nIn my case, I want to mimic my lib/ folder with test files in order to yield more succinct output with nested files.\nFurther, this is how other languages' test systems work and is analogous to those for me, personally - hence my decision to structure my tests like this.\nAs of now, I have to do ava test/**/*.js which is just fine, but it wasn't what I was expecting at first.\n\nThen again, I see that's not what's happening anyway.\nNested files, to me, should have their file prefixes (in my opinion). Right now it's flat. Not sure if this is because I'm specifying files on the command line, or what.\n\n. All AVA really needs to do is include the file in question and ignore it if it doesn't import the test function.\n. @Ariporad just submit a pull request :)\n. @Ariporad just submit a pull request :)\n. :+1: couldn't find it in the issue search for some reason. Thanks.\n. ava -v would be wonderful. Not sure about individual tests, though I see the point being made.\n. > Yes, we could, but I have a feeling that comes with a lot of edge-cases\nNot really. Patching process.stdout, process.stderr and fs.write(1|2, ...) could do it completely. The only thing that this wouldn't cover is native modules writing to either of those, though that's hard to cover anyway.\nFor the first two, you could even .pipe() them to a logger stream that AVA would provide. Would be much cleaner than an override.\nDefinitely not against t.log() though, however that doesn't stand up if the code being tested (not the tests themselves) have any sort of console output.\nNot sure what #415 does (haven't looked into it yet) but I figured I'd jump here and comment. Ava is a standalone application that has the special ability to be able to set up hooks in the environment to do what it does (it's a testing framework, after all). I'm surprised we don't hook more things (side effect testing and whatnot; t.assertFileWritten(filename), etc).\n. Yeah, this is an issue for me.\nI have a simple object that is two levels deep and I'm seeing\nObject{model:\"rgb\",value:#Array#}\nwhich doesn't help me at all.\n\nEDIT: this is cripplingly awful.\n```\n  \u2716 models \u203a rgb\n  t.same(cc.rgb(), { color: { model: 'rgb', value: [0, 0, 0, 1] } })\n            |\n            Object{color:#Object#}\n1 test failed\n\nmodels \u203a rgb\n  AssertionError: {} === { color: { model: 'rgb', value: [ 0, 0, 0, 1 ] } }\n    Test.fn (models.js:6:4)\n    handleMessage (child_process.js:322:10)\n    Pipe.channel.onread (child_process.js:349:11)\n\nnpm ERR! Test failed.  See above for more details.\n```\n:person_frowning: \n\n@twada \n\nBut sometimes I also feel it's too shallow\n\nNode's default depth is 3 for require('util').inspect(). That seems to be pretty sane.\n. thanks everyone!!\n. Aha.\nt.same() isn't checking non-enumerable properties (color on the object returned from cc.rgb() in the above example wasn't enumerable).\nIs this desired?\nThe workaround is this:\njavascript\nt.same(cc.rgb().color, {model: 'rgb', value: [0, 0, 0, 1]});\nWhich won't give me as nice output if cc.rgb() returns something that doesn't have an object property color (Javascript will throw an error instead of power-assert showing me the values):\n```\n  \u2716 models \u203a rgb failed with \"Cannot read property 'color' of null\"\n1 test failed\n\nmodels \u203a rgb\n  TypeError: Cannot read property 'color' of null\n    Test.fn (models.js:8:9)\n    handleMessage (child_process.js:322:10)\n    Pipe.channel.onread (child_process.js:349:11)\n```\n\nI'd much rather power-assert's pretty output.\n. I'm fine with it failing due to non-enumerable properties though the lack of decent output explaining it caused me test-related blockage which I hate D:\nMaking those stand out would be great, if even it still fails (which is completey okay).\n. Preferably AVA could have a command such as ava --inspect that would simply run node --inspect and then issue debugger; right before it runs the tests.. @unional We do literally the exact same thing. Nobody else has reported any issues on Windows until this bug. There's no difference here. This is probably a configuration issue happening inside of AVA.. It is the same. Check supports-color level reports, here. Nobody has reported anything on windows other than in this ticket. This is an AVA issue until I see definitive reasoning for it being an ansi-styles problem.. The github mobile site doesn't allow inline comments, so apologies for this being out of line.\nFor the child workers, why don't we just utilize support-color's new FORCE_COLOR environment variable instead of pushing down arbitrary flags?. The parent AVA process is passed --colors, etc. Then it sets the environment variable FORCE_COLOR for child processes to force them to match whatever color level the main (parent) process is running at.\nNo need to do any messy flag juggling or whatever. No need to determine which flags to pass down. No need to update flags in AVA in the future if supports-color changes how it handles flags. This is the perfect use case.\nAlso, this is exactly what the environment was made for. :man_dancing: . Issue hunt link is auth-walled :/ Any way to see it without associating it with my account?. Edited the OP to mention https://nodejs.org/api/worker_threads.html.. Does match detect extra properties?\nWhat I need is a way to check that an instance of a class strict-deep-equals an object sans constructor, since many of the constructors in this particular project have side effects. Plus, to be able to show object literals, I no longer have to guess the structure of the object (and I don't need to worry about encapsulation in these cases, so it's fine to assert the inner structure of these objects).\nDoesn't seem like all that uncommon of a use-case.. Still open, clearly.. Sorry for my thickness, but what is this actually doing? I mean, what is the intent? Isn't it just filtering out anything that's not in the project directory? What is this improving/fixing?\nIn an edge case, what happens if I require() something outside of my immediate project directory? It will be dropped, too.\n. ;) Might want to do that haha.\n. > When setup and/or teardown is required, you can use test.before() and test.after(), used in the same manner as test(). The test function given to test.before() and test.after() is called before/after each test.\n. ;) No problem haha\n. Mind updating the regex?\n(?:^(?! {4}at\\b).{6})|(?:\\((?:[\\\\\\/](?:(?!node_modules[\\\\\\/]ava[\\\\\\/])[^:\\\\\\/])+)+:\\d+:\\d+\\))\nThis won't kill the error message and won't filter out lines that don't start with at.\n. Something makes me feel iffy about re-assigning it back to the error itself. Perhaps it could just go into a temporary variable?\nThoughts @sindresorhus?\n. I think he means why just return from 119 and not doing anything on the catch.\n. @sindresorhus don't we go for !files.length? Or am I just thinking back to my C days?\n. Should use path.relative() here.\n. If a directory has test- then that's a problem. If they're on windows, \\/ might be a problem.\n. Use path.basename(..., '.js').\n. Use path.sep here.\n. Is this going to pose some sort of problem with concurrency? // @sindresorhus\n. :+1: disregard then.\n. Fair enough.\n. 5 points to whomever can spot the difference.\n. Gah, thanks. Will fix.\n. Because the next line is pretty much the same as this. Parallel == Concurrently. It's redundant.\n. I'm a little lost, could you explain a bit more?. Nvm, Sindre explained it to me. See chalk/chalk#142.. ",
    "jenslind": "What is .skip() supposed to do? Skip the whole test? Wouldn't it be easier to have something like:\nava.skip(function () {\n...\n})\nSo the test never runs? Or do I miss something here?\n. Updated the PR. The following should now work:\n```\ntest('chai', function (t) {\n  t.plan(2);\n  t.assert.equal(true, true);\n  t.expect(true).to.equal(true);\n})\ntest('should', function (t) {\n  t.plan(1);\n  t.should(true).equal(true);\n})\ntest('assert', function (t) {\n  t.plan(1);\n  t.equal(true, true);\n})\n```\nImplementing support for a.should.equal(b) will need some more work, not sure how to achieve it. :cat2:\n. @vdemedes hmm okey so how would you patch the modules? Should we rather aim to add a counter to the module itself and then let t.plan() get the count from the module? But how would that work? A counter on the module won't know about which test it's in. \n. You mean like this?\nRunner.prototype.setAssertModule = Test.setAssertModule;\n. To set the default assert lib on start. Might not be the best solution.\n. @vdemedes most tests fails if I run it there.\n. Yes ;) At work atm so haven't had time to investigate.\n. Okey my misstake, tests passing now. But I have one concern about putting it in the constructor, it will run for every declared test. Not sure if we want that, ofc we could have some bool to check if  it has already been set but that doesn't make any sense to me.\n. ",
    "tunnckoCore": "@jenslind it won't output his title and exits immediately when it's called.\nIt can't be used (currently, if it is bug) with t.end(). Just tried it yesterday.\n. @vdemedes it looks great! I also was thinking for something like it before few months, but I think this .warning would looks better as t.warning.\njs\ntest('regular test', function (t) {\n  t.warning('custom warning message')\n  t.is(true, true)\n  t.end()\n})\nThis will allow to be used in any type of test - regular, todo, node, browser.\njs\ntest.todo('test to implement soon', function (t) {\n  t.warning('but hey, be careful')\n  t.end()\n})\n. Yes, can agree with you, but i don't think that signature of test.warning() is okey.\njs\ntest.warning('this test has some weird shit going on', 'some test', fn);\n. It would be easy if use async-done + co combo under the hood. But if it's done, we should forget \"sync\" tests - in which i dont see value and dont see problem. But also with that it comes other problem - assertion, it cant stay as it is now (passing t to the test) - in which, again, i dont see problem, because this would be possible\n``` js\nvar test = require('ava');\n// which would be just claim\nvar assert = test.assert;\n// or you can just use any other assertion library\n// by requiring it.\ntest('foo bar', function (done) {\n  assert.true(true, true)\n  done() // like t.end()\n})\n```\nOr default (builtin) assertion can be in the context of the test function - i chose that way and built benz - which is thin layer on top of async-done and now-and-later (tools behind undertaker) with passing custom context. benz accepts any type of functions and any type of returns and have parallel and series flow. Vez built on benz - passing custom context, instead of passing arguments to functions as in benz.\n. > The thing is, nobody uses async/await yet and god knows when it will land in node.js.\n:+1: \nOne fairly simple and built in seconds test runner, built on benz, showing its power and flexibility.\n``` js\n'use strict'\nvar Benz = require('benz')\nvar util = require('util')\nfunction Runner (options) {\n  if (!(this instanceof Runner)) {\n    return new Runner(options)\n  }\n  Benz.call(this, options)\n  this.tests = {}\n}\nutil.inherits(Runner, Benz)\nRunner.prototype.test = function (name, fn) {\n  this.tests[name] = fn\n  return this\n}\nRunner.prototype.start = function(done) {\n  // series by defaul,\n  // pass .enable('parallel') for parallel execution\n  this.run(this.tests)(done)\n};\nvar count = 0\nvar contents = ''\nvar fs = require('mz/fs')\nvar foobar = new Runner()\nfoobar\n.option('context', require('claim'))\n.option('generatorify', require('co').wrap)\n// .enable('parallel')\n.test('foo bar', function (next) {\n  this.same(count, 0)\n  console.log('foo bar')\n  count++\n  next()\n})\n.test('baz qux', function (next) {\n  this.same(count, 1) // try change it and it will throw\n  console.log('baz qux')\n  count++\n  next()\n})\n.test('generators', function * (next) {\n  this.same(count, 2)\n  contents = yield fs.readFile(__filename, 'utf-8')\n  this.same(contents, 'expected content')\n// little tweak,\n  // will fix it in core\n  yield function () {\n    next()\n  }\n})\n.test('test content', function (next) {\n  this.true(contents.indexOf('Runner') !== -1)\n  console.log('test content')\n  next()\n})\n.start(function (err, res) {\n  if (err) return console.log(err)\n  console.log(res)\n  console.log('end')\n})\n```\nSorry if something, but it is pretty simple example.\n. > @vdemedes: added in a few lines\nyes it can. im just showing how can be done one simple runner with absolute flexibility, low dependencies and working out of the box with all kind of things - generators, promises, callbacks, series and parallel flow, pluggable assertion library and context (which context can be used for sharing contents and/or hooks and timeout as in mocha for example) and sweet controling options, which can be driven by cli.\nSomebody can want other control flow, somebody can want other promisify function and etc.. benz allow it through options.\n. Actually yes, maybe you have right and it looks that installing it one time globally is the way. But I think it's not cool, when someone want to do PR and must install it (if he dont use ava).\nAnother side is that: iojs have fast release cycle, more and more of the cool features comes in and we all dont need transpilier for most of the things. And if someone want to use ES6 he can use it without transpilier and that required dependency can be more than what he want/use from ES>=6\n\nMaybe Babel should have a Node specific package without those?\n\n:+1: \n. ",
    "MadcapJake": "Despite your strong argument @Qix- , I think there still is a case for t.skip() and that was mentioned in the referenced tape issue: test-driven development.  Writing skipped tests and sharing them with your team allows other developers to look at how you intend the functionality to work and remove the skip if they try and implement the code needed for the test to be unskipped. Or change the test to make it fit within your team's current API.  \nI do agree though, using t.skip() in any kind of conditional way is definitely a big no-no.  And even more broadly, you'd probably want to avoid t.skip() by the time your project uses any kind of CI service.\n. @vdemedes :+1: That looks great!\nTwo possible additional modifiers:\nunless\nOpposite of test.when i.e., only executes when testFn returns false.\ncritical\nIf this test fails do no proceed with other tests.  This could be a kind of modified serial test so that it's executed first (or at least just with the other serial tests).\n. @vdemedes oh, I didn't realize that a .before() failed test would block the rest of the tests from proceeding.  Do you think it would make sense for there to be .before() tests that don't block?  I could see the usefulness of both.\n. Ok! I see that now! I think I was a bit thrown because it's written as test.before which evokes it being a loophole for writing non-atomic tests :stuck_out_tongue_closed_eyes: .  It's documented and written the same as tests so maybe would be a good idea to separate it out and add a little warning note.\n. Right. I understand what it does now, I just meant that it's a bit confusingly laid out in the docs and the API is so similar to tests that it can be a bit decieving.\n. Generators don't have good stack traces though.  Also, async/await is cleaner looking and is easier to read. \n. ",
    "schnittstabil": "I don't think browser and node are very useful - What will be next? firefox, ie, rhino, ie8, node>=0.10 \u2026?\nPersonally, I prefer feature detection over environment detection:\njs\ntest.when(() => !!Promise, 'handle a resolved promise', t => { });\nBut I think the following would be even more flexible:\n``` js\ntestPromise ? 'serial' : 'skip';\n// Sadly there is no test.concurrent:\ntest.concurrent = test;\ntestPromise ? 'concurrent' : 'skip'; \n```\nUse cases are libraries like async-done:\n\nHandles completion and errors for callbacks, promises, observables, child processes and streams\n\nIf one want to test the library in environments without e.g. Promise support, some of the tests have to be skipped.\n. I think the common use case is watching the source directory and the test directory (not only the current existing files). And I suppose that the sources change far more often than the test.\n. > You're saying that it shouldn't be failing, great :+1:\n@blakeembrey Mhm, \"it shouldn't be failing\"? Well, the situation is a bit confusing :wink: \nI expect the following:\n1. Promise.resolve(new Error('abc')) evaluates to a resolved Promise, and not to a rejected one\n2. t.throws(\u2026) asserts that its Promise argument will become rejected\n3. the test fails (which it does not)\n. @vdemedes Not according to the readme.md:\n\n.throws(function, error, [message])\n\u2026\nerror can be a constructor, regex or validation function.\n. @vdemedes Thanks a lot.\n. With #83, removing does not have to be a real \"either-or decision\". It would be up to the user:\n\n``` js\n// test.js\nimport test from 'ava';\ntest(t => {\n    t.pass();\n    t.end();\n});\ntest.runAndExit(); // or a similar one-liner\n```\n. @twada Thanks a lot. Only the quotes does not match.\n. > stringify strategy that as same as JSON\nThat's reasonable. Thanks again.\n. Currently ava supports multiple before(Each)/after(Each) hooks.\nIt would be convenient to know which one failed (e.g. by its title).\n(And also which test triggered that hook.)\n. > How is not the line number the quickest way? With the line number you can just go directly to that line using your editor shortcut. With the title you have to search for the correct hook.\nThe same applies to titles of regular tests :smirk:, but I would vote for keeping them, at least in this case.\nConsidering the following, I think it would be absolutely sufficient to display the error stack, which includes the function names:\njs\ntest.before(function createFixtures() {\u2026});\ntest.before(function createTestdir() {\u2026} );\nDisplaying the error stack would also indicate that the test script itself is erroneous, and not the system under test.\nThe same holds for beforeEach/afterEach. However, it would be helpful to know the test title. Therefore, I would suggest to wrap the Error e.g. with nested-error-stacks:\njs\nthrow new NestedError(`beforeEach for '${test.title}' failed`, beforeEachError);\n. Arrow functions work different! From MDN:\n\nAn arrow function expression\u2026 and lexically binds the this value (does not bind its own this, arguments, super, or new.target).\n\nAn example:\njs\nconst fn = () => {this.unicorns = 'many'};\nglobal.unicorns; // => undefined\nfn();\nglobal.unicorns; // => 'many'\nTherefore, at least the examples are misleading.\n. Nice, t.context already works like expected:\njs\ntest(function(t) {\n    t.is(t.context, this);\n    t.end();\n});\n. > I guess all we have to do is revert\u2026\nWe haven't to, but I think it is cleaner. \n\nMaybe we should make t.context non-writable\n\nThat would be reasonable. Or we support it:\n``` js\ntest.beforeEach(t => {\n    t.context = 'foo';\n    t.end();\n});\ntest('some data', t => {\n    t.is(t.context, 'foo');\n    t.end();\n});\n``\n. There is an unnecessary(?)this.context = {}within theTest` constructor in lib/test.js remaining.\n. Nice challenge! But,  to clarify: computability theory tell us, that we cannot only take the diff of two source states and calculate which tests are affected. \nPossible approaches to solve this:\n1. we run the test before the change and gain information from it (we may persist this in some way or other)\n2. we use some restrictions, e.g.: if you use this or that naming pattern for your tests, we run the tests in cases of changes\nIt also means we cannot do it in a perfect way, we would always run tests unnecessarily, the benefit would be to decrease the number of tests to run.\n. >  We're still dealing with the halting problem here; we have no idea if the program will run differently given different inputs.\nWe should not be worrying too much about obfuscated code, it should only be reasonable and practical\u2026\njs\nrequire(new Date().getTime() === new Date(2017,3,16).getTime() ? './upload-genisys.js' : './');\n. @Qix- I think it is ok to only consider code patterns usually used. As you already mentioned, this feature can not be theoretically sound.\n. Clearly this feature shouldn't be preset. I would imagine a --quick CLI flag, or similar, --help may note its inadequacies\u2026\nWe may also use this in some watch mode: We run the possible affected test first and run the remaining tests afterwards.\n. > there's no way we can modify previous output\nWe haven't to, \u2716 bar ....\u2716.....\u2716 is outputted afterwards, but I would agree that the dots and x's would indicate some order, which is misleading.\n. @jamestalmage The syntactical order need not match the run-time order:\njs\nsetTimeout(() => t.ok(true), 2000);\nsetTimeout(() => t.fail(),  500);\n. Doesn't t.context do the job?\n. @SamVerschueren see my comment here if you want to fix that.\n. @vdemedes It is not a shortcut:\n``` js\nPromise.reject('err').catch(() => null).then(() => console.log('success'))\n//=> outputs: success\nPromise.reject('err').then(() => console.log('success'), () => null)\n//=> no output\n```\n. > \u2026  and rationalize to yourself that at least the code is \"less coupled\" or something.\nI don't do that :bowtie:\n\nthere is almost never a reason to use .then(success, fail)\n\nI mentioned one above.\n.catch(\u2026).then(\u2026) does not work, because, x.throws does not throw an AssertionException if fnErr is the expected one, and hence the .then function will be called.\n. @vdemedes if you really, really, really want to avoid .then(\u2026, \u2026) then you may prefer:\njs\n    if (fn && fn.then) {\n        return new Promise(function (resolve, reject) {\n            fn.then(function () {\n                try {\n                    x.throws(function () {}, err, msg);\n                } catch (e) {\n                    reject(e);\n                }\n            }).catch(function (errFn) {\n                try {\n                    x.throws(function () {\n                        throw errFn;\n                    }, err, msg);\n                    resolve(); //<-- dunno any sensible value :-(\n                } catch (e) {\n                    reject(e);\n                }\n            });\n        });\n    }\n(The .then .catch order doesn't matter\u2026)\n. > By default, AVA will only instrument your tests, not production code.\nThat seems to be true, but Babel polyfills are also available in production code, making AVA almost unusable for testing polyfills:\njs\n// sut.js\nmodule.exports = function () {\n    return !!Object.getOwnPropertySymbols;\n};\n``` js\n// test.js\nimport test from 'ava';\nimport sut from './sut'\ntest(t => {\n  t.true(sut());\n  t.end();\n});\n```\n```\n$ node --version\nv0.10.40\n$ node -e 'console.log(!!Object.getOwnPropertySymbols)'\nfalse\n$ ava\n  \u2714 [anonymous]\n1 test passed\n```\nOr is this a bug?\n. @vdemedes That is true, it works but:\n1. it is not documented, so the behavior might change \n2. .throws(function|promise [, error][, message]) currently would not work as expected with #125:\njs\ntest(t => {\n    t.throws(()=>{}, 'some message');\n    t.end();\n});\nBefore #125:\n```\n  \u2716 [anonymous] Missing expected exception. some message\n1 test failed\n```\nAfter:\n```\n  \u2716 [anonymous] Missing expected exception..\n1 test failed\n```\nAbout .regexTest:\njs\nt.is('actual', 'expected');\nt.regexTest(/expected/, 'actual');\n. @SamVerschueren That is possible and I would recommend that, even though switching from assert.throws  is no longer so simple.\n. @SamVerschueren I knew that and I like #125, the problem is, in contrast to most assertions in AVA, the name (throws) is the same and developers who switch to AVA may experience unexpected hard to grasp behavior.\nMaybe we should introduce our own throws assertions with a different name, in particular, because nodes throws and doesNotThrow suffer from many problems:\n``` js\ntry {\n    assert.throws(()=>{ throw new Error('abc')}, TypeError);\n} catch (err) {\n    console.log(err instanceof assert.AssertionError);\n    //=> false\n    console.log(err.message);\n    //=> 'abc'\n// I would expect something like: \nconsole.log(err.message);\n//=>'Missing expected TypeError. Got unwanted exception Error('abc').'\n\n}\n``\n. I think of more powerful assertions, maybe with different parameters. As this would also support Promises,rejects` seems reasonable. \nEDIT: or raises\n@SamVerschueren I don't suggest to change the current name, I think we should introduce a new assertion with a new name, because the behavior is not compatible with the well known assert.throws.\n. Nice, I like clarifications, but I don't think he put it best. Mainly because the examples explain more than the descriptions, but introduces (probably unknown) terms: multitasking, single-core, multicore, processor.\n. I thought about that description and has come to the conclusion that it is at least misleading.\nSequential and Concurrent is about algorithms and language design.Serial and Parallel is about hardware and run-time.\nInstead of covering these aspects they are messed up.\nAlso the answer mentions Sun's Multithreaded Programming Guide, which (obviously) only talking about Threads and states that: Concurrency is a more generalized form of parallelism. That is only valuable in that guide, but can't be transferred to newer concepts used in Node or Go\u2026\n. > I personally keep them in the test/ folder, yes.\n@jankuca Aren't your test files run? I thought they should, because:\nUsage\n    ava [<file|folder|glob> ...]\n\u2026\n  Default patterns when no arguments:\n  test.js test-*.js test/*.js\n. @vdemedes That seems unnecessarily complicated to me. Just a suggestion:\njs\n    if (fn && fn.then) {\n        return fn\n            .then(function () {\n                x.throws(function () {}, err, msg);\n            }, function (fnErr) {\n                x.throws(function () {\n                    throw fnErr;\n                }, err, msg);\n            });\n    }\n. That will catch errors from fn and errors thrown at line 75\u2026\n. x.throws(function () {}, err, msg); throws an error, and is evaluated if fn isn't rejected.\n. 1. two or more tasks?\n2. I wouldn't say starting and completing is necessary\n3. I'm not a native speaker, but I wouldn't use the word when in that context to express a condition.\n. two or more tasks can start, run, and complete in overlapping time periods, that is also true for Parallelism, isn't it?\n. ",
    "mdibaiee": "We do emit an error when the assertion count doesn't match planned. Why do we need the _pendingAsserts in the first place?\nI have probably misunderstood this bug, can you please clarify? @kevva \n. @sindresorhus: I see, I'll take a look to see if I can fix this when I get some free time.\n. @sindresorhus: Hey,\nI took a look, couldn't find a solution, it might be possible if we have a timeout, but then, we can't just wait for the timeout to pass for every test only because it might be doing something wrong.\nA funny idea hit my head! Inspecting the test body's code, searching for <first-argument>.assert and similar method calls :laughing:\n. @Qix- yeah that was a joke.\nHow can we know if there are async functions still in progress? process._getActiveHandles() and process._getActiveRequests() only work for IO handles/requests.\n. Hey, I'd like to work on this. I opened a pull-request and I want your feedback on that. #29 \n. About promises, I couldn't find a way of finding the stack trace for them, and as it's self-explainatory that the the returned promise was rejected, I think we can just ignore printing and stack trace for that.\n. I see that this approach doesn't work in some cases, consider this example:\n``` javascript\ntest('something', function(t) {\n  t.is('hey', 'hey');\n  doSomeTest(t, 2);\n  doSomeTest(t, {});\n  t.end();\n});\nfunction doSomeTest(t, x) {\n  t.same({x: x}, {x: {}});\n}\n```\nThis is the output for this example:\n1. something\n  { x: 2 } === { x: {} }\n    at doSomeTest (/Users/mahdi/Documents/Workshop/github/ava-test/test.js:13:5)\nBut we should also show the line for:\nat Test.fn (/Users/mahdi/Documents/Workshop/github/ava-test/test.js:9:5)\nI think that's what @Qix- means. I'm going to see if I can find a better approach.\n. Okay, I think I found the ultimate solution \u270a\nWe can filter out the stack lines which start with the directory of the test file, see:\n``` javascript\nlet dir = path.dirname(module.parent.filename);\nvar split = (result.error.stack || '').split('\\n');\nlet related = split.filter(line => line.indexOf(dir) > -1);\nvar beautiful = result.error.message + '\\n' + related.join('\\n');\n```\nWhat do you think?\n. @Qix- Why? I tested it with the example I said above, it's working correctly, have you tested this?\nOutput:\n```\n0:50 \u00bb ~/Documents/Workshop/github/ava-test $ node test.js\n  \u2716 something { x: 2 } === { x: {} }\n1 test failed\n\nsomething\n  { x: 2 } === { x: {} }\n    at doSomeTest (/Users/mahdi/Documents/Workshop/github/ava-test/test.js:13:5)\n    at Test.fn (/Users/mahdi/Documents/Workshop/github/ava-test/test.js:7:3)\n```\n. @sindresorhus Do you think I should squash my commits into one?\n. What's the plan now? Anything else has to be done in this PR?\n. @Qix- Can you give me the actual test that produces this stack?\n\nat String (unknown source)',\n  '    at util.js:38:25',\n  '    at String.replace (native)',\n  '    at Object.<anonymous> (util.js:35:23)',\n  '    at Object.<anonymous> (console.js:25:36)',\n  '    at EventEmitter.<anonymous> (' + __filename + ':16:21)',\n. @Qix- My point is knowing what your fault was in the error is good enough for debugging most codes, and starting debugging of others.\n. @Qix- You know the answer, yes. And how does that relate?\n. I don't think so. I don't understand your worries, almost all commands have a --verbose option, do you know why? Because I don't need to know all the details everytime, I don't want ls to be like ls -l everytime, it takes spaces, it takes my time and my attention.\nIf you really think all relevant paths are important, alias ava='ava --full-trace'. Problem solved. I'm not continuing this discussion.\n\n@sindresorhus if we are going to ship this, should I implement --full-trace in this PR or a new one?\n. @sindresorhus So what's the plan now? Don't filter anything out? Use @Qix- 's RegExp?\n. @sindresorhus Alright, sorry for the delay, I was busy working on another projet, are we ready to go?\n. @sindresorhus done\n. \u270a\u270c\ufe0f\n. I think it's a good idea but I'm not sure if it has any drawbacks as I lack experience in this kind of thing.\n. @sindresorhus Then how should we check for flags inside test interface? Probably a helper function like this?\njavascript\nfunction hasFlag(key) {\n  return process.argv.indexOf(`--${key}`) > -1 || process.argv.indexOf(`-${key}`) > -1;\n}\n. Great idea! I used async functions in my last project and I have to say it's awesome!\nFrom my experience, you have to enable asyncToGenerator, too. I don't remember why but I couldn't get it to work with es7.asyncFunctions alone.\n. This is fixing the issue defined here.\nIt doesn't necessarily have to show the stack for your required modules, users are least concerned with how the error was generated in the module they are using, they're mostly interested in how their code causes an error, afterall they are testing their own code, not their modules.\nIt's about defaults, what most people prefer, users usually prefer simplicity, and if they really want the full stack, we can provide an option for that (--full-stack as you said).\n. ",
    "SamVerschueren": "Agree on this one, but should be implemented after https://github.com/sindresorhus/ava/issues/27 I guess. If people want the full blown output, they can then enable another reporter.\n. :+1: \n. Should AVA already work with nyc? Trying it out right now but it reports no coverage at all which is quite odd.\n. Not sure when the strict variants where introduced though. They do not exist in 0.12 so maybe we should use a ponyfill for older versions?\n. It would make testing for an error really nice and clean.\n. It would make testing for an error really nice and clean.\n. @floatdrop Yes I know that's possible, and I already do that at the moment. But I think it would be a very nice addition that the throws assertion method can handle rejected promises. This would make some tests methods a lot cleaner.\n``` js\ntest('throws error', async t => {\n    t.throws(await fn(), Error);\n});\ntest('throws error 2', async t => {\n    try {\n        await fn();\n        t.fail();\n    } catch (err) {\n        t.ok(err);\n    }\n});\n``\n. Awesome! Thanks @vdemedes for the PR and thanks @sindresorhus for merging! Will make testing for erros way cleaner!\n. Not sure if I should create a new issue for this, but when using promises (await statements),power-assert` does not show the correct value.\nt.same(await fn(fixture1), {})\n             |\n             Object{}\nThe result of the promise is not empty, that's for sure :).\n\nSeems I was too fast on this one. Apparently, the object shown underneath is the same as the second argument of the assert, which is not the actual result.\nt.same(await fn(fixture1), {foo: 'bar'})\n             |\n             Object{foo: \"bar\"}\n. Thanks for the feedback!\n. Sorry, should have looked in the issue list first.\n. Awesome! :+1: \n. $ ava --serial doesn't help. Will pull the latest changes in tomorrow and try to reinstall my dependencies.  Will keep this updated.\n. Couldn't resist... Pulled in the latest version, did npm install, ran the tests and it all works just fine. :+1: \n. Good to know! Thanks :+1: \n. Nice! :+1: \nWill have a look back at pageres if this PR is merged (and released :))\n. This is probably also an issue in pageres with nyc.\n. This is probably also an issue in pageres with nyc.\n. Apparently, this prints undefined instead of Promise.\njs\ntest(t => {\n    console.log(t.throws(Promise.resolve()));\n    t.end();\n});\nThis is probably the reason why \"long running promises\" are not working. The await is not working because the returned value is not a promise.\n. Can be closed as #127 was merged.\n. This is odd, because of this minor change, 40 tests are failing. Will have a look at it later.\n. @schnittstabil The reason I created this PR was to make the code cleaner, not make it more complex :). So your example is definitely a no-go.\n@vdemedes Suggestion doesn't work\nExample\n``` js\nvar Promise = require('bluebird');\nPromise.reject(new Error('foo'))\n    .catch(function (err) {\n        console.log(err.message);\n    })\n    .then(function (success) {\n        console.log(success);\n    });\n// => foo\n// => undefined\nPromise.resolve('bar')\n    .catch(function (err) {\n        console.log(err.message);\n    })\n    .then(function (success) {\n        console.log(success);\n    });\n// => bar\nPromise.reject(new Error('foo'))\n    .then(function (success) {\n        console.log(success);\n    }, function (err) {\n        console.log(err.message);\n    });\n// => foo\nPromise.resolve('bar')\n    .then(function (success) {\n        console.log(success);\n    }, function(err) {\n        console.log(err.message);\n    });\n// => bar\n```\nIt looks like the only option is the then(success, err) one. If you don't want to use that one, just keep it the way it is :).\n. Things can always be done better!\n. No problem, always happy to contribute ;).\nAny idea when these latest changes (string support in throws, promise fix in throws) will be released? They are needed in pageres and gh-user :). (and many others off course)\n. That's not an easy one :). Will have a look at it later today.\n. \n. Sure, here you go.\nhttps://gist.github.com/SamVerschueren/ec24a03117021ed561fb\nThis results in the following output\n\n. Sure, here you go.\nhttps://gist.github.com/SamVerschueren/ec24a03117021ed561fb\nThis results in the following output\n\n. Awesome!\n. Actually, this is possible by \"abusing\" async :).\njs\ntest(async t => {\n    t.same([1, 2], [1, 2]);\n});\n. If all tests where written in ES2015, t.end could be dropped entirely. Either they are sync\njs\ntest(t => {\n    t.same([1, 2], [1, 2]);\n});\nor they are async\njs\ntest(async t => {\n    t.same([1, 2], [1, 2]);\n});\nBut I guess lot's of people are still using callbacks which makes t.end necessary. We can't force them to use promises, right? :)\n. Then the notation should be something like this\n.throws(function|promise [, error [, message]])\n. This is mentioned by @sindresorhus a couple of times, as well in #125.\n\nWe only use the core assert so not to duplicate code. If anything can be done better, we'll do it. Also the core Node.js people have said assert is done and won't change anymore.\n\nThat assertion module is not a binding contract. That's also the reason why AVA isn't using the same names as the assert library, to abstract away the implementation.\n. Not sure about a name change, throws seems the shortest and most explanatory name for this. Something like t.exception doesn't feels right.\nIMO people should read the docs, and I understand that some of them might think it has the same syntax, but then again, RTFM? ;).\nI guess @sindresorhus would be happy to give his thoughts on this.\n. raises seems more appropriate.\nIf we go for a name change, I would not keep throws. It will confuse people because it has two methods that are almost identical, just to be consistent with assert. Again, that is not the intention of AVA (but I guess you already know :)).\nIt's certainly not up to me to make this decisions.\n. I'm with @vdemedes on this one. Would leave it as well.\n. Would be very nice indeed! Maybe use something like .es6.js as extension? This way you don't need to update the whitelist.\n. I'm ok with changing this. In one-line modules, most of the time you only have one test. But it still forces you to write t.end or t.plan.\njs\ntest('result', t => {\n    t.is(fn('foo'), 'bar');\n    t.end();\n});\nWith these changes, this can be written to\njs\ntest('result', t =>  t.is(fn('foo'), 'bar'));\nIt's possible at this moment as well by abusing async.\njs\ntest('result', async t =>  t.is(fn('foo'), 'bar'));\nBut that's quite hacky I guess.\nI never use t.end in a callback, because promises (await/async) all-the-way! Maybe sometimes when using streams if it's not possible to use get-stream.\nSo in general, good for me!\n. @Qix- I guess the legacy can't be dropped. How would one test something like this without t.plan or t.end support?\n. IMO in that case, you have to call t.end because you explicitly \"subscribed\" for that behaviour.\n. It's actually hard, or maybe impossible, to write a failing test for this as it might be Babel related.\nSo here is a small code snippet\n``` js\nimport test from 'ava';\nfunction delay() {\n    return new Promise((resolve, reject) => {\n        setTimeout(() => {\n            reject(new Error('foo'));\n        }, 500);\n    });\n}\ntest('await', async t => {\n    const start = Date.now();\n    await t.throws(delay, 'foo');\nt.true(Date.now() - start > 500);\n\n});\n```\nwhich results in\n1. await\n  Missing expected exception..\n  AssertionError: Missing expected exception..\n. It's actually hard, or maybe impossible, to write a failing test for this as it might be Babel related.\nSo here is a small code snippet\n``` js\nimport test from 'ava';\nfunction delay() {\n    return new Promise((resolve, reject) => {\n        setTimeout(() => {\n            reject(new Error('foo'));\n        }, 500);\n    });\n}\ntest('await', async t => {\n    const start = Date.now();\n    await t.throws(delay, 'foo');\nt.true(Date.now() - start > 500);\n\n});\n```\nwhich results in\n1. await\n  Missing expected exception..\n  AssertionError: Missing expected exception..\n. That indeed solves the issue in my first post. Refactored the tests and now they are working correctly. My second post actually looks like another issue. Closing this one and will create a new issue. Have to investigate first.\n. The code snippet of my second post is wrong, that's why it wasn't working. It had to be t.throws(delay(), 'foo').\n. So from what I've read, only supporting * and ! would do? Seems pretty simple and straightforward so :+1: for that. Full regex support will only make sure people won't understand why a certain test isn't running (because the regex is not 100% correct).\n. @sindresorhus Oops, missed that one. \nWould like to see support for the * as well. I think it offers a lot more flexibility without making it difficult for the user (like RegExp support for instance).\n. @sindresorhus Thanks! Need it in another module of mine as well :).\n. @kasperlewau yes\n``` js\nconst cli = meow();\nconsole.log(cli.flags);\n```\n$ ./cli --foo=1 --foo=2\n{ foo: [ 1, 2 ] }\n. Very nice James!\n. :+1: \n. I would remove the alias. This is a breaking change, no need to keep that stuff.\nBut off course the decision is up to the maintainers :).\n. Ah missed that one :) \n. It seems to be an error in stack-utils. It fails on https://github.com/sindresorhus/ava/blob/master/lib/beautify-stack.js#L13 at this piece of code\njs\nstackUtils.clean(stack).split\n. It's the last line of the clean function. The stack trace of the error looks like this\n/Users/sam/Projects/playground/ava-test/node_modules/ava/lib/assert.js:78:7\nFrom previous event:\n    Test.exit (/Users/sam/Projects/playground/ava-test/node_modules/ava/lib/test.js:195:10)\n    Test.run (/Users/sam/Projects/playground/ava-test/node_modules/ava/lib/test.js:142:8)\n    Runner._runTest (/Users/sam/Projects/playground/ava-test/node_modules/ava/lib/runner.js:114:14)\n    Runner.<anonymous> (/Users/sam/Projects/playground/ava-test/node_modules/ava/lib/runner.js:101:15)\nFrom previous event:\n    eachSeries (/Users/sam/Projects/playground/ava-test/node_modules/ava/lib/runner.js:39:17)\n    Runner._runTestWithHooks (/Users/sam/Projects/playground/ava-test/node_modules/ava/lib/runner.js:91:9)\n    each (/Users/sam/Projects/playground/ava-test/node_modules/ava/lib/runner.js:35:27)\n    Runner._runConcurrent (/Users/sam/Projects/playground/ava-test/node_modules/ava/lib/runner.js:124:9)\n    /Users/sam/Projects/playground/ava-test/node_modules/ava/lib/runner.js:182:16\nFrom previous event:\n    Runner.run (/Users/sam/Projects/playground/ava-test/node_modules/ava/lib/runner.js:181:4)\n    process.<anonymous> (/Users/sam/Projects/playground/ava-test/node_modules/ava/index.js:85:10)\n    process.<anonymous> (/Users/sam/Projects/playground/ava-test/node_modules/ava/lib/test-worker.js:102:10)\nBecause of the internals of stack-utils the result array in the clean method is empty which results in null.\n. There is actually a test that to check if it returns null if it are all internals.\nhttps://github.com/tapjs/stack-utils/blob/master/test/test.js#L57-L60\nWill create an issue and do a PR.\n. @jamestalmage bumped stack-utils and removed previous changes.\n. You're welcome :)\n. You could compile it first to JavaScript and then run that file. Just like with TypeScript. Although it might be cleaner if AVA could handle .coffee files directly.\n. What's the callCount of console.log?\n. Think I found the issue. npm test -s does not run your tests in serial, which is why it doesn't work. Update your npm script to \"test\": \"ava -s\" and try again.\nYou can also make use of the context object to store your original log.\n``` js\nimport test from 'ava';\nimport sinon from 'sinon';\ntest.beforeEach(t => {\n    t.context.log = console.log;\nconsole.log = sinon.spy();\n\n});\ntest.afterEach(t => {\n    console.log = t.context.log;\n});\ntest('first', t => {\n    console.log('first');\n    t.true(console.log.calledOnce);\n});\ntest('second', t => {\n    console.log('second');\n    t.true(console.log.calledOnce);\n});\n``\n. @sindresorhus Cool, didn't know it was possible. Thanks :).\n. @kentcdodds They don't require that tests run serially. It's becauseconsole.logis global, so shared between the two tests. This means thatconsole.logis referring to the same spy. So in this situation, it's necessary to run them serially. If you are able (which is not the case here) to use thecontext` object, you don't have to run them serially because it's a totally new object.\n``` js\nimport test from 'ava';\nimport sinon from 'sinon';\ntest.beforeEach(t => {\n    t.context.log = sinon.spy();\n});\ntest.afterEach(t => {\n    t.context.log.restore();\n});\ntest('first', t => {\n    t.context.log('first');\n    t.true(t.context.log.calledOnce);\n});\ntest('second', t => {\n    t.context.log('second');\n    t.true(t.context.log.calledOnce);\n});\n```\n. > And it doesn't make sense to me that the tests are using the same spy because the spy should be reset before each test.\nThey are, but AVA is just too fast :).\nLook at the execution flow\nbeforeEach\nbeforeEach\nfirst\nsecond\nafterEach\n\nNote: Not sure though why the second afterEach isn't called. Bug? @sindresorhus \n\nAs you see, the console.log will be the same for first and second, being the one created in the second call to beforeEach. So yes, they are shared :).\n. @sindresorhus He was pointing to the use case without the context object\n\nOn a side note, the above would be great as a recipe. Hint hint ;)\n\nWill do my best to not forget. Should create a repository to keep track of my todos :).\n. #### Shared variable\n``` js\nimport test from 'ava';\nlet i = 0;\ntest.beforeEach(t => {\n    i++;\n});\ntest.afterEach(t => {\n    i--;\n});\ntest('first', t => {\n    console.log(i);\n});\ntest('second', t => {\n    console.log(i);\n});\n```\nOutput:\n2\n2\nContext variable\n``` js\nimport test from 'ava';\ntest.beforeEach(t => {\n    t.context.i = (t.context.i || 0) + 1;\n});\ntest.afterEach(t => {\n    t.context.i--;\n});\ntest('first', t => {\n    console.log(t.context.i);\n});\ntest('second', t => {\n    console.log(t.context.i);\n});\n```\nOutput:\n1\n1\n. > Ah, I see. That appears to be a bug to me.\nIt's not a bug, running tests concurrently comes with a \"cost\", this is one of them. Take a look at the number example (2 posts above this one) again. It shows the behaviour you experience.\nThis is not something you can have in mocha (or others) because they are running tests serially and where the execution flow is easier to follow.\n. > In my actual use case, my tests are asserting that something else is calling console.log\nWhat I do if I have a similar use case is only running that particular test serially.\n``` js\nimport test from 'ava';\nimport fn from './';\n// Do spy stuff here\ntest.serial('test log', t => {\n     fn.logger('hello world');\n     t.true(console.log.calledOnce);\n});\ntest('test something else', t => {\n     t.true(fn.somethingElse());\n});\n``\n. Exactly. From the moment you have something shared (which might not seem to be shared in the first place), you will have to execute them serially.\n. It might be a good idea to add thed.tsfile to thepackage.jsonas described [here](https://github.com/Microsoft/TypeScript/wiki/Typings-for-npm-packages). To make sure that the typings are still found when the file changes to something else in the future (ava.d.tsfor example).\n. @sindresorhus fair enough :)\n. Currently it's not possible to use thecontextobject to share context between thebeforeEach/afterEach` and the test as describe in this section.\nBut beware! It's only available in test, beforeEach and afterEach. It's not available in before and after.\n. @sindresorhus We should write a recipe for using TS to write tests. Will do it when this PR lands.\n. @ivogabe Didn't had my morning coffee when I mentioned the extends AssertContext. Realised it works as expected :).\n\n\nWhat do you mean exactly? Should context only be available in test, beforeEach and afterEach?\n\nExactly. From the docs:\n\nThe beforeEach & afterEach hooks can share context with the test:\n``` js\ntest.beforeEach(t => {\n    t.context.data = generateUniqueData();\n});\ntest(t => {\n    t.is(t.context.data + 'bar', 'foobar');\n});\n``\n. @ivogabe then my comment was good for something after all :).\n. Think thecontext` property isn't supported yet. Hard to see on mobile though :). \n. From the documentation\nContext sharing is not available to before and after hooks.\n\nOpen a new issue if you want to discuss that as a feature request.\n. I don't think a context makes sense in a before and after hook to be honest.\nLet's suppose (for the sake of this discussion), that we had a context.\n``` js\nt.before(t => {\n    t.context.foo = 'bar';\n});\ntest('this is a test', t => {\n    console.log(t.context.foo);\n    //=> 'bar'\n});\n```\nThis is exactly the same as\n``` js\nconst foo = 'bar';\ntest('this is a test', t => {\n    console.log(foo);\n    //=> 'bar'\n});\n```\nThey are shared over all the tests, so are constants/variables defined in the beginning of the file.\nNot sure if @jamestalmage has to say something extra on this :).\n. @jamestalmage It would be possible to inherit the context passed in beforeEach from the context of before. But as stated before in my example, I don't think that makes sense.\n. @sindresorhus What link? Link to the AVA readme? Can't find something similar in the other recipes so guess I'm a little confused :p.\n. Damnit, just screwed up squasing my commits. Let me try to fix it.\n. Alright, don't gonna touch the commits anymore :). Not sure if I should add the outDir property to tsconfig.json?\n@sindresorhus tips for squashing when you had to sync with the remote in between? It totally got screwed up the previous time :).\n. We can always improve depending on the feedback via twitter/gitter/issues.\n. @sindresorhus Alright, thanks for the tip :)!\n. Glad I could help :)\n. Thanks @kasperlewau . Will save me some headaches in the future :).\n. @sindresorhus yes with Error. But what if someone throws a string or a number? It isn't parsed to an Error object anymore, right?\n. > Oh yeah. Even though nobody should do that...\nWell, we all know how that goes ;)\n. I think that doc is outdated because they changed that recently. It has to be es2015. Will link the docs later, on my phone now an hard to find the release notes.\n. I think that doc is outdated because they changed that recently. It has to be es2015. Will link the docs later, on my phone now an hard to find the release notes.\n. Can't seem to find the official announcement anymore. I could find this issue however\nhttps://github.com/Microsoft/TypeScript/issues/6734\nAnd when running tsc -h, this is the target description\nSpecify ECMAScript target version: 'ES3' (default), 'ES5', or 'ES2015' (experimental)\nHowever, because I can't find the official announcement, I'm not 100% sure I'm correct...\n. Can't seem to find the official announcement anymore. I could find this issue however\nhttps://github.com/Microsoft/TypeScript/issues/6734\nAnd when running tsc -h, this is the target description\nSpecify ECMAScript target version: 'ES3' (default), 'ES5', or 'ES2015' (experimental)\nHowever, because I can't find the official announcement, I'm not 100% sure I'm correct...\n. Oh ok, I thought that it was still working with 0.13.x. My mistake then. Good to know :).\n. You're right!\n. Just to clear things up a bit regarding this issue. Should AVA handle the case where a dev throws something else then an Error (string, boolean, number, ...)? Or should it show an error that he should throw an error instead? Like implemented in my 2 PRs?\n. Just to clear things up a bit regarding this issue. Should AVA handle the case where a dev throws something else then an Error (string, boolean, number, ...)? Or should it show an error that he should throw an error instead? Like implemented in my 2 PRs?\n. @iamstarkov Why do you prefer rejecting with strings?\n. From the first post of @novemberborn in this thread, I assumed that AVA stopped asserting literals being thrown in the code\n\nThat is expected. With 0.13.0 t.throws() only tests against errors, before it would try and cast other types to errors, which was wrong.\n\nBut when reading further down, it looks like somehow that behaviour broke or was removed without thinking about the consequences.\n\nI have a double feeling about the solution of throwing an error when a literal is thrown and force the user to throw/reject with an error. Yes in a perfect world that would be very nice. But this might not be the task of a test runner to force a best practice. It might be up to a linter to force the developer to not throw literals.\n\nIf we would accept errors and literals, please don't add an extra method like throwsAny. I think it can be easily avoided.\nt.throws(fn(), Error)\nThis is quite clear, we expect an Error object.\nt.throws(fn(), 'Foo Bar') or t.throws(fn(), /Foo Bar/)\n2 options here\n1. fn() throws an Error\nIf fn() throws an Error (see is-error or something alike), assert that err.message is Foo Bar.\n2. fn() throws a string literal\nIf fn() throws a string literal, assert that err === Foo Bar.\nWhat's wrong with something like this? Maybe I'm missing something but this would work for everyone, no?\n. > It shouldn't. If you want to do that you can easily try/catch yourself.\nI'm ok with that. Willing to write a small recipe for asserting errors to explain when to use try-catch and when to use t.throws.\n. @jamestalmage I know, but people will create issues for that as to why it throws an error regarding literals. The recipe could clarify the behaviour and could encourage throwing errors is best practice. A lot of developers don't know that and throw strings for instance.\n. Also added the types to actual and expected. Not sure if necessary as it isn't being printed in the console?\n. > Not really necessary as they're only useful in an AssertionError.\nBut it's ok to keep it like this?\n. Hi @novemberborn. Thanks for looking into this. Can you explain why it's not a solution for #661? I read your post multiple times but can't really find an answer :). After looking into it again, I might have found a solution that's \"cleaner\" and might fit your approach more. I will make another PR with the new solution so we can compare and make sure I understand you correctly.\n\nThese lines implement that error message comparison validation. err here is not the error thrown by fn(), it's how that error should be checked.\n\nYes I know, I had it wrong in the original issue. I'm not touching those lines. I thought that was the issue but after diving deeper I noticed it's a custom compare function for assert.throws.\n. I agree with @jamestalmage although you might argue if AVA should enforce this best practice...\n. I agree with @jamestalmage although you might argue if AVA should enforce this best practice...\n. > I wonder if this should be an optional option in resolve-cwd to throw with a nice error message?\nThat would definitely make it easier for libraries using resolve-cwd.\n. It's also easy to catch it here though.\njs\nthis.options.require = (this.options.require || []).map(function (module) {\n    var ret = resolveCwd(module;\n    if (ret === null) {\n        throw new Error('Could not resolve ' + module);\n    }\n    return ret;\n});\n. From the docs:\n\nUnlike require.resolve() it returns null instead of throwing when the module can't be found.\n\nWilling to do a PR to another library to make this work.\n. > This does allow us to throw a more specific error, redirecting the user to their AVA config.\n:+1: \nAnd probably if we throw it somewhere down the chain, we will eventually try-catch the error to throw a custom message.\n. Fixed\n. Processed all the feedback\n. Thanks guys for the feedback and the merge! :beers: \n. Seems ok, not much to it :).\n. Yes, this will only allow you to write test.todo('message'). But todo doesn't allow chaining right?\n. Yeah sure.\n. I think you're right (not tested yet). It looks like test.serial.cb is not defined.\n. \ud83d\udc4d  on the todo chaining. It really doesn't make sense to have test.serial.todo('Add a test for foo').\n. Aha, thanks for the clarification. In this case, other stuff has to be fixed as well.\n. @jamestalmage no problem. Will try to add it today.\n. Yes, I was thinking the same yesterday \"we should have tests for the type definitions\". That would definitely be an improvement to somewhat validate them as the definitions getting more complex and more features are added.\n. LGTM, except if test.always.after should work as well, that will not work. But I have to take a look if swapping keywords can be solved elegantly so I'm ok with a merge and I will solve it in a separate PR together with the others.\n. Good to know :) \n. I would make the file executable. './make.js'\n. I did not test the output though but generally everything looks good. If you want me to test it more thoroughly, I'm ok with that. If you want to merge, that's ok as well. We can always finetune in follow up commits.\nGreat work @ivogabe!\n. Good point. Don't see any downside of doing that.\n. Can it be executed on postinstall maybe? This would overwrite the generated.d.ts shipped with npm though.\n. @ivogabe I can't find features that can only be ran on Node 6 (spread operators for instance). Node 4 supports the majority of those features as well.\n. Nice work @ivogabe !\n. Will have a look when I find some time\n. Good catch James, you're right.\n. @ChristianMurphy I'll try looking into it this evening or tomorrow. If not, don't hesitate to ping me back in a couple of days, quite busy lately :).\n. Sorry for the late reply guys. Only have one comment. If that's answered/addressed, seems good to merge.\n. I'm not entirely sure to be honest. It doesn't feel 100% right to me as a macro could actually be anything.\nThis implementation was based upon the example in the readme.\n``` js\nfunction macro(t, input, expected) {\n    t.is(eval(input), expected);\n}\ntest('2 + 2 === 4', macro, '2 + 2', 4);\ntest('2 * 3 === 6', macro, '2 * 3', 6);\n```\nBut what if I want to do this\n``` js\nfunction macro(t, obj) {\n    for (const key of Object.keys(obj)) {\n        t.is(eval(obj[key]), key);\n    }\n}\ntest('2 + 2 === 4', macro, {4: '2 + 2'});\ntest('2 * 3 === 6', macro, {6: '2 * 3'});\n```\nThis doesn't follow the input, expected order as defined by the typescript definition. And because you can actually pass as many arguments to the macro function as you want, I would suggest adding a couple extra like this\nts\nexport function test<I, E> (name: string, run: Macros<ContextualTestContext>, arg1?: any): void;\nexport function test<I, E> (name: string, run: Macros<ContextualTestContext>, arg1?: any, arg2?: any): void;\nexport function test<I, E> (name: string, run: Macros<ContextualTestContext>, arg1?: any, arg2?: any, arg3?: any): void;\nexport function test<I, E> (name: string, run: Macros<ContextualTestContext>, arg1?: any, arg2?: any, arg3?: any, arg4?: any): void;\nexport function test<I, E> (name: string, run: Macros<ContextualTestContext>, arg1?: any, arg2?: any, arg3?: any, arg4?: any, arg5?: any): void;\nThe current implementation forces the user to use it like the example provided in the readme. But macros are much more powerful then that.\nThis is just my opinion. If you guys are fine with what we have now, It's ok to merge. Just wanted to bring this up to everyones attention.\n. Thanks for looking into this @thejameskyle! Nice to see how they compare. Although I don't think it's a 100% fair comparison :).\n\nOne of the things that I thought was unusual with the TypeScript definition you generate is that it reduces the API methods down to act like they can't chain forever despite option-chain supporting the ability to chain infinitely. I ignored that piece because it seemed unimportant and that it would only serve to greatly complicate the type definition, not to mention now it's matching what happens at the runtime.\n\nThis is actually the reason why the TypeScript definition is this big. Although test.serial.serial might work, we discussed this when writing the definition and decided to not allow this because it doesn't makes sense. (Almost) all the code from line 175 is to prevent doing things like test.serial.serial and other invalid modifier combinations. Thus TS is much stricter in the test modifiers resulting in such a huge TS file.\n. > Also, people keep taking this as a comparison between Flow and TypeScript. I wasn't bragging about Flow\nI believe a lot of people assumed that you where bragging about Flow, looking at the questions you received on Twitter :).\nAfter reading your comments, I must admit that I believe you might be right. Type definitions represent the internal/runtime structure of the code, which isn't the case for the TS definition at the moment. So I will open a new issue to discuss this. \nBecause you are the Flow community manager (is that the right title :p?), you probably know much more about it than us. \nAnd when reading the runtime structure answer, something came in my mind. TS has a private modifier which actually does nothing like prefixing variables with underscores. It's just a way for the TS compiler to figure out that the property shouldn't be accessed from outside. This isn't entirely representing the runtime structure either, right? Because at runtime, that property is just accessible like all the rest. Actually, in my opinion they should prefix that with an underscore. I always end up doing private _foo which looks like doing the same thing twice...\n. > lol idk\nLet's call it that ;)\n\nCould you give me an example of this?\n\nSure, this is an example with private properties and methods.\n``` ts\nclass User {\nprivate firstName;\nprivate name;\n\nconstructor(firstName: string, name: string) {\n    this.setFirstName(firstName);\n    this.setName(name);\n}\n\nprivate setFirstName(firstName: string) {\n    this.firstName = firstName;\n}\n\nprivate setName(name: string) {\n    this.name = name;\n}\n\n}\n```\nResults in\njs\nvar User = (function () {\n    function User(firstName, name) {\n        this.setFirstName(firstName);\n        this.setName(name);\n    }\n    User.prototype.setFirstName = function (firstName) {\n        this.firstName = firstName;\n    };\n    User.prototype.setName = function (name) {\n        this.name = name;\n    };\n    return User;\n}());\n. It should work in beforeEach and afterEach, not in before and after.\n. From the docs\n\nContext sharing is not available to before and after hooks.\n\nIt was asked for multiple times. There is no valid use case why you want the context in before or after. Those functions apply to all the tests only once. If you want to setup some stuff in there, you should do it globally.\n. On mobile now so hard to check, but beforeEach and afterEach shouls return that. What do you get?\nAnd what is missing from the definition? always?\n. @tonyeung Feel free to improve the recipe though. PR's are more then welcome on things like that :).. In what situation does line 75 throw an error? It seems you are correct though, if I change it to your suggestion, it works.\njs\n            .then(function () {\n                x.throws(function () {}, err, msg);\n            }, function (fnErr) {\n                x.throws(function () {\n                    throw fnErr;\n                }, err, msg);\n            });\n. Definitely cleaner, but then I guess it should be like this.\n``` js\nif (typeof err === 'string') {\n  var e = err;\n  err = function (err) {\n    return err.message === e;\n  };\n}\nassert.throws(fn, err, msg);\n```\nIn your example, at the time the function is executed, err is not the string anymore but it is the function. So it will check if err.message is equal to the function instead of to the original string.\n. Apart from the discussion if this is necessary or not, I think this can be written with rfpify.\njs\nvar killedPromise = rfpify(ps.on.bind(ps), Promise)('exit').then(function (code) {\n    killed = true;\n    return code;\n});\n. Should we add a \"why\" to this? Something like to prevent you from doing unnecessary work.\n. t.plan(2)\n. Doesn't work. See https://github.com/Microsoft/TypeScript/issues/4885#issuecomment-149049003\n\n. :+1: \n. Not sure about the outDir. AVA looks for test.js or test/*.js if I'm not mistaken. So what should I use as value for outDir then?\n. Isn't it quite specific per project? I mean, some people will use TypeScript for the entire project, some might only use it for the tests. I don't want to force them in using a specific approach or directory structure. This recipe is just for the basics, they can always go from here and do some more advanced stuff with it, no?\n. Not sure if this check is good enough in every case though. Should I use something like is-error instead? Because now someone can still throw {foo: 'bar'}.\n. This is necessary in case a developer writes this test\njs\ntest(t => {\n    t.throws(() => {\n        throw 'foo';\n    });\n});\nassert.throws will validate that the function indeed throws an error (a string) and thus does not jump to line 113.\n. Good one!\n. Do you mean an empty line between } and return?\n. Using t.throws here, can't get around the extra create function right?\n. Damnit. This is what happens when you're used to AVA :D\n. fixed\n. I think that's the best way to handle this situations as well. It will indeed throw a compile error because it could not find the same method.\n. It would be nice if it was exported from within the library https://github.com/avajs/ava/blob/master/lib/runner.js#L11. Then if something changes, the generate script shouldn't be adjusted.\nBut then again, should libraries export stuff because of \"documentation\" purposes.\n. I would use allParts.forEach() here.\n. I would set the output variable at the top to an empty string.\n``` js\nif (output === '') {\n    return children;\n}\nreturn 'export namespace ' + ['test'].concat(prefix).join('.') + ' {\\n' + output + children;\n```\nAnd drop the empty variable\n. Empty line above this one\n. This is quite weird. You add up booleans? Why not has('only') || has('skip') ...?\n. Although it works, it feels kinda hacky. I think I'd rather prefer something like\njs\nhas('only', 'skip', 'todo').filter(Boolean).length\nOr something like that.\n. 'use strict'\n. \ud83d\udc4d on this. I believe it will work as package-cli stuff also works...\n. Actually I would put the T argument at the first position. Doesn't it make more sense? Because the macro also provides the testcontext as the first argument.\nts\nexport interface<T, I, E>\nSame for Macros\n// @ivogabe \n. Last small remark. Should we call this title: string instead of providedTitle: string? Autocompletion tools show the param names and title: string might be better.\n// @ivogabe @sindresorhus \n. @novemberborn Can you provide an example on when to use 0 or 1 argument?\nI believe the syntax would then become\nrun?: Macros<ContextualTestContext, I, E>, input?: I, expected?: E\nOr is test just passing all arguments to the macro?\n``` js\nfunction macro(t, foo, bar, baz, unicorn) {\n    t.is(unicorn, 'unicorn');\n}\ntest('2 + 2 === 4', macro, 'foo', 'bar', 'baz', 'unicorn');\n```\nThen I believe the syntax would become run: Macros<ContextualTestContext, I, E>, ...args: any\n. That makes sense.\n. ",
    "alubbe": "Just some quick feedback from an early user that it would be awesome to have custom reporters :)\n. Awesome, I just hope that the concurrency doesn't end up posing a problem for TAP.\n. Awesome, thanks!\nAm 04.12.2015 10:44 vorm. schrieb \"Sindre Sorhus\" <notifications@github.com\n\n:\nClosed #272 https://github.com/sindresorhus/ava/issues/272 via 8a67704\nhttps://github.com/sindresorhus/ava/commit/8a6770423dd69c700826fbd5a43c3a85b427808f\n.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/272#event-482504482.\n. Awesome, thanks!\nAm 04.12.2015 10:44 vorm. schrieb \"Sindre Sorhus\" <notifications@github.com\n:\nClosed #272 https://github.com/sindresorhus/ava/issues/272 via 8a67704\nhttps://github.com/sindresorhus/ava/commit/8a6770423dd69c700826fbd5a43c3a85b427808f\n.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/272#event-482504482.\n. How is AVA able to transform the generator when it is handled by bluebird.coroutine before ava.test? Is this a pre-runtime transformation by babel?\n\nLeaving off the Promise.coroutine to feed AVA generators directly doesn't work for all of my tests, in particular those that use a promisifed version of request, like this:\n``` coffee\ntest = require \"ava\"\nPromise = require \"bluebird\"\nrequestAsync = Promise.promisify require \"request\"\ntest \"peter\", (t) ->\n  yield requestAsync url: \"www.google.com\"\n```\nThis time, the error has no stack trace:\nAssertionError: Promise rejected \u2192 Error: No element indexed by 1\nIf I add Promise.coroutine and run it in AVA 0.6.1, my test suite runs just fine.\n. Good news, 0.9.x fixes this issue due to the new babel version!\n. t.regex has now effectively solved one of my 2 issues.\nAny chance to get a better helper for string includes?\n. Just to chime in from the sidelines, I see two quick points in favor of keeping it:\n1) Long stack traces are a really beautiful feature, and much less expensive with bluebird than with most alternatives\n2) My main helper file requires (my own version of) bluebird (which is the same as ava's), so if ava switches to another implementation, it will actually make it slightly slower for me.\nGranted, point 2 doesn't hold for everyone and at some point in time there may be a mismatch between my version of bluebird and ava's. But it's not uncommon and something to keep in mind.\n. To reproduce, I ran ava --tap test/*.js | tnyan on 0.9.1\n. Somewhat better, using tap-mocha-reporter I get\n```\n  t.ok(res.statusCode === 202)-----,\n           |                   /_/\\ \n           200                ( ^ .^) \n 228 --------------------------------------------,------,\n 1   --------------------------------------------|  /_/\\\n 0   --------------------------------------------~|( x .x)\n--------------------------------------------_ \"\"  \"\"  \n1 passing (4s)\n  1 failing\n1) :\n     (unnamed error)\n```\nNow the cat output is part of the assert (not a biggie), but something ate the stack trace. Here is what it looks like with the default reporter:\n```\n  1 passed  1 failed\n\nfoo \u203a bar\n\nt.ok(res.statusCode === 202)\n           |                \n           200                \nAssertionError: false == true\n    at Decorator.concreteAssert (node_modules/empower-core/lib/decorator.js:95:20)\n    at Object.decoratedAssert [as ok] (node_modules/empower-core/lib/decorate.js:48:30)\n    at Test._callee$ (test/test.coffee:28:5)\n    at tryCatch (node_modules/babel-runtime/regenerator/runtime.js:88:40)\n    at GeneratorFunctionPrototype.invoke [as _invoke] (node_modules/babel-runtime/regenerator/runtime.js:341:22)\n    at GeneratorFunctionPrototype.prototype.(anonymous function) [as next] (node_modules/babel-runtime/regenerator/runtime.js:121:21)\n    at onFulfilled (node_modules/co-with-promise/index.js:66:19)\n```\n. ",
    "jamestalmage": "We likely will have to buffer and output one whole test at a time.\n. :+1: \n. My vote is to wait. Having to support browsers will slow us way down. I think browser support should be just about the last thing before 1.0.0.\n. My vote is to wait. Having to support browsers will slow us way down. I think browser support should be just about the last thing before 1.0.0.\n. The following will be helpful when we do move forward with this:\nhttps://code.google.com/p/chromium/issues/detail?id=485425\nSo we can run test files in individual tabs.\n. The following will be helpful when we do move forward with this:\nhttps://code.google.com/p/chromium/issues/detail?id=485425\nSo we can run test files in individual tabs.\n. It depends on how easy it would be to modify karma to run multiple files at once. But karma would definitely be the first thing we look at.\nI would like to see it be a bit more than recipes. If nothing else, worst case a shareable karma baseconfig that you can use without modification if you do things \"the AVA way\"\n. It depends on how easy it would be to modify karma to run multiple files at once. But karma would definitely be the first thing we look at.\nI would like to see it be a bit more than recipes. If nothing else, worst case a shareable karma baseconfig that you can use without modification if you do things \"the AVA way\"\n. @jokerhyme if that actually works, might make for a good recipe.\n. @jokerhyme if that actually works, might make for a good recipe.\n. @jokeyrhyme - The eventual goal is to build in real-world browser testing. If not directly using karma's browser launchers, then something very similar.\n. @jokeyrhyme - The eventual goal is to build in real-world browser testing. If not directly using karma's browser launchers, then something very similar.\n. How far back does browser support for WebWorkers go? Especially IE. I think it's not very far.\nCouldn't we accomplish much the same thing via iframes? \n. There is a new flag in Chrome that causes requestAnimationFrame to work, even when the window isn't visible. I linked to it above, but the issue has since been restricted to collaborators (you can't even read it). I'm not sure what that means, but it's a flag we probably want to enable.\n. I think we should try to figure out how to reuse karma wherever possible. Probably launchers. Maybe the browser to console relay.\nKarma already allows concurrent testing across multiple browsers (though not concurrency within the test run for an individual browser).\nWe should try to reuse whatever we can.\n\nI'd be happy for test results to be reported back to the terminal, and in the Dev Tools console, but not shown in the web page itself\n\n:+1:\n. > Personally I wouldn't even mind if those tests would not run in parallel\nI think that's a fine first goal, and if it gets us to browser support faster, so be it.\n\nGoing for hassle with web workers just seems like overkill.\n\nWe've identified other problems with web workers (see above), so that's definitely off the table. \nThat said, isolation is way more important than parallel execution. The two best solutions for isolation (WebWorkers and iframes) also facilitate parallel execution, so I'm not sure we need to punt on either for long.\nI think iframes should work OK. We may need to come up with something to allow for apps that care about window.top, but we can punt on that for now.\n\nI am wondering if there is anything that I could do to help with this\n\nReviewing and commenting on #887 is a good start. If you've got alternate ideas, proof-of-concept PR's wouldn't hurt.\n. Hmm. Maybe we should start with running test files one at a time, top-level, launching a new browser/tab for each one. That will be really slow, but it's probably the easiest to accomplish. Then we can work towards parallelism.\n. From my reading of the TAP spec, I it does not seem very well suited to concurrency. You would need to buffer individual tests and then log them all at once so they don't walk all over each-other.\n. @jprichardson - We decided to push #279 to the next milestone.\n. I have been pretty consistently been annoyed by this \"feature\". Yes require(../some/path) is relative, but if I am using require - it's at the top of my test, and easy to find. If I am using cwd, well, that could be anywhere. And it's not always easy to search for:\njs\n// relies on cwd - but it isn't apparent how use my IDE's search tool to find it.\nexeca('node foo.js');\nMoving a test that relies on child_process.exec or execa wreaks havoc since we implemented this.\nConversely paths relative to the module file are pretty easy to locate. Look for __dirname and require (./ if things get really desperate). If I am moving a test file around (and not moving some resource with it) the search and replace is pretty easy.\n\nI would have preferred that cwd was always set to to the same directory as package.json (found via pkgUp). That has the advantage of keeping cwd consistent, even when you execute ava from a subdir, and it allows you to treat cwd as a base for creating \"intra-project-absolute paths\".\n. > though that might be a little tricky for monorepo's\nNot sure why it would be. Usually there is a package.json for each package in the monorepo - so it would just be relative to that. If there's not (some people auto-generate at deploy time) then you would be relative to the root package.json.\n. We could add a cwd config option to package.json\n. Another bug caused by our current behavior: https://github.com/dtinth/babel-plugin-coverage/issues/39\nWe should revert this for the next release.\n. Locking in favor of duplicate #222.\nPlease move all discussion there.\n. > Maybe by using https://github.com/bahmutov/cache-require-paths by @bahmutov.\nI'm not so sure about that. NPM 3's automatic flattening and deduping of the dependency tree really sped up resolve times. Also, I don't see if or how it decides to invalidate the the cache. That seems iffy to me, especially for a testing framework where it's fairly reasonable to expect their dependency tree will change frequently.\n\nNode.js 5.2.0 is also ~45% faster to startup! \n\nThat is huge.\n. I think the biggest improvement will come from moving Babel to the main thread.\n. I guess it's worth a shot. I just have doubts it is going to be effective with us spawning so many processes (it doesn't coordinate across processes), and dealing with a relatively volatile dependency tree. Seems better suited to servers than dev boxes.\n. > unless it's also doing babel compilation for each fork\nThat is currently the case, #349 fixes it. Currently, assuming a cache miss - we spend 700ms just requiring stuff in each child process. In #349 that drops to 150ms. Those numbers are from my laptop with super speedy SSD's, people using traditional hard drives will see significantly worse performance in either case, but benefit a lot more from #349.\nI don't think we should be concatenating files. That is a lot of complexity, for what I think is going to be minimal gains.\n. Static analysis should get us all the \"cross-fork magic\" without a lot of downside.\n. @spudly - I actually have a start on this already. I will post some code soon.\nBasically, it needs to help identify test.only, test.serial.only, etc.\n. Technically, people don't need to use test as their variable name when importing ava:\n``` js\nimport ava from 'ava';\nava(t => t.is(...));\n```\nAlso, I have done this in the past:\n``` js\n// now all the tests are serial:\nvar test = require('ava').serial;\ntest(t => t.is(...));\n```\nI have since decided that is bad form, and that it's just better to do test.serial each time (clarity is more important than brevity). Still, I think we could support it.\n\nMore problematic is if use of only is wrapped in a conditional:\n``` js\n// fictional module that analyzes math expressions in strings:\nimport calculator from 'calculator-parser';\nimport ava from 'ava';\nfunction test(input, expected, only) {\n  const method = only ? ava.only : ava;\n  method(input + ' === ' + expected, t => {\n    const actual = calculator.calculate(input);\n    t.is(actual, expected);\n  });\n}\ntest('3 + 4', 7);\ntest('5 - 2', 3);\ntest('6 * 3', 18, true); // causes this to be interpreted as an exclusive test\n// ...\n```\nI do not see any way to reliably do static analysis on this. That is a bit of a bummer.\n. @ORESoftware \nWe transpile tests in the main process, the forks don't transpile tests at all. The remaining problem is how to handle transpiling sources. Currently, you have two choices:\n1. Transpile your sources before running the tests. This results in probably the best performance, but requires a good deal of boilerplate - especially to support watch mode.\n2. Use babel-register to transpile sources on the fly. This is much easier to setup (just --require babel-register), but your per-fork cost is significant.\n\nforking cps is not slow at all in my experience, it's more or less the standard ~50ms of waiting per fork, it's babel that is really slowing you down\n\nAgreed, but some users have hundreds of test files, and they have reached the point where they are experiencing memory/disk thrashing that is impacting performance. #791 will address that.\nAlso, we intend to add support for reusing forked processes at some point (#789). That will have some major downsides (mainly that it breaks test isolation), so it will be opt-in. 50ms per file is not insignificant when dealing with hundreds of files.\n. > So have we actually profiled AVA to determine that babel is indeed the culprit? Or is it speculation?\nDefinitely not speculation. Moving Babel to the main thread for tests created significant speed improvements. It stands to reason the same improvements could be realized by moving source transpilation out of the forks.\n\nSomething else to consider - forking for every test at once is going to cause problems.\n\nAlready considered. See #791. \n. We use Babel to transpile your test files. We do this in the main process, then write the transpilation result out to disk. When we fork a child process, we don't actually load your test file, we load that transpilation result. (We use node_modules/.cache to store the transpilation).\nDoing the same for sources is much more difficult, because we can't tell what needs to be transpiled without attempting static analysis.\n. I am closing this, it's just too general. We are tracking specific improvements in the following issues.\n- #789 Reuse test-worker processes\n- #594 Investigate use of cacheData support to improve require times.\n- #577 Transpile sources in the main thread\n- #720 Transpile helpers in the main thread\n- #719 Cold start forks in watch mode\n- #718 Implement pool for managing forks (solved in #791)\n- Almost everything else labelled performance\n. Closed via: https://github.com/sindresorhus/ava/commit/c5d02f1f3c8ba7dabbdb53fdc9710f0cd117a9d3\n. @SamVerschueren - async/await fix is in master.\n. It is more than just \"ava transpiles tests only\".\nIt is that ava allows you to write tests in ES2015 with zero impact on your production environment.\nBy zero impact, I mean no transpilation an no polyfills. (IMO polyfills are the sneakier harder to detect bug).\nFor example, we make sure Promise is available inside your tests, without impacting it's availability in production code.\njs\n// Always works in tests. Only works in production in Node >= 0.12\nnew Promise(...)\nTrickier still is a prototype polyfill. Something like String.prototype.includes(). If Babel actually modifies the prototype, your test suite no longer protects you against mistakenly using str.includes() in production. Users will report errors in Node 0.12 that you can't write a failing test case for (because it has been polyfilled even in your production environment).\nAVA uses the runtime plugin to provide polyfill aliasing. An example of how that works is here. This means that:\njs\n\".... some string ....\".includes()\nWill work in your tests on any platform. But will throw in production code on platforms that do not support str.includes()\n. Because your tests are not used in the wild. Your production code might be. The idea is to let you use ES2015 for test code, even if you are not allowed to use it for production (because you want to support old Node without transpilation).\nAnother big issue with providing transpiled dependencies is, again, polyfills. For your code to run standalone, babel needs to include the polyfills in the output. This is not good if you want a small reusable library (you don't want Node loading up 10 copies of a string.includes() polyfill)\n. I don't think a cli-hook has been ruled out.\nIn the mean-time, require('babel-core/register') at the top of every test file should work.\n. A repo is worth a thousand comments: \nhttps://github.com/jamestalmage/__ava_with_babel_for_production\n// @codyhatch @BarryThePenguin @alexbooker @madbence @theaqua \n. You need to install as dependencies of your own project to use outside of tests\n. Cool!\nYour use of the --require flag is spot on for what we intended. #407 should make the experience even better.\nWe would value any feedback on pain points you encountered during the transition.\n. I have cross-posted here: https://github.com/power-assert-js/babel-plugin-espower/issues/6\nI can't tell which library is causing the error. Please close if you are confident it is the power-assert plugin.\n. Closing since the issue is in power-assert\n. > This also makes it easier for contributors\nOnly if they are familiar with AVA's api. The number of people familiar with assert > those familiar with AVA. Nearly all the AVA assertions have an assert equivalent, so from a certain perspective you have renamed / aliased that.\nWhy did the team choose to use different names than the core assert library? Just your preference? Is it based on some other library?\n. fair enough.\n. Caching is in master. PR #352 \n. @vdemedes \nAttaching kill() to the promise is doable.\nYour second request is problematic. It forces the process to exit before the results promise is resolved. This is problematic for situations where the tests are done, but something is keeping the process running (i.e. you launched a server and fail to shut it down).\n. @vdemedes \nAttaching kill() to the promise is doable.\nYour second request is problematic. It forces the process to exit before the results promise is resolved. This is problematic for situations where the tests are done, but something is keeping the process running (i.e. you launched a server and fail to shut it down).\n. > The forked processes will exit anyway\nNot always, see https://github.com/bcoe/nyc/issues/52.\nThat was primary motivator of this PR. \n. > The forked processes will exit anyway\nNot always, see https://github.com/bcoe/nyc/issues/52.\nThat was primary motivator of this PR. \n. Here is the relevant comment: https://github.com/bcoe/nyc/issues/52#issuecomment-153145235\n\nFirebase has a known issue where the process doesn't die until you explicitly call process.exit(). But I don't understand this issue well enough / have the right context to know if that's related to this issue\n\nFirebase won't be the last place people run into this (launching an express instance that you fail to destroy, etc).\nThis PR waits until all the results are in, and then goes back and forcibly kills any child processes that haven't shut themselves down already.\nIt's necessary to keep process closure and test result promise resolution as separate things.\n. Here is the relevant comment: https://github.com/bcoe/nyc/issues/52#issuecomment-153145235\n\nFirebase has a known issue where the process doesn't die until you explicitly call process.exit(). But I don't understand this issue well enough / have the right context to know if that's related to this issue\n\nFirebase won't be the last place people run into this (launching an express instance that you fail to destroy, etc).\nThis PR waits until all the results are in, and then goes back and forcibly kills any child processes that haven't shut themselves down already.\nIt's necessary to keep process closure and test result promise resolution as separate things.\n. @vdemedes \n@4d907aa incorporates the \"kill() on promise\" request.\nPersonally, I felt the code read better the way I had it originally, but I will defer to you guys.\nAs an alternate, what if I defined a non-enumerable kill() property on result?\n. @vdemedes \n@4d907aa incorporates the \"kill() on promise\" request.\nPersonally, I felt the code read better the way I had it originally, but I will defer to you guys.\nAs an alternate, what if I defined a non-enumerable kill() property on result?\n. Is t.end() required when using t.plan()? I thought it was one or the other.\n. ``` js\ntest('foo', t => {\n    t.plan(2);\n    var a = 2;\n    var b = 3;\n    var c = 4;\ntry {\n    t.ok(a === b);\n    console.log('a === b did NOT throw');\n} catch (e) {\n    console.log('a === b DID throw');\n}\n\ntry {\n    t.ok(b === c);\n    console.log('b === c did NOT throw');\n} catch (e) {\n    console.log('a === b DID throw');\n}\n\nt.end();\n\n});\n```\noutput:\na === b did NOT throw\nb === c did NOT throw\n  \u2716 foo   \n  t.ok(b === c)\n       |     | \n       3     4\n. > This is a bug within t.ok(), then...\nNo, I don't think so. The API seems modeled after node-tap. Search those docs for reference to bailout (implying that it does not bailout on a test failure by default).\nIt is a bit confusing coming from mocha, etc. But can be fairly handy (especially when your test framework actually shows you every failed assertion :wink:). It might just be that the second/third/... failed assertion is the one that has the truly helpful hint that helps you identify the problem.\nThis is why I prefer Option 1 from above\n\nIt would be better if either:\n1. ~~Both~~All assertion failures printed.     <-- Do this one!\n2. If there can only be one, print the first.\n. @sindresorhus even with --fail-fast, the above code still shows the last assertion. Probably because the code is synchronous and ava assertions don't throw (I am guessing --fail-fast does not make assertions start throwing, but just short circuits the wait for t.end()/t.plan() in async situations).\n\nI think we all agree that the first test failure is the most important, but I don't think it is necessary to change to a throwing behavior if you don't like the verbosity of printing out all those failures - just don't print them (or print them in an ultra compact form).\nHere is the current output:\n```\n  \u2714 foo\n  \u2716 bar \n  t.is(b, c)\n       |  | \n       3  4 \n1 test failed\n\nbar\n  AssertionError: \n  t.is(b, c)\n       |  | \n       3  4 \n    at Test.fn (/Users/jamestalmage/WebstormProjects/promisify-firebase/test.js:86:5)\n```\n\nIt might be cool to print all the assertions with dots and x's:\n```\n  \u2714 foo .........\n  \u2716 bar ....\u2716.....\u2716\n  t.is(b, c)\n       |  | \n       3  4 \n1 test failed\n\nbar\n  AssertionError: \n  t.is(b, c)\n       |  | \n       3  4 \n    at Test.fn (/Users/jamestalmage/WebstormProjects/promisify-firebase/test.js:86:5)\n```\n\nI could also get behind slightly more output:\n```\n  \u2714 foo .........\n  \u2716 bar ....\u2716.....\u2716\n  t.is(a, b)\n       |  | \n       2  3 \n  t.is(b, c)                      // compact output from every assertion failure up top\n       |  | \n       3  4 \n1 test failed\n\nbar                          // full output of only the first assertion failure down below. \n  AssertionError: \n  t.is(a, b)\n       |  | \n       2  3 \n    at Test.fn (/Users/jamestalmage/WebstormProjects/promisify-firebase/test.js:86:5)\n```\n\nAlso, the node-tap, bailout feature is not without merits. It helps with the situation @vdemedes describes: \"If this first assertion fails, there is no point continuing (the rest will fail), throw and bail so my output is not cluttered with a bunch of errors\". (I am talking about the per-assertion bailout option. --fail-fast already provides a test-suite-wide solution).\n. By the time the promise test bar has resolved and we are printing \u2716 bar, we should know the state of every assertion that happened inside that test.\n. There is order within an individual test.\n. @schnittstabil\nTwo options there: \n1. Just use runtime order. Easy for us. Users just need to be made to understand how it works.\n2. power-assert already embeds the line number of the assertion when it instruments code. It would be possible to write a custom reporter that exposed line numbers to ava, and we could give syntactical ordering.\n. What if we created an alternate one-line format for all the subsequent failures: \nhttps://github.com/power-assert-js/power-assert-formatter/issues/21\n. CRAP!\nHow did I miss that?!\n. @Lalem001 \nBy default, AVA will only instrument your tests, not production code. There is no good reason not to take advantage of Babel instrumentation in your tests.\n. > I plan on looking into creating Node.js version specific presets, so it only transpiles what's not supported natively.\nThat was on my radar as well. Lets not duplicate effort.\n. Ah - that is a good point. Not sure if @1d5ef4c5f6fcb70f9e90e584dfd08865db1e93b4 (landed in master, but not published) will address that.\n. Instead of \"Disable Babel\", How about an option to disable the lookup of the local version, and force ava to use whichever version it shipped with.\n. We should probably alias babel:false to babel: {}.\n. We should probably alias babel:false to babel: {}.\n. Try adding just this plugin: http://babeljs.io/docs/plugins/transform-es2015-modules-commonjs/\n. Try adding just this plugin: http://babeljs.io/docs/plugins/transform-es2015-modules-commonjs/\n. > the suggestion requires adding a new devDependency on a project that otherwise has no need for babel\nI'm just curious, what motivates the \"no babel\" decision. Missing out on async/await for your tests seems like a really big deal to me.\nTo answer your question, it is unlikely we are going to alter the lookup paths for the presets again (we used to, and it was problematic). npm@3 is the solution here.\n. > the suggestion requires adding a new devDependency on a project that otherwise has no need for babel\nI'm just curious, what motivates the \"no babel\" decision. Missing out on async/await for your tests seems like a really big deal to me.\nTo answer your question, it is unlikely we are going to alter the lookup paths for the presets again (we used to, and it was problematic). npm@3 is the solution here.\n. We have consensus in #244, and a proposed implementation in #253.\nI will leave #244 open until we merge a solution. Closing this now.\n. That works. My thought was to leave long running processes alone until we were all done (that's why I did it the way I did). Looking back, I can't remember why I thought that was important. If prematurely killing the process for test-file-a.js adversely affects test-file-b.js, you are doing something wrong anyway.\n. That works. My thought was to leave long running processes alone until we were all done (that's why I did it the way I did). Looking back, I can't remember why I thought that was important. If prematurely killing the process for test-file-a.js adversely affects test-file-b.js, you are doing something wrong anyway.\n. Oh wait - this is not going to work.\nYou need to do something like this:\njs\n// fork.js\nnew Promise(resolve, reject) {\n  var storedResult;\n  ps.on('result', function (result) {\n    // don't resolve here we MUST wait for the process to exit.\n    storedResult = result;\n    ps.kill();\n  });\n  ps.on('exit', function(code) {\n    if (code > 0) {\n      reject();\n    }\n    else {\n      resolve(storedResult);\n   }\n  });\n}\n. Oh wait - this is not going to work.\nYou need to do something like this:\njs\n// fork.js\nnew Promise(resolve, reject) {\n  var storedResult;\n  ps.on('result', function (result) {\n    // don't resolve here we MUST wait for the process to exit.\n    storedResult = result;\n    ps.kill();\n  });\n  ps.on('exit', function(code) {\n    if (code > 0) {\n      reject();\n    }\n    else {\n      resolve(storedResult);\n   }\n  });\n}\n. Because then this: https://github.com/sindresorhus/ava/pull/134/files#diff-2cce40143051e25f811b56c79d619bf5R161\nGets executed before all the child processes finish exiting.\nIf you kill the parent, the child processes are killed without warning, despite where they may be wrapping up the kill signal that was sent to the child process.\nnyc intercepts the kill signal, and delays letting the process actually die until it is done writing coverage results back to disk.\n. Because then this: https://github.com/sindresorhus/ava/pull/134/files#diff-2cce40143051e25f811b56c79d619bf5R161\nGets executed before all the child processes finish exiting.\nIf you kill the parent, the child processes are killed without warning, despite where they may be wrapping up the kill signal that was sent to the child process.\nnyc intercepts the kill signal, and delays letting the process actually die until it is done writing coverage results back to disk.\n. @vdemedes \nOK, I was working on a PR as well.\nSee #135.\nIt has a good failing test.\nI can carry that through with the fix or let you handle it.\n. @vdemedes \nOK, I was working on a PR as well.\nSee #135.\nIt has a good failing test.\nI can carry that through with the fix or let you handle it.\n. I would prefer to see it merged.\nIt provides a test that mimics the nyc issue and confirms the previous commits are actually doing what we want.\n. I would prefer to see it merged.\nIt provides a test that mimics the nyc issue and confirms the previous commits are actually doing what we want.\n. despite the branch name, it does not actually fix anything (you beat me to the punch on that).\nit's just a test\n. despite the branch name, it does not actually fix anything (you beat me to the punch on that).\nit's just a test\n. So,\nThe existing test covers making sure we force close some process that is being kept open due to a perpetual process (i.e. Firebase, express, etc). If that is all that is going on, then the process should exit immediately in once it receives the kill signal (no delay).\nThat's the setTimeout(15 seconds) in the long-running-test, and then an assertion that we DON'T wait for that setTimeout to finish.\nThe other scenario is when we send a kill signal to the process, and it intentionally intercepts and delays closure of the process. This is the case with nyc. Nyc specifically uses signal-exit to intercept the kill signal and delay the exit until it is done writing to disk. signal-exit only accepts synchronous callbacks. (I am guessing this is a safety measure to make sure you don't screw things up with a complicated async exit process, and fail to ever let the process exit). This is the the onExit portion.\nI run through a 2 second loop\njs\nvar start = Date.now()\nwhile (Date.now() - start > 2000) {} // just waiting 2 seconds synchronously because I have to be synchronous inside onExit\nprocess.send({..delay-completed-event...})\nBefore all our changes we were exiting the parent and killing the child before the delay-completed-event. Even though we fixed the problem, I realized my initial test actually didn't cover the full scenario.\nI agree, it's not a super straightforward test, but I am simulating a rather convoluted process and I don't know how to make it simpler. \n. That should do it.\nOnce you merge this, I will merge master back into #135 and it should pass.\n. That should do it.\nOnce you merge this, I will merge master back into #135 and it should pass.\n. The child process should always send a results message though right? Even if it had zero tests?\nMy point is that if the process exits without sending the results message, then something happened that prevented ava from sending it. That is out of the ordinary and should be an error.\nIf there are no tests in a file, and ava sends a result testCount: 0, that's fine (give the friendly warning and be done).\n. Closed via: https://github.com/sindresorhus/ava/commit/97409864e2f2f3ed103b13ae6602cad4744a6815\n. If we are killing the process because we think we are done that's one thing, but the following should detect an issue:\n``` js\ntest('a', t => {\n    t.plan(1);\n    t.ok(true);\nsetTimeout(() => t.ok(true), 1000);  // test-b will still be running when this happens.\n\n});\ntest('b', t => {\n    setTimeout(() => t.end(), 2000);\n});\n```\nAlso, in the case of multiple test files A, B, C (in order of increasing execution time), how undesirable would it be to leave the forked process for A running until B and C have also finished (just to increase the chance of catching a plan error). Node forks are pretty lightweight as I recall.\n. A setting that required you to always call t.end() might be helpful. In that case t.plan(x) just becomes another assertion for ... how many assertions you will have (meta).\nPerhaps just t.expect(3), means \"expect 3 assertions before t.done() is called\"\n. Or t.end(4) meaning - I'm done and should have 4 previous assertions\n. t.end(4) is the assertion without the auto-ending that comes with t.plan(4).\nI don't think there is a great way to do both, and auto-ending is problematic for exactly the reasons you describe.\n. t.end(4) would be useful if there were already more or less assertions than expected. The assertion happens at the time of invocation. It's just one more way to guard against mistakes.\n. #181 is way better solution than my idea.\n. This is closed by #181 and #253.\nt.plan() no longer auto ends for you. You must return a promise or  call t.end().\nt.end() can only be used in \"callback mode\", which is enabled with test.cb(t => {})\n. It is still on the table. Go for it!\n. Simply installing babel-runtime locally fixed.\nIt seems to me the requireFromString line is causing the error.\nrequires in the transpiled code are trying to resolve from the wrong directory to get access to babel-runtime. The resolution is happening from the base directory, but the runtime is in ./node_modules/ava/node_modules/babel-runtime, which won't resolve.\nIt should be possible to use derequire to change all the require(...) to ___uniquederquirename__ in the transpiled code to  else, then prefix the generated code with:\nfunction ___uniquederequirename___(path) {\n   try {\n      return require(path);\n   } catch (e) {\n       if (/^babel/.test(path)) {\n          // resolve from ava's root directory\n       }  else {\n           throw e;\n       }\n   }\n}\n. Yeah - that! Also not sure what happened in the middle of my last sentence.\n. Looks like work has already begun:\nhttps://www.npmjs.com/package/babel-preset-es2015-node5\nhttps://www.npmjs.com/package/babel-preset-es2015-node4\nhttps://www.npmjs.com/package/babel-preset-node5\nStill, I think it might be better to automate this.\nI am thinking of create a test-file for each of the available plugins that uses the specific feature that plugin provides.\nThen a script to require each in a try/catch and see if they throw / fail to compile. That gives us a nice automated update path as future versions are released. I am hoping babel's own test suite will make this all a simple matter of cutting and pasting code samples... There are a lot of plugins.\n. Yes - as part of the update process. Not live testing.\nAfter the test suite runs we output a file for that specific version in a subfolder:\njs\n// index.js\nmodule.exports = require('./plugin-lists/' + process.version);  // probably not necessary to the patch level, but you get the idea\nor something like that.\n. It might be cool to fetch the actual plugins from npm on demand as well (so npm install goes faster in newer versions of node). We could give back a few CPU cycles to travis-ci.org that way.\nDefinitely not something to target for the first release, but might be interesting to play with down the road.\n. Here is a snippet of the runtime plugin (I would link to the repo, but it is 404'ing, since they went monorepo - old url was: https://github.com/babel-plugins/babel-plugin-runtime)\n```\n  return new Plugin(\"runtime\", {\n    metadata: {\n      group: \"builtin-post-modules\"\n    },\npre: function pre(file) {\n  file.set(\"helperGenerator\", function (name) {\n    return file.addImport(RUNTIME_MODULE_NAME + \"/helpers/\" + name, name, \"absoluteDefault\");\n  });\n\n  file.setDynamic(\"regeneratorIdentifier\", function () {\n    return file.addImport(RUNTIME_MODULE_NAME + \"/regenerator\", \"regeneratorRuntime\", \"absoluteDefault\");\n  });\n},\n\n``\n. maybe we can wrap that plugin / override thepremethod (to do nothing), and introduce our own requires that we resolve.\n. What if we just link./node_modules/ava/node_modules/babel-runtime => ./node_modules/babel-runtime`\nThen it resolves for them no matter what.\nnpm prune would blow it away, but we would just put it back each time.\n. Wrapping the Babel-6 plugin with our own require paths would be easy enough:\nhttps://github.com/babel/babel/blob/master/packages/babel-plugin-transform-runtime/src/index.js\njust override the pre fuction\n. @sindresorhus @floatdrop \nSee https://github.com/jamestalmage/hack-babel-5-runtime-plugin\nIt is possible to wrap the existing babel runtime plugins (both v5 and v6) and manipulate the require paths they put in the source.\n. Should we be sending a different kill signal?\nhttps://nodejs.org/api/process.html#process_signal_events\n\n\nSIGTERM and SIGINT have default handlers on non-Windows platforms that resets the terminal mode before exiting with code 128 + signal number. If one of these signals has a listener installed, its default behaviour will be removed (node will no longer exit).\nSIGPIPE is ignored by default, it can have a listener installed.\nSIGHUP is generated on Windows when the console window is closed, and on other platforms under various similar conditions, see signal(7). It can have a listener installed, however node will be unconditionally terminated by Windows about 10 seconds later. On non-Windows platforms, the default behaviour of SIGHUP is to terminate node, but once a listener has been installed its default behaviour will be removed.\nSIGTERM is not supported on Windows, it can be listened on.\nSIGINT from the terminal is supported on all platforms, and can usually be generated with CTRL+C (though this may be configurable). It is not generated when terminal raw mode is enabled.\nSIGBREAK is delivered on Windows when CTRL+BREAK is pressed, on non-Windows platforms it can be listened on, but there is no way to send or generate it.\n\n\nMaybe the forks should just be in charge of terminating themselves. \n. Didn't mean to.\n. Or just default the whitelist to include test/_*, since the underscore prefix is already set aside to denote helpers. \nOr just everything in the test directory? Event test/fixtures/**?\n. Thats my feeling as well.\n. Because, one goal of this library is not to pollute any of your production code with babel transforms/polyfills (unless you specifically ask it to).\n. Yes. Still a problem (when executing ava directly from console - npm test works fine). Also fixed when I upgraded ava to match local version.\n. Are you open to github-pages documentation, or do you want to keep it all in the README?\nhttp://documentation.js.org/ looks interesting. (particularly this demo)\nAlso, you probably want to reserve ava at https://dns.js.org/ (that would get you https://ava.js.org)\n. Wow, documentation.js produces really nice output.\nEven the base output is pretty slick. Each method has a link to the source on GitHub (that works really well). Base types automatically link to developer.mozilla.org\n. Another plus for generation API docs using some JSDoc based system is that it improves typing hints on Webstorm significantly (I am assuming other IDE's as well).\n. The documentation should cover how to get AVA working with production code.\nThere is a lot of confusion #111 \nIt should incorporate the code from this comment.\nIt gets the point across pretty quick\n. @kentcdodds - That's awesome news!\n. We will have to run zen-observables through the Babel runtime plugin. As well as anything using Symbol.\n. We will have to run zen-observables through the Babel runtime plugin. As well as anything using Symbol.\n. @BarryThePenguin #200 - will fix the Node 0.10 error - it exists on master (I don't know how the latest master build passed)\n. :cry: If only it were that easy.\n. :cry: If only it were that easy.\n. Finally!\n@sindresorhus - I'm gonna let the latest push go green and then squash\n. Finally!\n@sindresorhus - I'm gonna let the latest push go green and then squash\n. @sindresorhus @vdemedes \nI've made the recommended changes. Anything else holding this up? It would be nice if peoples PR's could go green again.\n. I don't think there is anything stopping us from doing that now.\nI did have one thought with regard to #140, and that is to leave processes running as long as possible to give us the best chance of catching extra assertion calls (beyond plan), and repetitive calls to t.end()\nIn that case it would be up to the parent to close.\nBut if we think that is a bad idea, then no - there is probably no reason to do it the way we are.\n. >  I don't like global test timeouts -- I think they're a terrible idea.\nWhy?\n. >  I don't like global test timeouts -- I think they're a terrible idea.\nWhy?\n. Right now AVA hangs forever if you fail to call t.end(). That's no good. Especially if one of those tests make it to your CI server.\nOne thing I am always writing (in mocha) is:\njs\nif (process.env.TRAVIS) {\n   this.timeout(fiveTimesWhateverItTakesLocally) \n}\nI dislike having to wait to see my mistake during development, just to accommodate the slower CPU on my CI. It would be cool to build that in to AVA (though maybe not possible to do in a reliable way across all CI implementations out there).\nEither way, I think AVA needs global and per tests timeouts with sensible defaults.\n. Ooh. That is a hard question. :smile: \nLocally, I would prefer something really fast. 500 ms or something. Some users might find that surprising (though it might also help to gently remind people that they should be writing small, fast tests). It may also unfairly inconvenience users with really old/slow CPU's.\n. I think both cli and api hooks are a given. We are all just bikeshedding over what the defaults will be. \n. Me too. I found 2 seconds way too long! :laughing: \n. We need some sort of default, just as a kindness to CI providers.\n. > time in seconds after the last test completed that remaining tests should be aborted\nWould we measure that globally, or per test file?\n. There isn't a per test solution. Because we launch all your tests async, and they interleave, a per test timeout wouldn't be very reliable.\nInstead you just set a global timeout that measures time between test results. That's best used for preventing overlong builds on your CI server.\n. @MarkHerhold \nCurrently, the best workaround for achieving the behavior you desire is:\nNODE_ENV=test NODE_CONFIG_DIR=$(pwd) ava test/**/*.js\nAt the top of every test:\njs\nprocess.cwd(process.env.NODE_CONFIG_DIR);\n@sindresorhus @vdemedes  - Do we want to add a flag or environment variable to address this? \n. It might be good to make the same request of the community that other projects do. Save the ava- prefix for AVA internal packages (and maybe plugins down the road) and instead use an -ava suffix. \n. > What happens if you have a plan and call t.end?\nI don't know, but we probably should have a test for it.\n. @timoxley looking at the code again I'm pretty sure you are going to get a bad error message if you use plan and end with this.\nPlease write a test that does this:\njs\ntest(t => {\n  t.plan(1);\n  t.ok(1);\n  t.end(new Error('you should get this error message, not a plan count error'));\n});\nAnd check the message of the thrown exception.\n. @timoxley I think it's down to that one if statement and another test ensuring your use of ifError does not generate a plan count error message.\n. @timoxley I think it's down to that one if statement and another test ensuring your use of ifError does not generate a plan count error message.\n. @floatdrop \nWhat is your plan for the cache? Write it all to disk before launching forks?\nI'm actually firing up an http server and then using child_process.execSync('curl', ...) to fetch the transpiled code.\n. @floatdrop \nWhat is your plan for the cache? Write it all to disk before launching forks?\nI'm actually firing up an http server and then using child_process.execSync('curl', ...) to fetch the transpiled code.\n. The major problem with my solution is that it requires actually forking an entirely new process for every fetch, which is not free. Forking an execution of curl vs forking another node process is way better, but still not great.\nFor the initial test file, we could use IPC (maybe even do static analysis of the Babel AST to find additional require calls).\nDynamic requires will be a problem. That is where I think my strategy will help. If somebody does a lot of dynamic calls there is even a chance my strategy may hurt, so we probably need a way for people to turn it off.\n. The major problem with my solution is that it requires actually forking an entirely new process for every fetch, which is not free. Forking an execution of curl vs forking another node process is way better, but still not great.\nFor the initial test file, we could use IPC (maybe even do static analysis of the Babel AST to find additional require calls).\nDynamic requires will be a problem. That is where I think my strategy will help. If somebody does a lot of dynamic calls there is even a chance my strategy may hurt, so we probably need a way for people to turn it off.\n. Gotcha.\nSeems reasonable.\n. Gotcha.\nSeems reasonable.\n. @sindresorhus We need a good real-world library to performance test this against. I know pageres is mentioned in the readme, but given the nature of that project (I am just guessing), that a lot of time is spent on I/O. Is there one with a high test-file count where I/O is minimal (where improvements to the load time of AVA forks would be most noticeable).\n. @sindresorhus We need a good real-world library to performance test this against. I know pageres is mentioned in the readme, but given the nature of that project (I am just guessing), that a lot of time is spent on I/O. Is there one with a high test-file count where I/O is minimal (where improvements to the load time of AVA forks would be most noticeable).\n. This was fixed in ~0.10.0\n. TAP is a good idea, but a specific integration could offer some really cool stuff too.\nA few ideas:\n1. the power-assert instrumentation already captures the line numbers and values of every assertion. So we we could put check boxes next to every assertion in the code editor as it runs. When you mouse over (even a passed assertion), we could still output the espower graph in a popup. That could be handy if you follow the practice of writing failing test first and are surprised by a passing test. Also if you have some complicated assertion that's passing for reasons you don't understand:\njs\n   t.ok(/complicatedRegex/.test(stringThatPassesForUnforseenReason));\n                           |    |\n                           |    \"string value\"\n                           true\n2. Also, with AVA's concurrent test runs, logging statements from multiple processes are getting piped to stdout concurrently. That can cause confusion as to what output belongs to the test that is failing. Inside an IDE we could track those separately and only show stdout from the relevant test file.\n3. It would be pretty cool to somehow automatically / seamlessly turn just changed tests into test.only behind the scenes. (Though we could probably implement that kind of magic outside of AVA/Atom entirely).\n. You do need to commit the transpiled dist if you want to temporarily rely on your hotfix:\n$ npm install -S jamestalmage/somerepo#somebranch\nYou would need to have dist published in somebranch.\nThe brittle install is sufficient argument against.\n. yeah, I thought of that.\nIt would be cool if npm publish had a --scoped that allowed you to do that without modifying package.json\n. That requires ownership of the module being deployed. I have been added as a collaborator since making that comment, so it's moot for me in the AVA context, but still applicable to any module you don't own.\nI filed this proposal with npm. The gist is that I think there should be an easy way to deploy hot fixes for projects you don't own (under your own namespace).\n. NYC relies on istanbul. See this issue: https://github.com/gotwarlost/istanbul/issues/212\nistanbul has a source-map branch that looks like it is getting pretty close, but looks like it doesn't play well with some reporters.\nI have been told that integrating istanbul-combine solves the issue, but I never got around to it.\nIf you can't wait for istanbul and NYC to release source map support, I recommend trying to fork NYC to use the istanbul source-map branch. If you run into issues combining coverage files checkout istanbul-combine\n. NYC relies on istanbul. See this issue: https://github.com/gotwarlost/istanbul/issues/212\nistanbul has a source-map branch that looks like it is getting pretty close, but looks like it doesn't play well with some reporters.\nI have been told that integrating istanbul-combine solves the issue, but I never got around to it.\nIf you can't wait for istanbul and NYC to release source map support, I recommend trying to fork NYC to use the istanbul source-map branch. If you run into issues combining coverage files checkout istanbul-combine\n. @novemberborn With https://github.com/sindresorhus/ava/commit/fb98d5dcff7ac4810efa5ef5ee42b0d041c89d13 merged where do we stand on this?\nCan this be closed, or is this a different issue?\n. @bcoe,\nI want to make sure AVA is able to take advantage of the latest nyc source-map goodness. We are doing a couple things that I suspect may interfere. I'm just going to list them; If you can tell me if anything sounds problematic, I'd appreciate it.\n1. AVA transpiles test files only, and avoids babel-core/register by default. This allows users to write tests in ES2015, even when they prefer to keep their module code in ES5 (good for small modules where you don't want a build step). If users want transpile their main modules, they need to call the register hook themselves.\n2. We want to allow \"helper\" files in the test folder (utilities that are shareable between tests). Currently that just means, any file with a _ prefix. We will be hooking require to automatically transpile these as well (all other files will pass through unmodified by default). I see https://github.com/bcoe/nyc/pull/58 hooks require as well. I also saw your discussion here: https://github.com/babel/babel/pull/3062. Any pointers as I implement AVA's require hook so I don't clobber nyc source-map support?\n3. We also intend to implement transpilation caching. If a file has not changed since last run, we will reuse the old transpilation result. We will store cached results as files  in a temp folder, named according to the hash of their contents (with periodic deletion of least recently used items). This means the transpiled source (and attached sourcemap) will likely not be where nyc expects to find it. Assuming nyc instruments coverage on top of transpiled files, and then uses source-maps after the fact to generate coverage for the original source after the fact, we would probably need a way to tell nyc where to find source-map information for a given file.\nI look forward to your response. Thanks for your time, and for making nyc awesome! \n. Thanks for the quick response. We will follow up as we move those initiatives along.\n. diff\n\"scripts\": {\n-    \"test\": \"ava src/test/**/*.js\",\n+    \"test\": \"nyc --reporter=lcov ava src/test/**/*.js\"\n-    \"coverage\": \"nyc npm test\"\n}\n. diff\n\"scripts\": {\n-    \"test\": \"ava src/test/**/*.js\",\n+    \"test\": \"nyc --reporter=lcov ava src/test/**/*.js\"\n-    \"coverage\": \"nyc npm test\"\n}\n. have you installed ava locally?\nnpm install --save-dev ava\n. have you installed ava locally?\nnpm install --save-dev ava\n. Huh, \nDelete your node_modules folder then:\nnpm cache clean && npm install\n. Huh, \nDelete your node_modules folder then:\nnpm cache clean && npm install\n. @MoOx The ability to add custom assertions is on the roadmap.\n. @MoOx The ability to add custom assertions is on the roadmap.\n. :angry:\nWindows and Node 0.10 are quickly becoming annoying.\n. :angry:\nWindows and Node 0.10 are quickly becoming annoying.\n. It looks like AppVeyor is having some issues. Those pass just fine in my Windows VM. It's not making it through npm install on AppVeyor\n. What the heck? It was green after rebase! All I did was squash!\n. Crap. Adding the tap-spec reporter should not have made this pass.\nWe have intermittent test failures on CI that I can not reproduce locally at all.\n. @sindresorhus \nI think this is ready for merge.\nI can drop the tap-spec commits, but I wanted you to see the brittle test issue I'm having before I do. (and if it fails again when I revert to @2f16758, I may toss my computer out the window).\n. https://github.com/jamestalmage/babel-plugin-detective\nIt is based on Browserify's algorithm for finding dependencies.\nOnce we make the move to Babel 6, we can use this to precompile dependencies before ever launching the test. We will need other strategies for dynamic requires (possibly the server / client option I discussed earlier), but initially we can demand static requires if they want them transpiled (should cover 90% of use cases anyways). \n. @sohamkamani That build failure is almost certainly not your fault, so don't worry about that.\n. Failure noted in the tracking issue.\n. PR: #200 \nFailed Build: https://ci.appveyor.com/project/sindresorhus/ava/build/96\nFailed Assertions:\n- https://github.com/jamestalmage/ava/blob/732b4c231b7dfd208703dea55713580e0f96e276/test/test.js#L1061\n- (this one twice): https://github.com/jamestalmage/ava/blob/732b4c231b7dfd208703dea55713580e0f96e276/test/test.js#L955\nRelated to stderr / stdout: YES\n. PR: #205 \nFailed Build: https://ci.appveyor.com/project/sindresorhus/ava/build/103\nFailed Assertions:\n(this one twice): \nhttps://github.com/sindresorhus/ava/blob/0529ee2737e48d9e4e22217edcb9c5a10786b16c/test/test.js#L955\nRelated to stderr/stdout: YES\n. PR: #206 \nFailed Build: https://ci.appveyor.com/project/sindresorhus/ava/build/106\nFailed Assertions: https://github.com/sindresorhus/ava/blob/a54a6cf28b320359afc7ab5beb302bca079142a3/test/test.js#L1061\nRelated to stderr/stdout: YES\n. PR: #209 \nFailed Build: https://ci.appveyor.com/project/sindresorhus/ava/build/111/job/9jpeuk7w1iqhtgy4#L892\nFailed Assertions:\n(three times): https://github.com/GabiGrin/ava/blob/baf9e5ca45b8b5d105c1ddfb6ba3c54082406d39/test/test.js#L955\nRelated to stderr/stdout\n. This has adequately demonstrated that it's random errors with stdout / stderr not getting the complete output. The fix is known and submitted. No need to track errors anymore.\n. Yeah - I can't think of a good way to handle this other than tell people not to do it.\n. Passes on Windows 7 / Node 10 here.\nI think it is just AppVeyor again.\nGive me a moment. to get those logged in the tracking issue.\n. @lijunle \nI was actually taking a stab at this earlier when I got pulled away to some other stuff.\nSince you are pretty far down the road, I am holding off.\nI just squashed my work and pushed it up: https://github.com/jamestalmage/ava/commit/4568347c6ae230e8f03bb20f93f5430ce7bfff58\nUse it if it helps.\nMy other thoughts.\n1. I would really like to avoid the Mocha behavior of only allowing a single test in exclusive mode. I want to be able to put only on however many tests I chose and have them be the ones that run. This is especially important since we don't have grouping yet.\n2. I like your idea of test.serial.only, etc.\n3. I think creating stats at the beginning is making all this hard. Just put flags in each individual test and use this.tests.concurrent.filter to create the stats and get at the tests you want (see the diff I linked).\nGood work! And thanks for the help.\n. Yeah, sorry! \nI've got a lot going on today and am lagging way behind the conversation.\n. > Why combine the unhandledRejection/uncaughtException stuff with the rest? Seems it could be a separate PR?\nThe uncaught exception stuff is definitely coupled. These all started as separate PR's (see description).\n. My preference would be to get this merged so everybody can start getting green PR's again. I can then work backwards and see what can be pulled out. Working backwards from green is going to be easier than going the other direction.\n. Just Sayin...\n\n. @sindresorhus \nEvery comment except the separating out loud-rejection promises is addressed.\nI think I can pull it out if you really think it is necessary. I think we will still have reliable builds without it.\n. serialize-value is part of the fix for uncaught exceptions, and that is part of the solution for reliable builds. Allowed to go uncaught, they cause the child to exit unpredictably (we are not sure exactly what messages the parent has received). With this we notify parent \"hey, uncaught error here, bailing!\", wait for acknowledgement, and then shut down. Otherwise we must rely on stderr / stdout from the child to be properly piped to stderr/stdout on the parent. For reasons unknown to me, that is just too unreliable on Windows to be a solution.\nUnfortunately, I committed the unhandled-rejection stuff before the uncaught-exception, and there a number of conflicts between the two (here, here, etc.). Part of the frustration was that everything passed on my machine, yet failed on AppVeyor. Scroll through the AppVeyor history if you wanna see my name 3 dozen times next to a lot of red :angry:. It is probably not a huge deal to pull out the unhandled rejection stuff, but there will be merge conflicts to resolve before and after, and it just seems like extra work for not a lot of benefit.\nI think I did do a pretty good job maintaining a good git history, and have squashed relevant commits together. Perhaps do not squash this PR so git blame and the commit log maintain finer grained details for each change?\n. > no offence\nnone taken :smile: \nThe only thing that has offended me during this process is Windows!\n. Awesome!\n\nI would honestly have just given up and removed AppVeyor :p\n\nNow you tell me!! :wink: \n. We are not Babel v6 ready. #221 is in progress, but until some blocking issues get resolved - v6 is not an option. It will happen soon.\n. TypeError: (0 , _ava2.default) is not a function\nThis is a known issue in Babel v6: https://phabricator.babeljs.io/T6644\nThis is the primary blocker for AVA upgrading to Babel.\n. Oops - the actual error confirmed to be Babel6 is:\n(0 , _typeof3.default) is not a function.\nI copy and pasted without noticing the difference.\nThat said, I still suspect you have a babel 6 dependency hanging out.\nAssuming you are not on windows, can you run the following from your project directory:\n$ npm ls --depth=0 | grep babel\nAnd paste the contents here.\n. Is ava only installed globally?\n. Try reinstalling locally and running as follows:\n./node_modules/.bin/ava, that will execute the locally installed version.\nStill, the global one should work, as long as it is the same version as your local install.\nYou may want to also try the following, just to ensure you are not working off a corrupted install:\nnpm cache clean && npm rm -g ava && npm rm ava && npm i -g ava && npm i -D ava\n. That looks like a stack trace from running globally. What happens with a local run ./node_modules/.bin/ava\n. You probably got those because we had you delete babel in earlier steps.\nIf you put the following in package.json:\njson\n{\n  \"scripts\" : {\n    \"test\": \"ava\"\n  }\n}\nThen you can run npm test, and it will use the local version every time.\n. @mnzt - This issue is caused by installing babel 6 locally, not local vs global ava cli execution.\nFYI:\nAs of 0.7.0, the ava command will always run the local version of ava if available. Even if invoked globally. That release contains a number of breaking changes, so read the changelog before upgrading.\nIt should be safe to install 0.7.0 globally, even if your project depends on an earlier version locally. As long as you have run nmp install and the older ava dependency exists locally it should be used.\n$ npm install --global ava@latest\n. > Really wish the readme clearly stated ava doesn't work with babel 6. \nThat is a good point - I will fix that immediately.\n\nMaybe I'm not understanding things, but this really seems flawed.\n\nI agree. Unfortunately a solution that allows you to use whatever version of Babel you want is not trivial. We are waiting on a few blocking issues in Babel 6, and will transition at that time.\n. @novemberborn - we are headed in that direction already. https://github.com/sindresorhus/ava/pull/221#discussion_r46668343\n. @novemberborn - we are headed in that direction already. https://github.com/sindresorhus/ava/pull/221#discussion_r46668343\n. The solution discussed in https://github.com/bcoe/nyc/issues/70 could become of use to us in this situation as well.\n. The solution discussed in https://github.com/bcoe/nyc/issues/70 could become of use to us in this situation as well.\n. I've always been in the custom of putting \"fixes...\" at the end, but noticed you put it at the beginning in a number of commits. But here you changed it to the end.\nJust want to understand the process.\n. gotcha. Fixes are in.\n. Fix landed in @97409864e, you should be good to go on next release \n. IMO, the proper thing to do in a file without tests is to fail. Helper files should be prefixed with a _ character.\n\nThe only way I found around this, because of the complex process nature of the code, is to send a message once AVA runs on a file, and another message when babel finishes\n\nSee #208 for my solution to the problem. It's equally convoluted, but does not require sending messages between processes. \n. npm install issue: https://github.com/npm/npm/issues/9299\n. closing in favor of #206\n. Babel 6 has crazy long bootup.\n148 and #182 are going to become really important.\n. ~850ms: https://github.com/jamestalmage/__time-require-babel-5\n~3.6 seconds: https://github.com/jamestalmage/__time-require-babel-6\n. Never Mind:\nnpm dedupe improves babel 6 times drastically.\nI still had npm@2 lying around in my node 4.1.2 install\nSo it seems you trade a longer npm install, for faster requires.  Worth it, it seems to me.\n. closing in favor of #206 \n. I noticed in the bluebird codebase that it publishes unhandledRejections to a specific domain (at least I thought that was what it was doing), so we should be able to trace promise rejections back to a specific test.\nThere are still multiple ways to find yourself in a callback that is not wrapped in a promise. (event-emitters, setTimeout).\nI'm certainly not married to Domains (I've never played with them), if there are better solutions, lets do that.\n. Is async-listener landed? Doesn't seem to be. \n. Argghhh!!\n\nDespite the failure - This should be merged.\nThere is obviously something else in #213 that let it pass 5 times in a row, but this was definitely part of the solution.\n. Closing in favor of #206 \n. You probably have different versions installed locally and globally, run the following two commands.\n$ ava test/**/*.test.js\n$ ./node_modules/.bin/ava test/**/*.test.js\nThere is a way to fix this, but it is fairly controversial.\n. Is the repo you are using this with open source? If so can you post a link? \n. @Lokua the fix for that issue is in. We now include a check for empty test files. It's odd that it is only happening in local installs and not global, but the next release should give you a clearer error at least.\n. @Lokua \nI had some initial problems with global on your repo, but got it working pretty easy.\nTry the following in your project directory\nsh\n$ npm i -g npm@latest\n$ npm rm -g ava\n$ npm i -g ava@latest\n$ npm rm ava\n$ npm i ava@latest\nTry it both ways again and report back.\n. Confirmed. AVA 0.4.2, Windows 7, Node 5.0.0, NPM 3.3.6\nAlready fixed in master.\nnpm install -D sindresorhus/ava#78a6c56e\nnpm install -g sindresorhus/ava#78a6c56e\n. Duplicate of #121 \nFix PR: #259 \n. Ugh, why did they do that!\nI looked and can't find the original issue in the new tracker.\nI will look into this later today.\n. ~~Yeah, I searched for that and did not find~~\n~~They have released a couple runtimes since, so probably the easiest option is for me to try again. Won't take me long. Just has to happen after work.~~\nThanks @thejameskyle \n. Looks like there is a pending PR to fix our blocking issue.\n. Looks like there is a pending PR to fix our blocking issue.\n. > I believe this is also blocking => babel/babel#3139\nNo, shouldn't be. nyc already works around it. Babel's hook is perfectly fine if installed first. nyc has a solution in place that allows that allows the babel hook to be installed second, after nyc (but breaks if further hooks are installed). \ncapture-require will solve that for them, but it wreaks havoc on their test suite. capture-require can be used multiple times without breaking previous hooks. Their current test suite relies on the fact that nyc.wrapRequire() breaks the previously installed hook. That is basically fixed, just need to get some consensus around it.\n. > I believe this is also blocking => babel/babel#3139\nNo, shouldn't be. nyc already works around it. Babel's hook is perfectly fine if installed first. nyc has a solution in place that allows that allows the babel hook to be installed second, after nyc (but breaks if further hooks are installed). \ncapture-require will solve that for them, but it wreaks havoc on their test suite. capture-require can be used multiple times without breaking previous hooks. Their current test suite relies on the fact that nyc.wrapRequire() breaks the previously installed hook. That is basically fixed, just need to get some consensus around it.\n. > Another take on fixing the issue => babel/babel#3142\nYeah, I think that's the one that will land\n. > Another take on fixing the issue => babel/babel#3142\nYeah, I think that's the one that will land\n. Closing in favor of #333 \nThere is only one concern mentioned here that still applies, and I have transferred that to the new PR.\n. I want to add one more little spin on groups. \nSometimes I have a group of identical tests that I want to run across a number of inputs. Examples include:\n1. My transform works with esprima / acorn / babel generated tree AST's.\n2. My promise based utility works with bluebird, pinkie, etc. (Maybe if offers some added bonus features if the more powerful bluebird API is available, but still works with smaller implementations).\n3. I offer different optimization options for some algorithm, but I want to verify they all produce identical output.\nUsing mocha, I usually accomplish this as follows:\n``` js\nfunction testWith(opts) {\n  describe('with opts:  ' + JSON.stringify(opts), function () {\n    it(....);\n  });\n}\ntestWith(optsA);\ntestWith(optsB);\n```\nIf you want good examples of this, check out the power-assert test suite. This construct is all over the place. (cc: @twada).\n\nI think it would be awesome if our groups API allowed something like this.\n``` js\nvar bluebird = test.group('bluebird');\nvar pinkie = test.group('pinkie');\nbluebird.beforeEach(t => t.context.Promise = require('bluebird'));\npinkie.beforeEach(t => t.context.Promise = require('pinkie'));\nvar both = test.group(bluebird, pinkie);\nboth.beforeEach(...);\nboth.test(...); // common test\nbluebird.test(...); // bluebird specific test\n```\nThis has a huge advantage over the method I described for mocha, in that I can easily make some tests specific to bluebird, while reusing the beforeEach setup I already have in place. In my mocha example above, any bluebird specific test would need to be declared outside the testWith function, and the beforeEach setup would need to be duplicated.\nI still think we should provide a nesting API as well. With this in place, it would be pretty trivial:\njs\nvar bluebird = test.group('bluebird', group => {\n  // group === bluebird\n});\n. Well, let's flesh out a more complete, functionally identical example for both and see:\n``` js\nconst bluebird = test.group('bluebird');\nconst pinkie = test.group('pinkie');\nconst q = test.group('q.js'); \nconst all = test.group(bluebird, pinkie, q);\nbluebird.beforeEach(t => t.context.Promise = require('bluebird'));\npinkie.beforeEach(t => t.context.Promise = require('pinkie'));\nq.beforeEach(t => t.context.Promise = require('q'));\nall.beforeEach('common setup', t => {\n    // common setup\n});\nall('foo', async t => {\n    t.ok(await itSupportsFoo(t.context.Promise));\n});\nbluebird('blue foo', t => {\n    t.ok(blueFoo(t.context.Promise));\n}); \ntest(bluebird, q, 'no pinkie', t => t.ok(notPinkie(t.context.Promise)));\n```\n``` js\nfunction testWith(opts) {\n    test.group(opts.name, test => {\n        opts.bluebird && test.beforeEach(t => t.context.Promise = require('bluebird'));\n        opts.pinkie && test.beforeEach(t=> t.context.Promise = require('pinkie'));\n        opts.q && test.beforeEach(t=> t.context.Promise = require('q'));\n    test.beforeEach('common setup', t => {\n        // common setup\n    });\n\n    test('foo', async t => {\n        t.ok(await itSupportsFoo(t.context.Promise));\n    });\n\n    opts.bluebird && test('blue foo', t => {\n        t.ok(blueFoo(t.context.Promise));\n    });\n\n    (opts.bluebird || opts.q) && test('no pinkie', t => {\n        t.ok(notPinkie(t.context.Promise)); \n    });\n});\n\n});\ntestWith({\n    name: 'bluebird',\n    bluebird: true\n});\ntestWith({\n    name: 'pinkie',\n    pinkie: true\n});\ntestWith({\n    name: 'q.js',\n    q: true\n});\n```\n. Now imagine you have a larger test suite where you want to use this setup across multiple test files:\n``` js\n// _groups.js helper file\nexport const bluebird = test.group('bluebird');\nexport const pinkie = test.group('pinkie');\nexport const q = test.group('q');\nexport const all = test.group(bluebird, pinkie, q);\nbluebird.beforeEach(t => t.context.Promise = require('bluebird'));\npinkie.beforeEach(t => t.context.Promise = require('pinkie'));\nq.beforeEach(t => t.context.Promise = require('q'));\n```\n``` js\n// test.js\nimport {bluebird, pinkie, q, all} from './_groups';\nbluebird('blueFoo', ...);\n```\nI don't know what a convenient way to do this with wrapper functions would be.\n. js\n        test.beforeEach(t => { \n            t.context.Promise = require(opts.name);\n            // common setup\n        });\nWhile that optimization works here, there are plenty of scenarios where it won't. You may have very complex setups that differ widely (different database backends, parser API's that differ, etc).\nI completely disagree on one being \"easier to discover\". I have seen plenty mocha test suites where it can be really confusing which nested group you are in:\n``` js\ndescribe('foo', () => {\n  describe('bar', () => {\n    beforeEach(...);\n// pages later\n\nit(...); // Wait, am I still inside 'bar' or not?\n\n```\nI'm not saying you couldn't find ways to abuse my proposal. Just that there is plenty of rope to hang yourself with either.\n. To be clear, nesting would still be supported in my proposal. You could certainly elect to stick with it. I just strongly dislike look of the extra nesting required with your approach. I just feel my is cleaner and more readable.\nIf we were to implement my idea. I think a \"best practice\" would be to name your groups at the top of a file. I can't see how it would be much harder to write a Babel plugin to analyze my proposed structure (though I grant you it would be a little harder). That said, it also wouldn't be hard to expose the structure by emitting an event prior to firing off tests (if some of the concerns you are raising are wallabyjs related).\n. I could perform the same hacks to the array in your mocha example, creating the same difficulties. \n\nMy concerns are not wallaby related\n\nThen why are you bringing up the Babel plugin? Is that something someone would use IRL?\n. > Runner.prototype.tests\nWell... not on the prototype hopefully. :smile: \nBut yes, that is basically what is being discussed here. You should definitely hold off on creating a PR until:\n1. #466 lands\n2. We get consensus here on the API (it's still not settled that we actually want to add it).\n. You might have good reason to want individual files. Each separate file gets it's own process, and runs concurrently, so there might be some speed benefits.\n. > I've never really understood the use case for t.context (or using this inside mocha tests)... what does t.context give me that I can't accomplish with a simple variable?\nA simple variable doesn't work with AVA if you have async tests, consider this example:\n``` js\nimport test from 'test';\nimport delay from 'delay';\nvar a;\ntest.beforeEach(() => a = 'foo');\ntest(async t => {\n  await delay(20);\n  t.is(a, 'foo');\n});\ntest(async t => {\n  await delay(10);\n  t.is(a, 'foo');\n  a = 'bar';\n});\n```\nThe first test is going to fail, because a will be set to \"bar\" while it awaits the completion of delay(20).\n. @novemberborn \nWhat issue are you trying to address here? Correct line numbers in stack trace output of transpiled test files? Consuming source-maps? Give us an example of how AVA is not doing what you expect.\n. #254 fixes this for your test file.\nGetting it completely right will not be trivial. See #254 for discussion.\n. The issue is not with require-from-string.\nYou have deep-extend installed, which ships with it's own test/index.spec.js, and that is getting picked up with whatever glob string you are using.\nMaybe AVA should avoid picking up files in node_modules, even if their glob string says too.\nhttps://github.com/unclechu/node-deep-extend/pull/21\n```\n\u279c  ava-test  ava\n\u2714 dummy\n1 test passed\n```\n. @billyjanitsch \nWere you using the default glob string, or specifying your own? We should probably prevent AVA from picking that sort of file up by default.\n. happens to the best of us.\nI opened #226 to discuss the possibility of protecting against this sort of mistake.\n. Oh sorry, I wasn't clear - I intended it just for internal development use. Not public consumption. I guess it would not be a terrible idea to publish it. We could ask users to execute tests with it and send the output. Might help hunt down some difficult bug someday.\nWhat do you think? \n. That still requires forking a process. It doesn't capture everything sent via process.send.\n. Did somebody force push to master?\n. Known issue:\nhttps://github.com/scottcorgan/tap-spec/issues/40\n. > A custom compiler for my tests' dependencies as suggested in #111 would be inefficient, since if a test imported more than one file, webpack would have to compile them separately.\nOff topic here, but Babel suffers from the same issue across processes. Since AVA runs each test in a separate process, if you import it test-a.js, and test-b.js it compiles it once in each process. There is work on a solution (#189), that might be usable to help with that.\nRight now, the intricacies of getting this to work well and work fast are challenging enough targeting Babel only.\nAny custom pre-processor you wrote would have to handle the IPC protocol we already have in place, would have to apply the power-assert transform on its own. As well as integrate communication of uncaught exceptions, and unhandled promise rejections. All that is currently bundled in babel.js. Some would be fairly easy to extract and make generic, some would not.\nAn additional concern is that this project is currently moving REALLY fast. Maintaining multiple preprocessor solutions at this point will slow us down, which I think would be bad for the project overall.\nThat said, I think there is value to this at some point. I just don't think it should be a priority.\nMe of today: :-1: \nMe of the future: :+1: \n. > If you're at all convinced, I'm happy to work on a PR proposal.\nWait for a project maintainer to chime in.\n. It's almost there, we really need https://github.com/bcoe/nyc/pull/90 to land for this to work reliably (which is itself waiting on https://github.com/bcoe/nyc/pull/89). A single .js extension will probably work before then. Once it lands though, it will be a lot easier to integrate all this.\nWe may also be waiting for https://github.com/babel/babel/pull/3139 before it all works the way I want it to in the end.\nI have not looked at how webpack handles \"require aliasing\". Hopefully they extend require.extensions correctly.\nWe are getting there.\n. Closing in favor of #722 and https://github.com/sindresorhus/ava/issues/631#issuecomment-198973019.\nOnce we allow any extension for tests, this is already solved using --require.\n722 proposes a way to speed up alternate transpilers by allowing them to participate in the main thread and caching precompiler\n. Yes sorry,\nGot pulled away. I'll clean house here shortly.\n. > Or fix the tap-spec one\nI am minutes out from submitting a PR for exactly that (need to fix pinkie first though).\nI can not find any way of actually checking what the exit code of the process being piped in is. The issue with tap-spec is that it is not inspecting the plan line, and validating it against the number of results it sees. \nFor those who are interested, this is a plan line in TAP:\n1...48\nIt means \"plan for tests 1 to 48\"\nIt always comes at the end in tape, not sure about other runners.\n. Yes, if you clone this and run tests it passes (on OSX at least).\nTravis does not like this, so I've opted for another solution.\n. Do you know what promise library you are using?\n. Try switching to bluebird and see if it fixes it.\nWe just added unhandledRejection to pinkie earlier this week, and there could be a bug in the implementation.\n. In fact, I'm pretty sure I see the bug already\n. it's the chaining that is the problem.\npromise.catch() <- marks this as handled\npromise.then().catch() <- only marks the last one as handled.\n// @floatdrop - I'm working up a PR.\n. https://github.com/floatdrop/pinkie/pull/11\nnpm install jamestalmage/pinkie#fix-chained-rejection\nLet me know if it works for you. (note that is still an open branch, and might get some changes before the PR is done - don't use it IRL).\n. Yeah, we can switch back when tap-spec gets fixed. I have a PR in for it's dependency, so hopefully he follows through and pulls that upstream.\nI would prefer to continue working on AVA for now and just go with this. If he doesn't merge and pull the fix upstream in a few days, I will patch tap-spec myself and we can use that.\n. You need to throw an uncaught error a tape test (so the tape command exits suddenly).\nA failing test that does not cause the process to exit will still fail the build.\n. See this build: https://travis-ci.org/sindresorhus/ava/jobs/91510425\nExits passing after only 45 tests.\n. closed in favor of #234 \n. done.\nWhy does tap set the timeout per file? That is crazy. Why not per individual test?\nI understand why you guys were arguing for 30 seconds now.\n. Let's do \"per test\" timeouts in AVA.\n. I did 150 seconds.\n. done\n. @sindresorhus - I asked for this. Not sure where the source of his problem is (be it pinkie or something in AVA-- but probably pinkie)\n@TrySound Does this fail on local windows boxes, or just AppVeyor?\n. You can't just install the 32 bit version? \nnvm install 0.10.40 32?\nI've got a copy of 32 bit windows, so I am not sure\n. the version of nvm on windows is weird...\neverything about windows ....\n. @TrySound \nSo no problem in AVA or pinkie?\n. Opening private gitter chat so we can stop spamming everybody.\n. This should be closed.\n. Nice!\nYou taking a break now, or are you moving on to a major refactor?\n. Leave runner.js to me for a while if you don't mind.\n. OK\n. Yes, failure is different now. \"Missing plan\".\nIn TAP, the plan line can be emitted at the beginning or end. If it is the end, we already know forked processes often drop a line or two of IO from the end on Windows (especially Node 0.10). It could be something else entirely, but that is my initial suspicion.\n. Yes. Looks very relevant. Perhaps we need to avoid explicitly ending the process unless we have to.\n. I begrudgingly agree. Very disappointed this is the route we have to take:\nhttp://help.appveyor.com/discussions/problems/1352-output-of-commands-is-often-trimmed\nhttps://github.com/mapnik/node-mapnik/issues/257#issuecomment-45571226\nhttp://help.appveyor.com/discussions/problems/1257-console-output-suppressed\n. Here is the core issue: https://github.com/nodejs/node-v0.x-archive/pull/7196\nThe fix needs to be back-ported to the Node 0.10 branch. A number of people have plus oned the issue, without a lot of traction.\n@sindresorhus Do you know anybody working on node that could push that along? This will impact our ability to provide TAP output on Node 0.10 / Windows.\nIf we can't get this fixed upstream, we should at least do our users the courtesy of officially NOT supporting it, and documenting the lack of support. Something along the lines of \"We support all LTS versions of Node, with the exception of 0.10 on Windows.\" (and a link to the relevant issue).\n. >  I honestly couldn't care less about an ancient Node.js version on Windows\nI am perfectly fine leaving it at that.\n\nbut process.on('exit') won't work correctly ... \n\nI don't think process.on('exit') is being used anywhere in our code, we are using signal-exit which does things a bit different (not sure how it would be affected without testing).\n. I'm getting close to turning this into some cool chaining functionality.\nonly.serial(...)\nserial.only(...)\nskip.serial(...)\nafterEach.skip(...)\n...\netc. \nI need sleep now though. More tomorrow.\n. OK - saw my error - chaining should now work.\nThis needs some cleanup (see the various TODO's), documentation, and more test coverage.\nI should be able to wrap it up this weekend.\nWhen it comes time to make nested groups, we will continue along the same pattern here, but replacing the base _addTest with one that merges the group settings as well (so you can skip an entire group, or set serial as the default for a group for example).\nI also need to add the inverse of most of the boolean flags. (i.e. concurrent = {serial: false}). That would be used for making a few tests concurrent in a group of tests where the default is serial.\nIn all, I think doing it this way is going to open up some really cool stuff.\nOK - bedtime for real now.\n. I am starting to think all these flags belong on a metadata object, i.e.:\ntest.skipped => test.metadata.skipped\ntest.exclusive => test.metadata.exclusive\nOtherwise we need to always be careful the metadata properties never share a name with a useful property. (For example we can not use test.skip as the metadata property, because test.skip() is the method that sets that flag).\n. > Otherwise we need to always be careful the metadata properties never share a name with a useful property\nUseful properties are going to be hard to predict as well, especially once we enable custom assertions.\n. It's for beforeEach / afterEach. Which need to be created multiple times. It's just storing the constructor args for later invoke (multiple times)\n. :+1: \nThis is the problem I was trying to communicate with #199, and I think this change provides a great solution.\n\nI'd rather have it not exist to make sure I didn't make a mistake.\n\nLet's take that further:\njs\nObject.defineProperty(test, 'end', {\n  get: function () {\n    throw new Error(\"t.end is not supported in this context. To use t.end as a callback, you must explicitly declare the test asynchronous via `test.async(testName, fn)` \")\n  }\n);\n. In the case of an a function that has not been explicitly declared async, what becomes of t.plan()?\nI thoroughly dislike the use of t.plan as a shortcut for automatically ending tests, but a lot of people are used to that.\nIf we have an implied async function, I say t.plan is only used as another form of assertion (\"this is how many assertions we had\"). We do not shortcut the end of the test until the generator/promise/observable/whatever is resolved.\nIf we have a declared async function, my preference would be to behave the same (just another assertion validation how many assertions we had). However, I think there is room for debate given that declared async tests closely follow existing frameworks like tap, so mimicking t.plan behavior might be the least surprising thing to do.\nI have made up some terms here:\n- declared async => \ntest.async('title', fn)\n- implied async => \ntest('tiltle', generatorFn) / test('title', promiseFn) / test('title', async t => {})\n. @Qix- \nI never said plan was useless. Just that I do not like using it as a means of auto ending your test.\nIt certainly has value for all the things you just described. With auto-ending, you can do this:\nt.plan(3);\nfor (var i = 0; i < 3; i++) {\n  t.pass();\n}\nWith my proposed modification to plan, you would be required to do this:\njs\nt.plan(3);\nfor (var i = 0; i < 3; i++) {\n  t.pass();\n}\nt.end(); // this is no longer assumed for you, just because you made it to 3 assertions.\nThe issue that arises is this:\njs\nt.plan(3);\nfor (var i = 0; i < 3; i++) { // oops - I'm only expecting this to execute twice\n  t.pass();\n}\nsetTimeout(() => t.end(), bigNumber); // I expect this to be my third and final assertion\nIn this case, we may \"auto-end\" the test because you reached your plan and kill the process before your assertion inside the setTimeout ever fires. This produces a false pass for you - and some really painful debugging when you place confidence in what you think this test is telling you and search elsewhere. \n. OK,\nI took an initial stab at this in #253. \nIt depends on PR #243, which creates a pretty huge diff - so it is best to focus on the commit where I actually implement this proposal.\n. OK, next question:\nWhat should happen if they are using \"declared async\" (test.async), and they return a Promise?\nThe current behavior is to auto-end with the Promise, but it seems to me that once you have declared test.async, you have bought into the legacy API, and should be required to call t.end() at that point.\nConsider this scenario that might arise when you are forced to deal with a mix of both promisified and nodeback API's:\n``` js\ntest.async('mixed api example', async t => {\n  var a = await promiseReturningFuncA();\n  var b = await promiseReturningFuncB();\n// the async function will fall through and return a resolved Promise\n  // possibly before this callback is done.\n  callbackFunction(a, b, (err, result) => {\n    t.ifError(err);\n    t.is(result, ...);\n    t.end();\n  });\n});\n```\nI am aware that this contrived example is easy to promisify. But what if it wasn't?\n. OK, so when using callbacks, auto-ending with plan does have a use. Coordinating ending after two or more callbacks who will be called in indeterminate order:\n``` js\nt.plan(2);\nsetTimeout(function () {\n  t.pass();\n}, randomTime());\nsetTimeout(function () {\n  t.pass();\n}, randomTime());\n```\nThat is pretty niche though. So what about an opt-in (or opt-out) auto-ending behavior:\njs\nt.plan(2, true); // automatically end after two tests\n. OK, so when using callbacks, auto-ending with plan does have a use. Coordinating ending after two or more callbacks who will be called in indeterminate order:\n``` js\nt.plan(2);\nsetTimeout(function () {\n  t.pass();\n}, randomTime());\nsetTimeout(function () {\n  t.pass();\n}, randomTime());\n```\nThat is pretty niche though. So what about an opt-in (or opt-out) auto-ending behavior:\njs\nt.plan(2, true); // automatically end after two tests\n. Good call!\nString literal sounds less verbose, and I guess I don't see needing to shove a lot of options in there.\njs\nt.plan(2, 'auto-end');\nOptionally, the plan function could return a chainable object:\njs\nt.plan(2).autoEnd();\n. Good call!\nString literal sounds less verbose, and I guess I don't see needing to shove a lot of options in there.\njs\nt.plan(2, 'auto-end');\nOptionally, the plan function could return a chainable object:\njs\nt.plan(2).autoEnd();\n. > @jamestalmage Any way we could show the stack of the uncaught exception?\nFixed in #257. This was another Windows specific issue. Fun, fun, fun!\n. Because eventually I would like to see it just be npm install -g ava, and because that has not been discussed/approved by the rest of the AVA team.\nDid it solve your problem?\n. Fixed in #277 \n. @sparty02 - as of 0.6.1 we no longer have a git dependency.\n. :+1: \n. @sparty02 - I think we are ready to merge this once you incorporate @BarryThePenguin's advice. \n. @sparty02 - I think we are ready to merge this once you incorporate @BarryThePenguin's advice. \n. Duplicate of #255, Closed by #257\n. Yep - I asked that before you added me. \n. > All AVA really needs to do is include the file in question and ignore it if it doesn't import the test function.\nI'd prefer to have it throw an error. We fork for each of those and that is expensive. We automatically ignore files with a _ prefix, so use that for test helpers.\n\nNested files, to me, should have their file prefixes (in my opinion). Right now it's flat. Not sure if this is because I'm specifying files on the command line, or what.\n\nI'm not sure what you mean. Their directory as part of the prefix? Right now we only prefix with the filename (and just the filename) if there is more than one test file. I think it makes sense to prefix directories (find the common base directory of all test files and prefix from there).\n. Duplicate of #223.\nPR for Fix: #254\n. This becomes easier to solve with a helpful Error message with the attached metadata from #243 . \n``` js\nfunction checkContextAccess(test) {\n    if (test.metadata.type === 'before' || test.metadata.type === 'after') {\n      throw new Error('test.context access is not allowed in before/after hooks');\n    }\n}\nObject.define(Test.prototype, 'context', {\n  get: function () {\n    checkContextAccess(this);\n    return this._context;\n  }, \n  set: function (newContext) {\n    checkContextAccess(this);\n    this._context = context;\n  }\n});\n```\n. We would not need to do that. We would create a different object, and put methods on it that were bound to the test instance test:\njs\nTest.prototype._createPublicAPI = function () {\n  var api = {};\n  var self = this;\n  listOfPublicMethods.forEach(function (methodName) {\n    api[methodName] = self[methodName].bind(self);\n  });\n};\nOr we could use delegates.\nThe problem with non-enumerable is that they can still be clobbered. I am working on #25 (registering custom assertion methods) - and we don't want to have to force them to avoid our internals as they name their method.\n. Overall, I like it. \nThere are a few places where it creates some weirdness. All revolve around our async assertions (t.throws(rejectedPromise), t.throws(rejectedObservable)).\n``` js\ntest('this does not work', t => {\n  var promise = Promise.reject(new Error('foo'));\n// this assertion actually happens asynchronously\n  t.throws(promise);\n  // we do not return a promise, so AVA assumes this is a synchronous function\n});\n```\nThere are a number of easy ways around this this. The issue will be preventing user confusion.\n(Ordered from best to worst workaround IMHO)\n``` js\ntest('use async/await', async t => {\n  var promise = Promise.reject(new Error('foo'));\nawait t.throws(promise); \n});\n```\n``` js\ntest('return the assertion promise', t => {\n  var promise = Promise.reject(new Error('foo'));\n// t.throws returns a promise, so return it from our test function.\n  return t.throws(promise);\n});\n```\n``` js\ntest.async('legacy async with a plan', t => {\n  t.plan(1);\n  var promise = Promise.reject(new Error('foo'));\n// test is auto-ended once this resolves and we reach our plan count.\n  t.throws(promise);\n});\n```\nI like this last one the least. As previously stated, I think allowing t.plan() to auto-end tests is a bad choice that causes confusing false positives. Since we are biting the bullet with a major breaking change here, I would love to see us correct that as well.\n. This is ready for review.\n// @sindresorhus @vdemedes \nI have updated the docs to reflect the new behavior, with a few language tweaks along the way. \n. Today is a major holiday in the US, so I won't be wrapping this up untill tomorrow.\n. Ahhh, Windows!\n@sindresorhus - I'll start looking into it here, but can you just retrigger the build? I was watching that one, and it was running REALLY slow. I think something was wrong on that VM.\n. OK, CI passed.\nAre we good here? I think all concerns are addressed.\n. Because we use require-from-string, our source maps are never written to disk. This requires the implementation of the retrieveSourceMap callback. Final solutions for #189 and #177 will need to expand on this as well.\nI do not know how expensive it is to load up source-map-support in each forked process like this.\nSince all errors are serialized and transmitted over IPC before logging, It may be possible to defer the stack-trace transform until it has been transmitted back to the main thread. \n. ~~This needs #255 before it will pass on Windows. The Travis failure looks like a fluke.~~\nRebased onto master which contains referenced fix.\n. > When you reference tickets in the the title, can you do so in the description too? It makes it easier to click through. The issue references in the title aren't clickable.\nErrr...\n\nJames Talmage commented 6 hours ago ...\n. OK, I just followed up on a number of other PR's\n. Working on that and the documentation.\n. OK, I think that covers it.\n. You are on the right track. You need to expand on this using my comments from above.\n\nYou should also try to create a test or two. This will likely involve adding some files to the test\\fixture folder. Look at some of the tests in test/cli.js for inspiration, and get creative from there.\nA good way to verify you have written an effective test is to get rid of your changes to the production code (keep your tests in place). Your new tests should fail without your changes (otherwise what was the point?!), then add back in your changes and verify your changes to the production code take you from failing tests to passing tests.\nDone this way, everybody can understand the need for the new code you propose, and your tests become a way of documenting the importance of your PR.\n. :+1: \n. :+1: \n. tap/tape shows you every assertion failure.\nUsually, yes - you only need to see the first assertion. Though sometimes additional information can be helpful:\n``` js\nvar a = sum(2, 2); // => 4 (correct)\nvar b = sum(4, 4); // => 7 (wrong - for some weird reason)\nvar c = sum(a, b); // => 11 (correct for the wrong input).\nt.is(c, 12); // this will fail (telling us there is something wrong - but not what the root of the problem is)\nt.is(b, 8);  // this will also fail and in this case is the more helpful information\n```\nGranted, the solution above is pretty obvious - reorder the assertions. But that is not so easy to see in every problem.\n. tap/tape shows you every assertion failure.\nUsually, yes - you only need to see the first assertion. Though sometimes additional information can be helpful:\n``` js\nvar a = sum(2, 2); // => 4 (correct)\nvar b = sum(4, 4); // => 7 (wrong - for some weird reason)\nvar c = sum(a, b); // => 11 (correct for the wrong input).\nt.is(c, 12); // this will fail (telling us there is something wrong - but not what the root of the problem is)\nt.is(b, 8);  // this will also fail and in this case is the more helpful information\n```\nGranted, the solution above is pretty obvious - reorder the assertions. But that is not so easy to see in every problem.\n. Here is an example where I wish I could have seen every failure: \n  https://github.com/jamestalmage/babel-plugin-detective/blob/master/test/babel-6-test.js#L165\nI could not use deepEquals, and using assert which throws at the first error was annoying because you can not know ahead of time which failure will highlight your mistake. \n. Here is an example where I wish I could have seen every failure: \n  https://github.com/jamestalmage/babel-plugin-detective/blob/master/test/babel-6-test.js#L165\nI could not use deepEquals, and using assert which throws at the first error was annoying because you can not know ahead of time which failure will highlight your mistake. \n. > And it will for sure confuse users, when tests continue to run after failed assertions\nWe currently do continue test execution, and users aren't confused. We are already collecting every assertion result, so not doing anything with them is just wasteful. If we decide not to do this, then our assertions should throw Errors.\n\nHow does tap/tape handle these scenarios?\n\nThis:\n``` js\nvar tap = require('tap');\ntap.test('foo', function (t) {\n  t.is(1, 2, 'failing');\n  t.is(1, 1, 'passing');\n  t.end();\n});\n```\nProduces: \n```\ntest.js\n  foo\n    1) failing\n    \u2713 passing\n1 passing (296.424ms)\n  1 failing\n```\nAs @novemberborn brought up. There are times when a failed assertion certainly means the test will blow up: \njs\nt.ok(foo);     // if this assertion fails\nt.ok(foo.bar); // then this will blow up\nIf a test throws an error after a failed assertion, I say we assume this exact scenario has occurred. Suppress the thrown error and just report all the assertions leading up to it.\n\nI don't see a point in continuing test execution\n\nIt would probably be helpful for tests like this. Having a list of what passed and what failed might be helpful in hunting down your mistake. You could argue that each of those assertions would be better written as separate tests, but the current way is far more convenient.\n. > To clarify, right now we also report the test blowing up?\nNo. Right now we report just the first failed assertions. Anything that happens after that is ignored. It behaves almost like we threw an error to stop execution... but we didn't.\n. Hey @novemberborn,\nCan we also make sure we are covering the case where the test file itself may have a source map?\nThis will require making Babel consume incoming source maps. Looking at the Babel 5 options, it seems there is an inputSourceMap option. So it is probably just a matter of retrieving it via sourceMapSupport.retrieveSourceMap(...)\n. yep. It's probably a niche case, but it just makes our source-map support complete. If we ever implement alternate compilers, it will be good to have, since we will likely always apply the power-assert transforms.\n. yep. It's probably a niche case, but it just makes our source-map support complete. If we ever implement alternate compilers, it will be good to have, since we will likely always apply the power-assert transforms.\n. So it looks like the last time this passed, was on ava@0.3.0. \nAt that point we were using babel-core/register, which loads polyfills.\nSo maybe the runtime plugin has never handled this.\n. Couldn't you transform it to something like this:\n``` js\nvar x = arr.includes('foo');\n// =>\nvar x = includesHelper(arr, 'name');\nfunction includesHelper(maybeArray, ...args) {\n  if (Array.isArray(maybeArray) && !maybeArray.includes) {\n    // use polyfill\n  } else {\n   return maybeArray.includes.apply(maybeArray, args);\n  }\n}\n```\nObviously there is a performance hit for this way (especially when expanded to accommodate String.includes) . Is that the only reason it is considered unacceptable, or are there other reasons?\n. Couldn't you transform it to something like this:\n``` js\nvar x = arr.includes('foo');\n// =>\nvar x = includesHelper(arr, 'name');\nfunction includesHelper(maybeArray, ...args) {\n  if (Array.isArray(maybeArray) && !maybeArray.includes) {\n    // use polyfill\n  } else {\n   return maybeArray.includes.apply(maybeArray, args);\n  }\n}\n```\nObviously there is a performance hit for this way (especially when expanded to accommodate String.includes) . Is that the only reason it is considered unacceptable, or are there other reasons?\n. > If you're going to include the code to polyfill it, why not just polyfill it?\nAVA is trying to provide as much ES2015+ support inside tests, while not polluting the production environment at all.  ES2015 convenience in the tests, while still allowing you to write really tiny, dependency-less modules that do not require a build step. I thought the goals of the optional runtime plugin were similar.\n. > If you're going to include the code to polyfill it, why not just polyfill it?\nAVA is trying to provide as much ES2015+ support inside tests, while not polluting the production environment at all.  ES2015 convenience in the tests, while still allowing you to write really tiny, dependency-less modules that do not require a build step. I thought the goals of the optional runtime plugin were similar.\n. Interesting. Looking back at the 0.10 docs, it seems there is some historical flakiness. It would probably be pretty painful.\nThe list of prototype polyfills is not that long. A plugin might be easier.\n@thejameskyle - Does the runtime plugin provide non polluting polyfills for static methods? Array.of, String.fromCodePoint, etc?\n. Interesting. Looking back at the 0.10 docs, it seems there is some historical flakiness. It would probably be pretty painful.\nThe list of prototype polyfills is not that long. A plugin might be easier.\n@thejameskyle - Does the runtime plugin provide non polluting polyfills for static methods? Array.of, String.fromCodePoint, etc?\n.  That's what I thought. So, if I were to write a plugin to fill in the gaps for AVA, I would only need to tackle instance methods. transform-runtime  handles globals (Promise, Map, etc) and static instance methods (Array.of, etc).\n. Related: http://semver.org/\n\n4 - Major version zero (0.y.z) is for initial development. Anything may change at any time. The public API should not be considered stable.  \n...  \nFAQ\nHow should I deal with revisions in the 0.y.z initial development phase?\nThe simplest thing to do is start your initial development release at 0.1.0 and then increment the minor version for each subsequent release.\n\n\nUse Caret Ranges in package.json when it comes to pre-1.0.0 projects.\n(I personally use caret ranges just about everywhere)\n\nCaret Ranges ^1.2.3 ^0.2.5 ^0.0.4\nAllows changes that do not modify the left-most non-zero digit in the [major, minor, patch] tuple. In other words, this allows patch and minor updates for versions 1.0.0 and above, patch updates for versions 0.X >= 0.1.0, and no updates for versions 0.0.X.\nMany authors treat a 0.x version as if the x were the major \"breaking-change\" indicator.\n. @dralletje \nI think we answered your question. If not, please reopen this.\n. Agreed.\n\nI think the best solution is to stop rejecting the promise in that case. Rejection has the side affect of bailing out of Promise.all early, so other test files may not finish. We should resolve with an failure result, that will simply be summarized in the output. We should save rejection for unexpected/unrecoverable problems.\n. @sindresorhus - made the change.\n. The only hole I still see in this is someone using bluebird setScheduler and mucking things up.\n. I don't think it's likely to be an issue until we actually start supporting the Browser. That angular example is literally the only way I've ever seen setScheduler used.\n. :+1:\n. :+1: \n\nNote the second commit which is unrelated to input source map support but removes an unnecessary stringification of the transpiled source map\n\nBah! I meant to fix that. Thanks @novemberborn.\n. Closing since this is not an AVA issue, but a power-assert one.\nI opened https://github.com/power-assert-js/power-assert/issues/34 to track this issue there.\n. @sindresorhus - this seems like a no brainer to me. What we were doing before was essentially piping to stdout. This just uses the child_process api to do the same thing with less code.\nI'm gonna merge unless you object.\n. Or that! :smile: \n. I am not sure I understand how to use the debug module to accomplish that.\nThis way time-require is not setting it's require-hook unless the flag is set.\n. Got it. That works.\nHow should I handle --sorted vs not? It is nice to have both, but not required.\nIf I have to pick one, then I guess I'd prefer the default sequential sorting (easier to figure out which module is requiring acorn that way). \n. I just checked out both of those. They are a lot more verbose and don't provide the sort option. time-require provides nicer output, especially once #275 gets merged.\n. I think AppVeyor ate too much turkey today and is feeling sluggish.\n. The second commit wraps the entire CLI in an IIFE, to avoid a ParseError from xo.\nhttps://github.com/eslint/eslint/issues/1158\n. If https://github.com/sindresorhus/xo/issues/51 were to be implemented, the second commit would not be necessary.\n. This needs a test suite, which is doable but will involve a number of fixtures. I would like agreement on my general approach before completing.\nRight now, it outputs the following:\n- Run locally, or globally without a local version: no warning issued\n- Run globally, with a local version:\nUsing local install of AVA (v0.6.1), which differs from global (v0.6.0)\nor \nUsing local install of AVA (v0.6.0), which matches global (v0.6.0)\n. > Instead of wrapping it in an IIFE, just remove XO for now. We can add it back when we've sorted this out.\nThis is what led me to the shim behavior in fallback-cli, it allows you to do this:\njs\n// cli-shim.js\nrequire('fallback-cli')('ava/cli.js', function (opts) {\n  // returns are allowed inside the callback - linters stay happy.\n  if (opts.globalCli && opts.localCli) {\n    console.warn('Using the local install of AVA instead of global.');\n  }\n  require(opts.cli); // opts.cli === opts.localCli || opts.globalCli\n});\n. how about just formally exposing assertCount in the test object.\n. I think you're right. Closing.\n. I think this should go in the base directory:\njs\nrequire('ava/api');\n// is better than\nrequire('ava/lib/api');\n. @vdemedes - see my change. \nThe issue you were having was stats never gets emitted if there is syntax error in the test file (that is why it started failing on the babel-hook fixture, which intentionally has ES2015 syntax in a file that will not be instrumented by babel).\n. I propose we change this up just a bit.\nRather than api tracking counts for unhandledRejection, uncaughtException, etc.\nUse the array: api.errors, and every time one is encountered, push to it:\n``` js\non('unhandledRejection', function(e) {\n  self.errors.push({type: 'unhandledRejection', error: e});\n});\non('uncaughtException', function(e) {\n  self.errors.push({type: 'uncaughtException', error: e});\n});\n// ...\nself.errors.push({\n  type: 'nonZeroExit',\n  error: new Error('process exited with code: ' + code)\n});\n// ...\nself.errors.push({\n  type: 'assertionError',\n  // ...\n});\n```\nThen extend API with methods like:\njs\napi.hasErrors(); // true / false\napi.getErrors(); // returns all {type: typeName, error: errorObj}\napi.getErrors(typeName); // returns all errors of matching type.\nI think the promise returned by the API should almost always resolve, and almost never reject. Users will then use hasErrors, and decide what to do. This makes unhandledRejection, uncaughtException, and nonZeroExit consistent with what we do with a failed assertion (we resolve the promise with a result that indicates there were errors).\nWe should continue emitting events so people can implement their own realtime logging, but it should be possible to create basically the same log output after the fact using just the array returned from  getErrors().\nThoughts?\n. I've shared a number of thoughts, but overall, very much :+1: \n. >  use API in tests instead of child_process.exec\nI'm not sure of this one. I like that test/cli.js exercises the full stack. It currently acts as more of an integration test.\nWe might be able to move the majority of them to test/api.js, but some should definitely stay behind. Enough to fully exercise cli.js.\n. Sure no - problem. Sorry for messing you up.\n. I would wait for the release. We already have one major refactor in this release. I think that we let major changes sit on master for at least a few days before release.\n. I need to step up my GIF game.\n. Nope, missed a couple string constants. Everything should be good now.\n. This was an unnecessary change. You needed to npm install to get latest require-from \n. Yep, we are working on it. \nClosing as a duplicate of #117 \n. It seems like you have found a good solution to your problem. I don't think it is a good idea for us to start aliasing methods. Eventually we will introduce the ability to fully integrate custom assertions. When that happens, you will be able to modify our default assert plugin with the aliases you desire and apply it once across your entire project.\n. @alubbe \nAVA supports generators out of the box. No need to wrap with Promise.coroutine.\nTest:\n``` coffee\ntest = require \"ava\"\ndelay = require \"delay\"\ntest \"peter\", (t) ->\n  yield delay 500\n```\nOutput:\n```\n  \u2714 peter (504ms)\n1 test passed\n``\n. To me, this looks like a problem either withcoffee-scriptssource-map output, or something in source-map support. Either way - it's triggered by callingbluebird.coroutine` on a non generator function - which is what happens, because AVA transforms generators for you automatically.\n. @alubbe \n\nIs this a pre-runtime transformation by babel?\n\nYes.\nThe following works fine (needed the http://, prefix):\n``` coffee\ntest = require \"ava\"\nPromise = require \"bluebird\"\nrequestAsync = Promise.promisify require \"request\"\ntest \"peter\", (t) ->\n  yield requestAsync url: \"http://www.google.com\"\n```\nTurning sourcemaps off gave a cleaner error message with source-maps off.\nMight be related:\n https://github.com/webpack/webpack/issues/1071\nAlso, googling Error: No element indexed by 1, brings up a ton of issues - all having to do with source-maps.\n@novemberborn - Do you have any insights here?\n. @vdemedes - These are all coffee-script examples. You don't use * in coffee-script, generator functions are implied by use of the yield keyword.\n. Also, in these the Error: No element indexed by 1 error is always masking another error. In the second example it was improper use of the request API. The problem is that, instead of exposing the actual underlying error, source-map-support is choking on the provided source-maps when it tries to beautify the thrown error and throwing a different one entirely.\nThis issue (choking on source-maps, and masking error messages that might be helpful), are really what bothers me. At a minimum source-map-support should append the original stack trace to it's own if it fails to transform it.\n. https://github.com/evanw/node-source-map-support/pull/117\n. https://github.com/evanw/node-source-map-support/pull/117\n. > The compiled coffee-script code means that there is no async/await for AVA to compile. It just sees a test function which returns a promise. So that's not the issue.\nThis is a generator/yield example, and coffee-script outputs an actual generator function, (i.e. function *() {...}). async/await support is not in coffee-script yet. (I take your point though - the problem is a bad source map).\n\nTurns out Bluebird throws an error when it's async module is loaded. This then causes a crash from source-map-support.\n\nThat is wrapped in a try/catch. Any crash of source-map-support would be caught.\n\nNote that sources now points at test2.coffee rather than test2.js and that sourcesContent now has two entries, the first being null.\n\nThat certainly seems like a bug. Can you submit an issue on the Babel tracker? (or hunt down an existing one and link it here).\n\nMaybe it's already fixed in Babel 6.\n\nHopefully. The issue blocking our move to Babel 6 appears to finally be getting some traction. There is even a PR to fix it.\n. > The compiled coffee-script code means that there is no async/await for AVA to compile. It just sees a test function which returns a promise. So that's not the issue.\nThis is a generator/yield example, and coffee-script outputs an actual generator function, (i.e. function *() {...}). async/await support is not in coffee-script yet. (I take your point though - the problem is a bad source map).\n\nTurns out Bluebird throws an error when it's async module is loaded. This then causes a crash from source-map-support.\n\nThat is wrapped in a try/catch. Any crash of source-map-support would be caught.\n\nNote that sources now points at test2.coffee rather than test2.js and that sourcesContent now has two entries, the first being null.\n\nThat certainly seems like a bug. Can you submit an issue on the Babel tracker? (or hunt down an existing one and link it here).\n\nMaybe it's already fixed in Babel 6.\n\nHopefully. The issue blocking our move to Babel 6 appears to finally be getting some traction. There is even a PR to fix it.\n. > I'm thoroughly confused regarding their code base and newfangled issue tracker,\nWhy do you think I punted this to you? :trollface: \nSeriously though, the issue tracker is pretty straightforward once you dig in. As for the codebase, I feel your pain. I would recommend browsing a clean clone (don't run tests in the clean clone - it generates lots of additional sources that are just confusing until you get a basic grasp of the layout and build system).\n. > I'm thoroughly confused regarding their code base and newfangled issue tracker,\nWhy do you think I punted this to you? :trollface: \nSeriously though, the issue tracker is pretty straightforward once you dig in. As for the codebase, I feel your pain. I would recommend browsing a clean clone (don't run tests in the clean clone - it generates lots of additional sources that are just confusing until you get a basic grasp of the layout and build system).\n. I certainly think option 2 is the way to go. I can't really think of a scenario where it would cause a problem. If you want to test what is written to stdout you are likely forking inside your test, or mocking.\n. I certainly think option 2 is the way to go. I can't really think of a scenario where it would cause a problem. If you want to test what is written to stdout you are likely forking inside your test, or mocking.\n. > Rather than depending on coffeescript is it more illustrative to code up a simple .txt \"compiler\"?\nSure - I will probably steel from whatever simplified transforms you end up adding to the nyc test suite. :stuck_out_tongue_winking_eye: \n\nThis would work though wouldn't it? \n\nI'm not sure, but I really doubt it. The issue blocking our Babel 6 adoption is actually related to Babel running a particular transform twice. (It transforms all typeof statements, but the generated code contains a typeof statement, which gets transformed again).\nPerhaps (with Babel 6), we could fallback to only running the power-assert transform, instead of the full es2015 preset.\n. > Rather than depending on coffeescript is it more illustrative to code up a simple .txt \"compiler\"?\nSure - I will probably steel from whatever simplified transforms you end up adding to the nyc test suite. :stuck_out_tongue_winking_eye: \n\nThis would work though wouldn't it? \n\nI'm not sure, but I really doubt it. The issue blocking our Babel 6 adoption is actually related to Babel running a particular transform twice. (It transforms all typeof statements, but the generated code contains a typeof statement, which gets transformed again).\nPerhaps (with Babel 6), we could fallback to only running the power-assert transform, instead of the full es2015 preset.\n. The more I think about this, we really can't run the power-assert transform after regenerator or asyncToGenerator transforms. power-assert needs to see the original AST - it uses it to pass a copy of the source to it's renderers.\njs\nt.is(await foo, await bar);\nbecomes (very approximately):\njs\nt.is(t._expr(t._capt(await foo), {source: 't.is(await foo, await bar'}), ...\napplying after the asyncToGenerator transform, the source passed to power-assert would end up scrambled.\njs\nt.is(ctx.$0$0, ctx.$0$1)\nI am beginning to wonder if it would not be best to just adopt a \"take it or leave it\" approach to the transform setup we provide. If users want to provide their own transform they need to provide a complete one, and we just don't touch it. We can provide documentation (and possibly convenience methods) for including the power-assert transform.\nAt a minimum, I think we need to provide a way for them to tell us to skip our transform via a flag in the CLI.\n// @sindresorhus @vdemedes \n. The more I think about this, we really can't run the power-assert transform after regenerator or asyncToGenerator transforms. power-assert needs to see the original AST - it uses it to pass a copy of the source to it's renderers.\njs\nt.is(await foo, await bar);\nbecomes (very approximately):\njs\nt.is(t._expr(t._capt(await foo), {source: 't.is(await foo, await bar'}), ...\napplying after the asyncToGenerator transform, the source passed to power-assert would end up scrambled.\njs\nt.is(ctx.$0$0, ctx.$0$1)\nI am beginning to wonder if it would not be best to just adopt a \"take it or leave it\" approach to the transform setup we provide. If users want to provide their own transform they need to provide a complete one, and we just don't touch it. We can provide documentation (and possibly convenience methods) for including the power-assert transform.\nAt a minimum, I think we need to provide a way for them to tell us to skip our transform via a flag in the CLI.\n// @sindresorhus @vdemedes \n. @jkimbo \nYou have two options.\nPlace the process.nextTick patch in before and after. These only get run once - so they shouldn't clobber each other. \nIf your mock needs to maintain context, use t.context to pass information between beforeEach and the actual test (don't store it as a variable outside the functions). Note that t.context is not available in before/after, only the Each variants.\nUse t.serial for your tests that require these mocks. If you have any async IO in your tests (i.e. disk/network requests), then you will end up sacrificing some speed. Otherwise, it should be just as fast.\n. @jkimbo \nYou have two options.\nPlace the process.nextTick patch in before and after. These only get run once - so they shouldn't clobber each other. \nIf your mock needs to maintain context, use t.context to pass information between beforeEach and the actual test (don't store it as a variable outside the functions). Note that t.context is not available in before/after, only the Each variants.\nUse t.serial for your tests that require these mocks. If you have any async IO in your tests (i.e. disk/network requests), then you will end up sacrificing some speed. Otherwise, it should be just as fast.\n. @vdemedes beat me by six seconds!\nWith a much better point regarding using async/await to boot.\n. @vdemedes beat me by six seconds!\nWith a much better point regarding using async/await to boot.\n. @jkimbo - I don't know a lot about React, so I may be missing something.\nWhy can't you just do:\n``` js\ndeferred.resolve({...});\nawait deferred.promise;\nassert.equal(stub.state.searching, false);\n```\nIf that returns too quickly (i.e. await returns before state has settled), you could do:\njs\nimport delay from 'delay';\n// ...\nawait deferred.promise.then(delay(10)); // or delay(0), or delay(100), etc.\n. @jkimbo - I don't know a lot about React, so I may be missing something.\nWhy can't you just do:\n``` js\ndeferred.resolve({...});\nawait deferred.promise;\nassert.equal(stub.state.searching, false);\n```\nIf that returns too quickly (i.e. await returns before state has settled), you could do:\njs\nimport delay from 'delay';\n// ...\nawait deferred.promise.then(delay(10)); // or delay(0), or delay(100), etc.\n. > Do I need to confirm (with tests) that the ava --require moduleId works?\nThat would be my approach, does not need to be fancy.\n``` js\n// test/fixture/install-global.js\nglobal.foo = 'bar'\n```\n``` js\n// test/fixture/validate-installed-global.js\nimport test from '../../'\ntest(t => t.is(global.foo, 'bar'));\n```\nThen use the API to call validate-... using the require hook you've just implemented.\nOverall looks pretty good.\n. I don't know if our Babel hook should be disabled automatically. I still have not decided the best way to make it all work.\nMy current thinking is using something along the lines of #297, but using a better version of istanbul-lib-hook. https://github.com/jamestalmage/transform-chain would be part of it. Combine transform-chain with a way of detecting when hooks are installed, similar to how @novemberborn did it here in nyc.\n. I don't know if our Babel hook should be disabled automatically. I still have not decided the best way to make it all work.\nMy current thinking is using something along the lines of #297, but using a better version of istanbul-lib-hook. https://github.com/jamestalmage/transform-chain would be part of it. Combine transform-chain with a way of detecting when hooks are installed, similar to how @novemberborn did it here in nyc.\n. >  Unless I somehow accomplished this accidentally, I did not attempt to disable ava's babel hook\nUnderstood, we are still deciding if / when / how to do that. Once a decision on that gets made we will merge this, and augment it with whatever behavior we finally decide on. \n\nI'm sorry for not being very responsive.\n\nThat is not a problem. We still have to answer a few questions, so we are in no rush to merge.\n. Well, I'm having a hard time getting this to work on travis. Test #4 seems to work on OSX. Note that the --require flag does not work on Node <4\n. @sindresorhus @vdemedes - either of you develop on linux? Any advice on how he should fix this?\n. I don't know. Maybe it would just be better to manually implement the same semantics as the --require flag ourselves. You could just do require(somePath), for every --require== flag inside babel.js. Might end up being more reliable.\n. > Isn't this is a huge perf loss (due to all of the Babel spin-ups, \nBabel does not take long to spin up. It does take a good long while to require the first time. After that babel is in the cache, whether you register again or not. A call to require('babel-core/register') would result in a small eval of only a few additional files. Not worth worrying about.\n\nand the loss of its internal module caching)?\n\nNot sure what you mean here. Node caches modules for you already. babel-core/register shouldn't be going and deleting things from your require cache. (if it does, I'm surprised).\n. > Isn't this is a huge perf loss (due to all of the Babel spin-ups, \nBabel does not take long to spin up. It does take a good long while to require the first time. After that babel is in the cache, whether you register again or not. A call to require('babel-core/register') would result in a small eval of only a few additional files. Not worth worrying about.\n\nand the loss of its internal module caching)?\n\nNot sure what you mean here. Node caches modules for you already. babel-core/register shouldn't be going and deleting things from your require cache. (if it does, I'm surprised).\n. Yes. It is definitely Babel related, but also architectural in some regard. We spawn a whole new process for every test file. Each new spawned process currently loads up Babel to perform transforms in that individual process. Doing this for every process is redundant and is expensive. \nAlso, mocha is not inherently faster, it depends on what you are testing. If you have lots of small test files that execute really really quickly, then the expensive Babel load up times per process are likely what is killing your performance. If you have a bunch of slow tests (i.e. with network or disk access), spread across just a few files, then AVA will likely be far more performant, because you will be better utilizing your CPU's by running lots of tests concurrently across multiple forked processes.\nWe have plans to mitigate the Babel load up times by only loading it in the main thread, and having child threads defer to it for transformation. That is not an easy task, since require must happen synchronously, and inter process communication is async. I have a solution for that, but it will take some time for it to find it's way into AVA.\n. Yes. It is definitely Babel related, but also architectural in some regard. We spawn a whole new process for every test file. Each new spawned process currently loads up Babel to perform transforms in that individual process. Doing this for every process is redundant and is expensive. \nAlso, mocha is not inherently faster, it depends on what you are testing. If you have lots of small test files that execute really really quickly, then the expensive Babel load up times per process are likely what is killing your performance. If you have a bunch of slow tests (i.e. with network or disk access), spread across just a few files, then AVA will likely be far more performant, because you will be better utilizing your CPU's by running lots of tests concurrently across multiple forked processes.\nWe have plans to mitigate the Babel load up times by only loading it in the main thread, and having child threads defer to it for transformation. That is not an easy task, since require must happen synchronously, and inter process communication is async. I have a solution for that, but it will take some time for it to find it's way into AVA.\n. > Any idea why Mocha would be outperforming AVA by such a huge factor, in that case? Are there existing perf issues that you're aware of? (Sorry, not directly related to this PR)\n@billyjanitsch \nThere have been major performance improvements since this was discussed. If possible, could you give AVA a try again and tell us how we're doing?\n. > Our test suite is large enough (>100 files) ...\nWow! I would love to see a project that big converted. Is it open source? What is your current test setup? (mocha with chai for assertions, etc)\nYou may also want to wait for customizable assertions, so we can create one that matches your current assertion api exactly.\n. @billyjanitsch https://github.com/sindresorhus/ava/issues/424#issuecomment-172107059\n. @ariporad - yep, this was an exploratory thing I did when that whole discussion kicked off.\nI am going to be opposed to integrating pirates in any projects until https://github.com/ariporad/pirates/issues/5 is resolved.\n. That assertion will fail with just about any assertion library.  Using assert.deepStrictEqual (on which same is based).\n```\n  1 test failed\n\ncircular testing\n  RangeError: Maximum call stack size exceeded\n```\n\nSo, this is not a problem with our assertion per se, but that we are masking the true source of the error. I am pretty sure that this is actually related to power-assert.\n@twada - how should we handle this? A simple assertError instanceof assert.AssertionError would work for AVA, but not every assertion library extends assert.AssertionError\n. Ahh - you are right. I forgot we did that. Thanks @twada \n. #300 prevents the error masking.\n301 uses deeper to allow comparison of circular objects.\nSo, either way, the fix for this lands soon.\n. Needs a test so the regression doesn't happen again.\nOtherwise :+1: \n. They don't get copied in the rest param\n``` js\ntest(({context: foo, ...t}) => {\n  t.is(...); // throws \"t._expr is not a function\"\n});\n```\n. > I also don't understand why would anyone want destructuring.\nIt's available, people will use it. Anybody who hasn't hacked on power-assert is unlikely to guess that error message is related to their use of destructuring.\nHonestly, if #304 happened, I would likely never destructure t. Still, I think given the potential confusion, we should still guard against it. \n. > I also don't understand why would anyone want destructuring.\nIt's available, people will use it. Anybody who hasn't hacked on power-assert is unlikely to guess that error message is related to their use of destructuring.\nHonestly, if #304 happened, I would likely never destructure t. Still, I think given the potential confusion, we should still guard against it. \n. > Is there any power-assert issue I can follow about that?\nNot that I am aware of. I'll open one if I can't find it.\n. > Is there any power-assert issue I can follow about that?\nNot that I am aware of. I'll open one if I can't find it.\n. @mcmathja This looks good. Can you add a test or two covering this? It will need to go in cli.js (model your tests after those). To validate that it fixes #272 you will need to call two test fixtures in a single test (there are a couple examples of that in there). This is because we do not prefix the filename if you only run a single file.\nIf you add tests, I'm :+1: for merging.\n. AppVeyor console output is my nemesis.\n. FWIW, the implementation is super easy: #305 \n. So, coming from mocha, I used to be able to do this:\n``` js\nvar foo, spy;\nbeforeEach(() => {\n  foo = new Foo();\n  spy = sinon.spy();\n});\nit('foo.bar(a)', t => {\n  foo.bar('a', spy);\n  assert.spyCalledWith(spy, ...)\n});\nit('foo.bar(c)', t => {\n  foo.bar('a', spy);\n  assert.spyCalledWith(spy, ...)\n});\n```\nNow I am left repeating a bunch of boilerplate.\n\nThe latter actually has less characters\n\nOnly because you called the variable context, if you called it c, you would have saved a few.\n. So, coming from mocha, I used to be able to do this:\n``` js\nvar foo, spy;\nbeforeEach(() => {\n  foo = new Foo();\n  spy = sinon.spy();\n});\nit('foo.bar(a)', t => {\n  foo.bar('a', spy);\n  assert.spyCalledWith(spy, ...)\n});\nit('foo.bar(c)', t => {\n  foo.bar('a', spy);\n  assert.spyCalledWith(spy, ...)\n});\n```\nNow I am left repeating a bunch of boilerplate.\n\nThe latter actually has less characters\n\nOnly because you called the variable context, if you called it c, you would have saved a few.\n. I, like @novemberborn, use context a lot.\nHere is a crazy idea. What if we used some sort of AST transform to turn this:\n``` js\nvar x;\ntest.beforeEach(t => x = 'foo');\ntest('ends in \"oo\"', t => t.true(/oo$/.test(x)));\ntest('starts with \"f\"', t => t.true(/^f/.test(x)));\n```\nInto something that is async safe:\n``` js\nvar x = {};\ntest.beforeEach(t => x[t.uniqueId] = 'foo');\ntest('ends in \"oo\"', t => t.true(/oo$/.test(x[t.uniqueId])));\ntest('starts with \"f\"', t => t.true(/^f/.test(x[t.uniqueId])));\n```\n. > It is, indeed, crazy :D\nI know, I know.\n. It's a pretty crazy idea, I agree.\nThe fact of the matter is that moving from mocha to ava is a paradigm shift that takes some getting used to. \nI find that I'm using beforeEach less and less, and helper/init functions more. I still don't like how often my tests start with the same line:\njs\ntest(t => {\n  var foo = someInitFn();\n  // ...\n})\nSeems like a DRY violation.\n. @ariporad \nYour idea requires reading through the entirety of the test code to understand what arguments are available and how to access them, remember that a beforeEach can technically be written after the current test you are reading, so you need to scan down as well. If you ever reorder your beforeEach tests, you then need to refactor any test that depends on that ordering. People who objected to the original idea will be even more against this one.\nGoing back to the original idea (context as a second arg), you can get at what you are pursuing using destructuring.\n``` js\ntest.beforeEach(t => {\n  test.context.foo = something();\n  test.context.bar = somethingElse();\n});\ntest((t, {foo, bar}) => {\n  // now use foo and bar at will\n});\n```\nDestructuring is also useful currently, but it's not very DRY\njs\ntest(t => {\n  let {foo, bar} = t.context;\n});\n. sure.\n. sure.\n. Related: https://github.com/nodejs/node-v0.x-archive/issues/3452\n. It's impossible to fix this in a clean way in JS, the V8 related issues are here: \nhttps://code.google.com/p/v8/issues/detail?id=1914\nhttps://code.google.com/p/v8/issues/detail?id=1281\nThe only workaround I can see is to wrap Module.prototype._compile to temporarily disable our uncaughtException handler. Here is a quick prototype of what I am thinking:\n``` js\nprocess.on('uncaughtException', function (e) {\n  if (e.thrownInsideModuleCompile) {\n    throw e;\n  } else {\n    console.log('error swallowed this will not work');\n  }\n});\nvar Module = module.constructor;\nvar oldCompile = Module.prototype._compile;\nModule.prototype._compile = function (code, filename) {\n  try {\n    oldCompile.apply(this, arguments);\n  } catch (e) {\n    e.thrownInsideModuleCompile = true;\n    throw e;\n  }\n};\nvar requireFromString = require('require-from-string');\nvar badUserCode = 'function () {';\nrequireFromString(badUserCode, 'bar.js');\n```\nThis means any errors thrown synchronously when a module is required are just going to crash the whole test. This will be true even inside tests:\n``` js\nimport test from 'ava';\ntest('test A', t => t.pass());\ntest('test B', t => t.pass());\ntest('test C', t => require('./moduleThatThrowsSynchronously'));\n```\nThis change would also make AVA a less than ideal place to test any code that attempts to wrap / manipulate Module._compile. (It would have given me headaches while developing capture-require for instance).\n. sourcemap-support adds an unhandledException listener as well (disabling the helpful output). That can be fixed using the handleUncaughtExceptions option and setting it to false:\njs\nvar sourceMapSupport = require('source-map-support');\nsourceMapSupport.install({\n    handleUncaughtExceptions: false,\n        ...\nnot sure what additional problems that will cause.\n. OK, \nI think a much simpler solution would be to move this, down to here as well as setting the handleUncaughtExceptions flag I mentioned in the previous comment(set it here).\nThis is going to cause a few of our current api tests to fail.\nI can work on this later tonight unless someone else wants to take it up. @sotojuan - You are already familiar with those tests. You wanna give it a go? You could start with https://github.com/jamestalmage/ava/commit/b1986249b6b59efba05413be3a7b746c4f2bc27d (or just emulate what I have done there). I think it might mean moving the syntax error test back to cli territory.\n. How does signupDB throw? Does it do it synchronously? Then you need to wrap the call in a function: \njs\nt.throws(() => signupDB(...));\nDoes it return a promise that gets rejected? In that case, do not wrap it in a function, but do return the result of t.throws:\njs\ntest(t=> {\n  return t.throws(signupDB(...));\n});\n. If I recall correctly, you must supply a string for an === comparison. I think I noticed that in the source a few days ago and meant to add regexp matching to my todos. Thanks for the reminder.\n. Nope, I read the source wrong on that. Should support the same values for err that core-assert#throws does, with the added option of passing in a string for an === comparison\n. Just to clarify, there is a bit of a new idea here. That is to track the promises passed to t.throws() and t.doesNotThrow, and avoid the requirement to do explicitly return t.throws\n. :+1: from me.\n. > Sure. Not sure why it's failing though ~_~.\nThe CI builds are failing because you need to rebase.\n. > Sure. Not sure why it's failing though ~_~.\nThe CI builds are failing because you need to rebase.\n. ``` js\nimport test from 'ava';\ntest(t => t.fail())\n```\n. > Ah so it's an internal thing.\nNot sure what you mean. false fail false is the message AVA generates if you use t.fail()\n\nIs something like this better?\n\nI believe t.fail(message) should already show the custom user-provided message. The problem is the default message becomes false fail false which is really useless.\nI think the solution is simply defining a default message here\njs\nx.fail = function (msg) {\n    msg = msg || 'some better message';\n    test(false, create(false, false, 'fail', msg, x.fail));\n};\n. Maybe: Test failed via t.fail() ?\nI think it's best the message includes a hint that t.fail() was used. A simple \"Assertion failed\" would be confusing if the test contained multiple other assertions in addition to t.fail()\n. :shipit:\n. @mnzt Nice thought. But in practice, this is not a good idea. NPM 3 and later automatically dedupes and maximally flattens your dependency tree. In simpler terms:\ninstead of:\nprojectDir\n    node_modules\n        ava\n            node_modules\n                core_assert\nyou end up with:\nprojectDir\n    node_modules\n        ava\n        core_assert\nWith npm@3 the only reason you end up with nested node_modules folders is if two dependencies both want the same dependency but have incompatible semver ranges. (i.e. foo-dep wants core-assert@2.x and bar-dep wants core-assert@3.x).\nYour changes here are broken by the new behavior.\n. LGTM\n. @sindresorhus - ultimately your call on switching the example to Babel 6, otherwise LGTM\n. LGTM. \nDo we have to reapprove new commits like this, or is my original LGTM still valid?\n. Like -1 to change your vote?\n. My only other thought would be a conditional approval, maybe using the markdown checklist syntax.\nLGTM if:\n- [ ] fix this\n- [ ] change that\nOther collaborators can then check those off as the submitter handles the feedback. I think that would be especially helpful on the more heavily debated PRs with wall-of-text posts, where feedback can be lost amongst the noise. \n. LGTM\n. I am :+1: for this, but it takes us towards verbose mode, which was controversial.\nPersonally, I think once we get TAP in place, it's output should be verbose (showing every assertion, passing or failing). It should be the responsibility of reporters to suppress output they feel is overly verbose. \n. > AVA's default output won't change, this is only needed for TAP output\nI was under the assumption that our default output would simply be our own custom TAP reporter.\n. > Perhaps we should discuss this after TAP support is landed in master.\nYep, actual code would be easier to discuss. Are you able to land TAP support without this issue being fixed?\n. let's do that.\n. I'd ask you to hold off. I already started pursuing that in #340, but put it on hold until tap support landed (done), and until @twada and I can find consensus on which parts power-assert should handle (pending).\n360 will certainly make this easy, so thanks for that.\nIf you can find something else to work on that would be great.\n. > Do you mean t.deepEquals etc?\nYes. #340 was an initial stab at it. If you run AVA's test suite with tap test/*.js --reporter=spec you will see a line for every assertion, but it's not super helpful:\nfail-fast mode\n    \u2713 expect truthy value\n    \u2713 should be equivalent\n    \u2713 should be equal\n    \u2713 should be equal\n    \u2713 should match pattern provided\nWith power-assert we could make that way more helpful:\n\u2713 t.truthy(api.options.failFast)\n    \u2713 t.same(tests, [{ok: true,title: 'first pass'}, {ok: false,title: 'second fail'}]);\n    \u2713 t.is(result.passCount, 1);\n    \u2713 t.is(result.failCount, 1);\n    \u2713 t.match(result.errors[0].error.message, /Test failed via t.fail()/);\n. > worse, a stack of it shows up in the test results.\nYep, that is anticipated. Basically due to some V8 issues, it is impossible to extract the filename and/or line number from caught SyntaxErrors in JavaScript. Node includes a hack that will print that information to stderr, but it relies on native C++ code. This is why we are changing the position of where we start listening for uncaughtException, it allows the error to be handled natively by Node, so we see the correct filename and line-number. Ideally we wouldn't have to do it this way, but we are picking the lesser of two evils.\n. looks like you missed 1 t.match\n. > Node 0.10 gives an error code of 8 on no file fount, while other Node versions give an error code of 0\n0? Really? I would have expected 1\n. It looks like it was 1 - that's good. As long as it is non-zero.\n. AppVeyor takes about 20 minutes per commit to complete, you pushed a few commits in rapid succession, so it was way behind. I cancelled all of them. Go ahead and fix that last t.match and push again, and we should be good.\n. LGTM\n. Thanks @sotojuan!\n. @vdemedes - I know I linked it before somewhere, but here are the docs you need for the empower-core. \nFYI: @twada keeps stuff he still considers \"beta\" under his GitHub account and only moves stuff to the power-assert-js GitHub group when he is committing to a stable, long-term API. That is why you had trouble finding those docs.\nAs for powerAssertContext, there is a good example of it's contents here. @twada may be able to provide a better link. You will likely need powerAssertContext.source.content.\nFinally, we are going to need to come up with a solution for assertions like this:\njs\nt.same(foo, {\n  blah: 'blah',\n  bar: 'foo',\n  zippo: [1, 2, 3]\n});\nThat is probably a more than we want to log, even when compacted to a single line. We can probably do some pretty clever collapsing of the code eventually (i.e. t.same(foo, {blah: ...})). We could even change color to light gray for the parts we collapse.\nFor the first pass, maybe we don't use the powerAssertContext (or maybe only if source.content.length is less than some arbitrary number).\n. > powerAssertContext.source.content is always in a form of single line\nYes, my point was that even that will end up verbose given a large enough object declaration. I'm sure our eventual solution will share code with SuccinctReporter.\n. I looked into this a bit.\nTurns out, powerAssertContext only attaches when the code is sufficiently complex to warrant it:\nThese will not attach a powerAssertContext:\njs\nt.ok(true);\nt.is('a', 'a');\nThese will:\njs\nvar a = 'a';\nt.ok(a);\nt.is(a, 'a');\nt.is({a: 'a'}.a, 'a');\nIt should still be possible to build something close to the original source using args.\n. > defer the assert title complications for later\nI agree. I think it will take us a while to figure that out correctly, and I don't want to hold up TAP support. Unfortunately, onAssert currently doesn't even tell you which method was called. @vdemedes - see #340 for a workaround on that (the identity function hack, not the whole PR).\n. > defer the assert title complications for later\nI agree. I think it will take us a while to figure that out correctly, and I don't want to hold up TAP support. Unfortunately, onAssert currently doesn't even tell you which method was called. @vdemedes - see #340 for a workaround on that (the identity function hack, not the whole PR).\n. LGTM\nI have run it against a number of projects, and the output looks great when everything passes. Something feels off when there are errors though:\nclassic reporter:\n\ntail end of spec reporter: \n\nNeither tells you which test the failure came from (these are not uncaught exceptions, but assertion errors).\n. :shipit: :+1: LGTM\nWe can improve the messages incrementally.\n. LGTM\n. Bad news:  You can't use Babel 6 yet.\nGood news: The fix is in master (https://github.com/sindresorhus/ava/commit/d0c370a1c00db8c02e9d7c4bcbac835b1a6c8a08)\nAre you using Babel for your production code, or just in your tests?\nIf just in tests, then do not install Babel as a dependency at all (it will end up being a duplicate of what AVA installs). AVA will handle handle setting up babel automatically for your tests.\nIf you need to transform your own production code using Babel, then you are stuck using Babel 5 ATM (unless you want to be bold and work from master).\nClosing as a duplicate of #207 \n. > Oh wow, something incredible happened, AppVeyor passed, but Travis failed.\nMy whole world view is shaken.\n. Looks like it's because I used scoped packages, and our travis script doesn't upgrade us to NPM 3. Not sure what we should do here. Should we bump travis to use NPM 3? Then all our CI is using NPM 3, is that a problem?\n. @vdemedes - I just updated this to use the empower-core API @twada and I agreed on. It is still going to fail on travis because I am using a scoped package. I think you are pretty safe to start exploring how to use this for your TAP integration though.\n. @vdemedes FYI: Some documentation on the recent changes to empower-core.\n@sindresorhus @vdemedes - How should I proceed w/ regard to npm 1 not supporting scoped builds? \n1. Do we just want to add npm i -g npm@3 as the before_script? This would mask any errors that only surface with the NPM 2 directory structure.\n2. Do we want to only upgrade to npm@2-latest on Node 0.10 and stick with the default elsewhere?\n3. Just skip it and wait for @twada to ship those changes so we can bump versions? I'm sure @twada will get it done fast, but it would be nice to have a solution in place that allowed us to deploy hotfixes to npm in the future.\n. Dependency bumped :+1: \n. just rebased, but there weren't any conflicts. Not sure why GH was suggesting I \"update branch\"\n. LGTM\n. :tada: \n. I read it wrong. We were dynamically blacklisting regenerator function, not async-to-generator. The decision to do that dynamically was made back in #61. Now that we reformat stack traces with source-map-support I don't think it is that big of deal.\n. I read it wrong. We were dynamically blacklisting regenerator function, not async-to-generator. The decision to do that dynamically was made back in #61. Now that we reformat stack traces with source-map-support I don't think it is that big of deal.\n. OK,\nThe Babel docs no longer mention the ability to black list particular plugins. Unless it still exists and is just currently undocumented, our only option will be to build an equivalent to the es2015 preset ourselves. Not exceptionally difficult, but it will add to our overhead as the preset changes over time.\n\nit's a lot slower than native generators.\n\nDo we have good data on that? (I believe it, just curious).\n. OK,\nThe Babel docs no longer mention the ability to black list particular plugins. Unless it still exists and is just currently undocumented, our only option will be to build an equivalent to the es2015 preset ourselves. Not exceptionally difficult, but it will add to our overhead as the preset changes over time.\n\nit's a lot slower than native generators.\n\nDo we have good data on that? (I believe it, just curious).\n. Hmm - I'm seeing a blacklist option here, wonder if that is babelify specific or not. Need to check it out.\n. Hmm - I'm seeing a blacklist option here, wonder if that is babelify specific or not. Need to check it out.\n. transform-runtime would not be the right place. That only transforms what would otherwise be polyfilled globals into require statements to avoid polluting the global namespace:\njs\nvar x = new Promise(...)\n// => \nvar runtime = require('runtime');\nvar x = new runtime.Promise(...);\nI think it would need to be an config param to babel-preset-es2015 (can you even pass config params to presets?), or to whichever module would be the appropriate target for a blacklist option. Probably something near or around the OptionManager class; it looks like it does some work loading/configuring plugins.\n\nI would prefer we didn't have to maintain our own preset.\n\nYeah - that was my feeling.\n. transform-runtime would not be the right place. That only transforms what would otherwise be polyfilled globals into require statements to avoid polluting the global namespace:\njs\nvar x = new Promise(...)\n// => \nvar runtime = require('runtime');\nvar x = new runtime.Promise(...);\nI think it would need to be an config param to babel-preset-es2015 (can you even pass config params to presets?), or to whichever module would be the appropriate target for a blacklist option. Probably something near or around the OptionManager class; it looks like it does some work loading/configuring plugins.\n\nI would prefer we didn't have to maintain our own preset.\n\nYeah - that was my feeling.\n. > Maybe we can just have regenerator even for newer Node.js version and open a ticket about submitting an option to babel-preset-es2015?\n:+1: \n. > Anything else before we can merge this?\nThat depends on our priorities. It breaks Babel 5 support (You can't use babel-5 to instrument your production code). I have no good solution for that.\nPerhaps we could simply detect which Babel version resolves for a given test file, and then conditionally provide a Babel6 or Babel5 config based on what we find.\nOr we just rip off the band-aid and go for Babel 6.\n. What warnings are you seeing? I can not reproduce.\n. Hmm, I still don't see that error. \nEither way though, it is probably a good idea to stop using a deprecated API. My only objection is the additional nesting.\n. Well it i no where near 15 lines. But still, I tend to agree with you, not worth doing.\njs\nvar deferred = {};\ndeferred.promise = new Promise(function (resolve, reject) {\n  deferred.resolve = resolve;\n  deferred.reject = reject;\n});\nLGTM\n. Now it is even failing on Travis: https://travis-ci.org/sindresorhus/ava/jobs/96899493#L996\n. All you.\n. LGTM.\nI just noticed the same thing.\nHow much does this affect our download size? Is worth releasing a 0.8.1?\nI am guessing the impact is actually pretty minimal (most of the same dependencies are brought in anyways between babel-plugin-espower, power-assert-formatter, and power-assert-renderer.\n. @twada - don't worry about that test. It is super flaky and not the fault of this change. See #335 \n. @sotojuan - See this, it really should be prefixed before you see it.\nMaybe try using the event emitter?\njs\napi.on('test', function (test) {\n  // do the test and slice the array here\n});\n. Honestly, I'm surprised the way you were doing it initially didn't work. Seems like it would be more convenient if the API supported the way you originally had it. And honestly, as I read the code, it should have worked that way. I'm obviously missing something...\n. :+1:  Thanks!\n. This is a byproduct of https://github.com/sindresorhus/ava/issues/308. We are intentionally delaying the installation of our own uncaughtException handler so that the correct location for the SyntaxError will be printed. That has the unfortunate side effect of meaning we lose control of when those errors print.\nI think @sotojuan's first instinct was correct. Use hook-std to mask the error for that particular test.\n. Please read #308. There is no \"fixing\" this beyond suppressing stderr/stdout for this one test fixture using hook-std. Our options are:\n1. catch and display the error yourself (i.e. maintain control)\n2. See the correct source/line number for syntax errors.\n2 is more important. We are choosing the lesser of two evils\n. Extend fork.js to allow us to pass custom options to child_process.fork.\nThen use stdio option and provide a stream we can inspect. [null, stdout, stderr]\n. Yep. Relevant changes to empower-core are here\n. No worries!\nThe issue here would be building up TAP output that would actually show these assertion messages. I think I would like to get a couple other things done first:\n1. Bring our tap output more inline with node-tap. Specifically, the yaml output for our errors. This should make the output from all the tap-mocha-reporters much nicer. For example, our errors currently have expected and actual properties, but the tap reporters prefer found and wanted (A bit odd in the JS world, but TAP has it's roots in other programming languages - Perl mostly I think).\n2. Integrate clean-yaml-object over serialize-error. I think it will help with 1. Advantages over serialize-error are pretty printing of Buffers, removes large objects attached to some Errors by Domains and EventEmitters, and a few others. I extracted it from node-tap, but haven't gotten around to integrating it.\n3. Use tap-emitter. That's really early stages right now. The next step is creating a good way to print subtests. I would love for that to also end up being code shared with node-tap, but that won't be as straightforward as clean-yaml-object or stack-utils were (since it's not directly extracted from node-tap in the first place).\n. Yeah sure - I'll file a couple issues on tap-emitter for where I think it should go and we can bikshed there. \n. RE: actual/expected \nNeither is technically \"right\". The contents of TAPs yaml output is completely unspecified as far as I know. As I recall the fancy mocha style diffs only print when using found / wanted. Maybe it would be better to hunt down where that happens and file a PR so it could use either (i.e. prefer the current properties, but fallback to actual/expected).\n. Relevant:\n  https://github.com/isaacs/testanything.github.io/blob/tap14/tap-version-14-specification.md\n. Closing as this will never be merged and remaining todo's are tracked in #723\n. > Since ava spawns new processes, debugging cli.js directly is not possible. The new node processes try to listen to the same debug port and thus crash with EADDRINUSE :::5858\nThat is going to be additionally problematic since AVA will potentially spawn a number of processes at once if you have multiple test files.\n\nI thought that ... did that for us\n\nI think this is separate from remapping coverage/sourcemaps.\n\nAny downside adding this to AVA?\n\nOnly the multiple process issue. Can we detect if the debugger is in use and enforce serial execution of the test files?\n. > Since ava spawns new processes, debugging cli.js directly is not possible. The new node processes try to listen to the same debug port and thus crash with EADDRINUSE :::5858\nThat is going to be additionally problematic since AVA will potentially spawn a number of processes at once if you have multiple test files.\n\nI thought that ... did that for us\n\nI think this is separate from remapping coverage/sourcemaps.\n\nAny downside adding this to AVA?\n\nOnly the multiple process issue. Can we detect if the debugger is in use and enforce serial execution of the test files?\n. Yep, sounds like a bug.\nWe do change process.cwd to match that of the test file, but that should not affect the --require flag. The solution would be to resolve them to absolute paths in api.js.\nPR welcome. Please make sure to include a test that fails before / passes after your changes.\n. Yep, sounds like a bug.\nWe do change process.cwd to match that of the test file, but that should not affect the --require flag. The solution would be to resolve them to absolute paths in api.js.\nPR welcome. Please make sure to include a test that fails before / passes after your changes.\n. LGTM. \nDo we need to add a test to protect against future regressions?\n. LGTM. \nDo we need to add a test to protect against future regressions?\n. > one small thing is renaming babel.js\n:+1: Though you may want to wait until we get or #352 and/or #349 merged. They both make significant modifications to babel.js\nIn general, If you are embarking on something ambitious, you should always seek feedback/clarification by opening an issue or making sure there is consensus on an existing issue thread. Open a PR as soon as you have done the minimum amount of work to demonstrate your idea (just prefix the title with [WIP], and describe what you still have to complete, so people know not to make \"the tests are failing\" comments). In general - get your stuff in front of the community for feedback ASAP.\n. @tmaximini What issue? The only difference should be one of performance. If you've got other issues let us know.\n. > Rebase went wrong here.\nIt wasn't like that last night. \n. @sindresorhus I used rimraf over del-cli, because https://github.com/sindresorhus/del-cli/issues/3\n. Hmm, it seems implementing caching fixed my Windows errors. I previously saw some errors that made me suspicious the problem was in temp-write. I'm even more suspicious now.\n. OK, cool.\nI'm heading out for ~4 hours. I will apply all the suggested changes later tonight.\n. OK, This is ready for review.\n// @vdemedes @sindresorhus \n. closed in favor of #390 \n. closed in favor of #390 \n. Ugh. I tried on a couple actual projects that used Babel themselves (Babel 5 and Babel 6), I noticed the problem there, but thought it was related to trying to use babe-core/register in the tests.\nWorking on a solution now.\n. Done. #353\n. > using npm 3\nThat is basically a requirement of Babel 6. npm dedupe is the only other option.\n\ncaching\n\nI just finished implementing much the same for nyc with very good results.  It's part of my plan after I land #349\n. I don't think we should put cached files in the home dir. Users will not be able to find and clear them. For nyc we settled on node_modules/.cache/nyc, the equivalent for use would be node_modules/.cache/ava. This means users will clear the cache themselves if they trash node_modules and reinstall (even if they don't know the cache is there). If we convince enough modules to adapt that pattern, it also makes trash node_modules/.cache a standard way to clear caches.\nIf you are storing in the home dir, then you need to take extra care salting the hash with the versions of Babel and every plugin used. That would still be a good idea in a local cache, but less critical since it's easier to clear the cache. \n. also, I wrote caching-transform for nyc. My intent was to use it here as well. Similar to cacha, but intended specifically for transforms.\n. The cache is currently saved in ./node_modules/ava/node_modules/.cache/ava, whereas I think it should be in ./node_modules/.cache/ava. That is how I did it in nyc, and my hope was we would all store our cache content in a single directory that was easy to delete.\nI think the solution to this is to do:\njs\npkgDir.sync(path.dirname(filename))\nHowever, there is a chance they are using a global install, and do not have a package.json anywhere. I think we just disable caching in that situation, I do not like the idea of users having to hunt down were we are hiding the global cache.\nnyc is starting with caching disabled by default for the time being. It is enabled with a --cache argument. I think we should do the opposite. Enable by default and provide a --no-cache argument. That could help eliminate the cache as the source of some difficult problem down the road.\nWe should probably salt the hash with babel.version as well (maybe even with the version of every preset/plugin, but that may be overkill).\n. > Regarding cache, when do you think users would want to clear it?\nBecause caches cause problems. The one CSE joke I know is about cache invalidation. I can't think of a single piece of software I use that is so absolutely confident in its caching strategy that it does not provide a means for me to clear it. My browser, my IDE, npm, all of these give me an option to clear the cache.\nAs a practical example, the first thing I can think of is babel-plugin-xxx publishes a regression and the result of that regression makes its way into the cache. I am sure that is just the tip of the iceberg.\nPerhaps, when we can't resolve their node modules folder, we do put it with the global AVA install and just provide a --clear-cache flag. \n. >  Should at least then be ./node_modules/.cache/ava-0.8.0.\nThat makes it (only a little) harder to script removing the directory. I was going to suggest we include the AVA version in the salt. I'm fine with this approach though. I just really want it in their root node_modules folder so it is visible.\n\nActually, we should make the salt based on the versions of the various Babel related packages\n\nI think we are going to find that difficult and costly. Babel doesn't do that in it's own cache implementation. To actually do that right, you would need to resolve the package.json of all 20 (twenty!) dependencies of babel-preset-es2015, fetch and parse those to extract pkg.version, and hash them all together. Same for babel-preset-stage-2 (another 5 dependencies when you walk the full graph), and again for power-assert, and transform-runtime\n\n--clear-cache ... A flag comes with a lot of hidden overhead.\n\nAgreed. That was offered as a workaround for my concerns with global installs. If we disable caching for global installs, then this is moot. I still think --no-cache has value, the cache introduces so many moving parts, it would be nice to be able to disable it and eliminate it as the source when troubleshooting a problem with users.\n. @sindresorhus - updated.\n. > should an issue be raised for discussion first, then open a PR\nOh, absolutely. Does the line above not make that clear? Should we amend this:\n\nFor ambitious tasks, you should try to get your work in front of the community ...\n\nTo \n\nOnce consensus has been reached in the issue thread, and you've begun working on a solution, you should try to get your work...\n. Should have waited for a bit more consensus on this PR! :wink: \n. I thought so to, I was just pointing out the irony that we rushed the PR that said \"seek feedback ... find consensus\".\n. It is finally fixed here: https://github.com/sindresorhus/ava/commit/94797b10386e8d638a9efd09a9b8d08dc9481ac6\n\n... \nfor real this time.\n...\nI think! :smile:\nNote that if you want to use master, it's upgraded to Babel 6. You absolutely should (darn near must) upgrade to npm@3 to use Babel 6. Your only other choice is npm install && npm dedupe every time you want to change dependencies.\n. make sure you have a recent version of AVA installed globally. The global CLI commands actually default to the local installs. With a current global install, you should see no difference between ava and node_modules/.bin/ava.\n. If a test is hung, how do I tell which one it is? At least with the old reporter and --serial you could basically figure it out. Could you maybe make this one show just the current test title? (I know that would probably only work in serial mode). Maybe it only shows in serial mode and the test has taken longer than 2 seconds (so you aren't blocking to write to stdout every 100ms when the tests are fast)?\n. Oh shoot. That's right. We could maybe pass up a nextTest  property in serial mode. We can add that later though.\n. > Because we use Error for our internal messages\nIn that case, we should create our own internal error class and use it where we explicitly want to suppress the stack trace. \n. @ariporad - Good work! A few minor nits, but overall very well done!\n. Let's get https://github.com/sindresorhus/delay/pull/8 merged and published. Then use the updated function here. Once that happens, I think we are good to go.\n. Caching node_modules is a bad idea. If you delete a dependency from package.json, but your code still uses it, you won't get a Can't find module ... failure because it still is available in the cached folder.\nBuild caching should almost never be used. The exception being truly huge assets that almost never change.\n. Same as my comment in #361, I think caching node_modules is a bad idea. It is a lot more tempting for AppVeyor since it is so slow, but probably still not worth the risks.\nI'm confused by the second commit. If I compare the AppVeyor output from your PR, and previous AppVeyor runs, they seem identical. What am I missing?\n. > I think we should cache node_modules on AppVeyor, because npm on AV is a lot slower, and any dependency mess ups will be caught by travis.\nThat is a tempting thought. However, it doesn't seem the build is actually any faster between your first and second commit, should it be?\n. For your first commit there wouldn't have been a cache yet, so it should have been slow. Check the build history. Discounting hung builds, this PR takes about the same amount of time to build.\n. > Although that's awful strange\nI don't know. If I had to guess, I would bet AppVeyor proxies the npm repository, so installs happen very fast.\n. LGTM\n. :+1:\n. Also, unhandledRejection.\n. Copying a my suggestion here:\n\nInstead of separate errors and tests arrays, should it just be tests? A test could have a status attached (passed / failed), and all assertions with their respective results. Might be better than just standardizing errors\n  -- @jamestalmage\n\nand a valid counterpoint:    \n\nwe also need to store rejections and exceptions, which are not attached to any test and don't have test title, etc. So storing rejections and exceptions in tests doesn't sound right.\n-- @vdemedes \n. My vote is to punt on this until a later date. AVA and Babel both are currently moving at breakneck speed. I would prefer not to introduce an extra layer of complexity.\n. > browsers... \n\nAVA does not (yet) support browser side testing.\n\ninstall time\n\nMocha doesn't ship with Babel or power-assert, we do.\n\nLaunching time.\n\nMake sure you use npm@3 to install. Babel loads slow on npm@2. Transform caching is coming - major speed bump. Babel is always going to introduce extra startup delay.\n\nDefault formatter.\n\nI need more details before responding. Got a sample program and stack trace?\n. I think that is an AVA bug. We don't print stack traces if it's not an assertion error. The fix for that is already in development.\n. @dfcreative 0.10.0 is out and should improve on issues 2 (startup time) and 3 (output formatting)\n. Actually, it is only ~16.5 MB, @ariporad's number includes dev-dependencies. You need to delete node_modules and run\nsh\n$ npm install --production\nIf you create a new project and just do this:\n``` js\n$ npm init\ndefault for every prompt\n$ npm i babel-core babel-preset-es2015 babel-preset-stage-2\n```\nI get a relatively svelte 6.8 MB.\nAdding babel-plugin-espower brings me to 9.8 MB. That seems like a pretty significant jump. @twada - Any ideas how we can reduce this? \nAdding babel-runtime@6.X.X does bump me up to 14 MB, but that is due to massive duplication of babel-runtime@5.X.X. It gets copied into the node_modules folder of every babel-plugin-transform-*. This is expected, since current babel plugins are written in ES6, compiled with babel@5, and maintain  a dev-dependency on babel-runtime@5. This really does not hurt your install times though, since all those duplicates are definitely coming from the cache (babel-runtime is not re-downloaded 20 times).\n2.5 MB for the remaining 30 some dependencies we have seems pretty reasonable.\n. Actually, it is only ~16.5 MB, @ariporad's number includes dev-dependencies. You need to delete node_modules and run\nsh\n$ npm install --production\nIf you create a new project and just do this:\n``` js\n$ npm init\ndefault for every prompt\n$ npm i babel-core babel-preset-es2015 babel-preset-stage-2\n```\nI get a relatively svelte 6.8 MB.\nAdding babel-plugin-espower brings me to 9.8 MB. That seems like a pretty significant jump. @twada - Any ideas how we can reduce this? \nAdding babel-runtime@6.X.X does bump me up to 14 MB, but that is due to massive duplication of babel-runtime@5.X.X. It gets copied into the node_modules folder of every babel-plugin-transform-*. This is expected, since current babel plugins are written in ES6, compiled with babel@5, and maintain  a dev-dependency on babel-runtime@5. This really does not hurt your install times though, since all those duplicates are definitely coming from the cache (babel-runtime is not re-downloaded 20 times).\n2.5 MB for the remaining 30 some dependencies we have seems pretty reasonable.\n. > Actually if we have a dependency which merely reexports babel-runtime it would allow npm to dedupe babel-runtime@5 instead.\nI thought about that. Actually, I think it would be better if Babel did that (all the Babel 6 plugins should depend on a differently named module that exports babel-runtime@5). That way everyone benefits.\n. The real issue here is that npm is just slow at this. npm install ava creates hundreds of http requests that just get 3XX responses. It can't even do much of it in parallel, because it needs to resolve which version of direct dependencies will be used, before it can start querying one level deep for transitive dependencies.\nI don't see us dropping lots of dependencies. The only dependencies that would be easy to replace, are also ones that are not really going to cut down on download times.\nAlso, I really don't think comparing cleared cache download times is that valuable. I'm much more concerned with download times when we have mostly cache hits. (AVA is still pretty bad by that metric as well, but it's not as extreme).\n. LGTM\n. 1. :+1: \n2. The penalty for coverage is about to drop from 17 seconds to 3 seconds https://github.com/bcoe/nyc/pull/108. I don't think that is necessarily worth maintaining an additional build path.\n3. #70 is adding the feature to AVA, I think @ariporad means he wants a good way to use a file watch while hacking on AVA. That would be a tap feature request.\nMore on 2:\nEven without coverage: 20-25 seconds is still a really long time to wait for a failure.\nReally the best thing to do is learn the tap cli. I definitely did not fully understand it when I started hacking on AVA, and I think I even proposed something similar to your ideas here at some point. I (like @ariporad I believe) was most accustomed to testing in mocha, so there was a definite learning curve.\nWhen I am hacking on a particular AVA problem, and want to focus on a particular file (let's just say fork.js), this is the command I use:\njs\ntap -b test/fork.js\nThe -b is short for --bail-out, which makes tap exit as soon as it sees a single failure. Usually that is enough to quickly identify a problem you can fix and move on.\n. 1. :+1: \n2. The penalty for coverage is about to drop from 17 seconds to 3 seconds https://github.com/bcoe/nyc/pull/108. I don't think that is necessarily worth maintaining an additional build path.\n3. #70 is adding the feature to AVA, I think @ariporad means he wants a good way to use a file watch while hacking on AVA. That would be a tap feature request.\nMore on 2:\nEven without coverage: 20-25 seconds is still a really long time to wait for a failure.\nReally the best thing to do is learn the tap cli. I definitely did not fully understand it when I started hacking on AVA, and I think I even proposed something similar to your ideas here at some point. I (like @ariporad I believe) was most accustomed to testing in mocha, so there was a definite learning curve.\nWhen I am hacking on a particular AVA problem, and want to focus on a particular file (let's just say fork.js), this is the command I use:\njs\ntap -b test/fork.js\nThe -b is short for --bail-out, which makes tap exit as soon as it sees a single failure. Usually that is enough to quickly identify a problem you can fix and move on.\n. What did you have in mind for a watch script?\n. Well, when I have 100% coverage - yeah, I like having it there so I know when I'm introducing new code that isn't covered. Since we are so far off complete coverage, it's really hard to see exactly where you are dropping coverage, I guess we could turn it off.\n. WebStorm asks you about adding a file watcher whenever it sees ES6 code. If you accidentally click yes, this can happen. I keep meaning to figure out where the setting is just to turn that off.\n. test-file-launcher.js\n. Also, please paint the bikeshed green.\n. test-launcher.js or test-worker.js \n. Just wondering here. Now that #371 is merged, should we just use that for this? Is a stack trace really going to be that helpful? Especially since test.exit() is now always deferred via a promise?\n. We would be setting it via test._setAssertError, so we will still know which test triggered that.\nWe currently only check the assert count when the test exits, not for every assert, so you currently can't know which assertion put you over the limit now.\n(test._setAssertError is a bit of a misnomer now).\n. @ariporad - How close are you to finishing this?\n. @ariporad - How close are you to finishing this?\n. @ariporad - please see the edited description and title. All your PR's should contain a quick summary of the change, and a link back to the original discussion.\n. > In the format Fixes ...\nMy understanding was that the fix was split over a couple PR's. Do you still want that syntax for partial fixes?\n. > So I made that one Refs #XXX and this one Fixes #XXX.\nWorks for me!\n. LGTM.\nThough I think this highlights some deficiencies in our API.\nInstead of separate errors and tests arrays, should it just be tests? A test could have a status attached (passed / failed), and all assertions with their respective results. Might be better than just standardizing errors as described in #365 \n. I just realized I could use get-stream here.\n. OK, updated to use get-stream, removed concat-stream, and updated with @vdemedes suggested verbiage.\n. I was under the (very mistaken) impression that would not work following nyc. Fixed.\n. Using got 6.0.0 branch: \nmaster - first run (no cache)\n./node_modules/.bin/ava  22.74s user 1.81s system 372% cpu 6.587 total\nmaster - second run (with cache)\n./node_modules/.bin/ava  6.65s user 0.77s system 175% cpu 4.240 total\nthis branch - first run (no cache)\n./node_modules/.bin/ava  7.78s user 0.79s system 132% cpu 6.474 total\nthis branch - second run (w /cache)\n./node_modules/.bin/ava  6.76s user 0.77s system 178% cpu 4.226 total\n\nHuge advantage when the cache is empty.\n. Using got 6.0.0 branch: \nmaster - first run (no cache)\n./node_modules/.bin/ava  22.74s user 1.81s system 372% cpu 6.587 total\nmaster - second run (with cache)\n./node_modules/.bin/ava  6.65s user 0.77s system 175% cpu 4.240 total\nthis branch - first run (no cache)\n./node_modules/.bin/ava  7.78s user 0.79s system 132% cpu 6.474 total\nthis branch - second run (w /cache)\n./node_modules/.bin/ava  6.76s user 0.77s system 178% cpu 4.226 total\n\nHuge advantage when the cache is empty.\n. @sindresorhus, @vdemedes, et al. - ready for review.\n. @sindresorhus, @vdemedes, et al. - ready for review.\n. > could use a test though.\nYeah. How far do you want me to take it? For nyc, we run every test twice (once with caching on, once with caching off). That was easy to pull off with nyc due to how we build our test suite. \nThis is what I am thinking for tests:\n1. Test a fixture project with caching on. Once the test completes ensure there are X number of files in the cache directory.\n2. Delete the cache and test the same fixture from 1 again. Ensure the cache stays empty.\n3. Duplicate some of our source map tests with caching off? I might need @novemberborn's help here since I know next to zero about how a good source map test should look.\n. OK, Incorporated the last round of feedback from @vdemedes. Once CI passes again, I intend to merge.\n. Landed.\n. >  I think a solution here is to handle the logging so that it's outputted with the correct test in verbose mode \nThat would be hard to do with console.log unless we either:\n1. Force --serial when --verbose is used (I don't like this idea).\n2. Use domains to determine which test console.log is used from.\n3. Provide a t.log method.\nI think 3 is pretty interesting. It won't help much with capturing logs from production code (unless you use proxyquire to inject it some how). For the verbose reporter, we could always print t.log statements. For the mini reporter, only output logs with failed tests\n. use --verbose\n. > I don't think anyone would use t.log\nYeah, that was my concern as well. However, I think having a way to log output that you can be sure will be printed right underneath the :white_check_mark: of the related test is pretty valuable. Obviously --serial is a way to achieve that. \nAt some point I think we need to start developing a \"best practices / hints and tips / faq\" document with stuff like this in it (where we fail to find ideal / straightforward solutions to these types of problems). For example, a good heading for the discussion we are having here would be \"Debugging async tests\", that talks about the difficulty of debugging async tests (it's hard to reason about console.log statements unless you enable --serial).\n. > > For the mini reporter, only output logs with failed tests\n\nI think we should still log it with the mini reporter as it's still useful to quickly print out some values.\n\n@sindresorhus - for quickly printing out some values, console.log works just as well, assuming you intend to delete the log statement later. The output of the mini reporter offers no context for log statements in passing tests anyways. There are advantages to not displaying in the mini reporter unless there is a failure:\n1. The log statements can be given context (by being grouped with the failure output for that specific test).\n2. You can leave the log statements in the test. As long as the test passes, it is not cluttering up your output. If/when the test fails - you are given additional information beyond the simple assertion failure.\n. > Usually I just console log in one place so I don't really need a context\nIn that case the mini reporter should be fine. Though maybe we should somehow use hook-std or something to detect a line has been written and avoid deleting it.\n. > we don't even have to load anything if there's already a native promise\nYep, use pinkie-promise for that.\n\nWant me to have a go at implementing this?\n\nLets get consensus with the plan first.\n. > we don't even have to load anything if there's already a native promise\nYep, use pinkie-promise for that.\n\nWant me to have a go at implementing this?\n\nLets get consensus with the plan first.\n. Long stack traces might be a good reason to keep Bluebird around. \nSee #394.\n. Long stack traces might be a good reason to keep Bluebird around. \nSee #394.\n. Nothing jumps out:  https://gist.github.com/jamestalmage/873252568d8487bb1d95 \n. Yes, but if you create this:\njs\n// index.js\nrequire('time-require');\nrequire('./foo.js');\njs\n// foo.js\nrequire('bluebird');\nyou get this:\n\n32  bluebird (node_modules.../release/bluebird.js)  48ms  \u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 84%\n33  ./foo (foo.js)                                  48ms  \u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 84%\n\nThe output is cumulative of the require time for everything you require, plus your time to eval and return. While bluebird itself takes a long time, everything it requires is really fast (<1ms)\n. > I was thinking using https://github.com/s-a/iron-node flame graphs\nI just tried that, I am not finding it super useful either.\n\nMaybe something in Bluebird could be made lazy.\n\nMy only thought would be \"make everything lazy\".\n. I still need to write a test for this. I can get to it later today.\n. > It's already tested in Babel I assume\nYeah, but I could have sworn I already addressed this once before, adding a regression test ensures we don't accidentally drop it.\nTest added.\n. > Why wouldn't we want this?\nBecause it prevents them from using a .babelrc file for instrumenting their production code with different settings from the tests. We don't currently support people modifying the transforms applied to their tests (and if we ever do, it almost certainly won't be via a .babelrc file).\n. > While recommending the babel require hook for test helper files.\nWe will support compiling helpers soon (https://github.com/sindresorhus/ava/pull/177 was an early attempt). Once #390 is merged, I will work on that.\n. We are already doing that. Without this change, our babel settings from caching-precompiler/test-worker are being merged with their .babelrc, and the user has no recourse to change that.\nIf users really want power-assert, and stage-2 constructs for their production code then they should list that explicitly in their .babelrc.\n\nbecause being forced to use diffrent babel settings for tests and production code is really bad.\n\nI don't think that's true at all, in fact I think the opposite. If your production code requires Babel, then you need to include a build step as part of npm publish to transpile your code to ES5 so it is easily consumed on npm. A build step necessitates a complete.babelrc for your production code, not one that relies on some plugins your test runner automagically merges (Especially since AVA should be a dev-dependency which would not be deployed with your production code). A complete .babelrc also clearly documents what code constructs can be used in production.\n. > On top of that, the babel-runtime thing means that I can't use bluebird\nWhy not?\n\ndetect a competing .babelrc, and if it exists, use that (+ power-assert), instead of it's own config...\n\nI don't like that idea. I am certainly open to the idea of allowing users to customize the test transpiler as they see fit. (If you hunt back into the early 100's you'll find half a dozen PR's where I explore the idea). Doing it automatically isn't a good idea, users should have to be explicit about it.\n. >  it overrides 'Promise' anywhere it finds it, regardless of scope.\nWell that's annoying.\nAs for the rest of it:\n\nI am certainly open to the idea of allowing users to customize the test transpiler as they see fit ... [but] users should have to be explicit about it.\n. > > it overrides 'Promise' anywhere it finds it, regardless of scope.\nWell that's annoying.\n\nIt's also not true:\n``` js\n// foo.js\nimport Promise from 'bluebird'\nimport test from '../../'\nPromise.longStackTraces();\ntest(async t => {\n    await Promise.resolve('foo');\n});\n```\n``` js\n// transpiled foo.js\n'use strict';\nvar _regenerator = require('babel-runtime/regenerator');\nvar _regenerator2 = _interopRequireDefault(_regenerator);\nvar _asyncToGenerator2 = require('babel-runtime/helpers/asyncToGenerator');\nvar _asyncToGenerator3 = _interopRequireDefault(_asyncToGenerator2);\nvar _bluebird = require('bluebird');\nvar _bluebird2 = _interopRequireDefault(_bluebird);\nvar _ = require('../../');\nvar 2 = _interopRequireDefault();\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n_bluebird2.default.longStackTraces();\n(0, _2.default)((function () {\n    var _this = this;\nvar ref = (0, _asyncToGenerator3.default)(_regenerator2.default.mark(function _callee(t) {\n    return _regenerator2.default.wrap(function _callee$(_context) {\n        while (1) {\n            switch (_context.prev = _context.next) {\n                case 0:\n                    _context.next = 2;\n                    return _bluebird2.default.resolve('foo');\n\n                case 2:\n                case 'end':\n                    return _context.stop();\n            }\n        }\n    }, _callee, _this);\n}));\nreturn function (_x) {\n    return ref.apply(this, arguments);\n};\n\n})());\n```\n. > This can come with a warning.\nOr not. We already handle assertions that are not enhanced by the plugin (throws, doesNotThrow).\nPersonally, I would rather just wait until we figure out configurable transforms instead of adding a feature we intend to replace.\n@ariporad - Would the ability to include stage-1 and/or stage-0 satisfy your needs? I am thinking something like a --stage=0 flag. \n. I've never said we won't allow configuration of how your tests get transpiled, that's a feature I think is worthwhile. Just that it's not going to automatically happen because you have a .babelrc file present.\nI'm not sure what's \"most common\"/\"typical\", though I am sure ES2015 use is on the rise. I do find it hard to believe that stage-1/0 constructs are in high demand, especially for use in production code. That seems like a risky plan.\nHowever we solve configurable transpiling, I want it to be able to participate in the caching-main thread-precompiler stuff we just added that has given us a huge performance boosts.\n. @MoOx - as stated elsewhere. The problem is that without this, the configs end up getting merged with the one AVA automatically creates(which is not always what you want). Specifically, I would generally prefer only to ship production code with fully ratified language constructs (i.e. only the es2015 preset), whereas I'm perfectly comfortable using \"beta\" language features in my tests.  \nWe are working on a --no-babel option which will allow you to disable AVA's babel processing entirely, and let you do your own thing.\n\nAre you aware that babel have an option to specify config depending on the NODE_ENV variable\n\nI wasn't until just a few hours ago, and my thought was similar to yours. My question now is: Is there a way to specify an environment on a per-file basis via the API?\n. AVA automatically includes stage-2, which is pretty beta stuff. Also, AVA adds a transform required for power-assert which you definitely don't want instrumenting your production code. \n. Let's continue further discussion in #448.\n. Hmm. That should not be true. It's probably a bug from when we moved power-assert rendering to the main thread. I will look into it.\n. Seems specific to the tap-nyan reporter\nI had better success with:\n./cli.js --tap test/fixture/power-assert.js | tap-spec\n./cli.js --tap test/fixture/power-assert.js | tap-mocha-reporter nyan\n. >  I think that it would be a much simpler if we just modified the babel-runtime plugin. \nbabel-transform-runtime is not under our control, and I doubt the Babel team would ever accept a change that used absolute paths, the current behavior is intentional on their part.\n\nThis seems like something that could easily fall apart.\n\nYou will need to defend that statement with clear examples of how that might happen.\n. Only if there is a valid reason not to perform this transform. I don't see it causing any problems, and I think it solves other (i.e. they can require('bluebird') in npm@2 even if they don't have bluebird installed).\nI will look into rebasing this over the weekend.\n. rebased\n. AVAs tap output doesn't currently show individual assertions, just outputs tests.\nClosing as a duplicate of #324.\n. :+1: LGTM\nIf you can figure out a concise way of communicating what I've said in this discussion, that would be a welcome PR as well. It's a point of confusion for a lot of people.\n. Nice, latest changes are even better.\n. @mattkrick - Do you have time to update this PR with the recommended changes?\n. @mattkrick - One last question. I am not familiar enough with the supertest API to tell if the code is correct or not. Can you verify you have successfully tested IRL with code that looks similar to the example provided here?\n. :+1: LGTM\n. yep\n. > That's a convention nobody uses\nExcept for https://github.com/bcoe/nyc/issues/131  :smile:\n. > I'd also like to add a \"files\" section\nYeah, I think basically any config value is fair game. I can't think of one we shouldn't allow them to configure this way.\n. nyc is falling off that list very soon: https://github.com/bcoe/nyc/pull/132\n. Hmm. I don't know. It doesn't depend on powerAssertContext at all, just assertionErrror.expected and assertionError.actual, it certainly could be implemented as a custom reporter. I just don't know if that's the best choice.\n// @twada\n. Oh yeah, forgot about BinaryExpressionRenderer - we definitely want good diffs if they do: \njs\nt.true(someLongString === someOtherLongString)\nHow about extending BinaryExpressionRenderer with a \"minimumLength\" parameter that prevents short diffs from being displayed, a diff for t.true(\"foo\" === \"bar\") is unnecessarily verbose, and it's easy to spot the difference looking at the power assert graph output.\n. Gotta say, didn't think this was only going to end up being 4 LOC. Small reusable modules FTW!\n. > was about to send the same PR!\nI assigned myself before I started work. Sorry!\n. @vdemedes @sindresorhus - squashed and ready for review.\n. All recommended changes made.\n. > require on the command line shouldn't override the package.json, it should add to it\nNah, then you can't be explicit about ordering. If we append it then you are stuck if you wanted it prepended, and visa-versa. If you have a complicated --require setup that is a pain to edit on the command line, just edit it directly in package.json.\n. > Shouldn't this be part of the config object in package.json? Or am I missing something?\nThat got debated here: https://github.com/sindresorhus/ava/issues/405#issuecomment-168569929\nThe only advantage to using the config object is for build scripts, so they can easily access the config via the npm_package_config_ prefixed environment variables. There's no advantage for AVA, because we want you to be able to call ava directly - not be forced to always run via npm run scripts.\n. @sotojuan - are you working up a PR for this?\n. OK, I've updated the labels so people know someone is working on it.\n. For Node 0.10, lookup how Set and Map are referenced when transformed by Babel, should be something like:\nvar Map = require('babel-runtime/lib/fn/Map');\n// or \nvar Map = require('core-js/lib/fn/Map');\n. :+1: I agree. 2.3MB is just way to big. AVA already takes just this side of eternity to download. @twada and others have put in major effort slimming down their packages and our download size. This would be a major step in the wrong direction. \nSorry @mayankchd. Otherwise your PR looked great. It's not your fault that package is so bloated. \n. LGTM: \n\n\n. @vdemedes - see #412 \n. What is the stack trace of the unhandledRejection?\nLinks to real repos work best :smile: \n. https://github.com/tapjs/tap-mocha-reporter/blob/master/lib/reporters/base.js#L54-L75\n. I think it's a good idea. I wouldn't make the part of the build. Just something we can run through before cutting a release to make sure things look right\n. > It's just I'm sure those tests will be forgotten often\nWe will remember when we go to cut a release. Even so I don't think that's a big deal, these aren't mission critical tests. \n\nWhy can't we have automatic tests for comparing output?\n\nBecause what is output to the stream does not reflect what you actually see. Not every terminal emulator obeys the full set of ansi codes in the same way (here is just one example). A set of visual tests let us quickly check things look right locally and on whatever VM's we have available.\n. This is how mini reporter looks now (lorem ipsum content is being generated as process.stdout.write in child process). Note how the test count always stays below the logged console content.\n\nTerminal.app on OSX is problematic (I'm running Yosemite, there is reason to believe it's fixed on newer versions). I haven't tested on Windows.\n. I'd appreciate some help figuring out the last of the kinks in this from anyone who knows something about terminal cursor manipulation. I'm not sure where things are going wrong. Here's what works:\n1. Windows: It just works, always. (go figure)\n2. OSX iTerm, and Ubuntu terminal. It works until you end up with process.stdout.write lining up perfectly with the column width. Then you get some extra newlines. You can see this with:\n./cli.js test/visual/text-ends-at-terminal-width.js\nlook at the comments in that test to fully understand what it is doing.\n3. Terminal.app on OSX Yosemite. Very broken. There is reason to believe it works on El Capitan, if someone could verify, that would be great.\nTo test, use ./cli.js test/visual/<one-of-the-visual-test-fixture-files>.js\nThe lorem-ipsum and text-ends-at-terminal-width tests are the best ones to look at. \nNote lorem-ipsum is a misnomer now, as it prints the Gettysburg Address instead. I needed to be able to tell if characters/lines were missing from the final display and couldn't do that with lorem ipsum because I don't know Latin. \n// @Qix- @vdemedes @floatdrop @sindresorhus \n. @ecowden - darn. That's the same crap I get on Yosemite. I was really hoping to be able to blame that on an old OS.\nThe _clear function is basically a modified version of logUpdate.\nUsing logUpdate only works if you can clear the entire text. Right now the algorithm tries to do the following:\nA: New status update.\n\nClear the existing status update.\nWrite the new status update.\nStore the new status update as lastWritten. (existingStatus would have been a better name)\n\nThis is basically identical to log-update with the exception of storing the status.\nB: New log output\n\nClear the existing status update.\nAppend text to the existing log output\nRestore the existing status update from lastWritten\n\n\nA is pretty easy, a solved problem with log-update.\nB has proven difficult. Especially if what they write to the console does not end with a line terminator (\\n). My current solution is just to store the last logged line, and delete/rewrite that as well.\n. The latest push looks good on everything I've tried except Terminal.app.\n- iTerm on OSX\n- ubuntu desktops default terminal\n- Windows Command Prompt\n. Finally figured out the bug. Terminal.app does something weird with cursorLeft. The following code writes a line 20 characters wider than the terminal width, then writes the cursorLeft control character:\n``` js\nvar ansiEscapes = require('ansi-escapes');\nvar str = '';\nfor(var i = 0; i < process.stdout.columns + 20; i++) {\n    str += String(i % 10);\n}\nprocess.stdout.write(str + ansiEscapes.cursorLeft);\nsetTimeout(function() {\n    // delay so you can see where the cursor is\n}, 10000);\n```\nIn most terminal emulators the cursor ends up all the way to the left of the current (i.e. second) line:\n\nIn Terminal.app, it ends up all the way to the left of the first line. (It moves up a line for no apparent reason).\n\n. I think I have found a decent cross platform solution.\nI'd like a number of people to pull this down and run:\nnpm run visual\n. @vdemedes - updated with lots of code comments and clearer variable names.\nalso, https://github.com/jamestalmage/ava/commit/328638db98e18e1e957dd86b9d0410565451f898 \n. So, that last commit is maybe unnecessary:\n``` js\nvar through2 = require('through2');\nvar assert = require('assert');\nvar ct1 = 0;\nvar ct2 = 0;\nvar stream = through2(function (chunk, enc, cb) {\n    assert.strictEqual(ct1, ct2);\n    ct2++;\n    cb(null, chunk);\n});\nstream.pipe(process.stdout)\nfor (var i = 0; i < 160000; i++) {\n    stream.write(String(i % 10));\n    ct1++;\n}\n```\nIt seems through2 will always call the callback synchronously (at least when it's used the way I'm using it with last-line-stream). In that case, I just wonder if https://github.com/jamestalmage/ava/commit/328638db98e18e1e957dd86b9d0410565451f898 makes what's happening any clearer. Also, prior to that, I was using stream.push() to bypass the tracker, which is probably considered an abuse of the stream API.\n. All squashed up. What do you guys think, are we good to merge?\n. Done => #420 \n. Awesome!\n. I just tried your test code, and it just hangs indefinitely (which makes sense - AVA is waiting for you to call end). What output are you getting?\n. Before: \n\nAfter: \n\n. Before:\n\nAfter:\n\n. Before:\n\nAfter:\n\n. > Love love love this!\n:smile: \nIf you could review the issues and PR's on stack-utils that would be a big help.\n. I'm also wondering if we should do something like this:\n``` js\nvar opts = {};\n// If debug is enabled, don't delete stack trace lines.\nif (!debug('ava').enabled) {\n  opt.internals = StackUtils.internals().concat([ \n     // ...\n  ]);\n}\n```\n. If debugging is enabled, why not just print everything?\n. > Maybe we can remove the AssertionError?\nSure.  stack-utils actually chops the first line of the stack-trace (the message) off anyways. I actually manually add it back in beautify-stacktrace.js. The issue is the reporters only show that for assertion errors. unhandledRejections and uncaughtExceptions would be different. We would just need to rework the reporters so it's their responsibility to print the message. \n. > only display the logs when the test failed\n\n\nI'm strongly against this one as it's very irritating when obvious & simple things (like console.log()) don't work as expected.\n\n\nYou are probably right. I was more thinking this for t.log. My only irritation is that if I put console.log in a production file that is called by a lot of tests, I probably only want to see the output of the failing ones. Still - probably best not to.\n. > Do we really need this (t.log) if we manage to route console calls correctly?\nMy thought was that it would be similar to console.log, but hidden unless the test fails.\n\ncluster\n\nThat only gives you per-process resolution. We need domains to narrow it down per-test\n. > I actually thought about running a worker per test.\nIt's crossed my mind, and I definitely don't think it's insane, but probably not very performant.\n. > I actually thought about running a worker per test.\nCould be cool as a configurable option. Turn it off for faster testing on the local dev machine. On for your CI build so you ensure you are writing truly isolated tests.\nSee https://github.com/jamestalmage/forking-tap\n. One downside to this is that it hides the async nature of your code from you. Take the following:\n``` js\nvar l;\nvar r;\nfunction addLater(testName, a, b) {\n  console.log('saving values ' + testName);\n  l = a;\n  r = b;\n  Promise.resolve().then(function() {\n    console.log('adding values ' + testName);\n    return l + r;\n  });\n}\ntest(async t => t.is(await addLater('test 1', 3, 4), 7));\ntest(async t => t.is(await addLater('test 2', 5, 6), 11));\n```\nWith this proposal, the above will give you a nice, seemingly synchronous log:\nsaving values test 1\nadding values test 1\nsaving values test 2\nadding values test 2\nWhen what will almost certainly be happening is:\nsaving values test 1\nsaving values test 2\nadding values test 1\nadding values test 2\n. One downside to this is that it hides the async nature of your code from you. Take the following:\n``` js\nvar l;\nvar r;\nfunction addLater(testName, a, b) {\n  console.log('saving values ' + testName);\n  l = a;\n  r = b;\n  Promise.resolve().then(function() {\n    console.log('adding values ' + testName);\n    return l + r;\n  });\n}\ntest(async t => t.is(await addLater('test 1', 3, 4), 7));\ntest(async t => t.is(await addLater('test 2', 5, 6), 11));\n```\nWith this proposal, the above will give you a nice, seemingly synchronous log:\nsaving values test 1\nadding values test 1\nsaving values test 2\nadding values test 2\nWhen what will almost certainly be happening is:\nsaving values test 1\nsaving values test 2\nadding values test 1\nadding values test 2\n. My big comment above is directly related to this one.\n. My big comment above is directly related to this one.\n. It's just so very surprising to people, I think it's worth trying someday. \nIt definitely could get the low-priority label though\n. You frequently use console.log outside your test code (where you wouldn't have access to t).\n. > Running files in parallel will also be gone when run in the browser.\nNot sure why that would need to be true. Just open a tab per test-file (setting this flag).\n. > Running files in parallel will also be gone when run in the browser.\nNot sure why that would need to be true. Just open a tab per test-file (setting this flag).\n. Yep, the only issue would be:\njs\nbefore(t => setup database())\n. Yep, the only issue would be:\njs\nbefore(t => setup database())\n. Maybe add setup/teardown methods that maintain the before/after symantics regardless. They wouldn't be able to participate in t.context, or t.context would need to be serializable\n. Maybe add setup/teardown methods that maintain the before/after symantics regardless. They wouldn't be able to participate in t.context, or t.context would need to be serializable\n. That picture is awesome\n. I certainly don't think it should be the default behavior. But for certain situations (polyfills / etc) I think the benefits are so huge, they outweigh the downside.\n. I don't see why it should be considered insane. There are real downsides to performance, so I don't think it can ever be the default, but there are also real benefits.\nPerhaps the solution might be to use a fork method sugar:\n``` js\nfunction installPolyfill() {\n}\nfunction setup() {\n  // other setup??\n}\ntest.before.fork(setup); // run as before in this process, and every forked one.\ntest.before(installPolyfill); // only run in this process, not in forked ones.\ntest.fork('pre/post install check 1', t => {\n  // isolated process\n  // setup has already run, but installPolyfill has not\n  t.notOk(check1());\n  installPolyfill(); \n  t.ok(check1());\n});\ntest.fork('pre/post install check 2', t => {\n  // isolated process\n  // setup has already run, but installPolyfill has not\n  t.notOk(check2());\n  installPolyfill(); \n  t.ok(check2());\n});\ntest('foo', t => {\n  // shared process with 'bar'\n  // setup and installPolyfill have both run\n});\ntest('bar', t => {\n  // shared process with 'foo'\n  // setup and installPolyfill have both run\n});\n```\nThis gets around performance issues by only creating additional forks for the few tests where complete isolation is necessary (I think pre/post polyfill checks is a good example) - the majority of your tests can stay fast.\nI think the API also helps make sense of the \"single before\" vs \"before per process\" ambiguity discussed above.\nI still think we have higher priorities, but I would like to see this implemented eventually.\n. I don't see why it should be considered insane. There are real downsides to performance, so I don't think it can ever be the default, but there are also real benefits.\nPerhaps the solution might be to use a fork method sugar:\n``` js\nfunction installPolyfill() {\n}\nfunction setup() {\n  // other setup??\n}\ntest.before.fork(setup); // run as before in this process, and every forked one.\ntest.before(installPolyfill); // only run in this process, not in forked ones.\ntest.fork('pre/post install check 1', t => {\n  // isolated process\n  // setup has already run, but installPolyfill has not\n  t.notOk(check1());\n  installPolyfill(); \n  t.ok(check1());\n});\ntest.fork('pre/post install check 2', t => {\n  // isolated process\n  // setup has already run, but installPolyfill has not\n  t.notOk(check2());\n  installPolyfill(); \n  t.ok(check2());\n});\ntest('foo', t => {\n  // shared process with 'bar'\n  // setup and installPolyfill have both run\n});\ntest('bar', t => {\n  // shared process with 'foo'\n  // setup and installPolyfill have both run\n});\n```\nThis gets around performance issues by only creating additional forks for the few tests where complete isolation is necessary (I think pre/post polyfill checks is a good example) - the majority of your tests can stay fast.\nI think the API also helps make sense of the \"single before\" vs \"before per process\" ambiguity discussed above.\nI still think we have higher priorities, but I would like to see this implemented eventually.\n. > Of course at that point you could also make separate test files for the one or two forks you need.\nTrue. Though if that number is 6/7 this has a lot more appeal. Especially if there is some amount of code sharing (helper functions, etc)\n. We've talked it over and decided that a --no-babel flag is an acceptable (if possibly temporary) fix. It's going to slow things down a lot. Especially for npm@2 users. Any PR should include documentation warning that npm@3 is highly recommended with that flag.\n. @ariporad :+1: I thought you might :smile: \n. Should we have explained how to disable that behavior?\n$ DEBUG=ava ava test/foo.js\n. Good idea, my only concern is this implies a stable reporter API, and I don't think we're ready to commit to that yet.\n. I would prefer to have to install extra stuff / pipe to get the mini reporter.\n. Yeah, that is what I was thinking, but how should we make that decision?\n- process.stdout.isTTY will work for AppVeyor, but not Travis.\n- is-ci seems more universal. But do we still want to detect isTTY?\nI was thinking:\n- if not-a-tty - default to tap\n- else if is-ci - default to verbose\n- default to mini\nPros:\n- Defaults to mini on your own machine.\n- You don't need to specify the tap reporter when piping. It just works.\n- Travis looks good with the verbose reporter.\nCons:\n- It's a little complicated. People might get confused. (However, tap does basically the same thing and it doesn't seem to be a problem).\n- Runs on AppVeyor (and probably quite a few other CI's) would default to tap.\n. My only gripe with that would be tap support:\nava --tap | spec \n vs\nava | spec\nNot really a big deal (I think I will prefer our reporters in most cases).\n. works for me.\n. > always prioritize --tap over isCI check\n:+1: \n. --fail-fast\nNot sure if I like -f, since that is so widely used as force (though I'm not sure what people would think that means in the context of a test runner, so maybe not so confusing).\n-b for bail-out, but we don't use that term.\n. better: https://travis-ci.org/jamestalmage/option-chain/jobs/102697434#L754\n. > Wonder if we somehow could hide the Bluebird specific methods to the user.\nI had that thought. Just a wrapper. Not sure what the performance impact would be. But worth looking into.\n\n... or does the whole chain need to use Bluebird to get the long stack traces\n\nThat I don't know. Worth testing.\n\nLong stack traces for the test themselves aren't that useful\n\nThat's probably true. I noticed while incorporating stack-utils. In that case I was writing a fixture that created an entire long stack trace inside the test file. So the typical user experience is probably very different.\n. @thisconnect - which version of AVA are you on? The last few have all included major performance boosts.\n\nalways returning a Promise\n\nThat is only necessary for async tests (if that is what you have, great!). If you have synchronous tests, just let them end synchronously.\n\ntest.serial\n\nThat will have major performance implications, especially if your tests are truly asynchronous. The main way to realize faster tests with AVA is to embrace the asynchronous tests that pass in isolation. Remember, any tests created with test.serial(...) will run before everything else. If only a few tests depend on others. Mark only those as serial.\nIt's really better if your tests don't depend on each-other. Maybe you could use something like unique-temp-dir to create individual fixtures for each test.\n\ncost of forking\n\nRecent versions of AVA have gotten the cost per fork really low. Your test suite is pretty small, so you may not see an improvement. But IMO you shouldn't choose a monolithic test file just to save a few milliseconds.\n\nnot using Babel\n\nYou should (in your tests). You are already paying the price for it. AVA 0.10.0 moves transpiling to the main thread, so you only pay that price once, still - there's no reason not to take advantage.\n. I know I'm a bit late to the party, but I had just a little bit of follow up.\n\nI usually inline the text and lcov reporters:\njson\n{\n  \"test\": \"nyc --reporter=lcov --reporter=text ava\"\n}\nbcoe/nyc#137 is part of the reason why. nyc report can fail if anything happens to the cache between running the tests and generating reports. Safer to just do it all at once.\nSince the lcov reporter actually generates html reports in addition to lcov.info. I use the html report locally, and lcov.info in an after_script for Travis/Coveralls:\nyaml\nafter_script:\n - 'cat ./coverage/lcov.info | ./node_modules/.bin/coveralls'\nI prefer after_script to after_success because it's still helpful to have coverage info when reviewing WIP PR's that might still have a few failing tests (or if the project is plagued by a few finicky tests).\n\nThe --babel flag never made it into nyc. Instead we adopted a --require flag (very similar to AVA's).\nIn this case however, I wouldn't use it. Instead I would add it to AVA's config in package.json:\njson\n{\n  \"ava\": {\n    \"require\": \"babel-core/register\"\n  }\n}\nThe advantage of configuring for AVA instead of NYC is that you can just run ava from the command line (assuming a global install) - and it will pick up that config. Otherwise you would be required to add --require babel-core/register on the command line every time.\n. I think it would be really helpful if, along with the recipe, there was a \"conversion cheat sheet\" (maybe in a table or something). That listed all the commands:\n| node-tap | tape | ava | notes |\n| --- | --- | --- | --- |\n| t.equal | t.equal | t.is |  |\n| t.throws | t.throws | t.throws | ava also accepts a Promise in addition to functions. |\n| ... |  |  |  |\n| ... |  |  |  |\nI'm not sure if it's necessary to have separate node-tap / tape columns or not.\n. I think @kentcdodds may already have a good start on this. He's apparently been teaching AVA with React in class. Not sure if he's got time to make an actual PR, but given his extensive work in this area, it would be good if he was involved.\n. @MoOx - Not saying you shouldn't be the one to do the PR, just that @kentcdodds should be looped in.\nPlease let us know about your blog posts when they go live!\n. It's kind of covered by the code coverage recipe, but it might be good to have a simpler recipe that didn't include that.\nSome stuff from this comment would apply here as well.\n. See https://github.com/jamestalmage/__ava-0.8-with-babel-6 for a working example\nRepository name is a misnomer now - it uses ava@0.10.0.\n. > commenting this out does something!\nYeah - that is what I was thinking, but you raise a good point about that being an odd side-effect. If you have a different idea, this is the place to share.\n\nDo you have plans for the general case of a custom register hook.\n\nI think maybe --no-babel works in that scenario. My only concern is that leaves everyone else out in the cold when it comes to our caching and \"transpile in the main thread\" performance enhancements.\nIf anyone's got a list of alternative transpilers they wish they could use with AVA. I'd appreciate you sharing. Even better if you have in profiling information on those transpilers (require time via time-require and actual transpiling speed).\nThat said, this feels much more doable than providing performance enhancements for everyone. \n. I am :+1: for @spudly's proposal, with a few additions:\nI think it would be great if we allowed extension of existing configs (the default AVA one, or the users .babelrc). The keywords don't allow for this, instead I think we allow this:\njson\n{\n  \"ava\": {\n    \"babel\": {\n      \"extends\": \".babelrc\",\n      or\n      \"extends\": \"default\",\n      \"plugins\": [],\n      \"presets\": []\n    }\n  }\n}\nIdeally, we would not write code to merge plugins/presets, and instead use Babel's internal 0ptionsManager#mergeOptions method. Unfortunately, I don't believe OptionsManager has a frozen API, nor is it documented, which makes this a bit problematic. Perhaps we could discuss this with the Babel team (exposing a documented API, and/or adding tests to ensure the API remains relatively stable).\n. > This is why micro-libraries are reeeeally nice :-) Maybe someone (me?) could extract that logic into a well tested microlib that both tools use?\n:+1:  - It might be best just to make a PR to Babel that extracts it into a microlib within the Babel monolith repo (that would likely be what the Babel team wants).\n. > Would \"babel\": {} work?\nSounds reasonable. Only one way to find out for sure ;)\n. I think once #706 and #708 land, this issue is should be closed. The only issue not handled at that point is disabling babel. I think that is problematic, and will open a new issue to discuss.\n. https://phabricator.babeljs.io/T2892#71700\n. LGTM\n. I'm :-1: as well, for all the reasons discussed above.\n. I think it's going to end up cleaner / easier to understand - with better separation of concerns. Also has implications for using domains for logging / error tracking. This is real early stuff. Not ready for serious review. Just wanted you aware of what I was doing\n. closing in favor of #466 \n. Can you put this in a repo I can clone and reproduce?\n. >  I kinda feel the API itself should guarantee err is always an error.\nIt's supposed to already: https://github.com/sindresorhus/ava/blob/master/lib/test.js#L132-L138\nThere's a bug somewhere\n. > watch mode\nAVA is at an extreme disadvantage in watch mode until we implement our own watch mechanism. That's certainly on the roadmap, but we've been punting on it in favor of other improvements. Not sure if it's time to tackle that yet or not.\n. I have confirmed this commit caused a major performance hit. I think I've identified the problem and I am working up a solution.\nSee twada/empower-core#7\n. This relies on some changes to empower-core.\nhttps://github.com/twada/empower-core/pull/7\n. Further splitting that test suite into multiple files (https://github.com/beaugunderson/emoji-aware/pull/1):\n```\n  4363 passed\nava --verbose  9.28s user 0.65s system 458% cpu 2.164 total\n```\n. > Too bad jamestalmage@4e23943 was slow. It was more readable.\nYeah, unfortunately it is a big offender in this scenario.\n. Also a technically breaking change (though not one I am really concerned about):\njs\nt.is(1 + 1, 2);\nNo longer returns a Promise. \nOnly t.throws and t.doesNotThrow will return a promise.\n. @kentcdodds, et. all\nJust for comparison - I also created a branch converting emoji-aware to mocha\nAVA now ekes out slightly better performance:\nava --verbose  9.76s user   0.71s system    465% cpu    2.252 total\nmocha          2.07s user   0.31s system     96% cpu    2.460 total\nHowever, it's obvious the only reason AVA is winning is that it is leveraging multiple processes. Hopefully this means there are still lots of opportunities to make AVA even faster.\n. All working now, using benchmarks from jamestalmage/emoji-aware#no-assertion-plans:\nMaster:\nava --verbose  6.86s user 0.56s system 439% cpu 1.689 total\nava --verbose  6.78s user 0.55s system 438% cpu 1.669 total\nava --verbose  6.73s user 0.55s system 432% cpu 1.681 total\nThis PR:\nava --verbose  5.25s user 0.53s system 432% cpu 1.337 total\nava --verbose  4.97s user 0.50s system 419% cpu 1.303 total\nava --verbose  5.05s user 0.50s system 426% cpu 1.301 total\n. Using the following benchmark (no assertions - just tracking runner performance):\n``` js\nimport test from './';\nfor (var i = 0; i < 10000; i++) {\n    test('test' + i, t => {});\n}\n```\nMaster:\nnode ./cli.js benchmark.js --verbose  3.15s user 0.31s system 104% cpu 3.318 total\nnode ./cli.js benchmark.js --verbose  2.76s user 0.28s system 103% cpu 2.937 total\nnode ./cli.js benchmark.js --verbose  2.73s user 0.28s system 103% cpu 2.911 total\nThis PR:\nnode ./cli.js benchmark.js --verbose  1.44s user 0.25s system 107% cpu 1.573 total\nnode ./cli.js benchmark.js --verbose  1.40s user 0.25s system 106% cpu 1.544 total\nnode ./cli.js benchmark.js --verbose  1.38s user 0.24s system 108% cpu 1.499 total\n. This also seems to provide a slight performance boost for async tests:\n``` js\nimport test from './';\nfor (var i = 0; i < 10000; i++) {\n    test('test' + i, t => {return Promise.resolve()});\n}\n```\nMaster:\nnode ./cli.js benchmark.js --verbose  2.84s user 0.28s system 103% cpu 3.010 total\nnode ./cli.js benchmark.js --verbose  2.86s user 0.28s system 103% cpu 3.033 total\nnode ./cli.js benchmark.js --verbose  2.82s user 0.28s system 104% cpu 2.980 total\nThis PR:\nnode ./cli.js benchmark.js --verbose  2.68s user 0.28s system 103% cpu 2.843 total\nnode ./cli.js benchmark.js --verbose  2.69s user 0.28s system 104% cpu 2.854 total\nnode ./cli.js benchmark.js --verbose  2.72s user 0.27s system 104% cpu 2.880 total\n. Mixed sync/async:\n```\nimport test from './';\ntest.beforeEach(t => {});\ntest.afterEach(t => {});\nfor (var i = 0; i < 10000; i++) {\n    test('test' + i, t => {return Promise.resolve()});\n}\n```\nMaster:\nnode ./cli.js benchmark.js --verbose  4.78s user 0.43s system 102% cpu 5.075 total\nnode ./cli.js benchmark.js --verbose  4.64s user 0.42s system 102% cpu 4.953 total\nnode ./cli.js benchmark.js --verbose  4.75s user 0.43s system 102% cpu 5.041 total\nThis PR:\nnode ./cli.js benchmark.js --verbose  3.56s user 0.38s system 103% cpu 3.797 total\nnode ./cli.js benchmark.js --verbose  3.56s user 0.38s system 103% cpu 3.791 total\nnode ./cli.js benchmark.js --verbose  3.62s user 0.39s system 103% cpu 3.888 total\n. > Would be nice if we could follow up later with ways to simplify it.\nAgreed. Getting a good benchmarking suite (#460) would be really helpful as well. \n. Once merged, we should all dogfood this on existing projects before release, (maybe deploy with a next tag for a while as well). There are enough changes here that our regression exposure is pretty large. I've added pretty exhaustive tests, but we should still thoroughly vet it.\n. > This also catches the case of forgetting to remove an .only().\nI have a really crazy idea. Should the use of .only() cause a non-zero exit code? Or the ability to configure it to exit with a non-zero exit code in that case?\nWe could probably create a linter rule that handled warning about .only() usage, but I think it would be so nice if AVA just handled it for you.\n. I've thought about this some more, and decided I am against the idea.\nIf .only is used, I only want to see output for that test. Reporting all the skipped tests is just going to slow things down with lots of IPC calls communicating test titles I don't care about. Additionally, if I am using the --verbose reporter, which test failed gets lost in a sea of \"skipped\" logs for tests I don't care about.\nBasically, this proposal is kinda antithetical to the entire purpose of .only. Focusing in on just a few tests for quick feedback while tackling a specific problem.\n. I think it's fine for us only to report .only tests. I just don't see the value of all those skip reports. It's pretty easy to figure out what happened when your tests suddenly run faster and your count drops to 1.\nIt might be interesting to offer a --disallow-only flag that could be used on your CI server to ensure .only didn't make it into master. Better yet, automatically detect and disallow using is-ci (with --allow-only to override?).\n. Yeah, I get that (and tend to agree). Not everybody lints their code though. I think we had a similar discussion regarding t.end for non-cb tests and decided it was a better user experience to build in the error.\nI am pushing to silently ignore all tests not marked with .only, and that suggestion was mainly offered as possible way to mitigate concerns that might raise.\n. I believe @vdemedes modeled our output after node-tap (which I believe uses the draft v14 spec)\n. I believe v14 is backward compatible in most ways. Any additions were done as comment directives. node-tap still prints v13 because v14 isn't officially recognized yet. But tap-mocha-reporters recognize many v14 directives.\nAs I recall the v14 draft was born from the extensions node-tap added to the spec.\n. https://github.com/TestAnything/testanything.github.io/pull/36\n. > the TAP output that AVA produces will likely break most reporters in some way\nNot really. We are following the output of the two most popular tap producing libraries on Node (node-tap and tape). From the first line of that PR:\n\n[v14] seeks to ratify existing behavior of TAP harnesses and producers\n\nIn other words - codify the existing reality.\n\nit stands to reason that v14 will be backwards compatible to v13, but not the other way around\n\nNot really. v14 output should be interpreted just fine by v13 consumers (though perhaps not as pretty as it could).\n. > maybe it's just an opportunity to get faucet up to v14 spec\nIt's been 2 years since a faucet release. That it works at all is a testament to good backward compatible design built in to the TAP protocol.\nThat said - if the problem is just the # tests N line, that could be removed. (Though I think that's actually something we emulated from tape instead of the spec - so it is a bit weird it doesn't work with faucet as it's from the same author).\n. Yep. That is the currently defined behavior. Your use case makes sense, so maybe we should change it.\nThe current behavior is useful in other situations though (if after actually requires successful execution of your tests, etc).\nPerhaps a finally test type is in order (one that always runs, no matter the results from before)\n. Either that or just change after to do what you expect. Let's see what everyone thinks.\n. What if it's always, and chainable:\n- test.always - even if a beforeEach fails.\n- test.afterEach.always - even if a test fails.\n- test.after.always -  the OPs request.\nNot sure the first one makes sense.\n. I'm not sure Sequence will need to be modified.\nSequence takes a list of tasks and a boolean bail argument.\nWe currently build a list like this:\n+ bailing sequence\n  - beforeEach\n  - beforeEach\n  - test\n  - afterEach\nI think you could just wrap that in an non-bailing sequence:\n+ non-bailing sequence\n  + bailing sequence\n    - beforeEach\n    - beforeEach\n    - test\n    - afterEach\n  - afterEach.always (afterEach is promoted to the non-bailing wrapper if it has a `always` modifier)\n. This sequence building takes place in test-collection.js.\nChainable methods are defined here.\n. I think you are on the right track.\n. Should an afterEach.always execute if a beforeEach failed? Or should the always only apply if we made it to the actual test function?\n. > If for some reason your after hook should only run if tests succeed then you can keep track of that yourself\nThere are plenty of scenarios where you wouldn't want afterEach running, particularly if you are using mocks and want to call someMock.verify().\n\nshould always just be the default behavior? \n\nI am not sure. That certainly makes sense in a serial test runner, where it is very likely you are using afterEach to cleanup some state so it doesn't mess with the next test. For concurrent tests however, there really shouldn't be a whole lot of cleanup todo - you need to avoid messing with state anyways.\n. > If for some reason your after hook should only run if tests succeed then you can keep track of that yourself\nThere are plenty of scenarios where you wouldn't want afterEach running, particularly if you are using mocks and want to call someMock.verify().\n\nshould always just be the default behavior? \n\nI am not sure. That certainly makes sense in a serial test runner, where it is very likely you are using afterEach to cleanup some state so it doesn't mess with the next test. For concurrent tests however, there really shouldn't be a whole lot of cleanup todo - you need to avoid messing with state anyways.\n. Some mock libraries allow you to set up expectations, run your test, and then verify all expectations were actually called. Think of the verification step as the mocking equivalent of t.plan(). One example is nock. They have methods like nock.isDone() (or nock.done() which is the throwing equivalent).\n\nI'm just concerned that users may use after for cleanup where they really should have used always.after\n\nYeah, that's a potential problem, but I'm not sure how to avoid it.\n\nRE: naming\n\nafter and always.after is pretty clear naming once you understand the default behavior of after. I'm not sure of a better naming scheme if we flip the default behavior. afterEach.success?\n. Some mock libraries allow you to set up expectations, run your test, and then verify all expectations were actually called. Think of the verification step as the mocking equivalent of t.plan(). One example is nock. They have methods like nock.isDone() (or nock.done() which is the throwing equivalent).\n\nI'm just concerned that users may use after for cleanup where they really should have used always.after\n\nYeah, that's a potential problem, but I'm not sure how to avoid it.\n\nRE: naming\n\nafter and always.after is pretty clear naming once you understand the default behavior of after. I'm not sure of a better naming scheme if we flip the default behavior. afterEach.success?\n. I'm not understanding the last two comments.\n. nevermind - GH wasn't showing them inlined. I see what you are saying now.\n. > Can you make the title the same as this?\nI think I made all the changes you asked for. Make sure I didn't get :arrow_up: backwards though.\n. just saw one more typo\n. fixed.\n. This is awesome. Much cleaner than my mess. I need to run a few more benchmarks, but it seems really performant too.\n. Uh oh :cry: \n``` js\n// many-async.js\nimport test from 'ava';\nfor (var i = 0; i < 10000; i++) {\n  test.serial('test' + i, () => {\n    return new Promise(resolve => setImmediate(resolve))\n  })\n}\n```\nThis PR:\nava --verbose many-async.js  7.15s user 0.53s system 113% cpu 6.752 total\nMaster:\nava --verbose many-async.js  3.76s user 0.46s system 123% cpu 3.421 total\n. ``` js\n// alternating-async.js\nimport test from 'ava';\nfor (var i = 0; i < 10000; i++) {\n  if (i % 2) {\n    test.serial('test' + i, () => {\n      return new Promise(resolve => setImmediate(resolve))\n    })\n  } else {\n    test.serial('test' + i, () => {})\n  }\n}\n```\nThis PR:\nava --verbose alternating-async.js  4.52s user 0.44s system 119% cpu 4.158 total\nMaster:\nava --verbose alternating-async.js  2.94s user 0.42s system 128% cpu 2.608 total\n. I think Promise.each in bluebird has some magic we lose out on with this implementation.\n. Hmm. Is there a way to create a promise at runtime that does not use longStackTraces, yet keep longStackTraces turned on for user code?\n. http://bluebirdjs.com/docs/api/promise.longstacktraces.html\n\nLong stack traces cannot be disabled after being enabled, and cannot be enabled after promises have alread been created\n\nWith longStackTraces disabled, I am getting:\nThis PR:\nava --verbose many-async.js  2.14s user 0.36s system 136% cpu 1.834 total\nMaster:\nava --verbose many-async.js  2.24s user 0.36s system 134% cpu 1.930 total\nThis PR is faster (though I'd say a tenth of a second over 10,000 tests is fairly negligible). I think the decision on what to do here ultimately comes down to how much we want to keep longStackTrace support.\nI am of two minds:\n1. Merge this PR, and work towards a documented --debug flag that would:\n   - Turn on longStackTraces\n   - Set stackTraceLimit to Infinity (meaning we could pick a lower number by default).\n   - Set plan stack limit to Infinity\n   - Reduce our list of internals that we have stack-utils filter.\n   - Somehow enable the node debugger?? (maybe that's only possible with a single file and the profile scirpt? I don't know, and googling \"Node debugging forked processes\" is not providing answers).\nThe current debug strategy (DEBUG=ava), is a bit annoying because we also spit out ipc and time-require stuff that is helpful for contributors, but not really for users. I guess we could still use the debug module like two different flags: (DEBUG=ava-internals vs just DEBUG=ava).\n2. Stick with the implementation that we have, since it is faster over a wider array of situations, and leave longStackTraces on. This keeps things simple.\nI think I would prefer 1. Simplicity is good, but longStackTraces have a real impact on performance (for both implementations).\n. I understand the wildcard usage. I don't think there is anything special about the - vs : separator. But, : is probably a better choice since that is the convention they have established.\nI would just prefer DEBUG=ava* not include time-require stuff. So maybe just don't include the ava prefix for that? Just call it time-require? That is a bit against convention, but time-require is not using the debug module internally, so I think we can get away with it.\n:+1: for finer grained debug scopes (ava:ipc, etc).\n. You can now launch a subset of the suite:\nnode bench/run.js concurrent/sync.js serial/sync.js -- concurrent/sync.js -- serial/sync.js\nNote the -- separator. The above would be the same as benchmarking all three of the following commands.\nava concurrent/sync.js serial/sync.js\nava concurrent/sync.js\nava serial/sync.js\nAlso if you are benchmarking a suite that should fail, you must add the --should-fail flag in that group\nnode bench/run.js concurrent/sync.js -- --should-fail other/failures.js\nThe above benchmarks two commands, but expects the second one to fail.\n. I think this gets us a good start.\n@sindresorhus @vdemedes - Any other benchmark suites that should be added?\n. > I thought stack-utils converted all paths to forward slashes?\nIt should, I need to investigate.\n. Yep. Except for here, where Node 0.10 actually behaves differently (I had to modify the test). I was assuming it was just being flaky again when I opened this. Still, it would be nice to hunt down the problem.\n. > A case could be made though that --watch should simply never exit\n:+1: That is what I would expect. If I launch with watch and there are initial problems, I should just be able to fix them and save. What is the argument for the behavior in this PR?\n. > The delay is 10 milliseconds. If further changes are detected the watcher again delays by 10 milliseconds. Currently this delay is not configurable\nI agree it should not be configurable, but is 10ms long enough? What happens in popular IDE's when you have multiple large files open and you Save All? How fast does chokidar pick those up? (Especially on slow systems... Especially on slow Windows systems). Would a 50ms/100ms debounce really be noticeable? Would a higher value provide any benefit? I am not disagreeing with the chosen value, I truly don't know the answer, and I am just suggesting we put some thought into it.\nThe easiest solution might just be to go with 10 ms and wait for reported problems.\n. .nyc_ouput and coverage should almost certainly be in the default excludes. It wouldn't make a lot of sense to watch with coverage enabled, but we should protect against it.\nMaybe try to detect if we are wrapped with NYC and issue a warning in watch mode?\n. RE: include / exclude patterns:\nMight we just provide a single config option with a ! prefix for exclusion?\ndefaults:\nava --watch --watch-files={,lib/**/}*.js test*.js test/**/*.js\nwith exclusion\nava --watch --watch-files=**/*.js --watch-files=!node_modules/**\n. AVAError was intended to be used to avoid printing stack traces when they were unlikely to be helpful (i.e. the \"no tests in file\", or \" forgot to require AVA\" errors). I don't think this change will be that controversial\n. > How about ava test/*.js --watch=src/*.js --watch=!src/bundle.js\nThe issue with that is that the following is ambiguous:\nava --watch test/foo.js\nDoes it mean watch only test/foo.js and run the default test setup? Or only run test/foo.js and watch all the normal files?\n. > That's why we define types here,\nThat won't help here. They are both strings.\n\nAlso why I much prefer using --key=val instead of --key val\n\nI agree. But unless we parse args on our own, I don't think we can enforce that (nor do I think we should - it would confuse the people who prefer spaces).\n. I get that, what I want to be able to do is run --watch in focus mode, so:\nava --watch test/foo.js\nOnly run test/foo.js, but do it in watch mode. Having to specify:\nava --watch={src,lib,scripts,}**/*.js test/foo.js\nWould be annoying. I am probably going to set up what I want watched in package.json anyways, so I'd prefer --watch be a boolean.\nMaybe we add a --sources flag? I mean that's what we are really after right? We always want to watch the tests, if they change, we almost certainly want to rerun. For watch, we need to tell AVA what additional sources need to be looked at above and beyond the tests.\n. --sources would mirror nicely into package.json as well. (Specifying \"watch\": \"pattern\" might seem to suggest that you wanted it to default to watch mode).\n. > It would definitely simplify the logic exercised by the watcher in order to determine whether a changed file is a test file\nCouldn't you just run through all the parent directories and compare them to the glob?\n/foo/bar/baz/quz.js \n/foo/bar/baz\n/foo/bar\n/foo\n/\nIn all likelihood, they are only changing a few files at a time, so it's not like that extra work is going to cost us performance.\n. I think you've got a solid public API in  place. I think we should merge and open am issue regarding cleaning up the internals\n. This is just fantastic work @novemberborn! Well done Sir!\n. I am not really sure why the last commit is more performant. It would obviously be more performant to extend the prototype if we were creating many Runner instances, but we only create one per process.\nTwo comparisons before the last commit:\n\n\nafter the last commit:\n\n. @sindresorhus - made all the recommended changes except the global.Promise = one.\n. OK. Done now.\n. I think it looks nice. I like the Nav. I like that we get it almost for free just by generating a Readme.\n@zckrs - You made a number of edits (moving things in to doc folder, etc). That I don't understand why they were necessary. I'm assuming that's required by Docpress? That would be my one big objection. I don't want to spend time trying to make my Readme acceptable to some third party post-processor.\nMy gut is that it's got potential, but maybe a bit premature to start focusing on this / letting it take up the maintainers time.\n. Under what conditions does stack-utils return null? That seems like a poor choice. Sam, would you mind taking a look and seeing if it wouldn't be better to modify stack-utils? Maybe it should just return an empty string instead of null?\nPart of any stack-utils PR would need to include checking how changes would affect node-tap.\nSorry for complicating this. I'd do it myself but I'm slammed for the next few weeks\n. @jarrettmeyer Do you have time to follow up on the feedback? Would like to merge this soon.\n. Conclusion #4 from that article: \n\nSetting application-specific settings as environment variables globally or in your current shell is an anti-pattern\n\nThat said, not sure why this would not work. The NODE_PATH environment variable must be being ignored somehow.\n. > If it's a beginner-friendly issue\nI honestly don't know if it is. Feel free to give it a go. I would start by checking that neither this bit or require-precompiled are the problem.\n. It should be noted that it really doesn't slow things down unless bluebird long stack traces are turned on.\nIn #495 we discussed merging it on the condition that we would turn off long stack traces, but now we are having second thoughts on that as well.\n. Here is the difference with longStackTraces still on in both:\n\n. Note that most of these benchmarks have 10,000 tests.\nAt that point, I think anything under a half second is pretty negligible difference. I'm more concerned about the 3 second difference.\n. Turning shortStackTraces off  #495 is generally faster, but not by much.\n. > If we can do things directly in AVA without any negative side-effects, I think we should\nI agree.\n. @jarettmeyer on Gitter:\n\nI have a question about #517 . I am thinking of adding this check to TestCollection's add function.\nNext, a test needs to be able to return a warning, instead of an error. As I can tell, the runner does not support returning warnings, only errors. Is there another place I should look for warnings?\n\nI think we have two options:\n1. Create a new custom ipc event, especially for warnings. This would involve adding appropriate Logger hooks, etc.\n2. Just print warnings to stdout - it will get displayed by AVA.\nI think 1.\n. > It kind of feels lately, that we add more bloat to AVA, instead of fixing/improving monsters\nI can get behind that sentiment.\n. I take blocked to mean \"some other issue is holding this up\". I would just mark this low-priority.\n\nI do think this should land before 1.0.0, though, as it's a breaking change.\n\nIt's not a breaking change if it's just a warning.\n. > Are there any concerns around cleanup code not running when tests are aborted?\nI would say yes. We could make it configurable. That would mean watch support now adds three cli flags - which seems a lot.\nOr - the keyboard command could trigger the immediate rerun. I like this one I think. I only care about this on projects with really long running test suites.\n. If you have all concurrent tests then this behavior is essentially useless. Tests will have begun by the time you send an abort. So if we follow up with afterEach then after its really no different than a standard test run.\n. I think I know what you are asking, and I am not objecting to the concept in general, I am just saying I don't see a practical way of accomplishing it as currently proposed. \nHow are you planning on \"aborting\"? What do you do when you receive the \"abort\" message from the parent process? You can't interrupt tests that are already launched. You could elect not to execute the next hook, but so far everyone seems to be saying they want after/afterEach to run. You could avoid executing the next serial test, but then this is only really useful for people using a large number of serial tests (which, if we do a good job explaining the  core concepts of AVA, should be small subset of our user base).\nIMO the easiest way to get at your basic request (\"I want to see new tests immediately starting\") is to just kill the child processes. This is problematic if there is cleanup code in after / afterEach that means test fixture state is left uncleaned. Having your tests rely on system state is a terrible idea anyways, so I would be fine with just killing child process - but the consensus so for seems to be that we should still run those post test hooks.\nThat's how I came to the conclusion that we should allow either:\n-  A config option that tells AVA your after, afterEach are not needed for cleanup, and that processes can be killed immediately to run the next watch cycle.\nor\n- to avoid adding yet another config option, allow the keyboard shortcut to force that condition\n. There is no such thing as a \"next concurrent test\". Every concurrent test in the file is launched in a single turn of the event loop, once you have kicked off the \"run the concurrent tests\" phase, there is no turning back. Any ipc message received during initiation of concurrent tests will block until all concurrent tests have been started.\nThings are a bit different if they've got a lot of serial tests (especially async ones). In that case there is an opportunity to interrupt execution flow. I am just not sure it's worth the effort for what should be a niche situation (people should prefer concurrent). That said, large groups of async serial tests are likely to be the slowest part of your test suite, so maybe. I think we should just wait until someone presents a test suite where this would be of significant benefit.\n. This is not going to get implemented until someone can demonstrate an actual need. \nThere are plenty of tools that just use package.json (browserify, nyc, xo, etc.). Adding multiple ways to do something \"just because\" is a quick road to project bloat. Users then have to ponder which method is \"the better fit for their project\", instead of just getting on with making things.\nYou could conceivably use global installs of eslint and babel on projects that don't have a package.json because they aren't Node based (that is less true of babel@6 than it was of babel@5, but whatever). You could make the argument that a package.json requirement for those was restrictive / confusing (it's a weak argument, but it's an argument). Some people have hundreds of rules in their eslint config - another decent argument. Putting an editorconfig in package.json would be nonsensical - it's a separate standard that has nothing to do with Node / npm. \nMy point here, is demonstrate some actual need. \"Because I want\" is not sufficient. Neither is \"because project X does it that way\". If not having the option is legitimately preventing you from getting things done, and you demonstrate why, we will happily change our minds.\n. This is not going to get implemented until someone can demonstrate an actual need. \nThere are plenty of tools that just use package.json (browserify, nyc, xo, etc.). Adding multiple ways to do something \"just because\" is a quick road to project bloat. Users then have to ponder which method is \"the better fit for their project\", instead of just getting on with making things.\nYou could conceivably use global installs of eslint and babel on projects that don't have a package.json because they aren't Node based (that is less true of babel@6 than it was of babel@5, but whatever). You could make the argument that a package.json requirement for those was restrictive / confusing (it's a weak argument, but it's an argument). Some people have hundreds of rules in their eslint config - another decent argument. Putting an editorconfig in package.json would be nonsensical - it's a separate standard that has nothing to do with Node / npm. \nMy point here, is demonstrate some actual need. \"Because I want\" is not sufficient. Neither is \"because project X does it that way\". If not having the option is legitimately preventing you from getting things done, and you demonstrate why, we will happily change our minds.\n. > couldn't you change the search pattern for files to include test/*/.coffee\nUnfortunately, no. We are discussing a similar issue for jsx here.\n. I agree with @vdemedes. Educating users on the basics of npm and import/require are a bit out of scope for AVA's docs. Thank you anyways @corinna000, we appreciate the contribution regardless!\n. I actually was thinking we could move watcher support into the API itself. That was kind of the thinking behind the test-run event. Also, in my refactor branch lib/ava-files could probably incorporate some of the file-path processing in watcher.js (I'm thinking methods like makeTestMatcher and buildChokidarPatterns, etc).\nI'm thinking of adapting the run method a little:\n``` js\n// new testRun is returned immediately, it may not start receiving events until the current run finishes.\nvar testRun = api.run(files, {\n  runOnlyExclusive: boolean,\n  stopCurrentRun: boolean\n});\ntestRun.on('test', ...)\ntestRun.result.then(function (result) {\n});\n```\n. > Ideally the CLI should just be a tiny wrapper around the API, with CLI only stuff.\nAgreed. Our CLI is pretty huge. A lot of it is options processing (parsing the command line and merging with package.json). Part of the idea behind #700 is to reduce the amount of options processing API consumers would need to do.\n. I didn't mean \"move all of watcher.js into api.js\". Just expose a way to pass options {watch: true} to the API and receive test-run events as files change.\n. Performance regression. Nothing drastic, but it's definitely a regression.\n\n. Performance regression. Nothing drastic, but it's definitely a regression.\n\n. In general LGTM.\nHow is thorough is coverage of new code? It just seems like a high ratio of new code to tests (gut feeling after reviewing on mobile - I could be off base).\nMy only concern with the require hook is that it is easily broken by bad actors. Something like babel-detective might be more robust (but maybe wouldn't catch as thorough a dependency graph)\n. > Is there anything we could do to prevent this?\nMaybe instead of a conventional hook we wrap Module.prototype.compile? That is a no-no for conventional hooks (not really extendable), but since our hook doesn't manipulate the args (it just tracks files) I don't think it would be a problem\n\nI think an actual dependency graph is a lot better than what a static analysis could do. \n\nI agree.\n. @Kl0tl - Can you explain why you need this? What prevents you from using t?\n. It's feasible, it just isn't really practical (at least not with a wildcard *). You almost certainly don't want any method called same wrapped with power-assert stuff.\nWe would have to do static analysis for test() calls, and then see what variable name was used as the first parameter, and then build new patterns from that. It's certainly doable, but it involves additional AST analysis - which pretty much guarantees a minimum amount of difficulty.\n. Just use t.\nAllowing * creates a namespace conflict with outside objects that might have methods with the same name.\nWe could do static analysis to detect test methods and match the first parameter, but that is going to be unreliable.\n. Echoing what I said here: https://github.com/avajs/ava/issues/551#issuecomment-226838995\nI say we just close this. This would require a lot of effort in power-assert, ava-codemods, and our linter.\n. > id-length warnings in ESLint are triggered by t so it would be handy if we could use destructuring as a way to silence those.\nAdd t to  the exceptions property: http://eslint.org/docs/rules/id-length#exceptions\n. I think this should just be closed. Allowing this would require significant effort for power-assert, our codemods, and linter. All to allow something of dubious value.\n. > Any chance we could import the assertion functions from ava so there is no need for the t reference?\nNo, the t reference is specific to that particular test. We keep track of the assertion count, so you can use t.plan\n. Nice! We should create an  \"Avoid beforeEach\" recipe and link to that video!\n. Domains don't exist in the browser, so relying on them becomes problematic as we move towards browser support.\nThere's a PR to fix sync throws that will be merged shortly.\nAsync throws remain problematic, but we've got an idea for handling that via a custom Babel plugin.\nI am on mobile now, but will link tp the relevant issues later. \n. > @spudly did I mention I'm pretty excited about this landing? I'm pretty excited about this landing :)\nAgreed. This should be a high priority merge. Lots of people want this feature.\n. > Anyone wanting to use JSX in their test files is still using ava@0.9.2 until this lands\nSorry about that. When we made the decision to ignore .babelrc, it was a temporary fix and it wasn't supposed to take this long to reintroduce the feature.\n. > > Oh, and to confirm, Babel merges plugin definitions when you set babelrc to true?\n\nYes, I believe that is the case.\n\nShould we have an integration test confirming that?\nOtherwise, lets ship this.\n. > I was under the impression I had to explicitly add this config:\nYes - you should definitely be required to explicitly configure the \"inherit\" property. \n@spudly - when you debug that, please be sure to add a few integration tests to verify correct behavior. Specifically, that .babelrc is only consulted for inherit.\n. @spudly - see https://github.com/sindresorhus/ava/commit/1b01f16d9bcd6f5d62e3becb8d60e457e7ac9690\nYou should be able to merge the babel-plugin-for-integration-tests branch into this PR without conflict.\nTo test out what the babel plugin does:\nsh\n$ node_modules/.bin/babel --plugins ./babel-plugin-test-doubler test/fixture/fail-fast.js\nIt should be pretty easy to create integration tests from there using the API. (You should get double the number of tests when the plugin is used).\n. @tleunen\nPlease raise an issue.\n. > we have to use glob patterns to match all potential sources ...\nOr use babel-detective which is probably more performant (the AST is being scanned already, so it should be faster than a regexp). I created it with this idea in mind.\nEither way this won't do much good for dynamic requires. I did experiment with this, which could be used as a fallback for dynamic require calls.\nI think a first step should be supporting transpilation of helper files (files in the test directory prefixed with _, or in test/helpers). Sources would get a different Babel transform without power-assert, so helpers seems like an easier place to start.\n. For \"non-Babel\" transpilers (typescript, etc), we would have to create a plugin ecosystem. We would still need to finish up with Babel for powerAssert support. \nConceivably, users may want to chain custom transforms.\nSee transform-chain. It could help with all this.\n. LGTM. Just some minor nits.\n. Context is used to share data between the beforeEach and a specific test. If you have five actual tests, and they all share on common beforeEach hook, that hook will be called five times: once with each context for each of the five tests.\nPassing context to before doesn't make sense. It's only called once. Which context would you give it?\n. One thing that will be difficult about this when it comes to AVA, will be the fact that there's no good way to enforce ordering across multiple processes. That would only cause problems if multiple test files are accessing the same system resource (disk, database, etc). I don't think that is an easy problem to solve, and is probably best just ignored initially. Random ordering per file would provide a lot of value on it's own. \n\nThe asynchronous ones will start to interleave but synchronous tests won't. \n\nAgreed. Additionally, most asynchronous tests will currently interleave in a predictable/identical way for each test run. Only async tests with random delays (aka: those that access system resources) will end up interleaving randomly. \n. > These should remain tab indented.\nI just pushed up the fix-readme-improvements branch which removes the whitespace changes. I was trying to come up with a way to script that (I was unsuccessful).\n. It is probably a good idea to leave this branch accessible with no squashing of commits. It may be easier for translators to follow along commit by commit. We should still only push a single squashed commit to master, but lets leave the history here to help the translators.\n. > script what?\nA way to apply a series of commits, excluding whitespace only changes. git flags like --ignore-spacing-changes had me thinking it was doable. Turns out those flags don't do what I assumed they did\n. :+1: \n. \nI ran all the visual tests and they all look good on iTerm2, OSX El Capitan.\n. Should pass / fail count be on the same line?\nA: \n\nvs.\nB:\n\n. OK,\nI incorporated most of the PR feedback.\nAlso, I switched it to option A as described above (counts on separate lines). That is the last commit, so it can be dropped if there is not agreement.\n. All changes made. \n. What version of npm are you using on node 4 locally?\nTravis node 4 uses npm@2. If you've upgraded your local node 4 to use npm@3 it could be nom that is responsible for the weirdness.\nAVA should behave the same across environments, so I feel this is valid.\n. Looks like there is a limit on the number of processes Travis will handle\n\nMy build script is killed without any error\n...\n[potiential cause might be:] Tests running in parallel using too many processes or threads (e.g. using the parallel_test gem)\n...\nFor parallel processes running at the same time, try to reduce the number. More than two to four processes should be fine, beyond that, resources are likely to be exhausted\n\nSeems really likely this is the issue. It seems your test suites are able to handle a lot more than just four processes though. That document is targeted towards Ruby, so maybe Node processes are lighter weight and Travis can handle more.\nThrottling concurrency in AVA is likely to be non-trivial. The only solution I can think of for now is to batch via the cli:\njs\nava test/subdir1/*.js\nava test/subdir2/*.js\nava test/subdir3/*.js\nI get that that is really ugly. We will make fixing this a priority.\n. I think processes are getting killed before any tests start. The logs seem to show all the kill messages right away, then test results.\n. @thangngoc89 - There are added inefficiencies (and reduced complexities) by not implementing in AVA. \nBy all means - use this script until we \"fix it the right way\".\n. chalk stores a reference to the current enabled state every time you access a member. So our colors module will reflect whatever state chalk was in at the time that module was evaluated.\nYou should be able to get around this with something like:\njs\nvar chalk = require('chalk');\nchalk.enabled = true;\nvar colors = require('../lib/colors');\nObject.keys(colors).forEach(function (key) {\n  colors[key].enabled = true;\n});\n. I just checked the repo, adding an appropriate .babelrc should fix your issue.\nPing back if it doesn't.\n. > And then I realized, that existing code doesn't work with nested promise \u2014 .on is not a function.\nYou might be able to solve this with promise-delegates.\n. @develar - If you ever get around to updating this PR, we would still be interested in merging.  However, be aware, there is work happening towards official Node debugger support: https://github.com/nodejs/node/issues/2546#issuecomment-189311746. \n. Does this happen only when tests fail, or even for passing tests?\nIf it is happening only for failing tests, then power-assert is the likely culprit. Otherwise it's probably something with JSX and deeper.\n. This is related to the fact that we serialize objects in the child thread and pass them to the parent thread for rendering by power-assert formatters. ~~The React Dom exposes lots of internal state properties that we end up trying to serialize.~~\nWe did it this way because power-assert formatters introduce a non-trivial require time into tests.\nPerhaps a better way would be to revert back to rendering error output in child threads, but lazy-require the formatters. Things would slow down a bit if you have errors in every test file, but should be just as fast if not faster if you have no errors or only have errors in one or two test files.\n~~Alternatively, we could try to get the React Dom to expose less internal state (by making some properties non-enumerable). I wouldn't know where to begin with that. JSDOM appears to do a great job of hiding all internal state, so it's not that.~~\nUpdate: I'm not sure the crossed out sections above are the problem.\n. I just tried this on master:\n``` js\nglobal.document = require('jsdom').jsdom('');\nglobal.window = document.defaultView;\nglobal.navigator = window.navigator;\nimport test  from 'ava';\nimport { render } from 'react-dom'\nimport React from 'react';\nfunction CustomComponent({ value }) { return  }\nfor (var i = 0; i < 100; i++) {\n(function (i) {\n    test('does something slow ' + i, t => {\n      const div = document.createElement('div')\n      render(, div)\n      const originInput = div.getElementsByTagName('input')[0]\n      t.same(originInput.value, 33) // assert on a reference to the value\n    })\n})(i)\n}\n```\nWith the following ava config in package.json:\n``` js\n\"ava\": {\n    \"babel\": {\n      \"presets\": [\n        \"react\",\n        \"stage-2\",\n        \"es2015\"\n      ]\n    }\n  }\n```\nI get a runtime of ~2.0 seconds whether I use t.same or assert.deepStrictEqual.\n. @xjamundx - Can you try against master?\n$ npm install sindresorhus/ava\nYou will also need to set up your AVA config package.json to enable react. Either do what I did above, or use the inherit option:\njs\n\"ava\": {\n  \"babel\": \"inherit\"\n}\n. I just tried on 0.9.2 - your example is still fast.\n. The number of files is not going to have an impact. Each one gets it's own process. If you are using 0.9.2 you are paying a big price for multiple files, but that is not a React specific thing. Try on master.\n. Also, make sure you have npm@3:\n$ npm install -g npm@3\n$ rm -rf node_modules\n$ npm install\nBabel is way more performant with npm@3, and if you are using ava@0.9.2 that is especially important.\n. Just a thought, but what if we showed bothe the current test, and the last failed test. Or maybe just the counts and the last failed test (Since we intend to print pending tests on SIGINT - the last passed test isn't that valuable anymore). What failed is usually the most important info anyways.\n. Are people usually adding stage-3 to their Babel configs? Because AVA really needs that to provide all the async/await goodness. \nPerhaps we should automatically add stage-3 to the detected .babelrc? If the user goes so far as to actually specify the inherit option then we assume they know what they are doing and only add  power-assert?\nThere is a tension here between doing too much magic and asking the user to write extraneous config. Always a hard decision.\n. > However, I also had to \"require\": \"babel-register\" or nothing was transpiled\nYour tests should have been, it's a bug if they weren't. Compiling sources would need require: 'babel-register'. :+1:  for documenting that better. \n. > The features we describe in https://github.com/sindresorhus/ava#es2015-support\nExactly, I would want async-await to work for users regardless of whether they specified that in their .babelrc.\n. > And from my perspective, I don't think that people would use async await if they couldn't do so in their source because I generally like to write my tests and source in a similar fashion.\nI'm sure that's true for a lot of people. Most of the core AVA team is fully bought in to small-module design, and for small modules it is often easier to write them to work without transpilation. This isn't a new idea - I've seen plenty of modules that write sources in pure JS and tests in CoffeeScript.\nOne justification for the CoffeeScript approach is that it lowers the barrier for contribution for users who don't use CoffeeScript. Also, stack-traces pasted in GH comments are useless if you have transpiled and the user hasn't installed sourcemap-support\n. :+1: \n. What is the best way to fix? \nTruncate the title or try to account for the wrapping when deleting?\nThe first is probably easier than the second, but both should be doable.\n. I guess I'm not so sure checking the constructor is a terrible idea. Two objects with different constructors are not strictly equal. Specifically, you could certainly write an instanceOf check that one object passes, and the other fails. The empty diff is a bit problematic. Maybe we should provide a special error message in that case:\nAssertionError: t.same - objects differed only by constructor\nThe docs indicate this behavior was a specific choice in deeper. My gut tells me it's the correct choice for the default behavior. Perhaps @othiym23 can provide further rationale.\n. We moved away from deep-equal because of https://github.com/substack/node-deep-equal/issues/19. \nI see no reason not to emulate tap here, with  same and strictSame just as described by @othiym23.\n. > the set of assumptions the implementer chooses when designing an equality test is important\n:+1: this just leaves me more convinced that we want to stick with deeper/only-shallow.\nPerhaps AVA should add buffertools as a dependency if it's not too heavy. Or at least document somewhere that installing it will speed up buffer comparison.\n. This isn't completely analogous to is vs strictIs.  You can always make your test work using the strict-only method AVA offers, you may just need to swap t.is(x, null) with t.is(x, undefined). In this case AVA forces you to write a more explicit test without creating an undue burden.\nIt seems @mattkrick is claiming there are cases where a looser \"structurally equal\" comparison is desirable.\nMaybe we should keep t.same to be the strict variety, and have t.looseSame\n. In which case you could just use looseSame directly and be done with it?\n. OK,\nI guess I am sold on just using the loose comparison.\n@novemberborn Can you think of a reason for keeping the strict one around?\n@mattkrick are you interested in making a PR swapping deeper for only-shallow?\n. Extensions provide hints to the IDE as to which language features should be enabled.\nMy question is, do we just make a special exception for .jsx, or parse glob strings for extensions?\nava test/*.{js,jsx}\nWould enable js and jsx\n. > I think ideally ava would accept any extension, as long as there is a register hook to get the file to vanilla javascript.\nI like that thinking. The only difficulty is that we can't know the list of registered extensions in the parent process, we process require hooks in the child.\n. Why is matching the glob string problematic? If they explicitly define an extension just trust the user will add the require hook themselves (perhaps a helpful message if they don't).\n. > Yes, but this is the part we're currently missing. You can't swap out the Babel precompiler.\nYou can by combining babel:false and require: 'some-precompiler'. Let's take coffee-script support for example. A complete config should look like this:\njs\n{\n  \"ava\": {\n    \"require\": \"coffee-script/register\",\n    \"babel\": false,\n    \"files\":  \"test/**/*.coffee\"\n  }\n}\nJSX should be like this:\njs\n{\n  \"ava\": {\n    \"babel\": \"inherit\" // with .jsx support setup correction in .babelrc,\n    \"files\": \"test/**/*.{js,jsx}\"\n  }\n}\n\nSo, we agreed to accept .jsx extensions in our API?\n\nI would just as soon not hard code non-standard extensions into AVA if we can avoid it (which I think we can fairly easily in this case).\n. I think what @sindresorhus is getting at is the following.\nIf the glob pattern matches a directory explicitly:\nsh\n$ ava tests/some-dir\nWhat the user almost certainly meant was:\nsh\n$ ava test/some-dir/*.js\nSo we help them out and do that automagically.\nIf the users glob pattern is specific about extensions:\nsh\n$ ava test/some-dir/*.jsx\nThen we take them at their word. In the above example, .js files should be ruled out. If they want .js as well, they need to do:\nsh\n$ ava test/some-dir/*.{js,jsx}\nSo basically, we should be using strict glob pattern matching except in the case where they explicitly match a directory.\n. @azizhk - We don't transpile your sources at all. Only your test files.\nFor sources, instead of nodemon / babel-register - try something like this.\nIf you just want to be able to use JSX in your tests - that is easier. Just configure AVA's transpiler to use babel in package.json:\njs\n{\n  \"ava\": {\n    \"babel\": \"inherit\" // assuming you have a `.babelrc`,\n     // or\n    \"babel\": {\n      \"presets\": [\"es2015\", \"stage-2\", /*... whatever presets you want */],\n      \"plugins\": [/* your plugins here */]\n    }\n  }\n}\nYou still won't be able to use the .jsx extension, but just use .js, then you will be able to use the code constructs you want.\n. I kinda think if there are .only tests we should abandon the whole smart rerun thing and just run all files with a .only whenever any change is detected (regardless of dependency graph). It is understood that .only is a temporary way of focusing on just a few tests. If a user has specified .only on two tests in separate files at the same time, we should trust that they know what they want and always rerun both.\n. > I think we'd have to rerun all tests, in case you add a .only in another file\nUnchanged files that did not have .only before will not need to be run again.\n. Babel offers fairly robust configuration from the CLI, allowing plugins, presets, and all kinds of other options to be passed in:\n```\n  Usage: babel [options] \nOptions:\n-h, --help                           output usage information\n-f, --filename [filename]            filename to use when reading from stdin - this will be used in source-maps, errors etc\n--retain-lines                       retain line numbers - will result in really ugly code\n--no-highlight-code                  enable/disable ANSI syntax highlighting of code frames (on by default)\n--presets [list]\n--plugins [list]\n--ignore [list]                      list of glob paths to **not** compile\n--only [list]                        list of glob paths to **only** compile\n--no-comments                        write comments to generated output (true by default)\n--compact [booleanString]            do not include superfluous whitespace characters and line terminators [true|false|auto]\n--minified                           save as much bytes when printing [true|false]\n-s, --source-maps [booleanString]    [true|false|inline]\n--source-map-target [string]         set `file` on returned source map\n--source-file-name [string]          set `sources[0]` on returned source map\n--source-root [filename]             the root from which all sources are relative\n--no-babelrc                         Whether or not to look up .babelrc and .babelignore files\n--source-type [string]\n--auxiliary-comment-before [string]  print a comment before any injected non-user code\n--auxiliary-comment-after [string]   print a comment after any injected non-user code\n--module-root [filename]             optional prefix for the AMD module formatter that will be prepend to the filename on module definitions\n-M, --module-ids                     insert an explicit id for modules\n--module-id [string]                 specify a custom name for module ids\n-x, --extensions [extensions]        List of extensions to compile when a directory has been input [.es6,.js,.es,.jsx]\n-w, --watch                          Recompile files on changes\n-o, --out-file [out]                 Compile all input files into a single file\n-d, --out-dir [out]                  Compile an input directory of modules into an output directory\n-D, --copy-files                     When compiling a directory copy over non-compilable files\n-q, --quiet                          Don't log anything\n-V, --version                        output the version number\n\n```\nFrom that list, I think only a few are applicable here:\n- --presets\n- --plugins\n- --ignore\n- --only\n- --no-babelrc\nWhich of these do we actually want to implement, and how should it look?\n. > None \nI agree. \n\nusually it will always be inherit anyway\n\nIf that is the case, then why do a CLI option at all? IMO, CLI options should be for config options you want to change occasionally or contextually (i.e.: default to mini reporter, but use verbose on the CI server).\n. > but dont want to pollute package.json with ava specific configs.\nIf that's the only rationale for adding a CLI option, then :-1:. Is there some specific reason config via package.json can't be used other than personal preference?\n\nthe default value is not inherit\n\nCorrect, but if your tests need the inherit option, then set it once in package.json, and avoid littering all your scripts / command prompts with an unneeded flag that must be the same every time.\n. > Or at least make ava use the configs property in package.json instead of a \"root property\". I don't think it's the case at the moment\nWe chose the root property on purpose. If you have valid needs for either request (CLI or non-top-level config), you will find us happy to accommodate. Unfortunately, \"my preference differs from yours\" isn't generally a good enough reason for us to add overhead. \n. Also, I would prefer that the truncation character always be in the rightmost column and try to show as much of that last word as possible instead of just dropping it.\ni.e. when truncating to 6 characters\nfoo bar\nshould become\nfoo b\u2026\ninstead of\nfoo\u2026\nIt looks like this would require a PR to cli-truncate, or just use slice-ansi.\n. > I don't think it fits AVA's atomic tests philosophy. \n:+1: agreed\n. Wow. Good work debugging this @novemberborn.\nLGTM.\n. Instead of adding an option, what if we just check the provided patterns to see if they reference stuff outside of cwd? If they all do, then just change cwd automatically. trying to run files in the current project and a different project with a single command should just be an error\n. > how do we handle nested describe blocks with their own lifecycle hooks\nShort of implementing #222, I am not sure. We could try converting them to setup functions:\n``` js\ndescribe('outer', function () {\n  beforeEach(function() {\n     // outer beforeEach\n  });\nit('outerTest', function () {\n    // outer test\n  }\ndescribe('inner', function () {\n    beforeEach(function () {\n      // inner beforeEach\n    });\nit('innerTest', function() {\n  // inner test\n});\n\n});\n});\n```\nconverts to:\n``` js\nfunction outerBeforeEach() {\n  // outer beforeEach\n}\nfunction innerBeforeEach() {\n  // inner beforeEach\n} \ntest('outerTest', t => {\n  outerBeforeEach();\n  // outer test\n}); \ntest('innerTest', t => {\n  outerBeforeEach();\n  innerBeforeEach();\n  // inner test\n}); \n```\nHonestly though - it seems so complicated, it might just be better to implement grouping in AVA if we really want to make the transition easy.\n. A big problem will be handling variables declared / setup in beforeEach, etc.\n``` js\ndescribe('group', function () {\n  var fn, spy;\nbeforeEach(function () {\n    spy = sinon.spy();\n    fn = proxyquire('some-module', {\n      'some-dependency': spy\n    });\n  });\nit('test1', function () {\n     fn(someArgs);\n     assert(spy.calledWith(...));\n  });\nit('test2', function () {\n     fn(someArgs);\n     assert(spy.calledWith(...));\n  });\n});\n```\nI think maybe the best solution might be to just use .serial mode across the board (certainly if beforeEach is detected anywhere in the file). Users can selectively convert tests to concurrent mode as they gain an understanding of what is happening.\n. > I know jscodeshift doesn't allow to create new files or update files other than the currently evaluated one\nWhy not?\n. The way the jscodemod plugins are specified, you could use recast directly for complex transforms like this, and continue to use the simpler jscodemod API for stuff like t.same => t.deepEqual. \n. jamestalmage/ava-codemods created.\nEveryone in the conversation so far is added as a collaborator.\n. I have created issues for each of the different conversions being discussed in the other repo:\n- mocha: jamestalmage/ava-codemods#3\n- tap: jamestalmage/ava-codemods#4\n- tape: jamestalmage/ava-codemods#5\nLet's continue conversation over there.\n. node-tap already does this using at property of the yaml output.\n. Oh - missed that you wanted this for passing tests/assertions.\nWe could probably accommodate with a babel-plugin. I'm not opposed, but I think it's low-priority.\n. LGTM\n. LGTM\n. > This would only be useful for test suites that take a really long time to execute though, right?\nYep.\n\nInteractive test results would complicate the UX, imo\n\nIf the user doesn't take action (by hitting :arrow_up:) the experience doesn't change. \n\nI just want to get the important information (test failures) to the user as quickly as possible. Happy to entertain alternative ways to do that.\n. :+1: \n. Let's leave this open - my first idea wasn't great - but @vdemedes plan is. I'll update the description to say we've agreed on @vdemedes plan.\n. > Does the bench stuff enable us to test this? (I've never used it before)\nIt should. Docs here. I would love a second set of eyeballs on our benchmark tooling! :stuck_out_tongue_winking_eye: \n. Even with debounce there is an issue with this when someone has a long timeout set and the tests finish. There is no way to cancel the scheduled timer.\nI've raised an issue: https://github.com/component/debounce/issues/9\n. Yes - ship\n. Why is https://github.com/sindresorhus/ava/commit/ae45752597130fb86fa678b30a24613a619540d3 in this PR?\n. :shipit:\n. > Changing the mapFile extension to .js.map may also help.\nIt might if we weren't lying to the module system about the sources location (it's coming from the cache folder, not the modules source path). This is why we have to use a custom function for sourcemap support.\n. > Changing the mapFile extension to .js.map may also help.\nIt might if we weren't lying to the module system about the sources location (it's coming from the cache folder, not the modules source path). This is why we have to use a custom function for sourcemap support.\n. Closing in favor of #662\n. > ctrl+\\ (SIGQUIT) causes a crash. ctrl+z is ignored.\nDo people actually use those?\n\nctrl+c now causes npm to spit out an error log\n\nAgreed. That is a problem. Is there a way to 1) detect if you have a parent process, and 2) send a SIGINT to it? I would rather not exit with 0 (though that may be preferable to npms big old log when it sees an error.\nAside: It would be really nice if npm didn't print that huge mess of log statements when a script fails. So annoying to scroll up past that. I have a hard time believing all that \"this isn't a problem with npm\" language significantly reduces their support requests.\n\nBut there's a bunch of other shell behaviors that are broken by this. How much would we want to replicate them and is it worth the hassle?\n\nI guess that depends on what those behaviors are, and what problems we are creating for users. If it creates real problems, lets just revert to the r+enter behavior (one more keypress is not that big of deal). We should definitely make a decision on that before publishing this though.\n\nOne last thought. I believe the original r+enter behavior was modeled off existing libraries. I would assume they knew about rawMode as well. They may have evaluated this same behavior and rejected it for the reasons being discussed now. @novemberborn It might be worth doing an issue search on some of those repos to see if they discussed this.\n. Argument for keeping this:\nWe can change this back at any point. It's not going to break builds. People are just going to have to retrain \"r\" vs \"r + enter\". For all the problems we've listed, I haven't seen one I think is a practical problem yet.\nArgument for reverting:\nIt's the \"safer\" option. Guaranteed not to cause problems (however unlikely problems seem).\n\nIf anyone can describe a situation where this prevents you from using watch mode then let's revert. Heck, I would be convinced if just one person said \"I frequently use Ctrl + SomethingOtherThanC\".\n. > Even if we don't revert we need to fix the exit code upon Ctrl + C.\nOh yeah, forgot about that. I don't love the idea of exiting with 0 on Ctrl + C. It's another one of those \"probably not a problem, but just feels wrong\" issues.\nI think I'm convinced. Let's revert. @sindresorhus @vdemedes ?\n. > Even if we don't revert we need to fix the exit code upon Ctrl + C.\nOh yeah, forgot about that. I don't love the idea of exiting with 0 on Ctrl + C. It's another one of those \"probably not a problem, but just feels wrong\" issues.\nI think I'm convinced. Let's revert. @sindresorhus @vdemedes ?\n. I would just as soon see this fixed in babel. I have commented on an already open issue here.\nAs you mention, this doesn't help if users need babel-runtime@6 anyways (which seems increasingly likely). \nAlso, it's not quite so trivial as wrapping babel-runtime@5. Your wrapper can't just do this:\njs\n// unfortunately, our wrapper won't be this simple\nmodule.exports = require('babel-runtime');\nbabel-plugin-transform-runtime actually creates require statements with some pretty deep paths.\nHere are the first few transpiled lines of babel-plugin-transform-es2015-blocking-scope:\n``` js\nvar _classCallCheck = require(\"babel-runtime/helpers/class-call-check\")[\"default\"];\nvar _Object$create = require(\"babel-runtime/core-js/object/create\")[\"default\"];\nvar _Symbol = require(\"babel-runtime/core-js/symbol\")[\"default\"];\nvar _interopRequireDefault = require(\"babel-runtime/helpers/interop-require-default\")[\"default\"];\nvar _interopRequireWildcard = require(\"babel-runtime/helpers/interop-require-wildcard\")[\"default\"];\n```\nWe definitely could do this ourselves (and if the babel team decides they won't, that will be our only option). I just think fixing it in babel would be easier and better for the community at large.\n. Awesome. I just tried it out on a random project, and it \"feels\" faster (could be placebo effect though).\nHave you done any profiling with the change?\n. @SamVerschueren \nSorry I haven't gotten to review your PR's yet. There are two closed issues that discuss the expected behaviors of t.throws.\nhttps://github.com/sindresorhus/ava/issues/468#issue-128347888\nIt should return the thrown Error (if it's synchronous). Or a Promise for the rejection reason if an async object is returned (be it Observable or Promise). In other words, t.throws should turn a rejected promise into a resolved promise for the rejection reason. This allows you to await the rejection reason for additional assertions (if you are attaching additional properties).  \nhttps://github.com/sindresorhus/ava/issues/493#issue-129952442\nThe assertion should fail if non-Errors are used. This includes both the sync and async versions. The message should be a very helpful message that explains \"You threw a non-Error, which is bad form\". In other words, we should minimize confusion for developers who intentionally threw a non-Error and are trying to make assertions against it, that that is not supported behavior. If users want to assert against non-Errors, they need to catch them themselves.\n\nThis is complicated enough that we should pretty thoroughly document it. Perhaps it's enough to justify a recipe: \"Properly Handling Errors in AVA\" that includes detailed explanations on the pitfalls of non-Errors.\n\nThere is a limited value to grabbing non-Error's, and that is specifically when you are writing a framework or tool that executes user supplied callbacks. In that case it's often beneficial to write a few assertions to define how your code responds when your users code throws non-Errors. Since that's the rarer corner case, you should be responsible for jumping through the extra hoops to assert that behavior (by doing the more verbose thing and wrapping your tests with try/catch, etc). Again - there is probably room for a recipe here. \n. @SamVerschueren \nSorry I haven't gotten to review your PR's yet. There are two closed issues that discuss the expected behaviors of t.throws.\nhttps://github.com/sindresorhus/ava/issues/468#issue-128347888\nIt should return the thrown Error (if it's synchronous). Or a Promise for the rejection reason if an async object is returned (be it Observable or Promise). In other words, t.throws should turn a rejected promise into a resolved promise for the rejection reason. This allows you to await the rejection reason for additional assertions (if you are attaching additional properties).  \nhttps://github.com/sindresorhus/ava/issues/493#issue-129952442\nThe assertion should fail if non-Errors are used. This includes both the sync and async versions. The message should be a very helpful message that explains \"You threw a non-Error, which is bad form\". In other words, we should minimize confusion for developers who intentionally threw a non-Error and are trying to make assertions against it, that that is not supported behavior. If users want to assert against non-Errors, they need to catch them themselves.\n\nThis is complicated enough that we should pretty thoroughly document it. Perhaps it's enough to justify a recipe: \"Properly Handling Errors in AVA\" that includes detailed explanations on the pitfalls of non-Errors.\n\nThere is a limited value to grabbing non-Error's, and that is specifically when you are writing a framework or tool that executes user supplied callbacks. In that case it's often beneficial to write a few assertions to define how your code responds when your users code throws non-Errors. Since that's the rarer corner case, you should be responsible for jumping through the extra hoops to assert that behavior (by doing the more verbose thing and wrapping your tests with try/catch, etc). Again - there is probably room for a recipe here. \n. Is throwsAny really that useful? When would you ever use it?\n. > Given that the language allows you to throw or reject with whatever you want, I would think that t.throws should allow you to do so.\nWe want to promote best practices wherever possible. If you have control of the code, you should be throwing errors.\n\nbecause it is a String reason which is promoted on mdn page\n\nThey show an Error example, and mention that it is probably a good idea to throw Errors. I say it is a code smell.\n\nI guess I can live with t.throwsAny to make life simpler for people who are converting tests for legacy code that throws bad values. Ideally we wouldn't even do that. I don't think it's a big need.\n. > Willing to write a small recipe for asserting errors to explain when to use try-catch and when to use t.throws.\nI'm not sure how that would read. The best practice we want to promote is \"Always throw Errors\".\nIt's one thing if you are testing code outside your control, but that kind of begs the question: \"Why are you testing code outside your control?\". (TBH, I have written tests for some closed source API's to verify behavior, but that's really rare).\n. LGTM\n. I wish the functions were called truthy and falsy, I have never liked ok/notOk.\nIf we were to alias them, I would want to immediately deprecate ok and notOk and remove them at some later date.\nTruth be told, I wish a number of our assertion methods were named differently (i.e. It is not possible for you to know what same means without documentation, deepEqual would have been better IMO). Still, I think it is better to keep only one way to do it then make aliases. \nFact of the matter is, it took me just a few hours to commit AVA's limited list of assertions to memory, and now when I look at other peoples AVA tests I know what they mean without pulling up the docs. node-tap has a long list of aliases, and there has been more than one occasion where I've reviewed a test written for tap and needed to pull up docs to verify what I was seeing was an alias, and not a custom assertion.\nAlso, I think there is value in supporting custom assertions someday, and every alias we add removes another name from the namespace.\n. @kentcdodds - I personally like that plan. But I would wait for other maintainers to chime in before proceeding.\nThe codemod would be a must IMO. Maybe we could even ship the codemod with AVA for a while and offer a CLI prompt to apply it automatically. We already know which files are the test files from the config, so we could make it pretty easy for users. (I think deciding whether or not to ship it with AVA depends on how many dependencies the codemod would add). \n. You can google truthy, googling ok is useless. You need tap/tape experience to know what ok means, you only need sufficient javascript experience to immediately understand what truthy means.\nMy problem with same is the ambiguity. It took me a long time to stop thinking it meant \"the same instance\", as in ===. \n. IMO, the value of ok / notOk is for methods that might return a falsy value. Imagine a \"safe\" fs.readFileSync that returned null or undefined instead of throwing \"file not found\" errors. That is where I might use these to assert \"I got a value\" vs. \"I didn't\". The right answer might be to always return null and do t.is(x, null). But that sometimes requires coercing undefined to null in situations where it really isn't really important to do so. Still, I am willing to be convinced to drop them, though I'm skeptical.\nfalsy/truthy is pretty well defined. There are Wikipedia entries on both easily found by googling. Googling ok is not helpful. So I prefer those names if these assertions stick around.\n. truthy vs ok:\nI would argue you have at least a chance of understanding the intent of the truthy assertion using only knowledge of the Javascript language. truthy is probably the most commonly used word to describe JavaScripts treatment of non-booleans in conditional statements. Actually, if there are other terms besides truthy/falsy, I don't know them.\nIf I was describing some JS algorithm in English, I would never use the phrase \"If the returned value is ok, then do ...\". It's just too ambiguous, what does ok mean? I could imagine it meaning lots of things: \"properly formatted\", \"not an Error\", \"not a rejected promise\", etc. I think the only way you see t.ok and immediately understand what it means is if you are familiar with assertion libraries that use the convention. \nSo, that is my best argument for truthy/falsy. The ability to understand with basic Javascript knowledge vs. needing to have experience with certain assertion libraries. Neither is perfect, but I do think truthy/falsy is superior.\n\nsame vs deepEqual:\nI do like deepEqual better, but only because I think same is problematic. I come from a Java background, and in that world, most same assertions mean \"same instance\". I probably did have to look up what assert.deepEqual did the first time I saw it (I don't remember). It certainly feels more descriptive, and I don't think you are as likely to confuse it with an entirely different behavior.\n\nUltimately, I am willing to live with whatever we decide, but I do think this line from @kentcdodds bears repeating:\n\nI think that as a pre-1.0.0 project, there's a responsibility to get the API correct and obvious, even at the cost of breaking changes\n. truthy vs ok:\n\nI would argue you have at least a chance of understanding the intent of the truthy assertion using only knowledge of the Javascript language. truthy is probably the most commonly used word to describe JavaScripts treatment of non-booleans in conditional statements. Actually, if there are other terms besides truthy/falsy, I don't know them.\nIf I was describing some JS algorithm in English, I would never use the phrase \"If the returned value is ok, then do ...\". It's just too ambiguous, what does ok mean? I could imagine it meaning lots of things: \"properly formatted\", \"not an Error\", \"not a rejected promise\", etc. I think the only way you see t.ok and immediately understand what it means is if you are familiar with assertion libraries that use the convention. \nSo, that is my best argument for truthy/falsy. The ability to understand with basic Javascript knowledge vs. needing to have experience with certain assertion libraries. Neither is perfect, but I do think truthy/falsy is superior.\n\nsame vs deepEqual:\nI do like deepEqual better, but only because I think same is problematic. I come from a Java background, and in that world, most same assertions mean \"same instance\". I probably did have to look up what assert.deepEqual did the first time I saw it (I don't remember). It certainly feels more descriptive, and I don't think you are as likely to confuse it with an entirely different behavior.\n\nUltimately, I am willing to live with whatever we decide, but I do think this line from @kentcdodds bears repeating:\n\nI think that as a pre-1.0.0 project, there's a responsibility to get the API correct and obvious, even at the cost of breaking changes\n. > would it make sense to move it out of the PR?\n\nMeh. I think @kentcdodds was really after a discussion more than getting this merged.\n. > would it make sense to move it out of the PR?\nMeh. I think @kentcdodds was really after a discussion more than getting this merged.\n. :+1: I like this.\nWe have run into this a number of times (.avarc requests, config/ava vs ava in package.json, etc). It would be kinder to contributors to explain this up front.\n. We could, but we need to propagate that down to all the dependencies that might want to do that as well, including graceful-fs. I also think doing so would make testing most of those modules a pain (it is now harder to intercept what they are doing and mock it).   \nIt would just be nice if mock-fs didn't clobber the entire file system the way it does. I just think it's going to be hard to stay on top of every place it might cause problems.\n. It looks like fs-promise stores a reference to readFile before you mock out the function.\nYour only choice here may be to use require statements instead of imports :cry:.\njs\nconst mountfs = require('mount-fs');\nmountfs.patchInPlace();\nconst compiler = require('../compile-template');\n. It looks like fs-promise stores a reference to readFile before you mock out the function.\nYour only choice here may be to use require statements instead of imports :cry:.\njs\nconst mountfs = require('mount-fs');\nmountfs.patchInPlace();\nconst compiler = require('../compile-template');\n. > Maybe mocking fs with nyc is not a good idea.\n@thangngoc89 - can you upload a minimal reproduction to GitHub? It should contain the minimal amount of steps required to reproduce the problem.\n@novemberborn and I are maintainers on both the AVA and nyc projects. Understanding the problems you are having would help us to write up good documentation to help future users as well.\n. > Maybe mocking fs with nyc is not a good idea.\n@thangngoc89 - can you upload a minimal reproduction to GitHub? It should contain the minimal amount of steps required to reproduce the problem.\n@novemberborn and I are maintainers on both the AVA and nyc projects. Understanding the problems you are having would help us to write up good documentation to help future users as well.\n. OK, I am stumped. It is only happening with nyc, so let's continue discussing here.\n. OK, I am stumped. It is only happening with nyc, so let's continue discussing here.\n. @spudly \nAn alternative to writing vanilla js would be to use a build step to transpile your sources.\nlib/\n  foo.js\ntest/\n  test-foo.js\nbuild/\n  foo.js\nAnd test-foo.js does this:\njs\n import foo from '../build/foo.js'\nThat's definitely not ideal, but you get to keep writing in ES2015 (:+1:), and there is a very good chance we will improve this performance bottleneck in the future.\n. >  tape and mocha are also transpiling the source code, but they manage to keep it very fast.\nThat actually depends on your test layout. If you have a few test files with lots of slow tests in each file, then AVA still is faster even though you need to load babel-register in every test. If you have lots of files with just a few tests each, then yes, babel-register costs you big. The solution above should be about the fastest option available (with the downside of requiring extra setup).\nAlso, I would try on Node 5.9. I think there have been some speed improvements. (And always use npm@3).\n. > A good solution to this would be to print less empty lines;\n:+1: \n\nmy ideal solution would be that these messages should not show at all\n\n:-1: It is good to know it is still doing something (a simple spinner isn't reassuring enough). Also, you can technically also use this with the verbose reporter (--watch --verbose). \n\nany reason we can't just overwrite the output\n\n:+1: for doing that with the mini reporter. Verbose should just be an endless scroll.\n\nWe should also include some indicator that another test run has occurred. For small test suites it's already so fast that if you blink you might miss it (and you can't tell one 12 passed line from the previous). I am thinking maybe a timestamp (light gray):\n[10:51:03] 12 passed\n. ~~Does our tap output even work in watch mode?~~ Nope: #672\nRegardless, I would prefer it be something we do internally. I still want notifications when using the mini reporter.\n. > If we had a plugin architecture would this be something that could be provided in a separate module?\nProbably, though I think it's a nice enough feature to just include. When I have good TDD flow, I only need to see the terminal window occasionally. \n. > Was thinking maybe to not even start watch mode \n:-1: I agree with you, that would be annoying.\n\nif you specify the tap option in the config. Maybe print a warning?\n\n:+1: Ignore tap:true in the config if they use --watch.\nIf they specify both --watch --tap on the command line maybe it should just throw and bail. I could see someone trying that when they are trying to figure out why they aren't getting tap in watch mode. \n. > Should this mandate an issue link?\nThat's a cool idea. Optional maybe? Not everyone uses AVA for open source projects with issue trackers (though really, that's the target audience for this feature). We could create an eslint-plugin-ava rule to make it mandatory.\n. :+1: .see should throw a helpful message (including the test title) if used without .failing.\nAlso, if we move ahead, we should raise issues on eslint-plugin-ava to enforce use-see-with-failing and no-see-without-failing.\n. In my mind:\n- skip - implementation that isn't run.\n- todo - no implementation.\n- failing - implementation that is run, but expected to fail.\n. In my mind:\n- skip - implementation that isn't run.\n- todo - no implementation.\n- failing - implementation that is run, but expected to fail.\n. You should not need to modify the Runner methods.\n1. Add the failing metadata.\n2. Modify the return value of Test.run so it flips the return value;\n   a. If a synchronous success is returned, flip it to a synchronous failure.\n   b. If a synchronous failure is returned, flip it to a synchronous success.\n   c. If a Promise is returned, flip it's result.\nFor now, just modify successes to a an Error that says Test was expected to fail, but succeeded, you should unmark the test as failing.\nEventually, we want to modify the reporters so they still list test.failing tests in red/yellow (like a normal test that fails), but do not cause a non-zero exit code. We can do this in stages though.\n. @dcousineau \nThe rewire plugin is generally used on sources, not tests. AVA automatically transpiles tests for you, but not sources. You still need to use .babelrc and  --require babel-register to transpile sources. \nThere are other issues where we are discussing solutions:\n- #669 \n- #631 \n- #619 \n- #608 \n- #548 \n. > in general I only want rewire plugin operating in a testing context but not when building the client.\nAh, I see your dilemma. We still need to implement an extends option for your babel config:\njs\n// package.json\n{\n  \"ava\": {\n    \"babel\" : {\n      \"extends\": \"./babelrc\",\n      \"plugins\": [\"rewire\"]\n    }\n  }\n}\nIt was discussed along with #573, but never got implemented.\n. If you want the same config for both tests and sources, do babel: \"inherit\" in your config. The extends option is used for adding some extra transforms to test files that you don't want impacting sources.\nWith rewire you have an interesting case where you want to transpile your sources differently during testing than during production/publishing. I would recommend checking out the env option in .babelrc and setting an environment variable.\n. Your best bet is to have it default to installing rewire / everything you need for testing. That way your most common operation, running tests, needs no special environment variable set.\nUse envs to turn off that stuff when you go to build.\n. Your best bet is to have it default to installing rewire / everything you need for testing. That way your most common operation, running tests, needs no special environment variable set.\nUse envs to turn off that stuff when you go to build.\n. Interesting. \nThe downside is that it is not cross platform. I often develop with Webstorm full screen (navbar not visible). Also, you can't send text (i.e. the failing test title, etc).\n. Yeah - This is a legit issue. Related to #324, but not identical. I have tap-emitter in the works. \nhelp wanted!\n. Works for me.\n. What @novemberborn said.\n. > however would you wait for landing of #703 ?\nI don't think increasing to 3 would make things any worse. I believe the whole object get's serialized anyways. It is just the renderer that changes depth.\n. > t.throws() shouldn't reject literals. It should ensure something was thrown.\nI disagree. https://github.com/sindresorhus/ava/issues/468#issue-128347888\n. > t.throws() shouldn't reject literals. It should ensure something was thrown.\nI disagree. https://github.com/sindresorhus/ava/issues/468#issue-128347888\n. > you might argue if AVA should enforce this best practice\nI think it protects you from careless mistakes. \n. > you might argue if AVA should enforce this best practice\nI think it protects you from careless mistakes. \n. @rightaway, add \"require\": [\"babel-register\"] to your AVA config:\n\"ava\": {\n    \"babel\": {\n      \"presets\": [\n        \"es2015\",\n        \"stage-0\"\n      ]\n    },\n    \"require\": [\"babel-register\"]\n  },\n  \"babel\": {\n    \"presets\": [\n      \"es2015\",\n      \"stage-0\"\n    ]\n  },\nAVA currently only supports transpiling your test files for you. Not your source files. This is why you need to use babel-register. Note that we do intend to add the source transpilation feature at some point, so be sure to watch release notes when upgrading versions.\n. @rightaway, add \"require\": [\"babel-register\"] to your AVA config:\n\"ava\": {\n    \"babel\": {\n      \"presets\": [\n        \"es2015\",\n        \"stage-0\"\n      ]\n    },\n    \"require\": [\"babel-register\"]\n  },\n  \"babel\": {\n    \"presets\": [\n      \"es2015\",\n      \"stage-0\"\n    ]\n  },\nAVA currently only supports transpiling your test files for you. Not your source files. This is why you need to use babel-register. Note that we do intend to add the source transpilation feature at some point, so be sure to watch release notes when upgrading versions.\n. Hmm. That's weird. Try npm install babel-runtime\n. Hmm. That's weird. Try npm install babel-runtime\n. Can you create a project with a minimal reproduction and upload to github? It will be easier for us to troubleshoot that way.\n. Can you create a project with a minimal reproduction and upload to github? It will be easier for us to troubleshoot that way.\n. I think the best criteria by which to judge is this:\nIf someone who has never used tap/AVA/chai/etc is reading the test, what gives them the best chance of understanding what the assertion does without looking at the docs. Part of this includes avoiding ambiguity where possible. If a word could mean two different types of assertions, that's a big knock against it (we won't reach perfection here, because English).\nShort names should be low on the list of priorities. There is a limit at which something is too long, but a few extra characters is worth any added clarity.\n. I think the best criteria by which to judge is this:\nIf someone who has never used tap/AVA/chai/etc is reading the test, what gives them the best chance of understanding what the assertion does without looking at the docs. Part of this includes avoiding ambiguity where possible. If a word could mean two different types of assertions, that's a big knock against it (we won't reach perfection here, because English).\nShort names should be low on the list of priorities. There is a limit at which something is too long, but a few extra characters is worth any added clarity.\n. My concern with t.is is what happens to users who fail to update current t.is assertions. We leave them open to subtle bugs. Also, it doesn't pass the disambiguation test.\n. My concern with t.is is what happens to users who fail to update current t.is assertions. We leave them open to subtle bugs. Also, it doesn't pass the disambiguation test.\n. > Anyone interested in trying to make one (I especially encourage community members to try)? \nLooping in our AST friends: @twada @jfmengels\n. We will use GH' squash and merge.\n. A few notes, mostly related to making sure we are deprecating t.same without breaking it.\nOnce those are cleared up, LGTM.\n. LGTM.\n. I think this is better solved in resolve-cwd. Shouldn't it throw if it can't be resolved?\n. Rethinking my first response. This does allow us to throw a more specific error, redirecting the user to their AVA config.\n. @jfmengels points out another potential benefit of this in our linter: https://github.com/sindresorhus/eslint-plugin-ava/pull/56#issuecomment-204984896\n. > but I noticed that it will also make linting some stuff harder\nI am not sure if Eslint has an equivalent to Babel's bindings. It would make linting macros pretty easy. I have not seen anything in the docs though.\n. @jfmengels - What if, instead of storing a reference, you had to pass a string identifier, and it became just another macro.\n``` js\ntest.macro('myMacro', (t, input, expected) => ...);\ntest.myMacro('input', 'expected');\ntest.only.myMacro('input', 'expected');\n```\nI think that makes static analysis a little easier (especially if you define macros in helpers).\n. My only concern with your proposals is the verbosity. I was already concerned about that with my second proposal here, your proposals just make it worse.\nIdeally, we could use my first proposal, as it is the least verbose. My second one was just to address the static analysis problem.\n\nIMO, most of your concerns aren't that big a deal:\n\n1. tampering of the AVA import, which will not affect other tests but is still pretty ugly\n\nExactly - it won't affect other tests (because we fork), so no surprising behavior. You define which macros you load in each test (so no trouble figuring out what is happening).\n\n2. to import a file who'd have a side-effect of defining a macro, which seems counter to AVA's explicit imports (compared to Mocha where describe, it that are injected for instance). I like explicit imports.\n\nI think in the case of a helper defining macros - the user has decided to do that themselves. AVA's not doing that to them. They won't be surprised by it. In general, I agree with your sentiment, but this seems like a reasonable exception.\n\n3. Odd behavior when definining a macro called only for instance.\n\nWe could protect against macros that try to overwrite built in properties.\n. Hmm. Proposal 4 looks pretty good, and it should be really simple to implement.\nI like it.\n. > Alternatively, args could be added to the context: t.context.args === [2, 3].\n:-1: You can completely replace t.context in a beforeEach. t.context = something.\n. Rather than spreading the arguments ourselves, we should pass additional args as an array:\n``` js\nfunction macro(t, additionalArgs) {\n  if (additionalArgs.length === 2) {\n    // ...\n  } else {\n    // ...\n  }\n}\ntest(macro, 'foo', 'bar');\ntest(macro, 'foo');\n```\nThis allows us to pass additional args in the future breaking changes. With destructuring, defining your macro isn't really any more verbose:\njs\nfunction macro(t, [foo, bar]) {\n  // ...\n}\n. > What separates proposal 4 (test(macro, 2, 4)) from calling test with too many arguments?\nThere would be no such thing as \"too many arguments\". Everything past the test function would be passed as \"additional arguments\". The macro function itself would get an array as a second argument, containing all the \"additional arguments\"\n\nCan the macro define the test title? \n\nI was thinking about titles too.\nHere was my thought:\n``` js\nfunction macroFn(t, ...additionalArgs) {\n  t.is(...);\n}\n// optional: attach a title generator to macroFn\nmacroFn.title = function (...additionalArgs) {}\n// optional: attach a title template to macroFn (not sure which template language to use)\nmacroFn.title = '$1 does thing $2'\n// title is extracted from string:\ntest.macro('title', macroFn, ...additionalArgs);\n// title generated using titleFn or template\ntest.macro(macroFn, ...additionalArgs);\n```\n\nDo we want to support macros that create multiple tests?\n\nI think we could support it in a limited way by allowing arrays of macros:\n`` js\nfunction aPlusB(t, [a, b, expected]) {\n  var result = calculator.parse(${a} + ${b}`);\n  t.is(result, expected);\n}\naPlusB.title = \"${1} + ${2} === ${3}\";\nfunction bPlusA(t, [a, b, expected]) {\n  var result = calculator.parse(${b} + ${a});\n  t.is(result, expected);\n}\nbPlusA.title = \"${2} + ${1} === ${3}\";\nconst addition = [aPlusB, bPlusA];\n// these two lines create 4 total tests\ntest(addition, 3, 4, 7);\ntest(addition, 2, 2, 4);\n``\n. I think justtest().test.macro(macroFn)` feels too long\n. > Still not sure why you think the test should be given an array of args ... Do you expect a third argument to ever come into play?\nThat's why ... allowing for the possibility of a third argument.\n. Whatever we decide to support via static analysis, we need to have a well defined behavior for when it fails (either with a false-positive for .only usage, or a false-negative).\nIn the case of false-positives I think the solution is non-trivial, but pretty straightforward:\n1. Run the files we believe contain .only tests (based on static analysis).\n2. If we have a false-positive in a single file, and there are more files where we believe .only tests exist, do not run any tests from that file, just mark the file as a false-positive and kill it.\n3. If we get through the complete list of files we believe have exclusive tests, but find everything was a false positive. Go back and re-run any files marked as false-positive.\n4. In watch mode - use the dependency change algorithm to store and invalidate the false-positive markers.\nfalse-negatives on the other hand present quite few problems:\n- If all files appear (via static-analysis) to not use .only, but we discover one that does. Then just stop executing any non .only tests from that point on.\n- The big problem is when we have a mix of true-positives and false-negatives. If we only launch the true-positive files, then we never get to discover the false-negatives. This is problematic. If a user specified the .only extension however, I think it's really problematic to make the user wait while we launch additional files (possibly hundreds) to verify they have no .only calls.\n- In watch mode we could proceed to launch all the files we believe do not have .only calls, and verify while waiting for file changes to be reported. We could then store that verification and use the dependency change algorithm to determine which files need to be rechecked.\n. Just making sure I understand. Rewrite .only to .__only__ using a custom babel transform.Then during runtime __only__ will function as an exclusive test, .only works as a normal one?\nI am tempted to agree with it because it seems easier than static analysis. At the same time, I think this cripples a pretty key use case (watch mode and helper generated tests).\n. I think we cycle through all the tests once in watcher mode, guaranteeing all the only tests are found. In a single run, there is a chance dynamic onlys will be missed - If you are using dynamic only, you really need to use watch mode. \nWe could use is-ci to trigger the full scan, but people should not be checking in only tests so supporting that use case seems counter productive.\n. I have a start on static analysis here: https://github.com/jamestalmage/babel-plugin-ava-test-data/issues/1\n. I am not sure what to do with dynamic .only. There are no solutions I love.\nShould dynamic .only throw for is-ci?\nWe should definitely print a warning.\n. That's what I was thinking\n. > Now it will run all the tests before letting you know you have an issue and then you'll have to rerun the whole test file again. Just a thought.\nNo reason we can't report it as an error as soon as it's encountered. \n. Why not? If it increases the error count, that's noticeable immediately.\n. > we should limit the scope to what's actually needed\nYeah - drop the watcher stuff. The rest is definitely useful (I'm thinking ava-codemods and eslint-plugin-ava could both benefit).\n. > This doesn't look config related though. More like util.\nExcept the answer depends on your config. Those are the methods we really need. Actually, I missed the important ones:\ngetTestFiles(), getSourceFiles(), getHelperFiles().\n. LGTM\n. I think the message should include instructions to \"use test.skip instead\".\nWhen #673 lands we can amend it to say \"use test.skip or test.fail instead\".\nOtherwise :+1: \n. Use implementation.\n. I pushed a commit addressing all the comments. It's not showing up. https://status.github.com/ is showing yellow.\n. I want to take this on. #713 is a refactor in preparation for this. I'd appreciate a quick review on that\n. I would have, but on mobile\n. @scottnath - Scanning node_modules has all sorts of nasty implications, and it's really not where you want to be storing your tests (you shouldn't be packaging your tests when you deploy anyways).\nNow, If you really have some test helper functionality that you want to reuse across modules, that's great. Just make them a utility you can require.\nYou could do something like this:\n``` js\nimport test from 'ava';\nimport commonTests from 'my-common-tests';\nimport moduleUnderTest from '../';\ncommonTests(test, moduleUnderTest);\n```\nmy-common-tests/index.js:\n``` js\nmodule.exports = function (test, moduleUnderTest) {\n  test('it does this thing', function (t) {\n     t.is(moduleUnderTest.thing(), ...);\n  });\ntest('it does this other thing', function (t) {\n     t.is(moduleUnderTest.otherThing(), ...);\n  });\n// ...\n};\n```\n. How about this: https://github.com/sindresorhus/ava/commit/c78e7367d07ea1a27158a35972184de266d04230\n:stuck_out_tongue_winking_eye: \n. I set reporter.api to the new testData instance with each test-run event.\nI added a to-do comment that that should be cleaned up. I might just change it to pass testData as a param to reporter events. Some reporters could be stateless that way.\n. :shipit: \n. I started async-task-pool with this in mind.\n. @vdemedes brought up a good point that async-task-pool really is a poor substitute for Bluebird's concurrency controls (promise.map(handler, {concurrency: poolSize})).\n~~Note that would mean creating a disposer. http://bluebirdjs.com/docs/api/disposer.html~~\nNevermind. Not needed for this. Getting ahead of myself\n. Sorry for the incomplete answer, got called away.\n\nWould a default behavior of unlimited sized pool maintaining current behavior and adding a flag for limiting the pool with the caveat that test.only won't work be acceptable as an interim (beta) behavior?\n\nYes.\n\nIf so would extra hands (read: my time) integrating @jamestalmage's async-task-pool be useful\n\nYes, definitely (but use Bluebird's concurrency controls). \nHowever, it's going to take a pretty deep dive into AVA's codebase, and it's a fairly complex issue. We're also going to have strong opinions on how it impacts the programmatic API. I'm not trying to scare you off helping (we would love it, really!). I just want you to know what you're getting into.\n. Can you just open a PR? It's easier for us to evaluate that way\n. Wait for #713 to land. Then check lib/ava-files\n. Also, this would allow transforms to participate in transform caching. (transform plugins would need to expose a cache-salt in order to participate).\n. For tests you need https://github.com/sindresorhus/ava/issues/631#issuecomment-198973019 to land first\n. Trying to understand what we are fixing here:\nSeems like a few things:\n- Proper handling of relative paths that start with ../\n- Source patterns can specify node_modules/some_dir even though ignore patterns still have node_modules/**\n- Using a negation pattern in sources (!lib/somedir/**), will not override default ignores\n. It just seems we have a whole lot of code manipulating glob patterns.\nIgnoring for a moment our intent to refactor much of that to ava-files, is there a good reason we have made it so complicated?\nThe code looks good, just wondering if we need to rethink some of the design decisions that led us here.\n. :tada: Looks good @novemberborn \n. One minor nit on the docs. \nOtherwise LGTM\n. LGTM\n. > The default exclusion patterns for source files are relative to the current working (node_modules/**/*) however the test file exclusion patterns can apply anywhere (**/node_modules/**)\nI think the source file pattern should be made to match the test file pattern.\n\nI guess that makes sense, though I'm not sure how to override the default exclusion patterns. Maybe that's not necessary for test files?\n\nAren't you doing exactly that (overriding the defaults) in #614?\n. > coverage may be tricky.\nWhy?\n. Needs a rebase, but LGTM\n. :+1: The output looks really slick now!\n. Not without using babel-register. This is definitely on the list of future enhancements though.\nIf you really need it now check out the Babel config recipes in the docs, and use --require babel-register\n. You need to wrap any statement in a function. It's not possible for us to catch the Error otherwise.\nFor one liners like that, arrow functions are where it's at:\njs\ntest(\"Expect FeedAggregator to throw error when subclass has no getItems method\", t => {\n  t.throws(\n    () => new BadAggregator({feedName: 'test', feedType: 'test'}), \n    TypeError(\"Class must define a getItems method\")\n  );\n});\n. Thanks @sotojuan \n. Updated with relative file paths and colors:\n\n. > I know it is an overkill, but would be incredibly cool to have syntax highlight in errors (not just this one).\nLike this?:\n\n. I think maybe the middle section:\nThe following expression:\n  t.throws()\nIs kinda useless with the codeframe right above. \n. \n. > Should update the default plugin sections in the readme and babelrc recipe.\nI went with just the babelrc recipe. I feel like most user's don't care, so I don't want to pollute the readme with every side-effect free plugin we install. Really, we could just update the readme to say. \"AVA always includes a few internal plugins regardless of configuration, but they should not impact the behavior of hour code\", and then link to the Notes section of the babelrc recipe.\n. Feedback incorporated.\n. Yes we should. Issue #816 opened.\n. Yes we should. Issue #816 opened.\n. This is actually already fixed in master due to the API refactor.\nWe were doing:\njs\nself.on('timeout', ...);\nWhere self was the API instance, and that was happening for every rerun.\nWe now attach to individual RunStatus instances.\nI don't see a good way to write tests for this, but it's definitely fixed.\nClosing.\n. The API refactor was rather substantial. Substantial refactors introduce substantial regression risk. We keep them on master and dog food them on our own projects for a while before deploying. The warning is a nuisance, but can be safely ignored (you are unlikely to keep the watch daemon running long enough to be a problem). \nYou are welcome to install from master and help drive out bugs.\n. It's definitely an AVA bug, just not one worth rushing to release a major refactor that hasn't been dogfooded yet.\n. Throwing a TypeError is expected (our assertions are modeled after nod-taps, and don't throw).\nHowever, the failure message should print the result of the first failed assertion it sees.\nI just tested the following:\n``` js\nimport test from 'ava';\ntest(t => {\n    t.truthy({}.undef);\n    t.undef.uhoh;\n});\n```\nAnd I got the expected result (both 0.14.0, and master):\n```\n ./cli.js test/fixture/sync-throw.js\n1 failed\n\n[anonymous]\n\nt.truthy({}.undef)\n              |   \n              null  \n  Test.fn (synch-throw.js:4:4)\n\n```\n. I agree with @novemberborn \nnode --harmony-flag node_modules/.bin/ava\nNode already provides a working solution, no reason to create our own.\nClosing. If someone feels that's premature, feel free to reopen.\n. :-1: \nThe Node.js documentation pretty clearly states that uncaughtExceptions are a bad idea, and \"last resort\".\nIf you follow the documentations advice, using uncaughtException handlers only to \"perform synchronous cleanup of allocated resources (e.g. file descriptors, handles, etc) before shutting down the process\", then you may want to test that. But it is probably better to build testable abstractions into your code then attempting to install that handler inside a test runner.\nAs for asserting some code will throw an uncaught exception, I would recommend figuring out a way to abstract your code so you call the throwing code synchronously from your tests. (Use of lolex or mocking event emitters cover most use cases).\n. @carsy - I looked at your example. lolex should make it pretty easy to test nodeify.\n``` js\ntest(t => {\n  let clock;\n  try {\n    clock = lolex.install();\n    const wrapped = nodeify(function () {\n      throw new Error('foo');\n    });\n    wrapped();\n    t.throws(() => clock.tick(), 'foo');\n  } finally {\n    if (clock) {\n      clock.uninstall();\n    }\n  }\n});\n```\n. LGTM.\nI left a few comments, but they really require further discussion and shouldn't be holding up merging this.\nI say :shipit: \n. This is kinda what RunStatus has become. We definitely don't need the reset ability, because we create a new RunStatus for each pass. \n. OK, so apparently it was designed with RunStatus in mind. I will take another look this weekend, and close if I don't think it's worth doing anymore.\n. Yes. That sounds like a good plan. Not a big fan of writing code to design documents, but as long as we don't take them as gospel, and just use it to agree on some basic goals.\n. I've opened #865, which basically just documents what we have now.\n. > execSync is very slow.\nexecFileSync is better. I can't remember when it was introduced. (I wish the Node docs had a since: version notation).\n. Answering my own question:\nexecFile and execFileSync were both introduced in v0.12, so it would be better to just use execFileSync.\n. Answering my own question:\nexecFile and execFileSync were both introduced in v0.12, so it would be better to just use execFileSync.\n. > Might be useful if packageHash supports accepting an array of package paths to hash.\n:+1: \n\nMaybe even some an option with extra text to hash\n\nAre you mixing concerns at that point?\n. > Might be useful if packageHash supports accepting an array of package paths to hash.\n:+1: \n\nMaybe even some an option with extra text to hash\n\nAre you mixing concerns at that point?\n. You specify the date for lolex in a timezone independent way, but the output timezone dependent (it prints the local time in 24 hour format).\nI was seeing 1200 hours vs the expected 1700. I'm EST so that makes sense.\n. You specify the date for lolex in a timezone independent way, but the output timezone dependent (it prints the local time in 24 hour format).\nI was seeing 1200 hours vs the expected 1700. I'm EST so that makes sense.\n. Weird that Travis passes. Are they BST?\n. Weird that Travis passes. Are they BST?\n. Thanks!\n. Thanks!\n. This would be really helpful for our eslint plugin tests, which rely on eslints RuleTester utility. It throws plain old assertion errors.\n. This would be really helpful for our eslint plugin tests, which rely on eslints RuleTester utility. It throws plain old assertion errors.\n. @willin \nNot sure I understand the problem, but here's my thoughts on your three examples:\nYour first example will hang forever. It needs to use t.cb:\njs\ntest.cb('View 200', (t) => {\n  server.inject({\n    method: 'GET',\n    url: '/login'\n  }, (response) => {\n    console.log(response.statusCode); // 200\n    t.deepEqual(response.statusCode, 789); // not equal\n    t.end(); // you need to call t.end() when using `test.cb`\n  });\n});\nYour second example should fail since 200 !== 789.\nYour third example looks like it should pass, which it apparently does.\nCan you provide a more detailed description of what's going wrong?\n. @willin \nNot sure I understand the problem, but here's my thoughts on your three examples:\nYour first example will hang forever. It needs to use t.cb:\njs\ntest.cb('View 200', (t) => {\n  server.inject({\n    method: 'GET',\n    url: '/login'\n  }, (response) => {\n    console.log(response.statusCode); // 200\n    t.deepEqual(response.statusCode, 789); // not equal\n    t.end(); // you need to call t.end() when using `test.cb`\n  });\n});\nYour second example should fail since 200 !== 789.\nYour third example looks like it should pass, which it apparently does.\nCan you provide a more detailed description of what's going wrong?\n. > What's the problem here?\nI think it's that:\njs\n    console.log(response.statusCode); // 200\n    t.deepEqual(response.statusCode, 789); // not equal\nProduces output:\n\u2714 login \u203a View 200\n200\nWhich makes no sense. The log statement prints after the tests passes (and the test passes).\nI've tried to reproduce, thinking there's maybe some problem with deepEqual when used to compare literals (there does not appear to be).\n@willin - Something doesn't seem right. Can you post some code to a GitHub repo that we can use to reproduce?\n. @willin. \nPlease post some of your own code on GH in a repo we can clone that shows the problem.\n. Closing as OP is non-responsive.\n@willin - If you are able to elaborate on the problem more, and provide a minimal reproduction, please open a new issue.\n. Closing as OP is non-responsive.\n@willin - If you are able to elaborate on the problem more, and provide a minimal reproduction, please open a new issue.\n. @novemberborn - bump!\nI'd like to push forward with this. There are a number of TODO comments in this PR with your name on it (quite literally). I'd like your feedback on how we should proceed.\n. Assuming the rest of the AppVeyor runs pass, this should be mergeable. \n. @danilosampaio - Decent start. We probably want to colorize the output a bit. Here's mocha:\n\nI'm not sure what mocha uses, but we should look into that.\nAlso - Unexpected.js seems to put a lot of effort into creating beautiful error messages. It's worth looking into what they use as well.\n. It's definitely better than what we have. I think there is room for further improvements (like a diff view for larger objects / strings), but we can address that in a future PR.\nCan you add a test for this behavior. Probably in test/cli.js.\n. But it's strictly reporter related. I get the desire here, but if we are committed to keeping a small list of 3 reporters, giving those three files their own directory, while we let multiple reporter specific \"helpers\" proliferate in the parent directory seems silly.\nThe following files are all directly tied to reporting, but are cluttering up lib:\n- logger.js\n- colors.js\n- beautify-stack.js\nAdd enhanced-assertion to that list, and we've got more reporter related files living outside the reporters folder than in.\nCan we come up with a different folder name / structure that let's us move everything reporter related in there?\n. This needs a rebase. \nPlease ping me when it's done, and we will get this merged.\n. Nah. GitHub does that for us now. I now prefer people don't squash and let us do it from the web ui. Makes it much easier to track changes since I last reviewed.\n. This looks good to me. The reporters folder structure still feels a bit off, but we've let that hijack a PR that was never about folder structure in the first place.\nNice work @danilosampaio.\n. No, it would mean developing our own - similar to babel-plugin-ava-throws-helper\n. Hey @sotojuan - I've got a start on this already. I'm happy to give you a crack at it though. I'll push up what I've got and you can roll from there.\n. :+1: jscodeshift is a much more beginner friendly API, though neither it or Babel are well documented.\nYou should familiarize yourself with ast-types (look at ast-types/def/core.js or the estree project to learn the basic AST types). jscodeshift has it's own API that wraps ast-types, but inside a  jscodeshift forEach you are using the ast-types API.\nIf you find it overwhelming initially, then you're in the same boat as everybody else. Feel free to open issues on those projects or ping me on gitter. I watch all of them, contributing occasionally. I would like to contribute some docs to jscodeshift anyways.\n. Oh also make sure you start with astexplorer. It's a great resource to get started.\n. > is Babel capable of doing this for all types of asynchronous callbacks?\nIt can do it for the examples you listed, however there would be scenarios where it wouldn't work:\n``` js\nfunction commonCallback () {\n    throw new Exception('this is uncaught');\n}\ntest(t => {\n  client.get('redis_key', commonCallback);\n});\n```\n. I keep wondering if it wouldn't just be easier to do domains. Domains would neatly handle everything this proposal would, plus some stuff it wouldn't.\n. > domains don't work with ES6 promises yet, which is a big problem\nYeah, that's not great. They seem to work with bluebird and core-js promises though, so that would cover a significant portion of users.\n. > would it make sense to push it to npm or load it through GitHub? Feel silly publishing the same thing but with one extra =.\nDefinitely publish on npm. It's more than just one extra =. See the source. You will also need to rewrite a portion of the tests to reflect the updated behavior.\nI know @othiym23 has a number of these deep-equality comparison algorithms floating around. @othiym23 - Is there one that does what we are looking for? Basically deeper without recursing into the prototype or constructor? If not, it might be better to start with deeper, and eliminate this line\n. > largely because it seems absurd to me to have an identity test that says [] and {} are the same\nAgreed - that seems ridiculous. Do we need to rethink the decision to drop deeper? Perhaps bump the priority for adding custom assertions so people have a viable workaround if deeper doesn't work for them?\n. > This makes me think that it might be interesting to create some kind of identity generator, where you use an options argument and the function constructor to dynamically generate a JITtable equality test that does exactly what a given application requires, but I'm unlikely to have the time to hack on such a thing soon.\nInteresting idea. What options should it have?\n. It does remind you how long it's been since you last ran the tests. Minimally useful, but not very harmful to leave it as is.\nI'm indifferent on this really.\n. Yes. Very similar to deepStrictEqual.\n. Thanks!\n. Would #583 be enough to solve your problem?\n. Closing in favor of #583. (Hanging === Pending)\n. @calebmer - you still have a root package.json in your repo though, right?\nIf you do, just put a common ava config in there.\nroot/\n  package.json  (put ava config here)\n  packages/\n    pkg-1/\n    pkg-2/    (ava will traverse up the directories until it finds a `package.json`)\n. We don't support browser testing yet. You usually still have a package.json in Meteor projects anyways, so not sure how it would be a problem (I know almost nothing about meteor)\n. My feeling on options via cli.js: We should only add those where users would conceivably want to temporarily override the contents of their package.json config. Allowing different reporters on CI servers, running in --serial mode occasionally to debug, these are good examples.\nI haven't seen a use case yet where someone is arguing to change their babel config between test runs. Even if that was what they wanted, it would probably be best to use babels envs option to do that.\nI think this should be closed.\n. My feeling on options via cli.js: We should only add those where users would conceivably want to temporarily override the contents of their package.json config. Allowing different reporters on CI servers, running in --serial mode occasionally to debug, these are good examples.\nI haven't seen a use case yet where someone is arguing to change their babel config between test runs. Even if that was what they wanted, it would probably be best to use babels envs option to do that.\nI think this should be closed.\n. > The former: attempt to publish a package with unknown fields will result in HTTP status 400.\nWhat NPM server is doing that?\n. It's certainly closer to a legitimate need, but if it's in house no implementation of npm, just fix your implementation.\n. What does the ava section of your package.json look like.\n. > It appears that ava is not using the NODE_ENV or BABEL_ENV environment variables when transpiling test files.\nHow are you coming to that conclusion?\n. observable.forEach returns a promise for when the Observable has been consumed, so AVA will wait until it's consumed before finalizing the test. If you return an Observable directly, AVA will use observable-to-promise to accomplish the same thing. (observable-to-promise just uses forEach internally)\nYou can think of an Observable as an Array that arrives over time. It has lots of the same methods (forEach, map, etc). \nWe would track the expected iterable, and make sure it was empty once the returned promise ends.\n. > Would it not be simpler and friendlier if you had a method to compare a function to compare observable to an array? Or is that too narrow a use case?\nI think it's too narrow. Using the proposed solution, you do not necessarily have to pass the output of the iterable directly into an assertion. Here's another potential usage:\n``` js\ntest.cb(t=> {\n  const expectations = t.expectations([\n    {flagA: true, flabB: true, value: 'foo'},\n    {flagA: false, flabB: true, value: 'bar'},\n    {flagA: true, flabB: false, value: 'baz'},\n  ]);\nvar emitter = someEventEmitter();\nemitter.on('data', data => {\n    const expected = expectations.next().value;\nif (expected.flagA) {\n   t.is(data.A, XXX);\n}\n\nif (expected.flagB) {\n   t.is(data.B, XXX);\n}\n\nt.is(data.value, expected.value);\n\n});\nemitter.on('end', t.end);\n  emitter.on('error', t.end);\n});\n```\nIn the above example, as you add expectations that may or may not trigger assertions, you can see how it gets more difficult to count up what your plan should be. By using ensuring expectation is exhausted, you don't have to. That's a real big part of the benefit to this.\nFor your example regarding generated tests, you could just do this:\njs\n[1, 2, 3].forEach(function(i) {\n  test(t => {\n    const expectations = i === 1 ? ['foo', 'bar'] : ['foo', 'bar', 'baz'];\n    const expected = t.expectations(expectations);\n    return someObservable(i)\n      .forEach(val => t.is(val, expected.next().value));\n    });\n});\nOr better yet, when we implement macros\n``` js\nfunction macro(t, input, expectations) {\n  const expected = t.expectations(expectations);\n  return someObservable(input)\n      .forEach(val => t.is(val, expected.next().value));\n}\ntest(macro, 1, 'foo', 'bar');\ntest(macro, 2, 'foo', 'bar', 'baz');\ntest(macro, 3, 'foo', 'bar', 'baz');\n```\n. > The only gain I see with expectations (and I would like to see more if you see others) is that it's possible to track on AVA's side whether the generator has ended at the end of the test.\nWell, in a sense, you are correct, that's the only gain. But I think that is a pretty big one:\n``` js\ntest.cb(t => {\n  t.plan(3);\n  var emitter = someEmitter();\n  var fooExpectations = ['fooA', 'fooB'];\nemitter.on('foo', data => t.is(data, fooExpectations.shift());\n  emitter.on('bar', data => t.is(data, 'bar');\n  emitter.on('end', t.end);\n});\n```\nThe above example is problematic - I need to count up the number of listeners tied to each event, and how many times they will be triggered, and add them all up. Easy above, harder as the complexity of your test grows. Also, plan can't tell you which assertions got called, just how many. What if the bar event just triggered three times, followed by end? The test still passes (but definitely not why I thought). To make my test foolproof, I need to modify how I end:\njs\nemitter.on('end', () => {\n  t.is(fooExpectations.length, 0);\n  t.end();\n});\nThat should work, right! Oops no, I need to update my assertion plan to 4! \ud83d\ude0f \nMy point is that I am often trying to shoehorn plan to mean \"All these expectations got met\", and there are plenty of situations where that leads to verbose boilerplate, and plenty of ways to mess it up.\n\nI think that what I would like to see is an API that does not force the user to learn potentially new concepts (generators)\n\nThey wouldn't be exposed to generators. Just the iterator API, which is just the next() method.\n\nRE: validateArrayLikeThing\n\nI don't see how validateArrayLikeThing knows when it's done and it's time to end the test. ~~How does it know there is more data available to validate against?~~ Oh - I see, the promisification of the input. That means all those assertions don't run until the end though, with expectations, stuff is run as events are received - which means tests can be failed faster. Also, sometimes you need to run assertions at a given point in time, because the object your testing has mutable properties.\n. > Note: Both methods pass but should fail if you have this in your observable\n\n---foo---bar---baz---undefined--undefined--\n\nYeah, our iterator should detect if they call next() and get back {done: true} (they never should - that's an error meaning they tried to call next to many times - so their plan is off). We could extend the iterator API to give them a hasNext() implementation like Java (I much prefer the Java iterator style).\n. I just decided that using a spread argument is a bad idea. Instead t.expectations will take a single argument that is iterable (doesn't have to be an array).\nI went through and updated all the discussions above so they use the array form\n. I do think @jfmengels validator idea is pretty cool. I have some different ideas on how to do it though. I will open another issue.\n. See #795 for a more complex idea for building validators.\n. > we can use Bluebird's concurrency parameters instead of a new module\nFair enough. async-task-pool really only becomes valuable when there is an expensive resource tied to the concurrency (i.e. delegating tasks to long-lived child-processes, where the child-process only handles a single task at a time, but will handle multiple tasks before being closed). Bluebird disposers might do just as well as async-task-pool on that issue as well.\n. > > Tests we don't need to recompile should be launched first.\n\nThis one can be accomplished soonest of all, like this idea\n\nNo point until we have batching. Right now, it doesn't matter which order we launch them in, we wait for the stats on ALL of them before starting up tests.\n. @talexand \nPlease change your code as follows:\n``` diff\nfunction testButton(button) {\n--  t.equals(button.textContent, 'not clicked');\n++  let textContent = button.textContent\n++  t.equals(textContent, 'not clicked');\n    ReactTestUtils.Simulate.click(button);\n--  t.equals(button.textContent, 'clicked');\n++  textContent = button.textContent;\n++  t.equals(textContent, 'clicked');\n    t.end();\n  }\n```\nAlso, it appears you are using t.end() twice, which you shouldn't.\nIf the above changes fix your problem, or you've moved on, please close this issue.\n. @talexand \nPlease change your code as follows:\n``` diff\nfunction testButton(button) {\n--  t.equals(button.textContent, 'not clicked');\n++  let textContent = button.textContent\n++  t.equals(textContent, 'not clicked');\n    ReactTestUtils.Simulate.click(button);\n--  t.equals(button.textContent, 'clicked');\n++  textContent = button.textContent;\n++  t.equals(textContent, 'clicked');\n    t.end();\n  }\n```\nAlso, it appears you are using t.end() twice, which you shouldn't.\nIf the above changes fix your problem, or you've moved on, please close this issue.\n. > I think the tone could be warmer,\nYeah, I struggled with that. I don't want to add a lot of verbosity getting there though. It's already pretty long, and I don't want to add flowery language that doesn't help them just get on with it.\nI'm sure there are ways to improve it that would keep it short and sweet though. I'm open to suggestions.\n. OK, I incorporated the feedback, and tried to make it a little less intimidating with https://github.com/avajs/ava/pull/787/commits/4edabb9895425cbe3ec0de5b11c964748d6e8259 \n. https://github.com/jonschlinkert/to-clipboard\n. Yeah sure.\nWe could even force them to specify a single test file:\njs\nava --report test.js\nThen grab the source for the failing test and include it as well as the stack trace\n. > In that case tests could take up a lot of space in an issue. I suggest creating an anonymous gist with the contents of a test file. Authorization (api key) is not required for it.\nYeah, I was worried about the size of the test file being included as well. My thought was we could use some babel tricks to pick a single test function out of the larger test file. (maybe grab beforeEach methods too? That could still get pretty verbose.\nMy main concern with automatically posting a gist is that we have no way of knowing if it contains sensitive information (security keys etc). I like the idea, but the security hole is a pretty huge downside IMO. We could give them a big old warning prompt I guess.\n\nCan I post test/foo.js to an anonymous gist on Github? You should say NO if test/foo.js contains any sensitive information like private keys/ etc.\n. > I think the prompt is a better solution. Babel trickery will bring complexity and more bugs, as a result.\n\nWhere did it go? Did you change your mind? I think it's a fair point. We may be implementing some of that Babel trickery anyways. \nBetween the Babel helpers and Eslint stuff, we're getting pretty good at static analysis. We would definitely provide fallbacks. I think long term, getting good bug reports is going to save us a lot of maintenance time and overhead - so it is worth some investment. I know I waste a good amount of time chasing down bug reports that would have gotten solved a lot quicker with the right info.\n. > Did not understand what you mean by that.\nI had a notification and email with your message, but it wasn't showing up on the page. Thought you deleted it. I think GH was having issues. I had commits taking a long time to appear, etc\n. @jfmengels is assigned!\n. js\nMath.max((os.cpus().length || 1) * 2, 2)\nCan be simplified to just:\njs\n(os.cpus().length || 1) * 2\n. js\nMath.max((os.cpus().length || 1) * 2, 2)\nCan be simplified to just:\njs\n(os.cpus().length || 1) * 2\n. GH isn't letting me comment directly on test/api.js because the diff is too huge.\nhttps://github.com/dcousineau/ava/blob/process-pool/test/api.js#L920\nDoes this test cover the scenario where we have more files than concurrency? We would need tests for:\n- No match anywhere\n- No match in the first batch, match in a latter batch\n- Match in the first batch, no match in a latter batch\n. Similar here - https://github.com/dcousineau/ava/blob/process-pool/test/api.js#L769 \nDo we need a test that verifies the test counts are added up correctly across batched runs?\n. Let's kill ready in a separate PR.\nIf you can \"circle back\" with a few more tests, that would be great. We want to guard against regressions.\n. Well done!\n. > Hmm. Thinking out loud. Are we attacking this from the wrong side? Maybe we should color the test title in the reporters that needs it, not remove it here?\nMaybe, however, removing it here might be a good idea if TAP hates ansi codes in the test title.\nI was just writing a test macro:\n``` js\nfunction macro(obj, expected) {\n  test(util.describe(obj, {colors: true}), t => {\n    t.is(fn(obj), expected);\n  });\n}\nmacro({someObj}, 'foo');\nmacro({someOtherObj}, 'bar');\n```\nI thought the output with the verbose reporter looked awesome:\n\nMaybe allowing this causes more headaches than it is worth, but I thought it was pretty cool!\n. strongloop/fsevents#128 is merged and released as a patch release.\nI'm closing. Users still experiencing this should rm -rf node_modules && npm install\n. strongloop/fsevents#128 is merged and released as a patch release.\nI'm closing. Users still experiencing this should rm -rf node_modules && npm install\n. This sells me on #784 even more.\nWhile I'm not sure this is worthy of making a core part of AVA (emitter testing is a little niche). It would be pretty simple to implement as a separate module if #784 existed.\n``` js\nimport buildValidator from 'build-ava-validator';\nconst def = {...};\ntest.ct(t => {\n  const validator = buildValidator(t, def)\n    .start()\n    .foo()\n    // ...\n});\n``\n. I considered making thethisvaluet`:\njs\nconst validatorDefinition = {\n  foo(expected, message) {\n    this.is(message, expected)\n  }\n}\nBut it might be nice to have helper methods on your definition\njs\nconst definition = {\n  foo(t, expected, data) {\n    t.true(someSpecialConditionForFoo);\n    this._helper(t, data);\n  },\n  _helper(t, data) {\n    // perform assertions shared across multiple event types. \n  }\n};\nIf we mandate helpers have a _ prefix, we could build a special this value that has both the assertions and helper methods on it:\njs\nconst definition = {\n  foo(t, expected, data) {\n    this.true(someSpecialConditionForFoo);\n    this._helper(data);\n  },\n  _helper(data) {\n    this.is(...)\n  }\n};\n. https://github.com/avajs/ava/issues/795#issuecomment-215252710\n\nWhile I'm not sure this is worthy of making a core part of AVA (emitter testing is a little niche). It would be pretty simple to implement as a separate module if #784 existed\n. > And wouldn't this be better solved with Observables?\n\nEventEmitters emit multiple named events. Observables only emit one. In a sense, EventEmitters are essentially a bag of Observables with a common completion. \nThis could help you test the interleaving of events between two different Observables. Same idea as above, but pass the validator functions to two different Observables, instead of registering with two different event names on a single emitter.\n\nbut I honestly very rarely test events\n\nWould you if it were easier to do? \ud83d\ude09 \n. Yeah - I'm not sure how you test interleaved Observables though. I've seen the marble diagram type tests, but that seems convoluted to me for most scenarios (could be a lack of experience).\nI really don't think there's any reason for this idea to end up in core. If we do it, it could totally be a separate module that just relies on #784.\n\nBadly phrased\n\nI figured that's what you meant. \n. @twada - Is this a power-assert issue? Can we close it and open up another issue in your repo? What would it be babel-plugin-espower?\n. @twada - Is this a power-assert issue? Can we close it and open up another issue in your repo? What would it be babel-plugin-espower?\n. Hmm.\nWe should probably throw in a few integration tests for that somewhere, just so it stays up to date.\n. Hmm.\nWe should probably throw in a few integration tests for that somewhere, just so it stays up to date.\n. > my guess is it hangs because power-assert can't process the reply object.\nYes, you are almost certainly correct. This is a known, issue and we are tracking it in #703.\n. > my guess is it hangs because power-assert can't process the reply object.\nYes, you are almost certainly correct. This is a known, issue and we are tracking it in #703.\n. No worries!\nUnfortunately, destructuring is the only way for now.\n. No worries!\nUnfortunately, destructuring is the only way for now.\n. You need to run the bench tests on both branches (this one and master). Then run node bench/compare\n. You need to run the bench tests on both branches (this one and master). Then run node bench/compare\n. @jfmengels - I don't think you pushed the fixed tests yet!\n. @jfmengels - I don't think you pushed the fixed tests yet!\n. Yeah, those numbers certainly aren't very enticing\n. Yeah, those numbers certainly aren't very enticing\n. > or maybe the gains are so small that it willt tilt one way or another if I'm doing something else on my computer while running the benchmark test\nAlmost certainly. I was surprised how much just browsing GH issues on Chrome would affect benchmark results.\n. > or maybe the gains are so small that it willt tilt one way or another if I'm doing something else on my computer while running the benchmark test\nAlmost certainly. I was surprised how much just browsing GH issues on Chrome would affect benchmark results.\n. I guess I'm \ud83d\udc4e for doing this. Gains are very minimal, likely because we cache so aggressively. \n. I guess I'm \ud83d\udc4e for doing this. Gains are very minimal, likely because we cache so aggressively. \n. > I think we should run the test on something pretty big LOC-wis and that needs a lot of transpilation, in order to really mesure Babel's influence\nThe issue also becomes, how much of a performance gain is worth the modified code frames for errors? I don't love that we reformat what people see in their terminal to be different from what's in their editor. We want them to be able to find that code fast. Humans are naturally very good at visual pattern matching, simply changing the shape of the code will make it harder to find. \nI think we cache babel transpilation results more aggressively than babel itself does, so even with big codebases, it is unlikely to make much of a difference except for the initial run.\n. > I think we should run the test on something pretty big LOC-wis and that needs a lot of transpilation, in order to really mesure Babel's influence\nThe issue also becomes, how much of a performance gain is worth the modified code frames for errors? I don't love that we reformat what people see in their terminal to be different from what's in their editor. We want them to be able to find that code fast. Humans are naturally very good at visual pattern matching, simply changing the shape of the code will make it harder to find. \nI think we cache babel transpilation results more aggressively than babel itself does, so even with big codebases, it is unlikely to make much of a difference except for the initial run.\n. There is, but it's broken ATM. See #798.\nIf you go back a version or two it might work.\nYou can find information on how to use it in contributing.md or maintaining.md.\n. Closing as a duplicate of #798.\nFix is in #812 \n. > How are those two issues related?\nRead this: https://github.com/sindresorhus/ava/blob/master/maintaining.md#profiling\n\nWhat is the state of #812?\n\nYou can track progress by subscribing to that PR. Currently only tests on Windows are failing.\n\nIs there anything I can do to support this?\n1. Figure out why Windows tests are failing in #812\n2. Write a Recipe on using the profile script. The section in maintaining.md is a good start, but add some introductory information on iron-node, and add some screenshots, etc. Probably best to change the focus from profiling to debugging.\n. That guy in the plaid keeps mispronouncing the project name and totally butchers contributor names. \ud83d\ude08 \n. We should add a test verifying what happens with afterEach.always if a beforeEach fails.\n\nI'm not even sure what that should be!\n. Just FYI, we use the new GitHub \"squash and merge\" feature on almost every PR, so you don't need to self squash or use commit --amend, etc. It actually makes it harder for me to review just the changes you've made in response to my comments. (Not a big deal, though).\n. @cgcgbcbc - You are real close. The implementation looks good to me, we just need a few more tests.\n\ud83d\udc4d \n. > The after.always and afterEach.always will always execute after after hook and afterEach hook respectively\nI don't think that's a problem. \n. @cgcgbcbc - All that's really left here are a few minor nits on the docs, etc.\nIf you can handle those, I think we're good to merge. If anything we've said is ambiguous and you can't get an answer from us right away, just use your best judgement.\n. @cgcgbcbc - All that's really left here are a few minor nits on the docs, etc.\nIf you can handle those, I think we're good to merge. If anything we've said is ambiguous and you can't get an answer from us right away, just use your best judgement.\n. If one doesn't already exist, yes.\n. > If not, what good is it, other than babel-register\nIt could be used for other things besides babel-register. \nRegistering, your desired Observable implementation via any-observable for instance.\nIt has nothing to do with performance (though using it can have performance side effects, as @spudly mentioned). Use npm@3 to minimize those, and know we are working on ways to improve the performance.\n. > If not, what good is it, other than babel-register\nIt could be used for other things besides babel-register. \nRegistering, your desired Observable implementation via any-observable for instance.\nIt has nothing to do with performance (though using it can have performance side effects, as @spudly mentioned). Use npm@3 to minimize those, and know we are working on ways to improve the performance.\n. Also, you should always throw new Error(message), instead of throw message.\n. Also, you should always throw new Error(message), instead of throw message.\n. Fixed in https://github.com/sindresorhus/ava/pull/774\n. Fixed in https://github.com/sindresorhus/ava/pull/774\n. A few notes:\n- I don't really understand what this recipe accomplishes, or how it's worthwhile for our users. Exactly what problem does it solve? That should probably be made clear at the top of the document (I advise you to convince us of the need for this recipe before investing more time in it though).\n- I think some of the language needs to be reworked. It feels a little too flowery / sales pitchy at some points:\n  -  note how much time the tests take ...  - It's unrealistic to expect them to actually run the code from the recipe. Show a screen shot. Or just dryly stating this way is slower because ... is fine to.\n  - ! #mindblowing - I'm all for exciting language and exclamation points where warranted, but let's do it while communicating something of value. This doesn't add value, a statement like That's a whopping 300% speedup! communicates facts while still slipping in some fun language that keeps it a light read.\nAll that said, your first task really is convincing us of the problem this solves, and then that this is the best way to solve it.\n. Yeah. I just haven't had time to fire up a Windows VM and figure out why.\nUbiquitous bash support can not come quickly enough.\n. Yeah. I just haven't had time to fire up a Windows VM and figure out why.\nUbiquitous bash support can not come quickly enough.\n. The new behavior is intentional: https://github.com/nodejs/node/pull/5950\nBut problematic when calling fork.\n. The new behavior is intentional: https://github.com/nodejs/node/pull/5950\nBut problematic when calling fork.\n. There is an open PR to revert the change: https://github.com/nodejs/node/pull/6537\n. There is an open PR to revert the change: https://github.com/nodejs/node/pull/6537\n. Tests fixed\n. Tests fixed\n. My thought is to merge and track Node 6 progress. Once the old behavior is restored and stable, revert this out.\nThe code I've copied here out of child_process has remained pretty stable, so I don't think we're in danger of missing out on any updates in the core module.\nStill, I agree, this isn't something I want to maintain long term, hence all the extra comments letting us know what and when to revert.\nI don't want to wait for Node to sort this out before I can use npm link again. \n. @dcousineau - Good Catch!\n827 should fix your issue. NYC still will not work if you are using npm link ava on Node 6. But it will always work on all earlier versions of Node, and will also work on Node 6 as long as you haven't npm linked AVA.\n. FYI: The NYC issue is because spawn-wrap does not support the -e flag.\nI have opened https://github.com/tapjs/spawn-wrap/issues/22, requesting that support for -e be added, but #827 works to solve this issue.\n. I'm of the opinion that using after / afterEach for cleanup is a bad plan. I think you should have a conditional cleanup in a before / beforeEach instead.\nIt's impossible to provide both fast failure and reliable post-test hooks, and I would rather not crippple the fast fail behavior in favor of something I think is an antipattern\n. > I've always done cleanups in the after hooks too.\nI did for a long time too, but things just work better the other way. I have more than once gotten myself into a situation where failing tests leave bad state on the file system that would be cleaned up in an after hook, but after hooks never get called because tests don't complete. Having after.always kinda solves the problem, but it still requires you run all your tests (which will fail because of dirty state) before the after hook gets run. This means you need to do a run you know will fail, just to cleanup state (or manually cleanup).\nCleaning up in a beforeEach also has a distinct advantage over always.afterEach when using fail-fast, in that the offending state is left in tact so you can examine it.\n\nIt doesn't sound very atomic to have to clean up after the previous test in the before hooks\n\nDon't think of it as cleanup of the past state, think of it as ensuring a start state for the current one. Relying on the test before you to properly cleanup after itself is what isn't atomic.\n. >  I've learned to use temp files for fixtures instead of putting them in the current directory, and they don't need cleaning up\nYes, that's pretty much what I always do now. Especially with AVA, since you need unique temp dirs/files for each test if you want to take advantage of concurrency. The only downside is that it is harder to find and inspect that temp location (requires adding a logging statement and cd-ing into the system temp folder).\n. @gajus - Which Node version are you using? I've noticed problems with streams not flushing entirely before process.exit much more on Node 6 than previous versions (that behavior has never been guaranteed, but it's much more problematic now).\n. Try in Node 5 and see if you have the same problem.\n\nSorry, that was a code in a private repository.\n\nCreating an MCVE without proprietary/closed source code is the best way for us to troubleshoot the issue.\n. You are using t.end incorrectly. Don't use it as a timeout method (don't use it at all here). Avoid test.cb when returning promises.\nThe reason is because AVA needs to know when to end the test. You can defer ending a test by returning a promise, or using test.cb / t.end. We don't allow both because that makes the end of the test ambiguous.\n. Sorry, we don't add features unless they solve an existing pain point for our users. I am closing, but if you run into a specific problem, please share the details so we can discuss how to solve it.\n. If that starts happening to you in real life, let us know, and we can discuss from there.\n. tarball removed.\n. Hmm. The \"failing test\" commit passes on Travis, but not Windows.\n. LGTM\n. > so should I leave the test in or not?\nI missed @novemberborn's comment on the first pass. IDK. It guards against a regression, so I think keep it.\n. Currently you have to do it the same way you do it in mocha. We are discussing an alternate solution in #695\n. Agreed, needs quite a few more tests.\n- [x] failing sync test fails - result is passing\n- [x] failing test.cb test fails - result is passing\n- [ ] failing test with t.throws(nonThrowingPromise) - result is passing (test async assertions)\n- [ ] failing test returns a rejected promise - result is passing\n- [ ] failing sync test passes - result is a failure with helpful error message\n- [ ] failing test.cb test passes - result is a failure\n- [ ] failing test with t.throws(throws) - result is failure (test async assertions)\n- [ ] failing test returns a resolved promise - result is failure\n. > Just for tracking\nAny reason you haven't tackled the last two?\njs\n// this should pass\ntest.failing(t => {\n  t.throws(Promise.resolve('foo'));\n});\n// this should fail\ntest.failing(t => {\n  t.notThrows(Promise.resolve('foo'));\n});\n. This should fix your failing test:\n``` diff\ndiff --git a/test/test.js b/test/test.js\nindex d2f3f1b..9fd23a9 100644\n--- a/test/test.js\n+++ b/test/test.js\n@@ -676,10 +676,10 @@ test('failing test with t.throws(nonThrowingPromise) is passing', function (t) {\ntest('failing test with t.throws(throws) is failure', function (t) {\n    ava.failing(function (a) {\n-       a.throws(Promise.resolve('foo'));\n+       a.notThrows(Promise.resolve('foo'));\n    }).run().then(function (result) {\n        t.is(result.passed, false);\n-       t.is(result.message, failingTestHint);\n+       t.is(result.reason.message, failingTestHint);\n        t.end();\n    });\n });\n```\nYou should update the test title with t.notThrows\n. Awesome!\nLGTM!\n. @cgcgbcbc - Can you add documentation for this to the Readme?\n. > do you think it's necessary to check that the failing modifier can only be used with test and throws error when used with hooks?\nNo, it's conceivable you could write an afterEach test that failed.\njs\ntest.failing.afterEach('always leaves X in pristine state', t => {\n  t.true(isPristine(t.context.state));\n});\nThat's enough of a corner case that it should probably be flagged by the linter though.\nCan you open an issue in eslint-plugin-ava about that?\n. I just did it: https://github.com/sindresorhus/eslint-plugin-ava/issues/108\n. This looks great to me!\nJust need a second reviewer to merge.\n// @sindresorhus @novemberborn @vdemedes @sotojuan \n. What about @nfcampos argument? Should it be arrays both places? Spread both places? \n. > I'm leaning towards spread in both places.\nMe too, but that pretty much closes that portion if the API from further expansion. That's a big decision, so let's wait for more input. \n. Updated:\n- Switched to arg spreading everywhere.\n- Added array of macros support.\n. Should the macro.title function get passed title as the first arg?\n``` js\nfunction macroA() {...}\nfunction macroB() {...}\nmacro.title = (title, ...remainingArgs) => title + ' - macroA';\nmacro.title = (title, ...remainingArgs) => title + ' - macroB';\ntest('groupA', [macroA, macroB], 'some', 'args');\ntest('groupB', [macroA, macroB], 'more', 'args');\n```\nresults in the following test titles:\ngroupA - macroA\ngroupA - macroB\ngroupB - macroA\ngroupB - macroB\n. I think defaulting to the empty string is a good plan. That's still falsy. We should trim results both left and right I say.\n. For spacing, what if we allowed them to return an array of strings which we joined with a space character?\njs\nmacro.title = (title) => [title, 'macroA'];\nOr maybe we allow this:\njs\nmacro.title = {prefix: 'macroA - '};\n// or\nmacro.title = {suffix: ' - macroB'};\n. 51bed36 passes title as first argument (or empty string if none provided).\nI think this should be merge-able now.\n@sindresorhus - Haven't heard much from you on this. Specifically, are you good with spreading the args?\n. See #222\n. This is a pretty major bug, and a straightforward fix. Should be merged ASAP.\n. I didn't catch it. Reported on Gitter.\n. I will tackle the Babel transform. @cgcgbcbc is working on the logger output in #837.\n. That could get verbose. Part of the idea is to make it clickable in iTerm. You can always click into the file and read comments directly\n. My thought was to keep it succinct. For large projects, you may have a number of known issues that are long lived. I want provide just enough output to nag people into action, without flooding the console with stuff they will likely ignore.\n. @cgcgbcbc - see https://github.com/sindresorhus/eslint-plugin-ava/issues/110#issuecomment-219383866 for the desired mini reporter format. \nDon't worry about figuring out how to gather the issue links. We will figure that out in https://github.com/sindresorhus/ava/issues/836 .\nYou can ignore the issue links part entirely for this PR if you want. Or you can check for it conditionally. \njs\nif (test.issueLinks) {\n  test.issueLinks.forEach(...)\n}\n. @cgcgbcbc - We want to land this before cutting a new release. Any chance you can finish this up in the next day or two?\n. I like the red check for the expected failure in verbose mode. Nice touch!\n. :shipit: \n. I think we should pass an options object to the reporter constructor with static details/config like isWatching. \nIt definitely shouldn't be a boolean param. {watching: true}\n. Just so we're all on the same page about reporter constructor options. IMO, I think the reporters should avoid maintaining any mutable state, especially as it relates to the test run status. We are violating that rule in a couple spots now (my fault, I was supposed to follow up after merging the PR that introduced RunStatus). \nI do think it's fine to pass immutable config values that will never change during the test run (i.e. watching, basedir, pkgConf, debugging, etc). That should keep our reporters as state free and easily testable as possible, without necessitating passing around needless config data.\n. @sotojuan \nMove the two lines @novemberborn talked about here into the test that uses it.\nOnce you've done that (and tests pass), go ahead and merge.\n. Good catch! Can you add a test to test/cli.js that protects against future regressions?\n. LGTM\n. If we do this, it has to be B. You are allowed to completely replace the context:\njs\nt.context = someThing()\nAlso, it would have to be for after.always hooks (we don't run after on failures without that modifier). Note that feature not shipped yet (but should soon).\n\nthis is required for UI tests running on saucelabs and that feature in mocha was born out of the same need: mochajs/mocha#797\n\nThree stars for describing a real world need for the feature! \u2b50 \u2b50  \u2b50 \n. @rhysd - That is a really cool idea!\n. @rhysd - That is a really cool idea!\n. See #369\n. Thanks @nfcampos \n. Yes. I was thinking of Sinon when coming up with this.\n. I do like that Sinon allows you to attach predicates. What if we allowed the same thing, but they exactly matched existing assertions:\njs\nt.match(obj, {\n  foo: t.match.is('foo') // obj.foo must be strict equal\n  bar: t.match.deepEqual({baz: 'quz'}), // traditional deepEquals, must only have the baz property\n  baz: t.match.throws(), // a rejected promise?\n  quz: t.match.regex(),\n  promise: t.match.then.deepEqual() // same as t.match, but for promise values?\n});\nEssentially, t.match.XXX becomes a way to curry assertions with the expected argument.\nWe could of course provide shortcuts. Primitives would shortcut to t.match.is, and object literals would shortcut to t.match.match\njs\nt.match(obj, {\n  foo: 'bar',  // equivalent to t.match.is('bar')\n  bar: {   // not the same as above. Equivalent to `t.match` instead of `deepEquals`.  \n    baz: 'quz' // We only check the baz, property, ignoring the rest\n  }\n});\n. It would be awesome to allow you to build reusable predicates this way, so I think you expose the predicate builder on test as well.\n``` js\nconst isIdentifier = name => test.match({\n  type: 'Identifier',\n  name\n});\nconst isT = isIdentifier('t');\nconst isAssertion = name => test.match({\n  type: 'CallExpression',\n  callee: {\n    type: 'MemberExpression',\n    object: isT,\n    property: isIdentifier(name)\n  }\n});\nconst isThrowsAssertion = isAssertion('throws');\ntest(t => {\n  // ...\n  t.match(node, isThrowsAssertion)\n});\n``\n. Your right. \n. I don't think we would add two separate function names. The question is whether leaf nodes of theexpectedargument must be primitives checked with===, or if we allow something a bit more flexible.\n. Let's dolike. I think it implies a weaker comparison thanmatchdoes (and it is indeed a weaker comparison than deepEqual)\n. @SamVerschueren \n. It would be good to land this as part of the upcoming release as well.\n. Yes. The only checks againsttodo` are here.\n- It can't contain an implementation.\n- It must have a title\n. @SamVerschueren Does it need something additional to enable chaining?\n. > But todo doesn't allow chaining right\nWe aren't actively preventing it, but I can't really think of a reason you ever would:\ntest.serial.todo\ntest.after.todo\nNone of those make any sense. Maybe we should prevent modifiers on a todo\n. Merging. We can debate what to do about todo chaining in #847.\n. While you are at it, I think there's an error when it comes to using test.serial. \ntest.serial tests are allowed to return a promise, or have callbacks.\ntest.serial.cb is totally allowed, as is test.serial(promiseReturning).\nThe point of test.serial, is that it makes the tests run independently without contention. .serial tests are run first, and one at a time, whereas the default is to run with max concurrency. They are allowed to be async, via .cb or returning a Promise/Observable.\nRight now, it looks like .serial tests are not allowed to return a promise, and that .cb.serial and .serial.cb are invalid chains.\n. Also, from what I can tell, we don't really prevent todo chaining, and chaining a todo really makes no sense.\nSee https://github.com/avajs/ava/pull/846#issuecomment-220851074\n. Non comprehensive list of validations:\n- always used with anything besides after or afterEach - already checked here\n- todo may not have an implementation, but must have a title - already checked [here]\n- todo should not be combined with any other modifiers. It's simply for documentation. So no skipped, no failing, no afterEach, etc.\n- skip.only makes no sense - should be a failure\n- failing probably doesn't belong on hooks\n- failing probably shouldn't be skipped\n\nWhere do you want the validation tests to be called from?\n\nCreate a new module lib/validate-test.js, and a new set of tests in test/validate-test.js.\n. Integrate it. So maybe one/two integration tests to test/runner.js that validates your new module is called correctly.\n. You wouldn't need cleanAll in an after hook. The process is going to be killed, so there's no good reason to clean up mocks at the end. after cleanup is only necessary for persistent state (i.e. disk state). \nYou might want cleanAll in an afterEach if all your tests were serial.\n\nWhen it comes to testing real endpoints and handling concurrency, you have a few options. \nLet's assume a simple social app. You have a \"friends list\", you can add and remove people from the list, and you can get recommendations for \"friends of friends\".\n1. Avoid concurrency all together, and just use serial tests, and a clean application state each time. This avoids any interference and ensures no test relies on state left over from a previous one.\n2. Allow concurrency, but make sure you use disjoint operations that do not interfere with each other. In the social app described above, this would mean creating all new user accounts for every test (using randomized strings for account names or something). You could allow all these tests to run against the same server at once because there should be no data overlap. None of your friends in the current test are friends with any accounts created in other tests, so you shouldn't be getting any inter-test interference in the \"recommendations\". There is some risk here of creating inter test dependencies if you aren't careful, but if done correctly it also stresses your production code in a way that 1 does not.\n3. Launch a new app server for every test, using get-port to find open ports. This is probably only advisable for lightweight apps that can be spun up and down very fast.\n. download seems to have a decent test suite. We could use that as a template for our nock recipe. (note that doesn't really test an endpoint as much as mock one).\nOne thing I was surprised by: That test suite doesn't use serial. I think that's a bit problematic. It's conceivable the user could get themselves into a situation where they are getting the wrong mock if they unwittingly add an async operation before the underlying call to http.request. @kevva - Thoughts?\n. All tests in the file are launched at once, with no guarantee as to how they will interleave.\nIt's entirely possible your order could be this:\n- beforeEach for test1\n- beforeEach for test2\n- test1\n- test2\nIndeed if you made beforeEach asynd, it probably will interleave like that:\njs\ntest.beforeEach(async t => {\n  // ...\n});\n\nAnother possibility, if anything in the chain (download->got->http) introduced an async operation before the http.request call, surprising interleaving could happen again.\nBasically, nock is manipulating a global (http/ https) - so it's unsafe for non test.serial tests.\nI think it's certainly worth trying to write a couple experimental tests that intentionally try to break the download test suite. Maybe I'm missing something, and the current use is no big deal.\nIf I do turn out to be right on this, it's probably worth it's own recipe explaining how you need to be careful of these situations which can introduce subtle bugs. \n. Yes. https://github.com/kevva/download/commit/16ce6399d55decbec50e2b2f3f042767b2db7b51 improves the download test suite. That test suite has the advantage of expecting the same thing from a particular URL each time. Things get more complicated when mocking dynamic endpoints that return different values across time. I think .persist() as you use it now should be the preferred method if possible, and we need to explore how best to mock dynamic endpoints (I'm guessing you need to go to test.serial at that point, at least when using nock).\n. FYI: https://github.com/node-nock/nock/issues/578\n. Except you don't always want that to be the case:\n``` js\nimport test from 'ava';\nimport socialClient from './';\nimport nock from 'nock';\ntest('one friend', t => {\n  nock('http://social.com').get('/friendList').reply(['friendOne']);\nt.is(await socailClient.friendCount(), 1);\n});\ntest('one friend', t => {\n  nock('http://social.com').get('/friendList').reply(['friendOne', 'friendTwo']);\nt.is(await socailClient.friendCount(), 2);\n});\n```\nAbove you are requesting the same URL, but you want different replies for both.\n. > So you want the endpoints to sort of depend on each other to replicate a real use case (like getting your friends, add one and then get them again)? \nNo, testing a flow like that is probably best done with a real server running your real app (an integration test).\nI'm thinking something like a set of tests focused on the socialClient.listFriends() method:\n- listFriends returns correct response if person has no friends\n- listFriends handles one friends\n- listFriends handles two friends\n- listFriends handles more than 100 friends (paging limit)\n- listFriends handles a 401 (unauthorized) code by asking for credentials\nNone of the above tests would actually add friends using your backend API, you are just mocking the REST call, and making sure your client correctly handles a number of different possible return values from the same URL.\n. The createServer helper in got is a good example of creating real servers for testing.\n. I think it's probably worth mentioning that people should structure their API so they can swap out the base URL of their endpoint:\n``` js\nfunction MyGitHubApi(opts) {\n  this.endpoint = opts.endpoint || 'https://api.github.com';\n}\n// in the tests:\nnew MyGitHubApi({\n  endpoint: 'http://localhost:3030'\n})\n``\n. Thanks!\n. You wouldn't need to display the temp path. You would just cd into./.ava-temp-dirs` after a failed test, and everything there would be from failed tests, named in a way that was intuitive to navigate.\nI don't get the .dockerignore argument. Webstorm adds a .idea folder, NYC adds .nyc_ouput and maybe .coverage. If you're using source control, you're going to notice pretty quickly, because you likely won't have the files added to .gitignore.\n. > Talking specifically about Docker, not git:\nI know, I'm just pointing out that lots of stuff adds .dotDirs \n\nI don't think it's worth building .ava-temp-dirs feature to skip this tiny step.\n\nSure, I'm not convinced either.\n\nFix for Docker argument is to put .ava-temp-dirs inside node_modules/ava, like .cache/ava.\n\nThat's fairly controversial: https://github.com/avajs/find-cache-dir/issues/1\n. Macros are not yet published. Expect them in 0.15!\nIf you want to use them now, install from master:\nshell\nnpm install --save-dev avajs/ava\n. Oh, yeah. That's definitely true \n. This still needs to be fixed for watch mode. I've tried to figure out how to do it / how to test it, and come to the conclusion that I want to punt that to @novemberborn!\n. I think this gets even harder for watch mode because of --sources.\nIf they are cded into a child directory and use the --sources flag, that should probably be resolved from cwd, right? But if they don't specify --sources, then we probably want to use whatever is in package.json, and resolve from that directory? That's getting complicated.\nShould we eliminate the --sources flag? I have never used it, and I don't see why you really need it. Doesn't this qualify as a static config item that should live in package.json?\n. Thanks @rhysd.\n. Published in 0.15.1\n. Yes. I want to land this ASAP (i.e. add some tests then let's publish).\nHere is the test from ava-files I was thinking you should replicate: https://github.com/avajs/ava/blob/master/test/ava-files.js#L99-L106\nAlso, see these tests. Those launch a new process for the CLI (which in turn launches test processes), so they aren't fast tests. We could do one or two just to verify it's all glued together correctly, but we should build up a more thorough set of faster tests that fully exercise ava-files directly.\nFor now, let's just land one of each and ship a patch release.\n. Linter errors!\n. Closing in favor of #878 \nThanks @kentcdodds \n. Closing in favor of #878 \nThanks @kentcdodds \n. I agree. It's weird it's the only one we don't offer a negation for.\n. I agree. It's weird it's the only one we don't offer a negation for.\n. Hmm. Yeah. That's not great, but I don't think it will confuse many people.\nI think if we just enforce the string argument and provide a helpful error message, that will suffice. The few that are confused won't remain so long.\n. Hmm. Yeah. That's not great, but I don't think it will confuse many people.\nI think if we just enforce the string argument and provide a helpful error message, that will suffice. The few that are confused won't remain so long.\n. I cancelled the AppVeyor run, because I want to get a patch release out ASAP, and didn't want to wait for this.\n. I cancelled the AppVeyor run, because I want to get a patch release out ASAP, and didn't want to wait for this.\n. Indeed. Thanks.\n. Indeed. Thanks.\n. Hmm. I really liked the Logger facade. It makes testing a lot easier. Now you have to employ a workaround, replacing the write method.\nWhat does doing this get us?\nI was thinking of some functionality we should actually move IN to logger. Specifically the explicitPrefixes option we use to manipulate whether we prefix titles with the file name.\nUnless you've got a really good reason for doing so, I don't think we should get rid of it.\n. Hmm. I really liked the Logger facade. It makes testing a lot easier. Now you have to employ a workaround, replacing the write method.\nWhat does doing this get us?\nI was thinking of some functionality we should actually move IN to logger. Specifically the explicitPrefixes option we use to manipulate whether we prefix titles with the file name.\nUnless you've got a really good reason for doing so, I don't think we should get rid of it.\n. Except this change adds 100 lines of code more than it deletes, without any change in functionality, or additional tests.\n. Wouldn't you just follow the same pattern we do now with other reporters?\nBuild the message in the given event specific function, then render it to the output destination in write?\n. What if we do something like:\njs\nfunction reporterBind(reporter, emitter, events) {\n  events.forEach(function (event) {\n    emitter.on(event, function () {\n      var output = reporter[event].apply(reporter, arguments);\n      reporter.write(output);\n    });\n  });\n}\nThen within tap reporter (for instance):\njs\nTapReporter.prototype.init = function (status) {\n    reporterBind(this, status, ['test', 'error', 'finish']);\n};\nThat allows the individual methods to still return strings (for easy testing), but also allows them to maintain control of which events they register for, and allows the elimination of Logger. Make sense?\n. Still has a WIP title, btw\n. So, I looked into it more, and I have changed my mind. I think I'm \ud83d\udc4d now. (Sorry for all the noise @vdemedes). \nA few additional thoughts:\n- Should we pass process.stdout, and process.stderr in as constructor arguments? You'd mentioned being able to redirect output in the browser. Also might make tests easier.\n- Should we be unregistering all our listeners in watch mode? Theoretically they should never emit another event after a run finishes, so maybe it's no big deal. I don't think there are any garbage collection implications from not unregistering.\n. Are we good here? This all looks good to me. @SamVerschueren - should I merge?\n. Should we be storing generated.d.ts in the repo?\nWhy not add it to .gitignore and create it in a prepublish script?\n. I'd prefer ignoring it if possible.\nIt would be awesome if npm had a \"installed from GitHub\" hook\n. >  I think that TS users already know that it's usually not possible to install a package from GitHub.\nI say we start with it this way, and see if there are lots of complaints.\n. I think that script generation should happen in a prepublish script. Otherwise, I think we are good to go.\n. You need to polyfill .includes. I see no reason not to just require('babel-polyfill') for this.\n. You should always reject with Errors. Errors give you stack traces. AVA is intentionally opinionated about this.\nIf you insist on not rejecting with Errors, then your best option is the final workaround you mentioned. \n(But really, just reject with Errors).\n. @vdemedes - mind sharing where you are going with this? There are lots of different ways to attack browser support, and we probably should all agree on the best way.\nFor instance, in my mind api.js was never going to run browser side. We would have done that with an alternate to fork.js that launched browsers and then received events via socket.io.\n. Some thoughts after an initial review:\nI don't think we should try to run the API in the browser. Instead, we should just be reimplementing lib/send.js to send messages via socket.io - the API would continue to run in Node.\nThere is also already a whole lot to review for a single PR. I would like to see an incremental approach.\nI think the first step should be creating a build/bundle tool that packages a single test with browser safe versions of all the child process dependencies. I don't think socket.io interop would even need to be tackled in that first batch, we could just console.log messages sent to the parent (profile.js already contains polyfills for process.send that does just that).\n. > > I don't think we should try to run the API in the browser.\n\n\nWhy? \n\n\nWe have efforts underway to bring watcher into the API, and simplify cli.js so it's a simple wrapper around it.\nThe less complexity we push into the browser, the better.\n\nI had no issue with doing that.\n\nIt's not so much a matter of whether or not it's possible, but rather if it is a good idea. It's definitely adding complexity.\nI think it would be better if we started trying to make a browser fork that behaves like a child_process fork, replacing node's child process ipc, with socket.io stuff.\n. > Is there a discussion/issue/pr that I missed?\nhttps://github.com/avajs/ava/issues/533#issuecomment-205899266\nhttps://github.com/avajs/ava/pull/759#issuecomment-220843047\nhttps://github.com/avajs/ava/pull/865\n\nI don't want to rewrite AVA core for browsers\n\nNot suggesting that. I'm suggesting you rewrite less than you have.\n\nInstead, the plan was to replace node-dependent parts with browser-compatible ones\n\nI do not disagree with this. I just think we could get there simpler if we sent stuff over the wire to the console instead of trying to do stuff in the browser.\n. See: https://github.com/arve0/tty-test-helper/issues/1\nTesting tty directly is fraught. I think most people would be better served if there was an easy way to test question flow in Inquirer.js, or other similar library. \n. This is an NYC bug.\nYou might want to try out this branch of NYC to see if it helps anything and/or try out this template for using __coverage__ instead of istanbul.\nLooking at your code, I'm confused by the use of the global check variable here\n. Anyone want to do a documentation PR? It should just be adding a single short sentence. Just \"the verbose reporter is always used in CI environments unless tap is specified\"\n. > Should we place the filename:line:column first? Usually the function name isn't very useful, so it's usually the line:column you care about.\nThe main advantage of the current output is that it remains a valid stack trace. The only difference is that paths are relative to the basedir instead of absolute, and we filter some irrelevant ones. I think there are some advantages to keeping to the standard, namely for external tooling (WebStorm provides clickable stack traces).\n. Meh - scratch that, we also remove the leading at from each line. That is probably enough to break any stack trace regexp out there (I just tested, WebStorm doesn't linkify our stuff). Let's just do whatever looks best.\n. Could we do this to make stack trace clickable in iTerm2?\n. Unfortunately, no one on the core team uses Visual Studio, and it's unlikely to ever be an officially supported platform.\nCurrently we support the following:\n- The default terminal on OSX\n- iTerm2 on OSX\n- cmd on WIndows\n. That's almost certainly a result of the hard work put in our TypeScript team, but it's unrelated to console output.\nHave you tried ava --verbose?\n. I checked out VisualStudio Code for OSX. I couldn't figure out what a \"Visual Studio Code task\" is.\n. Still no luck. Cmd + Shift + T does nothing.\n. Verbose works great for me:\n\nHowever, using the mini reporter, it's obvious the VS Code console does not support terminal control characters. \n\nI recommend sticking with verbose.\n. I can't dedicate any more time to troubleshooting this, but if someone figures out why this is happening, we can discuss fixes \n. Duplicate of #520 \n. Use pify.\n``` js\nimport pify from 'pify';\ntest('amqp', async t => {\n  await ampq.assertQueue(queue, exchange, key);\n  await ampq.publish(exchange, key, message);\n  const msg = await pify(ampq).subscribe(queue);\n  t.is(msg.content, message);\n});\n``\n. Callingt.end()` multiple times throws an error.\n. See #884. I think that's probably the direction we are headed.\n// @ivogabe\n. Look again. I think the wrapper was making it too complicated.\n. Hmm. I've tested on a different module, and it seems to work fine on npm@2. Will need further investigation.\n. It ended up being a misconfigure in a dependents package.json.\n. LGTM!\n. I almost left this same comment, but couldn't come up with a better place to put it.\nThinking about it more, It definitely shouldn't go here - if users are interested in custom reporters, then they definitely don't care about this behavior at all.\nI think we should create a whole new \"reporters\" section that encompasses this.\n- It should show screenshots from both the verbose and mini reporter, and discuss how to set up TAP reporters in more detail.\n- We could explain this behavior there.\n- We could eliminate this FAQ\n- We could move the clean stack traces section in there.\n. @7373Lacym Are you able to update this PR as suggested above? No worries if the answer is no, someone else can take what you've done so far and run with it.\n. 0.15.2 deployed, should fix the issue.\n. I deployed 0.15.2 (on the 0.15 branch) to fix #900 \n. I'm sure we will!\n@twada, has the runtime dependency graph been reduced? Should we move rendering back into the child processes - we've had lots of problems trying to serialize large objects for rendering in the main thread.\n. @nfcampos. Can you bump the dependency as mentioned? Otherwise this looks good to me.\nWe can tackle moving rendering to the child process in another PR\n. Thanks @nfcampos! Want to work on moving the renderer into the child process?\nThe idea is to go back to the way power-assert behaves in other contexts - replacing the message on the error.\n. See: https://github.com/Hypercubed/chuhai\nI would use caution when benchmarking with AVA however. There is a lot going on in the background, and spread across multiple processes. This is going to interfere with benchmark reliability.\n. AVA looks for .js specifically.\nDuplicate of https://github.com/avajs/ava/issues/631\n. 0.15.2 was released from a branch based on the 0.15.1 tag, which didn't have your commit.\nYour best bet is to install from master for now.\n. I think the current pattern is here to stay. Instead I think we should try to better document the patterns.\nI think a structuring-your-project recipe is in order.\nAlso, I think our no tests found error should link to it.\n. > I wasn't even aware ava had a concept of helpers until I ran into this. Where are they documented?\nThat's what made me assume it was a documentation issue.\n\nI guess it would be possible to provide yet another glob pattern for users to define what constitutes a helper, but we're pretty against adding config options unless there is a strong need for it.\nCan you fully define your directory structure?\n. I'm wondering if we couldn't relax it just a bit, to something like this:\n```\nhelpers/          <- not excluded? - not sure on this one      \nsrc/\n  helpers/        <- not excluded\n  foo/\n    helpers/      <- not excluded\n  test-helpers/   <- excluded\n  tests\n    helpers/      <- excluded\n    foo/\n      helpers/    <- excluded\ntest/\n  helpers/        <- excluded\n  foo/\n    helpers/      <- excluded\n```\nSo basically, the helpers directory is not excluded unless it is already in a test specific directory: (./test/, and **/__tests__/ by default).\nThis gets complicated though, what if you have renamed your test directory in the options:\njson\n\"ava\": {\n  \"files\": [\n    \"foo/\"\n  ]\n}\nNow foo/helpers probably should be considered a helper directory and excluded.\nThis is all doable, I'm just wondering if it's worth it.\n. Also, this would be a breaking change for people with a helpers directory in the root of their project (first line of my example above).\n. Early on, we included every file in ./test/**, and **/__tests__/**. The former pattern allows you to create a test directory that mirrors your src directory (this is what most of the maintainers prefer), the latter allows multiple __tests__ directories within your src directory, keeping tests relatively near your sources. Occasionally, you want to share some common code amongst your tests (some test macro / validator / assertion / etc), and it would be pretty inconvenient if you weren't allowed to put them near your tests, so a _ prefix, or a helpers directory allows you to share that common code, even though it's in a directory where everything else is considered a test. \n. So, to be clear, I'm suggesting we relax the exclude, but not as far as you initially proposed.\nRather than just ./test/helpers, it would also exclude ./test/foo/helpers, and ./src/foo/__tests__/helpers. However,  ./src/foo/helpers would not be excluded.\n. While we are at it, should we also relax the _ prefix, if the suffix is .test.js or -test.js?\n. > So ./test/foo/helpers being excluded still precludes properly matching the src directory structure.\nThat's a really niche scenario, only a portion of our users use ./test/**. A much smaller portion have a helpers directory that they want to mirror inside. The cross section has got to be small. Not sure it is worth it.\nWe already keep a dependency list of what each test requires during it's run (so our smart watcher knows what to run when tests change). What if during our initial globbing, we scan for helper directories. If none of your tests ever require anything from within the helper directory, we issue a warning that explains helper directories to you. Requiring anything from that helper directory from a test outside the helper directory signals you understand the point of helper directories, and that we don't need to warn you.\n. Because being able to add a helpers directory without touching config is super convenient.\nIn my mind, the fact that your test directory can't be a perfect mirror of your src directory is not a problem. That we surprised you with a behavior that wasn't obvious without a careful reading of the documentation is a big one though. If we detect your misuse and report it to you, we minimize that surprise, without sacrificing convenience.\nAdding config is always an option if no other solutions can be found, but it is one we avoid until it becomes absolutely necessary.\n. Thanks!\n. We definitely will not add globals.\nThere is a discussion about allowing nested groups in #222.\nThere is a third party wrapper ava-spec, that provides a mocha-like interface.\n. You still need it to transpiled sources. Tests are transpiled in the main process. There are other issues tracking transpiling sources and helpers in the main process.\n. Yes. Hence the Transpiling Sources header on that section you referenced. It also explains the performance trade-off, and links to the open issue that proposes we extend the precompilation behavior to the main process.\n. Closed because I think the docs cover the current state well, and we have open issues to track improvements already \n. Because we create a hash from the input and avoid retranspiling if it's unchanged.\nLook in node_modules/.cache/ava - the filenames there are the hashes used for detecting cache hits / misses.\n. Nice!\nThis eliminates a lot of cruft. It looks good to me.\n@novemberborn is more familiar with the code, so I'll let him merge. I know he's busy lately, so if he doesn't respond over the weekend, I say let's just go for it.\n. We shut the process down immediately if an uncaught exception happens, so after.always isn't guaranteed to fire in that case.\nI guess we could trigger after.always in the case of uncaught exceptions.\n. You will always be able to create situations where your cleanup code fails to do what you want, even with after.always.\n``` js\nimport test from 'ava';\nimport mkdirp from 'mkdirp';\nimport rimraf from 'rimraf';\ntest.cb(t => {\n  setTimeout(() => {\n    mkdirp('some/deeply/nested/directory');\n  }, 20);\nsetTimeout(() => {\n    cb();\n    throw new Error('woops');\n  }, 10);\n});\ntest.after.always.cb(t => {\n  rimraf('some/deeply', t.end); // <= this is likely executing before the mkdirp line.\n});\n```\nInstead of banking on the system being left in a clean state by a previous run, I recommend using before / beforeEach to ensure system state before running the test.\n``` js\nfunction cleanup(t) {\n  if (fs.exists('some/deeply')) {\n    rimraf('some/deeply', t.end);\n  }\n}\ntest.before.cb(cleanup);\n// if leaving that directory hanging around bothers you:\ntest.after.cb(cleanup);\n``\n. I'm not sure this is the correct behavior. We may just want to runafter.always` on uncaught exceptions \n. > There's also the argument that cleanup should happen on startup, but then you get a dirty git repo.\nI usually get around that with .gitignore.\n\nFeels like there's something we could do to ease this for the user.\n\nWhat if we allowed a .cleanup modifier that ran before and after the tests?\nOr what if we allowed an and modifier:\njs\ntest.before.and.after(fn)\ntest.beforeEach.and.after.always(fn)\n. There is no good way for us to mimic the tap behavior exactly with our architecture.\nWith tests running concurrently, it's possible to jumble console output even more than what you've shown.\n. > I had no expectation of fixing this issue when not using --tap --serial but would think the tap reporter could be made consistent.\nDoing so would require a completely separate implementation of our TAP output, specifically for --serial execution.\nWhen a test completes in a child process, we send a single event to the parent process, and it prints the opening # ava test line, and the ok 1 - ava test line all at once (or appropriate failure message). \nAny output from the child process stdout / stderr is just printed as it's received.\nTo accommodate your request, we would need to:\n1. Separate our single test event into two separate events.\n   1. Send a test started event to trigger the opening # ava test. \n   2. Send a test ended event to trigger the ok 1 - ava test line.\n   3. These only make sense in serial, so that would mean two different reporting structures depending on the --serial flag.\n2. Capture stdout and stderr in the child process and send them in chunks over IPC (to ensure log statements got printed within the correct test). This might also mean we need to ensure stdout and stderr are flushed (nothing in their internal buffer) before moving on to the next test.\nIMO, it seems like a lot of work to support a somewhat niche use.\n. From the spec:\n\nAnything else\nAny output that is not a version, a plan, a test line, a YAML block, a\nsubtest, a diagnostic, a pragma, a blank line, or a bail out is\nincorrect.\nWhen the pragma +strict is enabled, incorrect test lines SHOULD\nresult in the test set being considered a failure, even if the test\nset is otherwise valid.  When pragma -strict is set, incorrect test\nlines MUST NOT result in the test set being considered a failure if\nthe test set is otherwise valid.\nHow or if the incorrect line is displayed to the user by the Harness\nis undefined.\n- Test::Harness silently ignores incorrect lines, but will become more\n  stringent in the future.\n- TAP::Harness reports TAP syntax errors at the end of a test run.\n- node-tap prints incorrect lines to standard output, but otherwise\n  ignores them for the purposes of parsing or determining test\n  success.\n\nBased on that, I think printing everything else to stderr is a good plan.\n. The YAML block is output by the test runner, not log statements in the test. In node-tap you certainly can write YAML blocks via console.log that might be interpreted by the reporter. That's either a feature or failing of that library depending on your perspective.\nRight now, there's really no way for you to inject additional metadata into a test for consumption by a custom TAP reporter (I think that's what you really want here, right?).\nI don't see why we couldn't allow something like this in the future. We would have to figure out a good API for it. IMO, that's not something we should address until we are serious about plugin support (custom assertions, etc.).\n. > Is the watcher still broken with this? (Didn't check why CI failed.)\nThe CI failure was unrelated (npm install checksum errors), I restarted and it passed. Neither this or #863 \"broke\" watcher. But it doesn't improve it either. With this you can do ava some-file.js from a subdir and it works. With watcher, you will still need to run it from the root directory of the package for things to work. \nIt is a little more complicated to make watcher work in a subdir. As agreed, the sources config in package.json should be evaluated from the basedir regardless. If you ava -w some-file.js from the test subdir, you still want to watch for sources from the basedir, but the pattern you passed on the command line needs to be converted to be relative to the subdir.  If we don't eliminate the --sources flag, that makes watching from a subdir even more complicated (I think).\n. I was thinking it would allow a means for users of other test runners to gradually transition to AVA asserts before switching to full blown AVA.\n. We could always do a monorepo!! \ud83d\ude09\n. I don't really think we are going to add that much overhead with this. Changing our assertions API should be a big deal anyways.\nThis could also open the door for pluggable/swappable assertions, which is something I was thinking about working on. We could wait to land this until it's proven necessary (though that would mean ava-assert might drift out of date - and might need to be redone completely if it takes to long for me to get around to pluggable assertions).\n\nI guess we can land this since it's already done\n\nThat is definitely not a good enough reason to land it. Let's not get in the habit of doing things for that reason.\n. It don't know that it would necessarily make it easier. But it would force us to write our our asserts as a proper plugin, dogfooding our own plugin API, and it could serve as a model for others who want to create custom assertions plugins.\n. > Read my statement in context\nYeah, I just wanted to be clear that my feelings wouldn't be hurt over the \"wasted\" effort. (It wouldn't really be wasted - I found some bugs regardless).\n. Oh - I also discovered we weren't directly testing assertions that act on Observables. We were doing it in test/observable.js, but that was rather indirect via lib/test. (just trying to make sure I capture all the improvements I made while I still remember, if we decide to abandon this and backport)\n. Good catch! Thanks!\n. I lean toward and, though I see problems with it as well.\nThe only reason is that I think it makes it more obvious what is going on without reading the AVA docs. The notion that cleanup is going to happen both before and after is going to surprise people. They are going to write rmdir code without checking to make sure the directory exists, and be surprised when it fails. I think and better optimizes for test readability and being a self documenting API. cleanup seems only to optimize for characters typed.\nMy concern with and is that it might suggest combinations we don't want to support.\n\n\nCan we make it cleanup even on uncaughtException? I'd like cleanup to work no matter what happens.\n\nWe can try. There is no guarantee it would work.\nThe only way to guarantee cleanup works would be to kill the first process immediately, then relaunch and run just the cleanup method. That falls apart if you use something like unique-temp-dir and store a reference for cleanup (not a good example - no need to clean up temp).\n. See https://github.com/avajs/ava/issues/928#issuecomment-227911443\n\nThat falls apart if you use something like unique-temp-dir and store a reference for cleanup (not a good example - no need to clean up temp\n. If we think this is a good idea, we should update to handle --debug-brk as well.\n\nNo tests yet.\nAlso, I wonder if we couldn't implement a simple proxy layer for the debug protocol that avoided the need for existing tools to implement automatic reconnection.\n. > Also, I wonder if we couldn't implement a simple proxy layer for the debug protocol that avoided the need for existing tools to implement automatic reconnection.\nIt looks like all we would have to do is proxy it and prevent the connection from the debugger from closing: https://github.com/nodejs/node/blob/master/lib/_debugger.js#L1704\n. @nfcampos - do you know which files?\n. @nfcampos \nAlso, from your .json file. Why is ava resolving to /Users/nuno/cp/ava/index.js and not something in node_modules?\n. Ah. I see.\n. > for some reason you're not picking up on re-exports as dependencies\nAh yes. I'm definitely not!\n. This should fix the re-exports problem.\n. @nfcampos - New version of babel-plugin-detective published, and updated here. Should fix the re-exports problem. Pull and try again.\n. @kentcdodds - Your stack trace is weird. Are you calling the AVA CLI within an AVA test? Based on the stacktrace, I would expect it was throwing in the top level process. But that line 6. Uncaught Exception, makes it look like a failure in an isolated test child process.\nEither way, I've wrapped the offending resolveFrom call in a try/catch and added some debugging output. If you could rerun and repost the stacktrace, that would be awesome.\n. FYI: I've opened PR https://github.com/babel/babel/pull/3564, that I hope gives us the best of both worlds: main file transpilation where possible, and deferring to babel-register for tests that have dynamic requires (those tests with dynamic requires would take the same performance hit they currently do, but hopefully that's only a portion of your test suite).\n. No, but we don't have a good solution for dynamic requires without it.\n. Also, that PR is just a refactor. It doesn't introduce added functionality to Babel. If it ever gets merged, it could be used to write our own version of babel-register that integrates with main process precompilation or a way to make babel-register not load up tons of dependencies until needed.\nWe could merge this, and work towards improving Babel separately \n. Added a commit documenting this and --concurrency (docs/experimental-features.md).\n. This seems like a Babel bug.\n. You are using ES6 modules which are not supported by Node yet, so there is transpilation happening somewhere.\nIf Nodes implementation of ES6 classes fails an instanceof check, then that's a Node bug. You would do well to transpile anyways until it is resolved.\nCan you reproduce without the jspm loader?\n. I am surprised Promise and Iterator are not built in types. Perhaps you need to set the target in your compiler options? http://www.typescriptlang.org/docs/handbook/compiler-options.html\n. You have two options: \n1. Use test.cb and t.end to explicitly end the test yourself.\n2. return the Observable from the test function.\n. See https://github.com/Microsoft/TypeScript/issues/5911\nIt seems to suggest that Promise must be used for async functions. We definitely want to allow async functions, so not sure this is a good idea.\n@SamVerschueren @ivogabe \n. > I don't know if I need more tests for this pull request.\nAdd a second one that tests what I talked about above, and I think you are covered.\n``` js\n// test\nvar r = new Runner();\nr.test(() => Promise.resolve());\nr.run()\nt.throws(() => {\n  r.test(() => {});\n}, {message: ...})\n```\n\nThis is my first pull request\n\nOne of the best first PR's I have seen. Very good job. \ud83c\udf89 \n. LGTM \n. Thanks @asafigan!\n\nAlso should we mention in the docs somewhere that you shouldn't have embedded tests or hooks. And also that we have no alternative to describe.\n\nThat probably belongs in a \"transitioning from mocha/tape\" recipe. Not sure it belongs in the core docs.\n. We don't have direct browser support yet.\nThere is a recipe for mocking the DOM here. Some preliminary work on a browser solution via karma (https://github.com/avajs/ava/pull/942), and work on a full standalone browser test runner (https://github.com/avajs/ava/pull/887).\n. 0.10 exits LTS in two months. It would be imprudent for us to dedicate resources to tracking down why this is happening / how to fix it.\n. Please post a reproduction of the problem. You've posted code you tell us does work, but not an example of the code that does not. Specifically, what does your test look like that uses app.ts?\nAlso, please refer back to the issue template, and provide anything relevant. Specifically your AVA config. \n. I think this is blocked by #923 - but yeah, seems reasonable to me.\nAlso, I think we should get #945 merged and released before spending a lot of time collecting data on this. I think it will have a dramatic impact on those using --babel-require, and possibly shift the ideal concurrency higher. I think disk and memory contention are equally as likely to be the culprit as CPU contention.\n\nUse time ava on OSX / Linux and post all timings\n. See get-port.\n. On unix systems you can use the time command:\ntime ava\nWindows makes it way more complicated. If you need Windows support, you might be better off using gulp-ava\n. > The gulp-ava link redirects me to my own question\nSorry. Writing markdown on mobile is not fun. https://github.com/avajs/gulp-ava\n\nCan't this be implemented in AVA instead?\n\nThat idea has already been rejected. Test run times are highly impacted by background tasks, our caching mechanisms, concurrency, etc. It would be a fairly useless metric.\n. AVA doesn't report assertion messages unless one fails. It only reports test failure / success.\nChanging the behaviour to be more like tape would be problematic because so many tests run and report concurrently. The onslaught of output would be very hard to decipher.\n. Try using t instead of assert as your parameter name (using t should enable power-assert).\nAlso, t.is does strict comparison, not deepEqual. Use t.deepEqual for that.\n. Improved TAP output would certainly be welcome. I'd like to see someone pick up where I left off in tap-emitter and land a PR that switches to using that.\n\nHappy to make the PR if this just needs a more thought out string converter for the tap reporter.\n\nIt's a little more complicated than that. It involves YAML conversion, among other things. Right now, our TAP output is very naive. The TAP spec actually has some complex rules that we aren't following very well, but tap-emitter aims to fix that.\nThe full TAP spec that we are trying to follow is defined in this PR. Despite it's draft status, it represents the \"reality on the ground\" for the tap output of most Node.js test frameworks, and it's definitely what tap-spec and most popular Node reporters expect. Specifically, it introduces some additional constraints around serializing objects into YAML (which is directly related to your issue).\nUnfortunately, that draft spec has languished for a year now, so it's claim to represent \"reality on the ground\" is likely out of date in some way. Still, I recommend starting with that spec.\nIf you're serious about contributing, take a look at https://github.com/avajs/tap-emitter/issues/1 first. It would be a great way to familiarize yourself with the spec, and push things along.\n. > Really?\n\nIt seems to work, but that's kinda voodoo.\n\nYes, yes it is. This is a byproduct of the way power-assert works. It uses a pattern matching scheme that makes it easier for implementors to wrap any assertion library with power-assert goodness without having to understand the ES AST at all (which is super awesome). That said, the AVA team certainly has members capable of creating a more robust matcher that would detect your use of a different variable name, so if power-assert exposed an \"advanced\" API, we could certainly leverage it to provide a more user friendly experience.\nSee https://github.com/power-assert-js/babel-plugin-espower/issues/18.\nIt would be non-trivial, but something worth doing.\n. Any way to test this?\nI'm thinking something like:\n``` js\nconst dir = uniqueTempDir({create: true});\nconst testFilePath = path.join(dir, 'test.js');\nconst cliPath = require.resolve('../../cli.js');\nfs.writeFileSync(testFilePath, `\n  import test from 'ava';\ntest(t => {\n    t.pass();\n  });\n`);\nexeca(process.execPath, [cliPath], {cwd: dir}).then(endTheTest);\n```\n. Any way to test this?\nI'm thinking something like:\n``` js\nconst dir = uniqueTempDir({create: true});\nconst testFilePath = path.join(dir, 'test.js');\nconst cliPath = require.resolve('../../cli.js');\nfs.writeFileSync(testFilePath, `\n  import test from 'ava';\ntest(t => {\n    t.pass();\n  });\n`);\nexeca(process.execPath, [cliPath], {cwd: dir}).then(endTheTest);\n```\n. > To restrict watch mode on the fly to a more specific pattern.\nThat seems unlikely to be truly useful. Why would you want to avoid rerunning when certain sources change, but not others?\n. I definitely wouldn't cache node_modules. If deps are removed from package.json, they won't be cleared. This introduces initial state to the build, meaning CI no longer verifies it will install from scratch.\nI am not sure about $HOME/.npm. If caching it does mask any problems, it seems certain those problems will be with npm.\n. The speedup is drastic enough on Appveyor that it's probably worth it there. I'd avoid it on Travis though.\n. pinging @thejameskyle . It actually is not that easy to implement because of the way power-assert recognizes patterns. See https://github.com/power-assert-js/babel-plugin-espower/issues/18. No comments on this PR yet, just some background:\nAs far as I know, we have never allowed assertions failures to throw. This is a relic from tap/tape which exhibit the same behavior. The big difference for those libraries is that they show the status of every assertion in their output. Not throwing allows them to check the status of future tests. We only show our first assertion failure. This is a result of our parallel execution of tests, which would make it difficult to inline assertions.\nIn tap / tape, this works as a good test for your add function:\njs\ntest('add', t => {\n  t.is(add(2, 2), 4);\n  t.is(add(2, 3), 5);\n  t.is(add(3, 3), 6);\n  // ....\n});\nIn AVA, you would stop at the first failure, so it's better to use macros:\n```js\nfunction macro(t, lhs, rhs, expected) {\n  t.is(add(lhs, rhs), expected);\n}\ntest(macro, 2, 2, 4);\ntest(macro, 2, 3, 5);\ntest(macro, 3, 3, 6);\n```\nThis moves each assertion into separate tests, so a failure of one does not mask failures down the line.\ntap offers some convenience here for very simple tests. You don't need to separate it out like you do in AVA to see every failure.\nRight now, what we are doing does not make a lot of sense. We only display the first failed assertion, but we continue on after that assertion collecting data we will never display. We have discussed a flag to display every failed assertion in #261.. Is the goal to test in the browser, or in Node?\nTake a look at karma-ava for how it was solved with browserify. I looked at webpack as an option when developing that, but I don't remember why I didn't pursue it further. Specifically, check out lib/process-adapter.js, which polyfills anything Node specific.\n. If I had to guess, it would be how we handle test.only when no concurrency is specified. We wait for all test files to load and specify all their tests and transmit the ready event.\nIf we can prove that to be true, I propose we abandon the current .only behavior, and have it behave similar to how it works when --concurrency is specified.\nWe can almost certainly catch 99% of t.only uses via static analysis with a babel plugin. I started on this a year ago, but my life got hectic and I never followed through, see: ava-test-data.. Right. When you specify concurrency, we run tests as soon as possible. When you don't, we pause, waiting for all tests to load and report back about t.only. > Inheriting comes with some downsides though. We would not be able to consume or transform the output at all.\nBut we don't do either of those things for either the tap or verbose reporters.\n\nAnother solution would be to add an option to not spawn AVA test files as child-processes\n\nWe should make that an option regardless. Not sure it would improve performance all that much.. > This might not be great for performance but presumably those tests would also work if the test spawned yet another child process which executed a fixture?\nNah, they grand-child process would still get a pipe instead of a tty type stream. TTY is only available in the top level process (and only when launched from a console), and to any descendants that get their stdio streams with the inherit option. Once a child has a pipe type stream, a grandchild that inherits from it will still get a pipe type stream.\n. > RE: global option\nThat would suck for a large test suite if you only needed it for a few tests. It would be nice to address some configuration options on a per test basis. I'm thinking this option, fail-fast, serial, and require:babel-register, etc. What if we wrote a Babel plugin that allowed you to export a config in your test:\njs\nexport const avaConfig = {\n  serial: true,\n  failFast: true,\n  require: ['babel-register'],\n  runTopLevel: true\n};\nComputed values wouldn't be allowed - that way we could interpret it via a Babel plugin without actually running the test file.. How would we enforce that? Can you tell a function is async before calling it?. See the ES2017 Support section of the documentation. If you already have a babel config you want applied to your test, use the babel: \"inherit\" option. If you want to disable babel, you could do:\njs\n\"ava\": {\n  \"babel\": {\n    \"presets\": [] \n  }\n}\nThat supplies an empty config, which essentially tells babel to \"do nothing\". AVA will still add a few transforms, but nothing related to async / await.. I'm +1 for zeit/now. We'll get documentation builds integrated into every commit / PR / etc for free.\nImplementing a drop down that lets you choose which release you're looking at will be pretty easy via Now and GH APIs. First, I want to make sure you've checked out this podcast by @kentcdodds. I find myself using beforeEach and friends very rarely any more. I'd almost call it an anti-pattern.\nSecondly, a trivial test I set up, does not cause the behavior you are describing:\n```js\ntest.beforeEach(t => {\n     t.context.foo = 'foo';\n});\ntest('foo', t => t.fail());\n```\nWe are going to need to see a minimal reproducible example in order to help point you in the right direction. I'm fairly certain we aren't logging the context on test failure, but that you are somehow getting a reference to it in an object you are passing an assertion.\nThat's entirely possible with code like this:\n```js\ntest.beforeEach(t => {\n  t.context.mySpy = sinon.spy();\n});\ntest('foo', t => {\n  t.context.mySpy('blah', 'blah');  // context captured here\n  t.true(mySpy.calledOn(t.context));\n});\n```\nIn the above example, sinon will capture and store a reference to t.context, because t.context is the execution context for that spy, and is necessary for the spy.calledOn assertion.\nYou could solve that with destructuring:\njs\ntest('foo', t => {\n  const {mySpy} = t.context\n  mySpy('blah', 'blah'); // mySpy `this` reference is undefined now\n});\n. You can use the --match flag.\nhttps://github.com/avajs/ava#running-tests-with-matching-titles\n. > You can get the test title from t.title, per docs: avajs/ava#t\nt.name is not documented. @novemberborn - The only other issue would be providing a way to bypass the main thread precompilation. In that case, the package sniffing I am doing wouldn't be needed. . It also replaces \"require\" for when we load the tests.. See: \nhttps://github.com/avajs/eslint-plugin-ava/blob/master/docs/rules/no-skip-test.md. I would start by using an in-memory SQLite instance as your backing database, or using something like sequelize-mock.\nSharing a single instance of Sequelize across tests, by definition, will create global state that is shared across tests. This is a an anti-pattern that AVA purposely discourages.. result.kill !== process.kill. It is a method I attached to the result. It returns a Promise that resolves with childProcess.on('exit',.... This is waiting for all the child processes to exit before killing the parent.\nresult.kill also makes sure process.kill is not being called on any processes that have already exited. Because: \n\nMay emit an 'error' event when the signal cannot be delivered. Sending a signal to a child process that has already exited is not an error but may have unforeseen consequences: if the PID (the process ID) has been reassigned to another process, the signal will be delivered to that process instead. What happens next is anyone's guess.\nNote that while the function is called kill, the signal delivered to the child process may not actually kill it. kill really just sends a signal to a process.\n---- https://nodejs.org/api/child_process.html#child_process_child_kill_signal\n\nprocess.kill may return synchronously, but we want to wait for children to actually exit. exit events certainly are not processed synchronously. \nProof: https://github.com/jamestalmage/process-kill-is-sync\n. Specifically, \nnyc uses signal-exit, which intercepts the kill signal and allows it to finish up writing files to disk before the process is actually killed.\nI updated jamestalmage/process-kill-is-sync to illustrate signal-exit delaying the exit.\n. We don't have a reference to the promise in cli.js#exit(), only the result. Hence the need to attach it to the result.\n\nThat way, we won't have weird stuff in test results.\n\nNot sure how this would create \"weird stuff\" in test results.\n. We need killedPromise in cli.js#exit to wait for all the child processes to exit before killing the parent.\nThis chunk of code certainly could live inside the other Promise, but then you have a Promise inside a Promise and need to to do something like the following (which seems less clean):\njs\nvar promise = new Promise(function (resolveOuterPromise, rejectOuterPromise) {\n  // ...\n  var killedPromise = new Promise(function (resolveInnerPromise) {\n    ps.on('exit', function (code) {\n      resolveInnerPromise(code);\n      if (code > 0) {\n        rejectOuterPromise(code);\n      }     \n  });\n});\n. Does ava already warn if a test file has no tests? (i.e. passCount === 0)?\nI would think that should be a failure, otherwise putting:\njs\nprocess.exit(0);\nat the top of your test makes it pass. (contrived I know).\nShould we be warning if the process exits with code 0, but never sends a result message?\n. I would agree, it needs a separate issue/discussion.\nI think your code is fine, no need to revert. Lets just create a new issue and follow up on it.\n. @vdemedes #137 created to track that discussion.\n. these assertion messages are bad. I should fix them\n. right - I've got them in the affirmative ('did NOT wait for setTimeout(fn, 15000')') -that is actually what we want (do not wait 15 seconds).\n. Does runtime have an option NOT to put the require statements in at the beginning.\nThat would be nice.\n. which is why I requireFrom('.', 'regenerator/runtime-module');\n. I thought I saw bluebird as a babel dependency and just assumed it was their polyfill. I'm very likely wrong on that.\n. We need to use core.js anyways with the runtime transform.\n. core.js aliasing\n.  I might have actually fixed this line too.\nDo we have a test that covers this?\nEverything passed on OSX / Node 4.1 without this and without my change (in other words, unless the problem this addresses is specific to an OS / Node version we do NOT have test coverage for it).\n. @vdemedes, this line is from #47 \nCan you remember what you were addressing there? Tests pass without it. (even before my changes).\n. I was supposed to clean these up in #135 but it got merged before I could.\n. @sindresorhus \n\nBluebird is huge and slow to eval\n\nI am not eval'ing the entire bluebird codebase just a require statement to that resolved path.\nActually this way is probably faster since we already require bluebird in test.js.\nHaving the babelfied code require a different implementation would mean Node is parsing two different implementations per forked process.\nAlso, in some ways this is a feature - you can use all the fancy sugar bluebird adds in your test, but we hide it from your production code so you can make sure your production code is not relying on bluebird specific features. \nI recall having to add bluebird somewhere to get unhandledRejection working. If babels implementation supports unhandledRejection notifications, maybe we should switch test.js to that.\n. This means the test process is now receiving messages.\nDo we need to monkey patch process.on so tests never see them? \n. tests that don't-> tests don't.\nWhere do I redeem my points? :smile: \n. done!\n. oops. Sorry - blame attributed it to you, and I didn't scan down the #47 far enough.\n. @sindresorhus ?\n. went with the first one\n. This shouldTranspile definitely needs to be improved. Very naive.\nI'm holding off investing time on it until we have a decision what the API and CLI should look like.\n. The require hook now outputs a different message when it can't find a file.\n. This shouldTranspile method definitely needs to be improved. Very naive.\nI'm holding off investing time on it until we have a decision what the API and CLI should look like.\n. It won't interfere with AVA test code.\nBut if someone attaches a process.on('message', handler), and then emits their own message events on process, that would interfere. Though it would be really odd behavior.\n. It wouldn't really ever make sense for them to do that though (not unless they fork an additional process).\n. why not just if (err) ?\n. If they've used t.plan(), won't this increase their assertion count causing a plan error message?\n. I'm still genuinely curious why you are checking argument length.\nIf arguments.length === 0, isn't err going to be falsy no matter what?\n. No, I'm saying\njs\nif (err) {\n  this.ifError(err)\n}\n. Hmmm.\nWe need it to behave consistently whether or not it's being used as a callback\njs\nt.plan(1);\nt.ok(true);\nt.end();\nshould perform the same as\njs\nt.plan(1)\nt.ok(true);\nsetImmediate(t.end);\n. that is fine, but for your plan test do something that is not async:\njs\ntest(t => {\n    t.plan(1);\n    t.ok(true);\n    t.end(new Error('something bad happened'));\n});\n. Yep. If the Babel polyfill doesn't support unhandledRejection, at least pinkie is a viable option now. \n. Note that pinkie-promise probably isn't a great alternative for us, since it defaults to global.Promise if available (which won't fire unhandledRejection on Node < 4.0.0)\n. Are we sure it doesn't support it? (I certainly don't know).\n. Nah - just set it. That doesn't do anything to prevent pollution of globals.\n. We are going to need a way to include the settings for the transform in the hash as well.\nPerhaps it is:\nMD5(MD5(source_file_contents) + MD5(\n    ava_version + '\\n' +\n    settings_file_contents + '\\n' +\n    ...???...\n  ));\nHow do we handle changing dependency versions? What happens when the babel-plugin-transform-runtime has a patch level bump (meaning the output of the transform may have changed some for the same input). Are we going to have to scan all the plugin version numbers and make those part of the hash? I can't think of an other way.\nHowever we do it, we should compute the part of the hash that is consistent across the test run once (the ava_version + plugin_hash part), and pass that to the forked processes so they only need to compute the hash of the file being required \n. @floatdrop I think the reason I need to depend on babel-runtime@5 here is because of the way we are mucking with module paths. Many of babels transforms depend on babel-runtime@5 internally, so I think our hack is forcing unintentionally pushing them back to v6.\nThat's my best guess right now anyways.\n. > the reason I need to depend on babel-runtime@5 here is because of the way we are mucking with module paths\nThis may not be true. I just discovered that running the babel test suite had created a whole bunch of global symlinks. Not sure if it was what was causing this, I will retest later.\n. I've come at this at least 5 times, and every time, it has required all these fixes.\nI don't know that I've ever applied this one last, so it could be unnecessary. I can pull it out with another commit and see what happens.\n. Yes, so much.\n. Not reject the promise, and instead fulfill it with the error information.\nEventually, I'd like to see cli.js -> error() callback only get called in the case of a bug (something we can't handle). In a testing framework, getting an error inside the test is an expected response, and shouldn't mean we bail.\n. If a user decides the do function foo(){}; throw foo. Terrible idea, but somebody is bound to. Only happens if they actually throw a function (nested functions will be discarded as normal).\n. needs a test.\n. #219 will tell us if this was necessary.\n. TDIL:\n```\nlet foo = function () {};\nlet bar = foo;\nconsole.log(bar.name);\n// => foo\n```\n\n. ~~not sure if it's just a babel thing or what.~~ - it is just a fancy thing Babel does for you\n. Does not work in node 5.0.0. Empty strings all around.\nWorks in Babel instrumented code though. \n. @sindresorhus \nFYI, looks like core-js does support unhandled rejection\n. this tries to reinstall after 30 seconds if the first npm install fails (which it does about every 10 times or so on AppVeyor).\n. @sindresorhus got the intent right.\nI originally had it as APPVEYOR, until a conversation with @BarryThePenguin \nThe only tests this affects is ones that parse the stdout / stderr from our CLI.\nThere is a chance by running the tests fast, that they miss the last line or two (a summary or the failed stack trace). It's highly unlikely they will be grepping our code CLI output to get a pass/fail on their own tests.\nThey will still always get the correct exit code - they might just be missing the last \"1 test failed\".\nIf users report it as a problem it is easy enough to correct down the road.\n. My only thought there was wether or not we wanted to monkeypatch process.on on the child process, so users never saw our communication.\nI can imagine a scenario where you are writing some code that might be a child process (or might be top level), and so they would will listen to process.on('message'). By choosing a unique property name here we could monkeypatch on and filter out these messages.  I reemit these on the child process as specific ava- prefixed events(process.on('ava-kill', ...), process.on('ava-cleanup'), etc) - that it is really unlikely users will listen to.\nNone of that is implemented. Just leaving the door open.\n. oops - I meant to delete that other one.\nThere should be one each stderr and stdout. This belongs in the \"it seamed to help\" pile. Definitely room to experiment with removing it down the road. \n. I chose 30 seconds pretty arbitrarily, so I\"m not opposed to shortening it and seeing what happens.\nWhy do you want it shorter?\n. > Probably because it's annoying to always have to wait for AppVeyor before merging a PR. \nYes. That.\nI got a lot of TV watched this weekend. Push a commit, watch 20 minutes... Still broken... Push a commit, watch 20 minutes...\n. cool, I'm assuming you took care of it in the merge? Or do you want me to go handle it now?\n. https://github.com/petkaantonov/bluebird/wiki/Promise-anti-patterns\n. TODO: We probably should look in to replacing some of these polyfills with core-js equivalents. Hopefully cutting down on duplicate downloads for users.\n. A few nits:\n- IMO, you are overusing emphasis here. Emphasizing whole paragraphs is overkill. Emphasize the one/two sentences I need to read to understand what the paragraph is about, so I can move on if it does not concern me.\n- ... run. In other words, it won't   => ... run. It will not (In other words is superfluous language)\n- transitively transpile - let's think of a simpler way to say that \n\nNote: AVA currently only transpiles the tests that you ask it to run. It will not ...\n. This is code is actually before matched files are found.\n\nAt this point, files is not an actually a list of files, but a list of glob strings (like \"**/*-test.js).\nYou may be seeing results on your machine here because your operating system is automatically expanding glob strings for you (I believe that is the case on OSX, but not on Windows).\n. This line is where we trade the incoming glob strings for a Promise of all the files found by globby using said glob strings.\nYour filter above should be moved down into the then method.\n. Wish I could use arrow functions :disappointed: \n. The previous Observable example was so trivial that I don't feel it adequately explained the advantage AVA offered.\nI think this:\n1. Is a simple enough usage of observable that you don't necessarily need to be familiar with them to understand what is happening.\n2. Shows a novel use of t.plan() in coordination with the Observable filter. It's a little closer to a test someone might actually right.\n@BarryThePenguin \nWhat I know about Observables I learned in the last 20 minutes, so feel free to tell me where I missed the mark. \n. Good call! Thanks!\n. Yep. I'm so tempted to go create tap support. We will self test someday, and then all will be right with the world.\n. Only so much time in the day. Don't hold your breath. :relaxed: \n. You mean just have t.end throw when executed?\nBecause we want to fail fast, and if passed as a callback:\njs\n   fs.readFile(path, t.end);\nWe are waiting for the function to actually be called instead of throwing ASAP.\n. So you are thinking?\njs\nif (isPromise(ret)) {\n  if (this.metadata.async) {\n    this._setAssertError(new Error('Do not return promises from....'));\n  }\n}\n. Yes - I think that is what you discussed above.\n. If they don't return a promise - we assume sync. We might mark the test done before the callback returns. \n. I am not sure why we forward both child output streams to stdout. I do it here simply because it is what we were doing before. Making that change will impact a lot of tests, and is a separate PR.\nIn my mind what we are currently doing makes the least sense of the the available options. I think we should either:\n1. pipe child.stdout -> process.stdout, and child.stderr -> process.stderr.\n2. pipe both to process.stderr. This keeps process.stdout pristine and will not interfere with our TAP output.\n. FYI - I did make the change so we throw if they return a promise / observable. However, I don't think the wording here needs to be changed. It is not an error to use Observables in test.async tests, it's just an error to return an Observable.\nWe could add more language here fully explaining that, but I'm not sure it is necessary. The semantics of returning an async object are now well defined (the test ends when the async object ends). It doesn't make sense to combine that behavior with t.end(). Telling them they can't do something that doesn't make sense seems to be more noise than helpful.\n. Sure. Webstorm has only global and per file setting for soft wrapping. No ability to set for just .md etc. That's why I have gotten in that habit. \nhttps://github.com/nicoulaj/idea-markdown/issues/57\n. I think resolve-from should adopt that behavior. I can't think of very many scenarios where you would want resolve-from, and not want to wrap the thrown exception.\nAlso, it looks like someone was already assuming that behavior here: babel.js#L41. This looks like the more common use-case.\nPerhaps retain the old behavior somewhere:\njs\nrequire('resolve-from/throws');\nor something like that.\n. Maybe add a typeof check?\njs\nif (typeof this[key] === 'function') {\n  this[key] = this[key].bind(this);\n}\n. _prefixTitle introduces new behavior not related to exposing an API. I like where you are headed, but it should probably be a separate PR.\n. Just use:\njs\nprocess.on('ava-run', ...);\n. ~~Looks like test.run() gets called a bunch of times.~~ Nevermind, I get it.\n. Why are you waiting for every test file to load before kicking off the tests? Once an individual test is ready, why not let it start?\n. OK. Honestly, that line was there to appease you: https://github.com/sindresorhus/xo/pull/32#issuecomment-155032045.\nI thought you brought up a good point, that silently changing standard behavior is a bad thing.\n@sindresorhus, what do you think?\n. so we do :+1: \n. I am not seeing it.\n. oh wait - yes i do.\n. TODO: That was a quick hack I meant to fix\n. I could just use string.split() here for a lot less code. But that would break absolute paths on Windows. Mocha just uses split. Absolute paths seem unlikely.\nThoughts?\n. I used --compilers before really researching why that is what mocha used.\nTurns out the only reason they use that over --require is for mocha --watch (they add .coffee to the list of extensions that should be watched).\nThat means, really only --require is needed until we add watch support ourselves.\n. This _compile is a mock Module implementation that prevents it from actually loading up the module (letting us further transpile the source).\n. > Mocha's watcher does weird things when it reloads the tests\nBy nature of the way we fork new processes for each test, that should not be an issue for us.\n\nI'm not convinced watching specific extensions is even necessary.\n\nThere is discussion of including an option to have the watcher only run tests affected by the change. Though perhaps a better solution would be to just monitor the keys of Module._extensions, or allow a list of extensions to be passed to the watcher flag --watch=jsx,coffee,...\n. Yep. This was quick and dirty. Final solution would need that. I didn't want to invest a lot of time in it, because I'm leery of mocking an undocumented API this way. To be honest, I anticipated a little more of an \"Are you nuts?\" reaction to this.\nDo you see any need to mock more of the implementation? I have not yet seen an extension that uses anything beyond _compile.\n. Just do \njs\nif (this.require) {\n // ...\n}\nYou have already guaranteed it is an array of at least length one above.\n. My preference would be that you not use reduce.\njs\noptions.execArgv = [];\nthis.require.foreEach(moduleId => options.execArgv.push('--require', moduleId));\n. Tell people what it does. Don't make them go read the Node documentation.\n. Overkill for something not public facing. Just do:\njs\noptions = options || {}\n. I agree.\n. Oh, it still would be if we want to avoid the unnecessary transform. I think maybe asyncToGenerator was rolled in a preset? There's certainly a way to continue doing what we were before.\n. http://babeljs.io/docs/plugins/preset-stage-3/\n. It was included in that preset, that's why I dropped it. Every intention of adding it back.\nWe are still waiting on https://phabricator.babeljs.io/T6644. I will approach this again when that closes.\n. Yep, at the time, latest was 6.0.18 which I couldn't get to work at all. That is why it is pinned.\nThey are on 6.3.x now, so this likely isn't necessary anymore.\n. This assertion is failing on AppVeyor (reliably). Are you able to test on Windows?\nUnfortunately, the tap spec reporter does not show the input string for failed t.match tests.\nI've been burned by this before.\nMaybe it's time we switched to the tap classic reporter. It's the default reporter on desktop (it defaults to TAP output  when stdout is not a tty). The classic reporter is basically dots, but with really verbose failures.\n// @sindresorhus @vdemedes \n. Not related to this PR, but I find false fail false to be a very awkward message.\n. @sindresorhus is on a semicolon rampage today.\n@semisorhus?\n. > I propose we just JSON.stringify() the data we want to pass instead of using CLI args.\n:+1: \n. I thought we only ignored tests/fixtures because it might have some esnext stuff in it, while production might not. Am i right on that?\nIt would be nice if XO had an easy way to provide multiple configs (and a way to specify which got applied based on glob patterns).\n. Either bump this to show ^6, or move this line\n\nWe don't yet support Babel 6...\n\nDown below the example code and preface it with Note:\n\njs\n{\n  \"devDependencies\": {\n   ...\n}\nNote: We don't yet support Babel 6, but you can still use it in your own project\n\nI personally think it would be better to just give the Babel 6 example. In essence, all we are doing right now is explaining to people how semver and dedupe works. I think it undersells the best part of this change: that you can now use Babel 6.\n. Are you talking about this test here? If this test passes, that seems fine to me. We are getting a failure. Users of the CLI or API will get \"can't find any files to test\", like they have. Anybody using lib/fork from outside AVA is already asking for trouble.\nWe need to investigate why it fails on Node 0.10.\n. use taps t.match(string, regexp) in all new tests. It gives us better error messages. See #256 \n. That is because we have to let Node print it to stderr natively. We could clean up the output of our own tests by allowing fork to take stdio options like child_process.fork does (and pipe them to something else besides stderr for that particular test). That's a future PR, IMO.\n. t.match\n. Do we have to make assert a class? Can't we just track assertions here? Demanding the assert class track assertions is going to make it harder to support custom assertions down the road.\nIs the issue simply that it is hard to know which argument is the message here?\n. crap, meant to move that to the top of the file.\n. @vdemedes - This event now happens for every assertion. Pass or fail.\nWe are changing event.type === 'error' / 'success' to event.assertionThrew === true/false\nsee https://github.com/twada/empower-core/pull/2 for further discussion.\nevent.originalMessage will contain the original message the user passed in. (It will be undefined if the user didn't supply one). That will help decide what we display in TAP.\nevent.originalMessage is provided for every assertion, including the \"non-enhanced\" ones.\n. The non-enhanced patterns are a new feature we are adding to power-assert. There will be no source code transform, but we do wrap the function and automatically extract the optional message parameter for you and pass it as event.originalMessage. It will be undefined if the user does not pass it in.\n. It's new - and it will be end up being called wrapOnlyPatterns. This is for functions that it does not make sense to provide the big graph for (i.e., any use of t.throws is likely to contain an inline function and be way to complicated for a power-assert renderer to do a good job with). By adding this though, we still wrap those and give you the assertion events, so you don't have to handle \"enhanced\" and \"non-enhanced\" separately. You get events for both.\n. I am not sure - it might. I need to test that. My initial thought is no, sense the only assertions that return promises is throws and doesNotThrow right? Those are non-enhanced. \nDefinitely worth me double checking though.\n. OK, then we are good. Those will never have powerAssertContext. Probably would be good to attach originalMessage though right? \n. Good point.\nWhen I started, I thought I only needed stage-3. I was unaware Object Rest/Spread arguments were stage-2.\n. The whole point of this test is that the file names get prefixed. We still need to test for this whole string.\n. Shouldn't windows users be allowed to use \\ as a path separator, since that is what they are used to?\nSeems to me that \nava --require .\\fixtures\\install-global.js .\\fixture\\validate-installed-global.js\nis the valid windows style alternative to \nava --require ./fixtures/install-global.js ./fixture/validate-installed-global.js\nI would prefer it if AVA accepted either path separator and just normalized it internally. \n. I am not suggesting we change the implementation. Just the line here in the test that normalizes the path to posix. We don't need to do that. And we want the test to fail if it doesn't allow windows users to use it. resolveCwd is going to end up normalizing it for the given operating system anyways.\n. cachePath + '_hash'\ninstead of invoking hasha twice\n. Not sure it is even necessary to make the filename part of the hash. If two files have identical content, it doesn't matter that they are located at different points on the file system - they will transpile identically right?\n. actually scratch that. I think power-assert may include the filename. In the transform.\n@twada?\n. Yep. Thanks.\n. OK, I tried trash-cli first but it fails if the directory doesn't exist. Was just about to push rimraf, does del-cli accept missing directories?\n. I believe there is from a performance standpoint (the VM actually has to create a new concatenated string), but I believe the results would be identical.\n. assertPromises -> assertions\n. self._assert() should work fine here. Promise.all converts array members to promise using Promise.resolve anyways.\n. use delay\n. make two separate tests.\n. this makes it sound like AVA will call t.end itself. I suggest something like:\nwaits for t.throws to resolve after t.end is called\nThis comment applies to multiple tests.\n. > number of assertions doesn't match plan\nmissing a word, and (minor nit) I think it reads better without the t.\n. js\nvar Promise = require('bluebird');\nPromise is not natively available in Node 0.10\n. doesn't look like you use these return values.\n. Agreed. I wrote this before I saw how you optimized the onAssertion callback. You can disregard.\n. I don't think we should make this particular change. It changes the default behavior of simply running ava. The default behavior should remain the same. If they want to use this feature they should have to run ava test.\n. I think what you are doing here is inconsistent and confusing.\nThe default is currently the same as running:\nava test.js test-*.js test/*.js\nIf we don't make this change that stays consistent. To engage the \"recursive directory behavior\" they should need explicitly provide a directory name they want recursed. There is less chance of an error that way. I find it far more confusing that there are .js globs for the base-dir, but we will recurse the test folder indefinitely.\n. Yeah sure. Probably worth matching style. \nThat said, after doing it the way you suggest for a long time, I am starting to think it's a mistake. ES2015 classes throw when function called, so magic auto-newing is not future proof. Also, it introduces an easy to overlook failure point when refactoring the function signature. I know I have forgotten to update that statement inside the if condition before.\n. Because the factory function used to be inlined, and I missed it. Good catch.\n. Where should it go?\n. I didn't modify this line, the only modification I made was the one discussed below (saving to disk in a different thread and loading here).\n. Oh,\nI forgot.\nI think this test can be dropped.\nThere is no longer a way to test a non-existent test file passed to fork, because fork now requires the file be precompiled. There is already a test for a non-existing files in api.js\n. Those are just presets, and don't reflect the actual versions of the underlying plugins (they use caret ranges). While it won't break anything to salt with preset versions, it may be providing a false sense of security.\n@thejameskyle - Any thoughts on this? Last time I browsed the babel source, it seemed you were only including the version of babel-core in the salt for the cache key.\n. I've replied to that thread: https://phabricator.babeljs.io/T6709#70444\n@thejameskyle - Happy to contribute a PR to babel if you want. \n. @sindresorhus \nMaybe we should default to caching off, and just use the \"compile in main thread\" optimization. That appears to give us 95% of the benefit anyways. \n. OK, changed it back to the original behavior (throw if they forget to use new).\n. How about just cacheEnabled?\n. This should either be in a before, or should use t.context over a closure:\n``` js\ntest.beforeEach(t => {\n   const app = express();\n   app.post('/signup', signupHandler);\n   t.context.app = app;\n});\ntest('signup: Success', async t => {\n  const res = await request(t.context.app)\n    .post('/signup')\n    ...\n});\n```\n. or\njs\nlet app;\ntest.before(() => {\n  app = express();\n  app.post('/signup', signupHandler);\n});\n. However my new preference is just to do async safe helper functions:\n``` js\nfunction app() {\n  const application = express();\n  application.post('/signup', signupHandler);\n  return application;\n}\ntest('signup: Success', async t => {\n  const res = await request(app())\n    .post(...)\n    ... \n});\n```\n. > could you explain why context is needed \nBecause everything executes concurrently in AVA. Assuming you are coming from mocha, you are accustomed to this order of execution:\n- beforeEach for  test 1\n- test 1\n- beforeEach for test 2\n- test 2\n- beforeEach for test 3\n- test 3\nWith AVA, your order of operations could be:\n- beforeEach for test 2\n- beforeEach for  test 1\n- test 1\n- beforeEach for test 3\n- test 2\n- test 3\nNote it is unlikely the order is exactly as above, the point I'm making is that you can't rely on order of execution. The only ordering you are guaranteed is that beforeEach for test X happens before test X. Where it happens in relation to other tests and their respective beforeEaches is not guaranteed.\nIf you use the --serial flag (ava --serial my-test.js), then your tests are ordered similar to mocha.\n. Each test file is it's own process (or thread if you want to think of it that way).\nt.context is shared between your beforeEach and the appropriate test. So in the above example, even though beforeEach is executed 3 times, it gets a unique t.context each time that is shared with just a single test run. If you are using a closure to pass data between a beforeEach and a test, that is almost certainly wrong, and should use t.context as well.\nIt's perfectly acceptable to use a closure to pass data created in t.before (in fact t.context does not work in t.before). \n. Just say async / await or something like that \n. yep - I was recommending language.\n. because then you can't focus on a single file.\n. Yep - that's what I was trying to say.\n. I'd need to decimalize all the keys before feeding into meow.options.defaults, so... decamilize-keys? Or just manually handle this one since it's currently the only option with a dash?\n. Or file a meow PR that decimalizes defaults for you? \n. I added it above the example.\n. Actually, it's required for this PR to work. https://github.com/sindresorhus/meow/pull/25\n. fixed\n. I just did\nnpm i -S tapjs/stack-utils#long-stack-traces\nand that is how it modified package.json. I want to get that PR merged before we merge this anyways.\n. Oops. This fixture isn't used anywhere. I just used it to create some of the above screenshots. It should be deleted.\n. Actually, stack-utils already does that. I'm not sure if it does it before the regexp gets applied, but it probably should.\n. Yep. Whitespace at the beginning of any multi-line string makes the yaml output much harder to read. stack-utils trims whitespace from the first line.\nI guess we could add an indent option to stack-utils.\n. Is there a good reason we can't just drop skipped: false from the line above this one?\n. So that should be the only select you need. @vdemedes - do we need that first value for a reporter?\n. I'm not sure I agree. The short flag first keeps everything aligned nicely. \nLong form first means the short flags are scattered instead of in a vertical line. It also means the short flags are creating noise in the middle of what is otherwise readable text\n. should be ./node_modules/.bin/nyc unless installed globally.\n. With this change, destructuring would no longer work. This is not allowed anymore:\njs\ntest({is, same} => {\n  is(1 + 1, 2);\n  same([1,2].slice(1), [2]);\n});\nYou have to preserve the this reference:\njs\ntest(t => {\n  t.is(1 + 1, 2);\n  t.same([1,2].slice(1), [2]);\n});\n. I would think all of cwd excluding node_modules.\n. Setting t.context in before or after will now throw an error. Previously we silently let them set it, but it was not passed to future tests.\n. oops. Yeah that should just be deleted. It's well covered in test-collection\n. True. (mistakes like this is why I prefer to throw instead of doing magic).\n. I probably should add a test covering that.\n. I think we are pretty close to agreement on how it should look. But the implementation is not written yet.\n. #448\n. I don't think any of our child-process code will ever call console.log.\n. done\n. I am just dropping the safe copies. If someone ever asks us to profile a suite that does something to console.log we can address it then.\n. Because plan needs to be passed to Error.captureStackTrace\n. Because that would add an extra line to the stack (one we don't need).\n. Actually, I think it always is. I got it in my head that method might be called directly in our unit tests, but I'm pretty sure all our unit tests use PublicApi anyways.\n. Yeah - that works.\n. The profiler seemed to startup async. \n. I put it here because I figured this is the only scenario where the difference might add up (generated tests are really the only way I see people getting to 10K+ tests and still caring about a half second difference).\nStill you may be right - it's probably not even worth mentioning.\nLose it?\n. This will avoid that error (and be faster):\njs\nvar i = fromIndex;\nfunction runAsync(ret) {\n  self._addResult(ret);\n  return self._run(i + 1);\n}\nfor(; i < length; i++) {\n  // ...\n  if (isPromise(result)) {\n    return result.then(runAsync);\n  }\n}\n. Any reason we can't just eliminate this if statement, and pass in 0?\njs\nreturn this._run(0);\n. Seems unnecessary to default to an empty array, just let it throw this.tests is undefined if it is misused.\n. done\n. Also, nextTick is deprecated since 0.10.0\n. Why not use a new Api instance each time instead of resetting?\n. Should the files parameter just be required? Move the handlePaths logic outside the Api entirely.\nThe Api is not documented or to be considered stable, so there's no worry about a breaking change here.\n. Same as above, why not just a new instance for a new run?\n. Looks like this TODO is done!\n. :+1: \nIt would be good to start doing that to anything we think we want to drop with 1.0.0\n. What does this comment mean?\n. We certainly seem to prefer informal contractions (i.e. can't) in the documentation. Not sure if that should carry over to error messages.\nIt seems cannot is more widely accepted - so, at a minimum, use that.\n. As discussed, we will want to ensure powerAssert is added as a plugin regardless.\n. :+1: I don't believe it is defined anywhere, but it should be.\n. Why do we care what the constructor is? Are we just trying to avoid cleaning primitives?\n. > I doubt that'll be noticeable given the cost of spawning a new process in order to run the tests.\nIt's definitely noticeable in real test suites.\nIMO, we need to stop comparing to \"the cost of spawning a new process\" as a reason to disqualify optimizations. That is a huge cost, nearly every optimization is going to be small in comparison at this point. There is a large subset of test suites for which AVA is demonstrably slower than the alternatives, and we are not going to close that gap without placing a high priority on optimizations.\n. This line could be better:\n\nSome libraries require browser specific globals (window, document, navigator, etc).\n\nAlso, You just need a single tick around window (not triple).\n. > Configure tests to use jsdom.\ntest => tests\n\"all\" seems extraneous\n. Drop this line. People can figure out they need to npm install\n. Configure AVA to require the helper before every test file.\n. triple tick to single around test/helpers\n. Link the word yet to the issue tracking browser support?\n. > Is there another way I can validate that the powerAssert plugin was added?\nRun a test with a bad assertion that generates powerAssert output. There is an existing test that already does that using the default config. You would just need to create a similar one that does it using custom config.\n. Why transform-runtime?\n. > Can you wrap the spinner char in chalk.dim()?\nI just gave it a try, I can't tell the difference.\n. currentStatus includes the spinner character. currentTest does not.\n. @sindresorhus usually objects to that. I don't have a strong opinion either way.\n. Also, this way leaves us open to having a command line option to configure your spinner:\n$ ava --spinner=balloon\nthen\njs\nvar spinnerDef = spinners[config.spinner];\n(I considered waiting until April 1st to post this as a feature request, but I'm pretty sure I would have forgotten by then).\n. Good call on todo. I implemented this before rebasing, so yeah - that almost certainly needs a fix.\nAre skipped tests shown yet? I thought there was still a pending PR for that.\n. Oh - and I also missed everything in final\n. As I recall, the API does track this, but it's not provided until the finish event. The mini reporter is the only reporter that cares about the live count.\n. This is a weird combination of callbacks and promises. Can we clean it up to just use promises?\n. Why would run just return this? Shouldn't it run something?\n. It gets filled with promises for run results here\n. I am guessing process.stdout.columns === undefined during testing. Same reason as here.\n\nThis is because tap forks the process and does not emulate a tty environment in the child process.\n\nYou could force the issue in those tests simply by setting process.stdout.columns to some large number in the tests.\n. That doesn't cover null.\n. To verify behavior when a user does that. (Not a recommended practice)\n. Huh?\n. :+1: (why 2?)\n. Because the arrow function is transformed to an anonymous function by Babel. Changing it back to an arrow function would not change the assertion. I felt this way the assertion was a little easier to understand.\n. Is there a linter rule we can use to reinforce that style?\n. Not yet apparently: https://github.com/eslint/eslint/issues/3092\n. :+1: for is-error.\n. There is no need for this function using debounce.\nJust do the following in the constructor:\njs\nif (this.options.timeout) {\n  this._onTimeout = debounce(this._onTimeout, ms(this.options.timeout));\n} else {\n  this._onTimeout = noop;\n}\nI think that will be easier to understand (and faster) than the current implementation or the no-clear branch.\n. Also, you shouldn't use debounce on a prototype method, as it will debounce across every instance and only call on the most recent.\nhttps://github.com/component/debounce/issues/8\n. There is no need for _restartTimer or _timer, just use lodash.debounce for onTimeout.\n. Ideally, powerAssert would run earlier so it captures source as written, instead of as transformed.\nhttps://gist.github.com/jamestalmage/885e1d0d732557cfb5cf854aee04fd19?ts=2\n. Looks like deprecate calls an undocumented method process.emitWarning.\nThat ends up causing a warning event to be emitted on process, which seems to be handled by writing to console.error.\nSo try console.error?\nOr just trust that util.deprecate works.\n. The power-assert enhancement actually has callbacks (onSuccess, onFailure) that are called with the result of each assertion. That is where we implement assertion tracking.\nI think you should probably keep t.same and t.notSame in the patterns until they are officially removed. Otherwise this will break assertionCount for them. Maybe add a test that ensures the assertion count is tracked for those (which we will remove when we drop support).\n. We don't transpile AVA. Can't use string templates on Node 0.10\n. Will removing these cause compiler errors for TypeScript? Does TypeScript have a way to deprecate in a d.t.s file?\n// @SamVerschueren \n. That's probably the only way to handle it. Looks like TS has a two year old issue on the topic:\nhttps://github.com/Microsoft/TypeScript/issues/390\n. Move the DEPRECATED warning to the start?\nIDE's use these comments for tooltips. So it would be good to have the warning as prominent as possible.\n. Tap does not have exclusive tests. That was my hacky way of getting them\n. :+1: **/__tests__/**\n. OK, If we make the change from #619 we need to revisit.\n. Once we land the API refactor, I would like to move all the logic of building include/exclude patterns from the config into lib/ava-files, and rewrite many of these tests so they are just:\njs\nt.true(avaFiles.shouldWatch('some/path'));\n. If they do --tap --watch it should throw. (\"watch is not allowed with tap reporter\")\nIf they do --watch and their pkgConf specifies tap:true, then it should fall through to picking verbose or mini reporter. (cli is overriding pkgConf in this case)\n. The .only modifier disables watch modes dependency tracking algorithm. When a change is made, all .only tests will be rerun, regardless of whether the test depends on the changed file.\n. This is better. However, if they do --tap --watch, AND have tap:true in their config they won't get this error message.\nThe only thing I can think of is to use hasFlag('--tap') instead of cli.flags.tap.\nAlso, they could specify {watch: true, tap: true} in their config and not get this warning.\nSo maybe:\njs\nif ((hasFlag('--tap') && hasFlag('--watch')) || (conf.watch && conf.tap)) {\n  // ...\n}\n... Or I'm just overthinking it.\n. @novemberborn - Currently, there is no way for users to set the exclude patterns. Is this a feature that needs to be exposed? \nYou access api.excludePatterns in watcher, but that's a static value that never changes (I guess an API consumer could, but we haven't published that, so ...).\nShould we open an issue about this, or is avaFiles.defaultExcludePatterns() all you need.\n. https://github.com/sindresorhus/has-flag\n. I did indeed\n. My thought was that a user might try both flags trying to troubleshoot why tap wasn't working in watch mode. I thought an explicit error would be better than hoping they had read the docs\n. I would suggest we set the assigned label even when a collaborator is assigned. That way they don't need to look two places.\n. Are we certain t.fail has it's this value bound in node-tap?\n. LGTM once that's handled.\n. Here? This one doesn't inherit anything you mean lower in the \"Extends\" section?\n. Shouldn't be. The babel-throws-helper wrapper is totally transparent to the code it wraps.\n. I considered that.\nBut if the user is green enough that that they don't fully understand how throw works, I was concerned about piling on another abstraction they need to understand.\n. The links still appear inline when rendered as HTML. No change to the rendered output. Just a bit cleaner in the raw document\n. OK - I changed them back.\n. Oh - Understood. Still not a problem though. power-assert wraps inline, just like ava-throws-helper.\njs\nt.true(_rec._expr(_rec._capt(_rec._capt(a, 'arguments/0/left') === 'bar', 'arguments/0'), {\n    content: 't.true(a === \\'bar\\')',\n    filepath: 'test/fixture/power-assert.js',\n    line: 6\n}));\nAlso, we don't enhance t.throws with power-assert.\n. I don't think this tracking behavior belongs in watcher.\nLet's enhance RunStatus to track both aggregate and per file failures, and give it the ability to track previous runs.\nHere is a possible API:\n``` js\nvar testRun = api.run([...listofFiles]);\n// someTime later\nvar previousRun = testRun;\ntestRun = api.run([...listOfFiles], previousRun);\n```\nAlternatively, maybe just create an \"AggregateStatus\" class:\njs\nvar combinedStatus = new AggregateStatus({\n  previousRun: RunStatus-instance,\n  currentRun: RunStatus-instance,\n  files: list-of-files-that-are-being-rerun, // or not being rerun, whichever makes more sense\n  deleted: list-of-deleted-tests\n});\n. > The current approach is relatively clean\nIt's clean enough, but watcher.js as a whole is getting pretty complex. I'm just looking for ways to separate what it does into logical chunks.\n. ?\n. This is more consistent with existing behavior in fork, etc. But maybe we should be emitting everything as absolute paths and have the reporters handle truncating to relative ones? Seems like it would be best to keep it explicit until we decide to show it to a human.\n. Agreed and understood.\n. Of those two I'd prefer !== 0, but I'm not sure why > 0 is unacceptable. Because it implies runVector might be less than zero? Or some other reason?\n. Sure - as stated below - none of my comments need to hold this up.\n. js\nJSON.stringify(this.babelConfig) || ''\nOr maybe we should allow undefined in md5-hash\n. Allowing arrays with holes in them, for this type of situation. We could just not call .update() when we find a hole. Or call update with a special placeholder set of data (so that the presence and position of holes in the array affects the final hash).\nStill, probably best just to fix it here\nNot sure it's a good idea.\n. Do that\n. I dunno. We have defaultExcludes, but that gets passed to watcher without modification. You had a test for watcher that assumed it \"could\" be modified at some point. We just don't have any logic for allowing users to do that. I just copied the test.\nIf there's no way for a user to modify it, is there a point to a test that verifies behavior when it's modified?\n. So, should isSource be renamed to trackedFile? Something else?\n. Not sure I understand the question. (this comment shows as outdated - looks like you're commenting commit by commit?)\n. Yes. But getting them to actually state \"I'm running version XXX\" eliminates one question-answer round. When trying to figure out the problem\n. I considered that, but you can't check them off in Preview mode, and we really do want them to delete the instructions stuff as they complete them, right? I thought people might be tempted to leave the check boxes there.\n. yeah - it's from combining the two codebases. nodePath was used in watcher, because he used a path variable inside methods. I can clean that up.\n. I think it worked well with watcher. A limited pool of people used it for a while (mostly people who watch AVA closely and contribute), and we released it a short time later.\nWe could add a mention in the release notes and tweet about it to get more eyeballs on it, but I think leaving it undocumented gives us more freedom to make breaking changes. We are pre 1.0.0, so it's not like breaking changes are a huge deal, but they are an annoyance. We have so many users now, I think it's a good idea to signal to them when they are playing on the bleeding edge (at least when we can).\n. after each => afterEach\n. after each => afterEach\n. This should still use _runNoPool if concurrency is less than the fileCount.\n. The contents of this and the catch block below are identical.\nHoist this function, pass it in directly here, and call it from the catch block.\n. This could also be fixed by making this file executable and adding a hashbang line.\nDoes iron-node, etc. tolerate hash bangs?\n. @cgcgbcbc - Can you add a few tests in test/test-collection.js for this throwing behavior?\n. Also, a few tests in test/test-collection.js verifying new tests are added correctly (this will involve a modification to the serialize function in that test).\n. Meh, I prefer it the existing way. Two argument then is considered an anti pattern now.\n. > Use test.after.always() to register a hook that will always run once your tests and other hooks complete. .always() hooks run regardless of whether there were earlier failures, so they are ideal for cleanup tasks.\n. what @novemberborn said, plus:\n\n.always() hooks are ideal for cleanup tasks.\n. > this will always run, regardless of earlier failures.\n. This kinda feels like the wrong place to be doing this.\n\nMaybe in the individual reporters?\nOr create a lib/reporters/_helper that contains this.\n. Why this? Why not just:\njs\nvar runStatus = new RunStatus();\n. same as above\n. We are ignoring the output of the cli run here?\nWe should be comparing this to stdout, not generating a new output.\nIf we do it this way, let's move it out of the CLI test, and do that somewhere else. CLI tests are slow.\n. But if I add a concurrency option to package.json, I want to be able to re-enable .only behavior. By doing \nava [files ...]\nWhere the number of files is less than concurrency.\n. That may be a cleaner way of saying it, but I'm trying to introduce the term. Maybe \"reusable macros\", but I think it's good as is.\n. I would like to expand on this a bit: \n\nYou can use the .failing modifier to document issues with your code that need to be fixed. Failing tests are run just like normal ones, but they are expected to fail, and will not break your build when they do. If a test marked as failing actually passes, it will be reported as an error and fail the build with a helpful message instructing you to remove the .failing modifier.\nThis allows you to merge .failing tests before a fix is implemented without breaking CI. This is a great way to recognize good bug report PR's with a commit credit, even if the reporter is unable to actually fix the problem.\n. People should link to the relevant issue. We should model that behavior in the docs.\n\nAlso, a bit of language/style cleanup.\njs\n// See: github.com/user/repo/issues/1234\ntest.failing('demonstrate some bug', t => {\n  t.fail(); // test will count as passed\n});\n. Yeah, eating my words on that. Though not as badly as I've been forced to eat these ones.\n\nThen why are you bringing up the Babel plugin? Is that something someone would use IRL?\n. isWatching => watching\n. No need to do this. It's already falsy.\n. make a save copy with objectAssign;\n. Sorry, I didn't phrase this right.\n\nThe correct question is, what about require('foo-bar.json') or require('foo-bar.coffee').\nDoes this mean smart walking won't work with .coffee?\n. I think you forgot to delete this\n. Just remove time. It only prints in watch mode now\n. Change the assertion to\nt.is(JSON.stringify(actual), JSON.stringify(expected)\n. Yes. My suggestion above will make the color chars visible \n. Argh. I'm realizing this is actually kinda hard to test.\nThis will actually find every file in the default-patterns folder.\nInstead of this argument, can you just process.chdir() to the default-patterns folder, and drop the constructor argument?\nYou are technically overriding the default here.\n863 introduce cwd and resolveTestsFrom options, that would make this doable without changing the directory.\n. Also, you should sprinkle in a few files that should NOT be found (and ensure they are not).\n. Use slash to convert the results from \\ results, to / results (I think).\n. Sorry Kent, \nI see why you did it this way - It's exactly like the test I told you to copy. I'm updating your PR now. Expect a new PR and release soon.\n. Should we wrap these options in a generator function? So we aren't exporting a mutable copy?\n. Shebang line and make it executable?\nDoes that even work on Windows?\n. Lots of tests being deleted here. Many of which guard specific regressions. If the intent is to reimplement later, I would prefer skipping instead:\njs\ntest('tilte', { skip: true }, function (t) {\n  ...\n});\n. simpler usage in fork.js\n. removes this platform dependent check/error message into it's own file.\nreducing clutter in index.js\n. no additional complication in child process.\n. the only place you could make the argument things are \"more complex\" anywhere is this line, and I don't think it's that bad\n. I do not see this test replicated anywhere.\n. I do not see this test replicated.\n. I see now you combined the following three tests into a single one. I would have preferred they stay separate, so each behavior is verified individually, but no biggie.\n. Yes - that's my point, it's not adding any complexity here.\n. it's not in wrap-send, it's in send-to-parent (it's own module - as requested), If there isn't a parent, it complains.\nIt seems entirely sensible that require('send-to-parent') would throw if you were no parent. \n. Really, we could do away with wrap-send entirely, and just inline in send-to-parent and fork.js. It's hardly worth it at this point.\n. if the module is called send-to-parent, I think it's reasonable to check if it's a child process inline.\n. Anything can be a function. Mix and match however you want.\nalways.after and after.always are the same thing.\nYou could technically do always.always.always.after - it would still be treated the same way.\nThe only thing disallowed is certain combinations:\nOnly one of todo, skip, failing should be allowed in the chain.\nalways only combines with the after(Each) hooks, etc.\nTechnically, the JavaScript allows any combination (you will never get a \"not a function\" TypeError) - we just validate the metadata to make sure the combinations make sense.\n. Please note, the verbose reporter => The verbose reporter \n. Yeah that was my concern. Let's just leave it as is.\n. This should probably be a prepublish script. Otherwise I am going to forget this step when publishing.\n. > Well written clients / services should emit errors if concurrency limits are exceeded, instead of transparently hanging execution. If they don't, you should consider submitting an issue request asking that they do. \n. IDK, the whole point seems a little unnecessary. Services are going to do what they do, we are just explaining the reality of how to make AVA work with services that behave this way.\n. const => var throughout. It's not supported on Node 0.12 or 0.10.\nHopefully, that's the last of it. I would like to get this merged.\n. > Assertion failures would throw\nOur assertions don't throw after being enhanced.\njs\ntest('foo', t => {\n  t.is(1, 2);\n  console.log('foo'); // this will get printed.\n});\n. Even when we do self test with AVA, the parallel processes will have their own process.execArgv. Test isolation FTW!\n. > But not tests in the same file ;)\nTrue. Though you can always just do test.serial for the few that would interfere.\n. Probably not Node.\nBut I have ideas for meow: https://github.com/sindresorhus/meow/issues/44\n. Meh. I forgot all the places colors is used. Not sure about having it here now.\nMaybe follow the same convention we use for test helpers?\nlib/reporters/_colors\nIDK.\n. Yeah. That can be dropped. A mirror existed in browser-adapter at one point, but that was early on. I just forgot to remove this.\n. > this will implicitly be merged with the .babelrc.\nYes. That's the idea. That is only precompiling your sources. We already precompile your tests in caching-precompiler.\n. @geowarin - see https://github.com/geowarin/repro-ava-precompile/issues/1 to fix yours.\n. All caps (TAP) is the preferred representation based on the website http://testanything.org\n. This could also happen with:\n``` js\ntest(async t => {\n  // stuff\n});\nsetImmediate(() => {\n  test(t => {\n    // not allowed!\n  });\n});\n```\nThe message should include something about declaring tests async.\n\nAll tests and hooks must be declared synchronously in your test file, and cannot be nested within other tests.\n. I am pretty sure moving this in here will break karma-ava. Anything that relies on the Node environment (vs the browser) should remain in process-adapter.\n\nprocess-adapter is swapped out by karma-ava when compiling for the browser with an implementation that polyfills any missing functionality.. We shouldn't force users to use t.pass in this scenario. You can pass t.end as an error first callback, so passing t.end really should count as performing an assertion.. I am not so concerned about breaking karma-ava in the short term. But we need a strategy for where we will locate Node stuff that needs to be polyfilled for the browser. This is going to necessarily create some \"oddness\", as you will encounter the need to abstract out stuff you might prefer to inline throughout the codebase.. Where is the finishDueToInactivity method defined?. I realize this is not from this PR, but a pending assertion technically hasn't passed yet. This message should be different.. Maybe add a test to validate the works it happens twice (pending resolve, but more pending have been added, and yet another sync will be added while you wait for those to resolve). . Basically, we are making sure that we use @std/esm's require function instead of the standard one when we load the test file later in the file.\nI am a bit surprised that this is necessary, since I figured @std/esm would install a require extension hook (via pirates or similar). @jdalton, am I mistaken here? Do you not implement a require hook via require.extensions['.js']?. Like \"cjs\"\\n is the full contents of the file?. precompile would only be short circuiting imports for tests, but those will have already used Babel to convert the import statements to require statements.. I poked around in the @std/esm codebase, and didn't see where you were actually hooking require.extensions (even in \"CLI mode\").. OK, so this is actually only a problem when Babel is configured not to transpile ESM modules. Setting babel:false does not actually disable Babel - we still apply a few transforms to the tests for power-assert support, and providing better errors for common misuses of the API. \nSince we're always going to load Babel, precompile: false becomes a huge performance hit, as Babel just takes too long to load.. > power-assert ships with a UMD build (no Babel required). The babel-plugin-throws-helper could always be optional or could be written for acorn+magic-string or other lighter solutions.\nInteresting ideas. Power assert already has an acorn based transform, so the whole stack could share that.. ",
    "ariporad": ":+1: \n. @sindresorhus: Do we want to start adding browser support? Or are we waiting on that?\nIf we're going ahead with it, I'd like to help (I'm going back to school soon, so I might not be as active). Would you want to just abandon multi-threaded-ness? Or should we run things in web workers? Or new tabs? iframes? Also, I presume you'd want to integrate with Karma?\n. @sindresorhus: Do we want to start adding browser support? Or are we waiting on that?\nIf we're going ahead with it, I'd like to help (I'm going back to school soon, so I might not be as active). Would you want to just abandon multi-threaded-ness? Or should we run things in web workers? Or new tabs? iframes? Also, I presume you'd want to integrate with Karma?\n. @jamestalmage: Agreed, @sindresorhus?\n. @jamestalmage: Agreed, @sindresorhus?\n. @BqrryThePenguin: (Awesome username, btw). We would almost certainly\nsupport running tests through karma as the preferred browser setup.\nOn Wed, Jan 13, 2016 at 3:23 PM Jonathan Haines notifications@github.com\nwrote:\n\nWould AVA provide a solution for browser testing through it's own web\nserver, browser loader and client, or is the plan to just support, and have\nexamples/recipes, for running tests in a browser environment?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/24#issuecomment-171469830.\n. @BqrryThePenguin: (Awesome username, btw). We would almost certainly\nsupport running tests through karma as the preferred browser setup.\nOn Wed, Jan 13, 2016 at 3:23 PM Jonathan Haines notifications@github.com\nwrote:\nWould AVA provide a solution for browser testing through it's own web\nserver, browser loader and client, or is the plan to just support, and have\nexamples/recipes, for running tests in a browser environment?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/24#issuecomment-171469830.\n. @jamestalmage: I haven't used karma in a while, but IIRC, karma just takes\nan array of files.\nOn Wed, Jan 13, 2016 at 3:35 PM James Talmage notifications@github.com\nwrote:\nIt depends on how easy it would be to modify karma to run multiple files\nat once. But karma would definitely be the first thing we look at.\nI would like to see it be a bit more than recipes. If nothing else, worst\ncase a shareable karma baseconfig that you can use without modification if\nyou do things \"the AVA way\"\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/24#issuecomment-171472048.\n. @jamestalmage: I haven't used karma in a while, but IIRC, karma just takes\nan array of files.\nOn Wed, Jan 13, 2016 at 3:35 PM James Talmage notifications@github.com\nwrote:\nIt depends on how easy it would be to modify karma to run multiple files\nat once. But karma would definitely be the first thing we look at.\nI would like to see it be a bit more than recipes. If nothing else, worst\ncase a shareable karma baseconfig that you can use without modification if\nyou do things \"the AVA way\"\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/24#issuecomment-171472048.\n. @sindresorhus: I have a really stupid idea: What if we webpack'd (or something similar) AVA and all of it's dependencies (except the CLI), and have the child processes use that? Then they wouldn't have to do any requiring. If we uglified that, it would eliminate requiring, and greatly reduce parsing time.\n. @sindresorhus: I have a really stupid idea: What if we webpack'd (or something similar) AVA and all of it's dependencies (except the CLI), and have the child processes use that? Then they wouldn't have to do any requiring. If we uglified that, it would eliminate requiring, and greatly reduce parsing time.\n. @sindresorhus: Or, a slightly more sane idea: Make a map of every file in our dependency tree mapped to it's contents, then hook require to use those instead of the file system. Example:\n\n`` javascript\nvar files = new Map([\n    [\"./lib/test.js\", \"'use strict';\\nvar isGeneratorFn = require('is-generator-fn');\\nvar maxTimeout = require('max-timeout');\\nvar Promise = require('bluebird');\\nvar fnName = require('fn-name');\\nvar co = require('co-with-promise');\\nvar observableToPromise = require('observable-to-promise');\\nvar isPromise = require('is-promise');\\nvar isObservable = require('is-observable');\\nvar assert = require('./assert');\\nvar enhanceAssert = require('./enhance-assert');\\nvar globals = require('./globals');\\n\\nfunction Test(title, fn) {\\nif (!(this instanceof Test)) {\\n\\t\\treturn new Test(title, fn);\\n\\t}\\n\\n\\tif (typeof title === 'function') {\\n\\t\\tfn = title;\\n\\t\\ttitle = null;\\n\\t}\\n\\n\\tassert.is(typeof fn, 'function', 'you must provide a callback');\\n\\n\\tthis.title = title || fnName(fn) || '[anonymous]';\\n\\tthis.fn = isGeneratorFn(fn) ? co.wrap(fn) : fn;\\n\\tthis.assertions = [];\\n\\tthis.planCount = null;\\n\\tthis.duration = null;\\n\\tthis.assertError = undefined;\\n\\n\\tObject.defineProperty(this, 'assertCount', {\\n\\t\\tenumerable: true,\\n\\t\\tget: function () {\\n\\t\\t\\treturn this.assertions.length;\\n\\t\\t}\\n\\t});\\n\\n\\t// TODO(jamestalmage): make this an optional constructor arg instead of having Runner set it after the fact.\\n\\t// metadata should just always exist, otherwise it requires a bunch of ugly checks all over the place.\\n\\tthis.metadata = {};\\n\\n\\t// store the time point before test execution\\n\\t// to calculate the total time spent in test\\n\\tthis._timeStart = null;\\n\\n\\t// workaround for Babel giving anonymous functions a name\\n\\tif (this.title === 'callee$0$0') {\\n\\t\\tthis.title = '[anonymous]';\\n\\t}\\n}\\n\\nmodule.exports = Test;\\n\\nTest.prototype._assert = function (promise) {\\n\\tthis.assertions.push(promise);\\n};\\n\\nTest.prototype._setAssertError = function (err) {\\n\\tif (this.assertError !== undefined) {\\n\\t\\treturn;\\n\\t}\\n\\n\\tif (err === undefined) {\\n\\t\\terr = 'undefined';\\n\\t}\\n\\n\\tthis.assertError = err;\\n};\\n\\nTest.prototype.plan = function (count) {\\n\\tif (typeof count !== 'number') {\\n\\t\\tthrow new TypeError('Expected a number');\\n\\t}\\n\\n\\tthis.planCount = count;\\n\\n\\t// in case theplanCountdoesn't matchassertCount,\\n\\t// we need the stack of this function to throw with a useful stack\\n\\tthis.planStack = new Error().stack;\\n};\\n\\nTest.prototype.run = function () {\\n\\tvar self = this;\\n\\n\\tthis.promise = Promise.pending();\\n\\n\\t// TODO(vdemedes): refactor this to avoid storing the promise\\n\\tif (!this.fn) {\\n\\t\\tthis.exit();\\n\\t\\treturn undefined;\\n\\t}\\n\\n\\tthis._timeStart = globals.now();\\n\\n\\t// wait until all assertions are complete\\n\\tthis._timeout = globals.setTimeout(function () {}, maxTimeout);\\n\\n\\tvar ret;\\n\\n\\ttry {\\n\\t\\tret = this.fn(this._publicApi());\\n\\t} catch (err) {\\n\\t\\tthis._setAssertError(err);\\n\\t\\tthis.exit();\\n\\t}\\n\\n\\tvar asyncType = 'promises';\\n\\n\\tif (isObservable(ret)) {\\n\\t\\tasyncType = 'observables';\\n\\t\\tret = observableToPromise(ret);\\n\\t}\\n\\n\\tif (isPromise(ret)) {\\n\\t\\tif (this.metadata.callback) {\\n\\t\\t\\tself._setAssertError(new Error('Do not return ' + asyncType + ' from tests declared via test.cb(...), if you want to return a promise simply declare the test via test(...)'));\\n\\t\\t\\tthis.exit();\\n\\t\\t\\treturn this.promise.promise;\\n\\t\\t}\\n\\n\\t\\tret\\n\\t\\t\\t.then(function () {\\n\\t\\t\\t\\tself.exit();\\n\\t\\t\\t})\\n\\t\\t\\t.catch(function (err) {\\n\\t\\t\\t\\tself._setAssertError(err);\\n\\t\\t\\t\\tself.exit();\\n\\t\\t\\t});\\n\\t} else if (!this.metadata.callback) {\\n\\t\\tthis.exit();\\n\\t}\\n\\n\\treturn this.promise.promise;\\n};\\n\\nObject.defineProperty(Test.prototype, 'end', {\\n\\tget: function () {\\n\\t\\tif (this.metadata.callback) {\\n\\t\\t\\treturn this._end.bind(this);\\n\\t\\t}\\n\\t\\tthrow new Error('t.end is not supported in this context. To use t.end as a callback, you must use \\\"callback mode\\\" via test.cb(testName, fn) ');\\n\\t}\\n});\\n\\nTest.prototype._end = function (err) {\\n\\tif (err) {\\n\\t\\tthis._setAssertError(new assert.AssertionError({\\n\\t\\t\\tactual: err,\\n\\t\\t\\tmessage: 'Callback called with an error \u2192 ' + err,\\n\\t\\t\\toperator: 'callback'\\n\\t\\t}));\\n\\n\\t\\tthis.exit();\\n\\t\\treturn;\\n\\t}\\n\\n\\tif (this.endCalled) {\\n\\t\\tthrow new Error('.end() called more than once');\\n\\t}\\n\\n\\tthis.endCalled = true;\\n\\tthis.exit();\\n};\\n\\nTest.prototype._checkPlanCount = function () {\\n\\tif (this.assertError === undefined && this.planCount !== null && this.planCount !== this.assertions.length) {\\n\\t\\tthis._setAssertError(new assert.AssertionError({\\n\\t\\t\\tactual: this.assertions.length,\\n\\t\\t\\texpected: this.planCount,\\n\\t\\t\\tmessage: 'Assertion count does not match planned',\\n\\t\\t\\toperator: 'plan'\\n\\t\\t}));\\n\\n\\t\\t//this.assertError.stack = this.planStack;\\n\\t}\\n};\\n\\nTest.prototype.exit = function () {\\n\\tvar self = this;\\n\\n\\tthis._checkPlanCount();\\n\\n\\tPromise.all(this.assertions)\\n\\t\\t.catch(function (err) {\\n\\t\\t\\tself._setAssertError(err);\\n\\t\\t})\\n\\t\\t.finally(function () {\\n\\t\\t\\t// calculate total time spent in test\\n\\t\\t\\tself.duration = globals.now() - self._timeStart;\\n\\n\\t\\t\\t// stop infinite timer\\n\\t\\t\\tglobals.clearTimeout(self._timeout);\\n\\n\\t\\t\\tself._checkPlanCount();\\n\\n\\t\\t\\tif (!self.ended) {\\n\\t\\t\\t\\tself.ended = true;\\n\\n\\t\\t\\t\\tglobals.setImmediate(function () {\\n\\t\\t\\t\\t\\tif (self.assertError !== undefined) {\\n\\t\\t\\t\\t\\t\\tself.promise.reject(self.assertError);\\n\\t\\t\\t\\t\\t\\treturn;\\n\\t\\t\\t\\t\\t}\\n\\n\\t\\t\\t\\t\\tself.promise.resolve(self);\\n\\t\\t\\t\\t});\\n\\t\\t\\t}\\n\\t\\t});\\n};\\n\\nTest.prototype._publicApi = function () {\\n\\tvar self = this;\\n\\tvar api = {skip: {}};\\n\\n\\t// Getters\\n\\t[\\n\\t\\t'assertCount',\\n\\t\\t'title',\\n\\t\\t'end'\\n\\t]\\n\\t\\t.forEach(function (name) {\\n\\t\\t\\tObject.defineProperty(api, name, {\\n\\t\\t\\t\\tenumerable: name === 'end' ? self.metadata.callback : true,\\n\\t\\t\\t\\tget: function () {\\n\\t\\t\\t\\t\\treturn self[name];\\n\\t\\t\\t\\t}\\n\\t\\t\\t});\\n\\t\\t});\\n\\n\\t// Get / Set\\n\\tObject.defineProperty(api, 'context', {\\n\\t\\tenumerable: true,\\n\\t\\tget: function () {\\n\\t\\t\\treturn self.context;\\n\\t\\t},\\n\\t\\tset: function (context) {\\n\\t\\t\\tself.context = context;\\n\\t\\t}\\n\\t});\\n\\n\\t// Bound Functions\\n\\tapi.plan = this.plan.bind(this);\\n\\n\\tfunction skipFn() {\\n\\t\\tself._assert(Promise.resolve());\\n\\t}\\n\\n\\tfunction onAssertionEvent(event) {\\n\\t\\tvar promise;\\n\\t\\tif (event.assertionThrew) {\\n\\t\\t\\tevent.error.powerAssertContext = event.powerAssertContext;\\n\\t\\t\\tpromise = Promise.reject(event.error);\\n\\t\\t} else {\\n\\t\\t\\tpromise = Promise.resolve(observableToPromise(event.returnValue));\\n\\t\\t}\\n\\t\\tpromise = promise\\n\\t\\t\\t.catch(function (err) {\\n\\t\\t\\t\\terr.originalMessage = event.originalMessage;\\n\\t\\t\\t\\treturn Promise.reject(err);\\n\\t\\t\\t});\\n\\t\\tself._assert(promise);\\n\\t}\\n\\n\\tvar enhanced = enhanceAssert({\\n\\t\\tassert: assert,\\n\\t\\tonSuccess: onAssertionEvent,\\n\\t\\tonError: onAssertionEvent\\n\\t});\\n\\n\\t// Patched assert methods: increase assert count and store errors.\\n\\tObject.keys(assert).forEach(function (el) {\\n\\t\\tapi.skip[el] = skipFn;\\n\\t\\tapi[el] = enhanced[el].bind(enhanced);\\n\\t});\\n\\n\\tapi._capt = enhanced._capt.bind(enhanced);\\n\\tapi._expr = enhanced._expr.bind(enhanced);\\n\\n\\treturn api;\\n};\\n\"],\n    // etc.\n]);\nvar actualRequire = require._extensions['.js'];\nrequire._extensions['.js'] = function (module, filename) {\n    if (files.has(filename)) return module._compile(filename);\n    return actualRequire.apply(this, arguments);\n};\nrequire('./lib/babel.js');\n```\nIf we uglified the files with \"whitespace only\" mode, it could totally* work!\n*Probobly\n. @sindresorhus: Or, a slightly more sane idea: Make a map of every file in our dependency tree mapped to it's contents, then hook require to use those instead of the file system. Example:\n`` javascript\nvar files = new Map([\n    [\"./lib/test.js\", \"'use strict';\\nvar isGeneratorFn = require('is-generator-fn');\\nvar maxTimeout = require('max-timeout');\\nvar Promise = require('bluebird');\\nvar fnName = require('fn-name');\\nvar co = require('co-with-promise');\\nvar observableToPromise = require('observable-to-promise');\\nvar isPromise = require('is-promise');\\nvar isObservable = require('is-observable');\\nvar assert = require('./assert');\\nvar enhanceAssert = require('./enhance-assert');\\nvar globals = require('./globals');\\n\\nfunction Test(title, fn) {\\nif (!(this instanceof Test)) {\\n\\t\\treturn new Test(title, fn);\\n\\t}\\n\\n\\tif (typeof title === 'function') {\\n\\t\\tfn = title;\\n\\t\\ttitle = null;\\n\\t}\\n\\n\\tassert.is(typeof fn, 'function', 'you must provide a callback');\\n\\n\\tthis.title = title || fnName(fn) || '[anonymous]';\\n\\tthis.fn = isGeneratorFn(fn) ? co.wrap(fn) : fn;\\n\\tthis.assertions = [];\\n\\tthis.planCount = null;\\n\\tthis.duration = null;\\n\\tthis.assertError = undefined;\\n\\n\\tObject.defineProperty(this, 'assertCount', {\\n\\t\\tenumerable: true,\\n\\t\\tget: function () {\\n\\t\\t\\treturn this.assertions.length;\\n\\t\\t}\\n\\t});\\n\\n\\t// TODO(jamestalmage): make this an optional constructor arg instead of having Runner set it after the fact.\\n\\t// metadata should just always exist, otherwise it requires a bunch of ugly checks all over the place.\\n\\tthis.metadata = {};\\n\\n\\t// store the time point before test execution\\n\\t// to calculate the total time spent in test\\n\\tthis._timeStart = null;\\n\\n\\t// workaround for Babel giving anonymous functions a name\\n\\tif (this.title === 'callee$0$0') {\\n\\t\\tthis.title = '[anonymous]';\\n\\t}\\n}\\n\\nmodule.exports = Test;\\n\\nTest.prototype._assert = function (promise) {\\n\\tthis.assertions.push(promise);\\n};\\n\\nTest.prototype._setAssertError = function (err) {\\n\\tif (this.assertError !== undefined) {\\n\\t\\treturn;\\n\\t}\\n\\n\\tif (err === undefined) {\\n\\t\\terr = 'undefined';\\n\\t}\\n\\n\\tthis.assertError = err;\\n};\\n\\nTest.prototype.plan = function (count) {\\n\\tif (typeof count !== 'number') {\\n\\t\\tthrow new TypeError('Expected a number');\\n\\t}\\n\\n\\tthis.planCount = count;\\n\\n\\t// in case theplanCountdoesn't matchassertCount,\\n\\t// we need the stack of this function to throw with a useful stack\\n\\tthis.planStack = new Error().stack;\\n};\\n\\nTest.prototype.run = function () {\\n\\tvar self = this;\\n\\n\\tthis.promise = Promise.pending();\\n\\n\\t// TODO(vdemedes): refactor this to avoid storing the promise\\n\\tif (!this.fn) {\\n\\t\\tthis.exit();\\n\\t\\treturn undefined;\\n\\t}\\n\\n\\tthis._timeStart = globals.now();\\n\\n\\t// wait until all assertions are complete\\n\\tthis._timeout = globals.setTimeout(function () {}, maxTimeout);\\n\\n\\tvar ret;\\n\\n\\ttry {\\n\\t\\tret = this.fn(this._publicApi());\\n\\t} catch (err) {\\n\\t\\tthis._setAssertError(err);\\n\\t\\tthis.exit();\\n\\t}\\n\\n\\tvar asyncType = 'promises';\\n\\n\\tif (isObservable(ret)) {\\n\\t\\tasyncType = 'observables';\\n\\t\\tret = observableToPromise(ret);\\n\\t}\\n\\n\\tif (isPromise(ret)) {\\n\\t\\tif (this.metadata.callback) {\\n\\t\\t\\tself._setAssertError(new Error('Do not return ' + asyncType + ' from tests declared via test.cb(...), if you want to return a promise simply declare the test via test(...)'));\\n\\t\\t\\tthis.exit();\\n\\t\\t\\treturn this.promise.promise;\\n\\t\\t}\\n\\n\\t\\tret\\n\\t\\t\\t.then(function () {\\n\\t\\t\\t\\tself.exit();\\n\\t\\t\\t})\\n\\t\\t\\t.catch(function (err) {\\n\\t\\t\\t\\tself._setAssertError(err);\\n\\t\\t\\t\\tself.exit();\\n\\t\\t\\t});\\n\\t} else if (!this.metadata.callback) {\\n\\t\\tthis.exit();\\n\\t}\\n\\n\\treturn this.promise.promise;\\n};\\n\\nObject.defineProperty(Test.prototype, 'end', {\\n\\tget: function () {\\n\\t\\tif (this.metadata.callback) {\\n\\t\\t\\treturn this._end.bind(this);\\n\\t\\t}\\n\\t\\tthrow new Error('t.end is not supported in this context. To use t.end as a callback, you must use \\\"callback mode\\\" via test.cb(testName, fn) ');\\n\\t}\\n});\\n\\nTest.prototype._end = function (err) {\\n\\tif (err) {\\n\\t\\tthis._setAssertError(new assert.AssertionError({\\n\\t\\t\\tactual: err,\\n\\t\\t\\tmessage: 'Callback called with an error \u2192 ' + err,\\n\\t\\t\\toperator: 'callback'\\n\\t\\t}));\\n\\n\\t\\tthis.exit();\\n\\t\\treturn;\\n\\t}\\n\\n\\tif (this.endCalled) {\\n\\t\\tthrow new Error('.end() called more than once');\\n\\t}\\n\\n\\tthis.endCalled = true;\\n\\tthis.exit();\\n};\\n\\nTest.prototype._checkPlanCount = function () {\\n\\tif (this.assertError === undefined && this.planCount !== null && this.planCount !== this.assertions.length) {\\n\\t\\tthis._setAssertError(new assert.AssertionError({\\n\\t\\t\\tactual: this.assertions.length,\\n\\t\\t\\texpected: this.planCount,\\n\\t\\t\\tmessage: 'Assertion count does not match planned',\\n\\t\\t\\toperator: 'plan'\\n\\t\\t}));\\n\\n\\t\\t//this.assertError.stack = this.planStack;\\n\\t}\\n};\\n\\nTest.prototype.exit = function () {\\n\\tvar self = this;\\n\\n\\tthis._checkPlanCount();\\n\\n\\tPromise.all(this.assertions)\\n\\t\\t.catch(function (err) {\\n\\t\\t\\tself._setAssertError(err);\\n\\t\\t})\\n\\t\\t.finally(function () {\\n\\t\\t\\t// calculate total time spent in test\\n\\t\\t\\tself.duration = globals.now() - self._timeStart;\\n\\n\\t\\t\\t// stop infinite timer\\n\\t\\t\\tglobals.clearTimeout(self._timeout);\\n\\n\\t\\t\\tself._checkPlanCount();\\n\\n\\t\\t\\tif (!self.ended) {\\n\\t\\t\\t\\tself.ended = true;\\n\\n\\t\\t\\t\\tglobals.setImmediate(function () {\\n\\t\\t\\t\\t\\tif (self.assertError !== undefined) {\\n\\t\\t\\t\\t\\t\\tself.promise.reject(self.assertError);\\n\\t\\t\\t\\t\\t\\treturn;\\n\\t\\t\\t\\t\\t}\\n\\n\\t\\t\\t\\t\\tself.promise.resolve(self);\\n\\t\\t\\t\\t});\\n\\t\\t\\t}\\n\\t\\t});\\n};\\n\\nTest.prototype._publicApi = function () {\\n\\tvar self = this;\\n\\tvar api = {skip: {}};\\n\\n\\t// Getters\\n\\t[\\n\\t\\t'assertCount',\\n\\t\\t'title',\\n\\t\\t'end'\\n\\t]\\n\\t\\t.forEach(function (name) {\\n\\t\\t\\tObject.defineProperty(api, name, {\\n\\t\\t\\t\\tenumerable: name === 'end' ? self.metadata.callback : true,\\n\\t\\t\\t\\tget: function () {\\n\\t\\t\\t\\t\\treturn self[name];\\n\\t\\t\\t\\t}\\n\\t\\t\\t});\\n\\t\\t});\\n\\n\\t// Get / Set\\n\\tObject.defineProperty(api, 'context', {\\n\\t\\tenumerable: true,\\n\\t\\tget: function () {\\n\\t\\t\\treturn self.context;\\n\\t\\t},\\n\\t\\tset: function (context) {\\n\\t\\t\\tself.context = context;\\n\\t\\t}\\n\\t});\\n\\n\\t// Bound Functions\\n\\tapi.plan = this.plan.bind(this);\\n\\n\\tfunction skipFn() {\\n\\t\\tself._assert(Promise.resolve());\\n\\t}\\n\\n\\tfunction onAssertionEvent(event) {\\n\\t\\tvar promise;\\n\\t\\tif (event.assertionThrew) {\\n\\t\\t\\tevent.error.powerAssertContext = event.powerAssertContext;\\n\\t\\t\\tpromise = Promise.reject(event.error);\\n\\t\\t} else {\\n\\t\\t\\tpromise = Promise.resolve(observableToPromise(event.returnValue));\\n\\t\\t}\\n\\t\\tpromise = promise\\n\\t\\t\\t.catch(function (err) {\\n\\t\\t\\t\\terr.originalMessage = event.originalMessage;\\n\\t\\t\\t\\treturn Promise.reject(err);\\n\\t\\t\\t});\\n\\t\\tself._assert(promise);\\n\\t}\\n\\n\\tvar enhanced = enhanceAssert({\\n\\t\\tassert: assert,\\n\\t\\tonSuccess: onAssertionEvent,\\n\\t\\tonError: onAssertionEvent\\n\\t});\\n\\n\\t// Patched assert methods: increase assert count and store errors.\\n\\tObject.keys(assert).forEach(function (el) {\\n\\t\\tapi.skip[el] = skipFn;\\n\\t\\tapi[el] = enhanced[el].bind(enhanced);\\n\\t});\\n\\n\\tapi._capt = enhanced._capt.bind(enhanced);\\n\\tapi._expr = enhanced._expr.bind(enhanced);\\n\\n\\treturn api;\\n};\\n\"],\n    // etc.\n]);\nvar actualRequire = require._extensions['.js'];\nrequire._extensions['.js'] = function (module, filename) {\n    if (files.has(filename)) return module._compile(filename);\n    return actualRequire.apply(this, arguments);\n};\nrequire('./lib/babel.js');\n```\nIf we uglified the files with \"whitespace only\" mode, it could totally* work!\n*Probobly\n. Ok, @sindresorhus: Here's the problem. Actual/expected are only included when no message is provided, see here and here. So we have to include that in the error message ourself. I'll submit a PR.\n. Ok, @sindresorhus: Here's the problem. Actual/expected are only included when no message is provided, see here and here. So we have to include that in the error message ourself. I'll submit a PR.\n. Fixed in #376.\n. Fixed in #376.\n. EDIT: Whoops... Not the issue I thought it was. Sorry!\n. @Hurtak: Yes. Babel 6 doesn't do anything by default.\n. @Hurtak: Yes. Babel 6 doesn't do anything by default.\n. @sindresorhus: Just my two cents, but that seems rather confusing. I wouldn't want to have to debug tests that magically fail on the CI but don't fail on my dev machine. At the very least, we need to log something about it.\n. @sindresorhus: Well, one thing to keep in mind is that Travis stops the test after 10 minutes without output, and caps it at 120 minutes period. AppVeyor has a maximum of 60 minutes. And I personally think it's out of scope for AVA, but it's up to you.\n. @sindresorhus: I agree. (I hope I didn't imply that there should be no timeouts period, I just meant on the CI). CIs defiantly do not just sit there for years waiting for infinite loops to finish. I'm happy to put together a simple little repo to prove that if you want.\n. @sindresorhus: For some context, the way I usually use describes in mocha is (for example), if I'm testing an object, I'll have one describe per method, and various tests within that. That's nice because you can easily see the hierarchy, you get code folding, and the output is formatted nicely.\n. @sindresorhus: Do you want it to be recursive by default, or just to display directories up to a common root?\n. @sindresorhus: Ok, I'll take care of it. Can you assign this to me (or give me perms to do that if you prefer)? \n. @jamestalmage: I really don't think it would be that big of a deal to have the polyfill... I can't think of any possible reason (if I knew that the tests ran in ES2015), that the polyfill would confuse me.\n. @jamestalmage: I really don't think it would be that big of a deal to have the polyfill... I can't think of any possible reason (if I knew that the tests ran in ES2015), that the polyfill would confuse me.\n. Hey, @sindresorhus, @vdemedes, @jamestalmage: Any idea when this might ship? I'd really prefer to not depend directly on master. Thanks!\n. Thanks @sindresorhus!\n. Yay! Thanks!\n. Two things:\n1. It's called istanbul-lib-hook, not istanbul-require-hook, FYI\n2. I think that it would be worthwhile to use pirates instead, as istanbul-lib-hook will (hopefully) soon just be a wrapper around pirates.\n. @jamestalmage: Fair. I've just posted an update in that issue, in the interest of now flooding this one.\n. @jamestalmage: I think that an AST transform is likely to do more harm than good, from a debugging perspective.\nI (personally) think that t.context is fine, especially if it's aliased it to t.ctx. It's not that long. But a note in the documentation that you can't use closures like you could in mocha would be good though, because it's not immediately obvious to newcomers. \n. @jamestalmage, @vdemedes, @sindresorhus: Here's an idea: what if we provided some way to pass arbitrary parameters to each test? Something like:\n``` javascript\n test.beforeEach('setup foo', t => {\n    const foo = makeFoo();\n    return [foo];\n});\n// I'm unsure about passing params from prior hooks through, but you'll\n// see why we need to below.\ntest.beforeEach((t, foo) => {\n    // You can still do whatever with t here, including t.context\n});\ntest.beforeEach(async (t, foo) => { // If a hook doesn't return something, then ignore it.\n    return [await connectToDatabase()];\n}); \ntest((t, foo, db) => {\n    // foo and db are a new for each test.\n});\n// If you think about it, we should probably pass these to afterEach hooks, because it's\n// not unlikely that you'll want to pass a database or something else that needs to be\n// cleaned up through.\ntest.afterEach(async (t, foo, db) => {\n    await db.close();\n});\n```\nThis accomplishes the majority of you get with mocha and closures, while still being totally async safe.\nWhile I think it's better to pass a bunch of parameters, this also opens up allowing people to pass context objects:\n``` javascript\ntest.beforeEach(t => {\n    return { foo: makeFoo() };\n});\n// This is why we need to pass params from prior hooks through\ntest.beforeEach('connect to db', async (t, ctx) => {\n    return { db: await connectToDatabase(), ...ctx };\n});\ntest(async (t, ctx) => {\n    t.is(await ctx.foo.getDbData(ctx.db), [/ Whatever /]);\n});\ntest.afterEach(async (t, { db }) => {\n    await db.close();\n});\n```\nThoughts?\n. @vdemedes: Take a look at my example, I added some more detail. Parameters from prior hooks get passed down to newer ones. If you want to have a context object, just do this:\n``` javascript\ntest.beforeEach('setup foo', t => {\n    return { foo: new Foo() };\n});\ntest.beforeEach('connect to db', async (t, ctx) => {\n    ctx.db = await connectToDB();\n});\ntest((t, ctx) => {\n});\n```\nThe more I think about it, the more I like this way. It's very DRY, looks nice, is convenient, and I can't think of a use case where it wouldn't work. For people who really want a context object, we could even include a helper function inside AVA:\njavascript\n// ava/index.js\nmodule.exports.ctx = function () {\n    runner.test.beforeEach(function(t) {\n        return t.context;\n    });\n};\n. @vdemedes: Take a look at my example, I added some more detail. Parameters from prior hooks get passed down to newer ones. If you want to have a context object, just do this:\n``` javascript\ntest.beforeEach('setup foo', t => {\n    return { foo: new Foo() };\n});\ntest.beforeEach('connect to db', async (t, ctx) => {\n    ctx.db = await connectToDB();\n});\ntest((t, ctx) => {\n});\n```\nThe more I think about it, the more I like this way. It's very DRY, looks nice, is convenient, and I can't think of a use case where it wouldn't work. For people who really want a context object, we could even include a helper function inside AVA:\njavascript\n// ava/index.js\nmodule.exports.ctx = function () {\n    runner.test.beforeEach(function(t) {\n        return t.context;\n    });\n};\n. @vdemedes: No, I don't think you understand. In my idea it simulates waterfall context, but each thing is shared only between each beforeEach, test, and afterEach.\nExample:\n``` javascript\nfunction Foo(arg) {\n    this.arg = arg;\n}\nfunction Bar(arg) {\n    this.arg = arg * 2;\n}\ntest.beforeEach('make foo', t => {\n    return [new Foo('this is a foo')];\n});\ntest.beforeEach('make bar', t => {\n    return [new Bar('this is a bar')];\n});\ntest('test 1', (t, foo, bar) => {\n    foo.bar = bar;\n});\ntest('test 2', (t, foo, bar) => {\n    console.log(foo.bar) // undefined\n});\ntest.afterEach((t, foo, bar) => {\n    console.log(foo.bar === bar) // Logs 'true' the first time, 'false', the second.\n});\n```\n. @vdemedes: No, I don't think you understand. In my idea it simulates waterfall context, but each thing is shared only between each beforeEach, test, and afterEach.\nExample:\n``` javascript\nfunction Foo(arg) {\n    this.arg = arg;\n}\nfunction Bar(arg) {\n    this.arg = arg * 2;\n}\ntest.beforeEach('make foo', t => {\n    return [new Foo('this is a foo')];\n});\ntest.beforeEach('make bar', t => {\n    return [new Bar('this is a bar')];\n});\ntest('test 1', (t, foo, bar) => {\n    foo.bar = bar;\n});\ntest('test 2', (t, foo, bar) => {\n    console.log(foo.bar) // undefined\n});\ntest.afterEach((t, foo, bar) => {\n    console.log(foo.bar === bar) // Logs 'true' the first time, 'false', the second.\n});\n``\n. @vdemedes: sorry (I don't mean this argumentatively), but what was your opinion? The way I read your last comment it seemed like you didn't want non-async safe context, but this isn't that.\n. @vdemedes: Yeah, @jamestalmage explained why it wouldn't work. (Thanks, btw). I think the best thing to do is get rid oft.context, and passctxas a second param, and use destructuring. \n. @vdemedes: because that way @sindresorhus won't get angry at us for having two ways to do something when we pass context as a second parameter. :stuck_out_tongue_closed_eyes: \n. @vdemedes: but I think it's worth considering what @jamestalmage was saying about being DRY. If you have context as a second parameter, then you can use destructuring to be rather dry. (Sorry, no example, on mobile)\n. @sindresorhus: If you don't mind, I'm going to give this a go. I've been discussing this with @jamestalmage, and here's what I think I need to do (I've started on this a little):\n- [x] Replace test._assertCount with test._assertPromises. Every assertion pushes a Promise (that can just be Promise.resolve/.reject if needed) onto that array, then test.exit() compares the length of the array with the planned number of assertions.\n- [ ] Add some tests. So far, I think I need to test (credit for most of this goes to @jamestalmage):\n  - [x] That you simply don't need to return the promise from t.throws\n  - [x]test.cb- [x]t.endis called with a non-yet rejected promise passed tot.throws- [x]t.endis called with a non-yet resolved promise passed tot.throws- [x] Test returns a promise that resolves before at.throwspromise.\n  - [x] Test returns a promise that rejects before at.throwspromise.\n  - [x] Multiplet.throws, with several resolving and rejecting promises.\n  - [ ] All the ways to mess upt.plan- [x] The proper number of assertions are there when the test is ended, but another one is preformed while doingPromise.all(test._assertPromises).\n      - I think this can be solved by simply checking the number of assertions after all the promises have settled.\n    - [x] The wrong number of assertions are there when the test is ended, but another one is added while doingPromise.all(test._assertPromises)`.\n    - [ ] ???\n  - [ ] ???\nIf anyone has any other ideas for things to test, or other feedback, that would be awesome!\n. Ok, This is fixed in #360.\n. @vdemedes, @jamestalmage: This would be really easy to do once #360 lands. So once #360 lands, I can take this on.\n. @jamestalmage, @vdemedes, @sindresourhus: ok, now that #360 is merged, should I start working on this?\n. Sure. :)\n. @jamestalmage: \n. @jamestalmage: Well, you have two options:\n1. It's not really that big a deal.\n2. I can add our own implementation of a deferred, which is ~15 lines of code.\nPersonally, #1 seems way more reasonable to me, but in the end it's up to you.\n. I'll look into this.\n. @jamestalmage: I know.\nAfter some debugging, I found out that somehow the child process bypasses the parents stderr, as I've tried hook-std, several other npm packages, and several implementations that I've done myself, and none have worked.\n. @jamestalmage: Tried that. Didn't work. Tried just manually passing in 'ignore'. Didn't work. My theory is that fs overrides the options passed to fork, in an evil fashion.\n. @jamestalmage: Tried that. Didn't work. Tried just manually passing in 'ignore'. Didn't work. My theory is that fs overrides the options passed to fork, in an evil fashion.\n. Closed by #346.\n. Ok, @vdemedes, @jamestalmage, @sindresorhus: This should be ready for real now. Sorry about that.\n. Ok, @vdemedes, @jamestalmage, @sindresorhus: This should be ready for real now. Sorry about that.\n. @jamestalmage, @sindresorhus: Now this is doing what was happening before, require is throwing an error when you use \\\\ in require. \n. @sindresorhus: Thanks! One more question if you don't mind: I rather like AVA, it's a lot more lightweight than mocha. But in looking at the issue tracker, and hacking the source, there are some things that I would like to see implemented (one small thing is renaming babel.js, it's rather misleading). If I want to start submitting some PRs, is there anything I should know to make them easy to merge, and is there a volume I should stay below to avoid overloading you?\n. @sindresorhus: Check out hax/babel-features. I'll turn it into a preset if you want. Or we could just use it directly.\n. @sindresorhus: Actually, I went ahead and turned that into a preset anyway: babel-preset-features. It's a drop-in replacement for babel-preset-e2015, I'll submit a PR.\n. @jamestalmage: Should be fixed now.\n. @jamestalmage: Hang on a second actually, delay doesn't like node@<4.0.0\n. @sindresorhus: Yeah, I was trying to avoid that, but it's not that big a deal.\n. Ok, @jamestalmage, @sindresorhus: Everything should be fixed now.\n. Ok, I just rebased onto master.\n. @sindresorhus: Ok. Fixed, and rebased onto master. Ready for merging.\n. I literally started working on this 30 seconds ago! Great minds think alike!\n. I also disabled the spinner and progress bar on npm to make the output look a lot nicer. I can enable the http logging so you can still see what's going on, if you like.\n. @jamestalmage: Huh, AppVeyor must have changed it. Never mind.\nAlthough, I think we should cache node_modules on AppVeyor, because npm on AV is a lot slower, and any dependency mess ups will be caught by travis.\n. @jamestalmage: No. It should be faster between this branch and master.\n. Hmm... I guess never mind then. (Although that's awful strange).\n. Yeah, Probably. I've just always heard that they're slow.\n. @sindresorhus, @jamestalmage, @vdemedes: How would you want this implemented? Should everything just get wrapped in an error? A promise? A custom object? \n. Grr... A dependency has a require with an uppercase letter, which is killing travis. I submitted a PR, but the author is on the other side of the world. Give it a few hours, then it should be ready.\n. @sindresorhus: Check the 2.x (currently in beta) branch, it was last worked on 5 days ago :wink:. It even already supports Babel 6.\nI think there's a little overhead, but not a whole lot. I'm not sure how we'd go about it any other way, regardless. And I guess we'd just assume that V8 isn't going to ship something horrifically broken. The overhead is going to down once we move babel to the main thread.\n. @dfcreative, can this be closed?\n. @sindresorhus: Why not just do delete error.stack? \n. @sindresorhus: Why not just do delete error.stack? \n. @sindresorhus: That appears to be a bug on your machine. On mine it works fine:\n\nls spits out the size kind of weird (in raw bytes, floored), it's actually 33.2MB:\n\nnpm ls:\n/Users/Ari/Developer/.tmp\n\u2514\u2500\u252c ava@0.8.0\n  \u251c\u2500\u2500 arr-flatten@1.0.1\n  \u251c\u2500\u2500 arrify@1.0.1\n  \u251c\u2500\u252c ava-init@0.1.3\n  \u2502 \u251c\u2500\u2500 arr-exclude@1.0.0\n  \u2502 \u251c\u2500\u2500 pify@2.3.0\n  \u2502 \u251c\u2500\u252c pinkie-promise@2.0.0\n  \u2502 \u2502 \u2514\u2500\u2500 pinkie@2.0.1\n  \u2502 \u251c\u2500\u252c read-pkg-up@1.0.1\n  \u2502 \u2502 \u251c\u2500\u252c find-up@1.1.0\n  \u2502 \u2502 \u2502 \u2514\u2500\u2500 path-exists@2.1.0\n  \u2502 \u2502 \u2514\u2500\u252c read-pkg@1.1.0\n  \u2502 \u2502   \u251c\u2500\u252c load-json-file@1.1.0\n  \u2502 \u2502   \u2502 \u251c\u2500\u252c parse-json@2.2.0\n  \u2502 \u2502   \u2502 \u2502 \u2514\u2500\u252c error-ex@1.3.0\n  \u2502 \u2502   \u2502 \u2502   \u2514\u2500\u2500 is-arrayish@0.2.1\n  \u2502 \u2502   \u2502 \u2514\u2500\u252c strip-bom@2.0.0\n  \u2502 \u2502   \u2502   \u2514\u2500\u2500 is-utf8@0.2.1\n  \u2502 \u2502   \u2514\u2500\u2500 path-type@1.1.0\n  \u2502 \u251c\u2500\u2500 the-argv@1.0.0\n  \u2502 \u2514\u2500\u252c write-pkg@1.0.0\n  \u2502   \u2514\u2500\u252c write-json-file@1.2.0\n  \u2502     \u2514\u2500\u252c sort-keys@1.1.1\n  \u2502       \u2514\u2500\u2500 is-plain-obj@1.1.0\n  \u251c\u2500\u252c babel-core@5.8.34\n  \u2502 \u251c\u2500\u2500 babel-plugin-constant-folding@1.0.1\n  \u2502 \u251c\u2500\u2500 babel-plugin-dead-code-elimination@1.0.2\n  \u2502 \u251c\u2500\u2500 babel-plugin-eval@1.0.1\n  \u2502 \u251c\u2500\u2500 babel-plugin-inline-environment-variables@1.0.1\n  \u2502 \u251c\u2500\u2500 babel-plugin-jscript@1.0.4\n  \u2502 \u251c\u2500\u2500 babel-plugin-member-expression-literals@1.0.1\n  \u2502 \u251c\u2500\u2500 babel-plugin-property-literals@1.0.1\n  \u2502 \u251c\u2500\u2500 babel-plugin-proto-to-assign@1.0.4\n  \u2502 \u251c\u2500\u2500 babel-plugin-react-constant-elements@1.0.3\n  \u2502 \u251c\u2500\u2500 babel-plugin-react-display-name@1.0.3\n  \u2502 \u251c\u2500\u2500 babel-plugin-remove-console@1.0.1\n  \u2502 \u251c\u2500\u2500 babel-plugin-remove-debugger@1.0.1\n  \u2502 \u251c\u2500\u2500 babel-plugin-runtime@1.0.7\n  \u2502 \u251c\u2500\u252c babel-plugin-undeclared-variables-check@1.0.2\n  \u2502 \u2502 \u2514\u2500\u2500 leven@1.0.2\n  \u2502 \u251c\u2500\u2500 babel-plugin-undefined-to-void@1.1.6\n  \u2502 \u251c\u2500\u2500 babylon@5.8.34\n  \u2502 \u251c\u2500\u2500 bluebird@2.10.2\n  \u2502 \u251c\u2500\u2500 convert-source-map@1.1.2\n  \u2502 \u251c\u2500\u2500 core-js@1.2.6\n  \u2502 \u251c\u2500\u252c detect-indent@3.0.1\n  \u2502 \u2502 \u251c\u2500\u2500 get-stdin@4.0.1\n  \u2502 \u2502 \u2514\u2500\u2500 minimist@1.2.0\n  \u2502 \u251c\u2500\u2500 esutils@2.0.2\n  \u2502 \u251c\u2500\u2500 fs-readdir-recursive@0.1.2\n  \u2502 \u251c\u2500\u2500 globals@6.4.1\n  \u2502 \u251c\u2500\u252c home-or-tmp@1.0.0\n  \u2502 \u2502 \u251c\u2500\u2500 os-tmpdir@1.0.1\n  \u2502 \u2502 \u2514\u2500\u2500 user-home@1.1.1\n  \u2502 \u251c\u2500\u2500 is-integer@1.0.6\n  \u2502 \u251c\u2500\u2500 js-tokens@1.0.1\n  \u2502 \u251c\u2500\u2500 json5@0.4.0\n  \u2502 \u251c\u2500\u252c line-numbers@0.2.0\n  \u2502 \u2502 \u2514\u2500\u2500 left-pad@0.0.3\n  \u2502 \u251c\u2500\u2500 lodash@3.10.1\n  \u2502 \u251c\u2500\u252c minimatch@2.0.10\n  \u2502 \u2502 \u2514\u2500\u252c brace-expansion@1.1.2\n  \u2502 \u2502   \u251c\u2500\u2500 balanced-match@0.3.0\n  \u2502 \u2502   \u2514\u2500\u2500 concat-map@0.0.1\n  \u2502 \u251c\u2500\u252c output-file-sync@1.1.1\n  \u2502 \u2502 \u2514\u2500\u252c mkdirp@0.5.1\n  \u2502 \u2502   \u2514\u2500\u2500 minimist@0.0.8\n  \u2502 \u251c\u2500\u2500 path-exists@1.0.0\n  \u2502 \u251c\u2500\u2500 path-is-absolute@1.0.0\n  \u2502 \u251c\u2500\u2500 private@0.1.6\n  \u2502 \u251c\u2500\u252c regenerator@0.8.40\n  \u2502 \u2502 \u251c\u2500\u252c commoner@0.10.4\n  \u2502 \u2502 \u2502 \u251c\u2500\u252c commander@2.9.0\n  \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 graceful-readlink@1.0.1\n  \u2502 \u2502 \u2502 \u251c\u2500\u252c detective@4.3.1\n  \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 acorn@1.2.2\n  \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 defined@1.0.0\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 glob@5.0.15\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 iconv-lite@0.4.13\n  \u2502 \u2502 \u2502 \u2514\u2500\u2500 q@1.4.1\n  \u2502 \u2502 \u251c\u2500\u252c defs@1.1.1\n  \u2502 \u2502 \u2502 \u251c\u2500\u252c alter@0.2.0\n  \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 stable@0.1.5\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 ast-traverse@0.1.1\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 breakable@1.0.0\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 simple-fmt@0.1.0\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 simple-is@0.2.0\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 stringmap@0.2.2\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 stringset@0.2.1\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 tryor@0.1.2\n  \u2502 \u2502 \u2502 \u2514\u2500\u252c yargs@3.27.0\n  \u2502 \u2502 \u2502   \u251c\u2500\u2500 camelcase@1.2.1\n  \u2502 \u2502 \u2502   \u251c\u2500\u252c cliui@2.1.0\n  \u2502 \u2502 \u2502   \u2502 \u251c\u2500\u252c center-align@0.1.2\n  \u2502 \u2502 \u2502   \u2502 \u2502 \u251c\u2500\u252c align-text@0.1.3\n  \u2502 \u2502 \u2502   \u2502 \u2502 \u2502 \u251c\u2500\u252c kind-of@2.0.1\n  \u2502 \u2502 \u2502   \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 is-buffer@1.1.0\n  \u2502 \u2502 \u2502   \u2502 \u2502 \u2502 \u2514\u2500\u2500 repeat-string@1.5.2\n  \u2502 \u2502 \u2502   \u2502 \u2502 \u2514\u2500\u2500 lazy-cache@0.2.7\n  \u2502 \u2502 \u2502   \u2502 \u2514\u2500\u2500 right-align@0.1.3\n  \u2502 \u2502 \u2502   \u251c\u2500\u2500 decamelize@1.1.2\n  \u2502 \u2502 \u2502   \u251c\u2500\u252c os-locale@1.4.0\n  \u2502 \u2502 \u2502   \u2502 \u2514\u2500\u252c lcid@1.0.0\n  \u2502 \u2502 \u2502   \u2502   \u2514\u2500\u2500 invert-kv@1.0.0\n  \u2502 \u2502 \u2502   \u251c\u2500\u2500 window-size@0.1.4\n  \u2502 \u2502 \u2502   \u2514\u2500\u2500 y18n@3.2.0\n  \u2502 \u2502 \u251c\u2500\u2500 esprima-fb@15001.1001.0-dev-harmony-fb\n  \u2502 \u2502 \u251c\u2500\u252c recast@0.10.33\n  \u2502 \u2502 \u2502 \u2514\u2500\u2500 ast-types@0.8.12\n  \u2502 \u2502 \u2514\u2500\u2500 through@2.3.8\n  \u2502 \u251c\u2500\u252c regexpu@1.3.0\n  \u2502 \u2502 \u251c\u2500\u2500 esprima@2.7.1\n  \u2502 \u2502 \u251c\u2500\u2500 regenerate@1.2.1\n  \u2502 \u2502 \u251c\u2500\u2500 regjsgen@0.2.0\n  \u2502 \u2502 \u2514\u2500\u252c regjsparser@0.1.5\n  \u2502 \u2502   \u2514\u2500\u2500 jsesc@0.5.0\n  \u2502 \u251c\u2500\u2500 repeating@1.1.3\n  \u2502 \u251c\u2500\u2500 resolve@1.1.6\n  \u2502 \u251c\u2500\u2500 shebang-regex@1.0.0\n  \u2502 \u251c\u2500\u2500 slash@1.0.0\n  \u2502 \u251c\u2500\u2500 source-map@0.5.3\n  \u2502 \u251c\u2500\u252c source-map-support@0.2.10\n  \u2502 \u2502 \u2514\u2500\u2500 source-map@0.1.32\n  \u2502 \u251c\u2500\u2500 to-fast-properties@1.0.1\n  \u2502 \u251c\u2500\u2500 trim-right@1.0.1\n  \u2502 \u2514\u2500\u2500 try-resolve@1.0.1\n  \u251c\u2500\u252c babel-plugin-espower@1.1.0\n  \u2502 \u251c\u2500\u2500 array-find@1.0.0\n  \u2502 \u251c\u2500\u252c escallmatch@1.4.2\n  \u2502 \u2502 \u251c\u2500\u2500 deep-equal@1.0.1\n  \u2502 \u2502 \u251c\u2500\u2500 esprima@2.7.1\n  \u2502 \u2502 \u251c\u2500\u252c espurify@1.5.0\n  \u2502 \u2502 \u2502 \u2514\u2500\u2500 isarray@1.0.0\n  \u2502 \u2502 \u2514\u2500\u2500 indexof@0.0.1\n  \u2502 \u251c\u2500\u252c espower@1.2.1\n  \u2502 \u2502 \u251c\u2500\u252c escodegen@1.7.1\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 esprima@1.2.5\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 estraverse@1.9.3\n  \u2502 \u2502 \u2502 \u251c\u2500\u252c optionator@0.5.0\n  \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 deep-is@0.1.3\n  \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 fast-levenshtein@1.0.7\n  \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 levn@0.2.5\n  \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 prelude-ls@1.1.2\n  \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 type-check@0.3.1\n  \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 wordwrap@0.0.2\n  \u2502 \u2502 \u2502 \u2514\u2500\u2500 source-map@0.2.0\n  \u2502 \u2502 \u251c\u2500\u2500 is-url@1.2.1\n  \u2502 \u2502 \u2514\u2500\u2500 isarray@0.0.1\n  \u2502 \u2514\u2500\u2500 xtend@4.0.1\n  \u251c\u2500\u2500 babel-runtime@5.8.34\n  \u251c\u2500\u2500 bluebird@3.1.1\n  \u251c\u2500\u252c chalk@1.1.1\n  \u2502 \u251c\u2500\u2500 ansi-styles@2.1.0\n  \u2502 \u251c\u2500\u2500 escape-string-regexp@1.0.4\n  \u2502 \u251c\u2500\u252c has-ansi@2.0.0\n  \u2502 \u2502 \u2514\u2500\u2500 ansi-regex@2.0.0\n  \u2502 \u251c\u2500\u2500 strip-ansi@3.0.0\n  \u2502 \u2514\u2500\u2500 supports-color@2.0.0\n  \u251c\u2500\u252c co-with-promise@4.6.0\n  \u2502 \u2514\u2500\u252c pinkie-promise@1.0.0\n  \u2502   \u2514\u2500\u2500 pinkie@1.0.0\n  \u251c\u2500\u252c core-assert@0.1.3\n  \u2502 \u2514\u2500\u2500 buf-compare@1.0.0\n  \u251c\u2500\u252c debug@2.2.0\n  \u2502 \u2514\u2500\u2500 ms@0.7.1\n  \u251c\u2500\u2500 deeper@2.1.0\n  \u251c\u2500\u252c empower@1.1.0\n  \u2502 \u251c\u2500\u2500 array-filter@1.0.0\n  \u2502 \u251c\u2500\u2500 array-foreach@1.0.1\n  \u2502 \u251c\u2500\u2500 array-map@0.0.0\n  \u2502 \u251c\u2500\u2500 array-some@1.0.0\n  \u2502 \u251c\u2500\u252c define-properties@1.1.2\n  \u2502 \u2502 \u2514\u2500\u2500 foreach@2.0.5\n  \u2502 \u2514\u2500\u252c object-create@0.1.0\n  \u2502   \u2514\u2500\u252c object-define-property@0.1.0\n  \u2502     \u251c\u2500\u2500 function-bind@0.1.0\n  \u2502     \u2514\u2500\u2500 has@0.0.1\n  \u251c\u2500\u252c empower-core@0.2.0\n  \u2502 \u2514\u2500\u2500 call-signature@0.0.2\n  \u251c\u2500\u2500 figures@1.4.0\n  \u251c\u2500\u2500 fn-name@2.0.1\n  \u251c\u2500\u252c globby@4.0.0\n  \u2502 \u251c\u2500\u252c array-union@1.0.1\n  \u2502 \u2502 \u2514\u2500\u2500 array-uniq@1.0.2\n  \u2502 \u2514\u2500\u252c glob@6.0.2\n  \u2502   \u251c\u2500\u252c inflight@1.0.4\n  \u2502   \u2502 \u2514\u2500\u2500 wrappy@1.0.1\n  \u2502   \u251c\u2500\u2500 inherits@2.0.1\n  \u2502   \u2514\u2500\u2500 once@1.3.3\n  \u251c\u2500\u2500 has-generator@1.0.0\n  \u251c\u2500\u2500 is-generator-fn@1.0.0\n  \u251c\u2500\u252c is-observable@0.1.0\n  \u2502 \u2514\u2500\u2500 symbol-observable@0.1.0\n  \u251c\u2500\u2500 is-promise@2.1.0\n  \u251c\u2500\u252c loud-rejection@1.2.0\n  \u2502 \u2514\u2500\u2500 signal-exit@2.1.2\n  \u251c\u2500\u2500 max-timeout@1.0.0\n  \u251c\u2500\u252c meow@3.6.0\n  \u2502 \u251c\u2500\u252c camelcase-keys@2.0.0\n  \u2502 \u2502 \u251c\u2500\u2500 camelcase@2.0.1\n  \u2502 \u2502 \u2514\u2500\u2500 map-obj@1.0.1\n  \u2502 \u251c\u2500\u2500 minimist@1.2.0\n  \u2502 \u251c\u2500\u252c normalize-package-data@2.3.5\n  \u2502 \u2502 \u251c\u2500\u2500 hosted-git-info@2.1.4\n  \u2502 \u2502 \u251c\u2500\u252c is-builtin-module@1.0.0\n  \u2502 \u2502 \u2502 \u2514\u2500\u2500 builtin-modules@1.1.0\n  \u2502 \u2502 \u251c\u2500\u2500 semver@5.1.0\n  \u2502 \u2502 \u2514\u2500\u252c validate-npm-package-license@3.0.1\n  \u2502 \u2502   \u251c\u2500\u252c spdx-correct@1.0.2\n  \u2502 \u2502   \u2502 \u2514\u2500\u2500 spdx-license-ids@1.1.0\n  \u2502 \u2502   \u2514\u2500\u252c spdx-expression-parse@1.0.2\n  \u2502 \u2502     \u2514\u2500\u2500 spdx-exceptions@1.0.4\n  \u2502 \u251c\u2500\u252c redent@1.0.0\n  \u2502 \u2502 \u251c\u2500\u252c indent-string@2.1.0\n  \u2502 \u2502 \u2502 \u2514\u2500\u2500 repeating@2.0.0\n  \u2502 \u2502 \u2514\u2500\u2500 strip-indent@1.0.1\n  \u2502 \u2514\u2500\u2500 trim-newlines@1.0.0\n  \u251c\u2500\u2500 object-assign@4.0.1\n  \u251c\u2500\u2500 observable-to-promise@0.1.0\n  \u251c\u2500\u252c plur@2.1.2\n  \u2502 \u2514\u2500\u2500 irregular-plurals@1.1.0\n  \u251c\u2500\u252c power-assert-formatter@1.3.2\n  \u2502 \u251c\u2500\u2500 acorn@2.6.4\n  \u2502 \u251c\u2500\u2500 acorn-es7-plugin@1.0.11\n  \u2502 \u251c\u2500\u2500 array-reduce@0.0.0\n  \u2502 \u251c\u2500\u2500 eastasianwidth@0.1.1\n  \u2502 \u251c\u2500\u2500 estraverse@4.1.1\n  \u2502 \u251c\u2500\u2500 googlediff@0.1.0\n  \u2502 \u251c\u2500\u2500 object-keys@1.0.9\n  \u2502 \u251c\u2500\u252c stringifier@1.2.1\n  \u2502 \u2502 \u251c\u2500\u2500 array-reduce-right@1.0.0\n  \u2502 \u2502 \u2514\u2500\u2500 traverse@0.6.6\n  \u2502 \u2514\u2500\u2500 type-name@1.1.0\n  \u251c\u2500\u2500 power-assert-renderers@0.1.0\n  \u251c\u2500\u252c pretty-ms@2.1.0\n  \u2502 \u251c\u2500\u252c is-finite@1.0.1\n  \u2502 \u2502 \u2514\u2500\u2500 number-is-nan@1.0.0\n  \u2502 \u251c\u2500\u2500 parse-ms@1.0.0\n  \u2502 \u2514\u2500\u2500 plur@1.0.0\n  \u251c\u2500\u2500 require-from-string@1.1.0\n  \u251c\u2500\u252c resolve-cwd@1.0.0\n  \u2502 \u2514\u2500\u2500 resolve-from@2.0.0\n  \u251c\u2500\u2500 serialize-error@1.1.0\n  \u251c\u2500\u2500 set-immediate-shim@1.0.1\n  \u251c\u2500\u252c source-map-support@0.4.0\n  \u2502 \u2514\u2500\u252c source-map@0.1.32\n  \u2502   \u2514\u2500\u2500 amdefine@1.0.0\n  \u251c\u2500\u252c squeak@1.3.0\n  \u2502 \u251c\u2500\u2500 console-stream@0.1.1\n  \u2502 \u2514\u2500\u252c lpad-align@1.1.0\n  \u2502   \u251c\u2500\u2500 longest@1.0.1\n  \u2502   \u2514\u2500\u2500 lpad@2.0.1\n  \u251c\u2500\u252c time-require@0.1.2\n  \u2502 \u251c\u2500\u252c chalk@0.4.0\n  \u2502 \u2502 \u251c\u2500\u2500 ansi-styles@1.0.0\n  \u2502 \u2502 \u251c\u2500\u2500 has-color@0.1.7\n  \u2502 \u2502 \u2514\u2500\u2500 strip-ansi@0.1.1\n  \u2502 \u251c\u2500\u2500 date-time@0.1.1\n  \u2502 \u251c\u2500\u252c pretty-ms@0.2.2\n  \u2502 \u2502 \u2514\u2500\u2500 parse-ms@0.1.2\n  \u2502 \u2514\u2500\u2500 text-table@0.2.0\n  \u2514\u2500\u252c update-notifier@0.5.0\n    \u251c\u2500\u252c configstore@1.4.0\n    \u2502 \u251c\u2500\u2500 graceful-fs@4.1.2\n    \u2502 \u251c\u2500\u252c osenv@0.1.3\n    \u2502 \u2502 \u2514\u2500\u2500 os-homedir@1.0.1\n    \u2502 \u251c\u2500\u2500 uuid@2.0.1\n    \u2502 \u251c\u2500\u252c write-file-atomic@1.1.4\n    \u2502 \u2502 \u251c\u2500\u2500 imurmurhash@0.1.4\n    \u2502 \u2502 \u2514\u2500\u2500 slide@1.1.6\n    \u2502 \u2514\u2500\u2500 xdg-basedir@2.0.0\n    \u251c\u2500\u2500 is-npm@1.0.0\n    \u251c\u2500\u252c latest-version@1.0.1\n    \u2502 \u2514\u2500\u252c package-json@1.2.0\n    \u2502   \u251c\u2500\u252c got@3.3.1\n    \u2502   \u2502 \u251c\u2500\u252c duplexify@3.4.2\n    \u2502   \u2502 \u2502 \u251c\u2500\u2500 end-of-stream@1.0.0\n    \u2502   \u2502 \u2502 \u2514\u2500\u252c readable-stream@2.0.5\n    \u2502   \u2502 \u2502   \u251c\u2500\u2500 core-util-is@1.0.2\n    \u2502   \u2502 \u2502   \u251c\u2500\u2500 isarray@0.0.1\n    \u2502   \u2502 \u2502   \u251c\u2500\u2500 process-nextick-args@1.0.6\n    \u2502   \u2502 \u2502   \u251c\u2500\u2500 string_decoder@0.10.31\n    \u2502   \u2502 \u2502   \u2514\u2500\u2500 util-deprecate@1.0.2\n    \u2502   \u2502 \u251c\u2500\u2500 infinity-agent@2.0.3\n    \u2502   \u2502 \u251c\u2500\u2500 is-redirect@1.0.0\n    \u2502   \u2502 \u251c\u2500\u2500 is-stream@1.0.1\n    \u2502   \u2502 \u251c\u2500\u2500 lowercase-keys@1.0.0\n    \u2502   \u2502 \u251c\u2500\u2500 nested-error-stacks@1.0.2\n    \u2502   \u2502 \u251c\u2500\u2500 object-assign@3.0.0\n    \u2502   \u2502 \u251c\u2500\u2500 prepend-http@1.0.3\n    \u2502   \u2502 \u251c\u2500\u252c read-all-stream@3.0.1\n    \u2502   \u2502 \u2502 \u2514\u2500\u252c pinkie-promise@1.0.0\n    \u2502   \u2502 \u2502   \u2514\u2500\u2500 pinkie@1.0.0\n    \u2502   \u2502 \u2514\u2500\u2500 timed-out@2.0.0\n    \u2502   \u2514\u2500\u252c registry-url@3.0.3\n    \u2502     \u2514\u2500\u252c rc@1.1.5\n    \u2502       \u251c\u2500\u2500 deep-extend@0.4.0\n    \u2502       \u251c\u2500\u2500 ini@1.3.4\n    \u2502       \u251c\u2500\u2500 minimist@1.2.0\n    \u2502       \u2514\u2500\u2500 strip-json-comments@1.0.4\n    \u251c\u2500\u2500 semver-diff@2.1.0\n    \u2514\u2500\u2500 string-length@1.0.1\nSo I'm really not sure why you're seeing that. Maybe file a bug report with npm?\n. @sindresorhus: That appears to be a bug on your machine. On mine it works fine:\n\nls spits out the size kind of weird (in raw bytes, floored), it's actually 33.2MB:\n\nnpm ls:\n/Users/Ari/Developer/.tmp\n\u2514\u2500\u252c ava@0.8.0\n  \u251c\u2500\u2500 arr-flatten@1.0.1\n  \u251c\u2500\u2500 arrify@1.0.1\n  \u251c\u2500\u252c ava-init@0.1.3\n  \u2502 \u251c\u2500\u2500 arr-exclude@1.0.0\n  \u2502 \u251c\u2500\u2500 pify@2.3.0\n  \u2502 \u251c\u2500\u252c pinkie-promise@2.0.0\n  \u2502 \u2502 \u2514\u2500\u2500 pinkie@2.0.1\n  \u2502 \u251c\u2500\u252c read-pkg-up@1.0.1\n  \u2502 \u2502 \u251c\u2500\u252c find-up@1.1.0\n  \u2502 \u2502 \u2502 \u2514\u2500\u2500 path-exists@2.1.0\n  \u2502 \u2502 \u2514\u2500\u252c read-pkg@1.1.0\n  \u2502 \u2502   \u251c\u2500\u252c load-json-file@1.1.0\n  \u2502 \u2502   \u2502 \u251c\u2500\u252c parse-json@2.2.0\n  \u2502 \u2502   \u2502 \u2502 \u2514\u2500\u252c error-ex@1.3.0\n  \u2502 \u2502   \u2502 \u2502   \u2514\u2500\u2500 is-arrayish@0.2.1\n  \u2502 \u2502   \u2502 \u2514\u2500\u252c strip-bom@2.0.0\n  \u2502 \u2502   \u2502   \u2514\u2500\u2500 is-utf8@0.2.1\n  \u2502 \u2502   \u2514\u2500\u2500 path-type@1.1.0\n  \u2502 \u251c\u2500\u2500 the-argv@1.0.0\n  \u2502 \u2514\u2500\u252c write-pkg@1.0.0\n  \u2502   \u2514\u2500\u252c write-json-file@1.2.0\n  \u2502     \u2514\u2500\u252c sort-keys@1.1.1\n  \u2502       \u2514\u2500\u2500 is-plain-obj@1.1.0\n  \u251c\u2500\u252c babel-core@5.8.34\n  \u2502 \u251c\u2500\u2500 babel-plugin-constant-folding@1.0.1\n  \u2502 \u251c\u2500\u2500 babel-plugin-dead-code-elimination@1.0.2\n  \u2502 \u251c\u2500\u2500 babel-plugin-eval@1.0.1\n  \u2502 \u251c\u2500\u2500 babel-plugin-inline-environment-variables@1.0.1\n  \u2502 \u251c\u2500\u2500 babel-plugin-jscript@1.0.4\n  \u2502 \u251c\u2500\u2500 babel-plugin-member-expression-literals@1.0.1\n  \u2502 \u251c\u2500\u2500 babel-plugin-property-literals@1.0.1\n  \u2502 \u251c\u2500\u2500 babel-plugin-proto-to-assign@1.0.4\n  \u2502 \u251c\u2500\u2500 babel-plugin-react-constant-elements@1.0.3\n  \u2502 \u251c\u2500\u2500 babel-plugin-react-display-name@1.0.3\n  \u2502 \u251c\u2500\u2500 babel-plugin-remove-console@1.0.1\n  \u2502 \u251c\u2500\u2500 babel-plugin-remove-debugger@1.0.1\n  \u2502 \u251c\u2500\u2500 babel-plugin-runtime@1.0.7\n  \u2502 \u251c\u2500\u252c babel-plugin-undeclared-variables-check@1.0.2\n  \u2502 \u2502 \u2514\u2500\u2500 leven@1.0.2\n  \u2502 \u251c\u2500\u2500 babel-plugin-undefined-to-void@1.1.6\n  \u2502 \u251c\u2500\u2500 babylon@5.8.34\n  \u2502 \u251c\u2500\u2500 bluebird@2.10.2\n  \u2502 \u251c\u2500\u2500 convert-source-map@1.1.2\n  \u2502 \u251c\u2500\u2500 core-js@1.2.6\n  \u2502 \u251c\u2500\u252c detect-indent@3.0.1\n  \u2502 \u2502 \u251c\u2500\u2500 get-stdin@4.0.1\n  \u2502 \u2502 \u2514\u2500\u2500 minimist@1.2.0\n  \u2502 \u251c\u2500\u2500 esutils@2.0.2\n  \u2502 \u251c\u2500\u2500 fs-readdir-recursive@0.1.2\n  \u2502 \u251c\u2500\u2500 globals@6.4.1\n  \u2502 \u251c\u2500\u252c home-or-tmp@1.0.0\n  \u2502 \u2502 \u251c\u2500\u2500 os-tmpdir@1.0.1\n  \u2502 \u2502 \u2514\u2500\u2500 user-home@1.1.1\n  \u2502 \u251c\u2500\u2500 is-integer@1.0.6\n  \u2502 \u251c\u2500\u2500 js-tokens@1.0.1\n  \u2502 \u251c\u2500\u2500 json5@0.4.0\n  \u2502 \u251c\u2500\u252c line-numbers@0.2.0\n  \u2502 \u2502 \u2514\u2500\u2500 left-pad@0.0.3\n  \u2502 \u251c\u2500\u2500 lodash@3.10.1\n  \u2502 \u251c\u2500\u252c minimatch@2.0.10\n  \u2502 \u2502 \u2514\u2500\u252c brace-expansion@1.1.2\n  \u2502 \u2502   \u251c\u2500\u2500 balanced-match@0.3.0\n  \u2502 \u2502   \u2514\u2500\u2500 concat-map@0.0.1\n  \u2502 \u251c\u2500\u252c output-file-sync@1.1.1\n  \u2502 \u2502 \u2514\u2500\u252c mkdirp@0.5.1\n  \u2502 \u2502   \u2514\u2500\u2500 minimist@0.0.8\n  \u2502 \u251c\u2500\u2500 path-exists@1.0.0\n  \u2502 \u251c\u2500\u2500 path-is-absolute@1.0.0\n  \u2502 \u251c\u2500\u2500 private@0.1.6\n  \u2502 \u251c\u2500\u252c regenerator@0.8.40\n  \u2502 \u2502 \u251c\u2500\u252c commoner@0.10.4\n  \u2502 \u2502 \u2502 \u251c\u2500\u252c commander@2.9.0\n  \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 graceful-readlink@1.0.1\n  \u2502 \u2502 \u2502 \u251c\u2500\u252c detective@4.3.1\n  \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 acorn@1.2.2\n  \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 defined@1.0.0\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 glob@5.0.15\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 iconv-lite@0.4.13\n  \u2502 \u2502 \u2502 \u2514\u2500\u2500 q@1.4.1\n  \u2502 \u2502 \u251c\u2500\u252c defs@1.1.1\n  \u2502 \u2502 \u2502 \u251c\u2500\u252c alter@0.2.0\n  \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 stable@0.1.5\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 ast-traverse@0.1.1\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 breakable@1.0.0\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 simple-fmt@0.1.0\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 simple-is@0.2.0\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 stringmap@0.2.2\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 stringset@0.2.1\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 tryor@0.1.2\n  \u2502 \u2502 \u2502 \u2514\u2500\u252c yargs@3.27.0\n  \u2502 \u2502 \u2502   \u251c\u2500\u2500 camelcase@1.2.1\n  \u2502 \u2502 \u2502   \u251c\u2500\u252c cliui@2.1.0\n  \u2502 \u2502 \u2502   \u2502 \u251c\u2500\u252c center-align@0.1.2\n  \u2502 \u2502 \u2502   \u2502 \u2502 \u251c\u2500\u252c align-text@0.1.3\n  \u2502 \u2502 \u2502   \u2502 \u2502 \u2502 \u251c\u2500\u252c kind-of@2.0.1\n  \u2502 \u2502 \u2502   \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 is-buffer@1.1.0\n  \u2502 \u2502 \u2502   \u2502 \u2502 \u2502 \u2514\u2500\u2500 repeat-string@1.5.2\n  \u2502 \u2502 \u2502   \u2502 \u2502 \u2514\u2500\u2500 lazy-cache@0.2.7\n  \u2502 \u2502 \u2502   \u2502 \u2514\u2500\u2500 right-align@0.1.3\n  \u2502 \u2502 \u2502   \u251c\u2500\u2500 decamelize@1.1.2\n  \u2502 \u2502 \u2502   \u251c\u2500\u252c os-locale@1.4.0\n  \u2502 \u2502 \u2502   \u2502 \u2514\u2500\u252c lcid@1.0.0\n  \u2502 \u2502 \u2502   \u2502   \u2514\u2500\u2500 invert-kv@1.0.0\n  \u2502 \u2502 \u2502   \u251c\u2500\u2500 window-size@0.1.4\n  \u2502 \u2502 \u2502   \u2514\u2500\u2500 y18n@3.2.0\n  \u2502 \u2502 \u251c\u2500\u2500 esprima-fb@15001.1001.0-dev-harmony-fb\n  \u2502 \u2502 \u251c\u2500\u252c recast@0.10.33\n  \u2502 \u2502 \u2502 \u2514\u2500\u2500 ast-types@0.8.12\n  \u2502 \u2502 \u2514\u2500\u2500 through@2.3.8\n  \u2502 \u251c\u2500\u252c regexpu@1.3.0\n  \u2502 \u2502 \u251c\u2500\u2500 esprima@2.7.1\n  \u2502 \u2502 \u251c\u2500\u2500 regenerate@1.2.1\n  \u2502 \u2502 \u251c\u2500\u2500 regjsgen@0.2.0\n  \u2502 \u2502 \u2514\u2500\u252c regjsparser@0.1.5\n  \u2502 \u2502   \u2514\u2500\u2500 jsesc@0.5.0\n  \u2502 \u251c\u2500\u2500 repeating@1.1.3\n  \u2502 \u251c\u2500\u2500 resolve@1.1.6\n  \u2502 \u251c\u2500\u2500 shebang-regex@1.0.0\n  \u2502 \u251c\u2500\u2500 slash@1.0.0\n  \u2502 \u251c\u2500\u2500 source-map@0.5.3\n  \u2502 \u251c\u2500\u252c source-map-support@0.2.10\n  \u2502 \u2502 \u2514\u2500\u2500 source-map@0.1.32\n  \u2502 \u251c\u2500\u2500 to-fast-properties@1.0.1\n  \u2502 \u251c\u2500\u2500 trim-right@1.0.1\n  \u2502 \u2514\u2500\u2500 try-resolve@1.0.1\n  \u251c\u2500\u252c babel-plugin-espower@1.1.0\n  \u2502 \u251c\u2500\u2500 array-find@1.0.0\n  \u2502 \u251c\u2500\u252c escallmatch@1.4.2\n  \u2502 \u2502 \u251c\u2500\u2500 deep-equal@1.0.1\n  \u2502 \u2502 \u251c\u2500\u2500 esprima@2.7.1\n  \u2502 \u2502 \u251c\u2500\u252c espurify@1.5.0\n  \u2502 \u2502 \u2502 \u2514\u2500\u2500 isarray@1.0.0\n  \u2502 \u2502 \u2514\u2500\u2500 indexof@0.0.1\n  \u2502 \u251c\u2500\u252c espower@1.2.1\n  \u2502 \u2502 \u251c\u2500\u252c escodegen@1.7.1\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 esprima@1.2.5\n  \u2502 \u2502 \u2502 \u251c\u2500\u2500 estraverse@1.9.3\n  \u2502 \u2502 \u2502 \u251c\u2500\u252c optionator@0.5.0\n  \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 deep-is@0.1.3\n  \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 fast-levenshtein@1.0.7\n  \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 levn@0.2.5\n  \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 prelude-ls@1.1.2\n  \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 type-check@0.3.1\n  \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 wordwrap@0.0.2\n  \u2502 \u2502 \u2502 \u2514\u2500\u2500 source-map@0.2.0\n  \u2502 \u2502 \u251c\u2500\u2500 is-url@1.2.1\n  \u2502 \u2502 \u2514\u2500\u2500 isarray@0.0.1\n  \u2502 \u2514\u2500\u2500 xtend@4.0.1\n  \u251c\u2500\u2500 babel-runtime@5.8.34\n  \u251c\u2500\u2500 bluebird@3.1.1\n  \u251c\u2500\u252c chalk@1.1.1\n  \u2502 \u251c\u2500\u2500 ansi-styles@2.1.0\n  \u2502 \u251c\u2500\u2500 escape-string-regexp@1.0.4\n  \u2502 \u251c\u2500\u252c has-ansi@2.0.0\n  \u2502 \u2502 \u2514\u2500\u2500 ansi-regex@2.0.0\n  \u2502 \u251c\u2500\u2500 strip-ansi@3.0.0\n  \u2502 \u2514\u2500\u2500 supports-color@2.0.0\n  \u251c\u2500\u252c co-with-promise@4.6.0\n  \u2502 \u2514\u2500\u252c pinkie-promise@1.0.0\n  \u2502   \u2514\u2500\u2500 pinkie@1.0.0\n  \u251c\u2500\u252c core-assert@0.1.3\n  \u2502 \u2514\u2500\u2500 buf-compare@1.0.0\n  \u251c\u2500\u252c debug@2.2.0\n  \u2502 \u2514\u2500\u2500 ms@0.7.1\n  \u251c\u2500\u2500 deeper@2.1.0\n  \u251c\u2500\u252c empower@1.1.0\n  \u2502 \u251c\u2500\u2500 array-filter@1.0.0\n  \u2502 \u251c\u2500\u2500 array-foreach@1.0.1\n  \u2502 \u251c\u2500\u2500 array-map@0.0.0\n  \u2502 \u251c\u2500\u2500 array-some@1.0.0\n  \u2502 \u251c\u2500\u252c define-properties@1.1.2\n  \u2502 \u2502 \u2514\u2500\u2500 foreach@2.0.5\n  \u2502 \u2514\u2500\u252c object-create@0.1.0\n  \u2502   \u2514\u2500\u252c object-define-property@0.1.0\n  \u2502     \u251c\u2500\u2500 function-bind@0.1.0\n  \u2502     \u2514\u2500\u2500 has@0.0.1\n  \u251c\u2500\u252c empower-core@0.2.0\n  \u2502 \u2514\u2500\u2500 call-signature@0.0.2\n  \u251c\u2500\u2500 figures@1.4.0\n  \u251c\u2500\u2500 fn-name@2.0.1\n  \u251c\u2500\u252c globby@4.0.0\n  \u2502 \u251c\u2500\u252c array-union@1.0.1\n  \u2502 \u2502 \u2514\u2500\u2500 array-uniq@1.0.2\n  \u2502 \u2514\u2500\u252c glob@6.0.2\n  \u2502   \u251c\u2500\u252c inflight@1.0.4\n  \u2502   \u2502 \u2514\u2500\u2500 wrappy@1.0.1\n  \u2502   \u251c\u2500\u2500 inherits@2.0.1\n  \u2502   \u2514\u2500\u2500 once@1.3.3\n  \u251c\u2500\u2500 has-generator@1.0.0\n  \u251c\u2500\u2500 is-generator-fn@1.0.0\n  \u251c\u2500\u252c is-observable@0.1.0\n  \u2502 \u2514\u2500\u2500 symbol-observable@0.1.0\n  \u251c\u2500\u2500 is-promise@2.1.0\n  \u251c\u2500\u252c loud-rejection@1.2.0\n  \u2502 \u2514\u2500\u2500 signal-exit@2.1.2\n  \u251c\u2500\u2500 max-timeout@1.0.0\n  \u251c\u2500\u252c meow@3.6.0\n  \u2502 \u251c\u2500\u252c camelcase-keys@2.0.0\n  \u2502 \u2502 \u251c\u2500\u2500 camelcase@2.0.1\n  \u2502 \u2502 \u2514\u2500\u2500 map-obj@1.0.1\n  \u2502 \u251c\u2500\u2500 minimist@1.2.0\n  \u2502 \u251c\u2500\u252c normalize-package-data@2.3.5\n  \u2502 \u2502 \u251c\u2500\u2500 hosted-git-info@2.1.4\n  \u2502 \u2502 \u251c\u2500\u252c is-builtin-module@1.0.0\n  \u2502 \u2502 \u2502 \u2514\u2500\u2500 builtin-modules@1.1.0\n  \u2502 \u2502 \u251c\u2500\u2500 semver@5.1.0\n  \u2502 \u2502 \u2514\u2500\u252c validate-npm-package-license@3.0.1\n  \u2502 \u2502   \u251c\u2500\u252c spdx-correct@1.0.2\n  \u2502 \u2502   \u2502 \u2514\u2500\u2500 spdx-license-ids@1.1.0\n  \u2502 \u2502   \u2514\u2500\u252c spdx-expression-parse@1.0.2\n  \u2502 \u2502     \u2514\u2500\u2500 spdx-exceptions@1.0.4\n  \u2502 \u251c\u2500\u252c redent@1.0.0\n  \u2502 \u2502 \u251c\u2500\u252c indent-string@2.1.0\n  \u2502 \u2502 \u2502 \u2514\u2500\u2500 repeating@2.0.0\n  \u2502 \u2502 \u2514\u2500\u2500 strip-indent@1.0.1\n  \u2502 \u2514\u2500\u2500 trim-newlines@1.0.0\n  \u251c\u2500\u2500 object-assign@4.0.1\n  \u251c\u2500\u2500 observable-to-promise@0.1.0\n  \u251c\u2500\u252c plur@2.1.2\n  \u2502 \u2514\u2500\u2500 irregular-plurals@1.1.0\n  \u251c\u2500\u252c power-assert-formatter@1.3.2\n  \u2502 \u251c\u2500\u2500 acorn@2.6.4\n  \u2502 \u251c\u2500\u2500 acorn-es7-plugin@1.0.11\n  \u2502 \u251c\u2500\u2500 array-reduce@0.0.0\n  \u2502 \u251c\u2500\u2500 eastasianwidth@0.1.1\n  \u2502 \u251c\u2500\u2500 estraverse@4.1.1\n  \u2502 \u251c\u2500\u2500 googlediff@0.1.0\n  \u2502 \u251c\u2500\u2500 object-keys@1.0.9\n  \u2502 \u251c\u2500\u252c stringifier@1.2.1\n  \u2502 \u2502 \u251c\u2500\u2500 array-reduce-right@1.0.0\n  \u2502 \u2502 \u2514\u2500\u2500 traverse@0.6.6\n  \u2502 \u2514\u2500\u2500 type-name@1.1.0\n  \u251c\u2500\u2500 power-assert-renderers@0.1.0\n  \u251c\u2500\u252c pretty-ms@2.1.0\n  \u2502 \u251c\u2500\u252c is-finite@1.0.1\n  \u2502 \u2502 \u2514\u2500\u2500 number-is-nan@1.0.0\n  \u2502 \u251c\u2500\u2500 parse-ms@1.0.0\n  \u2502 \u2514\u2500\u2500 plur@1.0.0\n  \u251c\u2500\u2500 require-from-string@1.1.0\n  \u251c\u2500\u252c resolve-cwd@1.0.0\n  \u2502 \u2514\u2500\u2500 resolve-from@2.0.0\n  \u251c\u2500\u2500 serialize-error@1.1.0\n  \u251c\u2500\u2500 set-immediate-shim@1.0.1\n  \u251c\u2500\u252c source-map-support@0.4.0\n  \u2502 \u2514\u2500\u252c source-map@0.1.32\n  \u2502   \u2514\u2500\u2500 amdefine@1.0.0\n  \u251c\u2500\u252c squeak@1.3.0\n  \u2502 \u251c\u2500\u2500 console-stream@0.1.1\n  \u2502 \u2514\u2500\u252c lpad-align@1.1.0\n  \u2502   \u251c\u2500\u2500 longest@1.0.1\n  \u2502   \u2514\u2500\u2500 lpad@2.0.1\n  \u251c\u2500\u252c time-require@0.1.2\n  \u2502 \u251c\u2500\u252c chalk@0.4.0\n  \u2502 \u2502 \u251c\u2500\u2500 ansi-styles@1.0.0\n  \u2502 \u2502 \u251c\u2500\u2500 has-color@0.1.7\n  \u2502 \u2502 \u2514\u2500\u2500 strip-ansi@0.1.1\n  \u2502 \u251c\u2500\u2500 date-time@0.1.1\n  \u2502 \u251c\u2500\u252c pretty-ms@0.2.2\n  \u2502 \u2502 \u2514\u2500\u2500 parse-ms@0.1.2\n  \u2502 \u2514\u2500\u2500 text-table@0.2.0\n  \u2514\u2500\u252c update-notifier@0.5.0\n    \u251c\u2500\u252c configstore@1.4.0\n    \u2502 \u251c\u2500\u2500 graceful-fs@4.1.2\n    \u2502 \u251c\u2500\u252c osenv@0.1.3\n    \u2502 \u2502 \u2514\u2500\u2500 os-homedir@1.0.1\n    \u2502 \u251c\u2500\u2500 uuid@2.0.1\n    \u2502 \u251c\u2500\u252c write-file-atomic@1.1.4\n    \u2502 \u2502 \u251c\u2500\u2500 imurmurhash@0.1.4\n    \u2502 \u2502 \u2514\u2500\u2500 slide@1.1.6\n    \u2502 \u2514\u2500\u2500 xdg-basedir@2.0.0\n    \u251c\u2500\u2500 is-npm@1.0.0\n    \u251c\u2500\u252c latest-version@1.0.1\n    \u2502 \u2514\u2500\u252c package-json@1.2.0\n    \u2502   \u251c\u2500\u252c got@3.3.1\n    \u2502   \u2502 \u251c\u2500\u252c duplexify@3.4.2\n    \u2502   \u2502 \u2502 \u251c\u2500\u2500 end-of-stream@1.0.0\n    \u2502   \u2502 \u2502 \u2514\u2500\u252c readable-stream@2.0.5\n    \u2502   \u2502 \u2502   \u251c\u2500\u2500 core-util-is@1.0.2\n    \u2502   \u2502 \u2502   \u251c\u2500\u2500 isarray@0.0.1\n    \u2502   \u2502 \u2502   \u251c\u2500\u2500 process-nextick-args@1.0.6\n    \u2502   \u2502 \u2502   \u251c\u2500\u2500 string_decoder@0.10.31\n    \u2502   \u2502 \u2502   \u2514\u2500\u2500 util-deprecate@1.0.2\n    \u2502   \u2502 \u251c\u2500\u2500 infinity-agent@2.0.3\n    \u2502   \u2502 \u251c\u2500\u2500 is-redirect@1.0.0\n    \u2502   \u2502 \u251c\u2500\u2500 is-stream@1.0.1\n    \u2502   \u2502 \u251c\u2500\u2500 lowercase-keys@1.0.0\n    \u2502   \u2502 \u251c\u2500\u2500 nested-error-stacks@1.0.2\n    \u2502   \u2502 \u251c\u2500\u2500 object-assign@3.0.0\n    \u2502   \u2502 \u251c\u2500\u2500 prepend-http@1.0.3\n    \u2502   \u2502 \u251c\u2500\u252c read-all-stream@3.0.1\n    \u2502   \u2502 \u2502 \u2514\u2500\u252c pinkie-promise@1.0.0\n    \u2502   \u2502 \u2502   \u2514\u2500\u2500 pinkie@1.0.0\n    \u2502   \u2502 \u2514\u2500\u2500 timed-out@2.0.0\n    \u2502   \u2514\u2500\u252c registry-url@3.0.3\n    \u2502     \u2514\u2500\u252c rc@1.1.5\n    \u2502       \u251c\u2500\u2500 deep-extend@0.4.0\n    \u2502       \u251c\u2500\u2500 ini@1.3.4\n    \u2502       \u251c\u2500\u2500 minimist@1.2.0\n    \u2502       \u2514\u2500\u2500 strip-json-comments@1.0.4\n    \u251c\u2500\u2500 semver-diff@2.1.0\n    \u2514\u2500\u2500 string-length@1.0.1\nSo I'm really not sure why you're seeing that. Maybe file a bug report with npm?\n. @jamestalmage: that's strange, I did go above and beyond my usual intelligence level and create a new project to install AVA, but I'll try again when I'm home.\n. @jamestalmage: that's strange, I did go above and beyond my usual intelligence level and create a new project to install AVA, but I'll try again when I'm home.\n. Actually, I'm just going to make them one PR.\n. Hey, @jamestalmage, @sindresorhus: just a thought, should AvaErrors have .stack = null?\n. Hey, @jamestalmage, @sindresorhus: just a thought, should AvaErrors have .stack = null?\n. @sindresorhus: What's the status on this? Is there anything blocking it from getting merged?\n. @sindresorhus: What's the status on this? Is there anything blocking it from getting merged?\n. @sindresorhus: sorry about that, I had mentally sorted it by issue, but that doesn't make much sense outside my head. Since I implemented them sequentially, I could split them into seperate PRs, should I do that?\n. Closing this to split into multiple pull request.\n. @jamestalmage, Thanks, thats a good tip to know! But I still think that we don't need to run coverage by default. And I did mean watch for AVA itself. I was just thinking nodemon as a dev dependency.\n@sindresorhus: child.js?\nCan I go ahead and implement #70? It seems no one has done so.\n. @jamestalmage, Thanks, thats a good tip to know! But I still think that we don't need to run coverage by default. And I did mean watch for AVA itself. I was just thinking nodemon as a dev dependency.\n@sindresorhus: child.js?\nCan I go ahead and implement #70? It seems no one has done so.\n. @sindresorhus: Ok.\n. @sindresorhus: Ok.\n. Oh, @sindresorhus: Should I implement a coverage-less test option and a npm run watch too?\n. Oh, @sindresorhus: Should I implement a coverage-less test option and a npm run watch too?\n. @sindresorhus: How does test sound, and we can rename the test with coverage test:cov?\n. @sindresorhus: How does test sound, and we can rename the test with coverage test:cov?\n. @sindresorhus: I have one more larger idea, which I'm not sure if you've ever considered: What about adopting the Angular commit message guidelines? They make it really easy to see what a commit is about, and it allows you to do cool things, like semantic-release (which could be really helpful for some of your smaller projects). I use them for all my projects, and it's really nice. (I'm sure you've probably seen it, but I thought I'd suggest it. If for whatever reason you don't want to implement it, that's 110% fine). \n. @sindresorhus: Fair enough. (Although if you ever change your mind, GitCop can do that automagically).\n. Ok @sindresorhus, that should be fixed now.\nI have no clue why that happened... It looks like WebStorm automagically un-babel'd it.\n. No, I get that. But it appears that WebStorm un- babel'd the code. It's\nvery strange.\nOn Sat, Dec 26, 2015 at 9:46 PM James Talmage notifications@github.com\nwrote:\n\nWebStorm asks you about adding a file watcher whenever it sees ES6 code.\nIf you accidentally click yes, this can happen. I keep meaning to figure\nout where the setting is just to turn that off.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/pull/375#issuecomment-167382407.\n. @vdemedes: what about \"subprocess.js\"?\n. @jamestalmage, @vdemedes: I still like subprocess.js or child.js.\n\nAnd the bikeshed is to be blue.\n. Ok, I'll change it to test-worker and update the commit.\n. @sindresorhus: It's not related to the stack. Check my comment in #102. It's a power-assert thing.\n. @sindresorhus: It's not related to the stack. Check my comment in #102. It's a power-assert thing.\n. @sindresorhus: Message fixed.\n. @sindresorhus: Message fixed.\n. @sindresourhus, @jamestalmage: yeah, I think we probably should. I'll work on that in the morning.\n. @jamestalmage, @sindresorhus: OK, rebased onto master, and now uses AvaError.\n. @jamestalmage, @sindresorhus: OK, rebased onto master, and now uses AvaError.\n. @sindresorhus: Of course they are. Give me a second.\n. @sindresorhus: Of course they are. Give me a second.\n. Naturally, They're all false negatives.\n@sindresorhus: Do AvaError's need .operators, .actuals or .expecteds?\n. Naturally, They're all false negatives.\n@sindresorhus: Do AvaError's need .operators, .actuals or .expecteds?\n. @jamestalmage: ^^^^^^^^^^\n. @jamestalmage: ^^^^^^^^^^\n. Thanks @sindresorhus!\nAnd no one should ever commit on master. GITHUB FLOW FTW!!!!!\nAlso, should do you prefer internal branches or forks? (I have no preference, but I thought I'd ask),\n. Sounds great. Thanks again!\nOn Sat, Dec 26, 2015 at 8:21 PM Sindre Sorhus notifications@github.com\nwrote:\n\nAlso, should do you prefer internal branches or forks?\nUse your fork for now.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/377#issuecomment-167378753.\n. @jamestalmage: Sorry, I usually do. Thanks!\n. @jamestalmage: The other one was significantly smaller, and had already been merged. So I made that one Refs #XXX and this one Fixes #XXX.\n. @sindresorhus, @vdemedes, @jamestalmage: Anyone have any objections to merging?\n. Ok:\n- [x] Commits Squashed\n- [x] 2 LGTMs\n  - [x] LGTM\n  - [x] LGTM\n- [x] Builds Passing\n  - [x] Travis\n  - [x] AppVeyor\n  - [x] Coveralls\n- [x] Merged.\n. @vdemedes: on mobile atm, but I'll post a screenshot when I get back. Basically, it properly displayes subdirectories with the default reporter.\n. @vdemedes: I uploaded some screenshots.\n. @vdemedes: Everything else look good about this PR?\n. @sindresorhus: Sorry if this is stupid, but why isn't there a LGTM status check for this PR?\n. @sindresorhus: Ok, thanks.\n. @sindresorhus: I'm going to merge this, is that OK?\n. @juanpr2: It looks like your tests are for browser code, (leaflet is for the browser too). AVA currently doesn't support browsers, it currently runs in node. It's not really that we don't support browser globals, per se, it's just that we don't yet support running in a browser.\n\nIn the future, we're planning to add support for running in the browser, but if you want to test in the browser today, you'll have to use a diffrent test framework.\n(The following might be full of stuff you already know, so feel free to ignore it if that's the case.)\nI personally like mocha, but be warned, it's a little different from ava. If you're looking for something similar to AVA,  tape looks nice. Both of those support browsers. If you want to run code in the browser that uses modules (require or import), you'll need something like browserify or webpack (browserify is going to be a lot easier getting started). Then to actually load all this into the browser, you're going to want something like karma (formerly testacular), which takes your tests and lets your easily run them in a bunch of browsers.\nSorry about that, and let me know if you have any other questions\n. @jamestalmage: Looks pretty good, expect for the few things I mentioned. Also, it looks like there aren't very many tests on the CachingPrecompiler itself (I don't know if there needs to be).\n. @jamestalmage: Looks pretty good, expect for the few things I mentioned. Also, it looks like there aren't very many tests on the CachingPrecompiler itself (I don't know if there needs to be).\n. @sindresorhus, @jamestalmage: We could also try hijacking/monkey-patching wither process.stdout/stderr or console.log.\n. Hmm... Seems like a great idea. And we don't even have to load anything if there's already a native promise.\nWant me to have a go at implementing this?\n. Hmm... Seems like a great idea. And we don't even have to load anything if there's already a native promise.\nWant me to have a go at implementing this?\n. Wait, @jamestalmage, @sindresorhus: Why wouldn't we want this? (Sorry if I missed some earlier conversation).\n. @jamestalmage: Ok, but as a user of AVA, having AVA transpile my tests with babel, yet not follow my .babelrc, while recommending the babel require hook for test helper files (which does follow my .babelrc) is incredibly confusing. Like really confusing.\n. @jamestalmage, @sindresorhus: I would like to say that I really think that if we're going to block .babelrc (I don't think we should). Then we should prioritize allowing additional babel config, because being forced to use diffrent babel settings for tests and production code is really bad.\n. @jamestalmage: That's not what I meant. Here's my situation: I use the babel-require hook, with babel-preset-es2015, and babel-preset-stage-0 in my .babelrc. I'd like for AVA to use that. Otherwise there's stuff I can do in my code but not in my tests. On top of that, the babel-runtime thing means that I can't use bluebird, which is extremely annoying. So I think that the best course of action would be to make AVA smart enough to detect a competing .babelrc, and if it exists, use that (+ power-assert), instead of it's own config.\n. @jamestalmage: your can't use bluebird because it overrides 'Promise' anywhere it finds it, regardless of scope.\nWhy would it ever be a bad thing for babel to read babel configuration?\nWhat I've personally done for a project that i started yesterday was to transpile it with babel ahead of time.\nSo while I think that es2015 support is a cool idea, I think that has having a '--no-babel' flag for more complex projects. might be a good idea. (And, you know, being able to babel config would be great too).\nThoughts?\n. @jamestalmage: Weird... It didn't work for me, and according to the maintainers, it shouldn't. I still disagree that AVA shouldn't read the config automatically, but it's not that big of a deal if we document it.\n@sindresorhus: Thoughts on a --no-babel option as a hold over till we allow customizing babel config?\n. @sindresorhus: Until we can modify babel config, I pre-compile my code with babel. Running the transpiled code through babel again doesn't make any sense (This can come with a warning like If you use --no-babel, you must use the power assert babel plugin!).\n. @jamestalmage: Not really. I also use babel-preset-features, although I guess loosing that wouldn't be the end of the world. But really I think the solution is to allow custom babel config.\n. @jamestalmage: Think about it like this: AVA says that it invokes babel for you. If you've explicitly said that you want babel to do X, it would be confusing when AVA's babel doesn't.\nSince this seems to be a matter of opinion, why don't we just do a poll?\n. @sindresorhus: what are your thoughts on having AVA's babel automatically ignore the explicit babel configuration that users have set. (In my mind, this seems absolutely crazy and the wrong thing to do).\nAnd since this seems to be a debated topic, would everyone be open to a poll of AVA users on what they'd prefer?\n. @jamestalmage: I think that it would be a much simpler if we just modified the babel-runtime plugin. This seems like something that could easily fall apart.\n. I think this is a really great idea. Also, I'd like to appreciate the relevant issue number.\n. Actually, What if we made this a wiki page and linked to it from the README? That way others could contribute?\n. Agreed. I'd also like to add a \"files\" section, and have it be \"config.ava\", vs. \"ava \"\n. @sindresorhus: Except nyc and comitizen (off the top of my head).\n. @jamestalmage: one quick thing: require on the command line shouldn't override the package.json, it should add to it? (Sorry for no example, on mobile)\n. @sindresorhus: do you know about 'npm I unicorns'?\n. @sindresorhus: try it ;). It's awesome.\n. :+1:\n. :+1:\n. @sindresorhus: You changed your profile picture!\nAs for the actual topic at hand, I'm not really sure either way. On one\nhand, it would be nice, but on the other, there would be a huge\nperformance penalty. And some unexpected behavior (read: it should be\nbehind a flag). It kind of starts to remind me of Google Apps Script, which\nisn't a good thing.\nAlso, then before === beforeEach? That's something else we'd have to think\nabout\nOn Wed, Jan 13, 2016 at 2:07 PM \u2301 s\u026a\u0274\u1d05\u0280\u1d07 s\u1d0f\u0280\u029c\u1d1cs \u2301 notifications@github.com\nwrote:\n\nOh, didn't realize that was possible. Make sure the mention that in #24\nhttps://github.com/sindresorhus/ava/issues/24 ;)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/421#issuecomment-171452504.\n. @sindresorhus: You changed your profile picture!\n\nAs for the actual topic at hand, I'm not really sure either way. On one\nhand, it would be nice, but on the other, there would be a huge\nperformance penalty. And some unexpected behavior (read: it should be\nbehind a flag). It kind of starts to remind me of Google Apps Script, which\nisn't a good thing.\nAlso, then before === beforeEach? That's something else we'd have to think\nabout\nOn Wed, Jan 13, 2016 at 2:07 PM \u2301 s\u026a\u0274\u1d05\u0280\u1d07 s\u1d0f\u0280\u029c\u1d1cs \u2301 notifications@github.com\nwrote:\n\nOh, didn't realize that was possible. Make sure the mention that in #24\nhttps://github.com/sindresorhus/ava/issues/24 ;)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/421#issuecomment-171452504.\n. @sindresorhus: I know, but then the before hook would have to be run for\neach test (because it would have to be run once per thread), therefore\nmaking it equal to beforeEach.\nOn Wed, Jan 13, 2016 at 3:41 PM \u2301 s\u026a\u0274\u1d05\u0280\u1d07 s\u1d0f\u0280\u029c\u1d1cs \u2301 notifications@github.com\nwrote:\n@ariporad https://github.com/ariporad Test hooks would still work as we\nwould execute the file as normal, but only run a specific test (with hooks)\ninstead of all.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/421#issuecomment-171473111.\n. @sindresorhus: I know, but then the before hook would have to be run for\neach test (because it would have to be run once per thread), therefore\nmaking it equal to beforeEach.\nOn Wed, Jan 13, 2016 at 3:41 PM \u2301 s\u026a\u0274\u1d05\u0280\u1d07 s\u1d0f\u0280\u029c\u1d1cs \u2301 notifications@github.com\nwrote:\n@ariporad https://github.com/ariporad Test hooks would still work as we\nwould execute the file as normal, but only run a specific test (with hooks)\ninstead of all.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/421#issuecomment-171473111.\n. @jamestalmage: Yeah, didn't you hear? @sindresorhus has decreed that there shall be only one possible way to do anything! :smile: \n. @sindresorhus: that was like 98% joke, sorry.\n\nOn Fri, Jan 15, 2016 at 6:14 AM \u2301 s\u026a\u0274\u1d05\u0280\u1d07 s\u1d0f\u0280\u029c\u1d1cs \u2301 notifications@github.com\nwrote:\n\n@ariporad https://github.com/ariporad Not at all. I just want us to\nexplore other options before adding more API surface. Adding new methods\nand options are easy. Keep things minimal while powerful is very hard.\n[image: horrible-ui]\nhttps://cloud.githubusercontent.com/assets/170270/12355055/68b5df7e-bb9a-11e5-90eb-285aab88f633.png\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/421#issuecomment-171971180.\n. @jamestalmage: I'll take that on, I need something to do on the plane\ntomorrow.\nOn Fri, Jan 15, 2016 at 2:04 PM James Talmage notifications@github.com\nwrote:\nWe've talked it over and decided that a --no-babel flag is an acceptable\n(if possibly temporary) fix. It's going to slow things down a lot.\nEspecially for npm@2 users. Any PR should include documentation warning\nthat npm@3 is highly recommended with that flag.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/424#issuecomment-172107059.\n. :+1:\n. Hmm... Great idea @jamestalmage! However, This still seems a little weird to me. If we're already using the .babelrc, What if we just had a --custom-babel flag? (We could set the NODE_ENV to test so they could still customize their config).\n. @sindresorhus, @jamestalmage, @vdemedes: Thoughts?\n. @jamestalmage: it's iffy, because node doesn't allow that (AFAIK), and 'require' doesn't allow that, so it would be a strange thing to do, but I'll add it if you really want.\n. @sindresorhus: I'd say no, no you don't.\n. @jamestalmage, @sindresorhus: Ok, I removed that line.\n. I'm not sure if this should be here, or it should be a method, or if it should be an external function which accepts self. Thoughts?\n. I think we should keep that here, as it makes it more obvious what we're doing, and it makes it a _lot_ easier to reason about this.assertions.\n. Fixed.\n. Done.\n. @sindresorhus: Fixed\n. Naturally, I spend 30 minutes on something to find out that there's already something that does that.\n. @sindresorhus: Sorry, I'm really terrible with wording. Would something like \"Directories will be expanded recursively\" be better?\n. @sindresorhus: But that doesn't really indicate that any directories that you pass it, nor does it include the bit about fixtures and helpers or underscores.\n. @sindresorhus: Maybe something like \nDirectories will be recursively expanded. Files in directories named fixtures and helpers are ignored, as well as files starting with _. This can be useful for having helpers in the same directory as your test files.\n. I didn't... I have absolutely no clue why that's there.\n. @jamestalmage: yes, it would be a breaking change. But otherwise we have Inconsistent and confusing behavior.\n. @sindresorhus: I kind of see where @jamestalmage is coming from, the some things recursive some not is a little confusing.\n\nI'd like to purpose that we do something like: \n**/test.js **/test-*.js test/**/*.js, **/*.spec.js **/*.test.js\n. @sindresorhus: why isn't it safe to recurse the entire directory? We already ensure that node_modules is ignored.\nAnd I'm asking for it. I always structure my modules with the test next to the file. (I have a really cool system, I can explain more if you like).\n. I'll fix that.\n. @sindresorhus: Fixed.\n. @sindresorhus: Well, I may have oversold it a little, but here's my setup:\nI start with a file structure like this:\nmy-app\n\u251c\u2500\u2500 src/                     # Code\n|   \u251c\u2500\u2500 foo.js               # Some fascinating module\n|   \u251c\u2500\u2500 foo.test.unit.js     # The unit (doesn't exit the process, <150ms/test) tests for `foo.js`\n|   \u251c\u2500\u2500 bar.js               # Another fascinating module\n|   \u251c\u2500\u2500 bar.test.int.js      # The focused integration tests for `bar.js`\n|   \u2514\u2500\u2500 ...                  # Other stuff too\n\u251c\u2500\u2500 test/                    # Tests that don't belong to a single source file, utils, mocks, fixtures, etc.\n|   \u251c\u2500\u2500 utils/               # Test utilities\n|   |   \u251c\u2500\u2500 findPort.js      # Find an available port.\n|   |   \u251c\u2500\u2500 portAvail.js     # Test if a port is available.\n|   |   \u2514\u2500\u2500 ...              # Other stuff too\n|   \u251c\u2500\u2500 fixture/             # Test fixtures\n|   |   \u251c\u2500\u2500 some-test        # Find an available port.\n|   |   \u2514\u2500\u2500 ...              # Other stuff too\n|   \u251c\u2500\u2500 mocks/               # Some mocks \n|   |   \u251c\u2500\u2500 SomeClass.js     # A mock for `SomeClass`\n|   |   \u2514\u2500\u2500 ...              # Other stuff too\n|   \u251c\u2500\u2500 webapp.test.e2e.js   # End2End tests for the web app\n|   \u251c\u2500\u2500 mobile.test.e2e.js   # End2End tests for the mobile app\n|   \u251c\u2500\u2500 api.test.e2e.js      # End2End tests for the API\n|   \u251c\u2500\u2500 setup.js             # Register some global helper functions, and bluebird.\n|   \u2514\u2500\u2500 ...                  # Other stuff too\n\u2514\u2500\u2500 ...                      # Other stuff too\nThen you use app-module-path for my-app, and then your tests look like this:\n``` javascript\nimport test from 'ava';\nimport rewire from 'rewire';\nimport findPort from 'test/utils/findPort';\nimport portAvail from 'test/utils/portAvail';\nimport FakeSomeClass from 'test/mock/SomeClass';\ntest.beforeEach(async t => {\n    t.context.port = await findPort();\n    t.context.foo = rewire('./foo');\n});\ntest(async t => {\n    await t.context.foo.createServer(t.context.port);\n    t.ok(await portAvail(t.context.port));\n});\n```\nAnd the actual files:\n``` javascript\nimport http from 'http';\nimport config from 'app/config';\nimport connectToDb from 'app/db';\nexport async function createServer(port) {\n    var server = http.createServer();\n    var db = await connectToDb();\n    await Promise.promisify(::server.listen)(port);\n    return { server, db };\n};\n```\nAnd I have build scripts set up so I can test, test:cov or watch based on test type:\n``` bash\n$ npm run test\n\nnpm run test:all\n\n100 tests passing.\n\n$ npm run test:cov\n\nnpm run test:all:cov\nava\n    100 tests passing.\n\n100% coverage.\n\n$ npm run test:{unit,int,e2e}\n\n...\n\n$ npm run test:{unit,int,e2e}:cov\n\n...\n\n$ npm run watch:{unit,int,es2}\n\n...\nWatching...\n\n``\n. @sindresorhus: Ok, Sure. It's not really that big a deal.\n. @jamestalmage: What we do everywhere else is just invoke ourself withnew` and return it. Maybe switch to that?:\njavascript\nif (!(this instanceof CachingPrecompiler)) {\n    return new CachingPrecompiler(cacheDir);\n}\n. @jamestalmage: Why is this here? There are no closures.\n. @jamestalmage: See above, this could just be this.\n. I think this should be if (!sourceMapCache[source]) {.\n.  ~~Two things:~~\n~~1. It seems like this should be moved into the compiler.~~\n~~2.Why are we writing maps to disk just to read them again. Wouldn't it be way faster to just not write them in the first place, and get them from memory here?~~\nEDIT: Never mind. It's a diffrent process. But this feels like the wrong place for this logic.\n. @jamestalmage: Why not just modify process.env.NODE_PATH (I think that still works)? Or you could use app-module-path and just pass in the path to node modules.\nOr, you know what? This is probably fine. But the bikeshed is to be blue.\n. Hmm... I'm not sure. I guess it's fine.\nOn Wed, Dec 30, 2015 at 7:03 PM James Talmage notifications@github.com\nwrote:\n\nIn lib/test-worker.js\nhttps://github.com/sindresorhus/ava/pull/390#discussion_r48645161:\n\n@@ -34,73 +30,40 @@ sourceMapSupport.install({\n        if (sourceMapCache[source]) {\n            return {\n                url: source,\n-               map: sourceMapCache[source]\n-               map: fs.readFileSync(sourceMapCache[source], 'utf8')\n\nWhere should it go?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/pull/390/files#r48645161.\n. This is a bit out of scope, but it shouldn't really be a problem.\n. \n",
    "andrepolischuk": "May be use flag --esnext?\njson\n{\n    \"scripts\": {\n        \"test\": \"ava --esnext\"\n    }\n}\n. It would be better then with flag.\n. I have no idea why execFile failed in travis\n. I will fix\n. @sindresorhus I fixed conflicts\n. Thank you for review and comments\n. You mean that I should remove this test? Or rewrite?\n. I changed this because XO without --esnext failed on checking test/es2015.js\n. ",
    "mattdesl": "Found a bit more discussion upstream: https://github.com/substack/insert-module-globals/pull/40\n. In the browser process.stdout doesn't exist and browserify doesn't try to shim it. Tape uses the following to print in the browser:\nhttps://github.com/substack/tape/blob/master/lib/default_stream.js\n. Added a ticket about exit: https://github.com/Jam3/hihat/issues/27\nNot sure about shimming stderr/stdout in browserify. Seems like browserify's process module could provide a polyfill, but all the stream stuff could add a lot of extra bytes to bundles for mostly no gain.\n. With that said; maybe a dumb polyfill could just write console.log and error synchronously, but not attempt to support any other aspect of the stdout/stderr streams. \n. Hmm.. window.close would also work with smokestack but it seems like a pretty big assumption. \nIn hihat the goal is interactive testing (fast save -> reload workflow). If ava exits the process on fail, it would break this development cycle. Maybe ava should not quit at all in the browser?\n. It seems sorta weird that an API would ever call process.exit. I feel like this should be left up to the user.\nI feel like only the CLI should exit and handle errors itself, since in that case the user isn't handling them.\n. I see. It's a bit of a special case then. \n. I'm also interested in the progress of this thread and how we can combine efforts with devtool to provide better debugging/profiling all around.\ne.g. The setTimeout issue was resolved in a recent commit to devtool, since now we are using true Node.js timers instead of the built-in browser timers.\nhttps://github.com/s-a/iron-node/issues/85\nRelevant commit:\nhttps://github.com/Jam3/devtool/commit/11e491c2bb4540f2a2b9bf6290b3e2b82bcbba89\n. I think the differences will be subtle between the two tools, though my end goal is to have devtool support most Node.js use cases with minimal effort (which involves lots of tweaks for require.main, process, and all that). \nYou can use console.profile(\"foo\") and console.profileEnd(\"foo\"). So you could programmatically add profiles for files or whatever. \nI am currently writing a blog post on all things devtool, you can see it here: \nhttp://mattdesl.svbtle.com/debugging-nodejs-in-chrome-devtools\n. Not sure why there would be a difference. You could try --no-node-timers to see if that is the reason setImmediate was needed.\n. I've cloned ava and I'm trying to run some tests with devtool.\nsh\ndevtool node_modules/ava/profile.js <test-file>\nAre there specific files to use for <test-file> that I should be profiling? Or are you just npm linking ava and testing it on random projects?\n. Pulled from master. Testing with devtool@1.7.6, node@5 and npm@2.14.12, adding the console.profile/profileEnd works fine without setImmediate for the above fixture. Which test is failing?\n. ",
    "architectcodes": "@vdemedes thanks for the fix! The setImmediate error is busted.\nBut now I\u2019ve got another bastard there:\nUncaught TypeError: Cannot read property 'write' of undefined\n\u2013 coming from https://github.com/kevva/squeak/blob/v1.2.0/index.js#L144.\n. Great! I can\u2019t wait to check out this awesome trio: hihat, ava and quixote!\nquixote doesn\u2019t go well with tape, because it\u2019s an assertion library. So I end up with lots of glue code in tests and lose the pretty format of error messages.\n. Is it possible to disable babel via CLI? minimist translates --no-babel to babel: false \u2013 we could use that pattern here as well:\njs\n\u23f4 minimist(['--no-babel'])\n\u23f5 { _: [], babel: false }\n. ",
    "danielepolencic": "same issue with https://github.com/kevva/squeak/blob/v1.2.0/index.js#L144 :(\n. Browserify shims process, but looks like it doesn't shim process.stderr.\nI think that causes https://github.com/kevva/squeak/blob/v1.2.0/index.js#L26 to be undefined and line 144 to fail.\n. I tweaked the constructor and injected the console in squeak like you suggested. \nThis seems to fix the issue. I can run ava in browserify + hihat. :tada: :tada:\nI get some errors about process.exit and bluebird, though.\nATM, I think it would be better to monkey patch process.stderr and process.exit rather than just injecting console in squeak - like you suggested in that link.\nEDIT: maybe these patches should be done in hihat?\n. This is the line that fails with the process.exit.\nAs for process.stderr I think the issue is not as straightforward as I thought.\nava IS browserify friendly (a part from process.exit), but squeak is not. In theory, the latter should be patched and not ava.\nThe issue with squeak is that even if I patch the stream to redirect to console, all output would be redirected to console.log and we lose all log types (i.e. error, warn, info, etc.). What it would be better is to patch squeak (or use another library) so that it falls back to console and still maintains all the right logging levels.\n. With the following (hacky) change in index.js:\njs\nvar log = new Squeak({separator: ' ', stream: {write: console.log.bind(console)}});\nall the logs from type success and error are redirected to console.log.\nIdeally, one would like to have the following mappings between squeak and console\n| Squeak Type | logging level |\n| --- | --- |\n| info | console.info |\n| success | console.log |\n| error | console.error |\n| warn | console.warn |\nwinston offers such functionality (and multiple transports too), but it's not browserify friendly.\nI think the quickest win in this case would be to use console directly? \nas an example log.error could be emulated with:\njs\nvar logError = console.error.bind(console, figures.cross);\nThat would work in node and in the browser.\n. ",
    "tgvashworth": "Hey ava peeps \u2014 could someone please make issues for the work that needs doing on ava to make running in a browser possible (or document how to do it!) so I or someone else could work on it? Thanks!\n. Hey ava peeps \u2014 could someone please make issues for the work that needs doing on ava to make running in a browser possible (or document how to do it!) so I or someone else could work on it? Thanks!\n. Ok. It's disappointing but I totally understand \u2014 do what's right for this project :smile: \nOut of interest, is the test & assertion assertion code (require('ava')) decoupled enough from the runner that it could be used made to work in a browser context? It might be possible to keep the babel & parallelisation code node-only, but get the test and assertion code browser-compatible.\n. Ok. It's disappointing but I totally understand \u2014 do what's right for this project :smile: \nOut of interest, is the test & assertion assertion code (require('ava')) decoupled enough from the runner that it could be used made to work in a browser context? It might be possible to keep the babel & parallelisation code node-only, but get the test and assertion code browser-compatible.\n. ",
    "jokeyrhyme": "I found karma-tap, but there may or may not be an issue with how we end up using it: https://github.com/tmcw/karma-tap/issues/10\n. If you're happy with a simulated browser environment, we can probably already write ava tests with this today: https://github.com/assaf/zombie\n. If you're happy with a simulated browser environment, we can probably already write ava tests with this today: https://github.com/assaf/zombie\n. @vdemedes any thoughts about how we could accomplish DOM tests (or other things not available in Web Workers)?\n. Apparently it's just babel in package.json now, not babelConfig: https://babeljs.io/docs/usage/babelrc/#use-via-package-json\n. With #296 merged, it seems that we might be able to resolve this now...\n. @billyjanitsch custom require hooks are supported now via the --require CLI option.\n. @billyjanitsch custom require hooks are supported now via the --require CLI option.\n. Unless I somehow accomplished this accidentally, I did not attempt to disable ava's babel hook. My only purpose with this PR was the pass through the --require.\nAlso, I hope to get time to address the PR feedback over the next few days. I'm sorry for not being very responsive.\n. I still haven't pushed tests yet. They don't seem quite as trivial as I'd hoped, even with @jamestalmage's suggestions. Soon...\n. Really stuck with the testing part. I'm getting similar errors when I try to use api.js instead of cli.js.\nI've tried a few simple approaches, and none of them work. But this functionality doesn't seem complicated enough to warrant a complicated test. Is there an obvious flaw in this approach? Or do I need to take this up a notch?\n. Thanks for the hint, @jamestalmage. At least now, the errors are all the same one:\n\nTest files must be run with the AVA CLI\n. I could use mockery and just confirm that Node.js is called with the expected parameters, without actually calling Node.js? /shrug\n. So I should detect the version of Node in the tests and only run them for >=4.0?\nAnd there's still something funny going on with the paths. The fact that it's so hard to test properly makes me wonder if I've taken a good approach in the first place. :S\n. Okay, well I'll have another crack at it over the next few days. I'll try implementing --require internally so that it works regardless of the Node.js version.\n. As an interim solution, we could check to see if \"babel-core/register\" has been --required, and automatically disable the built-in babel hook?\n. As an interim solution, we could check to see if \"babel-core/register\" has been --required, and automatically disable the built-in babel hook?\n. I'll take another look at this once #319 is merged.\n. I'll take another look at this once #319 is merged.\n. Okay, so I think this is working now and the tests are passing.\n\nI had to compensate for the new api.js tests in #313, as they call new Api() without a 2nd parameter.\n. Also, I added 4 tests to check different argument orders, but we should probably just pick the one we like and get rid of the rest?\n. Okay, so in total I have 1 api test and 1 cli test. I left the cli test in there to stress the end-to-end process a bit. Of course, if you think this is overkill then I'm happy to drop the cli test completely.\n. I gave it a whirl, but it seemed infeasible to mock the boundary from cli.js to the others, and my other approach to CLI argument tests seemed unnecessarily complex (and still didn't work).\nIf we still feel strongly about having CLI tests, we can have a separate Issue / PR.\n. Okay, I've removed the direct dependency on \"has-flag\" (although istanbul still pulls it in), and the child process just uses the parsed JSON options instead of using hasFlag.\n. Okay, I've removed the direct dependency on \"has-flag\" (although istanbul still pulls it in), and the child process just uses the parsed JSON options instead of using hasFlag.\n. Okay, fork now has the signature: fork(file: String, opts?: Object) => Promise, which means I was able to undo the changes I had to make to tests previously.\n. Okay, fork now has the signature: fork(file: String, opts?: Object) => Promise, which means I was able to undo the changes I had to make to tests previously.\n. I very thoroughly checked for \"serial\" and \"failFast\" property access before removing that assign(this, options); in api.js, but I've reverted as you suggested.\n. I very thoroughly checked for \"serial\" and \"failFast\" property access before removing that assign(this, options); in api.js, but I've reverted as you suggested.\n. It's weird that the tests were still passing when I dropped the assign(this, options) in api.js...\n. It's weird that the tests were still passing when I dropped the assign(this, options) in api.js...\n. Done. :)\n. Done. :)\n. @sindresorhus I think your \"LGTM\" came before my squash :)\n. @vdemedes in my squashed commit, I include moving that over to this.options.serial:\nhttps://github.com/jokeyrhyme/ava/blob/eeeec706c285ed30889c6bf602f7797f31720270/api.js#L150\n. :tada: :smiley_cat: \n. Is detecting Node v6 possible or an option? Would we still want babel to be part of ava when the environment is ES2015-compliant?\n. @jfmengels we might be able to install babel in a pre/post-install npm hook, and only install exactly the transforms needed to get us from the current version of node to stage-2 or whatever our threshold is. That would mean fewer dependencies for newer Nodes?\n. Is this related to #365 ?\n. Another approach might be to drop / deprecate all truthy / falsey behaviour and encourage developers to use strictly boolean expressions.\n. @sindresorhus that kitten slays me every time. So cute! Your participation in any GitHub conversation is a welcome presence. :D\n. I did get this working by explicitly annotating the types, something like this:\njs\n/* :: import type { ContextualTest } from 'ava' */\ntest('my test', (t) => { t.pass('yay') } /* : ContextualTest */)\nBut I don't want to have to do this for every single test.\nAnother interim solution would be to tell FlowType to ignore ava. /shrug\n. Just FYI: I'm having this issue with FlowType 0.36.0, too.. Oooo, nice work! Yay! <3. @btipling most recent release looks like 0.17.0 2016-11-17, whilst this was only merged 2016-12-25. Worth switching to execa at the same time?\n. Weird. I love my semi-colons, too. I wonder why ESLint didn't bust my chops about this? :)\n. Should I completely drop the \"time-require\" module from package.json dependencies and the related --sorted stuff?\n. assign(this, options); has been replaced with objectAssign(this, options);\n. Nope. In the API tests, new Api() is called without the second parameter, which means it ends up being an empty Object, without a \"require\" property. Should I change the fallback default value in api.js to include an empty \"require\" array? e.g. this.options = options || { require: [] }; ?\n. Done.\n. Done, see https://github.com/sindresorhus/ava/pull/296#issuecomment-163786259\n. ",
    "BarryThePenguin": "Would AVA provide a solution for browser testing through it's own web server, browser loader and client, or is the plan to just support, and have examples/recipes, for running tests in a browser environment?\n. Happy to write some documentation. I'll keep looking at the failing tests, I'm not sure what the problem is yet...\n. I think 'es6-symbol' is causing the tests to fail. What is the preferred solution for Symbols in 0.10 and 0.12?\n. Hmm... also zen-observable requires a Promise polyfill...\n. fyi is-ci\n. Is there a test for multiple exclusive tests in multiple files? \n. Also, noting the api change, this allows for test.skip(). Not sure how helpful that is to people\n. I'd be happy to investigate the possibility of having both a .skip() and .todo() modifier.\nThoughts so far: \n- skip has optional title, requires a test function\n- todo requires a test title, won't accept a test function\nWould also require adding todo to the tap reporter output\n. I've had a go at the todo modifier. I also had a go at implementing the tap output for both todo and skip directives\n. Just went through the review.\nAdded Mini and Verbose reporter handling for todo + tests\n. Updated. Happy to squash/rebase to make the commit history a little more sane :+1: \n. @sindresorhus sure, I'll see if I can work these out\nI'm starting to question my reasoning for initially marking todo tests as failed. I've been following the TAP spec and I'm not sure it's quite clear.\n\nThese tests represent a feature to be implemented or a bug to be fixed and act as something of an executable \u201cthings to do\u201d list. They are not expected to succeed. Should a todo test point begin succeeding, the harness should report it as a bonus. This indicates that whatever you were supposed to do has been done and you should promote this to a normal test point.\n\nThis implies that todo tests fail until they are implemented. This PR currently expects todo tests to have no implementation. Does this mean we mark them as failed? Again, I'm not sure how tightly we want AVA coupled with the TAP spec.\n. Also, thanks everyone for providing feedback and spending the time to help out. It's been a really enjoyable experience :hamster: \n. Updated.\n@vdemedes @novemberborn todo is no longer skipped as well. Instead, in test-collection we check for the skipped and todo modifier.\n@novemberborn I've added the test/api.js test\n@sindresorhus I'm not quite sure about the exit code being 1, when should the exit code be 1? These are the outputs for the examples you provided above\nmini reporter\n```\n2 todo\n```\nverbose reporter\n```\n\nfoo\nbar\n\n0 tests passed\n  2 tests todo\n```\ntest + todo example\n```\n1 passed  1 todo\n```\n. I've just spent some time building a reporter that logs to node-notifier on BarryThePenguin/ava/notifier\nedit: link\n. I think this comes back into todo and skip functionality too. I canrethink and come back with some suggestions\n. I think this comes back into todo and skip functionality too. I canrethink and come back with some suggestions\n. @alexbooker the npm docs elaborate on passing arguments to npm run-script\n. I know RxJS has tried to solve this a couple of times\nI really like the idea of having enumerable expectations.\n. I know it's intended, using is-ci, as shown in cli.js L147-L149\n435 explains the history\n. I think you need to use test.cb here. Check out Assertion planning with t.end()\n. The watch recipe goes into detail on watch mode.\nIn watch mode AVA only runs tests that have been modified. There is also an option to manually rerun all tests\n. Sure, totally understand. It was fun learning :grinning: \n. Looking at the definitions for RxJS they have a PartialObserver that is used in their subscribe() function.\nAVA could define something similar eg:\n```js\ntype PartialObserver<-T> =\n  | {\n    +next: (value: T) => mixed;\n    +error?: (error: any) => mixed;\n    +complete?: () => mixed;\n  }\n  | {\n    +next?: (value: T) => mixed;\n    +error: (error: any) => mixed;\n    +complete?: () => mixed;\n  }\n  | {\n    +next?: (value: T) => mixed;\n    +error?: (error: any) => mixed;\n    +complete: () => mixed;\n  }\ninterface ISubscription {\n       unsubscribe(): void;\n}\ninterface ObservableLike {\n    subscribe(observer: PartialObserver): ISubscription;\n};\ntype SpecialReturnTypes =\n    | PromiseLike\n    | Iterator\n    | ObservableLike;\n```\nI'm not sure if this is the best approach to move forward with though.... To allow the Observable sequence to continue after you've caught it, you'll need to return an Observable sequence to continue with.\nFrom the documentation for Observable.prototype.catch:\n\nReturn: An observable that originates from either the source or the observable returned by the catch selector function.\n\nA couple of options are, return an empty Observable after you've run your assertions\njs\ntest('Observable throws an error', t => {\n  t.plan(1);\n  const error = new Error('Failed');\n  return Observable.throw(error).catch(err => {\n    t.is(error, err);\n    return Observable.empty();\n  });\n});\nor, catch and then assert your errors by mapping over them\njs\ntest('Observable throws an error', t => {\n  t.plan(1);\n  const error = new Error('Failed');\n  return Observable.throw(error).catch(err => Observable.of(err))\n    .map(err => t.is(error, err));\n});. Ok, and the Promise related tests too? I'm thinking the end result would be:\n- test.js\n- test.Promise.js\n- test.Observable.js\n. I'm not sure if this is a Great Idea\u2122\nAny opinions?...\n. It's in test/test.js for Observables only as we want a lightweight implementation of the observable spec to test with. zen-observable \"Requires ES6 Promises or a Promise polyfill\" and this looked like a simple way to do that without also having a Symbol polyfil at the same time.\nMaybe @blesh or @zenparsing have a suggestion?\n. function names\njs\nconst bar = function baz() {};\nconsole.log(bar.name); // baz\n. What about...\njs\nstats.testCount = tests.only.length || stats.testCount;\n. It's the small things :sparkles: \n. Would it be simpler to use Promise.resolve and Promise.reject?\n. You could change map with .do(() => t.pass());. Otherwise, looks good.\nRxJS has some docs on marble testing. But I'm not sure how they would fit with AvA\n. Cool, can do. Was also wondering if || is enough, or should I explicitly be checking for typeof fn === 'undefined'?\n. Ha, great. Will do :hankey: \n. :+1: \n. I can change it. Anyone else want to clarify before I do?\n. Up until now I've been treating todo as a skipped test, that has failed, mainly based on the TAP spec. Kind of depends on how tightly integrated the runner implementation is with the reporter output.\n. This will need to change, yes\n. FYI...\njs\nvar title = 'todo';\ntypeof typeof title !== 'string';\n// === false\n:sleeping: :zzz: :zzz: \n. Any other suggestions on what to do here?\n. If the todo test needs to be explicit, I'm happy to make that change\n. Done\n. Maybe this check would be better suited in lib/runner.js#L52-L77\n. How does that compare to installing lodash and using the bellow?\njs\nvar debounce = require('lodash/fp/debounce');\n. ",
    "mattkrick": "Any thoughts on building browser compatibility against a selenium server, something like http://nightwatchjs.org/ or https://theintern.github.io/? \n. Ahhh, it was the return that got me, thanks!\nAny word on the error param? Seems like it's optional, but if supplied, it can also take the Error's name (in a regex), as well. If it's functioning correctly let me know & I'll write a little PR for the readme.\n. That works for me, thanks!\nI'll leave this open in case you wanna use it as a reminder, feel free to close. \n. Ah, that explains it then, from core-assert: \nassert.throws = function(block, /*optional*/error, /*optional*/message) {...\n. Thanks for following up on this!\nIn the throws section on the readme, it'd be nice to show an example explicitly pointing out that the statement must begin with return, that was the big gotcha for me since it's the only one that requires a return.\nWith regards to the assertion, I'd say for now just add in that error can also be optional. In a future version, it'd be nice to be able to test for Error.name as well as Error.message. Currently, I'm only regex testing on the name since I can't return two throws. As a workaround for now I can do a try/catch block, but that's just not as clean. \n. Agreed, a nice example or 2 in the readme would be awesome. I'll play around with some packages & see if any good patterns emerge\n. that's good to see! I decided to keep it simple & just use the native (or polyfilled) fetch:\n```\ntest('AuthController:signup:Success', async t => {\n  t.plan(4);\n  const res = await postJSON('/auth/signup', {email: 'AuthControllerSignup@Success', password: '123123'});\n  const parsedRes = await res.json();\n  t.is(res.status, 200);\n  t.ok(parsedRes.authToken);\n  t.is(parsedRes.user.email, 'authcontrollersignup@success');\n  t.false(parsedRes.user.strategies.local.isVerified);\n});\n//in my utils file\nexport function postJSON(route, obj) {\n  return fetch(hostUrl() + route, {\n    method: 'post',\n    credentials: 'include',\n    headers: {\n      'Accept': 'application/json',\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify(obj)\n  })\n}\n``\n. That's a good suggestion. Until now I've been able to do without one (just webpack + npm) but I guess that's the best choice. Thanks!\n. I totally forgot those existed! Oh that's awesome, cheers!\n. @jamestalmage thanks for your thoughtful explanation! Let me know if it looks good & I'll squash the commits.\n. rebased & rewritten to cover our discussion, cheers\n. yep, will get a PR out today, sorry for the delay!\n. ok, should be good to go\n. I like how you think!\nYep, it works, but don't take my word for it. Here it is in a repo: https://github.com/mattkrick/meatier/blob/master/src/server/controllers/__tests__/auth-tests.js\nFeel free to give it a download & runnpm run test. \n. Ah, I found a better way to do it. It uses the nice factory function you introduced me to, but now I use it for a server that creates a worker (and a worker has an HTTP server + WS server in it). Still not foolproof, with 3+ tests I occasionally get really strangeEADDRINUSEerrors that point to a temp file in~/var/folders` (there's a race condition somewhere, but I dunno where).  For the googlers at home: \ntest.cb(t => {\n  t.plan(1);\n  const port = parseInt(Math.random() * 65536);\n  const socketOptions = {hostname: 'localhost', port};\n  makeServer(() => {\n    const socket = socketCluster.connect(socketOptions);\n    socket.emit('foo', 'fooData', () => {\n      t.pass();\n      // close event calls `close` on the websocket in the test server. `t.end()` must come after the worker is closed, otherwise the server can stay open\n      socket.emit('close', null , () => {\n        t.end();\n      });\n    })\n  }, port);\n});\nI'll leave it open in case someone comes up with a cleaner solution, feel free to close if you want to clean up your issues.\n. That's weird, FWIW JSX works natively in my tests, not sure what the difference is: https://github.com/mattkrick/redux-socket-cluster/blob/master/tests/index-tests.js#L46\n. ohhhh good to know. you just saved me a future headache, cheers\n. Are you open to swapping out deeper for something else? I know I've used\nsimilar packages than don't check the constructor, but can't remember the\nnames...\nOn Thu, Mar 10, 2016, 10:54 AM Sindre Sorhus notifications@github.com\nwrote:\n\n[image: :+1:] Do you have a suggestion on how we could solve this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/630#issuecomment-194952194.\n. I've used deep-equal and it's really lightweight. It checks for prototype\nequality instead of constructor equality so I think it'd be the best of\nboth worlds. There's also deep-eql which is what chai uses. Thoughts?\n\nOn Thu, Mar 10, 2016, 5:39 PM Forrest L Norvell notifications@github.com\nwrote:\n\nIf you want looser constructor checking, take a look at only-shallow\nhttp://npm.im/only-shallow, which exists so that tap can follow pretty\nmuch exactly @mattkrick https://github.com/mattkrick's rationale in the\noriginal post. tap differentiates between looser structural equality (\nt.same, using only-shallow), and prototype-chain validation (t.strictSame,\nusing deeper) for those (comparatively rarer cases) in which you want to\nensure that the prototype chains match. It turns out that only-shallow\nhas slightly more complicated logic as a result, but for a test library\nlike ava, any performance consequences should be unnoticeable.\nI'd be super amused if somebody found a use for deepest\nhttp://npm.im/deepest, but probably a terrible idea for ava.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/630#issuecomment-195100574.\n. I'd still argue against having 2 sames simply because getting a JSON response (like GraphQL) will  return undefined as the constructor for every object. So even though a warning would be a step forward, it'd still require recreating the object from scratch, which would include so much code it wouldn't be practical to use it.\n. Personally, I avoid making decisions based on the constructor function & go off of the shape of the object, or a signature property, or the constructor.name as a last resort, but that's because the multiple instance problem has bitten me more times than i care to admit. If I really cared about constructor equality, I'd write a series of t.is tests to shallowly compare anyways. But my opinion is biased :smile: \n. good to know, thanks! a before would probably suffice, but I wasn't sure if it was a \"best practice\" since someone could mutate app within a test.\n\nif you don't mind, could you explain why context is needed & a let doesn't suffice? I'm guessing it's because the let won't carry through to all the child processes?\n. ahhhh, that kinda clears it up. I was thinking that beforeEach was guaranteed to run immediately before the test on that thread. So is beforeEach guaranteed to run on the same thread as the test?\nPS, I like your 3rd option of creating the app, I'll update the readme to reflect it\n. can do, PS anyone know a way to squash commits within github without doing the pull/rebase/push dance?\n. A test to make sure we ignore constructor equality would be a good one. I\ngoofed up and never added that last time.\nOn Sat, Apr 23, 2016, 2:37 PM Sindre Sorhus notifications@github.com\nwrote:\n\nIn test/assert.js\nhttps://github.com/sindresorhus/ava/pull/777#discussion_r60833744:\n\n@@ -120,6 +120,10 @@ test('.not()', function (t) {\n });\ntest('.deepEqual()', function (t) {\n-   t.throws(function () {\n-       assert.deepEqual({a: false}, {a: 0});\n-   });\n\nSure, that, and maybe some tests from\nhttps://github.com/sotojuan/not-so-shallow Just to make sure we never\nregress, even if we were to replace your lib with something else any time\nin the future (hopefully we won't though).\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/pull/777/files/43e64dcb23886dc0657e4136d6be318ddcfdcdd3#r60833744\n. function Bar(a) {\n  this.a = a;\n}\nvar y = new Bar(1);\n. \n",
    "bfred-it": "@jokeyrhyme I'm not sure about the value of generic browser testing, considering that actual browsers vary a lot. Just because something works there it doesn't mean it works in real-world browsers.\n. @jokeyrhyme I'm not sure about the value of generic browser testing, considering that actual browsers vary a lot. Just because something works there it doesn't mean it works in real-world browsers.\n. Web Workers are not good enough for all situations; even iframes might not be, since self !== top and some tests might fail because of this discrepancy.\nI think the best solutions would be using iframes, given its wide support of browsers and capabilities (DOM access, etc), plus a full mode, perhaps as an option, that requires full window access, which if we cannot open multiple tabs \"visible\" it would mean that tests become serial. \"Visible\" is the keyword here, requestAnimationFrame among others wouldn't work in hidden tabs and things sometimes behave differently.\n. Are you talking about my reference to \"visible\"? As long as the tab is the foremost tab, every iframe in it will be \"visible\". Once the test in the iframe is done, it can be removed and the output/visibility is irrelevant, I'd think.\n. Yeah, surely iframe issues are rare, but if you never know what hacks polyfills and such have to rely on. Also what happens when two iframes try to interact with the History API? That's another one\n. I'd use this on Travis+SauceLabs to verify real compatibility with browsers, rather than \"compatibility with jsdom.\"  My issue is I have no experience in setting this up, so documentation specific to AVA+selenium would be great, perhaps that's the only thing missing.\n. Yeah, sorry if I wasn't more specific. ",
    "hoschi": "@jamestalmage commented on 14 Jan\n\nIt depends on how easy it would be to modify karma to run multiple files at once. But karma would definitely be the first thing we look at.\n\nIn a first version it would be great to have serial tests, which run in a real browser. For performant dev tests you can use JSDOM or generic fake browsers. Test runs in real browser can run in a CI environment, where performance doesn't matter.\n. I give it a try with my current setup and use cases. On local (developer) machine tests get executed in jsdom environment for best performance. On CI server these tests should run in a real browser environment. If one of these tests fail, I want to be able to debug it with browsers dev tools.\n\nDo you want to see test results in the browser? In the CLI? Both?\n\nCLI or file to save it for each build.\n\nDo you start browser tests manually by navigating to a URL, or should AVA launch your browsers?\n\nBoth is needed to run a specific test in a specific browser and automate the process in a whole?\n\nHow would you run browser tests in CI? Through SauceLabs / BrowserStack / others, or local VMs?\n\nNot used these kind of tools for a long time, no real opinion here, but BroswerStack sounds good.\n\nDoes your code need to be compiled specifically to be tested in a browser environment?\n\nAs this are unit tests, I think dev compiled is ok. Productivity compiled code is for integration tests, I would say. Test code/setup should be configurable like the \"--require\" flag already does, at the moment I setup jsdom with this flag.\n. What is the problem/bug with .only and the flag?\n. Does \"properties\" also mean return values from function calls? E.g. I have this line in my test:\nt.truthy(header.find(TitleMedium).prop('title') === props.title);\nwhich should not log header.find(TitleMedium)\nI don't know how much properties you want to log for each object in a test line, but probably it makes sense to count all the lines you logged already so you can check if this exceeds a limit. At the moment the statement above logs around 5 pages of screen space :grinning: . > It's pretty hard to find the correct trade-off though.\ncorrect, because you not know how much lines a nested object produce when you log \"3 props from each, down 4 levels\". I thought that is the main problem with this approach?\nSo I suggested to do this not with such a static approach, but in a recursive like way to have more control on the output:\nt.truthy(header.name === config.title)\nhere header and config are objects which say have a lot of properties which are nested objects too, only title and name are strings. Then this could be an example of the idea:\n\nrender name and title (2 lines)\nprocess first level of props of header and config, there is still a config for this e.g. process only 3 props\nif prop is object or array, put it on stack\nif prop is string, number, bool, date (one liners) render them\n\n\ncheck if line count exceeds limit\nprocess item from stack\nrecursive into properties which are of type array or object\n\nWith this you would make sure that \"small\" props (bool, date, string, ...) are rendered with an higher priority. With checking the rendered lines before recursing one level deeper, you know if you have already rendered a screen page of information and can stop here. This can be configurable to match font configs in IDEs/terminal.\nI hope it makes more sense now :grinning: . ",
    "novemberborn": "Some quick thoughts given #887:\n- It seems sensible for AVA to spawn a proxy server to serve tests, sources and other fixtures\n  - Fixtures need to be served with correct mime types\n  - Transpilation should happen in AVA, shipping babel-register to the browser seems fraught\n  - We'd somehow need to bundle the required JavaScript, figure out compatibility with the various loaders, etc\n- Certain use cases can be solved by printing a URL to access the proxy\n  - We may need to be able to serve with HTTPS certificates\n- Other use cases would involve AVA controlling browsers, e.g. Saucelabs, WebDriver, etc\n  - What can we learn / use from Karma?\n- AVA would have to collect code coverage\n- Let's not get hung up on running test files concurrently\n- However each test file still needs a unique environment\n  - Do we sandbox in iframes?\n  - Or reload between tests?\n- I'd be happy for test results to be reported back to the terminal, and in the Dev Tools console, but not shown in the web page itself\n. I'm curious what kind of browser support people are looking for.\n- Do you want to see test results in the browser? In the CLI? Both?\n- Do you start browser tests manually by navigating to a URL, or should AVA launch your browsers?\n- How would you run browser tests in CI? Through SauceLabs / BrowserStack / others, or local VMs?\n- Does your code need to be compiled specifically to be tested in a browser environment?\nPlease add anything else that seems relevant to the discussion.\n. @mAAdhaTTah we're still evolving AVA's internals too much to really commit to browser support, sorry.. Hey all, it's hopefully become apparent since this issue was first opened that browser support is not a priority for AVA. I am therefore closing this issue.\nWe are interested in letting you use AVA to drive browser tests, through Karma or other solutions. If you've got experience with this we'd love to see updates to the browser testing recipe, or indeed new recipes.\nIf you want to help out integrating browser tests by building on top of AVA then please get in touch. We don't have the resources to spearhead these efforts ourselves, but we'd love to include mature solutions in our GitHub and npm organizations, or promote them in recipes. Forking karma-ava might be a good start.\nThank you all for your interest in AVA and participation in this issue \u2764\ufe0f . Hey all, it's hopefully become apparent since this issue was first opened that browser support is not a priority for AVA. I am therefore closing this issue.\nWe are interested in letting you use AVA to drive browser tests, through Karma or other solutions. If you've got experience with this we'd love to see updates to the browser testing recipe, or indeed new recipes.\nIf you want to help out integrating browser tests by building on top of AVA then please get in touch. We don't have the resources to spearhead these efforts ourselves, but we'd love to include mature solutions in our GitHub and npm organizations, or promote them in recipes. Forking karma-ava might be a good start.\nThank you all for your interest in AVA and participation in this issue \u2764\ufe0f . @bj0 you'll have to stub those APIs in Node.js. Have a look at our browser testing recipe \u2014 the libraries mentioned there may already implement these APIs. (Or not, I haven't actually checked for you, sorry.). I think we've become quite comfortable with AVA's built-in assertions since this issue was raised. We won't be adding tight integration for other assertion libraries any time soon, so I'm closing this issue.\n. Relative imports are more cumbersome than relative file paths. I like setting cwd to the package.json directory, though that might be a little tricky for monorepo's.\n. @a-s-o let's discuss your suggestion here, not in https://github.com/avajs/ava-codemods/issues/19#issuecomment-220839269 :)\n\nHi, may I also request not using the nearest package.json as the CWD (i.e looking package.json upwards from the test file). This may cause issues in multi-package repos where the test runner is in the root but there are multiple packages with their own package.json in the project (example the babel repo).\nI would like to suggest sticking with the directory where ava command is executed as that is the most obvious.\n\nCurrently we look at the package.json to load AVA config. We start the search for this file in the current working directory, so if you're executing AVA in the monorepo root, it would use the root's package.json. I imagine we'll do the same when determining the CWD for the test files.\nIs this what you mean by \"sticking with the directory where ava command is executed\"?\n. > So, my suggestions is as follows:\nThat's how it would work, but due to the presence of the package.json.\n\nBasically, cwd should be the working directory where the ava command was launched.\n\nThat becomes a problem when you launch ava from project_root/api/tests. We try to pick a consistent directory no matter where you start the tests from. Originally that was the directory the test file was in, but now it'll be the directory the package.json is in.\nThis behavior breaks down a little for monorepos with multiple package.json files, but to be fair so does npm test which doesn't look past the first package.json file.\n. \ud83d\udc4d \n. Fixed by 476c65396155a7d16fbdd55c65863ecaabd873de.\n. I'd be happy to take this on. I've used watchers extensively when building dev stacks.\n. I don't know enough about modern scheduling regarding forks and CPU cores, but presumably we want to max out CPU so tests run faster and spinning up as many forks as possible achieves that. It'd be more concerned with available RAM and causing swap. That's probably heavily dependent on the code under test though.\nThere might be an argument for specifying the number of concurrent forks if this is a concern, however it would limit us in other ways. Currently we gather tests from all forks before running any of them, and things like .only are supposed to apply across forks. We might have to carefully spin forks up and down to do this analysis and then start the actual test run, assuming the test files don't change in between. (I guess we could copy them to a temporary directory?)\nWe've worked hard to reduce Babel overhead for the test files themselves but users probably still run into this themselves using custom --require hooks. Conceivably we could provide a framework where even those hooks are loaded in the main process, similarly to the test files. An alternative approach would be to transpile in a separate build process (this is my preferred approach).\n. Tracking my last suggestion in #577.\n. It's become clear recently (#604 #669) that AVA's current forking behavior is problematic. React projects especially end up with a lot of test files and consequently a lot of forks. This can make tests impossible to run on dev machines, let alone CI systems.\nCurrently AVA forks for each test file. Stats are collected (number of tests, whether any tests are exclusive). Once all stats have been received all tests from all files are run. Within a test file the tests may execute serially, and tests may execute serially across all files. Regardless the fork persists until it's finished running its tests.\nThe simplest approach would be to only run tests from a few forks at a time, but still fork for all test files before any tests are run. However I doubt this will be sufficient, especially for projects with dozens of test files and a babel-register dependency.\nUnfortunately we can't fork just a few test files and run their tests since we won't know whether other test files contain exclusive tests. I see two options:\n1. fork, exit, fork again: fork for each test file to gather stats, then exit that fork. Fork again to run the tests. We could keep the last X forks alive and immediately run the tests from those forks.\n2. use static analysis to detect exclusive tests. We're transpiling the test files anyway prior to forking. We can't really use this to compute other test stats but I don't think we need to know those prior to running the tests.\nStatic analysis seems preferable as we won't have to fork unnecessarily.\nThe next issue is how many forks we should run concurrently. The simplest approach would be to pick a per-core number. That number could probably be larger than 1, allowing the OS to do context switching when the processes are blocked on IO. We'll probably bump into memory limits though. And if the number gets too great then there might be too much of an overhead due to the context switching.\n(Please note that I'm far from an expect on this subject!)\nIf we make this configurable then that config should work on CI machines and the various machines of a project's collaborators.\nI wonder if instead we can observe the system impact of the forks and dynamically ensure the best utilization.\nThoughts?\n. Tracking the management of the forks in #718.\n. @ORESoftware yup, that's how it works. The problem is executing all child processes at once.\n. @dcousineau yup, see #577.\n. @sindresorhus is there any more feedback you're looking to collect in this issue, or shall we close it?\n. @sindresorhus @vdemedes @jamestalmage is there anything we want to improve here beyond the existing --require support? If not I'll close and remove the link to this issue from the README.\n. > Instead of \"Disable Babel\", How about an option to disable the lookup of the local version, and force ava to use whichever version it shipped with.\n390 takes care of this. Closing.\n. @callumlocke try with \"ava\": { \"babel\": {} }. That prevents AVA from applying the default presets. It will still run Babel though, as it needs to add power-assert transforms and some other helpers.\n. @callumlocke try with \"ava\": { \"babel\": {} }. That prevents AVA from applying the default presets. It will still run Babel though, as it needs to add power-assert transforms and some other helpers.\n. > However the suggestion requires adding a new devDependency on a project that otherwise has no need for babel.\nYou could use CJS' require() rather than import.\n\nFor me, at the time I originally posted in this thread, the \"no babel\" idea was purely in the interest of reducing the time it takes to test.\n\nAVA will still apply a Babel transform, I doubt disabling the default presets will dramatically speed up your tests.\n. @paulmillr could you elaborate?\n. Let's move the discussion to #1043.\n. Just saw this: https://github.com/gabmontes/babel-preset-latest-minimal\n. Is fixed by #1193.. I believe this was fixed by #390.\n. @platy11 that line is in master but not yet in a release.\n. @wildaces215 you can browse our current code coverage at https://codecov.io/gh/avajs/ava. Look for a file that has little coverage, try and figure out what it does, and see if you can write a test that extends the coverage.\nWe'd be interested in covering larger pieces of logic rather than small edge cases.. @yowainwright still not a priority until we ship a 1.0 release. We do need to reorganize the documentation though, I think mostly by splitting it out from the main README. You might be interested in the discussion in #1454.. Closing this until we're in a place where we'd want a website.. Closing this until we're in a place where we'd want a website.. Since we run many tests concurrently I'm not convinced the timeout should apply to individual tests. Rather it should be an idle timeout, e.g. we're still waiting for at least 1 test and it's been 30 seconds since the last test completed.\nFurther when AVA is interrupted we could print out which tests are still pending. Even without defining a timeout this would let you debug things.\nThis may deal with the CI issue as well, assuming it interrupts AVA when killing the CI run.\n. Changed title to \"Add idle timeout support\". Presumably --idle-timeout, time in seconds after the last test completed that remaining tests should be aborted.\nWould be cool if it could print out which tests were aborted.\nI've opened #583 to track the interrupt idea.\n. > Would we measure that globally, or per test file?\nGlobally. I only care about lack of progress.\nI suggested --idle-timeout before but that seems misleading. The tests aren't idle, they're just not progressing. Just --timeout is probably sufficient.\n. Done via #654!\n. @jamestalmage,\n\nDo we want to add a flag or environment variable to address this?\n\nYou mean to change the effective working directory of the test files?\nI think generally code should walk up the directory tree looking for the file it wants, or otherwise resolve relative to the package.json (which it needs to find by walking up the tree\u2026). I don't think AVA should change it's behavior.\nClosing this now given the lack of discussion over the past three months.\n. Also see discussion in #445.\n. There's also https://github.com/SitePen/remap-istanbul. This seems to work for me:\nnyc --reporter=json npm test\nmv coverage/coverage-final.json coverage/coverage.json\nremap-istanbul -i coverage/coverage.json -o coverage/coverage.json\nistanbul report lcov\n. @jamestalmage it's a different issue. Code coverage is computed over the compiled output, so the stats will be wrong and it's hard to see which line in the original code was not covered. remap-istanbul uses source maps to compute the coverage of the original source.\nThe approach I mentioned above works for my project, which uses a watcher to transpile source files and write them to disk, with their source maps. That output is what's tested. It not quite a one-line though and it depends heavily on your project is set up.\nThis isn't an issue with AVA though.\n. > This isn't an issue with AVA though.\nBy which I mean, since AVA isn't responsible for the coverage reports, it's not a bug with the project. Workarounds could be explained (maybe I should do so! :wink:) but it's fairly tricky.\n. Here's the writeup of my workaround: https://medium.com/@novemberborn/code-coverage-with-babel-istanbul-nyc-83b8c2f1093\n. > We might find that the nyc -> AVA -> babel-core/register interaction is an issue (perhaps nyc will need to store a stack of require hooks).\nhttps://github.com/bcoe/nyc/pull/65 takes care of that.\n\nnyc delegates to AVA's require hook which will return the transpiled code with a source-map appended.\n\nThis might have to be documented better on nyc's side, I'll look into that.\nThat said, the test files don't need to be instrumented, the source files that are being tested do. It would help though if AVA's documentation could explain the requirements so users don't have to read up on nyc. I'll look into that when things settle with nyc.\n. > We might find that the nyc -> AVA -> babel-core/register interaction is an issue (perhaps nyc will need to store a stack of require hooks).\nhttps://github.com/bcoe/nyc/pull/65 takes care of that.\n\nnyc delegates to AVA's require hook which will return the transpiled code with a source-map appended.\n\nThis might have to be documented better on nyc's side, I'll look into that.\nThat said, the test files don't need to be instrumented, the source files that are being tested do. It would help though if AVA's documentation could explain the requirements so users don't have to read up on nyc. I'll look into that when things settle with nyc.\n. @sparty02 what Node and npm versions are you running?\n. @sparty02 wait you're on Windows right? nyc's Windows support isn't quite there yet, we'll be tracking progress in https://github.com/bcoe/nyc/issues/81.\n. Let's continue discussion in #446. Any future improvements will benefit from our solution to #25.\n. Please see #448.\n. Given the discussion in #292 I'm not sure AVA even needs to use the Babel that's in user's projects. AVA should ship with its own Babel dependency for compiling test files. If users want different behavior they should be able to override test file compilation, which will enable them to use any Babel version they want.\n. Given the discussion in #292 I'm not sure AVA even needs to use the Babel that's in user's projects. AVA should ship with its own Babel dependency for compiling test files. If users want different behavior they should be able to override test file compilation, which will enable them to use any Babel version they want.\n. @sindresorhus npm won't be able to dedupe if the user depends on a version outside of the SemVer range of AVA's dependencies. Which is effectively where we are with Babel 5 at the moment. Babel 6 is incompatible and shouldn't be used. In the future a user won't be able to force Babel 7 either.\nMy understanding of #292 is that AVA provides certain transformations of the test source. If different transformations are required the best way to achieve that is by swapping out AVA's transform process entirely.\nIf that's the way to go then AVA already shouldn't be pulling in the Babel dependency from the user's project.\n. @sindresorhus npm won't be able to dedupe if the user depends on a version outside of the SemVer range of AVA's dependencies. Which is effectively where we are with Babel 5 at the moment. Babel 6 is incompatible and shouldn't be used. In the future a user won't be able to force Babel 7 either.\nMy understanding of #292 is that AVA provides certain transformations of the test source. If different transformations are required the best way to achieve that is by swapping out AVA's transform process entirely.\nIf that's the way to go then AVA already shouldn't be pulling in the Babel dependency from the user's project.\n. AVA uses Babel 6 now, and if necessary should work with Babel 5 source projects.\n. Best I can tell, the lack of this feature hasn't really been an issue. There's only a select few use cases where user code that is actually called by a test (rather than setup code) later causes an uncaught exception. Given our detection of tests that remain pending or do not run any assertions, and our t.throws() handling, we can cover most cases. A good approach to debug other cases is to run tests serially.\nI'm closing this for now. It'll be interesting to see where Async Hooks ends up and whether it's useful for us, but that's for future Node.js releases.. > From a quick skim this looks like the same as #62 just with a different name.\nAh yes, apologies for not finding that myself.\n\neven if we are going to add this, it's not going to happen right now. I want to focus our efforts on making a really good minimal core first.\n\nFair enough.\n\n\nIf you think you need nested tests or groups, you're probably better off splitting it up into multiple files (and get the benefit of parallelism) or simplifying your tests.\n\nI'm writing some tests for functions that have reasonably complex behavior, based on stub objects that were set up in a beforeEach. Unfortunately these tests aren't easy to write or maintain. The best approach I've found is to define the various code paths in a single file with nested groups (using Mocha's describe/context helpers), each group inheriting the parent context and then refining it for the particular tests within it. Somehow breaking this up into many different files would just lead to a lot of repetition.\nI'll see if I can think of a sane way to set up these tests with multiple files though.\n. > We'll still consider this, but we have a lot on our plates, and maintaining something like this with the level of code churn we currently have is not something I want to take on right now.\nI appreciate that. I may be able to help once the code churn settles down.\n. @spudly I like that! Another option would be to use .after to register an after handler without having to make test return a promise:\njs\ntest('getCwd() returns the current working directory', t => {\n  setup();\n  t.is(getCwd(), '/fake/path');\n}).after(teardown);\nOne nicety of groups though would be to use a beforeEach that modifies t.context. You could chain those and not have to worry about calling setup() in each test.\n. I've gotten considerably more experience writing tests in AVA than I had when I raised this issue back in November. I've now come round to the idea that nesting tests is an anti-pattern. Keeping the test hierarchy in your head takes considerable effort, especially in larger test files. Breaking up tests across multiple files is less obvious because that breaks the hierarchy. It seems easier to write tests for tests sake.\nIf this reeks too much of personal feelings that can't be backed with hard evidence then rest assured there's also a more technical reason it's difficult for AVA to support nested tests: it starts executing all your tests at once. Consequently it can't output a test hierarchy in real time. Taking @halhenke's example, AVA might give you the following output:\nComponentA:\n    does something else\n  does that\n  when its raining:\n  does this\nava-spec works by repeating the hierarchy on each line:\nComponentA:  does this\nComponentA:  does that\nComponentA:  when its raining: does something else\nSo if you prefer the test syntax ava-spec is the best solution for you.\n(Also, AVA already prefaces the test output with the filename, so you don't even need to mention it in your test titles.)\n\nThe above doesn't help if you're nesting your tests in order to for those tests to have ever more refined context.\nAgain, nesting can lead you down an unnecessarily complicated path. In many cases simple helper methods are sufficient. Test macros are also very useful.\nBut, rightfully complex test suites do exist. Composing helper functions is possible but problematic, especially if you need to clean up part of the context after tests run.\nI'm currently experimenting with ways to set up these context hierarchies separately from the test hierarchy. (Incidentally for the project that caused me to open this issue in the first place.) I'll check back in here once that's further along.\n. > I guess my comment mostly stemmed from the fact that the Mocha/RSpec file seems to provide a nice way to organise/describe what you are testing & from what I've seen of Tape/AVA the alternative is to shove more info into the title string. It may be a very superficial thing and/or maybe just poor usage on my/others part but it is something that i miss.\n@halhenke fair enough. Hopefully ava-spec will work for you. Once we figure out a good technique for creating context hierarchies tools like ava-spec can help with nesting beforeEach calls etc.\n. @c0b have a look at https://www.npmjs.com/package/ava-spec. It doesn't support everything but maybe it's sufficient for your use case.\n. I ported my Buoyant project from Mocha to AVA. It's this project that made me open this issue in the first place.\nI found that explicitly calling helper functions in my tests, as well as using test macros, removed most of my need for nesting. The flat structure actually helped reduce repetition and made it easier to grok what was being tested. That said the tests are still really complex, but IMO that's due to what I'm trying to test. (For those looking at the test code, I did overlook that test() can take an array of macros\u2026)\nI did have some use cases where I wanted to refine test context and run tests within that refined context, see example. I wrote some helper code to allow forking the context.\nIf we implement this in AVA the interface would be something like:\n``` js\nimport test from 'ava'\ntest.beforeEach(t => {\n  t.context.forAllTests = true\n})\nconst refined = test.fork().beforeEach(t => {\n  t.context.refinedTestsOnly = true\n})\ntest(t => {\n  t.true(t.context.forAllTests)\n  t.false(t.context.refinedTestsOnly)\n})\nrefined.test(t => {\n  t.true(t.context.forAllTests)\n  t.true(t.context.refinedTestsOnly)\n})\n```\nava-spec would also be able to use this infrastructure to implement nested beforeEach() calls etc.\n@avajs/core thoughts?\n. @jamestalmage \n\nCan we also make sure we are covering the case where the test file itself may have a source map?\n\nTo clarify, do you mean a case where the test file itself is the result of a compilation step, and then AVA's Babel hook transpiles it again? So we need to provide the input source map, if any, making sure the output source map can map all the way back to the original source?\n. @jamestalmage that PR seems like a good step to at least avoid not receiving any stack trace.\nI'll try and reproduce the issue, see if I can step through the source-map code to find out more.\n. The compiled coffee-script code means that there is no async/await for AVA to compile. It just sees a test function which returns a promise. So that's not the issue.\n0.7.0 contains https://github.com/sindresorhus/ava/pull/273 which passes an input source map to the Babel compiler used by AVA. If I disable the input source map I see this source map for Babel's output:\njs\n{ version: 3,\n  sources: [ '/private/var/folders/_6/p8qxp_3n62zg9081tvb0lcc80000gn/T/tmp.TAYPmrUt/test2.js' ],\n  names: [],\n  mappings: ';;;AACA,CAAC,YAAW;AACV,MAAI,OAAO,EAAE,IAAI,CAAC;;AAElB,MAAI,GAAG,OAAO,CAAC,KAAK,CAAC,CAAC;;AAEtB,SAAO,GAAG,OAAO,CAAC,UAAU,CAAC,CAAC;;AAE9B,MAAI,CAAC,OAAO,EAAE,OAAO,CAAC,SAAS,CAAC,WAAU,CAAC,EAAE;AAC3C,WAAQ,MAAM,OAAO,CAAC,KAAK,CAAC,GAAG,CAAC,CAAE;GACnC,CAAC,CAAC,CAAC;CAEL,CAAA,CAAE,IAAI,WAAM,CAAC',\n  file: '/private/var/folders/_6/p8qxp_3n62zg9081tvb0lcc80000gn/T/tmp.TAYPmrUt/test2.js',\n  sourcesContent: [ '// Generated by CoffeeScript 1.10.0\\n(function() {\\n  var Promise, test;\\n\\n  test = require(\"ava\");\\n\\n  Promise = require(\"bluebird\");\\n\\n  test(\"peter\", Promise.coroutine(function*(t) {\\n    return (yield Promise.delay(500));\\n  }));\\n\\n}).call(this);\\n\\n//# sourceMappingURL=test2.js.map\\n' ] }\nWith the input source map I get:\njs\n{ version: 3,\n  sources: [ 'test2.coffee' ],\n  names: [],\n  mappings: ';;;AAAA,CAAA,YAAA;AAAA,MAAA,OAAA,EAAA,IAAA,CAAA;;AAAA,MAAA,GAAO,OAAA,CAAQ,KAAR,CAAA,CAAA;;AACP,SAAA,GAAU,OAAA,CAAQ,UAAR,CAAA,CAAA;;AAEV,MAAA,CAAK,OAAL,EAAc,OAAO,CAAC,SAAR,CAAkB,WAAC,CAAD,EAAA;ACM5B,WDLF,MAAM,OAAO,CAAC,KAAR,CAAc,GAAd,CAAN,CAAA;GADY,CAAd,CAAA,CAAA;CCSC,CAAA,CAAE,IAAI,WAAM,CAAC',\n  file: 'test2.js',\n  sourcesContent:\n   [ null,\n     '// Generated by CoffeeScript 1.10.0\\n(function() {\\n  var Promise, test;\\n\\n  test = require(\"ava\");\\n\\n  Promise = require(\"bluebird\");\\n\\n  test(\"peter\", Promise.coroutine(function*(t) {\\n    return (yield Promise.delay(500));\\n  }));\\n\\n}).call(this);\\n\\n//# sourceMappingURL=test2.js.map\\n' ] }\nNote that sources now points at test2.coffee rather than test2.js and that sourcesContent now has two entries, the first being null.\nTurns out Bluebird throws an error when it's async module is loaded. This then causes a crash from source-map-support.\nThis seems like a bug in Babel's handling of input source maps. Maybe it's already fixed in Babel 6.\n. > This is a generator/yield example, and coffee-script outputs an actual generator function, (i.e. function *() {...}).\nIt outputs test(\"peter\", Promise.coroutine(function*(t) { actually. I haven't checked but I assume that returns a function which returns a promise. The test passes if input source maps are disabled.\n\nThat is wrapped in a try/catch. Any crash of source-map-support would be caught.\n\nAh yes that's the call to prepareStackTrace. The error is stored in Async.firstLineError which is used in the final build for debugging purpose (see build tool line). And that's when it crashes.\n\nCan you submit an issue on the Babel tracker? (or hunt down an existing one and link it here).\n\nI'm thoroughly confused regarding their code base and newfangled issue tracker, so that may be tricky. Even if it's not yet fixed in Babel 6 I doubt they'd backport any fixes?\nI'll see if I can reproduce the transformation with Babel 6 instead. That'll be a good start.\n. Rather than depending on coffeescript is it more illustrative to code up a simple .txt \"compiler\"?\n. > I am not sure this is the right strategy. While it provides an easy path to use ES2015 goodness in your coffee-script tests, there will likely be scenarios where users want to specify their own Babel setup, and just use our runner.\nThis would work though wouldn't it? I bring my own Babel set up, and make sure my require hook is loaded first. Then AVA compiles power assertions and async/await on top of that. I don't have to worry about configuring either of those and AVA doesn't have to care how I write my code.\n. > Perhaps (with Babel 6), we could fallback to only running the power-assert transform, instead of the full es2015 preset.\nThat's what I was thinking, yes.\n\nThe more I think about this, we really can't run the power-assert transform after regenerator or asyncToGenerator transforms. power-assert needs to see the original AST - it uses it to pass a copy of the source to it's renderers.\n\nYes that sounds like a show-stopper for this. Too bad! (Unless we revert that line using a source map\u2026 but that gets too complicated.)\n\nI am beginning to wonder if it would not be best to just adopt a \"take it or leave it\" approach to the transform setup we provide. If users want to provide their own transform they need to provide a complete one, and we just don't touch it. We can provide documentation (and possibly convenience methods) for including the power-assert transform.\nAt a minimum, I think we need to provide a way for them to tell us to skip our transform via a flag in the CLI.\n\n:+1: \n. @scottmas it's pretty much impossible for AVA to synchronize before/after function hooks in their current form, hence our advocating of pretest and posttest scripts. #1366 has an intriguing suggestion for managing resources during an AVA invocation. Somebody would have to flesh out a proposal on quite how that would work, but I'd be open to that.. @scottmas it's pretty much impossible for AVA to synchronize before/after function hooks in their current form, hence our advocating of pretest and posttest scripts. #1366 has an intriguing suggestion for managing resources during an AVA invocation. Somebody would have to flesh out a proposal on quite how that would work, but I'd be open to that.. I don't think that provides enough value over wrapping AVA's invocation in a shell script, however messy that shell script may be.. I don't think that provides enough value over wrapping AVA's invocation in a shell script, however messy that shell script may be.. nyc@5.0.0 is out so I've updated the copy to reflect.\n. @vdemedes no problem! :smile: \n. @sindresorhus thanks! Semi-colons added.\nAny idea why xo didn't flag this? I ran npm test before submitting\u2026\n. > I realized reporters want to know the assertions for each test.\nDo you mean t.deepEquals etc?\n. Ohhhh that makes sense. And is awesome.\n. Can't recall any recent AppVeyor flakiness. Closing.\n. Should we track the remaining work in some issues rather than this PR?\n. Should we track the remaining work in some issues rather than this PR?\n. @jamestalmage perhaps you could open issues to track remaining work, and then close this PR?\n. This seems to be a duplicate of #324.\n. > Another problem on AVA side: CachingPrecompiler produces source map, but... debug doesn't work in any case because generated source file doesn't contain sourceMappingURL AVA CachingPrecompiler must add sourceMappingURL explicitly.\n@develar this was resolved in #662.\n. Closing this since there are PRs, but they're effectively blocked by #1043.\n. Closing in favor of #148.\n. @gajus we need some changes to our globbing to make this happen. See #736 and https://github.com/avajs/ava/issues/736#issuecomment-228608287.. One could argue the same for assuming test/*.js folders contain AVA tests. We think it's a good default, and there is an open issue that would make it possible for the test/helpers folder to not be treated as containing helpers.. @sotojuan it's up for the taking. Maybe we need to bikeshed the standardization details, though we could do that in a PR as well if you have a good idea on where to start.\n. While this might improve our code quality, it's not actually preventing any functionality. We've also not made progress on this refactor in over a year. I'm closing the issue, we can deal with it organically when the need arises.. Closing due to inactivity. #148 is the issue for this feature. Would love your help if you have the time @ariporad!\n. Closing due to inactivity. #148 is the issue for this feature. Would love your help if you have the time @ariporad!\n. What are we hoping to achieve here? With changing dependencies this is a never ending struggle.\nThat said, looks like all Babel dependencies require babel-runtime@^5, whereas we explicitly install ^6. There's FIFTY copies of babel-runtime@5.8.35.\nPresumably we could downgrade to ^5 for the time being and we'd save a lot space. In my testing that takes us from 77MB to 29MB.\nOf course there's other dependencies doing weird stuff, like fsevents which pulls in 7MB just for node-pre-gyp.\n\nTo measure I did a fresh install from NPM (in an empty directory) and ran du -k -d 1 node_modules | sort -n -r. Also check out snyk-resolve which shows the actual dependencies, not deduped.\n. > Pretty much have this still open for when I have time to do the same here.\nSure, fair enough. Somebody should write a tool for that :stuck_out_tongue_winking_eye: \nAlso thinking about it more I'm doubtful we can change our babel-runtime version. Presumably the runtime transformer plugin builds against v6, not v5.\n. > Also thinking about it more I'm doubtful we can change our babel-runtime version. Presumably the runtime transformer plugin builds against v6, not v5.\nActually if we have a dependency which merely reexports babel-runtime it would allow npm to dedupe babel-runtime@5 instead.\n. https://github.com/siddharthkp/cost-of-modules is also a nice tool:\n```\n$ cost-of-modules --no-install --less\nCalculating...\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name                           \u2502 children     \u2502 size   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 babel-preset-stage-2           \u2502 57           \u2502 23.66M \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 babel-core                     \u2502 35           \u2502 15.96M \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 babel-preset-es2015-node4      \u2502 33           \u2502 11.36M \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 chokidar                       \u2502 130          \u2502 5.44M  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 babel-runtime                  \u2502 2            \u2502 3.82M  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 babel-plugin-espower           \u2502 9            \u2502 2.33M  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 babel-plugin-ava-throws-helper \u2502 6            \u2502 1.18M  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 jest-snapshot                  \u2502 10           \u2502 0.87M  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 bluebird                       \u2502 0            \u2502 0.58M  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 meow                           \u2502 25           \u2502 0.30M  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 + 66 modules                   \u2502              \u2502        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 76 modules                     \u2502 345 children \u2502 44.53M \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```. With npm 5.3, in a temp directory:\n```\n\u276f npm init --yes\n\u276f npm i ava\n\u276f npx cost-of-modules --no-install --less\nnpx: installed 16 in 2.28s\nCalculating...\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name      \u2502 children     \u2502 size   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ava       \u2502 750          \u2502 46.04M \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1 modules \u2502 413 children \u2502 17.22M \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\nThe final size matches what Finder reports. I think that's pretty good! Plus, installation times are a lot better with npm 5.\nWhilst we can always improve this, I don't think there's much use in keeping this issue open.. Closing this since the initial issues have been addressed in AVA or are issues with third-party dependencies. Future suggestions on how we can make AVA easier for you to hack on are most welcome though!\n. @twada @jamestalmage perhaps you could provide some insight into this?\n. AVA now prints all values.. How does #394 help with long stack traces? Presumably this only helps us if there is a crash in the test worker? Is it meant to help debug crashes in before/after hooks?\nOther than that it helpfully enables long stack traces for users who also rely on bluebird, but that seems more like an undocumented side-effect.\nIf the stack traces are useful for end-users then we should close this issue IMO.\n. @jamestalmage what are our next steps on this?\n. The issue here is that our TAP output does not include the power-assert graph. I'm not sure if that's even possible within the TAP specification, and how that would interact with say tap-nyan. #723 suggests using the TAP v14 specification which may or may not be a prerequisite for resolving this.\nSee also #324.\n. We now do include the formatted values (based on power-assert) to the TAP output. It's a proprietary format and thus not very useful, but that's a separate concern.. :+1: \n. :+1: \n. Sounds like a reasonable idea. We could then wrap babel-runtime to reduce installed package size too.\n. Another one: if you run AVA in Docker as part of your CI, you need to forward the appropriate environment variables (see https://github.com/watson/is-ci/blob/master/index.js), or configure the verbose reporter. See #751.\n. - [ ] Cached files being used after configuration changes that AVA did not detect\n. I've run into this lately. It makes it getting tests to pass quite cumbersome. Maybe we should step up the priority?\n@twada are you saying this should be fixed in power-assert-renderers? Is it this issue: https://github.com/twada/power-assert-renderers/issues/1? What can we do to help?\n. @twada cool, I just started watching the new repo :smile: \n. @hughrawlinson that's in the release, so yup, this issue can be closed. Thanks for reminding us.. @mattkrick would be great to have a recipe for testing servers like this.\nClosing though as I'm cleaning up the issues :smile: \n. Thank you @davewasmer for your input. We are prefixing lines with non-color indicators, so I think we're covered.\nIf anybody comes across this issue because we've ended up with unreadable output please let us know. We care, you matter.. Is this something we need to solve, or a pitfall that should be documented? (See #404).\nPresumably invoking with --serial and using console.error() gives you synchronous output. Alternatively you can use t.only(). Normaly console should only be used for debugging anyway.\n. @0x1mason what's your experience using this t.comment() interface, versus relying on console.log()?\nSupporting console methods would have the least developer friction, but is a lot trickier given asynchronous test methods. Reading the thread here it sounds like we were hoping for a solution that would work with console methods, but that hasn't happened in 11 months so having a t.log() / t.comment() solution would be great to have now. (We can bikeshed the naming later.). > Nobody on my team has complained about having to use t.comment instead of console.log, but it's a pretty tiny sample of Ava users. :)\nSure, but you've already been experimenting with it so that's useful input.\n@avajs/core any thoughts? I'd be happy to see t.comment() land, as proposed by @0x1mason.. Cool. All yours @0x1mason!. For nyc @jamestalmage wrote forking-tap which splits a single test file into a bunch of files, one per individual test. Then tap can run the tests in isolation. This was critical since nyc modifies the process its run in.\nA forking-ava pretest step could do the same for us.\n. > Perhaps the solution might be to use a fork method sugar:\n:+1:\nOf course at that point you could also make separate test files for the one or two forks you need.\n. This just seems far too niche and therefore destined to be a low priority issue that we'll never land. I'm closing this issue.\nIf you're reading this and are thinking \"but this would be perfect for my use case\" please chime in.. This just seems far too niche and therefore destined to be a low priority issue that we'll never land. I'm closing this issue.\nIf you're reading this and are thinking \"but this would be perfect for my use case\" please chime in.. @lavie thanks for your comment.\nI'd put each test in its own test file. Or, alternatively, write a fixture that is run in a child process. Then you can have all the tests in one file but still control the streams inside the fixture. (You'd probably want to use test.serial to avoid spinning up too many processes all at once.). #448 should let you specify an empty babel config. If I'm not mistaken this makes Babel a no-op, achieving the same as a --no-babel flag.\n@sindresorhus I suppose we could close this in favor of #448? Could make a new issue for documentation purposes.\n. @yatki whilst I agree that the TAP output isn't as useful as we'd might like, I'm still reluctant to support custom reporters within AVA. I'd be interested in providing high-fidelity NDJSON reports, properly specified and implementable by other test runners, that custom reporters could then be build on top of. However we currently do not have the bandwidth to even begin undertaking such a project. Similarly we don't want to support more reporters within AVA.. > We could consider including more AVA specific info in the YAML data block in the TAP output.\n@sindresorhus, we could, but to what end? Reporters need to be written specifically for AVA. We'd probably want to scope it under an \"ava\" key anyway.\n\n@yatki are you referring to an NDJSON report spec?. @yatki I'm just not sure it's worth the trouble of adding extra, custom information to the TAP output. Third-party tools need to recognize our properties, and we'd have to document them, and we have to maintain compatibility. It's a lot of trouble, and the resulting tool chain would be quite brittle.\nAnd though the answer may be \"we need a new standard\" that too is a lot of trouble, does not guarantee interoperability, requires effort from third-party tools, requires us to maintain compatibility, etc\u2026 still, it seems like a more solid approach for the long term.. > I'll fork the project and implement my own reporter which is going to bring a bit pain, in terms of keeping ava updated.\nI appreciate your needs, @yatki, but given how the reporters are currently implemented, maintaining this is currently too much of a burden. We have more important issues to resolve so we can get a 1.0 release done. It'll be good to learn from your experiences though.\n\nFor instance, which tests are constantly taking more time or failing more often.\n\nFailure rates you could do using the TAP reporter. Timing information is a lot trickier for the common use cases, given that tests are run concurrently.\n. IMO the user should pick a promise library if they want more features than come with the ES2015 built-in (or core-js fallback implementation). This probably has to be consistent across their source and test code, meaning we can't replace the Promise references in the test files with Bluebird.\nI don't think there's anything actionable for us here so closing for now.\n. I'm going to guess that source-map-support only rewrites the stack, not the error message.\nMaybe power-assert could recognize the prefixes in type errors and rewrite them? Seems like a long shot though.\n@twada?\n. @twada hmm you mean get those from the source-map adjusted stack trace? I guess showing the entire line would be better than showing the compiled code.\n. I don't believe this is an issue any longer.. Closing due to inactivity.\n. Closing as a duplicate of #178.\n. I think we're kinda already covering this. Of course if you use new language features that are supported by your targeted Node.js version you don't need to do anything special. If you need to transpile your source code we've covered that in the Transpiling imported modules section.\nI'm closing this issue, but if you have ideas on how to improve the documentation please open a new issue or PR!\n. @masaeedu I think that's something that could be built on top of AVA though.. @photonstorm AVA is merely a test runner. (Though a pretty awesome one, of course!) It can test anything provided you know whether the actual output is acceptable.\nLooks like js-imagediff has an equal(a, b, tolerance) method, which I assume returns a boolean. You could then do t.ok(imagediff.equal(a, b, tolerance) in your AVA test and it will pass or fail based on how equivalent the images are.\nGoing to close this but feel free to reopen if you've got further questions.\n. Let's move the JSX recipe discussion to #446.\n. Sounds like #549. Nice find @tomazzaman, it's being fixed :smile: \n. Nice!\nSome further feature thoughts:\n- It'd be cool to only re-run a test if its test file changed.\n- It'd be really cool to track the dependencies of a test and rerun it when those change (e.g. when you change a file that is being tested, only run the test that loads that file).\n- Could then observe package.json to pick up potential node_modules changes.\n. Thinking through this some more, if a test file i deleted that test should no longer run, conversely when a new file is added it should be run.\n. @corinna000 the way I read it is that t.throws() lets you assert something about the error message. If the error doesn't have a message then your assertion should fail. Right now a string is treated as the error message, and it shouldn't.\n. @jamestalmage \n\nWe magically handle detecting it's not an error and test the expectation directly against the string instead of error.message.\n\nLooking at the assert code just now it seems we convert non-Error rejection reasons to Error instances, with the reason being cast to the message string. Then the expectations are applied against that message string. This has your described effect when the rejection reason is a string, but may have other effects if the reason is a different type.\nIMO we should pass the rejection reason to core-assert and let it determine whether it matches, without any conversion.\n. @tomazzaman I've opened #577 for your source file transpilation suggestion. Watch mode is in 0.12 now (undocumented) and #448 would allow Babel to be disabled if you so desired. Thanks!\n. I can see .only being an inverse .skip, allowing you to home in on a particular failing test. In that case I wouldn't want to see any skipped tests (or todos) in the output, other than for the final summary. Though as @BarryThePenguin mentioned it might make sense to show them on CI servers.\nCurrently reporters aren't pluggable so we need to figure out what behavior we want from the built-in reporters. Presumably the test events could convey a test is being skipped due to a .skip or .todo or because there's an .only present elsewhere.\n. @jamestalmage do you think we should output the count even if we don't report the individual tests? Though I suppose that would be illegal under TAP and then things get all weird between the different reporters.\n. > Basically, this proposal is kinda antithetical to the entire purpose of .only. Focusing in on just a few tests for quick feedback while tackling a specific problem.\n:100: \nClosing, unless @sindresorhus or @vdemedes object.\n. Ideally each test file controls its own fixtures (e.g. different database name from other test files). If that's not possible you'd have to run a setup process before invoking AVA and a teardown process after. I wonder if we could support global --setup/--teardown options.\n. No worries @therealklanni!\n. Sorry to bring this up again, but should always just be the default behavior? If for some reason your after hook should only run if tests succeed then you can keep track of that yourself. I imagine that'd be rare though.\n. Sorry to bring this up again, but should always just be the default behavior? If for some reason your after hook should only run if tests succeed then you can keep track of that yourself. I imagine that'd be rare though.\n. @jamestalmage in your example, do you mean verifying mocks in your afterEach? Hadn't considered that.\nI'm struggling with what to name the \"only if there were no failures\" after hook. always is easier. I'm just concerned that users may use after for cleanup where they really should have used always.after, but don't notice until a test starts to fail in CI. It's hard to test your tests \ud83d\ude09 \n. @jamestalmage in your example, do you mean verifying mocks in your afterEach? Hadn't considered that.\nI'm struggling with what to name the \"only if there were no failures\" after hook. always is easier. I'm just concerned that users may use after for cleanup where they really should have used always.after, but don't notice until a test starts to fail in CI. It's hard to test your tests \ud83d\ude09 \n. > after and always.after is pretty clear naming once you understand the default behavior of after. I'm not sure of a better naming scheme if we flip the default behavior. afterEach.success?\nYea I think I prefer after and always.after.\nGiven that these run after the normal after hooks, maybe .finally would be better? But maybe that's too strong.\n. > How do we enforce the match option in package.json to be an Array?\nThis is a bit of a distraction. Just follow the precedent by the require and source options. They're singular CLI flags that can be repeated, and singular package.json options that are arrified in cli.js.\n\nHow do we differentiate between a passed-in CLI flag and a configured value in package.json?\n\nThe flag parser is passed the default config from package.json. You don't need to differentiate.\n\nAnd how does one test the behaviour of options defined in package.json? I couldn't find a test suite that does that, not yet anyways.\n\nThere's a pkg-conf: cli takes precedence test in test/cli.js. I just noticed that --watch and --source aren't included in that test either though.\n\nI'm a little unsure as to what the relationship between the runner and cli looks like, if there even is one.\n\ncli.js creates an Api instance from api.js, passing options. That Api finds tests and creates child processes. You probably want to pass the match option down that way. See #477 for inspiration.\n. > As it's not part of the subject of the PR/issue I'd prefer not to add --watch and --source expectations.\nYea that's fair. Just had a look and those are actually not testable in the same way.\n. @jamestalmage maybe. It's hard though without stubbing some dependencies in cli.js. Otherwise we'd need some rather complicated fixture setup. What do you think?\n. @jamestalmage yes @kasperlewau added tests for --match like you described. I was talking about --source. Opened #626 to track.\n. > My remaining question is whether or not we should shorten them globally. I am not overly concerned about the performance impact of failing assertions, more so other libraries (like bluebird longStackTrace support) that might be creating overly long stacks and slowing things down.\nChanging the stack trace limit may also impact code-under-test. Instead we should set it to Infinity when recording the assertion failure, and then reset it afterwards.. We set the limit here: https://github.com/avajs/ava/blob/dd9e8b2effe541f9f232ee622452343dac5895dd/lib/main.js#L25\nInstead we should modify the AssertionError constructor to capture an infinite stack trace: https://github.com/avajs/ava/blob/dd9e8b2effe541f9f232ee622452343dac5895dd/lib/assert.js#L31:L54\nWe need to do the same in the getStack() method: https://github.com/avajs/ava/blob/dd9e8b2effe541f9f232ee622452343dac5895dd/lib/assert.js#L57:L61\nWe have some existing code for this already: https://github.com/avajs/ava/blob/dd9e8b2effe541f9f232ee622452343dac5895dd/lib/test.js#L24:L31. @kasperisager does this help?\njs\ntest(t => {\n  const expected = new Error()\n  const actual = t.throws(() => { throw expected })\n  t.is(actual, expected)\n})\njs\ntest(t => {\n  const expected = new Error()\n  const actual = t.throws(() => Promise.reject(expected)\n  t.is(actual, expected)\n})\nIn other words t.throws() should return whatever was thrown (or, if a rejected promise was returned, the rejection reason).\n. @kasperlewau,\n\nSorry @novemberborn - if your poke was meant for me, I'm afraid you mistyped ever so slightly. Didn't get any notification(s). Regardless, you did make it clearer - tyvm!\n\nToo many Kaspers already! :stuck_out_tongue_winking_eye: \n\nLooking at @jamestalmage's original example again I believe the intent is that if t.throws() passes, then the error should be returned. This then allows you to apply further assertions to the error. If it fails then your test has already failed. There's no more need to access the error object.\nThat would imply wrapping the call to fn in https://github.com/sindresorhus/ava/blob/f77ded959a0e7de6e9bf65d8c2fd6625bc59ba47/lib/assert.js#L99 to capture the thrown error and returning it after the assert.throws() call. If assert.throws() itself throws then the assertion has failed and no error is returned (because an AssertionError is thrown). There may not even be an error in case fn didn't throw one.\nYou'd need to return x.throws in the promise-handling code as well for it to bubble up correctly.\nNo changes should be needed to Test.prototype.exit.\n468 is related but should not have any bearing on this issue.\n. Yea please open a PR.\n. I'm sure this idea will come back to us if we're frequently doing bench runs. I'm pruning the list of open issues, and since the lack of this utility hasn't been a problem since the bench tests were introduced I'm closing this one (for now).. > > A case could be made though that --watch should simply never exit\n\nThat is what I would expect. If I launch with watch and there are initial problems, I should just be able to fix them and save. What is the argument for the behavior in this PR?\n\nNo particular argument, that's why I brought it up :) Will change.\n\n\nThe delay is 10 milliseconds. If further changes are detected the watcher again delays by 10 milliseconds. Currently this delay is not configurable\n\nI agree it should not be configurable, but is 10ms long enough? What happens in popular IDE's when you have multiple large files open and you Save All? How fast does chokidar pick those up? (Especially on slow systems... Especially on slow Windows systems). Would a 50ms/100ms debounce really be noticeable? Would a higher value provide any benefit? I am not disagreeing with the chosen value, I truly don't know the answer, and I am just suggesting we put some thought into it.\n\nI don't know either. Note that the current implementation is somewhat naive in that it debounces again for the full 10ms if a change happened in the interval. Increasing the delay would make that hurt more, but some extra code complexity could deal with that. It'd be good if tests rerun within 150ms if possible though, so that it feels instantaneous.\n\nThe easiest solution might just be to go with 10 ms and wait for reported problems.\n\nYea.\n\n.nyc_ouput and coverage should almost certainly be in the default excludes. It wouldn't make a lot of sense to watch with coverage enabled, but we should protect against it.\n\nI agree with excluding those, but note that you may have watch in one console and run coverage in another.\nI don't think we should bail if you run watch inside of nyc, or a CI environment for that matter.\n\nRE: include / exclude patterns:\nMight we just provide a single config option with a ! prefix for exclusion?\n\nYes that was my thinking too. That's how the files are matched.\n. Currently the process exits if a test file cannot be run (an AvaError is thrown). That shouldn't happen when using --watch since it may run test files before you've had a chance to write the AVA boilerplate. We can make sure the watcher doesn't exit, sure, but the behaviour in the API itself still seems subpar. The exception propagates and the results of other (concurrent) tests are ignored.\nGetting sensible log output for these scenarios is difficult. Ideally we see as much output as possible, ending with the number of passes, fails, rejections and exceptions, including the AvaErrors.\nI'm proposing to handle these AvaErrors as exceptions and logging them in the various reporters. Tests will continue to run and the errors will be shown at the end (depending on the reporter). Then any exception coming out of Api#run() is unexpected, meaning we could even crash the process with an uncaught exception.\nI'm adding a commit to this effect. Apologies if it's a tad controversial :smile:\n. @sindresorhus \n\nDo you know why rs\\n was chosen? Just r would be faster to type.\n\nWill investigate.\n\nOk, good point. Was just trying to not have to introduce yet another flag... --sources sounds good.\n\nYes, I quite like --sources! Will add.\n. OK I found the introduction of rs\\n: https://github.com/remy/nodemon/issues/162#issuecomment-17227818\nIt's muscle memory for me but I don't really care either way.\n. OK, status update.\nSummary of changes:\n- Implements --watch (and -w) flags\n  - These are undocumented for the time being\n- Reruns specific tests when changes to those tests are detected, including running new tests\n- Reruns all tests (according to the files patterns) when source changes are detected (non-test changes)\n- Type rs and hit return to rerun all tests, like nodemon\n- AvaErrors that occur during test runs are treated as unhandled exceptions\n  - Tests continue to run\n  - Fixes #511 \n- Existing tests where updated to reflect changes in behavior\n- 100% test coverage for watcher module\nStill to do:\n- [x] Implement --sources flag so users can customize which source changes should cause all tests to be rerun\n- [x] Extract default exclusion patterns for sources into a standalone module (these were copied from nodemon but should be extended, and could be shared)\nFuture enhancements:\n- [ ] Track which tests depend on which sources, rerun just those tests when sources change (will open new issue to track)\n. Added --sources support.\n. > I don't think we should wait for the previous tests to finish, if new ones are in a queue. If developer made a code update, he obviously does not care whether tests fail/pass before that last update. He wants to see the immediate test log after his change, right?\nIf I modify a test that is currently running perhaps I'd like that to be interrupted and restarted. We could see if there's a strong need for that. Alternatively AVA could simply start a second run of the test but I imagine that would make for some very confusing log output.\nRight now the API runs multiple test files and retains state during those runs. That state can only be reset after a run has finished, which is why the watcher has to wait. I don't think that's so bad, you'll keep an eye on your test output and wait for it to stop changing.\n\nImpressive work, @novemberborn!\n\nThanks! :smile: \n. And finished the final to-do, ignore-by-default is now used to control the directories ignored by the watcher.\nForce-pushed to include some bugfixes. Aside from further feedback this is good to go :rocket: \n. > I meant, we could cancel the \"old\" test run (so that we no longer get results/callbacks/etc from it) and start a new one along with a new output.\nYea that'd be an option. Though that would probably preclude after cleanup code from running in the tests so may not be ideal.\n. Regarding instantiating new a new API for each run, and new reporters too. It's icky because cli.js creates the API with its options object. Same goes for the reporters, which hold a reference to the API instance. Further, reporters assume they're the only thing writing to stdout.\nWe could wrap instantiation in a method and pass that to the watcher but that doesn't seem much cleaner than resetting the existing instances.\n\nRegarding the optional files argument to API#run(). Originally I moved it out of the constructor but then noticed #83. We could still make that consistent but it'd be a breaking change.\nI might still want a second parameter to force the test titles to be prefixed, as mentioned earlier in this PR.\n\nhandlePaths() is wicked. It executes glob patterns but then if a directory is matched it globs for JavaScript files inside that directory. I'd much rather it required glob patterns that matched files and not directories. Then the defaults could be moved into cli.js (and perhaps exposed through some other module for programmatic use). It would definitely simplify the logic exercised by the watcher in order to determine whether a changed file is a test file.\n. > Most users don't really grok glob patterns completely and it's nice for them to be able to just do ava dir instead of ava dir/*/. I don't think we should sacrifice this convenience.\nSure, fair enough. \n\nCouldn't you just run through all the parent directories and compare them to the glob?\n\nThe current implementation tests the first directory, but actually that won't cover foo/*/bar patterns where foo/qux/bar/ is a directory.\n. Force pushed some updates:\n- Fix merge conflict\n- Remove completed TODO as noted by @jamestalmage \n- Fix test matching to cover foo/*/*bar patterns hwere foo/qux/bar is a directory\n- Changed sources parameter to --source to be in line with --require (both can be repeated)\n  - This means the key in package.json is also source, which is different from files, but consistent with require. Thoughts?\n\nAny comments on these points?\n\nRegarding instantiating new a new API for each run, and new reporters too. It's icky because cli.js creates the API with its options object. Same goes for the reporters, which hold a reference to the API instance. Further, reporters assume they're the only thing writing to stdout.\nWe could wrap instantiation in a method and pass that to the watcher but that doesn't seem much cleaner than resetting the existing instances.\n\nRegarding the optional files argument to API#run(). Originally I moved it out of the constructor but then noticed #83. We could still make that consistent but it'd be a breaking change.\nI might still want a second parameter to force the test titles to be prefixed, as mentioned earlier in this PR.\n. > \u2728 Amazing work on this @novemberborn. It turned out really good! :beers:\n\n:boom: !\n@forresst tracking in #532 now.\n. Full list of follow-up issues:\n- #532 document --watch\n- #533 investigate instantiating new API and reporter in --watch\n- #518 abort old tests and start new ones after a code change in watch mode\n- #534 consider moving files patterns from API constructor to run() method\n- #535 run affected tests when sources change\n(Feel free to open more for any issues I missed)\n. Looks like this might be covered by #542?\n. Looks like this might be covered by #542?\n. I'm sure this idea will come back to us if we're frequently doing bench runs. I'm pruning the list of open issues, and since the lack of this utility hasn't been a problem since the bench tests were introduced I'm closing this one (for now).. In https://github.com/sindresorhus/ava/pull/502 I change how these \"no results received\" errors are propagated. They're now treated as uncaught exceptions and, crucially, don't stop the tests from running. It seems to fix this case too!\n```\n$(npm bin)/ava foo.js\n2 exceptions\n\n\nUncaught Exception\n  Error: foo\n    Immediate._onImmediate (foo.js:5:15)\n\n\nTest results were not received from foo.js\n```\n\n\n```\n(npm bin)/ava --verbose foo.js\nUncaught Exception: foo.js\n  Error: foo\n    Immediate._onImmediate (foo.js:5:15)\n\u2716 Test results were not received from foo.js\n0 tests passed\n  2 uncaught exceptions\n``\n. Though I must admit that I haven't rebased on top ofmaster` yet. Will check again once I've done that.\n. Output is the same.\n. Yea\n. @tusharmath it doesn't output any uncaught exception though. Maybe you're running into the issue discussed here: https://github.com/sindresorhus/ava/issues/604#issuecomment-199516941\n. #604 discusses an issue where Travis kills the test processes. I suspect you're running into that problem.\nThe behavior described in this issue is by design.\n. @tusharmath try running with the --serial CLI flag to work around the issues described in #604.\n. @tusharmath could you open a new issue to discuss this in?\n. Closing this in favor of #448, under the assumption that the implementation would be obvious once that lands.\n. Closing this in favor of #448, under the assumption that the implementation would be obvious once that lands.\n. > I will follow up with another PR in coming days regarding this issue.\nThat's #539 right?\n. > I will follow up with another PR in coming days regarding this issue.\nThat's #539 right?\n. I'm not convinced AVA should be keeping track of all test titles and warn you when there are duplicates. Assuming test failures come with stack traces they'll also come with line numbers, letting you find the offending assertion and thus test.\nThe linter rule seems like a sufficient solution.\n. Would have to restart the old tests as well as any new ones.\nAre there any concerns around cleanup code not running when tests are aborted?\n. > Or - the keyboard command could trigger the immediate rerun. I like this one I think. I only care about this on projects with really long running test suites.\nAt least that's more or less equivalent to sending a SIGINT and restarting manually.\n. > What I was thinking is that even though we have started the concurrent tests we can still choose to ignore their result and pretty much unref / mark them as aborted, and ignore. Like an unref'd/detached child process, it still runs to completion, but the parent process don't care and have no communication with it anymore.\nThat would work as long as there are no implicit assumptions that test files are not executed concurrently. If a test file resets a static database table whilst cleaning up that may break a concurrent run of that same test file.\n. > but the idea is that the after/afterEach will run right away when it's unref'd.\nWhile the tests themselves are still running? That'd be fine for reads (they'll fail but their output is discarded) but not for writes, which will no longer be cleaned up.\n. With #78 in place we should make sure any scheduled test files are aborted. #710 will give us the ability to abort test runs. This is blocked until those issues are resolved.\n. It's been nearly two years since this issue was first raised, and yes alternative configuration files have seen more adoption since then, as have monorepo projects. I'm not ignoring the need. We're trying hard not to overcomplicate what AVA does and this has also applied to how it's configured. There's some effort involved in revisiting this decision that goes beyond the number of lines of code an implementation takes. To that end:\nWe'd need to consider where AVA would find these alternative files. Currently it finds the closest package.json file from where AVA is invoked. This also determines the working directory for the test processes. Presumably these configuration files can only live next to package.json files? Or do folks here have use cases where alternative configuration files should determine the working directory?\nThe current trend seems to be for JS-based configuration files. Would folks be upset if AVA supported ava.config.js files but not .avarc files?\nDo monorepo folks expect .avarc files to support inheritance?. @jtag05 @pho3nixf1re, @danawoodman, thanks, I'm just trying to get a fuller picture.\nA --config flag might be interesting in monorepo projects. Any thoughts on that?\nI'm hesitant in supporting it just so folks can pick different names for the configuration files. And ironically you may want to configure that flag in say the package.json so you don't need to add it into the build tooling \ud83d\ude09 \nWhat would be the expected priority order between these various files? E.g. Babel 6 loads .babelrc before checking package.json. Babel 7 also checks .babelrc.js (and soon babel.config.js I believe), but throws an exception when it finds multiple configurations. How would you expect AVA to behave?\n. @danawoodman I'm also on the core team. We'll be taking all of this into account and go from there.\nMeanwhile, please keep the feedback coming!. I've been thinking about this a bit. I can definitely see some interesting use cases for an ava.config.js configuration file. For instance certain behaviors can be changed dynamically during CI, or when Babel 7 support lands you could inline a Babel plugin.\nI see less need for an .avarc file. If you want to write JSON you can do that in package.json. If you want to use a separate file there's ava.config.js. @danawoodman, looking at cosmicconfig it also ships with support for YAML files and config discovery, neither of which are needed with AVA.\nI propose the following behavior:\n\nWhen invoked, AVA walks the file hierarchy looking for a package.json file\nAVA checks whether a sibling ava.config.js file exists\nIf the package.json file contains an \"ava\" configuration, and ava.config.js exists, AVA will exit with a warning about ambiguous configuration\nOtherwise the package.json configuration takes precedence over ava.config.js\nava.config.js is loaded through esm so can be either a CJS or ESM file. The loader should support ESM transpilation by looking for a thruthy __esModule property: if set then the value of the default property is used\nAn actual configuration object may be exported, or a factory function\nIf a factory function is exported it'll be called with the project directory (for future compatibility). It must return a configuration object\nThe configuration object must not have a then property\n(These are the same rules as for .babelrc.js files in Babel 7)\n\nDown the line I'd be interested in exploring a --config argument. It would take precedence over both package.json and ava.config.js files, without ambiguity warnings. Multiple projects in a monorepo could use the same configuration through this argument (this is where passing the project directory to the factory function comes in). Perhaps we'd also support a string value for the \"ava\" key in package.json to point at a separate configuration file.\nEventually it'd also be good to ~~support ESM for the configuration files, and~~ allow asynchronous values. It'd also be good to have logic that can validate the configuration objects. In the meantime we could ship TS and Flow definitions (though we won't support ava.config.ts configuration files yet).\nThank you @jtag05, @pho3nixf1re and @danawoodman for pushing us on this issue. Please let me know if you're interested in implementing this before I get round to it.. @trusktr while less convenient than a --config flag, an initial implementation without it would still let you write an ava.config.js file which re-exports the config from your shared package.. @good-idea well the feature is approved as of https://github.com/avajs/ava/issues/520#issuecomment-358030492 \u2014\u00a0it's ready for somebody to pick up \ud83d\ude09 . @good-idea good catch. I meant to give the ambiguity warning if package.json contains an \"ava\" config property. I've updated my above comment.\nI've also updated the comment to reflect that we should use esm to load the ava.config.js file. That way we can support export default syntax.. @adius currently test files must be JavaScript files, ending in .js. Are you trying to use a test file written in Coffeescript?\n. @adius currently test files must be JavaScript files, ending in .js. Are you trying to use a test file written in Coffeescript?\n. Closing in favor of #229.\n. Rather than modifying process.env.NODE_PATH in api.js you should probably pass a custom value in fork.js: https://github.com/sindresorhus/ava/blob/bdb8b6ebaa05f1dc31cdab7c84aed7d92317b8ed/lib/fork.js#L19:L22\n. Perhaps resolve the additional, absolute NODE_PATHs in the main process, once, and then pass the array through the opts to the test-worker.\nYou should add a test, probably in test/fork.js. Note that tests are written using node-tap. Maybe use t.spawn to spawn another test file (from the fixtures directory) with a NODE_PATH environment variable, then in that file use the lib/fork module to run an AVA test. You could set up NODE_PATH so the final test can do an import and have it be resolved correctly. I'm in Gitter if that wasn't clear https://gitter.im/sindresorhus/ava\n. LGTM!\n. > Thanks @novemberborn for your patience, I hope that if I'll ever have to contribuite again it will be a smoother process, I learned a lot anyway :)\nNo worries, looking forward to your next contribution!\n. > Wouldn't it be better to just pass all environment variables from the parent to the worker? That way we wouldn't have to special-case future environment variable needs.\nThe problem is that NODE_PATH can be relative and the tests run in a different working directory.\n. In cli.js? Anywhere else and it may interfere with the programatic API.\n. @ingro I think what @sindresorhus is saying is that you can add an env object in fork.js which sets NODE_PATH to the corrected value. Then you don't need the changes in test-worker.js.\n. > I should create another pull request?\nYep! :dancers: \n. @jamestalmage given your refactoring work what's your take on this?\n. > I actually was thinking we could move watcher support into the API itself.\nThe watcher is fairly big though. I wonder if we could make the watcher instantiate its Api. We couldn't before due to how the loggers were coupled.\n. @jamestalmage that sound cool.\n. The lack of this refactor hasn't been an issue. Closing this, but I expect we'll do something like this eventually as needs dictate.. Here I was thinking I had original ideas\u2026\n. Sadly this breaks --watch. To be fair there is no actual integration test from cli.js with --watch so you wouldn't have noticed. The test for lib/watcher stubs all its dependencies. I've opened #537.\nfiles and excludePatterns were assigned so lib/watcher.js could reference them. You'll have to pass them to start() now so the watcher can run the initial tests or specific tests. This probably means defining the defaults in cli.js or perhaps as an export from api.js for programatic convenience. They no longer need to be stored on the API instance. (Looks like it'd be valid to retain the excludePatterns definition in the API constructor though).\nexplicitTitles is there so the watcher can force a test title prefix even when running a single test. Your change makes it so test titles are always prefixed. If we're willing to go with that we can clean up the code. Otherwise run() should take a second forcePrefixTestTitles argument the watcher can use.\n. > Exporting files and excludePatterns could work but it'd require a big change in a lot of files (I think), so we'd have to get permission from the powers that be\nThose patterns where hidden away inside a function before --watch landed so it should be OK. Real question is where they should be defined such that programmatic usage of the API is still possible even if the patterns are provided when instantiating the API or running the tests.\n\nAgain, just exploring/messing around. It \"works\" but the tests for watcher still fail. Will look into it this night.\n\nYup that should do it. Just need to provide compatible values in https://github.com/sindresorhus/ava/blob/bdb8b6ebaa05f1dc31cdab7c84aed7d92317b8ed/test/watcher.js#L23 and https://github.com/sindresorhus/ava/blob/bdb8b6ebaa05f1dc31cdab7c84aed7d92317b8ed/test/watcher.js#L95.\n. @sotojuan hard to say. Could you push some code so I can debug it here?\n. @sotojuan just had to declare the variables and update more api.files and api.excludePatterns references: https://github.com/novemberborn/ava/commit/aa4e211268bff3ed386ce5c4579c0624ff3c4358. \n. I think you should keep the default excludePatterns in the API constructor. It only needs to be defined once since it's used for every run. Then the watcher can simply access it like before.\nStill need to resolve whether we always want to prefix test titles (even if there's only a single test file) or only when tests are rerun via the watcher. How about making it an option on the API itself, and if --watch is enabled set it to true? That way the watcher output is consistent but regular output is still minimal, and we won't need a second argument to run() in order to differentiate.\n. > All right, I think I got it but I feel like I am missing something. Let me know!\nGetting close! The code is now doing a little bit of the old behavior, and a little bit of the new behavior. \ncli.js should control the default files patterns. Api#run() should assume it's called with patterns and not force defaults. That means the watcher should call Api#run() with the files argument it received from cli.js.\nexplicitTitles is either on or off, it doesn't need to change based on the arguments to Api#run(). It's configured correctly by cli.js based on the --watch flag.\n. > Should I update the test so it sets the explicitTitles option?\nYes.\n\nIt's the same case with a couple of the tests in api.js. I added explicitTitles: true to those that were failing and the changes you mentioned above and all tests are passing, but I'd like to know before pushing!\n\nPush away! :)\n. @sotojuan The watcher tests run assertions on a stubbed API. They pass even if the watcher is broken. https://github.com/sindresorhus/ava/commit/b8c444de57ebd97ec5aaa679f4d2fc60f5aa15e0 adds an integration test to test/cli.js which fails with your changes so far. I should have pointed it out when that commit landed, it's not currently in this branch.\nhttps://github.com/novemberborn/ava/commit/651343f9d864b45aae187bad0958a908546a23f8 solves the problem.\n\nBuilds and tests pass fine in my Macbook, not sure what's up with AppVeyor.\n\nThere's a merge conflict with master :(\n. > Like I mentioned earlier, the test for failFast does need explicitTitles to be set to true as the output it tests for has prefixes.\nYes but if you check the full diff you'll notice you added those prefixes in this PR ;-)\n. > I'm not very experienced with conflicts so any instructions on how to deal with that would be great!\nhttps://help.github.com/articles/resolving-a-merge-conflict-from-the-command-line/ may help.\nThis will pull in the new watcher test in test/cli.js, so don't forget about https://github.com/novemberborn/ava/commit/651343f9d864b45aae187bad0958a908546a23f8 ;-)\n. > When you are not busy, can you review? Thanks!\nOh didn't notice you fixed the merge conflict. Next time ping me more explicitly when you've updated the PR :smile: \nThe watcher test in test/cli.js is failing. The solution is in https://github.com/novemberborn/ava/commit/651343f9d864b45aae187bad0958a908546a23f8 (minus the console.error call I left in by mistake).\n. LGTM otherwise.\n. @sotojuan copy yea. It only lives within my fork :)\n. @sotojuan are you running all tests?\n. Great! :shipit: :exclamation: :tada: \n. Thanks @sotojuan!\n. I'll take care of this.\n. Ah @vdemedes this is for #516?\n. Ah @vdemedes this is for #516?\n. Updated.\n. > > Does this have any perf side-effects? Maybe worth running the benchmark?\n\nSure, will do and post the results.\n\nCurious about those results now :) Also with regards to Sequence#run() not consistently returning a promise, and @vdemedes' comment in https://github.com/sindresorhus/ava/pull/539/files#r54347365. I'd much rather have consistent code than checking for non-promises and having a/synchronous branches.\nAny thoughts on going back to runner.tests.concurrent and runner.tests.serial?\n. @jamestalmage once this lands should #504 be closed?\n. @jamestalmage once this lands should #504 be closed?\n. @sindresorhus @vdemedes @jamestalmage any feedback?\n. @sindresorhus @vdemedes ping\n. > By test, I assume you mean test file?\nYes. Naming is hard\u2026\n\nLGTM from me, will clean up later myself ;)\n\nLet me take the feedback and improve on this :)\n. > By test, I assume you mean test file?\nYes. Naming is hard\u2026\n\nLGTM from me, will clean up later myself ;)\n\nLet me take the feedback and improve on this :)\n. Updated! This should be good to go now.\nNew PR description:\n\nTest workers keep track of test dependencies for all registered file extensions. These are sent back to the Api as part of the 'teardown' event, ensuring they're captured irrespective of whether tests passed.\nThe watcher rejects any dependencies that are not source files, matching how Chokidar watches for source file modifications. It maintains a list of source dependencies for each test file.\nThe watcher will find the test files that depend on modified source files and rerun those (along with any other modified test files). If any modified source files cannot be mapped to test files all tests are rerun. This is necessary because only require() dependencies are tracked, not file access.\nRun DEBUG=ava:watcher $(npm bin)/ava --watch in a project to see the matching in action.\nThis required some extensive refactoring of the watcher code itself. Worth it in the end since the changes drop in quite nicely. This will conflict heavily with #536 so best if that lands first.\nAdditionally I found that test workers may receive multiple teardown events. This made the emits dependencies for test files Api test flaky since it would occur for uncaught exceptions. That's fixed in a separate commit.\n\n\n@vdemedes \n\n3) I'd replace arrayUnion with merge\n\nBut @sindresorhus wrote array-union! :wink: If you're referring to https://www.npmjs.com/package/merge that seems to merge objects, not arrays?\n. Pushed a commit to fix Windows. Paths are insanely confusing so I've tried to simplify a few assumptions:\n- test files always use platform-specific slashes\n- Chokidar emits file paths using platform-specific slashes\n- tracked dependencies use platform-specific slashes\n- test file and source patterns always use POSIX slashes\n- on Windows, file paths are converted to use POSIX slashes before matching patterns\nThis way the actual calls to the various matcher libraries are the same no matter the platform and we don't have to make assumptions on how the library adjusts. Just for example with minimatch on El Capitan:\n```\n\nrequire('minimatch').match(['nested/file.js'],'nested/.js')\n[ 'nested/file.js' ]\n require('minimatch').match(['nested\\file.js'],'nested/.js')\n[]\n require('minimatch').match(['nested\\file.js'],'nested\\.js')\n[]\n require('minimatch').match(['nested/file.js'],'nested\\.js')\n[]\n```\n\nOn Windows 10:\n```\n\nrequire('minimatch').match(['nested/file.js'],'nested/.js')\n[ 'nested/file.js' ]\nrequire('minimatch').match(['nested\\file.js'],'nested/.js')\n[ 'nested\\file.js' ]\nrequire('minimatch').match(['nested\\file.js'],'nested\\.js')\n[ 'nested\\file.js' ]\nrequire('minimatch').match(['nested/file.js'],'nested\\.js')\n[ 'nested/file.js' ]\n```\n\n:confounded: \n. @sindresorhus @vdemedes @jamestalmage rebased on master now.\n. @jamestalmage,\n\nHow is thorough is coverage of new code? It just seems like a high ratio of new code to tests (gut feeling after reviewing on mobile - I could be off base).\n\nProbably looked that way due to the refactor I did. That had little test impact though.\n\nMy only concern with the require hook is that it is easily broken by bad actors. Something like babel-detective might be more robust (but maybe wouldn't catch as thorough a dependency graph)\n\nYea it's icky. If it's any consolation the require hook is added after the custom requires have loaded, so unless hooks are added afterwards we'll be able to capture any dependencies. I'll make sure to explain this in the documentation.\nbabel-detective would let us capture the direct dependencies of the test file but not transitive dependencies. That's where this feature really shines: you have tests for Foo and Bar and Baz, the latter two depending on thingybob. When you change thingybob you don't need to retest Foo.\n\nMaybe instead of a conventional hook we wrap Module.prototype.compile? That is a no-no for conventional hooks (not really extendable), but since our hook doesn't manipulate the args (it just tracks files) I don't think it would be a problem\n\nI don't think that would work for native and JSON dependencies?\n\nMerging. We can follow-up with any improvements/requests. Really good stuff @novemberborn :)\n\nThanks @sindresorhus!\n. > Yup, that's true, since they don't call ._compile, but those are also rarely overriden, so a combination of ._compile + hooks for JSON/native could work.\nOh that sounds good. I considered extracting it initially but then figured it wasn't quite interesting enough. Combining it with ._compile does make it more interesting. I've added it to my personal to-do list\u2026\n. This landed in #556.\n. Destructuring t for each test you write seems cumbersome.\nFrom the id-length description:\n\nVery short identifier names like e, x, _t or very long ones like hashGeneratorResultOutputContainerObject can make code harder to read and potentially less maintainable.\n\nI don't think that applies to AVA tests.\n\nClosing this because the effort required isn't worthwhile.\n. @sindresorhus it's this line: https://github.com/sindresorhus/ava/blob/e28e24ceb8ca2647c38f3e53d9880a5ea32dacde/api.js#L117\nWithout a message isError is false, and then it unsets it here: https://github.com/sindresorhus/ava/blob/e28e24ceb8ca2647c38f3e53d9880a5ea32dacde/api.js#L136\nWill make a PR.\n. #555 should fix this.\n. @sindresorhus \n\nAny chance you could handle the below case too, which causes the same false-positive:\njs\ntest(() => {\n  throw '';\n});\nI know it's a big anti-pattern, but it should not cause false-positives no matter how bad it is.\n\nHad a quick look but it's a little trickier. Since AVA runs in strict mode this line throws if test.error is a literal. Incidentally the original check for test.error.message guarded against that. With this PR (as it stands now) falsy literals are rejected but truthy literals would still result in a crash.\nI reckon the best approach is to wrap non-errors in Test#_setAssertError(). E.g.\njs\nif (!(err instanceof Error)) {\n  err = { name: 'AvaWrappedError', value: err }\n}\nAnd then update the reporters to unwrap and log the original error. Perhaps using util.inspect().\nThis is a bit more involved so would appreciate any feedback first. I don't think this PR should land before this is resolved though, since the original code accidentally prevents actual AVA crashes by checking for .message.\n. > Can't we just wrap thrown non-errors in an error? Something like this:\nSure. It's kinda useless though if it comes back with Non-error thrown with value: [Object object]. Hence my inspect() suggestion.\n\n\nSince AVA runs in strict mode this line throws if test.error is a literal.\n\nGood thing. I didn't even know that was possible in sloppy-mode.\n\nIt's a noop in sloppy mode.\n. @sindresorhus updated.\nFollowed the approach for rejected promises, which I then updated to use inspect() as well. And then noticed the inconsistent callback error handling so changed that too. Hopefully I understood the various cases correctly!\n. I'm wondering how this should work with --watch mode. Let's say we have two test files: foo.js and bar.js. The latter contains a .only.\nInitially all test files are run and because bar.js contains a .only, only that test is actually executed.\nSubsequently foo.js is changed. The watcher will run just foo.js which does not contain a .only. Now all tests in foo.js are executed.\nLater a non-test file is changed and all test files are rerun. Only the .only test from bar.js is executed.\nThis isn't necessarily bad but it may be surprising to some. I wonder if .only should be sticky so that when changes are detected the watcher still only executes .only tests. But then if you remove the .only it starts executing all tests again. Similarly adding a .only switches back into the sticky mode.\nThis is a little tricky to implement though #544 requires similar per-test-file housekeeping.\nAnyway not meant as a blocker to this PR landing. We can discuss in a separate issue if people think .only should stick across watcher runs.\n. I'm wondering how this should work with --watch mode. Let's say we have two test files: foo.js and bar.js. The latter contains a .only.\nInitially all test files are run and because bar.js contains a .only, only that test is actually executed.\nSubsequently foo.js is changed. The watcher will run just foo.js which does not contain a .only. Now all tests in foo.js are executed.\nLater a non-test file is changed and all test files are rerun. Only the .only test from bar.js is executed.\nThis isn't necessarily bad but it may be surprising to some. I wonder if .only should be sticky so that when changes are detected the watcher still only executes .only tests. But then if you remove the .only it starts executing all tests again. Similarly adding a .only switches back into the sticky mode.\nThis is a little tricky to implement though #544 requires similar per-test-file housekeeping.\nAnyway not meant as a blocker to this PR landing. We can discuss in a separate issue if people think .only should stick across watcher runs.\n. Solution makes sense to me. Great job given the amount of (necessary) indirection involved in coordinating test runs :+1: \n. > I'm not actually sure how it works with --watch. At the very least I should add some documentation for that.\nWatch itself isn't documented yet so don't worry about it. We can get this in and then discuss repercussions in a new issue.\n. @sindresorhus that's just one test in Node 0.10 though.\n. @naptowncode LGTM!\n. Please see #593 for how .only should interact in watch mode.\n. More bikeshedding took place in #565. We're going for t.todo(\"title\") instead, where a function is not allowed. t.skip() will continue requiring a function. Stay tuned all :smile: \n. More bikeshedding took place in #565. We're going for t.todo(\"title\") instead, where a function is not allowed. t.skip() will continue requiring a function. Stay tuned all :smile: \n. > But I'll just say that I would recommend that skip require a function and the only way to avoid having to specify a function is if you use todo. Fewer ways to do the same thing is :+1: \nYes agreed. Would be good to make test.skip('my test') fail with a helpful error, pointing at test.todo().\n@BarryThePenguin changes are looking good generally, nice work. Would be good to clear up some of the logic and state management so it's easier to follow how these special test cases (pun very much intended) are handled.\n. @BarryThePenguin would you mind adding a test in test/api.js that verifies the passCount, skipCount, todoCount and failCount values? There is no explicit test for at the moment. The reporter tests use mocked values so everything seems fine until it's verified by hand.\n. > I think we should rather enable users to fail todo tests in the linting step, not in AVA. Todo tests should be like TODO comments, informative, but not affecting.\n:+1: \n. @ORESoftware some relevant fixes were made in 0.12. Which version are you using?\n. @vdemedes makes sense.\nPresumably using t.plan() would prevent the test from passing (or indeed ending).\n. @vdemedes makes sense.\nPresumably using t.plan() would prevent the test from passing (or indeed ending).\n. @loris your example triggers a test.cb() bug that's been fixed in master. Regardless, the best way to write such tests is to wrap the async functions in a promise and use async/await. That way errors will bubble up to the individual test and can be reported correctly.\nThis is low-priority because the problem of asynchronous errors goes way deeper than AVA, and even the workarounds we can do here require a large amount of effort. We have other, more pressing concerns.\n. @ORESoftware that's a bad example. If you use pify or alternatives you need to ensure you resolve/reject the promise with the asynchronous callback results, and do any further processing in a .then() callback, which will correctly propagate exceptions.\n. #214 already discusses AVA's inability to trace uncaught exceptions back to their tests. I've opened #1045 to document the resulting pitfalls, with a suggestion to promisify callback-taking functions.\nClosing this as a duplicate of #214.\n. Haven't looked at the tests yet, sorry. Been staring at AVA issues all day :)\nCould you follow up on https://github.com/sindresorhus/ava/issues/448#issuecomment-190286370?\nLooking really good though, nice work.\n. Haven't looked at the tests yet, sorry. Been staring at AVA issues all day :)\nCould you follow up on https://github.com/sindresorhus/ava/issues/448#issuecomment-190286370?\nLooking really good though, nice work.\n. The tests exercise caching-compiler, which is good. I'd like to see some integration tests in test/cli, though that does not have to hold up this PR. Happy to undertake that myself or help you @spudly if you're interested. It might be a bit tricky to verify the differing Babel behaviors.\n\nRight now package.json/ava/babel allows any Babel option to be changed. To avoid footguns though I think filename, sourceMaps, ast and inputSourceMap should not be configurable.\n\n@spudly thanks for the hard work on this. It's solving so many of our issues!\n. @spudly did a fresh checkout and install, master still passes but this branch does not.\n. @spudly ah! The failing tests use api.js directly, without setting the babelConfig option. Now I feel bad :cry: \nI still think using the default config in cli.js is the right move, but maybe add the !babelConfig || babelConfig === 'default' clause back in caching-precompiler.js. Quite bothersome passing it in the 44 tests that instantiate Api.\n. @spudly,\n\nif before step 3 I ran npm test, the test at step 5 would fail with 44 errors even though the source code was fixed.\n\nDo you have xo, nyc and/or tap installed globally? Else npm test wouldn't even be able to cache anything. I wonder if there's some local/global mismatch, presumably in nyc. Which nyc version do you have globally?\n\nIs this ready for another look?\n. > I don't have any of those modules installed globally.\nThen I'm stumped, sorry :disappointed: \n. @spudly did I mention I'm pretty excited about this landing? I'm pretty excited about this landing :)\nJust one suggestion regarding testing the plugins.\nOh, and to confirm, Babel merges plugin definitions when you set babelrc to true?\n. LGTM! Great work @spudly.\n. > I'm pretty sure this is the issue I was having during a couple of days ago. Simply removing ./node_modules/.cache probably would have solved my problem.\nOh of course!\n@spudly you can use the --no-cache flag to disable the cache.\n. @spudly we'll have to include the babelConfig option in the salt for the caching transform, see https://github.com/sindresorhus/ava/blob/e697629a77b511e069eb0a2991c8ee72cd09034c/lib/caching-precompiler.js#L68.\nAlso there's a merge conflict now :(\n. > If I didn't include t.threw to handle the rejection, the reason for tests failure would just be that there was a missing test, which is not enough information for debugging. This should probably be added to a lot of the other tests as well.\nAVA now requires a version of tap which supports promises. You should be able to return the promise and tap will report the failure.\n. @spudly could you return the api.run() promise and remove the t.threw bit? And just a question about the babel-cli dependency. LGTM otherwise!\n\nWhen squashing would be good to keep @jamestalmage's commit separate.\n. Please make sure to rebase on master to fix a linting issue and run the latest XO configuration against your PR. Thanks!\n. > Is there a reason we have return statements on cli.js:13 and cli.js:96?\nThey're shorthand for exiting the CLI.\n. @spudly what's the reason for adding the babel-cli dependency in package.json?\n. > That dependency was added by @jamestalmage in the babel-plugin-for-integration-tests branch, which is merged into my code.\nAh!\n@vdemedes are you in Gitter at the moment? Let's see about landing the integration test plugin in master etc.\n. Landed in https://github.com/sindresorhus/ava/commit/a03f8264edbd99d9c6a86827106ad40dfdc42b10 yay! (Messed up the commit so it didn't close automatically\u2026 oops)\n. Awesome work, thanks @spudly!\n. > I'd like to work on this\n@sotojuan yes that'd be great!\n\nwould this just require a check on test()'s arguments and an appropriate error message?\n\nYea I think so. It's pretty weird right now, using the mini reporter it just hung, and using --verbose it got stuck in a loop complaining about the test() call causing an error to be thrown. Besides adding some assertions around required usage and communicating that back to the user I'm rather curious why it got stuck.\n. @sotojuan good luck with your exam!\nAlso note that https://github.com/sindresorhus/ava/pull/565 is already adding some of these messages.\n. This is probably due to https://github.com/novemberborn/ava/commit/9dd03d5d2b20de48e72cb59e29573dd412201b1b (see #502). There's some variance between the reporters, looks like the verbose one should render a cross.\nMaybe this particular exception should be treated differently in the mini reporter. Or perhaps a general case can be made when there is 1 exception and no tests where found.\n. > No no, this kind of top-level output from CLI should be written without reporters, as those are fatal errors.\nNot when watching though. It's legit to start a watcher even if there are no test files yet. Or to remove the only test file while the watcher is running.\n. If we can agree that these are not fatal errors then I think we can fix up the reporters.\n. I don't understand why you're changing lib/test. What am I missing?\nNo need to exhaustively test .throws(). Just need to verify it returns the thrown error. You do need a test for when a rejected promise is returned though. You shouldn't need to change the observable and promise tests.\n. I don't understand why you're changing lib/test. What am I missing?\nNo need to exhaustively test .throws(). Just need to verify it returns the thrown error. You do need a test for when a rejected promise is returned though. You shouldn't need to change the observable and promise tests.\n. Cool getting there!\n. Cool getting there!\n. @kasperlewau cool! Just some simplifications in the test. Feel free to push up a squashed commit when that's fixed and I'll merge.\n. @kasperlewau OK just the tiniest issue with the assertion. Feel free to force push again and I'll merge.\n. Thanks @kasperlewau!\n\n. #390 transpiles tests files. This is suggesting infrastructure to transpile source files in the main process as well.\n. Yes, that's the point of this issue. We'd have to use the approach from caching-precompiler to get the compiled sources into the child processes, and we have to use glob patterns to match all potential sources before tests run so they're available synchronously.\n. This will be tackled through RFC 001. See the project here: https://github.com/avajs/ava/projects/1.. Yea, I pulled the RFC recently. https://github.com/avajs/ava/issues/1908 is probably the best issue to track for now.. :+1: \n. The README says to install globally and then run ava --init. I'd rather it said npm install --save-dev ava and suggested using ava for the test script. Not convinced about the init command TBH.\n. > ava --init works, but very long, because npm install takes a long time.\nThat's silly though. The user just waited a long time to globally install AVA and then they have to wait again for the local install. Why bother with the ava --init route at all?\n. > We recommend users install it globally so they can use the local AVA directly by typing AVA.\nFair enough. I've gotten into the habit of using $(npm bin)/ava and I frequently update my local Node install in such a way that global packages don't carry over, so I rarely install anything globally.\n. Hi @davidhq, I use OS X with zsh so this may not apply to your situation. npm bin returns the node_modules/.bin folder relative to the closest package.json. In the Terminal $(npm bin)/ava expands to /path/to/node_modules/.bin/ava which runs the AVA installed in my current project. \n. > I remember we had some back and forth with this decision, but atm I'd also like to merge this and strictly expect Error objects.\nWe all seem to be in agreement in #468 so let's not restart this argument :wink: \n. @sindresorhus this will be a breaking change.\n. @QuotableWater7 nope. Are you interested in taking this on?. @austinkelleher most definitely. Would you want to give it a go?. This will be easier to implement now that #1722 has landed, though perhaps we should implement it in the reporters, in which case it'd be better to wait for https://github.com/avajs/ava/pull/1776.. @vancouverwill I haven't had much time to work on #1776. I do think it should be implemented in the reporters, but I'm happy with some duplication if that unblocks.\nWould you be interested in working on this?. @vancouverwill we use XO for code style.\nThe reporters should receive events when tests are declared and when they finish, so it'll be possible to keep a list of pending tests (and test files, too). Then in lib/cli there should be a hook for SIGINT & SIGTERM to make the reporters stop listening to incoming events and shut down the workers, and then print the pending tests & test files. The same for --timeout handling.\nAt least that's the theory, it may be a bit different in practice \ud83d\ude04 . #1886 implements this for timeouts, in the --verbose logger.\nWe still want to implement this for when the process is interrupted (\"killed\"), and in the default mini reporter too.\nThank you @vancouverwill for getting us this far!. @avivr cool!\n. Thanks @avivr!\n. @kasperlewau nice work. However it doesn't actually work end-to-end. I've left some inline comments as to why. Could you add a test to test/cli to exercise the feature end-to-end? \n. > Tests without a title will not run if any match pattern is supplied.\nUnfortunately the runner does not have access to the full test title. It's later derived from the function name, for instance.\nMaybe that title logic needs to be moved to the runner. @vdemedes?\n. @vdemedes \n\nIt can't be moved to Runner, because we display full test titles only when there are multiple files and only CLI knows how many test files we have.\n\nThis isn't about test title prefixes. The runner calls TestCollection#add() which changes the title beyond what was determined in the runner, even though the runner has access to the same arguments.\n. > The one thing I haven't done is setting up an end-to-end test in test/cli to ensure it all works\u2122. Do you have any suggestions/pointers I can take with me on that voyage?\nSee the \"watcher works\" test, though it does a few more things than are required here.\n. My point is that the runner calls another method which figures out an implicit title. Those are not matched. I guess it's OK to say that match only applies to explicit titles but it shouldn't be too hard to apply it to implicit ones too.\n. > What about merging this PR as it is and opening a new issue to extend this functionality? I see some refactoring work involved here.\nYes that's fair.\n\nI don't think it makes sense to match on implicit titles. It should IMHO only match on what people write out in their test file.\n\nThat's fair too. Not sure we want to encourage test(function myTestTitle (t) { style tests anyway.\nApologies for the distraction :smiley: \n. Should this use exclusive rather than skipped? That is, set exclusive for matching tests. Perhaps in the Api report an error when no tests match. That makes --match similar to adding .only in the test files.\n. > I'm not familiar enough with the difference(s) between skip and exclusive when put into the context of say --watch or something of the sort. I bet you do though : )\nIt'll probably be OK, but we can make whatever improvements are needed once this lands.\n\nWhat I mean to say is that the semantics of match fit inclusion better than exclusion in my mind. Albeit that's a very fine line.\n\nHere's another angle. If I have a .only test and I invoke AVA using a --match, what is the expected behavior? IMO --match should win (just like CLI flags override package.json config). That would be easier to achieve if the match implementation changes the exclusive field of the test metadata. Non-matching exclusive tests need to have that field set to false.\n. I think it might make it easier to reason about the side-effects, e.g. in reporter output. Both --match and .only can cause a lot of tests not to run that would have run otherwise, unlike .skip which only causes skipped tests not to run.\n. @sindresorhus,\n\nDoes this still apply #592 (comment) ?\n\nThat's been resolved.\n\nIs the exclusive change something you want to do in this PR or can it be a follow-up?\n\nI think we should do it here, shouldn't be too much work. As the PR stands now, if you have a .only test and use --match it will only be applied to .only tests. I think that's a bug.\n. > As expected, non-matching tests no longer add to testCount, nor skipCount. That could open up for some confusion, as it's a fine line and a very personal opinion on whether a match pattern excludes or includes, but perhaps that sort of balancing act is outside the scope of what we're trying to achieve here.\n\nShould we introduce another counter to keep track of the tests that were not run due to the --match flag? ignoreCount?\nSounds a lot like skipCount to me , plus the fact that now we'd have to juggle even more words around; skip, only, match, exclusive, ignore. Gut feeling says that could be simplified, I'm just not quite sure how.\n\n471 covers this. Hopefully we can solve it in one fell swoop for all cases.\n\nIf --match takes precedence over .only, should it also override .skip?\n\nI'd argue no. You'd use .skip because a test is broken. You'd use .only or --match to select specific tests.\n\n\nI've changed some stuff around to add the exclusive flag onto the given test if its title matches --match, leaving the skipped flag for .skip.\n\nDoesn't look like you pushed any commits yet though? No worries if you were waiting for feedback on your questions, but figured you might have forgotten ;-)\n. LGTM! I'm in the midst of overhauling the readme (see #597) so don't worry too much about those changes.\n. > I agree with the changes you proposed to the Readme. Would you like for me to handle those, or defer to #597?\nSeems fine as it is now. I'll give it another read within the context of the changes in #597 and adjust as necessary once this is in master.\nNot sure why Travis failed in Node 5.\n. > Can we trigger a rerun and see what's up?\nJust did. Didn't realize I had that power :muscle: \n. @sindresorhus @vdemedes @jamestalmage LGTY?\n. Please make sure to rebase on master to fix a linting issue and run the latest XO configuration against your PR. Thanks!\n. > That's what I would expect.\nCool!\n. http://v8project.blogspot.co.uk/2015/07/code-caching.html provides a explanation of this feature:\n\nV8 uses just-in-time compilation (JIT) to execute Javascript code. This means that immediately prior to running a script, it has to be parsed and compiled - which can cause considerable overhead. (\u2026) code caching is a technique that lessens this overhead.  (\u2026)\nWhen a script is compiled by V8, cache data can be produced to speed up later compilations (\u2026) During later compilations, the previously produced cache data can be attached to the source object (\u2026). This time, code will be produced much faster, as V8 bypasses compiling the code and deserializes it from the provided cache data.\n\nHowever only the source passed to vm.Script() can be cached. To make most use of this feature I think we'd have to combine test-worker.js and it's dependencies into a single source string. We'd have to properly instantiate modules and populate the require cache when the combined source is evaluated.\nIf we ever get to that point we'll already have a fair amount of performance gain anyway just because there'll less disk access, though it'll be interesting to see to what extend caching the compilation data can help.\n. We kick off the tests in source order though. The asynchronous ones will start to interleave but synchronous tests won't. And synchronous tests will block asynchronous ones that started earlier.\n. This is a neat approach to finding which tests in which order cause a failure: http://make.bettermistak.es/2016/03/05/rspecs-bisect/\n. > These should remain tab indented.\nFair enough. I'll make them consistent in the other direction. Keeping spaces for JSON blocks and CLI output.\n\nI was trying to come up with a way to script that (I was unsuccessful).\n\n@jamestalmage script what? I used the Whitespace package in Atom which did the trick, although the right indentation varies by code block which is annoying.\n\nIt is probably a good idea to leave this branch accessible with no squashing of commits. It may be easier for translators to follow along commit by commit. We should still only push a single squashed commit to master, but lets leave the history here to help the translators.\n\nYes I was planning to squash and merge directly on master from my own machine and leaving this PR as is.\n\nUpdated. Added a bunch more changes (see commits after reword faq).\nTiming wise perhaps best to land #592 and #573 first. It's easier to update this PR so those new doc inserts read well with the rest of the changes here than it is to update them in their respective PRs.\n. Updated with improved --match documentation. @kasperlewau does it look OK to you (see last commit).\n. @vdemedes I'm waiting for #573 to land first.\n. Updated. I decided to combine the ES2015 and Babel config sections, it read a lot better that way. Also reworded it a little.\n@spudly are you happy with my changes? See the last commit.\nI'll merge if people are :+1: \n. Cool!\n. @sotojuan yes, please add the guards in the runner.\n. Please make sure to rebase on master to fix a linting issue and run the latest XO configuration against your PR. Thanks!\n. @sotojuan could you please add a comment when you push changes? There are no notifications for pushes.\n. @MoOx @ben-eb @thangngoc89 could you try with master? I have a feeling https://github.com/sindresorhus/ava/pull/642/commits/5435820752099fbd5eef8bfcc815e36b0042355a may be a relevant fix.\n. @ben-eb @thangngoc89 yes sorry I wasn't clear before. I was hoping the failures would at least be more consistent now.\nI'll try and have a closer look at this tomorrow.\n. > I started to think that this is a concurrency problem since I can't re-produce this locally. Can we limit the concurrency here ?\nYea that's my thinking too. Both the test suites have a large number of test files.\nNormally AVA will fail a test run if the child process exits with a non-zero exit code, but that's not happening here. When I reduce the memory the child processes exit with a null code and a SIGABRT signal. Using https://github.com/postcss/postcss-selector-parser/:\nconsole\n$ node --max-executable-size=10 --max-old-space-size=15 --max-semi-space-size=1 node_modules/.bin/ava --verbose 'src/__tests__/*.js'\nThere's a bunch more output that's not in the CI logs though:\n```\nFATAL ERROR: CALL_AND_RETRY_LAST Allocation failed - process out of memory\n<--- Last few GCs --->\n3515 ms: Mark-sweep 12.6 (50.1) -> 12.6 (50.1) MB, 17.9 / 0 ms [allocation failure] [GC in old space requested].\n3611 ms: Mark-sweep 12.6 (50.1) -> 12.6 (50.1) MB, 95.7 / 0 ms [allocation failure] [GC in old space requested].\n3625 ms: Mark-sweep 12.6 (50.1) -> 12.6 (50.1) MB, 14.3 / 0 ms [last resort gc].\n3642 ms: Mark-sweep 12.6 (50.1) -> 12.6 (50.1) MB, 16.4 / 0 ms [last resort gc].\n\n<--- JS stacktrace --->\n==== JS stack trace =========================================\nSecurity context: 0x896904e3ac1 \n    1: parse [native json.js:44] [pc=0x282455d13288] (this=0x896904da0c1 ,s=0x6758b0baa81 ,m=0x89690404189 )\n    2: arguments adaptor frame: 1->2\n    3: load [/private/var/folders/_6/p8qxp_3n62zg9081tvb0lcc80000gn/T/tmp.WOamj6WB/postcss-selector-parser/node_modules/babel-register/lib/cache.js:64] [pc=0x282455ddede9...\n```\nCould you try with https://github.com/sindresorhus/ava/tree/sigabrt? It'll log the SIGABRT exit reason and also include the exit signal in the \"no results received\" message.\n. SIGKILL, huh. Will see if Travis has limits or if there's a circumstance in which Linux kills procs like that. Anyway looks like this is a new scenario AVA should log for (besides not failing like this).\n. @thangngoc89 \n\nI ran the above command locally and got the process out of memory (node 5)\n\nYea, the commands reduce the memory until that occurs. Big takeaway is that we're not catching these kinds of crashes, resulting in misleading log output.\nI'll do a PR soon to improve AVAs output in this case. Let's discuss the forking related issues in #78. Thanks all for pushing AVA to its limits!\n. @ben-eb I guess you could write a wrapper script which launches a child process for each batch. nyc should be able to instrument that.\nRunning your tests in serial may also help, assuming Travis only kills the processes once they start executing the tests, not while they're first launched.\n. @kasperlewau are you interested in fixing this? No worries if not :smile: \n. Pushed a fixup which:\n- changes sports to comes with\n- links to ignore-by-default's index.js for the ignored directories\n- uses <kbd> to mark up the rs input, reworded the sentence as well\n- updated example source patterns\nStill to do:\n- [x] add -S as shorthand for --source\n- [x] support r + Enter to rerun all tests, in addition to rs + Enter\n- [ ] resolve whether source negation patterns should disable all default ignored directories, whether those default directories can be overriden, etc (this may need to be a follow-up issue)\n. > I meant just r. No Enter. What really is the point of having to press Enter?\nThis may be a bit tricky. It requires enabling raw mode on stdin which then seems to intercept ^C etc. It also seemed to affect my shell after killing AVA. Happy to open a ticket though as a future improvement?\nI've added r + Enter now. I'm inclined to keep rs + Enter for those users with nodemon muscle memory (or who may use AVA and nodemon in the same project). Can remove if others disagree.\n\nYeah, let's not dilute this PR with that. Can you open a new issue?\n\n614\n\nThis has now been rebased against master. -S shorthand has been added.\n. > Sure, that's fine, but r+enter should be the documented one.\nRight now they're both documented, with r + Enter being first.\n. Ticket: #615\n. > I don't see the point of documenting both. Your argument is that you have rs + Enter as muscle memory. I don't see why we would document rs to new users without that muscle memory and create overhead for them by making them have to choose which one to pick.\nHow would users realize rs also works? Other than accidentally. Maybe that's good enough though?\n. You'd have to read the docs to even find out about r+enter though.\n. OK will edit in the morning. Good to go otherwise?\n. Updated. Anything else?\n. > I think we shouldn't do it, because those libraries/components should be stable on their own\nSure, they'd just require valid options to be passed.\n. @avajs/core any thoughts on this? \n. We'll just do this when we're touching code. No need to keep the issue open.. > send is not used, as far I see, but I didn't delete it. Should I do it?\nIt's used as of a few hours ago :stuck_out_tongue_winking_eye: https://github.com/sindresorhus/ava/blob/1868204c1901f45b4f66a520ef6486fdd71fe1d2/api.js#L210\n. Hey @develar, would be great to have debug support for IntelliJ. Will you have time to follow up on the feedback here?\n. @develar no worries, that's great! :+1: \n. Perhaps !!node_modules/**/* can be used to disable the !node_modules/**/* pattern. Then we change the aforementioned logic to include the remaining patterns.\n. Hmm. The problem may be different actually. We should be able to merge the ignore patterns. However it should be possible to override a pattern.\nFor example you might want to treat a dependency as a source so you can more easily debug that dependency and it's impact on your own project. I'll need to verify whether adding node_modules/somedependency/**/*.js works or whether !node_modules/**/* overrides it. Will update the issue.\n. #730 ensures that:\n- You can specify negated patterns without having to declare the default exclusion patterns\n- You can override the default exclusion patterns. E.g. specifying node_modules/some_dir/**/* will select that directory as a source, while the default exclusion pattern node_modules/**/* remains in effect\n. It'd be hard to guarantee the behavior AVA promises (async/await for instance) if it might be affected by users own code.\nMaybe ava --init could detect a local Babel install and suggest this setting. And automatically set up babel-register too!\n. I'm coming at this from the other direction: I love to fiddle with Babel config for my source code but would rather not fiddle with AVA's config.\n@kentcdodds what presets/plugins do you see people using that are not in stage-2 or es2015? Is this perhaps a JSX issue? We can definitely support that better out of the box.\n. The features we describe in https://github.com/sindresorhus/ava#es2015-support should work out of the box though. Not opposed to merging the configs though. IIRC that could be achieved by making {babelrc: true} the default.\n. > And from my perspective, I don't think that people would use async await if they couldn't do so in their source because I generally like to write my tests and source in a similar fashion.\nI have a slight distrust of Babel's transform output for async/await, plus it's not necessarily a settled feature. Personally I'm happy to take that risk in tests but not necessarily in production code.\n\n@sindresorhus @vdemedes @jamestalmage thoughts on merging config by default?\n. @avajs/core bump :)\n. Our Babel plans are here: https://github.com/avajs/ava/blob/master/docs/specs/001%20-%20Improving%20language%20support.md. test.todo is new and not in 0.12. See https://github.com/sindresorhus/ava/compare/v0.12.0...master for the full set of changes. We'll probably release a new version in the next few days.\nIn the meantime you could use npm install sindresorhus/ava to get access to the new features.\n. Note that if you edit a file without exclusive tests, AVA will report 0 tests passed. Let me know if this needs a more helpful message.\n. > I feel like we really have to come up with a replacement for exclusive-related names. hasExclusive, trackExclusivity, updateExclusivity sound very weird and unclear. At least to me. Let's hear what others think.\nEnglish is weird :wink: \nhasExclusive comes from TestCollection and made its way into the stats object and Api. \"Exclusivity\" here refers to the updating the exclusivity of test files. I've changed some of the variable names to be more accurate, hopefully it's clearer now.\n. Pushed fixups for the feedback. Some of the changes were slightly more involved so would like feedback before squashing :smile: \n. > Can you show an example on when this would happen exactly? More helpful messages are always welcome.\nImagine you have two test files: test/foo.js and test/bar.js. foo.js contains a .only(). When you start the watcher it only runs that one test from foo.js. Now you edit bar.js. It runs the tests from bar.js in exclusive mode, but there are no such tests. It outputs 0 tests passed.\nMaybe it should report 0 exclusive tests found, but that would entail modifying the various reporters.\nIt'd also help if I added debug output so you can see the tests are being run in exclusive mode.\n. > Agreed. Mind opening issues for this? Can be follow-up improvements.\nSure. Anything else on this PR?\nI'm tracking down the intermittent CI failure but that's not related to this PR.\n. > > Maybe it should report 0 exclusive tests found, but that would entail modifying the various reporters.\n\nIt'd also help if I added debug output so you can see the tests are being run in exclusive mode.\nAgreed. Mind opening issues for this? Can be follow-up improvement\n\n635 #636\n. We've since removed the --source flag.. Thanks @forresst!\n. > Maybe we should keep t.same to be the strict variety, and have t.looseSame\nIMHO the strictest method should be the one you think of the soonest, so +1.\nI wonder if performance-wise it'd be feasible to run t.looseSame if t.same fails, and then report \"this test would pass using looseSame, did you mean to use that?\" It might help prevent people getting stuck debugging their assertion failures.\n. > Can you think of a reason for keeping the strict one around?\nNah, happen to loosen it up. If you're really concerned then as @mattkrick says write some t.is tests. I doubt stricter tests would catch many real word bugs.\n. I think we can support .jsx specifically. We could even add the Babel transform for .jsx files (though perhaps not depend on it).\nAccepting any extension is trickier. We might have to swap out the compiler. #577 discusses this with regards to source files but the same would apply here. I suspect we might solve that with an explicit extension registry, not just accepting whatever is matched by the glob patterns.\n. > If they explicitly define an extension just trust the user will add the require hook themselves\nYes, but this is the part we're currently missing. You can't swap out the Babel precompiler. Adding .jsx support is easier because it uses Babel.\n. > I would just as soon not hard code non-standard extensions into AVA if we can avoid it (which I think we can fairly easily in this case).\nWe are currently hard coding for .js so that would have to change.\n. @sindresorhus so remove code that restricts test files to .js extensions, but leave glob patterns as is? Presumably we're requiring .js extensions in case the glob pattern is too broad so that might become a problem.\n. Cool. What do we do about these lines though: https://github.com/sindresorhus/ava/blob/ad02c321ca5b537b59373643143575d1aea63adb/api.js#L334, https://github.com/sindresorhus/ava/blob/ad02c321ca5b537b59373643143575d1aea63adb/lib/watcher.js#L336.\n. Without getting into #229 territory we can quite easily support .jsx test files and automatically include the react preset. I suspect this would make AVA much easier to use for React projects. @kentcdodds?\n. @kentcdodds happy to accept a PR that adds React's test naming convention, if that removes a step.\nI don't think we should apply the react transform to all .js files. Still we could find ways to make that work out of the box, perhaps #619.\n. > There are many of these. Not really React specific. Definitely preference based.\nWell you got nyc to exclude some patterns (https://github.com/bcoe/nyc/pull/199). Conversely maybe AVA should include them.\n\n\nI definitely don't believe that we should arbitrarily add plugins and presets to AVA's built-in config.\n\nYea. Still, .jsx is pretty clear-cut IMO.\n\nMaybe having an init with informative questions would be helpful?\n\n:+1: \n. @sindresorhus @jamestalmage @vdemedes what's your stance on supporting .jsx test files out of the box, including the react transform?\n. Also, while we could support alternative extensions they'll still be treated as JavaScript. IMO that has less value than supporting .jsx.\n. > I guess we shouldn't run the React transform on everything.\n@sindresorhus indeed. My proposal is to add support for .jsx test files (update all locations where we're currently looking for .js test files) and only for .jsx test files add the react transform.\n. @joakimbeng running .jsx without also transpiling React isn't very useful though. We should commit to automatically transpliling .jsx test files with React support, and then allowing for test files with either .js or .jsx extension.\n. @niieani no, just allowing the extension won't magically let AVA transpile TypeScript tests.\n. > Small note on that - it's fairly common to use JSX without React at this point - see this and this\n@DrewML oh dear.\nDoes that mean .jsx files need their own Babel configuration? Or do the plugins refuse to transpile .js files?\n. Let's go for @joakimbeng's suggestion (https://github.com/avajs/ava/issues/631#issuecomment-221805713):\n\nPerhaps an ext flag/option like ESLint has would be a good solution? Which defaults to .js.\n\nYes, extensions in the config. No CLI flags, as per #1048. Default to .js, so if you want to support both .js and .jsx you'd need \"extensions\": [\".js\", \".jsx\"] in the config.\nNote that AVA will still assume the test files contain JavaScript. This is not yet a solution for supporting TypeScript test files out of the box, though you could run AVA via ts-node as a workaround.\n. @nkoder did you see #1159? That's the direction I'd like to take this in. I haven't made any allowances though for configuring Babel differently depending on the extension (mostly since Babel itself does not support this).. @rauschma this is blocked on the implementation of RFC 001, see the project overview. I've been focused on https://github.com/avajs/ava/pull/1341 instead.. > Maybe add an explanation for what happened to the property inherit?\nYea we'll document how to migrate to this new config.\n\nWithout babel.extensions, I can\u2019t currently work with AVA, because I even get errors if I mention files directly:\n\nRe-reading the RFC 001 and some of the discussion in this thread I think we can land support for babel.extensions now, without any of the other proposed changes. Specifying it under babel is paramount, as it conveys these files are transpiled using Babel.\nI'll remove the blocked label. If anybody wants to pick this up that'd be great.. Good point. We should make sure to retain compatibility with the current use of the babel options (until we change it all as part of RFC 001).. @Jamesernator there's an ongoing discussion around @std/esm in https://github.com/avajs/ava/issues/1512.. @vanga AVA performs some transpilations on the test files. These assume the test files contain JavaScript. Until those transpilations can be disabled we cannot support arbitrary file extensions, or indeed support non-JavaScript test files.. @feross I think we could recognize .mjs files, yes. We'd have to properly resolve imports and enforce strict mode though. I'd accept a PR for that, separate from completely disabling Babel and supporting an extensions option.. @Jaden-Giordano I've been focused on landing other improvements and breaking changes in preparation for a 1.0 release. That said we have enough pieces in place for extensions to be added. If you'd be willing to land a hand that'd be great.. @Jaden-Giordano it's not that easy I'm afraid. We need to hook this up to our Babel pipeline, and separately our compiled enhancements. (Though currently those are implemented using the Babel pipeline, say when we bring in TypeScript we may have a TypeScript-specific implementation.)\nSo ava.babel.extensions = [] should contain test file extensions to which the Babel pipeline is applied, as well as the compiled enhancements. ava.extensions = [] should contain test file extensions to which only the compiled enhancements are applied. To avoid ambiguities you shouldn't be allowed to specify the same extension in both lists. Also, we need to make sure the watcher recognizes these new extensions.\nWe need this refinement so you can specify ava.extensions = ['.ts'] and automatically opt-out of Babel compilation. Though until we have full TypeScript support you'll also have to set ava.compileEnhancements = false. . > So basically ava.babel.extensions should precompile with the CachingPrecompiler and ava.extensions should opt-out of that?\nYes, albeit with the nuance that the compileEnhancement setting is currently implemented through the precompiler. It's a bit roundabout, I know.\n\nWhich would mean the writer of the tests utilizing ava.extensions would need to precompile the tests themselves.\n\nThe test files may not even need precompiling, or alternatively you might use use something like ts-node/register to compile them on the fly.. > The global CLI just hands it directly off to the local AVA if available.\nYes, so if you're in dev/chalk the local AVA would be dev/chalk/node_modules/ava. The files pattern happens to be an absolute path though, and the test worker will require dev/acosh/node_modules/ava which might be a different AVA version. Similarly it might try to access the wrong .cache directory.\n. This would only be useful for test suites that take a really long time to execute though, right?\n. > Shouldn't this be implemented in the API? timeout can be useful for other consumers too, like the grunt/gulp plugins.\nThe watcher should follow this behavior as well.\n\nClearing and restarting timers can be expensive. Would be good to investigate the performance overhead. One simple solution may be to never clear the timer but track when the last test completed. Then the next timeout can be scheduled so it fires timeout ms since completion.\nTimers may sometimes fire a little early. Presumably this is only an issue when a very short timeout value is used though.\n. > Could you provide any links backing this info?\nThere's the infamous \"It sounds crazy, but disabling npm's progress bar yields a 2x npm install speed improvement for me\" issue from a little while ago. See the 3.7.0 release notes and https://github.com/iarna/gauge/commit/a7ab9c906bb72aa0ed8996a00db2cd35a22d5992.\nHard to say what the actual impact is on us of course.\n. @satya164 not yet, #583 will help with that though.\n. > It looks like the type of value we return from retrieveSourceMap inside test-worker.js changed from an object to a string sometime between 0.9.2 and 0.13 - I wonder if that has something to do with it.\nYea I think so. We should try to JSON.parse() the file.\n\nI am not entirely sure why source-map inlining is required (it worked fine without it in previous versions of AVA).\n\nIIRC some of the source map tools support map strings and objects, but others are more picky, or something like that. It might be that the retrieveSourceMap() result is being disregarded and the fallback is to look at an inline source map (or perhaps retrieveSourceMap() is the fallback for when there is no inline source map).\n. > It looks like the type of value we return from retrieveSourceMap inside test-worker.js changed from an object to a string sometime between 0.9.2 and 0.13 - I wonder if that has something to do with it.\nYea I think so. We should try to JSON.parse() the file.\n\nI am not entirely sure why source-map inlining is required (it worked fine without it in previous versions of AVA).\n\nIIRC some of the source map tools support map strings and objects, but others are more picky, or something like that. It might be that the retrieveSourceMap() result is being disregarded and the fallback is to look at an inline source map (or perhaps retrieveSourceMap() is the fallback for when there is no inline source map).\n. Oh wait got confused between test files, source files, and the source maps used by nyc.\nThe source map handling in AVA is so we can correct stack traces for errors coming out of the test files. Presumably this works fine even if retrieveSourceMap() returns a map string (due to the aforementioned liberal acceptance of JSON strings and objects).\nI suspect nyc is trying to generate coverage for the test files but can't find the source maps. Setting it to inline here might be rectifying that. I'll have to do some more digging to figure out how and why it's instrumenting the test files though, that shouldn't be necessary. I'm not clear at all on how it can even access the transpiled code.\nChanging the mapFile extension to .js.map may also help.\n. Oh wait got confused between test files, source files, and the source maps used by nyc.\nThe source map handling in AVA is so we can correct stack traces for errors coming out of the test files. Presumably this works fine even if retrieveSourceMap() returns a map string (due to the aforementioned liberal acceptance of JSON strings and objects).\nI suspect nyc is trying to generate coverage for the test files but can't find the source maps. Setting it to inline here might be rectifying that. I'll have to do some more digging to figure out how and why it's instrumenting the test files though, that shouldn't be necessary. I'm not clear at all on how it can even access the transpiled code.\nChanging the mapFile extension to .js.map may also help.\n. Sorry didn't have a chance earlier to play with this. I have some concerns around the keyboard interaction. For instance if you run the watcher through npm (say npm run test:watch), ctrl+c now causes npm to spit out an error log. Previously the SIGINT would be handled by npm run. One cannot use Enter to insert some linebreaks either, which can be useful to discern repeat output.\nctrl+\\ (SIGQUIT) causes a crash. ctrl+z is ignored.\nI suppose we could switch to a 0 exit signal for the npm interaction, and even make Enter insert linebreaks. But there's a bunch of other shell behaviors that are broken by this. How much would we want to replicate them and is it worth the hassle?\n. > Do people actually use those?\nThey can be useful when the app has trapped the SIGINT signal, but yea I don't really use them either. Just raising their existence and incompatibility with the restart mode in this PR.\n\nAgreed. That is a problem. Is there a way to 1) detect if you have a parent process, and 2) send a SIGINT to it? I would rather not exit with 0\n\nNot sure. I don't think the exit code matters all that much when in watch mode though.\n\nI believe the original r+enter behavior was modeled off existing libraries. I would assume they knew about rawMode as well. They may have evaluated this same behavior and rejected it for the reasons being discussed now.\n\nYes, see https://github.com/remy/nodemon/issues/162#issuecomment-17227347. Also:\n\nto listen for a keypress requires changing process.stdin to use setRawMode(true) which lets me read a character at a time, but this then passes on the stdin reading method to the child process (i.e. your node script) which would cause unexpected behaviour.\n\nNot sure how much this impacts us, it might be due to how the child processes are forked.\n. Just learned about SIGINFO https://twitter.com/robsmallshire/status/706763512903819265, another signal broken by this PR.\n(Not that it's very useful, it just outputs stuff like load: 1.85  cmd: node 18425 waiting 0.44u 0.06s)\n. Even if we don't revert we need to fix the exit code upon Ctrl + C.\nI'm for reverting though, seems like an uphill battle trying to solve the various edge cases when they surface.\n. Even if we don't revert we need to fix the exit code upon Ctrl + C.\nI'm for reverting though, seems like an uphill battle trying to solve the various edge cases when they surface.\n. @jamestalmage ah hadn't looked into it that deeply. Yikes.\n. @SamVerschueren that is expected. With 0.13.0 t.throws() only tests against errors, before it would try and cast other types to errors, which was wrong.\nNote that your examples are different. In the first you reject with a string, in the second with an error.\n. Node's assert.throws is rather overloaded isn't it? Maybe we could simplify the current implementation, especially given the changes from #493:\njs\nt.throws(fn) // Throws an Error (or subclass thereof)\nt.throws(fn, SyntaxError) // Throws a SyntaxError (or subclass thereof)\nt.throws(fn, 'string') // Throws an Error (or subclass thereof), whose .message === 'string'\nt.throws(fn, /regexp/) // Throws an Error (or subclass thereof), whose /regexp/test(err.message) === true\nWe remove support for validation functions, preferring you use the t.throws() return value instead:\njs\nconst err = t.throws(fn, TypeError)\nt.true(err.message === 'expecting integer')\nI'd still be in favor of the string and regular expression expectation shorthand if you're expecting an error but are not too fussed about its inheritance. Not opposed to removing that too and requiring users to run their assertion on the return value.\n(Also I haven't verified whether Node's assert.throws() can deal with the subclassing)\nWhile there's a strong convention for throwing proper errors we should still provide an assertion that can capture non-error exceptions. E.g.:\njs\nconst excp = t.throwsAny(fn)\nt.true(excp instanceof Foo)\n. Node's assert.throws is rather overloaded isn't it? Maybe we could simplify the current implementation, especially given the changes from #493:\njs\nt.throws(fn) // Throws an Error (or subclass thereof)\nt.throws(fn, SyntaxError) // Throws a SyntaxError (or subclass thereof)\nt.throws(fn, 'string') // Throws an Error (or subclass thereof), whose .message === 'string'\nt.throws(fn, /regexp/) // Throws an Error (or subclass thereof), whose /regexp/test(err.message) === true\nWe remove support for validation functions, preferring you use the t.throws() return value instead:\njs\nconst err = t.throws(fn, TypeError)\nt.true(err.message === 'expecting integer')\nI'd still be in favor of the string and regular expression expectation shorthand if you're expecting an error but are not too fussed about its inheritance. Not opposed to removing that too and requiring users to run their assertion on the return value.\n(Also I haven't verified whether Node's assert.throws() can deal with the subclassing)\nWhile there's a strong convention for throwing proper errors we should still provide an assertion that can capture non-error exceptions. E.g.:\njs\nconst excp = t.throwsAny(fn)\nt.true(excp instanceof Foo)\n. While the convention is for throwing errors (or subclasses) I don't think we should preclude users from asserting other values. E.g. I can easily imagine writing code that throws/rejects with object values that don't extend from Error, perhaps because they don't require an expensive stack trace computation.\nI'm in favor of t.throws(fn) requiring errors (regardless of the subsequent expectation parameter). This will help with accidental undefined throws (when throwing a variable reference). But we'd still need a way to capture any throw.\n. The language lets you throw any value. AVA should (easily!) let you assert the correct value was thrown, whether it's an Error or not.\nPerhaps t.throws(fn, 'message string') and t.throws(fn, /message regexp/) require the thrown value to be an error? t.throws(fn) wouldn't care and t.throws(fn, Constructor) would verify the prototype chain. If you want an Error but don't care about its message you'd use t.throws(fn, Error).\nI don't think we should have yet another argument, t.throws() is plenty overloaded as it is. And I'd like to remove support for validator functions.\n. > I assumed that AVA stopped asserting literals being thrown in the code\nNot really, see #582. AVA used to convert rejection reasons that weren't instanceof Error to errors by new Error(reason). That was crazy wrong.\nThe undefined undefined undefined issue you raised is due to Node's assert.throws() behaving differently than expected. That's related to throwing literals but I think it could also occur when using validation functions that reject errors. This is why I'm saying we should rewrite t.throws() to not use Node's assert.throws() at all, so we can get these things right.\nWhether t.throws() should accept non-errors is yet another discussion :smile: \n\n\nIf fn() throws a string literal, assert that err === Foo Bar.\n\nI don't think t.throws() should start comparing literals. Users should use the t.throws() error return value and perform their own secondary assertion.\n. It'd be good if after this long discussion somebody could summarize the expected API and behavior of t.throws(). There's still a lot of implicit complexity in the current implementation.\n. Please see #1047 for my proposal on simplifying t.throws().\n. Please see #1047 for my proposal on simplifying t.throws().\n. Whilst we haven't simplified the assertion interface yet, the error reporting has improved a lot. Closing this.. > Instead, I would recommend taking advantage of power-assert and just do t.ok(result == null);. That's even clearer than the word truthy, which many don't really know what means exactly anyways.\nMajor +1 on promoting power-assert. If only I could do t(result === null) :wink: \n\nI wish the functions were called truthy and falsy, I have never liked ok/notOk.\n\n\"Okay\" doesn't feel like an absolutist assertion. Seems on-par with truthy/falsy (and I never know how to spell falsey).\nSimilarly same feels sufficiently hand-wavy for a deep equals, with less typing required.\n\nThe codemod would be a must IMO. Maybe we could even ship the codemod with AVA for a while and offer a CLI prompt to apply it automatically. We already know which files are the test files from the config, so we could make it pretty easy for users. (I think deciding whether or not to ship it with AVA depends on how many dependencies the codemod would add).\n\nWould be good to have a codemod for breaking changes like this, but let's keep it as a separate package. Could implement a stub assertion that fails the test with a message pointing towards the codemod. If we ever get to that point.\n. Not opposed to changing this but I don't necessarily feel the need either \u00af(\u30c4)/\u00af\n. @jamestalmage \n\nI would argue you have at least a chance of understanding the intent of the truthy assertion using only knowledge of the Javascript language. truthy is probably the most commonly used word to describe JavaScripts treatment of non-booleans in conditional statements. Actually, if there are other terms besides truthy/falsy, I don't know them.\nIf I was describing some JS algorithm in English, I would never use the phrase \"If the returned value is ok, then do ...\". It's just too ambiguous, what does ok mean? I could imagine it meaning lots of things: \"properly formatted\", \"not an Error\", \"not a rejected promise\", etc. I think the only way you see t.ok and immediately understand what it means is if you are familiar with assertion libraries that use the convention.\nSo, that is my best argument for truthy/falsy. The ability to understand with basic Javascript knowledge vs. needing to have experience with certain assertion libraries. Neither is perfect, but I do think truthy/falsy is superior.\n\nWell that convinced me. +1 for changing to t.truthy()/t.falsy().\n. @jamestalmage \n\nI would argue you have at least a chance of understanding the intent of the truthy assertion using only knowledge of the Javascript language. truthy is probably the most commonly used word to describe JavaScripts treatment of non-booleans in conditional statements. Actually, if there are other terms besides truthy/falsy, I don't know them.\nIf I was describing some JS algorithm in English, I would never use the phrase \"If the returned value is ok, then do ...\". It's just too ambiguous, what does ok mean? I could imagine it meaning lots of things: \"properly formatted\", \"not an Error\", \"not a rejected promise\", etc. I think the only way you see t.ok and immediately understand what it means is if you are familiar with assertion libraries that use the convention.\nSo, that is my best argument for truthy/falsy. The ability to understand with basic Javascript knowledge vs. needing to have experience with certain assertion libraries. Neither is perfect, but I do think truthy/falsy is superior.\n\nWell that convinced me. +1 for changing to t.truthy()/t.falsy().\n. > I'd be fine with that as long as we provide a list of good utility libraries for doing common test cases (like deepEqual for instance).\nI think those batteries should come included.\n. > Even better\nHence t.same() :wink: \n. +1 for merging this as-is, we can do other clarifications as a follow-up.\n. See also https://github.com/sindresorhus/ava/issues/175#issuecomment-199014036\n. Couldn't we reference the required fs methods before loading any user code? Like we do for the various date and timer methods?\n. > We could, but we need to propagate that down to all the dependencies that might want to do that as well, including graceful-fs.\nHmm yea, fair enough.\n. :shipit: \n. @MicheleBertoli thanks for raising this and pushing AVA to its limits :) As mentioned #577 discusses ways to avoid the per-fork babel-register overhead. We're tracking fork performance issues in #78.\n. :+1: to overwrite the output, and the timestamp.\n@ben-eb thanks for using watch mode!\n. If we had a plugin architecture would this be something that could be provided in a separate module?\n. I've been closing issues from the backlog, but not always with an explanation \ud83d\ude04 \nI think this is something that can be provided in editor plugins, or otherwise by observing exit codes or through a TAP reporter. It's not something we want in AVA itself at this moment.. +1 to ignore the --tap flag in watch mode.\nWas thinking maybe to not even start watch mode but that's annoying if you specify the tap option in the config. Maybe print a warning?\n. Should this mandate an issue link? Bit chicken-and-egg but would save you digging through git blame etc.\n. Yea maybe not make it mandatory.\nHow's this?\njs\ntest.failing('this should work, but does not', t => { \n  ...\n}).see('https://github.com/sindresorhus/ava/issues/673')\n. > .see should throw a helpful message (including the test title) if used without .failing.\nOr it's only implemented on the .failing return value. We'll see.\n. @sindresorhus ,\n\nNot sure about .see, maybe .issue or .reference?\n\n.issue is quite specific to the GitHub issue tracker, other trackers might use different terminology. Hence .see which is easier to type than .reference :smile: \n. @sindresorhus ,\n\nNot sure about .see, maybe .issue or .reference?\n\n.issue is quite specific to the GitHub issue tracker, other trackers might use different terminology. Hence .see which is easier to type than .reference :smile: \n. @cgcgbcbc oh I missed that! I opened #836 for the .see() bit but forgot about the logger output.\nTo confirm, what we still need is to list the expected failing tests separately in the verbose and mini loggers?\n. The extends approach that @jamestalmage suggested should actually work. The env approach is another avenue you could try.\n@dcousineau I'm closing this for now. Please feel free to reopen if you still have issues.\n. @djskinner did you try specifying the transform-es2015-modules-commonjs plugin in pkg.ava.babel? I'm not sure if redefining presets with different options works the way you're expecting it to.. Nice catch @dcousineau!\nCould you perhaps add a test as well? Thanks.\n. > Yeah, I was going to copy the last test in the caching-precompiler test file but am hitting the end of my day. I'll circle back in a little bit and do so.\nNo worries. It's late here too ;-)\n\nAlso IRT the build failures: it looks like npm install failed on travis?\n\nWow all tests red! I guess we'll try again when you push the test.\n. Hey @dcousineau, I've taken your fix here and added a test, see #717. Hope you don't mind, we just want to try get a fix out soon :) Thanks for digging through the code and finding the bug!\n. How would we implement this? I wonder if we should have a plugin architecture with a documented internal API or something, then these things could be build by others.\n. Hi @will-weiss thanks for your report. I believe this is a duplicate of #324.\n. Actually I'm not sure now. @jamestalmage?\n. @thangngoc89 it seems fine locally (checked out https://github.com/MoOx/statinamic/commit/f89c11636d8d423f442e5f693e6f2779e740c024). Maybe somehow strangely it did cache something? The dependency cache is based on the package version which hasn't changed in master.\n. Oops hardcoded slashes in the assertions so the tests fail on Windows :(\n. CI passes now.\n. > Just thinking aloud, can we reuse the existing option --verbose to output objects without limit?\nCurrently --verbose selects a more verbose reporter which shows all test titles. I'm not sure it should impact power-assert depth, that feels like another level of verbosity.\n. I'm :+1: for changing the default depth to 3.\n. Yea, we can do this regardless of #703.\n. Hi @SamVerschueren. I don't think this PR is the correct solution to #661.\nFirst some background.\nt.throws() essentially wraps assert.throws(). That method supports constructor/instance checks, regular expression matches, and validation functions. We support those checks as well as an error message comparison validation.\nThese lines implement that error message comparison validation. err here is not the error thrown by fn(), it's how that error should be checked.\nIn this example the error message does not compare ('foo'.message !== 'foo'):\n``` js\nimport test from 'ava'\nasync function fn() {\n    return Promise.reject('foo');\n}\ntest(async t => {\n    t.throws(fn(), 'foo');\n});\n```\nThe reason you get undefined undefined undefined is that we're assuming a validation error like that causes an AssertionError to be thrown. However Node throws the actual error which is the 'foo' string.\nIn other words we need to check the error coming out of assert.throws() and call test() with the right arguments.\n. > Can you explain why it's not a solution for #661?\nt.throws() shouldn't reject literals. It should ensure something was thrown. If that something didn't match expectations it should correctly report that. Right now it's the reporting that's broken, not the validation.\n. > Can you explain why it's not a solution for #661?\nt.throws() shouldn't reject literals. It should ensure something was thrown. If that something didn't match expectations it should correctly report that. Right now it's the reporting that's broken, not the validation.\n. @SamVerschueren hope it's OK if I close this for now. I reckon resolving #661 will require some more changes.\n. @SamVerschueren hope it's OK if I close this for now. I reckon resolving #661 will require some more changes.\n. Regardless which name we pick we should carefully document which values are considered passing and which aren't. E.g. #630 surfaced that we were requiring constructor equality.\nI wonder if the \"same\" or \"equal\" part is implicit. You're testing an assertion with an actual and an expected value. t.same() is confusing because obviously you want things to be the same\u2026 and then what does t.is() mean?\nGiven that, how about t.deep()?\n. Regardless which name we pick we should carefully document which values are considered passing and which aren't. E.g. #630 surfaced that we were requiring constructor equality.\nI wonder if the \"same\" or \"equal\" part is implicit. You're testing an assertion with an actual and an expected value. t.same() is confusing because obviously you want things to be the same\u2026 and then what does t.is() mean?\nGiven that, how about t.deep()?\n. Which would lead to t.strict() for t.is(), if we want to go that far.\n. Which would lead to t.strict() for t.is(), if we want to go that far.\n. :shipit:!\n. Yea looks like the line could just be deleted, and then use strict can be added.\n. > t.same() isn't checking non-enumerable properties (color on the object returned from cc.rgb() in the above example wasn't enumerable).\n\nIs this desired?\n\nOh interesting! We use only-shallow under the hood which uses Object.keys().\nI'm not sure what the correct behavior is. I imagine non-enumerable properties are often used for internal properties so requiring them to be present in the expected value would be problematic. That would preclude t.same() from comparing such properties.\nWould be helpful if power-assert could list such properties, perhaps with an annotation. E.g.:\nAssertionError: { color [non-enumerable]: { model: 'rgb', value: [ 0, 0, 0, 1 ] } } === { color: { model: 'rgb', value: [ 0, 0, 0, 1 ] } }\n@twada?\n. @twada yes agreed. I wonder though if the error message could highlight non-enumerables so you stand a better chance of debugging your tests.\n. Oh I hadn't realized we had different output. Why wouldn't we repeat the initial output in the error summary? @jamestalmage?\n. With #1341 we have much improved value formatting and consistent object comparisons.. @dcousineau which version are you using? Could you try with master? If it still crashes perhaps you could share an example repo so I can fix the watcher crash? Thanks!\n. LGTMO!\n. :shipit: \n. :+1: on proposal 4.\nPerhaps we could expose the macro creating functionality through an ava/macro module. That way helper files won't accidentally become AVA runners.\n. > With proposal 4 macros are simple functions, and there's no need for a special macro creator.\nAh, I like it even more now!\n. What separates proposal 4 (test(macro, 2, 4)) from calling test with too many arguments? Can the macro define the test title? Do we want to support macros that create multiple tests?\n. Can you run a macro using just test() or should we stipulate test.macro()? I'm concerned about the test() overloading, e.g. with the array proposal, but also with the additional arguments.\nI like the macroFn.title suggestion.\n. > I think just test(). test.macro(macroFn) feels too long\nSure. It means that macros aren't anything specific, we're just giving the test implementation more powers.\n. Macros shipped in https://github.com/avajs/ava/releases/tag/v0.15.0.\n. Macros shipped in https://github.com/avajs/ava/releases/tag/v0.15.0.\n. We could do very conservative analysis and rewrite the .only to a private reference. Then if at runtime there are remaining t.only calls we can log a warning but ignore the exclusivity.\n. Yea, only statically recognized .only create exclusive tests. We can then warn for dynamic .only calls. That should still work for watch mode (it'll be better because the watcher can predict exclusivity). Not sure what to do about helper generated tests.\nYour suggestions in https://github.com/sindresorhus/ava/issues/696#issuecomment-204565550 seem on point. The only remaining issue seems to be \"If a user specified the .only extension however, I think it's really problematic to make the user wait while we launch additional files (possibly hundreds) to verify they have no .only calls.\"\nWe could modify the progress spinner to say \"looking for exclusive tests in [filename]\". Hitting Ctrl+C would abort and show the test results so far. That might strike a proper balance?\n. > If you are using dynamic only, you really need to use watch mode.\nYea happy to shift some of these advanced use cases into watch mode. It's AVA on steroids!\n\nWe could use is-ci to trigger the full scan, but people should not be checking in only tests so supporting that use case seems counter productive.\n\nIndeed.\nDo you think we should alert users when we encounter a dynamic .only call?\n. > Should dynamic .only throw for is-ci?\nIf it fails CI you'll notice.\n. In lieu of this we're going for #1472.. #78 will likely force our hand here. We're doing #696 so we can correctly run exclusive test but I doubt we'll try and suss out these issues statically. If we don't then we'll end up in situations where we're already running a whole bunch of tests before the fault is encountered. Then why not run to completion?\n. > No reason we can't report it as an error as soon as it's encountered.\nYou probably won't notice it though.\n. > Why not? If it increases the error count, that's noticeable immediately.\nFair enough.\n. This will be easiest to implement once https://github.com/avajs/ava/pull/1776 lands.. > Always rerun files that had failing tests regardless.\nI'd like to avoid that overhead.\nIf we tackle #672 then it should be fairly straightforward to inform the reporter of the previously failed tests. Shouldn't be too hard to track them in the watcher either.\n. We should do this on a case-by-case basis, if and when it makes sense for an external module. No need to keep the issue open.. Perhaps the methods in CachingPrecompiler could be reordered? There's a fair amount of jumping back and forth now when trying to understand the entire file.\nI'm not a fan of the Path suffix on the variable names. I'd prefer sticking to filename (since Babel uses it) and Dir for directories.\n. I like the smaller functions, makes the whole thing easier to understand.\n. :shipit: \n. I wonder whether we should change \"function\" here (and in the other messages) to \"implementation\". It's currently named callback in the docs. I struggled quite  a bit with the wording there.\n. Thanks @sotojuan!\n\nIf it looks good I can do a PR for the other messages.\n\nThat'd be great. Could you update the readme as well?\n. Just that typo. LGTM otherwise.\n. This will be done as part of RFC001.. We should support \"babel\": false. See also discussion around https://github.com/avajs/ava/issues/1488#issuecomment-322039248.. Closing in favor of #1556.. @Siilwyn no it's not! Here's a test I'm writing right now:\njs\ntest(`${prefix} babelConfig=false and compileEnhancements=false skips test file compilation`, t => {. @Siilwyn we're keeping Babel as an actual dependency for now, actually.. > I vaguely remember us talking about something like this being useful for watch mode.\nYes, #518. Fair amount of discussion in that issue.\nWith #78 we'll no longer be running all test files at the same time, so aborting the test run could let current test files run to completion without starting new ones.\n. @jamestalmage from #518 I think we should:\n- not start any serial tests that haven't yet been started\n- disconnect the reporters from the current run\n- as much as possible let the forks run to completion\n- but maybe kill the processes to avoid blocking the (upcoming) process pool\n. > This contains test results that we were storing directly on the API instance. That really doesn't make sense anymore given that the API instance is called multiple times by Watcher. I feel this way makes a little more sense.\nYea that's always stuck me as strange.\nNote though that the reporters reference these properties. Their tests use a stubbed api so they're still passing.\n. > Said emitter is an instance of the new TestData class (might be more appropriately named TestRun).\nRunStatus?\n. > I might just change it to pass testData as a param to reporter events. Some reporters could be stateless that way.\n:+1: \n. Currently blocked by #696.\n. Yes, thank you for noticing @bettiolo \ud83d\udc4d . Yes, thank you for noticing @bettiolo \ud83d\udc4d . > I imagine that we'll want to add a test for this, but I can't seem to find existing tests for the default inclusion glob...\nYea. I'm alluding to that in https://github.com/sindresorhus/ava/issues/736.\n. LGTM!\n. Our precompilation plans can be found here: https://github.com/avajs/ava/blob/master/docs/specs/001%20-%20Improving%20language%20support.md. This issue isn't directly actionable. I'm starting to update open TAP-related issues with the feature: tap reporter instead.. @florianb that's awesome, go for it!. > Ava seems to support ES2017 out of the box, there is no ES2015-sepecific section in the docs anymore.\nES2017 includes ES2015 \ud83d\ude09 \nhttps://github.com/avajs/ava/blob/master/docs/recipes/babelrc.md has further details beyond what is in the main README. The issue is that we can only transpile syntax, we don't modify built-ins. It's documented in https://github.com/avajs/babel-preset-stage-4, which is linked to, but we could spell it out better in at least the babelrc recipe.\nThis is why we should still document babel-polyfill.. IMHO we could remove the duplicate, invalid and wontfix labels. We've basically never used them.\n. @jamestalmage yup, perfect score!\n. @jamestalmage different use cases I think. Chokidar needs to watch tests and sources, and exclude particular directories. But we also want it to not exclude certain directories. Then we need to see if a changed file was a source or a test which again has different logic\u2026 Let me know where it isn't clear and I can add some comments.\n. Any further feedback aside from refactoring concerns?\n. Addressed feedback. Made some other changes to the docs, see the last commit.\n. Awesome!\n. > If they do --tap --watch it should throw. (\"watch is not allowed with tap reporter\")\nOops overlooked that requirement. Done now.\nAlso realized the error message when watch mode is used without chokidar being installed was written to stdout. Changed to stderr now.\n. Updated.\nPlease hold off on merging until a new release of has-flag is out.\n. > It's out. Bump it to 2.0.0.\nDone.\n. Updated.\n. > No need to do PRs for minor stuff like this. Just commit directly :)\nSure, but now folks now the documentation changed.\n. > I think the source file pattern should be made to match the test file pattern.\nYea maybe. Here's the list of source excludes. coverage may be tricky.\n\n\nI guess that makes sense, though I'm not sure how to override the default exclusion patterns. Maybe that's not necessary for test files?\n\nAren't you doing exactly that (overriding the defaults) in #614?\n\nYes, but that's based on the path prefix. So node_modules/whatever maps to node_modules/**/*. That's trickier when the exclude patterns are recursive (**/node_modules/**). Now every path component needs to be checked. Though I guess that answers my question :)\nMaybe we should move all this manipulation code into an external library.\n. @jamestalmage it's a fairly generic name when ignoring deeply nested directories. I doubt anybody would put legitimate code in say .sass-cache.\n. > Allow specified patterns to override the default exclusion patterns\nFor example if you mimick your lib directory inside the test directory and you have actual helper modules inside lib/helpers, you may want to put their tests in test/helpers. You should be able to add test/helpers to the files pattern so that AVA looks for test files in that directory. See https://github.com/avajs/ava/issues/909#issuecomment-228455326.\n. @buzinas not currently, no. You'll have to pick a different directory name.\n. > The default exclusion patterns for source files are relative to the current working (node_modules//*) however the test file exclusion patterns can apply anywhere (/node_modules/**). I guess that makes sense, though I'm not sure how to override the default exclusion patterns. Maybe that's not necessary for test files?\nCurrently this prevents tests from being shared through an npm package. Specifying an explicit node_modules/my-package/*.js pattern should select those tests in spite of the exclusion pattern. See https://github.com/avajs/ava/pull/1136#issuecomment-264709272.. There's more discussion in this PR: https://github.com/avajs/ava/pull/1320#issuecomment-288768089\n\nAt this point we're looking for help in summarizing the discussion into an actionable proposal, and then writing the code.. Rebased.\n. > One thing that might make it clearer is to add one more newline between persisted results.\n\nThis looks a bit cramped:\n\n@sindresorhus I'm not sure newlines will help. Just pushed a commit that inserts a horizontal sectioning line. No tests yet. Could you have play with that?\n. > Tests are failing.\nYea, probably due to the wip commit. Will patch that up tomorrow.\n. Updated!\nI changed the finish() output as well to be more consistent between the mini and verbose reporters. Each error/rejection/exception is now preceeded by two empty lines. Trailing whitespace is removed from stack traces. There's always an empty line at the end of the finish output.\nThis means that if the horizontal sectioning line is printed there will always be a single empty line before it, and a single empty line after it. Oh so very tidy! :stuck_out_tongue_winking_eye: \n. > This is definitely on the list of future enhancements though.\nSee #720.\n. LGTM!\nHad a quick look and I don't think we necessarily have any tests for enhance-assert so this should be fine as-is.\n. > The linked documentation isn't that helpful. We should probably make a short recipe about this\n:+1: \n\n(maybe a common-mistakes recipe?\n\n404?\n. Cool!\nShould update the default plugin sections in the readme and babelrc recipe.\n. :shipit: \nRegarding the link syntax, I suspect it might be easier for translators if the links are defined once, in a single place. It'd be easier to change the links to point at translated content instead.\n. @grvcoelho not at this moment. We have open issues around this, e.g. #909.. @Spy-Seth AVA checks environment variables to auto-detect your CI. You'll have to forward it to your docker container:\ndocker exec -it -e CI test-runner npm test\n(Like 80% sure on the -e shorthand)\nAlternatively you can force verbose output:\ndocker exec -it -test-runner 'npm test -- --verbose'\n. @Spy-Seth no worries. It's something that should be documented (#404).\n. @omnidan IIRC assertion failures don't throw within the test body, AVA just reports the first failure after the test has finished. This works as expected:\njs\ntest('sync', t => {\n  const obj = {}\n  t.truthy(obj.undef)\n  t.is(obj.undef.nope, 'whah?!')\n})\nDoing the same with test.cb causes the test to hang, that seems like a bug:\njs\ntest.cb('async', t => {\n  const obj = {}\n  t.truthy(obj.undef)\n  t.is(obj.undef.nope, 'whah?!')\n  t.end()\n})\nHowever your test doesn't hang, it results in an uncaught exception. I suspect supertest is rethrowing any exceptions from its .end() callback in a new turn. This causes the uncaught exception, which fails your entire test file and masks the previous assertion failure.\nTry wrapping supertest in a promise, like this:\njs\ntest('/register with valid POST data should create and login user', async t => {\n  const res = await new Promise((resolve, reject) => {\n    supertest\n      .post('/register')\n      .field('email', 'test@user2.com')\n      .field('password', 'test123') // not a good password ;)\n      .expect(200)\n      .end((err, res) => err ? reject(err) : resolve(res))\n  })\n  t.falsy(res.body.error) // shouldn't this fail already???\n  console.log(res.body) // { error: 'Registration failed' }\n  console.log(res.body.user) // undefined\n  t.truthy(res.body.user, 'this should be the error message')\n  t.is(res.body.user.email, 'test@user.com')\n})\nThis stops supertest from rethrowing any errors, and works around what looks like a .cb bug.\n. @omnidan if you return the promise it'll work correctly.\n. Looks like the callback bug was already tracked in #748. Will close this now.\n. This should also work, and is more explicit: node --harmony_destructuring node_modules/.bin/ava.\n. @vdemedes I'm pretty sure the argument would go to node, not AVA.\n. I'd prefer users do node --harmony_destructuring node_modules/.bin/ava.\n. I'd prefer users do node --harmony_destructuring node_modules/.bin/ava.\n. @LinusU by default only test files are run through Babel.\n. Throwing uncaught exceptions can occasionally be useful when writing application code (not small modules). I don't think we should have an assertion for this but we could conceivably make our uncaughtException handler extensible. Especially since it's too hard to mess with such handlers once they've been set up.\nSomething like this:\n``` js\nimport test, { uncaught } from 'ava'\ntest.serial(async t => {\n  doTheThing()\nconst err = await uncaught.catchNext()\n  t.true(err.message === 'The one I was waiting for')\n})\n```\nThe .serial modifier is needed so you can be sure to catch exceptions from this test. catchNext() would return a promise that is fulfilled with the first argument to the next uncaughtException event. AVA won't send this uncaught exception to the main process so the test keeps running. If no exception occurs the test will time out.\nUsing child processes is also possible but it'll be hard to serialize the exception and run assertions on it.\n. Nah, no need to do complicated stuff while there are better approaches.\n. Would be nice to draw up a desired architecture for how the API, CLI & Watcher interact with each other and how to abstract the shared pieces, including the logging. There's a fair amount of coupling at the moment, through methods that exist just to enable a slight variation of the behavior.\n. > Not a big fan of writing code to design documents, but as long as we don't take them as gospel, and just use it to agree on some basic goals.\nIndeed. Helps clarify direction.\n. It won't compute a diff when a package is installed from npm, so it shouldn't be too bad in practice.\n. It won't compute a diff when a package is installed from npm, so it shouldn't be too bad in practice.\n. I'll try and use execFileSync to get the diff. I like the idea of taking an array of paths, and it shouldn't be too bad to allow an optional seed, e.g. packageHash([path1, path2], seed).\n. I'll try and use execFileSync to get the diff. I like the idea of taking an array of paths, and it shouldn't be too bad to allow an optional seed, e.g. packageHash([path1, path2], seed).\n. Updated.\n. What issue did you encounter? The timestamp is in ISO8601 with a timezone included, and when I wrote this I was in BST not UTC so I'm surprised it would give different results.\n. What issue did you encounter? The timestamp is in ISO8601 with a timezone included, and when I wrote this I was in BST not UTC so I'm surprised it would give different results.\n. I'd imagine Travis is UTC. It's probably something else due to your environment.\nI understand your change now, it doesn't use the timezone so the time will always be correct. The original date (in UTC) was Nov 18th though :wink: \nHas anybody figured out the easter egg in the original timestamp yet?\n. Because each test file is executed in its own process the hooks implicitly cover the group of tests in the file. We're tracking support for test groups in #222.\nAlso, the top-level t.before() is nice when your setup code is asynchronous. For simple/synchronous test setup you don't even need to use a hook.\n. Closing in favor of #1047. See also #661.\n. At this stage I'm not convinced we need CLI flags to control AVA's usage of Babel. That's not to say this can't be improved or that AVA shouldn't integrate better with Babel improvements though. We're always open to suggestions there.\nWe won't add CLI flags to work around configuration issues. We're yet to be convinced we truly need support for non-package.json config files. Please continue to share your use cases.\n. Like with spies, shouldn't this be something that can be provided through a third-party package?\n. Yea maybe we can live with the duplication until we sort out .only, especially if pool limiting is marketed as beta. (Which is odd, because AVA is still major-0, but hey). Would be good to unblock users with lots of test files.\n. > tests are currently failing on the 3 .only test cases (since the entire API test case is run against the concurrency limiting code). There are a few options:\n\nFor now we do not run the 3 test cases against the concurrency limit code while we decide what to do with this feature going forward.\nWe wait until we've decided to either deprecate .only or reduce the scope of .only (only to current file) or we make another decision.\nI can easily implement suggestion 1.\n\nSuggestion 1 is good for now. Our primary objective is to unblock CI runs that fail due to the number of processes being spawn. Since this is an experimental opt-in feature we don't have to worry too much about .only not working.\n. @sindresorhus @jamestalmage @vdemedes what shall we do about the ready callback? Best I can tell it's unused so it should just be removed. With this PR it is emitted for every fork, rather than once.\n. \n. Just nits, looks good. Haven't tried it though, would be good to add a test case.\n. > not sure about testing because I don't think ANSI escape codes have been an issue or come up until the issue that led to this PR\nSure, but it'd be good to have a test to protect against future regressions.\n\nif anyone has a good idea of a test let me know\n\nCould add to the TAP reporter tests, one \"passing\" with a title that contains escape codes, and one \"failing\".\n. > The ANSI codes show up when you test more than one file\u2014is there a way to simulate that on the TAP tests?\n@sotojuan I don't think that matters, the test calls reporter.test() directly.\n\nAre we attacking this from the wrong side? Maybe we should color the test title in the reporters that needs it, not remove it here?\n\nSeems sensible to color when the test data is received, rather than in different reporters. Though admittedly there's more coloring going on in the reporters themselves. Maybe all of that can be improved in a future refactor?\n. > We should rather test the whole tap output to ensure there are no ANSI escape codes in any of the outputted information, not just the title.\n\ud83d\udc4d \n. Batching, as it were, has landed. Still I imagine there are easier performance wins than this. Closing to clean up the issue backlog.. > I really don't think there's any reason for this idea to end up in core.\nGoing ahead and closing this then \ud83d\ude04 \n. > I really don't think there's any reason for this idea to end up in core.\nGoing ahead and closing this then \ud83d\ude04 \n. I think you should check for metadata.always outside of the type !== 'test' condition. Currently AVA won't complain if you use say test.only.always(). It would also be good if the error message conveyed which combinations are valid, e.g.:\n``` js\nif (metadata.always && type !== 'after' && type !== 'afterEach') {\n    throw new Error('\"always\" can only be used with after and afterEach hooks');\n}\nif (type !== 'test') {\n    if (metadata.exclusive) {\n        throw new Error('\"only\" cannot be used with a ' + type + ' test');\n    }\nthis.hooks[type + (metadata.always ? 'Always' : '')].push(test);\n\n}\n```\nWould be good to test this error with test and beforeEach as well.\n(Maybe we should change \"test\" in the \"only\" message to \"hook\" too.)\n\n@cgcgbcbc almost there!\nI am wondering though whether this should just be the default behavior. Will discuss further in #474.\n. I think you should check for metadata.always outside of the type !== 'test' condition. Currently AVA won't complain if you use say test.only.always(). It would also be good if the error message conveyed which combinations are valid, e.g.:\n``` js\nif (metadata.always && type !== 'after' && type !== 'afterEach') {\n    throw new Error('\"always\" can only be used with after and afterEach hooks');\n}\nif (type !== 'test') {\n    if (metadata.exclusive) {\n        throw new Error('\"only\" cannot be used with a ' + type + ' test');\n    }\nthis.hooks[type + (metadata.always ? 'Always' : '')].push(test);\n\n}\n```\nWould be good to test this error with test and beforeEach as well.\n(Maybe we should change \"test\" in the \"only\" message to \"hook\" too.)\n\n@cgcgbcbc almost there!\nI am wondering though whether this should just be the default behavior. Will discuss further in #474.\n. That looks pretty great @CImrie! Would you be interested in turning it into a recipe? https://github.com/avajs/ava/tree/master/docs/recipes. Hi @rweng, this looks like a duplicate of #748. It should be fixed in master, could you try installing (npm install sindresorhus/ava#master) too see if that fixes it for you?\n. Hi @rweng, this looks like a duplicate of #748. It should be fixed in master, could you try installing (npm install sindresorhus/ava#master) too see if that fixes it for you?\n. Sure. AppVeyor is still failing though \ud83d\ude22 \n. What if we rewrite the require('ava') call in the workers to be a real path?\nI'm not sure about how often Node.js releases occur, do we really need to land this? I doubt we'd want to maintain this for the early Node 6 releases. On the other hand it'll help us during development, so maybe that's OK.\n. @Roshanjossey we'd like to have tests for https://github.com/avajs/ava/blob/master/bench/compare.js to start with. These could go inside a test/bench directory. Looking at the script it may need some refactoring though. Ideally the logic sits in another module that is passed a file path, which means you can test it against some fixtures. Right now it expects results to be in the bench directory which isn't ideal for unit testing (https://github.com/avajs/ava/blob/fa9abecad516b56391070c6324dc01110833da9f/bench/compare.js#L7).. We need to update the minimum convert-source-map version in package.json. Then we should be able to use absolute paths here: https://github.com/avajs/ava/blob/dd9e8b2effe541f9f232ee622452343dac5895dd/lib/caching-precompiler.js#L77:L78\n. Yup go for it @kugtong33!. @calebmer you should use the --timeout option to make AVA exit if a timeout is reached.\n. What additional information would you like to see? Could you elaborate on \"when I've tried using it every test file fails even if the timeout was met\"?\n. Fail fast causes the worker to be torn down without regard for test hooks.\nI think we should add a gracefulStop() (or something like that) to the runner, which prevents new serial tests from being started and waits for any asynchronous tests to complete, including their after hooks. Any further test results should be discarded.\nTests won't fail as fast as before, but cleanup will be more reliable.\n. > Cleaning up in a beforeEach also has a distinct advantage over always.afterEach when using fail-fast, in that the offending state is left in tact so you can examine it.\nfail-fast failing so fast that your code doesn't have a chance to clean up actually sounds like a feature. @vdemedes @sindresorhus @jamestalmage @sotojuan what do you think? If you agree we should document it as such.\n. #858 documents this. The issue is now closed since it's expected (and one might say desired) behavior.\n. We use source-map-support to readjust stack traces:\n\nThe module will by default assume a browser environment if XMLHttpRequest and window are defined. If either of these do not exist it will instead assume a node environment. In some rare cases, e.g. when running a browser emulation and where both variables are also set, you can explictly specify the environment to be either 'browser' or 'node'.\n\nhttps://github.com/evanw/node-source-map-support#options\nSo we'd need to set that option.\n. @nfcampos that'd be great!\n. I wonder if it's related to the Babel stack trace being spit out. Any chance you have a reproduction of this in a repo?\nWatcher log output has changed a lot in master, could you try with npm install sindresorhus/ava#master?\n. Watch mode no longer crashes when Babel compilation fails, which means that the cursor can be restored when exiting watch mode.\n. Watch mode no longer crashes when Babel compilation fails, which means that the cursor can be restored when exiting watch mode.\n. What problem are you running into that prompted this suggestion?\n. :shipit: \n. I don't think we need the failing test. Arguably this was due to a misconfiguration.\n. Thanks @nfcampos!\n. Looks good @cgcgbcbc!\nI have some nits but will follow up with a PR rather than putting you through more back-and-forth \ud83d\ude04 \n. LGTM!\n. Spread in the implementation does prevent us from adding any additional arguments, but having a third argument wouldn't be very pretty anyway.\nAn array in the declaration is just annoying and prone to confusion. So I guess I'm leaning towards spread in both places.\n. > Should the macro.title function get passed title as the first arg?\nI think that'd be good, but it makes it harder to write macros since it'd be optional. I'm specifically thinking about adding a space separator:\njs\nmacro.title = (prefix) => prefix + ' - macroA';\nLet's assume prefix defaults to the empty string:\njs\nmacro.title = (prefix) => (prefix ? prefix + ' - ' : '') + 'macroA';\nMaybe if the resulting title is always trimmed on its left so you don't have to worry about the spacing?\n. LGTMO!\n. Rather than extracting links, should we extract any immediately preceding comment?\n. It'll still be clickable.\n. Rather than adding more Babel transforms, we should solve this through linting, i.e. https://github.com/avajs/eslint-plugin-ava/issues/110.. Rather than adding more Babel transforms, we should solve this through linting, i.e. https://github.com/avajs/eslint-plugin-ava/issues/110.. > seems that I should just modify run-status? But that would influence a lot others?\nIt'll be fine. You're adding a new property anyhow.\nChanges looking good so far @cgcgbcbc!\n. See https://gist.github.com/novemberborn/8d0fabc0eefe26e055c8239ff8e26a29 for my test script. I tried it with the mini reporter and --verbose.\n. \n. > Would it be better to remove the time from all the modified tests if I have some specific tests that do test time on watch?\nYea. Having to add the time in all those tests should've been a clue that time was being output too much.\n\nPassing the boolean is a bit awkward, but I think it's OK. Ideally the logger understands test runs, so we don't have to interact with it as precisely as we do in the watcher.\n. Can probably move these two lines into the new test now.\nLGTMO!\n. Thanks @nfcampos!\n. @subash-canapathy could you provide an example (in Mocha) of how you use this information with your Saucelabs tests? The issue you linked to doesn't provide that kind of context.\n. @subash-canapathy great example, thank you. I agree would be good if AVA could provide the plumbing for this.\n. @fruch and others, PRs or implementation specs are very much welcome for this feature. I'm happy to give feedback as you familiarize yourself with the problem and the code.. > is afterEach.always being called also for skipped and todo ? (more ideas come in my head, but most of them are in the reporter category)\nNo it's not.. > Is it because of the new update in path.js in node 6 ?\nMost likely.\nCould you tell us which version of AVA you're using? And how you're using AVA?\n. @ThomasBem go for it!. @aemxdp interesting. https://github.com/avajs/ava/pull/1334 should fix that.. > I didn't add any test because there's no test file for run-status.js\nThere are title prefix tests in test/api.js, starting here.\n. > I've added a test, I don't see any way of testing this through the api without changing the current working dir so that's what I did\nThat should be possible, but I don't have time right now to play with it and see how. I can't quite tell how this test verifies that base is only removed at the beginning of the path, either.\nI suppose we should have tests for the __test__ filtering, but there's probably no tests for any of the other mutations either.\n. @nfcampos yea, sounds good!\n. @nfcampos looking good! The base test could still be a little better, but good to go otherwise I think. @avajs/core?\n. A bit like Sinon.JS matchers?\n. To be fair I've always found sinon.match rather confusing. t.like() comparing a subset of first-level keys is easy to understand, and could be a nice initial guard. Then you could add more assertions using destructuring.\n. Chai uses match for regular expressions.\n. @leewaygroups cool!\nI'm rereading the thread now and I can't figure out where we landed with deep objects. Perhaps we'll follow https://lodash.com/docs/4.16.6#isMatch as suggested by @jfmengels in https://github.com/avajs/ava/issues/845#issuecomment-220549923? We already use https://lodash.com/docs/4.16.6#isEqual in t.deepEqual. What was your take @leewaygroups?. > Drawback: 'chai-subset' is dependent on chai.\nThat's a show-stopper, unfortunately. Why do you consider it to be more robust than lodash.ismatch?\n\nSince lodash is one of the dependencies used\n\nWe don't depend on all of Lodash, so regardless we'd be adding a new dependency. That said, using modules of the same high-quality origin for our assertions seems like a good idea.. As of #1341 we're no longer using lodash.isequal. This feature will require partial comparison support in https://github.com/concordancejs/concordance.. @norbertkeri nope. It requires a solid proposal as to how this would work, ensuring the same kind of diff output as deepEqual gives us.\nWith regards to @frantic1048's example, if you can pass it the t object you can use t.log() which should work better.. > using t.true() is causing the erroneous output\nBy erroneous you mean \"not enough detail\"? That's a hard one to solve, hence the approach advocated by others.\nThe real solution is to have a partial-match assertion, of course.. It does make sense if you know you need to write a serial test (e.g. because all related tests in the same file are serial).\n. @zellwk we're always happy to see improved docs! It's probably easier to discuss improvements with a PR though, so there's some context to them.. @backspaces I think this issue relates to API endpoints more than browser behavior, though Karma might be one way to validate HTML responses. Recipe improvements are always welcome!\nWe'd also welcome work on https://github.com/avajs/karma-ava, though actually running AVA's test interface inside a browser is not a priority.. This has been solved with magic assert and the new formatting that landed in #1341.. @jamestalmage closing this since it's quite speculative.\n. @jamestalmage closing this since it's quite speculative.\n. Thanks @cgcgbcbc!\n. Probably won't have time for the next few days to look into this. There's no rush to get this out though?\n\ud83d\udc4d  on removing the --sources flag. Having those be relative to the package.json seems sensible. As does overriding files through the CLI, and those being relative to the actual CWD.\n. @avajs/core how interested in this are we? Clearly we've ignored this PR for 4 months now \ud83d\ude09  These kinds of documents are also hard to keep up to date. I'd rather we capture this stuff more clearly in the source code.\n. I don't think this is macro specific. Rather, we should show the relative file path and line on which a test was declared, perhaps in grey below the test title. So in the following:\n\nfoo would be followed by test.js line 3. The test() function could obtain a stack trace to figure out the line number. Of course the test could be declared in a separate file, so perhaps it should find the first call site from the test file itself. We need to see what the performance implications are of doing this.. @jamestalmage do we really need strip-bom?\n. We have\u2026\u00a0something, now. Closing since it's not clear to me whether there are any deficiencies in our current approach.. Yea for this to work properly we'd have to resolve the Babel config and use that as part of the salt for the cache.\nAlternatively we could print warnings when we detect Babel config inheritance/extension, with instructions on how to clear the cache.\n. @jamestalmage makes sense. Or a CLI command to wipe it?\nNote that the cache salt already includes the entire package.json, so it's really the .babelrc condition and further extends that we can't easily solve. Perhaps Babel exposes its configuration loader but IMO that's out of our scope.\n. > Perhaps disabling cache by default might be another way to this and add there a warning that cache is disabled and how to enable it to improve performance. That way you are making user aware that cache is there.\nAVA should be fast out of the box and not require more configuration in order to improve performance. And as @sindresorhus said regarding moving the cache directory next to the package.json:\n\nI don't think that will help much. You ignore the cache directory when adding AVA to a project and then forget. Later you experience a cache bug or another team member experiences it, and neither realizes it's a cache issue.\n\n\nSo let's focus on:\n- [ ] Resolving the actual Babel config as it's applied to the test files and using that as part of the cache hash\n- [ ] Improving documentation\n- [ ] Adding a command to clear the cache\n. @jescalan what AVA version are you using? We should be able to pick up Babel config changes much better with 0.19.0.. @jescalan could you share what changes you're making that are not causing test files to be recompiled?. Closing due to inactivity.. This looks pretty good @develar!\nI wonder if there's anybody who'd like to install this PR locally to debug their tests?\n. @melisoner2006 would you be interested in opening a PR to fix the recipe?. I like this @vdemedes! The watcher work required very tight coupling with the logger and specific reporters. Abstracting that to an event interface might restore some sanity.\n. I'm assuming this has been fixed, since #903 was merged.\n. I'm assuming this has been fixed, since #903 was merged.\n. @vdemedes:\n\nI'm only wondering whether it would mess up with people's expectations that everything should be working out-of-the-box. That is sort of how AVA works, it assumes good defaults and lets developers customize them only when necessary.\n\nAt this stage AVA can test Node.js code, so it shouldn't be too surprising you need additional setup to mimic a browser environment.\n@lukechilds are you still interested in adding your module to the browser testing recipe?\n. @vdemedes:\n\nI'm only wondering whether it would mess up with people's expectations that everything should be working out-of-the-box. That is sort of how AVA works, it assumes good defaults and lets developers customize them only when necessary.\n\nAt this stage AVA can test Node.js code, so it shouldn't be too surprising you need additional setup to mimic a browser environment.\n@lukechilds are you still interested in adding your module to the browser testing recipe?\n. Haven't really looked at the generate script, but I think it'd be fine for it to use ES2015 (and thus require Node 6). It's something we'd have to run locally and then check in the results.\n. I agree with @jamestalmage here. Let's continue the discussion in https://github.com/avajs/ava/issues/24#issuecomment-222362675?\n. Closing this since it's been languishing. Let's hash out our desired implementation in #24.\n. Are we OK with the TAP reporter writing output from the test worker to stderr?\nIs the remaining issue that we should implement t.comment()?\n. @jluchiji is this still an issue? If you're still using AVA, would you be able to share the project? Or the test output? How can you tell it hasn't finished all the tests?\n. > Hmm, wasn't .always supposed to serve this exact same purpose? To always run the clean up task, regardless of previous failure?\nThat was our assumption, yes. The question though is how --fail-fast is supposed to behave. The fastest way to fail is to forcibly exit the process. Trying to run just the always hook of the failing test whilst other asynchronous tests are active is tricky. For debugging purposes it can be useful if test state remains after a failure, --fail-fast would provide that behavior.\nAfter an uncaught exception there are no guarantees as to what code may still be able to run. Here too we end up forcibly exiting the process.\nNote that in both cases we first do IPC with the main process before actually exiting. I'm not quite sure how much code still has a chance to run. It's possible always.after is entered but if it does anything asynchronous then that gets interrupted. This needs some research.\nWe should decide what guarantees we want from --fail-fast and uncaughtException.\nAlso, our understanding of \"test cleanup\" has evolved to the point where we know see the best strategy of ensuring a clean test environment is to clean up before you run your test, and then clean up after to avoid leaving unnecessary files and test databases lying around. Hence the proposal for a .cleanup hook.\nNote that .always.after is still useful in other scenarios, e.g. #840.\n. @catdad \n\nWhile I don't disagree that it is a good idea to check that your environment is exactly as desired before a test, there has to be a stick in the ground saying that unit tests must not permanently alter the environment, even if there are other tools in the toolchain (such as .gitignore) which will handle that for you. And to the extent that that is the fault of the test framework, it should be treated as an egregious and urgent bug.\n\nA crash due to a bug in AVA would be something we'd fix, yes. But what if the crash is caused by the code being tested? There is no 100% reliable way to recover from that and undo changes to the environment. Similarly the --fail-fast feature is designed to leave the environment in the state in which the failure occurred.\nIn other words, unless your tests always pass, there will be moments where a test run leaves the environment in a different state. AVA can't know that's the case since it doesn't understand before/after code. At least the cleanup hook makes it easier to sanitize your environment before test runs (in case it was left in a dirty state), and after test runs (because cleaning up is nice).\n\nThere might potentially be issues with running unexpected cleanup code before the tests, as that is rather unorthodox among the other test frameworks.\n\nWe should clearly document the intent of the hook. But AVA is not afraid of being unorthodox \ud83d\ude09 \n. @catdad \n\nWhile I don't disagree that it is a good idea to check that your environment is exactly as desired before a test, there has to be a stick in the ground saying that unit tests must not permanently alter the environment, even if there are other tools in the toolchain (such as .gitignore) which will handle that for you. And to the extent that that is the fault of the test framework, it should be treated as an egregious and urgent bug.\n\nA crash due to a bug in AVA would be something we'd fix, yes. But what if the crash is caused by the code being tested? There is no 100% reliable way to recover from that and undo changes to the environment. Similarly the --fail-fast feature is designed to leave the environment in the state in which the failure occurred.\nIn other words, unless your tests always pass, there will be moments where a test run leaves the environment in a different state. AVA can't know that's the case since it doesn't understand before/after code. At least the cleanup hook makes it easier to sanitize your environment before test runs (in case it was left in a dirty state), and after test runs (because cleaning up is nice).\n\nThere might potentially be issues with running unexpected cleanup code before the tests, as that is rather unorthodox among the other test frameworks.\n\nWe should clearly document the intent of the hook. But AVA is not afraid of being unorthodox \ud83d\ude09 \n. @catdad It's only --fail-fast and crashes that prevent always.after from running. The benefit of cleanup over after is that it helps deal with those scenarios, too.\n. @sholladay I like your suggestion of having a --no-cleanup flag! @avajs/core?. >> I don't see the logic in skipping .afterEach when a test fails.\n\nIf it would be totally up to me, I'd consider removing .always and making it a default behavior for .afterEach.\n\nWe previously discussed this in https://github.com/avajs/ava/issues/474#issuecomment-217497254. The concern was that the after / afterEach hooks may have assumptions on the tests having passed / reached a certain state. Hence the .always modifier which acts as an explicit opt-in to having the callback called.\n. > I know AVA likes to pave its own path, and to good effect. But I think Intern really gets this right. It makes strong guarantees that if a test runs, its hooks run.\nYea that makes sense to me too. #840 discusses providing the test status to an afterEach hook. Combine the two and we can do away with .always. Users who need to guard against test execution state can either access the state or use another guard of their choosing.\n\n\nI think --no-cleanup may be a wrong way to approach this issue.\n\nI would ask \"why\", but it's going off on a bit of a tangent. I haven't personally needed something like that. But it does exist elsewhere and cleanly solves the \"leave the environment in the state in which the failure occurred\" story, even though I think that is a bad idea 95% of the time. If it needs to be a thing, it can be an option. I don't think it should be default.\n\nI think this is related, actually.\nWe have issues with bringing test execution to a halt when --fail-fast is enabled, see #1158. Always running after hooks makes that more complicated. Perhaps --fail-fast can imply --serial (while still allowing for test file concurrency). We can then add a --no-cleanup (only allowed together with --fail-fast) which is the only case in which after hooks do not run.\nI like how this simplifies the mental model for most use cases.\n. I'd like to focus on outstanding Babel interoperability issues.\nBut don't worry we won't shy away from making breaking changes when necessary, and we can support a deprecation path.\n. To recap, this adds a --debug option to the ava CLI command. AVA then starts its child processes (for running the test files) with debugging enabled (and the debugger listening at the specified port, if any). To ensure the port is available it only runs a single test file at a time.\nI think we should support --inspect now. I'm not sure how much value there is in opening the debugging port for other debuggers.\nWe'd need to check if the inspector URL is the same for each child process. The user would still have to reload the browser window every time a new child process is started. Ultimately perhaps we can use a proxy to inject an AVA experience into the inspector ;-)\n@jamestalmage any thoughts on the above? What's your work load like? Should this be closed and punted to an issue instead?\n. Closing due to inactivity. @jamestalmage has been incredibly busy.\nIf anybody wishes to take this on please comment.. This is fixed in master. If I were to guess, due to https://github.com/avajs/ava/pull/903.\n@tony-kerz please try installing AVA from GitHub until the next release is out. Or avoid bugs so your tests don't fail \ud83d\ude1c \n. This sounds good. @jamestalmage would you mind summarizing the design decisions? Which questions still need to be answered?\nAlso, \ud83d\udeb2 \ud83c\udfe0 wise, is it flaky or flakey? \ud83d\ude09 \n. > Is it flakey or flaky? (Both are correct, flaky seems to be more prevalent).\nI too prefer flaky because it's shorter. Also: Scotch whisky \ud83d\ude09 \n\nDo we alias the less prevalent spelling? Either way, only the more prevalent should appear in the docs. If we do alias, we should create a linter rule to allow users to enforce a particular spelling.\n\nWe could fail with a helpful error message if flakey is used.\n\nDo we allow for veryFlaky assertions? If so, how do we handle a scenario where the first test run has a flaky failure, the second has only a veryFlaky failure? I think you just tap out at veryFlaky max retries.\n\nI don't see the point. If it's going to be flaky it's going to be flaky. Configuring the number of retries is fine, but distinguishing levels of flakiness seems overkill. Let's start with just flaky.\n. > Are you suggestiong removing veryFlaky on assertions (t.flaky.is(...), etc), or eliminating it from the test API as well: test.flaky(t => {...})?\nBoth.\n\nI don't buy the argument for flaky assertion. IMHO if you have flaky assertions mixed with other working assertions, you should just move them into a separate test.\n\nYou could write a flaky assertion as a way to sanity check whether your test should be able to pass. E.g. a method that sometimes returns null, using t.flaky.true(result !== null) would be the sanity check, after which other assertion failures are actual test failures that don't require the test to be rerun.\n. > > You could write a flaky assertion as a way to sanity check whether your test should be able to pass\n\nYes, that is what I was thinking. It makes it easier to be explicit about what is allowed to be flaky. In that case though, you probably should write two or more tests. A normal test that tests only the non-flaky part, and a flaky test for the assertions that are.\n\nMy point was that you needed to test the result of a flaky function. There wouldn't be any non-flaky part to test without first verifying that the function wasn't flaky to begin with.\nTaken to an extreme you could argue you don't need test.flaky() if you can write flaky assertions.\n. I think we should:\n- Add t.flaky assertions\n- If a flaky assertion fails, retry the test\n  - Run afterEach.always\n  - Re-run any beforeEach hook\n  - Re-run test\n  - Max two times\n- Report flaky failures (https://github.com/avajs/ava/issues/934#issuecomment-228578745)\nI'm not in favor of test.flaky. Users should be encouraged to determine exactly what is making their test flaky.\n. I think we should:\n- Add t.flaky assertions\n- If a flaky assertion fails, retry the test\n  - Run afterEach.always\n  - Re-run any beforeEach hook\n  - Re-run test\n  - Max two times\n- Report flaky failures (https://github.com/avajs/ava/issues/934#issuecomment-228578745)\nI'm not in favor of test.flaky. Users should be encouraged to determine exactly what is making their test flaky.\n. @vdemedes I'm in favor of t.flaky actually \ud83d\ude09 \n. > I don't buy the argument for flaky assertion. IMHO if you have flaky assertions mixed with other working assertions, you should just move them into a separate test.\nThat might not always be possible.\n\nI would also consider having flaky tests an anti-pattern. It should only be temporary until you have time to make it non-flaky.\n\nOf course.\n. Decorating t seems brittle. I'd rather we do this properly.\ntest.flaky is probably easier to implement, so we could start there. I still like t.flaky as a way to force you to really think about what is flaky about your tests, but perhaps I'm overthinking it.. Hi @bbiagas thanks for your comment.\nI think the scenario you ran into is probably best solved by reproducing the flaky test in a small script that you can run in a loop.\nWhat I'm advocating for here is a way to deal with tests that are flaky due to circumstances out of their control, and ideally not bugs. It's helpful if they're retried once or twice, because you can't always fix those circumstances, but not so often that you end up ignoring bugs. I suppose it's the difference between \"ugh CI failed again because so-and-so timed out, let's just restart it\" and \"ugh this test fails a lot, I should look into that\". It's a tough balance to strike though.. Hi @bbiagas thanks for your comment.\nI think the scenario you ran into is probably best solved by reproducing the flaky test in a small script that you can run in a loop.\nWhat I'm advocating for here is a way to deal with tests that are flaky due to circumstances out of their control, and ideally not bugs. It's helpful if they're retried once or twice, because you can't always fix those circumstances, but not so often that you end up ignoring bugs. I suppose it's the difference between \"ugh CI failed again because so-and-so timed out, let's just restart it\" and \"ugh this test fails a lot, I should look into that\". It's a tough balance to strike though.. Very interesting suggestions @sholladay!\nAt this stage we're looking for somebody willing to champion this feature. It's not a priority for the core team.. https://github.com/avajs/ava/issues/1692 contains a proposal for tests to try some assertions, and if necessary discard the results. This is pretty close to the retry behavior talked about here.\nIntriguingly the retry logic can be built on top of this proposal, in a third-party module. I'm closing this issue in favor of that approach.\nIf you want to help out please drop by #1692. If you're writing such a third-party module let us know and we can give feedback and promote it.\nThanks all for the discussion in this thread. There's plenty of ideas for somebody to tackle.. See also #937, which may just be why watch mode isn't working as it should.\n. @sindresorhus no, the watcher runs in the main thread. The CLI behaves differently with watch mode depending on the directory you're in when you start it. This has been fixed for non-watch mode (#863).\n. ~~Somewhat blocked by #1048.~~\n. ~~Somewhat blocked by #1048.~~\n. @sotojuan yes.. @LasaleFamine go for it! \ud83d\udc4d . > Just to be sure, I can see a --source flag, not a --sources. Is right?\nYes.\n\nWe need to get that source prop from the pacakge.json?\n\nI think you can get it from conf.source. See #1046 for context.. I reckon they would for synchronous tests, but it'd require benchmarking, yes.\n. We should rewrite our reporters using ink. Still, we're OK at the moment. Closing to clean up the backlog.. With #1488 we'll be changing this behavior. Closing this issue, I doubt it'll lead to any edits to the recipe until those changes land.. I need to write up my thoughts but I'm doing some experiments with speeding up how we bootstrap Babel. That may make it feasible to load Babel in the test workers again, and even allow us to build in source compilation.\n. I need to write up my thoughts but I'm doing some experiments with speeding up how we bootstrap Babel. That may make it feasible to load Babel in the test workers again, and even allow us to build in source compilation.\n. Closing in favor of RFC001.. master no longer applies the runtime transform.. Yea, seems like the test runner should fail if a test is declared after it's started running tests (or a beforeEach hook etc).\n. It's experimental until we sort out remaining issues (notably .only behavior). You're free to use it though, and glad to hear it's helping with your use case.\n. Closing due to inactivity.\n. Closing due to inactivity.\n. @jgkim #32 changes the CWD to always be that of the nearest package.json. However if you were to run AVA from a different directory, with a relative NODE_ICU_DATA path, the path in the child processes will still be wrong.\nIn other words we still need to make the path absolute.\n. @jgkim #32 changes the CWD to always be that of the nearest package.json. However if you were to run AVA from a different directory, with a relative NODE_ICU_DATA path, the path in the child processes will still be wrong.\nIn other words we still need to make the path absolute.\n. We should do this, yes. It just completely breaks test.only(). I've been musing about how to \"fix\" that and it roughly comes down to:\n\nRefactor reporters so they keep a full buffer of their output, and can rewrite that buffer (kinda #939)\nWhen exclusive tests are detected, remove previous results from the output\nEnsure stdout/stderr from workers is routed through the reporters\nCache tests which in the last run had exclusive tests, and start those first\nThe above but better for watch mode\n\nThis would give us the experience of exclusive tests while not having to load all test files in advance (which then gives us other scheduling benefits).\nIt's not easy though\u2026 perhaps what we should do in the mean time is show an instruction when t.only() is used with multiple tests, and encourage users to run just that test file instead. And then we can run all tests within a worker pool and remove the current behavior in its entirety.. > It's a cool idea, but I think it's also too clever and not worth all the work and potential downsides. I'd rather spend energy on making it easier to filter down tests.\nAgreed.. @jordanh could you expand on how that interface works? What information specifically do you require out of AVA?. Given that AVA executes tests concurrently, both inside a worker process and across worker processes we think there's too much variability in execution time on the test case or even test suite level. It might average out OK for the entire test run (all test files and tests).\nI'm not sure how we organize \"suites\" in our TAP output. I personally wouldn't be opposed to adding timestamps for the \"global test suite\" (being all test files and tests) in our TAP output, but not at any more granular level.. > May I look into a patch and come back with real data vs. speculation?\nOf course. It's incredibly unlikely though that we'll expose time measurements on the individual test level, and even doing it for the total execution time may not like. Just trying to give you a fair warning \ud83d\ude04 . We're now open to including test duration in TAP output but only if --serial is used.  Please see #1668 if you're willing to help us implement this.. We're now open to including test duration in TAP output but only if --serial is used.  Please see #1668 if you're willing to help us implement this.. This is kinda Babel's problem, but it's still quite unfortunate. Not sure how much we should try and work around it, or whether the answer lies with AVA having improved support for use in Babel projects (i.e. not requiring babel-register).\nLeaving this open until we can find a solution.\n. This is kinda Babel's problem, but it's still quite unfortunate. Not sure how much we should try and work around it, or whether the answer lies with AVA having improved support for use in Babel projects (i.e. not requiring babel-register).\nLeaving this open until we can find a solution.\n. I think we could drop the source-map-support dependency. AVA catches all errors that it reports. We also write a source map comment to the precompiled files.\nIf Error.prepareStackTrace exists AVA should assume stack traces are accurate. If it does not exist, we can rewrite the call sites according to the source map when we're rewriting the stack trace. The necessary logic can be extracted from source-map-support.\nThis would work with @babel/register and the likes, since only one source-map-support instance would be loaded, and that instance will still rewrite stack traces for our precompiled files. This may also solve our nyc interoperability issues, provided the instrumentation takes into account any input source maps.\nThe downside is that printing the stack trace directly (like console.error(err.stack) or t.log(err.stack)) will result in uncorrected stack traces. This is probably fine, and we can document how to use source-map-support/register manually if necessary.. We currently have an inverse problem where @babel/register overrides AVA's source-map-support: https://github.com/avajs/ava/issues/1899#issuecomment-416204989.\nRegardless I think this issue is the best long-term solution.\n\nThe downside is that printing the stack trace directly (like console.error(err.stack) or t.log(err.stack)) will result in uncorrected stack traces. This is probably fine, and we can document how to use source-map-support/register manually if necessary.\n\nWe'll have to advocate the same in case AVA is used with files that it didn't compile, so it won't know how to rewrite the call sites. It can't quite go look for source maps, since other tools may have already rewritten the call sites before AVA got a chance to.. Oh latest source-map-support supports multiple source map retrievers: https://github.com/evanw/node-source-map-support/blob/595d533512ab294ce1fdca8501ad7a243173f723/source-map-support.js#L500\nSo if all tools would try to install the same version, things may actually work!. Wouldn't we have to continuously update the timestamp? (I assume this is to do with the watcher, right?)\n. I agree having a relative timestamp would be nice, but we'd have to keep it in sync which is a bit more work.\n. @simonbuchan \n\nIs there any reason ava --watch leaves the previous errors up? This would be out of scope for this issue, but ideally it would re-write to show only the still failing tests\n\nThe reporter buffer gets reset between test runs, so it can't clear earlier output. TBH I'm also not sure how terminals respond if you remove more lines than are currently displayed.\n\n939 implies a different approach to how we output logs, which could help with this feature request.\n. @simonbuchan \n\nIs there any reason ava --watch leaves the previous errors up? This would be out of scope for this issue, but ideally it would re-write to show only the still failing tests\n\nThe reporter buffer gets reset between test runs, so it can't clear earlier output. TBH I'm also not sure how terminals respond if you remove more lines than are currently displayed.\n\n939 implies a different approach to how we output logs, which could help with this feature request.\n. #939 is closed for now. Consequently I don't think we can make changes here anytime soon, so closing this as well.. The before hook isn't executed until the tests start. In this example you're passing the undefined values:\n``` js\nimport {test} from 'ava';\nimport {otherTests} from './otherTests'; \nimport {loadedDocument, loadedJson} from './asyncLoaders'; \nlet document, json;\ntest.before(async t => {\n    document = await loadedDocument(url); // async fetch stuff\n    json = await loadedJson(jsonUrl); // async fetch stuff\n});\n// call tests from another file\notherTests(test, document, json);\n```\nYou could do something like this:\n``` js\nimport {test} from 'ava';\nimport {otherTests} from './otherTests'; \nimport {loadedDocument, loadedJson} from './asyncLoaders'; \nconst documentPromise = loadedDocument(url);\nconst jsonPromise = loadedJson(jsonUrl);\nlet document, json;\ntest.before(async t => {\n    [document, json] = await Promise.all([documentPromise, jsonPromise]);\n});\notherTests(test, documentPromise, jsonPromise);\n```\nAnd in otherTests() repeat this bit:\n```\nlet document, json;\ntest.before(async t => {\n    [document, json] = await Promise.all([documentPromise, jsonPromise]);\n});\n```\nAlternatively use test.beforeEach() and assign to t.context:\n``` js\nimport {test} from 'ava';\nimport {otherTests} from './otherTests'; \nimport {loadedDocument, loadedJson} from './asyncLoaders'; \nlet document, json;\ntest.before(async t => {\n    document = await loadedDocument(url); // async fetch stuff\n    json = await loadedJson(jsonUrl); // async fetch stuff\n});\ntest.beforeEach(t => {\n  Object.assign(t.context, { document, json });\n});\notherTests(test);\n```\nNote that you don't necessarily need to pass test to otherTests. You can use import test from 'ava' in helper modules, they'll all get credited to the test file.\n. @asafigan how's it going? Wondering if you're still interested in completing this? Thanks :). Is the problem that you have a test file that intentionally contains the string //# sourceMappingURL=a.js.map, which then crashes?\n. Yea. I suppose the source map library we use just does a simple scan of the source. Will try and investigate further.\nAs a workaround you should be able to break up the string:\n`//# source${''}MappingURL=a.js.map`\n. I could only find https://github.com/babel/babel/issues/563 on this, and there is some TDZ related code  in the block-scoping plugin. So maybe this is a Babel bug or maybe Babel decided it's too hard to fix.\nAs far as AVA goes, we can do better by not transpiling language features that are supported by the Node.js runtime, which is #148.\n. I could only find https://github.com/babel/babel/issues/563 on this, and there is some TDZ related code  in the block-scoping plugin. So maybe this is a Babel bug or maybe Babel decided it's too hard to fix.\nAs far as AVA goes, we can do better by not transpiling language features that are supported by the Node.js runtime, which is #148.\n. @vdemedes the suggestion is to use path methods to do the comparison. Lowercasing wouldn't work for case-sensitive file systems.\n. This is the failing line:\njs\nt.throws(checkAgainst('4p455w0rd', await hash('4n0th3rp455w0rd')));\nIt's transpiled into:\njs\nt.throws(_avaThrowsHelper(function () {\n  return (0, _crypto.checkAgainst)('4p455w0rd', (yield (0, _crypto.hash)('4n0th3rp455w0rd')));\n}, {\n  line: 21,\n  column: 11,\n  source: 'checkAgainst(\\'4p455w0rd\\', await hash(\\'4n0th3rp455w0rd\\'))',\n  filename: '/private/var/folders/_6/p8qxp_3n62zg9081tvb0lcc80000gn/T/tmp.dkCnTDjWip/mawpedia/test/server/core/crypto.js'\n}));\nThe function wrapper was introduced in #742. It breaks the async/await transpilation, since the yield (0, _crypto.hash)('4n0th3rp455w0rd') is no longer used in a generator function.\n@nesukun the workaround would be to compute the hash outside the t.throws():\njs\nconst targetHash = await hash('4n0th3rp455w0rd');\nt.throws(checkAgainst('4p455w0rd', targetHash));\n@jamestalmage this is a gnarly bug. I suppose the workaround would be to hoist any await statements out of the wrapper but that might be rather difficult.\n. @nesukun another potential workaround:\njs\nconst p = checkAgainst('4p455w0rd', await hash('4n0th3rp455w0rd'));\nt.throws(p);\n. I've tried to improve our throws helper but got stuck on another edge case. If anybody's interested in taking this on, the PR is at https://github.com/avajs/babel-plugin-ava-throws-helper/pull/6.. Fixed in https://github.com/avajs/ava/commit/d924045a8f2f6ecafb27ff86e10a4396a38a60d5.. I've never used iron-node. @avajs/core any ideas?\nIs this something we should still include? Seems prone to breaking\u2026\n. Could anybody confirm whether this is still an issue?. Like #988, @avajs/core any ideas?\nIs this something we should still include? Seems prone to breaking\u2026\n. Could anybody confirm whether this is still an issue?. @rickmed could you clarify what git bash is? Going by https://git-for-windows.github.io/ it's a bash emulator thing, though with https://msdn.microsoft.com/en-us/commandline/wsl/about being available that may not be as necessary to use at it once was.\n. Lots of stuff has changed since this issue was opened. I'm closing due to inactivity, but please leave a comment if this is still a problem.. @LasaleFamine thanks for noticing this!. IMO we should change the runtime so you can't do these silly combinations. Having a stricter TS definition in anticipation of that change seems fine to me.\n. @thejameskyle if I'm not mistaken, currently our option-chain usage allows confusing combinations like test.after.beforeEach. I'd like that to raise an error. With typed JS flavors we should be able to exclude those combinations entirely.\n. I assume we're using option-chain because it's a nice way to create a chained API. But in actuality we don't want all those combinations to be used, and have written ESLint rules to prevent it. So then why not express the exact API and do away with option-chain?\n@avajs/core?\n. That, or use something else. Though I suspect @jamestalmage wrote option-chain specifically for AVA.\n. Opened #1182 to continue the conversation of which combinations are actually supported by AVA.. This should be solved using environment variables. Especially once we support a programatic API (#1043) it's hard to reason about whether --verbose was passed or the calling code set the option. Environment variables mean that AVA does not have to be involved.\n. You could get this via --verbose or --tap. It does mean you have to run the tests, but it sounds like you're planning to give this result after a test run anyway.\nAlso there's test.todo which you could you use as a placeholder. The number of such tests is reported after a test run.\n. @RyanAtViceSoftware this doesn't work due to how AVA spawns different processes when running the tests. There's some discussion in #929.\n. @ivogabe any chance you could rebase this? Changes look good otherwise.\n. I've updated #583 to also print pending tests after a timeout.\n. We're using chokidar, whose README does imply watching should work recursively. Does anybody else have experience with this?\n. @dchambers interesting! We can definitely do better in reporting these issues, I've just opened #1071 for that.\n. Yes the README should be clearer that you should really use t. We should add it to https://github.com/avajs/ava/blob/master/docs/common-pitfalls.md as well.\n. The Flow definition has been rewritten since this issue was raised.. Only skimmed the discussion so far, so my apologies if my summary is wrong.\nSounds like the suggestion is to prioritize rerunning the known failing tests, as well as any new test files. Track other tests that ordinarily would have rerun (due to source changes). Run those once there are no more failing tests.\nRight now that would require a new test run, though ideally we'd add to an existing test run. So there's some architectural limitations there.\n. @ngryman I like it!\nThe watcher is already quite complex; to be able to do what you're suggesting I think we need to do a fair bit of refactoring.\nMy thinking is that we should be able to start a test run and add more files to it before it finishes. That's a problem because currently (when not using the --concurrent option) no tests are run before all workers have spun up, but that's something we need to move away from regardless.\nThat way the runner can prioritize files with failing tests, while maintaining a second set of non-failing test files that should be rerun.\nIf we could reliably identify individual tests (e.g. if they have a unique title) we could prioritize those over other tests in the same file.\n. @istarkov are you implying you're no longer supporting babel-plugin-webpack-loaders? That'd be totally fine of course, but I suspect you may be the only person that can provide insight here. Could you let me know if you can't? If that's the case I'll close this issue.\n. Closing due to inactivity.. This seems like a problem that try/finally already solves, variable scoping aside, or indeed something that can be solved with before/after hooks. The extra syntax seems unnecessary.\nProps on the extensive test coverage though!\n. See #720.\n. > I will do the same thing I do with the knowledge of the time it takes to run individual tests. That's currently a feature in ava today right now.\nIt's been a summer without much AVA work for me so I might have forgotten, but IIRC we don't show individual test duration, do we?\nI would be interested in surfacing which tests are exceedingly slow, in order to help users optimize their test suits. I'm not convinced outputting raw numbers is a sufficient solution, especially given the variability in host machine activity, process scheduling, and test concurrency.\n. I don't see individual test time in my verbose output.\n. > I feel like can do something with the deltas though. If before PR X, tests ran in 1 minute faster, that's a big deal.\nThat's true. @avajs/core ?\nI don't think there's much value in tracking test duration when in watch mode, especially since the number of test files run can vary greatly.\n. @fearphage given how watch mode functions I'm sceptical whether it'll actually deliver the information as reliably as you're expecting it to. I'm also wary it encourages micro-optimization of test runs.\nAgain, we're happy to have this in regular output since large performance regressions will surface that way, but at this point we don't want this for watch mode.\n. That's this line:\njs\nvar pkgDir = path.dirname(pkgConf.filepath(conf));\nThat changed in #925. Looks like AVA now won't start without a package.json present. We use that file (or rather the directory it's in) to figure out various paths. I suggest that without it we just use the current working directory.\n. > Would be good if a package.json would be created in the cwd if it is not there\nI'd say that's up to you. ava init should just add to the existing package, not create a new one.\n. > Would be good if a package.json would be created in the cwd if it is not there\nI'd say that's up to you. ava init should just add to the existing package, not create a new one.\n. @pastak that error seems to come from Babel, not AVA.\n. @pastak that error seems to come from Babel, not AVA.\n. While yes, we do want to support this, I don't think we need to keep this issue open. We'll get back to this eventually.. @MKRhere no. I think it's more likely we'll require AVA to be run in a child process, but allow you to load your own extensions into that process.. @sotojuan I've found why the test was failing, there's another bug where we assume we can always find a cache dir. That's not true when running outside of a directory with a package.json file!\nPushed a fix for that. Also merged master into this branch which is a bit unconventional but I didn't want to force-push ;-) Squash & merge will take care of that for us.\n. @omerbn AVA loads config from package.json here: https://github.com/avajs/ava/blob/262911f02b0857ce3873e353a547bf009bf4e795/lib/cli.js#L24\nIt is passed to meow here: https://github.com/avajs/ava/blob/262911f02b0857ce3873e353a547bf009bf4e795/lib/cli.js#L74\nThe goal is to pass a different object to meow where the keys match the long option names in the meow configuration, so that we can change the package.json config to use different key names. E.g. pass source to meow but use sources in package.json.. Hi @yatharthk. There've been a few developments since I started this issue. We won't have an extensions flag, the require flag has been removed, and we're looking to remove the source flag (#937).\nI think a good place to start would be to stop passing default: conf in the call to meow. We could then merge the resulting cli object with conf. Hopefully that leads to the same behavior. It'll then allow us to decouple the config keys from the CLI flags where needed (though I don't think there are any such cases at the moment).. Hey @yatharthk,\n\nIs there a relevant issue or we need to create one?\n\nThis would be the one.\n\nShall I pick up the task of pulling out the default: conf when calling meow?\n\nThanks for volunteering!\nCould you check in with @ThomasBem first? He's running into an issue for which this would be the solution, though there's a workaround available to him as well. See discussion in https://github.com/avajs/ava/pull/1198#issuecomment-275073726.. @yatharthk looks like @ThomasBem is working around the issue. Do you want to take this then?. @yatharthk sure, feel free to post your questions here.. You're correct, and go ahead.. I think pick the correct default values out of conf, to pass to meow. Then read either from cli.flags or conf when passing values to the Api constructor etc.. You still need to provide default values based on conf to meow. I'm OK with copying the resulting cli.flags back into conf.. > The thing that's bit tricky for me here is understanding what you talk about picking default values out of conf.\nAh! I mean when providing default to meow, we should only pass those values that meow uses. So in https://github.com/avajs/ava/blob/b886c5b25ae54430f908c49f9932d8f998c41702/lib/cli.js#L73 change to default: { failFast: conf.failFast, serial: conf.serial } and so forth.. Conclusions so far:\n\nDoes t.throws(fn, Constructor) require the thrown value to be an Error?\n\nYes.\n\nShould t.throws(fn, 'string') and t.throws(fn, /regexp/) be supported at all, or would it be preferable to dereference the error and then use another assertion?\n\nWe'll support this interface.\n\nIs there a need for t.throws(fn, SyntaxError, 'string') and t.throws(fn, SyntaxError, /regexp/)\n\nLeaning towards not supporting this.\n\n@vdemedes \n\n\nDoes anybody have experience with / examples of asserting Error instances across contexts?\n\nDidn't completely understand the question, mind providing a quick example?\n\njs\nconst err1 = new Error()\nconst err2 = vm.runInNewContext('new Error()')\nerr1 instanceof Error // true\nerr2 instanceof Error // false\n\n@fearphage yea AVA just uses Node.js' API, and it gets messy fast. Hence this proposal.\n. > Is it difficult to code or reason about? Just a preference? I'm just curious.\nThe assertion becomes overly complex again, which is what we're trying to move away from.\n\nIf we're target an arity of 2, what about this as an option?\njs\nt.throws(fn, error => {\n  // insert error assertions\n});\nI like the encapsulation of the assertions related to the thrown error.\n\nThat'll be hard to distinguish from t.throws(fn, Constructor).\n. > We should also (and I don't remember whether this is already something that we're doing) fail and show an error when the thrown error is a string and not an Error.\nYes that's implied.\n\nFrom what I see, you're proposing the removal of the message too? That would make it inconsistent with the other APIs. Not sure I agree here.\n\nOh no! Forgot to add that\u2026\nI think that rules out syntax like t.throws(fn, SyntaxError, 'string'), since that collides with t.throws(fn, SyntaxError, 'message'). Padding it is just annoying (t.throws(fn, SyntaxError, undefined, 'message'). Though you would need to pad t.throws(fn, undefined, 'message'). I think that's OK though.\n. I like it!\n. @AlexTes we can go with @sindresorhus' proposal: https://github.com/avajs/ava/issues/1047#issuecomment-261483195\nMatch constructor by reference, name by equality, and message either by testing a regular expression or string equality.. > I had the need the other day to match against a custom error property. Should we support that too?\nI think that's better served by assigning the thrown error to a variable and asserting on that.. @AlexTes \n\nCan I ask, when would a name be preferred over a constructor?\n\nIt's usually better to compare errors using their .name. if (error instanceof MyError) doesn't necessarily work across VM contexts, for example. Libraries might also not bother exporting their error constructors.\n\nString\nCurrently unsupported by Node. Is this one to be an exact match of the string representation of the error? Or just the message of the error\n\nIt's currently implemented as comparing the message. We should do the same with the regular expression argument, IMO.\nHistorically the problem with the throws assertion is that it tries to do too many things, making it hard to understand what you're actually testing. Running a regular expression against the string representation of an error is an example of that. I didn't even know that's how it worked!\nPersonally I don't think we should maintain backwards compatibility, given how confusing throws is.\n. @AlexTes thanks! Be sure to check out #1314 which contains a few more changes.\n\nwithout the constructor, name is the preferred way to ID an error (should be more stable than message).\n\n@sholladay, I think Node.js is moving to standardizing error codes, though I couldn't find a definitive proposal on that. File system errors have code and errno properties. We should support whatever practice Node.js settles on, though we can add that later.. @StoneCypher When we implement t.throws() in AVA directly we'll make sure it explains why a thrown value still causes the assertion to fail.. Oh, I see that's #1440.. > other flags would we remove?\nJust require I think. Ideas for more?\n\n\nOn the other hand the --source flag may have to stay\n\nWhy?\n\nTo restrict watch mode on the fly to a more specific pattern.\n. Great! I've updated the relevant issues and opened #1069 to remove the --require flag.\n. Yes this is definitely an oversight with how we built macros.\n@avajs/core  / @twada I assume we treat any t.true expression in the test file as if it was a power assertion? I don't think we can scale that to every imported module (recursively!), especially if we'd want to cover macros distributed via npm.\nPerhaps we could add an ava/macro helper which compiles the received function source?\n. > Maybe we could come up with a naming convention for macros so we would know to transpile and power-assert'ify them, kinda like helper files\nWe could assume helper files contain macros? Intercept the require calls and transpile them separately.\n\n\nPerhaps we could add an ava/macro helper which compiles the received function source?\n\nCan you provide an example of how it would be used?\n\n``` js\n// my-macro.js\nimport macro from 'ava/macro'\nexport myMacro = macro((t, one, two) => {\n  t.true(one === two)\n})\n```\n``` js\n// ava/macro.js\nexport default function (macro) {\n  const source = macro.toString()\n  return applyPowerAssertAndEvaluate(source)\n}\n```\nIn other words we can get the function source and transpile it on the fly, then evaluate using the vm module. Perhaps with some caching. This would work for macros distributed as dependencies.\n. > > In other words we can get the function source and transpile it on the fly, then evaluate using the vm module. Perhaps with some caching.\n\nThat could work, but I would very much prefer something that just works, like now, than having to have a separate macro function.\n\nYea, I'm just trying to think about how to distribute macros via npm. I suppose you could distribute version compiled with power-assert and it may just be interoperable.\n. Leaving asides discussions on 1.0 releases and stable versions, I'd be happy to declare 0.16 / 0.17 to be a special < 4 compatible, critical bugfix only release. And then we can just move on.\nPresumably the critical bugfixes will be dealing with dependencies that start to require newer Node.js releases anyway\u2026\n. > Question is, when should we stop maintaining LTS for 0.10/0.12? It shouldn't be too long, because it will drag us down more and throttle development of the main branch.\nAfter the next release?\n. > Drop total support for 0.10/0.12 at the end of the year.\nSo no more bug fixes for 0.17? Fine by me.\n. > this is odd. You had to roll back to a previous version?\nI imagine it installed a newer, non-broken version.\n. I think https://github.com/avajs/ava/issues/643#issuecomment-248670505 should fix this.\n. @yamsellem I've expanded on the answer here: http://stackoverflow.com/questions/37900687/problems-with-ava-asynchronous-tests-when-stubbing-with-sinon#comment67089250_37900956\n. @sindresorhus \n\nDoes the caching take into account process.version?\n\nNo. Would have to add that in caching-precompiler.js.\n. And wow @jacobkahn that was quick!\n. \ud83c\udf8a\ud83c\udf89 Thanks @jacobkahn!\nIf you're up for another challenge, #937 is fairly similar \ud83d\ude04 \n. > I think we could simplify it to not show the stack trace if it's only the test function. @avajs/core Thoughts?\n\ud83d\udc4d . If you rebase against master you should be able to make CI pass.\n. > Ahh yeah, I see what you're saying. The fact that its even possible to create an Api() leading to an undefined pkgDir being given to fork is probably not a good idea.\nYea that's kinda what https://github.com/avajs/ava/issues/611 is about: making the interfaces more explicit.\n\nWhat about doing the following:\nSet pkgDir in the same way we do now, but in api.js instead, around https://github.com/alathon/ava/blob/2a65c1295588ebdf36dcc833484c34f3ed956add/api.js#L55\n\nI'd rather we keep passing it in, test issues aside. For instance https://github.com/avajs/ava/issues/1041 relates to this PR as well, and we should just solve that in cli.js.\nCould you go back to the previous implementation and fix the test instead? I'm actually OK with leaving the option implicit for the other places where tests create API instances.\n\nI changed things to reflect what I suggest here. The commit history is a bit of a mess now though, due to the rebase from master happening before a pull/push. My bad ^^\n\nNo worries, we'll just squash and merge it.\n. You might be about to push the commit, in which case, apologies, but don't forget to fix the whitespace in the README \ud83d\ude09 \n. Thanks @alathon!\n. @sindresorhus sorry :)\n. > I definitely wouldn't cache node_modules. If deps are removed from package.json, they won't be cleared. This introduces initial state to the build, meaning CI no longer verifies it will install from scratch.\nnpm still verifies all declared dependencies are present though. We could end up deleting a dependency that we still use, yes, but we have a similar problem with deduped dependencies.\nPerhaps we could run npm prune after installation?\n. > The latest XO prevents this through the import/no-unresolved rule.\nThat should help. Worth pruning then? Are you OK with caching in Travis given #1080?\n. Good start! This compiles the helpers as if they were test files, right? That seems correct to me.\nLet's see where that gets us before doing anything else for macros.\n. > Do we foresee people wanting to disable this behavior?\nSpecifically transpiling fixtures seems fraught.\n. Helpers should contain logic to make it easier to write test. Fixtures contain all sorts of files that relate to your test. If you're testing a Babel plugin you may write .js fixtures that you're using to exercise your tests. Those should not be transpiled.\n. The reason for upgrading, rather than leaving xo pinned at 0.16.0 is that when I'm editing using atom-linter-xo I still get all the errors from 0.17.0. This is annoying to me and possibly confusing to contributors.\n. @sindresorhus fixed conflict. Also added a commit for the loud-rejection thing.\n. @mightyiam their second example is go get around the bad typing, but that just causes a runtime error.\n@tugend yes the typing is wrong. Perhaps you'd be interested in fixing it? The type definition is generated here: https://github.com/avajs/ava/blob/master/types/make.js. I had to make some changes to the generator script and I think I found the issue. See https://github.com/avajs/ava/pull/1197.. Please try installing from master.. This will be fleshed out further when implementing RFC001.. > I think you linked to the wrong issue. Can you correct that?\nOops. Fixed.\n. Hi @guillaumevincent. This is a problem in execa, not AVA. Could you raise it there instead?\n. When you use --verbose AVA will persist the output. Is that what you're looking for?\n. > relevant, unresolved: #947\nThat would be the solution, yes.\nMeanwhile you could try something like this: https://github.com/novemberborn/buoyant/blob/4015fd703a4f26a4965f12a60c221fbb372ed3b3/test/lib/Scheduler.js#L11:L12\nLet me know how that works for you and I'll see if we can this to the Common Pitfalls documentation.\n. As of #1197 master no longer applies the runtime transform.. We now print all values through our formatter, so this issue is no longer applicable.. Assuming your browser code runs under Node.js without compilation then IMHO browser-env is still a good way of getting enough globals and DOM in place to exercise that code. AVA at least isolates different test files from each other. Our current browser testing recipe focuses on these kinds of tests.\nIdeally though you'd compile your browser code first and test that in a less \"hybrid franken-environment\".\nI'd like to see the recipe address this distinction. I'm curious how people exercise their browser code though, there may be quite a habit of running it directly in Node.js rather than properly isolated in jsdom.\n. Addressed with #1355.. > Have same exact issue if babel is not specified as inherit - ava fails to compile\n@dlebedynskyi Could you elaborate? How did you specify babel before?. @odigity There is no way to do this at the moment. To start we'd need a proposal on a) extending AVA, b) how to register new assertions, and c) how this would integrate with power-assert and our assertion output.. @odigity I'd do t.true(almostEquals(actual, expected)).. > But then you don't get the values for actual and expected, just a true or false value.\nFor t.true() we also print the values of the expressions in the argument, so you should see those values.. @lewisdiamond would you want to help us implement this?. @yomed if possible I'd recommend breaking up your tests into several files. Perhaps you could use a helper to generate them, and then create a test file for each language or variant?\nAlternatively you could build your own concurrency limiter. If you can get a promise for your test result then throat may be useful.\n. @tjbenton does limiting the concurrency of test file execution help (the  experimental concurrency option)? Due to the event loop CPU intensive tests will be blocking within a test file anyway, so I'm not sure how running them serially would make a difference really. I wonder if you're inadvertently executing too many intensive tests in concurrent processes.\nI think limiting the concurrency of tests can be helpful when the test run consumes too many resources, e.g. server capacity or memory.. > This is what happens If I change the tests in the 3 files with the most tests to be run serial\n\nava --verbose --concurrency 2    252.14s user 5.67s system 105% cpu 4:21.81 total\n\nWhat happens if you use --concurrency 2 without modifying the test files?\nYou're saying the tests are CPU intensive, but since Node.js uses an event loop that means the test that is currently executing blocks execution of the other tests, even if there is some initial asynchronicity that allows all tests to start at the same time. I don't see how making those tests execute serially would have a big impact, unless the problem isn't so much the CPU as it is disk I/O,  memory pressure or context switching between the worker processes.\nFor certain workloads where the CPU isn't the problem I can see how it could help to reduce how many tests execute concurrently within a single worker process.. Thanks. So for certain tests, reducing concurrency within the test file can help performance, besides reducing how many test files are executed concurrently. That's good to know.\nQuestion is, should AVA manage this or is this exceptional enough that it can be solved through a third-party module.\nI'm leaning towards this being picked up by a third-party module to start with. Down the line we could consider including it in AVA itself.. @Kikobeats something like this I think:\n```js\nconst limited = require('throat')(2)\ntest('foo', limited(t => t.pass())\ntest('bar', limited(t => t.pass())\ntest('baz', limited(t => t.pass())\n``. Your code does the same as my example, it just wraps the call to the limiter but withthroat` that's not necessary. It forwards the arguments.\nThe tests are invoked, yes, but the limit function means they don't do anything yet.\n. @Bareus have a look at https://github.com/avajs/ava/blob/master/docs/recipes/test-setup.md \u2014\u00a0you may want to avoid explicit hooks.\n\nI'm not sure how throat and ava handles concurrency internally so maybe a case could happen where all beforeEach hooks are processed first and then the tests.\n\nAVA will try and run hooks for all concurrent tests. If they return promises then AVA will pause the execution of that particular test until the promise fulfills.\nThe timeout resets every time a test result is received. I imagine you want to limit test concurrency because it's slowing down test execution. This shouldn't then impact the timeout, because presumably you'll have some tests complete faster.\n. @Bareus have a look at https://github.com/avajs/ava/blob/master/docs/recipes/test-setup.md \u2014\u00a0you may want to avoid explicit hooks.\n\nI'm not sure how throat and ava handles concurrency internally so maybe a case could happen where all beforeEach hooks are processed first and then the tests.\n\nAVA will try and run hooks for all concurrent tests. If they return promises then AVA will pause the execution of that particular test until the promise fulfills.\nThe timeout resets every time a test result is received. I imagine you want to limit test concurrency because it's slowing down test execution. This shouldn't then impact the timeout, because presumably you'll have some tests complete faster.\n. @thinkimlazy thanks for your PR.\nReading #843 I can't determine whether @sindresorhus wanted this fixed in chalk, or whether he wanted a flag in AVA. Ideally this just works without needing additional options.\n@sindresorhus?\n. I suspect [object Object] is because you're calling t.pass() with c:\njs\nconst valid = (obj, t) => {\n  return validation.validate(obj).then((c) => {\n    t.pass(c)\n  })\n}\nt.pass() takes an optional message argument. This is just a distraction though, the error is in:\n\n\u00d7 uuid Planned for 3 assertions, but got 2.\n\nLooking at your test that pans out. See annotations:\n``` js\ntest('uuid', (t) => {\n  t.plan(3)\nreturn valid({\n    uuid: 'f47ac10b-58cc-x372-a567-0e02b2c3d479' // FAILS, does not call pass()\n  }, t).then(() => valid({ // NEVER CALLED, because the previous valid() call failed\n    uuid: '123e4567-e89b-12d3-a456-426655440000'\n  }, t)).catch(ValidationError, (c) => { // HITS, first call to pass\n    t.pass()\n  }).then(() => valid({ // PASSES, second call to pass\n    uuid: 'a'\n  }, t)).catch(ValidationError, (c) => { // NEVER CALLED, because the previous valid() call succeeded\n    t.pass()\n  })\n})\n```\nThere are only two assertions called in your test, yet you've planned for three.\n. t.fail() will cause your test to fail right away. Which is appropriate for unwanted rejections.\nPerhaps try:\n``` js\ntest('uuid', async t => {\n  const first = await t.throws(validation.validate({ uuid: 'f47ac10b-58cc-x4372-a567-0e02b2c3d479' }))\n  t.true(first.name === 'ValidationError')\nt.notThrows(validation.validate({ uuid: '123e4567-e89b-12d3-a456-426655440000' }))\n  t.notThrows(validation.validate({ uuid: 'a' }))\n})\n```\nt.throws() and t.notThrows() work with promises, too.\n. > shouldn't it be showing what the exception were?\nI suppose, but now we're getting into the weeds of how Joi generates error messages. If you could share a repo I could npm install that would help. Can't immediately see if the Got unwanted exception message even comes from AVA.\n. This would be interesting, yes. I think we'd need to integrate more closely with Istanbul though. That's still a ways off so I'm going to close this for now.\nFor now you could use nodemon to rerun your coverage tasks when code changes. Not ideal but better than manually rerunning it.\n. > Would you be interested in a PR if we can work out the integration?\nOf course, but I don't think we can deal with integrating Istanbul for the foreseeable future.\n. Actually both t.is() and t.deepEqual() consider NaN values to be equal now. t.is() uses Object.is(), and t.deepEqual() uses that same function to compare numbers.. In the test file Set is replaced by babel-plugin-transform-runtime, see #947. There is a workaround, see #1089, but that hasn't made it into the documentation yet.\n(Edit: fixed link to workaround, thanks @ento!). Where are you setting / overwriting the variable?\n. I like to make a export default process module, then require that in order to access the process object. That way you can use something like proxyquire to stub it.\n. Since this started out as a documentation request, I'm closing in favor of #1505.. The plan is here, but there hasn't been progress on that lately: https://github.com/avajs/ava/blob/master/docs/specs/001%20-%20Improving%20language%20support.md. If we can run TypeScript through Babel then yea we can land some sort of support for that. Though you'd still need the equivalent to babel-register to load TypeScript source files.. I'm closing this issue since it's not directly actionable.\nOur plan for making AVA precompiler-agnostic can be found here: https://github.com/avajs/ava/blob/master/docs/specs/001%20-%20Improving%20language%20support.md \u2014\u00a0we'll need help achieving that.\nFurther, we'd be willing to support TypeScript test files sooner if we can run them through our existing Babel infrastructure.. @zixia well there are workarounds, but no proper solution, correct. Work on AVA is volunteer driven, so sometimes it just takes the time it takes.. Just adding the extension support makes it quite hard to understand which of AVA's behaviors is actually available, and as you say it requires further customizations to make the tests work. We want to land first-class support for TypeScript. We know how, but the work hasn't been done yet.. Hey @andywer! I've been thinking about this, and whilst I'm not adverse to linking to it I'd like to figure out an actionable plan to better support TypeScript directly, so we don't have to promote a fork for too long.\nFollowing up on my earlier comment: https://github.com/avajs/ava/issues/1109#issuecomment-326801788\n\nFurther, we'd be willing to support TypeScript test files sooner if we can run them through our existing Babel infrastructure.\n\nHere's what I think we can do pretty soon, which would provide minimal support for TypeScript. I'd like people's feedback on whether this would suit their needs.\nI'm assuming people are OK with using ts-node/register for on-the-fly test file and source file compilation. The reason we didn't want to support .ts file extensions before is that AVA doesn't know not to precompile them using Babel. However we're talking about disabling Babel altogether in (amongst other issues) https://github.com/avajs/ava/issues/1556:\n\nThere's something to be said for separating AVA's file manipulations from syntax transforms. The babel option could refer to syntax transforms only, while we have a separate transpileEnhancements option. This would supersede the current powerAssert option.\nSince we're still looking to integrate source transpilation and also support TypeScript, it makes sense (at least to me) that the enhancements are controlled separately.\n\nhttps://github.com/avajs/ava/issues/1556#issuecomment-340003308\nWith #1556 out of the way we could add support for an extensions option. AVA would treat matched files with the configured extensions as test files. It won't precompile them so it won't break on .ts files. ts-node/register then takes care of transpilation.\nOf course you could still precompile all files like you do now. It'd love for AVA to compile .ts files itself but that'd be follow-up work.\nIdeally you wouldn't need to configure extensions either but currently when the globbing rules match directories we look for .js files specifically. I'm sure there are possible improvements but that's for later.\nWhat do folks think about this approach?\n. > For registering ts-node itself, would you use \"require\" in the package.json#ava?\n\nSince --require was depreciated from the cli options, is it a limiation now that you can't seem scope what to require for what?\n\n@impaler pending 1st class support in AVA itself I think that's how it'd play out, yes. You disable AVA's Babel-powered treatment of test files and rely on language-node libs to transpile test files.\n\nBabel 7 has support for TypeScript, so all you have to do is use the typescript preset. I've found that enums and namespaces are not supported though. Everything else seems to work great.\n\n@lukescott does it do type checking as well, or does it just remove the syntax annotations? Regardless, it might be an option. With what I'm suggesting AVA would still default to applying Babel to test files and it's up to you to ensure Babel doesn't choke. So once we switch to Babel 7 you could go this route if it's the right fit for your project.. > As ava uses Babel 7\nTo clarify, that's the 1.0 beta releases.\nArchitecturally we're in a pretty decent place to implement built-in TypeScript compilation. It's still a big chunk of work though. So\u2026 any volunteers? \ud83d\ude04 \nWe can also start adding support for test file extensions that bypass the Babel pipeline, so that'd be a way to use AVA with the TS register hook. That has some performance issues of course, so it may not be right for some (or most) projects.. Yes, AVA can now be configured to recognize the .ts file extension. With TypeScript 3 the build tooling has improved a lot, so I think we can make AVA compile a TypeScript project, without requiring ts-node.\nI'd like to port some of my own projects to TypeScript so I'd really like to see improved TypeScript support in AVA. For now though my priority is getting the 1.0 release out.\nIf anybody would like to help out with this project please give me a shout.. @seacloud9 how are you adding that repository? I suspect it only distributes code using ES2015 module syntax, but if you install it into node_modules then babel-register won't transpile it. You'll need to configure Babel so it transpiles that particular dependency as well as your code, I suppose by tweaking the only option.. @zixia could you explain why you think register is needed in the workers? The CachingPrecompiler already compiles the test files, and the workers load the resulting code. On-the-fly compilation would only apply to files required by the test file. I'm arguing that, though not ideal, it's better to be consistent with our Babel implementation and not automatically transpile such dependencies. Thus, register would not be needed.\n\nWhere are the allowed extensions in cli.js? Did you mean we should save all supported extensions to an array in cli.js, then reference it in other js files?\n\nThere are none, currently, since .js is hardcoded in a few places. I'm suggesting you check for .ts after the hasFlag tests, before API is instantiated. Around here.\nZooming out a bit, CachingPrecompiler was really written for Babel. It uses the Babel config to salt the cache and it automatically loads Babel when it needs to compile something. Your proposed changes still cause the Babel code paths to execute and don't allow the TypeScript compiler to be configured. I think that's OK at this stage but we'll have to solve those issues at a later stage. @avajs/core thoughts?\n\n@jedmao perhaps we should revisit .tsx once this has landed? There's a parallel with .jsx which currently doesn't work either. Maybe we can treat these as aliases and just assume the existing .js / .ts setup supports JSX. It's complicated though because JSX doesn't necessarily mean React.. @zixia:\n\nThe reason I think register is needed in the workers is because if I do not do this, my test will not work anymore.\nI believe that's because my tests have to import pure TypeScript module file: there is all kind of TypeScript files in my source tree, and I never compile them, just import them in my tests.\nWill check them and reply you with the details later.\n\nRight, that's the same issue we have with our Babel support: source files are not automatically transpiled so users need to add babel-core/register to AVA's require configuration.\nThis is by no means ideal, but automatically including babel-core/register has a measurable impact on performance for those users who do not need it. I imagine the same applies to TypeScript.\nI've been playing with performance optimizations in this area (e.g. https://github.com/avajs/ava/pull/1082). Really I'd like to have users opt-in to a particular behavior for their tests and source files, and then have everything be automatic.\nFor now though I'm suggesting we do not automatically load TypeScript when running tests. The behavior would be the same as for users relying on babel-core/register.\nThis PR already introduces many subtle changes to AVA's behavior that should be factored out. That's a larger task and I think this PR provides enough value as it stands today, so we should try and land it. But not with automatic TypeScript loading.. Hi @zixia, I have some concerns with the direction we're taking here.\nAs already mentioned it's not clear how this would work for source files without running into the same performance issues we already see with Babel. The TypeScript test files won't get enhanced assertions or protection against improper t.throws() usage either. It's hard to communicate this to users when the way TypeScript support is enabled is to specify an extensions option.\nI've been doing a lot of thinking on how we can better solve these problems. Not only for TypeScript but for Babel-based projects as well. I've started writing an RFC with my specific proposal but I'm not ready to share it yet. I'm currently on holiday, and the weather is awfully nice (except when there's \u26c8 like right now), so I can't commit to any particular publication date.\nI didn't want to leave you hanging though. As I see it right now I don't think we can land this pull request. I'm really sorry about that, since you've put in a lot of work already.\nI hope you'll give my proposal a chance. It should enable us to make AVA's TypeScript support really awesome.\nThanks.. Hi @zixia, I have some concerns with the direction we're taking here.\nAs already mentioned it's not clear how this would work for source files without running into the same performance issues we already see with Babel. The TypeScript test files won't get enhanced assertions or protection against improper t.throws() usage either. It's hard to communicate this to users when the way TypeScript support is enabled is to specify an extensions option.\nI've been doing a lot of thinking on how we can better solve these problems. Not only for TypeScript but for Babel-based projects as well. I've started writing an RFC with my specific proposal but I'm not ready to share it yet. I'm currently on holiday, and the weather is awfully nice (except when there's \u26c8 like right now), so I can't commit to any particular publication date.\nI didn't want to leave you hanging though. As I see it right now I don't think we can land this pull request. I'm really sorry about that, since you've put in a lot of work already.\nI hope you'll give my proposal a chance. It should enable us to make AVA's TypeScript support really awesome.\nThanks.. @zixia see #1159.. @zixia see #1159.. We'll be taking a different direction with regards to TypeScript support, see RFC001. @zixia I'll let you know when the Babel side of that has stabilized sufficiently for you (or somebody else) to take on the TypeScript implementation.. @sindresorhus @thinkimlazy what is the actual bug here? Is it AVA overriding the chalk settings on a global basis? That seems like an issue with chalks design more than anything.. @creeperyang t.fail() only takes a message, just like the other assertions take a message as their last argument.\n@avajs/core perhaps it would be useful to typecheck that argument?. Cool. We're now looking to add a typecheck for the optional assertion message parameter, ensuring it's a string if it is provided.. @leewaygroups yes please! Thank you!. To clarify, per https://github.com/avajs/ava/pull/1831#issuecomment-396913731, we want to type check the message parameter when the assertion is invoked, and fail the assertion if the parameter is not undefined and not a string.. @pocesar going to close this, looks like you have a workaround, and #1104 should fix any remaining issues.. For future reference, I suspect the issue here is that dependencies were installed using sudo npm install. That causes node_modules to be created with unhelpful permissions. There's ways to change those but really you should avoid using npm with sudo.\nIf you've gotten into this habit for other reasons, perhaps see https://docs.npmjs.com/getting-started/fixing-npm-permissions on how to fix it.. > I often like to see the test results in serial order, when I'm developing.\nCould you elaborate on why?. @AutoSponge I'd write a little wrapper module that checks the Node.js version and requires async-to-gen/register if it doesn't support async/await.. Hi @ThomasBem! The test process that encounters the error simply exits if --fail-fast is enabled. This means the counts in the RunStatus will never add up to testCount.\nI think it'd be reasonable to compute a remainingCount when results are processed. Then the mini reporter and verbose reporter could report this number in their finish() implementation.\nI'm marking this as assigned, though I appreciate it took a while for somebody to get back to you, so if you're now busy then no worries \ud83d\ude04 \n. I noticed #1158 while researching this. Whatever we do there, it's likely that we won't have determined the total test count by the time a failure occurs. Thus the error message should say something like \"There are at least 5 tests remaining\". There might be more, but we just don't know.. > But is there a way for me to run the Ava code that i now have locally as a test-runner for a separate test project just to verify that everything works as intended all the way through?\nnpm link is your friend.. > But is there a way for me to run the Ava code that i now have locally as a test-runner for a separate test project just to verify that everything works as intended all the way through?\nnpm link is your friend.. Does specifying a ./node_modules/shared-tests as a test pattern work? (I'm not sure, and I don't have a dev environment on the laptop I'm tying this from.)\nDoes our macro support help your use case?\nWhat kind of tests are you sharing?. @mastilver ah, that traces back to https://github.com/avajs/ava/issues/226. https://github.com/avajs/ava/issues/736 discusses further changes to those patterns. IMO your setup should work, exotic though it is.\nClosing in favor of #736.. Could you provide more detail? Perhaps we don't resolve symlinks in test patterns but that doesn't explain the syntax error.\n. @mastilver I've managed to reproduce this issue, see #1143 for details.\nClosing this PR since we don't have a good way of accepting failing tests for AVA itself. Nice find though!. @vikfroberg I suspect you might need to put the --no-power-assert flag before the dist/test.js argument.. > Turns out ava --no-power-assert --no-cache dist/test.js works, so it seems to be a caching issue.\nIt should do. I don't think the --no-power-assert flag is taken into account when consulting the cache.\nThe salt is computed in this line.\nIt needs to include this.powerAssert.. modules: false disables transpiling the module syntax to Node.js' module system. You'll need to enable that for AVA to work.. @mandricore The modules setting needs to be true, which is the default. However the es2017 preset does not include module transpilation. It \"only compiles what's in ES2017 to ES2016\".\n\n@dlebedynskyi try extending your existing .babelrc. Then either load the es2015 transform with the modules enabled, or specifically add the transform-es2015-modules-commonjs plugin.. @dlebedynskyi that's odd. Not sure how to help you further though. Any chance you could set up a GitHub repo with a reproduction?. Ah! The issue is with babel-core/register using the .babelrc config, which isn't set up to transform module syntax. Currently there's nothing AVA can do about this.\nThat said, you could still simplify the ava.babel section to: \njson\n{\n  \"plugins\": [\"transform-es2015-modules-commonjs\"]\n}\nAnd the test files themselves will run.\nI've been thinking about how we could provide better support for Babel-based projects. I'll keep your use case in mind, where you want AVA to transpile source files using a different config than used by your build step.. @rnkdev please do create a pull request. It's easier to provide feedback that way \ud83d\ude04 \nYour approach looks fine, just needs a test. Looking forward to the PR!. > On the other hand, there is a new problem, when I added new direct symlink to a test file in the test, it fails to recognise that file exist.\nYou need to make sure it has the .js extension.\nLet's keep the test for symlinked directories, but also add one for symlinked files.. Ideally tests are independent from each other. If that's not feasible you could use test.serial(). That ensures that tests, within a test file, execute in the order in which they were declared.. AVA's philosophy is that tests should be independent of each other, and indeed should be able to run in any order. Sometimes tests require exclusive access to a particular resource, in which case you should use t.serial. Sometimes that resource is so exclusive you cannot even run test files in parallel, for which we have a global serial option.\nRegardless of where we end up with grouping tests the above will hold. The kind of test orchestration you're looking for is beyond the scope of AVA.\nThat said, AVA is pretty good with promises, and since by default all tests are started at the same time you could build a library on top of AVA which achieves the sequencing you're looking for.. You could have promises for the checkpoints, and then place relevant assertions in separate tests.. Yup. Or perhaps do all that setup in a test.before() and assign a promise for each stage to a variable. Then in your tests you could write const result = await buildStage and then do the assertions.. It should be async function findById().\nThis is unrelated to AVA so I'm closing this issue.. It should be async function findById().\nThis is unrelated to AVA so I'm closing this issue.. I wonder if some of the new files in lib can be placed in a subdirectory instead? lib is becoming quite hard to scan.. >> I wonder if some of the new files in lib can be placed in a subdirectory instead? lib is becoming quite hard to scan.\n\n@novemberborn Agree. Could we do it in a separate beginner-friendly PR instead?\n\n@vadimdemedes I mean, it's not a blocker, but it's probably easier for you to organize then for somebody who comes along later.\n. > Fix code excerpt extraction from non-test files when babel-register is involved (line number doesn't match the actual one)\nIs this a source maps issue? I've always assumed that babel-core/register causes conflicts in source-map-support. We'll be rolling our own solution with RFC 001 so perhaps this isn't worth worrying about.. @ORESoftware that only applies to Travis CI.. @ORESoftware that only applies to Travis CI.. You can reproduce this issue by creating one.test.js:\n```js\nimport test from 'ava'\ntest(t => t.fail())\n```\nand two.test.js:\n```js\nimport test from 'ava'\ntest(t => t.pass())\n```\nThen ava --verbose --fail-fast.. > So to keep the --fail-fast option working as intended, i would say shutdown any running processes. But i don\u00b4t know what problems that might cause since you talk about preventing cleanup\nTest files with failing tests are terminated quickly (but not immediately, in a synchronous, blocking sense). So --fail-fast already implies that any cleanup code you have in your test file may not run if there's an error. I'm fine with extending this behavior to other test files, too.\nI'm leaning towards forcefully stopping all other test processes and not logging any further output once a test has failed.. > So to keep the --fail-fast option working as intended, i would say shutdown any running processes. But i don\u00b4t know what problems that might cause since you talk about preventing cleanup\nTest files with failing tests are terminated quickly (but not immediately, in a synchronous, blocking sense). So --fail-fast already implies that any cleanup code you have in your test file may not run if there's an error. I'm fine with extending this behavior to other test files, too.\nI'm leaning towards forcefully stopping all other test processes and not logging any further output once a test has failed.. That seems unnecessarily complicated. The way I see it you'd use --fail-fast to quickly find errors so you can fix them. It helps if any state created by your tests is preserved (by virtue of not being cleaned up).. The state of the failing test would be preserved amongst potentially that of others.. Ah, so finish the currently running, non-failing tests, including their afterEach callbacks. That way most state would be cleared up, though you may see more than one failure in the test output.\nWe wouldn't be able to run the after callbacks though \u2014\u00a0they may assume the failing tests have completed and therefore crash.. #1693 ensures --fail-fast works with limited concurrency mode, preventing new test files from being run. It also fails fast after a timeout or if a test file crashed.\nAs part of #1684 I'll make sure we stop reporting subsequent test results once a test fails, while still letting the currently active tests run to completion.. > As part of #1684 I'll make sure we stop reporting subsequent test results once a test fails, while still letting the currently active tests run to completion.\nOK so what I ended up doing is that workers won't run other serial tests, or (as long as the error occurs early enough) concurrent tests. Tests that have started are run to completion, including hooks, and shown in the test output. If a test fails in one worker AVA will try and interrupt other workers.. > As my view, if we could get rid of the limitation in ava-files, ie: do not restrict the file extension, just let users define them...\nOne of the patterns we support is one that matches a directory. AVA automatically expands that to find all .js files within. We couldn't do that if any extension is allowed, yet not having to add **/*.js to your patterns is quite nice.\n\nThen we can use ts-node or babel-node to run ava with the specific language support, without any more code design/modification, and everyone will be happy. :P\n\nWhile tempting, I think the performance cost will be too high. It'll probably also stop us from doing things like #789.. > As my view, if we could get rid of the limitation in ava-files, ie: do not restrict the file extension, just let users define them...\nOne of the patterns we support is one that matches a directory. AVA automatically expands that to find all .js files within. We couldn't do that if any extension is allowed, yet not having to add **/*.js to your patterns is quite nice.\n\nThen we can use ts-node or babel-node to run ava with the specific language support, without any more code design/modification, and everyone will be happy. :P\n\nWhile tempting, I think the performance cost will be too high. It'll probably also stop us from doing things like #789.. We might not even include the TypeScript support module (I think that's what I proposed here, but on mobile so don't feel like checking right now).. > I wonder if we could come up with a better name for babelrc: true option, since it pulls config not only from .babelrc, but from babel prop in package.json.\nYea I hear ya. It's a (no longer documented, as far as I can tell) option in the transform APIs though. It refers both to .babelrc and the babel stanza in package.json. (In the CLI there is a negation flag: --no-babelrc.)\nWhat's tricky is that the sourceOptions and testOptions objects are supposed to be passed to Babel as-is. I'll do some more research and get in touch with Babel people for what the preferred setting is.. > I'll do some more research and get in touch with Babel people for what the preferred setting is.\nThough I haven't spoken with anybody, it seems pretty clear to me that it's babelrc. I've opened https://github.com/babel/babel/pull/5101 and https://github.com/babel/babel.github.io/pull/1134 to clarify Babel's documentation.\nAnything else or is this good to go?. I shall commence the project planning! \ud83d\udca5 . You should pass the bail option when RunStatus is instantiated: https://github.com/avajs/ava/blob/033d4dcdcbdadbf665c740ff450c2a775a8373dc/api.js#L145:L149\nIt'd be nice though if this wouldn't report when it knows no other tests didn't run. Which brings us back to the test counting logic.. You should pass the bail option when RunStatus is instantiated: https://github.com/avajs/ava/blob/033d4dcdcbdadbf665c740ff450c2a775a8373dc/api.js#L145:L149\nIt'd be nice though if this wouldn't report when it knows no other tests didn't run. Which brings us back to the test counting logic.. > It'd be nice though if this wouldn't report when it knows no other tests didn't run. Which brings us back to the test counting logic.\n@sindresorhus any thoughts on this? Happy to land this PR as-is and then do this as a follow-up.. Thanks @ThomasBem!\nOpened #1171 and #1172 for follow-up.. > with Node 7, I can run async/await without any babel plygin. I do run the code with Node(without plugin), and it works.\nAs far as I can tell Node.js 7 currently ships with V8 5.4, and async / await is only in 5.5: http://v8project.blogspot.co.za/2016/10/v8-release-55.html. How are you managing this?. > with Node 7, I can run async/await without any babel plygin. I do run the code with Node(without plugin), and it works.\nAs far as I can tell Node.js 7 currently ships with V8 5.4, and async / await is only in 5.5: http://v8project.blogspot.co.za/2016/10/v8-release-55.html. How are you managing this?. > v8 5.4 have async functions when use --harmony-async-await\n@eric7578 @cliguo are you using this flag?\n. > Ava shebang dont pass flags to node\nTo clarify, you mean the ava invocation in the npm test script?. Ah cool. Then yea, that makes sense. Your workaround is good, though honestly I think folks should stick to Babel until async / await is available without a flag.. > My suggested fix is to merge the Test and Macro types together since Macro is really a superset of the Test type, and we don't lose any type safety by just expecting to always get Macros.\nI haven't looked at (these, or any, really) Flow definitions much. From the test() method's perspective the t => {} function is a test implementation. \"Macro\" is more a way of explaining how to reuse test implementations, AVA doesn't care. Perhaps the types can be renamed Implementation rather than TestOrMacro?\nBesides that, if Flow-users could try out this PR that'd be great. I don't really know how to test it myself.. > My suggested fix is to merge the Test and Macro types together since Macro is really a superset of the Test type, and we don't lose any type safety by just expecting to always get Macros.\nI haven't looked at (these, or any, really) Flow definitions much. From the test() method's perspective the t => {} function is a test implementation. \"Macro\" is more a way of explaining how to reuse test implementations, AVA doesn't care. Perhaps the types can be renamed Implementation rather than TestOrMacro?\nBesides that, if Flow-users could try out this PR that'd be great. I don't really know how to test it myself.. I can't say I understand the reasoning for this. t.true and t.false are assertions for what is essentially a type check. If you use a typed language then this is verified at compile time, which pretty much makes the assertions unnecessary (other than to make sure the code doesn't crash).\nIn your example you return an Error if the argument is invalid. That seems odd, why not throw instead?. Great, thanks @sebald!. Thanks @goto-bus-stop!. > With #1160 AVA now prints a reminder when --fail-fast is used and a test fails. Any number of tests could have been skipped.\nActually, I believe it prints the reminder even if no tests have failed.. I think this is related to https://github.com/isaacs/node-glob/pull/301.\nWhen searching for test files, if we match a directory we search for nested *.js files. This ends up passing an absolute pattern to glob (via globby), but with relative ignore patterns.\nInstead we should make the pattern relative to this.cwd. \nSee https://github.com/isaacs/node-glob/issues/189#issuecomment-94215438 for more background into globs behavior.. @yatharthk you can take this if you want, yes \ud83d\ude04 . @Jolo510 actually I need to review the various globbing issues. I suggest you hold off until then. I think we may need a more comprehensive approach.. Hi @leewaygroups, just checking in \ud83d\ude04 . Can we use a Babel plugin to solve this?. Rephrasing yesterday's comment, now that I actually know what I meant to say\u2026:\nCould we support Babel plugins that rewrite t.jsxEqual() to automatically apply the JSON conversion? That way (assuming you don't mix JSX dialects within the same project) you don't need to do it manually. So option three, on steroids.. @ThomasBem @jarlehansen thanks for your collaboration \ud83d\udcaf \nI found one edge case. Tests in the file with .only() are not counted. This is because the test count is based on the scheduled tests (main.js), and non-exclusive tests are discarded (test-collection.js). This is why, before this PR, testCount was reset (run-status.js).\nI think we should count all tests in test-collection.js and use that value in main.js.\n. > Did not feel comfortable with changing some of the deeper behaviour of how tests where added and removed from collections.\n\nBut if you think its a good idea to change the behaviour to count all the tests even if there is .only tests present. Then i guess we can simplify the logic you linked to in test-collection.js similar to what was done run-status.js regarding exclusive?\n\nFair enough. But yes, let's change it. We should keep the counting consistent, regardless of whether .only was encountered in the test file being collected, or in a later test file.. @ThomasBem \n\n\u2026 I read this in the description of one of the api.js tests that was failing, https://github.com/avajs/ava/blob/ce42fcb7741039eeb26f0a167b27fd5923936a1b/test/api.js#L22\nIs this correct? That it will still run tests that are NOT tagged with .only() if they are in a file that has at least one .only() test?\n\u2026 this wording is still somewhat confusing to me. Because it does not run any other tests than the .only() as far as i can tell?\n\nThat's not how I read it. It's meant to say that if there are two test files, and only one contains exclusive tests, then none of the tests from the other file are run.. Great work @ThomasBem, keep it up \ud83d\udc4d . I assume currently the serial.afterEach callback is run after both serial and non-serial tests?\nFor your use case I'd suggest placing the serial tests in a separate file. It's an oddity of how the function chains are managed that you can type that combination.\nI think it'd be confusing to allow callbacks specifically for serial tests, more so since there is no test.parallel.afterEach equivalent.. Yes, those are potential issues, but easily solved by placing serial tests in separate files.\nI've opened #1182 so that eventually we'd remove support for test.serial.afterEach()-style hooks. Thanks for raising this!. Closing since this isn't an issue with AVA.. > test.serial.afterEach() (#1178)\nWhilst this one is potentially confusing, I don't want to stop people from using the import {serial as test} from 'ava' shorthand. It's not removed in #1670.. > test.serial.afterEach() (#1178)\nWhilst this one is potentially confusing, I don't want to stop people from using the import {serial as test} from 'ava' shorthand. It's not removed in #1670.. > I don't want to stop people from using the import {serial as test} from 'ava' shorthand. It's not removed in #1670.\nI've changed my mind on this. #1670 will remove hooks from test.serial, but #1684 will bring them back.. Thanks @gconaty!. It's 0 in Node.js 7.4:\n```console\n$ node\n\nfn=(foo = 'foo', bar = 'bar') => null\n[Function: fn]\nfn.length\n0\n```\n\nThis is a Babel issue. There'll always be a risk of subtle differences when using language features that require transpilation. The only solution is to carefully control what features you're relying on being available natively, and which you are compiling.\nThis is a nice edge-case though!. I agree with @mightyiam. Though it might be useful to exit with a warning if you use --watch inside CI.. Much like when --tap and --watch are combined we should print a warning when --watch is used in a CI environment, and then exit.. AVA refuses to start when --tap and --watch are combined. It doesn't correct it because that would be misleading. Given that in watch mode the process never exits, I'd find it misleading for that behavior to change in a CI environment.. @forresst yes that's my preferred solution. Sorry for not going your way @cncolder, but thank you for raising this ambiguity.. Tracking the \"require\" problem in #1506.. Thanks @ThomasBem.. > To my understanding this works because the setImmediate gets executed at the end of this 'tick' and the process.nextTick gets executed at the beginning of the next tick. However, my understanding of the node event loop isn't the greatest so I don't know if that technically correct.\nIt's the other way around, actually. process.nextTick schedules callbacks when the current turn of the event loop runs to completion. setImmediate is scheduled for the start of the next turn of the event loop.\nGiven that deasync hacks the event loop it's likely it's breaking the expected Node.js behavior. That's not something AVA will guard against. Perhaps try using execa to run your deasync'ed code in another child process from within your test.. Wow that's amazing @forresst!. > And what's wrong with stage 2? No one has to actually use any proposed future...\nBut they might, accidentally.\nWhen AVA started the language features that are now in ES2017 and earlier where not even shipping in Node.js. That's completely changed now, Node.js 8 should have full ES2017 support I think. AVA can still focus on making these features available to older runtimes but it doesn't need to advocate for features in earlier stages of development. This also aligns us with ESLint, which only supports stage-4 features.. > What about target detection? We are on target runtime. So why don't we detect and set the target accordingly?\nThat's the plan.. > Well, I've experienced a bug somewhere in some stack before, where something that was supposed to be watching files, wasn't catching all changes. One such experience is enough for me to desire to have clear indication as to what file changed and when. Yet, this information should be concise and stay out of the way.\nSee the documentation for debugging watch mode.\nOrdinarily this information is not required since the test output already shows you which files have rerun. Typically users shouldn't have to worry about why a particular test file ran. Sometimes watch mode is misconfigured, which is where the debug output comes in.\nClosing, but if you tried debug mode and it didn't solve your problem feel free to keep discussing here.. Landed in #1197.. > It works on the avajs/ava-files repository but not under avajs/ava.\nI assume it's used in the old tests? We're using AVA to run those tests, and of course AVA takes care of transpiling the spread operator. That's not the case with tap.\n. This is awesome. Thanks @forresst!. > It's good to go when it's done.\n@sindresorhus Could you approve it then? ;-) CI failure is just AppVeyor being confused.. > Don't you need to publish the extracted modules first and update package.json? Right now they reference Git commits.\nHence the DO NOT MERGE label \ud83d\ude09 . @danez yes, this sentence shouldn't mention stage-2:\n\nWe use our own bundled Babel with our @ava/stage-4 and stage-2 preset, as well as custom transforms for test and helper files.\n\nWe would appreciate a PR!. > Now I need to disable color output also if you try console.log with colors inside a test\nThat seems unnecessary. If you want to log colors then go ahead. It's just that one should be able to disable AVA's color output.. @ThomasBem I guess I disagree with @sindresorhus then \ud83d\ude04 \n@sindresorhus why do you think ava --no-color should disable colors not coming from AVA itself?. Ah, hadn't realized chalk takes a --no-color argument.\nWe've previously argued against forwarding command line arguments to the workers though.. > I don`t understand what this conf thing is and what its doing in default in the first place. It makes no sense to me :P\nThis may be relevant: https://github.com/avajs/ava/issues/1046#issuecomment-275071873\n\nI could not get it to work together with the new { color: true} so I just removed it for now and its causing a test to fail.\n\nThe test verifies that config in package.json overrides the default behavior. Currently the config is passed through the CLI parser (meow) using default, except that you removed that behavior :wink:\nIf you do Object.assign({ color: true }, conf) instead you should get the behavior you want, and a passing test. I think this is another argument for my suggestion in https://github.com/avajs/ava/issues/1046#issuecomment-275071873.\n\nI also need some support in where to start with passing the flag down to the child processes. That`s one of the things I have yet to understand with how Ava works :)\n\nSee here: https://github.com/avajs/ava/blob/872d2edb938ab1104b304f25aa586833b9875030/lib/fork.js#L39\nopts should already contain the colors value, you'll just have to translate it to the correct flag.. @Qix- I think your explanation makes sense, but I'll defer to @sindresorhus who I'm sure has got a better understanding of this area than me.. > The benefit of passing the flags is that we still respect FORCE_COLOR being passed to AVA by the user.\n\ud83d\udc4d . I see you're loading babel-register. Per https://github.com/avajs/ava/blob/master/docs/recipes/babelrc.md#transpiling-sources:\n\nNote that loading babel-register in every forked process has a non-trivial performance cost. If you have lots of test files, you may want to consider using a build step to transpile your sources before running your tests. This isn't ideal, since it complicates using AVA's watch mode, so we recommend using babel-register until the performance penalty becomes too great\n\nRFC 001 is the plan for improving this. See also https://github.com/avajs/ava/projects/1 where I'm starting to list the tasks.. babel-core doesn't have a default export. This isn't an issue for most Node.js modules but since babel-core was created using Babel it does trip you up when using Babel's module syntax transform.\nUse import * as babel from 'babel-core instead.. https://github.com/avajs/ava/issues/1222#issuecomment-280671306 is another example of this. I can reproduce that using:\n```\nimport test from 'ava'\nimport {jsdom} from 'jsdom'\ntest(t => {\n  const doc = jsdom('')\n  t.true(doc.documentElement.classList.contains('foo'))\n})\n```\nWith AVA 0.18.1 we dump out each property in that MemberExpression. With AVA 0.17.0 we only show:\nt.true(doc.documentElement.classList.contains('foo'))\n                                       |\n                                       false\nWhy did we remove the old formatter code? To get consistency in how we were displaying objects etc?\nWould it be an appropriate short-term fix to just show the last property, and hope it's not too massive?\nLonger term it might still be useful to show each property in the MemberExpression, but perhaps just their type? I don't think we can do that with pretty-format though.\n. Alternatively we could limit the depth to 1.. That doesn't leave us with much power-assert output does it?. Some more examples from @davewasmer (https://github.com/avajs/ava/issues/1278):\n\nI have several tests that assert that a file exists:\njs\nt.true(fs.existsSync(someFilePath), 'file exists');\nIn this case, magic-assert ends up printing the entirety of the fs object, which is ~170 lines long, and not super helpful here. In another case, I assert a value on t.context:\njs\nt.true(fs.existsSync(t.context.someFilePath), 'file exists');\nmagic-assert dumps t here, which ends up being nearly 800 lines long. This means lots of scrolling to find the info I actually care about (the value of t.context.someFilePath).\n\nWe should definitely filter out Node's built-in module, and the t object.. > If ava exposed more machine readable output, Denali could do some really interesting / cool things with that output since it knows much more about app than ava ever could.\nDespite my comment in https://github.com/avajs/ava/issues/1278#issuecomment-281412664, really interesting features can always sway us :). This now needs https://github.com/concordancejs/concordance/issues/12 to land.. > Does \"properties\" also mean return values from function calls?\n@hoschi, it does, yes.\n\nI don't know how much properties you want to log for each object in a test line, but probably it makes sense to count all the lines you logged already so you can check if this exceeds a limit.\n\nThe idea is to log a few properties for each object. It's pretty hard to find the correct trade-off though.. @hoschi yes thank you, that's a nice way of thinking about it. It's horribly complicated though! \ud83d\ude04 . My suggestion in #1204 is to show the type of arr but nothing else.. \ud83d\udc4d  on making titles mandatory.\nNot sure about forcing titles to be unique. I run into this from time to time when using macros, though perhaps that's only because the linter cannot resolve the macro.. > I would also be ok with requiring a test title for multiple tests, but I kinda like the ability to not have a title when you only have one test though. I use this a lot in my tiny modules. Example. Although, I'm happy to be convinced otherwise.\n@sindresorhus, that seems like an unnecessary exception. More work on our side for little gain on the user's side.\n@ThomasBem I think we have sufficient consensus on this feature. I'm adding the assigned label, so it's all yours. At the risk of being excessively clear, please note that the titles should be unique within a test file, not across all files.. @avajs/core everyone still on board making this change?. Thank you!. @hzlmn there isn't really anything to look at I'm afraid. Perhaps you could share a repository with a branch in which tests are working, and another in which they're failing?. Hi @mlewando, thank you for going through all this effort. We're changing how AVA handles test and source compilation. Supporting other extensions is inextricably linked to how compilation works (e.g. Babel compilation shouldn't apply to TypeScript files). This is documented in [RFC 001].\nAs it stands we won't be adding a global extensions. #631 is blocked by [RFC 001], though since I forgot to append the blocked label this wasn't as clear as it could be. So unfortunately I have to reject your PR at this stage.\nPlease do remain subscribed to #631 since I'll communicate there when it's ready to work on again.\nAlso in the future it's a good idea to check in with an issue before doing a significant amount of work on it. That way we can make sure somebody else doesn't work on it in parallel, and it can prevent situations like this.\n[RFC 001]: https://github.com/avajs/ava/blob/master/docs/specs/001%20-%20Improving%20language%20support.md. > For the sake of training I'm going to make it work on my fork anyway, so there is no lost effort from my side ;)\nGlad to hear!\n\nBy the way, yours tests runs very slow... after running npm run test-win it takes ages to finish and some error logs are so big that it is impossible to know whats wrong. Do you know any simple fix to run test faster and have better error reporting?\n\nWe use tap to test AVA, and it recently gained support for running tests in parallel. So there should be some speed-ups possible.\nWhen developing I tend to run specific test files, e.g. node_modules/.bin/tap test/ava-files.js.\n. I wonder if we could advocate JSX transforms that just do this for deepEqual. Was there any specific behavior in jsxEqual?. I think this comes down to #1175. We can't do a generic jsxEqual because we need to render the JSX somehow, which means choosing a particular implementation (e.g. React). The solution for that was to encourage Babel plugins that would translate the JSX for use with the assertion. But I'm not convinced those plugins need to latch onto t.jsqEqual calls. They can just detect the JSX AST nodes.. Detecting JSX nodes is pretty easy, yeah.\n\nOne thought: would this confuse users who'd use t.is with JSX too?\n\nYou can't use that to compare objects though, can you?. That confusion doesn't seem exclusive to JSX object comparisons.. We'd be happy to promote plugins written for this purpose, but this isn't something that AVA needs to support by itself.. I believe this is my comment: https://github.com/avajs/ava/pull/1154#issuecomment-276665936\nOur code assumes stack locations are accurate. We do this through source-map-support. Locations may of course still be wrong but there's little we can do about that, safe for taking ownership of the location mapping. Which we won't do for now.. > Is the issue that jest-snapshot is still waiting for the fix to be merged?\n@pho3nixf1re It's merged, but not released.. Actually, the jest-snapshot bug is exacerbated by AVA's implementation.\njest-snapshot always modifies the internal snapshot data when an assertion passes.\nWe're always saving the snapshot. This should be fine, because jest-snapshot tracks whether the existing snapshot is dirty, and those flags are not modified unless we tell it we're updating.\nHowever, as it turns out we're saving snapshots from inside the t.snapshot() assertion. This means there are unchecked keys in the snapshot, and the save actually completes.\nInstead we should save the snapshots after all tests in the test file have completed, but before the worker exits.. The reason we should save even without --update-snapshots is that this allows new snapshots to be automatically saved.. @gajus could you give this a try?. > Okay. What I have said is not true. ava is still 0.x. Therefore ^0.17 does not automatically pick up 0.18. Therefore, it can be a breaking change.\nYay us \ud83d\ude04  @gajus I agree that an incompatible change in typing is a breaking issue, though whether fixing a bug is a breaking change is a perennial SemVer debate isn't it.. @vadimdemedes I assume magic assert doesn't care about key order? I suppose the snapshot comparison does, which seems fair.. Please see https://github.com/avajs/ava/issues/1275 for context.. The issue is that we're actually comparing JSON strings inside jest-snapshot, so key order suddenly matters. That's not how jest-snapshot behaves otherwise (I think because pretty-format sorts the keys).. > plur doesn't handle was vs were so I decided to omit it unless it's fine for me to make a small function in both reporters to handle that.\nI think the current text is fine\u2026? Second-guessing my language skills now.\nYou can use plur('test was', 'tests were', runStatus.remainingCount) by the way, no need for an extra function.. @bjentsch yea, I can reproduce that problem. I think you're running into this issue: https://github.com/avajs/ava/issues/1204.. > Maybe you could used that instead of doing JSON.parse(JSON.stringify(actual)) ?\n@capaj yea that's a good idea.\n@vadimdemedes if we go this route we'd have to do string comparisons on the formatted values. Can we do string diffs in case the new value doesn't match the saved snapshot?. > Needs to handle removing stale snapshots, and do it correctly with test.only/test.skip. (As discussed on Hangouts) (Added by @sindresorhus)\n@sindresorhus what are you intending here? I'm under the impression that if all tests are run, the snapshots will be updated correctly. If you use t.only then at least you'll see an overly large snapshot diff. Is the only issue one where a test file is removed but we don't clean up the snapshot file? And does Jest handle this?. I've also added a few TODO items. Let's decide our direction first though: https://github.com/avajs/ava/issues/1275.. > Not just removing a test file. Imagine the user removes a single test. There will now be a stale entry in the snapshot file. I would expect AVA to remove the stale test entry in the snapshot when I run --update-snapshots.\nYes, I'd imagine it would do that. That's easy to do though. Removing a snapshot file after a test file has been removed / renamed isn't too hard either since we glob all test files and can compare that to the existing snapshot files.\n\nThis is made harder by t.skip()/.only() though\n\nI see. As you said, we can simply not clean up snapshots for skipped tests.. Humbly, I think this is superseded by https://github.com/avajs/ava/pull/1341#issuecomment-293959914 \ud83d\ude04 . Superseded by #1341.. We have @ava/stage-4 and @ava/transform-test-files now. What are you proposing @ava/babel does?. Ah. I think we should wait and see how the new presets get used, especially as we improve our Babel support.. See if and how people use @ava/stage-4 / @ava/transform-test-files, without adding a wrapper preset to AVA itself.. Turns out a preset like this is suggested in RFC 001:\n\ntestOptions: {}: specify the Babel options used to compile test and helper files. If provided this completely disables the default Babel configuration AVA uses to compile test and helper files. Like with sourceOptions, babelrc defaults to true. Set presets: [\"ava\"] to apply AVA's transforms.\n\nThough I suspect we may have to parse the options looking for that preset, and then replacing it with @ava/stage-4 and @ava/transform-test-files. The latter taking the power-assert option into account.. #1608 implements ava/stage-4 as an alias for our @ava/stage-4 preset. I've decided not to expose @ava/transform-test-files as I want to (conceptually at least) separate the enhancements it contains from AVA's Babel usage.. Could you elaborate on what specifically causes the crash? \n\nWhen parsing statements in lib/enhance-assert, we should ignore anything but plain objects and arrays.\n\nHow does this affect the output for t.true(some.spy.calledOnce)?. > Same goes with all the libraries. Unless it's a plain object or array, we can't pretty print it. Even if we could, why would we? The output needs to be meaningful and compact. So in the above example, t.true(some.spy.calledOnce) it's enough to print that some.spy.calledOnce is false.\nFair enough.\nI'd love to be able to print Map and Set though. And for those values we can't print, indicate their type.. > For example, you wouldn't want to pretty print an instance of Mongoose model, right?\nIt can be useful sometimes, but probably no \ud83d\ude04  Perhaps those kinds of instances could be restricted to showing the first three properties, with a little note saying \"59 more properties not shown\".. > I'd just go for a set of accepted types to be pretty-formatted in the \"statements\" output and push a fix. We can always make it more complex later ;)\n\ud83d\udc4d . @vadimdemedes do you recall the actual error? I assume you're using Sinon, but with sinon@1.17.7 I just get [Function proxy] for @ava/pretty-format output of the sinon.spy() result.. @vadimdemedes do you recall the actual error? I assume you're using Sinon, but with sinon@1.17.7 I just get [Function proxy] for @ava/pretty-format output of the sinon.spy() result.. As of #1341 we now limit how deeply we format values. There's loads of other suggestions in this thread we could still take on though.. The immediate problems are covered by #1204 and #1205. Beyond that we need to (eventually) support custom formatters for libraries like Sinon and Enzyme, but there's no point keeping this issue open for that.. IMO the parallel should be with the the form of notThrows that is passed a function:\njs\nconst val = t.notThrows(() => ({foo: 'bar'}))\nt.deepEqual(val, {foo: 'bar'})\nIs that a sensible thing to do, or is it not? Currently it doesn't work, and we should resolve that inconsistency one way or the other.. This is caused by optimistically transpiling helper files. With RFC 001 we'll only transpile them when they're required in the workers.. > With RFC 001 we'll only transpile them when they're required in the workers.\nClearly this hasn't happened yet.\nI think we should change how files patterns are interpreted, and then add support for helpers patterns:\n\nfiles patterns that match files cause those files to be treated as tests, not helpers\nWe support negation patterns to refine selections if necessary\nWe exclude {project-dir}/node_modules by default, though ideally you can override this by providing a more specific path\nfiles patterns that match directories mean that containing files are treated as tests, unless they're inside fixtures and helpers directories (that are children of the matched directories), or start with _ (regardless of how deeply they are nested)\n\nThis makes test file matching more understandable. We can then add support for helper patterns, so you can match those and we transpile them.\nA utility command that prints out what files are matched and how they're treated would also be useful.\n(Originally from https://github.com/avajs/ava/pull/1320#issuecomment-288768089). It should still be possible to explicitly select files that are otherwise ignored through .gitignore.. > the above case is an issue on the test code.\nCould you elaborate on the kind of issue? I'm assuming an error was thrown in systemjs?\nThe error must have had its stack trace corrected using source maps, but the original source is not in the systemjs package. I think code-excerpt should return null in that case.\nIt'd be helpful to know where the error came from and what the test looks like that triggered it. Why are we trying to generate a code excerpt for it in the first place?. @unional could you share the test that caused systemjs to throw?. I'm thinking we shouldn't show code excerpts for errors that don't come from the test or helper files.\nE.g. in this expression:\njs\nconst cacheKeys = (await result.getVerifier()).getCacheKeys()\nSeeing this isn't particularly helpful:\n```\n  main \u203a fromVirtual()\n  node_modules/md5-hex/index.js:9\n8:     const inputEncoding = typeof buf === 'string' ? 'utf8' : undefined;\n   9:     hash.update(buf, inputEncoding);\n   10:   };\nError: Data must be a string or a buffer\nupdate (node_modules/md5-hex/index.js:9:8)\n  module.exports (node_modules/md5-hex/index.js:17:9)\n  computeHashes (lib/Verifier.js:17:14)\n  Verifier.getCacheKeys (lib/Verifier.js:22:23)\n  Test. (test/main.js:329:50)\n  step (test/main.js:38:191)\n```\nGiven that the problem is somewhere in:\n```js\n      const hashes = from\n        .filter(item => envName ? item.envs.indexOf(envName) > -1 : !item.default)\n        .map(item => item.hash)\n  return md5Hex(hashes)\n\n```\n@vadimdemedes?. > It seems to break the watch mode when an error is thrown during an async test.\n@unional could you try https://github.com/avajs/ava/pull/1242? It should crash the watcher when this error occurs. That doesn't actually fix the magic-assert problem of course but at least in the future you won't be left hanging until you interrupt the watcher.. @vadimdemedes this part of the code also trips up when the error comes from an iterator:\njs\nArray.from({\n  *[Symbol.iterator] () {\n    throw new Error('Boom')\n  }\n}). I've opened https://github.com/avajs/ava/issues/1268 and https://github.com/avajs/ava/issues/1269 as follow-ups.\nI think for now we should show code excerpts when the error comes from a test, helper or source file, but not from dependencies. Let's leave .gitignore out of this. We can refine later based on user feedback.. This is primarily about comparing the order in which string and symbol keys are defined, right? E.g. in your example you'd still expect this to pass, based on http://www.2ality.com/2015/10/property-traversal-order-es6.html#traversing_the_own_keys_of_an_object:\n```js\nconst result = {\n  stderr: '',\n  stdout: 'Hello, World!\\n',\n  exitCode: 0,\n};\nt.deepEqual(result, {\n  stderr: '',\nstdout: 'Hello, World!\\n',\n  exitCode: 0,\n})\n```\nI hear ya that order could be important, but I don't think it's important enough to make t.deepEqual super finicky. If you want to make key order a part of your API you should test for it explicitly.\nI'd rather have a t.keyOrder(result, ['stderr', 'stdout', 'exitCode', Symbol.for('foo'), Symbol.for('bar')]) assertion than a t.strictDeepEqual(), but even that could be done through a third party library with the existing assertions: t.deepEqual(keyOrder(result), [\u2026]).. > As long as everyone is agreement that this feature is non-essential to the core, it is safe to close the issue.\nThere's agreement on that, yes. Thanks for bringing this up, it's an important distinction to be aware of.. Perhaps if we can't compute a diff then we should not display the \"Difference\" section at all.. > I saw that when I throw an error from another module the test.file refers to the module's file and not the file that is running the test.\nTo clarify, you mean test.error.source.file refers to the module's file?\n\nYour current approach means no stack traces are ever shown for errors that come out of the test file. I think that's an issue since users may write helper functions in the test file itself.\nInstead we should not output the stack when it only contains one line. So this should show the stack trace:\n```js\nconst testing = () => {\n  throw new Error('lol')\n}\ntest('testing', t => {\n  testing()\n})\n```\nBut this should not:\njs\ntest('another test', t => {\n  throw new Error('lolol')\n})\nThat's my interpretation of @sindresorhus' comment https://github.com/avajs/ava/issues/1072#issuecomment-276891510.. > I understood your point, actually initially I was thinking to check if the extracted stack trace was only one line, but I didn't know if that single line could contain some other info.\n\nI can proceed in this way if you agree.\n\nYes I think that's the way to go.. > Actually I don't like so much this solution because I'm calling twice the same function and I really need the comment to explain what I'm doing.\nYou could assign the stack to a variable:\njs\nif (test.error.stack) {\n  const extracted = extractStack(test.error.stack);\n  if (extracted.includes('\\n')) {\n    status += '\\n' + indentString(colors.errorStack(extracted), 2);\n  }\n}. \nThis is the output for the examples I posted in https://github.com/avajs/ava/pull/1234#issuecomment-277965444. Looks good to me! @sindresorhus?. Yep, see #1125. There is an in-progress PR at #1174.. Closing in favor of #1685.. I find it difficult to know where I need to fix the problem. Is the actual value wrong, or the expected value?\nMy example above refers to the code here. I'm inclined to take the - as \"remove this from the expected value\", but I need to do the opposite and add it instead.\nBut that's just in case the expected value is lagging behind, because I made the test more complicated. Just as easily there could be a bug, meaning there are missing values. Now if + means \"add this\", then should I add it to the actual or expected value? It's just as confusing.. @sindresorhus thoughts?. Yes. And we should improve the diff output, which #1341 achieves.. Strange, it works for me.\n\nInstall npm i avajs/ava#rethrow-watch-logger-errors\nModify node_modules/ava/lib/code-excerpt.js to const source = fs.readFileSync(file + 'foo', 'utf8');\nWrite test file import test from 'ava'; test(t => { throw new Error(\"uhoh\") })\nRun AVA: ./node_modules/.bin/ava --watch test.js\n\n```\n \u283c test \u203a [anonymous]\n1 failed/private/var/folders/_6/p8qxp_3n62zg9081tvb0lcc80000gn/T/tmp.vx8nC52ks6/node_modules/ava/lib/watcher.js:15\n        throw err;\n        ^\nError: ENOENT: no such file or directory, open '/private/var/folders/_6/p8qxp_3n62zg9081tvb0lcc80000gn/T/tmp.vx8nC52ks6/test.jsfoo'\n``. Even withimport test from 'ava'; test(t => Promise.reject(new Error(\"uhoh\")))the watcher exits for me due to the reporter error.. Thanks @despairblue!. Hi @jhnns. The code path wherechalkis used only applies when you run a test file with node itself, e.g.node test.js. I think the only change that's needed is to requirechalkinside theif (!isForked) {` body.\nThat said we closed #1124 because it seemed fixed. Are you still running into this issue with the latest AVA version?. > My test is failing if my patch is not applied, so I assume that the issue is still there.\nCould you share your test? Where / how specifically is it failing?\nI don't fully understand this color support stuff. I'd wait for @sindresorhus to chime in before updating this PR.. Perhaps we should ensure error.source.file is an absolute path. Then we can remove the path.join() from the reporters. That's in https://github.com/avajs/ava/blame/2dc47532543a7d5ef8b86b48396b7f53d20963a3/lib/serialize-error.js#L62.\n@vadimdemedes, thoughts?. > But we'll still need relative path to fix #1231 (skip code excerpts and magic-assert for errors coming from node_modules).\nWe can always compute the path back to the project directory if necessary.. > Perhaps we should ensure error.source.file is an absolute path. Then we can remove the path.join() from the reporters.\nI've opened https://github.com/avajs/ava/issues/1270 to track this. I'm working on that now.\nThanks for raising this issue @forivall. I'm looking forward to future PRs from you \ud83d\ude04 . Sorry, this one is Flow, not TypeScript :). Thanks @LasaleFamine!. > Could the exported value not just be a json object instead of a stringified json object?\nYes, that's what https://github.com/avajs/ava/pull/1223 is aiming to do.\n. Please see https://github.com/avajs/ava/issues/1275 for context.. > The Javascript community is moving towards a more functional style and being able to use methods functionally without depending on an object context is becoming quite common.\nCould you give an example of a test you would write with a functional style? I can think of this scenario:\njs\nsomeArray.forEach(t.true)\nBut that only seems useful for assertions that have no expectations, e.g. not t.deepEqual(). It also wouldn't work, since the array index will be passed as the assertion message, which is (will soon be) an error. You'd have to do:\njs\nsomeArray.forEach(value => t.true(value)\nAt which point I don't see the usefulness in binding the assertions to the t object.\nCurrently we also need the t. for power-assert.. .map(apply(deepEquals)); can also be written as .map(args => t.deepEqual(...args);. I'd recommend that over helper methods, actually.. With the recent assertion refactor I think we can make it so the assertion knows whether it's bound or not, so unbound enhanced assertions know that they're not wrapped by power-assert.\nThat said I'm somewhat concerned about the performance impact, and not convinced about the value-add.. > Oh, right, that's a good point. Will add a check to detect the assertion error.\nWe're only formatting the error if the showOutput flag is set: https://github.com/avajs/ava/blob/8d6f9bc2e2ed02b39e32cdf2b952b680c3b19338/lib/reporters/mini.js#L197:L198\nThat only happens for our own assertion methods: https://github.com/avajs/ava/blob/8d6f9bc2e2ed02b39e32cdf2b952b680c3b19338/lib/assert.js#L29\n\nShould be not so strict though, to support chai, expect, etc.\n\nWe'd need to validate whether thrown errors have the required properties and then set the flag. I don't think Chai assertions do this. I suppose we could open this as a feature request?\n\nIn all this is a duplicate of #1268.. Immutable objects contain state. This isn't shown in console.log() output, but it should be apparent when you use console.dir(). t.deepEqual() compares this state, which may differ.\nUse Immutable's built-in .equals() method instead:\njs\nt.true(actual.equals(expected)). > Is it ok to keep using toJS() or there is a possible problem with it ?\nThat's entirely up to you.. You can use --tap to enable TAP output, which can then be piped to any TAP reporter, which could then generate the kind of HTML report you're looking for.. @zusamann I don't use TAP reporters, so no. Perhaps try asking in Gitter or on Stack Overflow.\nGoing to close this since the TAP output is intended to accommodate for these kinds of features, without having to implement them in AVA itself.. We should disable the Babel cache, given that we're now really good at managing it ourselves. Need to set process.env.BABEL_DISABLE_CACHE = '1', perhaps in lib/cli.js.. No worries! I better write docs and release hullabaloo at 1.0 now though \ud83d\ude09 . With t.true(response.data === 'something'), power-assert will display the entire response object. This includes Node.js HTTP objects which are exposed by axios. These include symbol keys. Our version of pretty-format contains a bug which causes it to crash when it encounters a symbol key. I've opened a PR to fix that: https://github.com/avajs/pretty-format/pull/2\nUnfortunately this crash gets lost in a promise chain, which causes the test to hang. https://github.com/avajs/ava/pull/1265 should deal with that.\nThanks for finding this!. @RickyMarou a new version of @ava/pretty-format is out with a fix. I think if you do npm --depth 2 update it should pull it in.. The default src/**/*.js pattern is ignored by Chokidar because of the src/widgets/**/*.test.js pattern. This is fundamentally a Chokidar bug, so if anybody can help out that'd be great.\nIn the meantime I suppose we could clean up the patterns we pass to Chokidar and remove more specific ones if they're already covered by a generic pattern. I'm not sure if tools for that already exist, @sindresorhus? This may be easier than fixing Chokidar though.. Added tests to last commit. Let's hope they pass in CI \ud83d\ude04 . Thanks for raising this @davewasmer. Some of your points have been discussed elsewhere, I've opened https://github.com/avajs/ava/issues/1268 and https://github.com/avajs/ava/issues/1270 to track their progress.\n\n[We] could return a warning message instead of the code excerpt that could suggest using absolute sourcemap urls. We could go further, and do the same for the verbose reporter (i.e. print a warning that URLs will be incorrect).\n\nI think it's fine not to show a code excerpt. If we see many support requests asking why code excerpts are missing maybe we could output debug information, but let's see what actually happens first.. Hi @davewasmer. This is similar to #1246. We've decided to ensure error.source.file is an absolute path so the reporters don't have to deal with this. I'm working on that at the moment.\nThanks for opening this pull request. Keep them coming \ud83d\ude04 . > Just out of curiosity - how do you plan to get the absolute source url for files that have relative sourcemaps? I didn't see a way around that one, so I'm curious what you've come up with.\nSee https://github.com/avajs/ava/pull/1271. I'm basically assuming that V8 and any source map library make them relative to the working directory. It's best effort, really.. This is an issue in stack-utils. Fix in https://github.com/tapjs/stack-utils/pull/27.. > Builds are failing though\nFixed now. On my local machine tap was including source-map-support, but that didn't seem to be happening on CI. Perhaps a caching issue, but it's better to include it explicitly anyway.. https://github.com/avajs/pretty-format/commit/5b3cd6e7898e6951312eab4aad4a1ee73c2e98bd might be contentious. I suspect we may want to diverge in other ways too, and the upstream being in a monorepo doesn't make things any easier.. > which aspect of the monorepo is giving you trouble?\n@cpojer it makes it harder to maintain a fork of a project inside the repo \ud83d\ude09 . > Seems like a small case of NIH hit and you're replacing jest-snapshot/pretty-format entirely: #1341. It'll be interesting to see what comes out of it. Looks cool! \ud83d\ude04\n@SimenB yea if I get that over the finish line then we'll drop our dependency on pretty-format, forked or not. It's a rethinking of how deepEqual relates to snapshot, ensuring we can diff any value (and that the diff contains whatever it was that made the comparison logic think values were unequal), and bringing AVA's snapshot experience more in line with the other assertions. Including diff colors in snapshots \u2728 . With 0.20 we're no longer using pretty-format. The repository is still available if anybody wants to upstream our changes.. That is pretty weird! My guess would be that there are some characters being output that prevent the reporter from clearing its previously rendered lines. I can't immediately reproduce it with the %3A51 string though.. Could you record this using https://asciinema.org/ instead? That should hopefully give us AVA's exact output.. Closing due to inactivity.. > The reasons why I'd go without jest-snapshot are:\n\nIt brings unnecessary and unused dependencies, such as jest-diff and jest-matcher-utils. We aren't using jest-snapshot's error message, so the diffs provided are useless for us.\n\nAccording to cost-of-modules:\n| name                      | children     | size   |\n|---|---|---|\n| chokidar                  | 168          | 8.71M  |\n| babel-core                | 27           | 3.11M  |\n| empower-core              | 2            | 1.92M  |\n| bluebird                  | 0            | 0.58M  |\n| source-map-support        | 0            | 0.55M  |\n| meow                      | 26           | 0.40M  |\n| jest-snapshot             | 7            | 0.36M  |\n| update-notifier           | 37           | 0.34M  |\n| @ava/babel-preset-stage-4 | 29           | 0.33M  |\n| diff                      | 0            | 0.32M  |\nSo yes there are dependencies but it's not our worst offender.\n\nWe can't pretty-print or diff the values and display the same rich output other assertions provide.\n\nYes, but what should be the short-term priority?\n\nIt's not as open as I'd wish it could be for other JSX-based libraries, like Preact, Inferno or others.\n\npretty-format supports other serializers, so I think it's actually pretty open. We'd have to solve configuration but we need to do that even for a custom solution.\n\n\nno support for custom classes: instances are reduced to plain objects\n\nI don't think there's any possibility for that to work with any serialization/deserialization method, is there? We just can't \"recreate\" non-native (Map, Set, etc - language features) instances.\n\nNah, you'd just go \"the snapshot says it was a Foo, and this object also says it's a Foo, so they match\".\n\nAs for key order, I'm not even sure we'd want to check that. In what cases key order of objects/props (in React components) affects the output/functionality? I can't think of any at this moment, because I've never encountered such a problem.\n\njest-snapshot isn't sensitive to key order. We currently are because we pass it a JSON string.. > However, to generate these diffs we need actual objects, not the error message that jest-snapshot returns.\nThis isn't entirely accurate. The diffs are done using formatted objects. jest-snapshot stores and compares formatted objects. It's just that we cannot configure how it formats objects. This means we can't do code highlighting, and there'll be other differences in the snapshot diff compared to the diff we generate for t.deepEqual().\nI have a longer term plan that harmonizes t.deepEqual() with t.snapshot(), but for now I think it's fine if we use jest-snapshot as it was introduced in #1113. At least it'll work again!. Well, we've built our own, which has landed in #1341. A new release will be out soon. . Thanks @blake-newman!. Yea, I hear ya. #1204 and #1205 cover this. I've added your examples to the thread in #1204.\n\nOne option that came to mind would be to have a browser based reporter, i.e. running ava would launch a browser tab that displayed the test results, which would allow for a richer UI for interacting with test results than is possible in the terminal.\n\nThat's quite interesting! And difficult, I think. Instead we could better integrate with Node's debugging tools and trigger a breakpoint when an assertion fails, but that too sounds rather difficult.\n\nI don't have any solid solutions to offer, and I normally refrain from posting issues like this when I can't be constructive, but I figured I'd toss this out there in case someone smarter than I came along and had an idea. Feel free to close if this is just one of those inherently unsolvable issues.\n\nNo worries! I'm playing with a project that gives us more control over the diff output, hopefully we can use that to come up with good solutions for these problems.. > One option could be to have a flag to disable Magic assert, keeping power-assert enabled, resulting in the default power-assert style assertion messages that existed before Magic assert was added.\n@qaiser110, for one, how we should the power-assert messages has also changed. We're wanting to improve magic assert first, before adding an option to disable it.\n\nIf there was a way to expose magic-assert's info to TAP consumers, or some other kind of reporter that fed the magic-assert info out in a machine readable format, then this kind of thing could be explored in userland.\n\n@davewasmer this might be possible if my experiments pan out, but hopefully if they do we can just improve the default output \ud83d\ude04 . @brokenseal could you give some examples of how this is making you regret using AVA?. Thanks for sharing. I think #1205 covers this best, but it's just hard to make the right trade-off and then implement it.\nYou can disable this output if you set compileEnhancements: false in AVA's config. See https://github.com/avajs/ava/blob/master/docs/06-configuration.md. . It's attempting to show you how to fix the test, but I agree it's confusing. See #1238.. > I'm not sure if this is a bug in the diff implementation or maybe it's expected that a large random string will cause it to hang.\nI think we can safely say that's a bug. https://github.com/kpdecker/jsdiff/issues/163 seems relevant. There's a suggestion there to use a different diff library for string diffs. We'd accept a PR for that.. Interesting find. I think the error comes from this line, really: https://github.com/avajs/ava/blob/98dded553c8c213739cea9743ab4662f40d3125a/lib/run-status.js#L24\nUnhandled rejections are normalized: https://github.com/avajs/ava/blob/98dded553c8c213739cea9743ab4662f40d3125a/lib/run-status.js#L72\nPerhaps the message should be set to String(err) and stack shouldn't be set at all. Further, error objects that have a non-string message, name or stack should have those properties deleted. That would be in https://github.com/avajs/ava/blob/98dded553c8c213739cea9743ab4662f40d3125a/lib/serialize-error.js#L59.\nLong story short, this means errors have the expected shape in the reporters and we don't have to keep checking if certain properties are strings etc. @avajs/core thoughts?. @jakwuh yea. Though that's just for rejection reasons that aren't objects. We also need to sanitize objects with unexpected properties.. Interesting. I don't particularly mind to be honest, it's a bit of an edge case. What do you think?. Oh interesting. My merge button was set to Rebase and merge, which can't work, but I can squash and merge.. Thanks @lukechilds!. AVA needs to find your tests. It uses globbing patterns, which by default are quite wide (test.js test-*.js test/**/*.js **/__tests__/**/*.js **/*.test.js). If all your tests are in one particular folder you could specify the files option in the AVA config and point it at that directory.\n@avajs/core I think we can consider this a common pitfall, so we should at least document it. Perhaps we should log a warning if globbing takes X amount of time?. > Globbing files reported 13.409ms and globbing helpers reported 6406.377ms.\nWow, that's unexpectedly high.\n\nI looked around and I didn't see a way to change the helpers patterns. Assuming I didn't miss it, would this be a feature worth adding?\n\nActually I think we need to optimize how we find helpers. The search starts here: https://github.com/avajs/ava/blob/bd5ed603356f7a038fed62f64494b35598741d67/lib/ava-files.js#L138, which is called from https://github.com/avajs/ava/blob/bd5ed603356f7a038fed62f64494b35598741d67/api.js#L123 and in turn https://github.com/avajs/ava/blob/bd5ed603356f7a038fed62f64494b35598741d67/api.js#L149.\nIt strikes me that we already know where the test files are when we start looking for helpers. We can find test and __test__ directories based on that list, and then search for helpers/**/*.js and **/_*.js files inside those directories. That should prevent us from searching your entire project tree.\nI'm hesitant when it comes to making helper patterns configurable. I think we first need to get the default behavior right, lest the answer becomes \"nah just change these patterns\".\n\nFor what it is worth, if there were some debugs in the code with the timings for the globs and I could out find how to configure the default helper patterns, it may have prevented me from reporting this issue.\n\nI think it's a bug actually, so I'm glad you reported it! \ud83d\ude04 . > So what we would need to say is that the user must ensure he provides the same \"polyfilled\" environment for AVA as he does to the execution environment of his program (in the case he uses more features than defined in AVA's stage-4)\nYes.\n\nIn my opinion this is a more general topic and would better be pointed out at the beginning of the whole file, probably in addition to the default's transpiler behavior section?\n\nAVA's test transpilation doesn't impact the user's program. I agree it's a general topic which is why I was trying to get away with just hinting at it.. @florianb by all means.. > Firstly, 'xstream' and 'most' observables don't have .forEach() method. This could be solved by sindresorhus/observable-to-promise#4.\nThis PR has landed, and AVA depends on the latest observable-to-promise version.\n\nSecondly, 'xstream' and 'most.js' observables are not detected by isObservable() function at lib/test.js::onAssertionEvent() and at lib/test.js::run().\n\nThis seems to be resolved:\n```\n\nrequire(\"is-observable\")(require('xstream').default.fromArray([1,2]))\ntrue\nrequire(\"is-observable\")(require('most').from([1,2]))\ntrue\n```\n\nIn conclusion I believe these issues have been resolved. Thanks for your report @shofel!. Updated.. Tests are red across the board, but it works on my machine\u2026\nI'll look into that tomorrow. Please hit me with feedback regardless \ud83d\ude04 . > Tests are red across the board, but it works on my machine\u2026\n\nI'll look into that tomorrow.\n\nClearing CI caches helped. Pushed more changes too.. My hunch is that we should extract the stack when serializing (filter out unnecessary bits early), and beautify it in the mini and verbose reporters. Though I'm not even sure what beautify-stack does that extract-stack doesn't.. @neoeno will you have a chance to get back to this? If not, perhaps @bchapman, @zabawaba99 or @patrickrand want to give it a go? \ud83d\ude04 . > t.throws() and t.notThrows(), when passed a promise or observable, no longer reject the returned promise with the AsssertionError. Instead the promise is fulfilled with undefined. This behavior is consistent with when a function is passed, and with other assertions which also do not throw when they fail.\nNeed to update the type definitions to take this into account.. >> t.throws() and t.notThrows(), when passed a promise or observable, no longer reject the returned promise with the AsssertionError. Instead the promise is fulfilled with undefined. This behavior is consistent with when a function is passed, and with other assertions which also do not throw when they fail.\n\nNeed to update the type definitions to take this into account.\n\nThat said, it would make this example quite annoying:\nts\nconst err = await t.throws(promise)\nt.is(err.message, 'foo')\nBecause if err can be undefined, this would have to be written as:\nts\nconst err = await t.throws(promise)\nif (err) {\n  t.is(err.message, 'foo')\n}\nAnd yes the code will crash without that guard, but that error is ignored given the original assertion failure in t.throws(). How accurate do we want to be?. > What will change?\nIt'll no longer throw, just like t.throws(() => {}) doesn't throw, and t.true(false) doesn't throw.. > I see. I use the pattern of assigning t.throws to err a lot, so having to guard err would be inconvenient\nOK so with 0.18:\n```js\nimport test from 'ava'\ntest('sync', t => {\n  const err = t.throws(() => {})\n  console.error('After sync', err)\n})\n```\nThis results in:\n```\n\u276f \"$(npm bin)\"/ava --verbose test.js\nAfter sync null\n  \u2716 sync Missing expected exception..\n1 test failed [14:07:57]\nsync\n  /private/var/folders/_6/p8qxp_3n62zg9081tvb0lcc80000gn/T/tmp.HrBeKHcUVs/test.js:4\n3: test('sync', t => {\n   4:   const err = t.throws(() => {})\n   5:   console.error('After sync', err)\nMissing expected exception..\n```\nNow async:\njs\ntest('async', async t => {\n  const err = await t.throws(Promise.resolve())\n  console.error('After async', err)\n})\n```\n\u276f \"$(npm bin)\"/ava --verbose test.js\n\u2716 async Missing expected exception..\n1 test failed [14:08:26]\nasync\nMissing expected exception..\n```\nWhat happens is that the promise returned by t.throws() is rejected, so the AssertionError is thrown by await and the subsequent code never runs.\nWith this branch:\n```\n\u276f \"$(npm bin)\"/ava --verbose test.js\nAfter sync undefined\n  \u2716 sync\nAfter async undefined\n  \u2716 async\n2 tests failed [14:10:04]\nsync\n  /private/var/folders/_6/p8qxp_3n62zg9081tvb0lcc80000gn/T/tmp.HrBeKHcUVs/test.js:4\n3: test('sync', t => {\n   4:   const err = t.throws(() => {})\n   5:   console.error('After sync', err)\nasync\n```\n(OK just now noticing that the output for async seems to be truncated, that seems like a bug.)\nNow, both sync and async behave the same. Will subsequent code crash? If so, that's the same as the current behavior for async. Will an assertion fail? If so, that's the same as the current behavior for sync. Can the user tell? No, since we only show the first assertion. . > (OK just now noticing that the output for async seems to be truncated, that seems like a bug.)\nPushed a commit to fix this.. > Got it. My problem with this change is that it breaks a popular pattern that we also document:\n\njs\ntest('async', async t => {\n  const err = await t.throws(Promise.resolve());\n  t.is(err.message, 'foo');\n})\nWill result in:\n[TypeError: Cannot read property 'message' of undefined]\nWhich is not very helpful for the user.\n\nInteresting! That's happening because we now only check the pending assertions for failures after we've checked the return value. I think that's a bug, even if t.throws() returned a rejected promise we'd be wrapping it with a new AssertionError, since the Test code assumes that no AssertionErrors leak out of the test.\n\nDo you remember why and when we stopped throwing on assertion failures? Seems like that would have resolved all of this, as no code would run after a failed assertion.\n\nI don't know. It's hard to tell, currently empower doesn't rethrow but I don't know if it did before or if we disabled that.\n\nWhat is even the benefit of assertions not throwing?\n\nIf you have a callback test (or even when you wrap a legacy API in a promise), code may run in a separate call stack. If the assertion throws that might become an uncaught exception, completely crashing your test run.\n\nAnother workaround would be to just return an empty object until we implement #1047. I just don't want to force the user to change their code twice.\n\nThat's an option, though with the bug I described above it would mean the t.is(err.message, 'foo') assertion gets reported, not the t.throws() assertion failure. And once I fix that bug I think not throwing becomes harmless, since subsequent errors are not shown to the user.\n(Unless of course they cause an uncaught exception in some odd test, but even returning an empty object could cause that.). > That's happening because we now only check the pending assertions for failures after we've checked the return value. I think that's a bug\nI've pushed a fix:\n```\n\u276f \"$(npm bin)\"/ava test.js\n1 failed\nasync\n  /private/var/folders/_6/p8qxp_3n62zg9081tvb0lcc80000gn/T/tmp.KKt3WiWUVJ/test.js:4\n3: test('async', async t => {\n   4:   const err = await t.throws(Promise.resolve());\n   5:   t.is(err.message, 'foo');\nTest. (test.js:4:23)\n  step (test.js:9:191)\n  Test. (test.js:9:99)\n  Test.__dirname [as fn] (test.js:3:1)\n```\n. This sounds like something that could be explored outside of AVA itself. Any resulting plugins we'd happily make \"official\".\nThe webpack recipe was also updated recently, though it doesn't discuss extracting AVA itself into a separate chunk. Further improvements to the recipe are most welcome!. We could look at https://www.npmjs.com/package/@mzgoddard/jest-webpack for inspiration.. > Is this still an option that might come in the future?\n@Sawtaytoes yes, but it's not a priority.. @Sawtaytoes not sure. I don't think that recipe has changed much since you're question from September.. I'm surprised by the difference in CPU usage. Something seems to be doing significantly more work, though I doubt it's Bluebird#map(). Does it do the same if you set concurrency to 36?\nI assume you can't share your test suite?. @jamestalmage the surprising behavior in this issue is that specifying concurrency slows the tests down more than not specifying it does.. I think we should solve this by improving how test files are selected, and letting users specify patterns that match helpers. See https://github.com/avajs/ava/issues/1228#issuecomment-320452723.. I'm not sure about this\u2026 we already have issues with helper globbing performance (#1288). Helper exclusion isn't great either (#909). Perhaps we should solve those problems first.\nJust spitballing here, but what if:\n\nfiles patterns that match files cause those files to be treated as tests, not helpers\nWe support negation patterns to refine selections if necessary\nWe exclude {project-dir}/node_modules by default, though ideally you can override this by providing a more specific path\nfiles patterns that match directories mean that containing files are treated as tests, unless they're inside fixtures and helpers directories (that are children of the matched directories), or start with _ (regardless of how deeply they are nested)\n\nThis wouldn't really help your use case, but I think it makes test file matching more understandable. We could then add support for helper patterns, so you can match those and we transpile them. A utility command that prints out what files are matched and how they're treated would also be useful.. > I've read the discussions and I'm not sure if I understood the problem correctly, but the issue with #1288 is because of number of files and not globbing itself, right?\nI think it's because of the ** which matches any directory, leading to a lot of matching.\nI'm not saying this PR isn't an improvement, it's just that the whole helper story is a bit of a mess and I'm not sure whether we should add more twists to it.. >> files patterns that match files cause those files to be treated as tests, not helpers\n\n@novemberborn How is this different from how it already works? Or do you mean the ability to do $ ava _foo.js and have it run as a test instead of being a helper?\n\n@sindresorhus uhm, I hadn't even thought of it like that. I guess if you don't use a pattern it could be used as-is\u2026? Probably easier to never treat any _-prefixed file as a test.. > This approach seems to slow down my AVA runs a lot, I think it descends into my build/ dir but I don't really know, there is no debugging output for the globbing.\n@wmertens is this a comment on this PR, or AVA's current behavior?\n\nHow about not scanning for helper files, but instead somehow intercepting the require mechanism that Babel uses to transpile, only allowing it to transpile imports that match helper globs?\n\nThis is tricky to do performantly at the moment.\nLet's move the performance discussion to https://github.com/avajs/ava/issues/1228#issuecomment-320452723.\n. I still don't think adding more patterns is the right answer here. Improving the files matching, and supporting helpers patterns should be sufficient.\n@grvcoelho #1319 is valid though. I'm sorry this is taking so long.. > Another issue is that it will create different environments for the verbose and default reporter. So a test might fail with the default reporter, but pass with the verbose reporter.\n\ud83d\udc4d  Reporters shouldn't impact tests.\n\nAnother solution would be to add an option to not spawn AVA test files as child-processes.\n\nI can see that, though it would have to be a global option (apply to all files). I think we'd need more refactoring to make this work, and perhaps run the test files in a new script context rather than directly in the global.\n. > I had to abandon AVA as my test runner because it hijacked the native stdio streams (which I needed access to for testing).\nThis might not be great for performance but presumably those tests would also work if the test spawned yet another child process which executed a fixture?. Thanks @danez!. That's an interesting use case. I don't think it's a great use of t.notThrows() but it seems easier to make this work then to properly communicate why it doesn't.. @sindresorhus I like that.. @jamestalmage see https://github.com/avajs/ava/pull/1335.. Could you provide timing details? How many files are in the other directories? Could this be related to #1288?. > i would be happy to instrument for more details if anyone can provide a specific technique to use.\nThat'd be great. Could you time how long this handlePaths() call takes? https://github.com/avajs/ava/blob/v0.18.2/lib/ava-files.js#L138:L145\n\nif i point to a specific file, it starts almost immediately\n\nThough that doesn't suggest it's a helper problem. Perhaps try with --concurrency 3 or something like that? I'm wondering if your tests just take a long time to load (and by default they're all loaded at once).. > --concurrency=3 makes the tests start sooner, but it still takes a long time to run all of them.\nSeems to me like your tests are slow to start, and slow to run. Hard to give more pointers without seeing them though \ud83d\ude04 . > i'm baffled, but does it mean anything to those more traveled?\nBut, those are separate AVA runs! That is baffling (and I don't have anything more to add, sorry).. I've been collecting globbing related issues under https://github.com/avajs/ava/labels/feature%3A%20globbing. I suspect this is one of them. Closing it though since the concerns are covered elsewhere.. This also applies to callback tests ending more than once.. Note that this will still be best-effort. The test worker will exit once all tests have completed, even if there is a setTimeout() scheduled which double-ends a callback test, or makes an extraneous assertion.. With https://github.com/avajs/ava/pull/1776, we don't write test results until the test file has finished executing. So we'll be able to fail a test even if it seemed to pass at first.. > This would still run each test in parallel, right?\nYes.. Prompted by #1631 it might be interesting to still start workers, but reuse them for different test files. We'd use vm for sandboxing. Would be good to check Jest's sandboxing approach as well.. > Although, some parts of my server don't need to be transpiled by babel (node js compliant code) but I couldn't find a way to specify a per directory config set,\n@Magellol you should be able to specify an ignore pattern in your Babel options which causes it to ignore those files.\n. @sindresorhus see new commits \ud83d\ude03 . No, since we don't know a test is synchronous until the function returns.. > This is actually already caught by another check (nice!), but would be useful to have a better message for it.\nRight. throws and notThrows are also the only assertions that can be pending. Are you saying we should track which assertion was used and base the error message on that?. It's not too hard.\n\nWe detected t.throws or t.notThrows being used with a promise/observable, but the test is synchronous. Either use an async function or return a promise.\n\nUnfortunately even if a test is asynchronous, that doesn't mean it's waiting for the assertion to complete.. @joflashstudios for a generic solution you could await all t.notThrows() calls.. You could keep a reference to the returned promises, and then at the end of the test implementation do return Promise.all(arrayOfReferences). You could even wrap that up in a function so you don't need to change your test code, like:\njs\ntest('something', interceptNotThrows(t => {\n  t.notThrows(somethingAsync())\n})). AVA starts with cli.js. It tries to detect a local version, and loads ava/cli from that version instead. Otherwise it runs the CLI from ./lib/cli.js.\nThat file runs update-notifier (see code). We should provide {isGlobal: false} as an option to notify() to fix the version warning.\nI suggest adding an environment variable in cli.js:\njs\nif (!process.env.AVA_LOCAL_CLI) {\n  process.env.AVA_LOCAL_CLI = String(localCLI)\n}\nWhen running the local CLI that should be \"true\", but when running the global CLI without a local AVA installation it should be \"false\" (since it cannot find the local CLI). Then in lib/cli.js you can compare the environment variable against \"true\".\nAt least I think so, this logic is mind-bending without running some tests \ud83d\ude09 . @thisisrai have you had a chance to make progress on this?\n@servicelevel please feel free to make a start. It's possible of course that @thisisrai is nearly done. Regardless I'll add the assigned label.. @thisisrai no worries. Thanks for responding! \ud83d\udc4d . @tdeschryver\n\nIs there a better way of doing this, this seems a bit dirty to me?\n\nYou could change lib/cli.js so it does require('process') and then stub it\u2026 but I'd be fine with what you have currently as well.. Sure.. Go for it @timothyjellison!. I don't think we use transform-regenerator anymore, since we're now only targeting Node.js versions with generator support.\nWhen Node.js 8 comes out we'll update our transforms and we'll no longer transpile async/await for those Node.js versions.. @medikoo tests should work across Node.js versions. We want you to be able to use the latest syntax in your tests, even if you're developing for Node.js 6 or 4.. Makes sense. I agree that +0 shouldn't equal -0, and I doubt that most people would run into that. Making it easier to compare NaN seems sensible.\n@avajs/core?. > So, with that in mind, I figured I'd come back here in case the maintainers decide that actually that's too much of a change for the sake of supporting NaN and actually somewhat complicates things when is is used as an example.\nLet's see what @sindresorhus says.\nIn the mean time\u2026\n\nThe tests themselves don't contain much \"everything testing\", at least in the is/not section where I looked. They simply test that t.is('foo', 'foo') and assumes it's all good for the passing case. However, it'd make more sense to include 'all possible' passing cases, such as true, true, null, null, 0, 0, -0, -0, etc. in order to create a good document for the design of is, as well as ensure what is actually expected by the maintainers continues to be true forever.\n\nIt's hard to do this exhaustively. I'd be happy to see the edge-cases that Object.is() covers that === does not.\n\nSo I added a few of these in, but got a little stuck on things like [], [] and [1, 2], [2, 1] as the formatting is a bit complex, and not really testing what we want - first off, power assert is a bit useless here as the diff between [] and [] is '' (i.e. an empty string), and while I can write a test for this, it's only testing current behaviour rather than some realistic future expectation (that is says that the two arrays are not the same instance and that's why they're not equal despite looking the same, for example).\n\nWe also don't really have tests for power-assert, see #1324. Don't worry about it here.\n\nI changed is to use Object.is but then I had to change not to use Object.is. No major problem there.\n\n\ud83d\udc4d \n\nBut then I see that is and not both use an operator key which is currently '==='. Should I change it to Object.is, Object.is() or just remove it entirely like for deepEqual?\n\nRemove it for now I think. operator is used mostly for the TAP reporter and it tries to follow a convention, but a lot of our assertions don't have an operator equivalent and I'm not even sure what the convention is.\n\nPresumably the docs should be updated to say that is uses Object.is and not ===.\n\nYea, let's be explicit on how t.is() and t.not() do their comparisons.\n\nHere's where another issue arises - the macro examples (see lines 635/636 of readme.md) use is, and they assert (in English, not code) that 2 + 2 === 4 because we use is, but now that's not strictly true (it's now Object.is(2 + 2, 4) which is slightly less catchy in the example). If we any examples that use is like this, it could make these parts somewhat more confusing. That said, that section is the only real issue here.\n\nI think it's OK. Perhaps change it to use = in the test title, which is appropriate for a math-expression.. > I checked the issue #1114 which doesn't seem to be relevant to the fact that beforeEach and afterEach has a context...? So I'm not sure if the failing test is the sign of something wrong or just the unit test is what needs to be revisited.\nI think the unit test is wrong, and indeed the type definition. I've added serialization support to kathryn, and pushed a commit here which uses kathryn for snapshots.\nPerhaps controversially, Kathryn serializes to a binary format. Consequently so would AVA with this implementation. I'm writing a second test-file.readable.snap file which contains a readable version that can be diffed using version control as a way of verifying the new snapshot. We could consider combining the binary data with the readable data to have a single file, but that would make it harder for us to parse the snapshot file, and for users to consume the version control diff.\nTo keep things simple, snapshots are only updated when --update-snapshots is passed. This means we don't automatically prune snapshots as tests change. We could consider warning the user when a snapshot is unused, though I'm not sure if it's worth it.\nThere are some to-do items left:\n\n[ ] We don't prune snapshot files even if --update-snapshots is passed (that is, the file exist, but the tests no longer uses snapshots)\n[ ] If a test is removed I don't think we can know what snapshot files to remove\n[x] The directory is still __snapshots__. As discussed in #1223 we'd want it to only be __snapshots__ if inside a __tests__ directory, otherwise snapshots\n[x] The README hasn't yet been updated\n[x] The formatted sections of the readable snapshot could be surrounded with code fences so the snapshot can be viewed as Markdown\n\n@vadimdemedes @sindresorhus what do you think?. >> Perhaps controversially, Kathryn serializes to a binary format.\n\nCan you elaborate on why this is needed\n\nIt needs to serialize to some intermediate format. It can't really be JSON, because of dates, buffers, and certain number serializations.\n\nCan you document the binary format?\n\nYes, eventually. I'm still proving out the feasibility of it all.\n\nWhy not use something like Protocol Buffers?\n\nI can look into that. There isn't a lot to the current encoding though (at least in AVA).\n\nMy biggest concerns with a binary format is debuggability and it not being diffable in git, so each snapshot update will take the whole size of the snapshot. This can have big impact of projects with lots of large snapshots.\n\nAt least the snapshot files correspond to test files. I don't know how outrageously large they would get in practice.\nI don't think there is much value in the serialization being readable. Even if it's JSON it wouldn't be pretty formatted, and a single line diff is just as useless as a binary diff. If we do pretty format it would tempt people to make changes, and that's likely to break the snapshot. With the binary format we discourage all that and we get to use compression. I think that strikes the right balance.\n\nI think the readable snapshot should be the main one, so test.js.readable.snap => test.js.snap and we could do another one: test.js.binary.snap for the binary one.\n\nMaybe, yea. The readable snapshot isn't actually used though, it's just there so you can verify changes.\n\n\nSince we control the output now. I don't like the trailing commas\n\nFor the last property you mean? It'd be a bit more work to track which item / property / map entry is the last one, flag it, and then prevent the comma. Though it would be possible.\nThing is, this output isn't necessarily JavaScript. It just looks a lot like it. I like the consistency of always ending a property.\n\nI think we should drop the type for Object and Array as their type is already known with {} and [].\n\n[] are determined by the presence of an integer-value .length property. We could make an exception for true Objects and Arrays but I'm not convinced it's worthwhile.\n\nI also think we could make the date output nicer: 2017-04-19T06:58:30.166Z => 2017-04-19 06:58:30 166ms Z.\n\nSure. https://github.com/novemberborn/kathryn/issues/15\n\nI'm hoping to land theme support today, and I'm also doing a pass through this PR to revisit the assert integration and snapshot implementation.. > True Objects and Arrays are the most common output, and simplifying that simplifies the 95%. I think it's very much worth it.\nhttps://github.com/novemberborn/kathryn/issues/17\n\nHaving a trailing comma is distracting and makes the output more noisy.\n\nhttps://github.com/novemberborn/kathryn/issues/18\n\nI've forced-pushed some updates:\n\nActual / expected / difference values are no longer indented in the logger output\nMore direct usage of kathryn (more changes coming with theming)\nI'm now generating a \"Snapshot report\" in Markdown format. This is the readable snapshot output. It even includes the assertion message (t.snapshot(obj, 'this message here')).\n\nLastly there is a commit that switches to protocol buffers. It uses https://www.npmjs.com/package/protobufjs/. I'm vendoring a minimal implementation to avoid users having to install the full package, which seems to come to 13MB!. > How do you like it? Do you think it's worth using or do you prefer the custom binary handling?\nI'm not sure. On the one hand it's nice to not have to write the binary logic, on the other hand it's not that complicated. But it's quite likely that's just me \ud83d\ude09  There's overhead in managing the tooling too, and I don't know whether having the tooling makes it easier for others who end up having to deal with this code.\nI'm not convinced it'll help with kathryn either, mostly because kathryn tries to be very generic, plugins included, and having to write protobufs for plugins just adds more authoring complexity.\nWhat do you think, given the diff?\n\nIf we go for this, I think we should just publish vendor/protobufjs/minimal.js as a module instead of vendoring.\n\nYea. And then we can use Greenkeeper for updates too \ud83d\ude09  protobufjs is not semver compatible so abstracting it in a separate module is appealing.. > To keep things simple, snapshots are only updated when --update-snapshots is passed. This means we don't automatically prune snapshots as tests change. We could consider warning the user when a snapshot is unused, though I'm not sure if it's worth it.\nI wanted to come back to this. I'm not sure about the pruning behavior, either with the current AVA or with Jest itself. The more useful thing we have currently is that new snapshots are saved the first time they're asserted. This is nice with watch mode since you can just keep typing. On the other hand it seems strange that AVA would actually write files without being told to do so. The resulting snapshots aren't deterministic either. Files churn when run with --update-snapshots.\nI'm leaning towards only updating when --update-snapshots is passed. However in watch mode it'd be neat if you can type u and it reruns all tests, updating snapshots. We can even suggest this to users when a snapshot is missing.. I've pushed color support. Update dependencies and use node cli.js test/fixture/formatting.js to see examples.. > I'm curious why you're adding a header and version manually: https://github.com/avajs/ava/blob/1695c449960d6d3f86bd385754b0afcd46333696/lib/snapshot.js#L91-L92 ?\nThe header is so that people can see what generated the file. The version so that eventually, older AVA versions can detect a newer snapshot and not even try to decode it. Currently it's compressed and then within that there's the encoded index. If we change the compression then older AVA versions would just crash. If we change how the version is encoded inside the decompressed binary blob (easy to accidentally do with protobufs) then again older AVA versions would crash. Hence leaving it outside, which makes it easier to guarantee we never (accidentally) change it.\n\nI'm slightly in favor of Protocol Buffers, but it does add some overhead in tooling and I'm not seeing as much use for it as I had hoped, and you're right that the manual binary handling is not that advanced.\n\nAlso, a big use case for protobufs is when you need to share data between different programs. You can write a definition once and then generate parsers / generators in different languages. Here it's just AVA reading its own output.\n\nI tried latest now with my existing snapshot and got\n\nYes because I'm changing formats without regard for versioning in this work-in-progress PR.\n\nI also think the error output could be better here. We're saying Error 3 times. Ideally it would be:\nError thrown in test:\nSnapshot version is v7937, can only handle v1\n\nOh good observation! We should be able to remove the Error: line and just show the formatted error.\n\n[x] Remove unnecessary Error: heading before formatted errors\n\n\nI think Contains 1 snapshot from 1 test. See `test.js.snap` for the actual snapshot. should be one separate lines so it will diff better. Only the first part is dynamic.\n\n\n[x] Place snapshot file reference on its own line\n\n\nYou can't really see the snapshot in that file, so I would rather say:\n\nThe actual snapshot is in `test.js.snap`.\n\nOr something similar.\n\n\n[x] Update snapshot file reference copy\n\n\n\nOn the other hand it seems strange that AVA would actually write files without being told to do so.\n\nIt is being told so though, kinda. The user is explicitly writing a t.snapshot() assertion. I don't think it's very user-friendly to require a --update-snapshot every time the user creates a new t.snapshot(). That command is meant only for updating existing snapshots.\n\nThat's a good interpretation.\n\n[x] Automatically add new snapshots to existing file and report. @sindresorhus yea I can update to that. I'm using the indented code blocks though since there is no way for the formatted value to accidentally escape it.. > Been thinking about this some more and I think you're right. Not enough benefit of using Protocol Buffers, since we are the only writer and consumer, and we only really need a couple of fields. Let's go with your custom implementation.\n\nCool. Thanks for challenging me on this though. I enjoy writing these binary formats a little bit too much.. Pushed some changes:\n\nRemoved protobuf experiment\nMade linebreaks in mini and verbose reporters more consistent\nChanged test failure formatting in mini and verbose reporters: if the label of the first assertion error value starts with the assertion message, then the message isn't shown\nChanged labels for when tests throw exceptions, return rejected promises or invoke the callback with an error to start with the error message, which with previous change reduces the final output\nSnapshots should now be automatically updated\nHad to remove the snapshot and test stats from the report, since it is hard to keep updated without rebuilding the entire report, and that seems like unnecessary work in the off chance that a new snapshot has to be added\nUpdated report copy\nBased on this tweet added t.snapshot(expected, {id: 'my snapshot'}) which creates a snapshot that is identified by my snapshot, rather than the test title and t.snapshot() invocation count. This also means you can compare against the same snapshot in different tests in the same file\nNow using write-file-atomic to write the files\nThe binary file now includes an md5sum of the (compressed) snapshot, to detect corruption. Rebased, updated to the latest concordance version (renamed from kathryn) and added React support.\n\nRelease blockers:\n\n[x] Snapshots directory name must be based on the parent directories. If __tests__ use __snapshots__, else snapshots\n[x] Watch mode must not rerun tests when snapshot files are written\n[x] https://github.com/concordancejs/concordance/issues/14\n[x] https://github.com/concordancejs/concordance/issues/11\n[x] https://github.com/concordancejs/concordance/issues/20\n[ ] README must be updated\n[x] Need to detect old-style snapshots and bail early (else we'll see errors like Snapshot version is v25866, can only handle v1)\n\nFuture work in AVA itself:\n\n[x] --update-snapshots should cause files to be removed if test file no longer uses snapshots. Need to take into account skipped tests and assertions though. #1424\n[x] Skipping the snapshot assertion should still increment the internal counter so a subsequent snapshot assertion in the same test can pass #1425\n[x] Support a file option in the snapshot assertion so multiple test files can share snapshots #1426\n\n. @lukescott that makes sense, will keep that in mind, thanks.. Tests should be passing again. Snapshot diffs now use - gutters for the current values, and + gutters for the expected values that are contained in the snapshot.\nAVA now prints errors if the snapshot is too old, too new or corrupted:\n\n\n\n\n. @vadimdemedes \ud83e\udd23  whenever we meet in person I suppose. Or there's services like https://honestbrew.co.uk/ \ud83c\udf7b  \ud83d\ude1c . Watcher now knows not to rerun tests that just modified snapshot files. However once those initial file system events have been handled, it knows which tests to rerun if you say revert a commit that modified a snapshot.\nFor tests in a __tests__ directory, snapshots are stored in __snapshots__. For tests in a test or tests directory, snapshots is used. Other tests have their snapshots stored alongside the test files. (Note that the tests directory is not a standard files pattern in AVA, but since snapshots directory aren't currently configurable it seemed sensible to handle it.)\nTest coverage in AVA itself is pretty good now, so we're getting really close to landing this.\nCurrent status:\n\n[x] CI is failing in at least AppVeyor\n[x] Need to update the README\n[x] Need a Retina image for snapshots, and update the magic assert images\n[x] I don't necessarily expect https://github.com/npm/write-file-atomic/pull/25 to land, and if it does, ship, in time for this PR. Need to publish a fork to @ava/write-file-atomic and clean up later\n[x] Need to ship 1.0 releases of concordance and @concordance/react\n[x] Need to add **/*.snap to the standard watch sources\n[x] Need to open issues as mentioned in https://github.com/avajs/ava/pull/1341#issuecomment-302408283. I've now updated the readme. The image for snapshot testing isn't 2x though, and the magic assert images need updating too.. Just realized **/*.snap should be in the standard watch sources, otherwise the watcher will never pick up reverts and whatnot of the snapshot files without further customization of the source option.. Maybe let's not remove t.title https://github.com/avajs/eslint-plugin-ava/issues/176#issuecomment-310538200. \ud83c\udf89 . This works:\n\njs\ntest(async t => {\n    await t.throws(foo(), null, 'foo');\n});\nI think what you're saying is that we should always print something like \"Expected promise to be rejected, but it was resolved instead.\" And then if the message is provided add it. Like:\n```\n  [anonymous]\n  /private/var/folders/_6/p8qxp_3n62zg9081tvb0lcc80000gn/T/tmp.Vm1UQYRgMc/test.js:10\n9: test(t => {\n\n10:   t.true(false, 'hi')\n   11: })\nhi\nValue is not true:\nfalse\n\n``. Yes that sounds good.. I think you need tothrow new AssertionError(). That'll reject theintermediatepromise which [causes the test to record the failure](https://github.com/avajs/ava/blob/c4c5e7bb3ec43726c03d9834f55e157c5feacf8f/lib/test.js#L171). That should fix the test intest/assert.js` as well.\n\nIs it OK if I'm continuing with the issue?\n\nSeems to me you already solved it! \ud83e\udd47  Looking forward to the PR.. @gajus does this work for you?. I think this is good now. I suppose the changes may break something because some types have been removed? But even if so it seems like a good bugfix that we should just ship in a 0.19.1 release.. Thanks @zs-zs! I took out the dead type removal, we'll land that later: https://github.com/avajs/ava/pull/1352. t is the \"execution context\" \ud83d\ude09 . Yes, see https://github.com/avajs/ava/pull/1346. I'll probably get that out as a 0.19.1 patch release tomorrow.. What version of AVA, jsdom and Node.js are you using?\nI've tried to reproduce this with 0.19.0, 9.12.0 and 7.8.0 respectively, but though the output is massive, it doesn't crash:\n```js\nimport test from 'ava'\ntest(t => {\n  const document = require('jsdom').jsdom('', {});\n  const node = document.createElement('div');\n  t.is({node, a:1},{a: 1})\n})\n```\n(We're aware of the output size issues.). Ha, nice! With https://github.com/avajs/ava/pull/1341 we're hoping to control how many properties are output for power-assert assertions (t.is() is one of those). That will help with this scenario. Eventually it should also be possible to register helpers for say jsdom objects which can then print a more useful representation.. #1341 has landed. A new release will be out soon. While I haven't checked your example I'm hopeful it won't crash :). I agree. With https://github.com/avajs/ava/pull/1341 (pending input from others) the output would be:\n```\nDifference\n\n\n\n```\n\n. @dudu84 we'll be solving this with #1341.. #1341 has landed. A new release will be out soon. . Ah yeah. Buffer.from() was backported in 4.5. Since new Buffer() is deprecated in newer Node.js versions it's annoying to have to work around it. We should update the version range and see if that leads to any issues. There have been security releases since 4.5 even so ideally nobody is using such old versions in production.. > I can open a PR to update the version range\nThat'd be great!\n\ndebugging would have been quicker if npm had warned about engines\n\nDo you think AVA should flat out refuse to start if it detects an outdated Node.js version?. > I'd probably give people's environment the benefit of the doubt (like maybe they patched it up/polyfilled it to work somehow?), but print a warning when AVA starts \u2013 this would be more immediately visible than npm's warning (the user may have installed a long time ago before running ava in non-CI cases).\n\ud83d\udc4d . @southpolesteve interesting. I still think the right answer is to require 4.5 or above though. Even if this particular dependency is under our (mine) control, it's quite likely (over Node.js 4's maintenance lifetime) for other dependencies to assume the newer Buffer APIs exist.. @reconbot yes we could use that in dependencies under our control (e.g. that team members maintain) but I reckon we'll see other dependencies assume Node.js 4.5. It's a losing game.. LTS is only meaningful though if you actually update. There have been several security releases since 4.5.. @avajs/core thoughts? . @sindresorhus fair enough. @reconbot I'll try and get to your PR on the weekend.. window looks neat!\nI wonder if the recipe could start with that, and then lay out why (though ideal) it's not always a good fit. With the caveat that could lead up to browser-env. This way perhaps people realize window is indeed sufficient.\nWhat do you think?. Fair enough.. I'm tempted to say that we'd rather see libraries generate multiple tests, though there's an associated overhead of course. And I'm sure there are scenarios where that's not as feasible.\nCounting assertions may be a neat improvement to our timeout implementation, which currently measures the time since the last test completion.\nI'd be somewhat concerned about the increased load on the IPC channel, but we could debounce to one or two updates a second.\nI do like how this can be an additional signal of activity. It's good when the test run looks busy!. Great! I'll leave this open for a day or so in case anybody else wants to chime in.. > in ava 1.2.x with sinon 7.2.3, when I create a sandbox with mocked timers, I get an error that clearTimeout does not exist. I'm willing to open a new issue if this is unrelated, but it seems like it could be.\nIf you could open a new issue that'd be great. This issue is more about changing the fs module etc.. @mrozbarry this should be fixed with https://github.com/avajs/ava/pull/2057.. What is your Babel config? The error comes from babel-plugin-transform-runtime which isn't used by AVA itself (not since 0.18 actually).\n\nws-promise (ava worked at 1.0.3)\n\nWhat do you mean by \"ava worked at 1.0.3\"?. Sorry, it somehow hadn't clicked that you were using AVA to develop ws-promise, I thought it was a dependency!\nThis is due to a bug in the module we started using in 0.19 to resolve Babel options: https://github.com/novemberborn/hullabaloo-config-manager/issues/10\nFor now you can work around this by not putting [\"transform-runtime\"] in an array, or by specifying an empty options object ([\"transform-runtime\", {}]). I'll try and get a fix out soon.. hullabaloo-config-manager@1.0.1 is now available.. This is much improved with the recent 0.19 release. There is one last issue in #1342, and a plan for improving t.throws() in #1047 that is waiting for somebody to actually do the work.\nI'll close this now, but please reopen if the above issues or 0.19 don't improve your use case.. Across how many files are those tests distributed? --serial runs one test at a time, but still launches processes for each test file, all at once. Try --concurrency to reduce the number of concurrent processes, which should help with the memory consumption (assuming you have loads of test files).. > I tried run ava with --serial and --concurrency option, but nothing change.\nDid you specify a concurrency value? Try --concurrency=3.. Great! I've opened https://github.com/avajs/ava/issues/1367, it would have been useful if AVA had warned you that you didn't specify a concurrency value.. Interesting. If I'm understanding this correctly you're proposing a \"helper worker\" that AVA manages for you, and that can expose some config that test workers can read.. We should just fail hard. It should probably go somewhere around here. Tests can be done like with https://github.com/avajs/ava/blob/2b9f29133fc8306786615cae90ff0ebea0e3de28/test/cli.js#L337.. @yatharthk looks like you're on your way to tackling this issue. Mind opening a PR and we can discuss there instead?. Thanks @cncolder!. This is what npm does when a script exits with an error code. (And exiting with an error code is the correct behavior when a test fails.)\nThe output is less verbose in more recent versions of npm. Note that the log file says 2 info using npm@4.2.0. Try installing the npm@latest.\n. This is what npm test does. Alternatively you can run AVA directly (./node_modules/.bin/ava) or install it globally and invoke ava.. This is correct behavior: the function doesn't throw, so the assertion fails. AVA ignores the return value, so the promise rejection ends up being unhandled and is reported separately.\nThat said I think we should extend t.throws() and t.notThrows() so that if the function returns a promise without throwing, the assertion is applied to that promise instead.. > A valid use-case for this design pattern in case you are on the fence:\nWe're not on the fence, see the issue labels \ud83d\ude09  Help most wanted!. This was fixed in #1650.. This was fixed in #1650.. Nice find!\nWe're only setting the stack for promises / observables, and even then we ignore the stack from the error itself. See https://github.com/avajs/ava/blob/2b9f29133fc8306786615cae90ff0ebea0e3de28/lib/assert.js#L168.\nWe should fall back to actual.stack if the stack variable is undefined. If it's not we should (somehow?) concatenate the two stacks. Ideally the coreAssert.throws() bit gets excluded but not to worry if we can't get rid of it.\nIf anybody wants to land a hand here that'd be great.. The t.throws() implementation is being refactored in #1704. That PR does not address this issue, though hopefully the implementation is now easier to follow.\nWe're looking for the following solution:\n\nWe should fall back to actual.stack if the stack variable is undefined. If it's not we should (somehow?) concatenate the two stacks.\n\nThis refers to the AssertionError constructor in lib/assert.js which sets the stack property.. The t.throws() implementation is being refactored in #1704. That PR does not address this issue, though hopefully the implementation is now easier to follow.\nWe're looking for the following solution:\n\nWe should fall back to actual.stack if the stack variable is undefined. If it's not we should (somehow?) concatenate the two stacks.\n\nThis refers to the AssertionError constructor in lib/assert.js which sets the stack property.. @norbertkeri it's not on the priority list, no. That said the aforementioned refactor has landed so if you'd like to help out with a PR, we can get it in \ud83d\ude09 . @norbertkeri it's not on the priority list, no. That said the aforementioned refactor has landed so if you'd like to help out with a PR, we can get it in \ud83d\ude09 . We have an open issue for implementing a programmatic API: #1043. We're not ready to commit to that though. Perhaps you could share your use case on that issue?\n. Thanks @JPeer264 \ud83d\udc4d . > I could use some feedback on the approach too\nApproach seems good. I like how you take the relative path from the potential global directory, and then map it to a local import.\n\nI'm aware the existing test for this is failing. I was not sure how to properly mock this as I don't think you can use proxyrequire on sub-dependencies. @novemberborn Any suggestions?\n\nWe could stub import-local and see if the CLI code handles the return value correctly. Alternatively if you want more of an integration test I think you'll have to use fixtures to set up a \"global\" AVA and then see if it picks up the local version.. I think if you create a fixture directory that contains a node_modules/ava/cli.js file then that file will be treated as the local file, assuming you execCli() with dirname set to that fixture directory. From inside the cli.js file you could just do require('../../../../cli') and it'll execute normally (I didn't count the ../ so this may not be accurate).\nAt this point running the normal cli.js from execCli gives you global, and with the correct dirname it should switch to local.\nThis should replace https://github.com/avajs/ava/blob/2ed94853ddd2ca4614a82b2a79cc251c09212331/test/cli.js#L408-L436.. The problem is that we start the timeout before we launch the test workers. With a 100ms value it might trigger before any tests are ready to run, at which point it kinda gets lost.\nI think instead we should track whether we have pending tests, and ensure the timer is running whenever new tests are started, clearing it when tests end. And assume that test workers aren't stuck in a while loop before they can notify the main process of their tests.\nRegardless of the above, I don't think a value of 100ms is ever realistic given how AVA works. The designed use case of --timeout is to help catch hanging tests, not to limit how long a particular test can take. It'd be better to set the timeout to a few seconds instead.\n(Possibly we could consider changing the value to always be in seconds?)\n. > The global timeout documentation makes it sound like the timer starts when the actual testcase starts, and also that the timer is reset whenever a new testcase begins\nYea. There's definitely a bug: we start the timer before we start collecting tests, and that's wrong.\n\nIn general, I think it's important that it's possible to:\n\ndetect bugs where an expected callback is never invoked\n\n\nIn specific scenarios we can already detect this even without the --timeout option. There are other scenarios where that's impossible though (say if your test sets up a persistent database connection).\n\n\ndetect bugs where an expected callback is invoked two or more times\n\n\nWe currently have no way of failing a test that passed previously, see #1330.\n\n\ndetect bugs where a synchronous function accidentally ends up in an infinite loop\n\n\nGood point, this doesn't work. Opened #1378.\n\nLet's continue discussion in the relevant issues. I'll change the title on this one to deal with the timer starting prematurely.. > Users might expect --timeout covers this, but it merely calls fork.exit(), which only sends an init-exit message to the worker. If the worker is blocked then it'll never process that message, and it won't exit.\nWith #1722 we'll force the worker to exit without relying on IPC. If we fix #583 then we can clearly communicate which tests were active and thus may have caused the infinite loop.\n1718 proposes we don't automatically exit the process once the last hook has completed. This makes processes that never drain their event loop indistinguishable from those that have an infinite loop, as far as the main process is concerned.\nI suggest we set a default timeout to say 60 seconds, and then force any active worker processes to exit.. Yep, this is one of the things I'm hoping to fix with #1341. Will leave this open until then, thanks for raising it.. #1341 has landed. A new release will be out soon. . @shovon there is no one right answer here. For AVA we've decided that order is relevant.\nIt's a bit more work to compare values out of order, but you could do something like this:\njs\nt.is(mapA.size, mapB.size)\nfor (const [key, value] of mapA) {\n  t.true(mapB.has(key))\n  t.deepEqual(value, mapB.get(value))\n}. I suspect this comes from https://github.com/avajs/ava/blob/2ed94853ddd2ca4614a82b2a79cc251c09212331/lib/assert.js#L256, but since that's internal to AVA it's not in the stack trace that's shown.\nThis implies that jest-diff somehow doesn't provide us with a diff, which is annoying. Open to a bugfix, otherwise this will get superseded by #1341.. #1341 has landed. A new release will be out soon. . Hi @JasonRitchie, thanks for opening this PR. Please bear with us until we can find the time to go through this and suggest any improvements.. Hey @JasonRitchie, looks like I dropped the ball on this one. Thanks for writing this. I've gone through and made a few edits. Let me know if you have any thoughts on them. Otherwise I'll land this in a few days.\nThanks for making this PR, and I'm sorry it took so long to land.. What confuses me is that the recipe talks about precompiling source files, but then the example precompiles a test file, which is how you're using it. @danny-andrews what was the original intent? I can't quite decide how this is meant to be used based on #1212 and #1011.\n@sudo-suhas I think we need to clear this up (might just be that the recipe title is misleading). But then yes let's add the globbing approach in the paragraph after the example.. Thanks for the clarification @danny-andrews.\n@sudo-suhas I think you're right that this recipe could use globbing as standard. Should it output a file for each entry though? Otherwise all tests run in the same process.. I haven't used webpack that much, so I don't know the exact syntax, but I think it should be possible.\nIt can matter if tests modify globals Node.js built-ins. You wouldn't want that to leak across tests. There's a lot of nuance here, I'd be happy if we can clarify the specific use cases for which this approach is safe.. @danny-andrews both. Each file runs in its own process, configurably in parallel. And all test are started at the same time, so asynchronous tests also run in parallel.. > But this negates any performance benefit because each entry has its own copy of the src.\nStarting the worker processes may still be faster, even if the source is duplicated on disk.. The 1st/second run difference implies it might be because AVA precompiles the test file, which is much bigger with the webpack build. If you only run one file (no concurrency) you only pay that cost once, but with multiple files you end up paying it for each file.. @sudo-suhas it looks like you're precompiling and bundling the source files. You then run the resulting bundle with AVA, which treats the entire bundle as a test file. Unless you disable it (which may not actually be entirely possible, can't recall at the moment) it'll then transpile the test file. See https://github.com/avajs/ava/blob/master/docs/recipes/babelrc.md for more.. @sudo-suhas for your use case I suppose it's better to build everything into one file. Not sure where that leaves us with this recipe though.. > Maybe we should have this recipe recommend building everything to one file, but add a not explaining that all tests will be run in the same process.\n@danny-andrews that makes sense. @sudo-suhas?\n\nAlso, this use-case makes me think that there should be an option to opt-out of ava's auto test transformation.\n\nThere's a plan for that. Eventually we may be able to integrate webpack in the same way.. @sudo-suhas sounds good \ud83d\udc4d . Landed. Thanks @sudo-suhas for your work on this, and thank you @danny-andrews and @grvcoelho for your contributions in the discussion.. You can't use cluster inside AVA's test workers. Best bet would be to launch a child process from your test, which can act as the master, and then have that process launch the workers. You'd have to communicate with your test through stdout. Have a look at execa.. Let's focus this issue on the u command:\n\nOne of the commands is u, which updates snapshots of tests that just ran.\n\n. @masaeedu yes, see #1525.. Interesting. I don't think we'd take our Babel support in the direction that Jest has, which makes your babel-jest-import-glob workaround a non-starter for AVA. There's a lot of trade-offs that need to be made to ensure the test file compilation is performant. Non-idempotent transforms make that really difficult. I don't see a good way out of that. Probably the same limitations apply to Jest.\nI suppose you could run AVA multiple times, where --no-cache is only used for those files that can't be cached, but that's not ideal.\nOr you could precompile test files, or place import-glob in a source file that is precompiled? \nThe best approach may be to write a importGlob() method that works like the proposed import(). That way you could get the same behavior as import-glob but at runtime.. @kevva any thoughts on my last comment?\n@nowells does my proposal satisfy your concerns raised in #1455?. #1700 clears up what arguments are exposed to the test worker. Let's discuss the specifics around configuring color in #1701.. With #1341 we'll print all enumerable properties on the error object. I've just started a contract so I'm a bit pressed for time with regards to finishing that PR.\nWould this solve your problem? I'm not sure the callback would do anything more than you couldn't do with a try/catch in your test.. #1341 has landed. A new release will be out soon. . @tdeschryver in your current test, try this:\n```js\nconst notifySpy = sinon.spy()\nproxyquire('../cli', {\n  debug: debugStub,\n  'resolve-cwd': resolveCwdStub,\n  './lib/cli': proxyquire('../lib/cli', {\n    'update-notifier' () {\n      return {notify: notifySpy}\n    }\n  })\n});\n```\nYou should then be able to assert on the notifySpy arguments.. @tdeschryver could you push your attempt? I'll try on my end.. @tdeschryver yea I'll try and give this a go. #1376 landing also threw a wrench into the works.. @sindresorhus import-local breaks the approach taken in this PR. We need to set AVA_LOCAL_CLI before the local CLI module is required. import-local requires the module immediately. Perhaps it could export an existsSync method?. Hey @bapjiws sorry for the delay in getting back to you. Thanks for this PR. As it turns out this was previously spotted in #1385 which I just landed. There's many other improvements to the webpack recipe in that PR, too. I hope you'll enjoy it.. Yea this isn't ideal. I think the best thread is #736. There are ideas floating around on how to improve all this.\nI suppose you could remove the helper file that isn't being used?. Hey @dohomi, this looks good!\nI pushed a commit which replaces these comparisons with a regular expression instead. That led me to trying to understand the existing API tests which seem to be limited and a bit confusing (notably --inspect-brk and --debug-brk aren't tested). So there's some commits that add / fix comments for that too.\nCould you check whether --inspect-brk still works for you?\n@sindresorhus happy with the regexes?. Thanks @dohomi!. Which branch are you referring to?\nWe do generate a TypeScript definition, but that's about it. See https://github.com/avajs/ava/tree/master/types.. The problem is the match option. Note that with * it doesn't need to be set, since it'll match anything.\nThe way match is implemented is we mark a test as exclusive when it matches: https://github.com/avajs/ava/blob/212b5de5759fdb267d837620c3eec7e5695a23e1/lib/runner.js#L114\nUnfortunately we do the same when test.only() is used! We should set an additional property that tracks whether the test is exclusive because of test.only() or --match, and make .skip() only throw on test.only().. > The lack of feedback on the progress of avajs/ava/projects/1 is also pretty frustrating and until then these issues (relating to performance) will keep cropping up.\nYou should interpret that as there being no progress. That's frustrating (I'd love that project being finished too!) but there's only so many unpaid hours in a week.\n\nThere needs to be one issue where the main performance issues are made clear, at the moment, there seems to be no clear direction or strategy for resolving these issues.\n\nThere is a plan when it comes to precompilation performance. If somebody were to summarize the various issue threads so we can close them in favor of one summary issue that would be wonderful.\n. > I'm happy to contribute. @novemberborn can you point me to a few or did you mean \"search for\" and then \"summarize\" \ud83d\ude07 ?\nThanks @pixelass!\nIt'll take me a while to do the searching myself (due to time limitations), so if you could start that'd be swell \ud83d\udc4d . > How should we continue with this? My attempt would be to evaluate the listed issues and write a new issue that lists the different performance issues.\n\nI'd extract the gist of the problem (description/example/potential reason) and link the originals.\nThis way those other performance issues could be closed/marked when you're happy with the summary. But I'm actually not sure if thats what you meant.\nSadly I don't have a lot of time myself. The weekend is over so I won't be able to start for another week. I'm also really bad at reading long texts so this may take a while \ud83d\ude1b.\nI skimmed a few of the issues and realized this might be quite a chunk of work.\nAny suggestions on the format are welcome. I'd really hate to spend time on something you can't use.\n\n@pixelass that sounds good. Perhaps you could publish a draft in a gist, so we can discuss it before creating the resulting issue? And no worries regarding time commitments.. The error says it tried to parse js-resource-counter/.babelrc though:\nParseError: /home/cmcdragonkai/Projects/js-resource-counter/.babelrc: Error while parsing \u2014 Unexpected EOF at line 1 column 2 of the JSON5 data. Still to read: \"\"\nCould you try \"extends\": \"./test/.babelrc\"?. There's two separate issues at play here. AVA needs to compile the tests. Since you're requiring babel-register that will be used to compile any sources you may require.\nAVA resolves its Babel config starting at package.json#ava.babel. Separately, babel-register resolves it from .babelrc (or package.json#babel if the .babelrc file is missing).\nThe error in your original post came from AVA not being able to resolve its Babel config. Your current problem is probably babel-register not doing any compilation since it doesn't have any config.\n\nThis is pretty hard to resolve without seeing your actual project though.. > So\n\nwhat's the best way to have an ava specific babel config that also brings\nin babel register and babel polyfill?\n\nPut the Babel options for your source code in a .babelrc. babel-register will pick that up, and if necessary you should configure rollup to do the same.\nIt's not clear to me if you even need to customize AVA's Babel config (which is only applied to test files). If you do, specify it it package.json#ava.babel and extend .babelrc.. babel-register doesn't look at package.json#ava.babel.. @zellwk we already have a recipe that uses mongomem and mongoose, written by the creator of mongomem. Rather than adding a new recipe, how do we improve the current one?. > @novemberborn I can edit the file directly, would that work better? Regarding that recipe, I tested it and it doesn't work directly with my app because the Mongoose instances are different, which is why I decided to make this recipe.\nBoth recipes are very similar to each other. Ideally we have one. If there's some additional use case or nuance we can discuss it in the recipe.. >> I'm not sure what to do with /docs/recipes/isolated-mongodb-integration-tests.md@master though. Again it's more focused on just MongoDB testing, with a bit of Mongoose added on. It uses a different memory server though. Perhaps it'd be sufficient for these recipes to reference each other.\n\nI've tried the recipe before creating this one. The mongoose version doesn't work. Had a tough time understanding everything, until I hacked stuff together with @nodkz and @OmgImAlexis help in this PR.\nSo, I recommend removing the Mongoose parts, but keep the MongoDB parts for that recipe.\n\nThat's a good idea. Could you add that to this PR? I can fix it up afterwards to if you'd like.. > In mongodb-memory-server@1.4.1 ship some changes with babel-polyfill.\n\nNow your tests may work without babel-polyfill and regenerator.\nnodkz/mongodb-memory-server#9 (comment)\n\nThanks @nodkz! @zellwk could you check that's all good with this recipe, so we don't need the polyfill and stuff?. Thanks @zellwk et al. I think this is pretty much there! I did push some edits, so please let me know if I've butchered anything \ud83d\ude07 \nI've also changed the section on using test.serial: I'm pretty sure Mongoose already uses the same connection in the tests and the app. Notably, the connection is made in a test.before() hook, not a test.beforeEach() hook. Thus the real concern is tests interleaving and the database being in an unknown or conflicting state.\nPlease let me know if I got this wrong.\n\nIn that case, shall we continue with Mongomem with the mongodb recipe?\n\nWe can modify that recipe to use mongodb-memory-server in a follow-up PR.\n. @OmgImAlexis updated.. Thank you so much for your efforts @zellwk, @OmgImAlexis and @nodkz!. Correct, but this issue is for the scenario where the test file still exists.. > It's probably easier to fail the skipped snapshot assertion when updating snapshots, or if the skipped snapshot doesn't exist.\nhttps://github.com/avajs/ava/pull/1696 implements snapshot skipping. t.snapshot.skip() fails the test if used when snapshots are being updated. However I'm not checking if the snapshot file exists. This requires a lot more logic and seems like an edge-case. Somebody would have to write a test, skip the first snapshot assertion, and then write a second snapshot assertion, while never having run the test with unskipped snapshots. An error should still be thrown for the assertion so I think we'll be OK.. Agreed.. What happens if you try:\njson\n{\n  \"ava\": {\n   \"require\": [\n      \"ignore-styles\"\n    ],\n    \"babel\": {\n        \"babelrc\": false,\n        \"presets\": [\n           \"es2015\",\n           \"stage-0\",\n           \"react\"\n        ]\n    }\n  }\n}. @blake-newman @knpwrs is this PR still relevant?\n@suchmaske will you have time to revive this PR?. That's great, thank you @suchmaske.. I think as a start I'd like to split the docs across multiple Markdown files in this repository. I'd prefer to do that myself though, I can be a little particular when it comes to this kind of work \ud83d\ude04 . We've now split the docs into multiple files: https://github.com/avajs/ava/tree/master/docs\nI'd like to keep the documentation in this repository, but then publish when master is built. Then we should also have a way to browse docs for multiple versions. Also, preferably, a free or low-cost option.. Hey @nklayman, that looks nice! Apologies for waiting this long to get back to you, been a bit busy moving to another country \ud83d\ude04 \nI'm concerned about the lack of versioning support (https://github.com/vuejs/vuepress/issues/1018). I've seen a lot of confusion when it's unclear what version of the documentation you're reading, and ideally we can display documentation for previous versions. What do you reckon?\nWe could host this on GitHub Pages, sure. I ported my site to https://zeit.co/now the other day and that may also work quite nicely.. We could regenerate historical documentation on every build. Talk to the GitHub API to get releases, etc. Note that we'd start doing this once we publish the website, not for older versions that exist today.. @nklayman did you try the GraphQL API? https://developer.github.com/v4/. Ideally everything is automated. Releasing is enough of a process as it is.\nFor now you could use a personal API token. We can swap those out with mine when the time comes.. > Updated my fork to support versioning.\nWould you mind opening a PR?. Interesting. #1401 aims to forward --color only if the user has explicitly provided it. See also the discussion in #1393.\nOf course the forked processes still can't detect the color support, and we can't necessarily expect the user to know what to specify either. Does this mean we should provide a value for --color when the user provides it without a value in the ava invocation?\n@sindresorhus, @kevva?. >> Does this mean we should provide a value for --color when the user provides it without a value in the ava invocation?\n\nYes, the forked process should get the exact same color level as AVA.\n\nRe-reading my original comment I'm not exactly sure what I meant! I think the conclusion is:\n\nOnly pass --color to the worker process if it was provided to the main AVA process\nIf it was provided with a value, pass it exactly as used\nIf it was provided without a value, have AVA detect color support, and pass it with the appropriate value so that the worker process is configured with the same level of support\n\n@sindresorhus did I get this right?. >> If it was provided without a value, have AVA detect color support, and pass it with the appropriate value so that the worker process is configured with the same level of support\n\nThis should also happen if no --color flag was provided.\n\n@sindresorhus that's not my understanding based on our discussion in https://github.com/avajs/ava/pull/1401#issuecomment-307383459.. > It should just work as if the child process was run in the same process as AVA.\nYes, but per https://github.com/avajs/ava/issues/1393 we should only pass --color if it was provided to AVA. We need a different way of enabling colors if the flag was not provided. And I'm still not sure if we should rewrite the --color flag if it was provided to AVA without a value.. #1700 clears up what arguments are exposed to the test worker. Let's discuss the specifics around configuring color in #1701.. #1700 clears up what arguments are exposed to the test worker. Let's discuss the specifics around configuring color in #1701.. I like it!\nIMO the title function should remain on the macro (so no test.use(macro, title)).\nWhat would check.use(otherMacro) do? Perhaps throw an exception?\n@avajs/core?. The test API should be available on these produced functions, e.g. check.skip(), check.serial(), check.cb().\nThough perhaps not check.before(), check.after() etc.. @abouthiroppy npm test gives me the same coverage as on codecov.. @abouthiroppy that last results when nothing is applicable test still increases the code coverage though. Do you want to reopen the PR so we can land that?. Oh that's an interesting problem to have!\nOne option may be to resolve the original file location through source maps. That should probably deal with your use case?\nOf course source maps aren't always reliable, so I do see the need for a configured location. But then it should be one directory that is used for all test files. There is a risk of multiple test files having the same filename, but existing in different directories. I'm not sure how to handle that problem. We could reproduce the directory structure (relative to the package.json), or concatenate the relative file path (but I'm wary of running into file name length restrictions). What do you think @impaler?. I think we should:\n\nCheck if the test file has a source map associated with it, and attempt to resolve the source file path\nHave that source file path take precedence over the test file path when it comes to loading and saving the snapshot\nAdd a setting in package.json#ava to force snapshots to be stored in the specified global directory. This takes precedence over the source file detection\nWhen snapshots are stored in a global directory, mirror the file hierarchy of the test file (with the source file path still taking precedence) inside the global directory\nWe should also add an integration test where the test file comes from a TypeScript compiler\n\nIf anybody wants to pick this up that'd be great.. > it made sense to me to add another cli option --snapshot-location think its worth it?\nThat flag would have to be provided on every test run. We've been trying to move such flags into the package.json and not have them on the CLI at all.\n\nI guess that this feature is worthwhile not just for typescript though. Any \"compile to js\" language would need this.\n\nYup. Figuring out the actual test file source location seems like a good idea regardless.. > Do you think we should add a package.json#ava config to enable sourcemap detection? This feature will enforce a fs.readFileSync on every test where before there was none. Maybe this is not worth worrying about performance?\nThe snapshot manager is cached once it's loaded, so it's only loaded once for each test file that uses snapshots. It'll be OK.\n\nIn this branch it just manually fs checks if a *.map to know where to put the snapshot.\n\nI have some feedback on this, though at this point it's better if you could open a PR. It's fine if it's not yet complete.\nAs to not keep you in suspense, we'd need to find a way to pass the current test file path to the load() function, and use a source map library to parse the file for source map annotations, from which the source can be extracted. Anyway, let's collaborate on that in a PR.. This would indeed be useful, though users could use get-root-module directly, right?\nI'm wary of building this in to AVA, especially with the shake up that's going to occur in the next year when ES modules land in Node.js.. > Yeah, no reason why users can't install get-root-module directly. I just thought the import { test, module } from 'ava'; syntax was really neat, but you raise a valid point about ES modules.\n\nI think this would be a nice addition but I'm happy using get-root-module directly if you're worried about ES module compatibility.\n\nWould love to see a guide with solutions for this problem. get-root-module is a good one. A more involved one is to rewrite module paths using Babel (but I'm not expecting you to do a write-up of that approach \ud83d\ude09 ).. @lukechilds great, have fun!. @lukechilds these seem really useful! A recipe would be great, yes.\nRegarding @sindresorhus' comment:\n\nAVA already knows the root, so we could just expose that instead. (See projectDir here) \n\nIs that useful for your libraries @lukechilds?. @lukechilds are you still interested in writing a recipe for this?. @lukechilds I hope it'll be the second! \ud83e\udd1e . @lukechilds I hope it'll be the second! \ud83e\udd1e . Thanks @abouthiroppy!. --concurrency=0 seems like it should result in an error. I reckon it currently works without any concurrency limitation, and the timeout may just be because of your system slowing down. Which is why we have the option to begin with.\nIn other words, I'm not sure there's an issue here?. @avaly no worries. I've opened #1476 so AVA can give a helpful message earlier.. > [ts] Cannot invoke an expression whose type lacks a call signature.\nIt's TypeScript I think, given the [ts] prefix.. Closing due to inactivity.. You could use our TAP reporter and parse the output, and then try and correlate it with the test case\u2026 and depending on how you collect coverage, combine it all\u2026 somehow?\nTo be honest this sounds rather difficult! It could be interesting but it's beyond AVA's purview at this point.\nI'm closing this issue but please feel free to respond to it, or drop a line in our Gitter chat: https://gitter.im/avajs/ava. AVA considers those values to be different, both with t.deepEqual() and t.is(). You'll have to use t.true(-0 === 0).. > depending upon the direction we have to do certain computations. In such a scenario carouselX gets computed to -0\nThat sounds like the polarity of 0 is meaningful, so you should be testing it. If it's not meaningful then it's better if the code ensures it's always 0.. The previous version considered 0 to equal -0. As you say, the difference is meaningful, which is why we now consider those values to be unequal. You'll have to adjust your test so it expects the right value.. > This has the downside of making test.only() not be exclusive with multiple test files, but there are just too many limitations caused by it. I think this change is worth it.\nWe should follow tap and implement a --only flag, which then causes only test.only() tests to run. See http://www.node-tap.org/cli/.. > Travis is failing on FRESH_DEPS=true, but so is master, so I don't think that's relevant.\nI'm hoping https://github.com/avajs/ava/pull/1477 fixes that.. @lordfuoco this is a very nice approach!\nI'm not sure if your setup can be generalized enough for this to be a recipe. E.g. the shell scripts are quite particular (and won't work on Windows). Then there's the dependency on yarn.\nPerhaps this would work best as a blog post where you explain how you tackled this problem. We can link to it from the Code coverage recipe as well as https://github.com/avajs/awesome-ava/.\nWhat do you think?. Hey @lordfuoco I hope you don't mind me closing this PR. Am still looking forward to seeing this as a blog post, please let me know if I can help with that.. Hey @lordfuoco I hope you don't mind me closing this PR. Am still looking forward to seeing this as a blog post, please let me know if I can help with that.. On the one hand, I've only run into a situation where I needed NODE_ENV=test once. On the other, I can't think of any reason it would have broken my tests.\nI'm less interested in using it to control how babel-register selects its configuration though (I'd prefer the explicitness of BABEL_ENV). It's useful for testing databases and environment variable loaders like envalid, where you may need to set default values for a test environment.\nI'm leaning towards \ud83d\udc4d  on this, but I wonder if the default value ('test') should be configurable in the package.json#ava object.\n@sindresorhus what's your preference on this?\n. Cool, let's do this \ud83d\udc4d . With NODE_ENV, I think the absence of a particular value (production, test) is relevant, not whether process.env.NODE_ENV === undefined. For edge cases where this is critical you can use AVA's require option to modify process.env before your test files are loaded.. \ud83d\udc4d  on @sholladay's suggestion.\nI suspect we still need to tackle https://github.com/avajs/ava/issues/1047#issuecomment-288031293 first though.. > #1467 makes test.only() not be exclusive with multiple test files, so would be nice to have this flag.\nTo be exact, AVA won't switch into exclusive mode until the first test.only() file is encountered. For subsequent files only exclusive tests will be run.. @revelt,\n\nwhy can't we just run a glob against all recognised test files and look for any .onlys in the unit tests? Then, have a think what to do, run .onlys or run everything as it is now? Basically, a one, additional, separate round in the execution chain??? Because we're already globb'ing all test files, we just need to scan them first, before action commences.\n\nWe did consider this previously. There's many ways of using AVA though, and scanning for the occurrence of .only( is not sufficient.\n\nif we'll end up with functionality as Mark says, where t.only kicks in only after first only is encountered, it's pretty much a futile feature \u2013 I just checked, nearly always my util unit tests run first in the queue and therefore would contaminate the console. We need something more.\n\nYes, that is the behavior. The solution is for you to be able to specify that you're running .only() tests using the --only flag on the CLI. That way AVA will not run any tests that are not marked as .only(). This isn't fantastic but we've held back changing the default concurrency for a very long time because of this, and honestly the best solution we've found is this --only flag.\nPerhaps you might have some time to help us out here?\n\n\nI often separate the secondary library's functionality into util.js and therefore, unit tests go separately too. When I troubleshoot code, I rely on console.log's and it is especially important that I should be able to isolate that one particular unit test, otherwise, I'll get multiple console.logs from multiple unit tests.\n\nSounds like the new t.log() feature might be useful for you.. @kristianmandrup this works for me:\n```\n\u276f tree -I node_modules\n.\n\u251c\u2500\u2500 package-lock.json\n\u251c\u2500\u2500 package.json\n\u2514\u2500\u2500 test\n    \u251c\u2500\u2500 foo.test.js\n    \u2514\u2500\u2500 swagger\n        \u2514\u2500\u2500 _utils\n            \u2514\u2500\u2500 utils.js\n3 directories, 4 files\n\u276f npx ava --verbose\n\u2714 [anonymous]\n1 test passed [17:17:34]\n``\n. Ah, no, when you runava test/swagger/_utils/that *overrides* thefilesconfig. Moreover, if you pass a directory, AVA treats all contained.jsfiles are tests. Instead you need to repeat your glob, e.g.ava 'test/swagger/*.test.js'`.. @kristianmandrup Fair enough, I was just looking and it's not really explained. But I think you found the right issue to bring it up in \ud83d\ude04 \nWould you be interested in doing a PR to clear this up?. Hey @mliou8, thank you for your interest. As it turns out @tdeschryver already took this on in #1478. I hope you find one of the other open issues interesting to work on though!\n\nIn the end we figured to allow the 0 value. We now default to --concurrency mode, and the 0 value lets AVA pick a machine-appropriate number of worker processes. The flag should be able to override any package.json#ava value.. See https://travis-ci.org/avajs/ava/jobs/258140018 for a failing test.. Maybe. I suppose something removes spurious output that would get rewritten immediately? I don't think it's a problem if the mini reporter doesn't write that to stdout, and it only broke one test.\nNot sure if we should explicitly bump any dependencies?\nRegardless will merge this to unblock other PRs.. Thank you @tdeschryver!. Nice catch @ydaniv!. @russel how are you running that test? By invoking ava test.js, or node test.js? (I'm assuming your test file is test.js.). @russel despite the global install, you also need to install AVA within your project. Either with ava --init or npm install --save-dev ava. See https://github.com/avajs/ava/#add-ava-to-your-project.. > so I am guessing that globally installed modules are not on the default search path.\nAnd yes that does seem to be the problem for you. Still, it's good practice to install your dependencies inside your project too.. > This would seem to imply that global installs are actually useless: if the global location is not on the package search path, only the per project location is, what is the point of the global location.\nI suspect this is due to how Debian manages Node.js and npm though.. > What you seem to want is something like t.deepLooseEqual() and that we don't currently support.\nThere might be an approach here that could work but at a cost.\nt.deepEqual() is built on top of concordance, which compares enumerable properties, list and iterator items. It also looks at constructors, string tags (Object.prototype.toString.call(foo)) and has value-specific behavior, e.g. comparing Arguments to an array, or comparing non-enumerable Error names.\nIf we strip the comparison down to just enumerable properties and list and iterator items, it just might work for your use case @Qix-. However this comes at a cost. Diffs will not be as optimized as they are with the regular deepEqual. Extraneous or missing Map entries won't be detected at all, since we'd be comparing [key, value] arrays instead. Errors and typed arrays may compare in unexpected ways.\nMaybe this is worth it, or maybe we should advocate an approach where the actual value is stripped off its constructors and whatnot so it can be compared against object literals.. @sindresorhus ah yes, that could be another take. Would you say that it's only loose when it comes to enumerable properties (and constructors / string tags)? E.g. arrays still need to have the same number of items, that each need to (loosely) match.. @delvedor I haven't looked at tmatch too closely, but we'd need to understand how it differs with our t.deepEqual implementation when it comes to:\n\ncomparing primitives and boxed values\ncomparing negative zero\nlists with different lengths\niterables with a different number of items\ncomparing constructor values and string tags\n. >> Can they have different column and row values?\nNo, I don't think so.\n\n\n\nGood to know. I suppose we could change the tty option so we always pass it, with separate flags for stdout and stderr.. We do this because the worker code modifies the process in order to run the tests. This is a side effect that I can't imagine you want even in your test mode. Is there a way to run your meta-tests without actually requiring ava? Perhaps by stubbing the require cache?. > We do this because the worker code modifies the process in order to run the tests.\nTo follow-up on this, we could make it so the worker knows it's in a child process (e.g. through an environment variable) and bail out quietly if it isn't. However there is a second reason we force the process to exit. Libraries like node-tap let you do node test.js and they run the tests. AVA doesn't do that. We print a warning and exit to guide users in the right direction.\n\nIs there any supported way to ship test helpers like our use case that doesn't involve conditionally loading Ava?\n\nNot at the moment, but I'd be happy to find a way. We'd need to meet the following requirements:\n\nrequire('ava') must not modify the process unless ran inside an AVA worker\nnode test-file.js must print the warning and exit\n\nPerhaps in 'ava' we could detect if module.parent === require.main. That should only be true if it's required from a file that is run directly. We could make it so it only prints the warning if that's the case. As mentioned earlier we could find another way of making sure the process isn't modified.\nI'm not sure what the module should export though. A stubbed test which throws when it's used? Would wrapper libraries like ava-describe have to replicate this behavior so they behave correctly when the main module requires ava-describe and not ava?\nI'm not sure this is worth it. An alternative approach might be to support an environment variable which disables the warning, but to be honest at that point I'd prefer if these wrapper libraries worked around AVA's behavior in their tests.. Thanks for your PR @anshulwadhawan!\nI'm not sure why linting started to fail just because the Bluebird import was removed. I've pushed a commit that should solve that.\nFrom the issue:\n\nAs long as we process the require modules early enough, users can still enable long stack traces if they use Bluebird in their own code. It would be good to have an integration test for this though.\n\nWould you like to give this a go? We can leave it as a follow-up so no worries if not.. Hey @anshulwadhawan,\n\nIs my PR merged ?\n\nNot yet, but I'll do so soon.\n\nor you want me to use your commit.\n\nI've pushed it to your branch, so it's already included in this PR.. Thank you @anshulwadhawan!. > Snapshots in one context could be mocks in another context.\nUnfortunately the snapshots are descriptions of the original values. They're stored with enough fidelity to be compared against current values, but restoring them would only work for built-in object types (e.g. no custom classes), and wouldn't really work for functions, symbols, and some other value types. See https://github.com/concordancejs/concordance for the library we use.\n\nThen the following would be possible:\n\ntest the backend api via snapshot testing\ntest the front-end by using the snapshots of the API tests as mocks\n\n\nThis is probably best done through a dedicated library. Nock does this for HTTP requests in Node.js itself.. > Note that CI is currently failing due to a bug in npm@5.4.0: npm/npm#18324. Consequently we shouldn't land it until we can pin to a patched version.\nUnfortunately 5.4.0 was promoted to latest despite this bug. I've pushed a commit which tries to work around this by marking the binary as executable when tests are run. We can remove this workaround once npm installs the binary correctly again.. Turns out that due to https://github.com/npm/npm/issues/18135 and our chokidar dependency we'll always have churn in Travis. I'll be landing this without the churn detection, so that we can commit the churn caused by npm@5.4 (as compared to 5.3).. We're now running into https://github.com/npm/npm/issues/17671. I'm disabling tests with locked dependencies on AppVeyor for the time being.. Lovely work, thanks @philippotto!. Thanks for the report @Siilwyn. You're right, AVA should ignore nested node_modules too.. > I can't think of a real reason to want to distinguish between anonymous tests and empty title tests, so if you're fine with that then I'll make the change.\nYea me neither. I've scanned the discussion in https://github.com/avajs/ava/issues/476 and I don't think we ever considered this. Generally we'd recommend against using anonymous tests, so if you do use them and need to somehow exclude them I guess you might just have to name them \ud83d\ude09 . @ORESoftware we already display the title as \"anonymous\". This PR postulates that !*foo should match anonymous tests, since, by virtue of having no title, their title does not end in foo.. AVA takes a low-key -test()-functions-inside-a-file approach. We do support macros which let you reuse a test implementation:\n```js\nconst likeFoo = (t, Klass) => {\n  const instance = new Klass()\n  t.throws(() => instance.doThing())\n  instance.shouldThrow = false\n  t.notThrows(() => instance.doThing())\n}\nlikeFoo.title = (prefix, Klass) => ${prefix} ${Klass.name} behaves like Foo\ntest(likeFoo, Foo)\ntest(likeFoo, Bar)\n```. > I want to inherit tests.\nThat's not the testing approach taken by AVA. You have to built something on top of it, like you're already doing.. Awesome, thanks @forresst!. > If the parent ava process is passed --inspect=host:port it could pass it to the child process test runners.\nWe'd have to change the concurrency to 1 when a specific port is provided, otherwise all workers will try to bind to the same port.\nBesides that, perhaps a --debug option could be used which picks an available port and uses it for all the workers, running only a single worker at a time.. > Can you reference the location in the code where this change would need to be made and one of us open source contributors can get to work on it?\nSure thing. Concurrency is determined here: https://github.com/avajs/ava/blob/1df502df8cba18f92afcbaed730d8c724687ca45/api.js#L164:L174\nWe massage the --inspect and --debug flags here:\nhttps://github.com/avajs/ava/blob/1df502df8cba18f92afcbaed730d8c724687ca45/api.js#L177:L221\nLooking at that I think these are node-level arguments, e.g. node --inspect node_modules/.bin/ava. Without having thought about the full ramifications, it'd be good if we could support ava --inspect instead.. We should support the modern --inspect flags, passable to AVA directly. That should make it easier to debug with WebStorm too (see #1787).. Have folks seen https://github.com/GoogleChromeLabs/ndb? Would be great to make AVA work with that out of the box.. > This feature is really a necessity.\n@krisnye I agree! Would you like to help us make it a reality?. That's what the issue is for.. @FinnFrotscher would you like to give this a go?. @FinnFrotscher the Node.js flags passed to the worker processes are controlled here:\nhttps://github.com/avajs/ava/blob/ed7807e372f7f50d43597d966a2314bdd7a7d6ff/api.js#L269 \nAVA's CLI flags are here:\nhttps://github.com/avajs/ava/blob/ed7807e372f7f50d43597d966a2314bdd7a7d6ff/lib/cli.js#L56\nThere's more stuff in api.js around the worker concurrency.. @darthgera123 it's still open.. @JavierPons that's great! Beyond what's in the issue description at the top, what information are you looking for?. @JavierPons by all means, go ahead.\nAs the issue describes, you probably need to move up the adapter.installPrecompilerHook(); line. Test it on a local installation first (use npm link so you can test against your local AVA checkout). Helper files are files inside a test folder that start with an underscore. If you use import to require a dependency in that helper file it'll be a good way of checking whether the precompiled version was loaded.\nIf that's all looking good you'll have to make an integration test. There should be bunch of examples in test/cli.js.\nPlease open a PR with your initial changes so we can iterate on them more effectively. Thanks!. > This looks like a problem with npm.\nYea. This seems to use AVA 0.20.0, which depends on chalk@1. stripColor is removed in chalk@2, so AVA is using the wrong chalk version. Which is a problem with the installation, and thus npm.. Hi @jugglinmike, I haven't forgotten about you \ud83d\ude04  Glad to see the tests are passing, I'm hoping to merge this later this week.. Thank you @jugglinmike! There should be a new release out within the next few days.. Makes sense.\n\nThe only alternative is to restart AVA, which can take a very long time.\n\nYou can type r followed by the return key, and that'll rerun all tests without having to restart the watch process.. Oh this is a bug!\nThe watcher is observing .snap files: https://github.com/avajs/ava/blob/1df502df8cba18f92afcbaed730d8c724687ca45/lib/ava-files.js#L268\nHowever it fails to recognize them as source files: https://github.com/avajs/ava/blob/1df502df8cba18f92afcbaed730d8c724687ca45/lib/ava-files.js#L170\nThis means that it can't trace the removal of the file to the test file that created it, so it reruns all tests.\nThose arrays should have the same items, or perhaps we can even define the array as a constant. Just need to make sure it's not accidentally modified.. > Am I reading correctly that if you have custom paths, the watcher won't even watch .snap files? Those only get added if paths.length === 0?\nYes, but that's the same for the package.json and **/*.js paths. There has to be a way of overriding patterns (especially when they contain **), and the easiest approach is to make you restate what you need.. @mfainshtein2 as a first step try reproducing the issue. https://github.com/avajs/ava/issues/1511#issuecomment-328351206 has a proposed solution which hopefully is still up to date. You can find a previous attempt to fix this in https://github.com/avajs/ava/pull/1515.. @mfainshtein2 as a first step try reproducing the issue. https://github.com/avajs/ava/issues/1511#issuecomment-328351206 has a proposed solution which hopefully is still up to date. You can find a previous attempt to fix this in https://github.com/avajs/ava/pull/1515.. > it does show the following message if the other test has a mismatch. Is this intentional?\nIt is.. Yea go for it @itaisteinherz!. We won't support -r in AVA's CLI flags, nor add additional flags to the workers.\nYou should be able to use node -r \"@std/esm\" ./node_modules/.bin/ava, or alternatively write a file that creates the loader and require that through AVA's package.json configuration. Though I can't figure out from the documentation whether that loader is then used for all modules.. > Tried this: node -r \"@std/esm\" ./node_modules/.bin/ava; no dice w/ v0.8.3 of esm.\n@mAAdhaTTah , I had assumed that would be propagated to the workers, but maybe not.\n\nIt looks like AVA sends commands as JSON string arguments to its forked processes. I should be able to sniff them. Is this a standard thing or something AVA cooked up?\n\n@jdalton it's our own thing. Note that we do have a stalled PR that aims to forward remaining arguments to the workers, but they'd come after the filename argument so that wouldn't quite help with -r.\n\nit's running into AVA's built-in Babel use (AVA is transpiling the import/export). I tried to disable it with some Babel configs but not having any luck. Any guidance @novemberborn?\n\nIt is possible to disable this, yes. You'd lose all other transforms though (currently not a problem with Node.js 8, but it'll hurt the compatibility of your tests with earlier versions). But we can find a way of running https://github.com/avajs/babel-preset-stage-4 without the module transform.. > Can you go into detail. I know some Babel presets like babel-env allow you to disable module transpilation with \"module\": false option. How does that map to AVA?\n@jdalton we currently do not have that option, but we should. @dinoboff helpfully opened https://github.com/avajs/babel-preset-stage-4/pull/8.\nBesides this detail, is there anything that prevents @std/esm from working with AVA? And has there been consideration for enabling the processing of all files via an environment variable?\n. Yea that's not great. Thanks for reporting it.. Thanks @mliou8!. Hey @wmertens, do you have the time to finish this up, or would you rather I give it a go?. No worries @wmertens. I'll probably focus on other issues first.. No worries @wmertens. I'll probably focus on other issues first.. Hey @wmertens I'm going to close this, if only for housekeeping reasons. Feel free to reopen / make a new PR if you have the time to get back to this.\nI have some thoughts on how AVA should select test, helper and source files that will impact watch mode. I've added the appropriate label to #1511 so maybe I'll be able to solve it all in one go when I get to that stage.. Stack Overflow and Gitter are better venues for this kind of question:\n\nhttps://stackoverflow.com/questions/tagged/ava\nhttps://gitter.im/avajs/ava\n. > But how does ava determine the file sequence?\n\nWe use third-party modules to find the test files. There is no guaranteed order.\n\nMy test suite needs run couples of files in a particular sequence, say, test-file-1.js, test-file-2.js, test-file-3.js, etc. How can I specify the execution sequence with ava?\n\nThis\u2026 doesn't sound great. You should revisit your test setup so you do not have this dependency. The --serial option exists for debugging purposes. AVA assumes test files can be executed independently from each other.\n. > I made this change due to my assumption that because this package now contains a package-lock.json file and not a yarn.lock file that npm 5 is now the preferred method \ud83d\ude0f\nOops, my biases are showing \ud83d\ude09 \nThanks @ntwb!. The verbose reporter prints each test, even when successful. I don't think that's the issue here. It looks like when you run AVA through nyc the highlighted line contents aren't shown. That's quite odd, especially since the surrounding lines are displayed.\nCould you try nyc ava --no-color (or perhaps it needs to be nyc ava -- --no-color)? That'll disable the color output.. @niftylettuce would you be able to share your project? Or make a reproduction that is shareable? I don't know why this would be happening so I'm afraid I can't be of assistance without reproducing the issue locally.. Yea I would have expected the GitHub diff. Would you be able to share an isolated test case at https://github.com/concordancejs/concordance?\nThere's an existing issue for improving string diffs, but the issue here seems to be that Concordance isn't correctly detecting missing / extraneous array items. But still, see https://github.com/concordancejs/concordance/issues/29 \ud83d\ude04 . Yea sounds good. Help wanted! \ud83d\ude04 . Remove the React section, yes.. Remove the React section, yes.. Thanks @P-Seebauer!\n\nI haven't found a good place in the docs to put the \"warnings about the dangers\".\n\nPerhaps https://github.com/avajs/ava/#process-isolation comes closest, as it talks about the test process. Add another paragraph explaining what value of NODE_ENV to expect, and warning users that this may influence their code?\n\nAlso stops process.env.AVA_PATH from getting overwritten in fork.js. I did not find a reason why the Object.assign is inside the if statement. (I have had a look at #559 and the likes and it seems to me that there is a similar issue with the AVA_PATH waiting to happen). Since there are no tests, I believe that the overwrite was not intentional (and could move to another place, if it was).\n\nI think you're right. I must've assumed it was a copy already. The value isn't used in the main process though so it was harmless.\n. Thanks @P-Seebauer!. > This issue was a surprise for me after first submitting gh-1508 because I had been using npm version 3. That version does not recognize the package_lock.json file, so I had been vetting my changes with newer versions of the module dependencies than are available in the continuous integration environment (this is why the tests pass for the build that sets FRESH_DEPS to true).\nThis is odd. The package-lock.json can help ensure everybody has the same dependencies when developing, and stop us from accidentally using a dependency's API that came with a newer version of that dependency than is in our SemVer range. Sounds like there is an unexpected breaking change in perhaps chalk? If we update our version of chalk would that help? How does https://github.com/avajs/ava/pull/1401 impact this problem?\nI'm perpetually confused by color management in AVA, and this PR seems appropriate despite my questions in the previous paragraph. Still, I'd like to understand it all a bit better before hitting the merge button.\n(And, as it stands, there are some linting errors.). Makes sense, thanks @jugglinmike!. @rilut that's great!\nThe last run stays visible. Best approach right now would be to write this after we finish the logger:\nhttps://github.com/avajs/ava/blob/f43d5ae5d0b89a1cb80bbb0bf37ac37b75cf7518/lib/watcher.js#L125\nPerhaps we could implement another method on the mini and verbose loggers to write instructions.. Sounds great @KompKK!. This is odd. The inactivity implementation doesn't use timeouts though. It registers a hook for when the process exits, which will happen when the event loop is empty.\nIt seems as if you're running into a Node.js or even V8 error?\nI'm closing this since I don't think it's an AVA issue, but I'm happy to help debug this further. Though I suspect you can't share your code.. @pigcan could you provide details on your Node.js version, and if possible even your test suite?. @Smilebags could you share your example?. There is a longer term goal to enable AVA to transpile source files. Nearer term though I'm planning on fixing the helper globbing.. Sure.\nDoes Enter rerun all tests, like r does with AVA? That's quite neat actually.. > Enter reruns the filtered list of tests. They use a to rerun all tests.\nSounds like we should borrow that too \ud83d\ude09 . > However, I notice the code also check for rs: /lib/watcher.js@master#L278\n\nwhich does the same thing as r\nAny reason for that? if not, I can remove it (which simplifies the code).\n\nIt's what nodemon uses. But we can remove that.. > I initially wanted just u, but I no longer remember why we went with u + Enter.\nSee https://github.com/avajs/ava/pull/1533, raw mode breaks interrupts.. AVA won't guarantee any particular execution order.\nWhy do you need this?\n(I'm closing this issue, but please don't refrain from discussing it further.). @johhansantana try the --serial argument or serial option in AVA's package.json configuration.. @piercus the order within a test file is currently fixed, though asynchronous tests may complete in a different order based on the async APIs they're relying on.\nThe order by which test files run is not currently randomized, but we don't guarantee any particular order either.\n595 proposes we randomize the order, which as you found requires a way to reproduce that order, too. So ironically that would help you.\nI would use the --verbose logging option to see if there's actually a difference between failing and passing test runs. That may give you a clue, or it could turn out that the failure is not related to the execution order at all.\n. > This PR mostly works except killing the process requires hitting [Ctrl+C] twice, which guess is related to the test is running in a different process.\nI think this is what made us revert https://github.com/avajs/ava/pull/658. How does Jest handle this?. Hi @unional, I'm sorry to see that this is closed. Did you come to the conclusion that this isn't possible the way we currently want it, or did you close it because your otherwise busy, or perhaps for another reason?. Go for it @Lifeuser! \ud83d\udc4d . Ah yikes, this does look quite complicated @Lifeuser!\nWith the beforeEach and afterEach hooks, I think we need to output their logs together with the test itself. This hints at refactoring what counts as test completion, to be honest. See also https://github.com/avajs/ava/issues/1330.\nThe before and after hooks should probably just be printed. That's not hard to do but I don't know if we should land that without fixing this for the per-test hooks.. @Lifeuser thanks for the heads up \ud83d\udc4d . Also when things get weird, try npm rm ava followed by npm install --save-dev ava. That should force sub-dependencies to be updated.. Lovely, thanks @Couto!. Interesting. The timestamp was introduced with https://github.com/avajs/ava/pull/737 but admittedly it's not terribly useful outside of watch mode.\nWe're not looking to add test duration though. There's too much variability in how tests are run, and how, when they're run concurrently, they may affect each other's performance. Printing a number doesn't make it any easier to figure out why tests are slow.\nWould you be interested in changing this PR to only log the timestamps in watch mode?. I'd like to see an API exposing details around test runs so this kind of reporting can be provided through a third-party module. That's not an immediate priority though.\nMeanwhile, given how much the number depends on how your tests are set up, we don't want to provide it without making sure people have the context to avoid needlessly optimizing their test runs just to make that number go lower.\nThank you @fruch for opening this PR. Hopefully there are other parts of AVA you're interested in contributing to \ud83d\udc4d . I've been collecting these issues under the globbing label. I haven't quite summarized the solution, but I actually do think we should support separate helper patterns.\nI also like the idea of supporting __helpers__.\nI should have more time to work on AVA soon, and this is one of the areas I'd like to tackle. So stay tuned (but not too tuned, can't make any promises \ud83d\ude09 ).. > Installing from git URL doesn't work because ava isn't published as-is to the npm repository, it has a build step.\nThe build step generates the TypeScript definitions. It's not necessary for most users.\nLooking at https://docs.npmjs.com/misc/scripts I see there's a prepack option:\n\nprepack: run BEFORE a tarball is packed (on npm pack, npm publish, and when installing git dependencies)\n\nSounds like this should work for all our use cases. We'd need to change our prepublish to prepack and then verify everything still works as expected.. Hey @clemtrek thanks for your interest. We don't have a list of companies. Perhaps browsing https://www.npmjs.com/browse/depended/ava could yield some insights.\n. Closing due to inactivity, but @samhatoum please feel free to respond when you get back to this.. @samhatoum AVA supports the --timeout CLI flag, as well as a \"timeout\" option in its package.json configuration. There is no default timeout.\nAVA does not allow this value to be configured within the test files, but perhaps you could detect when the option is configured and await driver.debug is used, so that you can warn users that AVA may interrupt their test run?\nThere is no official way to access the runtime options, but I'm pretty sure you can (at the moment) access them through process.argv[2]. . @lukechilds @sindresorhus OK, I'm still leaning towards not capping this outside of a CI environment. Just seems like unnecessary logic.. @lukechilds I've taken the liberty to push these changes. Now also using is-ci rather than directly checking the environment.. Thanks @lukechilds!. Yea, I'd say this is due to power-assert. The generated code is:\n```js\n    for (let i = 0; i < n; i++) {\n      var _rec = new _powerAssertRecorder();\n  t.true(_rec._expr(_rec._capt(1 === 1, 'arguments/0'), {\n    content: 't.true(1 === 1, \\'1 should equal 1\\')',\n    filepath: 'test.js',\n    line: 13,\n    ast: '{\"type\":\"CallExpression\",\"callee\":{\"type\":\"MemberExpression\",\"object\":{\"type\":\"Identifier\",\"name\":\"t\",\"range\":[0,1]},\"property\":{\"type\":\"Identifier\",\"name\":\"true\",\"range\":[2,6]},\"computed\":false,\"range\":[0,6]},\"arguments\":[{\"type\":\"BinaryExpression\",\"operator\":\"===\",\"left\":{\"type\":\"NumericLiteral\",\"value\":1,\"range\":[7,8]},\"right\":{\"type\":\"NumericLiteral\",\"value\":1,\"range\":[13,14]},\"range\":[7,14]},{\"type\":\"StringLiteral\",\"value\":\"1 should equal 1\",\"range\":[16,34]}],\"range\":[0,35]}',\n    tokens: '[{\"type\":{\"label\":\"name\"},\"value\":\"t\",\"range\":[0,1]},{\"type\":{\"label\":\".\"},\"range\":[1,2]},{\"type\":{\"label\":\"true\"},\"value\":\"true\",\"range\":[2,6]},{\"type\":{\"label\":\"(\"},\"range\":[6,7]},{\"type\":{\"label\":\"num\"},\"value\":1,\"range\":[7,8]},{\"type\":{\"label\":\"==/!=\"},\"value\":\"===\",\"range\":[9,12]},{\"type\":{\"label\":\"num\"},\"value\":1,\"range\":[13,14]},{\"type\":{\"label\":\",\"},\"range\":[14,15]},{\"type\":{\"label\":\"string\"},\"value\":\"1 should equal 1\",\"range\":[16,34]},{\"type\":{\"label\":\")\"},\"range\":[34,35]}]',\n    visitorKeys: _powerAssertVisitorKeys\n  }), '1 should equal 1');\n}\n\n```\nOn my machine, with Node.js 8.7.0 the performance is a lot better though:\n```\n\u276f npx ava\nfunction ava t.true compare used: 2603 ms\nfunction plain compare used: 5 ms\n1 passed\n```\nAnd without power-assert:\n```\n\u276f npx ava --no-power-assert\nfunction ava t.true compare used: 1601 ms\nfunction plain compare used: 7 ms\n1 passed\n```\nAlso note that your 'plain compare' isn't a fair test, since t.fail() never executes.\n\nThank you for raising this @guotie. It's not currently a priority to wring every ounce of performance out of these assertions, and I think that as Node.js gets faster this isn't as big of an issue. Most tests tend to run but a few assertions, too.\n. Great observation @sholladay.\nCurrently we go straight for a diff: https://github.com/avajs/ava/blob/eebf26e6e0bafd3c96f8e20311377a11b9a4f695/lib/assert.js#L90-L97\nWe could perform a deep equal first and generate the diff when that doesn't pass, and otherwise print an appropriate message. That's a bit more expensive performance wise but IMHO we shouldn't optimize assertion failures as much as we should optimize the happy-everything-passes path.\nPlease drop a comment if you're reading this and want to take this on \ud83d\ude04 . Hi @rilut, just wondering if there's more commits forthcoming? Would be good to have some assertions on the logger.write() spy, like is done for the other logger methods. Thanks!. @rilut thanks for following up. I've just restarted CI, so hopefully all the tests will pass. Still need to give this a whirl, have been a bit busy sorry.. Ah no, the CLI integration tests are failing because there's new output. Could you look into that @rilut? \ud83d\ude04 . @rilut the CI failures seem to be intermittent. I'm running out of time for today but I'll try and have a look at this tomorrow.. @rilut I think what @sindresorhus is seeing is due to my earlier comment:\n\nUnfortunately this doesn't work with the mini reporter: the test output itself is overwritten.\n\nMy suggestion for resolving this is https://github.com/avajs/ava/pull/1555/files#r153833345.. Closing this since it has stalled. If anybody is looking to revive this PR please check in first, I'm considering refactoring the reporter pipeline which impacts this PR.. Yes. See also https://github.com/avajs/ava/issues/709 (don't worry I'll clean up the issues).\nSay we have a \"babel': false option. Should that also imply \"powerAssert\": false? I'm leaning towards yes, since power-assert requires transpilation, whether via Babel or https://github.com/power-assert-js/espower-loader. But it might be an unexpected way to lose that feature. Same goes for our throws helper. Should there be an easy way to opt back in to it?. There's something to be said for separating AVA's file manipulations from syntax transforms. The babel option could refer to syntax transforms only, while we have a separate transpileEnhancements option. This would supersede the current powerAssert option.\nSince we're still looking to integrate source transpilation and also support TypeScript, it makes sense (at least to me) that the enhancements are controlled separately.. There's something to be said for separating AVA's file manipulations from syntax transforms. The babel option could refer to syntax transforms only, while we have a separate transpileEnhancements option. This would supersede the current powerAssert option.\nSince we're still looking to integrate source transpilation and also support TypeScript, it makes sense (at least to me) that the enhancements are controlled separately.. #1608 includes a compileEnhancements option. If set to false, and babel is also set to false, then the Babel pipeline is skipped in its entirety.. @cdaringe that's not how AVA's options are set. You'll also still need to disable the regular Babel compilation that AVA applies:\njson\n\"ava\": {\n  \"babel\": false,\n  \"compileEnhancements\": false\n}. @cdaringe that's not how AVA's options are set. You'll also still need to disable the regular Babel compilation that AVA applies:\njson\n\"ava\": {\n  \"babel\": false,\n  \"compileEnhancements\": false\n}. @sholladay AVA won't apply the stage-4 preset, but power-assert will keep working, and we'd detect t.throws(throw new Error()).\nWhilst those last two features are currently provided through a Babel preset I think that's an implementation detail. We could change the compiler we use, and when we add native support for TypeScript we may still want to add those features to test files written in TypeScript.. @sholladay AVA won't apply the stage-4 preset, but power-assert will keep working, and we'd detect t.throws(throw new Error()).\nWhilst those last two features are currently provided through a Babel preset I think that's an implementation detail. We could change the compiler we use, and when we add native support for TypeScript we may still want to add those features to test files written in TypeScript.. @cdaringe are you per chance applying nyc in your test run? Feel free to pop in on https://gitter.im/avajs/ava so we can hash this out more directly (assuming our time zones have some overlap).. @cdaringe are you per chance applying nyc in your test run? Feel free to pop in on https://gitter.im/avajs/ava so we can hash this out more directly (assuming our time zones have some overlap).. @cdaringe aha! profile.js doesn't skip compilation. It's a bit of an odd duck. Nice catch!. @cdaringe aha! profile.js doesn't skip compilation. It's a bit of an odd duck. Nice catch!. The idea is that the new value should be adjusted to match the old value. If you'd used t.is():\n```\n  direct diff\n/private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.dcvW5BUyA5/test.js:8\n7: test('direct diff', t => {\n   8:   t.is(\\n${Date.now() - 42e5}\\n, \\n${Date.now()}\\n)\n   9: })\nDifference:\n`\u240a\n\n\n1508765718717\u240a\n1508769918717\u240a\n    `\n```\n\nWhat is definitely wrong though is that the colors are not inverted the way the gutters are. I've just opened https://github.com/concordancejs/concordance/issues/40 for that.\nIf that were fixed would you be happy with the current behavior?. In the t.is(actual, expected) example below, do the indicators make sense?\n\nThe - value is supposed to be \"bad\", with the + value being \"good.\" That should then translate to the snapshot, where the current value is assumed to be \"bad\", since it doesn't match the \"good\" snapshot.\nJust like a code patch in a bugfix takes you from a \"bad\" file to a \"good\" file, so would \"applying\" the diff in AVA's output help you make the actual value match the expectation. I think that holds for snapshots, too.. @nesbocaj so if I'm interpreting this right, we'd keep the - and + as they are now but make the - lines green and the + lines red?. @nesbocaj makes sense. @wmertens what do you think?. Let's start with adding - Received\\n+ Expected when we show diffs, and if they're from a snapshot show - Received\\n+ Snapshot. We should match the current colors, so the \"received\" line should be red and the expected/snapshot line green.\nIn essence, this line should track whether the diff comes from t.snapshot():\nhttps://github.com/avajs/ava/blame/334e15b4af06492c9aed2800a0764f245d6a908b/lib/assert.js#L12\nThen the message can be added here:\nhttps://github.com/avajs/ava/blame/334e15b4af06492c9aed2800a0764f245d6a908b/lib/reporters/format-serialized-error.js#L16. > My understanding is that ava provides no way of checking an assertion status as it completes.\nI'd love to support that. Please see the discussion at https://github.com/avajs/ava/issues/1485.. @mightyiam what happens without this type definition? I'd have expected this to work out of the box, that's why the Macro interface exists.. I'm confused as to why you're explicitly typing const macro: Macro<AssertContext>. Why is that necessary?. Thanks @mightyiam!. Here we're dealing with errors that are not handled in the test or by assertions. They could even be bugs (throw err where err is an undefined variable). I don't think we need to proselytize in the uncaught exception handler. That's what t.throws() is for.. This was fixed in #1722.. Very nice @codeslikejaggars!\nRegarding testing, I'd add some tests to test/babel-config.js which compare the plugins to see if babel-plugin-syntax-object-rest-spread is included. Override process.versions.node so that there's a test with a version older than 8.6.0, then 8.6.0 exactly, and then say a 9.0.0 version.\nWe could do an integration test but that's tricky since we need to test the Node.js version to see whether the test should pass or fail\u2026 let's instead just see if the Babel config is generated correctly.. Hey @codeslikejaggars I've pushed some minor changes, and added some documentation.\nTurns out Node.js has supported this since 8.3.0 so I've adjusted the version numbers accordingly.\nPlease let me know if this all looks OK to you! \ud83d\ude04 . Hey @codeslikejaggars I've pushed some minor changes, and added some documentation.\nTurns out Node.js has supported this since 8.3.0 so I've adjusted the version numbers accordingly.\nPlease let me know if this all looks OK to you! \ud83d\ude04 . @codeslikejaggars had to force push to fix a merge conflict, sorry.. @codeslikejaggars had to force push to fix a merge conflict, sorry.. Thank you @codeslikejaggars \ud83e\udd47 . Thank you @codeslikejaggars \ud83e\udd47 . p-timeout would be nice here:\n```js\nimport timeout from 'p-timeout'\ntest('foo bar', async t => {\n  await timeout(arbitraryDuration(), 5000)\n  t.pass()\n})\n```\ntimeout() will reject with a TimeoutError, which then causes the test to fail.\nI think asserting a max duration is sufficiently rare that we don't need to add a method for it.. Sure.\nIt'd also be a way to keep the process alive in case of unreferenced timers and such.. This is an issue in write-pkg: https://github.com/sindresorhus/write-pkg/issues/9. @Lifeuser I pushed a tiny fixup, but this is good to go as soon as CI passes. Thank you!. > Note that this doesn't totally rematerialize the original syntax since the tokens included with the power assert context don't retain quote or whitespace information.\nYea that's fine.\n\nThis implementation outputs strings with single quotes to match the output of concordance.\n\nNice touch \ud83d\ude04 . > Note that this doesn't totally rematerialize the original syntax since the tokens included with the power assert context don't retain quote or whitespace information.\nYea that's fine.\n\nThis implementation outputs strings with single quotes to match the output of concordance.\n\nNice touch \ud83d\ude04 . \ud83c\udf89 . \ud83c\udf89 . Awesome work @codeslikejaggars \ud83d\ude80 . Thank you for making the change here @ahmadawais \ud83d\udc4d . Thank you for making the change here @ahmadawais \ud83d\udc4d . Our TAP reporter doesn't really include the information I guess tap-diff relies on. I'm not even sure if there's a spec on how that should be included, especially for values that aren't easily serialized to JSON.\nAVA's regular reporters do show diffs. I recommend you use those, but I appreciate that might not help if you have a workflow that depends on TAP.\n(I'm closing this issue since I don't think there's anything AVA can do here, at least at this point. Please feel free to continue the discussion.). Thanks for your hard work on this @codeslikejaggars. We have a lot of issues to do with globbing and performance and I have a feeling that those issues are all connected, and require a holistic approach. I need to do more thinking and research on this, and I'd like to avoid making any changes until then.. Hi @hzoo and @loganfsmyth, thanks for reaching out.\nI think for the time being AVA will depend directly on Babel, as one of our selling points is that you get stage-4 features across Node.js versions out of the box. That said we can inspect the package.json to see if a newer Babel version has been installed in the project itself.\nPerhaps not immediately but eventually we'll switch to depending on Babel 7 directly, and falling back to Babel 6 if it's installed in the project being tested.\nWe'll make sure our own presets are compatible with both versions, and we'll have to update our config resolution to support the new .js files.\nWill babel-core@latest see a v7 release, or will this solely be @babel/core?. > Fair enough. As people switch to v7 you'll likely start seeing issues from users who use \"babel\": \"inherit\" since that will try to load Babel 7 plugins into Ava's Babel 6 core.\nRight, but I'm thinking we'll detect when people have v7 installed and use that instead. Then a little while after v7 has come out and the wider ecosystem has had a chance to adjust we can switch to run v7 by default (and perhaps detect and use v6 when installed).\n\nI don't think I realized Ava has its own implementation of our config resolution. I'd love to talk eventually to see if any of those performance improvements could be built into Babel's core itself to avoid you needing the separate implementation.\n\nYup, happy to talk. There's a variety of (somewhat subtle) reasons. The new dynamic configuration files do make it trickier though \ud83d\ude09 \n\nThe plan at the moment is to have @babel be Babel's new home. The only thing I've proposed, which isn't guaranteed, is that we could technically make a babel-core package that had its own peerDep on @babel/core as kind of a bridge so packages could write a peerDep on \"babel-core\": \"6.x | 7.0-bridge\" rather than making packages introduce a breaking change to remove babel-core with @babel/core, but not decided on that.\n\nThat sounds great.. > Can't we just upgrade to Babel 7 in a new AVA version and people that can't upgrade to Babel 7 yet can just wait on upgrading AVA?\nWe could, yes. I haven't looked enough into the changes to venture a guess as to how disruptive it'll be.. I've posted the plan in #1598. Thanks all.. > I think you are right that this is not a problem with babel, but AVA with Babel 6 too much under it's skin. I hope that AVA will sort this out, it's excellent tool.\n@dacz AVA does use some plugins that, as you found, were written for Babel 6. You can follow #1598 for progress on making AVA work solely with Babel 7 instead.. @dacz you'll have to install from GitHub, yes. It should work with npm install avajs/ava#babel7.. I don't think we're ready to settle on any kind of stable interface to the workers, or indeed making these low-level parts extensible, sorry. The best bet at this stage is to use the CLI. The profile.js stuff is quite the hack in itself and not fully baked IMHO.\n(Closing now to keep the issue count down, but please feel free to continue the discussion.). @rossoneri520 I assume you have more than ten asynchronous tests in one test file? What AVA version is this with?\n\nthen some test fail with \"Rejected promise returned by test\", I guess memory leak take chrome launch timeout, so can I only reduce the parallel unmber?\n\nI'm not sure what you mean here with memory leaks and chrome launch timeout.\n\nBTW, Can I define the number of the parallel test in one JS file?\n\nNo, but you can use test.serial to run one test at a time (within that test file).. @sindresorhus interesting:\njs\nconst arr = require('negative-array')([1, 2, 3)\nArray.isArray(arr) // true\nObject.prototype.toString.call(arr) // '[object Array]'\narr.length // undefined\nSo arr walks like a duck, but it doesn't talk like a duck. I think this causes Concordance to walk through the list items until the process runs out of memory, given https://github.com/concordancejs/concordance/blob/262782ac7f9de37d05c0f5460884ddc4b92e0c85/lib/complexValues/object.js#L116-L124.\nI'm inclined to say the real bug is with negative-array, and all Concordance can do is to throw an error when it thinks it's dealing with a list but length is not a number.\nrequire('concordance').format(arr) works when I patch negative-array to return .length.. Yes I was thinking about that the other day. I'm not sure why that's not allowed at the moment.\nWe'd have to decide whether to ensure tests still receive a unique context, since they shouldn't modify a shared object. Either we document that as a pitfall, or we require the context that's assigned in test.before() to be an actual object and we do Object.create(context) to create a new reference for each test. What do you think? And @avajs/core?. No, the beforeEach hook is the first time context can be created currently.\nWe just need to make sure that whatever value is assigned to t.context in a before() hook is copied before any beforeEach hook (or the test itself, if there is no such hook) is run. Otherwise we'll see hooks modifying the context and affecting other tests.\nThis might mean t.context is restricted to primitive values, plain objects and arrays, since we can't copy functions or Maps and so forth. I don't think we need to worry about deeply nested values.. > Having t.context contain deeply nested plain objects and arrays would still be fine though?\nYes, but we won't worry about making deep copies of them or anything like that.. The functionality has landed in #1657 but it still needs to be documented. @kugtong33 is awesome and on the case though!. Nice catch, thanks @jedmao! I've updated the release notes as well.. You could also put require('console-caller')() in the test files themselves, or in a test.before() hook.\nI'm really not sure what the poster on Stack Overflow was trying to achieve.. Besides familiarity with Jest, could you elaborate on why you'd find this information useful?\n\nNew snapshot files would be apparent in source control\nWe report snapshot failures already, and we're working on printing the available commands in watch mode (#1525)\nWe have an open issue for removing obsolete snapshot files (https://github.com/avajs/ava/issues/1424) but again I think source control is sufficient here\n. Ah. The Markdown file is appended to, rather than rewritten, for performance reasons. It's not used by AVA though. I don't know how big of a problem this is, but then to be honest I tend to explicitly update snapshots before committing them.\n\n\nThis isn't obvious, because there's no messaging that tells me my snapshots need to be updated. Technically, the snapshot itself perhaps doesn't need to be updated, but the Markdown file does (at least).\n\nThey don't need to be updated manually, though I appreciate you're trying to avoid extraneous changes in the future when you're explicitly updating snapshots for unrelated reasons.. Right so you're saying you'd like a reminder from AVA that snapshots were modified, so you can immediately rebuild them. That makes sense I think.\nNote though that we shouldn't communicate \"and now update your snapshots\", since everything is working fine. This is for those of us who can't abide the thought of future churn in the Markdown files \ud83d\ude09 . \"Updated 2 snapshots\" would be sufficient I think. Folks like us will catch on and know what to do, especially when we land #1525.. @niftylettuce is that index.js file the result of a Babel compilation in any way? And if so are you inlining the source map, or writing it to disk?. No accidental babel-register usage either? AVA shouldn't rewrite stack traces without a source map. The character offsets seem really high though (666, 582) which implies some sort of transform.. Could you push a branch where the above error occurs?. Closing in favor of https://github.com/avajs/ava/issues/974#issuecomment-390478448.. How about:\n\nCould not find snapshot. Note that snapshots are not created automatically when in a CI environment.\n\n(This would be shown as the message for the t.snapshot() assertion failure.). @AJamesPhillips would you be willing to help out with this?. > In this case they were run locally inside a docker container.\nIf you run tests in a Docker container, how do you update the snapshots?\nThe idea is that during development you'd notice the new snapshot files (since you need to commit them). I think if we implement the CI flag then you could set CI=1 in your container and AVA will treat it as such. Would that be sufficient?. > In our case it would be reasonable as it would sit in the script file we call on the ci server to setup, build and run the tests in the container.\nArguably you'd want to forward env variables from Jenkins to your container, so I think it's reasonably that you'd need to do the same when testing your container setup outside of Jenkins. I admit it's not immediately obvious, but that also goes for having a configuration option.. > Pragmatically, it complicates scenarios like running a test suite as a commit/push/prepublish hook. I've often successfully pushed commits (relying on my git hook to fail if the tests do anything other than succeed) only to realize that the test suite passed but dirtied the repo, so my (incorrect) commit shouldn't have gone through.\nOh that's a good point, I wonder if we could detect that, too?\nI hear what you're saying @billyjanitsch, though I'm reticent about having a flag to disable this behavior. Instead, what if we'd write snapshot updates to a temporary location, exit with an instructive failure and exit code of 1, and then have a npx ava --commit-snapshots command (pending bike-shedding) to write them into your source tree?\n. I'm sorry about the confusion. Without a dedicated documentation website this is bound to happen. We're quite happy to receive documentation updates along with pull requests when new features are introduced. It wouldn't be feasible to land these later when we do a release.\nFor future reference (and for others coming across this PR), you can browse the recipes by version tag though, see the v0.23.0 bit in the URL here: https://github.com/avajs/ava/tree/v0.23.0/docs/recipes.\nThank you for your PR and interest in AVA.. > I would like to be able to control where my snapshots end up during run time.\n\nHere, I have a for loop that iterates over some items of interest at runtime, and generates DOM snapshots based on what it finds.\nHowever, this creates one giant file and personally I'd rather be able to pass in the desired file name for each individual call of t.snapshot in my project.\n\nI considered this. It's tricky though since it means multiple test files could be using the same snapshot file. Since we run test files concurrently this makes it hard to reliably update the snapshot file.\nI'm open to this if we can figure out graceful solutions to those race conditions.\n\n(On a side-note, if anyone here is able to figure out why my project's snapshotLocation isn't being honored, I'd be grateful. So far I've had no luck creating any solid reproduction case other than pointing to the entire project linked above)\n\nIt's because the documentation was wrong, Use snapshotDir instead, see https://github.com/avajs/ava/pull/1581.\n. We'd need to transmit the snapshot data to the main process and only write the file once all tests have run. But what happens if you run AVA with specific test files and update snapshots, thus removing the snapshots created by a different file?\nSimilarly there'd be collisions if tests in both files have the same title. All this gets pretty complicated!. Your src directory contains .ts files. By default AVA will be loading .js files instead. You could add ts-node/register to AVA's require configuration and it should work.. See https://www.npmjs.com/package/ts-node.\nConfigure the require option as shown in https://github.com/avajs/ava#configuration \u2014\u00a0but use ts-node/register instead of babel-register.. AVA will find and use the closest package.json from the current working directory. What I've done previously in a monorepo project is to run all tests from the top-level directory, and using globbing patterns to run tests from a specific package.\nAlternatively you could use the extends option in the Babel configuration to point at a .babelrc file in a higher directory. That way you don't have to duplicate the file.\nI'd like AVA to work well with monorepos but I'm not convinced yet we'd need to change how we load the configuration.. Node.js loads files differently from the <script> tag. Each module has its own scope, so your message function is not actually a global. You'll want to use a module loader and a build tool like webpack, so you can author and test your modules and then separate make a bundle that can be used in the browser.. Yea we can give it a shot. It's a bunch more function allocations so it'd be interesting to see what happens when there's loads of tests in a single file, but on the other hand I doubt it'll really have an impact, especially in the most recent Node.js.. @reddysridhar53 it's all yours!. I've pushed a rewrite that focuses this recipe on using @std/esm, with AVA 1.0 in mind.\n@jdalton is the configuration OK, and the example accurate?. Thanks all!. I'd say this is a duplicate of #583, yes. Closing it accordingly.\n\ni want to be able to detect the failing tests then an automated build is being run\n\nCould you elaborate on this? Why does the proposed solution for #583 not solve your problem?. #583 is the issue that tracks improvements to the timeout feature, so that it would indeed show the pending tests when a timeout is detected. For now I'd run the test suite with the --serial flag to see track down the offending tests.. Nice work @ace-n! Test state jumps through a lot of hoops on its way to the main process so well done following it all.\nI'd like to suggest a slightly different approach though, one that is a bit more explicit and requires fewer low-level changes. See the inline comments below. Let me know what you think!. > Another side-question: do we want to report pending after() calls? e.g. if all your tests pass, but AVA doesn't terminate automatically - should we tell users which after() call is pending?\n\nPerhaps this should be part of another issue altogether? I've personally ran into this in the past, but I don't know whether this feature would be useful for others.\n\nThat would be useful, yes, but it could be done in a separate issue.. There's been a lot of changes recently that impact this PR. I'm also looking to refactor how test results are collected and logged. That'll make this feature easier to implement.\nClosing this for now. @ace-n if you want to pick this up again, please check in first to see if I've actually managed to do that refactoring \ud83d\udc7c Meanwhile thank you for putting in so much effort!. Thanks @vjpr. This is part of the work, but there's a bit more, see #1598. We won't be making Babel a peer dependency yet either.\nI'm going to close this for now, but thanks for prompting me to sort out the plan \ud83d\ude04 . > Any changes that are required in the dependencies will be released to npm when they're ready.\nLooking at https://github.com/avajs/babel-preset-stage-4/pull/11 this is tricky though, since all the v7 plugins are still released as betas. We'll probably have to install our dependencies from GitHub until Babel 7 is released officially.. @razor-x bug fixes could still be backported (they count as patch releases anyhow), but as a general rule the AVA team does not have the resources to maintain earlier versions.\nI think this would impact users who customize the Babel configuration that AVA applies to their test files. As far as the official Babel plugins are concerned the Babel 7 versions will be published under new package names, so the test configuration could use new versions whereas sources can still be compiled using Babel 6. Another workaround would be to precompile test files using Babel 6.\nI did consider supporting both versions, but as I'm doing the work on the dependencies that seems increasingly complicated.. > I'd say it's dangerous to assume code that passes tests run through Babel 7 but compiled with Babel 6 will run identically in production.\nAVA doesn't compile source code though, it only compiles test files, and by default only to deliver stage-4 syntax to older Node.js versions (plus some other enhancements under our control). I don't expect any breakage there once we finish the upgrade.. Other than helper files, AVA does not compile source files (the stuff you're testing). You have to use babel-register for that (or precompile), so this change will not affect source files.. @vjpr, @citycide, @cwonrails Please give https://github.com/avajs/ava/pull/1608 a whirl if you have a moment. Note that .babelrc.js files are not yet supported.. @citycide thanks for the heads up. I've updated the dependencies, please give it another go!. I did mean maintenance, yes.\nI'll see if I can add this to the README and other docs where appropriate.. See https://github.com/avajs/ava/issues/1043#issuecomment-339370294.. @davewasmer It's more that there are other parts of AVA's implementation that should be improved, and I personally don't want to also have to worry about how those changes impact a programmatic API. Lots of those issues are covered by the scope:scheduling label if you're interested.. Lovely, thanks @ajafff!. Any hooks need to be registered in each test file, but you could do that in a helper script and include it via the require option in the AVA's package.json configuration.\nHowever you can't currently access the test status from such hooks. We're open to that feature though, see #840.. Thanks for the research @james-s-turner! Is it in a repo somewhere we could use as a future benchmark?\nI think the relevant starting point is now #1332, though it's a little different from #789 as stated. Nothing to stop anybody from implementing #1332 in workers though.\n\nAny chance of upping the priority of #789 ?\n\nSomebody would have to pick it up \ud83d\ude09 . Thanks @james-s-turner. I'm closing this issue for now, let's continue discussing in #1332.. There is a section on Debugging in the README: https://github.com/avajs/ava#debugging\nTo be honest I've never actually used that myself, but it should work. See also https://github.com/avajs/ava/blob/master/docs/recipes/debugging-with-chrome-devtools.md.\n(I'm closing this issue, but please let me know if this helps you, or if I've misunderstood your problem.). Yea this isn't as easy at it should be. We do have #1505 open, but it looks like we should improve the documentation too.\nThis works for me:\njs\n// test/test.js\nimport test from 'ava'\ndebugger\nconsole\n$ node --interactive --inspect-brk node_modules/ava/profile.js test/test.js\nThen if I open chrome://inspect I can target the profile.js process, press Continue and it'll break at debugger.\n. I'd put the debugger in the test or source code that you actually want to debug. That way you can be sure everything is set up exactly as you need it to.. Definitely \ud83d\udc4d on doing this by default in CI.\nI suppose --check-skip could have a corresponding checkSkip configuration option, and if that's defined and false, this could override the CI default for those who still don't want the failures.\nI'm not keen on supporting configuration through environment variables. If we'd support a JS file to build the configuration though that could be implemented locally.\n. > What would a JS file do better exactly? I guess you were thinking it could do if (isRunningInGitHook) { ... } somehow?\nYes. You could make up your own env var for that, too.\nThat might be a better place to start than reasoning through which settings should have env vars and which shouldn't.. Thanks for your PR @troysandal! Those execution paths may be the cause of test failures, though. Rather than suggesting this as the default invocation, perhaps you could repeat the configurations section with the --serial flag included, and a brief explanation of why this might be useful?. Thanks @troysandal. I've made a few more changes, please let me know if you're happy with them.. @edbrannin, thanks, that's a good catch.\nt.log() currently only accepts strings. At the very least we should throw immediately when called without a string.\nExpanding support for logging non-strings makes sense too. We should use the same formatting as we do for other values (this line, basically). Would folks want to log more than one argument in that case?. @kugtong33 #1645 does not impact this.\n\nI will still write my thoughts here, on what I understood and if I have the correct context of the problem, to have feedback from you guys as I just started digging up the codes :)\n\n\ud83d\udc4d . > If we want it to behave the same as console.log we can format the arguments and append all of them on a single string before pushing it to the logs array here\n\nDoes it sound right?\n\nYes. We should use the same formatting as we do for other values (this line, basically).. Good find @Adam-Paterson1, thank you for your detailed report.\nIdeally we'd fix this upstream: https://github.com/vadimdemedes/code-excerpt/issues/1\nIf necessary we could normalize the linebreaks before feeding the source to that library.. Go for it @kugtong33! It'd be best to do a PR against the code-excerpt library. Mention me in the PR and I'll make sure Vadim sees it.. Sure, I don't particularly care about version numbers anyway.\nI think historically most releases have contained breaking changes. Most issues in the current 1.0 milestone also require breaking changes. I thus don't foresee us doing fewer releases with broking changes, but it would encourage doing more releases when smaller fixes or features land.. Thanks for raising this @razor-x. Closing, since I just released a 1.0 beta! https://github.com/avajs/ava/releases/tag/v1.0.0-beta.1. Thanks for raising this @razor-x. Closing, since I just released a 1.0 beta! https://github.com/avajs/ava/releases/tag/v1.0.0-beta.1. Thanks for the doing the research @kugtong33!\n\nIf ever the cache directory path is not absolute, the relative map path would be added instead.\n\nBest I can tell the cache dir is also absolute. We should thus be able to use mapPath as is, without making it relative.. > not sure about the failed appveyor node v4 build\nIt's flaky sometimes, don't worry about it.\nThere were a lot of changes in the package-lock. I've reverted them minus the convert-source-map version bump. Did you by chance delete the old file and regenerate it?. Thanks @kugtong33!. When I modify the config I get the following error:\n```\n/private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.Zn9QS0NO2v/webpack-explorer/js/registry.js:9\nexport default {\n^^^^^^\nSyntaxError: Unexpected token export\n```\nNote that this is not in the import test from 'ava' line. This makes sense because the ava.babel configuration is only applied to the test files. babel-register now no longer has any configuration to transpile the sources with.\nI would use this configuration instead:\njson\n  \"ava\": {\n    \"require\": \"babel-register\"\n  },\n  \"babel\": {\n    \"presets\": [\n      \"@ava/stage-4\"\n    ]\n  },\nThis way, test files are compiled using the default configuration, and babel-register takes care of transpiling sources such that they'll run in Node.js.\nI don't know why you got the error on the import test from 'ava' line. This crops up every once in a while but I've never been able to reproduce it.. Hey @willnode, I'm sorry you got stuck here.\n\nI believe that because I saw the example from this documentation.\n\nThat section only applies to AVA's compilation of test files. Indeed it starts with the words You can override the default Babel configuration AVA uses for test transpilation in package.json.\nThe section on transpiling sources introduces babel-register. Then there is another section which combines both approaches.\nDo you have suggestions on how to improve this recipe?\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). AVA does not recognize test files with .mjs extensions. See https://github.com/avajs/ava/issues/631#issuecomment-357733734. That comment still assumes use of a transpiler or @std/esm. I'm not sure how much we should change AVA to work with experimental Node.js features, though I'm open to this as long as the changes are not too intrusive.\nI imagine you can forward the --experimental-modules flag to the worker processes by running node --experimental-modules ./node_modules/.bin/ava. Perhaps you can then import .mjs source files from your tests.. @jrgleason you need to be using the latest AVA release (ava@next on npm), and configure it to recognize the mjs extension. But even then it may not work. https://github.com/avajs/ava/issues/1810 has more discussion.. @willnode I've pushed an edit, does it seem OK to you?. @sindresorhus this is useful, actually. I don't think we can rely on npm updating transitive dependencies if the currently installed version is still within the acceptable range. It might also pull an existing version from a cache. Explicitly changing the version range ensures fixes are included in the next release.. I think the trick is that npm might dedupe dependencies according to the allowed SemVer ranges. This means that it's not necessarily guaranteed you'll receive the latest version. You'll receive the most widely compatible instead.\nI've also seen this in application projects with package locks, where the best way to upgrade was to remove and then reinstall a package. Perhaps it's better with newer npm versions.\nFor us maintainers then, if there's a dependency upgrade that comes about due to a bug report, we should upgrade the minimum version so there's no circumstance under which we are linked to the buggy version.\nSimilarly our use of package locks helps ensure we're not accidentally coupled to a newer dependency feature whilst still allowing a previous version to be used. . Thanks @cdaringe!. What do you think about still running the tests in a separate worker process, but reusing that process?. It also protects the main process from any shenanigans the tests may be up to. We'd still get process concurrency.. > The use-cases are things that need to be run in the same process as the AVA CLI, for example, debugging or Electron.\nInteresting, we can start with top-level process then.\nI think rather than mimicking the rather low-level fork(), we should abstract the \"pool\" and how it communicates with the API and run-status.. > I think rather than mimicking the rather low-level fork(), we should abstract the \"pool\" and how it communicates with the API and run-status.\nI've been doing a fair bit of refactoring in this area, and have some ideas on how to make run-status better too. That should make this feature easier to implement.\n. > What improvements specifically do you have in mind?\nGetting rid of the various events that are emitted and forwarded and rewritten and reemitted, as well as the stats counting and test tracking we do all over the place.. I'd really like this to be a first class implementation, rather than a mimicking of how fork and test-worker interact. Cause I've been looking a lot at that code lately and I still can't load it all into my head.\nI have been doing and will continue to do refactoring to clear up the existing code, and hopefully that makes it relatively straight-forward to do that first class implementation. It's definitely in the back of my mind.. Hey @vadimdemedes, this is quite outdated by now. Going to close if you don't mind \ud83d\ude04 . > However, the package uses ES6 import / export syntax\nI imagine you can use it with require(), despite their documentation.\n\nWith the usage of this package, the tests work fine, except for the error stacks. Those seem to be off, because of the transpiling (?!)\n\nProbably. There isn't anything we can do about that though.. mongodb-memory-server is distributed as CJS:\nconsole\n$ npm i mongodb-memory-server\n$ node -pe \"require('mongodb-memory-server')\"\n{ MongoMemoryServer: [Function: MongoMemoryServer],\n  MongoInstance: { [Function: MongodbInstance] childProcessList: [] },\n  MongoBinary: { [Function: MongoBinary] cache: {} },\n  default: [Function: MongoMemoryServer] }. @gisderdube you're welcome! The example relies on AVA transpiling the import syntax to CJS, the result of which does work with mongodb-memory-server. But you're right that if you use require() directly you need to make further adjustments.. Would flow-node's require hook work for your use case?. I much prefer the register approach, actually. One concern I have with your suggested approach is it may break AVA's assumption that the same Node.js version is used in the main and worker processes. \nGiven that flow-node really doesn't do much other than loading the register module I don't think it adds enough value. Thank you for taking the effort to make the PR though!. Thanks @okyantoro. Would you like to try writing an integration test for this?\n\nIt'd be good to have an integration test where the default stack limit is set to say 1, and then cause an assertion failure that should have a deeper stack available.. @okyantoro These kinds of tests tend to end up in test/cli.js. See this example: https://github.com/avajs/ava/blob/bcb77fc46dbb77ddb14ac8eda8f2cc7e856c2416/test/cli.js#L78:L84\n\nThe idea is to write a fixture that is an an AVA test file. In your case it should contain an assertion that fails. Then in cli.js you can match stderr for various lines from the expected stack trace.. Thanks @okyantoro \ud83c\udf89 . Going by https://github.com/wallabyjs/public/issues/1474#issuecomment-359390677 this is an issue with Wallaby. Please let me know if this is something on AVA's end after all \ud83d\ude04 . > Broken on node 4 and 6\nasync is only supported in Node.js 8.\nI don't think it matters whether the function is truly an AsyncFunction. Instead we should check the return value of fn and if it's a promise or an observable wait for it to resolve, like we already do when you pass a promise or observable directly.. > So we need to try to execute the fn inside a try {} catch(e) {}, right?\n@okyantoro close. The way you're doing it now you're calling fn() twice if it doesn't return a promise/observable. It should only be called once.\nI think the trick is to capture fn()'s return value, and then when coreAssert.throws() itself, well, throws, see if a promise/observable was returned and run the promise handling routine (which should be extracted into a function, and you'd have to convert the observable to a promise as well).\nThis isn't great because of how we wrap coreAssert.throws(). #1047 contains the proposal that removes this dependency, but it's not a deal breaker for this feature.. > In notThrows(), the fn() is executed inside coreAssert.doesNotThrow() directly. Is it okay to wrap the fn() by using arrow function so we can check the returned value of fn()?\n@okyantoro yup.. > I think flow will not allow if we have two function signatures with subset type like:\n\n(value: () => PromiseLike, error?: ErrorValidator, message?: string): Promise;\n(value: () => mixed, error?: ErrorValidator, message?: string): any;\ncorrect me if I am wrong.\n\nI don't know @okyantoro. Don't worry about it, we can always open an issue for somebody else to refine the signature.\n\n\nDon't forget to add pass and failure cases to /test/assert.js@a051d3e#L624:L659 and /test/assert.js@master#L701:L727.\n\nisn't it already covered by these lines:\n\nThose tests are specifically about the expected pass/fail behavior, given all the ways the assertion can be used. The other tests are about their return value. They exercise the same code paths but they're a little different in intent. It's useful to see all possible behaviors in one place.. @okyantoro this is great! I pushed some minor tweaks but will merge when CI passes.\nRegarding my previous comment:\n\nThose tests are specifically about the expected pass/fail behavior, given all the ways the assertion can be used. The other tests are about their return value. They exercise the same code paths but they're a little different in intent. It's useful to see all possible behaviors in one place.\n\nTurns out I was wrong here, test/assert.js doesn't contain explicit assertions for promises and observables. These are in test/promise.js and test/observable.js. There's some legacy test organization in the way there \ud83d\ude04 I've made the necessary changes in the commit I just pushed.. @okyantoro this is great! I pushed some minor tweaks but will merge when CI passes.\nRegarding my previous comment:\n\nThose tests are specifically about the expected pass/fail behavior, given all the ways the assertion can be used. The other tests are about their return value. They exercise the same code paths but they're a little different in intent. It's useful to see all possible behaviors in one place.\n\nTurns out I was wrong here, test/assert.js doesn't contain explicit assertions for promises and observables. These are in test/promise.js and test/observable.js. There's some legacy test organization in the way there \ud83d\ude04 I've made the necessary changes in the commit I just pushed.. I'm afraid this is a known issue: #1173. There's currently no solution. As you can see in the scope:globbing issue list we have a bunch of problems in this area, it's my next priority to figure this stuff out.. @jy95 I've pushed some updates. What do you think?. Thanks @jy95!\n@forresst I think @jy95 was interested in doing a PR to https://github.com/avajs/ava-docs/blob/master/fr_FR/docs/common-pitfalls.md to reflect the changes here.. Thanks @kugtong33!. That would make the formatting inconsistent with all our other value output.. > if it was that easy then it should have been a good-for-beginner issue :D, thanks for the detailed review, will work on it\nI probably spend half an hour thinking about your code and the expected behavior so I wouldn't say it's that easy! \ud83d\ude09 . @kugtong33 if you could push your best try I'll have a look tomorrow. Don't worry if it doesn't actually work.. @kugtong33 if you could push your best try I'll have a look tomorrow. Don't worry if it doesn't actually work.. @kugtong33 I've just merged #1670 which changes the TypeScript and Flow type definitions. Currently they're hardcoded to set t.context to null in before and after hooks. Could you update them to pass Context instead? You'll have to merge the master branch into this one, and change AfterInterface<null> and BeforeInterface<null> to AfterInterface<Context> and BeforeInterface<Context> in the index.d.ts and index.js.flow files. Thanks!. @BusbyActual woohoo!\nThere's no tests, no. These are descriptions of how you would use AVA for certain use cases. I suppose we could host a repository where you can put \"playgrounds\", as it were. Not sure if it makes sense for all recipes though.. @BusbyActual the configuration needs to be cleaned up: \"babel\": \"inherit\" is now the default. I don't know if that extension hook works with Babel 7.  There's also an outdated link to the Babel recipe.. @lili21 if you could file a separate issue for this, with a small GitHub project that reproduces the problem, that'd be great.. Thanks for your hard work on this @BusbyActual!. @Jolo510 awesome, go for it!. @amslv awesome, thanks!\nSince this issue was opened, we added support for ava.config.js files. We need to make sure the arguments can be stringified as JSON and then parsed in the worker processes.\nWe could do the stringifying and the parsing in the main process first, and then use our \"deep equal\" logic to make sure the values are the same: https://github.com/avajs/ava/blob/6d12abfdff4478a1b6f4e87237764b89eb05f18d/lib/assert.js#L282. @kelvinman all yours, thanks! Let me know if you need any more detail.. @liewrichmond yea, that'd be great! The issue contains links to the various files and one dependency that needs fixing. Code may have changed a little bit since then. Let me know if you've got any questions.. @liewrichmond top of my head, in the .babelrc:\n{\n  ignore: ['test.js']\n}\nAnd then npx ava test.js should trigger this. You may need to delete node_modules/.cache if you were trying to reproduce this previously.. @okyantoro yay!\nI don't know \u2014 the spec is at http://testanything.org/tap-version-13-specification.html.\nThis may require a PR to https://www.npmjs.com/package/supertap which we use to format the TAP output.. It should work with tap-spec, tap-summary etc. Perhaps have a look at what other test runners do, e.g. node-tap.. I'm tempted to defer to node-tap, though I must admit I don't fully understand how \"subtests\" match AVA's behavior. Perhaps it's fine.\nCan you find out how the typical Node.js TAP parsers handle duration?. We can follow node-tap's lead here. We do have #324 so we can show individual assertions in the TAP output.\n\n@okyantoro could I ask you to hold off with this for now? I've started refactoring how the test workers communicate with the main process and how all that communication flows to the reporters. This new architecture should let us fix most open reporter issues and add new functionality, but it's going to conflict with this issue.\nI'll try and focus the refactor so it doesn't change functionality, so you can still work on this issue when once it lands.. I'd like to tackle https://github.com/avajs/ava/pull/1776 before changing this behavior.. Finished the TS refactor and did a similar one for Flow too. Also wrote some documentation for Flow, though still need to add the best way of stripping the type annotations. Will file a follow-up ticket for that too.. Finished the TS refactor and did a similar one for Flow too. Also wrote some documentation for Flow, though still need to add the best way of stripping the type annotations. Will file a follow-up ticket for that too.. I've documented how to strip flow type annotations and improved typings for t.throws() and t.notThrows(). TS should be fine without explicit type hints, though Flow needs some help. I've updated the documentation to reflect this.. I've documented how to strip flow type annotations and improved typings for t.throws() and t.notThrows(). TS should be fine without explicit type hints, though Flow needs some help. I've updated the documentation to reflect this.. @sindresorhus Thanks. Will make the fixes and open follow-up issues. Will sneak in TypeScript 2.7 as well.\nI'm considering revisiting the import {serial as test} / test.before() use-case in this PR. I think having accurate imports is more important than trying to \"help\". We should make serial with hooks work as expected too, and not support it before that's done.. Thanks for the detailed report @JayAndCatchFire! The bad news is that AVA isn't meant to run those tests. It should have told you though! The good news, as it were, is that you found a bug so now we can improve things. To that end, please see #1674.. Thanks for the detailed report @JayAndCatchFire! The bad news is that AVA isn't meant to run those tests. It should have told you though! The good news, as it were, is that you found a bug so now we can improve things. To that end, please see #1674.. I think t.log() is best suited for debugging test failures. You could manually prefix the log messages if you want to use it as a communication channel. I don't think AVA needs to implement full-fledged logging.\nThe logs should also be available in the TAP output if that makes it easier to process.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). I think t.log() is best suited for debugging test failures. You could manually prefix the log messages if you want to use it as a communication channel. I don't think AVA needs to implement full-fledged logging.\nThe logs should also be available in the TAP output if that makes it easier to process.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). @jamiebuilds could you elaborate on the use cases for t.warn()? Are you thinking about libraries that perform assertions for you? I don't see why you'd use this in your own test files.\nWould https://nodejs.org/api/util.html#util_util_deprecate_fn_msg_code be sufficient for a deprecation scenario?. Yea it should have complained about those arguments. Yet another reason we need to refactor it: #1047.. Yea it should have complained about those arguments. Yet another reason we need to refactor it: #1047.. I think we'll have to leave null as an allowable value. It's impossible to specify an assertion message otherwise:\njs\nt.throws(fn, null, 'shown when assertion fails')\nBut as part of #1047 I'll be introducing a more explicit way of defining constructor expectations.. I think we'll have to leave null as an allowable value. It's impossible to specify an assertion message otherwise:\njs\nt.throws(fn, null, 'shown when assertion fails')\nBut as part of #1047 I'll be introducing a more explicit way of defining constructor expectations.. This is by design. Tests run concurrently. b is started regardless of whether a finishes. Starting b means its beforeEach hook is also run.\nUse test.serial() to ensure tests run serially.. @jviotti this is a very interesting use case. Typically our response is that you should wrap the invocation of AVA in some other script.\nYou're right that stdin forwarding would work in --serial mode, but it wouldn't work in watch mode. There's just too many ways in which users may expect to use AVA that your proposal is not compatible with.\nNote that you could write a wrapper script that uses Inquirer. If you then load ava/cli it should behave the same as if you invoked ava directly.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). > Inquirer can easily be mocked, which would be a better way to test it. Alternatively, spawn a separate file and check the output.\nThe issue seems to be that test success depends on hardware state. It's not that Inquirer can't be mocked, it's that @jviotti requires actual user input before the tests continue.. @jviotti ah apologies, I overlooked that you wanted the user to confirm whether the test passed. That is trickier to do with wrapper scripts. Regardless I think it's not a use case we should support with AVA.\nAn outline of how you could make this work:\n\nWrite a companion script that starts a server (either HTTP or unix domain socket)\nRun the tests\nIn the test, send a request to the companion server\nIn the companion server, ask for user input\nSend the input back to the test and run an assertion\n. @ianwalter the work in https://github.com/avajs/ava/pull/2011 would let you run tests in the main process, and there you'd have access to stdin. Would be great to see some experiments with that before adding any additional features to AVA itself.\n\nI can't give you a timeline on when that's ready (mostly because I'm busy with work and it's a large PR) but perhaps you could give the in-progress branch a go?. @jviotti thanks for digging into the code base to figure out how this could be implemented. I'm closing this  given my issue comment https://github.com/avajs/ava/issues/1682#issuecomment-363746586.. This has caused an issue in the past: #1236.. @kugtong33 that'd be great, thanks!. > How does ifError differs from falsy if both checks the value then fails it when value is truthy?\nI know\u2026\u00a0I've just opened #1720.\n. This is better discussed in eslint-plugin-ava. Looks like you ran into this before @niftylettuce \ud83d\ude09 https://github.com/avajs/eslint-plugin-ava/issues/180#issuecomment-363087470. @niftylettuce could you expand on the use case? What kind of code would you run before and after a test suite?. @niftylettuce which AVA version are you using?. This is fully supported in the 1.0 betas.. @rinu it sounds like you're using spread syntax in your source files? AVA does not compile sources out of the box, so if you're running on a Node.js version that doesn't support this syntax then it won't work.\nIt does work in test files, because we do compile those.. @rinu it sounds like you're using spread syntax in your source files? AVA does not compile sources out of the box, so if you're running on a Node.js version that doesn't support this syntax then it won't work.\nIt does work in test files, because we do compile those.. @ToniChaz given that the error is coming from babylon this seems to be a problem with your precompiler setup, not AVA.. @danr interesting!\nAVA doesn't let you create new tests once tests have started running. I can't quite tell how testcheck-js gets around this. But as you say, this is about using AVA's assertion library. Perhaps we can support creating a new ExecutionContext (the t argument) that can be passed to the user's test implementation.\nTaking this example:\n```js\nconst test = require('ava')\nconst { check, gen } = require('ava-check')\ntest('addition is commutative', check(gen.int, gen.int, (t, numA, numB) => {\n  t.true(numA + numB === numB + numA)\n}));\n```\nWhat if check() could do (in pseudocode):\njs\nfunction check(genA, genB, implementation) {\n  return async t => {\n    while (true) {\n      const result = await t.try(implementation, genA(), genB())\n      if (result.passed || cannotMinimizeFurther()) {\n        return result.commit()\n      }\n    }\n  }\n}\nWith a rough type definition of:\n```ts\ninterface ExecutionContext {\n  try (implementation: (t: ExecutionContext, ...args: any[]) => void, ...args: any[]): Promise\n}\ninterface TryResult {\n  passed: boolean\n  assertionErrors: AssertionError[]\n  commit (): boolean\n}\n```\nAnd in actual English: t.try() can be used to invoke a test implementation. It returns a promise with the result of the invocation. If passed is true then no assertions failed. Otherwise assertionErrors is populated with one or more errors. The usual rules around requiring assertions and awaiting asynchronous ones apply. commit() either passes the test or fails it with the assertionErrors. Not calling commit() results in a test failure unless another assertion is used.\nI like how this is low-level, which means we can support interesting higher-order tests.\n@avajs/core what do you think?. > Your try improves my observeTest but I would simplify the type to:\nPerhaps. We have a pattern of supporting additional arguments that get passed to the implementation, seems nice to retain that.\n\n(A name that is not a javascript keyword must be used instead of try)\n\nIt should work as a property.\n\nI am a bit skeptical about this behaviour:\n\nNot calling commit() results in a test failure unless another assertion is used.\n\nIs this not overly restrictive? There should be no particular reason to make this a test failure by default.\n\nThat's how AVA behaves out of the box. Tests must use assertions. IMHO not committing a t.try is akin to not actually using assertions.. @sindresorhus what do you think about my proposal?. @sindresorhus what do you think about my proposal?. Expanding on my earlier comment https://github.com/avajs/ava/issues/1692#issuecomment-363750906 there's the question of what to do with t.context.\nWe do support t.context being created in .before() hooks. We then shallowly clone it for use by each test. We're hoping this strikes a balance between letting tests (or .beforeEach() hooks) make modifications without impacting other tests.\nPerhaps we should do the same when passing the execution context to the t.try() implementation? Regardless I think we can agree that assigning t.context should not affect the \"parent\".\nShould we allow recursively calling t.try()? I suppose ideally there is no difference between t.try() implementations and test implementations. This might just be how it shakes out anyhow \u2014 so let's assume we do allow this and reconsider if it proves difficult.\nDoes t.try() count as one assertion for t.plan() or is it transparent? Conversely can t.try() implementations use t.plan()? I'm leaning towards it being transparent, which means that implementations mustn't be able to call t.plan(), and even when the result is committed the implementation still must have at least one passing assertion.\nOrdinarily assertions are implemented in lib/assert.js but I don't think try() can do its work there. It should be added to ExecutionContext in lib/test.js instead. \nTest needs to support a new assertion type, along the lines of addPendingAssertion. pendingAssertionCount should be incremented so the test fails if the promise returned by t.try() is not awaited. We need to instantiate a new ExecutionContext, but we can't pass it the Test instance. Perhaps inside the ExecutionContext we should rename \"test\" to \"host\". That way t.try() can create a new host object. There could be a Host class which is extended by Test and which implements the assertion counting and snapshot-supporting methods.\nThis is the point where my architecting runs into \"this should really be played with in code\" so I'm going to stop. @danr let me know if you want to take this on, happy to review code and give pointers!. > Does t.try() count as one assertion for t.plan() or is it transparent? Conversely can t.try() implementations use t.plan()? I'm leaning towards it being transparent, which means that implementations mustn't be able to call t.plan(), and even when the result is committed the implementation still must have at least one passing assertion.\nI've thought about this some more and I think it's wrong. Potentially a third-party module is used which then calls a user-provided implementation. Users may want to plan the number of assertions in their implementation. The third-party module wouldn't be able to do this.\nInstead we should not allow t.try() when t.plan() is used, but allow t.plan() inside an implementation.. We should allow the test result to be discarded, perhaps without even waiting for the promise returned by t.try() to be fulfilled. Perhaps by adding a discard() method on the promise. This would allow calling code to manage timeouts, see https://github.com/avajs/ava/issues/1918#issuecomment-416196601.. Additionally:\nFixes #1684. Fixes #1416. Refs #1158.\nProperly support serial hooks. Hooks are divided into the following categories:\n\nbefore\nbeforeEach\nafterEach\nafterEach.always\nafter\nafter.always\n\nFor each category all hooks are run in the order they're declared in. This is different from tests, where serial tests are run before concurrent ones.\nBy default hooks run concurrently. However a serial hook is not run before all preceding concurrent hooks have completed. Serial hooks are never run concurrently.\nAlways hooks are now always run, even if --fail-fast is enabled.\nInternally, TestCollection, Sequence and Concurrent have been removed. This has led to a major refactor of Runner, and some smaller breaking changes and bug fixes:\n\nUnnecessary validations have been removed\nMacros can be used with hooks\nA macro is recognized even if no additional arguments are given, so it can modify the test (or hook) title\n--match now also matches todo tests\nSkipped and todo tests are shown first in the output\n--fail-fast prevents subsequent tests from running as long as the failure occurs in a serial test. And finally, if a failure occurs in one worker, attempt to interrupt other workers. This only works as long as the other worker has not yet started running concurrent tests. Fixes #1158.. @BusbyActual I think they have a beta website for Babel 7 somewhere. You'll want to be looking at that one, the current site still covers Babel 6.. Thanks for the update @BusbyActual. I was looking at the new version of this recipe and it just seemed too complicated to me. Lots has changed since it was first written. I've done a PR to simplify it instead: https://github.com/avajs/ava/pull/1710.\n\nWhat do you think?. > Do we want to integrate the babel changes into the new docs instead?\nI'm not sure it's needed. I mean ideally the nyc documentation gets updated but we already have documentation on how to configure Babel, use @babel/register, etc.\nI'm going to close this in favor of #1710. Hopefully you'll do more PRs for the other recipes! And if the recipes seem overly complicated please feel free to raise that, rather than converting them.. Rather than using beforeEach hooks to set up your context, call a function. Then you could move the loop inside your test implementation and create a clean context for each iteration.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). You could assign the beforeEach implementation to a variable. Use it with test.beforeEach and then repeatedly call it inside your test.. > Fix t.snapshot.skip(). Ensure subsequent t.snapshot() calls compare the correct snapshot. Forbid snapshots from being skipped while updating them. Fixes #1425.\nSee https://github.com/avajs/ava/issues/1425#issuecomment-364766611 though.. Thanks @mdvorscak!. Thank you @forresst \ud83e\udd47 . Thank you @forresst \ud83e\udd47 . Fantastic as always @forresst.. Fantastic as always @forresst.. > I'm concerned that chalk / supports-color will behave inconsistently in the test processes.\nI got thrown off by chalk's odd constructor access pattern. The trick is for AVA to use its own chalk instance and to configure it in the main process based on the package.json options and CLI flags, and in the worker processes based on what it was told by the main process.\nI'm refactoring a lot of code at the moment and Chalk's instantiation came up, so I'm tackling that part now.\nOnce that lands we should support the color level option in the package.json and forward it to the worker processes.. See #1722.. With #1722, AVA now uses its own chalk instance. It's loaded quite early on in lib/worker/subprocess.js. Right now it processes the --color / --no-color argument passed by lib/fork.js.\n\nInstead, lib/fork.js should stop passing those arguments. We remove them anyway.\nlib/worker/subprocess.js should require lib/worker/consume-argv before lib/worker/load-chalk.\nload-chalk.js should access the color option through requiring lib/worker/options and use it to instantiate chalk.\nconsume-argv.js should only remove process.argv[2], not also process.argv[3].\n\nAdditionally (though this could be done as a follow-up), we should expand the --color CLI flag and package.json#ava.color to support the 256 and 16m string values from support-color.\n. This is ready to be worked on again, if anybody wants to pick it up \ud83d\ude00 . The expectation object works now.\nThe various examples run to hundreds of lines, so please expand the details below:\n\nthrows.js\n\n```js\nimport test from 'ava';\nimport Observable from 'zen-observable';\n\ntest.serial('fails, not a valid thrower', t => {\n  t.throws(true);\n});\n\ntest.serial('fails, not a valid expectation', t => {\n  t.throws(() => {}, true);\n});\n\ntest.serial('fails, not a valid expectation (of)', t => {\n  t.throws(() => {}, {of: true});\n});\n\ntest.serial('fails, not a valid expectation (message)', t => {\n  t.throws(() => {}, {message: true});\n});\n\ntest.serial('fails, not a valid expectation (name)', t => {\n  t.throws(() => {}, {name: true});\n});\n\ntest.serial('fails, not a valid expectation (unknown)', t => {\n  t.throws(() => {}, {unknown: true});\n});\n\ntest.serial('fails, does not throw', t => {\n  t.throws(() => true);\n});\n\ntest.serial('fails, not an error', t => {\n  t.throws(() => {\n    throw {foo: 'bar'};\n  });\n});\n\ntest.serial('fails, wrong instance', t => {\n  const err = new Error();\n  t.throws(() => {\n    throw new Error();\n  }, {is: err});\n});\n\ntest.serial('fails, wrong name', t => {\n  t.throws(() => {\n    throw new Error();\n  }, {name: 'TypeError'});\n});\n\ntest.serial('fails, wrong message (string)', t => {\n  t.throws(() => {\n    throw new Error('foo');\n  }, {message: 'bar'});\n});\n\ntest.serial('fails, wrong message (regexp)', t => {\n  t.throws(() => {\n    throw new Error('foo');\n  }, {message: /bar/});\n});\n\ntest.serial('fails, does not reject', async t => {\n  await t.throws(Promise.resolve(true));\n});\n\ntest.serial('fails, does not error', async t => {\n  await t.throws(Observable.of(true));\n});\n\ntest.serial('fails, returned promise does not reject', async t => {\n  await t.throws(() => Promise.resolve(true));\n});\n\ntest.serial('fails, returned observable does not error', async t => {\n  await t.throws(() => Observable.of(true));\n});\n\ntest.serial('fails, returned promise does not reject with an error', async t => {\n  await t.throws(() => Promise.reject(true));\n});\n\ntest.serial('fails, returned observable does not error with an error', async t => {\n  await t.throws(() => new Observable(observer => observer.error(true)));\n});\n```\n\n\noutput\n\n```console\n$ npx ava throws.js -v\n\n  \u2716 fails, not a valid thrower `t.throws()` must be called with a function, observable or promise\n  \u2716 fails, not a valid expectation The second argument to `t.throws()` must be a function, string, regular expression, expectation object or `null`\n  \u2716 fails, not a valid expectation (of) The `of` property of the second argument to `t.throws()` must be a function\n  \u2716 fails, not a valid expectation (message) The `message` property of the second argument to `t.throws()` must be a string or regular expression\n  \u2716 fails, not a valid expectation (name) The `name` property of the second argument to `t.throws()` must be a string\n  \u2716 fails, not a valid expectation (unknown) The second argument to `t.throws()` contains unexpected properties\n  \u2716 fails, does not throw \n  \u2716 fails, not an error \n  \u2716 fails, wrong instance \n  \u2716 fails, wrong name \n  \u2716 fails, wrong message (string) \n  \u2716 fails, wrong message (regexp) \n  \u2716 fails, does not reject \n  \u2716 fails, does not error \n  \u2716 fails, returned promise does not reject \n  \u2716 fails, returned observable does not error \n  \u2716 fails, returned promise does not reject with an error \n  \u2716 fails, returned observable does not error with an error \n\n  18 tests failed\n\n  fails, not a valid thrower\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:5\n\n   4: test.serial('fails, not a valid thrower', t => {\n   5:   t.throws(true);                               \n   6: });                                             \n\n  `t.throws()` must be called with a function, observable or promise\n\n  Called with:\n\n  true\n\n  Try wrapping the first argument to `t.throws()` in a function:\n\n    t.throws(() => { /* your code here */ })\n\n  Visit the following URL for more details:\n\n    https://github.com/avajs/ava#throwsfunctionpromise-error-message\n\n\n\n  fails, not a valid expectation\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:9\n\n   8: test.serial('fails, not a valid expectation', t => {\n   9:   t.throws(() => {}, true);                         \n   10: });                                                 \n\n  The second argument to `t.throws()` must be a function, string, regular expression, expectation object or `null`\n\n  Called with:\n\n  true\n\n\n\n  fails, not a valid expectation (of)\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:13\n\n   12: test.serial('fails, not a valid expectation (of)', t => {\n   13:   t.throws(() => {}, {of: true});                        \n   14: });                                                      \n\n  The `of` property of the second argument to `t.throws()` must be a function\n\n  Called with:\n\n  {\n    of: true,\n  }\n\n\n\n  fails, not a valid expectation (message)\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:17\n\n   16: test.serial('fails, not a valid expectation (message)', t => {\n   17:   t.throws(() => {}, {message: true});                        \n   18: });                                                           \n\n  The `message` property of the second argument to `t.throws()` must be a string or regular expression\n\n  Called with:\n\n  {\n    message: true,\n  }\n\n\n\n  fails, not a valid expectation (name)\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:21\n\n   20: test.serial('fails, not a valid expectation (name)', t => {\n   21:   t.throws(() => {}, {name: true});                        \n   22: });                                                        \n\n  The `name` property of the second argument to `t.throws()` must be a string\n\n  Called with:\n\n  {\n    name: true,\n  }\n\n\n\n  fails, not a valid expectation (unknown)\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:25\n\n   24: test.serial('fails, not a valid expectation (unknown)', t => {\n   25:   t.throws(() => {}, {unknown: true});                        \n   26: });                                                           \n\n  The second argument to `t.throws()` contains unexpected properties\n\n  Called with:\n\n  {\n    unknown: true,\n  }\n\n\n\n  fails, does not throw\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:29\n\n   28: test.serial('fails, does not throw', t => {\n   29:   t.throws(() => true);                    \n   30: });                                        \n\n  Function returned:\n\n  true\n\n\n\n  fails, not an error\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:33\n\n   32: test.serial('fails, not an error', t => {\n   33:   t.throws(() => {                       \n   34:     throw {foo: 'bar'};                  \n\n  Function threw exception that is not an error:\n\n  {\n    foo: 'bar',\n  }\n\n\n\n  fails, wrong instance\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:40\n\n   39:   const err = new Error();\n   40:   t.throws(() => {        \n   41:     throw new Error();    \n\n  Function threw unexpected exception:\n\n  Error {\n    message: '',\n  }\n\n  Expected to be strictly equal to:\n\n  Error {\n    message: '',\n  }\n\n\n\n  fails, wrong name\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:46\n\n   45: test.serial('fails, wrong name', t => {\n   46:   t.throws(() => {                     \n   47:     throw new Error();                 \n\n  Function threw unexpected exception:\n\n  Error {\n    message: '',\n  }\n\n  Expected name to equal:\n\n  'TypeError'\n\n\n\n  fails, wrong message (string)\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:52\n\n   51: test.serial('fails, wrong message (string)', t => {\n   52:   t.throws(() => {                                 \n   53:     throw new Error('foo');                        \n\n  Function threw unexpected exception:\n\n  Error {\n    message: 'foo',\n  }\n\n  Expected message to equal:\n\n  'bar'\n\n\n\n  fails, wrong message (regexp)\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:58\n\n   57: test.serial('fails, wrong message (regexp)', t => {\n   58:   t.throws(() => {                                 \n   59:     throw new Error('foo');                        \n\n  Function threw unexpected exception:\n\n  Error {\n    message: 'foo',\n  }\n\n  Expected message to match:\n\n  /bar/\n\n\n\n  fails, does not reject\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:64\n\n   63: test.serial('fails, does not reject', async t => {\n   64:   await t.throws(Promise.resolve(true));          \n   65: });                                               \n\n  Promise resolved with:\n\n  true\n\n\n\n  fails, does not error\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:68\n\n   67: test.serial('fails, does not error', async t => {\n   68:   await t.throws(Observable.of(true));           \n   69: });                                              \n\n  Observable completed with:\n\n  [\n    true,\n  ]\n\n\n\n  fails, returned promise does not reject\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:72\n\n   71: test.serial('fails, returned promise does not reject', async t => {\n   72:   await t.throws(() => Promise.resolve(true));                     \n   73: });                                                                \n\n  Returned promise resolved with:\n\n  true\n\n\n\n  fails, returned observable does not error\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:76\n\n   75: test.serial('fails, returned observable does not error', async t => {\n   76:   await t.throws(() => Observable.of(true));                         \n   77: });                                                                  \n\n  Returned observable completed with:\n\n  [\n    true,\n  ]\n\n\n\n  fails, returned promise does not reject with an error\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:80\n\n   79: test.serial('fails, returned promise does not reject with an error', asy\u2026\n   80:   await t.throws(() => Promise.reject(true));                            \n   81: });                                                                      \n\n  Returned promise rejected with exception that is not an error:\n\n  true\n\n\n\n  fails, returned observable does not error with an error\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:84\n\n   83: test.serial('fails, returned observable does not error with an error', a\u2026\n   84:   await t.throws(() => new Observable(observer => observer.error(true)));\n   85: });                                                                      \n\n  Returned observable errored with exception that is not an error:\n\n  true\n\n```\n\n. The expectation object works now.\nThe various examples run to hundreds of lines, so please expand the details below:\n\nthrows.js\n\n```js\nimport test from 'ava';\nimport Observable from 'zen-observable';\n\ntest.serial('fails, not a valid thrower', t => {\n  t.throws(true);\n});\n\ntest.serial('fails, not a valid expectation', t => {\n  t.throws(() => {}, true);\n});\n\ntest.serial('fails, not a valid expectation (of)', t => {\n  t.throws(() => {}, {of: true});\n});\n\ntest.serial('fails, not a valid expectation (message)', t => {\n  t.throws(() => {}, {message: true});\n});\n\ntest.serial('fails, not a valid expectation (name)', t => {\n  t.throws(() => {}, {name: true});\n});\n\ntest.serial('fails, not a valid expectation (unknown)', t => {\n  t.throws(() => {}, {unknown: true});\n});\n\ntest.serial('fails, does not throw', t => {\n  t.throws(() => true);\n});\n\ntest.serial('fails, not an error', t => {\n  t.throws(() => {\n    throw {foo: 'bar'};\n  });\n});\n\ntest.serial('fails, wrong instance', t => {\n  const err = new Error();\n  t.throws(() => {\n    throw new Error();\n  }, {is: err});\n});\n\ntest.serial('fails, wrong name', t => {\n  t.throws(() => {\n    throw new Error();\n  }, {name: 'TypeError'});\n});\n\ntest.serial('fails, wrong message (string)', t => {\n  t.throws(() => {\n    throw new Error('foo');\n  }, {message: 'bar'});\n});\n\ntest.serial('fails, wrong message (regexp)', t => {\n  t.throws(() => {\n    throw new Error('foo');\n  }, {message: /bar/});\n});\n\ntest.serial('fails, does not reject', async t => {\n  await t.throws(Promise.resolve(true));\n});\n\ntest.serial('fails, does not error', async t => {\n  await t.throws(Observable.of(true));\n});\n\ntest.serial('fails, returned promise does not reject', async t => {\n  await t.throws(() => Promise.resolve(true));\n});\n\ntest.serial('fails, returned observable does not error', async t => {\n  await t.throws(() => Observable.of(true));\n});\n\ntest.serial('fails, returned promise does not reject with an error', async t => {\n  await t.throws(() => Promise.reject(true));\n});\n\ntest.serial('fails, returned observable does not error with an error', async t => {\n  await t.throws(() => new Observable(observer => observer.error(true)));\n});\n```\n\n\noutput\n\n```console\n$ npx ava throws.js -v\n\n  \u2716 fails, not a valid thrower `t.throws()` must be called with a function, observable or promise\n  \u2716 fails, not a valid expectation The second argument to `t.throws()` must be a function, string, regular expression, expectation object or `null`\n  \u2716 fails, not a valid expectation (of) The `of` property of the second argument to `t.throws()` must be a function\n  \u2716 fails, not a valid expectation (message) The `message` property of the second argument to `t.throws()` must be a string or regular expression\n  \u2716 fails, not a valid expectation (name) The `name` property of the second argument to `t.throws()` must be a string\n  \u2716 fails, not a valid expectation (unknown) The second argument to `t.throws()` contains unexpected properties\n  \u2716 fails, does not throw \n  \u2716 fails, not an error \n  \u2716 fails, wrong instance \n  \u2716 fails, wrong name \n  \u2716 fails, wrong message (string) \n  \u2716 fails, wrong message (regexp) \n  \u2716 fails, does not reject \n  \u2716 fails, does not error \n  \u2716 fails, returned promise does not reject \n  \u2716 fails, returned observable does not error \n  \u2716 fails, returned promise does not reject with an error \n  \u2716 fails, returned observable does not error with an error \n\n  18 tests failed\n\n  fails, not a valid thrower\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:5\n\n   4: test.serial('fails, not a valid thrower', t => {\n   5:   t.throws(true);                               \n   6: });                                             \n\n  `t.throws()` must be called with a function, observable or promise\n\n  Called with:\n\n  true\n\n  Try wrapping the first argument to `t.throws()` in a function:\n\n    t.throws(() => { /* your code here */ })\n\n  Visit the following URL for more details:\n\n    https://github.com/avajs/ava#throwsfunctionpromise-error-message\n\n\n\n  fails, not a valid expectation\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:9\n\n   8: test.serial('fails, not a valid expectation', t => {\n   9:   t.throws(() => {}, true);                         \n   10: });                                                 \n\n  The second argument to `t.throws()` must be a function, string, regular expression, expectation object or `null`\n\n  Called with:\n\n  true\n\n\n\n  fails, not a valid expectation (of)\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:13\n\n   12: test.serial('fails, not a valid expectation (of)', t => {\n   13:   t.throws(() => {}, {of: true});                        \n   14: });                                                      \n\n  The `of` property of the second argument to `t.throws()` must be a function\n\n  Called with:\n\n  {\n    of: true,\n  }\n\n\n\n  fails, not a valid expectation (message)\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:17\n\n   16: test.serial('fails, not a valid expectation (message)', t => {\n   17:   t.throws(() => {}, {message: true});                        \n   18: });                                                           \n\n  The `message` property of the second argument to `t.throws()` must be a string or regular expression\n\n  Called with:\n\n  {\n    message: true,\n  }\n\n\n\n  fails, not a valid expectation (name)\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:21\n\n   20: test.serial('fails, not a valid expectation (name)', t => {\n   21:   t.throws(() => {}, {name: true});                        \n   22: });                                                        \n\n  The `name` property of the second argument to `t.throws()` must be a string\n\n  Called with:\n\n  {\n    name: true,\n  }\n\n\n\n  fails, not a valid expectation (unknown)\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:25\n\n   24: test.serial('fails, not a valid expectation (unknown)', t => {\n   25:   t.throws(() => {}, {unknown: true});                        \n   26: });                                                           \n\n  The second argument to `t.throws()` contains unexpected properties\n\n  Called with:\n\n  {\n    unknown: true,\n  }\n\n\n\n  fails, does not throw\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:29\n\n   28: test.serial('fails, does not throw', t => {\n   29:   t.throws(() => true);                    \n   30: });                                        \n\n  Function returned:\n\n  true\n\n\n\n  fails, not an error\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:33\n\n   32: test.serial('fails, not an error', t => {\n   33:   t.throws(() => {                       \n   34:     throw {foo: 'bar'};                  \n\n  Function threw exception that is not an error:\n\n  {\n    foo: 'bar',\n  }\n\n\n\n  fails, wrong instance\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:40\n\n   39:   const err = new Error();\n   40:   t.throws(() => {        \n   41:     throw new Error();    \n\n  Function threw unexpected exception:\n\n  Error {\n    message: '',\n  }\n\n  Expected to be strictly equal to:\n\n  Error {\n    message: '',\n  }\n\n\n\n  fails, wrong name\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:46\n\n   45: test.serial('fails, wrong name', t => {\n   46:   t.throws(() => {                     \n   47:     throw new Error();                 \n\n  Function threw unexpected exception:\n\n  Error {\n    message: '',\n  }\n\n  Expected name to equal:\n\n  'TypeError'\n\n\n\n  fails, wrong message (string)\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:52\n\n   51: test.serial('fails, wrong message (string)', t => {\n   52:   t.throws(() => {                                 \n   53:     throw new Error('foo');                        \n\n  Function threw unexpected exception:\n\n  Error {\n    message: 'foo',\n  }\n\n  Expected message to equal:\n\n  'bar'\n\n\n\n  fails, wrong message (regexp)\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:58\n\n   57: test.serial('fails, wrong message (regexp)', t => {\n   58:   t.throws(() => {                                 \n   59:     throw new Error('foo');                        \n\n  Function threw unexpected exception:\n\n  Error {\n    message: 'foo',\n  }\n\n  Expected message to match:\n\n  /bar/\n\n\n\n  fails, does not reject\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:64\n\n   63: test.serial('fails, does not reject', async t => {\n   64:   await t.throws(Promise.resolve(true));          \n   65: });                                               \n\n  Promise resolved with:\n\n  true\n\n\n\n  fails, does not error\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:68\n\n   67: test.serial('fails, does not error', async t => {\n   68:   await t.throws(Observable.of(true));           \n   69: });                                              \n\n  Observable completed with:\n\n  [\n    true,\n  ]\n\n\n\n  fails, returned promise does not reject\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:72\n\n   71: test.serial('fails, returned promise does not reject', async t => {\n   72:   await t.throws(() => Promise.resolve(true));                     \n   73: });                                                                \n\n  Returned promise resolved with:\n\n  true\n\n\n\n  fails, returned observable does not error\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:76\n\n   75: test.serial('fails, returned observable does not error', async t => {\n   76:   await t.throws(() => Observable.of(true));                         \n   77: });                                                                  \n\n  Returned observable completed with:\n\n  [\n    true,\n  ]\n\n\n\n  fails, returned promise does not reject with an error\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:80\n\n   79: test.serial('fails, returned promise does not reject with an error', asy\u2026\n   80:   await t.throws(() => Promise.reject(true));                            \n   81: });                                                                      \n\n  Returned promise rejected with exception that is not an error:\n\n  true\n\n\n\n  fails, returned observable does not error with an error\n\n  /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.toBbmRf1AV/throws.js:84\n\n   83: test.serial('fails, returned observable does not error with an error', a\u2026\n   84:   await t.throws(() => new Observable(observer => observer.error(true)));\n   85: });                                                                      \n\n  Returned observable errored with exception that is not an error:\n\n  true\n\n```\n\n. > From the original issue:\n\n\nWe should also have much better validation logic to prevent mistakes. For example, using is-error-constructor on the constructor matcher.\n\nCould you do this?\n\nI'm not sure it's necessary. We always check that the value is an error, so if it's not then you'd never get to the instanceof check. is-error checks the string tag, which works across realms, whereas is-error-constructor does not, so that wouldn't be consistent either.\n\n\nt.throws(fn, {of: SyntaxError}) // err instanceof SyntaxError\n\nI don't think of is clear enough. Maybe name it instanceOf or constructor?\n\nI like the brevity of of but I've also had some doubts about it. instanceOf seems most appropriate, though the capital O is cumbersome to type. constructor would work but it doesn't quite signal how we run the assertion. instanceOf then?\n\nWhat do you think about dropping support for t.throws(() => {}, TypeError);. It's not that commonly used and can now be done with t.throws(() => {}, {of: TypeError}); instead, which is only slightly longer. Just checking the type of the error is usually an anti-pattern, as it's too generic of a check.\n\nThat depends how tightly you want to couple your test to any current error message. E.g. when validating input type checks, looking for TypeError can be sufficient. Similarly if you're testing an API with custom errors then often the error class is the relevant bit and the message may be static.\n{instanceOf: TypeError} is a fair bit longer, too.. > From the original issue:\n\n\nWe should also have much better validation logic to prevent mistakes. For example, using is-error-constructor on the constructor matcher.\n\nCould you do this?\n\nI'm not sure it's necessary. We always check that the value is an error, so if it's not then you'd never get to the instanceof check. is-error checks the string tag, which works across realms, whereas is-error-constructor does not, so that wouldn't be consistent either.\n\n\nt.throws(fn, {of: SyntaxError}) // err instanceof SyntaxError\n\nI don't think of is clear enough. Maybe name it instanceOf or constructor?\n\nI like the brevity of of but I've also had some doubts about it. instanceOf seems most appropriate, though the capital O is cumbersome to type. constructor would work but it doesn't quite signal how we run the assertion. instanceOf then?\n\nWhat do you think about dropping support for t.throws(() => {}, TypeError);. It's not that commonly used and can now be done with t.throws(() => {}, {of: TypeError}); instead, which is only slightly longer. Just checking the type of the error is usually an anti-pattern, as it's too generic of a check.\n\nThat depends how tightly you want to couple your test to any current error message. E.g. when validating input type checks, looking for TypeError can be sufficient. Similarly if you're testing an API with custom errors then often the error class is the relevant bit and the message may be static.\n{instanceOf: TypeError} is a fair bit longer, too.. @sindresorhus addressed feedback.. @sindresorhus addressed feedback.. @shellscape we don't have consensus in the Core team to change this behavior.\nWould it be possible though to inherit CssSyntaxError from Error?. It seems to be working with ava@next and Babel 7: https://github.com/billyjanitsch/ava-esm/pull/1.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). It seems to be working with ava@next and Babel 7: https://github.com/billyjanitsch/ava-esm/pull/1.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). > if I understand your PR correctly, {\"babel\": \"inherit\"} is becoming the default behavior?\n@billyjanitsch yep.\n\nCan this be added to the @std/esm cookbook example. I can see folks reaching for something like this again.\n\n@jdalton good idea, will do.. > Can this be added to the @std/esm cookbook example. I can see folks reaching for something like this again.\n@jdalton where is this cookbook?. @jdalton I think that's up to date? It links to the Babel recipe for disabling AVA's default ESM compilation too. Let me know if you think it needs more changes.. @jdalton are you referring to https://github.com/billyjanitsch/ava-esm/pull/1? That just fixes the Babel setup for Babel 7. With that setup AVA is still compiling ESM to CJS, though that doesn't impact @std/esm.. @billyjanitsch it merges, yes.. > it's surprising to me that an ESM->CJS transform is among the transforms automatically applied by AVA if the user has explicitly set something like [\"@babel/preset-env\", {\"modules\": false}] in their config\nThat sounds nice, but I really wouldn't know how to do that reliably. I think it's better to communicate what AVA does and how to disable certain things if necessary. Let me know if you have suggestions for our Babel recipe in that regard.\n\nAlso, isn't the point of adding {\"require\": \"@std/esm\"} to an AVA config to use std/esm for test files?\n\nI've assumed it's so you can use ESM in your source files and require them directly from your tests, without having to precompile them. I'm not sure about the difference in semantics between AVA's ESM->CJS transform and @std/esm, but nuances aside you shoudn't need @std/esm to load your test files.\n. The documentation is for a newer AVA version. Since you're using v0.22 you'll want to be looking here: https://github.com/avajs/ava/tree/v0.22.0.\nThis feature is available as of 1.0.0-beta.1, which you can install through npm install ava@next. Here's the release notes: https://github.com/avajs/ava/releases/tag/v1.0.0-beta.1.. The documentation is for a newer AVA version. Since you're using v0.22 you'll want to be looking here: https://github.com/avajs/ava/tree/v0.22.0.\nThis feature is available as of 1.0.0-beta.1, which you can install through npm install ava@next. Here's the release notes: https://github.com/avajs/ava/releases/tag/v1.0.0-beta.1.. @leegee after Babel 7 does. But if your project doesn't use Babel itself, or is already using Babel 7, then you should use AVA's beta releases.. @kugtong33 looks like I beat you to it, sorry. Wanted to get this in so I can do a new release. Let me know if there's anything you'd like to see changed in my version though.. @kugtong33 a recipe would be good. It's too much to capture in the README itself.. Thanks @jamiebuilds. What do you think about bundling chalk and supports-color with AVA? That's the contention in #1701 that still needs to be resolved. (Please respond there instead.). We should make some more changes, see https://github.com/avajs/ava/issues/1701#issuecomment-377975554. I still need to land #1722 though.. Closing due to inactivity. There's been changes in how AVA uses chalk, so we need a slightly different approach. See https://github.com/avajs/ava/issues/1701#issuecomment-377975554.. This is intentional. We want to show these otherwise invisible characters. In fact we'd like to show more.\nDo you find this behavior surprising? Did you think it impacts the test outcome?\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). By \"weird star\" do you mean \u235f? I agree it's a bit obscure \u2014 I think it's only used with React elements. The intent is to show the difference between built-in types (they end up being strings) and function types that could have the same name.. I don't think there's a difference between assigning to an explicit type and casting with as. The documented code samples compile for me.\nCould you share more of your actual code? Perhaps there's something else awry.. @samuelli right you are. I've added a regression test, and we're now running TypeScript in strict mode. See my commits. Does it look good to you?. > Could AVA only output some error to tell a user that a certain test suite was still running instead of waiting for all the I/O to finish? If all the tests completed, but something is still running, that test suite should be considered failing right away.\n@vadimdemedes I like that, but we'd still need to give the test files a little bit of time to exit on their own. And then we may have to let users configure that\u2026\n\nI'm fine with this as long as we display what tests are blocking AVA from finishing on its own.\n\n@sindresorhus it'd be test files, but yes we should show this.\nI think if we land #583 we could set a default timeout (perhaps 10 seconds?). It'd automatically apply to this use case since eventually the worker pool gets filled up with workers that won't exit, and then the timeout is triggered. This feels to me like the cleanest approach.. >> And then we may have to let users configure that\u2026\n\nConfigure the time, or whether to exit right away or wait to drain?\n\nWe need an option so users can make AVA exit right away. Or an API so they can do it from a test.after.always() hook but I'd rather make it an option.\nBut the wait-to-drain period also needs to be customizable. There may be a complex shutdown sequence or a remote database teardown that takes seconds. I don't really want an option for that, hence my suggestion of using a default (but long) timeout value and applying that to the entire test run.. I don't know\u2026 it's a bother to write and not all that readable in the tooltips. Given that the method signature is shown, referring to the variables should be sufficient IMHO. Personally I'd love to see a succinct summary of what the assertion does. Specifics can be found in the online documentation.\nWith the deepEqual example, actual doesn't have to be an object. It can be any value. It seems pointless to spell that out, and in any case the method signature shows it. Moreover, it makes it clear that actual and expected should have the same type. The message parameter is really hard to succinctly summarize, and I'd argue it's not even necessary to do so (except for t.throws() where it can be misleading).\n. Thanks @kugtong33. I've pushed an updated TS definition. I may have gone overboard a little\u2026\n\n\nRefactor the Assertion interface so the .skip() modifier can be added to each assertion.\nRefactor the ExecutionContext interface to remove top-level .skip and allow .log() and .plan() to be skipped.\nAdd error value to .end() in CbExecutionContext.\nAllow plain implementations to be passed to hooks without a title\nDocument all assertions, execution context methods and hook & test declarations and modifiers.\nExplode the t.throws() definitions to give better information depending on how the assertion is used.\nLink to documentation where appropriate.\n\n\nWhat do you think? I like how, when writing tests, the documentation tells you what the functions do. I suppose it reads a bit different from other API docs but I think it's more useful for AVA.\nHow did you do your screen recordings?\nI haven't updated the Flow definition yet, and the changes I made mean it's a bit trickier to copy things across. I feel I can barely ask if you'd be up for doing that\u2026 but are you? \ud83d\ude04 . @kugtong33 that would be ideal. It makes it easier to update both files when we add features, since you can diff them against each other and see where you need to make changes.. Wow great work @kugtong33! I've previously had trouble getting Flow to work well for me. If you can see the right descriptions then I'm happy. I did push one minor tweak.\n\nI see that tests for flow and typescript definitions are only done when there is a need to regress, what if we add dedicated tests, is it possible?\n\nThat'd be great to have. It's just a lot of use cases to write \ud83d\ude04 . > When you select from the existing methods\n\n\u2026\nWhen you write the method name with parenthesis\n\nThat difference is fine I think. I tried test.serial.after.always(() => {}) in VS Code and it showed the right information when hovering each part of that chain. I'm going to land this, thanks @kugtong33!. @kugtong33 since you reminded me to raise this, it's yours if you want it! \ud83d\ude09 . @Jolo510 yup. The type definitions and documentation need to be updated sa well.\n\nsince it's a breaking change, what's the rules for versioning AVA?\n\nWe're currently doing 1.0 beta releases so there's no concern.. Fantastic, thanks @kugtong33!. I tried to run CI on the reporter test changes before I added all the other code that now causes tests to fail. With Node.js 6 on Windows I ran into an odd issue where stdout and stderr from the worker processes was missing. See the failure here: https://ci.appveyor.com/project/ava/ava/build/job/xtg4qmqy9wy1rqfk\nThis may be related to https://help.appveyor.com/discussions/problems/4177-nodejs-child-processes-dont-get-complete-output. We'll have to see if this issues persists once the refactor is complete.. > With Node.js 6 on Windows I ran into an odd issue where stdout and stderr from the worker processes was missing.\nBased on https://nodejs.org/api/process.html#process_a_note_on_process_i_o this may be due to the process exiting before the output has been written. #1718 might help with that.. Thanks @sindresorhus.\nI've updated the mini reporter now:\n\nIt depends on https://github.com/sindresorhus/ora/pull/69\nI've removed the additional indentation\nNo more truncation (https://github.com/avajs/ava/issues/629)\n\nThe mini reporter closely follows the previous output. I'm very tempted to improve it a bit though, make it more consistent with the verbose reporter, and see where we can reuse code. The verbose reporter definitely needs another take even without output changes.\nI also suspect our visual tests might be obsolete given the new reporter tests.. I've finished the main refactor. See the PR post above for updated details, as well as the commits themselves. Just trying to get tests to pass now.\nI have some ideas on merging the mini and verbose reporters which would make the remaining reporter issues a lot easier to tackle, so I'm hoping to still do that before merging.. Oh my! Tests are passing!\nI think I'll try and get a release out before landing this. I'd also like to see if we can communicate process-shutdown over IPC and if that helps with the job concurrency on Windows. In the meanwhile, any other feedback @avajs/core?\nUnfortunately I'm out of time as far as this weekend goes, so perhaps next weekend.. The release is out, but unfortunately I've run out of time again \ud83d\ude04 . I don't think we'll support a way to run test files concurrently and then serially, within a single ava invocation, which would be one way of solving this problem.\nSo really this is a question for the TAP tools out there. I'm sure our TAP output could be improved and we'll accept PRs as necessary.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.)\n. @forresst were you making suggestions? They don't seem to have come through.\n@BusbyActual been a bit busy lately, so I haven't had a chance yet to look at your changes here. Thanks for your work on this though!. @BusbyActual yes I saw that later, thanks \ud83d\udc4d . @BusbyActual I've pushed a few tweaks, let me know if you're happy with them.\nLooking at this recipe I have no idea why it even mentions Babel, but that's a worry for another day I suppose.. Thanks for the PR @martypdx. Do you think we Should generally advocate for npx ava? Both for --init and just plain running tests? And remove the global install advise? @sindresorhus?. Right. I think if we promote npx ava --init then it wouldn't make sense to install globally.\nI agree that normally you should run tests through npm t, but often when developing I reach for npx ava. \nI think that means we no longer have to advocate installing AVA globally.. Thanks @martypdx!. @Seiyial that's great! Have a look at some of our other recipes to see what we're looking for. Some of the more recently updates ones are:\n\nhttps://github.com/avajs/ava/blob/master/docs/recipes/babel.md\nhttps://github.com/avajs/ava/blob/master/docs/recipes/flow.md\nhttps://github.com/avajs/ava/blob/master/docs/recipes/test-setup.md\nhttps://github.com/avajs/ava/blob/master/docs/recipes/typescript.md\n\nFork the repository, create a new Markdown file in the docs/recipes and go from your experiences. Then do a PR and we can help with any improvements.. Go for it @mh81. If you could retain @Seiyial's commits that'd be great.. This should be fixed in the 1.0, which is currently in beta. Install using npm i -D ava@next.\nPlease let me know if this is still an issue after you upgrade though.. > When AVA@next (by default) merges its own Babel config with that of the user's, it seems to put its own config in front of the user's.\nInteresting. We could apply our stage-4 preset last.\n\nIt's especially odd if the user's config already includes the ESM->CJS transform (in the correct order wrt other plugins). AVA seems to still reorder that transform, so a build that works when run through Babel will break when run through AVA.\n\nI think Babel might be choosing not to run the transform twice, so AVA's ends up running too early. This might be where \"pass per preset\" comes in but I've never quite understood that. Perhaps the new documentation is clearer on that front.\nWe construct our Babel config starting here. Between createConfig and extend calls hopefully it's easy enough to follow. I'd appreciate somebody trying either \"pass per preset\" or reordering when we apply our stage-4 preset.. @ivan-kleshnin you're using a Babel 6 preset. AVA 1.0 uses Babel 7. You should either stop AVA from extending your project's Babel options, upgrade your project to Babel 7, or stick to AVA 0.25 for now.. @ivan-kleshnin you're using a Babel 6 preset. AVA 1.0 uses Babel 7. You should either stop AVA from extending your project's Babel options, upgrade your project to Babel 7, or stick to AVA 0.25 for now.. Hopefully https://github.com/avajs/ava/pull/1798 covers this.. Could you give #1722 a try? I think it should fix the repeated timeout exits.\nI just need to do a final refactor of the reporters before landing that PR. This refactor will make #583 much easier to implement, which would provide the information you're looking for.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). > Is there any way the message could include the specific tests or test files that should be running but aren't? I'm assuming Ava knows what they are.\n583 covers that.. > The commit spacing looks strange however in my local it's fine?\nYour lines use spaces, but the rest of them use tabs. If you configure your editor to show invisible characters you'll be able to see the difference.\nI've pushed some tweaks. Why did you add the env preset example? Initially I upgraded it to the recommended preset syntax from Babel 7, but then I realized that might depend on the user installing the beta version of babel-loader. So instead I think it's better to not mention any of that.. @BusbyActual I've pushed some tweaks. Notably AVA now by default extends the project configuration so normally no further setup is necessary. What do you think?. This is available since 1.0.0-beta.2. You're using 0.22. You can install the latest 1.0 beta using npm i -D ava@next.. @sshetty if you could share your code or reproduce this that'd be great. Perhaps make a post in our Spectrum community?. Great idea! Ergonomically, how would you feel about using @ava/init? We're trying to scope our supporting packages.\n@sindresorhus thoughts?. > Happy to update this to @ava/init if the package is republished there. \ud83d\ude0a\nIf you could do the PR to https://github.com/avajs/ava-init to change the name and anything else that might need changing, I can publish it \ud83d\udc4d \nThis is advanced but I wonder if npx @ava/init should offer to install either latest or next? I think as your PR is currently written we make it really hard to install the beta releases.\n\n(Does yarn have an npx equivalent? Maybe it doesn't matter, since Yarn users presumably also have npx installed.)\n\nI'm happy with that assumption. npx is open source so Yarn can always adopt it \ud83d\ude09 \n. > If you do want the feature, I think latest should remain the default, but I could add a --next flag to the @ava/init CLI.\nThat sounds good.. > Personally I don't think this is necessary. Upgrading to next is easy enough: npx @ava/init && npm i -D ava@next.\n\nIf you do want the feature, I think latest should remain the default, but I could add a --next flag to the @ava/init CLI.\n\nTo give you a longer response, it's a little awkward but I don't know how long we'll have to be in beta (it really depends on Babel). I'd like to avoid too many hoops so people get the beta releases.\n\nOk, can I do it in another PR? The existing one is already pretty big.\n\nJust landed the first one so go ahead :) Let me know if you need a beta release published for testing.. > How do you feel about the instructions defaulting to --next? Seems more appropriate since the rest of the readme reflects the beta release.\nYea that's perfect.\nWhat are your thoughts on advocating purely npx @ava/init? I suppose folks may figure out they can also do npm i -D ava@next (and the Yarn equivalent). I wonder if we should have an explicit \"Manual installation\" section? Similarly perhaps we should point out that npx @ava/init works with Yarn?. > How do you feel about the instructions defaulting to --next? Seems more appropriate since the rest of the readme reflects the beta release.\nYea that's perfect.\nWhat are your thoughts on advocating purely npx @ava/init? I suppose folks may figure out they can also do npm i -D ava@next (and the Yarn equivalent). I wonder if we should have an explicit \"Manual installation\" section? Similarly perhaps we should point out that npx @ava/init works with Yarn?. By the way I've published @ava/init and deprecated all ava-init releases. Thank you for pushing this @billyjanitsch!. By the way I've published @ava/init and deprecated all ava-init releases. Thank you for pushing this @billyjanitsch!. > Not sure what's going on with CI. I can try rebasing?\nWe've got some tests that flake out. I've restarted the specific jobs in Travis.. > \u2716 Extraneous file. Rejected promise returned by test\n\n\u2716 Exceed max files. Rejected promise returned by test\n\nThese test themselves are returning a rejected promise. Neither of them seem to be using a t.throws() assertion.\nGoing by the printed error I think the error originates from the fetch() call.\nExtraneous file.\n  /home/travis/build/jaydenseric/apollo-upload-server/node_modules/node-fetch/lib/index.js:1369\n  Rejected promise returned by test. Reason:\n  FetchError {\n    code: 'EPIPE',\n    errno: 'EPIPE',\n    message: 'request to http://localhost:44731/ failed, reason: write EPIPE',\n    type: 'system',\n  }\nThe EPIPE error might mean it can't connect to your server, but I'm not familiar enough with the libraries you're using to help there.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). In what way is the test failing? Is it defined before other failing tests?\nCould you try with ava@next (see Releases) and see if the behavior is any different?. Closing due to inactivity.. Closing due to inactivity.. Thanks for adding the symbol @jdalton!. Thanks for adding the symbol @jdalton!. @git-jiby-me I'm hoping to get a release out this weekend.. This is on purpose. It tells npm to stop consuming arguments and forward the remaining --watch to the invocation of the test script.. There's two things here we could do better. The code excerpt is based on where the assertion lives. I think we can keep that but if it's not from the test file we should present it differently.\nIf the assertion's stack trace contains call sites from multiple files, or multiple call sites from the test file, we should print it in its entirety. That way you should be able to trace it back to the actual test declaration.\nWhat do you think @billyjanitsch?\nWe have a related issue at #867. That's a compile-time enhancement however. It'd be an enhancement but I think it's different from what I'm proposing here.\nSince this affects the reporters it's blocked by #1722 which makes a lot of changes.. There's two things here we could do better. The code excerpt is based on where the assertion lives. I think we can keep that but if it's not from the test file we should present it differently.\nIf the assertion's stack trace contains call sites from multiple files, or multiple call sites from the test file, we should print it in its entirety. That way you should be able to trace it back to the actual test declaration.\nWhat do you think @billyjanitsch?\nWe have a related issue at #867. That's a compile-time enhancement however. It'd be an enhancement but I think it's different from what I'm proposing here.\nSince this affects the reporters it's blocked by #1722 which makes a lot of changes.. > Do we always know where the test file is? If so, I feel like the stack trace should always include everything from the assertion up to the test (which would normally be just the assertion and the test, except in the case of macros).\n\nI'm not sure that it's sufficient to only do this if the trace spans multiple files, because it would still be difficult to debug a failing assertion in a macro with an automatically generated title in the same file.\n\nYes. We filter out call sites from AVA itself, so async stack traces aside we have all the relevant call sites. Indeed it's not sufficient to check if the trace spans multiple files, which is why I suggested checking if there are multiple call sites from a single file. Does that make sense?. Hi @Seiyial, thanks for your PR. I've been busy lately so it may take a bit longer before I can properly look at this.. Hi @Seiyial, thanks for your PR. I've been busy lately so it may take a bit longer before I can properly look at this.. > By assertions do you mean t.deepEqual() etc? I personally thought of using console.log() because the readers may not yet be familiarised with ava's syntax. But come to think of it, the assert syntax looks quite foolproof so I guess using assert statements might be better.\nYes. People will have to learn those assertions at some point, so I'd rather the examples are close to actual use cases rather than simplified.. Hey @Seiyial are you still interested in working on this?. Heya @Seiyial. I just reread the Test context section in the README. I think it describes the behavior quite succinctly. Perhaps the recipe could start by repeating that section?\nThen for the examples, we should have one example that modifies of t.context properties, and a second example that modifies a nested object (which isn't cloned). Both examples should use all hooks.\nWhat do you think?. Thanks @Seiyial. I'm closing this issue for housekeeping purposes.. @ava/init will land in https://github.com/avajs/ava/pull/1738. I've removed the deprecation message from npm since it's not actually useful for anybody installing AVA.\nBefore you make a new PR for the promise change, could you elaborate on why it's necessary?. @ava/init will land in https://github.com/avajs/ava/pull/1738. I've removed the deprecation message from npm since it's not actually useful for anybody installing AVA.\nBefore you make a new PR for the promise change, could you elaborate on why it's necessary?. @Jaden-Giordano no worries. I'm happy to give pointers if you want?. @Jaden-Giordano no worries. I'm happy to give pointers if you want?. Hi @Jaden-Giordano, thanks for continuing to work on this. I've been a bit busy lately so it may take a few more days before I can get back to you on this, sorry.. > Maybe you have some idea what is going on in the CI?\nA newer Babel 7 is being picked up in one of the test jobs, and it's causing test failures. I'm a little surprised at that, to be honest! Feel free to ignore the Travis run with FRESH_DEPS=true, and AppVeyor as well since it doesn't run any other tests after the fresh-deps one fails.\nIt's going to be a few days before I'll have a chance to look at this. Thanks for your hard work @Jaden-Giordano!. @Jaden-Giordano if you could merge in (or rebase on) master, that should fix some of the CI issues.. @Jaden-Giordano I haven't had a chance yet to look into this. I just realized we need some changes with Babel (https://github.com/avajs/ava/issues/1789) so I might try and land that before doing my work here.\nAnyway this is super close so once that unblocks we should be good to go.. @Jaden-Giordano just wanted to say I haven't forgotten about this \ud83d\ude04  #1789 is pretty much done so I'm hoping to land that next weekend. I'll then try and get this in as well before cutting a release.. Phew! This is ready now, @Jaden-Giordano please have a look at the commits I just pushed.\nI've updated the Babel and ES module recipes. I've fixed the top-level extensions / enhancements-only code path, and we can now handle extensions that have periods in them.. I don't think profile.js script supports custom extensions, but I'll file a follow-up issue for that.. @brandonweiss have a look at https://github.com/avajs/ava/blob/master/docs/recipes/watch-mode.md#debugging \u2014\u00a0it should give you some clues as to what file system activity is causing the tests to rerun.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). @brandonweiss have a look at https://github.com/avajs/ava/blob/master/docs/recipes/watch-mode.md#debugging \u2014\u00a0it should give you some clues as to what file system activity is causing the tests to rerun.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). We have #1125 which enforces message to be a string. I just realized we should try and land that for 1.0.\n\nWe could consider supporting an array as an argument or having a t.format() method you pass to it.\n\nI like the idea of having a function that does the formatting and returns an object that is recognized by the assertions. Perhaps not on t though. How about test.message(any, 'possible', argument, 'like t.log')? And then t.is(true, false, test.message('hello')? t.log() could also accept this value.. Thanks for the PR @rhendric!\nAVA loads source-map-support before loading the require options, so we'll never be able to detect babel-register this way. There's also no guarantee of how source-map-support is laid out on disk. Ideally this is solved in source-map-support, but I admit I wouldn't know how. What do you think?. I've never quite figured out why the various source-map-support usages are incompatible. The solution may be as simple as ensuring all packages require the same version. Currently @babel/register pulls in ^0.4.2 and AVA pulls in ^0.5.3.\nUpdating the register packages seems doable. Ideally source-map-support starts to follow SemVer (release a 1.0) so that as long as packages specify the same major npm can dedupe and everybody uses the same instance.. Per https://github.com/avajs/ava/issues/974#issuecomment-390478448 we need to revisit AVA's approach.. Go for it @Briantmorr. Let me know if you have any questions!. If I change your example to this:\n```js\nimport test   from 'ava';\nimport PQueue from 'p-queue';\ntest('PQueue fail', async t => {\n    let queue = new PQueue({ concurrency: 1 });\nawait t.throws(queue.add(() => { console.error('1');throw new Error('error'); }));\nconsole.error('after 1')\nawait t.throws(queue.add(() => { console.error('2');throw new Error('error'); }));\nconsole.error('after 2')\n\n});\n```\nI only see 1 and after 1. It doesn't look like the second function ever executes. I think this may be an issue with p-queue instead.\n(I'm closing this issue for housekeeping purposes, but of course we can reopen if it turns out to be an AVA issue.). Thanks for your PR @harrysarson!\nLooking at Wikipedia I don't think the backticks have any meaning in IPA. For instance, from the Leiden page:\n\nLeiden (/\u02c8la\u026ad\u0259n/; Dutch pronunciation: [\u02c8l\u025bi\u032fd\u0259(n)] (About this sound listen); in English and archaic Dutch also Leyden) is a city and municipality in the province of South Holland, Netherlands. \n\nSee also https://en.wikipedia.org/wiki/Help:IPA/English.\nMy guess is that the backticks are used for visual effect rather than semantics.. @harrysarson I have no idea, sorry. @sindresorhus I assume you put this in originally?. > I guess the problem is @babel/register. @babel/register will read the config from .babelrc unless You specify option\nOh that makes sense!\n\nkind of weird to me, that test file and source file using different way to handle babel config.\n\nI think in your case you should require babel-register. You shouldn't need that or @babel/register to run your tests, unless you want to use it to compile sources on-the-fly. And then you need to use the register that is compatible with how you compile your source files.\nWe should update https://github.com/avajs/ava/blob/master/docs/recipes/babel.md#compile-sources so it recommends the correct register module based on how source files are compiled.. @xxxxxMiss given that you're commenting here, are you using babel-register? That's Babel 6, so it won't work with Babel 7 configs or plugins. You need to either use @babel/register and Babel 7 configs, or babel-register with Babel 6 configs.. @xxxxxMiss I get the following:\n```console\n\u276f npm t\n\nava-test@1.0.0 test /private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.ki5NUCbmLQ/ava-test\nava\n\n1 failed\ntest dynamic import\n/private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.ki5NUCbmLQ/ava-test/test/test.js:11\n10: test('test dynamic import', t => {\n   11:   return import('lodash').then(m => {\n   12:     const _ = m.default || m\nRejected promise returned by test. Reason:\nError {\n    message: 'Not supported',\n  }\n```\nI think the problem you're seeing now is that import() is not supported by @babel/preset-env.\nCould you post any follow-up to this on Spectrum instead? https://spectrum.chat/ava. @ashimagarwal that sounds great! Perhaps this article will be useful: https://medium.com/@vadimdemedes/making-your-first-contribution-de6576ddb190. Now that Babel 7 is out, I think we can assume people are upgrading. We probably shouldn't be making our documentation more complicated by covering Babel 6.. The .snap.md Markdown files that AVA generates are write-only. We could support an API for parsing the binary .snap files. One limitation is that we currently store fixed-width hashes of test titles, so you can't get the titles back out. Tests may have multiple snapshots as well.\nValues are concordance serializations,  not strings. Deserializing requires you to provide the concordance plugins that were used in the serialization. That's not a problem at the moment because we don't let you customize plugins, but it will be in the future. I'm not clear how you'd render these deserialized values in the browser for visual diffing though.\nThe deserializing-using-plugins problem is surmountable, being one of configuration. The snapshot name extraction problem requires a change to the .snap format. We could either add a reverse lookup or store the test titles directly, though that makes parsing more difficult.\nIn light of #1769, do you see this as an entirely separate process or as a complement to an AVA run? In which case we could surface the failing snapshots, you could render in the browser, then rerun a specific test and let AVA update the snapshot. Then we wouldn't need to change the .snap format either.. > It would be really useful to have the name back, otherwise snapshots can very easily get mixed up.\nYes. Roughly, the binary format is:\n\nReadable prefix (AVA Snapshot v1\\n)\nBinary version header\nChecksum\nCompressed data\nHeader length\nSnapshot headers\nHash\nNumber of snapshots\nFor each snapshot, pointers to the start and end of the concordance serialization\n\n\nA body, consisting of combined binary concordance serializations\n\nI think we could add a trailer section:\n\nTrailer type (\"titles\")\nFor each test title:\nHash\nUTF-8 string length\nTitle\n\nWe wouldn't need to parse this while running tests, but once we've parsed the header it's easy to know where the trailers start. We'd have to bump the snapshot file version number but we could still read v1 snapshots since none of that encoding has changed.\n\nCould you elaborate on what kind of values you're snapshotting? Only primitives (except for symbols) can be safely round-tripped through the serialization. Object values are described, so if you're storing React.createElement() results it'll be hard to  revive them.. If we're adding trailers anyway, we could add an \"annotation\" type trailer:\njs\nt.snapshot(visual(<Comp/>), {annotations: {visual: true}})\n(You'd probably want to wrap that as visual(t, <Comp/>).)\nThe nice thing about the .snap files is that they can contain more data than is shown in the report files.. Interesting. Do you see this as a side-kick that runs in the main AVA process, without needing a complete programmatic API? The changes in #1722 would come in handy for this.. Cool, I'm \ud83d\udc4d on this.\nWe'd need to decide what configuration syntax is used to load this module. I think it's fine if it's restricted to configuration files only. It shouldn't be something you do on a one-off basis.\n\nWe should load the module using esm\nThe ava object should include the version number for compatibility purposes\nI'd prefer to return an optional cleanup function\n\nWhat test results are you interested in? I'm loath to expose AVA's internals, at least at this stage, since I don't think that has necessarily stabilized yet.. That's not really the data we communicate back to the main process. What we can expose now:\n\ntest file\ntest title\nresult\ntypeof first failing assertion\nrichly formatted failure explanation\n\nThe latter is a string. We could also provide the concordance-serialized value, but I worry that might be unnecessarily expensive so may have to be an opt-in.\nIf we also expose a function for resolving the snapshot location based on the test file, you'd have enough to build on top of.. We have various issues about running all assertions, and communicating all assertion results up to the main process: #261, #1356, #1330, #324. I'd be more than happy to expose those to this side-kick. You could then filter out the snapshots.. Not from AVA itself. You could redirect stdout and stderr to a file if that's what you're looking for.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). If you're looking to parse the test output you may want to use our TAP reporter (--tap on the CLI) and use a TAP utility.. How odd! Thanks for your repo, I can reproduce this on my end. Doing require('glob').sync('src/**/*.test.js') works so I wonder if the other files are somehow filtered out.\nIt'll be a few days before I can dig deeper though.. Hey @boneskull, thanks for dropping by!\nWe're currently shipping beta releases that use Babel 7, which is itself in beta. So unfortunately the latest package on npm is looking a little outdated. But yea, farewell Node.js 4 \ud83c\udf88 . See my answer at https://stackoverflow.com/a/49957671/15421.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Yea there's no way around this. You could perhaps reduce the connection pool size so that's a little quicker?\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). It may be worth considering not printing non-AVA output unless --verbose is set. https://github.com/avajs/ava/issues/1849#issuecomment-402502807. Closing for now.. @hallettj wow, this is quite a change they're making!\n\nFlow's libdefs changed in v0.70.0 to use overloaded signatures for then. It is still possible in v0.70.0 to call then with no arguments. But as far as I can tell there is no way to write PromiseLike in a way that allows then to be called with no arguments that is compatible with both Flow v0.70.0 and Flow v0.69.0.\n\nAre you saying that if we drop interoperability with Flow v0.69 we can support .then(null, func)? I'd rather be more fully compatible with how people write JavaScript than support older Flow versions.\nIt does raise SemVer major questions, but we're still in beta so we can do what we want \ud83d\ude07 It does make me wonder whether we should publish type defs in separate packages so we can break them without having to bump AVA versions. @sindresorhus thoughts on that?. Thanks for the explanation @hallettj. This is merged now \ud83d\udc4d . From https://github.com/avajs/ava/commit/a76d462b4cdccd86028e8c9b1bbf84f12af8ab84#commitcomment-28919209:\n\nThis seems to have broken using async-await\n\n@jamiebuilds could you elaborate?\nAlso, we could just type Promise and remove this PromiseLike stuff. See https://github.com/avajs/ava/issues/1794.. Yes hoping to put one out soon. We've hit a blocker updating to the latest Babel beta, but I'm about to resolve that.. I reckon your beforeEach is setting up these listeners? It's run for each test, so with four tests it's run 4 times.\nI don't think this error is coming from AVA itself but I can't tell without a longer stack trace.\n. This is the expected behavior.\nBoth --match and .only() are ways to refine what tests run, typically used during development. I suppose it could be argued that when both are used, AVA should exclusively run .only() tests with matching titles, but I'm not sure how useful that would actually be.\nGoing by your config you seem to be annotating tests you don't want to run by putting \"skip\" in their titles. Did you see AVA has test.skip() which does the same thing, but without using --match?\nP.S. there currently is no --only option.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.)\n\n[ ] . > I think we've already discussed this before, but I couldn't find a ticket.\n\nMe too!\n\nSkipping a test might be temporary and you don't want to lose the snapshot\n\nYea. I was concerned about people committing .skip(), but that's what .failing() is for.. > Maybe there's a better way of collecting and running Sass tests and keeping them as separate tests, without writing both .scss and .js files for each test?\nBest I can come up with is to write your own loader in require.extensions which generates the test code on the fly. Then you can configure AVA to treat .scss files as test files.\nThis is a discussion better suited for https://spectrum.chat/ava though.. Which Flow version is this? Definitions have been completely rewritten, does this occur with ava@next?. Thanks for clarifying @hallettj.\nI'm not really interested in backporting to v0.25.0. If it hadn't been for Babel 7 being in beta we'd be at v0.29.0 by now, and we wouldn't have backported anything either.. @gajus #1778 has landed, so you could install a Git dependency from master until the next release is out. That may be next weekend. Or downgrade Flow.. Yikes that's not right! IIRC the snapshot code assumes paths have been normalized, but perhaps that's not the case after all, or maybe something changed.\nTo be sure, could you try with ava@next?. We definitely need to normalize slashes in this line: https://github.com/avajs/ava/blob/434cb7424db72c8475311184cf07a82bcb754343/lib/snapshot-manager.js#L132\nPreferably to POSIX.\nLooking at the code I don't know why .snap files would change. They contain hashes of the test titles, and value serializations. The titles don't include any path components. It may be that the values are file system sensitive but that'd be a problem with the tests. @masaeedu could you point me at a commit where the .snap file changes?. I'm on macOS. I see changes to the .snap files but not the Markdown ones. Depending on how the snapshots evolved they may see some churn when updating.\nIf you could update snapshots on Linux, commit that, and then do the same on Windows, that should give us a good sample of the changes within the snapshot files. Right now I can't figure out why they would be affected.\nTo be clear, there's definitely a bug with the Markdown reports. I'm just not sure about the .snap files.. Thanks @masaeedu. The issue apparent when you hexdump the snapshot files and get their diff:\n```diff\n  0000000 41 56 41 20 53 6e 61 70 73 68 6f 74 20 76 31 0a\n< 0000010 01 00 10 39 72 6e 44 3c 60 7f f6 2d f5 4c 87 d9\n< 0000020 14 ca 1f 8b 08 00 00 00 00 00 00 03 9b c6 c0 c0\n\n0000000 41 56 41 20 53 6e 61 70 73 68 6f 74 20 76 31 0a\n\n0000010 01 00 7b ed eb 99 70 b6 66 ab ef 1c 21 e8 d3 cb\n0000020 41 a8 1f 8b 08 00 00 00 00 00 00 0a 9b c6 c0 c0\n```\n\nOpen https://github.com/avajs/ava/blob/2167c575c925d104dc5ecab3d45143a328e9f33a/lib/snapshot-manager.js#L215:L220 for reference.\nThe first 16 bytes is the readable header, followed by two bytes indicating the snapshot version, and then a 16 byte checksum. The compressed output starts at byte 34 (the 1f 8b 08 00 00 00 00 00 00 part). Byte 43 is 03 on Linux but 0a on Windows.\nWe should solve this by rewriting byte 43 to always be 03. It's the 10th byte in the compressed array.\n\nTogether with https://github.com/avajs/ava/issues/1786#issuecomment-387119194 we should be able to get rid of these cross-platform inconsistencies. Thanks for sharing the reproduction!\nWould you be keen on making these changes?. Where-ever we build up the path, we should use path.posix.join(). That ensures the proper separators are used regardless of underlying platform.. > I can debug with Node.js but then I have to duplicate AVA arguments from my package.json into the run configuration in WS\nIs there any reason you can't configure AVA in the package.json file itself, rather than by passing arguments to the ava invocation?\nI don't use WebStorm so I don't fully understand the options you're laying out here, but if you find a satisfactory approach we'd greatly appreciate any improvements to the recipe.. > But the way I understand things currently, I can either follow the \"Setup using Node.js\" section in the recipe, which doesn't use NPM (and thus doesn't look at package.json, requiring me to duplicate AVA config in my IDE)\nIt sounds like you have config like \"test\": \"ava --serial tests-from-here/ and-also-here/\"? You should move those into a top-level \"ava\" object in the package.json.\n. > I don\u2019t think webstorm even looks at package.json at all in this \u201crun/debug with node\u201d mode, as opposed to the npm alternative?\nEither should just be an invocation of ava, which itself then looks at the package.json#ava field for its options.. > But given that AVA itself inspects the package.json I suppose I could try to somehow hide this ugly $NODE_DEBUG_OPTION somewhere in the AVA config, assuming I can affect the execArgv that spawned processes get?\nThat's not customizable.\nWe do have logic (contributed by @develar) that forwards appropriate --inspect flags though. Perhaps ava $NODE_DEBUG_OPTION will work?. > It has to be node $NODE_DEBUG_OPTION ava, or at least according to my experiments thus far.\nAh yes, sorry. This code still reads from execArgv which is the arguments for node itself. #1505 covers AVA itself supporting --inspect.. > Regarding WebStorm I\u2019m inclined to give up \u2014 they apparently make a point of fixing their own debug port and make it available via that $NODE_DEBUG_OPTION. Which might be well and good but I don\u2019t see a pretty way of injecting that in the \u201cDebug via NPM\u201d variant in the recipe.\nIt doesn't need to be pretty. We just need to explain how to make it work. Would you be interested in updating the recipe?\n\nI found two methods but both have to be done separately for each package script I might wish to debug (the one I mentioned earlier which is to insert the env var into package.json and another which hacks the shebang line in .bin/ava.\n\nLet's not hack the .bin/ava stub.\n\nI think the other issue you linked won\u2019t (can\u2019t) help with having to figure out which port WS picked for a given run.\n\nThe way I understand it #1505 could make AVA recognize Node.js' --inspect arguments and map / forward the debug protocol to the workers. That should work nicely with WS.. > The debug port forwarding is not going to work \u2014 if I understand things properly. But maybe I don\u2019t :-)\nnode $NODE_DEBUG_OPTION node_modules/.bin/ava should work. Oh, perhaps try node $NODE_DEBUG_OPTION -r 'ava/cli', too.\nEverything else requires code changes in AVA, which are tracked in #1505.. > Sorry, this completely fell off my radar, and I am swamped for the time being. If this isn\u2019t actually appearing to bother many, perhaps simply close the issue?\n@erikkemperman that's fine. We'd still like to improve our documentation, but don't feel that it's on you \ud83d\ude04 \n\nAny progress?\n\n@stavalfi it's safe to assume that if an issue is still open, there has been no progress. You could have a look at the release notes since the last comment on the issue thread to be sure.. Closing in favor of #1789.. I think this is covered by https://github.com/avajs/ava/blob/master/docs/recipes/flow.md#using-tthrows-and-tnotthrows. You have to explicitly type the function.\nSee https://github.com/avajs/ava/issues/1794 though, perhaps if we remove observable support this problem goes away.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). @israeleriston this is a new npm feature and as such I don't think they have fully worked out their communication strategy, and as a community we haven't yet considered how to engage on these issues.\nAVA being a development dependency it's quite unlikely anything dependency vulnerability can actually lead to a compromise (except for code that is so nefarious that npm will just remove it from the registry).\nOpening issues on all projects that have a transitive dependency on a vulnerable dependency version won't be very productive for anybody involved.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Sorry you're running into this @maritz. We do need to change how we select files but it's a larger change. I haven't been able to focus on that part of the project unfortunately.\nIgnoring EACCESS errors isn't great either, as it means one day helpers work, and then the next they won't, and you wouldn't know why.\nI've done a lot of thinking on this previously, but then didn't get round to implementing anything. Regardless of where we end up eventually, I think it should be possible to disable helper compilation altogether. That is something we could add now. What do you think?\nSetting \"helpers\": false in AVA's configuration could do it. We can later expand that to take custom patterns.. > Not sure it's the best long-term solution for everyone though.\nIt's not, but it's a feature we'll end up having anyway, and it fixes some problems now.\n\nWhy would printing an error and ignoring potentially cause breakage in your opinion?\n\nI assume the error will mean we don't find any helper files. So one day AVA compiles your helpers, and the next it doesn't, because there's a directory it can't traverse. Know you're left wondering why helper compilation stopped working.. We only get all glob results or an error, so we can't warn. Long term we'd want to exit with an error and have users exclude those directories from the helpers. For now let's ship a way to disable helper compilation.. Wow, thanks for the detailed response @hallettj!\n\nWe just need to express that if the first argument is a function that returns a PromiseLike or ObservableLike then the return type is a Promise; otherwise the return type is not a Promise (which is expressed here as any but perhaps could be given as ?Error);\n\nWe could refine this to Error, however if the assertion fails it returns undefined. I may have typed it as any so you can cast it to an Error if appropriate without really having to worry about type safety in case of an assertion failure.\n\nIf we change throws to behave only according to the second signature, and create a new assertion called rejects with the behavior of the first signature that would solve the problem.\nBut a nice feature of async / await is that it makes dealing with async exceptions pretty similar to dealing with sync exceptions, and it would be unfortunate if the test framework could not accommodate that analogy.\n\nInteresting. We're trying to keep the number of assertions to a minimum, while still covering most use cases.\n\nAnother option is to change throws so that it returns a Promise even on synchronous exceptions (\u2026) It could be slightly annoying to have to use async code to wait for synchronous\nexceptions in cases where the test needs to reference the exception that was\nthrown. But it would make the type ambiguity problem go away.\n\nAn added complication is that when used with a promise / observable, you must await the assertion. It'd be weird to not mandate that for synchronous exceptions.\nThat said, perhaps the current behavior is also confusing. We could make t.throws synchronous, and add an asynchronous t.errors() that still works with synchronous errors. It'd be cumbersome to port existing tests though we can make sure to print helpful error messages.\n\nUnfortunately Flow does not provide a promise interface - the definition for the native Promise is a class, because promises can be constructed, and Promise has static methods. It would be nice if they shipped a Thenable interface compatible with the native promise definition.\n\nWould Flow's Promise class match say a Bluebird promise? I think that'd be sufficient.\n\n\nI don't quite think it's worthwhile enforcing tests to return proper Promise instances rather than thenables, and properly typed promise implementations should match the Promise interface anyhow.\n\nI tend to agree; but I think that both options are reasonable.\n\nYea I'm torn on this. I don't want to force users to await a Bluebird promise, basically.\n\nTo recap, @hallettj are you saying that if we had an t.errors() that always returns a promise, we can overload that interface to accept observable-like things, streams, async iterators, regular functions, and promises? And then we'd have t.throws() always be synchronous?. >> We could refine this to Error, however if the assertion fails it returns undefined. I may have typed it as any so you can cast it to an Error if appropriate without really having to worry about type safety in case of an assertion failure.\n\nSo it could be Error | void to indicate that the value may or may not be a defined error.\n\n@hallettj yes that's worth another shot. I can't remember why I did not do that.\n\n\nAn added complication is that when used with a promise / observable, you must await the assertion. It'd be weird to not mandate that for synchronous exceptions.\n\nI think it would work to fail the test synchronously if an error is thrown synchronously, but to require the user to wait on a promise to examine the error that was thrown regardless. In any case that would make the types work out.\n\n@hallettj the problem is that when used asynchronously you must await, regardless of whether you want to examine the error. I don't like having an asynchronous way of getting the error that you may or may not need to await.\n\nI'm fine with removing observable support from t.throws though. It always felt out of place. And I think we should definitely remove support for generator functions in the test implementation. It's really just a leftover from the early days when generators were a popular alternative to async/await.\n\n@sindresorhus great, let's do that.\n\nI think that trouble in front of us is not due to descriptions of promises, but is due to Flow not having enough information to choose between multiple function signatures in some cases.\n\n@hallettj does this problem go away if we remove ObservableLike?\n\n\ntl;dr: I think that the simplest option is to split throws into multiple assertions: throws for functions that should throw a synchronous error, rejects for functions that should return a rejected promise,\n\nI've considered this before, but with async/await the naming becomes unnatural, as you usually use the throw keyword in async functions instead of Promise.reject(), so it doesn't really seem like it's rejecting.\n\n@sindresorhus even with the type problem resolved, we still have t.throws() sometimes returning a promise and sometimes not. Perhaps it's fine. Or perhaps we could have t.throwsSync() that only works with synchronous errors, and t.throws() which always returns a promise that must be awaited?\n\n\nIt feels weird having to add/change APIs because of Flow/TS limitations...\n\nIt's definitely possible to type flow correctly here. This sounds like a separate API decision to me\n\n@jamiebuilds if you have some time to help out here that'd be great. Does master have a problem now with async/await? I don't think @hallettj can reproduce that.. > even if there are only two options, a function that returns a promise (() => PromiseLike), or a function that throws an error synchronously (() => any), Flow does not have enough information to choose one signature because the second option will be applicable in every case where the first is applicable. What we would need to disambiguate would be a type like () => any type that is not a promise. Unfortunately Flow does not have a way to express that.\n@hallettj oh so you're saying the problem is the any return type?\nI remembered more about why I typed it like that, but we can reconsider:\nTechnically, the assertions can return undefined if they fail. But practically we don't really care about runtime crashes after an assertion failure, so we could always type as Error.\nCan Error still be upcast to a specific error instance? If not, that would've been another reason for using any. But perhaps we can use a generic that defaults to Error, so users can type their assertion as appropriate?\nIf all that works out then we could retain the current synchronous / asynchronous t.throws() behavior, but with better typing. Which perhaps @jamiebuilds was referring to?. > I'm sorry; the problem is not the return type of t.throws() - it is the return type of the function that is given to t.throws(). If that is restricted to something other than any or mixed then users would get errors that they probably do not expect.\nOh I see. That does leave us in a pickle. @jamiebuilds any ideas?\n@sindresorhus any thoughts on a t.throwsSync() that never returns a promise (and thus fails with async errors) and a t.throws() that works like our current one except it always returns a promise that must be awaited?\n@hallettj any thoughts on typing the assertions to return an Error / Promise<Error>, and perhaps t.throws<MyExpectedError>()?. I've opened #1866 and #1867 to remove the generator and observable cruft.\nThe remaining issue is that we've overloaded t.throws() & t.notThrows() to return a promise, if they're called with one, or with a promise-returning-function. We do not return a promise when called with a regular function, in which case the function argument is typed as () => any. Flow cannot distinguish between these two scenarios.\nI think the cleanest solution is to make t.throws()/t.notThrows() always return a promise. This way the return value of the function does not matter, simplifying our type definitions. It would still work with functions that throw synchronously, however the t.throws() & t.notThrows() assertions must now be awaited on. This is a breaking change.\nWe can add t.throwsSync() and t.notThrowsSync() for functions that must not return a promise. We can't type this restriction, but we can fail the assertion if a promise is returned.\nI also want to consider typing the return value as Promise<Error> / Error respectively. I'll have to see if these can be upcast, or else be specified as a generic in the assertion itself. This may preclude a resolution to #1841, or if using generics perhaps not.. Fair enough.\nNow that we've removed observables, is there a case for a t.rejects() that fails if passed a function that throws synchronously? Rather than t.throwsAsync().. I meant instead of throwsAsync():\njs\nawait t.rejects(Promise.reject(new Error()) // passes\nawait t.rejects(async () => { throw new Error() }) // passes\nawait t.rejects(() => { throw new Error() }) // fails test. > ```js\n\nawait t.rejects(() => { throw new Error() }) // fails test\n```\nWhat's the use-case for supporting this? I really think if we split up the methods > we should not allow synchronous stuff in t.throwsAsync nor asynchronous stuff in t.throws.\n\nThat's what my example does. A synchronous throw is not allowed in throwsAsync(), and thus the test fails.\nWhich is why I suggested possibly going for rejects, but I agree that you'd still throw in an async function, so I'm happy with throwsAsync.. Tracking the remaining work in #1893.. Nice catch, thanks @isnifer!. CI willing this is ready. I'm facing a busy week but hopefully I can land this next weekend or shortly after.\n@billyjanitsch could you give this PR a try?. Yes would be great to have a recipe for this! \ud83d\udc4d . Both your AVA and nyc versions are outdated. It's possible it works better with the most recent versions.\nAVA already includes source-map-support (and so does babel-register, incidentally) so you may not need that. I can't tell from your example but if you don't require your sources to be compiled using Babel you should avoid babel-register.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Yea I don't know. This stuff gets really complicated. AVA doesn't compile your router.js file so that's an interplay between nyc and babel-register. There might be some other interplay with regard to source-map-support, see #974.\nIf you could share your code perhaps somebody could try and reproduce this issue and see where the problem really lies.. I think you're running out of file handles. We use chokidar in watch mode, perhaps some of the issues listed for ENOSPC contain a workaround? https://github.com/paulmillr/chokidar/search?q=enospc&type=Issues\nYou may also try upgrading listr to ava@next which might have a newer chokidar version.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Yea, I'd like to change this. See #1718.. Wow that's surprising, nice find @isnifer!\nCan you share this reproduction? And does it occur when you install ava@next?. Looking through the source code we're assuming that Babel returns us a source map, and that's probably a wrong assumption: https://github.com/avajs/ava/blob/2167c575c925d104dc5ecab3d45143a328e9f33a/lib/caching-precompiler.js#L71\nThis leads to the 'null' string.\nWe'll have to update the worker/precompiler-hook module accordingly. We don't need the sourceMapCache value at all, we can compute the source map location from the options. We just need to catch any errors if the file does not exist.\nDo you happen to have source maps disabled in your Babel options?. Note that we may remove our source-map-support integration entirely: https://github.com/avajs/ava/issues/974#issuecomment-390478448\nWe should still fix this bug while #974 is pending.. Fixing this should wait until https://github.com/avajs/ava/pull/1798 lands.. The underlying serialization is not meant to be readable, editable or diffable. It has a binary encoding. What you see here is a GZip compression of all serializations.\nWhy are you concerned about the format?\nI suppose we could write Base64 to the file but I don't think that adds any value.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Did you see the Markdown files AVA generates? They\u2019re the ones meant for us humans :smile:. @iamstarkov why are you concerned about the format?. @iamstarkov do you know what in particular they don\u2019t like?. > have routines based on text-based snapshots\n@iamstarkov could you elaborate on what those routines are?. > Coming from Jest, the feature I miss the most is the interactive mode that Jest has. It really makes updating one snapshot at a time a breeze though that has nothing to do with storing the snapshot as a binary.\nThat sounds like a nice feature to have!\n\na problem I found with binary snapshots is when you update a snapshot in two different branches which comes from the same origin branch. Git can diff and merge the markdown files fine but there is always a merge conflict with the binary. What is your thoughts on this?\n\nInteresting. I haven't run into that but that's probably more due to the kind of projects I work on. A custom merge driver which re-runs the appropriate tests may be a solution.\nHow often do you run into this where the Markdown file also conflicts? Or, if you can recall, where Jest snapshot files had such conflicts?. Interesting. I do agree that t.true() communicates an assertion on a property value, not so much an expression result. The power-assert output is not always expected or welcomed either. I wonder if it may be an improvement to restrict power-assert to one purposefully named assertion, e.g. t.expr(foo >= bar). That restriction may also make it easier for us to integrate the desired behavior more closely with AVA itself.\n@avajs/core?. > I don't think t.expr (assert that its argument is an expression?) is clearer or more descriptive than t.assert.\nAs a general rule we don't alias assertions, so since we already have t.truthy() we wouldn't introduce t.assert().\nHowever if we were to restrict power-assert to a single assertion, then perhaps t.expr() communicates \"assert that the expression in the assertion call gives truthy, or else we'll print lots of detail about what was in the expression\", and I think that would be sufficiently different from t.truthy() that we could introduce it.\n\n(P.S. I think this is a feature/enhancement request rather than (or as well as) a question :-)\n\nYes, will add such labels when we come to a satisfactory conclusion on this.. I believe the purpose of t.true() / t.false() to be providing explicit boolean comparisons. You're right that they can be distilled to t.is(), but I think they have enough value to exist on their own.\nThat said, I do not think they communicate power-assert, which I believe is also your motivation for opening this issue.\nI'm not wedded to t.expr(), I was just raising the possibility of restricting power-assert to a particular assertion. I think that would make power-assert easier to understand, and provides a better argument for adding an additional assertion.\nWithout that restriction I'm not sold on the need to make any changes (documentation improvements aside). But let's hear from @vadimdemedes or @sindresorhus first.. > I would personally prefer having a separate assertion method for power-assert and remove it from the others.\nGreat!\nTo recap:\nWe're removing power-assert from:\n\nt.truthy()\nt.falsy()\nt.true()\nt.false()\nt.regex()\nt.notRegex()\n\nWe're adding a new assertion that is power-assert enabled. I think t.assert() is sufficient here. It should behave like t.truthy().\nThis will require a breaking change to https://github.com/avajs/babel-preset-transform-test-files which encodes the patterns that need replacing.\n\n@chocolateboy I think we'll want to keep t an object. That way it's easier to compose behavior on top of it without having to forward calls to t itself.\nAre you interested in working on this?. >> I think we'll want to keep t an object. That way it's easier to compose behavior on top of it without having to forward calls to t itself.\n\nNot sure I follow this. Do you have a particular extension/plugin in mind this change would make things harder for?\n\nFunctional composition is a big part of how people use AVA. That may include adding additional fields to t. Syntax like {...t, additional () {}} wouldn't work if t was itself a function.\nAlso t() is pretty obscure, compared to assert().. > Why wouldn't that still work?\nt2() would fail with a t2 is not a function.\n\nWell, that's a consequence of AVA requiring the name to be t (currently). How hard would it be to (also) allow it to be assert?\n\nWe need something reasonably unique to hook our transforms on to. We don't have good enough AST / type analysis to see which functions are receiving our execution context, so the best we can do is transform t.something() calls.. Go for it @eemed. Don't hesitate to ask questions \ud83d\udc4d . A PR would be best, thanks @eemed.. Whilst support remains experimental we won't support it directly, no. You can use AVA with the esm package though, see https://github.com/avajs/ava/blob/master/docs/recipes/es-modules.md.\nThat said perhaps we should support a way of passing options to the worker processes. We could add --node-args CLI flag, and support nodeArgs in AVA's configuration files. What do you think?. @PhilT yes but that's landing soon (#1746).\nI imagine AVA should use import() to load the test file too. That'll be trickier, since we currently expect a synchronous require('ava'). Still, our esm-the-package support means there's a way to hack into that process:\nhttps://github.com/avajs/ava/blob/f0f0c3b1f9812583d207c50bf36944e13b12362e/lib/worker/subprocess.js#L110\n. @jim-king-2000 configurable extension support has landed, so AVA can now run .mjs files. You can disable AVA's compilation of test files, or disable just the conversion of ESM to CJS.\nI think the one remaining question is how AVA would actually load the files. I don't think it can use require() with --experimental-modules.. I could make a ticket for the nodeArgs option, but that won't help with --experimental-modules.\nWe should also extend AVA's handling of esm so alternative loaders can be configured, including supporting asynchronous loaders. Then somebody could publish a package that wraps import() so it works with AVA.\nLeaving this open for somebody to spec this out further.. > So, when es module becomes the fully supported feature by node and can be used without the \"--experimental-modules\" argument, AVA still requires \"esm\" to be installed. Is it right?\n@jim-king-2000 we'll follow the default behavior in Node.js.. We won't support it directly while it's behind a flag, but I'd be happy to take PRs that allow AVA to opt-in to experimental Node.js features, if controlled through the appropriate flags.. @jim-king-2000 this discussion may be relevant: https://github.com/avajs/ava/issues/1980\nThere's a lot of different tools interacting here. I don't fully understand what's going wrong either.\n\nAgain I'd be happy to find a way so you can configure experimental Node.js flags and have them apply to AVA's worker processes.. Landed in #1812.. I'm not concerned about the grouping. It\u2019ll never really be sensible anyway. The hashing makes sense from a CompSci perspective but sure we can split a sorted list as well.. > Actually, the one big problem with the hash approach is that it creates surprising groupings of tests.\n\nIt seems reasonable to expect tests in the same sub-directory to be mostly in one job instead of distributed across all of them.\n\n@jamiebuilds I don't know if that's such a big problem. Even with sorting there is no guarantee of sensible grouping.\n\nI had a look at hughsk/path-sort which is the dependancy added in this Pr.\nThere is a pull request there that has been open for a year without any response so I dunno if the package is maintained at all.\n\nI'd be happy to use lexical sort and not use a dependency at all.\n. @jamiebuilds I've pushed some updates. Notably, the distribution algorithm has been changed. I didn't really like the floating point arithmetic. If I've figured out the maths correctly then earlier jobs will run 1 extra file each compared to later jobs. This leads to a nice predictable distribution.\nRegarding path-sort2, why can't we rely on Array<string>#sort()?\nWe still need test coverage before we can land this. If you happen to have a project set up in CI you could test this with that'd be great too (in addition to the test coverage), just to get some confirmation it all works as expected.. Yes.\nAlso, and I'll try and do this today, I think we should pass the index and total values as parameters to the Api constructor, and resolve them in lib/cli. That way there is no unexpected behavior when using ava/api directly (not that that's officially supported, but hey).. I'm leaning towards not doing any specific sorting for now. Or, if we do want to keep this, only doing it after we've collected the files, if we're doing job splitting.. > If you do end up using #sort(), I would make sure it is case-insensitive:\n\nfiles.sort((a, b) => {\n  return a.toUpperCase().localeCompare(b.toUpperCase());\n});\n\nBest I can tell localeCompare already takes care of that. The numeric option is nice though.. @jamiebuilds @sindresorhus I think this is ready now:\n\nCLI determines whether parallel runs are enabled, not Api\nI've removed path-sort2\nI've added documentation\nI've added an integration test\n\nI've temporarily included #1874 which ensures integration tests actually run\u2026. @jamiebuilds thanks for the suggestion, kicking it off, and your patience! \ud83d\ude04 . It looks like you've correctly configured this syntax for AVA's compilation of your test files. I imagine this error is coming from @babel/register, which doesn't know about the ava.babel.testOptions config. You'll have to add Babel options instead, e.g. in your package.json:\njson\n{\n  \"babel\": {\n    \"plugins\": [\n      \"@babel/plugin-syntax-jsx\",\n      \"@babel/plugin-syntax-dynamic-import\"\n    ]\n  }\n}\nAlso, you don't need to specify @ava/stage-4. AVA applies that automatically.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). @gajus depending on your beta version AVA should have its own instance of @babel/core, so I wouldn't expect it to stop working. Perhaps you could try with master?. > Master works. 1.0.0-beta.4 does not.\nThat's weird. Beta 4 has pinned its Babel dependencies so it shouldn't end up using Beta 49. Perhaps your installer is deduping in unexpected ways.\n\nWhen is the current master getting released?\n\nI'm trying to land two features that were blocked on this Babel compatibility issue.. @gajus not sure what's going on then. Hopefully the next beta fixes it correctly.. Ah, perhaps you have some plugins / presets that are making its way into AVA's Babel pipeline? I suggest downgrading to Babel 7 beta 44 (or 47) for the time being. Make sure to install exact dependencies, the ^ includes all beta releases.. We need to auto-detect it though, like we do .babelrc files. That's not implemented yet. We also need tests for that implementation, including what happens when both files are present.\nThen, I'd like to update our documentation to use babel.config.js rather than .babelrc.. @kevva yup, I just didn't want to do that without having tests and an understanding of how it all behaves.. master now supports babel.config.js files, but we still need to update the documentation.. I'm hoping to have a release out this week that should get us to beta 48. Maybe a newer one if I have time to update our dependencies.\nOnce Babel 7 is out of beta we'll stop pinning an exact release and then deduping in the installation process should take care of this.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). @goooseman yes, that's what this issue is about. Would you be able to help us fix it?\nOr, perhaps, implement a proper debug mode? https://github.com/avajs/ava/issues/1505. It's an aversion from having too many ways of specifying config. Though perhaps odd at first, hopefully it's actually fine in practice. Let's use this issue to see if there are any problems and then we can add support for module.exports = {}.. @cantremember thanks for the report. I suspect this is an issue with esm itself. I've opened https://github.com/standard-things/esm/issues/501.. @vjpr I imagine it is, yea.\nWhat are you hoping to achieve?\nesm does expose an import.meta object I believe.. It's not ideal but perhaps you could use https://www.npmjs.com/package/resolve-from? \nOr maybe it's possible to provide a relative path to the plugin? Assuming this goes into AVA's Babel pipeline, I'd expect it to be resolved relative to the project directory, so you should be able to reference the subpackage.. @vjpr I reckon esm (which we use to load the ava.config.js files) masks it.. I agree that should work, @vjpr.\nIt's an interesting problem. Our default Babel pipeline supports importing from CJS, with __esModule interoperability. But, we don't want to apply that to loading the config file, especially if Babel is disabled through that file.\nBetween ESM, CJS and interoperability there's a lot of ways a config file could be written. At least ESM is more forward looking, and we can use the esm module to load the file without any other precompilation.\nCurrently esm is configured to only allow ESM modules. That way we don't introduce any non-standard behavior or rely on esm implementation details. It's great if you're already using esm in the rest of your codebase but of course that's not necessarily the case.\nLooking at the esm options we could enable the cjs.vars option. That would expose __dirname, __filename and require to the ava.config.js file and other imported ESM modules. That'll allow you to use require() to load CJS modules.\nWould this work for you?\nAnother option is to run esm in full compatibility mode, or just inherit the esm settings in your project. The downside is that it really couples us to esm's behavior.\n@sindresorhus what do you think?. > My current workaround is to fire ESLint twice with different options, but this is not great and complicates my setup quite a bit when coupled with files watching.\nYou can use overrides to switch the source type:\njs\n{\n  'overrides': [\n    {\n      'files': ['ava.config.js'],\n      'parserOptions': {\n        'sourceType': 'module',\n      },\n    },\n}. This looks good!\nI realize this may be a bit of a stretch, but perhaps you could try and write an integration test for this? See https://github.com/avajs/ava/blob/master/test/reporters/mini.js (and verbose.js) and https://github.com/avajs/ava/blob/master/test/helper/report.js for the starting points.. Thanks @btkostner! I've taken the liberty to push some changes as I'd like to get this into today's release. Weirdly I couldn't get compile errors out of the code you pushed.. Phew, fixed the CI failure in the TAP reporter by just not checking the TypeScript output \ud83d\udc7c . The availability of the Test instance was quite accidental. I'm reluctant to restore access to it. Ironically, TestCheck.js doesn't pass it on to the test implementation either: https://github.com/leebyron/testcheck-js/blob/c7dce0773142546e1b4e208c13b87e7f8f9be253/integrations/ava-check/ava-check.js#L40\nWe have an open issue that will enable first-class support for tools like TestCheck.js. See https://github.com/avajs/ava/issues/1692. I've put out a call with TestCheck.js as well: https://github.com/leebyron/testcheck-js/issues/91\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Thanks!\nSome of these links end up in the reporter integration tests. You can update those logs by running:\nconsole\n$ UPDATE_REPORTER_LOG=1 npx tap --no-cov --timeout=300 --jobs=4 test/reporters/{tap,mini,verbose}.j\nHowever you'll need to add a sanitizer that rewrites the current version to something else, otherwise the tests will fail every time we publish a new release.\nLet me know if you'd like some more pointers with this.. Fantastic, thanks @TheDancingCode!. Heh sorry, misread the code, this is as expected \ud83d\udc4d . Now that Babel 7 is out, I think we can assume people are upgrading. We probably shouldn't be making our documentation more complicated by covering Babel 6.. I agree with you, but the rest of @avajs/core was leaning the other way. We've since cleaned up the assertion though, so perhaps we can reconsider.\nSpecifically, we could only support const err = t.throws(fn) and t.throws(fn, {is: err}) for non-Error values. E.g. t.throws(fn, {instanceOf: MyNonErrorClass}) wouldn't be allowed.. (I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). I'm a little surprised by the ../ paths. AVA starts in the directory that has your package.json file. Paths are relative to this directory. ava.config.js is only used if it's in the same directory as the package.json file.\nI'm pretty sure you can't run tests from directories above that one, or even if it seems to work it's not supported. Which is what may be going on here. Why do you have the ../ paths?\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). On the one hand, I'd like to have plugins for t.snapshot() and even t.deepEqual(), which could generate their own diffs.\n1692 would deliver an API you could use to build your mismatchFn implementation. What do you think of that proposal?\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Yea the ideal would be something like:\n```js\nimport Image from 'ava-image-snapshot'\ntest('image is the same', t => {\n  t.snapshot(Image.from(buffer))\n})\n```\nBut that would require the fictional ava-image-snapshot package to actually integrate with AVA and the underlying systems, which is going to take a lot more work under the hood to make possible and keep compatible.\nThe idea with #1692 is that you could do something like:\n```js\ntest('image is the same', async t => {\n  const result = await t.try(t => {\n    t.snapshot(imageBase64)\n  })\nif (result.passed) {\n    result.commit()\n  } else {\n    t.fail('image changed')\n  }\n})\n```\nThis would be better with a helper function, so you'd end up with test('image is the same', checkImage, imageBase64).. I assume this is because the default reporter only shows details for failed tests. The logs should show up if you use npx ava --verbose.\nThough I would have expected console.log() to show, it's asynchronous so perhaps the worker exits before it's read. I'm looking to make some changes to the reporters that should make this more reliable, see #1776.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). You can use test.serial() in your test files. AVA will still run files concurrently.\nI wonder if you're asking whether you can run tests in each directory serially, but then run multiple directories concurrently. That's not possible.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Nice find!\nThe appending of the additional snapshot should happen at this line: https://github.com/avajs/ava/blob/fd73358d9ee85c3623a10acfebb5af88c51d628b/lib/snapshot-manager.js#L149\nI reckon the line-ending removal code is overly eager:\nhttps://github.com/avajs/ava/blob/fd73358d9ee85c3623a10acfebb5af88c51d628b/lib/snapshot-manager.js#L84. Thanks for the PR!\nTo be honest I'm not sure if this is worth an integration test. Will have a play next week when I'm back from holiday.. Works great, thanks @coreyfarrell!. It's to separate AVA's output from your program's output. For example with the TAP reporter we can't mix them up.\nThe answer might be to configure your logger to not output anything when tests are running, or to discard stderr in CI.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). That's pretty much what we do, except that all non-AVA output is written to stderr.\nI wonder if, unless --verbose is set, AVA should not print any non-AVA output. Perhaps with a message at the end that output was silenced.. Thanks for the clear example @NickHeiner. Currently we limit the diff depth, but when comparing lists we should print an additional level.\nSee https://github.com/concordancejs/concordance/issues/41.. This should work with recent releases. What version did you encounter this in?\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Could you share a reproduction?. Which Flow version is this? I'm pretty sure this worked when I wrote the definition, but I don't use Flow so I may have overlooked this.\nFixes to the definition are most welcome. Would be good to have a type-check regression test for this too.. Thanks for your comment @vinsonchuong. It prompted me to double-check our Flow recipe, and your solution is what's documented there: https://github.com/avajs/ava/blob/master/docs/recipes/flow.md#typing-tcontext\nI'd still love if this wasn't necessary, of course.. Yes we left this as a follow-up. Some things to consider:\n\nShould this silently override config from package.json#ava / ava.config.js?\nShould we allow a package.json#ava.config value so you don't need to provide the argument?\nAnd if so, what if package.json#ava contains other properties?\n. > my opinion\nShould -c xxx override(disable) all config\nno because -c means I don't want to use package.json#ava.config\n\n\n\n@bichikim could you clarify your answer? It doesn't quite map to my questions, and I don't want to presume anything \ud83d\ude04 . >> Should this silently override config from package.json#ava / ava.config.js?\n\nSeems sensible. If someone explicitly specifies a custom config file, probably not surprising if that becomes the sole config.\n\nI agree. The implicit question is whether we should print a warning.\n\n\nShould we allow a package.json#ava.config value so you don't need to provide the argument?\n\nThe command line argument would work fine for me. I set up an npm script to run ava, so that's a fine place to put the arg. I do JSDoc that way - one config file, one arg in the script.\n\nI frequently use npx ava. Having to specify the --config argument would be bothersome. So I'm \ud83d\udc4d  on a package.json#ava.config option, but in that case I do think we should exit if package.json#ava contains other options.. We already check for a TTY in the reporters, though this broke in 1.0.0-beta.5. I just ran this in my terminal and it looks like Node.js (at least 10.6 on macOS) detects the output being piped:\n```console\n\u276f node -p \"process.stdout.isTTY\" | cat\nundefined\n~\n\u276f node -p \"process.stdout.isTTY\"\ntrue\n```\nSo I believe there is no issue when using 1.0.0-beta.6. I'll close this for now, but happy to reopen.\nWhich version are you using @christroutner?\n. Ah. I suspect that the spawned process inherits the TTY behavior from the process that's calling it. In your case, try running execa.shell(RUN_HAPPY_PATH_PATENT, {stdout: test1Log, stderr: test1Log}). Or since you're using shell(): execa.shell(RUN_HAPPY_PATH_PATENT + '|cat').\nI'm reluctant to add CLI flags to disable this behavior, since the TTY behavior is (I believe) the standard way this should behave.. This is not due to AVA, but the output from ts-node/register. Which I imagine is the same as if you'd use the tsc CLI.\nI found importing non-ESM modules to be a bit confusing in TypeScript. Try this instead:\ntypescript\nimport Sequelize = require('sequelize')\nimport path = require('path'). The library underlying t.snapshot() and t.deepEqual() is concordance. It was designed to be extensible (see  @concordance/react, but we've never exposed an interface to do so within AVA.\nWriting such extensions is a fair bit more difficult than for Jest's snapshot mechanism, but I guess it won't get easier until we start encouraging people to write their own \ud83d\ude04 \nQuestion though, how do we want to configure these extensions? Programmatically?\njs\nimport {registerConcordancePlugin} from 'ava/extend'\nOr in the configuration file, with a factory method pattern?\njs\nexport default {\n  plugins: [\n    ({registerConcordance}) => registerConcordance(\u2026)\n  ]\n}\nWe'll also need some metadata regarding the plugin that's been registered, so that if AVA encounters a snapshot without the corresponding plugin being available, it can give a useful error message.\nIt's been a while since I've looked at the underlying code but these are the main questions I think.. Hi @sanchitbansal10, there's already a PR for this: #1871. Hopefully you'll find another issue you'd like to work on \ud83d\udc4d . Odd. It is supposed to clear those lines. We rely on Node.js to tell us whether that's supported, so something is going wrong somewhere.\nCould you try with Node.js 10?. Did you also add more test files? I wonder if this is triggered by something in the newer test files, or newer tests. Perhaps you could try and whittle it down to a single test file?. > I'm a bit surprised I'm the only one experiencing this, but I suppose in that case it's a not a super high priority.\nYea me too. Which leads me to think it's something in your test output that prevents the line erasure from working, but honestly that's just a wild guess.. @jacobg oh no, that sounds bad! There have been a lot of changes internally since 0.25, but I'd be surprised if something like that slipped through.\nI'm a bit busy at the moment, perhaps you could write a simple reproduction? A test, an after, and an after.always, all of them perhaps calling console.error with a message?\nThanks!. I can't reproduce this:\n```js\nimport test from 'ava'\ntest('test', t => {console.error('test'); t.pass()})\ntest.after(() => console.error('after'))\ntest.after.always(() => console.error('after.always'))\n```\n```console\n\u276f npx ava test.js -v\ntest\n  \u2714 test\nafter\nafter.always\n1 test passed\n```\nClosing this for now. If you could share a reproduction that'd be great.. Thanks @sh7dm. I've made some tweaks to the error message and removed the console output. What do you think?. Thanks @sh7dm!. Thanks @padmaia!\nDoes this mean that Flow can distinguish between the following overloads?\nflow\nthrows(() => any): Error\nthrows(() => PromiseLike<any>): Promise<Error>\nWe came to the conclusion in #1794 that we had to separate them. There's a PR ready to land for that: #1869.\nThat may still be the right thing to do, but would be good to know if we maybe don't have to.. @jamiebuilds any idea on the above?\n@sindresorhus regardless of whether the overloads are possible, do you still think we should make t.throws synchronous, and add t.throwsAsync?. Thanks for clarifying @jamiebuilds.. Thanks @padmaia!. That's a neat idea @jamiebuilds.\nThis could be done using a try {} finally {} block inside the test implementation, but that's not very pretty. I'd be OK with adding a teardown util.\nI think the callbacks should execute within the test, but orchestrated by AVA. They have access to the t object through the closure so they could still perform assertions.\nPresumably callbacks are executed in order, serially, waiting for any returned promises to settle? Errors wouldn't stop the next callback from executing?. The before & after hooks run concurrently by default, are invoked in order, and can be made to run serially.\nI don't want to add t.teardown.serial(), so the question is what the default behavior should be. I propose we make them execute in order and serially. You can always compose different behavior in an initial teardown callback, if needed for performance reasons.\nBefore & after hooks can execute their own assertions, as they receive their own t value. I reckon the callback will be a closure with access to the t value of the test. We could make it crash if it performs an assertion on that object, but that seems like a pitfall. Providing its own t value just leads to shadowing:\njs\ntest('test', t => {\n  t.teardown(t => {\n    // Which value is `t`?\n  })\n})\nThat doesn't seem ideal either. However if the teardown can perform assertions on the test, then I don't think we should report teardown failures separately.\nTests keep executing even if there is an assertion failure. We can do the same for teardown callbacks. There's an open issue for AVA to report multiple assertion failures though.\n. @godu thanks for this PR!\n\nBuckleScript use lib/bs and lib/js as output directories. It's not a breaking point, but it's a little weird to mix them with ava's files.\n\nThis is the bsb -make-world output? I think that's fine. Perhaps though don't ignore the directories themselves, but add a README.md which explains why they're there, and nest a .gitignore that ignores all other files inside.\nSpeaking of, could you remove the .DS_Store file? It's probably best if you add that to your machine's .gitignore file, not AVA's.\nIs the bsconfig.json necessary for publication? If so, is it odd that the test/bs-types files are not published?\nCould you put the definition files in a different directory than src? bucklescript maybe?\nThe name value inbsconfig.json should probably just be ava, like the package.json.\n\nIgnore lib/bs and lib/js on npm pack. I test an ava's installation using npm pack and npm install ava.tgz. lib/bs and lib/js were both in the tarball. NPM should use .gitignore to filter files. I don't know if it's only a npm pack command bug or if npm will do the same thing on publish.\n\nIt'll definitely do the same on publish. Perhaps because lib is in package.json#files it overrides. I think you could add !lib/js and that should fix it.\n\nThe definition files are quite intimidating, by the way. Hopefully you'll stick around to help keep them up to date \ud83d\ude04 Note that the throws assertion has recently changed, see #1869.\nI'm excited to see this land. Let's get the ReasonML community to use AVA! \ud83d\ude80 . OK I've been trying to read up on BuckleScript and Reason. It's a bit of a challenge as the documentation is biased to starting a new project, rather than writing bindings for existing ones.\nThe idea seems to be that you write modules in Reason. They can \"interop\" with existing packages. This PR adds those interoperating modules to the ava package.\nAVA's test interface is built on the notion that functions are objects, too. Judging by your code I imagine that's not the case in Reason. I'd like to keep the conceptual differences to a minimum though. Is there a way we can support test() and test.failing() without having to resort to underscores? Perhaps test.declare() and test.failing.declare()?\nSkipping assertions poses a similar problem: t.deepEqual() versus t.deepEqual.skip(). In that scenario I guess t.deepEqual_skip() is preferable.\nIs Ava.Sync used for synchronous test implementations, Ava.Async for test.cb() calls, and Ava.Promise for asynchronous test implementations? How does Ava.Promise handle promises? Are these distinctions meaningful? That is, could we get away with always creating asynchronous test implementations, without support for test.cb()?\nGiven that this is less a type definition than a wrapper implementation, should we perhaps publish this as @ava/bucklescript? It's hard enough to keep the type definitions up to date, but at least they raise fewer existential questions about how to express AVA's untyped behavior \ud83d\ude04 . @godu sounds good!\nI think you'll need to add me as an owner of your repo, and then I can transfer it to this organization. I'll also set up the permissions so you have write access etc, and do the same for @ava/bucklescript on npm.\nWe can start some new issues to discuss the tradeoffs between matching AVA's API exactly, or doing something that makes more sense for Bucklescript.\nExciting!\n(Closing this PR in anticipation.). You may want to re-enable these rules as well: https://github.com/avajs/ava/blob/c4f607cc376846363320691f65c6e96ef401996d/package.json#L174:L178. Which AVA version are you using?. I'm not sure, I haven't encountered this myself in Node.js 10. Are you perchance using an early release? IIRC something changed and they're only reporting this warning for first-party code.\nWill leave this open for now, in case others have run into the same.\nNote that v0.25 won't receive any updates and predates Node.js 10.. There was talk at JSConf.eu to come up with a new way of doing this. I reckon newline-delimited JSON.\nUnfortunately I then went traveling and haven't had the time to follow up on this.\nI did speak to somebody from CircleCI who expressed frustration that people often get the JUnit formatting wrong.\nIn short\u2026 I hear ya, and I'd like to have a non-TAP output format that has more information, but it won't be JUnit.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). > Do you want to start a new TAP/JUnit-like spec along with people at CircleCI? I can also get people from Bitbucket Pipelines involved\nYes that's my long-term plan. Something that can provide a high-fidelity representation of test runner behavior, both for third-party formatters and tooling integration.\nWe set up https://github.com/testjswg but nobody's had the time yet to get it going.. Hi @rishavsharan, thanks for chiming in. I can see the usefulness, however we won't support any output format that has lower fidelity than we (want to) achieve in our own logger.\nWe're open to supporting a NDJSON-based format that captures the richness of events we can (or want to be able to) output. I don't particularly care how much of a standard it is, or who standardizes it. I'd then like to see other tools to translate that output into TAP, JUnit, etc.\n. The problem seems to be that ES modules aren't transpiled. Looking at the config you provided I can't immediately tell why that'd be the case. I'd love to figure it out though, AVA 0.25 won't receive any patches so you're going to have to upgrade at some point.. Looking at this again, I noticed the error is in src/App.jsx, not your test file. You're using @babel/register but the only Babel configuration you've shared is in package.json#ava.babel.testOptions. I think this means @babel/register is running without any options, and thus your src files are not really transpiled.\nIndeed in your old setup you have a .babelrc file which is used by both AVA and Babel.\nIn short, stick to the .babelrc file and you should be fine.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Thanks @CanRau!. Oh nice find! I don't think the watcher provides the extensions to the file globber: https://github.com/avajs/ava/blob/3ac2a8f813947588832fccd065d160fc533cd75c/lib/watcher.js#L83\nWe also need to provide them to the watcher itself: https://github.com/avajs/ava/blob/3ac2a8f813947588832fccd065d160fc533cd75c/lib/cli.js#L237\n@midgethoen would you be interested in working on this?. That sounds great @midgethoen. Though this is a bit of an embarrassing bug, we should tackle it before we do the 1.0 release.. @midgethoen oh no, I stepped on your toes!\nI got as far as https://github.com/avajs/ava/tree/watcher-custom-extensions. Perhaps you could test that with your setup if you still have it lying around? npm install avajs/ava#watcher-custom-extensions should do the trick.\nIf that works we just need some tests in test/integration/watcher. That's not particularly glamorous work though, so feel free to punt that back to me \ud83d\ude09 . Glad to hear, thanks @midgethoen.. Hey @ronen, thanks for the minimal reproduction!\nThis seems to be an issue with @babel/register. Removing it from AVA's options fixes the problem. I've also commented out the relevant line in @babel/register that installs https://www.npmjs.com/package/source-map-support and that fixes the problem too.\nCould you do me a favor and take this up with @babel/register instead? Perhaps post the link here. I'm running out of time for this weekend to follow up myself. Thanks!\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Yea no worries. I think I need to check a few more interactions between AVA and alternative configurations of source-map-support. Plus @babel/register ends up compiling test files again, so maybe something goes weird there.\nIn case the double compilation is what trips up the source maps, have a look at the last section here which has a suggestion on how to stop @babel/register from compiling test files: https://github.com/avajs/ava/blob/master/docs/recipes/babel.md#compile-sources. Aha! The problem is that the source map that AVA generates no longer contains the correct path to the original source. Should have a PR soon to fix. Thanks again for finding this and for creating the reproduction, @ronen!. @ronen see #1892.. Fantastic work, thanks @jagoda!. > I'll push a commit which restores it for the timeout tests.\n@vancouverwill could you allow edits from maintainers on this PR so I can push my fix?. > With regard to access it looks like it already is set to that as far as I can see \ud83d\ude04\nYes. My \"push to the PR branch\" script got confused about the slash in the branch name. I've pushed my fix now, let me know if you have any further questions!. @vancouverwill awesome.\nWas about to merge this and then I realized the PR said this would work on interrupts too, and I don't think we're testing that yet. Will have a look to see what state this PR is actually in, and then merge it with the correct commit message \ud83d\ude09 . @vancouverwill awesome.\nWas about to merge this and then I realized the PR said this would work on interrupts too, and I don't think we're testing that yet. Will have a look to see what state this PR is actually in, and then merge it with the correct commit message \ud83d\ude09 . > better yet get this out and we can do another\n\npr for the interrupts?\n\nYup that's the plan. Just need to have another look at it.. > better yet get this out and we can do another\n\npr for the interrupts?\n\nYup that's the plan. Just need to have another look at it.. Thank you so much @jagoda!. Thank you so much @jagoda!. Good call, thanks @ronen!. Nice catch @ivanschwarz!. I wouldn't use TAP snapshots for this. Instead commit the expected files in one location, and then use AVA to generate new files in a different location. Then, compare them to make sure they're byte-for-byte the same.. I wouldn't use TAP snapshots for this. Instead commit the expected files in one location, and then use AVA to generate new files in a different location. Then, compare them to make sure they're byte-for-byte the same.. Thanks @sharkykh, will try and have a look soon.. > I also added a commit excluding test/fixture/symlinkfile.js from XO because the lint check was failing for me locally.\nCould you elaborate? It passes for me, but I reckon perhaps you're developing on Windows \ud83d\ude09 You may need to enable symlinks in your Git config, we needed these two lines for our AppVeyor tests: https://github.com/avajs/ava/blob/0ecd0be849b2cb1b01db8e0834827f4b411b3ea0/appveyor.yml#L28:L29\nPerhaps we need to update our contributing docs to make this clear. Though I'm not opposed to ignoring the file altogether. Let's do that in a separate PR / commit though so we can tell why we changed it.\nWill land this when CI passes.. Maybe it'll work better in a fresh clone?. Thanks once more for the detailed report @ronen.\nI think in case two this is an issue of Babel miscomputing the source map, and / or the source map conversion of the stack trace not mapping back correctly. It might be out of our hands.\nWith the first case you may be bumping into https://github.com/avajs/ava/issues/974 \u2014\u00a0where @babel/register prevents our source map conversion from working.\nThis all needs a bit more digging so I'll leave this open as a question for now.. Right! So, AVA installs source-map-support first. However this module only guards against repeated installations if the same version is used, see https://github.com/evanw/node-source-map-support/issues/200. @babel/register uses ^0.4.2 which selects 0.4.8, whereas AVA uses 0.5.9.\nConsequently in the first scenario, @babel/register replaces AVA's installation, which means source maps for the test file are not applied. Of course if the error is thrown in any code compiled through @babel/register it'd be fine.\nIn the second scenario it may just be that @babel/register pulls in an outdated source-map-support, which itself pulls in an outdated source-map (^0.5.3 rather than ^0.6.0). All these non-SemVer releases don't help either.\nUltimately, if we can get rid of source-map-support we'd have a better chance of dealing with this mess. That's what #974 is about. Of course the tricky bit would be how to handle files compiled outside of AVA, without @babel/register, but perhaps we can suggest users to require source-map-support/register themselves.\nI'll also try and nudge the related packages to update their dependencies and switch to SemVer releases, so that package deduplication reduces these conflicts.\nOf course at that point AVA's source-map-support would no longer be overwritten by @babel/register and we'll still have incorrect output. But that's what #974 is for \ud83d\ude09 \n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Thanks!. This was modeled after the .code property in Node.js, which is a string: https://github.com/avajs/ava/issues/1708\nBut happy to support numbers as well.. @okyantoro let's see if @ronen was keen on tackling this soon, given that he raised the issue. Appreciate the enthusiasm though \ud83d\udc4d . All yours @okyantoro!. Thanks @okyantoro!. Thanks @okyantoro!. Which version are you using now? Where you using a different version before? Which one?\nCould you share your AVA config so we can see how you're specifying the files?. Which version are you using now? Where you using a different version before? Which one?\nCould you share your AVA config so we can see how you're specifying the files?. Aha! As of beta 7, import { test } from 'ava'; should be import test from 'ava';.\nThat should have been reported though. I've opened https://github.com/avajs/ava/issues/1906 to track the issue.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Aha! As of beta 7, import { test } from 'ava'; should be import test from 'ava';.\nThat should have been reported though. I've opened https://github.com/avajs/ava/issues/1906 to track the issue.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). I couldn't quite reproduce this behavior, but https://github.com/avajs/ava/pull/1923 improves the output for the default reporter when there's files without tests.. Yea makes sense that the problem is something else. Looking at https://github.com/avajs/ava/issues/1903#issuecomment-412838425 again, I noticed the test file is inside a helpers directory. I don't think that'll work, see #909.\nHowever, I don't know of any change since 0.25 that would impact this specifically.\nI assume there are other files that suddenly aren't being picked up? Could you share some example file paths?\n. > Is there ways around that?\nNot at the moment, but I've been considering a stop-gap measure to at least make the helper matching configurable.\n\nEven if its because helpers/ is ignored why did it work previously? I made no changes and they have always been in helpers\n\nI'm curious about that, too. Is it just the test files in src/helpers that aren't working, or are there other files as well? Could you share some example file paths?. > Is there ways around that?\nNot at the moment, but I've been considering a stop-gap measure to at least make the helper matching configurable.\n\nEven if its because helpers/ is ignored why did it work previously? I made no changes and they have always been in helpers\n\nI'm curious about that, too. Is it just the test files in src/helpers that aren't working, or are there other files as well? Could you share some example file paths?. @havenchyk yea, sorry about that. #909 is the issue to track. As I mentioned above I've been wanting a way to change the glob patterns for helpers (including so you can disable it). This is part of a larger, complicated story with many trade-offs\u2026 which I haven't had time to properly summarize. Thanks for the offer though, perhaps keep an eye on the globbing issues for the time being?. @havenchyk yea, sorry about that. #909 is the issue to track. As I mentioned above I've been wanting a way to change the glob patterns for helpers (including so you can disable it). This is part of a larger, complicated story with many trade-offs\u2026 which I haven't had time to properly summarize. Thanks for the offer though, perhaps keep an eye on the globbing issues for the time being?. @ericmorand it's supposed to make it easier to group them in say test/helpers, but the current pattern is too aggressive and we haven't yet tackled that problem.. @ericmorand it's supposed to make it easier to group them in say test/helpers, but the current pattern is too aggressive and we haven't yet tackled that problem.. @wmik try with the latest beta instead.. Sorry @Christilut, this is a bit confusing. We should detect the ava key in the exported config. I've opened https://github.com/avajs/ava/issues/1943. Perhaps something you'd be interested in tackling?\n\nI'm closing this since I don't think there's anything that isn't already covered by other issues.. Do you have any Babel configuration in your project?\n. Do you have any Babel configuration in your project?\n. @bcomnes unfortunately I can't reproduce this with a plain test file. If you could share more from your project code that may help.\nI imagine you may have a local branch from https://github.com/netlify/netlify-cli that perhaps you could push up?. @bcomnes unfortunately I can't reproduce this with a plain test file. If you could share more from your project code that may help.\nI imagine you may have a local branch from https://github.com/netlify/netlify-cli that perhaps you could push up?. Sorry you're having problems with this @onexdata. The issue is that AVA only compiles your test files. But since you're using ES module syntax, you need to compile source files too.\nGoing by what you've posted here I don't think you have a Babel compilation step? That would explain why the Babel recipe didn't help \u2014 you may not have configured Babel for @babel/register to work.\nYou may be interested in using the esm package as is described here: https://github.com/avajs/ava/blob/v1.0.0-beta.7/docs/recipes/es-modules.md\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Sorry you're having problems with this @onexdata. The issue is that AVA only compiles your test files. But since you're using ES module syntax, you need to compile source files too.\nGoing by what you've posted here I don't think you have a Babel compilation step? That would explain why the Babel recipe didn't help \u2014 you may not have configured Babel for @babel/register to work.\nYou may be interested in using the esm package as is described here: https://github.com/avajs/ava/blob/v1.0.0-beta.7/docs/recipes/es-modules.md\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Yes it's all a little bit confusing. I wonder if we should default to CJS instead, see #1908. Would love to know if that would've helped you in this case.. Yes it's all a little bit confusing. I wonder if we should default to CJS instead, see #1908. Would love to know if that would've helped you in this case.. I can't quite reproduce the behavior described in #1903 but the default output when AVA can't find test files (in the mini reporter) can definitely be approved. PR soon.. Thanks @sholladay. I agree with all your points.\nFor Babel projects, I agree that the biggest hurdle is that AVA doesn't compile sources. Especially now that new language features are adopted more quickly this leads to projects that may only be compiled for distribution, either using Babel or webpack. Users get syntax errors when they import non-compiled source files.\nFor projects using webpack, the extra difficulty is that, if they're also compiled using Babel, that Babel configuration typically does not compile ESM, since that's left to webpack. AVA's test-file only support for ESM can be hugely misleading.\nFor projects using esm, there's the counter-intuitive step of having to disable AVA's ESM compilation.\n\n\nThe solution here is better docs.\n\nWe could revisit our recipes with these project types in mind. We could also detect these syntax errors and point users to the relevant documentation.\n\nI was hoping AVA would rather move to a model where it either transpiles everything (or the specified files to be transpiled) or nothing. That way there's no confusion.\n\nNote though that if user compiles ESM syntax through a different tool (webpack, or esm) AVA won't pick up on this automatically.\n\nI do agree the esm is a better than transpiling the syntax with Babel, but I don't think it's worth this big of a breakage.\n\nPerhaps we need to make it easier to opt-out of AVA's default ESM handling. Right now it involves specifying our ava/stage-4 preset and disabling module compilation. Perhaps it should be a flag, e.g. package.json#ava.babel.compileEsm: Boolean. If we do that, perhaps we could default to compiling ESM even for sources.\n. Yes that makes sense. I'd like something similar for TypeScript.\nI was thinking to have @ava/babel and @ava/typescript packages, which are required as soon as you enter configuration for babel and typescript. They themselves could have peer dependencies on AVA and Babel / TypeScript respectively.\nAgreed that this is for after 1.0.. We probably need to add projectDir to the object here: https://github.com/avajs/ava/blob/7f974ccd1f2320eaf010db917f17b09922342b86/profile.js#L79:L87\nThere's a bunch of other problems with profile.js, see https://github.com/avajs/ava/issues?utf8=%E2%9C%93&q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+profile. I'd like to see if AVA can work with https://github.com/GoogleChromeLabs/ndb.\n. > I can confirm it fixes the issue.\nWould you like to submit a PR? It's hard to proactively maintain profile.js but we can definitely land bug fixes when we inevitably break it \ud83d\ude04 . This esm behavior has now been reverted. If we can deal with the partial test failures I may just change this PR to update to the next esm release, without changing the helper code.. Closing in favor of #1920.. Thanks @sharkykh!. @sindresorhus @sh7dm happy with my changes?. Thanks @Phrynobatrachus!. Hey @Asing1001, sorry you ran into this. Until we place the documentation on a separate website it's hard to clearly communicate what version it applies to. Unfortunately it's not manageable for us to include versions everywhere we mention a feature.\nHopefully we can ship a 1.0 release as latest soon, and do more frequent releases afterwards, which should alleviate this problem.. For now I suggest you keep your plugin / preset dependencies pinned to rc1.\nBabel 7 should be coming out imminently. I'll try and do another beta release this weekend where we change our dependencies to ^7.0.0.\n. > suggesting to hardcode a dependency version is not a good solution.\nIt is when your dependencies are pre-releases.\nBabel 7 release is now likely to be Monday, so I'll try and follow up with an AVA beta after that.. @gajus the beta 8 release uses Babel 7 proper. There's two issues left in our 1.0 milestone: https://github.com/avajs/ava/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+milestone%3A1.0\nI reckon we'll do another beta once those are resolved, and then finish up towards a proper 1.0 release.. What do you mean by \"log\"? stdout / stderr?\nI think it may be less confusing if you use verbose mode, IIRC that always writes the separator. But maybe this makes sense for the default mode. It's just that the reporters are a bit of a mess.. Yea, makes sense.\nI tried to figure out how this stuff currently works and I couldn't in a reasonable amount of time. Will add the appropriate labels and leave this open, but it may be something to be done as part of #1776.. Yea, makes sense.\nI tried to figure out how this stuff currently works and I couldn't in a reasonable amount of time. Will add the appropriate labels and leave this open, but it may be something to be done as part of #1776.. Could you elaborate on what you're looking for beyond what AVA already supports? E.g. there is a timeout feature, but it's to help find inactivity in your test suite as a whole, not a particular test.\nAre you looking for the timeout error to have feedback on how far pending tests managed to progress towards their execution plan? Or are you hoping to limit how long a particular tests is allowed to run for?\n. Right, gotcha. AVA's timeout option isn't quite suitable for this.\nYou could partially get there by calling t.fail() after a timeout, but you'll have to detect whether you've reached the plan count yourself.\nI think ideally we'd support this through https://github.com/avajs/ava/issues/1692, where you can use Promise.race() with a timeout promise, and fail the test if the assertions didn't complete on time.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). When you run more than one test file, AVA prefixes with the file path. Albeit with prettier separator characters.\nIn my tests I tend to either use separate files, or prefix in the test title itself.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Yea it takes the directory names and the file name, so for foo/bar/test.js you'd get foo > bar > test > [name of first test].. Yea, I'm hoping to do a new beta release soon. I'll also need to update three other dependencies and check some things.\n@Dianoga if you'd like to help, I'd appreciate PRs against these projects, setting the version range to ^7.0.0:\n\nhttps://github.com/avajs/babel-preset-transform-test-files\nhttps://github.com/avajs/babel-preset-stage-4\nhttps://github.com/avajs/babel-plugin-throws-helper\n\n. @Dianoga I'm going to close this, it's a little easier to do the necessary changes on a local branch. Hope you don't mind, and hope to see you make more PRs in the future!\n--\n@billyjanitsch thanks! That saves me a nice bit of time waiting for CI to run \ud83d\ude04 . We're erring on the side of caution here. Sets do have an order. I don't want users to write an assertion which they think tests an expected order, and passes, even though the actual order did not match their expectations. If the order is truly insignificant there are still other ways to set up your assertions.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Plan also sets a maximum number of assertions. If the test ends and you've exceeded that maximum, the test will fail.\nAVA can be configured not to throw, for users who don't use the built-in assertions (see the failWithoutAssertions option).\nIn conclusion t.plan(1) does have additional value over AVA's default behavior.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.)\n. Plan also sets a maximum number of assertions. If the test ends and you've exceeded that maximum, the test will fail.\nAVA can be configured not to throw, for users who don't use the built-in assertions (see the failWithoutAssertions option).\nIn conclusion t.plan(1) does have additional value over AVA's default behavior.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.)\n. Yea that's an oversight. It's documented in the Babel recipe but we should document it in the main README as well.. Yea that's an oversight. It's documented in the Babel recipe but we should document it in the main README as well.. @Akiyamka in ava.config.js, export the config directly, not wrapped in an ava object:\njs\nexport default {\n  babel: false,\n  compileEnhancements: false\n}. @Akiyamka in ava.config.js, export the config directly, not wrapped in an ava object:\njs\nexport default {\n  babel: false,\n  compileEnhancements: false\n}. Ah sorry about that. As of Beta 7, t.throws() only works for synchronous errors. You'll need to change your test to use t.throwsAsync().\nHowever we should provide better feedback in the test output. I've opened #1932 with some pointers.\n. Ah sorry about that. As of Beta 7, t.throws() only works for synchronous errors. You'll need to change your test to use t.throwsAsync().\nHowever we should provide better feedback in the test output. I've opened #1932 with some pointers.\n. @grant37 sounds like you're nearly there! Please make the PR and we can get it in.. Cheers. Have added this to the 1.0 milestone, but we tend to update packages before releasing so we should get round to it before the next beta.. Cheers. Have added this to the 1.0 milestone, but we tend to update packages before releasing so we should get round to it before the next beta.. Yup, we can do that. The restriction was added to reduce the complexity of adding this feature in the first place.\nWe'll have to make lib/cli work asynchronously:\nhttps://github.com/avajs/ava/blob/bfe16300e07c12a1bad2e4f02898b1e7486daf70/lib/cli.js#L24\nWe'll also need to figure out how to make this work in our ESLint plugin:\nhttps://github.com/avajs/eslint-plugin-ava/blob/6bb239fe1cd97fd671318253debc153fab7db071/util.js#L47. Wow that's unexpected! I've opened an issue in the underlying library: https://github.com/concordancejs/concordance/issues/44.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Thanks @goooseman!. Thanks @robertbernardbrown!. Which React version is this?. Looks like the name is now a symbol, rather than a string. Should be fixable in @concordance/react.. We should fix this at the call site: https://github.com/concordancejs/react/blob/7f8101f6f67fd03249af28828b16e69e9d914b60/lib/elementFactory.js#L109\nAssuming the symbol is registered, we could detect it and then use React.Fragment as the name. But there should be a fallback for other symbols.. > well, Im sorry. you lost me on registered symbol.\nSee https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Symbol/for.\n\nBut I still think it will be a good idea to defend lib/formatUtils.js@wrap against non string instances. What do you think?\n\nNo, consider it typed as taking value as a string. The problem is in @concordance/react. I'd been meaning to take a look at that. There's a flurry of Concordance activity all of a sudden! \ud83d\ude00 . This should be fixed with https://github.com/concordancejs/react/pull/14.\nI have one more PR that needs to land in concordance itself, and then I need to do the upgrade dance to get these changes into AVA itself.. @iamstarkov the necessary changes are landing in #1942.. @grant37 Node.js 6 doesn't support async functions. Use a regular function (or arrow function) that returns a promise instead.. Thanks @grant37!\nI don't think the messaging is quite right. The problem is that the function returned a promise. We don't actually know if it rejected or not. I'll push a commit that changes the messaging, please let me know what you think.\nI've also added a try / catch around the retval.catch(noop) call, in case the object isn't quite a promise.. Nice!. > I'm wondering what the proper approach is to set up a global before and after hook because I only want to init and destroy my database once... I don't want to do this per file\nCould you give more context on how you're using this helper file? Are you putting it in the require config? Or importing from a test file?\nAs long as you import the helper file, from the test file, it should still work. (Unless this is wrapped in a function which you're calling asynchronously.)\nHowever I think you're implying an expectation that these hooks were only run once per test run. That's definitely not the case. Even the require config is executed per worker process, and each test file gets its own worker. You should spin the database up before you run AVA, and spin it down after.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.)\n. > Is it then possible to have all files use the same instance of https://github.com/nodkz/mongodb-memory-server ?\nNo. But then ideally your test files are independent of each other, so that shouldn't matter.. Hi @mzavaletavargas, thanks for the PR.\nNow that Babel 7 is out, I think we can assume people are upgrading. We probably shouldn't be making our documentation more complicated by covering Babel 6.\nI hope you'd still like to tackle a different issue?. Thanks!. Oh wow, this is very exciting @qlonik!\nI must admit I've kind of forgotten how Test and assertions work\u2026 It may take me a little while to review this and figure out what we need to do in order to land this PR. There's some other Babel related work that needs to take priority, we really need to get the 1.0 release out.\nI'm terribly sorry about that. I'm rather geeked about shipping this feature!. Go for it. I\u2019ve been terribly busy of late, hoping to have a look after next week.. Another thought. If we're limiting to one concurrent t.try() per test, we should explore reusing the t value inside the callback. That way you could write:\n```javascript\ntest('inline', async t => {\n  const result = await t.try(() => {\n    t.pass()\n  })\n  result.commit()\n})\nconst reusable = t => t.pass()\ntest('reuse', async t => {\n  const result = await t.try(reusable)\n  result.commit()\n})\n```\nWe prefer the t variable name is used for the execution context, so it'd be confusing if an outer context is used in an inline t.try() callback.. > But, the code as it is now does not change the title of the running test. It creates new test with a different title. And as long as order of attempts is the same, the attempt test title will correspond.\nYou're assuming that the number of attempts required is deterministic. That won't apply to all use cases, for example using t.try() as a method of retrying flaky tests.. Hey @qlonik sorry for the delay in getting back to you. Will review now and make a checklist of whatever work is left.\nRe concurrent attempts, I think we can allow them as long as we throw when both attempts save a snapshot.. @qlonik I've pushed two commits. What do you think about this checklist?\n\n[ ] Update test assertCount with the number of assertions made within a committed attempt\n[ ] Type t.try() so it accepts macros\n[ ] Investigate refactor so attempts don't need to instantiate a new Test\n[ ] If not possible, add metadata to indicate the test is \"inline\" and adjust handling of test.failing() and t.end() based on that\n[ ] Revisit how attempt titles are generated\n[ ] Ensure snapshots created in each attempt start at the correct invocation count\n[ ] Fail the test if concurrent attempts that created snapshots are committed\n[ ] Determine documented properties of AssertionError and add to type definitions\n[ ] Update Flow definition\n[ ] Update ESLint plugin to recognize the assertion\n[ ] File issue to discuss suggested argument name (tt?) for inline functions, and update power-assert rewrite rules to handle it\n[ ] Review test coverage\n[ ] Update documentation\n\nLet me know which items you'd like me to pick up.. Sorry, still haven't gotten round to this PR \ud83d\ude22 . > As an attempt, I refactored assert.js and test.js in such a way that assertions dont need to be bound to the test instance. Instead ExecutionContext is extending the Assert class, and Test exposes needed functions for ExecutionContext. Would this refactor be useful to submit? Maybe as a separate pull request.\nYes would love to see that.\n\nWhile working on typing the try function, I stumbled upon the solution to #1970. I found that we can get rid of that clever crafty extends and infers and conditional types. It appears that TS didn't like the fact there was no definition for macro without the title. When I added that, it worked without problems. Would you like me to file PR with that refactoring?\n\nSure that'd be great. I remember something along those lines but the trouble is that the title function is optional.\n\nAlso, as a passing question: is there interest in porting ava to TypeScript?\n\nYes on the one hand, no on the \"that sounds like a lot of work and code churn that maybe we shouldn't deal with right now\" hand.. Ha! I always overlook badges. Oops!. Thanks for picking this up @wmik. I've made some minor changes, what do you think?\nI've also updated some other documentation.. @miguelsolano are you still interested in working on this?. Closing due to inactivity.. TypeScript isn't transpiling ES Modules, and out of the box Node.js doesn't support ESM. I'd combine this with esm, see our recipe: https://github.com/avajs/ava/blob/master/docs/recipes/es-modules.md\nSince you're not using Babel all you have to do is install esm and add it to the require config.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). > 'merge_anything_1.default is not a function'\nThis looks like a runtime error coming from your compiled TypeScript. That's not something AVA is responsible for.\nLooking at merge-anything, the \"main\" points at a CJS file, not an ESM file. I think you might have to use import merge = require('merge-anything') instead.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Babel isn't touching your test files though. Presumably they're being loaded by ts-node/register. You should be able to reproduce this outside of AVA.. Can you reproduce this without using AVA? The way you've set it up it doesn't change module loading at all so this should be independent of AVA.. Works for me:\n```console\n\u276f npx ava -v test.js\n_0\n_1\n_2\n  \u2714 test wait (3s)\n1 test passed\n```\nNote that AVA writes any console calls to stderr, not stdout.\nWhat OS, shell and terminal app are you using?\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Hmm. I'm the same, except for Zsh. I don't know why this would be happening, sorry.. Aha!\nHere's a minimal reproduction:\n```js\nimport test from 'ava'\nimport delay from 'delay'\ntest('slow log', async t => {\n  console.log('_0')\n  await delay(1000)\n  console.log('_1')\n  await delay(2000)\n  console.log('_2')\n  t.pass()\n})\ntest('other test', t => {\n  t.pass()\n})\n```\nWith the default (mini) reporter:\n```console\n$ npx ava test.js\n\u2827 _0\n\u2819 other test\n\u2827 other test\n2 tests passed\n```\nWith the verbose reporter:\n```console\n$ npx ava test.js -v\n_0\n  \u2714 other test\n_1\n_2\n  \u2714 slow log (3s)\n2 tests passed\n```\nThe mini reporter gets tripped up by the console output. I think that may be a regression cause I distinctly remember testing for this. See https://github.com/avajs/ava/blob/ff09749e936fb005155fff715b0325a0b02a36c1/lib/reporters/mini.js#L197:L216.\nThanks for persisting, @mesqueeb \ud83d\ude04 . > Does this handle babel.config.js? I'm asking since that file has different semantics from the old babelrc.\nThat's my next PR.\nBabel handles the semantics for us. All we need to do with regards to caching is include the file contents in the hash.. I've added the Flow typing too.. Power Assert relies on compileEnhancements, which you can't use because you're using TypeScript.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). I'm not quite sure how we'd combine the two. Perhaps something to revisit once we have more integrated TypeScript support.. It's odd. When you run the standalone script there's build output, but not when it's run through AVA. I think Nuxt is acting a little different, but I'm not sure why.\nFor instance, the port 4000 is definitely open but it's not responding even if I curl it directly.\nIf there's something wrong with the test environment we'd be happy to fix it. That said though, given what Nuxt is doing I'd recommend starting it outside of AVA, not from inside a test file.\nClosing this for housekeeping purposes, but again if somebody can figure out if there's something in the test environment that throws off Nuxt we'd be happy to fix it.\n. @p1pchenk0 the simplest approach would be to assume the build server is running. You should be able to script it in CI flows. It's what I do with database or queue servers.. Heh, turns out GitHub did post some of my attempts to comment. And when I deleted the copies, it aggressively deleted the last one too\u2026 so one last time:\nI need to update the type definitions to make it clear that the first argument to the title function may now be undefined.. You could also count how many connects are still pending, decrement on each connect and within that callback if you hit zero, call t.end().\nNone of this helps if a callback is invoked twice though.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). You could also count how many connects are still pending, decrement on each connect and within that callback if you hit zero, call t.end().\nNone of this helps if a callback is invoked twice though.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). I think that's functionality one could add atop of AVA once #1692 lands.. As long as you can import an npm library and drive your tests from a Node.js process, you can probably use AVA.. I think our macro functions cover this use case: https://github.com/avajs/ava#test-macros\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Could you elaborate what you're proposing with each1 and each2?. You could achieve that with a for loop, which I find this easier to understand:\njs\nfor (const [expression, result] of [\n  ['2 + 2', 4],\n  ['2 * 3', 6],\n  ['2 - 2', 0],\n  ['4 / 2', 2],\n  ['2 * 16', 32],\n  ['2 + 8 * 2', 18]\n]) {\n  test(`${expression} = ${result}`, macro, expression, result)\n}. @babel/register doesn't consult AVA's configuration. You haven't configured how it should compile the source files. Thus you get a syntax error when you try to load a source file (through @babel/register). It doesn't get compiled so the JSX syntax ends up breaking it.\nUsing .babelrc or babel.config.js is the correct approach.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). @qlonik wow that's brilliant! I managed to solve the zero parameter case with another overload. We should support it.\nHave a look at my commit, what do you think?\n. > 1. In the case of overloads, TS requires to have most precise/specific type on the top followed by less specific types. I'm not sure if the function with 3 arguments is more specific than the one with 2 arguments or vice versa. Because of this, functions might need to be reordered in the TestInterface (and others)\n@qlonik I don't think that's the case with interfaces, rather it only matters when defining multiple overloads followed by the implementation.\nRegarding the Implementation type, I think that's more stylistic than anything. We explain macros as being \"a bit more\" than a regular test implementation, even though there are few practical differences. It's definitely valid to write a macro that takes no additional arguments and does not implement the title function, just as it's valid to provide an inline \"implementation\" that takes the additional arguments (which makes it a macro, of course).\nI'd prefer to keep the type definitions as they are so that the (perhaps more common) use case of inline implementations is more easily discerned.. > What about the case of untyped macros functions being passed to the test? As shown in my last message. Those do not pass type check. So what should be the recommendation, for people using test function like that?\nThey seem to work fine, as long as you type the t argument. I think that's OK. Will add it to the documentation.. > Will add it to the documentation.\nOr actually, this is such an edge-case, perhaps not.. @sindresorhus any thoughts?. @gajus that's an interesting approach for sure. I'd be careful to disable AVA's Babel transforms since they'll suddenly also impact your source code. \n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). With the above setup, ts-node/register is compiling your TypeScript files. I suppose it should compile import * as config from '@config/common' to import * as config from '../configuration/common' or something. Hard to say why it isn't, but I don't think this is related to AVA itself. You should be able to reproduce this in a separate script, e.g. node -r ts-node/register repro.js where repro.js requires a TypeScript file that then has a @config dependency.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). @havenchyk thanks for the added detail. Would you like to open a PR to add that to our TypeScript recipe?. Bizarrely, the Push build passed, and the PR build failed due to the reporter logs including crlf-style linebreaks.. > Bizarrely, the Push build passed, and the PR build failed due to the reporter logs including crlf-style linebreaks.\nGoing to merge and hope for the best.. Yea, I've had a similar use case, but for loading fixtures after resolving a source map.\nNot sure about attaching it to the per-test execution context. Maybe it can be available through the 'ava' import somehow. import { filePath } from 'ava' and test.filePath?. > Wouldn't import { filePath } from 'ava be tricky to implement since it has to be statically defined? Or is AVA compiled per-process? Should be interesting. :)\nAVA loads the test file, so it can set that up in advance.. Thinking about this more, perhaps test.meta.file would be better.. Hi @coreyfarrell. I've been a busy with life / work and trying to finish the 1.0 release. Please bear with me, we can ship things like this a lot more easily once the 1.0 is done.. Awesome, thanks @coreyfarrell!. Could you try configuring @babel/register so it ignores the test files? See the last example in https://github.com/avajs/ava/blob/master/docs/recipes/babel.md#compile-sources.\nI suspect the double compilation may be causing trouble. Perhaps AVA loads its internal source map and the one from @babel/register is ignored. Or perhaps both @babel/register and AVA are trying to set up source map handling and it's failing\u2026 unfortunately this stuff is really hard to get right.. > 1. ignore: ['src/test_*'] doesn't work\nThat may need a file extension.\n\nSeems that the docs are wrong about how to configure @babel/register. Would you accept a PR on the docs?\n\nAlways!\n\nErrors in the source file are still reported on the wrong line (a failing assert in the test file is correctly reported). I tried disabling sourceMaps in babel/register, but it still doesn't work.\n\nOK. It may be a little while until I have time to look into this further.. The --sources flag doesn't exist\u2026 Try setting the DEBUG environment variable so it logs the file changes: https://github.com/avajs/ava/blob/master/docs/recipes/watch-mode.md#debugging\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). @sindresorhus pushed changes.. You could\u2026 but that's an internal API and may break without warning.\nCould you elaborate on your use case?. https://github.com/avajs/ava/pull/1977 introduces a meta object we could hang this off. We should implement a memoizing getter for snapshotDir since it's takes a bunch of disk operations to compute.\nI'm not sure what to do about the update flag. I can see the value in exposing it but I'm concerned it may prevent us from building interactive update UIs later. Though perhaps at that point we won't expose it if run interactively.\nThere's other open issues to stop AVA from creating new snapshots in CI that may impact this. What do you think?. > You could expose the raw command line arguments that were passed to ava.\nAVA actually forwards all arguments after a --. So if you do npx ava my.test.js -- --update-visual-snapshots you could use that to control some custom snapshotting module.. Nice find @jamiebuilds. Turns out the same problem existed in the TypeScript definition.. Yea AVA doesn't do this. Watch mode merely notes which source files are depended on my test files, and uses that to re-run specific test files.\nIt should be possible to do this outside of AVA by analyzing the dependency tree to select the test files to run. It'd be neat if AVA could do the same to prioritize which tests to run.\nWould be great if somebody could spec out how this should work and we can take it from there.. @AgentBurgundy you can already pass specific file paths to AVA, so you could do this without changing AVA itself.. This is too much of an edge-case for how we see snapshots being used, sorry.\nIt sounds like you may not need the readable files at all, given that the data is perhaps generated elsewhere. You could ignore those files from source control, you only need the .snap files committed for use in CI.. There's some features that should just be configured once and for all, not constantly tweaked through CLI flags.. > Wouldn't it simplify the code to not special-case the configuration?\nWith say the require option, it makes it easier to know which files are required in which order. Specifying different types of glob patterns on the CLI can easily lead to confusion.. I can see the utility, but I'm not sure about the actual use cases. @Siilwyn in your example you have promise code, which is better served with async / await. Assertions also take an optional final assertion message, which makes passing them to array functions impossible (since a number will be passed to the message, which if we're not already throwing on we should throw on).. Ah, I missed the ava/curried import. I was thinking about having the assertion method count the number of arguments to return a curried function instead. That won't work though with messages.\nI don't think we should ship something like ava/curried. It makes writing helpers & macros much more difficult, since you don't know what kind of t object you'll receive. It's something that could be done in a third-party library.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Good catch!. Thank you @itskolli!. Not sure, perhaps something to do with npm link or your local setup that changes the ESLint rules for that file? Could disabling ESLint altogether maybe, just in that file.. Not sure, I guess the import plugin ends up working a little differently in CI. Funny enough eslint-comments/no-unused-disable cannot itself be disabled, which would have been one way out. Instead I've ignored the failing file.. Yup that fixed it. Remaining failure because npm@6.6.0 churns the lockfile. Will address that separately.. Agreed, see #909. I've been meaning to summarize my thoughts on our various globbing issues into a concrete proposal but haven't gotten round to it yet.. The stack traces shown are fine, but we also show where the assertion took place. Compare:\n\n\nI think we should show the code snippet for t.throws() not new Error().. Thanks @dflupu!. A PR would be great!. Ah I thought I saw the PR, but then it didn't show up in the links here. Turns out the reference to this issue was incorrect (I've since fixed that). Thanks @niktekusho!. Ah yes it should be added to https://github.com/avajs/ava/blob/master/docs/06-configuration.md. Thanks for noticing, @niftylettuce.. That'd be great @Chrisyee22!. Thanks for the kind words, @incompl!\nTo me, \"drag\" is pretty low on the unpleasantness scale. And yet, I'm never quite itching to write tests. It's not really on the fun scale either. But perhaps I'm too cynical.\nI'm curious to hear what other's experiences are.. We could leave it up to your own interpretation:\n\nTesting. AVA helps you get it done.\n\n@incompl may read this as \"Testing! Yay, AVA helps me get it done\" and I can read it as \"Testing\u2026 AVA helps me get it done.\". What is the underlying cause of this problem? Is it AVA executing multiple test files at once?\nDisabling allowJs seems like a bad workaround. Perhaps concurrency should be limited to 1 instead if you're using allowJs.. Closing due to inactivity.. One thing that jumps out is that you're loading esm, but it doesn't look like you've disabled Babel. AVA will still be compiling ESM syntax, so esm won't have much to do\u2026\nSee https://github.com/avajs/ava/blob/master/docs/recipes/babel.md#preserve-es-module-syntax and https://github.com/avajs/ava/blob/master/docs/recipes/es-modules.md. Try disabling the ESM compilation and see if that helps.. Could you be more specific which any's you'd like to replace? For our assertions, any seems more appropriate since we're typing what kinds of values you can provide. unknown is useful as a way to force yourself to refine the type of value you received.. > Actually, looking at the type definition in more detail now, I see that there are no any being returned. I was sure there was. Maybe there was before 1.0.\nWe used it for the return values of throws / throwsAsync but we've typed those as returning an Error now.\nI still think any is more self-documenting for those calling our assertions than unknown would be.. Because it literally says you can pass any value.. I think it might be caused by the combination of nyc and esm. It works great if those tools aren't using cached transforms (like in CI) but can fail locally if you run concurrent test files. Try with the -c 1 option.\n\nI did find if I did a git reset --hard which shouldn't affect anything fixed ava?\n\nAVA, nyc and esm cache files in node_modules/.cache. Try deleting that directory instead.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Oh fascinating!\nWe're passing options to the worker processes as a command line argument. I think it ends up being too large. The line is here: https://github.com/avajs/ava/blob/010914b9e5232312d2b611695776919084a65717/api.js#L232\nThis includes all precompiled files which in your case may be quite a lot. There may be other unnecessary data as well. I'll have a think about how else we can share these options.. @bongofury could you give #2035 a try? npm install --save-dev avajs/ava#config-over-ipc.. @bongofury great. This is out now in v1.2.1.. You're probably best off running AVA twice, e.g. in the npm scripts:\njson\n{\n  \"test:parallel\": \"ava test/parallel/**/*.js\",\n  \"test:stress\": \"ava test/stress/**/*.js\",\n  \"test\": \"npm run -s test:parallel && npm run -s test:stress\"\n}\nIt's not great because you get two sets of results, but it'll work in CI where you just want all tests to pass anyway.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Thanks @itaisteinherz!. Thanks @jdalton!. You should use a Babel plugin to rewrite those imports, so that Node.js doesn't try to import them as if they were JavaScript. I don't have the right plugin handy for you though.\nAlternatively you could try and run your Webpack bundle within AVA but I'm not quite sure how to go about that either.\nIf anybody has tackled this before, it'd be great to update the documentation to solve this problem.. > I think this should be somewhere in docs, or better add support for something like ignoreExtensions to configuration so that AVAs internal babel pipeline can ignore some modules (because this solution works only with @babel/register).\nAVA doesn't follow your imports, it just rewrites them to require() calls. Node.js then loads the files and assumes they're JavaScript.. Your code, as written, assumes a Webpack build step. If you want it to work in Node.js you'll need to adjust your approach. Pre-compiling using Babel, or @babel/register, or perhaps you can test your Webpack bundle itself.. I think the title is fine \ud83d\ude04 . Thanks for reporting this @mtfurlan. I've also seen this happen.. Would we shim these as no-ops?. Yes please, @okyantoro!. Nice find, thanks @kagawagao!. Yea that does look cumbersome.\nI'm pretty sure we do take babel.extensions into account when looking for files, however you have custom patterns for the test files. I can't really think of a reason we shouldn't also look for .spec.js files. @sindresorhus?\nI think you should also be able to specify src/ as the source pattern and AVA should select all files with the right extensions within. Or if it doesn't work that way then it should.\nI hadn't realized @babel/register required the dots. I wonder if in hindsight that would have been a better choice. I'm reluctant to support both but we can change over in the next major version. @sindresorhus?. It seems odd to test for either result. Ideally you have one test for a 200 response, and another for a 302 response.\nIf you do have such variability in your test, I would use t.true(res.statusCode === 200 || res.statusCode === 302). And perhaps an if () {} condition before comparing the result.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Yea. I need to write up my thoughts on how to merge the verbose and mini reporters and what we want out of that new default reporter\u2026. Hi @mesqueeb, I can reproduce this on your repo, but I don't understand where the error is coming from. However if I remove the package-lock.json and node_modules, then run npm install again, the problem goes away. I think there's a faulty or outdated dependency somewhere in your package lock.\nP.S. You don't need to manually install @ava/babel-preset-stage-4 and @babel/plugin-proposal-object-rest-spread. And @babel/preset-env doesn't seem to be used.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Within a test file there's plenty of Node.js modules to help you achieve this. I assume you want to coordinate resources across test files. Could you elaborate on that use case?. I think this is a duplicate of https://github.com/avajs/ava/issues/1366. @be5invis what do you feel about moving the discussion there?\nIn order to make progress on this we need to flesh out use cases further, and make the argument for why this can't be done by other modules building on top of AVA. Our proposal process is lacking at the moment, so for now bullet points in GitHub comments will suffice.. Yes but what if we can find a solution that works for both? Roughly, if you could start a \"resource manager\" that can communicate with test workers, you could use that to share random URLs or mutex states.. I'm interested in solving the commonalities between those issues, so that specific solutions can be built on top of that.. Nice find @lo1tuma!\nYou're right, the dependency tracker works by decorating the functions in require.extensions. Thus, it misses any additional functions added when AVA loads the modules from its require configuration.\nWhat we should do is after each module is loaded, go through require.extensions again to decorate any now undecorated functions. That way we can track most of the dependencies.. See these lines which installs the tracker for the first time:\nhttps://github.com/avajs/ava/blob/334e15b4af06492c9aed2800a0764f245d6a908b/lib/worker/subprocess.js#L104:L107\nNote that the comment applies to precompilerHook.install().\nThen, the module is loaded here:\nhttps://github.com/avajs/ava/blob/334e15b4af06492c9aed2800a0764f245d6a908b/lib/worker/subprocess.js#L111. Thanks @KompKK!. Thanks for making the PR @nklayman. It may take me a little while to get round to reviewing this and considering how we'd publish to https://zeit.co/now.. > I don't know why node 8 on windows fails.. Have you got any ideas?\nLooks like an intermittent failure. I've restarted it.. Thanks @KompKK!. Wow, nice find! Looks like this never worked since number code expectations where introduced in #1902.\nSome of the tests can be a little flaky, that's probably what's happened here.\nI'm pushing a simplification (there's no need to check code since at this point the expectations object has already been validated). If CI passes I'll ship this. Thank you @qlonik!. Sure, thanks @SleeplessByte.. This looks like a bug to me.\nI think this line: https://github.com/avajs/ava/blob/d97f11ddbbf824fd72165266770b9ae046e62324/lib/run-status.js#L62\nShould be moved to this else branch:\nhttps://github.com/avajs/ava/blob/d97f11ddbbf824fd72165266770b9ae046e62324/lib/run-status.js#L86. > I can take a look\n@vancouverwill great!\n\nI wouldn't mind adding the pending tests line writePendingTests() to tap as it is pretty major feature not to show as the main goal of a test suite is to show the tests which have not passed.\n\nSure, if you think that's useful. Note that this is only used when a timeout occurs or if you interrupt the test run. You may have to update the reporter code as well. Is there a good way of expressing that in TAP? (This is probably best as a separate PR.). Looks great @eemed. Would you want to have a stab at updating the documentation?. > Do I just commit doc updates to this pr aswell?\nYep!. @eemed I've pushed some documentation updates, let me know what you think.\nLet's hold off merging until we can address https://github.com/avajs/eslint-plugin-ava/issues/218. Perhaps you'd like to tackle that one too? \ud83d\ude04 . Yup, see https://github.com/avajs/ava/issues/1857.\nRe-reading that issue I see that my questions in https://github.com/avajs/ava/issues/1857#issuecomment-402513251 have been resolved (https://github.com/avajs/ava/issues/1857#issuecomment-405091537), so it's ready to be worked on.. No it still allows --config, it's just that it also allows package.json#ava.config. (Happy to accept a PR just for the CLI flag, by the way.). You could do that detection in an asynchronous test.before(async () => { await startup() }) hook.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). I can see it being useful if the watcher picks up on files written by another process, and those files are written more slowly than it currently allows for. I don't recall anybody reporting such problems, though.. Thanks @vancouverwill!. > The alias plugin is looking for require statements, and since your ES6 imports haven't yet been transformed to require calls, no alias resolution takes place.\nThe joy of Node.js-style require() calls and ES modules \ud83d\ude04 You could transform the imports earlier by adding the necessary Babel plugin.\nWill leave this open as it sounds like our Vue recipe could be improved. @sprguillen @aparajita either of you interested in opening a PR with changes?. Thanks @amokmen!. Thanks @okyantoro!. Yes this is most definitely welcome!. We have --tap output which is parseable by other programs. I like the idea of an integration you can load into the main process, which can then report progress to services like these.\n(Note that this would not be a custom CLI reporter.)\nIs this something you want to investigate implementing, @arestov?. This sounds like there's a memory leak when running watch mode. That's definitely a bug.. I think there's a bug with the \"previous failure in test files that were not rerun\". I've also noticed it showing even though I resolved all failures.\n\nDid you know you could press r + enter and AVA reruns all test files?. Yes that makes sense to me.. Thanks for the PR @IgnusG.\nThis isn't esm specific, is it? I'm not sure how much value there is in documenting this in AVA itself if it's just adding it to AVA's require option.. I think it should work if you change that value so that it points at your actual AVA installation.\n(I'm closing this issue for housekeeping purposes, but let's keep the conversation going.). Throw if any of these are empty? This isn't a documented Node API so people probably copy/paste from other require hooks and will dutifully pass the same filename but if they don't it may be quite tricky to debug.\n. Interestingly even without the --compiler flag being used for .js extensions this would load the default compiler. You could fall back to the .js compiler and never readFileSync the source above.\n. > That means, really only --require is needed until we add watch support ourselves.\nI'd prefer --require. It's easier to reason about when writing your own test bootstrap code and still works with other require hooks.\nMocha's watcher does weird things when it reloads the tests so I've just been using nodemon to restart the process entirely. I'm not convinced watching specific extensions is even necessary.\n. > I'm leery of mocking an undocumented API this way.\nnyc mocks it but ignores the second argument to _compile().\n\nI have not yet seen an extension that uses anything beyond _compile.\n\nProxyquire does but that wouldn't impact the test file.\n. I assume you're advocating deduping so that if the code-under-test requires babel-core it doesn't load two versions? If so, is that really worth mentioning?\n. Should the example use ^6 instead?\n. Ah, thanks for elaborating. I did not find that obvious from the documentation though. Same with the version pinning below which follows a note on Babel 6 support.\n. > Since we depend on a loose Babel 5 version, the user can set an exact version and it will be deduped to the user's choice.\nThis just doesn't seem very relevant to me. AVA uses Babel, fine. The version may change, even in patch releases, as long as the test behavior remains the same. Why should a user be concerned with setting an exact version in the project in order to influence the version AVA uses?\nWhat I find more important as a user is knowing that I can install whatever Babel version I like and it doesn't break AVA. \n. > I think it undersells the best part of this change: that you can now use Babel 6.\n:100: \n. I think no chokidar code should be run unless cli.flags.watch is true. Currently if it's false it means Chokidar probably won't emit changes, but I don't find the documentation entirely clear on this.\n. You may be able to specify the cwd option and then keep these patterns relative. Might not hurt excluding other VCS paths either.\n. Since the tests run asynchronously, you may end up starting a new batch before the previous execution has finished. It doesn't look like api.run() has any logic to defend against that (which it wouldn't have needed before).\nAlso filepath is unused.\n. IMO you should place this guard in the cli module, the logger shouldn't have to know about watchers.\n. Heh, old habits die hard. Will change to setImmediate(). It's needed to rethrow as an uncaught exception.\n. Ah yes, will remove, thanks.\n. NODE_PATH can contain multiple values. Not sure if the env var needs to be split and concatenated with ret here. See https://nodejs.org/api/modules.html#modules_loading_from_the_global_folders for more.\n. This isn't necessarily the project root. Perhaps parentWorkingDirectory? Bit verbose though.\n. Perhaps use is-windows. A separate package is likely preferable over inline regex fu.\n\n\nIf the NODE_PATH environment variable is set to a colon-delimited list of absolute paths, then Node.js will search those paths for modules if they are not found elsewhere. (Note: On Windows, NODE_PATH is delimited by semicolons instead of colons.)\n\n(source)\nThe non-Windows separators are colons, not commas.\n. Should use path.resolve in case additionalPath is already absolute.\n. I think it is, but I don't have final say ;-)\n. Sure\n. No worries! This stuff is maddeningly difficult.\n. Heh yea, fair enough.\n. Use process.platform === 'win32' though rather than a regular expression.\n. Object.assign() already takes care of mixing in the opts.\n. Could replace with:\njs\nreturn ret.concat(nodeModulesDir, additionalPaths);\n. process.cwd() rather than opts.projectRoot right?\nPerhaps nodePaths as the variable / option name? And personally I'd do something like:\njs\nvar nodePaths\nif (process.env.NODE_PATH) {\n    var osSplitChar = process.platform === 'win32' ? ';' : ':';\n    nodePaths = process.env.NODE_PATH.split(osSplitChar).map(function (p) {\n        return path.resolve(process.cwd(), p)\n    });\n} else {\n    nodePaths = []\n}\n. Yep :)\n. Could you insert a linebreak at the end of the file?\n. Needs a semi-colon after the }. And could you insert a linebreak at the end of the file?\n. Should test with multiple paths, using the correct separator based on platform.\n. Probably by adding an env argument after dirname and before cb. It should be optional though, no changes to the other tests should be necessary.\nCould leave dirname required if env is provided, so execCli([], { NODE_PATH: 'foo' }) is invalid.\n. Just realized that path.resolve(p) defaults to path.resolve(process.cwd(), p) if p is relative. In other words you don't need to provide process.cwd() here :boom: \n. @sindresorhus do you mean path.join('node-paths', 'modules')? path.delimiter is used for building the environment variable itself, which comprises multiple paths.\n. The test passes either way so you can't obviously tell which branch passed, hence the comment output. Though that still requires you to look at raw TAP so probably not that useful.\n. Node probably handles any platform issues with the slashes in node-paths/modules, so no need to change anything then.\n. Could use .concurrent and .serial in the collection as well, not sure it needs the tests container.\n. Perhaps change Sequence#run() to always return a promise. At the moment it usually returns a promise which seems odd.\n. Perhaps restore the // workaround for Babel giving anonymous functions a name comment. This looks like a magical string otherwise.\n. How about:\njs\nreturn [].concat(beforeHooks, this._buildTest(test, context), afterHooks)\n. I doubt that'll be noticeable given the cost of spawning a new process in order to run the tests.\n. Could compute this outside of the function so it can be reused for subsequent forks.\n. If you look at the overall diff you'll see that these lines were moved above this.excludePatterns. Would be nice to leave them undisturbed.\n. Not needed since this is already set in the constructor.\n. Not needed since cli.js takes care of setting defaults. The watcher also receives those defaults, which is what it should provide when calling run().\n. Since files is provided to the watcher you no longer need to assign it to this.files. Just use the variable.\n. The code shouldn't change options. cli.js already takes care of enabling explicitTitles when in watch mode.\n. Would be nice to keep {exludePatterns: []} here, even though the test doesn't fail without it.\n. I suppose technically this doesn't have to be restored before each test, but it won't do any harm either. If you do want to move it you should add it to the api declaration https://github.com/sotojuan/ava/blob/1d6d84442b99407ae0c329a48c75125ad6a1e4f9/test/watcher.js#L42:L44\n. Adding event here is unnecessary.\n. Why did you comment these out? They pass on my machine.\n. This test should be preserved. It makes sure test titles aren't prefixed when testing a single file.\nThe test title prefixes \u2014 single file (explicit) will cover behavior of the explicitTitles option.\n. No need to set this option in this test, it's meant to test failFast.\n. Need to remove this now.\n. Might be useful to link to a background explanation for this. I found https://github.com/Microsoft/TypeScript/issues/2242#issuecomment-83694181, not sure if there's official documentation for this behavior elsewhere?\ntl;dr TypeScript's behavior doesn't seem without merit, and explicitly exporting a default here should be okay.\n. Perhaps do:\nvar directive = '';\nif (test.todo) {\n  directive = '# TODO';\n} else if (test.skipped) {\n  directive = '# SKIP';\n}\nWhich simplifies the todoDirective || skipDirective || '' statement below and removes the unnecessary || '' fall-through.\n. I'd prefer a single if (opts.todo) {}, then nest the title check. Took me a while to figure out fn = noop was used when title was not a function :smile: \n. The error message could be more informative. Perhaps TODO tests need a title or something along those lines.\n. It might be better to not set skipped: true here. That would require further clauses where skipped is used to determine whether the test is skipped, but it also removes exceptions where you have to check for !todo.\n@vdemedes thoughts?\n. If I'm understanding things correctly this needs to be changed to reflect test.todo right?\n. If we're sticking to having both skipped and todo in the metadata then you'll have to add skipped: true here.\n. Could you add a comment as to why the function is returning here? It's because if we're running exclusive tests, but the test file does not include exclusive tests, then we don't want those tests to be counted, right?\n. Similarly why is this being reset? Because we've already counted tests we're now not going to run, right? I appreciate test-collection has similar logic without comments but I reckon it'd be easier to understand for the next person if we'd explain it here.\n. Maybe pass runOnlyExclusive to runner.run()? It can still create its stats object inside the run() method, but bail out if it has no exclusive tests.\n. master moves the files argument to api.run(files). You'll have to rebase this branch, sorry :)\n. is-promise only checks for then() methods though, yet the promise chain below assumes result is actually a Bluebird promise. That's probably guaranteed given the rest of the code but feels like a refactoring hazard.\nWhy not always use Promise.resolve(result) and the chain below? Or is that too expensive performance wise? I couldn't immediately find a \"is this a Bluebird promise\" function in their docs, but maybe result instanceof Promise will work for us?\n. Nah it's good enough as is, just requires some extra thinking.\n. Change test to hook? Perhaps cannot over can not?\n\"only\" cannot be used with a before hook etc.\n. Could do return (tests || []).map() below.\n. Should initialize to null so removeEmptyProps({}) is consistent with removeEmptyProps([]).\n. Similar issue in https://github.com/sindresorhus/ava/pull/542#issuecomment-189892779.\nAt the risk of splitting up the discussion too much, one thing I've done in the past is to return a wrapper result from mixed synchronicity functions, either with a promise property or a value property. Then you can do if (result.promise) without having to do further type checking.\n. This should be test.skip, from https://github.com/BarryThePenguin/ava/blob/bb3c972cc4df4b3df0139b5d6f738cb18da0e6e7/lib/runner.js#L99.\n. You still need to increase the todoCount, as well as initialize it to 0 in the run() method. Then in https://github.com/sindresorhus/ava/blob/0e1886582833986b896c31859aa61d8da404e996/api.js#L233 you need to compute the total todoCount. These counts are mocked in the reporter tests.\n. Oh and here https://github.com/sindresorhus/ava/blob/0e1886582833986b896c31859aa61d8da404e996/api.js#L207.\n. I guess on the one hand todo tests should be skipped, but on the other hand they should not be counted as being skipped. I'm not even sure whether they should be counted as tests now!\nNot annotating the test as skipped requires explicit guards in the runner where it decides whether to skip a test, but also reduces skipped && !todo checks where doing skip counts. Bit of a mixed bag, personally I prefer the more explicit approach.\n@vdemedes @sindresorhus?\n. If only we could use parameter defaults :wink: \n. errMsg is the expected message. The workaround above stems from #125 and was done because assert.throws doesn't take a string argument as the expected message, but we did want that behavior in AVA.\nIn other words it's not something to be returned.\n. This means fn() is invoked twice. Instead I think you need something like:\njs\nvar result;\nassert.throws(function () {\n  try {\n    fn();\n  } catch (error) {\n    result = error;\n    throw error;\n  }\n}, err, msg);\nreturn result;\n. > Returns the error thrown by function or the rejection reason of promise.\n. I'd prefer a strict identity check:\njs\nvar expected = new Error();\nvar actual = assert.throws(function () {\n  throw expected;\n});\nt.equal(actual, expected);\nAlso note that you don't need the t.doesNotThrow() wrapper, just run a single t.equal() assertion. No need at all for the next test.\n. Yes, you're returning a very different value here.\n. No need for the empty object here, baseOptions is already fresh so it's safe to mix other properties into it.\n. \"inherit\" to be consistent with \"default\" below?\n. Some nits:\n- This needs a heading.\n- The README isn't entirely consistent but I'd prefer package.json (with the backticks). Same for .babelrc.\n- It's named \"ava section\" (without quotes) earlier: https://github.com/spudly/ava/blob/f928959b9ad106a70fc3091a462ff3d96a60f469/readme.md#configuration\n- Could you capitalize Babel throughout?\n. Yea. No need to pass any secondary arguments either, the existing tests don't seem to bother with that either.\n. Rather than checking for a falsy value here you could define the default in cli.js. Change this line to:\njs\nvar conf = pkgConf.sync('ava', {\n    defaults: {\n        babel: 'default'\n    }\n});\nThis will also save you a test :sunglasses: \n. Unnecessary diff churn\n. AVA's tests are run using tap which runs each test file in its own process. You don't have to worry about modifying the babel-core module, or about cleaning it up afterwards. Just add the spy in the module itself, outside of a test.\n. You no longer need the _reset() method now that run is always called.\n. Given that this is an internal method we should assume it's always called with an options object. You'll have to update test/fork.js though. Maybe some other places too.\n. I like this approach! Seems cleaner than the this.hasExclusive approach on top, though it can't fix the testCount. Maybe the test count should be incremented here as well and we can do away with _handleStats?\nNote though that tryRun is also called when on test.catch(). You'd have to wrap it there to avoid passing the error as the stats object and then add a guard for stats being undefined.\nOr maybe stick with the _handleStats() approach. Just not both :wink: \n. Just use eventOptions here I think. And rename it to options.\n. returns rather than becomes. Could lowercase Error as well.\n. This should work:\njs\nassert.throws(Promise.reject(expected)).then(function (actual) {\nassert.throws() already returns a promise because you pass one rather than a function. That promise fulfils with the rejection reason.\n. There are.\n. Remove the if-guard as well, simply throw fnError. This makes it equivalent to t.throws(() => { throw 'foo' }).\n. Perhaps:\n\nCreate a helper file and place it in the test/helpers folder. This ensures AVA does not treat it as a test.\n. Oops wait. assert.same() means \"deep equal\". We want strict equality here. You should use assert.is(actual, expected) instead. Though the tests here are for AVA's assert module. Perhaps use t.is(actual, expected) which is from tap's assertion library (AVA's tests are run using tap).\n\nSame below.\n. Sure. It's going to conflict with #576 though :wink: \n. Should pass 'default' here (and below).\n. Sure, but then why not use self.hasExclusive here?\n. Yes. Tests aside there's only one place the \"run chain\" is called and it's from the Api.\n. Nah there's no clear consensus either way. It's fine as it is, just different trade-offs in different areas.\n. You need to initialize todoCount to 0 in the Runner#run() method.\n. Need to add an assertion for the todoCount.\n. Once option forwarding is fixed you'll always have a value here. However you must make sure to only run the next line if this._match is not empty. By default it will be.\n. This function is also called when adding hooks, which themselves can be titled. Would be weird to skip those. I think you should be able to guard against that by checking opts.type === 'test'.\n. Could you add a configured plugin on line 98, and then verify not the plugins length but the expected values? Would be good to test that the configured plugins are preserved and the power-assert and runtime transform plugins are appended.\nCould then change the plugins test in the default case to match for those plugins too. Similarly the inherit case should check the plugins were added.\n. Try this, after you instantiate new CachingPrecompiler()\njs\nvar powerAssert = function () {};\nsinon.stub(precompiler, '_createEspowerPlugin').returns(powerAssert);\nAnd then compare against the powerAssert variable inside your test.\n. Reset to null since Node uses object refs?\n. Should this be currentStatus instead?\n. How about using var dots = require('cli-spinners').dots above and just referencing dots.frames/dots.interval?\n. Could abstract that into writeStatusWithSpinner() or something.\n. No need for the parentheses around the match check.\n. Why is this necessary? Presumably .only already works without needing this prop.\n. I think you should move this down below the Only-tests section. Maybe retitle to Running test that match a pattern. Should mention that this is more specific than .only.\n. Drop the sindresorhus/ part from the link title.\n. Maybe \"without an explicit title\".\n. Nice!\n. Need to retitle to Skipping tests to be consistent with the other titles.\n. Why this dependency?\n. Maybe typeof fn !== 'function'?\n. Could use the same error as in line 65 I think:\n\nExpected a function. Use test.todo() for tests without a function.\n. The '' seems unnecessary here.\n. Ah didn't realize there was already a currentStatus property.\n. Yea that should be fine.\n\nWould be good to have a test for this new behavior as well.\n. This is asynchronous whereas onForkStarting is not. Probably best to build up tests similarly (and synchronously) to how it was done before and leave the rest of the code as it is. (Note that I just changed this you so you'll need to rebase).\n. We should just bite the bullet and use .promise in the tests as well.\n. Perhaps extract this into a memoizing function? Less logic in this function and no need to recompute for every file.\n. I've wondered that too.\n. Shouldn't you keep the - prefix?\n. > How about we just not show them all together? So if a slow test is running, the last actual test is shown, not todo/skip.\nYep!\n. Yea. I'm just preallocating it to the right size.\n. We're splicing out, actually. It's just a weird API. It seems fairly clear to me that one branch adds the file, if necessary, and the other removes, if necessary.\nCould use the union() and diff() helpers if that would make it clearer?\n. Sure. Opted for runOnlyExclusive which is the same as the option passed to the runner.\n. Should truncate test.title and assign the result to var title, rather than reassigning title.\n. Presumably this will crash if babel is set to null. I'd prefer this to be more explicit:\njs\nif (!conf.babel || (typeof conf.babel !== 'object' && conf.babel !== 'default' && conf.babel !== 'inherit')) {\nNote that conf.babel should default to default so !conf.babel would only occur if it's explicitly set to a falsey value.\n. How about:\n\nUnexpected Babel configuration for AVA. See https://github.com/sindresorhus/ava#es2015-support for allowed values.\n\nEven if users don't recall where they configured Babel, the GitHub link will make that clear. I do think we should mention the \"for AVA\" part to make clear we're not talking about .babelrc stuff.\n. > That way it will fail when babel config is not even present.\nIt won't, it has a default value of \"default\".\n. Can remove the of inactivity bit I think. Should add ms.\n. Ah didn't read that closely enough :smile: \n. Huh, TBH I didn't try that because I just figured node wanted the exact file paths. Looks like it tries the various extensions instead. Neat!\n. > assert.throws will validate that the function indeed throws an error (a string) and thus does not jump to line 113.\nWhich is correct. Used like that you're only asserting something was thrown. You don't care what it is.\n. The expectation depends on the original err argument, be it a constructor, string, regular expression or validation function.\nI'm starting to think we need to reimplement assert.throws() here. It's too hard to format our assertion error without duplicating its internals.\n. Minor nit, maybe moduleId rather than module? module is in the file scope as the module object. resolve-cwd uses moduleId as well.\n. Ha of course. It implements the approach I was advocating. Missed it due to context switching :blush: \n. Is there any particular dependency order we're striving for?\n. Maybe inline this logic in _factory? CachingPrecompiler is really a wrapper around caching-transform. init() implies it needs to be explicitly initialized which isn't the case.\n. Is this a bugfix or harmless?\n. :clap: for making this more explicit.\n. @vdemedes bump :smile: \n. This is now unused.\n. Ah cool, that's the strategy I've been employing myself. I got thrown off here by the reordering of fs and path, looks like that plugin doesn't enforce any specific sorting within the categories?\n. Could just return status now.\n. Why this change?\n. Should combine this with the if-block below which also applies to opts.todo.\n. capitalizer\n. Should we add these now, or update later?\n. Should also add a link in the ES2015 section.\n. Try setting fn = noop after testing fn.\n. observeFork?\n. ?\n. No need to reset, you can just use .returns to change the value.\n. https://github.com/sindresorhus/ava/pull/713/files#r58517908 notwithstanding, it is indeed good practice to reset stub state in the beforeEach :smile: \n. Neat trick!\n. Well before it was concatenating with a newline. That's been removed so presumably you could just assign this.currentStatus = str?\n. convertSourceMap is used elsewhere in this file.\n. Yea would be good to centralize the pattern matching logic.\n. Yes much better, thanks.\n. Where would hasFlag() come from? I didn't look too closely at meow but I assumed I couldn't really see whether cli.flags.tap came from --tap or conf. Probably not a good idea to reparse argv either.\n\n... Or I'm just overthinking it.\n\n--tap would be superfluous if it's already in the package.json, and there won't be an error in the package.json anyway, so I don't think this is a big deal.\n. @jamestalmage you could add exclude patterns to files. We should just make sure to still use the default include patterns if files only contains negated patterns. That's how the watcher behaves with source patterns.\n. https://github.com/sindresorhus/ava/issues/736\n. Are all **/*.js files inside __tests__ tests? If so I think you just need to match **/__tests__ and AVA's built in directory recursion will take over.\n. Documentation wise it should be **/__tests__/**/*.js, just like the test directory in ava-files is explained as test/**/*.js.\n. Ah. What about\n\n--tap would be superfluous if it's already in the package.json, and there won't be an error in the package.json anyway, so I don't think this is a big deal.\n\n?\n. @jamestalmage did you mean to rename this to RunStatus?\n. Sure, I can do that.\n. Promote arrow functions?\n. Probably need to guard against err being null or undefined here?\n. Without checking the power-assert transformations, are there cases the Babel plugin can no longer catch?\n. Looks like it \n. > This is great when testing modules that do not need to be transpiled before you can publish them to npm.\n. The parentheses for the stage-2 link are messed up. Perhaps just use [stage-2] and define the links at the bottom?\n. Sure, but power-assert might rewrite the t.throws() argument expression such that it is computed outside of t.throws(). Again, I don't know if it does, but if it did then you wouldn't be able to catch it.\n. Yea fair enough.\n. Ha! I knew you'd written one! It didn't show up when searching for string repeat :smile: \n. Good catch!\n. The current approach is relatively clean. We could do something like that if we're going to move more of the watcher functionality into the API maybe, or when we have a clearer idea of what programmatic interface we need to present to editors plugins.\n. Sure. Would be nice to do that later though :)\n. It was inconsistent.\n. Sure, but that should be done as a separate task. fork emits more of these files so I figured I'd make it consistent with that first.\n. I'm trying to avoid implicit conversions. The vector is increasing so > 0 signifies that more than !== 0 would, but not fussed either way.\n. @jamestalmage I could also force the default in the constructor.\n. AVA's tests aren't transpiled, so you can't use arrow functions I'm afraid.\n. @sotojuan I think this is meant to test that deepEqual does not check the constructor?\n. Maybe add a comment that these tests are here to detect regressions when the underlying comparison library changes?\n. Walker?\n. Bad capitalization of Harmonize (my bad!)\n. Remind me, what are we looking to exclude here?\n. You're specifying test patterns though?\n. It's a trick question. Tests aren't sources, but currently the watcher deems everything that's not a test to be a source. It assumes Chokidar watches the union of tests and sources, and no more.\nisSource is used to filter test file dependencies.\n. Fixtures and helpers are sources, not tests.\n. That's out of scope for dependency detection. The watcher won't be able to determine which test file depends on that source, so it'll rerun all tests.\nFunny situation though if one test file requires a source, and the other fs.readFile's it. Only the first test file will be rerun.\n. It saves an assignment, I suppose.\n. Ah, I think this is just stubbing the default exclude or something.\n. Does it matter? The source patterns are likely to match test files, so the two sets may sometimes intersect, but depending on the config they won't always.\n. Yea I went commit by commit. It's clear in the full diff that this tests the defaults.\n. This is an optimization because we know we're dealing with file paths, so only on win32 do we need to convert the slashes.\n. I don't think so. slash itself doesn't know where the string comes from, just because it's not running on win32 doesn't mean the string doesn't contain slashes that should be replaced.\n. We tend to repeat the chalk.red() etc calls in the test, it makes it easier to change things later. You may also find the compareLineOutput helper useful. Here's an example: https://github.com/sindresorhus/ava/blob/7dd7055d360569b937515e907a5af22df07cbb22/test/reporters/mini.js#L268:L281\n. Could you add a test for this as well?\n. Please add a newline at the end of the file.\n. Perhaps we shouldn't document this option until it's solid. On the other hand that's what we tried with the watcher and without documenting it people won't find it.\n. Human linting nitpick, I'd prefer stripAnsi actually \ud83d\ude09 \n. title should suffice.\n. Should also use this in _runNoPool.\n. Maybe add a comment that _runNoPool is used to preserve .only behavior?\n. This probably shouldn't be emitted for every fork. Doesn't look like it's used for anything though.\n. --match only runs test with matching titles. Each fork will emit hasExclusive if it contains matching tests.\nThe logic here will give false warnings until a file is run that contains a matching test. Instead this exception should be handled once all forks have run and there were no matching tests.\n. Currently _generateRunner does a bit too much. I think once it's simplified it'll just fit here instead.\n. .then(resolve, reject). resolve won't throw, so we can save the extra promise allocation.\n. I'd be happy with results.filter(Boolean), not sure if there's an official stance on that @sindresorhus?\n. Seems clearer to reassign results = [], rather than repeat the processResults call and return statement.\n. How about:\n\nUse test.after.always() to register an after hook that is called even if tests or other hooks fail.\n. How about:\nUse test.afterEach.always() to register an after hook that is called even if other test hooks, or the test itself, fail.\n. guaranteed cleanup?\n. Perhaps:\nthis runs after all tests and other hooks, even if they failed\n. Perhaps:\nthis runs after each test and other test hooks, even if they failed\n. always (spelling)\n. Indentation should be a tab \ud83d\ude09 \n. OK, reverted to the original succeeded, you should form.\n\nESLint rule?\n. Did you mean failingCount?\n. Maybe rename \"failing\" to knownFailure / knownFailureCount.\n. \"known failures\" rather than \"failing\"?\n. Since the known failure count is shown after the number of passing tests, perhaps output these before the actual failures.\n. Maybe don't output a description until we can extract the links etc?\n. How about:\n``` js\nvar prefix = path.relative('.', file);\nif (prefix.indexOf(this.base) === 0) {\n  prefix = prefix.slice(this.base.length);\n}\nprefix = prefix\n  .replace(/.spec/, '')\n  // etc\n```\nSaves us a dependency and (to my eyes) is more obvious than the regular expression.\n. Might be better to add a filter after splitting:\njs\n.split(path.sep)\n.filter(function (component) {\n  return component !== '__tests__';\n})\n.join(separator);\nThat way you don't have to worry about the slashes.\n. I like the last one! Perhaps add a comment \"Only replace base if it's at the start\"?\n. @cgcgbcbc, @nfcampos is running into that as well: https://github.com/avajs/ava/pull/844#issuecomment-220554007.\nMaybe leave that for now, focus on the reporter tests (which use stubbed data anyway).\n. Should probably use a base that's a bit more than a /, e.g. foo/ with foo/bar/foo/baz.js, which should then result in 'bar' + sep + 'foo' + sep + 'baz' + sep (I think?).\n. failure should be pluralized, see https://github.com/cgcgbcbc/ava/blob/8d5a0c21583ae12ca5174e78651a0d72de181fbe/lib/reporters/mini.js#L144 for an example.\n. This needs to be indented with another space. For consistency with the other messages we should also add a tick or something at the start, perhaps use colors.error(figures.tick + ' ' + test.title)?\n. This doesn't seem to work for me when I run your code against a test file (will post the code in a PR comment).\n. This should probably just say \"2 known failures\". \"2 tests known failure\" is kinda weird \ud83d\ude04 \n. @nfcampos yes makes sense. So then you could test with bar/foo/baz.js right?\nprefixTitle replaces slashes so IMO it's not a great base value for this test.\n. Sorry I'm being silly. You're testing the exact case you set out to fix. \ud83d\udc4d \ud83d\udcaf \n. https://github.com/avajs/ava/commit/1d512315bc14a639155d0197b06d5ac966ba3770 \ud83d\ude09 \n. I would rephrase this though:\n\n\u2026 they are ideal for cleanup tasks. There are two exceptions to this however. If you use --fail-fast AVA will stop testing as soon as a failure occurs, and it won't run any hooks including the .always() hooks. Uncaught exceptions will crash your tests, possibly preventing .always() hooks from running.\n. This always seemed weird to me\u2026 but now that the variable is shorter why not conf === 'default' || conf === 'inherit'?\n. This should stub process.execArgv rather than needing a test-only option.\n. Yes, if they occur after .after has run. With asynchronous tests it's a race between a test completing and an uncaught exception occurring.\n. > do you mean that we should replace process.execArgv during test?\n\nYes.\n\nI think, it is not acceptable to touch global state.\n\nTrue, but tests are special.\nSomething I've done in other projects is to have a process module which exports the global process, and then stubbing that through Proxyquire. But let's not do that unless we really have to.\n. > I think, some day AVA tests will be also executed in parallel, so, it is not wise to \u0441hange properties in the global process object.\nWe can deal with it then.\n. \"you should return the promise\"\n. I wonder if we should advise promisifying the function. Assertion failures would throw, which depending on the async function's implementation may cause an uncaught exception.\nAlso you'd need t.end() in this example.\n. Maybe link these variables? It makes the link more descriptive.\n. Same here linking \"this issue\".\n. OK, but the next assertion could try to access a property of a null value and it'll still crash.\n. This should throw: new EnhancedAssertion() should be used instead. See Hook for an example.\n. Could you remove the .js in this require call?\n. Could you leave the whitespace as it was?\n. Other links are relative (see Recipes for instance).\n. Zero or more arguments can be provided to a macro. Rather than specifying input and expected I think this should just be ...args: any[] (not 100% certain on the syntax).\n. > Or is test just passing all arguments to the macro?\nIt is.\n. pkgDir seems good to me. IMO cwd should equal process.cwd().\n. \ud83d\udc4d . No need to actually change the working directory.\n. Check if pkgConf.filepath(conf) returns null instead. If it does, use process.cwd(), else call path.dirname() with the return value.\n. I think @sindresorhus is saying that Date.now() is accurate enough. I think that's true for this use case, no point optimizing test runs down to the nanosecond, and there is more cognitive overhead with hrtime.\nThat said I don't think there's much value in tracking test duration when in watch mode, especially since the number of test files run can vary greatly.\n. Before, cli.flags.require would default to the config from package.json, through this line. Api still needs the require option, so this should now be require: arrify(conf.require),.\n. This test doesn't do what you think it does.\nThe apiCreator executor doesn't actually set the pkgDir option when it creates the api. This means that the child process is forked with an undefined cwd option, so it ends up using process.cwd(). Because we run tests from the project root that happens to be the expected directory, but that's just luck.\nInstead let's find the correct pkgDir value in the test itself, and provide it when creating the API on this line and this one.\nThe next test is good (since it tests a non-default value). Just make sure not to accidentally break it with the aforementioned changes.\n. How about simply:\n\nAVA tries to run test files with their current working directory set to the directory that contains your package.json file. \n\n(Please mark the package.json bit as code.)\n. That'll break in 2018 when Node.js v10 ships. I'd rather just do it properly.\n. Actually this should be concatenated with the stringified babel config. packageHash only takes two arguments.\n. You need to add process.version.split('.')[0] to this. Note that it's currently broken: https://github.com/avajs/ava/pull/1068/files#r83534244.\n. Could you recompute this on line 11? It's run every time a test needs to create an API which seems unnecessary.\n. @alathon still seeing a diff here.\n. Is it even necessary?\n. Well it renders the same so I'm not too bothered I suppose. @sindresorhus?\n. For consistency, should use backticks around skip, only and failing.\n. Changing to this probably fixes the AppVeyor error:\njs\n'import test from ' + JSON.stringify(avaPath) + ';\\ntest(t => { t.pass(); });'\nI reckon on Windows the path contains backslashes which are interpreted as escape characters.\n. Why this change?\n. The pkgDir and resolveTestsFrom directories can be passed to Api though. That's how its meant to be used, implicit defaults aside. We shouldn't need to change the working directory of the tap process.\n. Could you change this to:\n\nWhy are the enhanced assertion messages not shown?\nEnsure that the first parameter passed into your test is named t. This is a requirement of how AVA uses power-assert, the library that provides the enhanced messages. works.\n\nI see that this paragraph originally comes from @jamestalmage's comment in https://github.com/avajs/ava/issues/1031#issuecomment-244474530. It's quite technical though. I don't think we need that kind of detail here.\n. Missing ); at the end here.\n. Could you revert these to their original two space indentation? It's how package.json files are formatted.\n. Nah, the issue still requires a lot of careful reading and background to fully understand why it is this way.\n. Yes. There should be no unrelated whitespace changes in this PR.\n. path.extname(filePath).slice(1) seems cleaner.. This could be simplified to:\njs\nvar isCustomExt = fileExt !== 'js' && this.extensions.indexOf(fileExt) > -1\n. Should just be an else, assuming the body for if (isCustomExt) assigns result or throws.. Rather than logging and then rethrowing the original error construct a new error like in https://github.com/avajs/ava/blob/0f0400158ec4192fc32ccc603877d0b088302662/lib/cli.js#L99 and throw that. It'll be logged correctly.. Looks like ts-node is closer to babel-node than babel-core (and ts-node/register is equivalent to babel-core/register). IMO this module should just use typescript to transpile the code, and no other functionality is required.\nFor consistency we should not automatically transpile source files, at least until we have a better answer for the .js use case.. @zixia could you change this to an if statement?. Let's check for allowed extensions in lib/cli instead.. @zixia per the above discusison, could you simplify this module to just the transpile function?. Just use exports.transpile = typescriptTranspile.. Please remove these changes. Right now I'd prefer our TypeScript support is consistent with our Babel support, where dependencies are not automatically transpiled.\nI know that's not ideal but we can revisit it later.. What you're passing here are patterns that the API then resolves to actual test files. In this case the pattern is a symlink, and the API needs to resolve tests within the directory it's pointing at.\nCould you add another test where you provide a symlink to a test file? The API should use that path as-is, whereas if you provide a (symlinked) directory other code gets in between which may decide to resolve the path for us.. Could use a single template string.. Should use .slice(4).. Can just specify resolveTestsFrom, if you like.. Only if you set babel. So out of the box the behavior stays exactly the same as it is now (with helper compilation).. this.testCount - this.passCount - this.failCount is clearer.\nYou need to include other test states in this count, see the highlighted lines in https://github.com/avajs/ava/blob/033d4dcdcbdadbf665c740ff450c2a775a8373dc/lib/run-status.js#L41:L45.\nThe computation should be tested. That's probably best in test/run-status.js, though most of its behavior ends up being tested elsewhere.. I think this needs failCount: 1, like with the verbose test?. Meh, it still needs to be wrapped with colors.information(), so this combination seems fine to me.\nPreviously we were targeting Node.js 0.10, which meant that we couldn't use template strings. Now we can, but it's no big deal if new code follows the pattern already used in the file.. > You have some shared strings here. Shared with the mini reporter.\nI find that to be the kind of repetition that is too bothersome to dry up.. This should be computed in lib/run-status.js, which @jarlehansen is working on in #1179. Note that as I commented there (https://github.com/avajs/ava/pull/1179#discussion_r95819806) you need to include other test states in this count, see the highlighted lines in https://github.com/avajs/ava/blob/033d4dcdcbdadbf665c740ff450c2a775a8373dc/lib/run-status.js#L41:L45.. Use plur(), so \"1 test was\" and \"2 tests were\".. This should check if there were any failures. Otherwise the warning is also printed when .only() is used and --fail-fast is on:\n```\n$ \"$(npm bin)\"/ava --fail-fast\n1 passed\n--fail-fast is on. Any number of tests may have been skipped\nThe .only() modifier is used in some tests. 135 tests were not run.\n```\n. @ThomasBem this is very petty, but I just noticed there's a period at the end of the message here (and in the verbose reporter). None of the other messages seem to end in a period. If you've got the time could you do a PR to remove them? \ud83d\ude04 . Without looking into meow, I'm surprised this isn't a boolean. Does it become false if --no-color is set? Comparing against false seems better than comparing against undefined.. I'd personally prefer a for\u2026of loop here.. It seemed unnecessary yesterday but I think I figured it out. This config is used even if all transforms have been cached. So we should keep the laziness in place.. Restored the lazy behavior, though in a somewhat simplified form.. You'll have to update https://github.com/avajs/babel-preset-transform-test-files/blob/master/espower-patterns.json as well. Which is a breaking change \ud83d\ude1d . Nudge.. Could you add backticks around skip, only and failing? It's more consistent with the other messages.\nI've just clarified with @sindresorhus that's what he meant to say originally: https://github.com/avajs/ava/pull/980#issuecomment-261449141. This is better as:\njs\nt.throws(() => {\n    runner.beforeEach.only('', noop);\n}, TypeError, '`only` is only for tests and cannot be used with hooks');\nSame goes for the other tests. See http://www.node-tap.org/asserts/#tthrowsfn-expectederror-message-extrafor specifics.. If we're forking, we could publish to @ava/pretty-format.. The explanation seems a bit long for a CLI error. Contrast with the TAP reporter where we just say \"sorry, not available\".. How about Watch mode is not available in CI, as it prevents AVA from terminating.. Please use const.\n(@sindresorhus is this something xo should enforce?). Why this change? (I must admit I didn't read all of the back and forth earlier though\u2026 forgive me if I overlooked the answer). Assigning this.options seems unnecessary since the constructor didn't receive options before. Nothing else is using this.options.. _build. @sindresorhus you suggested to remove the color comparison from the test, but this removes it from the actual AVA code.\nWhy do the changes in this PR require colors to be removed elsewhere? Should we then also remove colors from https://github.com/avajs/ava/blob/master/lib/cli.js#L100:L108?. Can we make the jsx key a bit more obscure? E.g. __ava_jsx. Given the deserialization if your tree happens to include a top-level jsx key you're bound to get odd results.. Need to remove this line.. Could you add:\n\nConfigure patterns for the source files to avoid this.\n\n. Could you undo the whitespace changes in this file? Your editor may be doing it automatically but the only change that should be here is removing the --source flag.. > Specifically, ava doesn't care what values these throwing functions return, but doesn't produce a return value.\nt.throws() returns the error (and throws an assertion error if no error was thrown). Perhaps we could do something like:\ntypescript\nthrows<E>: {\n        (value: PromiseLike<mixed>, error?: ErrorValidator, message?: string): Promise<E>;\n        (value: () => mixed, error?: ErrorValidator, message?: string): E;\n}\nI don't really know how generics work in Flow though. And of course we wouldn't want to require people to specify the error type if they're not going to use it.. Interesting. This isn't actually supposed to return the value, but looking at the code I think it does\u2026. We don't necessarily know how many tests would have executed when running in --concurrency mode. So to be accurate this should be At least 1 test was skipped... Should have a test for multiple skipped tests.. Opened https://github.com/avajs/ava/issues/1227. I'd just read the file and ignore any errors:\n```js\nlet contents\ntry {\n  contents = fs.readFileSync(this.filePath);\n} catch (err) {}\nthis.tests = contents ? JSON.parse(contents) : {}\n``. Why export theSnapshotclass at all?. Could you document that this is only loaded in a worker process, so we only need one snapshot instance for the file that worker is testing?. Jest convention aside, is there a reason we should stick to this format? Our [naming scheme ishelpersandfixtures](https://github.com/avajs/ava/blob/314ef001ab1c085e2057738e2d2588bde3e792cc/lib/ava-files.js#L76:L80), notfixtures.. Fair enough. Perhaps export bothSnapshotandgetSnapshot` separately (rather than exporting the class as the main?)\nAlternatively you could use proxyquire and inject the globals dependency, though you'd have to reload for each test.. I think it better separates the \"public\" API (getSnapshot()) from the truly internal API (Snapshot constructor) which is needed for tests only.. I know. Just voicing my preference \ud83d\ude04 . Should this be configurable though? Folks who follow a __tests__ scheme will want to use __snapshots__.. > I would really prefer not introducing more options. Can we make it snapshots if inside a tests directory, otherwise snapshots?\nYea that sounds great to me.. It's only called in three places. I'd prefer letting the error propagate correctly, and then treating it as an uncaught exception.\nThat said I've changed this code to process.emit('uncaughtException', err) and moved the fallback stuff to the uncaughtException handler in test-worker.js.. I think this really does need to go here: https://github.com/avajs/ava/blob/98dded553c8c213739cea9743ab4662f40d3125a/lib/serialize-error.js#L59\nWe don't want to modify err since it comes from user code. There may be side-effects. But we can't easily copy error objects since name and message aren't always enumerable. After the call to cleanYamlObject in serialize-error.js we have a copy we can safely modify.\nAlso, in this case, I'd prefer a little bit of duplication over your forEach trick. IMHO this is a lot easier to follow:\njs\nif ('name' in err && typeof err.name !== 'string') {\n  delete err.name\n}\nif ('stack' in err && typeof err.stack !== 'string') {\n  delete err.stack\n}\nEven better, delete only deletes own-properties, so the 'name' in err guard is unnecessary.. @sindresorhus any thoughts on this? XO doesn't complain \ud83d\ude09 . This isn't necessary for babel-polyfill, so let's leave it out.. Perhaps Polyfill tests? I'm trying to avoid ES2015 mentions since the standard is continuously evolving. AVA tries to support stage-4 syntax which means currently we're supporting more than ES2016, but not ES2017 since it isn't official yet.. This is a good start. I think it could be clearer on what AVA does out of the box, and what kind of features end up being missing. There's also a big downside to using babel-polyfill, which is that it impacts how your program behaves (not just the tests!)\nWhat do you think about this:\n\nAVA lets you write tests using new JavaScript syntax, even on Node.js version that do not support it otherwise. However it doesn't add or modify built-ins. For example Array.prototype.includes() is not available in Node.js 4 even when you use AVA.\nYou can opt in to these features by loading Babel's Polyfill module. Note that this modifies the environment, which impacts how your program itself behaves.\nYou can load babel-polyfill by adding it to AVA's require options:. Figured it may have opinions on if condition indenting.. I think this should be 'color' in conf ? color.conf : true. The idea is that it defaults to true if not configured at all, but otherwise the config value should prevail.\n\nIt's strange that this doesn't fail any tests though.. Would you mind sorting the keys alphabetically?. Object.assign(conf, \u2026) modifies and returns conf. There's no need to reassign it (since you're reassigning it to itself).. You should add this to your machine's .gitignore, rather than adding it in this repo.. Let's just use Test<T>.. No, that'll still force the value to be true if conf.color is false.\nIf color is in conf, assume it's a usable value, else use true.. Why not 'color' in conf ? color.conf : true?. Yes, not sure why I thought I could get away with the manual approach \ud83d\ude09 . There's a typo in NodeJ.js.. > able to opt in to these features.\nLet's get rid of itself in how your program itself behaves.. Perhaps:\n\n\u2026 require option:\n\n(Marking require as code, and singular \"option\".). --fail-fast implies all tests stop. This still immediately fails the test. The behavior should be similar to before this refactor, except that the error can no longer be caught by calling code.. I think you should combine this into a singular regular expression.. Yea I can clean that up. I probably just copied these from elsewhere.. Because of promises.  The alternative is maintaining an index variable to resume the iteration. That's what iterators do all by themselves though.. Yea I was having my doubts about this. On the one hand it's an internal API so hey, but on the other it's a lot of arguments\u2026. > This feature, while useful, is going to hit a lot of people. Any chance we could add support for assertions throwing AssertionError?\nThat's not the problem, actually. Any custom assertion that throws an error causes the test to fail, just because tests shouldn't throw errors.\nThe issue is when all custom assertions pass and no errors are thrown. The intent is for the test to pass, but we don't know that custom assertions were executed. So it'll fail. That's why I added this option.\nIMHO the question is what the default behavior should be.. I'm not sure how we can make that detection reliable and consistent across test files.. karma-ava is pretty experimental though. We don't have a stable API so IMHO it's fine if this breaks karma-ava.\nIf necessary this could live in its own file. It's odd in process-adapter. Heck it's a bit odd to have it in test-worker versus just main.. Good point. Will push a fix imminently.. Sure, let's cross that bridge when we get there.. In Test. See https://github.com/avajs/ava/pull/1314/commits/3e3b2131ff08838e86e34e99940a0a8ec1208e30.. > By loading Babel's Polyfill module you can opt in to these features. \u2026. Let's just call it JavaScript.\nHow about:\n\nAVA lets you use some nifty JavaScript features in your tests, like async functions. To make this work on older Node.js versions AVA transpiles the tests and helper files using the @ava/stage-4 Babel preset. This is great for projects where you don't otherwise use Babel for your source, but do want to use the newest JavaScript features in your tests.. I think jest-snapshot just treats these files as CommonJS modules so rewriting the header should be harmless. Will do.. Ah Promise here is indeed from bluebird. I guess I'll rename it to Bluebird.try().. Sure.. Actually no, jest-snapshot requires snapshot files to start with this header: https://github.com/facebook/jest/blob/38ca942f6f2979ed25f54510173dbd2761dd7e04/packages/jest-snapshot/src/utils.js#L37\n\nWe could add a comment after but that becomes annoyingly complex.. > Can you open an issue on Jest to make it configurable? They seem open to making it more generic when needed.\nI don't know, if you look at the error messages it very much assumes running inside Jest.\n\nI'm happy to land this as is for now, but long-term we'll want our own naming and versioning.\n\nI think we want our own alternative, really.. I considered that when I first wrote it, but the nuance seemed superfluous. And since #1330 it's not even shown anymore.. Done.. Our main page talks about JavaScript, so I don't see why we should start using a different term here.. @sindresorhus It's not OK, that's what #1330 is about. This merely verifies that the internal handling doesn't cause a crash, since the error cannot currently be communicated.. Remove the \"in\" after \"features\".. Thanks for changing this to JavaScript (though it says ECMAScript in the second paragraph). I still think this reads better though:\n\nAVA lets you use some nifty JavaScript features in your tests, like async functions. To make this work on older Node.js versions AVA transpiles the tests and helper files using the @ava/stage-4 Babel preset. This is great for projects where you don't otherwise use Babel for your source, but do want to use the newest JavaScript features in your tests.\n\nThe latest Node.js version support most of ES2017, so it helps to explain why AVA still needs to do transpilation. I feel the sentences read a bit better as well.. Perhaps keep the beforeEach, but remove this $ExpectedError line.. It's used elsewhere in this file, so might be best to keep it.\nI think you have to be in the AppVeyor team to do that.. I suspect this outputs 0, rather than -0? https://github.com/avajs/ava/pull/1341 would fix, but I think we should work around this in this branch too.. Perhaps we can take a leaf from the Object.is() description and say Must be the same value as:.. We could say:\n\nAssert that value is the same as expected. This is based on Object.is().. The \"using\" seems superfluous. Either This is based on or This relies on I think. Also for .not().. It's AVA in all-caps \ud83d\ude04 . Did you intend for this to be on its own line? Cause GitHub renders it right after \"run the server.\". Perhaps add the import test from \"ava\" line as well.. We're pretty vocal in advocating async with AVA. I  don't think we should include this example.. This may work even nicer with before:\n\njs\ntest.before(async () => {\n  await MongoDBServer.start()\n})\nNow tests don't have to await hasStarted.\nThis should also work but might not be as obvious:\njs\ntest.before(() => MongoDBServer.start()). Perhaps:\n\nThis will allow the MongoDb server to print connection errors when it's starting.. Could you spell out ODM? I assume it means Object-Document-Mapper but it's a little obscure.. Could you write this as:\n\n```console\n$ npm install --save-dev mongomem\n```\n\nThe code block is a little nicer and it matches our readme.. Perhaps we can assume that mongoose is already being used?. The \"by importing mongoose\" bit seems unnecessary.. Rather than \"isolated with tests\", perhaps end with \"with per-test isolation\"?. This should probably use const.\nWhat should users do next, once they have the connectionString?. What kind of connection errors would occur? mongomem implies it's an in-memory server, so what could be going wrong?. > You should isolate Mongoose instances between your tests, \u2026. Should odm.connect() be awaited?. Is db a more common name for this variable?. There is a modelNames() function. I think this might be better:\njs\nfor (const name of mongoose.modelNames()) {\n  odm.model(name, mongoose.model(name))\n}. You should say something like \"this uses the mongomem\" module and then provide a link to npm or GitHub.. Perhaps:\n\nYou will have to manually copy models from the global instance to your new instance.. Perhaps all this is better done in a beforeEach:\n\n```js\ntest.beforeEach(async t => {\n  const db = new mongoose.Mongoose()\n  await db.connect(await MongoDBServer.getConnectionString())\nfor (const name of mongoose.modelNames()) {\n    db.model(name, mongoose.model(name))\n  }\nt.context.db = db\n})\ntest('my mongoose model integration test', async t => {\n  const {db} = t.context\n  // do something\u2026\n})\n``. \"which is available\". This doesn't look like it has to beasync`?. Too much indentation here.. Too much indentation here as well.\nThe \"do something\" comment is a bit flippant (my bad!). Perhaps something like // Now use the isolated DB instance in your test.. This has tab indentation, unlike the other code samples. It looks a bit odd when the Markdown is rendered.. For consistency, this should end with a ; \ud83d\ude1c . Let's go with the readme, which uses tabs (except for package.json which uses two spaces because of npm).. Why?. These tests can be hard to get right. I'd start with running the CLI directly (node cli.js test/fixture/concurrency/test.js etc) to make sure it works correctly. Then try invoking it from the test. . CLI invocation with --concurrency and -c without a value. Presumably there are tests where a value is provided, but if not those should be added.. What does stderr actually contain?\nI had some trouble running this directly from the command line, it seemed to start running TAP tests instead. Try execCli(['test.js', concurrencyFlag], \u2026) instead.. Babel just uses require hook.\nWhile we're updating this though, we should recommend using babel-register, not babel-core/register which is undocumented (and, without verifying, might very well be going away in Babel 7).. I'm not sure the sample code needs to prescribe this. What does webpack show in its examples?. The recipe discusses precompiling source files. I'm not sure why the entry here is src/tests.js but it seems even more incongruent to add test/**/*.js entries.\nHow are you using this?\nPerhaps the globbing approach can be mentioned in the paragraph that follows the example code, with a link to the Stack Overflow post?. \ud83d\udc4d . Rather than:\n\nThis recipe various approaches using webpack. (These examples use webpack 2.0).\n\nPerhaps:\n\nThis recipe discusses several approaches using webpack v2.\n\nThis seems distracting:\n\nThis might not be necessery once this is completed.\n\nAnd the discussion URL should have https://github.com in front of it.. This assumes you only have one test file (named tests.js)? Perhaps the file name should be test.js then? (At least that's how I name such a file, analogous to having a test directory.). Perhaps Test against precompiled sources?. Let's not bring code coverage into this discussion, since that's a whole other can of worms.\nWhy _src here versus _build above?\nThere's a typo in workflow (it's missing the l).\nHow would users do this precompilation?. How about putting it like this?\n\nMultiple test files can be compiled into a single file. This may have the best performance but that does come at a cost. All tests will be in the same file, which can make it harder to know which test has failed, since AVA can't show the file name the test was originally in. You'll also lose process isolation.. This code can be improved a bit:\n\njs\nconst testFiles = glob.sync('./test/**/*.js').reduce((entryObj, file) => {\n  entryObj[path.basename(file, path.extname(file))] = file;\n  return entryObj;\n}, {});\nThen use entry: testFiles below.. I'd replace \"babel cli\" with babel-cli. Rather than \"compile the src file\" I'd use \"compile the source files\". And I'd invert the sentence structure from:\n\nThen we compile the tests with a require path rewrite\"\n\nTo:\n\nRequire paths in tests are rewritten when compiling them in webpack.. Same as above.. This constructor is private, so we should be able to clean this up.. We're targeting Node.js 4 so this syntax is a bit too fancy I'm afraid \ud83d\ude09 . I think we should export this as require(\"ava/CallSite\").. Is the assumption that [[CallSite]] comes from the toString() method above? Could there be more than one of those in the stack? Why is it OK to replace it with an empty string?. There could be a CallSite.js which reexports CallSite from lib/assert.js. Manipulating runner.chain like you do here doesn't seem ideal.. This bit (localCLI && path.relative(localCLI, __filename) !== '') needs to be cast to a string: String(localCLI && path.relative(localCLI, __filename) !== ''). Otherwise the comparison on the next line won't be true.. Shouldn't this be !== 'true'?. I think it's clearer to just repeat this property.. The catch is necessary. We don't expose the AssertionError instance to the test author. Instead add a .catch() callback in the pending() implementation here: https://github.com/tdeschryver/ava/blob/4a19a96da89a1533025295a7cd78b74c12528f04/test/assert.js#L15.. Rather than extracting the comparison logic into testFailResults, if you capture lastFailure through the pending() function you could write an eventuallyFailsWith():\n\njs\nfunction eventuallyFailsWith(t, promise, subset) {\n  return promise.then(() => failsWith(t, () => {}, subset))\n}\nGiven that assertion.throws() should return a promise that always resolves, and the error will be captured separately.. My bad, overlooked that failsWith() resets lastFailure.\nInstead extract the failure testing code into its own function, e.g.:\n```js\nfunction assertFailure() {\n  if (!lastFailure) {\n        t.fail('Expected assertion to fail');\n        return;\n    }\nt.is(lastFailure.assertion, subset.assertion);\nt.is(lastFailure.message, subset.message);\nt.is(lastFailure.name, 'AssertionError');\nt.is(lastFailure.operator, subset.operator);\nif (subset.statements) {\n    t.is(lastFailure.statements.length, subset.statements.length);\n    lastFailure.statements.forEach((s, i) => {\n        t.is(s[0], subset.statements[i][0]);\n        t.match(s[1], subset.statements[i][1]);\n    });\n} else {\n    t.same(lastFailure.statements, []);\n}\nif (subset.values) {\n    t.is(lastFailure.values.length, subset.values.length);\n    lastFailure.values.forEach((s, i) => {\n        t.is(s.label, subset.values[i].label);\n        t.match(s.formatted, subset.values[i].formatted);\n    });\n} else {\n    t.same(lastFailure.values, []);\n}\n\n}\n}\n```\nAnd call that from failsWith() and eventuallyFailsWith() (so the latter doesn't call failsWith() anymore). eventuallyFailsWith() should also reset lastFailure.. No worries. Good luck with the move!. > Thought it would be better to acknowledge the difficulty of precompiling and mention that a better permanent solution is in the works. Would you prefer I reword it or remove it?\nI'd rather these recipes focus on what's possible now, rather than what might be possible eventually. I mean I wrote that RFC in January\u2026 it's not a quick fix \ud83d\ude09 \n\nI thought it might be better to have relative links. And that / would go down to the root. I'll just change this to use full URL. There is also the option of using avajs/ava#1385. Let me know if you'd prefer it.\n\nYea I know. I suppose I just don't trust linking within GitHub that much, plus it means the links won't work if the file is rendered outside of GitHub.. I hear ya. We do talk about test.js in the readme though.\nAlternatively: your-test-file.js?. > I was thinking _build for tests and _src for source.\nSure. I'm probably overthinking this \ud83d\ude04 \n\n\nHow would users do this precompilation?\n\nI was thinking babel-cli.. I'll change this to mention babel-cli watch mode instead of webpack. Sound good?\n\n\ud83d\udc4d . Needs a space between typing and <kbd>.. Should this update snapshots in all tests, or just those tests that just ran? I'm leaning towards the latter. Do you know what Jest settled on?\n@lukescott?. Let's do that here as well then. Storing the specificFiles || files argument here should be sufficient.. It's there in Node.js 8: https://nodejs.org/api/debugger.html#debugger_v8_inspector_integration_for_node_js. This breaks the --update-snapshots option, since runStatus.updateSnapshots defaults to false.. This should be false rather than 0 (it's not a counter).. Would be better to inline the setupMongoose function here.. This isn't reloaded for every test. Better to require it outside the test hook.. This seems pretty critical actually. How does app use the db instance?. https://github.com/avajs/ava/blob/master/docs/recipes/isolated-mongodb-integration-tests.md doesn't do this. I wonder if it should.. Apologies, I meant passing raw as a property in opts.. Modules are cached after the first time they're loaded, so requiring ./server for each test shouldn't make a difference.. I assume this is asynchronous? That'll mean the assertion is run when the test is ended. We don't currently warn about that, but it means the assertion is ignored. You'd have to get a promise out of this or use test.cb() with t.end().. Could you fix it so this stays in the package-lock.json?. Let's not worry about this file, it's used by maintainers so we won't run it on Node.js 4 \ud83d\ude09 . Could you add a comment explaining the logic here? Like, why use 2 on single-core machines?\nWhen would os.cpus() return an empty array?. More actually.. Typo in stil run.\nRather than specify just that test file to AVA, say provide just that test file to AVA?. If log contains linebreaks, should the indentation apply on those new lines? If so, this should use https://www.npmjs.com/package/indent-string (like elsewhere in the reporters). May not be a bad idea to use that anyway.. Same question here around linebreaks in the log value.. Along the lines of https://nodejs.org/api/console.html#console_console_log_data_args, perhaps:\n\nPrint a log message contextually alongside the test result instead of immediately printing it to stdout like console.log.. This might be better as:\n\njs\nconst logLines = indentString(colors.log(log), 1, {\n  indent: `    ${colors.information(figures.info)} `,\n  includeEmptyLines: true\n});\n(Though that indent value should probably be a constant, but feel free to redeclare it in the verbose logger.)\nSee indent-string's API. Including empty lines would be good given the figure prefix.\nYou'll have to update the indent-string dependency to 3.2.0 since the includeEmptyLines feature is pretty new.. Oh! No that's fine, I misread the code.\nI suppose the replacement string could be a constant, but on the other hand it's clearer if it's near the indentation values. So hey :). I think this should be Number.isInteger(Number.parseFloat('cli.flags.concurrency')). That way even values like 9.5 are rejected, which parse to 9 with parseInt().. The default should remain 0. This allows AVA to pick an optimal value for your machine, see https://github.com/avajs/ava/blob/465fcecc9ae0d3274d4d41d3baaca241d6a40130/api.js#L164:L167.. parseFloat doesn't take a radix.. Need to move \"it\" around:\n\nif you want it to listen on a specific port.. This should be nonnegative integer, since 0 is allowed. I feel a bit bad about this nitpick \u2639\ufe0f . Would be nice to link to http://mongoosejs.com/ here.. The capitalization of Mongoose should be consistent throughout the recipe.. I find this misleading. It just needs regenerator-runtime, right? Going by https://www.npmjs.com/package/regenerator-runtime (which is depended on by babel-polyfill) you could install that as a dependency and then require regenerator-runtime/runtime. That should only be done if you're testing on Node.js releases older than 8.\n\nbabel-polyfill impacts your environment in many other ways. This recipe should focus on what is absolutely necessary.. Requiring babel-register can have unintended consequences. I don't see how it's required for this recipe?. While the watch mode is nice (it's what got me involved with AVA) I don't see how this is relevant to this recipe.. I think you should mention here that the recipe assumes you have an Express server. Here you should repeat that this example is based on Express. It's not a requirement, so stating it up front may make it easier for others to see how they can apply it to their own setup.\nPerhaps place the app.listen bit after the code sample. You could phrase it like \"Note that this doesn't call app.listen(). We'll need to do that ourselves in our tests. For your application make sure you have a startup file that imports app and calls listen().\". AVA takes care of this for you. I'd assume (especially when regenerator-runtime is loaded this recipe works all the way back on Node.js 4).. I'm thinking this should just link to https://github.com/zellwk/ava-mdb-test/blob/master/server.js. Not sure it it needs to be copied into the test. Similarly you may want to link to https://github.com/zellwk/ava-mdb-test/blob/master/models/User.js which, not knowing anything about Mongoose, I find quite relevant.. You should explain why these tests must use .serial. What happens if I just use test()?. All this could be covered in the earlier paragraph.. While a useful tip, I don't think you need to show a full example.. Perhaps resolve the absolute path here: conf.snapshotLocation ? path.resolve(projectDir, conf.snapshotLocation) : null. This would even enable absolute paths, and remove complexity in the snapshot manager code.\n\nTotal bikeshed, but how do you feel about snapshotDir? Seems snappier.. If this.snapshotLocation is either null or an absolute path, this could be renamed from location to fixedLocation.. When options.location becomes options.fixedLocation this comment becomes unnecessary since it's clear what is expected of the code.. Shouldn't this refer to testDir, rather than options.testDir?\nAlso, you should be able to use path.relative(projectDir, testDir) here.. Use https://www.npmjs.com/package/convert-source-map, like in nyc: https://github.com/istanbuljs/nyc/blob/fb3ab927796963379edfddd4d2385002fa236f65/lib/source-maps.js#L16\nThis will support inline source maps as well as source map references. It does presume that these references are in the source file, but I think that's a fair assumption.. This logic isn't quite right. According to the source map \"spec\" file is optional. There might be a sourceRoot which should be prepended to the sources.\nI couldn't quite find a library which exports the correct source resolution. I think this will be sufficient for our use case (though I haven't tested it):\njs\n// Assume there is but a single source. Multiple sources imply multiple possible\n// snapshot locations, which won't work.\nconst firstSource = sourceMapContent.sources[0];\nconst rootedSource = sourceMapContent.sourceRoot ?\n  sourceMapContent.sourceRoot + firstSource :\n  firstSource;\n// `convert-source-map` doesn't provide the path to the source map file, if the\n// source map came from such a file. Assuming that if it did, it is stored right\n// next to the test file. Or, alternatively, if it is stored elsewhere,\n// resolving `rootedSource` relative to the test file results in a path that\n// is the same as if it had been resolved relative to the source map file.\n// Note that `options.relFile` is relative to `options.projectDir`.\nconst sourceFile = path.resolve(path.join(options.projectDir, options.relFile), rootedSource);\nreturn path.dirname(sourceFile);\nI'm not sure how much we need to guard against malformed source maps (e.g. no or empty sources), or source maps that unexpectedly resolve to an HTTP source.. I'd prefer:\n\nYou can specify a fixed location for storing the snapshot files from each test under the \"ava\" key in your package.json:\n. > The snapshot files will be saved in a directory structure that mirrors that of your test files.. > If you are running AVA against precompiled test files, AVA will try and use source maps to determine the location of the original files. Snapshots will be stored next to these files, following the same rules as if AVA had executed the original files directly. This is great if you're writing your tests in TypeScript (see our TypeScript recipe).. Could you make sure these files have a newline at the end of them?. This doesn't remove anything though?\n\nYou should remove these files at least prior to running the test, otherwise on local setups you won't be able to tell if the code has broken if they're persisted from previous runs.. Does this assume tsc is installed globally? I know we have a dev dependency on it but running build inside this directory won't pick it up. This should either refer to the top-level node_modules/.bin/tsc or use npx tsc.\n(Also, going by the GitHub output here, the indentation seems off in this file and tsconfig.json.). Rather than this paragraph, update the sample tsconfig.json to enable source maps. Honestly it should always be enabled, since it helps AVA show correct stack traces and code snippets when assertions fail.. It should be summarized in the recipe. Otherwise people may either think that they should always use test.serial(), or they'll go \"that can't be right\" and remove it.. Use path.relative(options.projectDir, testDir).. tryRead() returns null if the file doesn't exist. You'll have to read it into a variable and bail out of if it's null.\nPerhaps this function should return null if no source-mapped directory could be determined. Then determineSnapshotDir() can do testDir = determineSourceMappedDir(options) || options.testDir.. There is still room for the file to be deleted right in between existsSync and unlinkSync. Would be better to do https://github.com/impaler/ava/blob/12f1ff8d73e0ae47c109eed9737470c9d421fa8e/test/cli.js#L568:L574, even if it's more verbose.. I'm not too concerned about how DRY this is. The other test looks good, just need to take that same approach here.. Nitpick alert: the indentation in this file is inconsistent. It's just a fixture, but once seen I can't not comment \ud83d\ude09 . Fair enough.. Ah I took that from the document title, but they don't refer to it that way elsewhere. Will change back, thanks!. The explicitness doesn't hurt I think.. I think it may be sufficient to remove the title !== null check. matcher([null],[\"!*oo\"]) gives [null] which passes the length === 1 check. Perhaps use matcher([title || ''], this.match), since the matcher expects strings.. What's the origin of this typeof error === 'function' check?. We're targeting Node.js 4 still, so this destructuring won't work unfortunately.\nPerhaps just use Object.assign() without checking for values?. We're targeting Node.js 4, which means spread isn't available \ud83d\ude1e . Could you remove these changes from your PR?. You probably meant for these two fields to be at the top-level?\nNode.js doesn't care about them though. Feel free to remove them.. import Promise from './enable-trace';. Would be good to add a comment linking to http://bluebirdjs.com/docs/api/promise.longstacktraces.html, since that's where the code comes from.. Indentation seems a bit off here, though looks like we're not linting this file so hey \ud83e\udd37\u200d\u2642\ufe0f . Shouldn't this forward --no-color if specified? And forward --color exactly as it was received?. opts.color already is a boolean, courtesy of https://github.com/avajs/ava/blob/1df502df8cba18f92afcbaed730d8c724687ca45/lib/cli.js#L78.\n. That's an odd test. Just remove it \ud83d\ude04 . The code comment seems informative: https://github.com/avajs/ava/blob/15bd79402c55468a4b13aedceba503ee26e964a1/lib/serialize-value.js#L11:L12\nThis is no longer applicable I think. We catch and wrap all errors from tests, so it's only uncaught exceptions and unhandled rejections that are passed directly to serialize-error. And right now we don't even guard against throw null. Let's remove this edge-case and we can open a follow-up for handling non-Error exceptions / rejections.. > unless they are specifically ignored\nSwap they for these, I think.\nThis should have an example of how to ignore them.. How about this?\n\nAVA will set process.env.NODE_ENV to test, unless the NODE_ENV environment variable has been set. This is useful if the code you're testing has test defaults (for example when picking what database to connect to, or environment-specific Babel options). It may cause your code or its dependencies to behave differently though. Note that 'NODE_ENV' in process.env will always be true.\n\nThe Express example seems overly specific, as do the references to code coverage. And the result of 'NODE_ENV' in process.env will be true, not false.. Yes let's get rid of this check.\nThe code needs to be more defensive here but we'll handle that in a follow-up issue.. Although XO doesn't complain, I'd have expected a regular string here, not a templated one.. I don't quite see how the new observeStdin would throw this error where the old one doesn't. Why does it need to be handled here?. Instead of exiting directly, can we stop all workers and let the process exit on its own?. I wonder if you could rephrase this a little. What's interesting is that you're adding additional global variables. jQuery could be an example of that:\n\nYou can expose more global variables by assigning them to the global object. For instance jQuery is typically available through the $ variable:\n. Not quite. The child processes should be sent the SIGINT signal. See https://github.com/avajs/ava/blob/bb91862eaac1a3e89f651afb15ec9691745ee222/lib/fork.js.. I don't think this needs to be prefixed with [ava]. Currently the key needs to be followed by Enter in order to be recognized. How about:\nTo rerun all tests, type 'r', followed by Enter\nTo update snapshots used in the previous tests, type 'u', followed by Enter\n\nPerhaps using chalk.gray.dim as the color.. You never actually execute fn though \ud83d\ude09 \nI'd return the result of fn which makes avoids some odd indentation in the tests themselves. For example:\njs\nfunction withNodeVersion (version, run) {\n  setNodeVersion(version);\n  const promise = new Promise(resolve => {\n    resolve(run());\n  });\n  promise.then(resetNodeVersion, resetNodeVersion);\n  return promise;\n}. This should use a regular string, not a template string.\nAs for the label itself, how about:\n\nValues are deeply equal to each other, but they're not the same:\n\nThis references both t.deepEqual and the \"Assert value is the same as expected\" explanation of t.is().. This would be clearer with Array#some().. You could create the allTests sequence in this branch.. This could be an empty sequence, since presumably all tests are skipped if it gets to this point. That would make the logic easier to grasp IMHO.. Looking at this in VS Code the type of t is inferred. Given how often you'd end up typing that parameter perhaps the example shouldn't explicitly type it.. Ah that makes sense \ud83d\udc4d . > I also moved assignments up, so we don't run .describe second time in deeply equal case.)\nNice!\n\nI used it because of \"it's\". Linting style doesn't allow doublequotes.\nSo I changed your label a bit for this reason (they're ->> they are) and used regular string.\n\nI don't know if we're consistent in this regard, but \u2019 is a good apostrophe to use. Or else the single quote can be escaped. Anyway, the copy is fine now.. I think this should say:\n\nThe .mjs extension can work in web browsers too, but such files have to be served with the correct media type\n\nAnd:\n\nThere is an ongoing effort on standardizing .mjs.\n\n(an rather than a). It'd be nice if this would have an actual example of a .mjs file. Notably the test files are still transpiled using Babel, which means the above really readsconst test = require('ava'), and there are no .mjs files being used. Same goes for the TypeScript example below.. I don't know\u2026 perhaps the recipe should say to use npm install and not show any package.json? That would make copying really hard.. How does this work given that TestCollection#finalize() does the finalizing via setImmediate()?\nI think it'd be better to get an array of pending tests from TestCollection and then iterate through it, calling addTestResult(). I'd then modify addTestResult() to take a second pending argument and provide the correct value here and in run() below. That way Test doesn't have to be aware of pending at all.\nTestCollection#finalize() can then be removed. For clarity I think buildStats() should be renamed to exit().. At first blush you'd think this is now always 0, but that wouldn't be the case with --fail-fast not emitting the pending tests. Perhaps a comment would help clarify that possibility.. This doesn't apply to pending tests.. This relies on the signal still being propagated to the worker processes, right? I think we need to take a more explicit approach. For one the API needs to know not to start new workers when concurrency is being limited. Since the API is managing the worker pool it could shut down the workers, and the workers could then emit their pending tests. See https://github.com/avajs/ava/blob/e401bd151b1a81f79ccb106f034b2e18715ee125/api.js#L245:L247\n(Though looking at that code I have a suspicion timeouts don't prevent new workers from being started!)\nWe'd need to make sure the signal isn't propagated, or otherwise is ignored in the workers.. I'm sure it's on your list, but this still needs to print the titles of these tests.. This only applies to when --fail-fast is used. I don't think we don't need to print pending tests in that circumstance.. Looking at this in my terminal I think this should just be chalk.gray. But we then should also update the timestamp output in the mini & verbose loggers to not be dimmed.. Most tests use t.match for this. You could do an assertion for each line.. Unfortunately this doesn't work with the mini reporter. I think finish() should take an optional second parameter, call it footer perhaps, and both the mini and verbose loggers can then include this in their regular output.. > I imagine people want to be able to identify any non-passing tests with --fail-fast. \nNo, all tests should stop running as soon as a single failure occurs. See #1158.. Why must they be cleared?. I'd like to make sure we try intercepting SIGINT in the main process, and then using the Api instance to explicitly exit all workers. We may then need to trap SIGINT in the workers to ensure they don't exit before they're told to do so.. > I feel like adding such a loop at the top of buildStats() (or exit(), if we were to rename it) is a cleaner way to do this.\nWhich is what you've done, right?. dumpError doesn't return an Error instance, which supertaps API says is expected. Perhaps the API documentation should be loosened, or clearer on what properties are expected on non-Error objects?. This should start with [/\\\\]@std I think, to better match just @std/esm.. AVA prints this as an uncaught exception, which isn't ideal. Do you think this even needs to be printed, or can these test files be skipped silently?. This will trigger \"memory leak\" warnings when there are more than 10 test files. Each test is pushed onto the tests array though so you could use that to set up one SIGINT listener and exit each active test.\n. > iterative development/debugging. Personally, I'd find knowing which tests are pending useful in this case - especially if I'm trying to troubleshoot why said tests are pending.\nIt would show tests that haven't yet completed when another test fails. I don't think that's what we're trying to solve with this PR.. Is this sufficient? I don't see a reference to stackTraceLimit as an instance property in https://github.com/v8/v8/wiki/Stack-Trace-API.\nInstead I suspect we need to manually call Error.captureStackTrace(this) below, if opts.stack is falsy.. Since this test is run in its own process you could set the limit at the top level, before the test() call.. Rather than instantiating the error, call say t.fail() from inside some nested functions:\njs\nconst a = () => b();\nconst b = () => c();\nconst c = () => t.fail();\nAVA should print the full stack trace which you can then assert against in test/cli.js. . handlePromise is an arrow function, so .call(this isn't necessary. Just do return handlePromise(promise).. Could you add a test for this code path?. If I run the fixture locally (node ./cli.js test/fixture/infinity-stack-trace.js) I get:\n```\n  1 failed\n[anonymous]\n/Users/mark/GitHub/avajs/ava/test/fixture/infinity-stack-trace.js:6\n5: test(t => {\n   6:   const c = () => t.fail();\n   7:   const b = () => c();\nTest failed via t.fail()\nc (test/fixture/infinity-stack-trace.js:6:20)\n  b (test/fixture/infinity-stack-trace.js:7:18)\n  a (test/fixture/infinity-stack-trace.js:8:18)\n  Test.t [as fn] (test/fixture/infinity-stack-trace.js:10:2)\n```\nRight now you're matching the code snippet but not the stack trace. Try t.match(stderr, /c \\(.+?infinity-stack-trace\\.js:6/) and so forth for b and a. (Note that I haven't actually tried this regular expression, apologies in advance if it's wrong! \ud83d\ude04 ). For the next few months we're still targeting Node.js 4, which doesn't support rest parameters. You'll have to use plain old arguments instead.. Since you're really mapping args anyway this would be better as:\njs\nconst args = Array.from(arguments, value => {\n  // ...\n});. Let's be consistent and name this contextRef throughout.. We should expose the context to the after hooks as well. This should be the same object as was created by the before hooks.. _hasUnskippedTests() depends on allTests having been created. Wrapping the test building in sequenceTests() won't work. Looks like this behavior isn't covered by an integration test though.\nI think it's fine to still build beforeHooks and afterHooks. The trick is to not run them if all tests are skipped. We still \"run\" the skipped tests since we want to show them in the test results, but they don't actually execute.. It's actually context.context that needs to be (shallowly) cloned. Let's use https://www.npmjs.com/package/lodash.clone.\nYou'll have to create a context ref for the before and after hooks, and before those hooks have run you need to pass a cloned ref to the beforeEach and afterEach hooks as well as the tests. Something like this should work if you modify the t.context getter and setter.\n```js\nclass ContextRef {\n  constructor () {\n    this.value = {}\n  }\nget () {\n    return this.value\n  }\nset (newValue) {\n    this.value = newValue\n  }\ncopy () {\n    return new LateBinding(this)\n  }\n}\nclass LateBinding extends ContextRef {\n  constructor (ref) {\n    super()\n    this.ref = ref\n    this.bound = false\n  }\nget () {\n    if (!this.bound) this.set(clone(this.ref.get()))\n    return super.get()\n  }\nset (newValue) {\n    this.bound = true\n    super.set(newValue)\n  }\n}\n```\nThe same copied ref should be passed to the beforeEach and afterEach hooks as well as the tests. The first time the context is accessed it'll be \"bound\" to the value set in the before hooks. You can pass the original ref to the after hooks.\n. Let's go with:\n// Assert that the promise rejects, or the function throws or returns a rejected promise.. Let's go with:\n// Assert that the promise resolves, or the function doesn't throws or returns a resolved promise.. Perhaps replace this example with the async function one you added above, and remove it there.. > Assert that function does not throw an error, promise does not reject with an error, or function returns a promise that does not reject with an error.. > .throws() returns the rejection reason of a promise returned by the function. > .throws() returns the error of an observable returned by the function. > .notThrows() returns undefined for a fulfilled promise returned by the function. > .notThrows() returns undefined for an observable returned by the function. I find relying on the setter's behavior to be quite surprising. It's not immediately clear that this.bound and this.boundContext are actually updated.. This should be @babel/register I think.. Babel seems to be documenting these as @babel/preset-env rather than just env.. I suppose transform-runtime is not needed if @babel/preset-env is used?. Perhaps \"Cause the test to fail\"?. Would this ever be shown to the user? I agree that we shouldn't document it for each assertion though.. No need to include \"\" in this list.. Perhaps \"strictly equal\". Using this with objects or arrays can be a valid use case so the reference to deepEqual seems distracting. There's also edge cases with negative zero and NaN but that's perhaps too detailed for inline documentation \ud83d\ude09 . \"match\" rather than \"follow\". The \"never returning\" part is a type detail that's not relevant to the documentation. It's so that you can write t.notThrows(() => { throw new Error() }) without type errors.. Similarly here, returning \"any value\" is besides the point. Just \"Assert that a function does not throw.\" is sufficient.. Try and describe what the modifier does, rather than what it is. E.g. here \"Specify hooks that run once all tests have completed\". Or below, \"Skip this test.\". It's 5.2.0, not 0.5.2 \ud83d\ude09 Perhaps add a link to https://github.com/npm/npm/releases/tag/v5.2.0?. This should be @babel/register. It'd be neat to link to the right anchor.. Does this need changing?. I don't think we need to explain the virtues of @babel/preset-env.. This line ought to be removed as well.. This line ought to be removed as well.. This line ought to be removed as well.. It's odd to have this section here. There's no instructions on @babel/register either. Perhaps just leave it out, so it's like it was before?. Could you change this to \"Babel recipe\", rather than \"babelrc recipe\"?. Yes but IMHO that's part of Babel 7's upgrade documentation, not AVA's.. Given that x here is an absolute path I'm a little nervous about just looking for /esm/. That seems like it could yield false positives more often than @std/esm did.\nIt's probably fine if we'd check for node_modules as well. Alternatively perhaps esm could tag its exported function with a registered symbol that we can check for?\njs\nconst required = require(x)\ntry {\n  if (required[Symbol.for('esm')]) {\n    require = required(module)\n  }\n} catch (_) {}\nDo you think we should drop support for @std here?. Could you remove this addition? I don't think we've seen this file in PRs at all so it doesn't seem necessary.. Personally I have it in ~/.gitignore. I'd just rather not balloon .gitignore with OS level stuff.. FYI apiOptions.babelConfig can be null. See:\nhttps://github.com/avajs/ava/blob/977cf4f4a6e11c44d8f9e472c4626a20cf39e050/lib/cli.js#L145\nand:\nhttps://github.com/avajs/ava/blob/977cf4f4a6e11c44d8f9e472c4626a20cf39e050/lib/babel-config.js#L18\nThat said when it's not null it'd be good if it could have a default extensions value of ['js']. The validate() function in babel-config should take care of that.\nSince these extensions won't change during test runs, perhaps compute them in the constructor?\nYou may still run into some CI problems since not all tests properly set up the Api instance. It'd be OK to set a default value if fixing the tests proves too much effort.. This will fail CI in Node.js 4, but don't worry about that. We're dropping support since it's pretty much end of life anyway. I just need to open the necessary issues.. You've changed defaultIncludePatterns  to take a single pattern, but here you're calling it with an array. That's probably why CI is failing. Looks like you need the this.extensionPattern value from below.. That last true parameter seems unused?. The Object.keys() bit was there to ensure only known keys were used. So we still need that, but it's gotten a bit trickier since more than 1 key is allowed.\nIt should be possible to provide extensions and not testOptions.. I think this should be guarded against in the calling code.. Seconded.. Let's simplify this. We can keep the current pkg-conf logic to find the package.json, if any. Afterwards, the ava.config.js file must be in the projectDir. We won't walk the file system to find it. (This means the findFileUp logic above is not necessary.). Looks like you're using Prettier. Could you exclude unrelated changes from the PR?. Oh! Since we're no longer targeting Node.js 4, this could be:\njs\nconst {extensions: doNotCompileExtensions = []} = apiOptions;\nI wouldn't set the default 'js' value here, since it depends on whether apiOptions.babelConfig === null. Instead:\n```js\nconst {extensions: babelExtensions = []} = apiOptions.babelConfig || {};\nif (!apiOptions.babelConfig && doNotCompileExtensions.length === 0) {\n  doNotCompileExtensions.push('js');\n}\nif (apiOptions.babelConfig && babelExtensions.length === 0) {\n  babelExtensions.push('js');\n}\n```\nSome of this could be done through destructuring but it gets rather difficult to understand.. This needs a check to ensure any extension in doNotCompileExtensions is not also listed in babelExtensions.. How about:\njs\nreturn Object.keys(conf).every(key => key === 'extensions' || key === 'testOptions')`. I think it might be easier not to set a default here, but always ensure extensions is an array. Then the code in api.js could be simplified further.. This needs to check whether extensions, if present, is an array.\nIt might be worthwhile creating a let valid = true variable and then splitting the checks up into multiple if conditions, each of which could assign false, before throwing the error only if !valid.. Why this change? (Same for the other files.). Did you intent to remove the \"babel\": \"inherit\" line, rather than the\"require\"bit?. Could you change this to \"in the Babel recipe\"?.esmshouldn't impactrequire()calls though. Happy to see the Node.js builtins loaded first, but your comment hints at anesmbug. Is this still happening for you?. Doesesmtake care of this?. Let's pass{projectDir}, so we can extend this with other options later.. This copies thedefaults*into*config. The_meta` bit seems unnecessary to me.\nBetter would be Object.assign({}, defaults, config, {projectDir}).. Don't forget to remove \ud83d\ude09 . If you wouldn't mind, let's be fancy here and use [...doNotCompileExtensions, ...babelExtensions] \ud83d\udcaf \u2763\ufe0f . Would this be too much? [...new Set([...doNotCompileExtensions, ...babelExtensions])] \ud83d\ude0a \nThe Set constructor will deduplicate, and we can then turn it back into an array using the spread syntax.. This second line needs to start with invalid = invalid || \u2026.\nIt'll be clearer to put the condition in an if clause and then explicitly assign true.. Could you revert this change?. This just needs to ensure there is no overlap between both arrays. You could compare the length of allExtensions to the combined length of doNotCompileExtensions and babelExtensions: it should be equal.\nWe should probably include the ambiguous extensions in the error message though. Some applications of lodash.difference should do the trick.\nGiven that we're now throwing an error, could you move this logic into the constructor? Feel free to save allExtensions on the instance.. I suspect you may have had a merge conflict here? Use transformSpy rather than babel.transform in this line.. OK so the tricky thing is we still want to compile enhancements, unless that's also disabled. This currently does use Babel, but it won't apply AVA's regular presets. I think this means instantiating a second precompilation object, and doing a second build of Babel configuration.\nI feel bad for not spotting this earlier, and it's a bit tricky to line all this up correctly. Let me know if you want me to push a commit for this instead.. This works for me, Node.js 8.11.0, and esm@3.0.22.. Ah, I meant doing config({projectDir}), not loadConfigFile({projectDir}).. Let's do const requireEsm though. We don't need to change any other imports.. The package-lock.json diff seems surprisingly large given this one change. Might be good to git checkout master package-lock.json and npm i again. (At least, I think that's the right git incantation.). This comment can be removed now.. Would be good to have a fixture with an __esModule export.\nLet's ignore the edge case of export default {__esModule: true, default: {}} \ud83d\ude09 . @sindresorhus I can't quite recall. I can see users outputting this file from a build step though, so it'd be annoying if that wouldn't work.\nBut we could start there. I think esm can be configured to not allow CJS.. You need to compare two arrays for difference to return anything. But then it actually gives the extensions that are not repeated, so:\njs\nconst notRepeated = difference(doNotCompileExtensions, babelExtensions);\nconst duplicates = this.allExtensions.filter(ext => !notRepeated.includes(ext));\nI suppose this still needs a test?. Missing comma.. This needs some elaboration. How about:\n\nExtensions of test files that are not precompiled using AVA's Babel presets. Note that files are still compiled to enable power-assert and other features, so you may also need to set compileEnhancements to false if your files are not valid JavaScript.. How about:\nExtensions of test files that will be precompiled using AVA's Babel presets.. I want to play around with the esm options. I think we can make it always treat these files as ESM, so we don't have to support the module.exports workarounds etc. Would be neat to start with that. What do you think @sindresorhus?. The config({projectDir}) return value also must not be a promise. Happy to fix this up myself.. Perhaps:\nThis function must return a config object, it mustn't return a promise.. projectDir will be an absolute path, so it'd be good if the example didn't show dots which imply it's somehow relative.\n\nProviding the directory is useful if you have share AVA config amongst projects, e.g. in a work setting or a monorepo. You could then differentiate between projects when necessary. Perhaps this is something we should cover in a recipe, outside of this PR.. I'm not sure about advocating AVA to be run with different NODE_ENV settings. It'd be more interesting to detect CI environments \u2014\u00a0although at that point we should forward the detection to the function. Perhaps remove for now and track in a follow-up issue?. Does import test from 'ava' not work?. Hopefully we can land https://github.com/avajs/ava/issues/1836 first, and then this should be removed.. We show the default export everywhere else so let\u2019s keep doing the same here.. This line is unnecessary. However, rather than the line at the end of this section, perhaps this should say something like:\n\nThe below example uses @babel/register and assumes you're using Babel 7 to compile your source files. If you're using Babel 6, use babel-register instead.\n\nWhat do you think?. Ah yes, I guess I should be careful not to close that issue. Ran out of time though on Sunday, barely got this PR out \ud83d\ude04 . We should only shim this in Node.js versions where it's expected to be available.. Perhaps leave as undefined if there is no getColorDepth(), and use that as a signal of whether to shim or not?\nOr null if you prefer.. Could you rename the fixtures (and this integration test) to node-assertions? \"Custom\" implies support for a wider range of assertions, like Chai.. This should be await t.notThrowsAsync(Js.Promise.resolve(true\"));.\n. We don't tend to put \"should\" in test titles. \"1 equals 1\" would be more AVA-like.. t.pass() is unnecessary here.. This should be test.cb() with a t.end() call in the timeout.. Use a Map.. Use a Set.. To be honest we don't really bother with these comments \ud83d\ude04 . timeout applies to the set of test files that are currently running. However unless --fail-fast is set, AVA will continue to run remaining test files. This means multiple timeouts may occur.\nWe should include the timed out files when the event is emitted: https://github.com/avajs/ava/blob/55e40213398dbcfd7f9941697e556b335f5229f2/api.js#L77\nThen if you pass the evt to runningTestSummary() we could print the files appropriate for each particular timeout.. This timeout test ought not to be here. I think I left it in on purpose since it was reasonably small, but obviously that's no longer the case \ud83d\ude04 Let's remove these additions and write a reporter test instead.. It'd be nice to lint fixtures though.. Why do you need to push process.stderr (and stdout, below)?\nIn isatty, fd is supposed to be a number according to the Node.js documentation, so pushing the numbers should be sufficient.. fakeTTYs may be better as a Set rather than an array. You could then simplify this function to fd => fakeTTYs.has(fd) || isatty(fd).. This assigns colorDepth. Better to destructure options and assign just columns and rows:\n```js\nconst simulateTTY = (stream, {colorDepth, columns, rows}) => {\n  Object.assign(stream, {isTTY: true, columns, rows})\nif (colorDepth !== undefined) {\n```. In all of these, you could write the assertion method like this:\n```js\nconst assertNoColor = (t, stream) => {\n  // assertions here\n}\ntest('title', assertNoColor, process.stderr)\n```\nThis uses test macros.. I reckon we need to change slash() snapFile too.. Let's camelCase the type values, like with failFast.\nLet's use timeoutInMultipleFiles rather than \"multi files\".. Use type.startsWith('timeout').\n. Could you remove this comment?. Could you remove the new assertions in this test, and undo the changes in long-running.js?. Use this.prefixTitle(file, title) instead.. Change test to title.. You don't need to add the space. However let's put a \\n in front and put a colon after running.. This no longer works. Use const test = require('ava').. We're not very keen on globals with AVA. Instead, export a setup function that takes a test object. Then use that to hook up before and after.\nThis function also gets a t object. I would assign browser and page to t.context.browser, etc. Then in the tests you can access them via t.context and you can avoid the globals.. It might be neat to be able to pass the URL to the setup function instead.. This would be easier in a separate writeTimeoutSummary function, which would let us use lineWriter.writeLine rather than concatenating strings.. We can actually take currentFile.size. runningTestsCount is a global count.. You should move this condition down to after the retval = fn() line, and check if isPromise(retval). Currently this calls fn twice, which is not what we want.\nI'd only hook up the no-op catch() once we know retval is a promise. There's a noop function already declared that you can use.. I don't think printing the fn function is useful in this scenario. Let's remove values altogether.. \ud83e\udd14 . You can type npm t rather than npm run test if you prefer. (npm test works as well.). It's a type exception. I think maybe there's a way to indicate that to tsc?. Done.. Would that interfere with people's editor setup?. I'd add a meta argument to createChain, and assign that to root.meta.\nLet's see what other use cases there are before extending this further.. Jest used to have \"painless\". They're using \"delightful\" now. Mocha is \"Simple, flexible, fun.\"\nPerhaps I'm cynical but to me testing is something I need to get done. I may get some satisfaction out of covering difficult scenarios, but I don't need it to be fun or delightful.. I like \"can be a drag,\" by the way.. I'd think this would no longer be the case. Is it still needed?. It may make sense to rename opts.stack. Perhaps opts.objectWithStack? Same for the stack variable and getStack() function.. I'm not sure how this impacts the captured stack trace. Perhaps the AssertionError constructor needs to do this stackTraceLimit dance, and explicitly capture the stack trace excluding the getStack() function?. Same here, I reckon we should still capture the stack trace without the start function.. Agreed. Why does this resolve with null?. Let's make this assertionErrors: AssertionError[]. That gives us future compatibility for when we start reporting multiple assertion errors in a test.\nI guess we''ll have to type AVA's assertion error as well then\u2026. I don't think duration, title, logs and metadata need to be exposed.. Rather than overriding these, I think you should add a top-level option to indicate the test is running as part of t.try(). Perhaps inline: true. Then, logic surrounding failing and t.end() should adjust its behavior based on that option.. I don't think you need to adjust the title.. That said, you should start the snapshotInvocationCount of the new test at value of this.snapshotInvocationCount. That way you could use t.snapshot(), then t.try(u => u.snapshot()).\nOf course that creates a problem if you have concurrent t.try(). We could allow that, but we can't let you commit more than one if both contain snapshots. So instead, perhaps we should ensure there's only one concurrent t.try().. I don't think you need to track the number of attempts.. Wrong error message.. Wrong error message.. I think you need to forward the assertCount of the attempt, and add it to this.assertCount. Rather than passing an includeCount boolean let's make sure we call a different function which discards the attempt.. You can use function tryTest(fn, ...args) { here.. Perhaps Can't commit result that was previously discarded.. Do you reckon this should be a no-op instead?. Could you remove the toCommit argument? It's odd to pass false in order to get discard behavior. That should be a separate function.. It might be better to have one commitAttempt(ret) function call which performs all the relevant actions based on ret's state.. t => fn(t, ...args) should work.. This seems to be missing a few properties that are declared in the TypeScript definition.. I think that's fine, it's only a problem if it was committed rather than discarded.. Perhaps this should be a no-op.. We should change that code to pass the error itself.. I can't quite recall how these APIs work. If we can make these modifications when actually reading the stack trace perhaps it'd work fine, and then we could just use new Error() rather than getStack().. Let's use Object.freeze() to ensure the meta object is read-only.. Could you change Currently to be lower-case?. Could you duplicate this section in https://github.com/avajs/ava/blob/master/docs/02-execution-context.md?. // 100 milliseconds, rather than # \u2026. Perhaps // Write your assertions here. Rather than wrapping the assertions, I think it would be sufficient to add a call to refreshTimeout() in countPassedAssertion(), addPendingAssertion() and addFailedAssertion(). The timer should be cleared once again when the pending assertion resolves.. This should probably be a no-op if the test has already finished (or failed).. When the test finishes or fails, the timeout should be cleared to ensure it doesn't keep the worker process alive unnecessarily. Though, at the moment the process is exited when all tests have completed, see #1718 for a proposal to change that.. You can use findIndex() for this.. Interesting. I don't think we'd be able to stop execution, unless we implement some form of cancellation signaling.\nPerhaps then we should remove discard-before-resolve for now.. Discarding should be explicit. You could have a straight-forward assertion t.true(enabled()) and then forget to commit the t.try() even if it failed. Then your test seems to pass even though it shouldn't have.. The method would sit on Test.prototype, so you can call this.commitAttempt(ret). That implementation can safely access Test internals and you wouldn't need the additional count methods etc.. Yes of course.. No, I meant that it shouldn't matter whether you discard the promise or the result, even if the promise has been fulfilled.. The snapshots are indexed on the title, so if currently you only commit the third attempt, then later if the first attempt is committed you won't actually compare the snapshot. See also #1585.. Not sure at the moment, sorry \ud83d\ude04 . Not sure at the moment, sorry \ud83d\ude04 . I'm not at all opposed to tracking this stuff here, pending a larger overhaul of the reporters. However there may be race conditions where we time out one worker, and receive events from another right before we make it exit. I don't think you can reset pendingTests like this without adding guards in the other case statements, and that starts to feel iffy.\nThat's probably why the logic was in the (verbose) reporters.\nevent.tests is perhaps not the best naming either. Perhaps activeTests or concurrentTests? But then again it may be best to revert to the existing logic. We can backport to the mini reporter later.\n. Why does this explicitly exit (with an exit code), but when AVA runs without the watcher it doesn't?. Perhaps remove the -T argument to make it clearer this only deals with SIGINT.. The original copy seems better for the --timeout scenario. What made you change it?. I'm not sure why this should be the last line?. Yea I can see that. The old text was a very technical description of the timeout behavior, but I suppose it doesn't matter. Let's go with your simplified version.. I think we still need this check. Timed out workers may still emit events after we determine they've timed out, which shouldn't cause the timer to restart.. This should be timeout not bailing.. We should only interrupt the currently active workers. Any scheduled workers should still execute.. I think it's fine to use process.chdir(). I'm not particularly concerned about cases where AVA is loaded into an existing process.. This may lead to false positives. You should really check each arg individually.. The Boolean() cast seems unnecessary?. If there's 2 files, and concurrency is 3, we should still use the fork pool right?\nFor debugging, changing to the single process pool may impact behavior. I wonder if for now we should retain the existing behavior and improve debugging later?\nSimilarly, if you run just a single test file you may end up using the single process pool and see different test behavior. Perhaps we should change the configuration so if --fork is passed then that forces forking, if --no-fork is passed then we never fork, and otherwise you get the behavior defined here (minus the debugging changes).. AVA please \ud83d\ude04 . In the original code, the required module is not invoked. Why is it invoked here?. Should the main module be invoked?. Why did this need moving?. I think we can remove profile.js with this PR. It's pretty awkward and you could now run npx ava --no-fork --inspect my-test.js instead.\nThen we can also remove the overrideConfig from the lib/cli.. Use a Set instead.. Looks like run-status needs updating?. This should be decided higher-up I think.. Use a for/of loop.. This\u2026 gives a single Fork the responsibility to run multiple test files? I can't quite tell how Fork implements this but it also seems a little odd. You could end up with a slow fork that still has two pending files while other slots are idle.. Use for/of.. Need to rethrow other errors.. You can throw, no need to return a rejected promise.. I'm not a fan of this approach. I'd rather build on the state change notification mechanism to then interact with specific forks or the pool.. throw err. Since you provided the 'utf8' encoding readFileSync() already returns strings.. I'm not sure we need to have any test file concurrency within the process. That would also simplify the implementation.. I don't think we do this any other time? Is it because when you launch a child process that already takes care of resolving the symlink?. You can just return.. Why communicate through process?. Errors should be errors. Let's find a different way of communicating these events.. This changes the current behavior. If process.env.NODE_ENV === '', that should be preserved.. ?. I'd like to see if we could type this to accept a macro type. Happy to try that out myself.. We don't currently return an array of errors do we?. Let's not forget to add inline documentation.. \"a result\" rather than \"the result\" I think.. This should be the number of assertions made in the attempt, not hardcoded to 1.. Add or discarded.. I think we need to check for both symbols, to maintain compatibility with projects using older esm versions. Can check for the new symbol first of course.. Why expose this?. let TestPool?. Within the fork, the tests are not isolated, right? We should mention that (and update the derived documentation).. We should mention the required Node.js version.. Which combinations do you think should be allowed? I'm tempted to argue it's either --share-forks, or--single-process, *or* --worker-threads.. Why change the behavior here?. Why is this necessary?. Why are you pushing onto this.forks here?. resolve(this.forks[i].promise).. Should reject the outer promise if there's an error.. Let's rename this to ready or something along those lines.. Ideally we start running tests as soon as each fork is ready, rather than waiting for all.. Would it be clearer if revertGlobal() was initRevertGlobal()? And perhaps assign to this rather than a module level variable.. Is there anything else in options that shouldn't be passed to Runner?. Why disable code coverage, here and elsewhere?. This looks like a rather different approach, I guess because of caching we can't build as good a dependency map. But children here won't give us a full picture either. Is there a case for not emitting any of these dependencies? It'll affect the watcher so we should document that. The same will go for shared forks.. I'm really not a fan of JSDoc to be honest. Do we need it?. We can't introduce globals. You could still rely on the require cache to access a shared runner.. The option is timeout (singular).\nThe link is wrong, it should be for https://github.com/avajs/ava/blob/master/docs/07-test-timeouts.md. Rather than \"tests\" I'd say \"See our [timeout documentation] for more options.\". You should require chalk from ./chalk. That way you get a properly configured instance.. It'd be clearer if you replace whitespace from END_MESSAGE here rather than at the top, since the string assignment looks so much like the one in lib/watcher.js.. I think it'd be good to write another \\n at the end. That way if the tests re-rerun (because you change files) there is an empty line before the test run separator.. There's a typo here.. Let's keep this test case, but then with t.assert().. I guess @chocolateboy's point is that you also changed the value of foo. But, in the end, the test still fails, so it doesn't change too much.. I'd have expected codecov to combine coverage reports across all the Node.js versions we test on. This isn't happening?. ",
    "cloudkite": "Web workers are only supported in IE11+ see http://caniuse.com/#feat=webworkers\nAlso you don't have access to a bunch of things in web workers such as the window or DOM in web workers so it would limit the usefulness of browser testing\n. ",
    "FredyC": "I am wondering if there is anything that I could do to help with this. \nPersonally I wouldn't even mind if those tests would not run in parallel. If I delegate test running to some service like Saucelabs and it will run all tests in series for each browser, it should be more than fine. Going for hassle with web workers just seems like overkill.\n. > I think iframes should work OK. We may need to come up with something to allow for apps that care about window.top, but we can punt on that for now.\nSounds to me like trying to cover for some eventuality that is actually rarely needed. Or at least I don't have any experience when I would need to rely on window.top. \nIn my case it's merely about having ability to run library unit tests in browser environment to ensure there isn't some incompatibility. As much as its nice to have tests written in same way, I wouldn't probably choose Ava for writing/running E2E tests as there are surely some other more mature tools for that.\nI will have a look at that PR, thanks.\n. I must say this was great source of frustration for me as well. At certain point I was sure that my config is correct, but tests runs were still saying it's wrong all over again. I ended up disabling cache while I was configuring when I found out about it.\nPerhaps disabling cache by default might be another way to this and add there a warning that cache is disabled and how to enable it to improve performance. That way you are making user aware that cache is there. Sounds better to me then trying to cover whole babel configuration chain.\n. ",
    "andrewmclagan": "That sounds like a splendid idea.\n. ",
    "gajus": "Can the title of this issue be set to \"Browser support\". The \"Browserify\" part makes it sound off-topic.. I am lost \u2013 what is the current accepted way to set timeout per test? Is it:\njs\ntest((t) => {\n    setTimeout(t.end, 1000);\n});\n?\n. How to make an exception? (disable this behaviour). Because I want to structure my test folder hierarchy to mirror the project hierarchy. My project has folder helpers that I want to test.. It is a bit arrogant of \"ava\" to assume that \"helpers\" is a folder that will be used exclusively by the test runner. Instead of changing the globbing pattern, change the folder name to \"avahelpers\".. I am running into a similar issue. The company I am consulting is running in-house NPM server, which restricts package.json (that is being published) to specific properties.\nI would like there to be either an .avarc file or ability to pass all options (such as the one described in this post) using the CLI parameters.\n. > @gajus does it reject the package.json when publishing? Or are their other rejections in place that prevent you from even committing (e.g. CI).\nThe former: attempt to publish a package with unknown fields will result in HTTP status 400.\n. Again: in house and probably totally not compliant with the specs (not that such spec exists in the first place).\n\nOn May 11, 2016, at 20:05, James Talmage notifications@github.com wrote:\nThe former: attempt to publish a package with unknown fields will result in HTTP status 400.\nWhat NPM server is doing that?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\n. > AVA adheres to the official npm.\n\nThere is no such thing as the official NPM (there is no spec for what the API, there is no spec defining the structure of the package.json [apart from the supported properties]). \n\nIt's a very common pattern to have config just in package.json.\n\nUh, really? I have not needed to use it once.\nFurthermore, this limits code portability. It is easy to copy .eslintrc/ .babelrc from one directory to another. It is not so much obvious to copy some arbitrary properties from package.json.\n. Not listening to what community wants and driving the project solely by ones personal preferences will simple result in ava-io...\nNot that I care. We stopped using ava for this and several other reasons. Just chipping in to the discussion.\n. I agree that feature creep is a thing. I disagree that wanting to have a separate config file is an edge-case.\nThere are multiple reasons for wanting to have a separate config file:\n- Version control (being able to see what is the scope of the changes without viewing the diff).\n- Project scanability \u2013\u00a0I see .avarc therefore I know that you are using ava test runner.\n- Scaffolding.\n- Convention.\n- Tight coupling with NPM. What if we are not using NPM in the first place.\nFor these reasons, I would want ava to accept configuration only through .avarc.\nI agree that these reasons are opinionated and have little practical impact.\n\nI would appreciate if you would elaborate on what other things stopped you from using AVA :)\n\nNo business case. Our current test suits work. I have suggested ava as an improvement to our development, CI flow. My main argument was that it will enforce atomic tests and parallelisation will reduce the waiting time in day to day development.\nNote, when I am talking about \"our\", I am referring to the current company I am consulting. I am already using ava myself in open source and other personal projects.\n. and having one giant package.json that includes config of arbitrary software is Okay?\nI come from a different planet.\n. > I wonder if it's related to the Babel stack trace being spit out. Any chance you have a reproduction of this in a repo?\n@novemberborn Sorry, that was a code in a private repository. I have not had a chance to test it using #master.\n\n6.1.0\n\n@jamestalmage v6.1.0.\n. The same issue is present with Flow type.\ncall of method `todo`: /dev/test/Plugin/#constructor.js:26\nFunction cannot be called on any member of intersection type\nintersection: /dev/node_modules/ava/index.js.flow:159\nMember 1:\nfunction type: /dev/node_modules/ava/index.js.flow:160\nError:\nstring: /dev/test/Plugin/#constructor.js:26\nThis type is incompatible with the expected param type of\nfunction type: /dev/node_modules/ava/index.js.flow:160\nMember 2:\nfunction type: /dev/node_modules/ava/index.js.flow:161\nError:\nundefined (too few arguments, expected default/rest parameters): /dev/test/Plugin/#constructor.js:26\nThis type is incompatible with\nfunction type: /dev/node_modules/ava/index.js.flow:161\nMember 3:\nfunction type: /dev/node_modules/ava/index.js.flow:162\nError:\nstring: /dev/test/Plugin/#constructor.js:26\nThis type is incompatible with\nunion: type application of polymorphic type: type `Macro` | array type: /dev/node_modules/ava/index.js.flow:162\nMember 4:\nfunction type: /dev/node_modules/ava/index.js.flow:163\nError:\nundefined (too few arguments, expected default/rest parameters): /dev/test/Plugin/#constructor.js:26\nThis type is incompatible with\nunion: type application of polymorphic type: type `Macro` | array type: /dev/node_modules/ava/index.js.flow:163. > Where are you setting / overwriting the variable?\nSorry, forgot an important fragment.\nIn test block.\n``` js\nimport test from 'ava';\nimport findNearbyGoogleCountries from '../../src/services/findNearbyGoogleCountries';\ntest.beforeEach((assert) => {\n  assert.is(process.env.GOOGLE_API_KEY, undefined);\n});\n@see https://github.com/avajs/ava/issues/1106\ntest('makes a request using the GOOGLE_API_KEY', async (assert) => {\n  process.env.GOOGLE_API_KEY = 'foo';\nconst scope = nock('https://maps.googleapis.com')\n    .get('/maps/api/geocode/json')\n    .reply(200);\nfindNearbyGoogleCountries(1, 1);\nassert.true(scope.isDone());\n});\n```\n. Oh. This just made me realize it is not a bug in ava.\nI guess the way to fix this is to use commonjs require in a test block and de-cache it.\n. Right. I apologize. I was under impression that there is a default global timeout.. To clarify, this proves that ava reads and interprets .babelrc even though it is provided an explicit babel configuration in the ava configuration object of ./package.json.\n.babelrc intentionally includes a plugin that does not exist to trigger an error:\njson\n{\n  \"plugins\": [\n    \"test\"\n  ]\n}. Note that adding \"babelrc\": false, does not change this behaviour.. On a related note, this issue can be used to replicate another issue that I cannot figure out:\n```bash\ngit clone git@github.com:gajus/ava-issue-1146.git\ncd ./ava-issue-1146\nnpm install\necho '{}' > .babelrc\nava --verbose ./test/EventEmitter/#off.js\n```\nThis will give an error:\n```\n/Users/gajuskuizinas/.Trash/ava-issue-1145/node_modules/babel-core/lib/transformation/file/index.js:600\n      throw err;\n      ^\nSyntaxError: /Users/gajuskuizinas/.Trash/ava-issue-1145/src/EventEmitter.js: Unexpected token, expected ; (5:5)\n  3 | / eslint-disable promise/prefer-await-to-callbacks /\n  4 |\n\n5 | type SubscriberType = (data: any) => void;\n    |      ^\n  6 |\n  7 | type SubscriptionsType = {\n  8 |   [key: string]: Array\n```\n\nWhich makes no sense since ava is configured to use transform-flow-strip-types transpiler.. There are two assumptions that mislead me:\n\nI thought that if I configure {\"ava\": {\"babel\": {}}} it will be used to transpile all the code, not only whats in ./test.\nAfter I have been pointed out that the latter is not the case, my first thought was to add custom babel-register config that will not ignore ./src. This would not have worked too, because babel-register would have picked up .babelrc config anyway and not the Babel config I have set in \"ava\".\n\nThe reason I got into this situation in the first place is because ava requires ES modules transpilation, while the platform for which I am building the project does not.. I have ended up using different .babelrc settings for development, production and testing.. Can this be marked as a bug?\nI am not sure how to fix it... I would raise a PR otherwise.\nIt is breaking our integration tests. Annoyingly, I cannot even use FlowFixMe comment because it then throws error in ava definition. Therefore, we had to comment out async tests just to make Flow stop complaining about these assertions.. @leebyron Could you have a look at this issue please?\n(Since you have tackled a similar issue here https://github.com/avajs/ava/pull/1164.). I am so excited about this. Cannot wait to see it out. :-)\n\n. > In 0.15.2 it works if you cd into the module that contains the tests and then do ../.bin/ava with all of the flas and path that you need.\nAssuming you mean something like ava ./test/**/*, yes that works. However, behaviour of ava should not change depend on the current directory of the project, i.e. ava should work just as well.. I haven't had time to property isolate this, but using the latest code I am getting a lot of ambiguity errors that didn't exist before, e.g.\ntest/showtime-data-sources-management-api/loaders/DataSourceByDataSourceIdLoader.js:19\n 19: test.after.always(() => {\n     ^ call of method `always`. Could not decide which case to select\n159: type ContextualTestMethod = {\n                                 ^ callable object type. See: node_modules/ava/index.js.flow:159\n  Case 1 may work:\n  160:  (              implementation: ContextualTest): void;\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ function type. See: node_modules/ava/index.js.flow:160\n  But if it doesn't, case 3 looks promising too:\n  162:  (              implementation: Macros<ContextualTestContext>, ...args: Array<any>): void;\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ function type. See: node_modules/ava/index.js.flow:162\n  Please provide additional annotation(s) to determine whether case 1 works (or consider merging it with case 3):\n   19: test.after.always(() => {\n                              ^ return\nFor the following code:\njs\ntest.after.always(() => {\n  mockKnex.unmock(knex);\n});. Adding :void return type annotation fixes it, though.. > @gajus - that's a good sign, it means the original flow error you encountered was solved so now flow is finding other unrelated issues.\nWhile I agree that it is an expected behaviour, it will brake all of my CI deployments. :-(\nI suppose that when it comes to making breaking Flow annotation changes, it is still a grey zone.. Okay. What I have said is not true. ava is still 0.x. Therefore ^0.17 does not automatically pick up 0.18. Therefore, it can be a breaking change.. > I'm not sure that this is a breaking change - you should only be seeing other issues if they were originally masked by this issue - that is this change can bring you from being broken in two ways to being broken in one way, but not from working to being broken.\nI use continuous delivery. Every time someone pushes the code, CI performs npm install & flow before deploying the service. Suddenly the CI breaks because of a (suppose) patch update to ava. Thats a breaking change. Don't you agree?\nImagine if we discover a bug in production and urgently need to push the update. Discovering that Flow suddenly started breaking would be unfortunate.. cc @jfmengels . > I'd rather have a t.keyOrder(result, ['stderr', 'stdout', 'exitCode', Symbol.for('foo'), Symbol.for('bar')]) assertion than a t.strictDeepEqual()\nThis assumes {[key: string]: boolean | number | string | null} array result, i.e. no nesting. Assuming nesting...\n\nbut even that could be done through a third party library with the existing assertions: t.deepEqual(keyOrder(result), [\u2026]).\n\nIt cannot.\nHow are you going to deep equal two objects where result object property order is important?\n\nbut I think this only matters in pretty rare cases. \n\nI agree that it is a pretty rare case, though. Furthermore, breaking the object into primitive value objects and comparing Object.keys values could be used as a way to check that property order is correct.\nRegardless, I am raising this issue because I (a) ran into such a case and (b) because the previous issue was closed on false grounds.\nAs long as everyone is agreement that this feature is non-essential to the core, it is safe to close the issue.. This works. :+1:. Whats the shipping plan?. Seems like there is already a PR to fix this.\nhttps://github.com/avajs/ava/pull/1778. CC @hallettj I am assuming this could be related to https://github.com/avajs/ava/pull/1778.. > I'm not really interested in backporting to v0.25.0. If it hadn't been for Babel 7 being in beta we'd be at v0.29.0 by now, and we wouldn't have backported anything either.\nWhats the solution?. My first thought was that maybe Babel is messing up the regex, but that does not appear to be the case:\n```bash\nbabel test/validators/validateVenue.js\n\"use strict\";\nvar _ava = _interopRequireDefault(require(\"ava\"));\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n// @flow\n(0, _ava.default)('foo', t => {\n  const URL_RULE = /^((https?|ftp):\\/\\/)([\\0-\\x08\\x0E-\\x1F!-\\x9F\\xA1-\\u167F\\u1681-\\u1FFF\\u200B-\\u2027\\u202A-\\u202E\\u2030-\\u205E\\u2060-\\u2FFF\\u3001-\\uFEFE\\uFF00-\\u{10FFFF}]+(:[\\0-\\x08\\x0E-\\x1F!-\\x9F\\xA1-\\u167F\\u1681-\\u1FFF\\u200B-\\u2027\\u202A-\\u202E\\u2030-\\u205E\\u2060-\\u2FFF\\u3001-\\uFEFE\\uFF00-\\u{10FFFF}])?@)?((?!10(.[0-9]{1,3}){3})(?!127(.[0-9]{1,3}){3})(?!169.254(.[0-9]{1,3}){2})(?!192.168(.[0-9]{1,3}){2})(?!172.(1[6-9]|2[0-9]|3[01])(.[0-9]{1,3}){2})([1-9][0-9]?|1[0-9][0-9]|2[01][0-9]|22[0-3])(.(1?[0-9]{1,2}|2[0-4][0-9]|25[0-5])){2}(.([1-9][0-9]?|1[0-9][0-9]|2[0-4][0-9]|25[0-4]))|(([0-9_a-z\\xA1-\\uFFFF]+-?)[0-9a-z\\xA1-\\uFFFF]+)(.([0-9a-z\\xA1-\\uFFFF]+-?)[0-9a-z\\xA1-\\uFFFF]+)(.([a-z\\xA1-\\uFFFF]{2,})))(:[0-9]{2,5})?(\\/[\\0-\\x08\\x0E-\\x1F!-\\x9F\\xA1-\\u167F\\u1681-\\u1FFF\\u200B-\\u2027\\u202A-\\u202E\\u2030-\\u205E\\u2060-\\u2FFF\\u3001-\\uFEFE\\uFF00-\\u{10FFFF}]*)?$/iu;\n  t.true(true);\n});\n```. Never mind. Just realised that Babel produces invalid Regex. Didn't notice that the compiled regex is different.. It appears that Ava stopped working with Babel 49 update.\nThe test files are no longer compiled using .babelrc.\nIs this related?. Master works. 1.0.0-beta.4 does not.\nWhen is the current master getting released?. > That's weird. Beta 4 has pinned its Babel dependencies so it shouldn't end up using Beta 49. Perhaps your installer is deduping in unexpected ways.\nI am using npm@5.6.0.\nIt is not an issue with the installed version of @babel/core either.\n```bash\n$ cat ./node_modules/ava/node_modules/@babel/core/package.json | jq .version\n\"7.0.0-beta.44\"\n```. > @gajus not sure what's going on then. Hopefully the next beta fixes it correctly.\nThis is currently breaking all of our builds.\nNot one package, but all of the builds.\nWhatever change happened between Babel ^7.0.0-beta.47 and ^7.0.0-beta.49 is breaking latest Ava release behaviour.. There is something even more wrong with this:\n```js\n// @flow\nimport untypedTest from 'ava';\nimport type {\n  TestInterface\n} from 'ava';\ntype ContextType = {\n  server: ServerType\n};\nconst test: TestInterface = untypedTest;\n```\n```\nCannot assign untypedTest to test because property server is missing in object type [1] but exists in ContextType [2] in\ntype argument Context [3].\n    test/integration.js\n     24\u2502   server: ServerType\n     25\u2502 };\n     26\u2502\n[2]  27\u2502 const test: TestInterface<ContextType> = untypedTest;\n     28\u2502\n     29\u2502 const SERVER_URL = 'http://127.0.0.1';\n     30\u2502\n\n    node_modules/ava/index.js.flow\n\n[3][1] 461\u2502 export interface TestInterface {\n```. @vinsonchuong Is there a more recent solution to this?. > Babel 7 should be coming out imminently. I'll try and do another beta release this weekend where we change our dependencies to ^7.0.0.\nBabel stable release didn't happen and suggesting to hardcode a dependency version is not a good solution.. @novemberborn Whats the release plan now that Babel v7 is out?. Just noticed:\nhttps://github.com/avajs/ava/issues/421. How I did not know about pretty-format. :-(\nSeems like I reinvented the wheel https://www.npmjs.com/package/prettyprint.. ",
    "elaijuh": "I am finding a way to do angular 2 unit test. All I could find is karma + jasmine + browser launcher. What I expect is I could use AVA to tackle it, so this issue looks like working on providing a browser environment for client side code.\nI have tried browser-env with no luck working together with zone.js, client side support is a solid thing to me.. ",
    "mAAdhaTTah": "I'm currently playing around with using AVA to unit test a framework I'm working on (wooo Observables!), but without being able to run the unit tests themselves in the browser, it's a non-starter. I'm definitely interested in helping out; I played around with karma-ava but couldn't get it working.\nIs there any movement on this, or anything I can look at or help out with?. Tried this: node -r \"@std/esm\" ./node_modules/.bin/ava; no dice w/ v0.8.3 of esm.\n\nwrite a file that creates the loader and require that through AVA's package.json configuration\n\nThis sounds possible, but (I think) we would have to overwrite the global require function. I attempted (ava-esm.js):\njavascript\n// Nope\nrequire('module').prototype.require = require('@std/esm')(module, { cjs: true, esm: 'js' });\n// Nah\nrequire('module').prototype.require = require('@std/esm')(module, true);\n// Still no\nrequire('module').prototype.require = require('@std/esm')(module);\nDifferent errors with each one, but none of them work. Is this the right approach to take? Any other ideas?. @jdalton Ava transpiles its test files by default, but not external files. I'm trying to use @std/esm for those so I don't need babel at all.. If AVA is handling transforming the test files via babel, shouldn't @std/esm just treat them like cjs files?. > I made the cjs option of @std/esm add the _esModule property to its export and that fixed it with Babel unit tests.\nAwesome. We can close this out then, yeah?. ",
    "wearhere": "\nI'm curious what kind of browser support people are looking for.\n\nTo be really specific, the ideal outcome for Mixmax would be for Ava to work with the Karma test runner. What Karma lets you do is take the same test files that you would run in a Node environment, and run them using real browsers or devices or PhantomJS or headless Chrome instances. Karma launches the browser, and logs results to the CLI (at least how we use it); and as far as CI goes, headless Chrome works just fine, without special configuration, on Travis. The tests do need to be compiled for the browser i.e. bundle any dependencies (including Node modules) into a single IIFE or load them as globals.\nWe see that there is a Karma plugin for Ava here but it says \"alpha level\" currently, not sure if that's blocked on anything in this project.\n(Want to try out Karma? See installation and configuration instructions here, though we can't vouch for those, we use/maintain https://github.com/mixmaxhq/erik i.e. currently use Jasmine for running unit tests in the browser, Ava only for server tests\u2014though it would be nice to use Ava everywhere!). ",
    "bj0": "sorry to bump an old issue, but I just tried using ava for the first time to test some client side js that uses Blob and SubtleCrypto apis, but they are not defined i node (I guess?  I've never used node...).  \nIs the only way to use these APIs to run in browser?. ",
    "ColCh": "Thanks. Any tutorial on how to do that? Code sample will be cool. \n. ",
    "scottcorgan": "I know that a PR is better than this question, BUT, is this still a priority?\n. Just curious if there's been any progress for the browser. Only thing keeping me from Ava.\n. Just curious if there's been any progress for the browser. Only thing keeping me from Ava.\n. Perhaps a way to specify a regex, or something like it, for file naming conventions. I'm not opposed to opinionated libraries, but I know that some people use _url_test.js or url.test.js or __test__/url.js, etc.\nMaybe something like a .avarc file would be helpful for this (and things like tap output).\n. test/url.js, tests/urls.js is all I can think of\n. You could probably just use the onchange module.\njson\n{\n  \"scripts\": {\n    \"test\": \"onchange 'test/*.js' -- ava test/*.js\"\n  }\n}\n. You could probably just use the onchange module.\njson\n{\n  \"scripts\": {\n    \"test\": \"onchange 'test/*.js' -- ava test/*.js\"\n  }\n}\n. ",
    "jprichardson": ":+1: \n. After I saw this closed by @vdemedes I assumed it was part of 0.7.0. But didn't see it listed https://github.com/sindresorhus/ava/releases/tag/v0.7.0. Since this issue is still part of 0.7.0, does that mean that it's in 0.7.0 and hence forgotten about in the release announcement or that a new milestone should be associated with this issue?\n. I have a very big project that I wanted to try ava on. After having the Activity Monitor (Mac OS X 10.11) open and seeing 50+ node process, it brought my system to its knees. It'd be cool if there was some kind of limit.\n. If you add TAP support, I can get AVA running really easily in my plugin: https://github.com/jprichardson/trinity. Sorry, no screenshots yet. Works similarly to the others. But, I use it all day, every day and it works great. Supports Mocha and Tape out of the box as it just parses TAP output. \n. > Can we defer the assert title complications for later and focus on just getting some TAP output in this PR? We can just go for naive assert titles for now.\n:+1: It's (no tap) the only thing that's preventing me from deep diving into Ava :)\n. > Can we defer the assert title complications for later and focus on just getting some TAP output in this PR? We can just go for naive assert titles for now.\n:+1: It's (no tap) the only thing that's preventing me from deep diving into Ava :)\n. > Yep. I'm so tempted to go create tap support. \nPretty please, with sugar on top :blush: \n. ",
    "jonathanKingston": "\n@jamestalmage From my reading of the TAP spec, I it does not seem very well suited to concurrency.\n\nThat is the case certainly, I am happy to discuss some of my suggestions that have been aired on TAP specification discussions however likely won't be adopted very quickly at all (Having a different output might be simpler)\n. > @jamestalmage From my reading of the TAP spec, I it does not seem very well suited to concurrency.\nThat is the case certainly, I am happy to discuss some of my suggestions that have been aired on TAP specification discussions however likely won't be adopted very quickly at all (Having a different output might be simpler)\n. ",
    "stevemao": "I think this is more intuitive for people who haven't used mocha. But would this confuse people who just learned require(./) and fs.readFile(./) as the path are relative to different base?\n. > That's not correct. If you start the node script from the file location they're the same. The point is that running a test is kinda like doing node test.js from the test file location.\nTrue, very intuitive because this is what I thought when I first used mocha :)\n. ",
    "a-s-o": "\nIs this what you mean by \"sticking with the directory where ava command is executed\"?\n\nI think you have it right, but to clarify, let's say I have the following directory structure\nproject_root\n  +-- api\n  |     +-- tests\n  |     |     |-- api_test_1.js\n  |     |     |-- api_test_2.js\n  |     |-- api.js\n  |     |-- package.json\n  |\n  |-- package.json\nSo, my suggestions is as follows:\n~/project_root> ava **/tests/*.js         <-- process.cwd is ~/project_root\n~/project_root/api> ava **/tests/*.js     <-- process.cwd is ~/project_root/api\nBasically, cwd should be the working directory where the ava command was launched. If it gets launched from a npm script, then the cwd is wherever that npm script is located.\nHope that helps\n. You are right, that is the default npm behaviour. Okay then that is a good\nenough convention to follow. I can live with that. Thanks for the info.\nOn May 22, 2016 1:57 PM, \"James Talmage\" notifications@github.com wrote:\n\nWe could add a cwd config option to package.json\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/avajs/ava/issues/32#issuecomment-220846264\n. \n",
    "KidkArolis": "What's the status of this, changing cwd to the test file location is really annoying.\n. I ran into this same issue just now. In my mocha tests I always have a mocha.opts --require test/setup config, where in the test/setup.js I could set things such as my NODE_ENV and NODE_CONFIG_DIR. I can't do the same in ava using the ava.require pkg.json config because those get required relatively to each test file...\nI wish there was a way to solve this, so that I could just run ava to run my tests without an additional npm script or makefile wrapper. Hmm.\n. I suppose in the spirit of being able to just run node foo/test.js, each test file could do a require('../_setup').\nOn the other hand, that already doesn't work cause babel and ava settings in package.json such as ava.require. So if there was a way of prerequiring a module for each test that is local to the project, that'd be great.\n. Made this for now if anyone else runs into this issue.. https://www.npmjs.com/package/ava-config\nI can also rename the pkg if ava folk think the name is too confusing.\n. ",
    "shinzui": "@sindresorhus I don't think the codemod should block this issue. I just started using ava and I wasted a lot of time figuring out why hapi was not loading my models correctly in the tests. I initially blamed hapi-shelf (plugin to load bookshelf models), but after tracing the code I discovered this issue and easily fixed my test to work around it. \n. I am still experiencing this behavior with ava 0.18.1. ",
    "wmertens": "So is anybody working on this? I'm not volunteering, just doing a \"ping\" \ud83d\ude00 \n. I would like to point out that you can already get grouping with pretty low effort:\n``` js\nconst groupTest = name => {\n  let sharedPromise\n  const withShared = fn => t => {\n    if (!sharedPromise) {\n      sharedPromise = getPromised()\n    }\n    return fn(sharedPromise, t)\n  }\ntest(${name}: test1, withShared(async (sharedP, t) => {\n     t.is(await sharedP, 5)\n  }))\n  test(${name}: test2, withShared(async (sharedP, t) => {\n     // ...\n  }))\n}\ngroupTest('group1')\ngroupTest('group2')\n```\nYou can even make the withShared simpler with a helper:\njs\nexport const sharedSetup = getPromise => fn => {\n  let promise\n  return await t => {\n     if (!promise) {\n       promise = getPromise()\n     }\n     return fn(await promise, t)\n  }\n}\njs\nconst withShared = sharedSetup(() => openedDatabasePromise())\ntest('meep', withShared((db, t) => /*...*/))\nso maybe any added API surface would be limited to helpers?\n. There are a number of issues related to this and the fix is very simple: clear/customize the helper matcher patterns.\nWould it not be possible to roll out a quick fix for this by making a helperPatterns option that replaces the default one? For people that transpile everything they can then clear that array.. Oops somehow I missed this. Yes, the goal is testing in Node. I also use webpack to run my server, and thanks to that I can do hot module reloading of my API code, great for development.\nI have been thinking some more about this, and there is an easier first option, which already works:\n Run webpack in watch mode with target: \"node\" on one entry per test file (glob them in a function you give to entry\n Run AVA in watch mode on this directory\nAll loaders, aliases  etc work as they should. As long as you don't add test files, this works. (webpack doesn't let you add entries on the fly AFAIK)\nSo the concept is working, but it would be a lot nicer if this could be done in a single process; apart from the convenience, hot module reloading could then probably be used to re-run tests faster? (although that would mean keeping each test process running, possibly not a good idea)\nEDIT: Possibly the globbing process could assign multiple tests to concurrencyNum entries, and then the processes could stay active, only running tests whose modules are hot reloaded\u2026. This approach seems to slow down my AVA runs a lot, I think it descends into my build/ dir but I don't really know, there is no debugging output for the globbing.\nHow about not scanning for helper files, but instead somehow intercepting the require mechanism that Babel uses to transpile, only allowing it to transpile imports that match helper globs?. I just went into the module, prepended the helper globs with my source dirs (e.g. '{src,lib}/**/__tests__/helpers/**/*.js'), and ava run time decreased by 20s!. Turns out the second run is for the helpers, and AVA is trying to precompile a failing helper that is not being used at all.\nMaybe this could be solved by #1292, having excludes also apply to helpers? Or allowing specifying the helpers glob?. I also believe this is an important issue: https://github.com/avajs/ava/pull/1320#issuecomment-320258236\nTry this: \n\nI just went into the module, prepended the helper globs with my source dirs (e.g. '{src,lib}//tests/helpers//*.js'), and ava run time decreased by 20s!\n\nSo in node_modules/ava/lib/ava-files.js I changed the helper patterns to:\njs\nconst defaultHelperPatterns = () => [\n        '{src,lib}/**/__tests__/helpers/**/*.js',\n];\nyou need to adjust the glob for your project of course.\n. @davidnagli are you sure? It works fine for me - I use https://github.com/avajs/ava/blob/master/docs/recipes/babelrc.md#transpiling-tests-and-sources-the-same-way. Oh wow! That's handy! I think it would be good to print that on a line\nafter the tests when in watch mode :)\nOn Sun, Sep 10, 2017 at 5:30 PM Mark Wubben notifications@github.com\nwrote:\n\nMakes sense.\nThe only alternative is to restart AVA, which can take a very long time.\nYou can type r followed by the return key, and that'll rerun all tests\nwithout having to restart the watch process.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/avajs/ava/issues/1509#issuecomment-328350153, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AADWlkMn4GW_sy0LLqdLNQ5x7nUtVtpTks5shACrgaJpZM4POeSR\n.\n. Am I reading correctly that if you have custom paths, the watcher won't even watch .snap files? Those only get added if paths.length === 0?. I would argue that package.json and .snap files are internal to AVA, and should therefore always be added. Doesn't the ignore list take precedence?\n\n. Addressed your comments, but I'm not sure how to test the ignore pattern\u2026. Hmm, no time at all right now, maybe next week; so feel free to assist, thanks! :). Well, the colors are the primary indicator for me, but I am so used to the\npatch view that reversing the order is odd to me, - means \"previous\", not\n\"wrong value that you should fix\".\nOn Mon, Oct 23, 2017, 4:50 PM Mark Wubben notifications@github.com wrote:\n\nThe idea is that the new value should be adjusted to match the old value.\nIf you'd used t.is():\ndirect diff\n/private/var/folders/2_/qczp184x76b2nl034sq5hvxw0000gn/T/tmp.dcvW5BUyA5/test.js:8\n7: test('direct diff', t => {\n   8:   t.is(\\n${Date.now() - 42e5}\\n, \\n${Date.now()}\\n)\n   9: })\nDifference:\n`\u240a\n\n\n1508765718717\u240a\n1508769918717\u240a\n    `\n\nWhat is definitely wrong though is that the colors are not inverted the\nway the gutters are. I've just opened concordancejs/concordance#40\nhttps://github.com/concordancejs/concordance/issues/40 for that.\nIf that were fixed would you be happy with the current behavior?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/avajs/ava/issues/1558#issuecomment-338684389, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AADWlqOsoB5v_2zAw7RumpYqfpqOobomks5svKe0gaJpZM4QAMuE\n.\n. Yes, the way Jest does it looks good!\n\nOn Thu, Mar 1, 2018 at 12:53 PM rzec-r7 notifications@github.com wrote:\n\n@novemberborn https://github.com/novemberborn any update on this? I\nhave been tripped up by the + / - difference with AVA so many times.\nI think what @nesbocaj https://github.com/nesbocaj said makes sense.\nFor me the colors are not as important as the - / + and since snapshots\nare tests that are just diffing 2 values, it make the most sense to have\nthe - be the existing value in the snapshot and the + be the new value\nfrom the test (I have not opinion on the colors, probably makes to to just\nmatch what jest does).\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/avajs/ava/issues/1558#issuecomment-369568743, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AADWlhVQfoQ2qiR_dK-UIn26li2_F1Wlks5tZ-FSgaJpZM4QAMuE\n.\n. ok fixed. \n",
    "floatdrop": "Fair enough.\n. Yeah, this would be sweet :sparkles: \n. Even if t.plan is specified in test - same result.\n``` js\nvar test = require('ava');\ntest('no asserts', function (t) {\n    t.plan(4);\n    // ignored and ava exit's fine\n});\n```\n. @sindresorhus yes, tracking pending assertions will cover this case also.\n. In fact I would be cool, if ava could run only changed test in a file, not a whole file with tests.\n. @SamVerschueren why not try-catch?\njs\ntest('throws error', async t => {\n    try { await fn(); } catch (err) { /* ... */ }\n});\n. @Qix- ava have test case for this.\n@alebelcor judging by test case ifError behaves correctly (throws, if value is truthy - double negative is always tough, thou).\n. > Asserts that error is truthy.\nWell, no. It asserts that error is falsy. It throws if error is truthy. \n. @sindresorhus forking is fast, spinning up Node and babel is quite slow.\n. @sindresorhus I'm afraid, cache should be implemented by hands.\nco must be replaced (or patched to use Promise polyfill, since babel now will not do this for us).\n. @vdemedes yeah, I know, but you can't use co in Node.js 0.10 without some transpiler (and I don't think, that co will adapt to it soon https://github.com/tj/co/pull/244). So we need to come up with something. I believe, that transpiling only test code will improve performance compared to patched require function.\n@sindresorhus okay, but need to do some performance tests - I wonder, how fast cache file will fill up and parsing time eat all the benefit.\n. @vdemedes co uses global Promise object - so you can't use it right away in Node.js 0.10.\n. @vdemedes generators are not a problem, yes. But ava do require co at the top of test.js and this leads to errors in 0.10 - https://travis-ci.org/sindresorhus/ava/jobs/87312158\n. JFYI: seems like babel require hook have some polyfill side-effect - https://github.com/floatdrop/ava-in-node-0.10\n. Just to keep track - waiting for https://github.com/tj/co/pull/250\n. @sindresorhus sure. I'm started to use ava in some work related projects, so I will hang around for some time :)\n. @madbence you can require file with following code in tests as workaround:\njs\n// app.es5.js\nrequire('babel/register');\nrequire('./app');\n@sindresorhus besides this workaround, not really.\n. @jamestalmage yep, thats what that fix was for, it still need some work thou (#150).\n. @sindresorhus babel transpile files with async and prepends\njs\nvar _asyncToGenerator = require('babel-runtime/helpers/async-to-generator')['default'];\nAt the top, but in module.paths of test there is no babel-runtime module:\njs\nModule {\n  id: '/Users/floatdrop/github.com/got/test/unix-socket.js',\n  exports: {},\n  parent:\n   Module {\n     id: '/Users/floatdrop/github.com/got/node_modules/ava/node_modules/require-from-string/index.js',\n     exports: [Function: requireFromString],\n     parent:\n      Module {\n        id: '.',\n        exports: {},\n        parent: null,\n        filename: '/Users/floatdrop/github.com/got/node_modules/ava/lib/babel.js',\n        loaded: false,\n        children: [Object],\n        paths: [Object] },\n     filename: '/Users/floatdrop/github.com/got/node_modules/ava/node_modules/require-from-string/index.js',\n     loaded: true,\n     children: [ [Circular] ],\n     paths:\n      [ '/Users/floatdrop/github.com/got/node_modules/ava/node_modules/require-from-string/node_modules',\n        '/Users/floatdrop/github.com/got/node_modules/ava/node_modules',\n        '/Users/floatdrop/github.com/got/node_modules',\n        '/Users/floatdrop/github.com/node_modules',\n        '/Users/floatdrop/node_modules',\n        '/Users/node_modules',\n        '/node_modules' ] },\n  filename: '/Users/floatdrop/github.com/got/test/unix-socket.js',\n  loaded: false,\n  children: [],\n  paths:\n   [ '/Users/floatdrop/github.com/got/test/node_modules',\n     '/Users/floatdrop/github.com/got/node_modules',\n     '/Users/floatdrop/github.com/node_modules',\n     '/Users/floatdrop/node_modules',\n     '/Users/node_modules',\n     '/node_modules' ] }\nIt looks like we should add some paths to paths property, but Node.js behaves the same way. Will look further in require hook for solution.\n. Removing runtime from optional helps (but it breaks Node.js 0.10.x).\n. So including runtime into optional makes Babel place require at the top. Removing it will work for Node.js with generators.\nFor Node.js 0.10.x we need regenerator runtime, which is depends on global Promise - https://github.com/facebook/regenerator/issues/142 (like co). Babel includes regenerator runtime + core.js polyfills, when used with babel-core/register.\nWe could append regenerator runtime before test code, but if we append Promise polyfill - it will leak into test code scope, which is bad (but not in module which is good). Or make Promise local to regenerator with function wrapping. \n@sindresorhus wdyt?\n. > Are we including this now? If not, that's a regression too.\n@sindresorhus all polyfills now excluded from tests, because they affect tested code. ~~I guess fastest way is to revet this commit and use babel/register.~~\n@jamestalmage resolution is happening in right directory, but yes, there is no babel-runtime around, so we either need to append parts of it to code (regenerator and Promise).\n. @sindresorhus then it is worth to workaround require somehow:\n- install babel-runtime as peer-dependency\n- hack on m.paths in require-from-string\n- replace runtime require path with resolved one \n. @sindresorhus yeah. I think I give a m.paths a shot, because I don't want to mess with replacing require statements.\n. > Couldn't it just read the runtime from https://github.com/facebook/regenerator ?\nThis is what Make it less horrible part for ;)\n. Fixed.\n. > It is possible to wrap the existing babel runtime plugins (both v5 and v6) and manipulate the require paths they put in the source.\n@jamestalmage yes, this is the way to fix this :+1:\n. @sindresorhus most likely, because this line rejects to undefined and cli trys to get stack from undefined.\n. @sindresorhus probably because we kill process after all tests are done - and sometimes Node.js 0.10 will return SIGTERM exit code (143) which is trigger error handler.\n. @jamestalmage thought of this too. We can transpile files with babel in master process and load transpiled code from cache (as soon as I deal with #118).\n. @jamestalmage yes, but do it step by step (transpile one test and launch it), not all at once.\n. @jamestalmage yes, but do it step by step (transpile one test and launch it), not all at once.\n. Great work! - reliable builds on Windows is must have. Rough edges can be improved from there.\nI'm think that unhandled related code (and serialize-value replacement too?) --should-- could be moved in separate PR - it will be easier to find corresponding code to fixes later. Not critical thou.\n. @jamestalmage no offence at all (sorry if it seems so, English is not my strongest part), really appreciate work you done in ava and related modules. I understand, that rebasing unhandledRejection is hard and not insisting on it. Thanks for explaining serialize-value part.\n. Yeah, maybe we should fix babel-plugin-transform-regenerator to 6.3.26 version somehow, as suggested in issue about this - https://phabricator.babeljs.io/T2892 ?\n. Yeah, maybe we should fix babel-plugin-transform-regenerator to 6.3.26 version somehow, as suggested in issue about this - https://phabricator.babeljs.io/T2892 ?\n. I would like to bring it up with recent releases of Node.JS \u2013 what would ava miss without babel besides async/await (which can be transpiled with async-to-gen) and import? Babel taking considerable amount of time to install and run.. @f0rmat1k babel-register will not pick up configuration from ava.babel in package.json. You should configure babel-register elsewhere. See Transpiling Sources recipe for details.\n. Is this test relevant? setTimeout will block Node from exit anyway.\n. :ok:\n. Yes, it is lame to include polyfills in test files - but it is bad to include global polyfills either (because they leak to production code).\n. We need runtime, because of polyfils - https://github.com/sindresorhus/ava/issues/144#issuecomment-154691249\n. How about moving types to something like new Test.hook(title, cb) (can be done in separate PR)?\n. cache.clean() removes outdated files from cache directory (so it will not grow endlessly). Function name a little misleading, true.\n. > Why can't it just do that on every call for expired ttls?\nCalling clean on every get call will be expensive (for example, if cache is big enough, like 1000 files) - we need to read cache directory and check every file atime.\n. @sindresorhus yep. May be this is not worth a shot yet, but in future we could want to eliminate these string constants.\n. @sindresorhus yes, this makes sense. I'm up to something like update-notifier Configstore.\n. @sindresorhus done.\n@jamestalmage you are perfectly right. Until #202 is resolved we can hash with ava version.\n. ",
    "uiureo": "Because I don't intend this PR to be merged. I don't want the opened state to bother you guys.\n\nIt'd be nice if this translated into some way to hook into Ava to allow it to use any arbitrary assertion library, including support for t.plan().\n\nThat's the very thing I want.\nI'm looking into the code and I would try if I come up with some way.\nRelated: #25\n. > the result, while being very informative, is a bit noisy. foo === bar is a lot clearer and faster to grasp. Is there any way to simplify the output?\nI agree its output is a little bit noisy. It seems you can write custom output formatter in power-assert.\nhttps://github.com/power-assert-js/power-assert#optionsoutput\nhttps://github.com/power-assert-js/power-assert-formatter\nFYI @twada\n. I did quick hack to use the succinct renderer and fixed the merge conflict.\nNow the output looks like this:\n``` sh\n\u276f npm test    \n\nava\n\n\u2716 [anonymous] \n  t.ok(a.name === 'bar')\n         |            \n         \"foo\"          \n1 test failed\n\n[anonymous]\n  AssertionError: \n  t.ok(a.name === 'bar')\n         |            \n         \"foo\"        \n    at decoratedAssert (/Users/zat/.ghq/github.com/sindresorhus/ava/node_modules/power-assert/node_modules/empower/lib/decorate.js:42:30)\n    at Function.powerAssert (/Users/zat/.ghq/github.com/sindresorhus/ava/node_modules/power-assert/node_modules/empower/index.js:58:32)\n    at Test.ok (/Users/zat/.ghq/github.com/sindresorhus/ava/lib/test.js:85:15)\n    at Test.fn (/Users/zat/.ghq/github.com/uiureo/ava-power-assert-example/test.js:26:5)\n    at Test. (/Users/zat/.ghq/github.com/sindresorhus/ava/lib/test.js:124:19)\n    at tryCatcher (/Users/zat/.ghq/github.com/sindresorhus/ava/node_modules/bluebird/js/main/util.js:26:23)\n    at Promise._resolveFromResolver (/Users/zat/.ghq/github.com/sindresorhus/ava/node_modules/bluebird/js/main/promise.js:480:31)\n    at new Promise (/Users/zat/.ghq/github.com/sindresorhus/ava/node_modules/bluebird/js/main/promise.js:70:37)\n    at Test.run (/Users/zat/.ghq/github.com/sindresorhus/ava/lib/test.js:110:9)\n    at tryCatcher (/Users/zat/.ghq/github.com/sindresorhus/ava/node_modules/bluebird/js/main/util.js:26:23)\n    at ReductionPromiseArray._promiseFulfilled (/Users/zat/.ghq/github.com/sindresorhus/ava/node_modules/bluebird/js/main/reduce.js:105:38)\n    at ReductionPromiseArray.init [as _init$] (/Users/zat/.ghq/github.com/sindresorhus/ava/node_modules/bluebird/js/main/promise_array.js:92:18)\n    at ReductionPromiseArray.init (/Users/zat/.ghq/github.com/sindresorhus/ava/node_modules/bluebird/js/main/reduce.js:42:10)\n    at Async._drainQueue (/Users/zat/.ghq/github.com/sindresorhus/ava/node_modules/bluebird/js/main/async.js:128:12)\n    at Async._drainQueues (/Users/zat/.ghq/github.com/sindresorhus/ava/node_modules/bluebird/js/main/async.js:133:10)\n    at Async.drainQueues (/Users/zat/.ghq/github.com/sindresorhus/ava/node_modules/bluebird/js/main/async.js:15:14)\n\nnpm ERR! Test failed.  See above for more details.\n```\n. Squashed commits.  I'll write some test.\n. I've tried to add patterns like this:\njs\n    plugins: [\n        createEspowerPlugin(require('babel-core'), {\n            patterns: [\n                't.ok(value, [message])',\n                't.notOk(value, [message])',\n                't.true(value, [message])',\n                't.false(value, [message])',\n                't.is(value, expected, [message])',\n                't.not(value, expected, [message])',\n                't.same(value, expected, [message])',\n                't.notSame(value, expected, [message])',\n                't.regexTest(regex, contents, [message])'\n            ]\n        })\n    ]\njs\ntest((t) => {\n  var a = 'foo'\n  t.is(a, 'bar')\n  t.end()\n})\nBut I got weird output like this. \n\u2716 [anonymous] { powerAssertContext: { value: 'foo', events: [ [Object] ] },\n  source: { content: 't.is(a, \\'bar\\')', filepath: 'test.js', line === 'bar'\nt.ok() works correctly.\n. @twada \nAh, I missed the point. \nIt works right. Thanks for help.\n. @sindresorhus I've done some tweaks. Can you review again?\n. Thanks a lot @sindresorhus @twada. I couldn't do this without your great help :)\n. This might be because babel-plugin-espower does not use a local babel.\nhttps://github.com/sindresorhus/ava/blob/1d5ef4c5f6fcb70f9e90e584dfd08865db1e93b4/lib/babel.js#L12\njs\n    plugins: [\n        createEspowerPlugin(require('babel-core'), {\n            patterns: require('./enhance-assert').PATTERNS\n        })\n    ]\n};\nThis should be a mistake. I'll create a PR if I can reproduce it.\n. Reproduced in npm 2.14.2.\npackage.json\njs\n  \"devDependencies\": {\n    \"ava\": \"^0.3.0\",\n    \"babel\": \"5.8.21\",\n    \"babel-core\": \"5.8.21\"\n  }\ntest.js\n``` js\nimport test from 'ava';\ntest((t) => {\n  t.pass();\n  t.end();\n});\n```\n```\n\u276f node_modules/.bin/ava \n/Users/zat/Code/Sandbox/babel-resolve-test/node_modules/ava/node_modules/babel-core/lib/transformation/plugin.js:121\n      throw new TypeError(messages.get(\"pluginNotFile\", this.key));\n      ^\nTypeError: Plugin \"babel-plugin-espower\" is resolving to a different Babel version than what is performing the transformation.\n    at Plugin.buildPass (/Users/zat/Code/Sandbox/babel-resolve-test/node_modules/ava/node_modules/babel-core/lib/transformation/plugin.js:121:13)\n    at PluginManager.add (/Users/zat/Code/Sandbox/babel-resolve-test/node_modules/babel-core/lib/transformation/file/plugin-manager.js:216:55)\n    at File.buildTransformers (/Users/zat/Code/Sandbox/babel-resolve-test/node_modules/babel-core/lib/transformation/file/index.js:237:21)\n    at new File (/Users/zat/Code/Sandbox/babel-resolve-test/node_modules/babel-core/lib/transformation/file/index.js:139:10)\n    at Pipeline.transform (/Users/zat/Code/Sandbox/babel-resolve-test/node_modules/babel-core/lib/transformation/pipeline.js:164:16)\n    at Object.transformFileSync (/Users/zat/Code/Sandbox/babel-resolve-test/node_modules/babel-core/lib/api/node.js:133:37)\n    at compile (/Users/zat/Code/Sandbox/babel-resolve-test/node_modules/babel-core/lib/api/register/node.js:132:20)\n    at normalLoader (/Users/zat/Code/Sandbox/babel-resolve-test/node_modules/babel-core/lib/api/register/node.js:199:14)\n    at Object.require.extensions.(anonymous function) [as .js] (/Users/zat/Code/Sandbox/babel-resolve-test/node_modules/babel-core/lib/api/register/node.js:216:7)\n    at Module.load (module.js:355:32)\nTypeError: Cannot read property 'stack' of undefined\n    at error (/Users/zat/Code/Sandbox/babel-resolve-test/node_modules/ava/cli.js:50:19)\n    at processImmediate [as _immediateCallback] (timers.js:371:17)\nFrom previous event:\n    at Object. (/Users/zat/Code/Sandbox/babel-resolve-test/node_modules/ava/cli.js:206:34)\n    at Module._compile (module.js:434:26)\n    at Object.Module._extensions..js (module.js:452:10)\n    at Module.load (module.js:355:32)\n    at Function.Module._load (module.js:310:12)\n    at Function.Module.runMain (module.js:475:10)\n    at startup (node.js:117:18)\n    at node.js:951:3\n``\n. @TrySound @forabi The patch has been merged into master. Could you try it?\n. Thebabel-plugin-espowerissue was fixed in #129. Is it different?\nIs there still the problem in master?\n. oops\n. @twada Do you have any idea to avoid this? I don't understand much what they are...\n. Ok. I'll add a comment for now. \n. Thanks, I've updated it.\n. I've updated empower. Thanks.\n. Thanks. xo ignorestest/fixture/*by default and I missed it.\nIs there any way to exclude this file from lint ignores?\n. Without power-assert, the output should befalse == truebecause this assert ist.ok().\nI thought probably it renderst.ok(a === 'bar')` only when power-assert works.\n``` js\ntest(t => {\n    const a = 'foo';\nt.ok(a === 'bar');\nt.end();\n\n});\n```\nShould I test this renders the diagram output somehow?\nt.ok(a === 'bar')\n       |           \n       \"foo\"\n. nice idea.\n. I've added a test that it renders diagram output by regexp.\nLet me know if you want some changes.\n. ",
    "twada": "Hi, I'm creator of power-assert.\nThank you for mentioning me @uiureo.\nI'm very interested in AVA now.\npower-assert output consists of 4 parts, rendered by 4 renderers (file, assertion, diagram, and binary-expression).\nAnd you can omit each part from output through customization API.\nFor example, output below \n# test.js:26\n  t.ok(a.name === 'bar')\n       | |    |         \n       | |    false     \n       | \"foo\"          \n       Object{name:\"foo\"}\n  --- [string] 'bar'\n  +++ [string] a.name\n  @@ -1,3 +1,3 @@\n  -bar\nfile renderer produces,\n# test.js:26\nassertion renderer produces,\nt.ok(a.name === 'bar')\ndiagram renderer produces,\n| |    |         \n       | |    false     \n       | \"foo\"          \n       Object{name:\"foo\"}\nand binary-expression renderer produces\n--- [string] 'bar'\n  +++ [string] a.name\n  @@ -1,3 +1,3 @@\n  -bar\nTo disable verbose graph, remove diagram renderer from output.renderers by using customize method. For example, this configuration\njavascript\nvar assert = require('power-assert').customize({\n    output: {\n        renderers: [\n            './built-in/file',\n            './built-in/assertion',\n            './built-in/binary-expression'\n        ]\n    }\n});\nproduces output as below.\n```\n  # test.js:26\n  t.ok(a.name === 'bar')\n--- [string] 'bar'\n  +++ [string] a.name\n  @@ -1,3 +1,3 @@\n  -bar\n```\nAnd of course, I want to hear your request, and create custom formatter for AVA!\nFYI:\nYou can also create and add custom Renderer Class to configuration options.\n. Hi @Qix- \n\nTo be honest, the binary renderer is the part that seems noisy to me. The diagram is what makes it super useful.\n\nUnderstood. You can disable binary-expression renderer by removing './built-in/binary-expression' from output.renderers\njavascript\nvar assert = require('power-assert').customize({\n    output: {\n        renderers: [\n            './built-in/file',\n            './built-in/assertion',\n            './built-in/diagram'\n        ]\n    }\n});\nYou'll see output like\n# test.js:26\n  t.ok(a.name === 'bar')\n       | |    |         \n       | |    false     \n       | \"foo\"          \n       Object{name:\"foo\"}\n\nVery tempting to switch from should. I'll play with it a bit more.\n\nI'm glad to hear that! :smile: \n. Hi @sindresorhus, thank you for your request.\nI'll create succinct output renderer for AVA!\n. @sindresorhus I've extracted power-assert-renderers module out and now creating less-noisy succinct-diagram renderer for AVA. \nI would like you to see succinct-diagram-test.js.\n(You'll see assert or t.ok calls are still remaining in output, but it's a bit difficult to remove for now)\nIf you like it, I'm going to integrate it into this pull-req and also power-assert master.\n. @uiureo Thanks for responding so quickly, nice to see this being merged!\n. @sindresorhus\n\n@twada Can you review?\n\nSure!\n. @sindresorhus modifyMessageOnRethrow option indicates whether to modify AssertionError message on (re)throwing AssersionError or not.\n\nShould we enable it?\n\nYou are right. Should be true in this case. \n(power-assert overwrites empower's default for ease of use)\n. @uiureo Thank you for reporting.\nIn this case, intrumentation patterns in lib/babel.js should be the same.\nSo please configure lib/babel.js and try again.\njs\nfunction enhanceAssert(assert) {\n    empower(assert,\n        powerAssertFormatter({\n            renderers: [\n                powerAssertRenderers.AssertionRenderer,\n                powerAssertRenderers.SuccinctRenderer\n            ]\n        }),\n        {\n            destructive: true,\n            modifyMessageOnRethrow: true,\n            saveContextOnRethrow: false,\n            patterns: [\n                't.ok(value, [message])',\n                't.notOk(value, [message])',\n                't.true(value, [message])',\n                't.false(value, [message])',\n                't.is(value, expected, [message])',\n                't.not(value, expected, [message])',\n                't.same(value, expected, [message])',\n                't.notSame(value, expected, [message])',\n                't.regexTest(regex, contents, [message])'\n            ]\n        }\n    );\n}\n. @sindresorhus Wow I'm very glad to see this pull-req is getting merged!! :tada: \nThank you so much @uiureo for your great work!\n. First, as an author of power-assert, I'm so glad to see this happen! :smile: \nThank you @sindresorhus !\n\n@twada @uiureo I've documented the enhanced asserts. Would appriciate your review. Happy to change anything. 89219ce?short_path=0730bb7#diff-0730bb7c2e8f9ea2438b52e419dd86c9 :)\n\nI think it's well written.\n. @sindresorhus \nt.ok(foo.indexOf('bar') !== -1)\n         |\n         -1\n\n@twada Any way we could show the value of foo here too? I think that could be useful.\n\nAVA's power-assert output is configured at ava/enhance-assert.js (see renderers section)\nIf you change powerAssertRenderers.SuccinctRenderer to powerAssertRenderers.DiagramRenderer like below,\njs\n    empower(assert,\n        powerAssertFormatter({\n            renderers: [\n                powerAssertRenderers.AssertionRenderer,\n                powerAssertRenderers.DiagramRenderer\n            ]\n        }),\n        {\n            destructive: true,\n            modifyMessageOnRethrow: true,\n            saveContextOnRethrow: false,\n            patterns: module.exports.PATTERNS\n        }\n    );\nYou'll get verbose output like\nt.ok(foo.indexOf('bar') !== -1)\n     |   |              |\n     |   |              |\n     |   -1             false\n     [\"toto\", \"tata\", \"titi\"]\nIt seems nice to have verbosity configuration options in AVA, and change power-assert renderers according to the option value.\n. @sindresorhus \n\nIs there any way to have it output the old simple output when using direct inputs? Meaning no expressions.\n\nCurrently, t.is is configured to be enhanced by power-assert and power-assert cannot toggle output on and off by captured value for now.\nWhat you mean here is that if two arguments are both in form of Identifier or Literal, and its values are simple primitives, AVA's power-assert feature should be turned off automatically. It is not possible now but worth challenging.\n. @sindresorhus \nOh I missed your next question!\n\nBut this should use power-assert since it's an expression and could benefit from the power-assert output.\n\njs\ntest(t => {\n    const b = 'bar';\n    const c = 'baz';\n    const d = 'faz';\n    t.is(b + d, c);\n    t.end();\n});\nThank you for your clear example!\nMy answer is the same as former one --- It's not possible now, but I'll try to make it happen.\n. @sindresorhus Thanks, it's getting a bit easier.\n. @vdemedes Not yet. I'm wrestling with Babel6 now.\nAlmost done but still doesn't work with presets.\n. @SamVerschueren Thank you for reporting!\nCurrently, power-assert only supports ES6/ES2015 expressions. So AwaitExpression is not supported yet and to be supported in next version. BTW, I'll take a look about this behavior.\n. @SamVerschueren No problem!\n. @schnittstabil Thank you for reporting! I'll fix it ASAP.\nAnd please feel free to submit issue to top the level project power-assert when you are unsure where to report.\n. Just released stringifier 1.2.1 to fix this bug.\nstringifier is a dependency of power-assert-formatter, which follows versionings to semver, so this bugfix is already applied to new installation of AVA.\nava@0.3.0\n\u2514\u2500\u2500power-assert-formatter@1.1.0\n   \u2514\u2500\u2500stringifier@1.2.1\n@schnittstabil Would you check the fix?\n. @schnittstabil Unfortunately, it won't match since string does not have any information of its raw literal value such as quotes (single-quoted, double-quoted and template literals). I'm using double quotes as the default stringify strategy that as same as JSON.\n. @jamestalmage Thank you for reporting.\nAnd yes, this is caused by power-assert (Thank you for cross-posting this).\n. For now, please avoid using await expression INSIDE assertions.\nDoesn't work:\njs\ntest('foo', async t => {\n    t.plan(1);\n    t.ok(await foo() === false);\n});\nWould work:\njs\ntest('foo', async t => {\n    t.plan(1);\n    const result = await foo();\n    t.ok(result === false);\n});\nSorry for inconvenience, I'll fix it ASAP.\n. Just released babel-plugin-espower 2.0.0. Now power-assert works with Babel6!\n. @jamestalmage @schnittstabil \nFYI: extracting line number from power-assert would be easy enough.\nIf we change saveContextOnRethrow option to true in lib/enhance-assert.js,\njs\n    empower(assert,\n        powerAssertFormatter({\n            renderers: [\n                powerAssertRenderers.AssertionRenderer,\n                powerAssertRenderers.SuccinctRenderer\n            ]\n        }),\n        {\n            destructive: true,\n            modifyMessageOnRethrow: true,\n            saveContextOnRethrow: true,\n            patterns: module.exports.PATTERNS\n        }\n    );\nthen powerAssertContext property appears in thrown AssertionError.\npowerAssertContext has structure like this\njs\n{\n    source: {\n        content: \"t.is(foo, bar)\",\n        filepath: \"/path/to/some_test.js\",\n        line: 1\n    },\n    args: [\n        {\n            value: \"foo\",\n            events: [\n                {\n                    value: \"foo\",\n                    espath: \"arguments/0\"\n                }\n            ]\n        },\n        {\n            value: \"bar\",\n            events: [\n                {\n                    value: \"bar\",\n                    espath: \"arguments/1\"\n                }\n            ]\n        }\n    ]\n}\nsource.line is the line number of original assertion.\n. @sindresorhus I'm so sorry that I've missed your ping.\n@uiureo \n\nI'll create a PR if I can reproduce it.\n\nThanks a lot!\n. In my opinion, reducing APIs that are easy to make mistakes is the most valuable point of power-assert.\nSo I'm :+1:  on\nt.true(/ok/.test('oh ok'));\n(But for this purpose I must improve output depth of SuccinctRenderer! )\n. I really appreciate @jamestalmage's great contributions.\nFYI: I just released babel-plugin-espower 1.0.1 to fix the little breaking. Thanks!\n. @jamestalmage You mean yield, and async/await expressions? ;)\n. @MoOx Thank you for reporting and reproduction case.\nCurrently, power-assert only supports standard AST Nodes defined in The ESTree Spec.\nJSX nodes are not supported.\nI'll fix this when AVA gets Babel 6 ready.\nI'm so sorry for the inconvenience.\n. @MoOx \n\nSo why the first example is working?\n\nIn the first example, JSX element is on the outside of assertion argument expression (in this case, arguments of t.is) so power-assert ignored it.\nIn the second example, JSX element is on the inside of assertion argument expression, so power-assert tried to infer more information, and something wrong has occurred.\n. Now I'm working with it.\nrefs: [WIP][experimental] Big refactoring: Embed assertion's AST and tokens to make runtime side parser unnecessary\n. @jamestalmage Thank you for clarification.\n\nthis is not a problem with our assertion per se, but that we are masking the true source of the error. I am pretty sure that this is actually related to power-assert.\n\nYou mean AVA is masking the source of error here. I think so too. It converts RangeError to AssertionError, so power-assert (wrapping same) reported it with power-assert context.\nTo avoid it, yes, assertError instanceof assert.AssertionError may help.\npower-assert will rethrow RangeError without any modification.\n\nhow should we handle this? A simple assertError instanceof assert.AssertionError would work for AVA,\n\nYes, with AVA's default, underlying assert module is core-assert so you can trust it.\n\nbut not every assertion library extends assert.AssertionError\n\nIs this topic related to AVA? Or talking in generality?\n. @jamestalmage My pleasure! :smiley: \n. @sindresorhus @jamestalmage self-assigned  ;)\n. @sindresorhus @jamestalmage self-assigned  ;)\n. I agree with @sindresorhus and @shinnn.\nI've developed power-assert to prevent tests from bloating assertion methods. Less is more.\nI know it's a bit opinionated. But AVA is opinionated too ;)\n. For multiline assertions, power-assert normalizes the assertion lines to single line to render diagrams on reporting.\nSo powerAssertContext.source.content is always in a form of single line, no matter how much lines original assertion has.\nFor example, In @jamestalmage's case,  powerAssertContext.source.content is\nt.same(foo, { blah: \\'blah\\', bar: \\'foo\\', zippo: [1, 2, 3] })'\n. @jamestalmage @vdemedes @sindresorhus I just released empower-core 0.2.0.\n. @sindresorhus Looks good to me when bumped to empower-core 0.2.0\n. > How much does this affect our download size? Is worth releasing a 0.8.1?\n\nI am guessing the impact is actually pretty minimal\n\nTrivial under npm@3, still minimal under npm@2.\nSo it's okay to ship this with other PRs.\n. @sindresorhus @jamestalmage Thanks!\n. @sindresorhus @jamestalmage Sorry for my late response. I'm a little busy these days.\n\nWhat do you think about moving this logic into empower-core? Auto populating event.computedMessage or similar?\nMaybe we can offer a standard way to map wrapOnlyPatterns to messages?\n\nSure! Please send a pull-req to empower-core > @jamestalmage \nFor wrapOnlyPatterns, I do not want to change container type from Array to Object.\nSo I prefer\njs\n  wrapOnlyPatterns: [\n    {\n      pattern: 't.throws(fn, [message])',\n      defaultMessage: 'should throw'\n    },\n    {\n      pattern: 't.doesNotThrow(fn, [message]',\n      defaultMessage: 'should not throw'\n    }\n  ]\nto\njs\n  wrapOnlyPatterns: {\n    't.throws(fn, [message])': {\n       defaultMessage: 'should throw'\n    },\n    't.doesNotThrow(fn, [message]': {\n        defaultMessage: 'should not throw'\n    }\n  }\n. That's my TODO: SuccinctRenderer could be better. \u00b7 Issue #3 \u00b7 twada/power-assert-renderers\n. > It doesn't depend on powerAssertContext at all, just assertionErrror.expected and assertionError.actual\nIt makes sense when the assertion has two args (t.is, t.not, t.same, t.notSame).\nWith one arg (t.ok, t.notOk), power-assert should handle string diff and that's what BinaryExpressionRenderer does.\nRendering diffs between expected and actual could be handled by power-assert, and I'm thinking about implementing it though.\n. @sindresorhus Oh thanks\n. @novemberborn I've started a new power-assert-runtime modules in a monorepo style and deprecate power-assert-renderers.\nFirst alpha version is already published and the first beta version will be released soon, maybe this weekend.\n\n@twada are you saying this should be fixed in power-assert-renderers? Is it this issue: twada/power-assert-renderers#1? What can we do to help?\n\nTherefore, this issue would be fixed in new power-assert-runtime repo. I'll migrate power-assert-renderers issues to power-assert-runtime.\nSorry for inconvenience :bow:  Let's go forward together.\n. Thanks :)\n. Hmm it's a hard problem...\nOne thing I can think of is that when AVA catches TypeError and its message matches some pattern _recXX._capt(...) then seek target line and column from original code.\n. @novemberborn Agreed. Showing the entire line would be better.\n. Just released empower-core 0.5.0 :tada: (Thanks @jamestalmage !!)\n. @sindresorhus Understood. I'll fix babel-plugin-espower ASAP.\nThanks @hzoo @gzzhanghao !\n. Just released babel-plugin-espower 2.1.2 to fix the problem. Thank you for reporting and suggestions.\n. @MoOx would you reinstall ava and try again?\n. @Kl0tl Sorry, this is a current limitation of CallExpression matching and intended to propose sensible default variable name for consistency. However it's nice to have some callee pattern matching notation like wildcard. Thanks @sotojuan. \n. > Perhaps a better way would be to revert back to rendering error output in child threads, but lazy-require the formatters. Things would slow down a bit if you have errors in every test file, but should be just as fast if not faster if you have no errors or only have errors in one or two test files.\nRight.\n. @xjamundx Thank you for your great information.\nNow I started wrestling with this issue\n. @josdejong Right, and you can dump everything if you set maxDepth: 0\nHowever when you dump everything, you also find that some objects are really huge (e.g. DOM) and your terminal will be overwhelmed with such dumps. Therefore I decided to set default depth to shallow enough (yes, 1. But sometimes I also feel it's too shallow)\nI've found that formatter is used at ava/api.js, near enough to the options. However, \n\nAdding options is last resort.\n\n@sindresorhus Agreed.\n\nAny way we could handle this case nicely automatically?\n\n@sindresorhus Not yet. But it's worth trying.\n. I'm :+1: on changing the default depth to 3, however would you wait for landing of #703 ?\n. Okay, let's go ahead :wink: \n. @sindresorhus \nBecause assert.AssertionError is defined as read-only here?\nhttps://github.com/sindresorhus/ava/blob/e6b60bf3e9f1014391950af26716d59f9e22c51d/lib/assert.js#L10\n. @sindresorhus \nBecause assert.AssertionError is defined as read-only here?\nhttps://github.com/sindresorhus/ava/blob/e6b60bf3e9f1014391950af26716d59f9e22c51d/lib/assert.js#L10\n. The line (https://github.com/sindresorhus/ava/blob/e6b60bf3e9f1014391950af26716d59f9e22c51d/lib/enhance-assert.js#L38) could be deleted safely. @jamestalmage What do you think?\n. The line (https://github.com/sindresorhus/ava/blob/e6b60bf3e9f1014391950af26716d59f9e22c51d/lib/enhance-assert.js#L38) could be deleted safely. @jamestalmage What do you think?\n. I think, the correct behavior of t.same is the same as assert.deepEqual\n\nOnly enumerable \"own\" properties are considered. The deepEqual() implementation does not test object prototypes, attached symbols, or non-enumerable properties.\n\"Deep\" equality means that the enumerable \"own\" properties of child objects are evaluated also.\n\nAssert Node.js v5.9.1 Manual & Documentation\n. The graph below is a power-assert output.\n\u2716 models \u203a rgb\n  t.same(cc.rgb(), { color: { model: 'rgb', value: [0, 0, 0, 1] } })\n            |\n            Object{color:Object{model:\"rgb\",value:[0,0,0,1]}}\nHowever, message below is not produced by power-assert (but maybe by core-assert) ?\n1. models \u203a rgb\n  AssertionError: {} === { color: { model: 'rgb', value: [ 0, 0, 0, 1 ] } }\n    Test.fn (models.js:6:4)\nTherefore, it's nothing to do with power-assert currently.\nIt's nice to enhance default assertion message of base assertion library by power-assert, but also I think it's a bit overkill.\n. @dcousineau Thank you for reporting. That's a power-assert's problem and I'm trying to solve it. Sorry for inconvenience... I'll integrate it to AVA as soon as possible when the PR lands. So please stay tuned.\n. @jamestalmage Sorry for my late response :bow: \nI've started a new power-assert-runtime modules in a monorepo style.\nFirst alpha version is already published and the first beta version will be released soon, maybe this weekend.\n. @jamestalmage @sindresorhus I'm so sorry that I overlooked this. Please file this issue to power-assert-runtime.\n. @sindresorhus I'm going to ship new power-assert-runtime soon. It'll fix problems like this entirely.\n. @sindresorhus I'm going to ship new power-assert-runtime soon. It'll fix problems like this entirely.\n. Will be fixed when #903 is released.\n. Thanks @nfcampos! This is exactly what I wanted to do today \ud83d\ude03 \nbabel-plugin-espower 2.2.0 and the new power-assert-runtime comes with many architectural improvements with less dependencies. I hope AVA guys will like it.\n. > has the runtime dependency graph been reduced?\nYeah, reducing dependencies is one of the goals of new power-assert-runtime. There's no extra parser anymore. Many ponyfills are consolidated into single core-js.\nI also recommend @nfcampos to update empower-core to 0.6.1 to reduce dependencies much more.\n\nShould we move rendering back into the child processes\n\nYes. Now we can render power-assert output in subprocesses.\n. Thanks @nfcampos, looks good to me! \ud83d\udc4d \n. @nfcampos One thing to notice is that empower module is now a thin wrapper around empower-core for power-assert internal compatibility and there is no reason to use empower in AVA. It's better to keep using empower-core directly.\n. @nfcampos Great! \ud83c\udf89 \n. power-assert is just an assertion message string creator. It does not print messages by itself.\nTherefore, to determine the problem, \n@rickmed would you add failing assertion with simple assertion message that does not trigger power-assert\nt.true(false, 'THIS IS AN ASSERTION MESSAGE');\nand see what happens both on cmd and git bash?\n. @rickmed Thank you for clarifying!\nAnd, hmm... I don't have git bash env so I need some help.\n. Labeled as help wanted since I don't have environment to reproduce this issue.\n. > we should at least find a way to be able to power-assert macros defined in the same file\nI think macros in the same file (assume you mean test file) are already power-asserted if the pattern matches (like internalMacro in the PR body).\n. @jyboudreau @sindresorhus This is expected since AVA uses succinct renderer, not the verbose one used by power-assert by default .\n. @sindresorhus Agreed. Let's getting SuccinctRenderer better.\n@jyboudreau Thank you for reporting!\n. Thank you for reporting \ud83d\ude03\nI've cut a patch release 3.0.1. So would you try it again?. @sindresorhus Agreed. I'm going ahead.\n. @sindresorhus power-assert architecture consists of two parts. Instrumentation (i.e. transformation) side and Runtime side. This code is a runtime side. The diff you mentioned is an instrumentation side. Output rendering belongs to runtime side.\n. These two lines aren't needed.\n. @uiureo Hmm, this workaround is required for now, but looks a bit messy.\nTo clear this up, I have to create a module to enhance prototype, not function nor object.\nWould you wait some more days?\n. @sindresorhus Yes. It would be better to configure them explicitly.\n(Is t.assert public API?)\njs\nfunction enhanceAssert(assert) {\n    empower(assert,\n        powerAssertFormatter({\n            renderers: [\n                powerAssertRenderers.AssertionRenderer,\n                powerAssertRenderers.SuccinctRenderer\n            ]\n        }),\n        {\n            destructive: true,\n            modifyMessageOnRethrow: true,\n            saveContextOnRethrow: false,\n            patterns: [\n                't.ok(value, [message])',\n                't.assert(value, [message])'\n            ]\n        }\n    );\n}\nAnd if you want to enhance output of assertion methods other than t.ok, you can enhance them.\njs\nfunction enhanceAssert(assert) {\n    empower(assert,\n        powerAssertFormatter({\n            renderers: [\n                powerAssertRenderers.AssertionRenderer,\n                powerAssertRenderers.SuccinctRenderer\n            ]\n        }),\n        {\n            destructive: true,\n            modifyMessageOnRethrow: true,\n            saveContextOnRethrow: false,\n            patterns: [\n                't.ok(value, [message])',\n                't.notOk(value, [message])',\n                't.true(value, [message])',\n                't.false(value, [message])',\n                't.is(value, expected, [message])',\n                't.not(value, expected, [message])',\n                't.same(value, expected, [message])',\n                't.notSame(value, expected, [message])',\n                't.regexTest(regex, contents, [message])'\n            ]\n        }\n    );\n}\nIn this case, intrumentation patterns in lib/babel.js should be the same.\njs\nvar options = {\n    only: /(test|test\\-.+|test\\/.+)\\.js$/,\n    blacklist: hasGenerators ? ['regenerator'] : [],\n    optional: hasGenerators ? ['asyncToGenerator'] : [],\n    plugins: [\n        createEspowerPlugin(require('babel-core'), {\n            patterns: [\n                't.ok(value, [message])',\n                't.notOk(value, [message])',\n                't.true(value, [message])',\n                't.false(value, [message])',\n                't.is(value, expected, [message])',\n                't.not(value, expected, [message])',\n                't.same(value, expected, [message])',\n                't.notSame(value, expected, [message])',\n                't.regexTest(regex, contents, [message])'\n            ]\n        })\n    ]\n};\n. Just released power-assert-renderers 0.1.0\n@uiureo Would you update package.json?\n. Thanks! These methods capture each node value in assertion expressions then pass the values to power-assert formatter/renderer.\n. @uiureo \n\nTo clear this up, I have to create a module to enhance prototype, not function nor object.\nWould you wait some more days?\n\nYesterday I tried to add prototype enhancement feature to empower.\nHowever, enhancing prototype does not work well with AVA.\nSo this workaround is needed for a while (Thank you for the appropriate inline comment).\nBy the way, I just released empower 1.0.2.\nIt prevents _capt and _expr from being wrapped in Object.keys(assert).forEach loop.\nA little progress.\nTo recap, the workaround \njs\nTest.prototype._capt = assert._capt\nTest.prototype._expr = assert._expr\nis still needed, even if empower is updated to 1.0.2, but it makes a little progress.\nWould you update package.json again?\n. @uiureo Thank you!\n. Unfortunately, this workaround is necessary for now (_capt and _expr methods are needed at the top level).\nTo make them look less messy, I'll introduce small helper method like\njs\nempower.makeCapturable(Test.prototype);\n(looking for a better name though)\n. babel-preset-stage-2 includes babel-preset-stage-3 so you only need stage-2\n. @jamestalmage Yes. power-assert embeds filepath into transformed file. (Is this an answer for you?)\n. > we don't enhance t.throws with power-assert.\nRight. t.throws is listed as NON_ENHANCED_PATTERNS here. power-assert doesn't touch its arguments.\n. ",
    "joakimbeng": ":+1:\nOn my own laptop I usually want the tests to fail fast, but in an CI environment I think it's good to run all tests and get a report of what didn't work.\n. I think I prefer --bail as it's in line with other test runners.\n. I think I prefer --bail as it's in line with other test runners.\n. > I don't really care what other test runners do\n:+1: Then I agree with --fail-fast being a better name. Or maybe --abort-on-fail but that's too long to be good... \n. > I don't really care what other test runners do\n:+1: Then I agree with --fail-fast being a better name. Or maybe --abort-on-fail but that's too long to be good... \n. Perhaps an ext flag/option like ESLint has would be a good solution? Which defaults to .js.\nSo:\nbash\nava --ext .js --ext .jsx\nWould run both .js and .jsx files... That would require only small changes to the lines mentioned in @novemberborn's comment (I think).\n. Perhaps an ext flag/option like ESLint has would be a good solution? Which defaults to .js.\nSo:\nbash\nava --ext .js --ext .jsx\nWould run both .js and .jsx files... That would require only small changes to the lines mentioned in @novemberborn's comment (I think).\n. ",
    "ahdinosaur": "what about for projects that are using babel to transpile the rest of their app (e.g. by running their code with babel-node), wouldn't this force babel to only transpile the test files?\n. hey. :cat: it's worth mentioning that tape supports nested tests with t.test, e.g.\njs\ntest('a test', function(t) {\n  t.test('a sub-test', function (t) {\n    // do test\n  }) \n})\npersonally, i think this would be awesome for ava, especially if t.test included all the other methods of test, e.g. .before and .after methods:\n``` js\ntest('a test', function(t) {\n  t.test.before(function (t) {\n    // setup\n  })\nt.test.after(function (t) {\n    // teardown\n  })\nt.test('a sub-test', function (t) {\n    // do test\n  }) \n})\n```\n. okay sweet, saw that issue, but don't have any specific requirements not already listed, basically just want to minimally wrap the existing cli and pass in a default configuration. thanks for you help @novemberborn.  :smiley_cat: . ",
    "matthewmueller": "not a big deal either way but files usually correspond 1:1 with what file they're testing. it's often useful when testing say, a user model, to have different bootstrapping functions. \nalternatively, this could also be achieved with something along the lines of:\n``` js\ntest('a model', function (t) {\n  t.before(function() { ... })\n})\ntest('a model', function (t) {\n  t.after(function() { ... })\n})\n```\nwhich would work without nesting.\n. closing since there doesn't seem to be much interest by the authors.\n. ",
    "dbkaplun": "+1, would you accept a PR for this?\n. +1\nI am glad we all agree on the benefit of nested tests. Hope to see this implemented soon!\n. +1\n. ",
    "whitecolor": "The only good case I see use to have nested is to include (using .only) or exclude (.skip) bunch of tests.\n. ",
    "shinnn": "Yeah, ava should exit with non-zero code in this case.\n. Contrary to the PR author's opinion, t.contains won't simplify the test file, I think.\nThere are a lot of JavaScript objects that have indexOf method (array, string, Node's buffer, ...), but this t.contains function doesn't check its argument type. So whenever we use t.contains we should also check the object type.\n. asynchronously\n. FYI try-resolve-from works as you expected. https://github.com/shinnn/try-resolve-from/blob/50cd2cb89934ec0172e43db86fe78263759908ee/index.js\n. OK I'll unpublish try-resolve-from. https://github.com/sindresorhus/resolve-from/commit/4cb3cf0ff07f05b64463bb680f650a77c2da5c80\n. Instead you can use path.delimiter. https://nodejs.org/api/path.html#path_path_delimiter\n. diff\n-    nodePaths = []\n+    nodePaths = [];\n. I should have commented it earlier :)\n. ",
    "Whoaa512": "or nodemon\n{\n  \"scripts\": {\n    \"test\": \"nodemon test/*.js --exec ava\"\n  }\n}\n. Read through #47  and found nyc.\nChanged to that and everything works.\n. I don't think I should have to add require('babel/register') at the top of every one of my source files. Especially if I already have a separate build process in place.\nThe use case here is that I would like to test my source files written in ES2015 simply by requiring them in the test file because unit testing should not require the involvement of my build system.\nPerhaps exposing a flag in the AVA cli to disable of the only option within the call to babel/register.\n. There is no single entry point since I'm individually unit testing each file.\n. @kevva thanks for the example, I think this will suffice\n. @codyhatch, adding require('babel-core/register') at the top of the file will enabled it for further requires but not for the file in which it was required.\n. It would be useful if Github allowed you to pre-populate PRs and issues. I've had isaacs/github/#99 opened for over 2 years now and the best they can do is query string params :unamused:\n. Also something similar to @wenzowski and @paulyoung but with less moving parts:\n``` javascript\nimport difflet from 'difflet'\nconst diff = difflet({ indent: 2 })\nexport default function deepEqual(t, actual, expected) {\n    t.deepEqual(actual, expected, diff.compare(actual, expected))\n}\n```\ndifflet@1.0.1\n. AVA used to implicitly read in your local .babelrc file before 0.10 (See #398) which would allow you to use es7 or any arbitrary babel plugins. \nSee #448 to contribute to the discussion on how customizable babel options should be handled.\n. I haven't tried this, but couldn't you change the search pattern for files to include test/**/*.coffee either in AVA config in package.json or directly in your test command? Assuming you retain the --require coffee-script/register\n. ",
    "asbjornenge": "There is also the watch unix command. \nBut none of these can be compared to ava supporting filewatching internally. I love the fast feedback loop mocha -w gives me, and would :heart: to see ava have this feature.\n. ",
    "tomazzaman": "I too come from Mocha-world and this was by far bigger selling point than speed. I want continuous feedback loop more than speed. \n. Ha! Indeed I like it, but not sure whether my JS skills are there yet. I've written a few watchers for my own projects (so I know a thing or two about it), but nothing as complex as #115. Maybe I'm just needlessly intimidated though :)\n. @novemberborn @sindresorhus I've created a PR #465 , just a basic watcher for now, but works. I'd be more than happy to hear your feedback to learn whether I'm on the right track\n. @MoOx I've had the same issue just now, turned out I was running the file directly with node (as opposed to with ava cli)\nThis works:\nnode ./node_modules/.bin/ava path/to/test.js\nThis doesnt (regardless of the fact it requires ava in the file):\nnode path/to/test.js\nSee the worker, it expects three params.\n. Yep, that's what I'm doing now, here's a working test:\n``` JavaScript\nimport test from 'ava';\nimport React from 'react';\nimport ReactDOMServer from 'react-dom/server';\nimport FieldError from '../../../js/components/form/FieldError.js';\ntest('FieldError shows when the error is in array', assert => {\n  const errors = [{ attribute: 'email', code: '123' }];\nconst field = React.createElement(FieldError, {\n    errors,\n    attribute: 'email',\n    code: '123',\n  }, 'Error message');\nconst result = ReactDOMServer.renderToStaticMarkup(field);\nassert.same(result, 'Error message');\n});\n```\n. Sure, I'd be more than happy, just let me know what to do, I'm quite a noob on contributing on GH :)\n. Will do! Thanks for the step-by-step guide! I'll get on it once kids are in bed :)\n. @vdemedes I have another recipe that doesn't necessarily include AVA but I think it could be very useful for the users so I was wondering whether you'd be interested in publishing it here. I first planned a blog post, but I think it'll get bigger exposure here. It's about integration testing.\n. @vdemedes see this gist. It's not complete, I'd explain what this script does in much more detail and why some decisions were made.\n. Okay, no problem, I do have some other piece of code I'd like to share though; A basic continuous runner that keeps the process open for instant feedback. I can turn this into a recipe as well.\n``` JavaScript\nimport path from 'path';\nimport childProcess from 'child_process';\nimport program from 'commander';\nimport chokidar from 'chokidar';\nimport { isMatch } from 'micromatch';\nconst AVA = require.resolve('ava/cli.js');\nprogram\n  .version('0.0.1')\n  .option('-c, --coverage', 'Include code coverage')\n  .option('-s, --single-run', 'Exit the process after first run')\n  .parse(process.argv);\nlet commandArray = [AVA];\nif (program.coverage) {\n  const NYC = path.resolve('node_modules/nyc/bin/nyc.js');\n  commandArray = [NYC, '--cache', '--reporter=text', '--reporter=html'].concat(commandArray);\n}\nconst sourcesGlob = 'client/js//*.js';\nconst testsGlob = 'client/test//*.spec.js';\nconst watcher = chokidar.watch([sourcesGlob, testsGlob]);\nwatcher.on('add', function add(filepath) {\n  this.sourceFiles = this.sourceFiles || [];\n  this.testFiles = this.testFiles || commandArray;\nif (isMatch(filepath, sourcesGlob)) {\n    this.sourceFiles.push(filepath);\n  } else if (isMatch(filepath, testsGlob)) {\n    this.testFiles.push(filepath);\n  }\n});\nwatcher.on('ready', function ready() {\n  runSuite(this.testFiles);\n});\nwatcher.on('change', function change(filepath) {\n  runSuite(this.testFiles);\n});\nfunction runSuite(testFiles) {\n  console.log('Running tests...');\n  const command = testFiles.concat('--color');\nconst runner = childProcess.execFile(process.execPath, command, (error) => {\n    const exitCode = error ? error.code : 0;\n    if (program.singleRun) process.exit(exitCode);\n  });\nrunner.stdout.pipe(process.stdout);\n  runner.stderr.pipe(process.stderr);\n}\n``\n. @sindresorhus I'll have a go during the weekend\n. Sure, I'll make the necessary fixes. How does this work, do I close the PR, fix the code then resubmit? (Sorry if these questions seems silly, as I said, I'm a noob at contributing in this manner :))\n. Thanks for the guidance!\n. About caching, I've noticed AVA caches _test_ files, but not **source** files, maybe I missed that?\n. I was thinking whether it would be more effective watch the wholecwd` instead of just test files, but wasn't sure what kind of performance impact would that have. \n. ",
    "kasperlewau": "Any ETA on when this might be published?\nCurrently running with $ npm install git+ssh://git@github.com/sindresorhus/ava.git (which isn't that big of a deal), though I did get a wee bit stumped after having referred to the README and then receiving TypeError: _ava2.default.beforeEach is not a function :smile:.\n. > When we're ready to do another release. Probably in a few days.\nCoolio, thanks! \n. Is t.regex still on the table or do we resort to using t.true with an explicit regex.test? I'd love to contribute to the cause, got the necessary changes stashed and ready for tomorrows commute!\nOh and do excuse the potential necro. Just scouring through the help wanted issues.\n. > It is still on the table. Go for it!\nCommuting tomorrow morning is gon' be fun, cheers! : ) \n. I'd be happy to get cracking on this, unless of course someone beats me to it. Got 8 hours of day job to take care of now so there's a wide open window of opportunity to do so! \n. @develar Cheers! I managed to sneak in a couple o' minutes and I'm just about to push some changes and wire up a PR. Thanks. \n\n@sindresorhus - Before I push my feature branch, I'd just like to verify whether the the aim is to support * or not. Using matcher.isMatch and it inherently allows * as far as I can tell. \nI got a wee bit lost reading through the discussion. \n. @sindresorhus Cool! \nI'm guessing we want to support multiple patterns on a package.json#ava level too? If so, I'd suggest we enforce that value to always be an array so as to reduce the amount of type checks we'd need to do in the runner, and also to keep single/multiple matcher(s) the same on a config level. \n```\nyay\n\"ava\": { \n  \"match\": [\"*oo\", \"!foo\"] \n}\nnay\n\"ava\": { \n  \"match\": \"*oo\" \n}\n```\n. Good stuff. I'll get back to this asap. Got some other business I need to take care of for now. \nThe one thing that bothers me just ever so slightly is how to get the test/runner specs working with an array, whereas on the CLI level we're expecting a string value - potentially multiple times. \nThat is to say; \n``` js\n// works\nvar runner = new Runner({\n  match: ['*oo', '!foo']\n});\n// obviously this does not\nvar runner = new Runner({\n  match: '*oo',\n  match: '!foo'\n});\n```\nThe first example is what I would expect to utilise inside lib/runner along with matcher, whereas the second example is what I would expect to be passed through the CLI. \nOr is there some magic on the CLI level, converting multiple occurrences of the same string option to an array? If so, I'm quite confident I'll have a PR posted tonight. \n. @SamVerschueren Awesome! Not such a long ways to go then : ) \n. Alright, on my lunch break. Thinking of the following; \nHow do we enforce the match option in package.json to be an Array? Seeing as how meow will only cast to an array if multiple values are supplied through the CLI, we still need a type check inside lib/runner to function with matcher. \nHow do we differentiate between a passed-in CLI flag and a configured value in package.json? \nAnd how does one test the behaviour of options defined in package.json? I couldn't find a test suite that does that, not yet anyways. \nI'm a little unsure as to what the relationship between the runner and cli looks like, if there even is one. \n. Thank you so much for that @novemberborn! \n\nThis is a bit of a distraction. Just follow the precedent by the require and source options. They're singular CLI flags that can be repeated, and singular package.json options that are arrified in cli.js.\n\nGotcha. I was casting to an array using util.isArray inside lib/runner, but I'll shift that back to cli. \n\nThere's a pkg-conf: cli takes precedence test in test/cli.js. I just noticed that --watch and --source aren't included in that test either though.\n\nI should really get a new set of glasses (5 years since, I do believe), this makes it a lot clearer to me. As it's not part of the subject of the PR/issue I'd prefer not to add --watch and --source expectations. \n. np! :) \n. Been putting in a little bit of work into this, and I just wanted to get some clarification on the intent before finishing up and posting a PR; \njs\ntest(t => {\n  const err = t.throws(function () { throw new Error('foo'); });\n  t.is(err.message, 'foo');\n});\njs\ntest(t => {\n  const err = await t.throws(function () { return Promise.reject(new Error('foo')); });\n  t.is(err.message, 'foo');\n});\nRegardless of what one passes as the second/third argument to t.throws, you always want to get back the (or in the case of string, an) Error with the expected message. Is that a correct assessment or am I (miles) off? \nI must admit, I'm a little confused by your example - are you expecting a property on the given Error object of foo that should equal the expected message? \n\nI haven't taken #468 into consideration just yet - but I'd like to get cracking on that soon-ish as well. Babysteps. \n. Sorry @novemberborn - if your poke was meant for me, I'm afraid you mistyped ever so slightly. Didn't get any notification(s). Regardless, you did make it clearer - tyvm! \nIn my feature branch - t.throws returns whatever was thrown in synchronous assertions, so that's pretty much covered. \nIn the case of Promise.reject I've got something somewhat working. I say somewhat, because it doesn't actually become the rejection reason - the result of Test.prototype.exit is decorated with a message property (the message of the rejection, defaults to '' as per Error). This \"had\" to be done in order to not break the entire promise test suite, what with the expectations on result. \nHaven't posted it for review just yet as I know that isn't what the issue description calls for - but if one were to resolve in exit with the plain rejection of the Promise, we'd lose out on all of the default result properties from my (so far) limited findings.\nSidenote; First attempt(s) at handling this solely in .throws were futile :smile: \n\ntl;dr; I'm handling rejected promises by returning the result of t.throws to Test.prototype.exit, conditionally assigning RejectionError.message to the result of the assertion, if that makes any sense. I'll gladly post what I have so as to give a clearer picture of what's going on. \n\njs\ntest(t => {\n  const expected = 'foo'\n  const actual = t.throws(() => Promise.reject(new Error(expected))\n  t.is(actual.message, expected)\n})\nNot quite there yet I'd say. \n. On that note; If we're fine with dropping the properties received from executing _result() (that would be 'passed', 'result' and 'reason') - I can sort that out by switching some of my code around & removing some assertions out of test/promise. \n. Q: Do you reckon this behaviour should apply to an Observable as well? I made some changes in my branch to return the full Promise rejection which caused a bunch of failures in test/observable. \njs\ntest(t => {\n  const expected = 'foo';\n  const actual = t.throws(() => new Observable(function (o) { o.error(new Error('foo'); }));\n  t.is(actual.message, 'foo');\n});\n. @novemberborn One can never have too many Kaspers! : ) \n\nYour suggestion sounds close to what I've done so far.  \nThis commit handles the wrapping of fn to return the thrown Error. Simple enough, but it could probably be cleaned up ever so slightly. \nThis other commit handles Promise bubbling out of assert.throws to the caller. I've tried to keep changes to .exit to a minimum, but I have yet to figure out how to avoid it entirely. \n\nI can go ahead and post a [WIP] PR, perhaps that is a better way to go about this rather than referring to individual commits living in my fork? \n. > I don't understand why you're changing lib/test. What am I missing?\nThis might be a case of me not writing the tests as one should to get it working as expected. But when going back and reverting the changes made to lib/test makes the following spec break; \n``` js\n// test/promise.js\ntest('throws becomes the promise rejection', function (t) {\n    var rejection;\n    ava(function (a) {\n        a.plan(1);\n    rejection = new Error('foo');\n    var promise = Promise.reject(rejection);\n    return a.throws(promise);\n}).run().then(function (result) {\n    t.is(result.passed, true);\n    t.is(result.result.assertCount, 1);\n    t.is(result.reason, rejection);\n    t.end();\n});\n\n});\n```\nthis line is the real deal-breaker in this specific case. Without it, I don't see how one would return the rejection reason to the callee. \nAgain, it's not that far-fetched that this test does not in fact belong here and I'm just doing it wrong.\n\nOn that note, my pizza just arrived. I'll be back in a bit :pizza: \n. > I don't understand why you're changing lib/test. What am I missing?\nThis might be a case of me not writing the tests as one should to get it working as expected. But when going back and reverting the changes made to lib/test makes the following spec break; \n``` js\n// test/promise.js\ntest('throws becomes the promise rejection', function (t) {\n    var rejection;\n    ava(function (a) {\n        a.plan(1);\n    rejection = new Error('foo');\n    var promise = Promise.reject(rejection);\n    return a.throws(promise);\n}).run().then(function (result) {\n    t.is(result.passed, true);\n    t.is(result.result.assertCount, 1);\n    t.is(result.reason, rejection);\n    t.end();\n});\n\n});\n```\nthis line is the real deal-breaker in this specific case. Without it, I don't see how one would return the rejection reason to the callee. \nAgain, it's not that far-fetched that this test does not in fact belong here and I'm just doing it wrong.\n\nOn that note, my pizza just arrived. I'll be back in a bit :pizza: \n. Sorry, I phased out after having my supper last night (I blame Comedy Central). I got around to cleaning some stuff out, but I'm sort of at a loss on how to handle the test case for a Promise rejection.\nSo far I'm at this point; \n``` js\ntest('.throws() becomes the rejection reason of promise', function (t) {\n    var expected = new Error();\nassert.throws(Promise.reject(expected)).then(function (err) {\n    assert.same(expected, err);\n    t.end();\n});\n\n});\n```\nBut that's not actually testing the return value of throws. Still working on trying to figure that one out - I keep looking at test/promises and the way Promises are handled in there but, I haven't gotten it to work just yet. \n. Alright, I reckon we're good to go there with the latest changes made. \n- lib/test changes are gone. \n- test/[promises|observables] changes are gone. \n- Promise rejection assertion is in place in test/assert\n  - Could potentially be cleaned up ever so slightly? \n- Code has been touched up as per @novemberborn's suggestions.\nLet me know what you think, I've got an itch in my squash finger. \n\nI guess a brief look from @jamestalmage would be warranted as well, given he's the author of #493. \n. @vdemedes Sorry, I should've probably updated the OP. Like I stated before, I wasn't all too comfy with overriding the return value of test.exit to reflect what happened with test.throws. I overcomplicated the matter. \n\nThe way it works now is this; \njs\ntest(t => {\n  const expected = new Error();\n  const actual = t.throws(Promise.reject(expected));\n  t.same(expected, actual); \n});\nWhereas what I was doing in the beginning was something like so;\njs\ntest(t => {\n  const expected = new Error();\n  return t.throws(Promise.reject(expected));\n}).then(result) {\n  // result.reason === expected\n}\nI was trying to cater to the test/promise suite, as opposed to testing throws where it should be tested (in test/assert). That whole snafu is gone now however, after touching up on the parts Mark mentioned. \nI'll gladly edit the OP when I get a on the train. At work currently. \n. > Oh great, so it does return thrown Error from t.throws(), right?\nIt does indeed! :) \n. @novemberborn Resolved, squashed, pushed. Let the CI wait commence. \n. @novemberborn \nRight-o! I'm still fairly new to ava (and tap for that matter). Coming from a chai background so I've hit the t.equal is not a function several times over now :wink: \nOff topic; same is a little bit far fetched when you want to perform a deep comparison. If you think twice about it, it makes all the more sense though. imho. \nI'll get this sorted out!\n. Rebasing and pushing in a jiffy. \n. Glad I could help! : ) \n. I suppose yielding a custom Error to the consumer with a recommendation to reject Promises with actual Errors would be somewhat of a middle ground, if we're in 'back and forth land' that is? \nA missing expectation AssertionError doesn't explicitly highlight that fact (imho), but I didn't take a good look at what the error stack yielded as it stands. A custom Error could just be extraneous information. \n. Thank you(s)! \n. > Don't sync with git pull, but with git pull --rebase.\noff topic\nI'd like to chime in on this statement and say; put it in your global git config. Fire and forget. Be rid of any nonsense merge [remote]master into [local]X commits. \n```\ngit >= 1.7.9\ngit config --global pull.rebase true\ngit < 1.7.9\ngit config --global branch.autosetuprebase always\n```\nI for one can't think of a time and place where a straight up git pull would be preferential. Sorry for the off topic comment!\n. > There's one little thing though, could you please add some minimal docs to the readme along with a few examples?\nAbsolutely. I suppose somewhere in the cli section would make sense? Need to step away from open source for a couple of hours. Sometime tonight! :smile: \n. Hm. Not sure why the build is failing, ran just fine\u2122 on my machine. \nNode v5.7.0\nDarwin v15.3.0\noptions.match will skip tests with non-matching titles\n  not ok TypeError: Cannot read property 'runOnlyExclusive' of undefined\n. Nope. Still no sauce unfortunately. It does point in the direction of #556 , to some extent.\nCould it be that either match or runOnlyExclusive didn't add an entry in runner#chainableMethods? \nStabbing in the dark here : ) \n. this appears to be the failing line. Somehow I'm not passing options to the run method. \nEdit; Yup, a simple snafu by yours truly. Forgot to pass {} to the run method in the failing test. nothing to see here : ) \n. @vdemedes I've added docs for the --match flag (in the cli segment). We could probably shave off a sentence/example or two. \nI'm still on the fence regarding some wording. \n\nmatch titles ending with 'foo' [vs] match titles that end with 'foo'\n\nLet me know what you guys think.\n. @novemberborn Wuh-oh! I'll see what I can do about that. Cheers!\n. @novemberborn \n- We're now passing through the match option from api -> runner.\n- We're guarding against an empty match value. \n- We're guarding against non-test types in the 'apply-match-logic' if block. \nThe one thing I haven't done is setting up an end-to-end test in test/cli to ensure it all works\u2122. Do you have any suggestions/pointers I can take with me on that voyage? :smile: \n. @novemberborn @vdemedes \nI could reverse the behaviour by checking that title is not null, but to me as a user it'd feel weird to see tests without a title running when I've asked for a specific set of titles. \nThe one time I feel a non-titled test should run, would be with either no match pattern supplied, or --match=*. But I don't see why one would ever want to supply such a pattern. \n. Ok, I believe that's the e2e test for --match in the bag. Fingers crossed Travis doesn't kick me in the groin : ) \n. I realised just now I took care of @sindresorhus's comment about repeatabilty - but I did so in the 'wrong' place :smile: \nI'll take care of that on my way to work!\n. > Should this use exclusive rather than skipped? That is, set exclusive for matching tests. \nRight off the bat that sounds better to me @novemberborn. I can't really comment further than that - I'm not familiar enough with the difference(s) between skip and exclusive when put into the context of say --watch or something of the sort. I bet you do though : ) \nWhat I mean to say is that the semantics of match fit inclusion better than exclusion in my mind. Albeit that's a very fine line. \nIf anything it would change the conditional statement to something slightly more pretty.\n. > If I have a .only test and I invoke AVA using a --match, what is the expected behavior? IMO --match should win. \nAgreed. It's seldom that I pass CLI args to my test runner, and when I do pass them I expect them to be king of the hill.\nI'm not so fussed about the easier way (oh gosh, nitpicking...) to achieve the end goal, but more so that the produced code is simpler. If switching to exclusive rather than skip to achieve CLI dominance would generate simpler code in the end, I'm all for it :+1:\n. I've changed some stuff around to add the exclusive flag onto the given test if its title matches --match, leaving the skipped flag for .skip. \nAs expected, non-matching tests no longer add to testCount, nor skipCount. That could open up for some confusion, as it's a fine line and a very personal opinion on whether a match pattern excludes or includes, but perhaps that sort of balancing act is outside the scope of what we're trying to achieve here.\nShould we introduce another counter to keep track of the tests that were not run due to the --match flag? ignoreCount? \nSounds a lot like skipCount to me :unamused:, plus the fact that now we'd have to juggle even more words around; skip, only, match, exclusive, ignore. Gut feeling says that could be simplified, I'm just not quite sure how. \n\nAlso, consider the following: \nava --match=*oo\n``` js\ntest.skip('moo', t => {\n  t.pass();\n});\ntest('foo', t => {\n  t.pass();\n});\n```\nIf --match takes precedence over .only, should it also override .skip? \n. > #471 covers this. Hopefully we can solve it in one fell swoop for all cases.\nGotcha. I mean something like that could become a mess quite quickly. Glad it's on the roadmap :) \n\nI'd argue no. You'd use .skip because a test is broken. You'd use .only or --match to select specific tests.\n\nI suppose you're right about that one. I was halfway through an explanation on why I feel it should be so, but halfway through I realised I ended up at the same conclusion - no. \n\nDoesn't look like you pushed any commits yet though? No worries if you were waiting for feedback on your questions, but figured you might have forgotten ;-)\n\nI'd say that was a 50/50. I was curious as to if it made sense from those point of views, and my net connection died on the train back home as I was about to push. Hadn't gotten around to it yet :smile: \n. Rebased and good to go from my end. Not sure I nailed the exclusive flag, so I would love to get another checkup on this PR. \nJust waiting for those CI's to finish their business. \n. @novemberborn I agree with the changes you proposed to the Readme. Would you like for me to handle those, or defer to #597? \n. > Not sure why Travis failed in Node 5.\nBeats me :confused: Can we trigger a rerun and see what's up? \n. > Just did. Didn't realize I had that power :muscle:\nAnd with that, I will calmly await any further feedback/instructions. Flex them muscles, you re-runner you! \nAside; I was halfway past throwing my MBP right out the window in my previous PR. Not having the power to trigger a re-run had me shouting in agony, being someone who loves to push with -f and pull with --rebase. \n:beer: \n. > Just amend the latest commit without any new changes and force push and you trigger a rerun ;)\nSee I'm quite sure I did just that, but it simply wouldn't rebuild on Travis/AppVeyor, which struck me as odd as I am an avid wielder of the --force. Nevertheless, we be green now :smile: \n. > Please make sure to rebase on master to fix a linting issue and run the latest XO configuration against your PR. Thanks!\nDone!\n. My pleasure! And thanks :) \nSidenote; Didn't get no notification. As it turns out, there actually could be too many Kasper's around @novemberborn @sindresorhus :smiley:!\n\n. Had a read through and I think that looks just fine @novemberborn :+1: \n. :+1:\n. I'm up for getting this sorted out, although I'm not sure how much time I will be able to dedicate to the cause over the coming weeks. So I would leave the issue unassigned for now and/or assign someone else who may feel up for it! \nIf I do get enough time over I'll be sure to push often, and keep this thread going - so as to help out in any way possible :). \nWhy did I not write a test for this? Gah!\n. > because it is a String reason which is promoted on mdn page.\nIt doesn't explicitly promote the usage of a string rejection though, it's just another example - just like the Error rejection. \nSome (old) food for thought; http://www.devthought.com/2011/12/22/a-string-is-not-an-error/\n. > But we'd still need a way to capture any throw.\nHow about adding a 4th 'loose' parameter to t.throws to accept non-errors, as opposed to introducing yet another assertion method? Default would be false.\n``` js\n// loose\nt.throws(Promise.reject('foo'), null, 'foo', true);\nt.throws(() => throw 'foo', null, 'foo', true);\n// strict\nt.throws(Promise.reject(new Error('foo'), Error, 'foo');\nt.throws(() => throw new Error('foo'), Error, 'foo', false);\n```\nToo much? \n\nI don't suppose we could hijack the msg parameter of throws and say; \nif typeof msg === 'boolean' && !!msg - accept non-errors to be thrown / rejected.\n``` js\n// loose\nt.throws(Promise.reject('foo'), null, true);\n// strict\nt.throws(Promise.reject(new Error('foo'), Error, 'foo');\n```\nNo need for a fourth parameter - running assertions on the message property of non-errors doesn't yield anything anyways (afaik).\n\nAll of this would change the semantics of throws for the worse in that it won't return the non-Error (whereas it will return actual Errors, #576 & #651), which could prove confusing. Should throws always return the result regardless of what was thrown/rejected? \n. I'll chime in with my some thoughts on the subject. \nI try to stay away from truthy/falsy as much as possible, as I never thought either were clear enough to be fitting in a unit test. And coming from a Mocha background, I never really got deep into ok/notOk. \n\nI added ok initially since tape had it and it's useful for just checking that a property exists t.ok(obj.prop) or that a function returns something t.ok(fn()).\n\njs\nt.true(obj.hasOwnProperty('foo'));\nt.true('foo' in obj);\nt.true(fn() !== undefined);\nWhat's of importance to me when writing unit tests, is for my peers and coworkers to be able to look at the tests and just get it at first glance, without having to sift through docs. \nA lot of us are working on pet projects outside of our daily jobs, and the amount of API surface that you have to get comfortable with nowadays is quite vast. \nI think the smaller the API surface of ava, the less assertions methods available - the better. That brings us closer to the 'common ground' that is JS which assists in context switching between different projects and stacks, I think. \nKill' em all I say - I don't see myself using either truthy/falsy/ok/notOk to the extent that it would warrant their existence. I quite like @spudly's latest suggestion :smile: \n\n\nTruth be told, I wish a number of our assertion methods were named differently (i.e. It is not possible for you to know what same means without documentation, deepEqual would have been better IMO). Still, I think it is better to keep only one way to do it then make aliases.\nMy problem with same is the ambiguity. It took me a long time to stop thinking it meant \"the same instance\", as in ===.\n\nI'm in full agreement here. I really didn't grok same right off the bat which stumped me a fair bit. I (very briefly) touched on that in one of my previous PR's - it just feels a little far fetched and arguably too open to personal interpretation. Whereas I think deepEqual would be straight to the point.\nNot wanting to introduce aliases, I'd support a s/same/deepEqual change.\n. > I do wonder if deepEqual is clearer because you've heard it before, rather than actually being clearer.\nI suppose there's some merit to the notion that deepEqual is clearer because I've heard of it and used it before. I do think however that deepEqual in no way indicates that we're dealing with the same entity, whereas same for obvious reasons could potentially indicate that.\nThe issue as I see it regarding same is that it doesn't really do a good job indicating that the structure is the same between the two. Nor does it convey that we're dealing with the same instance. It's just not explicit enough for my taste. \nIf you wanted to be really explicit about it and communicate a (to me) crystal clear intent (which I think trumps character count any day of the week when it comes to writing tests), t.sameStructure could be something to look into. \nIt's not something I've ever come across before, and there's probably good reasons for that - but I think it does tick off some boxes: \n- It doesn't involve equal, so there's no risk of having a mix up in regards to the differences between equal and deepEqual. \n  - This point is sort of moot as there's a clear enough distinction between equal and deepEqual. same has no 'partner', which could contribute to the confusion caused.\n- It is explicit in what we're expecting - the same structure of the given entities. \n- It's not implicit like same, so we're not running the risk of someone misinterpreting it as ===. \nI'll end this comment off by saying I'm happy the discussion is alive and kicking, but would it make sense to move it out of the PR? It feels as though we're drifting too far out into off topic land and not discussing the code at hand. Sorry about that.\n. > I do wonder if deepEqual is clearer because you've heard it before, rather than actually being clearer.\nI suppose there's some merit to the notion that deepEqual is clearer because I've heard of it and used it before. I do think however that deepEqual in no way indicates that we're dealing with the same entity, whereas same for obvious reasons could potentially indicate that.\nThe issue as I see it regarding same is that it doesn't really do a good job indicating that the structure is the same between the two. Nor does it convey that we're dealing with the same instance. It's just not explicit enough for my taste. \nIf you wanted to be really explicit about it and communicate a (to me) crystal clear intent (which I think trumps character count any day of the week when it comes to writing tests), t.sameStructure could be something to look into. \nIt's not something I've ever come across before, and there's probably good reasons for that - but I think it does tick off some boxes: \n- It doesn't involve equal, so there's no risk of having a mix up in regards to the differences between equal and deepEqual. \n  - This point is sort of moot as there's a clear enough distinction between equal and deepEqual. same has no 'partner', which could contribute to the confusion caused.\n- It is explicit in what we're expecting - the same structure of the given entities. \n- It's not implicit like same, so we're not running the risk of someone misinterpreting it as ===. \nI'll end this comment off by saying I'm happy the discussion is alive and kicking, but would it make sense to move it out of the PR? It feels as though we're drifting too far out into off topic land and not discussing the code at hand. Sorry about that.\n. Fair enough :)\n. Fair enough :)\n. > Not to start a bikeshedding, but is there any other word we could use that might be even clearer?\nOff of the top of my head, here's a couple of candidates (of varying degrees of suck); \n- t.sameStructure\n- t.distinctEq\n- t.reflect\n  - t.reflection\n  - t.reflectionOf\n- t.mirror\n  - t.mirrorOf\n- t.comparable\n  - t.comparableTo\n- t.represent\n  - t.representation\n  - t.representationOf\n- t.isomorphism\n  - t.isomorphicTo\nThe ones that I do like to some extent would be sameStructure and comparableTo. Possibly distinctEq as well, even though it's somewhat of an oxymoron. Can probably bin the rest swiftly :smile: \nI reckon sameStructure is the winner here. I find it very hard to misinterpret, and it conveys the intent of the assertion very clearly. I don't quite see it trumping deepEqual though. Just gotta keep crossing off those candidates I suppose..\n\n\nHere's an idea: what if t.same becomes t.is? Couldn't be clearer and shorter to type. And t.is gets removed in favor of t.true(a === b).\n\nI agree, in parts. It's ridiculously short to type, and I think opting for t.true(a === b) is a good way to go so there's that going for it. But I don't think t.is adequately conveys the intention of comparing two distinct objects of the same structure.\nI'm sorry to say I don't have any better suggestions, aside from the list of verbose candidates in my first segment - along with deepEqual.\nMy dream scenario would be for the deepEquality assertion to die off entirely and we'd have something like t.true(Object.equals(a,b))\nIn all honesty, I find deepEqual hard to beat - purely based on the fact that it's \"widely adopted\". It's probably something most of us are already familiar with. If not, it's pretty easy to find out with a quick Google search as it's not widely adopted outside of computer programming from what I can find. same, is, and most of my suggestions above - are and could be. \n. > Not to start a bikeshedding, but is there any other word we could use that might be even clearer?\nOff of the top of my head, here's a couple of candidates (of varying degrees of suck); \n- t.sameStructure\n- t.distinctEq\n- t.reflect\n  - t.reflection\n  - t.reflectionOf\n- t.mirror\n  - t.mirrorOf\n- t.comparable\n  - t.comparableTo\n- t.represent\n  - t.representation\n  - t.representationOf\n- t.isomorphism\n  - t.isomorphicTo\nThe ones that I do like to some extent would be sameStructure and comparableTo. Possibly distinctEq as well, even though it's somewhat of an oxymoron. Can probably bin the rest swiftly :smile: \nI reckon sameStructure is the winner here. I find it very hard to misinterpret, and it conveys the intent of the assertion very clearly. I don't quite see it trumping deepEqual though. Just gotta keep crossing off those candidates I suppose..\n\n\nHere's an idea: what if t.same becomes t.is? Couldn't be clearer and shorter to type. And t.is gets removed in favor of t.true(a === b).\n\nI agree, in parts. It's ridiculously short to type, and I think opting for t.true(a === b) is a good way to go so there's that going for it. But I don't think t.is adequately conveys the intention of comparing two distinct objects of the same structure.\nI'm sorry to say I don't have any better suggestions, aside from the list of verbose candidates in my first segment - along with deepEqual.\nMy dream scenario would be for the deepEquality assertion to die off entirely and we'd have something like t.true(Object.equals(a,b))\nIn all honesty, I find deepEqual hard to beat - purely based on the fact that it's \"widely adopted\". It's probably something most of us are already familiar with. If not, it's pretty easy to find out with a quick Google search as it's not widely adopted outside of computer programming from what I can find. same, is, and most of my suggestions above - are and could be. \n. The only time I would see any value in this would be to catch cases where the number of assertions exceed the number of tests (personal preference of mine, max-assert set to 1) - and eslint-plugin-ava takes care of that just fine \ud83d\udc4c \nGut feeling says your linter would do a better job than your reporter at catching shoddy testing techniques. \n. Good call. I didn't consider the implications of calling fn outside of the call to core-assert.\n. Might actually be able to kill this if-block entirely given your suggestion at https://github.com/sindresorhus/ava/pull/576/files#r54448454. I'll give it a whirl! \n. I think if (assertRet.length) would do just fine on it's own, no need for assertRet &&. \n. Much better. I struggled for a good couple of minutes during my commute home on how to word that. ugh. \nThanks!\n. It's gone! \n. > I'd prefer a strict identity check \nI'll apply that to the whole lot of these return tests. \n\nAlso note that you don't need the t.doesNotThrow() wrapper, just run a single t.equal() assertion. No need at all for the next test.\n\nGotcha! I'll kill off some assertions. \n. I just went ahead and killed off the whole bunch of the specs - just the one test now to verify actual === expected. Regardless of secondary arguments passed to throws, it should always be the same. sound good to you @novemberborn? \n. > nope. nope. nope. get a grip. \n. Gosh. I really do like to overcomplicate all the things :disappointed: \n. Gotcha!\n. Would you like for me to change the subject of the Promise rejection while I'm at it? @novemberborn \n\n'.throws() becomes the rejection reason of promise'\n'.throws() returns the rejection reason of promise'\n. Very well! I'll go ahead and amend my commit if you don't mind, and save myself the trouble of having to squash later.\n. Right. I'll get on this once  #576 is merged. \n. Right, because they too are a part of chainableMethods. willfix!\n. Me being the inconsiderate Bash user, I opted for no quotes. I'll single quote 'em all :) \n. :+1: \n. It's not necessary. That's me mixing up two wholly different reasons for the full test suite failing. Removing in a jiffy! \n. Nope, not needed. This would be a case of me not killing my darlings. The amount of times I've been recommended to go and check out lisp is bordering on the ridiculous. \n. Sure thing!\n. :+1: \n. Yup, that's better.\n. : ) \n. Hm. How's about Matched-tests? Too succinct? \n\nThought it'd look better to keep in line with the other 'test scope' modifiers; \nSerial-tests, Only-tests, Skip-tests, Todo-tests\n. ",
    "silvenon": "I tried the provided solution, but it doesn't work for me:\njs\ntest(function * (t) {\n  t.throws(yield Promise.reject(new Error()), Error);\n});\nIt doesn't output the test. Am I doing something wrong?\n. @floatdrop that works, thanks :+1: \n. FWIW, using files as groups works wonderful for me (I didn't know AVA uses file name when reporting errors \ud83c\udf89). I don't think this feature is necessary.\n. I could expand it with nock, I'm just not sure whether cleanAll is needed when testing with AVA. I guess I'm still unclear about process isolation and if it's applicable to this situation.\n. Yeah, I was wondering how to test the API flow with AVA, but that probably isn't related to AVA, though it would be a nice addition to the recipe.\nI've only recently started testing for real, but from what I understand nock is a tool which tests if e.g. a function called the expected endpoint and handled the response well.\n. Wow, that was fast \ud83c\udf89 \n. Wow, that was fast \ud83c\udf89 \n. ",
    "mralexgray": "Here's a another simple example of t.plan()'s epic fail, along with the working snippet from the README...\n``` coffee\ntest 'bar', (t) ->\n    t.plan 2\n    setTimeout ->\n        t.is 'bar', 'bar'\n        t.same ['a', 'b'], ['a', 'b']\n    , 100\ntest 'async', (t) ->\n    t.plan 1\n    setTimeout ->\n        async.start\n            success: ->\n                t.pass()\n        fail: (err) ->\n                console.log \"errrored #{err}\"\n    , 500\n```\n\n\u2716 async Assertion count does not match planned\n\u2714 bar (107ms)\n\nAny chance this will be fixed anytime soon?   Seems like a pretty basic/necessary part of any \"asynchronous\" testing framework, no?\nCan I help?  How?\n. Huh, you're right.\nIn my original code (which I swear I had tried before posting)...\ncoffee\ntest 'async', (t) ->\n  t.plan 1 \n  setTimeout ->\n    async.start\n      success: ->\n        t.pass()\n      fail: (err) ->\n        t.fail()\n  , 500\nMaybe I had mistakenly put t.plan 2 ... or also had a t.end() in one of my callbacks?  Regardless, it's working now!  Sorry for the tongue-in-cheek dramatics.  Was just being cheeky \ud83d\ude07\n. @sindresorhus BTW, in your example of the passing test, are you using 'bluebird' promises?  If so, it may be worth mentioning in the README's \"promises\" section that that is what YOU guys are using, and is an option for test-writers, as well. :-)\n. ",
    "spudly": "\nCurrently we gather tests from all forks before running any of them, and things like .only are supposed to apply across forks.\n\nI don't think you should support this behavior across forks in AVA. I think you're going to have to choose between performance and all the cross-fork magic. Personally, I would choose performance, especially because it takes me 2 mins to run my AVA tests right now. If people really need to run just one test, they can still use --match on the command-line. You could still leave the .only() API as is, but make it apply to a single fork and not the entire test suite.\nAs far as statistics goes, I favor the static analysis idea.\n. What information would the static analysis code need to gather about each test? I'm new to using AST parsers and such but this issue is becoming more and more troublesome (I have hundreds of test files), so I'm really motivated to get this fixed. If I can get a clear idea of what needs to be done, I'll take a stab at it.\n. What problem is solved by adding nested groups? For me, the problem is isolating setup/teardown hooks to specific tests. I don't want to create a separate test file just to have separate lifecycle hooks. For example, I may have one test that stubs one utility function and another tests that stubs something else. These test need different setup/teardown logic, which is where nested groups come in handy to manage it. It's a constant annoyance to me that I have to ensure that my one 'afterEach' hook provides logic that supports the one test that needs it without breaking all the other tests.\nI think there's a better (simpler) way than adding nesting. How about we just register the teardown hook as part of our test? All we would need to do is add a single method to the api: t.after(func).\nConsider the following example:\n``` js\ntest('getCwd() returns the current working directory', t => {\n  sinon.stub(utils, 'doSomething').returns('/fake/path'); // setup\n  t.after(() => utils.doSomething.restore()); // teardown\nt.is(getCwd(), '/fake/path');\n});\n```\nWith more tests, this might be refactored into functions so the setup/teardown logic can be reused:\n``` js\nconst setup = () => sinon.stub(process, 'cwd').returns('/fake/path');\nconst teardown = () => process.cwd.restore());\ntest('getCwd() returns the current working directory', t => {\n  setup();\n  t.after(teardown);\nt.is(getCwd(), '/fake/path');\n});\n// ... lots more tests\n```\nIf you don't like that, perhaps another simple solution is just to return a promise from test(). The promise would resolve whether it failed or not, so the code would look like this:\njs\ntest('getCwd() returns the current working directory', t => {\n  setup();\n  t.is(getCwd(), '/fake/path');\n}).then(teardown);\n. I've never really understood the use case for t.context (or using this inside mocha tests)... what does t.context give me that I can't accomplish with a simple variable?\n. > Another option would be to use .after to register an after handler without having to make test return a promise\nAfter giving this some thought, I think this is the best approach. Perhaps you could also have a .before method as well and provide context to those two callbacks. Here's another contrived example, this time with context.\n``` js\nconst setup = t => {\n  t.context.realCwd = process.cwd;\n  process.cwd = () => '/fake/path';\n};\nconst teardown = t => {\n  process.cwd = t.context.realCwd;\n};\ntest(t => {\n  t.is(getCwd(), '/fake/path');\n}).before(setup).after(teardown);\n```\nThe obvious drawback here is that you have to include .before(setup).after(teardown) on each test instead of for the entire group, but I personally prefer that because it's more explicit.\nBased on the way test is currently implemented, it looks like the implementation would be chainable, so you could probably do something like the following:\njs\ntest\n  .before(setup)\n  .test(t => t.ok(t.context.foo))\n  .after(teardown);\nWe may have to create new functions instead of reusing the existing .before() and .after() chainable methods...\n. If I do that, the teardown function won't get called when the test fails.\nThat would likely cause lots of other tests to fail and make it difficult\nto debug.\nOn Tue, Mar 15, 2016, 9:04 PM Artem Govorov notifications@github.com\nwrote:\n\n@spudly https://github.com/spudly\nWhat problem is solved by adding nested groups?\nFor example this one #222 (comment)\nhttps://github.com/sindresorhus/ava/issues/222#issuecomment-157299689.\nFor me, the problem is isolating setup/teardown hooks to specific tests.\nIf that's the only problem that you'd like to address, then why bothering\nadding any API at all?\nHow what you are suggesting:\nconst setup = t => {\n  ...\n};\nconst teardown = t => {\n  ...\n};\ntest(t => {\n  t.is(getCwd(), '/fake/path');\n})\n.before(setup)\n.after(teardown);\nis better than simply doing this?:\nconst setup = t => {\n  ...\n};\nconst teardown = t => {\n  ...\n};\ntest(t => {\n  setup(t);\n  t.is(getCwd(), '/fake/path');\n  teardown(t);\n});\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/222#issuecomment-197092754\n. Here's my two cents...\n\nIt seems to me that expecting the user to change their .babelrc or package.json/babel configs to modify the behavior of ava is backwards. Behavior specific to ava should be in a .avarc file or in package.json/ava.\nFor example, your .avarc file might contain the following:\njson\n{\n  \"babel\": {\n    \"presets\": [\n      \"es2015\",\n      \"stage-0\",\n      \"react\"\n    ]\n  }\n}\n... and your package.json file would contain this...\njson\n{\n  \"babel\": {\n    \"presets\": [\n      \"es2015\",\n      \"stage-2\",\n      \"react\"\n    ]\n  },\n  \"ava\": {\n    \"babel\": {\n      \"presets\": [\n        \"es2015\",\n        \"stage-0\",\n        \"react\"\n      ]\n    }\n  },\n}\nThis allows you to have separate babel configs for test and source files if you want.\nWe could also have some special keywords for the babel key.\n- default: Use the default config specified by ava (babel stage-2 I believe?). This is the same as omitting the key altogether.\n- inherit: Tests use the same babel config as source files (from .babelrc or package.json/babel). This allows you to share the same config without configuring it twice.\nIn package.json, it might look like this:\njson\n{\n  \"babel\": {\n    \"presets\": [\n      \"es2015\",\n      \"stage-0\",\n      \"react\"\n    ]\n  },\n  \"ava\": {\n    \"babel\": \"inherit\",\n  },\n}\n. Here's my two cents...\nIt seems to me that expecting the user to change their .babelrc or package.json/babel configs to modify the behavior of ava is backwards. Behavior specific to ava should be in a .avarc file or in package.json/ava.\nFor example, your .avarc file might contain the following:\njson\n{\n  \"babel\": {\n    \"presets\": [\n      \"es2015\",\n      \"stage-0\",\n      \"react\"\n    ]\n  }\n}\n... and your package.json file would contain this...\njson\n{\n  \"babel\": {\n    \"presets\": [\n      \"es2015\",\n      \"stage-2\",\n      \"react\"\n    ]\n  },\n  \"ava\": {\n    \"babel\": {\n      \"presets\": [\n        \"es2015\",\n        \"stage-0\",\n        \"react\"\n      ]\n    }\n  },\n}\nThis allows you to have separate babel configs for test and source files if you want.\nWe could also have some special keywords for the babel key.\n- default: Use the default config specified by ava (babel stage-2 I believe?). This is the same as omitting the key altogether.\n- inherit: Tests use the same babel config as source files (from .babelrc or package.json/babel). This allows you to share the same config without configuring it twice.\nIn package.json, it might look like this:\njson\n{\n  \"babel\": {\n    \"presets\": [\n      \"es2015\",\n      \"stage-0\",\n      \"react\"\n    ]\n  },\n  \"ava\": {\n    \"babel\": \"inherit\",\n  },\n}\n. Is this something you guys need help implementing? If so, I might be able to scrape up a fix and pull request.\nAlso, what's the deal with this:\n\nThere are a couple questions that need to be answered in order to proceed. Specifically, can we change which env Babel uses on a per file basis? It might be as easy as setting process.env.BABEL_ENV before each call to babel.transform(..)\n\nI'm guessing the question there is whether we can use one set of babel rules for tests and another set for source files. If that's the question, I believe we've already resolved that. All you have to do is add '--require babel-register' to the command-line. When you do this, source files will use your .babelrc and test files will continue to use the config hardcoded into AVA. Is that right?\n. Added a pull request for all except the \"extends\" key. Let me know what you think.\n. > Would \"babel\": {} work?\nIt'll still use babel, but it will not have any presets, so it won't transform anything.\n. Fixed indentation, added tests, and added docs.\n. @vdemedes\nIt's been rebased.\n. Pushed a new commit & rebased. I believe I've addressed all of the concerns that were discussed, with the exception of the babel: false option, which I believe should be addressed in a separate pull request.\n. Can anyone help explain why these 44 tests are failing?\nhttps://travis-ci.org/sindresorhus/ava/jobs/112878667\nThey're unrelated to my changes - the master branch of AVA fails the same tests. These tests were passing yesterday, but today they fail. Perhaps one of the dependencies had a release that broke with semver?\n. My bad, I must have made a mistake when I checked out master and ran the tests again yesterday. Did it again this morning and the tests are passing. I'll fix this very soon.\n. I'm stumped guys. If I do the following, all tests pass as expected.\nbash\ngit clone https://github.com/sindresorhus/ava.git\ncd ava\ngit reset --hard 58e1c561296a409a9557ceb1c7684685404c4278\nnpm install\nnpm test\nBut if I do this, 44 tests fail:\nbash\ngit clone https://github.com/spudly/ava.git\ncd ava\ngit reset --hard 58e1c561296a409a9557ceb1c7684685404c4278\nnpm install\nnpm test\nBoth repos are being reset to the same initial commit (the one before any of my changes), so there should be absolutely no difference between the two, right?\n. my current theory is that I'm getting weird test results because of nyc cache. testing that theory now...\n. @moOx,\nI didn't know about that flag, but the following just worked for me:\n1. git clone...\n2. cd ava\n3. npm install\n4. [make edits]\n5. npm test\nif before step 3 I ran npm test, the test at step 5 would fail with 44 errors even though the source code was fixed.\nNot sure at this point if it was nyc cache or babel cache but it was definitely caching the source files somewhere and screwing everything up.\n. @novemberborn, I don't have any of those modules installed globally.\nYes, the pull request is ready.\n. Improved unit tests to check that powerAssert and transformRuntime get appended properly.\nReady for evaluation again.\n\nOh, and to confirm, Babel merges plugin definitions when you set babelrc to true?\n\nYes, I believe that is the case.\n. Apparently when ava compiles a file it caches the result (hence the name of the file, cachingPrecompiler.js). This cache is stored in node_modules/.cache/ava.\nI'm pretty sure this is the issue I was having during a couple of days ago. Simply removing ./node_modules/.cache probably would have solved my problem.\n@jamestalmage, is there an easy way to programmatically clear AVA's compile cache (or disable it)? I need to be able to do this so I can isolate the unit tests from each other. The issue is that if I reuse a test fixture (using test/fixtures/es2015.js right now), it will compile with one babel config and cache the result. Then, later, when another test is run with a different babel config, the test fails because it uses a cached version.\n. @kentcdodds, still looking at your issue, but is it possible that your issue is related to the compiler cache as well? I'm thinking maybe you added the react to the list of plugins, ran the compiler (cache files get created here), then removed it and expected it to fail but it worked anyway because it used the cached version the second time. Can you remove node_modules/.cache and try again?\n. yep, fixing the merge conflict...\n. Added babelConfig to the salt and re-enabled caching in unit tests. All working.\nShould be ready to merge now, right? (assuming tests pass in travis and appveyor)\n. only took 22 minutes... is there a reason we're running tests in both appveyor and travis? Seems like one is enough\n. > t.threw??\nThis gives me a better error if the test fails. If the promise in that test is rejected, it will call t.threw(error) and the tests will fail with that error. If I didn't include t.threw to handle the rejection, the reason for tests failure would just be that there was a missing test, which is not enough information for debugging. This should probably be added to a lot of the other tests as well.\n. Will do. I'll make these changes either tonight or Monday morning.\nOn Sat, Mar 5, 2016, 10:25 AM Mark Wubben notifications@github.com wrote:\n\nPlease make sure to rebase on master to fix a linting issue\nhttps://github.com/sindresorhus/ava/pull/601 and run the latest XO\nconfiguration against your PR. Thanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/pull/573#issuecomment-192671281.\n. Is there a reason we have return statements on cli.js:13 and cli.js:96? It's causing lint errors with the latest XO config.\n. Nevermind. running npm install fixed that issue. Still curious though, why are those return statements there?\n. made requested changes and rebased again.\n. @novemberborn,\n\nThat dependency was added by @jamestalmage in the babel-plugin-for-integration-tests branch, which is merged into my code.\n. Nice! Thanks for the help everyone!\n. @novemberborn, no complaints here. Looks good.\n. Here's a start for converting mocha -> ava. Certainly not comprehensive but it works for my current needs. Someone can start with this and build it up.\nAll it does for now is rename it, before, beforeEach, after, and afterEach to the ava equivalents.\nAlso adds the t parameter for the it/test calls.\nhttps://astexplorer.net/#/Uwl4f415pa\n. @vdemedes,\nLike I said, this is just a start. Changing describe blocks can get hairy because in mocha they might have their own set of lifecycle hooks (beforeEach, etc.)\nI probably don't have time to do a full codemod, or else I would have opened a pull request. Just thought it might be useful to people until a full one is implemented.\n. For completeness, here's a list of everything that I think would be needed for a complete mocha codemod:\n- [x] transform it() to test()\n- [x] add t parameter to a ArrowFunctionExpression passed to test()\n- [ ] don't de-sync async functions (the codemod above will mess those up currently)\n- [ ] add t parameter to a FunctionExpression passed to test()\n- [x] transform beforeEach() to test.beforeEach()\n- [x] transform afterEach() to test.afterEach()\n- [x] transform after() to test.after()\n- [x] transform before() to test.before()\n- [ ] remove describe() blocks, prepending their text to each test block's description\n- [ ] intelligently handle nested describe blocks if possible. how do we handle nested describe blocks with their own lifecycle hooks?\n- [ ] if the test body FunctionExpression or ArrowFunctionExpression is async, it should use test.serial instead of test\nAlso, it would be nice to have a separate codemod to transform chai assertions to ava assertions.\n. Even splitting into multiple files will get hairy, because nested lifecycle hooks get merged by mocha. For example,\njs\ndescribe('a', () => {\n  beforeEach(setupA);\n  describe('b', () => {\n    beforeEach(setupB);\n    it('testA', testA);\n    it('testB', testB);\n    it('testC', testC);\n  });\n});\nThis will execute functions in the following order\n- setupA\n- setupB\n- testA\n- setupA\n- setupB\n- testB\n- setupA\n- setupB\n- testC\n. Yes, I suppose you're right. We'd just have to make sure all applicable hooks are included in each sub-file.\n. I think that whether or not throwing a string is allowed should be up to the linter and not the test runner. Given that the language allows you to throw or reject with whatever you want, I would think that t.throws should allow you to do so. I would expect all of the following to be valid:\njs\nt.throws(Promise.reject('foo'), String);\nt.throws(Promise.reject('foo'), 'some string');\nt.throws(Promise.reject('foo'), /somePattern/);\nThat said, I would never use it, because I use a good linter and only throw Errors.\n. I wasn't going to spam everyone with a +1 (no need these days), but since I was specifically invited...\n\nwould be good to get more input from AVA users.\n\nI've always hated ok() and notOk(). I think the intent is not clear in the function name. Sometimes I want to assert that a property exists. Sometimes I want to assert that it is defined. Sometimes I want to assert that it is truthy.\nI'm all for adding t.truthy and t.falsy and deprecating ok() and notOk() but if we do so I think we should also add some other semantically named functions because t.truthy and t.falsy are only two of many use cases.\n. ... also if the recommended is always going to be to lean on powerassert, then I think that t.assert(condition) or t(condition) would be preferrable to t.ok(condition)\n. > I'm all for adding t.truthy and t.falsy and deprecating ok() and notOk() but if we do so I think we should also add some other semantically named functions because t.truthy and t.falsy are only two of many use cases.\nI take this back. I've been thinking a lot about this because I'm refactoring tons of mocha tests to be ava tests and I've decided that I was wrong. I think instead of adding truthy/falsy, we should just replace all of the assertion functions with one single function that fails the test if the provided value is falsy.\nThe following would all be deprecated (recommended replacement on the right):\nt.pass([message])                              // t.assert(true, [message])\nt.fail([message])                              // t.assert(false, [message])\nt.ok(value, [message])                         // t.assert(value, [message])\nt.notOk(value, [message])                      // t.assert(!value, [message])\nt.true(value, [message])                       // t.assert(value === true, [message])\nt.false(value, [message])                      // t.assert(value === false, [message])\nt.is(value, expected, [message])               // t.assert(value === expected, [message])\nt.not(value, expected, [message])              // t.assert(value !== expected, [message])\nt.same(value, expected, [message])             // t.assert(t.deepEqual(value, expected, [message]))\nt.notSame(value, expected, [message])          // t.assert(!t.deepEqual(value, expected, [message]))\nt.throws(function|promise, [error, [message]]) // t.assert(t.throws(function|promise, [error, [message]]))\nt.notThrows(function|promise, [message])       // t.assert(!t.throws(function|promise, [error, [message]]))\nt.regex(contents, regex, [message)             // t.assert(regex.test(contents), [message])\nt.ifError(error, [message])                    // t.assert(!error, [message])\nWe'd still have to have deepEquals or throws functions, but they wouldn't be assertions. They could just return true/false. The single assertion function could be t.assert(), t.ok(), or just t(). Doesn't matter to me.\n. I've been investigating a similar performance issue. Perhaps this is my problem too...\n. I cloned the above repo and hacked away at it for a while. One initial finding is that if I remove the requirement to transpile the react component (by writing it using vanilla js), and remove babel-register from package.json/ava/require, the time goes down from 52.22 seconds to 23.84 seconds (running 100 ava tests).\nThat's a drastic improvement in performance. The takeaway here is that each and every fork has to load it's own copy of babel-register, and since babel is so big, it makes it take over twice as long to run the tests.\n. Nice catch, dcousineau!\nSorry you had to waste so much of your own time fixing a bug I introduced.\n. Just require babel-polyfill also:\njs\n  \"ava\": {\n    \"babel\": \"inherit\",\n    \"require\": [\"babel-register\", \"babel-polyfill\"]\n  },\n. Just require babel-polyfill also:\njs\n  \"ava\": {\n    \"babel\": \"inherit\",\n    \"require\": [\"babel-register\", \"babel-polyfill\"]\n  },\n. @rightaway, I would definitely add it as an explicit dev dependency.\n. Whipped up a quick codemod for this: https://astexplorer.net/#/AbMVMKHZhp\nI'm happy to put this in a PR. Where should I put the files? Perhaps in /codemods?\n. @sindresorhus, @jamestalmage, @vdemedes, @novemberborn\nOpinions about where to put codemods? I was thinking in /codemods but @jfmengels recommended a separate repo. I'll do whatever.\n. @jfmengels,\nLooks good and nice refactor. Guess I forgot about notSame/notDeepEqual because I never use them...\n. Unfortunately, using --require actually slows it down, because it get's required in each fork (not in the main process). In my experience, requiring --babel-register actually makes it twice as slow.\n. I can certainly add a fake plugin, which would increase the expected plugins.length to 3. I can also verify that transformRuntime is added because I can just require() it in to get a reference to it and use === for comparison.\nBut I'm not sure how I can verify that powerAssert was added because it is instantiated inside of the _factory function, so I don't have a reference to it. That's why I only verified that lenth === 2.\nIs there another way I can validate that the powerAssert plugin was added?\n. If we don't include transform-runtime, users will have to import 'babel-polyfill' at the top of each test file if they want to use async functions. This has not changed in the pull request. It was like this before. This is only a documentation change.\n. ",
    "ORESoftware": "Why not transpile tests first, then execute all child processes with plain node? :) maybe you already do that?\nI am willing to bet that this would speed things up tremendously and you would catch transpile errors early too. But you'd have to have some temp directory somewhere which most people think is unsexy.\n. @novemberborn not from what I gather. On this thread the main complaint is that requiring babel is slow. TMK you wouldn't need to require babel if all test files were already transpiled. Am I right? What am I missing?\nforking cps is not slow at all in my experience, it's more or less the standard ~50ms of waiting per fork, it's babel that is really slowing you down.\n. @jamestalmage sorry I don't know what you mean by transpiling sources vs transpiling tests - I frankly have no idea what that means - can you briefly explain? it seems like the best chance of getting towards the speed criterion is not have to require an iota of babelness in your child_processes.\n. ok, by sources you mean the source code that the test file may require/reference? Sounds like it. Yeah, IDK.\n. nested tests are a good thing - if you take what would be nested and put it into separate test files, you will definitely see backwards progress in terms of overall speed. Allowing for nested stuff gives the developer choices - and the ability to put stuff that should be in the same file, into the same file. There are good patterns that you can use with nested describe blocks, that you can't without them. Here is a perfect example: \nhttps://gist.github.com/ORESoftware/e5ed7161d443c69730ae1dd1a9997a6a?ts=4\n``` js\nconst suman = require('suman');\nconst Test = suman.init(module);\nTest.describe('Test inits', {parallel: false}, function (Pool, assert, path) {\nconst data = {\n    size: 5,\n    filePath: path.resolve(__dirname + '/fixtures/sample-file.js')\n};\n\nconst pool = new Pool(data);\nvar size = pool.getCurrentStats().all;\n\nthis.describe('#remove workers', function () {\n\n    this.beforeEach(function () {\n        pool.removeWorker();\n    });\n\n    for (var i = 0; i < 5; i++) {\n        this.it(function () {\n            assert.equal(pool.getCurrentSize().all, --size);\n        });\n    }\n\n});\n\nthis.describe('#add workers', function () {\n\n    this.beforeEach(function () {\n        pool.addWorker();\n    });\n\n    for (var i = 0; i < 5; i++) {\n        this.it(function () {\n            assert.equal(pool.getCurrentSize().all, ++size);\n        });\n    }\n\n});\n\nthis.after(function(){\n    process.nextTick(function(){\n        pool.removeAllListeners();\n    });\n});\n\n});\n```\nas you can see if the beforeEach hooks applied to all test cases in the test file, then we couldn't do something even as simple/intuitive as the above. Pretty lame if you don't have nested blocks, and boring.\n. not sure what the status is on this one, but I am a fan of nested groups, I already added an example on this thread that demonstrates the usefulness of nested groups (it was awhile ago), but here is another example that just came up in my real life, where nested groups feel very natural:\n```js\nconst suman = require('suman');\nconst Test = suman.init(module, {\n    pre: ['create-test-dir']\n});\nTest.create('backpressure', (Queue, Rx, path, fs, before, it, suite, util, userData, after, describe) => {\nconst id = suite.uniqueId;\nconst pre = userData['suman.once.pre.js'];\nconst p = pre['create-test-dir'];\n\nconst q = new Queue({\n    port: 3500,\n    fp: path.resolve(p + '/spaceships' + id + '.txt'),\n});\n\nconst count = 9;  // expected count\n\nbefore('add items to queue', h => {                    // 1st before block\n    return Rx.Observable.range(0, count)\n        .map(function (i) {\n            return q.enq('frog(' + i + ')');\n        })\n        .concatAll();\n});\n\nit('has correct count', t => {             //  1st test case\n\n    return q.getSize()\n        .do(function(data){\n            if(data.count !== count){\n                throw(' => Count is incorrect.');\n            }\n        });\n\n});\n\n\n// here is our nested block/group\n\ndescribe('get count after draining queue', (before, it) => {\n\n    before('drains queue', h => {              //  2nd before block\n\n        return q.drain()\n            .backpressure(function(data,cb){\n                setTimeout(cb,500);\n            });\n    });\n\n    it('has correct count', t => {             // 2nd test case\n\n        return q.getSize()\n            .do(function(data){\n                if(data.count !== 0){\n                    throw(' => Count is incorrect.');\n                }\n            });\n\n    });\n\n});\n\n});\n```\nas you can see there are two test cases and two before blocks. The second before block only runs before the second test, it does not run before the 1st test, and that's the key idea. If there were no nesting, then the second before block would run before both tests, which is not what we want.\nThe above gives the dev way more control and expressivityity.\nThe above is the natural way to create a test, any other organization probably is less natural, and therefore less intuitive. Just my 2 cents.\nNote that this is the same exact need /use case as my prior example, where we only want before blocks to run before select tests cases, not all test cases.. @sindresorhus you are opening the door for every meta tool to now store info in package.json, just think about what that might mean :) It probably won't mean much, except small chance of namespace conflicts, and very large package.json files :)\n. yeah just fyi\nthe same thing will happen if you replace\nsetTimeout/setImmediate/process.nextTick with any other standard async\nfunctions such as\nfs.exists('foo', function(err){\n    throw new Error();\n});\nit's all about throwing an error inside a function called asynchronously.\nthe only real solution that I know of is the domain core module. but the\nproblem is ES6 promises do not support domains because domains are pending\ndeprecation. My strong belief is that without domains you won't be able to\nsolve this problem, so my only recommendation is to try to get Node.js TC\nto backtrack on domains. Bluebird library supports domains, but not ES6\npromises fyi.\nWithout domains you will just have to have an uncaughtException handler but\nyou lose all context of which test failed. I tried the same setup as above\nin Vows by the way and they literally fail out, on the first error, they\ndont seem to even have an uncaughtException error handler.\n@vdemedes https://github.com/vdemedes makes sense.\nPresumably using t.plan() would prevent the test from passing (or indeed\nending).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/567#issuecomment-186233272.\n. yeah just fyi\nthe same thing will happen if you replace\nsetTimeout/setImmediate/process.nextTick with any other standard async\nfunctions such as\nfs.exists('foo', function(err){\n    throw new Error();\n});\nit's all about throwing an error inside a function called asynchronously.\nthe only real solution that I know of is the domain core module. but the\nproblem is ES6 promises do not support domains because domains are pending\ndeprecation. My strong belief is that without domains you won't be able to\nsolve this problem, so my only recommendation is to try to get Node.js TC\nto backtrack on domains. Bluebird library supports domains, but not ES6\npromises fyi.\nWithout domains you will just have to have an uncaughtException handler but\nyou lose all context of which test failed. I tried the same setup as above\nin Vows by the way and they literally fail out, on the first error, they\ndont seem to even have an uncaughtException error handler.\n@vdemedes https://github.com/vdemedes makes sense.\nPresumably using t.plan() would prevent the test from passing (or indeed\nending).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/567#issuecomment-186233272.\n. You can replace what I wrote above with the following code which is even more realistic, and you should see the same problem as above.\n``` js\nconst test = require('ava');\nconst fs = require('fs');\ntest(t => {\n   return new Promise(function(resolve){\n     fs.exists('foo', function(){\n       throw new Error('123');\n       resolve();  // won't get reached, but here for clarity\n     });\n   });\n});\ntest(t => {\n   return new Promise(function(resolve){\n      setTimeout(function(){\n         throw new Error('123');\n         resolve(); // won't be reached, but here for clarity\n     }, 100);\n   });\n});\ntest(t => {\n      return new Promise(function(resolve){\n        setImmediate(function(){\n           throw new Error('456');\n           resolve(); // won't be reached, but here for clarity\n     });\n  });\n});\ntest(t => {\n return new Promise(function(resolve){\n     process.nextTick(function(){\n          throw new Error('789');\n          resolve(); // won't be reached, but here for clarity\n     });\n   });\n});\n```\n. You can replace what I wrote above with the following code which is even more realistic, and you should see the same problem as above.\n``` js\nconst test = require('ava');\nconst fs = require('fs');\ntest(t => {\n   return new Promise(function(resolve){\n     fs.exists('foo', function(){\n       throw new Error('123');\n       resolve();  // won't get reached, but here for clarity\n     });\n   });\n});\ntest(t => {\n   return new Promise(function(resolve){\n      setTimeout(function(){\n         throw new Error('123');\n         resolve(); // won't be reached, but here for clarity\n     }, 100);\n   });\n});\ntest(t => {\n      return new Promise(function(resolve){\n        setImmediate(function(){\n           throw new Error('456');\n           resolve(); // won't be reached, but here for clarity\n     });\n  });\n});\ntest(t => {\n return new Promise(function(resolve){\n     process.nextTick(function(){\n          throw new Error('789');\n          resolve(); // won't be reached, but here for clarity\n     });\n   });\n});\n``\n. also, Mocha and Tape can solve this problem because they run tests serially, which means they can set the \"current test\" or \"current hook\" in the global scope, and then on an uncaughtException they can attach that exception to the current test / hook, but with asynchronous code, you simply do not know what the current test is, at the global scope\n. also, Mocha and Tape can solve this problem because they run tests serially, which means they can set the \"current test\" or \"current hook\" in the global scope, and then on an uncaughtException they can attach that exception to the current test / hook, but with asynchronous code, you simply do not know what the current test is, at the global scope\n. nah, it's going to break down quickly, if you move the asynchronous functions outside the test cases, we now have two functionscheckFileExistence1andcheckFileExistence2` that do work outside the test cases themselves (just like you would if you were unit testing code that was not actually in your test suite).\nlike so\n``` js\nvar test = require('ava');\nvar fs = require('fs');\n///////////////////////////////////////////////////////////\nfunction checkFileExistence1(filename, cb){\n    fs.exists(filename, function(err){\n        throw new Error('123');\n        cb(err);  // won't get reached, but here for clarity\n    });\n}\ntest(t => {\n    return new Promise(function(resolve){\n        checkFileExistence1('foo',function(err){\n            resolve();\n        });\n    });\n});\n///////////////////////////////////////////////////////////////\nfunction checkFileExistence2(filename){\n    return new Promise(function(resolve){\n        fs.exists(filename, function(err){\n            throw new Error('123');\n            resolve();  // won't get reached, but here for clarity\n        });\n    });\n}\ntest(t => {\n    return new Promise(function(resolve){\n        return checkFileExistence2('foo');\n    });\n});\n```\nas you can see, the line numbers are not reported accurately - meaning - all we have is an uncaught exception, and we don't know which test the errors pertain to.\nwhich is why we get this output\n```\n  1. Uncaught Exception\n  Error: 123\n    test-ava.js:14:15\n    FSReqWrap.cb [as oncomplete] (fs.js:212:19)\n\n\nUncaught Exception\n  Error: 123\n    test-ava.js:34:19\n    FSReqWrap.cb [as oncomplete] (fs.js:212:19)\n\n\nTest results were not received from test/test-ava.js\n```\n\n\nI created a gist for this\nhttps://gist.github.com/ORESoftware/218d7aafd3e3049cd234\nI don't think you can solve these kinds of problems easily, so it's probably seriously best to just ignore that this is an issue for now, and perhaps future developments like asyncwrap which supposedly will replace domains will help solve the problem. As long as you have an uncaughtException handler (which this lib does of course), it should be ok (your test suite won't crash), but you won't be able to report the right test as having failed or the line number of the test that failed. Perhaps if more node.js libs get promisified (like fs core) these types of problems will go away, but I am not sure about that.\n. along with the gist above, which shows a bug\nthis also appears to be a bug, possibly related, thanks for listening, don't kill the messenger :)\nhttps://gist.github.com/ORESoftware/b56c9330e48ef225b03c\n. @sindresorhus  I tried to make it abundantly clear above that promisifying things won't solve everything, for example:\n``` js\nconst test = require('ava');\nconst fs = require('fs');\nfunction asyncFn() {\n    return new Promise(function (resolve) {\n        fs.exists('foo', function () {\n            // some moron decides to throw here :)\n            throw new Error('unlikely but very very possible'); \n        });\n    });\n}\ntest(t => {\n    return asyncFn().then(function (res) {\n        t.is(res.bar, 'ok');\n    });\n});\n```\nWe run the above and we get:\n```\n  2 exceptions\n\n\nUncaught Exception\n  Error: unlikely but very very possible\n    FSReqWrap.cb [as oncomplete] (fs.js:212:19)\n\n\nTest results were not received from test\\ava\\ava-ex.js\n```\n\n\n90% of APIs are still callback based. if Pify/Promisifying can solve this, maybe the above example is a good one to use to demonstrate this? On this open issue thread? Nevertheless, if you don't have control of the asyncFn because it's in someone else's library, then it gets harder, but let's pretend you do have 100% control.\n. @loris \nthe reason why your example hangs is most likely because you need to do this instead\n``` js\nconst test = require('ava');\nfunction asyncFn(cb) {\n    process.nextTick(function(){   // force async here\n        cb(null, null);\n    });\n}\ntest.cb(t => {\n    asyncFn((err, res) => {\n        t.is(res.bar, 'ok');\n        t.end();\n    });\n});\n```\nthe above works as expected, however, using the latest version of ava, if I omit the process.nextTick, it does hang, FYI, so you are correct in that observation.\n. @novemberborn  sounds like you'd have to save the run order to disk for every run. this will work for one level of code - good idea - but instead of letting the error go to global scope I would suggest just using the value for self/this since you already have it in hand.\n. is Babel capable of doing this for all types of asynchronous callbacks?\nfor example, Babel might recognize this:\n``` js\ntest('foo', t => {\n   setTimeout(function(){\n}, 100);\n});\n```\nand convert it to\n``` js\ntest('foo', t => {\n   setTimeout(t.__wrapFn(function(){\n}), 100);\n});\n```\nbut what about something like this:\n``` js\ntest('foo', t => {\n   client.get('redis_key', function(err, reply){\n});\n});\n```\nthat would need to become: \n``` js\ntest('foo', t => {\n   client.get('redis_key', t.__wrapFn(function(err, reply){\n}));\n});\n```\nI guess Babel can do that without any problems\n. Ok that's interesting, so yeah maybe on top of the Babel plugin you should make users aware of the potential need to manually call t.__wrap and in that case perhaps alias it with t.wrap = t.__wrap or whatever.\n. domains don't work with ES6 promises yet, which is a big problem, supposedly someone special is going to patch this at some point soon.\n. Well, it seems that AVA could have a central log file that could be written to by any project. Locking would mean that the central file could only be written to by one particular run at a time. Having a \"central\" file, as opposed to a log file in each individual project, would take some burden off the developer because they would not need to have separate terminal windows open for each log file, just the central one.\n. I don't currently have a problem, but considering that it can be resource intensive to run AVA with a high number of child processes, perhaps it would be quite prudent on your part to provide a locking mechanism? It was just an idea :)  If a developer ran AVA on a set of tests that took a long time to complete, it's possible that the dev would forget and then try to run AVA again, which might really overload the environment.\n. I think the most likely use case is this: 10 devs work on their Macs, and they have access to a shared linux box for development/testing. It's quite possible that different devs will logged into that linux box and attempt to run separate AVA runs in tandem which could cause problems. This is more likely than a developer running AVA runs in tandem on a single machine.\n. ah thanks\n. Thanks I was referring to:\nhttps://github.com/avajs/ava/blob/47f08e09558e808ddfc017b7fe10405f1a8372c4/docs/recipes/babelrc.md#transpiling-sources\n\"you will need to tell AVA to load babel-register in every forked process\"\n. RIght, I thought tests were transpiled in the main process, so you're saying that helper files/sources are the primary reason to use require('babel-register') ?\n. why not just transpile the entire \"test\" directory to \"test-target\"? Assuming all the test helpers etc were in the test directory?\n. well I am trying to convince my workplace to avoid mocha but having a hard\ntime finding verified info that shows the benefits even if I can spout them\noff verbally\nOn Oct 28, 2016 1:07 PM, \"Vadim Demedes\" notifications@github.com wrote:\n\nI'm not a fan of these kinds of comparisons. I think AVA users should be\nattracted to it by its features, not by mentioning how bad alternatives\nare. No product is perfect, that is a fact. AVA has its pros & cons as well\nand it's perfectly normal. Every testing tool loses to each other in some\nway.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/avajs/ava/issues/1097#issuecomment-256974932, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AKn56MeqG186a6Gh87TXcqndfr7vdpJnks5q4iu5gaJpZM4Kiz4U\n.\n. https://github.com/TypeStrong/ts-node. Try this:\n\nsudo chown -R $(whoami) $(npm root -g) $(npm root) ~/.npm\nafter that, you probably won't need to use sudo anymore. For example, one solution might be to somehow copy node_modules over from one place to another, from the filesystem, into each container, instead of running npm install.  I am looking for good ways to speed things up.. @sindresorhus thansk for that info - \nin the travis yaml file that you linked to, do you happen to know what this is doing:\n```\ncache:\n  directories:\n    - $HOME/.npm\n    - node_modules  // <<<< ???\n```\nis that caching node_modules directories...everywhere? that doesn't make sense right? What is that line doing? just curious. @sindresorhus thansk for that info - \nin the travis yaml file that you linked to, do you happen to know what this is doing:\n```\ncache:\n  directories:\n    - $HOME/.npm\n    - node_modules  // <<<< ???\n```\nis that caching node_modules directories...everywhere? that doesn't make sense right? What is that line doing? just curious. @novemberborn yes parallel, but not in separate processes, because require('vm') doesn't spawn a separate process (...or does it?). upon --timeout, send a SIGINT, followed by SIGKILL after a grace period...?. ok nevermind, lol. interesting. @lukechilds I am looking at the source for get root module:\nhttps://github.com/lukechilds/get-root-module/blob/master/src/index.js\nnot really sure what it's doing \n```js\nconst findRoot = require('find-root');\ntry {\n    const modulePath = findRoot(process.cwd());\n    module.exports = require(modulePath);\n} catch (err) {\n    module.exports = undefined;\n}\n```\nI looked at the find-root project, and it's just looking for the project root directory, so you are looking to find the \"index\" file of a project?\n. @lukechilds ok bro so that's cool\none alternative, is linking your project to itself with npm link:\nbash\ncd <your-project-root> && npm link\nnpm link your-project  # link your project to itself\nthat way you can use require('your-project') anywhere in your test scripts.\nThis is a safe and effective solution, and I feel, TBH, superior to yours :) Would like to see how other people feel about it, because I honestly haven't heard that many people discuss the pros/cons of linking a project to itself for the purposes of testing, etc.\n. @lukechilds there is actually a small problem with the PR\nin NPM land, requiring a directory will first look at\n{package.json}.main\nif that entry is not there, it will look for index.js\nif that's not there, no module can be returned/required, so you'd have to handle the case where nothing can be loaded. @lukechilds, yep the try/catch will work there, but as far as using npm link:\ngit clone && npm install && npm link && npm link itself && npm test\n:)\nto make it less verbose, I have a test.sh script, something like so:\njs\n// test.sh\nnpm link\nnpm link <your-project>\nava foo bar baz\nand in package.json\njs\nscripts: {\n\"test\": \"test.sh\"\n}\nThe primary advantage with using npm link to do this, is that you can now require('your-project'), from any file in your tests, not just a file that calls  require('ava');\njust my 2 cents\nAlso, I wouldn't name it module because this will unnecessarily shadow the module variable that's injected into a commonjs module (as in module.exports). I would name it \"index\" or something like that.\nThat's 3 cents\n. Seems like \"project\" would be a better name than \"module\". npm install -g npm@4\n. why not default to the file path for the name of the test if no test title is given. npm version 5.x strikes again lol. I think there was talk of passing the test result to every afterEach hook, something like this:\njs\ntest.afterEach(t => {\n   if(t.failed) { /* do your thing here */}\n});\nbut hopefully someone will explain further. @novemberborn aren't those NPM packages only those who have installed AVA as a dependency not a devDependency? Seems like in that case you would get a high number of newb projects in that list.. ",
    "dcousineau": "@jamestalmage ava could 'cheat' and use the --source flags as a hint to pre-compile everything that matches (even if it doesn't catch everything and it compiles more than it has to)\n. ## Additional notes\nSo after I submit this issue I set \"bablerc\": false in the override because I think I'm an idiot and remove rewire from the .babelrc file and the tests work, but I get a weird behavior:\n```\n./node_modules/.bin/ava --verbose\nInitializing CachingPrecompiler with: { babelrc: false,\n  presets: [ 'es2015', 'react', 'stage-0' ],\n  plugins: [ 'rewire' ] }\n_createTransform with: { babelrc: false,\n  presets: [ 'es2015', 'react', 'stage-0' ],\n  plugins: [ 'rewire' ] }\n++ About to actually execute transform...\n:: buildOptions()\nOverriding Babel Config { babelrc: false,\n  presets: [ 'es2015', 'react', 'stage-0' ],\n  plugins: [ 'rewire' ] }\nFinal Options { babelrc: false,\n  presets: [ 'es2015', 'react', 'stage-0' ],\n  plugins:\n   [ 'rewire',\n     { visitor: [Object] },\n     { __esModule: true, default: [Function], definitions: [Object] } ],\n  inputSourceMap: null,\n  filename: '/Users/dcousineau/Projects/grovo/frontend/tests/client/components/primitives-utilities.test.js',\n  sourceMaps: true,\n  ast: false }\nRewire is loaded and working...\n++ Finished transform\n++ About to actually execute transform...\n:: buildOptions()\nOverriding Babel Config { babelrc: false,\n  presets: [ 'es2015', 'react', 'stage-0' ],\n  plugins:\n   [ 'rewire',\n     { visitor: [Object] },\n     { __esModule: true, default: [Function], definitions: [Object] } ] }\nFinal Options { babelrc: false,\n  presets: [ 'es2015', 'react', 'stage-0' ],\n  plugins:\n   [ 'rewire',\n     { visitor: [Object] },\n     { __esModule: true, default: [Function], definitions: [Object] },\n     { visitor: [Object] },\n     { __esModule: true, default: [Function], definitions: [Object] } ],\n  inputSourceMap: null,\n  filename: '/Users/dcousineau/Projects/grovo/frontend/tests/client/components/progress-meter.test.js',\n  sourceMaps: true,\n  ast: false }\n++ Finished transform\n++ About to actually execute transform...\n:: buildOptions()\nOverriding Babel Config { babelrc: false,\n  presets: [ 'es2015', 'react', 'stage-0' ],\n  plugins:\n   [ 'rewire',\n     { visitor: [Object] },\n     { __esModule: true, default: [Function], definitions: [Object] },\n     { visitor: [Object] },\n     { __esModule: true, default: [Function], definitions: [Object] } ] }\nFinal Options { babelrc: false,\n  presets: [ 'es2015', 'react', 'stage-0' ],\n  plugins:\n   [ 'rewire',\n     { visitor: [Object] },\n     { __esModule: true, default: [Function], definitions: [Object] },\n     { visitor: [Object] },\n     { __esModule: true, default: [Function], definitions: [Object] },\n     { visitor: [Object] },\n     { __esModule: true, default: [Function], definitions: [Object] } ],\n  inputSourceMap: null,\n  filename: '/Users/dcousineau/Projects/grovo/frontend/tests/client/config/config.test.js',\n  sourceMaps: true,\n  ast: false }\n++ Finished transform\n++ About to actually execute transform...\n:: buildOptions()\nOverriding Babel Config { babelrc: false,\n  presets: [ 'es2015', 'react', 'stage-0' ],\n  plugins:\n   [ 'rewire',\n     { visitor: [Object] },\n     { __esModule: true, default: [Function], definitions: [Object] },\n     { visitor: [Object] },\n     { __esModule: true, default: [Function], definitions: [Object] },\n     { visitor: [Object] },\n     { __esModule: true, default: [Function], definitions: [Object] } ] }\nFinal Options { babelrc: false,\n  presets: [ 'es2015', 'react', 'stage-0' ],\n  plugins:\n   [ 'rewire',\n     { visitor: [Object] },\n     { __esModule: true, default: [Function], definitions: [Object] },\n     { visitor: [Object] },\n     { __esModule: true, default: [Function], definitions: [Object] },\n     { visitor: [Object] },\n     { __esModule: true, default: [Function], definitions: [Object] },\n     { visitor: [Object] },\n     { __esModule: true, default: [Function], definitions: [Object] } ],\n  inputSourceMap: null,\n  filename: '/Users/dcousineau/Projects/grovo/frontend/tests/client/reducers/ground-control.test.js',\n  sourceMaps: true,\n  ast: false }\n++ Finished transform\n```\nAnd it repeats slowly accumulating for each file AVA loads\nObviously the console logs are from some that I dropped trying to figure out how AVA is working internally, a copy of my file can be found in this gist\n. I think the/a bug is here in the options.plugins.push. Possibly a .concat wouldn't leak between executions?\n. That helps a lot actually, PR incoming\n. I'll dig in further thanks!\n. @jamestalmage aha I get it now. I'm curious, do you think there'd be a way to provide configuration for the babel-register for the sources? \nIn the mean time I'm making my webpack config force its own babel config and will use babelrc to include the rewire plugin, in general I only want rewire plugin operating in a testing context but not when building the client.\n. @jamestalmage if and only if I can specify the babel config for the source files as well (not just the test files) which I think is my real ask. (but an extends would be great as well)\nShould I close this issue or rename it as a feature request? After doing some other things and sleeping on it I realize this is not a bug given what you've told me about the differences in transpiling test files and transpiling source files.\n. @novemberborn Yeah, I was going to copy the last test in the caching-precompiler test file but am hitting the end of my day. I'll circle back in a little bit and do so.\nAlso IRT the build failures: it looks like npm install failed on travis?\n. @novemberborn \n. @novemberborn FYI my timing was impeccable: \n\nNo compatible version found: left-pad@0.0.3\n\nhttps://medium.com/@azerbike/i-ve-just-liberated-my-modules-9045c06be67c#.bmwp4a3le\nhttps://twitter.com/seldo/status/712414400808755200\n. @sindresorhus what a time to be alive~\n. @spudly to be fair my issue wasn't a bug it was a misunderstanding on my part, I just happened to find this digging in and trying to understand AVA's behavior :)\n. @novemberborn thanks! I've been so busy I haven't been able to write tests :(, and thats a much simpler test than I would have done...\n. We're chasing down huge memory usage in our project breaking CircleCI. With a pool we can certainly help clamp down on runaway peak usage (I'm still going to chase down other ideas).\nI do notice there is The Blocker\u2122: Would a default behavior of unlimited sized pool maintaining current behavior and adding a flag for limiting the pool with the caveat that test.only won't work be acceptable as an interim (beta) behavior?\nIf so would extra hands (read: my time) integrating @jamestalmage's async-task-pool be useful or has enough work been started that my time would be wasted effort?\n. @jamestalmage I have a bit of time and have been in worse codebases ;) Will find you on the gitter chat at some point, I should have a minimal POC and thoughts to run by today.\n. @jamestalmage So I pulled this minimal proof of concept out of my morning just to verify the approach.\nI think I should be able to pull out the final returned promise into two separate logic paths and use the new \"pooled\" version when a pool size is requested, leaving default behavior in.\nThe test.only problem will continue to rear its ugly head for those that use it (we don't, or at least not very often) but being able to control runaway process and memory usage on limited CI environments is worth the tradeoff for losing .only in that context for us.\n(I haven't pushed any of this to my fork yet because I wanted to get opinions before I moved forward. Currently I'm symlinking the latest master into the node_modules folder of our project and running our test suite as the \"real world\" example \ud83d\ude2c)\n. @jamestalmage certainly, I'll flesh it out some more. I'll indicate its also a POC\n. I suggest using the ?w=1 trick to view the diff for the test file changes. \nA link for the lazy: https://github.com/sindresorhus/ava/pull/791/files?w=1\n. @jamestalmage ^\n. @novemberborn I \"forked\" the code because of the logic that initializes each test looking for a .only being incompatible with the structure of the pooled code. But I can see what happens with unlimited concurrency and see if the new code passes the 3 exclusive tests and report back.\n. @novemberborn also concurrency of 1 makes more sense than me whining that serialMap doesn't allow concurrency :| I was so deep in the weeds I was like \"I can't guarantee proper execution order\" not realizing who cares as long as they ran one at a time\n. > Yea that makes sense. I'm proposing adding a logic branch for when concurrency is restricted, which I realize may be worse than duplicating code.\nIf we either de-scope exclusive tests/.only to only apply to the tests in the current singular file, or we eliminate the feature entirely, we can drop the old logic and stick with the concurrency pool logic.\nThe more I think about previous suggestions of static analysis for .only support the more I get heartburn but once thats implemented we can drop old behavior as well.\n. @sindresorhus I'm more familiar with multi-process in Python and a quick check shows that Celery likes to default to the number of CPUs.\n. @sindresorhus I'm cool with that. As a part of my most recent update Api.prototype._runLimitedPool now uses the concurrency pool size via a function argument so I can support serial execution. I haven't bolted it in yet but the code is ready for it.\nI noticed the conflicts and will have a resolution pushed soon\n. @sindresorhus I'm cool with that. As a part of my most recent update Api.prototype._runLimitedPool now uses the concurrency pool size via a function argument so I can support serial execution. I haven't bolted it in yet but the code is ready for it.\nI noticed the conflicts and will have a resolution pushed soon\n. @sindresorhus tests are currently failing on the 3 .only test cases (since the entire API test case is run against the concurrency limiting code). There are a few options:\n1. For now we do not run the 3 test cases against the concurrency limit code while we decide what to do with this feature going forward.\n2. We wait until we've decided to either deprecate .only or reduce the scope of .only (only to current file) or we make another decision.\nI can easily implement suggestion 1. \nMy long term suggestion is either deprecating or reducing the scope of .only.\nI'll wait for your positive confirmation on how you'd prefer to proceed. \n. @novemberborn Sounds good. I'll swing through tonight and get everything ready to go\n. @sindresorhus @novemberborn Sorry about the delay, made the updates.\n. I found an issue with the using --match ... tests where the test/fixture/match-no-match.js was actually failing trying to import ava instead of ../../. I repaired the test and added a positive test case and an additional fixture.\nThe updated and new tests can be found here since the diff tab isn't showing them: https://github.com/dcousineau/ava/blob/process-pool/test/api.js#L920-L967\n. All tests pass for the first time.\n\n. @novemberborn looking over the older code I see how self.emit('ready') is emitted only once which I missed.\nI can move it above the Promise.map easy, and the tests pass. I'll wait for a final word.\n. @jamestalmage Dropping the concurrency pool to 1 and re-running the tests ensured they still pass. I'm checking for matches after all test files have run so it shouldn't be an issue but I can circle back and come up with a few more tests\n. @sotojuan Ha! Next time grab me and say hi! (I'm at all of BoroJS meetups obviously :P). Sidebar: have you considered reaching out for EmpireJS OSS Night?\n. When running this fix on our test suite, nyc drops reported coverage from 35.95% (on v0.14.0) to 1.82% (running HEAD from master).\nNot sure if you were aware of this but you may want to delay publishing to NPM.\n. Makes sense. I'm not using npm link anywhere becuase I keep forgetting its a thing :P\n. Certainly. I was going off fabfile.org's as it was closest to my experiences. Ill change in the next push\n. I'll tinker and see what I can do\n. Everywhere that returns this object structure (but with real data) forms it themselves. On error we need to return an empty object so the runStatus.processResults(results); doesn't choke on an undefined so ideally no consumer gets the blank results structure. I only pulled it into its own variable because there are 3 failure cases that need to send it up stream. \nWriting this I realized I can simplify down to 2 and will be pushing a change to simplify as well as changing it to a getter Just In Case\u2122\n. I renamed it cb but the the cb will always be the resolve from the promise above on line 268. \n_generateRunner is creating the callback for the test.on('stats', ...) event and this generated callback is what should eventually resolve the outer promise that represents the process in the concurrency pool. \nA good chunk of this is still a pull over from the old runner since I don't feel confident enough on the self._runFile logic and the fork objects to refactor so I just mimicked old behavior.\n. Yeah I wasn't sure what it was for so I played it safe and kept it in the same area. Anything I didn't understand from the legacy code I left in the same logical place as much as possible.\n. Normally I would agree but with the current behavior, no matter what if the --concurrency flag is used the code behind _runLimitedPool is used. I vote we keep this as-is until we decide to drop _runNoPool entirely so that we can deterministically say if you use the concurrency flag you will use the new process pool code.\nIf enough people object to my opinion, however, I will change it.\n. I'm similarly worried about people using the --concurrency flag and seeing different behaviors based on their filename glob patterns. It's much simpler to convey in documentation \"If the concurrency flag/option is present, automatic .only behavior is not supported\" than a list of rules.\nAdditionally, a user manually running ava with a deliberate subset of tests is likely only doing so to run a few specific tests because they internally do not use .only (like my team) which actually cycles back up to my argument of nixing .only entirely since with the --watch flag and good file name globbing, there is no real need for .only.\n. ",
    "LinusU": "You are actually already using this, which funnily enough breaks the tests that I'm trying to write for loud-rejection :laughing: \nAva is built with meow which includes loud-rejection.\nMan was it annoying to track down as well,\n``` javascript\nconst org = process.on;\nprocess.on = function () {\n    console.trace(arguments);\n    org.apply(this, arguments);\n}\nconst fn = require('./');\nconst test = require('ava');\nfn();\nfunction fire() {\n    return new Promise(function (resolve, reject) {\n        setImmediate(function () {\n            const org = console.error\n            console.error = function () {\n                console.error = org\n                console.trace(arguments) // <------ Finally manage to find it with this\n            }\n            reject(new Error('unicorn'));\n        });\n    });\n}\n``\n. Another fun fact, callingconsole.trace` recursively will run out of memory before hitting max recursion. Found this out the hard way...\n``` text\n<--- Last few GCs --->\n5200 ms: Mark-sweep 1391.4 (1505.2) -> 1172.0 (1220.2) MB, 35.6 / 0 ms [allocation failure] [GC in old space requested].\n5218 ms: Mark-sweep 1172.0 (1220.2) -> 1172.0 (1212.2) MB, 18.1 / 0 ms [allocation failure] [GC in old space requested].\n5238 ms: Mark-sweep 1172.0 (1212.2) -> 1171.7 (1211.2) MB, 19.8 / 0 ms [last resort gc].\n5252 ms: Mark-sweep 1171.7 (1211.2) -> 1171.7 (1211.2) MB, 13.7 / 0 ms [last resort gc].\n\n<--- JS stacktrace --->\n==== JS stack trace =========================================\nSecurity context: 0x3ec84df37399 \n    1: replace(aka replace) [native string.js:~146] [pc=0x230e9b6af485] (this=0x23a9be604101 ,N=0x3ec84dfb84b1 ,O=0x3b956bc4e539 )\n    2: 0x53c78d8eaf9 > [/Users/linus/coding/loud-rejection/node_modules/ava/node_modules/babel-core/node_modules/core-js/modules/$.fix-re-wks.js:~15] [pc=0x230e9ae9...\nFATAL ERROR: CALL_AND_RETRY_LAST Allocation failed - process out of memory\n[1]    6026 abort      node_modules/.bin/ava\n``\n. Hmm, for some reason using master stopped myprocess.on('exit', ...)handler from ever firing... Any idea why?\n. Also, I can't getrejectionHandledto fire :( tearing my hair out...\n. I think I got everything working, except for therejectionHandled`... Let's continue the discussion in that repo\n. @bencooling Wont ava still transpile using babel though?\nAlso, Node.js 6 will be released on the 26th and then there won't be a need to run node with flags anymore...\n. I'm quite sure that all code (except node_modules) is run thru babel\n. I stand corrected, sorry about that\n. ",
    "jamiebuilds": "https://github.com/phated/async-done\nThis is the library used to resolve several different async types in gulp 4.0 (streams, promises, observables). You might want to make use of it.\ncc @blesh\n. It's the same issue number. https://phabricator.babeljs.io/T2954 which was closed as a duplicate of https://phabricator.babeljs.io/T6644\n. Ava not having describe() blocks or any kind of nesting is actually a big reason I like it.\n\nFrom this twitter thread: https://twitter.com/jamiebuilds/status/954906997169664000\nBefore:\n```js\ndescribe('foo', () => {\n  let foo;\n  beforeEach(() => {\n    foo = ...\n  });\nit('should do something', () => {\n    expect(foo.thingImTesting(bar)).to.be.true;\n  });\ndescribe('with bar', () => {\n    let bar;\n    beforeEach(() => {\n      bar = ...\n    });\nit('should do something', () => {\n  expect(foo.isThisABar(bar)).to.be.true;\n});\n\n});\n});\ndescribe('baz', () => {\n  let baz;\n  beforeEach(() => {\n    baz = ...\n  });\nit('should do something', () => {\n    expect(baz.hasBeenSetup()).to.be.true;\n  });\n});\n```\nAfter:\n```js\nconst test = require('ava');\nfunction setupFoo(opts) {...}\nfunction setupBar(opts) {...}\nfunction setupBaz(opts) {...}\ntest('something that needs foo', t => {\n  let foo = setupFoo({...});\n  t.is(foo.thingImTesting(), false);\n});\ntest('something that needs foo and bar', t => {\n  let foo = setupFoo({...});\n  let bar = setupFoo({...});\n  t.is(foo.isThisABar(bar), true);\n});\ntest('something that needs bar', t => {\n  let baz = setupBaz({...});\n  t.is(baz.hasBeenSetup(), true);\n});\n```\n\nThe setupX methods are written in a way that it can be configured in tests to provide options to the thing you are constructing. In the TDD/BDD way, you'd have to do more refactoring of unrelated tests when adding a new one that wants to be setup differently\nThe test methods are written so that the explicitly define all of their dependencies, so you aren't setting up extra things just because you'll need them in other tests\nThe tests can be very easily copied and pasted to create more cases and placed where-ever makes sense in the file without having to copy other bits of code around\nStatic analysis tools like linters and type checkers can follow the second way much easier because you don't have all these floating values which aren't clearly defined or are temporarily undefined. Ex: A type checker doesn't know what beforeEach means to the variable in tests\nThis is a bit more minor, but writing tests with a full description in the name (rather than spread out across blocks which might be hundreds of lines apart) is much easier to follow. I hate scrolling up and down just to understand whats going on with my test\n\n. @vitalets that doesn't actually work with arrow functions, but this isn't really about syntax. I don't care about the length of code, I care about the functionality and reusability.\nAs an exercise, look at both examples I gave above and walk through the steps it would take to add a test that needs foo, bar, and baz in each of their test suites.\nHere's it without the BDD-style:\njs\ntest('foo, bar, and baz', t => {\n  let foo = setupFoo();\n  let bar = setupBar();\n  let baz = setupBaz();\n  t.is(foo.withBarAndBaz(bar, baz), true);\n});\nBut then BDD-style requires either duplication or refactoring, the easiest way will be to shove everything into a global beforeEach and now you're running all sorts of code which isn't needed in the test.. >  I would have thought the runtime plugin would have picked this up and provided it via some helper function.\nBabel can't resolve instance methods on references to polyfills.\nThink about it this way, how can a compiler know what keys is in this case? Until it knows that it can't possibly know what includes is referring to.\njs\nkeys.includes()\nThere is the potential in this particular case to do something about it because we can infer that the result of Object.keys is an array. However, if you had a case like the following we couldn't possibly get it working.\njs\nfunction includesName(keys) {\n  return keys.includes('name');\n}\nThis inability to do things consistently is why Babel will never auto-polyfill like this.\n. >  I would have thought the runtime plugin would have picked this up and provided it via some helper function.\nBabel can't resolve instance methods on references to polyfills.\nThink about it this way, how can a compiler know what keys is in this case? Until it knows that it can't possibly know what includes is referring to.\njs\nkeys.includes()\nThere is the potential in this particular case to do something about it because we can infer that the result of Object.keys is an array. However, if you had a case like the following we couldn't possibly get it working.\njs\nfunction includesName(keys) {\n  return keys.includes('name');\n}\nThis inability to do things consistently is why Babel will never auto-polyfill like this.\n. If you're going to include the code to polyfill it, why not just polyfill it?\nAlso, I don't think we'd ever be interested in going down that route in core, you're welcome to try your hand at in in your own plugin.\n. If you're going to include the code to polyfill it, why not just polyfill it?\nAlso, I don't think we'd ever be interested in going down that route in core, you're welcome to try your hand at in in your own plugin.\n. Babel can actually do that through babel-plugin-transform-runtime\nhttps://github.com/babel/babel/blob/master/packages/babel-plugin-transform-runtime/src/definitions.js#L25\nThese are aliases to core-js functions:\njs\nArray.includes;\njs\nimport _Array$includes from \"babel-runtime/core-js/array/includes\";\n_Array$includes;\n. @sindresorhus https://github.com/kangax/compat-table is a bad test suite for ES6 features, it's very shallow. https://github.com/tc39/test262 is better\n. > Someone needs to do a pull request to babel-preset-es2015 and auto-detect generator support and conditionally include regenerator (preferred solution) or add an option for toggling regenerator.\nThe Babel preset would not accept a pull request for that, just because the environment that is running Babel supports generators does not mean the targeted environment will.\nIf you want to create a preset babel-preset-node-feature-detection that tests for support for everything before running it, that would likely be the best solution. However, it should warn heavily that you should not distribute the generated code with it (we don't want npm filled with node version specific modules).\n. https://phabricator.babeljs.io/T2756\n. I apologize I did not respond the first time this ended up in my inbox.\nI'm not sure what the limitations of TypeScript were that required writing a generation script. Hopefully the same problems do not exist.\nAs for Flow reading the TS definition files @sindresorhus is right, it would actually be much easier to generate TS definitions from Flow definitions as many of the semantics are intentionally more strict.\nI'll look at the AVA API and see if I can start it off for you\n. Okay so I was planning on just playing around with it but you nerd sniped me and I ended up doing the entire type definition. \nI was not prepared for how big the generated TypeScript .d.ts file is. I spent a long time trying to deduce what the API limitations were from that API. But then I started exploring the actual implementation in AVA and that was much easier to understand.\nOne of the things that I thought was unusual with the TypeScript definition you generate is that it reduces the API methods down to act like they can't chain forever despite option-chain supporting the ability to chain infinitely. I ignored that piece because it seemed unimportant and that it would only serve to greatly complicate the type definition, not to mention now it's matching what happens at the runtime. \nIt also adds support for .always since I noticed that was missing from the TypeScript definition.\nAnyways here is the current state of the definition, I'm happy to say it's only like 200 lines compared to the 1,200 line TypeScript definition: https://gist.github.com/thejameskyle/6a0f4d2ef41ed3a7554ce1fa208ae291 \nI'm sure it has a handful of issues, I haven't really done that much testing of it, I've just been keeping a lot of it in my head.\n. youkeepusingthatword.jpg\nThis isn't a matter of strictness, you're effectively enforcing a lint rule with the TypeScript definition which is not the purpose of a type system and just making it impossible to update the definition.\nAlso test.serial.serial is not an invalid combination. option-chain is designed so that the latter properties in the chain take precedence. All you are doing is setting the same option twice.\n. Also, people keep taking this as a comparison between Flow and TypeScript. I wasn't bragging about Flow, I was bragging about my own mad skillz trying to figure out that crazy type definition.\n. > is that the right title?\nlol idk\n\nTS has a private modifier which actually does nothing like prefixing variables with underscores.\n\nCould you give me an example of this? \n. Maybe this isn't the best place to discuss this particular issue.\nOne of the big differences between Flow and TypeScript is that in Flow if you strip the types you have plain JavaScript code and it behaves exactly as expected, we don't add any runtime features, and we don't modify any behavior whatsoever. We're actually unable to do that with our current setup. On the other hand TypeScript adds additional features to JavaScript that go beyond adding static typing, things like namespaces, enums, etc. which is fine until these things are added to JavaScript (enums is currently being proposed with different semantics than the TypeScript version).\nI'm not sure why TypeScript has added private fields to JavaScript while also not making the runtime enforce them at all, it seems like a weird straddling of both design decisions and could cause problems if private class members are added to JavaScript. But the risk there is hard to judge.\nHopefully that answers your question.\n. Sorry, I shouldn't have let this go off topic. But if @SamVerschueren @RyanCavanaugh and @DanielRosenwasser want to talk elsewhere I'm happy too. I don't think my words are being understood as intended.\n. Just to emphasize, if you move this to a lint rule where you can express yourself more dynamically you will keep your type definitions small and much more maintainable.\n. @novemberborn I'm having trouble imagining how the logic for that would work in option-chain that wouldn't make the library a bit ridiculous. \nSince you define chainable methods like this:\njs\nconst chainableMethods = {\n    foo: {foo: true},\n    notFoo: {foo: false},\n    bar: {bar: true},\n    both: {foo: true, bar: true}\n}\nI think it's totally reasonable to have a scenario like:\n``` js\nconst fn = optionChain({\n    broadOption: { foo: true, bar: true, baz: true },\n    refinement: { bar: false }\n}, ...);\nfn.broadOption.refinement(...);\n```\nAnything else you'd be looking up which options are set by which method chains and eliminate them based on that which could just be a really confusing result.\nThe API that exists today makes a ton of sense. I wouldn't change it because you want to enforce a particular code style that people are going to naturally follow regardless. It places a maintenance burden on the type definitions which just makes them impossible to contribute to.\n. Yeah it does allow that, but you're suggesting a change that impacts more than just that scenario which probably never happens in the real world anyways\u2013\u00a0and even if it did happen it would work fine.\n. I think I'll respond to all of that by saying please don't add complexity for complexity's sake when there is no noticeable positive impact to anyone.\nEnforcing this means:\nCons:\n- Writing generation scripts for types for both Flow and TypeScript\n  - Making contribution harder\n    - Making it far more likely to fall out of date\n- Writing an option to the option-chain library that adds a bunch of complexity\n  - You'll have to deal with conflict resolution\n  - And you still haven't dealt with broad => specific option groups, which is an api nightmare\nPros:\n- People won't be able to do the thing they are already not doing and doesn't cause any issues either way\n. Btw the reason for this file's location and naming is so that Flow doesn't require any configuration in order to use the type definitions for AVA. npm install ava + require('ava') and you're done.\n. @jeffmo @marudor @ryyppy could one of you review this?\n. Thanks for pointing that out @ryyppy, this actually uncovers a really interesting problem with intersection types that needs to be fixed.\nFor now I've worked around the issue with a bit more repetitive of a type definition. Once we fix the issue I can revert the previous change.\n. Yeah, this is good for now. But should update with no repetition once https://github.com/facebook/flow/issues/2358 is fixed.\n. Would it be weird to use A for actual and E for expected?\njs\n    Array [\n      Object {\n        children: Array [\n          Object {\n  A         type: \"text\",\n  A         value: \"[hello](world)\",\n  E         children: Array [\n  E           Object {\n  E             type: \"text\",\n  E             value: \"hello\",\n  E           },\n  E         ],\n  E         type: \"link\",\n  E         url: \"world\",\n          },\n        ],\n        type: \"italic\",\n      },\nProbably looks a lot better with highlighting.. js\n    Array [\n      Object {\n        children: Array [\n          Object {\n  \ud83d\ude15         type: \"text\",\n  \ud83d\ude15         value: \"[hello](world)\",\n  \ud83d\udc81         children: Array [\n  \ud83d\udc81           Object {\n  \ud83d\udc81             type: \"text\",\n  \ud83d\udc81             value: \"hello\",\n  \ud83d\udc81           },\n  \ud83d\udc81         ],\n  \ud83d\udc81         type: \"link\",\n  \ud83d\udc81         url: \"world\",\n          },\n        ],\n        type: \"italic\",\n      },. Maybe all the documentation could be pulled into the docs directory in separate files and published to a very simple markdown-to-html website using GitHub Pages (with Jekyll) or Netlify (with anything).\nNetlify is cool cause it will automatically run on PRs and give you a unique URL to preview the website on every change\n. You can make it so that you manually select which version of the site to deploy with Netlify.\n\nDeploy with Netlify on every master build\nTurn off auto-publishing\nWhen releasing, publish the latest deployed version. Docusaurus is also really nice for allowing docs to be translated and versioned. Maybe worth the complexity. I'd actually like to see a t.warn() along with a whole warning system. It'd be nice to see them collected in the final output:\n\n```js\n$ ava\n\u2714 foo\n    \u26a0\ufe0e blah() is deprecated, use blerg()\n  \u2714 bar\n  \u2714 baz\n    \u26a0\ufe0e blah() is deprecated, use blerg()\n3 passed\n2 warnings\n```. Cherry picked changes from #1455 into #1713 on latest master. > The snapshot name extraction problem requires a change to the .snap format. We could either add a reverse lookup or store the test titles directly, though that makes parsing more difficult.\nIt would be really useful to have the name back, otherwise snapshots can very easily get mixed up.\n\ndo you see this as an entirely separate process or as a complement to an AVA run? In which case we could surface the failing snapshots, you could render in the browser, then rerun a specific test and let AVA update the snapshot.\n\nI want it to be able to run in two modes:\n\nTake a diff (in like a branch or something) and display all the changed snapshots\nAs Ava is running, display snapshot mismatches\nlive updating as tests re-run\nwith buttons next to the visual diffs to update them\n\nAlthough if I had the \"complete\" event in #1769, I suppose it could be implemented as:\n```js\ntype Data = {\n  ...,\n  failedSnapshots: Array<{\n    name: string,\n    expected: T\n    actual: U\n  }>\n};\nava.on('complete', (data: Data) => {\n  browserWs.send(JSON.stringify(data.failedSnapshots));\n});\n```\nMaybe I could even do it as individual test results come in with a separate event.. I was thinking it would do this:\njs\nt.snapshot(visual(<Comp/>));\n// effectively:\nt.snapshot({\n  isVisualSnapshot: true,\n  value: \"<raw><html><string/>...\"\n});\nI'm okay making it just a plain string though. I could either use it in the main process or as another process that Ava starts up. I figure it's a little easier to build in the main process though.\nI don't think it needs much of a programmatic API, maybe that can be expanded upon in the future.\nIt might be interesting to expose some way to start or stop a test run or control what tests are being run based on some other logic. You could build an entire UI around Ava. Modified from: https://github.com/avajs/ava/issues/1768#issuecomment-381214630\n```ts\ntype Data = {\n  ...,\n  snapshots: Array<{\n    name: string,\n    expected: any,\n    actual: any,\n    passed: boolean\n  }>\n};\nava.on('complete', (data: Data) => {\n  browserWs.send(JSON.stringify(data.failedSnapshots));\n});\n```\nThis is basically all I care about. As long as I can get a list of the snapshots that failed in a way that I could later tell Ava to go update them, that would be enough for me. Could I subscribe to specifically snapshots?. > It feels weird having to add/change APIs because of Flow/TS limitations...\nIt's definitely possible to type flow correctly here. This sounds like a separate API decision to me. Awesome, thanks. I'll look into doing that today. Actually, the one big problem with the hash approach is that it creates surprising groupings of tests.\nIt seems reasonable to expect tests in the same sub-directory to be mostly in one job instead of distributed across all of them.\nWhat if we sorted the files ourselves instead? It'll be faster than creating hashes, especially since we can expect them to already be in a decently sorted starting state.\n. Updated. > Even with sorting there is no guarantee of sensible grouping.\nYeah, it might not be perfect, but we do this with Jest today and it is nicer than a completely random grouping.\nWe can even improve upon this and find a reliable way to keep folders together even when it distributes them unevenly.\n\n\nI had a look at hughsk/path-sort which is the dependancy added in this Pr.\nThere is a pull request there that has been open for a year without any response so I dunno if the package is maintained at all.\n\nI'd be happy to use lexical sort and not use a dependency at all.\n\nI used it because otherwise it completely changes the order that Ava runs tests. Right now the way it discovers files recursively is already giving us files ordered by their directories.\nI'm happy to fork path-sort and maintain it. I'll reach out to the original author about it\n. Alternatively, Ava might want to implement a randomized order feature like in RSpec. \nEdit: Existing issue: #595. Array<string>#sort() does produce a different order, nothing terribly dramatic though. In a single folder, files and sub-folders are mixed together:\n\nfile-dir-file-dir Array<string>#sort() \nfile-file-dir-dir pathSort()\n. If you do end up using #sort(), I would make sure it is case-insensitive:\n\njs\nfiles.sort((a, b) => {\n  return a.toUpperCase().localeCompare(b.toUpperCase());\n});. Thank you so much for getting this in \ud83d\ude04 \n. I've published a library that implements (most of) this API here: https://github.com/jamiebuilds/ninos. > How do you see cleanup working? In my experience the spy / stub setup is impacted by test concurrency, and therefore so is cleanup. Could you elaborate?\nMy library doesn't handle this well enough, but because t.spy() is only available within test() and friends, you can track the spies created as part of an individual test even while others are running.\nThis would also allow you to detect a few things:\n\nTests trying to spy on the same exact object at the same time\nTests calling spies/stubs that were created by other tests\nTests calling stubs after the test finished.\n\nI think we'd have to play around a bit with what warnings/errors are most useful and when, but we can do a lot more by hooking into internals.\nPeople will still likely have some amount of trouble with singletons, but I'm not sure that's ever going to be completely solved in JavaScript unless tests were completely isolated.\n. > Would we be able to return better error messages without adding assertions? Note that we're looking at adding t.assert() and removing power-assert from others (#1808). Does this become a question of improving output for t.assert() for recognized spies / stubs?\nI have to think about that more. I wasn't aware you were thinking about removing power assert from others. Maybe you could replace it with something lighter weight in other scenarios?\nEither way I think the experience is still pretty good without it being specialized (based on the errors I was getting with Ni\u00f1os).\n. All it would take to make this work as a separate library is to have a t extension point that can be aware of the current test context. You can actually get close by using t.context\n```js\nfunction ninos(test) {\n  test.beforeEach(t => {\n    t.context.spies = [];\n    t.context.spy = () => {\n      // ...\n      t.context.spies.push({ object, method, original });\n    };\n  });\ntest.afterEach(t => {\n    t.context.spies.forEach(({ object, method, original }) => {\n      object[method] = original;\n    });\n  });\n}\n```\n```js\nconst test = require('ava');\nrequire('ninos')(test);\ntest('spy', t => {\n  t.context.spy(object, 'method'); // reset automatically\n});\n```\nThis can be improved a lot with some sort of ExecutionContext.extend() method. However, I don't see a way that you can make it work with Flow/TypeScript types.. Separately, I would argue that this sort of thing should be built into the test framework itself for a nicer out of the box experience.\nI think that AVA should definitely have a ExecutionContext.extend() method, but I would reserve that for things like framework specific assertion methods. Installing a separate ava-react library makes sense, but stubbing/spying seems like it should be built-in.\nI'm going to propose an API for extending the test execution context which can handle test concurrency. But I suggest we continue exploring adding stubs/spies to AVA.\n. > We have plenty other challenges to tackle.\nIs the reason just that you're too busy? I would be happy to implement this. > Could you elaborate on cleanup and how it relates to concurrent tests?\nI hadn't fully considered the concurrency problem because building this in requires you to override globals which could be used by any concurrent test.\nI'm not sure it's even possible to (fully) detect what timer was created by what test. The closest thing I've got is looking at the stack trace, and that has all sorts of problems.\nThe other option here is always mocking timers. Which would be good for performance, but (based on Jest's early state) really bad for developer experience. Maybe this is the sort of thing opted into on a file basis:\n```js\nconst test = require('ava');\nconst c = test.clock(); // must be called immediately?\ntest('timers', async t => {\n  let called = 0;\nsetTimeout(() => called++, 1000);\n  await c.time(1000); // move timers ahead in milliseconds\n  t.is(called, 1);\n});\n```\n. > That is to say, perhaps we can create new test interfaces that have specific behavior, and that can decorate the execution context. They could force synchronous execution, etc.\nHmm... I would almost prefer a way of saying that \"this test needs to be run serially\" as part of creating the clock. Otherwise you'd almost certainly end up with multiple test functions.\njs\nconst test = require('ava');\nconst testWithClock = require('../../test-helpers/ava-with-clock');\nvs\njs\ntest('timers', async t => {\n  let c = await t.clock();\n  // Error: Cannot call t.clock() in tests that run concurrently (use `test.serial`)\n});\n\nAnother idea: If we could hook into the test scheduler, you could make t.clock() return a promise that resolves once other concurrent tests are complete and make sure that other tests aren't running at the same time.\n```js\nconst test = require('ava');\ntest('timers', async t => {\n  let c = await t.clock();\n  let called = 0;\nsetTimeout(() => called++, 1000);\n  await c.time(1000); // move timers ahead in milliseconds\n  t.is(called, 1);\n});\n```\nSince some test files can end up being hundreds of tests (where concurrency has a huge impact), it would be good if you didn't need to make every test in the file run serially. . I think those are the primary reasons for it.. That's a different type of mocking than being discussed here. This was interesting to explore, but I believe it should fix a lot of the issues people have been having with promises with AVA+Flow. js\nthrows(() => any): Error\nthrows(() => PromiseLike<any>): Promise<Error>\nNo, this won't work as you expect it to. any is the parent type of all types so it won't match.\nImagine a pattern match like this:\njs\nif (val is anything) { // always `true`\n  // do this\n} else if (val is specific thing) {\n  // do that\n}\nIt's always going to match the first case, so the second case doesn't really matter. But you also can't just flip them because the types don't have the same sort of control flow.\nIn order to say:\n\nIf I get a promise, return a promise\nIf I get an observable, return an observable\nIf I get anything else, return void\n\nYou need to be able to distinguish between promises, observable, and \"anything else\". Otherwise, if it overlaps, you'll get an intersection of the types you are returning (You'll \"return\" promise & void)\nThis is what it means to have \"discriminant unions\":\njs\ntype UnionT =\n  | { type: 'A' }\n  | { type: 'B' }\n  | { type: 'C' }\nWith a type like that, just by asserting type we know exactly which one of the three options we've got.\njs\ntype UnionT =\n  | { prop: boolean }\n  | { prop: any }\n  | {}\nBut if we checked that prop is a boolean here, it still matches all three options.\nSo short answer:\njs\nthrows(() => void): Error\nthrows(() => Promise): Promise<Error>\nthrows(() => Observable): Observable<Error>\nwould work (void, promise, observable are all distinguishable)\njs\nthrows(() => any): Error\nthrows(() => Promise): Promise<Error>\nthrows(() => Observable): Observable<Error>\nwould not (any can not be distinguished from promise or observable)\n. > Presumably callbacks are executed in order, serially, waiting for any returned promises to settle? Errors wouldn't stop the next callback from executing?\nSo I have two opposing thoughts:\n\nI think it would be good to mimic the semantics of before/after(Each) as much as possible.\nI think teardown code shouldn't care about being run in a set order and should be able to run concurrently.\n\n\nif you ignore those, subsequent test failures may be cryptic.\n\nI don't think you have to ignore them, I think it would be possible to report an error in teardown separately from the test itself passing. The test itself could also be failing and you'd still want the teardown code to run, which could then also fail (likely for the same reason as the test failure), and you'd want both errors to be reported.\nThat being said I can see arguments for both choices. . > I propose we make them execute in order and serially\nOkay, that sounds good \ud83d\udc4d \n\njs\ntest('test', t => {\n  t.teardown(t => {\n    // Which value is `t`?\n  })\n})\n\nIt seems a bit unnecessary to provide t.teardown's callback with t. Any scope where you can call t.teardown you should already have access to t.\nI also want this to work without any weirdness:\njs\nt.teardown(db.close);\n// Does db.close receive an unexpected `t` argument? What if that changed its behavior\n\nHowever if the teardown can perform assertions on the test, then I don't think we should report teardown failures separately.\n\nThat's fair \ud83d\udc4d . I had started on an implementation somewhere but if you're wanting to grab it feel free. - <testsuite> - I would just use file paths\n- <testsuite time=\"0.05\"> - Time to run a test suite (file)\n- <testcase time=\"0.01\"> - Time to run a test case (test())\n- <failure> Source location - /path/to/test.js:40 (t.assertion())\nOverview here. @novemberborn Do you want to start a new TAP/JUnit-like spec along with people at CircleCI? I can also get people from Bitbucket Pipelines involved. Okay so I thought the idea was so good I kinda just went ahead and started it: https://github.com/jamiebuilds/zap\nI'm happy to keep working on it, add others to it, reach out to other organizations, we can change it however people want, or even abandon it and join in on what others want to do.. If AVA could output ZAP then you could easily write a tool to convert it into JUnit XML or TAP. But you couldn't do it the other way around (at least very well).\nEvery CI/CD service I've spoken to about this has been frustrated with the existing formats today. \nIf you want to contribute something better to the open source community I would suggest heading in a direction that will push the community forwards. Anyone could add JUnit support to AVA, I was already willing to do it, but it's going to take a CI service adopting ZAP (or some other new/better format) in order to push it across the industry.\nAzure DevOps is in a very crowded ecosystem of very well established CI tools, it's going to have to do a lot of work to differentiate itself. ZAP will let you do something that no other CI service can do right now: Real time aggregate well-formatted results of dev tools.. Well I think you already have your answer from this issue then\n[1] https://github.com/avajs/ava/issues/1878#issuecomment-407671294. I mean console.log, but I'm assuming AVA already has pipes of all the user's stdout/stderr and can detect if they wrote anything. . There's really no good way of caching based on plugins, your solution is full of holes. The current recommendation is for users to wipe out their cache when they change their config.\n. I need to be able to configure the comparator, but I guess it could do that too.. ",
    "benlesh": "I don't think that async-done library supports RxJS 5 observables yet. The better path is to look for Symbol.observable on any incoming object, then call it and call subscribe on whatever object you get back. This will make it work with Kefir, Most and Bacon now as well I think.\n. > @blesh Can you open an issue/PR on async-done?\nSorry, this got lost in my GitHub inbox... Does this still need to happen? I'm not sure i have time to do the PR anytime soon, unfortunately.\n. Okay, I think I'm caught up with what the desire is here.\nObservables can \"end\" in one of three ways: \nThey terminate because: \n1. the producer errors.\n2. the producer completes successfully.\n3. the consumer unsubscribed.\nYou only really need to care about 1 and 2, because 3 would be caused by the test itself. To handle 1 and 2 for zen-observable, RxJS 5+, Kefir, Most and Bacon, you'd just need:\n``` js\nlet obj = getSomethingThatMightBeObservable();\nif (obj[Symbol.observable]) {\n  objSymbol.observable.subscribe({\n    complete() { / done successfully / },\n    error(err) { reportError(err); / done because error / }\n  });\n}\n```\nBut the problem there is while you've determined that it's \"done\", you haven't analyzed the output.\nIf the idea is to analyze the output, the best short-term solution is to subscribe to RxJS 5 or zen-observable Observables with forEach, which returns a promise:\njs\nreturn myObservable.forEach(x => doSomeAssertion(x === 'whatever'));\nThere would be no mileage from Kefir, Most or Bacon from that unless the spec changes to include that forEach must be implemented on returns from [Symbol.observable](). cc/ @zenparsing\n. > There would be no mileage from Kefir, Most or Bacon from that unless the spec changes to include that forEach must be implemented on returns from Symbol.observable\n... actually in that case it wouldn't matter anyhow, because the libraries that aren't RxJS 5 or zen-observable would need to be subscribed to in a different way.\nI think that using forEach is probably your safest bet. It returns a promise, so you wouldn't have to make a change.\n. > I would have preferred something like Promise.resolve(observable).\nGood luck with that one. It's not a bad idea, but I think it will be hard to get traction on changes to Promise to support Observable.\nHowever, with RxJS, you can just use Observable.prototype.toPromise() which will convert the Observable to a Promise that resolves with the last value of the Observable when the Observable completes, or reject with the Observable's error if it errors.\n. Also, I think it would be Promise.from(observable) ;)\n. The ideal way to test observables with RxJS 5 is actually to use the TestScheduler. However, in a broader sense, support for the [Symbol.observable] hook in a test suite might be a nice addition. Something like how you can return a promise in some test suites and it will automatically use that promise to create an async test. \nThe one catch is that unlike promises, sometimes observables are just cancelled, and cancellation does not fire an event for the consumer. That is easy to work around, but the user would need to know what they're doing.\n. @marcusnielsen: No I don't think it's an \"anti-pattern\" but it could result in slow test suites. Also if the source of your hot observable is something non-deterministic it could get hard. That's not usually the case, but it is the reason the TestScheduler exists. The TestScheduler enables synchronous testing of observables by putting all notifications into a virtual scheduler that can be flushed after tests are set up.\n. haha... I just read my previous comment and edited it. I think I dictated it to Google's voice-to-text, because it said \"test sweets\" instead of \"test suites\" haha... Sweet tests! \n. I'm unsure of what the goal is here for \"Supporting Observables\". What do you need them to do? Do you need to support subscription to them? Do you need to support returning them?\nIf it's the former, you should be able to test for obj[Symbol.observable] and call it (like a function) to get back an observable object with a subscribe() function on it that follows the zen-observable spec. For that, there's really no need to import a library.\n. ",
    "blakeembrey": "Was just looking at trying out Ava and I'm actually surprised if something like this has support. Why would a resolved promise fail? You should be using Promise.reject.\nEdit: Ok, sorry, that's a confusing comment/code. You're saying that it shouldn't be failing, great :+1: \n. @schnittstabil Yes, we're in agreement. For some reason I wrote all my comments in the context of the resolve, not the throws.\n. ",
    "paazmaya": "@vdemedes thank you for fixing this! :grapes: \n. ",
    "dylang": "In addition, if I use the fixture as execCli is doing, it will pass if the test is in /fixtures, but fails if the test file is in the root directory of the repo:\n\n. Okay, I thought the test wasn't working, but it must be user error. Closing, thanks!\n. @sindresorhus Thanks for the feedback! \n. It seems simple to change the message in https://github.com/sindresorhus/ava/blob/04b3db5d96d6530d6b03c5f8f4c915dea16cd125/lib/test.js#L155, or would you prefer to use the stored actual and expected properties in the rederer?\n. ",
    "lagden": "I've tried that before open this issue, but ran using ES6 code. :bug: \nAnd now, I run with transpiled files and works almost fine!! :smile: \nThe coverage file is created apparently right in output folder, but the report appears empty when I run the test. Like the image above in the first post.\nWell, I will waiting the new release.\nThanks for reply!\n\n. Fine!\nJust a small feedback! In Travis, the report output worked!\n\n. Don't worry...\nWe are here to try help and make the AVA even better. :metal: \n. ",
    "davidchase": "I seem to be still having the issue... im currently using ava@0.3.0 with nyc and my coverage is blank...\nIs the ava@0.3.0 you the release that was suppose to happen? i also tried with ava from the gihub master branch and same results\n. It seems like it was an issue how my nyc config was setup sorry about the previous comment... @sindresorhus do you know how fine tuned we can instrument excludes in nyc?\nthe setup below seems to include a lot more files than i need covered... maybe this is a better question for the nyc repo sorry to bug\njson\n\"config\": {\n    \"nyc\": {\n      \"exclude\": [\n        \"node_modules/utils\"\n      ]\n    }\n  }\n. hmm it seems like that .istanbul.yml does not work for me as i get no coverage and using the above json config i get too much :disappointed: cant find that happy medium...\n. @Qix- so i have some files that live the node_modules/utils folder and i understood from excluding-files setting up just the following config:\njson\n\"config\": {\n    \"nyc\": {\n      \"exclude\": [\n        \"node_modules\"\n      ]\n    }\n  }\nshould add my utils folder, etc in the coverage however it results in empty coverage ...\nalso specifying the exact folder node_modules/utils produces too many results... seemingly pull from other npm packages that live in node_modules\n. ",
    "timoxley": "Would be good to reverse this decision\u2026 the ability to run tests without the ava process simplifies things as it removes any complications and potentially subtle bugs created by having different versions of AVA installed globally and in node_modules/.bin, even if it required additional hoops such as loading a bootstrap file first e.g. via the node --require flag. \n. @sindresorhus ty.\n. @sindresorhus ahh understood, this makes sense. Can close.\n. > perhaps they should all be forcibly bound to the current instance\nThis could impact people interested in monkeypatching AVA Tests. To maintain some extensibility via clever thisArg trickery, could perhaps do the binding external to the Test constructor e.g. in the Runner\nThis may not be worth it though as I cannot think of a good use case. Perhaps do this if someone complains.\n. Also note we're opting for checking & printing error before checking if .end has been called more than once. This is how tape does it and I think it makes sense as the error message + stack is going to be more helpful messaging than simply 'called end more than once'. They'll get this message too, but only after they fix the actual source of the error.\n. oh yeah wtf am I doing\n. Originally this was !!err but linter told me to Boolean(err) so I followed blindly.\n. What happens if you have a plan and call t.end?\n. Updated.\n. Idea is to only log the assertion if there's some arguments passed. Alternative is have every test log an additional ifError assertion at the end.\n. You're right.\n. Perhaps it should always log the assertion if there's arguments.length? i.e. intent is to use as errback handler.\n. in neither of those situations will the t.end be called with any arguments though.\n. The use-case I'm thinking about goes something like this:\njs\ntest(t => {\n    fs.readFile('data.txt', t.end);\n});\n. ",
    "zweicoder": "+1, would like to see this fixed!\n. test.skip also seems to be stuck in some loop continually failing tests. I'll open a new issue if needed.\nEdit: Added a callback for test.skip, and it works fine now. So the weird behavior was due to test.skip()  not getting a callback. Issue with test.todo still persists!\n. ",
    "madbence": "yup, still the same, btw it might be the same as #108 ?\n. So right now there is no way to test modules directly? I mean, i have to transpile them first, and test the transpiled files?\n. what about allowing something like $ babel-node node_modules/.bin/ava ? i'm not really familiar with ava internals, so that's just an idea.\n. @sindresorhus \n- --transpile-all|-a\n- --compile-all|-a\n- or to be more flexible, do something like --compilers js:babel/register (like in mocha, istanbul, etc)\n. btw what's the rationale behind this behavior (ava transpiles tests only)? it's just slow?\n. hmm, i still don't get the point... why should i write my tests in es2015 if i can't write my lib in es2015? if i want to support some older node versions, i know that i have to publish a transpiled version of my lib (so i should test the transpiled code, which is not the default behavior right now). but maybe i'm just missing something about how one should develop modules in es2015... \n. imho just store dependencies of the tests, if some of them changes, we can rerun the tests. there is no better way to do this (that is simple).\n. fyi continuation-local-storage is built on top of async-listener, which provides a simple get/set api to track data across async boundaries.\n. @jamestalmage if you're working with a PR-based workflow, this won't be a problem, because cache is per PR (at least on travis), so the PR will fail before merge.\n. also the prs will be still slow, since they don't have cache on the first build\n. ",
    "axross": "\nI'm open to considering an option for enabling this, but it will not be by default.\n\nI agree. I want to run both tests and target codes for tests with Babel. I think that is proper and as it should be.\n. ",
    "evgenyrodionov": "Hm, any progress on that? Still can't use external files.\n. pm2 uses execInterpreter option. I think it's ok because most users will install custom interpreter (coffeescript or babel) as dependency and ava can search in local node_modules automagically. \n. ",
    "bookercodes": "@theaqua If I understand correctly, that is what Mocha does too.\n. @madbence I like --transpile-all. \nI think --transpile-transitive is more descriptive/accurate, though.\nThen again, \"transpile\"  a general term. As AVA specifically transpiles using Babel, maybe \"babelify\" would be a better word.\n. @Qix- -tt for short\n. @madbence https://github.com/sindresorhus/ava/issues/50\n. Good  points, @jamestalmage.\nMy situation is this: I am writing an Express application and using a Babel require hook in an entry-point to transpile my entire app (example.)\nI have seen @floatdrop's solution but it would be tedious for me to use, as I require many test files.\n@Whoaa512 has the same problem as me. Fair play to him, he actually sent a pr. It got turned down but now it sounds like @sindresorhus is considering it...?\n. By the way, I really like how Mocha allows you to plugin transpilers like Babel:\nmocha ./test --recursive --compilers js:babel-core/register\nIf Mocha can make Babel work in source files (despite all the valid concerns you raise, @jamestalmage), I am sure it can work for AVA, as well. Whether or not this is something the maintainers/community at large want implementing is another matter!\n. Thanks, @sindresorhus. I'll look into that now.\n. Thanks, @sindresorhus. I'll look into that now.\n. Hi,\nI gathered some data on the default timeouts used by other popular tools. Hopefully it will aid you in reasoning about a \"sensible default\":\n| Tool | Default timeout for async tests | Source |\n| --- | :-: | --: |\n| Mocha | 2000ms (2s) | source |\n| Jasmine | 5000ms (5s) | source |\n| Node Tap | 30000ms (30s)* | source |\n| Tape | None | N/A |\n* 240000ms (240s) if __coverage__ global is truthy. \nAs an aside, I think there should be a command-line argument and/or environment variable to configure the timeout value. This'll be useful when running tests in a slow environment.\n. Thanks for the input, @schnittstabil - I apprecaite it :smile:.\nI completely agree that it should be \"two ore more tasks\" - I edited the original answer and this pull request in response.\nBare in mind the answer has 400:heavy_plus_sign:   positive responses which tells me the answer is useful.\nI think it reads fine, to be honest. What's more, because I have included a link to the SO thread someone can go there for additional context and alternative answers or analogies.\n. @vdemedes You know what, mate? That is a good idea. \nDone.\n. @schnittstabil I meant to link to the question. \n. Hm. I still don't really understand why the -- is needed. \nWould it make sense to elaborate under the \"Running tests with watch mode enabled\" section?\n. Thanks, both.\n. Thanks for this, @leebyron! \nCan't wait for this to be merged \ud83c\udf86. In the meantime, is there a convinent way to apply this patch, do you know?. Thanks for this, @leebyron! \nCan't wait for this to be merged \ud83c\udf86. In the meantime, is there a convinent way to apply this patch, do you know?. Hm. I tried to apply this locally but ran into some trouble. If anyone has any ideas, please let me know! Thanks!. Hm. I tried to apply this locally but ran into some trouble. If anyone has any ideas, please let me know! Thanks!. @leebyron Ah, I guess you could say npm link was the missing... link. Gosh, I'm hilarious!\nJokes aside, I managed to apply your fix locally and things work smoothly in my project, thanks!\n. @leebyron Ah, I guess you could say npm link was the missing... link. Gosh, I'm hilarious!\nJokes aside, I managed to apply your fix locally and things work smoothly in my project, thanks!\n. @schnittstabil As is stated: \"concurrency enables parallism\". Of course there is an overlap.\nWhat do you suggest?\n. ",
    "codyhatch": "It doesn't seem to work for me.  I added require('babel-core/register') to the top of a test file, then ran ava ./example/test-file.js, and it spat out:\n(function (exports, require, module, __filename, __dirname) { import fetch from 'isomorphic-fetch';\n                                                              ^^^^^^\nSyntaxError: Unexpected reserved word\nAm I missing something?  For reference I'm writing my entire browser-based app in ES2015, and I just want a testing solution that'll efficiently transpile everything.  Mocha handles this really smoothly as @alexbooker noted.  I wouldn't mind adding the babel registration to the top of my tests if it would work, although it wouldn't be optimal.\nIt's nice that ava lets you write ES2015 tests on non-ES2015 projects, but if it won't work on ES2015 projects it seems like it's going to be pretty marginalised.  :)\n. So let's assume I have a project like:\nsrc/\n  app.js\n  modules/\n    module1.js\n    module2.js\n    ...\ntest/\n  app.spec.js\n  module1.spec.js\n  module2.spec.js\n  ...\nSo the workaround is to create a new module2.wrapper.js for every single test, which does nothing but register babel and then import the actual test?  That...seems to work.  But wow, that's a really ugly solution for a large project.  :(\nThanks though.\n. ",
    "billyjanitsch": "+1 for a --compilers (or perhaps --preprocessors) flag that lets you use custom register hooks, like mocha does (as @alexbooker alluded to) with a pretty clean syntax. Other test runners (Karma, babel-tape-runner) do similar things.\nIncluding import babel-core/register at the top of every test file is clunky enough, and gets much worse if you want to write a custom compiler (e.g. like this, which is necessary to test JS that uses CSS modules). You should be able to specify this when invoking AVA.\n. Ah, my mistake, thanks for clarifying!\n. I was specifying my own, and forgot that it would pick up node_modules.\n. Thanks for your thoughts!\n\nI think there is value to this at some point. I just don't think it should be a priority.\n\nIt depends on how you want to develop the project, of course, but FWIW, AVA is currently unable to test anything that uses webpack's require aliasing -- which includes a ton of projects these days. It would be great to at least think about a workaround for this case.\n\nAny custom pre-processor you wrote would have to handle the IPC protocol we already have in place, would have to apply the power-assert transform on its own. As well as integrate communication of uncaught exceptions, and unhandled promise rejections. All that is currently bundled in babel.js. Some would be fairly easy to extract and make generic, some would not.\n\nI agree that this would be nice, but I don't think it's immediately necessary. For now, I'd be fine with a simple layer between the tests and AVA which runs an (optional) pre-processor on each test before passing it off to the existing implementation. The assertion errors, etc. would all be on transpiled code, which certainly isn't ideal, but still a huge improvement over the current state.\nA better implementation could be figured out once the project is a little more stable.\n\nMaintaining multiple preprocessor solutions at this point will slow us down, which I think would be bad for the project overall.\n\nTrue, but I wasn't thinking that AVA would actually implement any custom pre-processors. It would just support them if provided.\nIf you're at all convinced, I'm happy to work on a PR proposal.\n. @novemberborn It's great that AVA allows custom Babel config now, but this issue was already specifically about custom non-Babel pre-processors. There are some sample use cases in my original post:\n- webpack with any non-standard loader, e.g. css-loader\n- a custom require hook like those allowed by Mocha (may include things like custom require aliasing, environment polyfills, global test environment behavior)\n. @novemberborn It's great that AVA allows custom Babel config now, but this issue was already specifically about custom non-Babel pre-processors. There are some sample use cases in my original post:\n- webpack with any non-standard loader, e.g. css-loader\n- a custom require hook like those allowed by Mocha (may include things like custom require aliasing, environment polyfills, global test environment behavior)\n. @jokeyrhyme See https://github.com/sindresorhus/ava/pull/296#issuecomment-172105935. Unless things have changed since that PR, --require hooks lib code imported by tests but not the test code itself.\nMocha's --compilers implements the behavior that I'm interested in: a custom pre-processor for the tests (which cascades into their imports)\n. @jokeyrhyme See https://github.com/sindresorhus/ava/pull/296#issuecomment-172105935. Unless things have changed since that PR, --require hooks lib code imported by tests but not the test code itself.\nMocha's --compilers implements the behavior that I'm interested in: a custom pre-processor for the tests (which cascades into their imports)\n. Sorry for the late reply; I didn't realize that progress was being made on #229. FWIW, I would definitely prefer that the Babel hook be disabled. Otherwise, if I'm using Babel as part of my custom register script, it needs to get loaded once to compile the tests, and then again for each individual test require. Isn't this is a huge perf loss (due to all of the Babel spin-ups, and the loss of its internal module caching)?\nFor reference, using the require('babel-core/register') workaround at the top of each of my tests (which, as I understand, is functionally equivalent to what passing --require babel-core/register would look like without disabling the hook) makes my test suite run around 12x slower in AVA than in its Mocha equivalent (passing --compilers).\nAlso, unless I happen to be using exactly the same Babel options as AVA, I'd prefer not to write my test and lib code in different subsets of ES6/7/JSX.\n. Sorry for the late reply; I didn't realize that progress was being made on #229. FWIW, I would definitely prefer that the Babel hook be disabled. Otherwise, if I'm using Babel as part of my custom register script, it needs to get loaded once to compile the tests, and then again for each individual test require. Isn't this is a huge perf loss (due to all of the Babel spin-ups, and the loss of its internal module caching)?\nFor reference, using the require('babel-core/register') workaround at the top of each of my tests (which, as I understand, is functionally equivalent to what passing --require babel-core/register would look like without disabling the hook) makes my test suite run around 12x slower in AVA than in its Mocha equivalent (passing --compilers).\nAlso, unless I happen to be using exactly the same Babel options as AVA, I'd prefer not to write my test and lib code in different subsets of ES6/7/JSX.\n. I'm not sure that's the best solution. I want to use a custom register hook that includes the Babel hook along with other patches (extending require for non-js imports), so I won't be passing babel-core/register to --require directly.\n. I'm not sure that's the best solution. I want to use a custom register hook that includes the Babel hook along with other patches (extending require for non-js imports), so I won't be passing babel-core/register to --require directly.\n. @jamestalmage Any idea why Mocha would be outperforming AVA by such a huge factor, in that case? Are there existing perf issues that you're aware of? (Sorry, not directly related to this PR)\n. @jamestalmage Any idea why Mocha would be outperforming AVA by such a huge factor, in that case? Are there existing perf issues that you're aware of? (Sorry, not directly related to this PR)\n. @jamestalmage \nThat's great to hear!\nCurrently this is the biggest migration blocker for us: it's important that our tests run in the same transpilation environment as our lib code (we now use several custom Babel transforms). Our test suite is large enough (>100 files) that I'd rather not rewrite it in AVA until I know this will be possible.\nThat said, I'd be happy to convert a couple of our tests to see what the perf looks like relative to Mocha.\n. @novemberborn \n\nMy approach to this is to transpile using babel-cli and run the tests on the transpiled code as it sits on the disk.\n\nI think you're talking about transpiling lib code (imported by tests). This PR provides an easier way to do that: --require 'babel-core/register'.\nI'm talking about having test code transpiled in the same way that your lib code is, rather than letting AVA transpile it. See the comment that I linked to for more info.\n@jamestalmage \n\nIs it open source? What is your current test setup? \n\nUnfortunately not. I work on Kensho's front-end and we're currently using Mocha/Chai (assert style). I'd like to move to a flat Tape/AVA-style interface.\nWe do have an open source React chart lib -- also using Mocha but faking a Tape-ish API -- that we might be able to convert as well.\n\nYou may also want to wait for customizable assertions, so we can create one that matches your current assertion api exactly.\n\nI'm actually not a huge fan of our current assertions, so I don't mind rewriting them when the time comes. AVA's defaults seem great.\n. It may also be worth thinking about install time as well as size. See #841 -- on my machine, AVA's install took ~50.1s. I wonder how much of that is I/O (in which case it will be fixed by addressing the size issue) vs. dependency resolution (in which case the deep nesting itself is also a cause) vs. network traffic (a bit of both, since package requests are made individually).\nAnother option is to pull out some of the larger deps that only cater to certain use cases into optional plugins, although I suspect the core team would prefer to avoid additional config.\n. @jamestalmage I agree with @ariporad -- it's unintuitive that if my lib code is using babel-preset-stage-[<2] and/or other custom plugins (e.g. I use one to extend import syntax), then I can't use these features in my test code.\nPersonally, I think the --require flag should disable the Babel hook in favor of the user-provided register hook, but something like a --no-babel option would work too.\nMore generally, AVA works well for the (increasingly less common) case of packages written in ES5 with no transpilation step (since you get to write your tests using modern syntax \"for free\"). However, I'd argue that it drops the ball for the (increasingly more common) case where library code is transpiled anyway: now, despite you having set up tooling to use the syntax you want in lib code, you have to write your tests in a predetermined (and typically strictly smaller) subset of new syntax.\n. > My proposal is that we allow the following in .babelrc:\nThis is a great start! To keep my AVA and base configs identical, do I write \"ava\": {}? This is a little different from how the .babelrc env field works in general:\njs\n{\n  \"presets\": [\"es2015\", \"stage-0\"], // base config\n  \"env\": {\n    \"development\" : {}, // commenting this out does nothing\n    \"ava\" : {} // commenting this out does something!\n  }\n}\n\nDo you have plans for the general case of a custom register hook, which may transpile imports using transpilers other than Babel? Is that what --no-babel is for?\nFor reference, here's the hook we're passing to Mocha (which is applied to test code via --compilers, as opposed to AVA's --require which targets tests' imports). The require extensions are used to nullify webpack's aliasing & loaders.\n. @jamestalmage In its current form, does --require transform test code, or only imports thereof? (i.e. does --require coffee-script/register allow me to write my tests in Coffeescript?)\n. Yikes, sorry, I missed this while searching -- I was surprised that it hadn't been brought up. Thanks!\n. Here's a repo: kensho/babel-preset-kensho@f7bbbf4064a0d0c623fc6b9e226d17623363081d. My mistake -- thanks for clarifying \ud83e\udd24. I would love this as a flag as well, and IMO it should be enabled by default in all environments -- not just CI. I've never liked the behavior of auto-creating new snapshots without -u: I imagine most users think of running ava as a pure function which returns a boolean describing whether their tests passed or failed. So it's odd for ava to have side effects by default, especially those which modify VCS-tracked files.\nPragmatically, it complicates scenarios like running a test suite as a commit/push/prepublish hook. I've often successfully pushed commits (relying on my git hook to fail if the tests do anything other than succeed) only to realize that the test suite passed but dirtied the repo, so my (incorrect) commit shouldn't have gone through.. >exit with an instructive failure and exit code of 1\n+1 -- this is all I really want (although the rest of your idea sounds great too \ud83d\ude04)\nI am a bit limited on OSS time at the moment but when I free up I can take a look!. (Apologies for the notification spam.) As the original reporter of standard-things/esm#197, I just want to express appreciation for everyone working on this. Always awesome to see OSS projects come together to support interoperability for users. Thanks y'all \u2764\ufe0f. Very cool @novemberborn, I didn't think to try the beta release. I haven't been following its development too closely, but if I understand your PR correctly, {\"babel\": \"inherit\"} is becoming the default behavior?. Very cool @novemberborn, I didn't think to try the beta release. I haven't been following its development too closely, but if I understand your PR correctly, {\"babel\": \"inherit\"} is becoming the default behavior?. @novemberborn why does AVA still transpile ESM to CJS in that PR? The Babel config has the env preset configured with {\"modules\": false}.\nOr does the new AVA version merge its Babel config with mine, rather than delegating entirely to mine?. @novemberborn Hmm. That makes sense, but it's surprising to me that an ESM->CJS transform is among the transforms automatically applied by AVA if the user has explicitly set something like [\"@babel/preset-env\", {\"modules\": false}] in their config (as in the case in your PR to my repo).\nAlso, isn't the point of adding {\"require\": \"@std/esm\"} to an AVA config to use std/esm for test files? To use std/esm for the pkg itself, I would have an index.js like the following (which my test files would presumably import from):\njs\nmodule.exports = require(\"@std/esm\")(module)('./lib'). Glad you like it, and npx @ava/init sounds great to me.. >If you could do the PR\nhttps://github.com/avajs/ava-init/pull/8 :)\n\nThis is advanced but I wonder if npx @ava/init should offer to install either latest or next? I think as your PR is currently written we make it really hard to install the beta releases.\n\nPersonally I don't think this is necessary. Upgrading to next is easy enough: npx @ava/init && npm i -D ava@next.\nIf you do want the feature, I think latest should remain the default, but I could add a --next flag to the @ava/init CLI.. Ok, can I do it in another PR? The existing one is already pretty big.. >To give you a longer response\nNo worries, I understand! PR's up: https://github.com/avajs/ava-init/pull/9. Updated this PR to reflect the new name. How do you feel about the instructions defaulting to --next? Seems more appropriate since the rest of the readme reflects the beta release.. >What are your thoughts on advocating purely npx @ava/init? I wonder if we should have an explicit \"Manual installation\" section? Similarly perhaps we should point out that npx @ava/init works with Yarn?\nGood points -- I especially think a \"manual installation\" note is important for the sake of being more transparent about what @ava/init is doing. Added to the PR.. Not sure what's going on with CI. I can try rebasing?. Thanks! I think this is good to go, then, unless you want any other changes.. Yeah, I agree that it's always useful to show the assertion excerpt.\nDo we always know where the test file is? If so, I feel like the stack trace should always include everything from the assertion up to the test (which would normally be just the assertion and the test, except in the case of macros).\nI'm not sure that it's sufficient to only do this if the trace spans multiple files, because it would still be difficult to debug a failing assertion in a macro with an automatically generated title in the same file.. >Indeed it's not sufficient to check if the trace spans multiple files, which is why I suggested checking if there are multiple call sites from a single file. Does that make sense?\nSorry, I had misread what you wrote! Yes, I agree completely.. This is awesome!. >It's an aversion from having too many ways of specifying config.\nI have the same aversion, but from the perspective of a user using multiple tools in projects. I try to write my config files similarly to each other, so that they're easy to write, read, and modify. This becomes difficult when tools have divergent config formats.\nSo, from my perspective, ESM is a different way of specifying config, in that it's different from other tools, e.g. webpack, Babel, postcss, ESLint, stylelint, husky.\n(That's why I'm a big fan of cosmiconfig, which is already used by many of the above -- by default, it supports a well-known, conventional set of formats which strike a good balance between user flexibility and complexity. It also reduces the burden on module authors to implement config lookup/loading, and provides consistent lookup/loading behavior between tools.)\nI understand the tradeoff on simplicity, and I know that @sindresorhus has a strong preference for it. But if AVA is determined to support a single format, would you consider only supporting CJS? I'd think this would be the expected behavior at least until ESM is widely embraced in node (whether via esm, .mjs, or something else). Until then, this doesn't seem like a useful divergence.. Thanks for the discussion, by the way -- thrilled to be getting file-based config in any case.. @novemberborn I went ahead and PRed the dependent projects \ud83d\ude42. ",
    "sparty02": "import babel-core/register is a really good, simple workaround until this feature lands.  @sindresorhus what do you think about putting this in the README until it's officially addressed?  (I wouldn't mind submitting a PR for it)\n. > Because, one goal of this library is not to pollute any of your production code with babel transforms/polyfills (unless you specifically ask it to).\nI actually want my source code (non-test) to be transpiled by babel as well..... i.e. I want to be able to import a module that is written in ES6 into my ava test module.  Is there a suggested technique for this today?  Or is this a pending enhancement?\n. @sindresorhus  ahh.... thanks for the reference to #111.  I'll take a look!\n. I tried nyc with ava and failed...  :cry:   I feel like I'm doing something really simple wrong.  \nHere's a snippet of my run scripts:\njs\n\"scripts\": {\n    \"test\": \"ava src/test/**/*.js\",\n    \"coverage\": \"nyc npm test\"\n}\nI ran npm run coverage and got back:  \n```\n\nnyc npm test\n\nmodule.js:339\n    throw err;\n    ^\nError: Cannot find module '[ path to my package root ]\\npm'\n    at Function.Module._resolveFilename (module.js:337:15)\n    at Function.Module._load (module.js:287:25)\n```\nAny chance someone on this thread could assist in some light guidance?  I'd be happy to follow up with some README updates.\n. I tried nyc with ava and failed...  :cry:   I feel like I'm doing something really simple wrong.  \nHere's a snippet of my run scripts:\njs\n\"scripts\": {\n    \"test\": \"ava src/test/**/*.js\",\n    \"coverage\": \"nyc npm test\"\n}\nI ran npm run coverage and got back:  \n```\n\nnyc npm test\n\nmodule.js:339\n    throw err;\n    ^\nError: Cannot find module '[ path to my package root ]\\npm'\n    at Function.Module._resolveFilename (module.js:337:15)\n    at Function.Module._load (module.js:287:25)\n```\nAny chance someone on this thread could assist in some light guidance?  I'd be happy to follow up with some README updates.\n. @jamestalmage with that change, running npm test   :\n```\n\nnyc ava src/test/*/.js\n\nmodule.js:339\n    throw err;\n    ^\nError: Cannot find module '[ path to my package root ]\\ava'\n    at Function.Module._resolveFilename (module.js:337:15)\n    at Function.Module._load (module.js:287:25)\n``\n. @jamestalmage with that change, runningnpm test`   :\n```\n\nnyc ava src/test/*/.js\n\nmodule.js:339\n    throw err;\n    ^\nError: Cannot find module '[ path to my package root ]\\ava'\n    at Function.Module._resolveFilename (module.js:337:15)\n    at Function.Module._load (module.js:287:25)\n```\n. Yeah, ava runs just fine by itself.  Here is a snippet from my dev deps:\n\"devDependencies\": {\n    \"ava\": \"^0.7.0\",\n    \"nyc\": \"^4.0.1\"\n}\n. Yeah, ava runs just fine by itself.  Here is a snippet from my dev deps:\n\"devDependencies\": {\n    \"ava\": \"^0.7.0\",\n    \"nyc\": \"^4.0.1\"\n}\n. Tried that, no dice.... same error as above....  :disappointed:   Here's the full stack trace:\n```\nPS [ path to my package root ]> npm test\n\n[my package name]@[my package version] test [ path to my package root ]\nnyc ava src/test/*/.js\n\nmodule.js:339\n    throw err;\n    ^\nError: Cannot find module '[ path to my package root ]\\ava'\n    at Function.Module._resolveFilename (module.js:337:15)\n    at Function.Module._load (module.js:287:25)\n    at Function.Module.runMain (module.js:467:10)\n    at Function.runMain ([my user folder].node-spawn-wrap-14548-317d756fdb99\\node:40:10)\n    at Object. ([ path to my package root ]\\node_modules\\nyc\\bin\\nyc.js:17:6)\n    at Module._compile (module.js:435:26)\n    at Object.Module._extensions..js (module.js:442:10)\n    at Module.load (module.js:356:32)\n    at Function.Module._load (module.js:311:12)\n    at Function.Module.runMain (module.js:467:10)\n```\n. Tried that, no dice.... same error as above....  :disappointed:   Here's the full stack trace:\n```\nPS [ path to my package root ]> npm test\n\n[my package name]@[my package version] test [ path to my package root ]\nnyc ava src/test/*/.js\n\nmodule.js:339\n    throw err;\n    ^\nError: Cannot find module '[ path to my package root ]\\ava'\n    at Function.Module._resolveFilename (module.js:337:15)\n    at Function.Module._load (module.js:287:25)\n    at Function.Module.runMain (module.js:467:10)\n    at Function.runMain ([my user folder].node-spawn-wrap-14548-317d756fdb99\\node:40:10)\n    at Object. ([ path to my package root ]\\node_modules\\nyc\\bin\\nyc.js:17:6)\n    at Module._compile (module.js:435:26)\n    at Object.Module._extensions..js (module.js:442:10)\n    at Module.load (module.js:356:32)\n    at Function.Module._load (module.js:311:12)\n    at Function.Module.runMain (module.js:467:10)\n``\n. @novemberborn \nnode v4.2.0\nnpm v3.5.2\n. @novemberborn Ah, yep, I'm on Windows (at least part time)....that must be it then.  I'll watch that issue for updates.... thanks!\n. @bcoe @sindresorhus sorry guys, I spaced out on this.  I'll give it a shot tomorrow morning and circle back to let you know.\n. @bcoe I pushed a sample repo at https://github.com/sparty02/nyc-ava-demo.  I stripped it down to almost nothing, and am still hitting the issue, so it definitely seems environmental.  Let me know if I can provide any more info!\n. @bcoe @sindresorhus Just a heads up that I saw some activity over on nyc that was done in the past few days that appeared to (at least loosely) be related to better spawn support on Windows.  Long story short, I just tried my use case again withava@0.9.1andnyc@5.2.0` and it worked!\n. @jamestalmage thanks for the update!\n. Do you want me to squash these updates into a single commit?\n. @jamestalmage  Good feedback, I've addressed this in the latest updates.\n. @jamestalmage done!\n. @jamestalmage done!\n. Can you clarify?\n. ",
    "franleplant": "Hi!\nimport 'babel-core/register' and import 'babel-register' are not working for me.\n. Already done that @jamestalmage \n. It was purely a problem with babel presets/plugins/config settings, nvm, thanks a lot\n. :)\n. ",
    "Hurtak": "for anybody trying to make import 'babel-core/register'; work in AVA, try installing babel-core 5.x.x, that did the trick for me, 6 doesn't work for some reason (you probably need have properly configured .babelrc in your directory or something)\n. for anybody trying to make import 'babel-core/register'; work in AVA, try installing babel-core 5.x.x, that did the trick for me, 6 doesn't work for some reason (you probably need have properly configured .babelrc in your directory or something)\n. - That doesn't seem to be problem, you could just output ${ testGroup } - ${ testName }... for these line-by-line test results\n- For CI/whatever you could have different reporting which would print the results after all tests are done, so the output can be nicely formatted and nested like in mocha\n. It looks like this\n```\n[20:10:39] Starting 'test'...\n[20:10:42] gulp-ava:\n8 passed\n[20:10:42] Finished 'test' after 2.67 s\n```\nWhat I would prefer in case of no errors\n[20:10:39] Starting 'test'...\n[20:10:42] Finished 'test' after 2.67 s. ",
    "kentcdodds": "This issue is solved for me by adding --require 'babel-register' (Babel 6) with a .babelrc file:\njavascript\n{\n  \"presets\": [\"es2015\", \"stage-2\"]\n}\n. If you're interested, here's a migration from Mocha to AVA in a small library: https://github.com/kentcdodds/starwars-names/pull/12/files?w=1\nedit the reason I bring it up is because it supports ES6 with Babel 6 in my source without having to import babel-register. Thought you'd all be interested.\n. Cool, I added a comment about that. I prefer to use config to avoid polluting the package.json keyspace :-) (It's also technically the recommended way to do what is being done).\nNot really any pain points. The only thing that I would say is the assertions are pretty limiting. I prefer the declarative syntax of something like Chai. But I expect that the small surface area of the API is intentional. I ended up bringing in another utility library to make testing a little more declarative though.\n. When you've got the website up, if you wanna have a listing for users, you can add PayPal to that list. We're happily using it as of today :-) PayPal.svg\n. Sorry to chime in here, but I'm looking around at issues looking for something else and noticed this. Another solution to address:\n\nYou do need to commit the transpiled dist if you want to temporarily rely on your hotfix\n\nIs there any reason you couldn't just publish a temporary release with npm publish --tag beta? docs\n. Ah, missed your use case there. Makes sense. And The scoped package is a good work around until your proposal is dealt with. :+1: \n. Add ghooks to the list of modules that use the config convention :-) But meh, it'll never conflict.\n. Shouldn't this be part of the config object in package.json? Or am I missing something?\n. > as well as a way to allow people to use fancy stuff like css modules in their code to test.\n\nAVA with React/webpack loaders.\n\nIf you can do webpack loaders in AVA tests, I want to see that. In my mind, this is a real challenge for any project using webpack heavily!\nI'd love to review what you come up with :D\n. I would recommend that the recipe only reference these abstractions but not actually use them. Just use the official React testing utils for the examples to give a good baseline and let other people choose what they'd prefer.\n. Also, I heard that using that plugin made things extremely slow. Has this been your experience?\n. While we're on the subject of Webpack, what do you do about webpack aliases and custom modulesDirectories?\n. That's a good way to go I think :)\n. > Babel plugin webpack \nJust to be clear, you're talking about this one?\n. > Babel plugin webpack \nJust to be clear, you're talking about this one?\n. Yeah, I'd prefer to limit the number of abstractions in a recipe personally. Then just reference the other abstractions.\n. > \"plugins\": [] //maybe we automatically append power-assert for them?\nMy knee jerk reaction is to say no in favor of less magic :sparkles:. I think that if you're going to override the default config, you should know what the default config is doing for you.\n. That makes sense. I'm in favor :+1: Thanks for keeping me in the loop :D\n. @spudly, I love the recommendation. I didn't really feel comfortable mucking around with babel's config. This seems much cleaner :+1:\n. @spudly, I love the recommendation. I didn't really feel comfortable mucking around with babel's config. This seems much cleaner :+1:\n. > Ideally, we would not write code to merge plugins/presets, and instead use Babel's internal 0ptionsManager#mergeOptions method.\nThis is why micro-libraries are reeeeally nice :-) Maybe someone (me?) could extract that logic into a well tested microlib that both tools use?\n. > \"babel\": false\n:+1:\n. > \"babel\": false\n:+1:\n. Thanks for taking this so seriously! :smile: I'm currently working on a repo to compare mocha and ava with a lot of tests with a lot of files. I'm no benchmark expert, but maybe this will be helpful. Will comment when I've pushed it up. Thanks!\n. \n. Totally. If we could make ava as fast as mocha on a per-file basis then ava would scream with speed. This is making me feel really positive about the potential for ava :D\n. Actually I'm not trying to pass the -s flag to AVA. I'm using it to suppress npm run output. I want aware that using beforeEach and afterEach require that tests run serially. I guess that makes sense, but I can't think of a use case where including those would be useful when running in parallel... Is there a better way to do what I'm trying to do?\n. That won't work for my actual use-case. And it doesn't make sense to me that the tests are using the same spy because the spy  should be reset before each test.\n. > They are, but AVA is just too fast :).\nAh, I see. That appears to be a bug to me.\n\nHe was pointing to the use case without the context object\n\nCorrect. In my actual use case, my tests are asserting that something else is calling console.log\n. Ah, I think I understand now. This is part of how AVA helps  (read: forces) you isolate your tests which is good. I'll submit a PR which mentions this.\n. Was going to, but @BarryThePenguin beat me to it :+1: \n. > Also, noting the api change, this allows for test.skip(). Not sure how helpful that is to people\nThat makes sense. I wonder though if we should think about explicitly disallowing that to avoid people abusing it and confusing others.\n. You know, I would actually prefer to leave skip as it is and introduce a todo that only accepts a message. I would still like to have the todo in the output so it's a constant reminder. I think that's the value of coding the todo rather than just using a comment.\n. I'm not certain I follow the results of all the changes. But I'll just say that I would recommend that skip require a function and the only way to avoid having to specify a function is if you use todo. Fewer ways to do the same thing is :+1: :D\n. This is awesome! Thanks everyone for your work on this. Looking forward to seeing this merged :-)\n. This is terrific. I love the level of thought for quality coming from this project. Thanks everyone (especially @BarryThePenguin)!\n. I'm running both of those series of commands now. I'll let you know what I get\n. Everything passed for me for both of those. Sounds like an environment issue...\n. But yes, this fails 44 tests for me:\ngit clone https://github.com/spudly/ava.git\ncd ava\nnpm install\nnpm test\n. > Lots of people want this feature.\nAnyone wanting to use JSX in their test files is still using ava@0.9.2 until this lands. So thank you so much everyone for working on it and making sure that the API is awesome! :tada:\n. Heh, I know that. That's open source. If I really really needed to get this in, I could have taken a more proactive approach to making that happen. #priorities ;-) I'm just glad that there's a shared priority here!\n. FWIW, I just did this:\n$ npm i spudly/ava#461c781fcaabe29eb29464c3d98f9e19b64a8af2\n$ npm test\nAnd all tests pass. Verified I had the right version\n$ node_modules/.bin/ava --version\nAnd got 0.12.0 which I think is correct.\nThe thing that surprised me is that I didn't need to do any configuration to make my tests work with JSX. I was under the impression I had to explicitly add this config:\njavascript\n\"ava\": {\n  \"babel\": \"inherit\"\n}\nBut it looks like my tests were transpiled with my default .babelrc anyway. Is that correct?\nEither way, it's awesome that I'll be able to use the latest AVA soon! :D\n. Success! It probably was the cache. Here's a reproducible command to show old/new behavior.\ngit clone https://github.com/kentcdodds/react-ava-workshop.git && cd react-ava-workshop && npm i && npm i spudly/ava#461c781fcaabe29eb29464c3d98f9e19b64a8af2 && npm t\nYou'll get a parser error.\nNow, to the package.json add:\njs\n{\n  \"ava\": {\n    \"babel\": \"inherit\"\n  }\n}\nAnd rerun npm t and you'll get 10 passing tests :+1:\n. \nThat's me, watching the the status of the AppVeyor build...\n. \nMe now that it's all passing!\n. > is there a reason we're running tests in both appveyor and travis? Seems like one is enough\nGotta make sure windows works (right?)\n. > only took 22 minutes...\nYeah, it only took that to actually run, but it took much longer to start. Believe me, I've been watching :-)\n. Hooray! \ud83c\udf89\ud83c\udf89\ud83c\udf89\n. To be clear, the difference is:\ndiff\nimport test  from 'ava';\nimport { render } from 'react-dom'\nimport React from 'react';\nfunction CustomComponent({ value }) { return <input value={value} /> }\n+ test('does something slow', t => {\n- test('does something fast', t => {\n    const div = document.createElement('div')\n    render(<CustomComponent value=\"3\" />, div)\n    const originInput = div.getElementsByTagName('input')[0]\n-    const val = originInput.value\n-    t.same(val, 33)\n+    t.same(originInput.value, 33)\n})\n. I'm thinking that this may have more to do with the DOM node rather than JSX. If I'm not mistaken, @xjamundx is using jsdom.\n. > It'd be hard to guarantee the behavior AVA promises (async/await for instance) if it might be affected by users own code.\nI think that people will understand that if their babel config doesn't support something, they can't use it. They're coding their ES6 in their source with this config all day every day. I think it'll be natural to continue using what they're used to using.\nBut if you'd rather, we could validate that the config has the properties necessary.\nI like the idea of an ava --init but not everyone will set up AVA that way and I think it'll still be confusing for people.\n\nPerhaps we should automatically add stage-3 to the detected .babelrc\n\nSeems like to much :sparkles: magic :sparkles: to me.\n\nAlways a hard decision.\n\nAgreed :-) But like I said, I hear over and over again that people have trouble understanding that the tests and source are compiled differently. It's just confusing to people. So I think we could do something about it. :-)\n. I think we've only begun to see people start to leverage babel plugins. I think that as time goes on, people will get used to using more and more plugins.\nRegardless though, I think that it's confusing for people that the config used to transpile tests is different from the config used to transpile tests \u00af_(\u30c4)_/\u00af\n. > I would want async-await to work for users regardless of whether they specified that in their .babelrc\nAnd from my perspective, I don't think that people would use async await if they couldn't do so in their source because I generally like to write my tests and source in a similar fashion.\nI like the idea of merging configs by default. That would definitely solve my concerns and allow people to use everything that AVA supports by default. :+1:\n. I know that some people use .jsx, but I know more people who just use .js. I think that both should be supported. But I don't know if it'll make it any easier for most people. There are already plenty of steps people must follow to get AVA testing their React stuff.\n. > React's test naming convention\nThere are many of these. Not really React specific. Definitely preference based.\nI definitely don't believe that we should arbitrarily add plugins and presets to AVA's built-in config. I just mean that if there is an existing babel configuration, users of AVA are obviously transpiling with babel and will assume that the test files will be transpiled in the same way as their source files.\nMaybe having an init with informative questions would be helpful?\n. > Well you got nyc to exclude some patterns (bcoe/nyc#199). Conversely maybe AVA should include them.\nAh, I think I misunderstood you. You mean so I don't have to do:\n\"test\": \"ava \\\"app/**/*.test.js\\\"\"\nBut instead I can:\n\"test\": \"ava\"\nThat'd be awesome! I'll look into making a PR for that :-)\n\nYea. Still, .jsx is pretty clear-cut IMO.\n\nAlso misunderstood you on this one as well. You're totally right. If they're using .jsx, it would definitely make sense to slap the react preset on there :+1:\n. Sorry, here's an example of my use case:\n``` javascript\ntest('passes with a relative path', t => {\n  const output = {filename: relativePath}\n  const result = validate(output.filename)\n  t.notOk(result)\n})\ntest('returns error if given an absolute path', t => {\n  const output = {filename: absolutePath}\n  const result = validate(output.filename)\n  t.ok(result)\n})\n```\nIn this scenario, it \"passes\" if it's notOk which is counter intuitive. Much easier to read this as: t.falsy(result) (note: result can be null or undefined and both are ok in this context).\n. Build broke because of linting. Sorry, I'm a creature of habit (http://kcd.im/semicolons) :-) I've force pushed a fix.\n. Totally understand the value of avoiding aliases and respect if it's decided to avoid them :+1:\n. Opened another PR to add a note to the CONTRIBUTING.md.\n. > If we were to alias them, I would want to immediately deprecate ok and notOk and remove them at some later date.\n\nIt is not possible for you to know what same means without documentation, deepEqual would have been better IMO\n\nI realize that there's a community with code that depends on these things, but I think that as a pre-1.0.0 project, there's a responsibility to get the API correct and obvious, even at the cost of breaking changes. If it's really that bad, we could write a codemod that people could run on their code.\n\nit took me just a few hours to commit AVA's limited list of assertions to memory\n\nI think this is awesome. It would be even more awesome if learning what these assertions do doesn't require a docs lookup. I feel like ok, notOk, and same are examples of things that are easy to learn, but require a docs lookup :-(\nI'd be happy to make the PR to alias things and add a deprecation message.\n. > My problem with same is the ambiguity. It took me a long time to stop thinking it meant \"the same instance\", as in ===.\nI've had the same experience.\nI don't want to bikeshed on stuff, and (so far) it looks like this is pretty subjective anyway :-)\n. If we're going to get people's opinions (this is great). I propose that we just use GitHub issue reactions to do a voting system.\n:+1: this comment if you're in favor of changing ok and notOk (to anything)\n:-1: this comment if you'd prefer to keep ok and notOk as they are.\n:confused: if you don't care either way.\n. > I think @kentcdodds did a Twitter poll about this, or if he hasn't maybe he should.\nFWIW, here are the results \u00af\\_(\u30c4)_/\u00af\n\n\nBut it is longer and more annoying to type. \n\nFor me this shouldn't make a difference. Readability/maintainability should go before how many characters there are IMO \u00af\\_(\u30c4)_/\u00af\n\nIf we do make a change I think it'd be better to soft deprecate for now till 1.0 (ok still works but it's undocumented) and see what people think.\n\n:+1:\n. > It's just too ambiguous, what does ok mean? I could imagine it meaning lots of things: \"properly formatted\", \"not an Error\", \"not a rejected promise\", etc.\nIf you all recall, this is the precise reason that I created this in the first place:\n\nIn this scenario, it \"passes\" if it's notOk which is counter intuitive. Much easier to read this as: t.falsy(result) (note: result can be null or undefined and both are ok in this context).\n\nThe intuition of an API is hard to get right for all use cases, but I think that we can all agree that there are more cases where falsy makes more sense and is more explicit than notOk.\nOn deepEqual vs same: I'm a big fan of this change. I think that deepEqual is common enough that people will get it. If we want to go for a name that people don't have to google to understand then: primitiveValuesAreTheSame might do, but that's just crazy talk... I think deepEqual's great.\n\nI think @kentcdodds was really after a discussion more than getting this merged.\n\nYes, this is true. But I think we've reached a point where we need to start seeing and talking about code (even if it's not quite agreed upon yet, the PR doesn't have to be merged :smile:).\nEither way, this PR will not be merged, so I'll go ahead and close it :-)\n. > It's just too ambiguous, what does ok mean? I could imagine it meaning lots of things: \"properly formatted\", \"not an Error\", \"not a rejected promise\", etc.\nIf you all recall, this is the precise reason that I created this in the first place:\n\nIn this scenario, it \"passes\" if it's notOk which is counter intuitive. Much easier to read this as: t.falsy(result) (note: result can be null or undefined and both are ok in this context).\n\nThe intuition of an API is hard to get right for all use cases, but I think that we can all agree that there are more cases where falsy makes more sense and is more explicit than notOk.\nOn deepEqual vs same: I'm a big fan of this change. I think that deepEqual is common enough that people will get it. If we want to go for a name that people don't have to google to understand then: primitiveValuesAreTheSame might do, but that's just crazy talk... I think deepEqual's great.\n\nI think @kentcdodds was really after a discussion more than getting this merged.\n\nYes, this is true. But I think we've reached a point where we need to start seeing and talking about code (even if it's not quite agreed upon yet, the PR doesn't have to be merged :smile:).\nEither way, this PR will not be merged, so I'll go ahead and close it :-)\n. > Another approach might be to drop / deprecate all truthy / falsey behaviour and encourage developers to use strictly boolean expressions.\nI'd be fine with that as long as we provide a list of good utility libraries for doing common test cases (like deepEqual for instance).\n. > I think those batteries should come included.\nEven better\n. And shall I just say that because the tests for this project are written in tap rather than AVA, I'm coming to appreciate AVA more and more :-) :heart:\n. And shall I just say that because the tests for this project are written in tap rather than AVA, I'm coming to appreciate AVA more and more :-) :heart:\n. Great to explore options for the best API possible while the topic is open for debate. I have no ideas though...\n. Great to explore options for the best API possible while the topic is open for debate. I have no ideas though...\n. I'd be fine with that.\n. I'd be fine with that.\n. > Short names should be low on the list of priorities. There is a limit at which something is too long, but a few extra characters is worth any added clarity.\nCouldn't agree more.\nBut I still have no suggestions to a better name than deepEqual :-)\n. > Short names should be low on the list of priorities. There is a limit at which something is too long, but a few extra characters is worth any added clarity.\nCouldn't agree more.\nBut I still have no suggestions to a better name than deepEqual :-)\n. > how about t.deep()?\nI still prefer deepEqual. To me it's confusing without equal.\n. Could definitely use some help on this to know why tests are failing. It's confusing with mix of ava and assert :-)\nI definitely want to make this happen soon! :D\n. Updated the deprecation to use util.deprecate (TIL) and added a note about the codemod repo. I'll try to give the failing tests another look, but last time I know I had a bit of trouble with them.\n. Alrighty, this should be ready to go I think. Feedback welcome!\n. Also, let me know if you want me to squash and clean things up with the commit message or if you're fine using GitHub's fancy new squash and merge feature :-)\n. Updated PR to respond to @jamestalmage's comments. Thanks! \n. Fantastic! Did we decided on doing truthy/falsy? I'm happy to do that work as well\n. Lol! I promised I searched issues first! Thanks! :-D\n. Responded to all comments :-)\n. I guess this is why we have Travis :-)\n\n. Updated. Should be good to go now :D\n. This has been updated :+1:\n. I imagine that we'll want to add a test for this, but I can't seem to find existing tests for the default inclusion glob...\n. > I would add a note about how to play with webpack\nThat's not really React specific. We should make that a recipe and reference it.\n. I think it'd be useful to reference my workshop as well.\n. Maybe you could say: \"For an in depth guide of seeing up AVA with code coverage on a React project, see this workshop\" or something like that. That'd probably go well at the start\n. So far, I've just used isMatch from lodash with AVA. Would be pretty handy to have built-in\n. Yeah, I vote for implementing the API described in the initial posting. t.match seems too complicated to fit the simplicity of the rest of ava's APIs\n. I've always used: https://www.npmjs.com/package/chai-subset\ncontainsSubset seems OK but long. I'd be fine with like. Either way is good with me.\n. I've updated the project to demonstrate some other patterns that I expect should work but don't. Here's the structure now:\n```\n~/Desktop/ava-bug-test-filename-conventions (master)\n\ud83d\udc7e  $ tree -I node_modules\n.\n\u251c\u2500\u2500 package.json\n\u251c\u2500\u2500 public\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 js\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tests\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 thing.js\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 test-thing.js\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 thing.test.js\n\u251c\u2500\u2500 test\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 thing.js\n\u251c\u2500\u2500 test-thing.js\n\u2514\u2500\u2500 test.js\n4 directories, 7 files\n```\nAnd here's the output (with --verbose):\n```\n~/Desktop/ava-bug-test-filename-conventions (master)\n\ud83d\udc7e  $ npm t\n\u2714 test \u203a works\n  \u2714 thing \u203a works\n  \u2714 test \u203a thing \u203a works\n3 tests passed [14:07:15]\n```\nI would expect the other tests to run and the output to be more like:\n```\n~/Desktop/ava-bug-test-filename-conventions (master)\n\ud83d\udc7e  $ npm t\n\u2714 test \u203a works\n  \u2714 thing \u203a works\n  \u2714 test \u203a thing \u203a works\n  \u2714 public \u203a thing \u203a works\n  \u2714 public \u203a thing.test \u203a works\n  \u2714 public \u203a js \u203a tests \u203a thing \u203a works\n6 tests passed [14:07:15]\n```\nOr something like that. Am I missing something?\n. @nfcampos is correct. Removing that from the CLI in my local node_modules fixes the issue and my output is like this:\n```\n~/Desktop/ava-bug-test-filename-conventions (master)\n\ud83d\udc7e  $ npm t\n\u2714 test \u203a works\n  \u2714 public \u203a js \u203a thing \u203a works\n  \u2714 thing \u203a works\n  \u2714 test \u203a thing \u203a works\n  \u2714 public \u203a thing \u203a works\n5 tests passed [14:18:04]\n```\n(Note, I was incorrect in what I expected before, this is now correct).\n. Haha, sorry, another update. I changed the test names to make things easier. Here's the output that I get (after removing the lines that @nfcampos mentions):\n```\n~/Desktop/ava-bug-test-filename-conventions (master)\n\ud83d\udc7e  $ npm t\n\u2714 test \u203a test.js\n  \u2714 public \u203a thing \u203a public/thing.test.js\n  \u2714 thing \u203a test-thing.js\n  \u2714 public \u203a js \u203a thing \u203a public/js/tests/thing.js\n  \u2714 test \u203a thing \u203a test/thing.js\n5 tests passed [14:20:44]\n```\nComparing it to the project tree:\n```\n~/Desktop/ava-bug-test-filename-conventions (master)\n\ud83d\udc7e  $ tree -I node_modules\n.\n\u251c\u2500\u2500 package.json\n\u251c\u2500\u2500 public\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 js\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tests\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 thing.js\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 test-thing.js\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 thing.test.js\n\u251c\u2500\u2500 test\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 thing.js\n\u251c\u2500\u2500 test-thing.js\n\u2514\u2500\u2500 test.js\n4 directories, 7 files\n```\nYou can see that public/test-thing.js is not present in the output. I'm guessing that was never supported and is not actually the expectation after all...\n. I'll try to file a PR to remove those lines from the CLI and see if I can get a test working. Thanks!\n. I hope this could be patch-released soonish because the feature was advertized as part of the most recent release but does not actually work :-/\n. I updated with one test. I think it's pretty solid. Let me know if you want an integration test with the CLI as well.\n. How embarrassing!\n. How embarrassing!\n. Fixed!\n. Fixed!\n. I'm not sure what's wrong with the windows build. It does look my test is causing the issue, but I'm not sure why...\n. I'm not sure what's wrong with the windows build. It does look my test is causing the issue, but I'm not sure why...\n. Wahoo! Thanks for working on this. I'll give it a look in an hour or two and report back.\n. Mine was interesting... So I just remembered that one of my tests has dynamic requires (so we instrument all our code for coverage), so I noticed the output indicating that which was cool. The tests went from 41 seconds to 19 seconds which is awesome. But I ended up with 5 test failures due to \"Unexpected reserved word\" and another test failure that's kind of interesting:\n6. Uncaught Exception\n   TypeError: Expected `fromDir` and `moduleId` to be a string\n    at module.exports (/Users/kdodds/Developer/paypal/p2pnodeweb/node_modules/resolve-from/index.js:7:9)\n    at /Users/kdodds/Developer/paypal/p2pnodeweb/node_modules/ava/lib/precompiler.js:68:11\n    at Array.map (native)\n    at Precompiler.normalizeDependencies (/Users/kdodds/Developer/paypal/p2pnodeweb/node_modules/ava/lib/precompiler.js:67:4)\n    at Precompiler.createHash (/Users/kdodds/Developer/paypal/p2pnodeweb/node_modules/ava/lib/precompiler.js:86:26)\n    at Api._runFile (/Users/kdodds/Developer/paypal/p2pnodeweb/node_modules/ava/api.js:56:39)\n    at Promise.map.concurrency (/Users/kdodds/Developer/paypal/p2pnodeweb/node_modules/ava/api.js:283:34)\n    at tryCatcher (/Users/kdodds/Developer/paypal/p2pnodeweb/node_modules/ava/node_modules/bluebird/js/release/util.js:16:23)\n...\nI did clear the cache (note, with no cache it actually takes 26 seconds, but subsequent runs are 19ish seconds). I do have the concurrency option enabled.\nSo far these results are promising though!\n. Sorry, not mine\n. I think that I've found why some of my tests are broken. This apparently doesn't transpile files that are required with proxyquire. This totally makes sense. Do you have any thoughts about what I could do to get around this? Or do you suppose this could be taken into account somehow?\n. Sounds good to me\n. Looks awesome!\n. LGTM. Thanks for working on this @jfmengels!\n. Great work @jfmengels! Thanks for working on that!\n. I was under the impression that there is soon to be a release that allows you to specify a .babelrc to use or some way to configure babel yourself. Is this not true?\n. In the interest of future-proofing this documentation, a reference to the relevant issue might be helpful here. Can't seem to find it right now though.\n. Found it\n. In addition, I would make the check be typeof title !== 'string' :+1:\n. lol\n. Is this default configuration defined anywhere else in the docs? If not, it should be. Then this should link to that.\n. Just noticed that even in this very PR it is evident that I confused .same() with .is() :-)\n. I expect we'll want something a little more robust :-)\n. Definitely. Would you like a more robust deprecation abstraction, or is this simple function and console.warn sufficient?\n. Turns out that when using util.deprecate, it actually logs to console.log which is fine, but I can't seem to get this test to work because when I reassign console.log it appears that my reassignment is never called. I'm not sure what's going on here... Any tips appreciated.\nAlternatively, we can trust that util.deprecate works and not worry about writing tests for this case... \u00af_(\u30c4)_/\u00af\n. > Or just trust that util.deprecate works.\nIf everyone's ok with that, I think that's what I wanna do :-)\n. Moved this down to the other deprecated APIs\n. This bit made things tricky for me in test/test.js which asserts t.is(result.result.assertCount, 1); and there was no real way for me to know that this is what causes assertCount to increment. Still not sure I follow but this fixed it so \u00af_(\u30c4)/\u00af\n. Ah, good to know. Will update.\n. I just left the original there and commented that they're deprecated. \u00af_(\u30c4)/\u00af\n. Done\n. I was going to go through and update some of these to a more sensible test, but decided to opt for a direct refactor. I'll go through and update these to be better :+1:\n. Updated. Thanks @jfmengels and @forresst    \n. Oh, is my glob wrong? Should it be **/__tests__/**? That's what I did for my PR to nyc. Forgive my ignorance with glob :sweat_smile: \n. Updated. :+1: I've subscribed to #713. As soon as that gets merged, I'll give this another look.\n. :+1:\n. updated\n. Good point. Updated :+1:\n. > with one of the most popular React testing library\nShould be:\n\nwith one of the most popular React testing libraries\n. Does this need to be highlighted as js?\n. seeing should be setting\n. I can do that. Unfortunately I've run out of time. Someone can feel free to branch off of my branch and make the improvements in another PR if it needs to be fixed sooner than I can get to it.\n. :+1:\n. Thanks. It's family time for me now. Sorry I didn't quite get to it.\n. I'd rather give this undefined a variable name (like var disablePowerAssert = undefined) so people looking at this later know what it's for.\n. Similarly here: var disablePowerAssert = true\n. agreed. So on the CLI it'd be:\nava --power-assert=false\n\nOr\n\nava --power-assert false\n\n?\n. ",
    "azhang": "This is what I use in my package.json file:\n\"scripts\": {\n    \"test\": \"ava --require babel-register --require babel-polyfill '**/*.test.js'\"\n  },\n  \"dependencies\": {\n    \"babel-core\": \"^6.4.0\",\n    \"babel-polyfill\": \"^6.3.14\",\n    \"babel-preset-es2015\": \"^6.3.13\",\n    \"babel-preset-stage-0\": \"^6.3.13\",\n    \"babel-register\": \"^6.3.13\"\n  },\n  \"devDependencies\": {\n    \"ava\": \"^0.9.2\"\n  },\n  \"babel\": {\n    \"presets\": [\n      \"es2015\",\n      \"stage-0\"\n    ],\n  },\n. I think skip may be broken. I have this:\n``` js\ntest.skip('validator (password): block hashedPassword and salt', async t => {\n  let credential = new Credential({\n    provider: 'local',\n    hashedPassword: 'asdf',\n    salt: 'asdf'\n  })\nlet error = credential.validateSync()\n  console.log(error)\n})\n```\nbut in tap output:\n```\nTAP version 13\n#authenticate\nok 1 - #authenticate\n#toSafeJSON\nok 2 - #toSafeJSON\nvalidator (email): valid\nok 3 - validator (email): valid\nvalidator (email): invalid\nok 4 - validator (email): invalid\nvalidator (email): non-local but email provided\nok 5 - validator (email): non-local but email provided\nvalidator (password): non-local but password provided\nok 6 - validator (password): non-local but password provided\nvalidator (password): minlength\nok 7 - validator (password): minlength\nvalidator (password): no password provided\nok 8 - validator (password): no password provided\nvalidator (password): non-string password provided\nok 9 - validator (password): non-string password provided\nvalidator (password): '' password provided, old credentials\nok 10 - validator (password): '' password provided, old credentials\nvalidator (password): '' password provided\nok 11 - validator (password): '' password provided\nvalidator (password): block hashedPassword and salt\nok 12 - validator (password): block hashedPassword and salt\n1..11\ntests 11\npass 11\nskip undefined\nfail 0\n```\n. ",
    "z-vr": "@Whoaa512 dude you rule . ",
    "ArtemGovorov": "Wallaby.js core developer here, here's our 2c of experience with it:\nAs it has been mentioned a few times in the discussion, there's no theoretically waterproof solution. We use runtime analysis powered by instrumentation collected data plus a bazillion of heuristic rules of various nature, plus framework specific hacks, plus additional data received directly from a specific code editor that we integrate with. So while a simple dependency/call tree analysis covers like 70% of cases, the rest 29.9999% is really painful to cover.\n. npm\n. Ok, thanks for the update!\n. Here are a couple events from Runner I'm interested in:\n- an event before a test starts, passing the Test instance.\n- an event when a test completes, passing the Test instance.\nI know there's the test event raised from Runner, it somewhat addresses the second even request, but it would also be great to have a hook property on the Test instance to be able to distinguish between tests and hooks in those events.\n. It's a pretty common need to set up a context, use it for a few tests, then have a nested describe to tune a few more things in the context and run a few more tests with the modified context.\nFor example, if one has a Project entity that needs to be created, then configured, then started, etc.:\n``` JavaScript\ndescribe('Project', () => {\n  beforeEach(() => { this.project = new Project() });\n  // a few tests verifying stuff about created project\n  it('...', () => ...);\ndescribe('when configured', () => {\n    beforeEach(() => this.project.configure());\n      // a few tests verifying stuff about created and configured project\n      it('...', () => ...);\n  describe('when started', () => {\n    beforeEach(() => this.project.start());\n    // a few tests verifying stuff about created, configured, and started project\n  });\n\n});\n});\n```\nIn a real code each step may contain much more to set up the context. While it's possible to express the same in a flat structure, it may require more code to do so and/or to introduce creating functions that otherwise could be avoided.\nAnyway, whatever you end up doing or not doing, please consider not doing it the way it's done in tape.\nThe way it's done there (nested tests) makes it impossible to run a nested test without running an outer one. This is a limitation that may affect lots of future design decisions and integration options. \nFor example, if you ever decide to introduce a filter/grep for tests within a single file, you'd better have a conceptual entity for a group/suite of tests that is different from a test, otherwise it's not going to be possible.\n. @jamestalmage To me the approach you've mentioned in mocha context looks cleaner. \n\nIn my mocha example above, any bluebird specific test would need to be declared outside the testWith function, and the beforeEach setup would need to be duplicated.\n\nDoesn't have to be outside if your opts contain something to distinguish contexts:\n``` JavaScript\nfunction testWith(opts) {\n  describe('with opts:  ' + JSON.stringify(opts), function () {\n    it(....);\n// bluebird specific test\nopts.bluebird && it(....);\n\n});\n}\ntestWith(optsA);\ntestWith(optsB);\n```\n. The second one can be expressed a bit shorter:\n``` JavaScript\n[\n  { name: 'bluebird', bluebird: true }, \n  { name: 'pinkie', pinkie: true }, \n  { name: 'q', q: true }\n].forEach(opts => {\n    test.group(opts.name, test => {\n        test.beforeEach(t => { \n            t.context.Promise = require(opts.name);\n            // common setup\n        });\n    test('foo', async t => {\n        t.ok(await itSupportsFoo(t.context.Promise));\n    });\n\n    opts.bluebird && test('blue foo', t => {\n        t.ok(blueFoo(t.context.Promise));\n    });\n\n    (opts.bluebird || opts.q) && test('no pinkie', t => {\n        t.ok(notPinkie(t.context.Promise)); \n    });\n});\n\n});\n```\nAnd still looks simpler to me because:\n- group doesn't expose any API, but just serves as a simple container. So less things to learn.\n- context (set up in beforeEach) is easier discoverable, I just have to walk up the hierarchy. I'm worried that for a test like bluebird('blue foo', t => t.ok(...)); it may be harder to trace where the context is set. mocha-like context hierarchy may not look as sexy and dynamic, but it imposes the easily discoverable and traceable structure.\n\nlarger test suite where you want to use this setup across multiple test files\n\nThis doesn't sounds like a scenario I would often encounter. I'd prefer to have some smaller and simpler test suites with self-contained beforeEach hooks rather than spreading the setup logic over multiple files.\n. > I have seen plenty mocha test suites where it can be really confusing which nested group you are in\nDiscoverability is not just readability. With the example you've provided, tools can easily help. Your editor code folding, or jumping to/previewing the parent function would help. Even a simple static analysis tool (like a simple babel plugin), without executing your code, can build you a nice tree showing how things are nested. Combining things dynamically on the other hand makes it from hard to impossible to view the structure before executing the code. \nI think your idea is certainly more flexible in terms of allowing to dynamically combine context than any imposed structure. It's not that it's easier to abuse, because anything can be abused. \nIt's just that when I open a test suite written in mocha/jasmine, I know what to expect there and where because of the static structure that everyone follows. With the dynamic context setup, I can easily see many different people doing it very differently creating a learning barrier for new contributors.\n. > how it would be much harder to write a Babel plugin to analyze my proposed structure \nThe issue is that you can't enforce the \"best practice\" structure because you're suggesting to expose an API, potential usages of which are limiting static analysis options. For example, please correct me if I'm wrong, but with your suggested API one can do:\n``` JavaScript\nexport const bluebird = test.group('bluebird');\nexport const pinkie = test.group('pinkie');\nexport const q = test.group('q');\nlet someGroups = [];\nsomeLogic && someGroups.push(q);\nexport const some = test.group(...someGroups);\n// Try creating a babel plugin that would tell groups of the test:\nsome.test(...);\n```\n\nit also wouldn't be hard to expose the structure by emitting an event prior to firing off tests (if some of the concerns you are raising are wallabyjs related).\n\nMy concerns are not wallaby related (it's actually structure agnostic and doesn't rely on static analysis of test structure for its essential functionality). Just thoughts from a user prospective rather that from an extender point of view.\n. @jamestalmage Mocha doesn't expose a way to combine groups via API, but you are suggesting to. You can perform \"hacks\" to abuse mocha structure, but in the case of your suggested API it is not in hack - test.group(...someGroups);, it's a normal use. And the normal use makes it impossible to run static analysis and determine your tests structure.\n\nThen why are you bringing up the Babel plugin? Is that something someone would use IRL?\n\nI have used Babel as an example of a tool that does static analysis. But generally yes, there are tools that use static analysis to highlight mocha/jasmine tests, run them by full name (that they can determine via static analysis in most of cases). Real life example is WebStorm IDE.\n. @jamestalmage here is the real life example:\n\nI think I have seen similar plugins in other editors as well.\n. > fear they may be a problem for parallelism\n\ncontext sharing\n\nIf test groups define tests and hooks (and not execute them immediately), then it should not affect parallelism at all and there's no context sharing. \nLet's take my example:\n``` javascript\ndescribe('Project', () => {\n  beforeEach(() => { fixture.project = new Project() });\n  // a few tests verifying stuff about created project\n  it('1', () => ...);\ndescribe('when configured', () => {\n      beforeEach(() => fixture.project.configure());\n      // a few tests verifying stuff about created and configured project\n      it('2', () => ...);\n  describe('when started', () => {\n    beforeEach(() => fixture.project.start());\n    // a few tests verifying stuff about created, configured, and started project\n    it('3', () => ...);\n  });\n\n});\n});\n```\nAll of these tests can be executed in parallel. It's just before the test 1 (and each tests for this level) only\njavascript\nfixture.project = new Project()\nwill be executed. For the test 2 (and each test for this level):\njavascript\nfixture.project = new Project()\nfixture.project.configure()\nwill be executed, etc.\nPut simple, test groups is just a convenient way to organise tests and context creation. And I don't think it really matters what kind of code you are testing: functional, procedural or object oriented. When testing a function, no matter pure or not, you will need to create and pass some state. Sometimes you may want to re-use the state creation code and test groups allow you to do it in a less verbose manner.\n. @spudly \n\nWhat problem is solved by adding nested groups?\n\nFor example this one https://github.com/sindresorhus/ava/issues/222#issuecomment-157299689.\n\nFor me, the problem is isolating setup/teardown hooks to specific tests.\n\nIf that's the only problem that you'd like to address, then why bothering adding any API at all?\nHow what you are suggesting:\n``` javascript\nconst setup = t => {\n  ...\n};\nconst teardown = t => {\n  ...\n};\ntest(t => {\n  t.is(getCwd(), '/fake/path');\n})\n.before(setup)\n.after(teardown);\n```\nis better than simply doing this?:\n``` javascript\nconst setup = t => {\n  ...\n};\nconst teardown = t => {\n  ...\n};\ntest(t => {\n  setup(t);\n  t.is(getCwd(), '/fake/path');\n  teardown(t);\n});\n``\n. @spudly Fair enough, missed the case.\n. @novemberborn Wallaby doesn't use AVA process. We create an instance of theava/lib/runnerclass and use it to run tests in our own process(es). To get the test result we patchaddTestResultand get theresult.assertError`.. @novemberborn Great, thanks. Made the requested changes and left the comment (new tests should also help not to remove the property accidentally).. @novemberborn Fair enough, thanks for your help! Yep, we are aware that we are using AVA's internal APIs and things get broken sometimes, but thankfully our AVA users report them fairly quickly. Made the requested change.. @novemberborn I like the idea of the abstracted \"test runner pool\". It might make it easier for wallaby (as well as others) to integrate with AVA. I think it is generally a good thing to separate the testing framework part from the test runner part, makes it easier to reuse/hook into/integrate with those parts.. ",
    "mrmlnc": "@vdemedes Yeap, it works! :+1: \n. ",
    "Ivan-Feofanov": "I have the same error when use gulp-ava package\n. 0.2.0\n. I use ava 0.3.0\n. ",
    "kittens": "Would be happy to have this in Babel core. Everything is fairly modularised internally so the only files you should need to touch are here and here.\n. ",
    "TrySound": "@sindresorhus I just don't know what is the problem :))\njs\n  \"devDependencies\": {\n    \"ava\": \"^0.3.0\"\n  },\nand global the same\n. @sindresorhus I use npm3. Maybe babel5 doesn't work on it?\n. @sindresorhus I use npm3. Maybe babel5 doesn't work on it?\n. @uiureo Seems like it works. Great!  @sindresorhus Need to make hot patch. When you can release?\n. Okay. Thanks.\n. @sindresorhus Thanks\n. @sindresorhus It's weird caz it worked yesterday\n. @vdemedes \nD:\\_station\\nvm\\v0.12.7\\node_modules\\ava\\cli.js:70\n. @vdemedes Yes ) My pr fixes the problem. But something wrong in node5\n. @sindresorhus Thanks )\n. @sindresorhus Oh cool about chdir. I'm just afraid a little new syntax :)) But will try. Still babel is out of the box :)\n. @jamestalmage pinkie-promise\n. Thanks :) We willl wait for your fix\n. @jamestalmage Yep, works fine. Thank you\n. @sindresorhus Sorry, forgot about description \nhttps://github.com/floatdrop/pinkie/pull/12#issuecomment-157350268\n. @jamestalmage I tried local. But I can't install node 0.10 on 64-bit with nvm. On node 0.12 test is passed. Trying here\n. @jamestalmage Oh, it just didn't let use short version 0.10 with weird notice\n. @jamestalmage Still passed. Okay, will try to remove chucks of cosmiconfig\n. @jamestalmage Something wrong with test files order in log :))\n. @jamestalmage Maybe in appveyor?\n. https://github.com/TrySound/rollup-plugin-string/blob/master/test/index.js\nI remember about chdir. Will remove\n. @sindresorhus Yeah, I think npm test runs ava from local node_modules. But as I said $ ava command doesn't work.\n. D:\\Workflow\\github\\rollup-plugin-string>ava --version\n0.5.0\n. @jamestalmage Why ava-cli is not permanent solution? By the way local ava runs without errors\n. @jamestalmage Yep, works :) \n. But how to debug?\n. Related https://github.com/power-assert-js/power-assert/issues/31\n. ",
    "forabi": "I have the exact same error on Linux, so apparently it does not have anything to do with Windows.\nHere is my devDependencies:\njson\n{\n    \"ava\": \"^0.3.0\",\n    \"babel\": \"^5.8.23\",\n    \"babel-runtime\": \"^5.8.25\"\n}\nMy .babelrc\njson\n{\n  \"stage\": 1,\n  \"optional\": [\"es7.classProperties\", \"es7.comprehensions\"]\n}\n. I have the exact same error on Linux, so apparently it does not have anything to do with Windows.\nHere is my devDependencies:\njson\n{\n    \"ava\": \"^0.3.0\",\n    \"babel\": \"^5.8.23\",\n    \"babel-runtime\": \"^5.8.25\"\n}\nMy .babelrc\njson\n{\n  \"stage\": 1,\n  \"optional\": [\"es7.classProperties\", \"es7.comprehensions\"]\n}\n. @TrySound Well, I thought that was the cause, so I switched to node 0.12 and npm 2, but the issue is still present. So it is not that either.\n. ",
    "Lalem001": "Hello @sindresorhus, I have a case you may wish to consider.\nI am looking to use AVA to test a node ^4.2.0 app that uses the built in ES6 features without babel.\nIs this a case you would be willing to support?\n. Trying \"ava\": { \"babel\": {} } gave me the following error (shortened for clarity)\n``` shell\nimport _Promise from 'project/node_modules/ava/node_modules/babel-runtime/core-js/promise.js';\n^^^^^^\nSyntaxError: Unexpected reserved word\n```\nThis is on Node@4.4.2, npm@2.15.1, and ava@0.14.0.\nLet me know if you would like the full log.\n. Trying \"ava\": { \"babel\": {} } gave me the following error (shortened for clarity)\n``` shell\nimport _Promise from 'project/node_modules/ava/node_modules/babel-runtime/core-js/promise.js';\n^^^^^^\nSyntaxError: Unexpected reserved word\n```\nThis is on Node@4.4.2, npm@2.15.1, and ava@0.14.0.\nLet me know if you would like the full log.\n. @jamestalmage That fixed it. However the suggestion requires adding a new devDependency on a project that otherwise has no need for babel.\nIt would be nice if we could use that plugin directly from ava's own dependency on babel-preset-es2015 which in turn depends on the transform plugin. May be possible with npm@3.\nNot a call to action, just sharing thoughts.\n. @jamestalmage That fixed it. However the suggestion requires adding a new devDependency on a project that otherwise has no need for babel.\nIt would be nice if we could use that plugin directly from ava's own dependency on babel-preset-es2015 which in turn depends on the transform plugin. May be possible with npm@3.\nNot a call to action, just sharing thoughts.\n. For me, at the time I originally posted in this thread, the \"no babel\" idea was purely in the interest of reducing the time it takes to test.\n. For me, at the time I originally posted in this thread, the \"no babel\" idea was purely in the interest of reducing the time it takes to test.\n. ",
    "callumlocke": "So what's the correct way to completely disable Babel compilation (for when I know my test scripts only use syntax that is 100% supported by my target Node version)?\n\"ava\": { \"babel\": false } causes a fatal error in ava v0.14 for me.\n. @jamestalmage:\n\nI'm just curious, what motivates the \"no babel\" decision. Missing out on async/await for your tests seems like a really big deal to me.\n\nFor me, it's because I put all my app code and tests through a single compilation step before I run my tests. So I don't miss out on async/await, or anything else from ESNext. I just put everything (both lib and test) into a root folder called src, and continuously compile this whole folder to dist. Separately, I run AVA against dist/test. Here's an example using Babel and AVA separately in this way.\nPersonally I don't want my test runner / assertion library (AVA) to concern itself with on-the-fly compilation; it's too many things in one tool. I don't think Babel is really stable enough to use transparently like that. Babel is a great tool, but imo it's better done as a distinct build step, not magically at runtime/testtime. This way, all your compiled code exists as real files on disk (not just fleeting ideas in RAM) that can run directly in your target runtime, and which you can debug directly without relying on sourcemaps (also flakey). I've had way fewer problems since switching to this approach.\n. > \"babel\": false\n:+1:\nIs there any workaround technique that would have the same effect, until this ships? Would \"babel\": {} work?\n. > inherit allows you to customize with whatever you want.\nDoes that mean it adds to the default config, or replaces it?\n. ",
    "dananichev": "@sindresorhus at least as a module loader. I mean, AVA is obviously supports ES2015 syntax, but JSPM includes SystemJS loader. Will AVA resolve modules correctly?\n. @sindresorhus at least as a module loader. I mean, AVA is obviously supports ES2015 syntax, but JSPM includes SystemJS loader. Will AVA resolve modules correctly?\n. @sindresorhus \nI suppose it's not that easy.\nThe problem is: you need to load somehow SystemJS config (generated by JSPM) before running the tests.\nExample of the project\nhttps://github.com/dananichev/ava-jspm-example\nnpm test\nresults in\n```\n/src/helpers.js:1\n(function (exports, require, module, __filename, __dirname) { import * as utils from 'underscore.string';\n                                                              ^^^^^^\nSyntaxError: Unexpected token import\n```\nAm i missing something?\n. @sindresorhus \nI suppose it's not that easy.\nThe problem is: you need to load somehow SystemJS config (generated by JSPM) before running the tests.\nExample of the project\nhttps://github.com/dananichev/ava-jspm-example\nnpm test\nresults in\n```\n/src/helpers.js:1\n(function (exports, require, module, __filename, __dirname) { import * as utils from 'underscore.string';\n                                                              ^^^^^^\nSyntaxError: Unexpected token import\n```\nAm i missing something?\n. @eirikb Thanks. But i think i will stick with Jasmine/Mocha/etc for now. Anyhow AVA seems pretty intriguing and i'm planning to wait for a full JSPM support and so on.\n. @jamestalmage Does it work with basic setup of nyc?\nE.g.:\n\"nyc\": {\n    \"lines\": 100,\n    \"statements\": 100,\n    \"functions\": 100,\n    \"branches\": 100,\n    \"reporter\": [\n      \"lcov\",\n      \"text\",\n      \"html\"\n    ],\n    \"include\": [\n      \"src/**/*.js\"\n    ]\n  }\nAnd w/o babel-plugin-istanbul or babel-plugin-__coverage__.\nBecause with babel-require nyc produces coverage report but not with --precompile\n. @jamestalmage nvm. Decided to remove babel-plugin-rewire and switch back to babel-plugin-istanbul (due to incompatibility between them). Works fine with --precompile.\n. Yeah, #945 is deal-breaker. Right now babel-require consumes a lot of resources. I believe it would drastically improve performance once shipped. Concurrency usually is not that bad (comparing to #945)\n. @sindresorhus sorry for my Perfect English skills (c) :)\n. @sindresorhus it seems the problem is somewhat related to https://github.com/sindresorhus/execa/issues/35. At least simptoms are similar (maybe it's just my imagination though):\n[Error: spawn undefined ENOENT]\n  errno: 'ENOENT',\n  code: 'ENOENT',\n  syscall: 'spawn undefined',\n...\nI'm trying to locate the problem behind this. But my VMs are pretty much slow (dunno why though). I will continue to investigate this further from home (with native Windows). \n. @sindresorhus i hope you are fine with usage of err.stderr in test/profile.js?\n. @sindresorhus the only thing i know about MiniReporter.clear() that it used in Logger.clear() which used in Watcher to separate different test runs. Changed test title to returns empty string (used in watcher in order to separate different test runs) but i'm not sure about it. Am i missing something?\n. It is some kind of bad habit. I mean using g in most of regex-expressions.\n. How about node_modules/ava/cli.js:27:11?\n. I mean due to \n/\\/ava\\/(?:lib\\/)?[\\w-]+\\.js:\\d+:\\d+\\)?$/\nand \n/\\/node_modules\\/(?:bluebird|empower-core|(?:ava\\/node_modules\\/)?(?:babel-runtime|core-js))\\//\nit should be either \n/node_modules/ava/node_modules/babel-runtime/\nor \n/ava/cli.js:27:11\nor \nnode_modules/ava/cli.js:27:11 (because it will work with first regex)\n. Okay \n. One of this CMD+Z's was too much i think.\n. ",
    "eirikb": "Hi @dananichev \nYour import..issue can be solved this way:\n1. Update AVA to latest version.\n2. Add \"ava\": { \"require\": [ \"babel-core/register\" ] to your package.json-file.\n3. Replace your .babel.rc-file content with {\"presets\": [\"es2015\"]}.\nYou will still have problems with modules not loading from jspm_packages.\nI don't know how to solve this properly, one super-ugly-don't-do-this-hack is to override require to check jspm_packages-folder first (since the layout is a bit different other resolvers might not handle it).\nHere is a working example (with the horrible hack): https://gist.github.com/eirikb/d8d9a20673166c02bf62\nNote that jspm is not actually used when testing.\n. ",
    "salfield": "This code below works fine for me. Slightly annoying boilerplate to add to each test file, but works.\n```\nimport test from 'ava';\nimport jspm from 'jspm';\njspm.setPackagePath('../../../');\nlet moduleName;\ntest.before(t => {\n  return jspm.import('path/to/module')\n    .then((module) => {\n      moduleName = module;\n      return\n    })\n});\ntest(t => {\n  console.log(moduleName);\n  / run a test here /\n});\n```\n. ",
    "fluffywaffles": "+1 on this.\nWe're having the issue that babel doesn't know it is supposed to look in jspm_packages for module resolution instead of looking in node_modules.\n@sindresorhus, I don't see any way of configuring babel to handle this directly. The way JSPM and Babel work together is via a JSPM plugin, but that doesn't appear to be the right way forward here, seeing as JSPM is not really meant to run tests. Like @dananichev said, the SystemJS config - which includes in it a mapping of module names to their source directories - probably needs to play a role.\n. @sindresorhus\nI want to be able to test my modules separately, though! The recipe ideas @dak shared helped a lot.\n@dak @forresst \nDue to the default behavior of Babel 6, where it doesn't do anything by default unless you've correctly configured your .babelrc, I think it is advisable to suggest setting up a .babelrc as part of the recipe. The JSPM babel plugin works around this by configuring Babel programmatically as you do here, but I find that a bit obtuse. It's too easy to let your test and build configurations get out of sync if you don't use a .babelrc. Better to keep all your Babel config in one place, at least for a default configuration.\n@dak Your JSPM config should already have a mapping for app/ (or whatever you named it during jspm init), so you should be able to do away with the ~ resolution code.\nHere's what I ended up with, which is a little simpler and seems to work better when importing JSPM-managed dependencies. It also looks to JSPM before NPM because I'm using JSPM as my primary package manager:\n``` js\n// Adapted test/helpers/jspm_loader.js\nconst Module = require('module')\n    , jspm   = require('jspm')\n    , path   = require('path')\n    , url    = require('url')\n    , load   = Module._load\n// These were causing me trouble prior to my adapting the original loader, but now this array can probably be deleted?\nconst SpecialSystemModules = [\n  '@empty'\n  , '@system-env'\n  , '@@global-helpers'\n  , '@@cjs-helpers'\n]\njspm.setPackagePath('..') // This seems to want to be the path relative to the test directory?\nconst System = jspm.Loader()\n/ No need for additional configuration with the '~' path. I can import my app using:\n *   import app from 'app/main'\n * which is the same as what I would write for my System.import call in a ",
    "dak": "Combining what @salfield and @eirikb offered, I managed to come up with a loader shim that will read your jspm config and allow you to import modules as normal at the top of your tests.\nFor the below examples, this file resides in ./test/helpers/jspm_loader.js. If you place it somewhere else, you may need to adjust some of the relative paths.\n``` js\n// NOTE: I needed this here as I wasn't getting babel-register to load the\n//       proper presets and plugins from package.json and I'm not using\n//       a .babelrc file. This may be optional for you, or you may want\n//       different presets/plugins.\nrequire('babel-register')({\n    presets: ['es2015'],\n    plugins: [\n        'transform-object-assign'\n    ]\n});\nconst url = require('url');\nconst path = require('path');\nconst Module = require('module');\nconst jspm = require('jspm');\nconst load = Module._load;\njspm.setPackagePath('../../../');\nconst System = jspm.Loader(); // Load your project's JSPM config\n// NOTE: You may need to add paths here that are in your jspm.browser.js\n//       config if you're using JSPM 0.17, as jspm.Loader() currently\n//       appears to only load your jspm.config.js file.\nSystem.config({\n    paths: {\n        '~/*': path.normalize(${__dirname}/../../src/app/*)\n    }\n});\n// NOTE: Still an ugly hack. There may be other solutions like\n//       shimming require? Open to improvements.\nModule._load = (name, m) => {\n    try {\n        return load(name, m);\n    } catch (e) {\n        var normalizedName;\n    if (name.indexOf('.') === 0) {\n        // HACK: Handle relative file paths\n        normalizedName = path.normalize(`${path.dirname(m.filename)}/${name}.js`);\n    } else {\n        normalizedName = url.parse(System.normalizeSync(name)).path;\n    }\n\n    return load(normalizedName, m);\n}\n\n};\n```\nYour package.json should have the following:\njson\n\"ava\": {\n  \"require\": [\n    \"./test/helpers/jspm_loader.js\"\n  ]\n}\nThat allowed me to write the following test:\n``` js\nimport test from 'ava';\nimport {default as analytics, optedOut} from '~/helpers/analytics';\ntest('Tracking Opt-out', assert => {\n    analytics.optOut();\nconst actual = optedOut();\nconst expected = true;\n\nassert.is(actual, expected,\n    'optedOut() should return true after calling analytics.optOut().');\n\n});\n``\n. I just ran into this as well and spent about an hour trying to debug why tests weren't being found until I stumbled upon the little note in the README about files inhelpers` directories being excluded.\nAs @jamestalmage noted, it's preferable to have a test directory structure that mirrors your src directory structure. The problem with the proposed solution is that it's not uncommon to have a helpers directory in a project's directory structure. So ./test/foo/helpers being excluded still precludes properly matching the src directory structure.\nRather than using a name that easily conflicts with common directory structures, why not pick something that will clearly stand out as being an ava helper and also be very unlikely to conflict?\nProposal: Always exclude ava_helpers, and never exclude helpers.\nFor bonus points, it might be nice to just make this whole thing configurable under an excludes option, so people have control over their own naming conventions and directory layouts.\n. I'm a little perplexed.  Earlier in this conversation you stated:\n\nEarly on, we included every file in ./test/, and /tests/**. The former pattern allows you to create a test directory that mirrors your src directory (this is what most of the maintainers prefer)\nThat seems to me to suggest that the ./test/** pattern is the most common.\n\nThe helpers directory is also in common usage, as can be seen by Ava's own use of it.\nBut even if the percentage of users that use the ./test/** directory structure and have a helpers directory is relatively small, why would you be content to inconvenience a still not insignificant percentage of your users (clearly there's some, as they've commented in this and other issues) over a legitimate issue when multiple options exist to fix it that do not require significant work or refactoring of ava, and, in the case of allowing excludes to be customized, would not have any impact on current installs?\nIt does appear that there has existed an intent to make excludes customizable as well.\n. I feel like we're talking past each other. Why can't we keep the \"super convenient\" helpers directory default that you prefer but allow others who prefer different defaults to change it?\nI'm fine with tools being opinionated when it has a point, but there doesn't seem to be a benefit from being opinionated about the names of directories to exclude. We're talking about a fairly straightforward option to overwrite the default excludes.\nI'll even happily try to submit the PR myself.\n. This appears to work for normalizing mappings, but not for paths, although the ava-jspm-loader also appears to only focus only on mappings.\nThis may also be because I'm only using JSPM to bundle dependencies and not the entire app.\nI wonder if we may need a better synchronous API from JSPM to properly let it handle loading modules using its configs.\n. I'm only bundling part of it because I'm hosting over HTTP/2 and the app is quite large, so the benefit of reducing the number of requests is near zero due to multiplexing (just the extra headers) compared with being able to return just the relevant templates/scripts for a given page.\nIt's a supported use case by systemjs builder: https://github.com/systemjs/builder#example---third-party-dependency-bundles\nI'm also using JSPM 0.17, which may? be part of the discrepancy, although I suspect it's more than that.\nStripped down, my config is basically the same as above:\njs\npaths: {\n    \"~/\": \"src/app/\"\n}\nbut I'll get errors like: Error: Cannot find module '~/path/to/module', which suggest the path normalization never occurred.\nLet me try to make a simple repo to demo the issue.\n. @skorlir See: https://github.com/dak/jspm-ava-test\nI think it may have to do with matching non-root files... in the above repo I can load ~/test fine but not ~/test/test\n. @skorlir I think that did the trick. :+1:\n. @skorlir @MeoMix I just noticed something similar actually... you need to change the order and prefer NPM modules over JSPM modules. It broke when I tried combining it with nyc for the same reason @MeoMix identified.\nReversing the loading order (with a try/catch) resolved the issue.\n. Rather than trying to duplicate how JSPM is loading modules, it would be nice if JSPM exposed one or two methods to handle this. /me nudges @guybedford\nI'm sure we're missing other edge cases still.\n. @skorlir See: https://github.com/dak/jspm-ava-test again\nYou'll need nyc installed globally, I didn't add it to the package (or npm install it manually). Then run npm test.\nI agree with the sync import issue: when I was first looking into this problem I spent a while trying to find a jspm.loadSync or similar functionality. normalizeSync exists, but it leaves open a whole host of edge cases that need to be handled to duplicate JSPM's functionality, which is why I think the ultimate solution is still for JSPM to provide an API to import synchronously.\n. @skorlir So I think I've found an even better solution by making use of Ava's support for async functions.  We kept trying to get around how JSPM loads modules asynchronously instead of just embracing it.\nThe ava-jspm-loader would simply be:\n``` js\nconst path = require('path');\nconst jspm = require('jspm');\nconst pjsonLoc = require('find-pkg').sync(process.cwd());\nconst pjsonDir = path.parse(pjsonLoc).dir;\njspm.setPackagePath(pjsonDir);\nglobal.System = jspm.Loader();\n```\nNo funky hacks.\nThen, you'd write tests using async functions:\n``` js\nimport test from 'ava';\ntest('Test name', async function (assert) {\n    const module = await System.import('jspm/import/path');\n    const actual = module.returnTrue();\n    const expected = true;\nassert.is(actual, expected, 'returnTrue() should return true.');\n\n});\n```\nThe only issue I've seen with this so far is that nyc doesn't seem to see any of the tests, probably because JSPM is doing its own loading rather than hooking in with Node... @sindresorhus are there any hooks available to us to make AVA aware of the loaded module?\n. @skorlir It does appear to need to run through Babel first... would it work with babel-register if you included the systemjs plugin for babel too? (I updated the repo from above to show it working)\n. Yes, I build them but don't bundle them, so they're still individual files.\nSee: https://github.com/dak/jspm-ava-test\nAfter you run npm test take a look at the dev folder it creates.\n. ",
    "forresst": "In order to improve the documentation, @dak's answer may be a good base for a recipe? What do you think?\n. :+1: \n@sindresorhus Done for french translation  sindresorhus/ava-docs@5ed5937218c193a6920ccd2bbb786f4327f189b2\n. @sindresorhus It would be interesting to add it in the doc (readme.md) ?\n. @sindresorhus :+1:\n. @jamestalmage \n\nIt is probably a good idea to leave this branch accessible with no squashing of commits. It may be easier for translators to follow along commit by commit. We should still only push a single squashed commit to master, but lets leave the history here to help the translators.\n\n:+1: \nIt's a very good idea\n. @novemberborn you\u2019re welcome\n. LGTM\n. LGTM\n. > Also, as a native french speaker, I'll be happy to make the translation to French once this is merged.\n@adriantoine  It is with pleasure that I will review (And feel free to review my translations)\n. Great @adriantoine \n\nAlso, as a native french speaker, I'll be happy to make the translation to French once this is merged.\n\n@adriantoine It is with pleasure that I will review . Ping me !\n. You're faster than me! (forresst/ava@70c3f73d32f3f3dfd057f49070135bb8a21308c8)\nLGTM\n. @adriantoine No soucy ! Good Job ! Thanks :smile: :tada: \n. See Recipe babelrc, section Transpiling Sources :\n\nNote that loading babel-register in every forked process has a non-trivial performance cost. If you have lots of test files, you may want to consider using a build step to transpile your sources before running your tests. This isn't ideal, since it complicates using AVA's watch mode, so we recommend using babel-register until the performance penalty becomes too great. Setting up a precompilation step is out of scope for this document, but we recommend you check out one of the many build systems that support Babel. There is an open issue discussing ways we could make this experience better.\n. LGTM\nThanks @silvenon \n. @sindresorhus Done !\n. I would do that. This will allow me to do anything other than translation.\n. I would like your opinion on the two remaining tasks. Thanks\n. > Change it. Remove moot text. And just link to https://github.com/paulmillr/chokidar#install-troubleshooting for troubleshooting.\n\nOk. I will do !\n\nWhy? Not really needed as ^1.4.2 matches it.\n\nMy question was poorly worded. I wanted to know if we still accept the version 1.4.x or if it was necessarily greater than or equal to 1.5.x. But seeing your answer, we can leave like that! :smile_cat:\n. Done\n. After avajs/ava@c9259200c7ab4d581ec12cfdc6653ca88d22230e Rebase done.\n. _After that, I feel very proud of what I did!_\n\n. @hoschi Check out this https://github.com/avajs/ava/issues/923\n. This is fixed in version 0.1.4 of not-so-shallow.\n@sindresorhus Should we add additional tests for ava?\n. I will do it\n. Done and number 1000 is for me !!!!! :tada: :smile_cat: \n. @ngerritsen npm update in your project\nThis will put the new version of not-so-shallow (0.1.4), used by AVA\n. To summarize the discussion, the solution would be to raise an error (exiting with 1 to allow stop the process in CI) with the following explanatory message :\nAVA will not run with the --watch (-w) option in CI, because CI processes should terminate, and with the --watch option, AVA will never terminate.\nIs that the right solution?\nI can do the PR if you want (Code, test and explanation in docs). Thanks You @sindresorhus ! It is with pleasure that I do this.. @sindresorhus I will look at these tests quickly. Sorry\nThere is a second point to consider: spread for Node 4 (https://travis-ci.org/avajs/ava/jobs/193048228#L231). \nI know spread does not work without the flag under node 4. It works on the avajs/ava-files repository but not under avajs/ava. Is there a tip or parameter under travis for this to work?\nFinally, should we use spread in avajs/ava?. @sindresorhus ok, i will remove spread\n@novemberborn Obviously, how could I miss out on this? I will read the doc in French! \ud83d\ude04 . All tests passing. Sorry for my mistakes.. Meanwhile, @knpwrs has written an article that is very well: http://knpw.rs/blog/testing-vue-in-node . french translation : done (avajs/ava-docs@87cfbdf2d0f19ad335eecfda7114f85ad446b92b). I do not have the solution, but maybe I have an idea of the problem.\nThe error message is normally created at this location.\nThis line must be I think what is problematic.\nTo confirm my idea, can you change 100 by 2000 in your test source and give me the result:\n```js\n'use strict';\nconst test = require('ava');\ntest('should succeed', async () => {\n    await new Promise((_, reject) => {\n        setTimeout(reject, 2000).unref();\n    });\n});\n```. Thank you for your answer. Unfortunately, I thought it would have the same as \"Removing the unref\". So my idea is not the right one. \nIf anyone has another idea? Sorry.\n. Travis CI and AppVeyor do not launch exactly the same scripts.\nTravis CI runs npm run test (default): \"test\": \"xo && flow check test/flow-types && tsc -p test/ts-types && nyc tap --no-cov --timeout=300 --jobs=4 test/*.js test/reporters/*.js\"\nAppVeyor runs npm run test-win:    \"test-win\": \"tap --no-cov --reporter=classic --timeout=300 --jobs=4 test/*.js test/reporters/*.js\"\nThe Travis CI script performs additional tasks (xo, flow and tsc): It is not useful to run them on both as they will do the same job of several syntax checking and good practice.\nI advise you to read how to do a Pull Request: to summarize, on your local repository, you have to do an npm install and after your modifications, run npm test. This will launch the same tests as Travis CI.\nFinally, the errors you see on Travis CI is XO asking you to change the order of 'import'.\nHopefully my answer will help.. @jy95 I note it. I will do the necessary on the French translation when the necessary will have been done on the  AVA repository. Thanks @jy95. . @BusbyActual @novemberborn sorry for the inconvenience. I think this can be handled by ora. The following code should be added at the beginning of the frame() method:\n```diff\n    frame() {\n+       if (!this.stream.isTTY) {\n+           return '';\n+       }\n    const frames = this.spinner.frames;\n    let frame = frames[this.frameIndex];\n\n    if (this.color) {\n        frame = chalk[this.color](frame);\n    }\n\n    this.frameIndex = ++this.frameIndex % frames.length;\n\n    return frame + ' ' + this.text;\n}\n\n```\n@sindresorhus @novemberborn your opinion ?. Note, it is also necessary that the sindresorhus/ora#82 is merged. Replace 'authenticating with valid credentials' by 'authenticating with invalid credentials' ?\n. Replace 'authenticating with valid credentials' by 'authenticating with invalid credentials' ?\n. add two spaces to align\n. console instead of js (is better)\n. Replace js by console\n. OK done\n. Oups ! I'm unmasked, I test under windows!. Yep Sir !. Proposal for a shorter version:\n' AVA not run if using watch mode in CI, as CI processes should terminate and AVA will never terminate.'\nbefore:\n* 147 characters\nafter:\n* 102 characters\nFor information, CLI error with Tap reporter : 57 characters. @novemberborn  why not ?. i will make this change. This flag should be added to readme.md (like -no-power-assert for example) in CLI paragraph. Please, fix indentation. I have not tested the recipe. But we should instead declare nodeExternals instead of nodeModules? I have a little difficulty understanding the logic. If what you wrote is correct then it might be good to explain why we should do this.\nWhat do you think?. @novemberborn Personally, I find the indication useful. It allows to know that the behavior is the same.. OK. I agree with you. ",
    "lijunle": "+1 on this feature.\nI am considering to migrate from mocha to AVA. However, keep easy test failure debug experience is one of my concerns - test.only is the key to do so.\n. @sindresorhus That is OK, please keep in mind that, there is a hack. I am not going to merge until resolve the hack with a good way (unless a direction).\n. > One question I have in mind, should only tests be executed serially on in parallel?\nMy question is no. If the guy wants to do so, he should do ava.serial.only. However such API is not implemented in this RP.\n\nBtw, mocha allows only one only test, so maybe we should also do that?\n\nMocha allow several only modifiers. For example, if there are 10 test cases, and 3 of them are marked as only, those 3 test cases will be run.\n. OK, do you want to modify the comment about the hack?\n. Rebased, let us wait for the CI. :smile_cat: \n\nHowever, although I removed the comment, I still consider that is a hack.\nThe stats should be collected from _runTestWithHooks function and pass back to the promise chain in the Runner.prototype.run function. At the end of the promise chain, calculate the final stats from the input.\nCode like this:\njs\nRunner.prototype.run = function () {\n    var self = this;\n    return eachSeries(tests.before, this._runTest.bind(this)) // resolve with { pass: 1, failed: 2, skipped: 3 }\n        .then(function (stats) {\n            return Object.assign(stats, self.serial(tests.serial)); // i.e. { pass: 2, failed: 3, skipped: 4 }\n        })\n        .then(function (stats) {\n            return Object.assign(stats, self.concurrent(tests.concurrent)); // object with { pass, failed, skipped }\n        })\n        .then(function (stats) {\n            return Object.assign(stats, eachSeries(tests.after, self._runTest.bind(self))); // object with { pass, failed, skipped }\n        })\n        .then(function (stats) {\n            self.stats.passCount = stats.pass; // calculate from the input of promise chain.\n            return stats; // external check could depends on return value of run function, more stable. \n        });\n};\n. I understand. It is up to you to design the pattern, that is just my suggestion.\n. The variable testCount is maintained by the addTest, addOnlyTest, addSerialTest function, which I think that is not suitable. A better way is to return such information (testCount, failCount) from the promise, then calculate the final stats from at the end of the chain.\nHowever, it involves more code change if doing so. Any ideas?\n. ",
    "af": "It's a young project, pre-1.0, so it's not too late :)\nIMO it helps for readability/comprehension to show that the test should be fully synchronous. It's not so much about saving characters (although that's nice), more to display the intent of the code more clearly, without any unnecessary elements.\n. 100%. This is not about saving characters (the initial issue description overemphasized this, I think). It's about signalling intent.\n. Agreed @taylorhakes, that would be the ideal solution from my perspective as well. Obviously a big breaking change though!\n. This looks great!\n. ",
    "taylorhakes": "I definitely prefer test.sync because of intent (with named export), but I feel I will be using test.sync on 90% of my tests. As @sindresorhus said, for me, it would have been really nice if sync was the default. \nAny chance sync by default could happen with test.async for async tests? I know that would be a huge breaking change, but I plan to use this library for a while and it will save so many keystrokes over time.\nIf not, it would be great to be used like this.\n```\nimport { sync } from 'ava';\nsync('foo', t => {\n  t.same(['a', 'b'], ['a', 'b']);\n});\n``\n. Doest.end()` exist in the sync version? Might be easier to upgrade, but add confusion later.\nMaybe test.callback might be a better name? I seems like async is still possible in the regular version.\n. Makes sense. I would be a little confused by t.end being a noop. I'd rather have it not exist to make sure I didn't make a mistake.\nI guess we disagree on what is async. I consider this async.\ntest(async () => {\n...\n})\n. ",
    "markthethomas": "@sindresorhus thanks so much for getting back to me! You've given me a great base for starting out. I'll check out some of those issues you mentioned and ask any questions I have here! Thanks, too, for the thorough response \u2014 I'm looking to get more and better involved with OSS and the best way I can do that I think is by learning and contributing to one project well first :+1: \n. Ah, bummer that people never do. But I guess that's the way of all documentation. \nI tend to like them because it gives me a sense of the project and how I might fit in etc., but it seems like that's just me :smile: Totally see your point about there being a mismatch in those who would need to read it and those who do. \n\n\nbut I think it's also important that the basics of git clone, npm test etc. are there\n\nWhy? I don't think every project should cover how to do generic and basic things like that.\n\nWas just thinking with beginners (like myself) in mind :+1: Nothing I care deeply about \u2014 just something I've occasionally appreciated. \n. Ah, bummer that people never do. But I guess that's the way of all documentation. \nI tend to like them because it gives me a sense of the project and how I might fit in etc., but it seems like that's just me :smile: Totally see your point about there being a mismatch in those who would need to read it and those who do. \n\n\nbut I think it's also important that the basics of git clone, npm test etc. are there\n\nWhy? I don't think every project should cover how to do generic and basic things like that.\n\nWas just thinking with beginners (like myself) in mind :+1: Nothing I care deeply about \u2014 just something I've occasionally appreciated. \n. Oo yes, now to learn how to amend my commit :+1: Thank you @sindresorhus!\n. Oo yes, now to learn how to amend my commit :+1: Thank you @sindresorhus!\n. Sounds good! Need to learn though \u2014 I'm a solo dev where I work and don't get to use Git in these ways all that often (usually just me talking to me lol) :)\n. Sounds good! Need to learn though \u2014 I'm a solo dev where I work and don't get to use Git in these ways all that often (usually just me talking to me lol) :)\n. Yes indeed! I think I just rebased and squashed the PR correctly? \n. Yes indeed! I think I just rebased and squashed the PR correctly? \n. wait no \u2014 left the old TOC in there. Fixing!\n. wait no \u2014 left the old TOC in there. Fixing!\n. Oh ok \u2014 thanks for letting me know! I'll abandon this and create a new branch\n. Oh ok \u2014 thanks for letting me know! I'll abandon this and create a new branch\n. :+1: (thanks for your patience while I fumble & learn)\n. :+1: (thanks for your patience while I fumble & learn)\n. should be good to merge unless there's anything else that needs adding :)\n. Argh -- my bad\nMark\nSent from my iPhone\n@markthethomas everywhere\n\nOn Nov 9, 2015, at 11:22 PM, Sindre Sorhus notifications@github.com wrote:\nI've commented about https://github.com/sindresorhus/ava/pull/168/files#r44228402 twice (* \u2192 -), but I can fix it in the merge.\n\u2014\nReply to this email directly or view it on GitHub.\n. Argh -- my bad\n\nMark\nSent from my iPhone\n@markthethomas everywhere\n\nOn Nov 9, 2015, at 11:22 PM, Sindre Sorhus notifications@github.com wrote:\nI've commented about https://github.com/sindresorhus/ava/pull/168/files#r44228402 twice (* \u2192 -), but I can fix it in the merge.\n\u2014\nReply to this email directly or view it on GitHub.\n. @jamestalmage @alexbooker I agree w/ the need for global/local timeouts and for me a faster default would be preferable and, to your point, encourage me to write smaller, faster tests :+1: Maybe it's crazy/overkill, but what about using something like os.cpus() and the speed prop to smart-set the timeout? I know estimating cpu perf is tough, but just a thought I had :) \n. @MarkTiedemann sounds good to me \u2014 just a thought I had :) \n. Sorry, the \"sounds good\" was in reference to using the os module :)\n\nMark\nSent from my iPhone\n@markthethomas everywhere\n\nOn Nov 9, 2015, at 3:42 PM, Vadim Demedes notifications@github.com wrote:\n@markthethomas AVA is supposed to be fast, but that has nothing to do with user's tests. 500 ms is too small imho, I think we'd better choose tap's default timeout - 30 seconds.\n\u2014\nReply to this email directly or view it on GitHub.\n. Not sure if I'm fully understanding the question, but ava is a library more akin to tape/mocha and is built for unit testing, while magellan looks like it's more of an integration suite for browser testing. Is that what you're getting at? If so, these are different libs for different testing types and at least as far as I know there aren't plans to move ava in the integration direction :) happy for anyone else to chime in though, I'm pretty new to watching/helping w/ava\n. @mattfysh pretty much, yeah \u2014 parallelization is a feature, not the feature. I suppose you could use the runner to run other tests, but really then you're creating a modified version of the gorilla-banana problem with no extra inherent benefit :)\n\n\n. I agree w/ @vdemedes; one of the most frustrating things I've encountered is when a plugin is out of sync with the api it's supposed to be helping me with. I usually just have to end up disabling it till it gets updated.\n. I agree w/ @vdemedes; one of the most frustrating things I've encountered is when a plugin is out of sync with the api it's supposed to be helping me with. I usually just have to end up disabling it till it gets updated.\n. Will do! :+1: \n. ",
    "rmg": "I think the docs could use a little more clarifying. A common problem with test plans is forgetting to update them when new assertions are added. In cases where the assertions happen asynchronously (the primary use case for test plans), they are not seen even if they fail. The suite will continue to pass, even though failing tests have been added. \nWhile this is less of a problem if one is disciplined about verifying their failing tests before fixing them, it would be nice if the tools helped. And if the tools can't help, then at least the readme should offer a warning about this scenario.\nI suspect that if this was \"fixed\" in the code there would be a number of bug reports claiming AVA suddenly broke but really it just stopped hiding broken tests. This is a problem we have had to address in node-tap and it \"broke\" a lot of test suites when we did.\n. I'm planning on submitting a docs PR if I can remember to when I'm on a real computer. Meant to mention that in my comment above. \n. I'm not sure I see the usefulness of t.end(4).\nI think the root of the conflict is that using t.plan(4) for auto-end and using t.plan(4) to assert than only 4 assertions were made are two someone contradictory goals. I'm not sure how to do both well at the same time.\n. ",
    "boneskull": "Thanks.  I don't expect everyone to be in love with Mocha; it has some fundamental flaws that need addressing.  If we end up stealing some of your ideas, please consider it flattery. :smile:\n. great\u2014was just going off of latest published version. . ",
    "hzoo": "Haven't implemented it yet but I would like to see at least a preset where you can specify that environment you want to target and it would have a predetermined list of transforms based on what it supports. You could just use the data or use env to do different things. https://github.com/babel/babel/pull/3476\n. Looking..\nAlso a simpler snippet from what you had that fails\n``` js\nimport test from \"ava\";\ntest(\"a\", t => {\n  t.is(\n    {\n      // for testing\n    },\n    1\n  );\n});\n```\nIf the comment is removed that error is gone. \nIt's possibly from https://github.com/babel/babel/pull/3283, @gzzhanghao? It's hard to say. I mean it does say it's calling _findToken but this.tokens is []\n. And then fixing that original issue by reverting that PR,\nor adding something like if (!tokens.length) return; in https://github.com/babel/babel/blob/master/packages/babel-generator/src/whitespace.js#L18,\nit only leads me to find an acorn error?\n``` bash\nhzhu/dev/test/node_modules/power-assert-formatter/node_modules/acorn/dist/acorn.js:945\n  throw err;\n  ^\nSyntaxError: Unexpected token (1:17)\n    at Parser.pp.raise (/Users/hzhu/dev/test/node_modules/power-assert-formatter/node_modules/acorn/dist/acorn.js:943:13)\n    at Parser.pp.unexpected (/Users/hzhu/dev/test/node_modules/power-assert-formatter/node_modules/acorn/dist/acorn.js:1503:8)\n    at Parser.pp.parseIdent (/Users/hzhu/dev/test/node_modules/power-assert-formatter/node_modules/acorn/dist/acorn.js:670:10)\n    at Parser.parseIdent (/Users/hzhu/dev/test/node_modules/acorn-es7-plugin/acorn-es7-plugin.js:93:19)\n    at Parser.pp.parsePropertyName (/Users/hzhu/dev/test/node_modules/power-assert-formatter/node_modules/acorn/dist/acorn.js:554:127)\n```\nNot sure about this\n. Merged https://github.com/babel/babel/pull/3438 and released in 6.8.0\n. No, I haven't - we had some discussions on how we could reliably do that but I'm not sure if we came up with anything since running a benchmark on travis didn't seem to be a good idea given the variance. I think it \"felt\" faster to me too lol\n. ",
    "sotojuan": "I said this on Gitter but putting it out here for further discussion:\nIn the async function support section, I think a short explanation of what async/await functions could be helpful for newbies. We could add something about how they make asynchronous code look synchronous. Perhaps add one more \"real world\" example to the examples there already? I just want people to find out about async/await and not just gloss over the section!\nAlso willing to help with commenting functions for documentationjs.\n. I said this on Gitter but putting it out here for further discussion:\nIn the async function support section, I think a short explanation of what async/await functions could be helpful for newbies. We could add something about how they make asynchronous code look synchronous. Perhaps add one more \"real world\" example to the examples there already? I just want people to find out about async/await and not just gloss over the section!\nAlso willing to help with commenting functions for documentationjs.\n. The main thing I can think of is lots of examples for different kinds of tests. Simple assertions, testing promises/async, testing APIs, etc. Not too many (overwhelming users is bad), but examples help newbies a lot. They don't have to be interactive, just the code and what the output would be.\n. The main thing I can think of is lots of examples for different kinds of tests. Simple assertions, testing promises/async, testing APIs, etc. Not too many (overwhelming users is bad), but examples help newbies a lot. They don't have to be interactive, just the code and what the output would be.\n. That reminds of Elixir test tags so I'm all for it. Looks clean.\n. I'll submit my PR later for this, but I \"fixed\" it doing two things:\n- Add !node_modules to the array here.\n- Add:\njavascript\n    files = files.filter(function (file) {\n        return (file.indexOf('node_modules') === -1);\n    });\nBelow the if statement in the same place as above. This makes sure all paths that have node_modules in them are gone in case the user did something like ava **.\nI am not sure if this is the most efficient way to do it, but it works.\n. Wouldn't that only work if the user runs ava with no arguments? When you specify files like in my test or have some sort of custom match (which was the source of this issue), it fails (just tested it).\n. Thanks, !**/node_modules/** seems to work best if pushed before passing the files to globby.\n. Ha, I meant adding the extra /** at the end made it work!\n. I got this \ud83d\udc4d\ud83c\udffc, will look into it today. For the CLI tests, it's just a matter of changing the child_process stuff to new Api... with the correct arguments and then api.run()... right?\n. I see. Well, I'll begin with converting some over the weekend, but can you give some examples of more precise tests?\n. I see. Well, I'll begin with converting some over the weekend, but can you give some examples of more precise tests?\n. \ud83d\udc4d\ud83c\udffc even if you mentioned the wrong person : )\n. \ud83d\udc4d\ud83c\udffc even if you mentioned the wrong person : )\n. @vdemedes so I am working on converting the CLI tests to use the API, but I am just wondering if this makes sense.\nFor example, for the first CLI test (ES2015 support):\n``` javascript\ntest('ES2015 support', function (t) {\n  t.plan(1);\nvar api = new Api([path.join(__dirname, 'fixture/es2015.js')]);\napi.run()\n    .then(function () {\n      t.ok(true);\n    });\n});\n```\nThe test passes, but I am not sure if this is the best way to test. The original CLI test just checked that it did not return an error, so perhaps doing t.ok(true) has the same effect. I tried doing t.doesNotThrow but then it said it was pending.\nThanks and sorry for any silly questions!\nUpdate: I can also do something like t.same(api.passCount, 1) or another similar tests on the API object. I think this is more explicit.\n. I'll give it a shot\u2014probably later today or tomorrow.\n. Just tested out a super basic JSON endpoint with supertest:\n``` javascript\n// This is an express app\nimport app from './index.js';\ntest.cb('get', t => {\n  request(app)\n    .get('/hello')\n    .expect('Content-Type', /json/)\n    .expect(200)\n    .end(function (err, res) {\n      t.notOk(err);\n      t.is(res.body.id, 1);\n      t.is(res.body.username, 'sotojuan');\n      t.end();\n    });\n});\n```\nSeems good enough for me, and there's promise versions of supertest that may be even easier to work with.\n. I can test on Windows during work tomorrow. I'll let you know!\nEdit: Forgot there's a windows computer I can try in one of my classrooms. Gonna give it a try.\n. @jamestalmage Just tested on Windows. The error is trivial. I am testing for:\ntest \u203a fixture \u203a one-pass-one-fail \u203a this is a failing test\nBut on Windows it's:\ntest \u00bb fixture \u00bb one-pass-one-fail \u00bb this is a failing test\n(Notice the arrow!).\nShould be easy to fix.\n. Thanks a lot for the help. I'm not very experienced with regex\u2014what would be a good loose regex to use?\n. \\w wasn't working but \\S (not whitespace) did.\n. Sure. Not sure why it's failing though ~~.\n. Sure. Not sure why it's failing though ~~.\n. Hm not very experienced with rebase, sorry. What should I be doing?\n. Hm not very experienced with rebase, sorry. What should I be doing?\n. All right, figured it was time to learn more git stuff. Fixed it (I think), will push again soon.\n. All right, figured it was time to learn more git stuff. Fixed it (I think), will push again soon.\n. No problem, and thank you guys for your help and patience!\n. What would be better? Also I'm trying to look where that message is generated, but can't seem to find it. Seems like this would be an easy fix!\n. Ah so it's an internal thing.\nIs something like this better? Got it working.\n``` javascript\nimport test from 'ava';\ntest(t => t.fail('I failed fast!'));\n```\n. > false fail false is the message AVA generates if you use t.fail()\nThat's what I meant! I thought \"false fail false\" was something someone wrote, not something AVA generated. \nSo I guess a better message would just be \"Test failed\" or \"Assertion failed\"? Let me know!\n. That sounds good. I'll send the PR in a few minutes. \n. Sounds good!\nSeems like my suspicion was right. Node 0.10 gives an error code of 8 on no file fount, while other Node versions give an error code of 0, which is why my test was failing. It's a quick regex change luckily.\n. Yeah sorry I mistyped. Well either way I got that fixed. The AppVeyor builds all passed\u2014not sure why the check isn't there yet.\n. I was playing around with/learning about syncing forks and accidentally pushed, so I guess I'll need your LGTM back @jamestalmage. Sorry about that ~_~.\nI'm gonna go ahead and fix some inconsistencies in my tests (I think t.ok(err) should always be first).\n. (Closing because I accidentally added the code for another PR I am working on).\n. @vdemedes Thanks! I am still learning git stuff. I'm going to wait until my other PR gets resolved though because my changes there are in the master branch of my fork (something I also need to stop doing) and I don't want to break anything else accidentally.\n. The commit message is a little weird, but I got it (had never squashed commits so I figured I'd play it safe).\n. Oh I guess it's fine then (just thought it was curious that the message for the CLI one was in the long description part). Thanks and I will!\n. Need to start testing in 0.10 before pushing :baby_chick: \n. Should be trivial to make it an API test since the output it's testing is also inside the api object. I'm watching a film right now but I'll give it a go afterwards unless someone wants to fix it now.\n. @jamestalmage Do you know if this is fixed? Guess we have to wait and see if people's tests pass.\n. Hm the api object doesn't have both the title and the filename in the same \"place\". That's something done either by the logger or CLI code. That said, I could test that (for example) the first test has the title async function and its index corresponds to the index of async-await.js in the files array.\n. @jamestalmage This is a bit convoluted but it gets closer to what the CLI tests. Otherwise I am not sure how to replicate the CLI test.\n. Wow I feel dumb, I completely forgot about that! Give me a minute and I'll fix it.\n. I see. Either way, testing through .on('test') works and is much less code and easier to read.\n(Installing Windows on VMWare to see why it's failing).\n. Actually that's just output from the rejects on error and streams output test in fork.js, which tests the error output when given a nonexistent file (broken.js is not supposed to exist!). While we test for the correct message in the err object, the fork function logs the ENOENT error to the console. It's expected and not a \"real\" error! That said if you know of a way to suppress that output, I'd be happy to implement it. Maybe with hook-std?\n. Makes sense. I'll look into it.\n. Would this just mean adding something related to those directories here? If so I can work on this during my flight.\n. What's the status of this?\n. semantic-release looks awesome and I am personally going to use it in my projects but at this point switching to it would be more pain than it's worth.  I think Sindre and the team are going to make sure the version numbers stay sane. Same with enforcing commit message guidelines (even with tools like commitizen).\nOn another note, I admit that whenever I am working on AVA I delete the coverage option in the test script, which means one of these days I am going to forget it to put it back before committing. A regular test script would be great!\n. Should we make a common_pitfalls.md in the documentation directory with these?\n. What abut source vs files in the AVA package.json options?\n. Just did on an IRC!\n. Ah, sorry I should've just said everything in the first comment. My friend was just confused about the difference between both (what do they do?)\u2014they do look very similar. Maybe not a pitfall but worth a sentence or two in the readme.\n. @calebmer I don't think so, want to help?\n. Sorry I deleted my comment asking about the color because in the tests for the mini reporter it uses cyan for skipped.. Though I think for the actual reporting it should be yellow or no color as you say!\n. @jamestalmage Yep, working on it now.\n. Hi, sorry for the late reply. Maybe a check if api.skipCount is greater than 0 before adding it to the results would fix it?\nI added this to TapReporter.prototype.finish in lib/tap.js, removing from the output array when necessary:\n``` js\nif (this.api.skipCount > 0) {\n  output.push('# skip ' + this.api.skipCount);\n}\noutput.push('# fail ' + (this.api.failCount + this.api.rejectionCount + this.api.exceptionCount), '');\n```\nI also wrote a new test without any skipped tests and checked that information about skipped tests was not printer. It passed.\n@sindresorhus Let me know if this is a good solution for this problem. Seems to work!\n. I don't know much about the spec but if that's what it says then that's a simple change (just output 0 instead of nothing). \n. @azhang Yup, thanks for the example. It's a simple fix and I have it ready, just need to hear back about the TAP spec for skipped tests.\n. Tape seems to output only when there are skipped tests.\n. Seems like tap doesn't output when there are no skipped tests either. Going to go ahead and push a PR.\n. Does this consist of anything more than doing this?\njs\n    default: conf,\n    {\n        alias: {\n          t: 'tap',\n          v: 'verbose',\n          r: 'require',\n          s: 'serial'\n        }\n    }\nAlso, would you guys want tests for these? I'm not sure there should because that's meow stuff.\n. @boutros What do you mean? Rollup is a module bundler\u2014surely you aren't bundling AVA for production right?\n. I can't seem to replicate this.\nI have a file with two failing tests and one passing test with .only()... I only get \"1 test passed\" and a green check next to that test, with verbose. Can you share the code you are using?\n. @tomazzaman ping ~\n. Currently in class but from what I skimmed it looks good :-) Awesome work!\n. \n. Does this only happen in the TAP reporter?\n. Hm I see. I'll give it a try tomorrow. @jamestalmage @sindresorhus do you guys have any leads? I have a hunch it might have to do with the API. \n. All right, figured this out. Turns out there was no skipCount in the API. Seems like I got it working. Gonna clean some things up and send a PR later today.\n. This was fixed in #514! Feel free to close it.\n. This is the offending line. Should it just return an empty string?\n. Flaky Travis tests strike again?\n. Returning an empty string does put an extra line there, and returning nothing (just return) gives an XO error. I opted to return undefined, which seems to get the job done (no extra line or extra output).\n. null:\n\nundefined:\n\n'':\n\n. Seems like a way to test this would be to make sure the contents of the api.tests array are in a specific order right? I've been trying that out with this fixture, but sometimes the results of the array are out of order (once every ten tries, approximately), even with serial on.\n. Yep, I only included it because Sindre mentioned in the issue.\n. I went and fixed all references to doesNotThrow in the other tests; everything's working now. I was wondering what to do with the test for the alias because it prints the deprecation notice when the tests run (unless that's not a problem).\n. Oh I haven't pushed yet (been busy), was just asking! And thanks, will push it later :-)\n. Let me know if it's ready for a squash!\n. Looks good! Unfortunately the flaky 0.10 Travis test is failing. @sindresorhus can you restart it?\n. AppVeyor is very rude sometimes :cry: \n. Also agree that this is good and currently working on it!\n. Exporting files and excludePatterns could work but it'd require a big change in a lot of files (I think), so we'd have to get permission from the powers that be (@sindresorhus @jamestalmage).\nSome more exploration/updates:\nThis makes the watcher work:\nwatcher.js\njs\nexports.start = function (logger, api, files, excludePatterns, sources, stdin) {\n  var isTest = makeTestMatcher(files, excludePatterns);\n  var patterns = getChokidarPatterns(sources, files);\n  ...\n}\ncli.js\n``` js\nvar files = cli.input.length ? cli.input : arrify(conf.files);\nif (files.length === 0) {\n  files = [\n    'test.js',\n    'test-*.js',\n    'test'\n  ];\n}\nvar excludePatterns = [\n  '!/node_modules/',\n  '!/fixtures/',\n   '!/helpers/'\n];\n...\nwatcher.start(logger, api, files, excludePatterns, arrify(cli.flags.source), process.stdin);\n```\nAgain, just exploring/messing around. It \"works\" but the tests for watcher still fail. Will look into it this night.\n. In test/watcher.js:\n``` js\nfiles = [\n  'test.js',\n  'test-*.js',\n  'test'\n];\nexcludePatterns = [\n  '!/node_modules/',\n  '!/fixtures/',\n  '!/helpers/'\n];\n...\nvar start = function (sources) {\n  subject.start(logger, api, files, excludePatterns, sources || [], stdin);\n};\n```\nUnder the \"Chokidar is installed tests\" I get not ok test unfinished for any I try:\n```\ntest/watcher.js ....................................... 2/4\n  chokidar is installed > watches for default source file changes, as well as test files\n  not ok test unfinished: watches for default source file changes, as well as test files\n    at:\n      line: 128\n      column: 19\n      file: test/watcher.js\n      function: test\n    source: |\n      pending.push(_t.test(name, fn));\nnot ok missing plan\n    results:\n      ok: false\n      count: 1\n      pass: 1\n```\n. Done, let me know if you can help!\n. > I think you should keep the default excludePatterns in the API constructor. It only needs to be defined once since it's used for every run. Then the watcher can simply access it like before.\nSure, that makes sense.\n\nStill need to resolve whether we always want to prefix test titles (even if there's only a single test file) or only when tests are rerun via the watcher. How about making it an option on the API itself, and if --watch is enabled set it to true? That way the watcher output is consistent but regular output is still minimal, and we won't need a second argument to run() in order to differentiate.\n\nAlso makes sense to me, doesn't seem that hard to implement. I'm interested in what the other mantainers think though. If they're on board with it, I'll work on it tonight.\n// @sindresorhus @jamestalmage @vdemedes \n. All right, I think I got it but I feel like I am missing something. Let me know!\n. Okay, I'll give these a shot throughout the day. One thing I am confused about though, look at this test. Supposedly it's testing for prefixes, but it's a single file, so it fails with the changes since by default there's no explicit titles for just one file. Should I update the test so it sets the explicitTitles option? Or is it a problem with my changes?\nIt's the same case with a couple of the tests in api.js. I added explicitTitles: true to those that were failing and the changes you mentioned above and all tests are passing, but I'd like to know before pushing!\n. Builds and tests pass fine in my Macbook, not sure what's up with AppVeyor.\n. OK, well for now I fixed all the small stuff. Like I mentioned earlier, the test for failFast does need explicitTitles to be set to true as the output it tests for has prefixes.\nI'm not very experienced with conflicts so any instructions on how to deal with that would be great!\n. > Yes but if you check the full diff you'll notice you added those prefixes in this PR ;-)\nHaha I totally forgot I did that! It's been a while!\n. @novemberborn When you are not busy, can you review? Thanks!\n. @novemberborn When you are not busy, can you review? Thanks!\n. Thanks, um should I just copy what's in that commit you linked me to or is that something that will be merged?\n. I had some stuff I hadn't merged with master yet, should be good now.\n. Phew, all green on Travis (AppVeyor will take longer), finally!\n. Just by skimming the power-assert docs I don't really see support for \"multiple\" patterns at once (like a * operator), but this is a question for @twada.\n. I think it was just an idea he had\u2014but so far it doesn't seem like it's feasible.\n. I think I fixed this, does this look fine?\nTurns out  that the stack getSourceFromStack in the TAP reporter was getting did not match the index, so I just decreased the index (line number now in index 2, not 3).\n\n. I think I fixed this, does this look fine?\nTurns out  that the stack getSourceFromStack in the TAP reporter was getting did not match the index, so I just decreased the index (line number now in index 2, not 3).\n\n. This is pretty sweet, thanks! Looks great to me.\n. I'd like to work on this\u2014would this just require a check on test()'s arguments and an appropriate error message?\n. All right, I'll take a look at it later today or Monday. I have an algorithms exam to study for!\n. What's the status of this? A simple edit to reporters/mini.js fixes the extra lines, but what about the cross? Also an easy fix in the reporter file.\n. Closing because this was fixed a while back in #704.\n. Looks good to me :-)\n. This is awesome, thanks!\n. Just tried it, also works, and I wouldn't have to mess with TestCollection tests. I'll wait for someone from the core team to confirm though.\n. Um, don't think I did anything to cause the failing in 0.12. When I test with 0.12 in my computer, everything passes.\n. @novemberborn Sorry about that, I always forget! I'll make the changes and add the tests later today :-)\n. @novemberborn ping :baby_chick: \n. When I do chalk.enabled = true; on the mini reporter (and others I am guessing), a lot of tests fail.\nI'm guessing I need go to back and fix them? What is happening is that the lib/colors.js usage in lib/mini.js is making the tests fail. If I go back to lib/mini.js and change all of those back to chalk, the tests pass.\n. All right. Had to change some tests to make them pass (one of the recent changes removed the colors of some test titles, is that fine?). Not sure what's up with AppVeyor.\n. Personally I'd argue that there's no point in using .jsx files in general, but that's for another debate. This would only require changing one line, so I am for it.\n. That might be because the if checks for just \"less than\". That function gets iter count title may just be the same \"size\" as the CLI. If I change the check to be like \"if title is greater than CLI minus X\" it should be fine. I tried doing that but for some reason that specific test title never shows for me, but other looks fine.\nAlternatively, I could just get rid of the if altogether, though I'll have to figure out how to make the failing tests pass (unless just adding the ellipses to them works).\n. Forced process.stdout.columns in the mini reporter tests, so no need for the if statement anymore.\n\nAlso, I would prefer that the truncation character always be in the rightmost column and try to show as \nmuch of that last word as possible instead of just dropping it.\n\nI'll look into this\u2014sounds like a good idea.\n. As per @novemberborn's suggestion, truncated test.title directly instead of reassigning.\n. Added comment explaining the use of process.stdout.columns = 5000;.\n. @sindresorhus Would a recipe/docs for RN be welcome? Once we confirm a solution to this.\n. Looking good here too!\n. Looking good here too!\n. I'm with you but I think I am biased\u2014ok sounds fine to me but that may be because I've used it for months now. A beginner's perspective may be better. I think @kentcdodds did a Twitter poll about this, or if he hasn't maybe he should.\nThat said, I think truthy shows intent and meaning way more directly. Sort of how throws tells you all you need to know from the name. But it is longer and more annoying to type. Hard choice!\nIf we do make a change I think it'd be better to soft deprecate for now till 1.0 (ok still works but it's undocumented) and see what people think.\n. @jamestalmage For now is it fine to submit a PR that changes\nformat('not ok %d - %s', ++this.i, test.error.message),\nwith\nformat('not ok %d - %s', ++this.i, test.title),\nIn lib/reporters/tap.js? Seems like that would emit the output @will-weiss wants.\n. Thanks for the tips, fixed.\n. You guys decide! Let me know\u2014either seems clear to me.\n. All right, changed to \"implementation\". If it looks good I can do a PR for the other messages.\n. @xjamundx Can you post the actual tests that are giving you the strange XML output? I just tried tap-xunit with some simple tests and it worked fine.\n. Thanks for the feedback. I'll look into it later today. \n@sindresorhus would this be something we can fix (the arrows are not valid XML characters) or is that the responsibility of tap reporters?\n. Opened #792 to help with the ANSI codes. Once it gets merged please test again with the latest master @xjamundx.\n. @xjamundx We fixed the ANSI escape codes\u2014let us know how they are now.\n. @xjamundx We fixed the ANSI escape codes\u2014let us know how they are now.\n. Fixed by #744.\n. Might have to change some stuff because the output can be redundant when used with other TAP stuff for example nyan:\n\n. Looks good to me!\n. Is there a babel plugin out there to do this?\n. Looking into doing this since I'm interested in learning Babel stuff\u2014just need more info. Are we just turning all anonymous callbacks inside tests to arguments to t.__wrapFn?\n. @jamestalmage It's fine! It was either this or the tape -> AVA codemods. I'll work on those instead.\n. > Maybe we should fork only-shallow\u2026 not-that-shallow? \ud83d\ude09\nI'm down for doing that\u2014would it make sense to push it to npm or load it through GitHub? Feel silly publishing the same thing but with one extra =.\n. So I went into node_modules/only-shallow, switched out == with === and tested deep equality of {a: false} and {a: 0}\u2014it failed as it should. Obviously, a bunch of only-shallow tests themselves failed but that seems easy to fix. I think we're good to go for our version (not-so-shallow!)\u2014I'm more than willing to create it but there might be some edge cases I'm missing.\nAnd thanks @othiym23 for the explanation!\n. I sort of agree\u2014especially because if you're doing any serious React testing you'll use enzyme. I don't know much about browser testing, but it seems maybe we can just use a regular button with a click event handler?\n. As far as I can tell lib/reporters/mini.js doesn't know if it's reporting on a watcher test\u2014what would be a good way to pass down that data without coupling stuff too much?\n. Fixed the buffer stuff and added some more tests.\n. Used buffer-equals in not-so-shallow and bumped the version in the PR. Will get to the tests later, lot of school work to do D:\n. Added some random tests for arrays, functions, and objects.\n. Fixed arrow function, added another constructor test.\n. Yep. Not sure why\u2014can anyone help?\n. Yep, fixing now\u2014I'm too used to typing let!\n. not-so-shallow fixed for older Node versions.\n. Added comments explaining the regression tests.\n. Hm this seems to be working for me in latest master already, can someone confirm?\n. Are we good to close based on the PR results?\n. Great work @dcousineau! By the way I sat in front of you at ManhattanJS yesterday :p\n. @dcousineau If I am still in NYC by then I'll go. Thanks!\n. The nipicks are fine, but not sure about testing because I don't think ANSI escape codes have been an issue or come up until the issue that led to this PR. That said, I have tested it locally and it works, but if anyone has a good idea of a test let me know.\n. @novemberborn The ANSI codes show up when you test more than one file\u2014is there a way to simulate that on the TAP tests?\n. I would say remove it just from TAP like I am doing now\u2014seems like TAP stuff in general doesn't really like coloring in the raw reporter.\n. Thing is this was mentioned in issue #723, which is not specific to this.\n. Added a test\u2014let me know if it can be improved.\n. Added a test\u2014let me know if it can be improved.\n. Done!\n. Done!\n. All right, sorry about that (got a ton of things going on right now), should be fine now I hope \ud83d\ude0e \n. All right, sorry about that (got a ton of things going on right now), should be fine now I hope \ud83d\ude0e \n. LGTM, thanks!\n. Thanks @cgcgbcbc!\n. Good catch!\n. Fixed tests.\n. Good idea\u2014just pushed some changes to make that work.\n. Pushed the changes (and thanks for the explanation, makes sense).\n. @asafigan You're more than welcome to!\n. \ud83d\udc4d\ud83c\udffc\n. Agreed. At the very least we may do this on 1.0.\n. Did this fix the issue? Issue creator deleted his account.\n. Working on this\u2014looks good to me.\n. BTW are we doing this for the type of errors shown or also the ones under knownFailureCount and rejectionCount/exceptionCount?\n. I think you are missing a period at the end but otherwise looks good!\n. Updated.\n. I'll add it! We can close the PR when we think we have enough to get started. Will write it up after I finish breakfast.\n. Updated.\n. \ud83d\udc14\n. Got it\u2014put it in Support if that's a good place.\n. Updated.\n. Fixed all the docs stuff\u2014sorry about the unrelated commits (still not good at merge conflicts...), how would I fix that? Rebase?\n. All right, sorry about that, I did that commit in my work computer and probably didn't have editorconfig. Should be better now (?).\n. Another vote for cleanup. .and. reminds me of Mocha/Chai\u2014cleanup just looks for AVA-ish if that makes sense.\n. Taking care of this over the weekend.\n. Are we still doing this? Making a list of things to take care of this weekend!. Agreed with above but IIRC James is quite busy right now and probably not able to work on this.\n@avajs/core This is the most needed open PR in my opinion, but it's not an easy problem to solve right? Anything we can do to help?\n. @avajs/core Almost done with this but let me know if I can take care of any nitpicks\u2014might have to pull down my branch to check it out but will try to post screenshots tomorrow.\n. Thanks! Las thing is the error stack's first line having a different color/indentation than the rest. Currently it's all one big string\u2014what's a good strategy here @sindresorhus? Manually take the first line out?\n. All right, getting close to being done. Below is a screenshot (not sure why the stacks got longer from when I started):\n\nProbably a lot of nitpicks to do but let me know how it looks :-)\n. Should be aligned now!\n. That's weird, I guess I did. I'll check when I get to work and look at the rest of the issues\u2014thanks.\n. All right, should be better now.\n. Huh, weird. On my computer it looks a bit different (no power assert). I'll take care of the spacing in the stack trace first as that's simple and investigate.\n. Here's what I am seeing (after indenting stack one more space). Not sure why ours differ so much\u2014I merged latest master, re-linked, and reinstalled Node modules.\n\n. Ah, I got it\u2014happens on power assert stuff. Will get it fixed up!\n. This is what I have now:\n\nLet me know if it looks good so I can take care of the tests and push!\n. Good catch on the newline\u2014I think I accidentally removed it earlier. About the stacks, they just randomly got long while I was working on this so I'm not sure when or why it happened \ud83d\ude05 . Is next-tick something on master that we're using?\n. That's not a typo, that's an effect from using AVA \ud83d\ude0e \n. \ud83d\udc4d great job!\n. Awesome work @asafigan!\n. Few nitpicks but overall LGTM. Should be merged by the next push!\n. @nesukun Sorry for the delay. Are you having the same issues in 0.16?\n. Sorry, didn't see the 0.16.x! I'll investigate.\n. LGTM \ud83d\udc4d \n. Does your recent clean-stack module not help with this?\n. Updated!\n. Good idea!\n. Node 6 failing tests are fixed in https://github.com/avajs/ava/commit/20ab39de046e527dec7b369f375d6c8e5fd4f5e1 so they don't apply to this PR.\n. Might just be an issue with the glob. **/* means everything as you know\u2014AVA's file resolver might be getting overwritten by the results of that glob and not \"re-ignoring\" stuff. IMO this is something we should fix.\n. @gravitypersists, it turns out it's a glob issue and not an AVA one.\nTry: assets/tests/**/*.js Should work.\nSee:\nhttps://github.com/isaacs/minimatch/blob/master/test/patterns.js#L130-L131\n. Ha, right sorry. I mean, we need to do something about this. Maybe if we see ./ at the beginning of a blog we take it off? Might as well not have users worry.\n. Doing that ^ fixes this example.\n. So in the runner tests there's the \"test types and titles\" test, and for some reason the beforeEach and after are being set as fn Only happens with those two. Investigating!\n. Problem seems to be with the fn-name module.\n. This code:\n``` js\nvar fnName = require('fn-name')\nvar fn = function (a) {\n  a.pass()\n}\nconsole.log(fnName(fn))\n```\nrunning in 6.4.0 logs out null.\nIn 6.5.0, it logs out fn.\n. ...And further down...\n6.5.0:\n``` js\nvar fn = function (a) {\n  a.pass()\n}\nconsole.log(fn.name) // 'fn'\n```\n6.4.0:\n``` js\nvar fn = function (a) {\n  a.pass()\n}\nconsole.log(fn.name) // ''\n```\n@sindresorhus So what should be done in fn-name to handle this?\n. Ah, I meant the search led there\u2014fn-name works as intended!\n. A quick way to fix this is to add a rule to the part of AVA's code that sets titles when they're null to also handle them the same way when they're fn. Just tested it and it works but again, that's hardcoding for the specific test.\n. @sindresorhus, what do you think is the best way to go forward with this? If nothing else, we can just check for Node versions...\n. Yeah it depends on the purpose\u2014I feel like it's just to fix the test right? So it'd make sense to rollback the code changes and maybe have a test for Node versions before and after the change?\n. Done!\n. Right, of course :p Fixed.\n. We're more than happy to tell you reasons to use AVA but it's not our place to tell you why Mocha is a bad choice\u2014that's on you.\nI have no experience with Mocha as I've always used tap, tape, or AVA and if I'm not the only one then we can't give you unbiased, informed opinions. This is probably as much as you can get from us and the fact that Sindre cut his Pageres test duration by almost a third with AVA.\n. IIRC we're releasing one very soon. @novemberborn I think Sindre said he was going to be busy, do you think we can do a release soon?\n. @avajs/core What do we think of this? I kind of like the idea, though I find mini with --fail-fast pretty \"minimal\".. I've updated the text to At least 2 tests skipped. plur doesn't handle was vs were so I decided to omit  it unless it's fine for me to make a small function in both reporters to handle that. More tests added too.. All right, sorry about that. Text fixed!. Thanks Lee!. I definitely think it should at least be a common pitfall. Thanks @taylor1791 for the detailed issue!. LGTM as well. Great!. I agree with the change \ud83d\udc4d . This seems to be a bit beyond basic AVA. Maybe what you need is an npm package that, given two floats, tells you if they are approximately equal to a given degree (I'm sure this exists).. > We need to investigate why it fails on Node 0.10\nIt seems to fail in the regex test for the error message, so maybe the error message changed from 0.10 to others. I can give it a try with nvm.\n\nIf this test passes, that seems fine to me.\n\nYep it works and passes but I was just curious about the fact that a stack is printed to the screen during that error. Maybe it's not a big deal.\n. Gotcha :+1: \n. That's what I get for copy and pasting without looking. Thanks!\n. Great point\u2014agreed. I'll do this after my flight. @sindresorhus after I make that change should I squash that commit with the rest?\n. Yeah, I copied the example from the meow readme for this, but I can see how that's better. @sindresorhus let me know which style you prefer!\n. So basically take out the whole if block?\n. Just me forgetting to uncomment before pushing \ud83d\ude11\n. Actually the test fails without explicitTitles set to true. The prefixes the test tests for don't show up because it's a single file.\n. I think it's because the test function already has 'at' in it. Either way, if you add at to the Regex the space trimming does't work.\n. I used to not have the check but then for some reason the mini tests were failing. Didn't have time to diagnose why though.\n. That's what I was doing before but it wasn't working because of the fn = noop line (which seems to be used to \"override\" any given function), and if I removed that other stuff broke.\n. Got it working, thanks!\n. xo rule.\n. Do you mean same values but different order?\n. Ah, missed that while copy/pasting.\n. Done.\n. Ha, no problem, just realized that.\n. Ahhh, keep missing silly stuff, my bad.\n. Nitpick: It's \"AVA\" in all caps.\n. You should have a blank like before and after Markdown headings (you do elsewhere, so this is more about consistency).\n. \ud83d\udc4d \n. This was part of the merge conflict (this part was an old version from whenever I made my fork). Fixed when I merged readme from master.\n. Ah, knew I'd miss something! Thanks.\n. Yeah, that's the issue. I'm trying to think of a way that we can handle both falsy values and the test's function names but that also won't affect anyone else :/\n. Keep the comma :-) I like Oxford Commas.\n. ",
    "EEtancelin": "Fine, i dev beginner and i will be happy to take \u00e0 look and see how improve code coverage.\n. ",
    "platy11": "@sindresorhus I was looking at cli.js100 and in my testing (in your cat-names repo) I cannot get the deprecation error to appear - is this just me?\n. @novemberborn Ok, thanks! :smiley: \n. @sindresorhus I opened PR #1001 \n. @sindresorhus is this right: platy11/ava@83899e77b89264ba134142106c3ee55ef5662c2d\n. @sindresorhus I opened #999 for this.\n. @sindresorhus Looking at https://github.com/avajs/ava-codemods/blob/master/lib/ok-to-truthy.js, would you only need to change the references: ok to error etc.? (And add a test)\n. @sindresorhus Thanks, doing that now\n. @sindresorhus is this correct for codemods.json?\nhttps://github.com/platy11/ava-codemods/blob/master/codemods.json\nAnd is this everything I need to do: https://github.com/avajs/ava-codemods/compare/master...platy11:master? Thanks for your time, I can see you are working on multiple projects at once.\n. @sindresorhus What should I put under Available upgrades? 0.14.x \u2192 0.17.x?\n```\n$ ava-codemods --help\nUsage\n    $ ava-codemods [ ...]\nOptions\n    --force, -f    Bypass safety checks and forcibly run codemods\nAvailable upgrades\n    - 0.13.x \u2192 0.14.x\n``\n. @sindresorhus I did0.16.x \u2192 0.17.x` and opened PR avajs/ava-codemods#27. Feel free to share any things I've missed or improvements that could be made there.\n. @sindresorhus The tests are failing because they use t.doesNotThrow a lot, I'm not quite sure how to replace this yet\n. @sindresorhus OK, tests now pass\n. ",
    "wildaces215": "Where can I start writing tests for ava it seems like a good place to start contributing to open source software!. ",
    "denar90": "Hi guys. Talking about site. It can have pages generated form docs and pages with some content, like authors, links to some resources, compare stuff etc. So I propose to build all stuff using grunt/gulp etc. Build tool will grab all content and make html from it, would it be doc file or jade or whatever. \nSo site can consist of pages like\n- main page (why AVA, AVA vs other, contributors)\n- sources/plugins\n- docs\n- examples\n- changelog\n  What do you think guys?\n  Also what do you think about creating repo in AVA org where issues can be created? Eg. I can prepare some PR's and issues while core team is busy with v1. Someone from core team can from time to time look at it.\n. ",
    "yowainwright": "@novemberborn could I get insight into the website priority from the Ava team? \nI'm finding the readme/docs folder to be difficult to navigate for myself\u2014although the documentation is clear. \ud83d\ude4f\nIs there a way to better navigate Ava's readme and docs folder that I'm not aware of?\n. ",
    "zenparsing": "@sindresorhus You can't directly await an observable.  We've discussed it at length and implicit conversion to promise for use by await is controversial.  You can consume the sequence with forEach and await the returned promise, or we might add something like observable.toPromise() in the future.\n. ",
    "MarkTiedemann": "@jamestalmage Well, I'm just kinda annoyed of having to change the default timeout in other test frameworks again and again.\nI'm actually fine with having a global timeout, as long as it is disabled by default.\n. @jamestalmage You changed my mind. :) I agree, in CI defaults do make sense.\n. 500 ms seems reasonable to me since AVA is supposed to be fast. And it's also supposed to be minimal; that's why I think timeouts should be disabled by default.\n@markthethomas - I don't think smart-setting the timeout based on CPU information is helpful. For normal tests, you don't really need timeouts anyway. At least, when I implement timeouts, it's usually because of possible network issues, like \"if you can't connect to service X within Y seconds, abort the tests\".\n. 500 ms seems reasonable if you want to enforce small, fast local tests. 30 s seems reasonable if you just want to prevent CI from hanging.\nSo both defaults kinda make sense, in their own narrow context. So actually they don't: Because how do you know the context? You don't know.\nBut the solution is simple, I think. If the timeout is disabled by default, we don't need to pick a pseudo-reasonable default -- just let the developer decide!\nIf someone wants to run quick AVA tests locally, let him use a timeout option:\n--timeout 500\nAnd if someone just wants to prevent CI from hanging, let him set his own reasonable timeout:\njavascript\n// timeout still disabled\nif (process.env.TRAVIS) {\n  test.setTimeout(30000); // timeout now enabled!\n}\nHow about this idea? :)\n. > The Mocha default at times drove me mad for being too short.\nYeah, I know that feeling. In Mocha, I usually set the timeout to 10 s just so I didn't have to worry about tests failing due to the timeout. Then again, I don't think that's good practice either. If a single test takes more than 1 s, most likely, there's something wrong with the code or the test itself (unless we are talking about stuff that is supposed to be slow, like connecting to that 20-year-old Oracle instance your underfinanced university is using to scare its undergraduates away :no_mouth:).\n\nIf we were to have a default (...)\n\nWhat do you think about not having a default, about having timeouts disabled by default and allowing devs to set one if they need one?\n. Well, yeah, kindness to CI providers is a good reason indeed. :)\nWhich reminds me: Most CI providers (including Travis, Circle and Codeship) set the CI environment variable.\nSo how about setting a default timeout of 30 s if process.env.CI?\n. @novemberborn Sorry I forgot to include that. I'm using v0.25.0. But I guess it's true for any version of ava running Node 10 since package-hash hasn't be released again yet.. Early release and NODE_PENDING_DEPRECATION=1 was set. Upgrading to latest Node and unsetting the env var fixed it.\nThank you very much for helping me!. ",
    "piuccio": "Would be nice to have it. I've tried ava for the last couple days and not having timeouts is really annoying, the feedback loop is too long and you have no clue of what's broken. I hate having to setup babel + node-tap but I prefer that over hanging tests.\n. ",
    "MarkHerhold": "I hit the exact same issue with AVA and the config module. The working dir needs to be the directory where I launch test my process, not where the test file lives.\n. @sindresorhus OK, that is fine. This is a major turn-off in terms of adoption of AVA. (https://github.com/sindresorhus/ava/issues/222 does not help either). I looks like the config module lets me define an environment variable called NODE_CONFIG_DIR which should solve the problem. I don't like the idea that my application runs from a different working directory during test as compared to development and production though as it could cause some really nasty and hard to detect bugs.\n. ",
    "mattfysh": "I see... it sounds like this is a test framework with parallelization built-in as a feature, as opposed to a parallelization framework that works to parallelize test suites written in other frameworks (mocha, jasmine, etc)?\n. ",
    "nvartolomei": "Those could be written as codemods btw https://github.com/facebook/jscodeshift\n. ",
    "huan": "I just switched from tap/tape to ava, which benifit me reducing the test time from dozens of minutes to around 3 minutes. ava is great.\nwhen I made the transition, the main pain to me is the following 2 changes:\n1. rename t.equal to t.is\n2. rename t.ok to t.truthy\nthe reason I found this issue is because I have the same idea with @timoxley . but after I saw the explains from @sindresorhus , I agree there will not easy to switch between ava and tap/tape. (not like switch between tap and tape, which is only need to change one line)\nso my sugestion is: at least we alias those two methods(equal & is, ok & truthy), because they have the same behaviour, and will let developer switch more easier, without cause any confusing.\n. @sindresorhus ah, cool. it will help much. thanks!\n. @niieani I think what @LiTiang asking, is the same question as I have:\n1. we have a test file named: case.spec.ts\n2. we want to run case.spec.ts directly(without any pre-compile). if we use ts-node, we can run it by ts-node case.spec.ts\n3. we try to run ava --require ts-node/register case.spec.ts, but failed by the message: \n\n1 exception\n  \u2716 Couldn't find any files to test\n\nI had looked through the issues, it seems this is a feature that we will support in the future version? looking forward for that.\n. I think the goal of this issue Allow tests files to have any extension (i.e..jsx,.ts) is quite straight forward.\nand the solution should be very easy to compatible with an extension other than the default(and forced) js: comment out the line limit to .js ext.\nand about the other technics, i.e. cache from ava main process, I think I can accept that it be disabled if that will cause a problem when use ts ext.\nwe should stop sticking here. because now I still can not use ts-node ava test/*.ts to run typescript tests. :(\nso I have to use back to tap in my new project.\nthis is how to run typescript with ts-node in tap, hope it can help others: \nhttps://github.com/tapjs/node-tap/issues/313#issuecomment-250067741\n. @niieani thanks! definitely I want to have Jest a try.\n. I'm writing a hack for support to run typescript without precompiling. Please see #1109 :)\n. @rauschma No, this will not work because of the AvaFiles. See also: https://github.com/avajs/ava-files/pull/9. Hi,\nIs there any feedback or suggestion?\nI want to know:\n1. is the parameter name all right? now they are:\n   1. --ext / -e for AVA CLI\n   2. exts a new key for AvaFile options \n2. how about to set --ext automatically? if we run ava test.ts then let ava know .ts should support typescript.\nlooking forward to getting feedback, then I can make the Pull Request fit better.\n. Got it.\nI'll follow your great feedback then make the PR later. :)\n\n[x] It should be possible to set --ext multiple times.\n[x] .js files should always be included.\n[x] TypeScript should not be a dependency of AVA, but rather in the user's package.json. We bundle Babel because we depend on it in AVA for other things too.\n[x] If it's not, we should throw a friendly error message. \n[ ] Setting extension automatically works fine when you specify exact files, but most users specify globs, so not sure whether it's worth the inconsistency of only working in some cases.\n[x] I think --extension/-e CLI flag and extensions option name would be better.\n[x] Make sure you handle caching correctly.\n. Hi @sindresorhus ,\n\nI had followed all your feedbacks and made two PRs: one is for AvaFiles, the other is for AVA.\nPlease notice that CI for AVA will be expected to report failure because the ava-files NPM modules do not support extensions parameter now.\nAll unit tests had passed in my dev box with the PR code.\nPlease let me know if I missed anything in PR, thanks.\n. @jedmao Hi buddy thanks you for like my work, and I'm sure you are a TypeScript fan like me. \nIn theory, I think it should work with the --watch flag, because I modified all the parts in AVA to support ts, including Watcher class and all the Watcher unit tests. But I did not test it yet, and there's possible I missed part of it. So if you find any problem with that, please let me know, and I'll get it worked.\nIf you want to try it before @sindresorhus merge this PR, you can pull my clone at https://github.com/zixia/ava (branch: typescript) , and also don't forget to link ava-files too: https://github.com/zixia/ava-files. @patrick91 Thanks!\nAVA decide to re-design the total code structure of how to use Babel, which delayed the TypeScript support progress.\nSee: https://github.com/avajs/ava/pull/1159. Looking forward it, or I have to consider other solutions like tap which had already supported the TypeScript(https://github.com/tapjs/node-tap/issues/313) and Concurrency.. It's a pity that we still have no workaround for this after waiting almost a year.... @andywer You are awesome!. @sindresorhus Yes, I'm still fighting with the unit tests.\nBecause of the new extensions option parameter, some unit tests broken.\nWill push after I fixed them.. Just finished all the unit tests with the latest code in this PR branch.\nava-files has updated PR to latest version as well.\nPlease let me know if I missed anything. :). Hey @vdemedes , \nI had just followed all your comments. \nAbout the comments in lib/ext/ts.js, they are just comments. I also had made this file clearer with new code.\nThanks for your reviews.. @novemberborn I love ES2015 too. Had just followed your suggestions and resolved all the conflicts.. Thank you guys for all the replies of this PR. \nPlease let me know if I missed any important replies because this conversation is growing fast. ;-p. @novemberborn got it.\nThe reason I think register is needed in the workers is because if I do not do this, my test will not work anymore. \nI believe that's because my tests have to import pure TypeScript module file: there is all kind of TypeScript files in my source tree, and I never compile them, just import them in my tests.\nWill check them and reply you with the details later.. Hi @novemberborn ,\nGot your idea, and thanks for the explanations.\n\nThere are none, currently, since .js is hardcoded in a few places. I'm suggesting you check for .ts after the hasFlag tests, before API is instantiated. Around here.\n\ndone.\n\nRight, that's the same issue we have with our Babel support: source files are not automatically transpiled so users need to add babel-core/register to AVA's require \n\ndone.\nI had removed extTs.register() part from lib/test-worker.js, and add the following json part to package.json, TypeScript work without any problem again.\njson\n  \"ava\": {\n    \"require\": [\n      \"ts-node/register\"\n    ]\n  },\nFor more about TypeScript register, here's some discussions about it: https://github.com/Microsoft/TypeScript/issues/1823\nHave a good weekend!. I found a strange unit test failure in test/cli.js at here: https://github.com/zixia/ava/blob/d7bfa76fb708b6d918f1c821df0417d79e96cde4/test/cli.js#L383\nWe run cli.js in cwd, however, we pass resolveTestsFrom to AvaFiles at here, which is /tmp/ instead of /tmp/xxxx/, as the result, test fail.\nI have no idea of it because I did not touch this part this week and everything is ok before. Will check it later.. @asvetliakov Before this PR be merged, you can try this new feature by clone the source with npm link:\n```shell\n$ git clone https://github.com/zixia/ava-files.git\n$ cd ava-files\n$ npm install && npm link\n$ cd ..\n$ git clone https://github.com/zixia/ava.git\n$ cd ava\n$ git checkout typescript\n$ npm install & npm link ava-files && npm link\n```\nThen you can use ava with TypeScripe in your project like:\nshell\n$ cd my-project\n$ npm link ava\n$ ./node_modules/.bin/ava --extension=ts\nPlease let me know if there's any issue with your project if you have time to try it.\nBTW: I love your Huject.\nGood luck! :). @novemberborn Understand, and please let me know when you made new progress on your RFC. :)\nHave a nice holiday!. @novemberborn Understand, and please let me know when you made new progress on your RFC. :)\nHave a nice holiday!. @novemberborn No problem, looking forward hearing from you.. Great, hope we can add support to those languages soon.\nAs my view, if we could get rid of the limitation in ava-files, ie: do not restrict the file extension, just let users define them... \nThen we can use ts-node or babel-node to run ava with the specific language support, without any more code design/modification, and everyone will be happy. :P. Great, hope we can add support to those languages soon.\nAs my view, if we could get rid of the limitation in ava-files, ie: do not restrict the file extension, just let users define them... \nThen we can use ts-node or babel-node to run ava with the specific language support, without any more code design/modification, and everyone will be happy. :P. Ok.. I just run an npm install arrify source-map-support --save by mistake\nRestored old version.. Ok. fixed.\nbut this part looks to be a little weird:\njs\n  var isBabelPath = /^babel-runtime[\\\\/]?/.test(path.node.value);\nRef: http://stackoverflow.com/a/40315390/1123955\nAnd a question: How did the AVA pass the CI test? I just clone the AVA repo, run npm install && npm t, XO will throw no-useless-escape error.\n. done. fixed. :goat:. done. fixed. done.\nthrow new Error('`--extension ' + ext + '` is not supported by AVA.'). done.. done. I changed to this:\njs\n  if (cli.flags.extension) {\n    cli.flags.extension = arrify(cli.flags.extension);\n  } else {\n    cli.flags.extension = ['js'];\n  }. done.\nthrows the same error as the above.. @jedmao great idea, thanks!. deleted.. removed.. shorted. . I use extensions at first. I try to make the name more clear by changed it to extensionList .\nHowever, I had followed your suggestion, renamed it back to extensions.. removed.. removed.. done.. done.. done.. done.. removed. yes and no.\nts-node will do this if we use transpile function from ts-node. However, the current version of lib/ext/ts.js is not using the ts-node by default, it use the raw typescript module, so we have to do this by ourselves.\nI leave the ts-node in lib/ext/ts.js is because at the beginning I'm not sure which one is better, so I implement both, but at last I decided to use pure typescript by default:\njs\nmodule.exports = module.exports.default = {\n  register: typescriptRegister,\n  transpile: typescriptTranspile,\nDo you think we should use ts-node, or I need to get rid of the additional ts-node code in lib/ext/ts.js?. done.. done.. done.. done.. ok. done. \nNow we only have one extension: ts. If we support more extensions in the future, there might come with 'lots of same ifs'. Hi @novemberborn ,\nI had simplified this module, removed all the ts-node parts, cleaned the code, but not just the transpile.\nThe register() function has to be kept because if we run test written in TypeScript, the lib/test-worker.js must has the ability to transpile TypeScript, which is enabled by extTs.register() at here: https://github.com/zixia/ava/blob/typescript/lib/test-worker.js#L38 \n. Had changed to if. \nWhere are the allowed extensions in cli.js? Did you mean we should save all supported extensions to an array in cli.js, then reference it in other js files?. I think we need to keep register() with tranpile(), as I described here: https://github.com/avajs/ava/pull/1122#discussion_r91015553\nIf there has any alternative way to do this, please let me know, thanks.. I use .ts only and I do not know anything about react(I use angular). \nCould TypeScript transpile .tsx by default? . I don't think the TypeScript support would still work after we removed those changes. Because if no extTs.register(), then test-worker.js will not work with .ts test files.\nThis TypeScript support will only be enabled when we specify --extension ts arg with AVA, which is the case we need to automatically transpile all TypeScript file with extension: .ts\nAnd about the dependencies, I think for TypeScript is different as for JavaScript, because:\n1. There are almost no TypeScript dependencies in NPM(node_modules)\n1. The .ts extension is not the default JavaScript file extension, so it's almost only in our development environment.\nPlease correct me if I'm wrong, or missed anything. Thanks.\n. ",
    "AutoSponge": "If we can get t.equal to alias t.is then spok  just works.. @sparty02 I just set this up on a windows machine and it works but I couldn't use nyc npm test in my scripts. nyc ava works just fine.\n. I'm running into an issue with this decision. While using zeit/micro for a server, which uses async-to-gen, I don't get accurate coverage data when using:\njs\n\"require\": [\n  \"async-to-gen/register\"\n]\nHowever, the tests run fine. If I want accurate coverage (from nyc), I have to go to latest node version and remove the require config. However, my build server can not be upgraded to latest yet. I'd like to have ava run tests under a cli switch and keep require out of my config so I can run coverage locally.. @novemberborn great idea. This worked for me.\njs\nif (!require('is-async-supported')()) {\n  require('async-to-gen/register')\n}. ",
    "limafelipe": "Something like this https://atom.io/packages/atom-mocha\nI belive that can help to run tests directly in Atom to optimize time to alternate screens.\n. Something like this https://atom.io/packages/atom-mocha\nI belive that can help to run tests directly in Atom to optimize time to alternate screens.\n. In the WebStorm the test suite is pretty nice! See this print:\n\n. In the WebStorm the test suite is pretty nice! See this print:\n\n. Perfect, dude!\n. Perfect, dude!\n. Really, the tree view is more beautiful than useful, the only cool thing is when clicking in the icon with the test title it goes to file and line of the test. So, when a test has error is very easy go to him.\n. Really, the tree view is more beautiful than useful, the only cool thing is when clicking in the icon with the test title it goes to file and line of the test. So, when a test has error is very easy go to him.\n. ",
    "varemenos": "I understand this isn't an immediate goal for the project but I would like to recommend an atom testing package for inspiration: https://github.com/jacobmendoza/rspec-tree-runner\nImho it's UI is spot on for testing\n. I understand this isn't an immediate goal for the project but I would like to recommend an atom testing package for inspiration: https://github.com/jacobmendoza/rspec-tree-runner\nImho it's UI is spot on for testing\n. ",
    "jedwards1211": "The one thing that disappoints me about transpiling in prepublish is I have to transpile for the lowest common denominator, i.e. babel-preset-es2015.  If you transpile in a postinstall you could use babel-preset-es2015-node to get the most optimized transpilation for the user's version of node.  But I agree there are to many other problems with transpiling during postinstall.\nBut I guess I could publish transpilations for node 4, 5, and 6 though and have a bit of extra code to automatically select the right version based on the node version.  I wonder if some genius has already figured out an ideal approach for this?\n. ",
    "FuzzOli87": "@jamestalmage \nInteresting. I should of done a bit more research before asking but I was stuck in traffic and I had it in my head. I'll give it a spin and report back.\n. ",
    "mindmelting": "I think the latest release of nyc now works out the box... (or at least it does for me!)\n. ",
    "MoOx": "Note: this is assertion is doable using https://github.com/algolia/react-element-to-jsx-string\n. Note: this is assertion is doable using https://github.com/algolia/react-element-to-jsx-string\n. Also please note that another method is really handy when testing react component: includeJSX. This one ignore whitespace differences which can be very handy.\n. Also please note that another method is really handy when testing react component: includeJSX. This one ignore whitespace differences which can be very handy.\n. FYI, here is how I do for now\n``` js\nimport \"babel-core/register\"\nimport test from \"ava\"\nimport React, { Component } from \"react\"\nimport { createRenderer } from \"react-addons-test-utils\"\n// import ReactDOMServer from \"react-dom/server\"\nimport expect from \"expect\"\nimport expectJSX from \"expect-jsx\"\nexpect.extend(expectJSX)\nimport MyComponent from \"..\"\ntest(\"MyComponent default render\", (t) => {\n  const renderer = createRenderer()\nrenderer.render(\n    \n  )\n  expect(\n    renderer.getRenderOutput()\n  )\n  .toEqualJSX(\n    \n  )\n})\n```\nResult\n``` console\n\u276f npm run ava\n\n~@ ava ~\nava \"web_modules/*/tests/.js\"\n\n\u2716 MyComponent default render Expected '\\n  \\n' to equal ''\n1 test failed\n\nMyComponent default render\n  Error: Expected '\\n  \\n' to equal ''\n    at Object.assert [as default] (~/node_modules/expect/lib/assert.js:20:9)\n    at Expectation.toEqual (~/node_modules/expect/lib/Expectation.js:69:26)\n    at Expectation.toEqualJSX (~/node_modules/expect-jsx/index-dist.js:27:93)\n    at Test.fn (~/web_modules/Portal/tests/index.js:40:56)\n    at tryCatcher (~/node_modules/bluebird/js/release/util.js:11:23)\n    at Object.gotValue (~/node_modules/bluebird/js/release/reduce.js:145:18)\n    at Object.gotAccum (~/node_modules/bluebird/js/release/reduce.js:134:25)\n    at Object.tryCatcher (~/node_modules/bluebird/js/release/util.js:11:23)\n    at Promise._settlePromiseFromHandler (~/node_modules/bluebird/js/release/promise.js:489:31)\n    at Promise._settlePromise (~/node_modules/bluebird/js/release/promise.js:546:18)\n    at Promise._settlePromiseCtx (~/node_modules/bluebird/js/release/promise.js:583:10)\n    at Async._drainQueue (~/node_modules/bluebird/js/release/async.js:134:12)\n    at Async._drainQueues (~/node_modules/bluebird/js/release/async.js:139:10)\n    at Immediate.Async.drainQueues [as _onImmediate] (~/node_modules/bluebird/js/release/async.js:16:14)\n```\n. FYI, here is how I do for now\n\n``` js\nimport \"babel-core/register\"\nimport test from \"ava\"\nimport React, { Component } from \"react\"\nimport { createRenderer } from \"react-addons-test-utils\"\n// import ReactDOMServer from \"react-dom/server\"\nimport expect from \"expect\"\nimport expectJSX from \"expect-jsx\"\nexpect.extend(expectJSX)\nimport MyComponent from \"..\"\ntest(\"MyComponent default render\", (t) => {\n  const renderer = createRenderer()\nrenderer.render(\n    \n  )\n  expect(\n    renderer.getRenderOutput()\n  )\n  .toEqualJSX(\n    \n  )\n})\n```\nResult\n``` console\n\u276f npm run ava\n\n~@ ava ~\nava \"web_modules/*/tests/.js\"\n\n\u2716 MyComponent default render Expected '\\n  \\n' to equal ''\n1 test failed\n\nMyComponent default render\n  Error: Expected '\\n  \\n' to equal ''\n    at Object.assert [as default] (~/node_modules/expect/lib/assert.js:20:9)\n    at Expectation.toEqual (~/node_modules/expect/lib/Expectation.js:69:26)\n    at Expectation.toEqualJSX (~/node_modules/expect-jsx/index-dist.js:27:93)\n    at Test.fn (~/web_modules/Portal/tests/index.js:40:56)\n    at tryCatcher (~/node_modules/bluebird/js/release/util.js:11:23)\n    at Object.gotValue (~/node_modules/bluebird/js/release/reduce.js:145:18)\n    at Object.gotAccum (~/node_modules/bluebird/js/release/reduce.js:134:25)\n    at Object.tryCatcher (~/node_modules/bluebird/js/release/util.js:11:23)\n    at Promise._settlePromiseFromHandler (~/node_modules/bluebird/js/release/promise.js:489:31)\n    at Promise._settlePromise (~/node_modules/bluebird/js/release/promise.js:546:18)\n    at Promise._settlePromiseCtx (~/node_modules/bluebird/js/release/promise.js:583:10)\n    at Async._drainQueue (~/node_modules/bluebird/js/release/async.js:134:12)\n    at Async._drainQueues (~/node_modules/bluebird/js/release/async.js:139:10)\n    at Immediate.Async.drainQueues [as _onImmediate] (~/node_modules/bluebird/js/release/async.js:16:14)\n```\n. So why the first example is working? (Reminder I use babel 5.8 for the moment since lots of things are not working/ready for 6)\n. This is a really weird breaking change. I am using AVA to test some react (jsx yeah) and now I just cannot (=> I cannot upgrade to 0.10 and I am stuck with 0.9 until there is a solution).\n\nAre you aware that babel have an option to specify config depending on the NODE_ENV variable?\nBecause it seems that the thing that should be used here: http://babeljs.io/docs/usage/babelrc/#env-option (make sense in the use case of @JaKXz).\nAVA should use babelrc by default, otherwise UX is like superweird.\n. > the configs end up getting merged with the one AVA automatically creates(which is not always what you want).\nIs there something uncommon that AVA need?\n\nIs there a way to specify an environment on a per-file basis via the API?\n\nNo idea. But maybe process.env.BABEL_ENV/NODE_ENV can be adjusted during the runtime (because babel read from that)?\n. I have to admit that long multilines string can be a pain\n```\n  \u2716 Button render a MaterialUI  \n  t.deepEqual(actual, expected)\n              |       |      \n              |       \"\"\n              \"\"\n1 test failed\n\nButton render a MaterialUI \n  AssertionError: '<FlatButton\\n  backgroundColor=\"transparent\"\\n  disabled={false}\\n  hoverColor=\"rgb(238, 238, 238)\"\\n  label=\"Click\"\\n  labelPo === '<FlatButton\\n  backgroundColor=\"transparent\"\\n  disabled={false}\\n  hoverColor={{values: {alpha: 1, cmyk: [0, 0, 0, 7], hsl: [0\n    Test.fn (index.js:39:5)\n\n```\nAnd in the terminal, it's even worst because you cannot scroll horizontally :)\n. My 2 cents:\n- t.equal => t.is\n- t.deepEqual => t.same\n- no need for t.end() in most case\n. Also relevant #186 \n. I will be happy to handle that as soon as recent change in master are released, as well as a way to allow people to use fancy stuff like css modules in their code to test.\n. Ok, just ping me if you need a hand. I am going to write a blog post or 2 about AVA and AVA with React/webpack loaders.\n. I will (maybe even before, during post review) :)\n. I use this exact babel plugin. Enzyme seems like going backward to me. jQuery syntax seriously? expect-jsx is clearly enough and make test very simple and clear.\n. @kentcdodds totally\n@adriantoine https://github.com/sindresorhus/ava/issues/186#issuecomment-161317068\n. @kentcdodds It's slower because (for now I hope) you have to disable babel cache to don't miss update in css or any other files consumed by webpack that are not recognized by node/babel. You can subscribe to this thread if you are curious about how this will end :) \nhttps://github.com/MoOx/statinamic/issues/301\n. I don't use those. I avoid webpack whenever I can. I only use it for loaders (transformation) and try to make my code reusable/universal.\n. Sounds a bit overkill to me. I am going to release some simple helpers (around 20 lines of code) and will let you know about it.\n. I just published this small module https://github.com/MoOx/jsx-test-helpers\nCode is simple https://github.com/MoOx/jsx-test-helpers/blob/master/src/index.js\nAnd I pushed an annotated test file https://github.com/MoOx/jsx-test-helpers/blob/master/src/tests/index.js\nComments welcome.\nFor the webpack thing, I already pushed something on my js-boilerplate but I need to make a proper article about the entire setup.\n. > \"plugins\": [] //maybe we automatically append power-assert for them?\nMake sense too. It's not like you are adding tons of transformations.\n. Sure, done.\nconsole\n$ git clone https://github.com/MoOx/statinamic.git\n$ cd statinamic\n$ git reset --hard d3b265a87c9f7c484331e30b0d75ee07c66900d1\n$ npm install\n$ ava --verbose src/filename-to-url/__tests__/index.js\n$ ava --verbose src/filename-to-url/__tests__/index.js\nTo confirm, I am trying this too.\n. Well, the demo effect does not seems to work. Closing for now unless I can reproduce.\n. Ok I found the issue.\nI removed the .cache folder and now I cannot reproduce neither in my previous workspace.\n. node_modules/.cache/ava\n. Unfortunately, I kept this cache folder to try to reproduce, but it seems I cannot anymore... Sorry :/\n. @tomazzaman Maybe we should use in the example import { createRenderer } from \"react-addons-test-utils\" with const renderer = createRenderer(); renderer.render(... ? It's the prefered way to test React components at the moment.\n. FYI, I am facing this issue on multiple (all?) projects (eg: https://travis-ci.org/postcss/postcss-import/jobs/108972544#L631) poke @hzoo\n. Thanks guys! This is now ok.\n. Failures may be similar to https://github.com/MoOx/statinamic/issues/229 (or may not)\n. Maybe try BABEL_DISABLE_CACHE=1.\n. Oh. Totally sorry, I posted this issue too quickly. It's not especially related to \"empty\" files.\nWell it seems AVA have a more global issue with node@4 (at least on Travis, cause I cannot even reproduce my problem locally)...\nWill try to investigate.\n. Latest default node@4 (nvm install 4 && nvm use 4).\nI did something stupid: I just used nvm to run tests using node@4 and didn't clean node_modules. So npm@2 but with a node_modules from npm@3. Will try with a clean npm@2 install.\n. Btw, if you look correctly, it's not empty files that report failure (eg:  src/configurator/tests/index.js do have tests).\n. Not sure why but we currently are having issue with travis/push but not travis/pr, this is realllly weird...\nhttps://travis-ci.org/MoOx/statinamic/builds/117449079 vs https://travis-ci.org/MoOx/statinamic/builds/117449085\nAnd this have nothing to do with empty file, since we are now using ava.todo()\n. Nice.\n. I would add a note about how to play with webpack, loaders for assets etc by poiting users to this https://github.com/istarkov/babel-plugin-webpack-loaders#dynamic-config-path\n. I would also add a note on the second solution to let user know that this one can \"simulate\" events too ;)\n. Actually I prefer this simple method that involve nothing else than what we already have https://github.com/MoOx/jsx-test-helpers/blob/master/src/tests/index.js#L98\n. Currently, I found that the example with onClick for both enzyme or jsx-test-helpers does not provide a relevant tests: instead of testing that a callback is called, we should test that render (because that's what React is about) is indeed different that for the same render without a onClick called.\nAnd again, I also found that using TestUtils.Simulate.click is overkill according to the explanation I just gave.\nFor this reason, I think testing via calling props on the render is enough, as I am doing here https://github.com/MoOx/jsx-test-helpers/blob/ab9260513e36845c98aee897a5616926f4dbeebe/src/tests/index.js#L71-L110\nIt's very simple and much more focused on the end result, not what happen under the hood (React probably have well tested that click on elements that have onClick actually call the onClick callback).\n. You might need to use \"babel\": \"inherit\", see https://github.com/sindresorhus/ava#es2015-support\n. Why don't you add react preset in your babelrc?\n. Sorry I don't have the answer. I use a single babel config for browser and node (for tests I use babel-plugin-webpack-loaders)\n. I were not using babel webpack plugin & were using ignore-styles package.\n. about about t.plan() before and a simple t.pass as a button callback?\n. ",
    "GabiGrin": "I got this error too.\nIn my case, I saw that it was caused by padding ava an empty .js file. \nI was able to recreate from scratch by running ava via cli on a directory containing one file with a valid test and another empty file.\n. It wasn't intentional :)\nOn my first attempt to use AVA I ran it on a folder with another empty file and got into this issue. I thought that debugging and trying to fix it will be a nice way to understand how AVA works.\n. Much more elegant!\nI actually got into this issue while running AVA on directly on a folder (via gulp-ava) with other files, but that was just by mistake.\nI'll close thi. Thanks!\n. ",
    "sohamkamani": "@sindresorhus It's for branch coverage, which is metioned in the report you get after running npm test (% Branch). \nAlso, if you, for example comment this line out, the other tests still pass. The new unit tests handle those cases too. \n. ",
    "tobbbles": "My bad, there was a stray babel package in node_modules\n. @scarletsky \nGo into your own node_modules and remove babel packages, then remove them from your package.json. After that, npm install - -save-dev ava should get it working. \nI think there may still be an issue with ava using local babel packages, I'll do some bug testing tonight and see if that's the case. \n. ",
    "scarletsky": "@mnzt \nafter removing babel packages with rm -rf node_modules/babel*, I run tests again.\njs\n// test.js\nimport test from 'ava'\ntest('foo', t => {\n    t.pass();\n    t.end()\n});\nshell\n$ ava test.js\n``` shell\nUncaught Exception:  /Users/scarlex/Projects/v1/test.js\n  TypeError: (0 , _ava2.default) is not a function\n    at Object. (/Users/scarlex/Projects/v1/test.js:9:22)\nUncaught Exception:  /Users/scarlex/Projects/v1/test.js\n  TypeError: (0 , _ava2.default) is not a function\n    at Object. (/Users/scarlex/Projects/v1/test.js:9:22)\nError: Never got test results from: /Users/scarlex/Projects/v1/test.js\n    at ChildProcess. (/usr/local/lib/node_modules/ava/lib/fork.js:61:12)\n    at emitTwo (events.js:87:13)\n    at ChildProcess.emit (events.js:172:7)\n    at Process.ChildProcess._handle.onexit (internal/child_process.js:200:12)\nFrom previous event:\n    at module.exports (/usr/local/lib/node_modules/ava/lib/fork.js:25:16)\n    at run (/usr/local/lib/node_modules/ava/cli.js:120:9)\n    at Array.map (native)\n    at /usr/local/lib/node_modules/ava/cli.js:206:22\nFrom previous event:\n    at init (/usr/local/lib/node_modules/ava/cli.js:198:4)\n    at Object. (/usr/local/lib/node_modules/ava/cli.js:243:2)\n    at Module._compile (module.js:435:26)\n    at Object.Module._extensions..js (module.js:442:10)\n    at Module.load (module.js:356:32)\n    at Function.Module._load (module.js:311:12)\n    at Function.Module.runMain (module.js:467:10)\n    at startup (node.js:136:18)\n    at node.js:963:3\n```\n. No, in fact, I am using 0.5.0...\n. Hello @adriantoine , I follow your react recipes with some problem.\nIt works as expected when your component is in the test file. But when you require the component from another file (some thing like import Foo from './Foo'), it will throw errors like https://github.com/sindresorhus/ava/issues/458.\nAny guides ?\n. @MoOx \n Yes, \"babel\": \"inherit\" works when I add \"react\" to my .babelrc file.\nBut I want to know why my config do not work.\n``` js\n// .babelrc\n{\n  \"presets\": [\"es2015\", \"stage-1\"],\n  \"plugins\": [\n    \"transform-decorators-legacy\"\n  ],\n  \"env\": {\n    \"development\": {\n      \"presets\": [\"react-hmre\"]\n    }\n  }\n}\n// package.json\n{\n  \"ava\": {\n    \"verbose\": true,\n    \"files\": [\n      \"tests/client/App.js\"\n    ],\n    \"require\": [\n      \"babel-register\",\n      \"babel-polyfill\"\n    ],\n    \"babel\": {\n      \"babelrc\": true,\n      \"presets\": [\"react\"]\n    }\n  }\n}\n```\nFrom the docs:\n\nWhen specifying the Babel config for your tests, you can set the babelrc option to true. This will merge the specified plugins with those from your babelrc.\n\nAccording to my understanding, my config should work too.\nDo I miss anything ?\n. @MoOx Because I use babel in server side. In client side, I do something like:\njs\n// webpack.config.js\nvar babelrc = JSON.parse(\n  fs.readFileSync('./.babelrc', 'utf8')\n)\n// .....\n{\n  test: /\\.js$/,\n  exclude: /node_modules/,\n  loader: 'babel',\n  query: Object.assign(\n    {},\n    babelrc,\n    {\n      presets: babelrc.presets.concat('react'),\n      cacheDirectory: true\n    }\n  )\n}\nThis works very well.\n. @MoOx Can you tell me why my prev config do not work ?\n. ",
    "hellsan631": "@scarletsky I had problem (TypeError: (0 , _ava2.default) is not a function) as well and was tinkering around trying to fix. I deleted my node_modules folder and accidentally ran ava and it worked.\nApparently if you have a local copy of ava and a global copy of ava, it gives you the error you had above. So deleting your local copy of ava and running ava might get it to work.\n. I wasn't using babel v6 when the error happened.\nTypeError: (0 , _ava2.default) is not a function \nStill using babel v5, both locally and globally.\n. I don't think so. I am on windows, but I have cygwin installed, so grep still works.\nHere are my versions\nNPM: 3.3.12\nNode: 4.2.2\n$ npm ls | grep babel\nBrought up nothing in the local repo. Going for global:\n```\n$ npm ls -g | grep babel\n\u2502 \u251c\u2500\u252c babel-core@5.8.34\n\u2502 \u251c\u2500\u252c babel-plugin-espower@1.1.0\n\u2502 \u251c\u2500\u2500 babel-runtime@5.8.34\n\u2502 \u251c\u2500\u252c babelify@6.4.0\n```\nbabelify@6.4.0 didn't get babel 6 until babelify@7.0.0 \n. Yep. It works when its installed only globally. \nWhen ava is installed only locally, the ava command doesn't work.\nWhen ava is installed only globally, ava works fine\nHowever in the guide, you recommend one of the first steps being ava --init, which installed ava as a dev-dependency. So ava is installed then both globally and locally. ava then triggers the type error above when I try and run it\n. I get various errors when trying to run it locally using the path to bin instead.\nSyntaxError: Unexpected token case\n...\nSyntaxError: Unexpected token ILLEGAL\nTried the cache clean and still no dice. My full error is here as follows (the same as scarletsky above)\n```\n$ ava\nUncaught Exception:  C:\\Users\\HellsAn631\\Zend\\workspaces\\Pippo\\test\\reader.test.js                          \n  TypeError: (0 , _ava2.default) is not a function                                                            \nUncaught Exception:  C:\\Users\\HellsAn631\\Zend\\workspaces\\Pippo\\test\\reader.test.js                          \n  TypeError: (0 , _ava2.default) is not a function                                                            \nError: Never got test results from: C:\\Users\\HellsAn631\\Zend\\workspaces\\Pippo\\test\\reader.test.js           \n    at ChildProcess. (C:\\Users\\HellsAn631\\AppData\\Roaming\\npm\\node_modules\\ava\\lib\\fork.js:61:12)\n    at emitTwo (events.js:87:13)                                                                            \n    at ChildProcess.emit (events.js:172:7)                                                                  \n    at Process.ChildProcess._handle.onexit (internal/child_process.js:200:12)                               \nFrom previous event:                                                                                        \n    at module.exports (C:\\Users\\HellsAn631\\AppData\\Roaming\\npm\\node_modules\\ava\\lib\\fork.js:25:16)          \n    at run (C:\\Users\\HellsAn631\\AppData\\Roaming\\npm\\node_modules\\ava\\cli.js:120:9)                          \n    at Array.map (native)                                                                                   \n    at C:\\Users\\HellsAn631\\AppData\\Roaming\\npm\\node_modules\\ava\\cli.js:206:22                               \nFrom previous event:                                                                                        \n    at init (C:\\Users\\HellsAn631\\AppData\\Roaming\\npm\\node_modules\\ava\\cli.js:198:4)                         \n    at Object. (C:\\Users\\HellsAn631\\AppData\\Roaming\\npm\\node_modules\\ava\\cli.js:243:2)           \n    at Module._compile (module.js:435:26)                                                                   \n    at Object.Module._extensions..js (module.js:442:10)                                                     \n    at Module.load (module.js:356:32)                                                                       \n    at Function.Module._load (module.js:311:12)                                                             \n    at Function.Module.runMain (module.js:467:10)                                                           \n    at startup (node.js:136:18)                                                                             \n    at node.js:963:3                                                                                        \n.\n./node_modules/.bin/ava\n```\nWell, that worked. hmm. Weird, because just a second ago i tried it and got some syntax erros (listed above). but now, after uninstalling it and reinstalling it it works\n. ",
    "mariuslundgard": "Same error here. This package doesn't seem to be compatible with babel v6.\n. ",
    "joshuajabbour": "Really wish the readme clearly stated ava doesn't work with babel 6. I use babel 6 to write my app with es6, but that means it's impossible to use ava to test my app until this issue is resolved. Spent way too long trying to figure this out.\nMaybe I'm not understanding things, but this really seems flawed. So if I want to use ava, I'll have to make sure the version of babel used by ava stays in sync with the version I use for my app? \n. @mnzt Does it matter? Both give me the same error, the one you posted in the original issue.\n. @mnzt To be more clear, my issue is definitely that I have babel 6 installed and ava doesn't understand how to use it. It sounds like a flaw in the design of ava, because I'm using babel 6 to write my application code. ava should be able to use whatever babel I prefer to parse the test files.\n. ",
    "chiefy": "Wanted to switch from tape to ava, but I am locked into babel 6 :+1: \n. ",
    "jankuca": "A quick Google query popped out a few examples:\n1) http://backbone-testing.com/\n\nChapter 1: Setting up a Test Infrastructure\n- Trying out the test libraries: Some first basic unit tests using Mocha, Chai, and SinonJS.\n  - hello.spec.js\n- Test Failures: Different types of test failures.\n  - failure.spec.js\n- Test Timing: Tests that take different times, which Mocha annotates for \"medium\" and \"slow\" tests. Also has one test timeout failure.\n  - timing.spec.js\n\n2) http://blog.revolunet.com/blog/2013/12/05/unit-testing-angularjs-directive/\n\nHere\u2019s our example \u201cfiles\u201d section :\nfiles: [\n  ...\n  \"src/angular-stepper.js\",                           <-- our component source code\n  \"src/angular-stepper.spec.js\"                       <-- our component test suite\n]\n\n3) https://review.openstack.org/#/c/184543/28/openstack_dashboard/static/openstack-service-api/common-test.spec.js,unified\n4) The latter (whateverSpec.js) convention is used by jasmine-node for instance \u2013 https://github.com/mhevery/jasmine-node\n\nNote: your specification files must be named as spec.js, spec.coffee or spec.litcoffee, which matches the regular expression /spec.(js|coffee|litcoffee)$/i; otherwise jasmine-node won't find them! For example, sampleSpecs.js is wrong, sampleSpec.js is right.\n. > And where do you keep these *.spec.js files? In a test folder? In root?\n\nI personally keep them in the test/\u00a0folder, yes.\n. > \u2026when you have both the test and the actual code in the same directory\nYes, I've seen that a couple of times and even tried it. I've come to the conclusion that it is more practical to have spec files in a separate top-level folder.\nThe real reason I keep using the suffix is mostly because it is possible to determine the type of a file (implementation/spec/mock) from its basename.\n\n. > Aren't your test files run?\nNo, they are, everything's working fine. Only the result prefixes are wrong:\n\n. Thanks!\nThe reason I've put the if there was to handle a single spec file naming convention at once.\n. ",
    "Lokua": "Both are version 0.4.2, and both of the above commands run successfully, however npm test still errors at the end - likewise if I use ./node_modules/.bin/ava test/**/*.test.js as the npm test script. An odd thing I just noticed is that npm test does pass every so often - about 1/4 runs, which is even more confusing. \n. I saw the empty files issue, I don't think this is the same (no empty *.test.js files, though there are non test files in the same folder). You can see my current test setup here (https://github.com/Lokua/o-/tree/postcss).\n. Same thing. Global and local ava cli calls work fine, only from npm test does the same command error. I'm on Windows 10 btw. Perhaps that or npm has something to do with it. I noticed that putting the ava call in a shell script and running that from npm test also produced the same error, so yeah, I'm beginning to think this isn't really an ava specific issue. In any case I'm totally fine running ava from cli, just thought it was worth bringing up.\n``` sh\n\nnode -v\nv5.0.0\nnpm -v\n3.3.6\nava --version\n0.4.2\n./node_modules/.bin/ava --version\n0.4.2\n```\n. Awesomeness. Thank you.\n. I'm still getting one these traces for every test in 0.10\n\nWarning: Promise.defer is deprecated and will be removed in a future version. Use new Promise instead.\n    at Test.run (/home/puz/lnet/lokua.net.route/node_modules/ava/lib/test.js:87:25)\n    at Runner._runTest (/home/puz/lnet/lokua.net.route/node_modules/ava/lib/runner.js:127:14)\n    at Runner.<anonymous> (/home/puz/lnet/lokua.net.route/node_modules/ava/lib/runner.js:114:15)\nFrom previous event:\n    at eachSeries (/home/puz/lnet/lokua.net.route/node_modules/ava/lib/runner.js:16:17)\n    at Runner._runTestWithHooks (/home/puz/lnet/lokua.net.route/node_modules/ava/lib/runner.js:104:9)\n    at Array.map (native)\n    at each (/home/puz/lnet/lokua.net.route/node_modules/ava/lib/runner.js:12:27)\n    at Runner._runConcurrent (/home/puz/lnet/lokua.net.route/node_modules/ava/lib/runner.js:137:9)\n    at /home/puz/lnet/lokua.net.route/node_modules/ava/lib/runner.js:204:16\n    at processImmediate [as _immediateCallback] (timers.js:383:17)\nFrom previous event:\n    at Runner.run (/home/puz/lnet/lokua.net.route/node_modules/ava/lib/runner.js:203:4)\n    at process.<anonymous> (/home/puz/lnet/lokua.net.route/node_modules/ava/index.js:85:10)\n    at emitOne (events.js:77:13)\n    at process.emit (events.js:169:7)\n    at process.<anonymous> (/home/puz/lnet/lokua.net.route/node_modules/ava/lib/test-worker.js:102:10)\n    at emitTwo (events.js:87:13)\n    at process.emit (events.js:172:7)\n    at handleMessage (internal/child_process.js:689:10)\n    at Pipe.channel.onread (internal/child_process.js:440:11)\nWarning: a promise was created in a handler but was not returned from it\n    at processImmediate [as _immediateCallback] (timers.js:383:17)\n. ",
    "rightaway": "@ArtemGovorov Well put. Similar use case here.\n\nplease consider not doing it the way it's done in tape. The way it's done there (nested tests) makes it impossible to run a nested test without running an outer one.\n\nRight, ideally just the selected test and only its associated before/after/beforeEach/afterEach hooks (at its own level and parent levels in the nesting) would run.\n. My vote is actually to have two of the suggested options rather than just one (both option 1 and option 2 from the options listed in https://github.com/avajs/ava/issues/222#issuecomment-250399133).\nI think option 1 is almost necessary as many people really like using it and think of their tests most naturally in a nested way, and it will make it easier for people to migrate to ava. And option 2 is great for people who don't like nesting and want something different, modern, and 'cleaner' than what's already out there in various test frameworks.\nIn fact I can see most people starting off with option 1 when they migrate to ava, and eventually switching to option 2 as they adopt the new paradigm. In fact, this is also my own intention with our projects.\nI think limiting the solution to just one of the choices would leave a large segment of users feeling like they're constrained by the test framework. Which I think is a big part of why so many of us immediately took a liking to ava, coming from other frameworks. But having both of these would be a great solution. I'm not sure how feasible it is from a coding perspective to make that happen, but in my opinion it would be the way to go.. Ok. Now if I have a file test/test1.js and test/subtest/test2.js (each with one test called runTest in it) and run ava test/**/*.js (glob pattern in ZSH) on the command line, the output looks like this\ntest2 > runTest\ntest > test1 > runTest\nBut it should look more like\ntest1 > runTest\nsubtest > test2 > runTest\nAt least while nested groups aren't available (https://github.com/sindresorhus/ava/issues/222) using directories is a good way to organize structure, so the output should be in line with that.\nThe beginning test doesn't appear above since that's the 'root' test folder, but if there's no option to specify a directory and a recursive flag then I don't see how it would know that test shouldn't be printed there.\nThat's why I think allowing to specify a directory with a recursive flag (so it's not the default and not a breaking change) would be useful, because then ava can format the results appropriately. Because using glob patterns ava just receives a list of filenames to test and isn't aware of a 'root'.\n. A suggestion of how it could look. After seeing all the files passed in with the glob pattern it should strip the leading directories that are all in common. The way if someone passes in a pattern like some/sub/directory/**/*.js, and the only files that appear are some/sub/directory/sub/path/test1.js and some/sub/directory/sub/path/test2.js, then the part in common some/sub/directory/sub/path should not appear in the output.\n. Now it looks like this and has resolved that problem. But I'm getting ReferenceError: regeneratorRuntime is not defined. Seems like I need babel-polyfill to solve this? But where should I define this in package.json or elsewhere?\n\"ava\": {\n    \"babel\": \"inherit\",\n    \"require\": [\"babel-register\"]\n  },\n  \"babel\": {\n    \"presets\": [\n      \"es2015\",\n      \"stage-0\"\n    ]\n  },\n. Now it looks like this and has resolved that problem. But I'm getting ReferenceError: regeneratorRuntime is not defined. Seems like I need babel-polyfill to solve this? But where should I define this in package.json or elsewhere?\n\"ava\": {\n    \"babel\": \"inherit\",\n    \"require\": [\"babel-register\"]\n  },\n  \"babel\": {\n    \"presets\": [\n      \"es2015\",\n      \"stage-0\"\n    ]\n  },\n. Have just tried it but I get the same error. Any other ideas?\n. Have just tried it but I get the same error. Any other ideas?\n. @spudly works, thank you. Just out of curiosity, it seems like babel-register and babel-polyfill were already in my node_modules folder (probably as dependencies of other babel stuff). Is it good practice to explicitly declare both of these under devDependencies in package.json, or should I just leave it as is?\n. That would be great to have a new object. Is Object.create(context) currently being used for context in beforeEach hooks?. > I don't think we need to worry about deeply nested values.\nHaving t.context contain deeply nested plain objects and arrays would still be fine though?. ",
    "matthewbauer": "I'm thinking of an implementation like this:\njs\ntest.group(function(test) {\n  test(\"test a\", function() {\n  });\n  test.beforeEach(\"before test a but not test b\", function() {\n  });\n});\ntest(\"test b\", function() {});\nBasically test.group just generates another runner that is added into the Runner.prototype.tests array. All of the hooks work as expected, just with the new test in context.\n. > Well... not on the prototype hopefully. :smile:\nWoops I was thinking of that backwards. I meant \"runner.tests\" where runner is an instance of Runner.\n\nBut yes, that is basically what is being discussed here. You should definitely hold off on creating a PR >until:\n466 lands\nWe get consensus here on the API (it's still not settled that we actually want to add it).\n\nDefinitely.\nThe big thing with this is that without some way to group tests there are a whole class of tests that you can't really work with. For my use case:\nI have an emulator that needs to be tested. I have written common tests that I want to run for each ROM in my fixtures. I don't want to have to write a test file for each ROM because they should all have the same tests (render 50 frames, check video size, audio checks...) and I'd just be doing the same thing over and over. But I also need for a fresh ROM to be loaded for each individual test. So I need some way to make the \"beforeEach\" run differently depending on which ROM I'm testing.\n. I mean to be honest for my use case all that's needed is just to search for a string literal in the titles. I guess I just wanted to make sure it's versatile for different use cases, although globs would be more intuitive (and match the globs in the file args).\n. For now, my PR is just going to do --match just because RegEx/glob matching is fairly expensive when done on large test suites. Since titles are pretty easy to change I think that should cover most if not all use cases.\n. So here are the use cases for this flag as I see them:\n- Run a single test from the CLI.\n- To run only parts of a single large test file that for whatever reason can't be split up (tests are generated based on a JSON file for instance).\n- To skip tests when incompatible with certain environments (for instance skip tests that sudo in a container environment).\nI'd be interested to see if there are any other possible use cases.\nAdditionally, I'd like to make it versatile enough so that it's not necessarily dependent on the CLI flag so it could work in a web context.\n. The main reason I'm hesitant with globs is that at least for me they only make since when you're dealing with file paths. I've certainly seen them used elsewhere but they can lead to some issues if you aren't aware of the file path association. For instance:\njs\ntest(\"test group 1: 4 - 2 = 2\", function(t){})\ntest(\"test group 1: 2 + 2 = 4\", function(t){})\ntest(\"test group 1: 4 / 2 = 2\", function(t){})\nYou might expect for --glob 'test group 1: *' to run all of them but the '/' would throw it off. I suppose --glob 'test group 1: **' would resolve it though.\n. > Maybe we should combine the test name with the file path\nI wasn't really considering multiple test files at this point. If you only wanted it to match tests/foo/bar.js couldn't you just modify your path glob? I suppose this would enable some even finer control over what's being run though: for instance, run all tests in tests/foo/bar/** and any other tests with the words \"foo\" or \"bar\" in them. I don't know if anyone would ever use that but kind of a neat feature.\n. I'm thinking there should be some way to selectively run tests on based from the CLI. For me, travis times out if I run the whole test file.\nAn alternative would be to let test scripts read command line flags and then skip or run a test based on its own rules. This might be a more KISS approach. A helper function could be defined like:\njs\nimport test from 'ava'\nfunction _test(title, fn) {\n  if (title.indexOf(test.flags.match) !== -1) {\n    test(title, fn);\n  } else {\n    test.skip(title, fn);\n  }\n}\n. this should be return new Test(title, fn, contextRef, report);\n. ",
    "axyz": "Probably nested tests may be useful in some situations, however I fear they may be a problem for parallelism and in general they introduce dependencies and context that start to get quite hidden and cryptical as the test suite grows.\nBefore implementing test groups I think it would be really helpful to collect some opinionated alternatives to them that may encourage context-free and parallel-friendly tests.\ne.g. if you have this entity Project and need to test different things when it is initialized, configured or started... you may have a factory function like this:\n```\nfunction getProjectMock(opts) {\n  const project = new Project();\n  opts.configure && project.configure();\n  opts.start && project.start();\nreturn project;\n}\n```\nThis way the only repetition you have is calling the factory function that in my opinion is a \"good repetition\" as it somehow document the actual mocked object you are using in each test.\nBeside that you always have a freshly created Project instance without the risk of sharing unwanted internal states.\nSaid that I strongly believe that enforcing pure functions and avoid context sharing on Object methods is usually a good thing, but of course it is not always the case and you may have existing codebase that use a lot of shared globals or being in need of some complex transactions that may be easier to express using a shared state... so groups may still be a useful feature, but to be somehow \"discouraged\" showing possible more idiomatic alternatives for various common cases.\n. +1 for @spudly proposal\nalso just to have a clarification, react won't be included on default \".avarc\" ? if so, why? To not have the babel profile as a dependency?\nI'm considering ava for a new project and at the moment the jsx problem is the only concern I have, so I'm glad that everybody agree on this proposal.\n. +1 for @jamestalmage \nI was thinking the same thing: extending with possible overrides would allow to add needed plugins or presets without needing to worry about keeping the rest in sync with ava defaults.\n. ",
    "ianstormtaylor": "Just wanted to leave a random thought here for future API implementation: In Mocha, it's nice that you can opt not to define an its testing implementation and it will show up as a placeholder:\nit('should do something i have not implemented yet')\nBut it's frustrating that you can't do the same thing for a describe call. If the groups implementation ends up looking like some of these examples, it would be nice is groups too can have their implementation functions omitted and show up as placeholders.\n. ",
    "halhenke": "I've always liked the nested describe/it style as much for the reporting style it enables as much as anything else i.e.\nComponentA:\n  does this\n  does that\n  when its raining:\n    does something else\ncant imagine this is considered valuable by many but it just seemed to make testing more meaningful to me.\n. @novemberborn Hey. I have to say i didn't find the examples from the provided link to be very convincing in terms of nested (I'd probably prefer the term \"grouped\") tests as an anti-pattern. For whatever reason the Mocha style code had big messy comments that line-wrap & generally help to try and make it look more confusing than the other example. If anything it seemed to be an argument against the dangers of beforeEach/afterEach use/abuse.\nCan't really comment on the test execution thing. I guess my comment mostly stemmed from the fact that the Mocha/RSpec file seems to provide a nice way to organise/describe what you are testing & from what I've seen of Tape/AVA the alternative is to shove more info into the title string. It may be a very superficial thing and/or maybe just poor usage on my/others part but it is something that i miss.\n. @novemberborn Yeah to be honest I'm a little ashamed to say that I only glanced at ava-spec before but it looks like it addresses what I'm talking about really well. With a reporter that generates similar output to the documentation format flag in RSpec i'd be very happy \ud83d\ude03 \n. ",
    "c0b": "hi, since this issue created last year, is there any implementation ready for preview?\nI'm in a project want to switch testcases to ava (from mocha) to write new cases, but all existing test files are written in mocha describe it style well organized in a hierarchy, looks like will be blocked by this #222; wonder is there a simple drop in replacement that doesn't require me to rewrite all describe it ?\n. ",
    "Overdrivr": "I pretty much agree with @ORESoftware \nI used mocha in a previous to test a loopback server application, and I looked at AVA mainly for the \"each test (suite) runs in its own process\", because properly clearing up the server/npm cache between test suites was starting to be a nightmare.  \nSo for this AVA is a big winner, however, the lack of nested tests or at least hierarchical ordering of information inside the test report is a bigdownside.\nI find this\nSome API model\n   Doing POST /foo should do this\n   Doing DELETE /foo should do that\n   etc\nmuch clearer than\nSome API model Doing POST /foo should do this\nSome API model Doing DELETE /foo should do that\netc\nI don't think it's possible to get the first output with ava-spec, can you confirm @sheerun ?\nOther than that, AVA looks close to perfect so I do hope some solution can be found\n. ",
    "gravitypersists": "For me, I really like having tests aligned with one assertion. So something like:\njavascript\ntest.group('When modifying something in particular', t => {\n  t.context.store.dispatch(someMutatingAction())\n  test('x becomes true', t => t.true(t.context.store.getState().x))\n  test('y becomes 1', t => t.is(t.context.store.getState().y, 1))\n}\nis far less verbose than something like:\n``` javascript\nconst doMutatingAction = t => t.context.store.dispatch(someMutatingAction())\ntest('When modifying something in particular x becomes true', t => {\n  doMutatingAction(t)\n  t.true(t.context.store.getState().x))\n})\ntest('When modifying something in particular y becomes 1', t => {\n  doMutatingAction(t)\n  t.is(t.context.store.getState().y, 1)\n})\n```\nAnd I don't want to use .before here of course because I don't want this mutation applied to all my tests in the file, and I'd also rather not create many different files for something that is pretty well contained (such as testing a single action or reducer, as in that case).\nAnd as others have mentioned, I also enjoy the ability to setup a context for a subset of tests without resorting to different files, as there's definitely cognitive overhead involved in file switching.\nI think it all ultimately comes down to the need to pass along a context, which can only be adjusted with before hooks (or by using globals for static things), which can only be declared once per module, a seemingly arbitrary constraint. Regardless of whether the nesting was used as an \"antipattern\" or not, grouping will add needed expressivity to this library.\n. ",
    "fearphage": "I think the best thing about the first solution is the nesting and it's familiarity. It's a visual cue that you're going deeper and adding complexity. I disagree with your con. I feel like that makes it more difficult to miss tests.\nI like the flatness of .fork and the ability to compose tests with .group, but I feel these could be much harder to see visually and reason about in the future. I also think they make it easier to hide code but not in a desirable way.\n\nWe're writing code for developers to read. It's secondary that computers can interpret it.\n\nIf I were to open a mocha test file right now, I could find every context that's nested 3 levels deep in my editor /^\\s{6}describe/ and visually. There's nothing distinct about .fork or .group to highlight that complexity. I could potentially have helpers that fork that I wouldn't even see in the same file. It feels like it allows things to get overly complex easily but not in an obvious way. When I'm 10 levels  of nested describes deep, you know it. You feel it. (You're probably crying by then.) I could fork 10 times in a helper and then 5 more times in my test and it may not be obvious the person reading my code. I could see me having to open up several files just to figure out what the setup for one test looks like.\nNested tests feels a bit like safety rails/guards. Even if you don't describe your test setup well, I can still usually take it all in one file and reason about it. The other 2 flat solutions feel a bit footgun-y to me. They open up a lot of options, but with that comes a lot of responsibility. They place the onus on the developer to name things well and be very descriptive: updateFailedAndBeforeRetryRequest.test vs complexSetup.test(...). Even the \"well named\" solution in my example feels bad to me. That's just my 2 cents.\n. While we're sharing usage, my describe usage is more about state management and steps in a process. This is the pseudo code version of many of our tests.\n```\ndescribe('upload view') {\n  beforeEach('create view + fake server')\n  it('should make a request on submit');\ndescribe('after submit') {\n    beforeEach('submit upload form');\nit('should handle failure');\nit('should handle success');\n\ndescribe('when upload fails') {\n  beforeEach('respond with failure from fake server')\n\n  it('should retry');\n\ndescribe('when upload succeeds') {\n  beforeEach('respond with 201 from fake server')\n\n  it('should redirect to new view');\n\n```\nAs tests become more deeply nested, they are adding new steps to the process building on the previous state. That's why the first suggestion resonates so much with me. It's very visual like mocha tests. It's not trying to hide/obscure the complexity.. While we're sharing usage, my describe usage is more about state management and steps in a process. This is the pseudo code version of many of our tests.\n```\ndescribe('upload view') {\n  beforeEach('create view + fake server')\n  it('should make a request on submit');\ndescribe('after submit') {\n    beforeEach('submit upload form');\nit('should handle failure');\nit('should handle success');\n\ndescribe('when upload fails') {\n  beforeEach('respond with failure from fake server')\n\n  it('should retry');\n\ndescribe('when upload succeeds') {\n  beforeEach('respond with 201 from fake server')\n\n  it('should redirect to new view');\n\n```\nAs tests become more deeply nested, they are adding new steps to the process building on the previous state. That's why the first suggestion resonates so much with me. It's very visual like mocha tests. It's not trying to hide/obscure the complexity.. > Test run times are highly impacted by background tasks, our caching mechanisms, concurrency, etc. It would be a fairly useless metric.\nYour statement applies to individual test timings as well. Are those an equally useless metric? Should I submit a PR to remove them?\n. Why not feature test it instead? \njs\nvar supportsFunctionNames = (function testing() {}).name === 'testing';\nThis has the benefit of working in every environment consistently. Imagine if this feature were to be broken in v6.6 or removed in the future, the tests would always know how to do the right thing. This would make the tests less brittle.\n. Restarting tests.\n. Test failures are due to a change in Node v6.5 as outlined in #1028.\n. Picture and text format:\n\n``` shell\n\u279c  ava git:(add-test-run-duration) node cli.js --watch test/fixture/watcher/tap-in-conf/test.js test/fixture/watcher/tap-in-conf                                                                                                                               \n1 passed [16:21:22] (598.783ms)\n  1 failed\ntest \u203a fails\n  Test failed via t.fail()\n    Test.fn (test.js:8:4)\n    _combinedTickCallback (internal/process/next_tick.js:67:7)\n    process._tickCallback (internal/process/next_tick.js:98:9)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n1 passed [16:21:22] (235.709ms)\n  1 failed\ntest \u203a fails\n  Test failed via t.fail()\n    Test.fn (test.js:8:4)\n    _combinedTickCallback (internal/process/next_tick.js:67:7)\n    process._tickCallback (internal/process/next_tick.js:98:9)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n1 passed [16:21:32] (209.869ms)\n^C%                                                                                                                                                                                                                                                            \u279c  ava git:(add-test-run-duration) node cli.js test/fixture/pkg-conf/defaults                                                                                                                                                                                  \n1 passed (199.979ms)\n\u279c  ava git:(add-test-run-duration) node cli.js test/fixture/long-running.js          \n2 passed (5.202s)\n```\n. > What problem does it solve?\nIt shows you the total duration of a test run. \n\nWhy not just $ time ava when you want to know the total test run time?\n- How do you use that with --watch exactly? \n- time works horribly in windows (read: not at all).\n\nAnother problem with time is it includes the timings of the scaffolding and setup for ava itself as well as any tear down or waiting for file descriptors to close or whatever the case may be. I've tried to make this time measurement as close to the tests themselves running and finishing.\n. > That's the solution, not the problem.\nThe problem then is prior to this PR, I had no idea how long all the tests took to run together. There was information that is easily captured that was not being presented to me.\n\nWhat will you do with the knowledge of the time it takes to run the test suite?\n\nI don't fully understand what you're asking. I will do the same thing I do with the knowledge of the time it takes to run individual tests. That's currently a feature in ava today right now. Do you have a problem with that functionality as well? What's the difference between individual test times and suite run times?\n\nWhy do you need it on every run?\n\nIt's beneficial to be able to see at a glance the length of time a test suite takes to run. \nI have a few questions for you. Why would you not want to see how long your tests take to run? What is it detracting from your code, tests, and/or life? \n. > You can't do much about the total time unless you know the time of the individual tests.\nFair enough. I feel like can do something with the deltas though. If before PR X, tests ran in 1 minute faster, that's a big deal.\nAlso do you want me to add duration to the other reporters?\n. > no point in having it in watch mode\nWhat about adding an option for it? It's inconvenient to have to leave watch mode just to see how long the tests are taking.\nWhat's the point of printing the timestamp in watch mode? Is it so you can see that things are changing at a glance? If that is the sole purpose, could this replace the timestamp? It's more useful in terms of testing than showing the time.\n. > @novemberborn already commented why showing total test time in watch mode is not useful.\nUnless I'm missing it, @novemberborn only said s/he didn't find it useful. S/he did not elaborate on the why. Much like novermberborn, I have an opinion too. I believe it is useful. That's the reason I added it and the reason I expressed how inconvenient it would be to have to leave watch mode to figure out how long the tests were taking to run. I'm trying to see if there's a compromise that would allow it to remain in some form or another.\n\nthe fact that the test time could be the same as previous  \n\nThis is a possibility. The lack of precision in the timing adds to that factor. Hitting the same millisecond is definitely possible. The same nanosecond is less probable.\n\nand you would have to remember the previous test time to be sure it's different.\n\nHow does this differ from printing the timestamp? By printing the time, you have to remember the time it displayed last time you saw it or know what time it is now including seconds. Neither one of them let you know the tests just ran without using your memory or external factors.\n. > objectively useless\nI have a use for this \"objectively useless\" thing.\n\nEven if we used nanosecond precision we would only show a prettified output to the user. Let's say a test takes 1.4243242 seconds to run the first time and 1.45435353 the second time, we would only show 1.4 second for each.\n\nI had it setup for 3 decimal places so the output in your examples would be 1.424s and 1.454s respectively I believe.\n. > pretty-ms converts it to 1.4, which is what we want.\n\nthe fact that the test time could be the same as previous\nThis has nothing to do with precision.\n\npretty-ms in its current configuration increases the likelihood of repeated output. You cited repetitious output as a problem. You also said this has nothing to do with precision. However as you increase the precision, repeated output becomes less likely. So it directly addresses one of the problems that you cited. I don't understand why you're saying it has nothing to do with this.\n\nEither you're not understanding how it's useless or you have a use-case you haven't being able to explain\n\nReally?! Since I already explained the use case a few weeks ago, I guess it's the former in this false dichotomy. Are you going to elaborate on the uselessness of my use case? Help me understand how useless it really is. \n. > I do realize more precise output would make one of my arguments less relevant, but that's irrelevant as it's not something we want.\nSo it seems you do see what precision has to do with this. You prefer the opportunity for repetition more than you want precision. I understand. It seems unfair to complain about a situation that you requested.\n\nwatch mode only runs tests that changed or had their source change\n\nThat's precisely what I want. Imagine I'm writing useless-use-case.js and I'm writing a test and then writing the code to make the test pass. Then I refactor and my test time increases 30 seconds. That's a good time to revert the refactor, find the bug, or at a minimum mark the test as slow to remove it from the tdd flow. Do you understand now or is this equally useless in your objective opinion? \n. > I'm sceptical whether it'll actually deliver the information as reliably as you're expecting it to\nUnderstandably so. I've been using it currently and it's delivering the expected results thus far.\nDoes ava support external reporters currently?\n. The current implementation seems to almost match your proposal. t.throws(fn, type, message) doesn't work (0.16.0). I can't tell if this is a regression or a mix up in the documentation. I went back 10 or so commits and never saw where this worked. There are no tests for this format either.\nI was coming here to file that bug report and submit the PR, but I won't if you're planning to remove it altogether. \n. > > Is there a need for t.throws(fn, SyntaxError, 'string') and t.throws(fn, SyntaxError, /regexp/)\n\nLeaning towards not supporting this.\n\nIs it difficult to code or reason about? Just a preference? I'm just curious.\nIf we're target an arity of 2, what about this as an option?\njs\nt.throws(fn, error => {\n  // insert error assertions\n});\nI like the encapsulation of the assertions related to the thrown error.\n. Is this suggestion due to process not being supported in the browser or is there another reason? I'm just asking, because I'm curious.\nMy rationale for choosing hrtime over Date was providing the most amount of information to the reporters. One can always get milliseconds from nanoseconds, but the reverse is not true.\n. Ping?\n. Updated.\n\nThat said I don't think there's much value in tracking test duration when in watch mode, especially since the number of test files run can vary greatly.\n\nIn that case, it's not very useful. When you're doing TDD or just updating a single test in general, it makes a lot more sense.\n. ",
    "Akkuma": "My vote is for 2 or 5. Flatness wins to me, otherwise we'd never see async/await.\n. I implemented what I think is the easiest and cleanest approach without any pre-compile step as webpack-loaders seems like a dirty hack to begin with that didn't even work for me outside of using null transformer/loader for webpack. I use node-hook-filename to handle specific extensions and compile my pug files into functions as if it were handled by webpack. The scss files are inconsequential to testing, so I let it return the default, which is just a path.\n``` js\nimport { join, dirname } from 'path';\nimport nhf from 'node-hook-filename';\nimport pug from 'pug';\nnhf(['.scss']);\nnhf(['.pug'], (path, parent) => pug.compileFile(join(dirname(parent.id), path)));\n```\n. To defend browser-env. I feel it does exactly what it should/could do. I'm using it for testing a browser web extension, which is inherently even more difficult to test. I already have browser automation tests to guarantee if X is done Y happens. However, I can see the need for it to go away once AVA supports running in a real browser.\n. ",
    "hasLandon": "I'm pro number 1 or 2 but would love to see some sort of test grouping ability introduced. I really miss that from mocha.\n. ",
    "micimize": "Nested tests are necessary for testing the success of a function that wraps a test.\nFor instance, I have a wrapper that takes a function with metadata containing examples, and tests the examples:\n```javascript\nimport test from 'ava'\nconst META = Symbol('META')\nfunction add(a, b){\n  return a + b\n}\nadd[META] = {\n  description: 'adds two numbers',\n  examples: [{ input: [ 1, 2 ], output: 3 }]\n}\nfunction testExamples(f){\n  let { description, examples = [] } = f[META] || defaults(f)\n  test(description, t => {\n    examples.forEach(({input, output}) => {\n      if(typeof output == 'object'){\n        t.deepEqual(f(...input), output);\n      } else {\n        t.is(f(...input), output);\n      }\n    })\n  })\n}\ntestExamples(add)\nI'd like to test this with ava, but can't without nested testing:javascript\ntest('testing examples works', t => {\n  let add = (a, b) => a + b\n  add[META] = {\n    description: 'adds two numbers',\n    examples: [{ input: [ 1, 2 ], output: 3 }]\n  }\n  testExamples(add)\n  add[META] = {\n    description: 'adds two numbers',\n    examples: [{ input: [ 1, 2, 3 ], output: 6 }]\n  }\n  t.throws('invalid example throws', _ => testExamples(add), AssertionError)\n})\n``\n. @mightyiam my current approach is just testing withnode-tap`, which I guess is similar\n@sonicdoe a custom macro would definitely make my code cleaner in a lot of respects, but doesn't solve the fundamental \"test a test\" problem.. ",
    "mightyiam": "@micimize perhaps you can test it by mocking ava?. @skawaguchi it seems rather typical to me. Thank you for the examples.. Please be efficient with dependencies.\nLodash is seems responsive and kind. Perhaps your changes will be welcomed there.. Are we green :traffic_light: for lodash.ismatch? I've just used it inside t.true in a project.. t.deepIncludes?. Is this documented anywhere, please? Does it need to be, or will it simply work with anything that debugs node, like Chrome DevTools?\n. How likely is this to ever work in Chrome DevTools?\n. Thanks!\n. Year has ended. Is this still an open issue?. @tugend the second, runtime, error seems correct. It is telling you that a test that has the .todo modifier must not have an implementation. An implementation is a function. In your case an empty arrow function.. Thank you!\n. The changes requested by @sindresorhus were committed.\n. The changes requested by @sindresorhus were committed.\n. Upstream was fixed in less than two hours after reported :tada:\nI wonder whether I should even ask for a release.. @jdalton please :smile_cat: . There has been a release of v4.17.4 but not of lodash.isequal, which is what we use. @jdalton, will you please publish that?. Yes, I realize that. Well, at least I'd have this choice. And it will be better than --serial because the run will finish x times faster.. Another reason for this is that in CI I use --verbose so that I could review the test output (computed test names and such), and I almost always prefer to see the results ordered, because it is obviously easier to read that way.. Seeing an ordered set of test names gives me a familiar feeling. I see the names of my tests and I see that they're passing. I can recognize my tests much better when they are in order. Really, not a big thing. I've also found myself not using --verbose lately, in development. The number of tests was enough.. That would be a welcome addition.. \"`--fail-fast` is on. Any number of tests may have been skipped\" would be fine as a start, no?. @ThomasBem to install AVA that's in the working directory, in whatever project, I would do npm pack in the working directory and then in the 'whatever project', do npm install <path to package you got from npm pack>. That's what I would do.\nAlthough this wouldn't replace decent tests in my mind.. @ThomasBem to install AVA that's in the working directory, in whatever project, I would do npm pack in the working directory and then in the 'whatever project', do npm install <path to package you got from npm pack>. That's what I would do.\nAlthough this wouldn't replace decent tests in my mind.. Perhaps instead of\n\"There are tests with the .only() modifier in use\"\nbe it\n\"The .only() modifier is used in some tests\". Perhaps by async findById(table, id, columns) { you actually mean await findById(table, id, columns) {?\n. This line should be:\njs\nimport MyComponent from './MyComponent';\nI would try that.. \"Threw non-error\" seems sweet.. On the subject of +/-/A/E perhaps there are suitable characters in Unicode?. @thejameskyle I like your thinking.\nHere's another:\njs\n    Array [\n      Object {\n        children: Array [\n          Object {\n  \u2260         type: \"text\",\n  \u2260         value: \"[hello](world)\",\n  \u2606         children: Array [\n  \u2606           Object {\n  \u2606             type: \"text\",\n  \u2606             value: \"hello\",\n  \u2606           },\n  \u2606         ],\n  \u2606         type: \"link\",\n  \u2606         url: \"world\",\n          },\n        ],\n        type: \"italic\",\n      },. I would look for how to set up npm cache on the CI environment, in general. That should speed all the installations for you.. My thoughts:\n\ndo not run any more test files\ndo not run any more tests\nallow tests to complete, including their afterEach\nallow test files to complete, including their after\n\nOnly then, exit.. My thoughts:\n\ndo not run any more test files\ndo not run any more tests\nallow tests to complete, including their afterEach\nallow test files to complete, including their after\n\nOnly then, exit.. > Test files with failing tests are terminated quickly (but not immediately, in a synchronous, blocking sense). So --fail-fast already implies that any cleanup code you have in your test file may not run if there's an error. I'm fine with extending this behavior to other test files, too.\n\nI'm leaning towards forcefully stopping all other test processes and not logging any further output once a test has failed.\n\nHow is this reasonable? This should perhaps be a feature, if you find it useful, but not --fail-fast. This should be --fail-very-fast or some other naming scheme.\nThere should be a feature to fail on the first failing test, yet, let the run finish gracefully, like I described. And it should be the more approachable, defaulty, recommended option.\nIs how I see it.. It will probably not help that state created by your tests is preserved because that would be state created by tests other than the one which actually failed. Confusing, I think.. The state of the failing test is the only one we'd be interested in. The others' could be confusing and a hassle to manually clean up. That's why I propose: let them finish and clean up after themselves. No new tests will start. Just the ones already running.. @cliguo async function findById(table, id, columns) { perhaps\nhttps://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/async_function. Unless chalk adds unicode, I do not see unicode here... Enlighten me, because I must be missing something.. Unless chalk adds unicode, I do not see unicode here... Enlighten me, because I must be missing something.. Good question. I would try the official support channels.. The issue may be that the callback for getCurrentPosition is not an async function. I've very little experience with async functions, though.. What about what I suggested?. > If there are test files without .only in them, it will print out the following.\n\n\nThe .only() modifier is used in some tests. At least', remainingTests, 'tests were not run.\n\n\nAre you sure you got this part correct? I don't see the sense in it.. I get it now. The first quote is the current message (result of your previous PR). I did not understand this, at first.\nBTW, why do you put quotes around the -v?. \"1 tests were not run\" is fine with me.. Isn't it incidental that the tests you wish to run some code after are serial to each other?. This kind of thing is sometimes done using environment variables. Granted, you'll have to (conditionally) require inside the test files, perhaps.. I don't see the need for shelljs. If you run export FOO=\"foo\" ava won't the child processes have FOO set as well?. This doesn't mean that \"test\": \"ava -w\" should be encouraged. Seems like an anti-pattern to me. $npm test should mean the same thing on development and CI, except for inconsequential things, right?. Yes. \"Skipping\" watching, with a warning, seems reasonable. But, if anyone is running with -w in CI it most likely means that his test script has ava -w. Shouldn't we \"educate\" by not letting that pass, and instead provide a kind message?\n\n\"Error: AVA will not run with the --watch (-w) option in CI, because CI processes should terminate, and with the --watch option, AVA will never terminate.\". I'm :+1: better. I just didn't see it as better. Are you using \"test\": \"ava --watch\" in packages.json?\n\nI think that it is better to display a super helpful error message when ava is used wrongly, then to allow it to be used wrongly.\nBut I'm not sure whether it should be a warning or an error in this case.. Warning or error? Warning could allow exiting with 0, while error should mean exiting with 1. I find this distinction important.. I'm looking forward to this. Debugging should be more clear without unnecessary polyfills.\nDid we think about automatically detecting target?\nAnd what's wrong with stage 2? No one has to actually use any proposed future.... Alright. What about target detection? We are on target runtime. So why don't we detect and set the target accordingly?. Well, I've experienced a bug somewhere in some stack before, where something that was supposed to be watching files, wasn't catching all changes. One such experience is enough for me to desire to have clear indication as to what file changed and when. Yet, this information should be concise and stay out of the way.\nSo, I would prefer a relative path of the file.\nAnd, a stopwatch of the time since the change, in granularity of seconds. I'm not sure what terminal powers allow this. But, if a progress bar is possible then this should be, as well.\n6 seconds ago changed: lib/index.js somewhere. Perhaps at the bottom. Perhaps with some colors.\nAnd the stowatch is actually running, yes?\nI think that this would make clear which file changed last and how long ago. So basically, when I hit save in my editor and look at the running watch, I can see that it did, indeed, re-run my tests.. > No, only use syntax supported on Node.js 4.\n:1st_place_medal: tough leadership decisions. Can't wait for next release.. Thanks, @novemberborn !\nHey, but, for the record, the context is t.context, and not t, as suggested above, right?. @novemberborn I confirm that using this branch I don't get the type error. I did not try to understand the fix.. @novemberborn \nMy macro, like any other macro\u2014I figure\u2014is a function that is created by the user.\nWhat do you mean by \"out of the box\", please?. I am explicitly typing macro as Macro in order to be able to assign to macro.title.. I am explicitly typing macro as Macro in order to be able to assign to macro.title.. @novemberborn better?. @novemberborn better?. Use native JavaScript template string instead of some library's API, I think.. DRY first sentence.. You have some shared strings here. Shared with the mini reporter.. Shared string.. And template string.. You're right, because:\nhttps://github.com/avajs/ava/blob/64b7755d97cba07ed62d797c6ce2f85ffa11a01e/types/base.d.ts#L118\n. t is implicit, @novemberborn.. ",
    "sonicdoe": "@micimize Your use case seems perfectly suited for AVA\u2019s test macros.. ",
    "jedd-ahyoung": "Just my two cents - I use \"describe\" functionality mostly to group my tests so that they appear grouped in my reporter. I generally will create a test file for a module, and I will use describe grouping for specific subsets of functionality. (Often, this is actually split by method, but it can be split in other ways as well.) The individual tests will test different conditions and inputs for a given output (general cases, edge cases, what happens if something is null, etc.)\nI know this isn't the general consensus but I wouldn't mind a simple describe functionality that had no effect on the tests themselves - only the reporter. More functionality would be nice, but I could get away with this for my needs.. Just my two cents - I use \"describe\" functionality mostly to group my tests so that they appear grouped in my reporter. I generally will create a test file for a module, and I will use describe grouping for specific subsets of functionality. (Often, this is actually split by method, but it can be split in other ways as well.) The individual tests will test different conditions and inputs for a given output (general cases, edge cases, what happens if something is null, etc.)\nI know this isn't the general consensus but I wouldn't mind a simple describe functionality that had no effect on the tests themselves - only the reporter. More functionality would be nice, but I could get away with this for my needs.. ",
    "skawaguchi": "Here's another example that's kind of similar to what @fearphage mentioned above.\nMy group often uses gherkins to structure our tests. We have some pretty hairy business requirements, so our tests also serve as documentation of behaviour. We follow TDD and also do a bit of BDD, so we also try to use gherkins in our requirements to loosely capture the details of user stories. It sounds like a lot of overhead, but we've found it very useful guide our TDD process. This thought-experiment-turned-pseudo-practice may not be for everyone, but it's proven pretty useful for our group and wouldn't be possible without nested groups.\nI'll use a non-async example, in this case, validation for some imaginary input field:\njs\ndescribe('Given input for my overly prescriptive input validator', () => {\n    beforeEach() // do some setup, like say create sinon sandbox \n    afterEach() // restore sandbox\n    describe('and there are fewer than the minimum number of characters', () => {\n        beforeEach() // set up test string, call validation method \n        it('should indicate the minimum length has not been met')\n        it('should allow entry as normal')\n    })\n    describe('and the input exceeds the character limit', () => {\n        beforeEach() // set up test string, call validation method \n        it('should indicate that the maximum length has been exceeded')\n        it('should prevent entry of any additional characters')\n    })\n    describe('and the input is within character limits', () => {\n        describe('and contains valid characters', () => {\n            beforeEach() // set up test string, call validation method \n            it('should allow entry as normal')\n        })\n        describe('and contains invalid characters', () => {\n            beforeEach() // set up test string, call validation method \n            it('should prevent additional entry')\n            it('should indicate that specific characters are invalid')\n        })\n        describe('and contains swear words', () => {\n            beforeEach() // set up test string, call validation method \n            it('should wash your mouth out with soap')\n        })\n    })\n});\nThis is contrived, but there are often cases where we want to piggy-back on setup and where we want to create readable test output for our specs:\nGiven input for my overly prescriptive input validator\n  and there are fewer than the minimum number of characters\n    should indicate the minimum length has not been met\n    should allow entry as normal\n  and the input exceeds the character limit\n    should indicate that the maximum length has been exceeded\n    should prevent entry of any additional characters\n  and the input is within character limits\n    and contains valid characters\n      should allow entry as normal\n    and contains invalid characters\n      should prevent additional entry\n      should indicate that specific characters are invalid\n    and contains swear words\n      should wash your mouth out with soap\nI'm sure this isn't a typical practice in the JS community, but it's an example of how far some could be taking their tests.. ",
    "thomas4019": "Has anybody done any work on this issue? If not, I may try implementing it soon to migrate a bunch of existing tests.. ",
    "dideler": "Worth mentioning how Elixir has approached this in their standard testing library, ExUnit.\nExUnit doesn't allow nested describe blocks and doesn't support the context block. They intentionally designed it this way because they found (based on experience with other testing frameworks) that nesting would lead to very difficult to read and understand tests.\nWith nested groups you may write your tests as\n```\ndescribe 'Foo'\n  describe '.do_something'\n    context 'when state is x'\n      beforeEach\n        set up x\n  it 'does primary behaviour'\n  it 'does secondary behaviour'\n\n  context 'when state is also y'\n    beforeEach\n      setup y\n\n      it 'does some behaviour'\n      it 'does another behaviour'\n\n...\n```\nWith the limitations on groups in ExUnit, they introduced named setups to easily reuse setup code. So your tests may look like this instead\n```\ndescribe 'Foo.do_something when state is x'\n  setup x\n  it 'does primary behaviour'\n  it 'does secondary behaviour'\ndescribe 'Foo.do_something when state is x and y'\n  setup x, y\n  it 'does some behaviour'\n  it 'does another behaviour'\n```\nIf you want to find out more about this approach to testing, here's an internal talk I gave on the topic or view just the presentation slides.. ",
    "vitalets": "@jamiebuilds I agree with you in common. \nBut your examples seems to be written in favour of non-grouped tests. I think both approaches have own props and cons.\nIn grouped variant you may use mocha's context this and demonstrate that more tests are added more easily: \nBefore: \n```js\ndescribe('foo', () => {\n  beforeEach(() => {\n    this.foo = setupFoo();\n  });\nit('should do something', () => {\n    expect(this.foo.thingImTesting(bar)).to.be.true;\n  });\ndescribe('with bar', () => {\n    beforeEach(() => {\n      this.bar = setupBar();\n    });\nit('should do something', () => expect(this.foo.isThisABar(bar)).to.be.true);\nit('should do something else', () => expect(this.foo.isThisABar(bar)).to.be.true);\nit('should do something else else', () => expect(this.foo.isThisABar(bar)).to.be.true);\n// more tests, focused on actual assertion\n\n});\n});\ndescribe('baz', () => {\n  beforeEach(() => {\n    this.baz = setupBaz();\n  });\nit('should do something', () => {\n    expect(this.baz.hasBeenSetup()).to.be.true;\n  });\n});\n```\nAfter:\n```js\nconst test = require('ava');\nfunction setupFoo(opts) {...}\nfunction setupBar(opts) {...}\nfunction setupBaz(opts) {...}\ntest('something that needs foo', t => {\n  let foo = setupFoo({...});\n  t.is(foo.thingImTesting(), false);\n});\ntest('something that needs foo and bar', t => {\n  let foo = setupFoo({...});\n  let bar = setupFoo({...});\n  t.is(foo.isThisABar(bar), true);\n});\ntest('something that needs foo and bar else', t => {\n  let foo = setupFoo({...}); // duplicated setup\n  let bar = setupFoo({...}); // duplicated setup\n  t.is(foo.isThisABar(bar), true);\n});\ntest('something that needs foo and bar else else', t => {\n  let foo = setupFoo({...}); // duplicated setup\n  let bar = setupFoo({...}); // duplicated setup\n  t.is(foo.isThisABar(bar), true);\n});\ntest('something that needs bar', t => {\n  let baz = setupBaz({...});\n  t.is(baz.hasBeenSetup(), true);\n});\n```. ",
    "danny-andrews": "I agree with @jamiebuilds on this one. Nested context blocks make tests horribly complex to read. You should always favor extracting common setup into a helper/factory which can be called in each test.\nThis thoughtbot article gives a very compelling case against nesting context blocks.. Late to this party but I just wanted to affirm the decision to have AVA not add polyfills by default (global polluting or no). Jest took a different route (and includes babel-polyfill by default) which led to tests passing code which broke in production. BAD BAD BAD!. @greyepoxy Dude, this was a life-saver. I think this should be added as a recipe. Also, I found this library, which removes the need for the manual code you have here.. Added the fixes you requested. @sindresorhus. @sindresorhus Anything else you'd like me to add?. I second this. I'm using the fantastic expect library, and I get unhappy error messages like the following:\n```\n  /Users/danny/Projects/circleci-weigh-in/node_modules/expect/lib/assert.js:29\nError thrown in test:\nError {\n    message: 'Expected { threshold: { targets: \\'yoyo.js\\', maxSize: 2, strategy: \\'any\\' }, offendingAssets: [ \\'yoyo.js\\' ] } to match { message: \\'\"yoyo.js\" (35B) must be less than or equal to 2B!\\', threshold: { targets: \\'yoyo.js\\', maxSize: 2, strategy: \\'any\\' }, offendingAssets: [ \\'yoyo.js\\' ] }',\n  }\nassert (node_modules/expect/lib/assert.js:29:9)\n  Expectation.toMatch (node_modules/expect/lib/Expectation.js:138:28)\n  _ramda2.default.forEach (tests/bundle-size-utils.test.js:115:33)\n  forEach (node_modules/ramda/src/forEach.js:43:5)\n  Object.f2 [as forEach] (node_modules/ramda/src/internal/_curry2.js:25:16)\n  Test.fn (tests/bundle-size-utils.test.js:113:9)\n``\nwhen usingexpect.toMatch. @novemberborn Does ava parallelize tests across *files* or *tests*?. I'll put together a sample github repo sometime this week, so we can play around with the webpack config and demonstrate an example use-case.. Maybe we should have this recipe recommend building everything to one file, but add a note explaining that all tests will be run in the same process. Also, this use-case makes me think that there should be an option to opt-out of ava's auto test transformation. @sindresorhus what are your thoughts on this?. @sudo-suhas I agree. There is a lot of nuance surrounding this setup. It might be better to list the different approaches discussed here along with the pros and cons of each without explicitly recommending one over the other, as there doesn't seem to be one best solution as of yet. When [this](https://github.com/avajs/ava/blob/master/docs/specs/001%20-%20Improving%20language%20support.md) is completed, this may no longer be the case.. I tried to give this a whirl. I'm gettingCannot find module 'hullabaloo-config-manager'. @novemberborn I used yarn. I'll just hold off until things progress. Found a workaround to get pipeline-operator to work without upgrading to babel 7. :) (I'd happy to test in out once it's in beta or whatever, though.). Just for clarification, does ava currently supportbabel.config.js? i.e., is this simply a documentation/test issue?. No, that was just a typo. It's fixed now.. It's a file with the contents of all the transpiled test code and all of its dependencies. It's really ugly and wouldn't really help to see.. The entry is the test files, because they in turn include the source files that need to be precompiled. No reason precompiling files that aren't ever tested.. Also, the bundle in this case will be the test files and all the source files that are included in those tests.. @sudo-suhas Also, am I reading that CI output correctly?31 min 39 secto5 min 49 sec`!? Wow!. Yeah, I think this is a good changeset. It's far more common to have a bunch of test files that you would want to glob for.. ",
    "niftylettuce": "Can we please document how to do this with nyc + transpiled code in the README for Ava?\nIt's super frustrating to not have proper source map support, and it's nearly been a year and there's still no solution to the source map issue with transpiled code and/or using nyc.. Probably related to #1584 and #1805. Related https://github.com/istanbuljs/nyc/issues/619. @sindresorhus do you think the node env in particular should have this as added as a default to ava babel plugin since it's supported since node v8.3+ though?  it's weird to not be able to write tests out of the box with ava that use object rest spread.. if you have something like a TypeError, the actual output will not bubble up to nyc so it will look like:\n\nversus:\n\n. fyi this is run with nyc ava, and I have tried both nyc ava --verbose and also a block in package.json for ava such as \"ava\": { \"verbose\": true }. None of that works @novemberborn - it simply does not work with nyc.  I've tried no-color and your other variation as well.. It's highlighting the wrong line and not showing the proper output.. Ah, I see, \"t.context is not available in before tests\".  What's the alternative?. I just defined a let foo outside scope and then set foo = bar inside the test.cb.before.. Rather we might want to use https://github.com/gemini-testing/looks-same instead. @bokuweb cool!  hmm yours checks for an exact match, I think I'm going to use @limonte approach at https://github.com/limonte/sweetalert2/blob/3c5b196fbf9bf3780c091a6e22a860228c9486ae/test/puppeteer/visual-tests.js#L167 probably, thank you.  Currently using https://github.com/Huddle/Resemble.js and dislike that it requires canvas etc.. No it is pure JavaScript/Node, no Babel compilation at all.. Not using babel anywhere in the project.  I am using cross-env NODE_ENV=test nyc ava to run tests, but I'm pretty sure this happened even with pure ava.\nversions:\nnyc 11.1.0\nava 0.22.0\ncross-env 5.0.5\nyou can view source here:\nhttps://github.com/cabinjs/cabin. Related to #1805 as well, just experienced it again still.. can you please @mention me when you release this to npm? thanks!. np thank you!. closing as we don't need this. Note that this only happens when object rest spread is used in the test files.. For example, I get this error:\nsh\nTypeError: undefined is not a function\n    at promise.then.args (/Users/test/Projects/test/frisbee/src/interceptor.js:4:81)\n    at process._tickCallback (internal/process/next_tick.js:68:7)\n    at Function.Module.runMain (internal/modules/cjs/loader.js:721:11)\n    at runMain (/Users/test/.node-spawn-wrap-32523-715159df5212/node:68:10)\n    at Function.<anonymous> (/Users/test/.node-spawn-wrap-32523-715159df5212/node:171:5)\n    at Object.<anonymous> (/Users/test/Projects/test/frisbee/node_modules/nyc/bin/wrap.js:23:4)\n    at Module._compile (internal/modules/cjs/loader.js:678:30)\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:689:10)\n    at Module.load (internal/modules/cjs/loader.js:589:32)\n    at tryModuleLoad (internal/modules/cjs/loader.js:528:12)\nand on line 4 of interceptor.js it is a blank line..\n```js\nmodule.exports = class Interceptor {\n  constructor(API, interceptableMethods = []) {\n    this.interceptors = [];\nif (!API) {\n  throw new Error('API should be passed to the Interceptor');\n}\n\nif (interceptableMethods.length === 0) {\n  throw new Error('no methods were added to interceptableMethods');\n}\n\n// ...\n\n```. I've already tried this as well in my ava config:\n\"ava\": {\n    \"failFast\": true,\n    \"verbose\": true,\n    \"files\": [\n      \"test/**/*.test.js\"\n    ],\n    \"babel\": {\n      \"testOptions\": {\n        \"babelrc\": false // <---- attempted to use this \n      }\n    }\n  },. Sorry this is 100% due to nyc.  I'm on latest ava and nyc beta.  It's just source mapping in the console output is not correct when using nyc with ava.. this is a duplicate of #974 - the issue has existed for quite some time (but most likely because of istanbul/nyc source mapping). Ah, I see that it accepts human-friendly Strings.  Perhaps we should document this!\nhttps://github.com/avajs/ava/blob/bccd297f38c9f4cf6dbb16dffa7ee8753dbbd12f/test/api.js#L269. ",
    "JakeChampion": "Was 3 minor versions behind.\n. ",
    "silverwind": "@jamestalmage you could just do a PR on node with that backport, targeting the v0.10 branch. Maybe it'll even make it in time for the upcoming release, see https://github.com/nodejs/node/pull/2805. I don't think anyone would object merging it.\n. ",
    "ben-eb": ":+1: \n. @jamestalmage I find optional boolean parameters to be an anti-pattern, would either prefer a string literal or an options object. :smile:\nhttp://ariya.ofilabs.com/2011/08/hall-of-api-shame-boolean-trap.html\n. @jamestalmage I find optional boolean parameters to be an anti-pattern, would either prefer a string literal or an options object. :smile:\nhttp://ariya.ofilabs.com/2011/08/hall-of-api-shame-boolean-trap.html\n. Found this whilst writing a failing spec for a module I'm working on; gives this stack trace:\n```\n  1 passed  1 failed\n\nshould return the smallest colour\n  AssertionError: should convert percentage based rgba values (5)\n    at Decorator.concreteAssert (/Users/beneb/projects/colormin/node_modules/empower-core/lib/decorator.js:95:20)\n    at Object.decoratedAssert [as same] (/Users/beneb/projects/colormin/node_modules/empower-core/lib/decorate.js:48:30)\n    at Test.fn (/Users/beneb/projects/colormin/dist/tests/index.js:43:7)\n    at tryCatcher (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/util.js:11:23)\n    at Object.gotValue (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/reduce.js:145:18)\n    at Object.gotAccum (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/reduce.js:134:25)\n    at Object.tryCatcher (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/util.js:11:23)\n    at Promise._settlePromiseFromHandler (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/promise.js:489:31)\n    at Promise._settlePromise (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/promise.js:546:18)\n    at Promise._settlePromiseCtx (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/promise.js:583:10)\n    at Async._drainQueue (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/async.js:130:12)\n    at Async._drainQueues (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/async.js:135:10)\n    at Immediate.Async.drainQueues [as _onImmediate] (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/async.js:16:14)\n```\n\nTest is simply:\n``` js\nimport test from 'ava';\nimport min from '..';\ntest('should return the smallest colour', t => {\n  t.same(min('rgba(100%,64.7%,0%,.5)'), 'invalid', 'should convert percentage based rgba values (5)');\n});\n```\n. Found this whilst writing a failing spec for a module I'm working on; gives this stack trace:\n```\n  1 passed  1 failed\n\nshould return the smallest colour\n  AssertionError: should convert percentage based rgba values (5)\n    at Decorator.concreteAssert (/Users/beneb/projects/colormin/node_modules/empower-core/lib/decorator.js:95:20)\n    at Object.decoratedAssert [as same] (/Users/beneb/projects/colormin/node_modules/empower-core/lib/decorate.js:48:30)\n    at Test.fn (/Users/beneb/projects/colormin/dist/tests/index.js:43:7)\n    at tryCatcher (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/util.js:11:23)\n    at Object.gotValue (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/reduce.js:145:18)\n    at Object.gotAccum (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/reduce.js:134:25)\n    at Object.tryCatcher (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/util.js:11:23)\n    at Promise._settlePromiseFromHandler (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/promise.js:489:31)\n    at Promise._settlePromise (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/promise.js:546:18)\n    at Promise._settlePromiseCtx (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/promise.js:583:10)\n    at Async._drainQueue (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/async.js:130:12)\n    at Async._drainQueues (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/async.js:135:10)\n    at Immediate.Async.drainQueues [as _onImmediate] (/Users/beneb/projects/colormin/node_modules/bluebird/js/release/async.js:16:14)\n```\n\nTest is simply:\n``` js\nimport test from 'ava';\nimport min from '..';\ntest('should return the smallest colour', t => {\n  t.same(min('rgba(100%,64.7%,0%,.5)'), 'invalid', 'should convert percentage based rgba values (5)');\n});\n``\n. :+1: \n. I'd love this feature by the way, but I'm not at all familiar with AVA's code base...\n. Updated. :+1: \n. Just tried the ES2015 version without the reporters, it needs at least one otherwise it throws, so I'll leave in the text reporter for now.\n. No problem. :smile:\n. We used./node_modules/.bin/nyc report --reporter=text-lcov | ./node_modules/.bin/coverallsbecause it outputs to stdout, so there's no need for saving it to a file andcat`ing it back out to stdout again. Rest looks good to me but I can't spend more time on this.\n. I'm getting this error too but on 0.12.\nhttps://github.com/postcss/postcss-selector-parser/issues/44\n. @novemberborn Still failing, unfortunately. https://travis-ci.org/postcss/postcss-selector-parser/jobs/117308186\n. https://travis-ci.org/postcss/postcss-selector-parser/jobs/117547344\n. Do you have a good solution for running tests in batches with global code coverage using nyc? As far as I'm aware, running in batches will leave lines uncovered which would be hit in other test files...\n. Just FYI, this script I wrote should be able to fix anyone's builds that are running into this error. It allows you to run AVA processes on each of your files serially, and supports code coverage. Note that it is slow, especially compared to running AVA on your local machine, so I recommend using an alternate script when not using Travis CI.\nhttps://github.com/postcss/postcss-selector-parser/commit/081a5367a47a1f931ef97159958f87ef12b45892\n. Sorry if it wasn't clear, but yes anyone is free to take this script and use it under the terms of the MIT license.\n. nyc report --reporter html works fine. Should I change anyway?\n. Mine isn't. :smile:\nhttps://docs.npmjs.com/misc/scripts#path\n. ",
    "kevbook": "Same problem \n. ",
    "EduardoLopes": "Same Problem. The test works if i run npm test and i got that same error if i run ava test.js\nava version: 0.6.1\nhere's the project i was trying on: https://github.com/EduardoLopes/palette-lovers. i'm on linux.\n. ",
    "MaffooBristol": "Is there any movement on this? I'd like to be able to see all failures in a single test without having to split them into multiple ones- I assume there's still no way of doing this yet?. @sindresorhus Ha, I know. But sometimes things get fixed or change unrelated to an issue on Github, or there are many asking the same thing and people don't communicate between, so no harm in asking... imo!. I was looking into https://github.com/axross/tap-notify, which is rather cool. But not being able to do it on watch kinda defeats the point. There has to be some way of getting them working together.\nOr what about even having desktop notifications built into ava?. ",
    "radiovisual": "Update: I am currently working around this issue by employing Babel 5.x in my app, which allows me to use AVA without incident. \nHere are my Babel 5.x dependencies which allow me to test with AVA:\n\n\"dependencies\": {\n    \"babel-runtime\": \"^5.8.29\"\n  },\n  \"devDependencies\": {\n    \"ava\": \"*\",\n    \"babel\": \"^5.8.23\"\n  }\n\nI think I may have just gotten a little ahead of myself and posted this issue without realizing that the AVA team is currently working on getting AVA ready for Babel 6.x.\nThanks for all your hard work! :smiley_cat: \n. I am using AVA 0.8.0.\n. Ah yes. @vdemedes  You're right. I figured I was staring directly at the problem. Thanks. \n. This would be awesome. \ud83d\udc4d \n. Sure. Right now I am troubleshooting a part of my application, and I am writing a test to ensure that an error is thrown, I have the following test, which sometimes causes the test run to hang in the terminal:\njs\ntest('should throw when local-config.js is not found', t => {\n    const birdwatch = new Birdwatch({server: false, testData: false}).feed('testfeed');\n    t.throws(birdwatch.start(), Error('local-config.js file not found.'));\n});\nIt took me a little while to figure out which test was causing the test run to hang, which is why it would have been a big time-saver to have some sort of flag that could help narrow down the pending/hanging tests. \n. Yes, in that example I am using t.throws() incorrectly, which does cause the test run to hang. However, I wasn't able to narrow down my problematic test until after I had gone through a pretty tedious test-by-test comparison to figure out which test was hanging.\nThough, I am still working to find the exact bug/reason why the application is hanging in my tests.\n. Update: The only reason I can gather that makes that particular test to hang is the incorrect usage of t.throws(). When I use t.throws() correctly, the test run no longer hangs.\n. yeah, I recall seeing that a warning for improper use of t.throws() was merged into master not long ago, so as soon as I realized the problem I was having with my tests, the first thought that came to mind was how that feature would have helped me. \ud83d\ude04\n. @jamestalmage, I do think that #583 would be enough to solve the problem, as long as a \"hanging\" test would be classified by AVA as a \"pending\" test. \ud83d\udc4d \n. ",
    "jamen": "I was playing around and discovered if you reassign the name in the destruction it doesn't fail.  For instance:\njavascript\ntest('test name', ({is, true: isTrue}) => {\n  // is(), isTrue()\n});\nThis does not throw.\nSo you can choose to merge #290 or not.\n. Sounds good :+1:\n. Sounds good.\n. ",
    "jkimbo": "@vdemedes ah ok that makes sense! I missed the difference between the test files being run in parallel and the tests being run concurrently. Since the tests get run concurrently how would you recommend patching global objects for tests? The original reason I came across this issue was because I was trying to patch the process.nextTick method so that I could make Promises synchronous in my tests however the beforeEach (where I patched the method) was being run before the corresponding afterEach had a chance to tear down the patch.\nThanks for the quick answer anyway!\n. @vdemedes ah ok that makes sense! I missed the difference between the test files being run in parallel and the tests being run concurrently. Since the tests get run concurrently how would you recommend patching global objects for tests? The original reason I came across this issue was because I was trying to patch the process.nextTick method so that I could make Promises synchronous in my tests however the beforeEach (where I patched the method) was being run before the corresponding afterEach had a chance to tear down the patch.\nThanks for the quick answer anyway!\n. Haha thanks guys! Very helpful!\nTo give some more context around why I'm patching process.nextTick, I'm running some code that triggers a fetch request. I patch fetch out to be a promise I control in the test and then I assert some state, resolve the promise and assert the resultant state. The problem I had with that workflow was that the promise resolve would queue up a microtask and so I couldn't assert the next state synchronously. For the readability of the test I patch the nextTick method, store up the tasks queued up and run them at once so that I don't have to make the test async. Some sudo code that hopefully makes it clearer (React test):\n``` js\n// mock fetch\nglobal.fetch = sinon.stub();\nconst deferred = defer(); // create a deferred promise\nfetch.returns(deferred.promise);\nTestUtils.Simulate.click(ReactDOM.findDOMNode(stub.refs.button));\nassert(fetch.calledOnce);\nassert.equal(stub.state.searching, true);\n// return a response\ndeferred.resolve({\n  'status': 200,\n  'json': () => ({\n    'foo': 'bar',\n  }),\n});\nrunAllTicks(); // this runs all the tasks queued \nassert.equal(stub.state.searching, false);\n```\nAnyway I hope that explains why I can't use async/await etc.\nThanks again for the help!\n. Haha thanks guys! Very helpful!\nTo give some more context around why I'm patching process.nextTick, I'm running some code that triggers a fetch request. I patch fetch out to be a promise I control in the test and then I assert some state, resolve the promise and assert the resultant state. The problem I had with that workflow was that the promise resolve would queue up a microtask and so I couldn't assert the next state synchronously. For the readability of the test I patch the nextTick method, store up the tasks queued up and run them at once so that I don't have to make the test async. Some sudo code that hopefully makes it clearer (React test):\n``` js\n// mock fetch\nglobal.fetch = sinon.stub();\nconst deferred = defer(); // create a deferred promise\nfetch.returns(deferred.promise);\nTestUtils.Simulate.click(ReactDOM.findDOMNode(stub.refs.button));\nassert(fetch.calledOnce);\nassert.equal(stub.state.searching, true);\n// return a response\ndeferred.resolve({\n  'status': 200,\n  'json': () => ({\n    'foo': 'bar',\n  }),\n});\nrunAllTicks(); // this runs all the tasks queued \nassert.equal(stub.state.searching, false);\n```\nAnyway I hope that explains why I can't use async/await etc.\nThanks again for the help!\n. FYI I got this idea from jest: https://facebook.github.io/jest/docs/api.html#jest-runallticks\n. FYI I got this idea from jest: https://facebook.github.io/jest/docs/api.html#jest-runallticks\n. @jamestalmage that works really well thanks!\n. @jamestalmage that works really well thanks!\n. ",
    "mcmathja": "Hmm, test passes without issue on my box. I'll look at it again in the morning.\n. Today I learned AppVeyor doesn't properly handle unicode in console output. =)\n. ",
    "ecowden": "Just checked out the fix and it's giving the stack trace I'd expect. Thanks! :+1: \n. Humble lurker here, so feel free to tell me I'm wrong --\nUsually, when I find myself dropping in log statements for debugging, it's in the code under test, not the test itself. If the data I'm trying to learn about is accessible from the test, I'd just write an assertion anyway. I doubt I'd use t.log very much.\nI've found debugging particularly tricky when experimenting with AVA since it also seems to give debuggers like node-inspector a hard time. In an ideal world, I'd love to see the log output for failed tests alongside the failed assertion or error. Successful tests could hide logs unless handed --verbose. Of course, easier said than done, right?\n. Awesome progress!\nHere's lorem-ipsum.js in Terminal.app on El Capitan:\n\n. ",
    "i5ting": "```\nvar superkoa = require('superkoa')\ntest.cb(\"superkoa()\", t => {\n  superkoa('./koa.app.js')\n    .get(\"/\")\n    .expect(200, function (err, res) {\n      t.ifError(err)\n      var userId = res.body.id;\n      t.is(res.text, 'Hello Koa', 'res.text == Hello Koa')\n      t.end()\n    });\n});\n```\n. thanks @marr @kevva another practise with generator from me\n```\n// *  GET    /users[/]        => user.list()\ntest('GET /' + model, function * (t) {\n  var res = yield superkoa('../../app.js')\n    .get('/' + model)\nt.is(200, res.status)\n  t.regex(res.text, /table/g)\n})\n```\nfor atomic test\n```\n// *  GET    /users/:id       => user.show()\ntest('GET /' + model + '/:id show', function * (t) {\n  var res1 = yield superkoa('../../app.js')\n    .post('/api/' + model)\n    .send(mockUser)\n    .set('Accept', 'application/json')\n    .expect('Content-Type', /json/)\nuser = res1.body.user\nvar res = yield superkoa('../../app.js')\n    .get('/' + model + '/' + user._id)\nt.is(200, res.status)\n  t.regex(res.text, /Edit/)\n})\n```\n. ",
    "marr": "@i5ting here is an example without the 'callback style' assertion\nhttps://github.com/marr/tests/commit/05506ed154b4a3403cf674e8aff910c6218daf0d\n. ",
    "scottmas": "@mattkrick It would be awesome if this could be re-opened. The biggest thing is if we want to have mandatory initialization and cleanup scripts running before the tests. It's a HUGE pain. \nTo get a before global initialization hook (with async http requests, etc), I can do so with a --require init-script.js ava and then inside that script do a gross child_process.spawnSync to delay execution of Ava until the http requests inside the script completely finish. \nAnd then, to get a global post execution hook, I have to some gross Bash hackery to get the exist code and the test output. I'm not proud to commit into my code base:\nTEMP_OUTPUT=$(ava ./server/e2e/**/*.e2e.js --timeout 300000 2>&1); \\\nexport TEST_EXIT_CODE=$?; export TEST_OUTPUT=$TEMP_OUTPUT; \\\nnode post-script.js && exit $TEST_EXIT_CODE\n. A function hook isn't strictly necessary. What about a global before and after script file invocations?  Like the existing --require option but actually waiting for the file execution to complete before continuing on. E.g. I can imagine my Ava config looking like so:\njson\n{\n  \"ava\": {\n    \"globalHooks\": {\n        \"before\": \"./e2e-setup.js\",\n\u00a0       \"after\" : \"./e2e-cleanup.js\",\n        \"force\": [\"after\"]\n    }\n  }\n}\nThe resource approach outlined does looks amazing, but I can imagine this being way simpler to implement. What do you think?. ",
    "brunoqueiros": "I think that is easier to have more assertions because the test file will be cleaner and more easy to maintaining. I think there should have both, that way the user can choose what use.\n. It's not even necessary, your arguments are right. I'll close the PR.\n. In my opinion some users want to see the total execution time, if you don't want show the time for all users could be a good solution have a flag, that way only users which set the flag will see.\n. OK, if you doesn't see that is a useful feature I'll close the PR.\n. ",
    "fernandofleury": "Any feedback when this feature might be finished? Any points we could help?\n. ",
    "hberntsen": "v5.1.1 (from the Debian experimental repository). Simple debugging works:\n```\n$ node debug testdebug.js\n< Debugger listening on port 5858\ndebug> . ok\nbreak in testdebug.js:1\n\n1 console.log('1');\n  2 console.log('2');\n  3 debugger;\ndebug> c\n< 1\n< 2\nbreak in testdebug.js:3\n  1 console.log('1');\n  2 console.log('2');\n3 debugger;\n  4 console.log('3');\n  5 \ndebug> \n```\n. v5.1.1 (from the Debian experimental repository). Simple debugging works:\n\n```\n$ node debug testdebug.js\n< Debugger listening on port 5858\ndebug> . ok\nbreak in testdebug.js:1\n\n1 console.log('1');\n  2 console.log('2');\n  3 debugger;\ndebug> c\n< 1\n< 2\nbreak in testdebug.js:3\n  1 console.log('1');\n  2 console.log('2');\n3 debugger;\n  4 console.log('3');\n  5 \ndebug> \n```\n. \n",
    "develar": "We, JetBrains, cannot support debug AVA tests without changes on your side. I will send you pull request next week.\nGeneral information \u2014 https://intellij-support.jetbrains.com/hc/en-us/community/posts/206193339-Debugging-node-child-processes?page=1#community_comment_206895729\nMy solution \u2014 https://github.com/sindresorhus/ava/compare/master...develar:debug?expand=1 (not yet reviewed by my experienced college).\nRun configuration \u2014\n\nPlease note \u2014 babel debug in JetBrains IDE is broken in node 5.x, I use my custom build of node 5.x (https://github.com/nodejs/node/pull/4231).\n. Another problem on AVA side: CachingPrecompiler produces source map, but... debug doesn't work in any case because generated source file doesn't contain sourceMappingURL AVA CachingPrecompiler must add sourceMappingURL explicitly. \nSo, IDE cannot map source file to transpired because no sourceMappingURL. And breakpoints don't work correctly.\nYes, it is very strange that babel does so, I found related issue \u2014 https://github.com/babel/grunt-babel/pull/13\nThis change will be also in my pull request. For now, you can set both. Do you have any reasons to produce external source maps instead of inline?\nPlease note again \u2014  babel debug in JetBrains IDE is broken in node 5.x, I use my custom build of node 5.x (nodejs/node#4231). Also, to speed up, enable lazy compilation \u2014 https://github.com/nodejs/node/issues/877#issuecomment-75248842\nAfter set sourceMaps to both, don't forget to detele cache (node_modules/.cache/ava) to apply change.\n. @JimmyBoh Only WebStorm currently provides the best AVA debug. https://github.com/avajs/ava/pull/874#issuecomment-227066303\n@azakordonets Please see https://github.com/avajs/ava/pull/874 I hope it will be accepted soon.\n. @vdemedes Am I right that discussion is over (initial proposal + add power-assert automatically)? And we need just implement it?\n. @vdemedes Am I right that discussion is over (initial proposal + add power-assert automatically)? And we need just implement it?\n. > Like an editor plugin running the test your cursor is in.\nTo support AVA in WebStorm, this functionality is required, yes.\n. @kasperisager To save your time \u2014 grab https://gist.github.com/develar/da7d3c01c3b8e6e0eace \u2014 it is a patch to apply to master (original PR is outdated). You only need to use proposed matcher here.\nIt works for me, but I am very busy now to prepare PR (well, can do in two days if you will not do first :))\n. > > fork now returns object with custom methods (on, run) and promise field. We don't add custom methods to promise anymore.\n\nWhat's the benefit of this?\n\nMy first approach was just add getPort().then And then I realized, that existing code doesn't work with nested promise \u2014 .on is not a function. But we cannot return promise from fork directly until promise of getPort() is not resolved \u2014 so, we return nested promise.\n. @novemberborn Sorry for delay, it will be addressed this week.\n. New PR \u2014 #874.\n. >  you should have a few integration tests\nIntegration test added. Maybe on CI it will be passed, but on my machine I got \"Test files must be run with the AVA CLI:\" from child stderr if use nyc.\n. >  you should have a few integration tests\nIntegration test added. Maybe on CI it will be passed, but on my machine I got \"Test files must be run with the AVA CLI:\" from child stderr if use nyc.\n. CI failed \" not ok AvaError: test/fixture/debug-arg.js exited with a non-zero exit code: 1\"  Hmm... it works for me \u2014 I use fork of ava in my project and can debug tests in IntelliJ IDEA. \nSomething is wrong with nyc, but I don't see how it it is connected. \nIf you will have no clue, I will investigate what's wrong  with nyc.\n. CI failed \" not ok AvaError: test/fixture/debug-arg.js exited with a non-zero exit code: 1\"  Hmm... it works for me \u2014 I use fork of ava in my project and can debug tests in IntelliJ IDEA. \nSomething is wrong with nyc, but I don't see how it it is connected. \nIf you will have no clue, I will investigate what's wrong  with nyc.\n. > Try bumping your tests to use something higher.\nPort here doesn't matter \u2014 for child we use random free port.\n. > The build runs fine locally on OSX.\nI use OS X and it is failed with nyc.\n\nThen what does --debug=0 mean?\n\nArgs of the main process. In the tests we don't start main process, we start child only (if I am not wrong). Purpose of this fix is \u2014 replace port in the main process args to another free port.\nOk, I will investigate what's wrong with nyc.\n. AppVeyor build succeeded only because npm run test-win \u2014 in the test-win nyc is not called. \n. Sorry for delay, I will investigate it soon. Currently I am working to support new v8 inspector in the IntelliJ IDEA. --inspect has the same problem \u2014 no way to ask node to use any port.\n. Rebased \u2014 tests passed.\n--inspect and combination --inspect + --debug-brk supported (this combination should be used to debug tests because in case of just --inspect tests can be executed before debugger attached).\n. @jamestalmage Why? User just run debug in the IDE. Or run debug using standard node args. That's all. Nothing more. No need to reinvent the wheel and introduce new tool-specific API. User just use standard tools and way to debug.\nJetBrains IDE \u2014 we reimplement multiprocess debug specially for AVA. It just works. No multiple debug tabs anymore, it is like chrome workers (you can select \"thread\" using combo box if need).  You just hit \"debug\" and it works (in context runner for AVA not yet implemented, but it is another task).\nChrome using --inspect \u2014 copy url and debug. Yes, it is not seamless as in the IDE, but... IDE is IDE. I think, you told exactly about it \u2014 simplify debug using some Ava-specific tool.\nBut, at first, I think, we should support debug using standard tools and way.\n. > Is that available for us to take a look at?\nYes, these changes available since 2016.1 release. You can try 2016.2 EAP \u2014 IDEA or WebStorm. --inspect not yet supported for child process (it is in the unreleased 2016.3 (163 branch)).\nVideo: https://youtu.be/C75UwuZXI98\n\nHaving multiple processes running at a time sounds cool, but I'm not sure how useful it is.\n\nI use --match if I want to debug test. I run npm run test in terminal and then debug failed tests in the IDE.\n\nUsing execArgv with --debug-brk causes it to pause the main process.\nSo you need to manually continue it each time before child processes are launched.\n\nIDE uses this break to set breakpoints, find sourcemaps and then continues debug automatically. \n. #929 is about how to simplify AVA debug in the CLI using special ava specific CLI. This PR is to make AVA debug working using standard way (i.e. IDE vendor should not explicitly support AVA). Strictly speaking, this PR is a workaround of https://github.com/nodejs/node/pull/5025\n. > Maybe you could debug two test-files that interleave disk operations in some way with clever breakpoints? IDK.\nYou can use conditional breakpoints.\n. BTW, AVA developers can get open source WebStorm/other IDE licence for free.\n. @jamestalmage Checked, cool, you already have open source licence. Feel free to ping me directly in case of any JetBrains IDE issue.\n. Now my turn to ping ;)\n. @sindresorhus This PR doesn't add ava-specific implementation \u2014 just fix nodejs \"bug\". I.e. IDE can debug AVA using standard non-specific way. #929 only for users, who want to use terminal to debug (simplification).\n. Branch rebased to fix conflicts.\n. Conflicts resolved again :)\n. @sindresorhus Sorry, forget to mention \u2014 https://github.com/nodejs/node/pull/5025 only about --debug-port=0, but currently no issue about --inspect \u2014 please see https://github.com/nodejs/node/pull/5025#issuecomment-225323248 \"it should be possible to extend this to --inspect after this PR lands.\" Currently, --inspect should be used to achieve the best user debug experience (yeah, nodejs 7 is required).\n. @korsmakolnikov Please specify IDE version, do you use ava from sources?\n. @KeKs0r Please use WebStorm 2016.3 EAP\n. @mightyiam https://github.com/avajs/ava/blob/master/docs/recipes/debugging-with-webstorm.md\n. @mightyiam In addition to --inspect you need to pass --debug-brk So, Chrome will stop on first line.\n\nThis will output a link that you can open in Chrome. After opening that link, the Chrome Developer Tools will be displayed, and a breakpoint will be set at the first line of the Jest CLI script (this is done simply to give you time to open the developer tools and to prevent Jest from executing before you have time to do so). Click the button that looks like a \"play\" button in the upper right hand side of the screen to continue execution. When Jest executes the test that contains the debugger statement, execution will pause and you can examine the current scope and call stack.\n\nDocs from Jest is true and correct for AVA as well. (Irony that currently Jest debug doesn't work due to NodeJS bug https://github.com/nodejs/node/issues/7593, only AVA debug works ;)) So, you can debug in Chrome DevTools also.\n. For non-IDE users it will be simpler than #874 . But IDE user relies on IDE and for IDE it ts better to use standard, tool-agnostic way :)\nThis PR:\n- avoid main process stop and debug. Otherwise CLI user will need to manually continue main process. Doesn't matter for IDE user.\n- automatically set concurrency to 1 to avoid debug port number collision. Doesn't matter for IDE user.\nI have no relation to AVA, and I will not use this (because I am IDE user), but as happy AVA user I am glad that AVA will be suited for all, \ud83d\udc4d \n. @rhalff Please specify WebStorm version. Due to nodejs changes, WebStorm < 2016.1 doesn't support NodeJS 6+ (explicit --expose-debug-as=v8debug is required). Please use 2016.2\n. @rhalff Thanks for clarification, if you use WebStorm, please use another PR https://github.com/avajs/ava/pull/874#issuecomment-227298029\nThis PR is not about IDE support.\n. Blocked on https://github.com/nodejs/node/issues/8080 (--inspect).\n. @prigara will do it next week.\n. Fixed.\n. Fixed (hmm... linter doesn't throw error).\n. has-flag returns boolean, but we need position of debug argument. In the future, es6 includes can be used here to avoid === 0.\n. Because we don't append flag, but change existing. By default child_process.fork pass process.execArgv to child process as is. What we do here \u2014 we find debug argument and change its value to any free port.\n. > This should stub\nNot quite understand, do you mean that we should replace process.execArgv during test? I think, it is not acceptable to touch global state.\n. > True, but tests are special.\nI think, some day AVA tests will be also executed in parallel, so, it is not wise to \u0441hange properties in the global process object.\n. > the parallel processes will have their own process.execArgv\nBut not tests in the same file ;)\nOk, up to you, I removed option and replace process.execArgv.\n. @sindresorhus Will be addressed as https://youtrack.jetbrains.com/issue/WEB-19540\ncc @segrey\n. @melisoner2006 Please add note that --inspect-brk works only in the node 7+.\nNot all users uses latest 8 :). Probably will be better if text if you want to listen will be in the start of sentence, because it is advanced and optional instruction (english is not my native language, so, I don't known how much it is relevant).. ",
    "azakordonets": "The issue is still open - does it mean that it's still impossible to debug AVA tests in IDEA or Webstorm ? \n. @JimmyBoh i prefer products from Intellij :) \n. @develar Thanks a lot :) I've subscribed for that issue\n. ",
    "thedev": "What do you think about using a post install script ?\nNoticed the following discussion: https://github.com/npm/npm/issues/7340\n. ",
    "buzinas": "@jamestalmage performance always matters, and it's always good to have as much as possible a generated code side-by-side with the source one.\nIf we have native support, we should always use it, there is absolutely no reason to use something ugly and gross on top of something that already exists and performs well.\nI can understand people using bluebird instead of the native Promise, for example, but using a feature like regenerator, only if targeting browsers.\nAnother great difference: Debugging is terrible with regenerators, and it's awesome with native generators.\n. @novemberborn Exactly this example is occurring in my project. Is there any way to bypass this?\n. ",
    "jescalan": "Ok, I'll use master for now. Ping this issue and close when it's released so I know? I am on 3.x for npm :grinning: \n. Just for the record, everything worked fine using master. Thanks! :tada: \n. Has the issue been resolved on a stable release of ava? If not I might keep it open until that point :grinning: \n. Thanks for the awesome work guys. You are the best :tada: \n. Ah great, don't know how I missed that one. Thanks! :tada: \n. So, for those of us stuck with this issue in the meantime, how would one clear the cache manually?. @novemberborn on the latest version, 0.19.1. ",
    "portgasd666": "I didnt get how I'm supposed to fix this. Can someone help ? Thanks. \n.  Oh okay, thanks mate !\n. ",
    "krasimir": "@portgasd666 Instead of setting the following in your package.json file:\n\"ava\": \"0.8.0\"\nyou may use:\n\"ava\": \"git+https://github.com/sindresorhus/ava.git#master\",\nHowever, that's not really a good idea since you are going to get the very latest thing every time when you run npm i. So I hope (@jamestalmage) that the change which is now in master will go in the next released version.\nFor the record: indeed it works, but it's kinda slow. I mean the version in master. 0.8.0 is instant but the same setup and test with master is ~2 seconds.\n. @sindresorhus good point!\n. ",
    "maboiteaspam": "side note, using \"ava\": \"sindresorhus/ava#65ae07c\" as a dependency, i can work using the local bin, ie : node node_modules/.bin/ava instead of ava. \nAs secondary side node, same sentence for xo. The global bin would not work properly.\n. ",
    "hax": "Sorry I have cold last week and not check the github notifications.\n\nWhat is the runtime overhead of doing a bunch of feature detections?\n\nThe runtime overhead of feature detection < 10ms, so I think it's ok for most applications.\n\nWhat if the native implementation has bugs and we want to use Babel for that?\n\nYes they have. But Babel also have :grinning: . So we need to check it case by case. For example, whether to use regenerator. (node 0.12+ have generators, but missing some minor features such as Generator.prototype.return. babel-preset-es2015-node-5 decide to not use regenerator. If there is any other case like that please let me know.)\n. @sindresorhus \nThis is due to V8 (and other implementations) seem focus on es2015 features not performance up to now. Even simple const/let could cause deoptimization.\nBut I never see a real case such performance degradation is significant for a webapp (browser-side) in my daily work. Maybe heavy-load Node.js apps would affected by it. Anyway they can just switch from babel-preset-es2015-node-* to normal babel-preset-es2015.\n. ",
    "dy": "I wanted to switch to ava in audio-buffer-utils, but will wait for a moment. var test = it found it's way for now :)\n3. Example test.js:\n``` js\nvar test = require('ava');\nfunction f () {\n    xxx\n}\ntest('xxx', function () {\n    f()\n});\n```\nRunning ava:\n\nMocha goes:\n\nIt is git bash in windows 7, that may be an issue, IDK. \n. Sure, if each notice is known. Thank you.\n. ",
    "jfmengels": "@jokeyrhyme AVA still injects features that have not yet landed in ES2015 or ES2016, such as async/await. Additionally, it uses Babel for internal transforms (enhancing assertions with power-assert for instance).\nI don't know if it's possible to correctly detect the Node version, but that'd be at runtime, not at install time. The team could remove the ES2015 preset when all supported Node versions support it, but that could take a while.\n. Should we also link to eslint-plugin-ava in the common pitfalls section? We can probably push more people to using it.\n. Sure, will do that later.\n. I have created this repo for the same purpose: https://github.com/jfmengels/mocha-vs-ava-performance\nIt's pretty small, but it makes it obvious how big the difference is between Mocha's instant feedback in watch mode, and Ava's \"watch\" with Nodemon, or even single runs.\n. It should probably execute even if a beforeEach fails I think. Maybe you opened a DB connection or wrote a file, and you want to clean that part up even if something else failed in the beforeEach. You could catch it in the beforeEach I guess, but then you'd duplicate the clean up in both places.\n. Finally!\n. Finally!\n. Thanks @BarryThePenguin!\n. @spadin If you have a package.json, you could make AVA a script:\njson\n  \"scripts\": {\n    // either or both of them\n    \"test\": \"eslint . && ava\",\n    \"ava\": \"ava\"\n  },\nthen run npm run ava or npm run test to run AVA without having it globally.\n. @spadin If you have a package.json, you could make AVA a script:\njson\n  \"scripts\": {\n    // either or both of them\n    \"test\": \"eslint . && ava\",\n    \"ava\": \"ava\"\n  },\nthen run npm run ava or npm run test to run AVA without having it globally.\n. Adding to the list\n- [\u00a0] Transform tests containing a callback ( it('foo', function (done) { ... }) ) to test.cb(t => ... t.end();\n:+1: on the separate codemod for chai assertions.\n. For nested describes, or too many side-by-side describes, we could maybe split tests up into multiple files. I know jscodeshift doesn't allow to create new files or update files other than the currently evaluated one, but what we can do is split them up into clearly separated parts in the same file, so that the user can easily cut-paste each section in a new file.\nUsing your previous input example\n``` js\n<<<<<< innerTest.js\ntest.beforeEach(...); // outer beforeEach\ntest.beforeEach(...); // inner beforeEach\ntest('innerTest', t => {\n});\n<<<<<< outerTest.js\ntest.beforeEach(...); // outer beforeEach\ntest('outerTest', t => {\n});\n```\nWe could then create a tool that applies the jscodemod, then reads the updated files, creates new files and cut-paste the code inside the appropriate file. We don't have to do a 100% jscodeshift transformation tool. \n. > > I know jscodeshift doesn't allow to create new files or update files other than the currently evaluated one\n\nWhy not?\n\nWell, it's a simple AST transform tool, but you're right that we don't have to play \"by-the-rules\", and we could call fs methods from there. Might not be a bad idea\n. @spudly Isn't that the same thing as?\njs\ntest.beforeEach(setupA);\ntest.beforeEach(setupB);\ntest(testA);\ntest(testB);\ntest(testC);\n. Food for thought: In Mocha's watch mode, we get the time the tests took to run.\n\nI like this since you have early feedback about whether your tests are fast or not, since Mocha re-runs all tests every time. In AVA, only the tests for the saved file are re-run, but it might be a nice addition too.\nI usually look at the time it took to see whether tests have re-run. Look at previous run time, save file, compare with run time, if it's different, it has run. :+1: on either the timestamp, the run time or both.\n. @spudly :+1:!\nI'd create a new project containing the codemods (ava-codemods?). Don't see the value of putting it in the AVA project if it's only going to be run once at the very most.\nThere is talk about writing other (complex) codemods too in https://github.com/sindresorhus/ava/issues/644.\n. @spudly I improved your code a bit by providing support for notSame --> notDeepEqual (and some necessary refactoring to avoid duplicating) https://astexplorer.net/#/yXo9G9wx9m/1\n. I love the idea for test macros, but I noticed that it will also make linting some stuff harder (finding out that .only, .skip is used etc.). It's a trade-off.\n. Interesting idea, but then you'd have\n1. tampering of the AVA import, which will not affect other tests but is still pretty ugly\n2. to import a file who'd have a side-effect of defining a macro, which seems counter to AVA's explicit imports (compared to Mocha where describe, it that are injected for instance). I like explicit imports.\n3. Odd behavior when definining a macro called only for instance.\nAn alternative would be\njs\n// Proposal 1\n// Defining\ntest.defineMacro('myMacro', (t, input, expected) => ...);\n// Using\ntest.macro('myMacro', 'input', 'expected');\nBut you'd still have problem 1 above.\nAn even better alternative:\n``` js\n// Proposal 2\n// Defining (call it createMacro, defineMacro, whatever)\nconst myMacro = test.defineMacro((t, input, expected) => ...);\n// Using the macro\ntest.macro('title', myMacro, 'input', 'expected');\n// Proposal 3\n// or even alternatively, by re-using test, that adapts its behavior \nconst myMacro = test.macro((t, input, expected) => ...);\n// Using\ntest('title', myMacro, 'input', 'expected');\n```\nI think proposal 2 is the best implementation. You get\n- static analysis capability\n- test creators in helpers that will not be linted as errors\n- Explicit imports of macros\n- No tampering of the AVA/test object\n- Explicit use of a macro (which is neutral, but worth pointing out)\n. > I think in the case of a helper defining macros - the user has decided to do that themselves. AVA's not doing that to them\nSure, but still, I myself as a user don't like it. And if AVA doesn't allow macros to be used without a variable, AVA will kind of force me to do that, should I want to create a macro used in multiple files.\nIt's more verbose, but not that much, but that's just my opinion. If you're that concerned about verbosity, my proposal 3 is pretty short. It can be even be shortened by simply having macro be a simple function and injecting the arguments\n``` js\n// Normal tests\ntest(t => { var a = 2, b = 3; ... });\ntest(t => { var a = 2, b = 4; ... });\n// Proposal 4\n// Macro/reusable test\nvar macro = (t, a, b) => { ... };\ntest(macro, 2, 3);\ntest(macro, 2, 4);\n```\nImporting would be easy, as it's a special function. No naming, no AVA tampering. Alternatively, args could be added to the context: t.context.args === [2, 3]. I like this version even more.\n. > :-1: You can completely replace t.context in a beforeEach. t.context = something.\nProposed it just in case proposal 4 could not be done because test already uses further arguments or so. Much prefer the args in function option too.\n. @novemberborn With proposal 4 macros are simple functions, and there's no need for a special macro creator.\n. > What separates proposal 4 (test(macro, 2, 4)) from calling test with too many arguments?\nNothing, except that the arguments may be used by the macro (I might not have understood the question right).\n\nCan the macro define the test title?\n\nSince the macro is simply a function, no. I'm guessing it might be wanted in some cases (if you're testing plenty of cases for add(a, b), having multiple titles might get cumbersome), but then again, it might get hard to identify it otherwise. test(addMacro, 2, 3, 5); test(addMacro, 2, -1, 1); Which one failed? You could put the args in the test title, but can get ugly when given complex objects.\nI'd argue that it's best if the macro doesn't set a title, and that it should be given to the test function, like\n``` js\nfunction addMacro(t, a, b, expected) {\n  t.is(add(a, b), expected);\n}\ntest('add 2 numbers', addMacro, 2, 3, 5);\ntest('add a positive and a negative number', addMacro, 2, -3, -1);\ntest('add a number and NaN', addMacro, 2, NaN, NaN);\n```\n\nDo we want to support macros that create multiple tests?\n\nThough I see the value in it for the user, one of the points for macros was to make static analysis easier/possible. I'd say that having macros that could create tests (potentially containing .only) makes it harder.\n. If we can avoid introducing the idea of macro, and just enhance test, I think it would make things simpler, so I'm all for test().\nI think the suggestion for macroFn.title, and would suggest that title in test(title, macro, ...args) should override it.\n:+1: For the title template, even though when given complex arguments, it will be unreadable, but not a lot of things we can do about that?\n:+1: on the array proposal too.\nStill not sure why you think the test should be given an array of args (t, [a, b]) => ... and not simply spread args (t, a, b) => .... Sure it's not that harder with destructuring, but it might confuse users a bit. Do you expect a third argument to ever come into play?\nAlso, do you think test might ever be given an other arg to change its behavior? If so, we might want to have the arguments given as an array, rather than as \"spread\" arguments test(macro, [1, 2, 3], someFutureArg) without changing what we will decide on how arguments are passed to the macro function.\n. **/*.test.js ! I use this all the time, would love it if it landed in the default files :heart: \nWe'll need to update the linter too.\n. Woohoo! This makes my day, thanks @kentcdodds!\n. :tada: \n. Under https://github.com/sindresorhus/ava#cli, you already have this documented.\nDirectories are recursed, with all *.js files being treated as test files.\nDirectories named fixtures, helpers and node_modules are always ignored.\nSo are files starting with _ which allows you to place helpers in the same directory as your test files.\n. How would you test that the observable only has those values and then ends? (not that familiar using Observables, so correcti me if I'm saying something stupid).\n``` js\ntest(t => {\n  // expectation generator is just built in.\n  const expected = t.expectations(['foo', 'bar', 'baz', 'NOOOOOOO']);\nreturn someObservable()\n    .forEach(val => t.is(val, expected.next().value)); // foo, then bar, then baz, and should not reach NOOOOOOO\n});\n``\n. > We would track theexpected` iterable, and make sure it was empty\nThat would be nice, but it sounds pretty hard or not user-friendly. Example:\n``` js\n[1, 2, 3].forEach(function(i) {\n  test(t => {\n    const expectedIfOne = t.expectations(['foo', 'bar']);\n    const expectedIfNotOne = t.expectations(['foo', 'bar', 'baz']);\nconst expected = i === 1 ? expectedIfOne : expectedIfNotOne;\nreturn someObservable()\n  .forEach(val => t.is(val, expected.next().value));\n});\n\n});\n```\nI may be overthinking this, but it seems to me that in this case, if you would track the created expectations, without being able to tell whether the generator was started (is it possible? Maybe with some Babel plugin), you'd fail the test because one of the expectations was not emptied.\nWould it not be simpler and friendlier if you had a method to compare a function to compare observable to an array? Or is that too narrow a use case?\nExample:\njs\ntest(t => {\n  return t.isObservableEqualToArrayFunction(someObservable(), ['foo', 'bar', 'baz']);\n});\n. > you do not necessarily have to pass the output of the iterable directly into an assertion\nOk, makes sense. But to me, there's only limited gain either syntax or assertion wise, compared to for example something like this.\n``` js\ntest(t => {\n  const expected = ['foo', 'bar', 'baz'];\nreturn someObservable()\n    .forEach(val => t.is(val, expected.shift()));\n});\n```\nNot fond of it myself, but it's shorter, potentially easier for users not used to generators (will admit I'm one of them), as you don't have to remember to call both next and value (less verbose and error-prone).\nThe only gain I see with expectations (and I would like to see more if you see others) is that it's possible to track on AVA's side whether the generator has ended at the end of the test.\nI think that what I would like to see is an API that does not force the user to learn potentially new concepts (generators), and simplifies the verbosity of the test. Maybe something like isObservableEqualToArrayFunction solves 90% of the cases, and that'd be pretty fine IMO.\nNote: Both methods pass but should fail if you have this in your observable\n---foo---bar---baz---undefined--undefined--\n(Sorry if I sound like a nagger/pessimist, not my purpose)\n\nFor your example regarding generated tests, you could just do this\n\nYes, though that was a simple case. I assume it can get more complicated.\n\nOr better yet, when we implement macros\n\nNow this is better :D and solves the use-case I proposed :+1:\n. ``` js\nt.validateArrayLikeThing = function (actual, _expected, validator=(x, y => t.deepEquals(x, y))) {\n  var expected = transform(_expected); // transform expected in whatever's convenient\nreturn promisify(actual)\n    .forEach(val => validator(val, expected.shift()))\n    .then(() => t.is(expected.length, 0, There were less items in ${actual} than expected));\n}\n```\nIn the case of @jamestalmage example with a complex validator:\n``` js\ntest(t => {\n  const validator = data => {\n      const expected = expectations.next().value;\n  if (expected.flagA) {\n     t.is(data.A, XXX);\n  }\n\n  if (expected.flagB) {\n     t.is(data.B, XXX);\n  }\n\n  t.is(data.value, expected.value);\n\n};\nreturn t.validateArrayLikeThing(emitter, expectations, validator);\n});\n```\nIt's bigger than what was originally proposed (or what we're used to in AVA I think), but it seems more useful IMO, and more in the spirit of what is actually wanted.\n. > I don't see how validateArrayLikeThing knows when it's done and it's time to end the test. How does it know there is more data available to validate against?\nI thought about making validateArrayLikeThing return a Promise, so you could either return it and append a .then() or use await (await validateArrayLikeThing(...); // then check stuff).\n\nWell, in a sense, you are correct, that's the only gain. But I think that is a pretty big one\n\nOkay, with your example, it's starting to make sense I think. Thanks for your patience on this :)\n. > We could extend the iterator API to give them a hasNext() implementation like Java\nI think something like this would do the trick:\njs\nt.expectations = function *expectations(expected) {\n  expected.forEach(function() {\n    yield * expected;\n  });\n  throw new Error('Too many items');\n};\n. I think the tone could be warmer, so that it feel less like you're going to get yelled at if you missed a step. You guys are all really nice and helpful, but that doesn't convey here :)\n. > Any specific suggestions would help ;)\nYeah I know, sorry about the lack of specificity. Will try to come up with stuff later tonight, but it's not my forte either.\nIn the meantime and for reading purposes, there is @benmosher's very nice contributing file https://github.com/benmosher/eslint-plugin-import/pull/258. It's not the same topic, but it can be a source of inspiration (which I'll probably re-read before thinking of improving this).\n. LGTM, except for the Gitter link. Sorry for blocking this :s\n. I'll take a stab at it :)\n. > I don't think you pushed the fixed tests yet!\nUgh, you're right. Done.\n\nYou need to run the bench tests on both branches (this one and master). Then run node bench/compare\n\nAh yes, found the section about benchmarking in the maintaining file.\nBenchmark results on my machine (can't say I'm excited about it... :confused:):\n\n. > I don't think you pushed the fixed tests yet!\nUgh, you're right. Done.\n\nYou need to run the bench tests on both branches (this one and master). Then run node bench/compare\n\nAh yes, found the section about benchmarking in the maintaining file.\nBenchmark results on my machine (can't say I'm excited about it... :confused:):\n\n. Re-ran it twice just to be sure\n\n\nAnd it looks better already, especially in the first case. Don't know if this is by chance (the gain is always pretty slim), or what happened during the first run. Maybe I had done the run in this branch before I applied the changes (can't remember, sorry for the confusion...), or maybe the gains are so small that it willt tilt one way or another if I'm doing something else on my computer while running the benchmark test.\n. Re-ran it twice just to be sure\n\n\nAnd it looks better already, especially in the first case. Don't know if this is by chance (the gain is always pretty slim), or what happened during the first run. Maybe I had done the run in this branch before I applied the changes (can't remember, sorry for the confusion...), or maybe the gains are so small that it willt tilt one way or another if I'm doing something else on my computer while running the benchmark test.\n. Well, one thing to take into account is the size of the test files. They are really minuscule, and I think that each pass only transpiles the currently tested file, so I'm not sure that Babel has a chance to shine here.\nLooking at it more, I don't think that the transpiled output is any different than the given input (no assertions to be changed with power-assert, no const, no async...).\nI think we should run the test on something pretty big LOC-wis and that needs a lot of transpilation, in order to really mesure Babel's influence. If the results look kind of the same, I would probably agree with @jamestalmage and give it a :-1: (if the performance is confirmed by other computers).\n\nare you sure you're running the benchmark with babel@6.7.7?\n\nThe version of babel-core I find in node_modules/babel-core is indeed 6.7.7.\nEdit: Yes, babel-generator and babel-types are also in 6.7.7\n. Well, one thing to take into account is the size of the test files. They are really minuscule, and I think that each pass only transpiles the currently tested file, so I'm not sure that Babel has a chance to shine here.\nLooking at it more, I don't think that the transpiled output is any different than the given input (no assertions to be changed with power-assert, no const, no async...).\nI think we should run the test on something pretty big LOC-wis and that needs a lot of transpilation, in order to really mesure Babel's influence. If the results look kind of the same, I would probably agree with @jamestalmage and give it a :-1: (if the performance is confirmed by other computers).\n\nare you sure you're running the benchmark with babel@6.7.7?\n\nThe version of babel-core I find in node_modules/babel-core is indeed 6.7.7.\nEdit: Yes, babel-generator and babel-types are also in 6.7.7\n. Might be worth putting in https://github.com/sindresorhus/awesome-ava also/instead.\nMaybe add a mention that it's an episode with the team behind AVA?\n. That is by design, but you might want to look at https://github.com/sindresorhus/ava/pull/806 (or the issue linked there). It just landed in master (not on npm yet), it adds a .always attribute to test (test.afterEach.always(...)) that will get run even if the tests fail.\n. There needs to be a test that makes the tests fail when a test passes I'd say.\nThanks for taking this on btw @cgcgbcbc :)\n. :tada: Yay, this is great @jamestalmage! Didn't know whether this was accepted, but glad it's going forward, it's a really nice feature IMO.\n. I would've preferred spread in both places, but it does get tricky if you want to add new args.\nIf we go for spread and want to go for arrays later, then I think that writing a codemod will be kind of tricky. (easy in most cases, but hard when the macro is defined in a separate file).\nI'd go for consistency at least: either spread in both places or arrays in both places.\n. > Should the macro.title function get passed title as the first arg?\n:+1: \n. I think the former is a bit unnatural. I'd say stick to a function, and seek to improve later it if it feels insufficient.\n. :tada: Folks, 0.15 is starting to get pretty exciting :D\n. For reference, looks a lot like Lodash' _.isMatch and _.matches (same thing, but the former does the check, while the latter returns a function that does the check)\n. For naming:\n- Lodash, Expect & Sinon use the term match (matches, isMatch...)\n- Ramda, Expect & Chai use match for matching in a string\n- Should.js uses contain\n- I have seen the term shape used for something similar (for proptypes, @kentcdodds's lib)\n(Expect uses match for both strings and objects)\n. The last proposal is not more reusable that just creating a new function.\n``` js\nconst isIdentifier = name => ({\n  type: 'Identifier',\n  name\n});\nconst isT = isIdentifier('t');\nconst isAssertion = name => ({\n  type: 'CallExpression',\n  callee: {\n    type: 'MemberExpression',\n    object: isT,\n    property: isIdentifier(name)\n  }\n});\nconst isThrowsAssertion = isAssertion('throws');\ntest(t => {\n  // ...\n  t.match(node, isThrowsAssertion)\n});\n```\n(exact same thing, just removed test.match)\nLooks simpler to me, and it doesn't introduce new APIs (not counting t.match obviously)\n. I agree with with implementing the proposed t.like, though I'd call it t.match to be consistent with the other libraries I've seen.\n. See https://github.com/avajs/ava/issues/845#issuecomment-220611096. A lot of them use match for this, some use match for strings (and some for both).\n. Looks like this is being handled in https://github.com/avajs/ava/pull/846\n. The link was corrected in https://github.com/avajs/ava/pull/852.\nI'm fine with leaving doesNotThrow in. Do we want to write a codemod for it?\nPS: Shouldn't this rather be in the 0.16 milestone if we want to leave it in 0.15?\n. No problem ;)\n. Released in v0.16.0 https://github.com/avajs/ava/releases/tag/v0.16.0\n. Cool! We need to add it to the linter too. I'll do it tonight except if someone wants to beat me to it ;)\n. Cool! We need to add it to the linter too. I'll do it tonight except if someone wants to beat me to it ;)\n. These look equal to me. The order in which the keys are defined is not important, as in the JS spec there is no guaranty whatsoever about the key order.\n. > Because the order of iteration is implementation-dependent, iterating over an array may not visit elements in a consistent order\n(https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for...in, as https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/keys links to that one)\nThe order of the keys is not important in JS, as it is implementation specific and not guaranteed by the language. V8 for instance (which powers Node and Google Chrome) will return the keys in one order, while ther engines may return them in an order. In your example, the first arg will be ['a', 'b'], while the second will be ['b', 'a'], but that is the normal behavior. If you want to compare the list of keys, then you should sort them.\nThe important thing, is that two objects all have the same keys, and the same values associated with it. If you have that, then the objects are equal. This is the behavior that all assertion libraries have (afaik).\n. I like the idea @vadimdemedes. But I'm not sure that would always work out nicely. I'm thinking of t.fail() which will fail the test anyway.\njs\nlet failed = false;\ntest('retry me', retry(async t => {\n  if (!failed) {\n    failed = true;\n    t.fail();\n  }\n});\nThis should retry the test and it should pass the second time, but IIRC calling t.fail() will forcefully fail the test whatever happens next.\nThat said, maybe the retry (maybe some specific ava-retry module then?) module could pass a modified t object whose t.fail() method is modified and won't forcefully fail the test?. Actually very surprised we didn't think of this for the ESLint plugin earlier. Created an issue over there https://github.com/avajs/eslint-plugin-ava/issues/131\n. I think it's because the test code is compiled by Babel, and the compiled code is not trying to cause ReferenceError to be thrown when a variable declared with let/const is used before its declaration.\nNot sure there is anything to be done about this.\n(By the way, has there been a discussion to only compile only what is needed based on what is supported in the node version that runs the tests? Couldn't find one)\n. I'd like to work on this. I want to get more accustomed to the AVA codebase.\n. Should this only be a setting available in the config, or also using the CLI?\nIf also in the CLI, what should the flag be named? --no-power-assert?\n. hey @eloquence!\nSounds to me like what debug offers you. It's not behind a --verbose flag, but a process.env variable. Would that do the trick for you?\n. I'm not against this, but I think that using a process.env variable would simply do the trick in your case. \njs\ntest('foo', t => {\n  if (process.env.VERBOSE) {\n    console.log('...');\n  }\n});\nand run AVA using VERBOSE=1 ava --versbose\nIt does not come automatically with --verbose, but it should be fine unless you are developing an external library. You could do this if this gets rejected or while waiting for it to be implemented if approved.\n. Updated\n. I addressed all comments, except https://github.com/avajs/ava/pull/1024#discussion_r76693217 (let me know if you want to me to do it), and renamed disablePowerAsssert to powerAssert throughout my changes in the codebase.\n. The tests work on my end on Node v6.4.0. Any hints to make those failing tests pass? :confused: \n. Hi @ngryman!\nThanks for the proposal!\nI'm not sure that this is a good idea though, as you would lose the information/feedback about whether your other tests (in the same file or the files affected by the modified file).\nIf you only re-run a previous failing test, then you might break 10 other tests without knowing about it.\n. > and my english does not help\nYou should not worry about that, it's good enough ;)\nOkay, I think I may have read the proposal a bit too fast and skipped over the fact that tests are (almost) all re-run when the tests pass.\nYou still won't get the feedback that you broke other tests as long as you don't fix all the previous failing tests. In case you start with a lot of failing tests or a very tricky one, you might break a lot of things before you notice and end up not knowing which parts broke the other tests.\nI'm not sure that's a very nice DX. I think that after a few minutes I'd get stressed out and re-run AVA just to make sure nothing else broke, but that might just be me.\nThere's also an issue with the algorithm. You only re-run the tests that failed, even if all previously failed tests pass this time. That means that you'd have to re-run AVA a second time to know which previously passing tests will now break. So in essence, this would be the workflow you'd get (Nice way of presenting it btw):\n| Modification | Test 1 | Test 2 | Test 3 | Test 4 | Test 5 | Comments |\n| --- | --- | --- | --- | --- | --- | --- |\n| Add test | \u221a | \u221a | \u221a | \u221a | \u2717 |  |\n| While test 5 fails | \u2205 | \u2205 | \u2205 | \u2205 | \u2717 |  |\n| When test 5 gets fixed | \u2205 | \u2205 | \u2205 | \u2205 | \u221a | Other tests are not re-run, because we don't know when running tests that Test 5 will be fixed |\n| First save after test 5 is fixed | \u221a | \u2717 | \u2717 | \u221a | \u221a |  |\n| While test 2 & 3 fail | \u2205 | \u2717 | \u2717 | \u2205 | \u2205 |  |\n| When test 3 gets fixed | \u2205 | \u2717 | \u221a | \u2205 | \u2205 |  |\n| While test 2 fails | \u2205 | \u2717 | \u2205 | \u2205 | \u2205 |  |\n| When test 2 gets fixed | \u2205 | \u221a | \u2205 | \u2205 | \u2205 |  |\n| Subsquent runs | \u221a | \u221a | \u221a | \u221a | \u221a |  |\n\u221a: Pass\n\u2717: Fail\n\u2205: Skip\nThis will be the scenario, unless AVA re-runs tests following the results of the tests, but then this becomes a pretty big change in how AVA works. Should AVA re-run everything when the failed tests are fixed, then you get to the scenarios you described.\n(Just FYI, I'm used to writing a lot more tests not dealing with I/O / HTTP, so my tests all run very fast, and don't end up using .only that much (really only when I console.log a lot of things and it's getting hard to find the ones related to my failing test))\n. I like the proposal :)\n\n\nDoes t.throws(fn, Constructor) require the thrown value to be an Error?\n\n\nYes. That is a best practice, or rather, using a string is a bad practice, and we should help the user remove that bad practice from their code IMO.\nWe should also (and I don't remember whether this is already something that we're doing) fail and show an error when the thrown error is a string and not an Error.\nIt's a bit different if an underlying library throws a string, but even then it should be simple for the user to get around that and do a normal try/catch instead.\n\n\nShould t.throws(fn, 'string') and t.throws(fn, /regexp/) be supported at all, or would it be preferable to dereference the error and then use another assertion?\n\n\nThat is common enough to keep IMO.\n\n\nIs there a need for t.throws(fn, SyntaxError, 'string') and t.throws(fn, SyntaxError, /regexp/)\n\n\nI don't custom Error types that much. I think it's rare enough and simple enough to get around it\njs\nconst err = t.throws(fn, 'string');\nt.true(err instanceof SyntaxError);\n\n\nDoes anybody have experience with / examples of asserting Error instances across contexts?\n\n\nNo :/\n\n\n\nFrom what I see, you're proposing the removal of the message too? That would make it inconsistent with the other APIs. Not sure I agree here.\n. @Sntax import test from 'ava'; should work fine.\n@carpasse When you set the Babel plugins to use, you are overriding the ones AVA uses by default. \nbabel-preset-es2017 only contains the additions to the language that were made in the ES2017 spec (or will be in this case), but the modules syntax is something that was added in ES2015 and is thus present in babel-preset-es2015.\nIf you use a recent version of Node, you may not need to use babel-preset-es2015, but if you wish to use the import syntax, you should also add the babel-plugin-transform-es2015-modules-commonjs plugin. Either that, or use babel-preset-es2015, babel-preset-es2016 and babel-preset-es2017.\n. I am getting the same error and can't seem to find a solution to it (except removing the macro possibility...) :thinking: \n. Just saw this pass by: https://github.com/facebook/jest/issues/2673, which I think would be a cool feature. (We could implement that later). The error output is a bit odd when using t.throws.\n\n\n\nThere is a comparison of actual and expected, which may make sense when comparing the thrown errors, but here there is one missing. I think it would be clearer by not having this comparison\n\n\nMissing expected exception has two dots at the end. I feel like this one has always been like this, so maybe for another PR. That said, I never found this error very comprehensive: Does t.throws expect me to pass an exception type/message, or did it run and the function simply did not throw? Happy to create a different issue to discuss this.\n\n\nEdit: pretty much the same thing for notThrows when you have an error. Comparing does not make much sense, and the two dots are there too.\n\nI don't think this is blocking, this could be improved upon in a later PR/version.. What about projects that use JSX with another tool than React (Inferno, Preact, Snabbdom, ...)?. > I run into this from time to time when using macros, though perhaps that's only because the linter cannot resolve the macro.\nThe linter has problems knowing whether two title expressions are similar, if they are a bit complex and have different contexts. That's why we don't report errors as soon as it's more complicated than a simple string.\nThe runner on the other hand would know the exact value, and doesn't have that problem. That's why I think it's a better idea to have this implemented in the runner than in the linter (not that we'd remove the rule, it can be still useful when working in the editor).. > I kinda like the ability to not have a title when you only have one test though\nAbsolutely. Though in the case of your example, I would probably have split the tests up and added titles like the following:\n```js\ntest('should return false if process.env.TERM_PROGRAM !== \"Hyper\"', t => {\n    process.env.TERM_PROGRAM = undefined;\n    t.false(m());\n});\ntest('should return true if process.env.TERM_PROGRAM === \"Hyper\"', t => {\n    process.env.TERM_PROGRAM = 'Hyper';\n    t.true(m());\n});\n// and more tests with the version argument...\n```\n...and I think that it would make the test a little easier/faster to understand, but that's my style of writing tests.\nIf you don't get the message of what you're testing across, you are making things harder for future readers. Having a test title is IMO one of the best ways to explain what you are testing. When the test implementation is well written and explicit, readers can get a good feeling of what is tested, but they almost necessarily need to read the code. When the test fails in your terminal/CI, having a good title show up should give you a good hint.\nAnyway, I think that a test can only be made clearer by having a title, and that allowing an optional title will incite the lazier users to just not add a title. If you really don't feel like thinking of a title for a single-test function, you're always free to give it a bogus one, like the name of the tested function (test('is-hyper', t => {})), which is not that big of a deal IMO.\n@ThomasBem I don't think that anybody has started on this yet. I was planning to go at it at some point, but if you feel like it, please do :). Jut so I'm sure I understand: Why do we wish to have a new jsxEqual assertion method? Is it because doing \njs\nt.deepEqual(<This/>, <That/>);\ncreates an unreadable object that does not look like what you expect to see in the DOM?. > t.notThrows(promise) should return a promise, but for an undefined value.\nWhy should it actually? Isn't it more useful if you get the Promise's value, so that you can make more assertions on it? I don't see the value in having it always return undefined.. I agree with @novemberborn and @vadimdemedes. I'm starting to understand why the order can be important, but I think this only matters in pretty rare cases. I would prefer this to be implemented in a new package.. The power-assert example right under this should be moved a bit to the right. The lines do not point to what is described.\n. Why not __tests__/**/*.js?\n. programmatically\n. Wouldn't it rather be \"creating reusable tests\"?\n. Link has changed: https://gitter.im/avajs/ava\n. It's nice that this is said explicitly :+1: \n. The CLI handler seems to handle no- well, to it sets powerAssert to false when usng no-power-assert. I think we should keep this.\n. I'm not well aware of how this works, but the message talks about only tests, not skipped tests. Are tests skipped due to the presence of an only marked as skipped, or is the message inconsistent with what is checked?\nNot sure why failing tests should not be skipped/only-ed by the way.\n. skip, only, or failing --> skip, only or failing (remove the last comma)?\n. Is the fact that hooks are \"skipped\" handled here?\n. I can't remember why I wrote that either :sweat_smile:\n. ",
    "achecopar": "Hello. I have been checking the list of packages, and found out that many of them already have a closed PR, specially from feross and substack authors. \nI submit an updated list here with some packages edited in bold, the ones with a new PR and the ones that have an .npmignore. I also wrote the state of the related PR for each one.\nThe reason and previous discussions on why the PRs were not accepted can be found in feross/is-buffer#12\nPACKAGE LIST\n\nacorn-jsx@3.0.1 -         master has .npmignore\namdefine@1.0.0 -        master has .npmignore\nargparse@0.1.16 -      master has files entry in package.json\narray-map@0.0.0 -      substack/array-map#6 Closed\narray-reduce@0.0.0 -  substack/array-reduce#4 Closed\nassert-plus@0.1.5 -     master has .npmignore\nassert-plus@0.2.0\nassert-plus@1.0.0\nasync@0.2.10 - master has .npmignore\nasync@0.9.2\naws-sign2@0.5.0 - no extra files\naws-sign2@0.6.0\nbabel-core@6.8.0 - Also has .npmignore file\nblock-stream@0.0.8 - master has .npmignore\ncaseless@0.11.0 - request/caseless#26 Open\ncaseless@0.6.0\ncombined-stream@0.0.7 - Also has .npmignore file\ncombined-stream@1.0.5\ncommondir@1.0.1 - substack/node-commondir#6 Closed\nconcat-map@0.0.1 - substack/node-concat-map#5 Closed\ncore-js@1.2.6 - has .npmignore\ncore-js@2.3.0\ncore-util-is@1.0.2 - isaacs/core-util-is#13 Open\ndashdash@1.13.0 - master has .npmignore\ndashdash@1.13.1\ndeep-equal@0.1.2 substack/node-deep-equal#39 Closed\ndeep-equal@1.0.1\ndefined@0.0.0 - substack/defined#3 Closed\neastasianwidth@0.1.1 - komagata/eastasianwidth#3 Merged\nescope@3.6.0 - master has .npmignore\nesrecurse@4.1.0 - master has .npmignore\nestraverse@1.9.3 - master has .npmignore\nestraverse@4.1.1\nestraverse@4.2.0\nevents-to-array@1.0.2 - isaacs/events-to-array#1 Open\nextsprintf@1.0.2 - master has .npmignore\nforever-agent@0.5.2 - request/forever-agent#35 Open\nforever-agent@0.6.1\nform-data@0.1.4 - master has .npmignore\nform-data@1.0.0-rc4\nformatio@1.1.1 - busterjs/formatio#8 Open\ngooglediff@0.1.0 - shimondoodkin/googlediff#6 Open\ninflight@1.0.4 - npm/inflight#2 Closed\ninherits@2.0.1 - isaacs/inherits#21 Open\nis-buffer@1.1.3 - Author refuses to fix: feross/is-buffer#12 Closed\nis-typedarray@1.0.0 - hughsk/is-typedarray#2 Open\nisarray@0.0.1 - juliangruber/isarray#9 Open\njs-yaml@3.0.1 - has files entry in package.json\njson-schema@0.2.2 - kriszyp/json-schema#68 Open\njsonify@0.0.0 - substack/jsonify#6 Closed\njsonpointer@2.0.0 - janl/node-jsonpointer#26 Open\njsprim@1.2.2 - master has .npmignore\nmime@1.2.11 - master has .npmignore\nminimist@0.0.8 - substack/minimist#88 Closed\nminimist@1.2.0\nmkdirp@0.5.1 - substack/node-mkdirp#104 Closed\nmute-stream@0.0.5 - npm/mute-stream#8 Open\nmute-stream@0.0.6\nnan@2.3.3 - master has .npmignore\nnode-pre-gyp@0.6.25 - master has .npmignore\nnpmlog@2.0.3 - npm/npmlog#32 Closed\noauth-sign@0.4.0 - request/oauth-sign#21 Merged\noauth-sign@0.8.1\noptimist@0.6.1 - substack/node-optimist#142 Closed\npath-is-inside@1.0.1 - has .npmignore\nprivate@0.1.6 - benjamn/private#11 Open\nprocess-nextick-args@1.0.6 - calvinmetcalf/process-nextick-args#9 Merged\npseudomap@1.0.2 - isaacs/pseudomap#5 Open\nregenerate@1.2.1 - master has .npmignore\nresolve@1.1.7 - substack/node-resolve#99 Closed\nresumer@0.0.0 - substack/resumer#3 Closed\nrx-lite@3.1.2 - Also has a .npmignore file\nrx@4.1.0\nslide@1.1.6 - does not contain extraneous files\nspdx-correct@1.0.2 - master has .npmignore\nspdx-exceptions@1.0.4 - no extraneous files\nspdx-expression-parse@1.0.2 - master has.npmignore\nsymbol@0.2.1 - seanmonstar/symbol#1 Merged\ntable@3.7.8 - master has .npmignore\ntext-table@0.2.0 - substack/text-table#8 Closed\nthrough@2.3.8 - dominictarr/through#42 Closed\ntime-require@0.1.2 - master has .npmignore\ntmatch@2.0.1 - tapjs/tmatch#1 Open\ntunnel-agent@0.4.2 - request/tunnel-agent#18 Merged\ntv4@1.2.7 - master has .npmignore\ntypedarray@0.0.6 - substack/typedarray#13 Closed\nuid-number@0.0.6 - no extraneous files\nuid2@0.0.3 - coreh/uid2#17 Recently opened\nutil-deprecate@1.0.2 - TooTallNate/util-deprecate#6 Recently opened\nvalidate-npm-package-license@3.0.1 - master has .npmignore\nwindow-size@0.1.0 - master has files entry in package.json\nwordwrap@0.0.3 - substack/node-wordwrap#13 Closed\nwordwrap@1.0.0\nwrappy@1.0.1 - npm/wrappy#2 Closed but merged as of v1.0.2\n. I am trying to access the links and get an error message, was it moved somewhere else?\n\nWE CAN'T FIND THAT PAGE.\nYou may have mistyped the address or the page may have moved - head back to the homepage Comradel!\n. ",
    "ntwb": "The latest stable is NodeJS v5.3.0 https://nodejs.org/en/\nTravis needs to update their docs, but here on the NVM #Usage section of the docs its documented about the stable alias being deprecated.\nIn https://github.com/creationix/nvm/issues/870 there is a discussion about future aliases such as LTS, and other options...\n. Using 4 and 5 per the pull request will use NVM to always test the latest 4.x.x (currently 4.2.4) and 5.x.x (currently 5.3.0) version/branches respectively so until NodeJS v6 is out things should be right here for a while :smirk: \nAlso see https://github.com/sindresorhus/grunt-sass/pull/263 and @sindresorhus :+1: for these changes\n. Yep, clear as mud all of this isn't it :smirk: \nI started down the road of updating WordPress' NPM dependencies and basically sending pull requests everywhere so that firstly the current stable versions of NodeJS are at least being tested in Travis CI, once thats done I can then look at updating any tests and/or dependencies :)\n. Yep, clear as mud all of this isn't it :smirk: \nI started down the road of updating WordPress' NPM dependencies and basically sending pull requests everywhere so that firstly the current stable versions of NodeJS are at least being tested in Travis CI, once thats done I can then look at updating any tests and/or dependencies :)\n. Thanks for merging, and all that jazz :+1: \n. Thanks for merging, and all that jazz :+1: \n. hehehe, I try to use US spelling when not in/on an Australian site, sometimes I forget \ud83d\ude02 . ",
    "JaKXz": "I'm using the given npm test: ava\nI've also tried the --require option, passing in the utils/util1.js for example but I get AssertionErrors that say the path is missing.\nEDIT: I just noticed #87, but I think this is a valid use case since these files are still within the context of the tests.\n. Damn! I missed this. Sorry!\nEDIT: there is one gotcha for those of us still using Babel 5.x - \ninstead of import 'babel-core/register';, use import 'babel/register';. Also, as documented, the utils/ folder should be called helpers/ or fixtures/ inside the test folder.\n. FWIW, using this branch fixed the issue I was having. Thanks again @jamestalmage.\n. @ariporad @billyjanitsch I disagree that it would be confusing, because of the docs outlining AVA's approach to ES2015 that say:\n\nAVA comes with builtin support for ES2015 through Babel 6. Just write your tests in ES2015. No extra setup needed. You can use any Babel version in your project. We use our own bundled Babel with the es2015 and stage-2 presets.\n\nOf course I'm a bit biased towards this because this PR solved the issue I was having with conflicting .babelrc settings breaking the test execution. Even so, I think it's better that AVA's set up is completely independent of my project because it's one less thing to worry about (say I'm porting older Mocha tests from a Babel 5.x project one day, and starting a brand new project the next - the syntax for my tests hasn't changed so I can write my tests first for both projects and not have to rethink structure/syntax/language features/etc). I also think it's better to test production code with esnext features that are more mature & less prone to inconsistent behaviour, for sanity's sake.\nA healthy compromise IMHO would be an --ignore-my-babelrc option that defaulted to true and could be configured via the command line or package.json. Something along those lines (with perhaps a snappier name) I would love to see for the day when I would like to use more advanced Babel settings in my tests for whatever reason.\n. Love it so far, thanks guys!\n. @sindresorhus would it be possible to get the documentation fixes merged in a separate PR so the --precompile flag can be shipped? cc @jamestalmage \n. ",
    "cameronroe": "When import ing a sass file with webpack and ava, the import fails. Any way to fix that?. @sindresorhus cheers! http://stackoverflow.com/questions/40811771/cant-import-sass-file-with-webpack-using-ava. @sindresorhus okay, so no one is alive on Stack overflow.. Any ideas on what's up with ava and webpack? Docs anywhere on the subject?. Any chance that this could be added in a future release? This would be really great to have!\n. ",
    "davewasmer": "Not sure if I should add this here, or file a separate issue, but one pitfall I ran into:\nIf your tests do anything memory intensive, you might end up hitting OOM process killers in CI environments because ava runs them in parallel. See this build as an example.. Moderate deuteranope weighing in here:\n\nIs it a big deal to you that AVA try to output colorblind accessible colors?\n\nAs long as additional, non-color indicators are present, it's not critical (i.e. + and - next to diff lines).\n\nIs it typical to make these adjustments at the OS level? I know Terminal.app on the mac allows you to change the ansi color scheme, so maybe that's the de facto solution for colorblind users (manipulate the color scheme to one they can see).\n\nFor me, yes. I've picked (and tweaked) a color scheme to have high contrast red/green, since that's a problem area of the spectrum for me.\n\nWould there be a better set of colors? Ideally one that offered the same intuitive feedback for non-colorblind users.\n\nI'd say leave it to \"red\" and \"green\" and let the user set their terminal to render whatever colors are sufficiently contrasty / intuitive for them. It would be difficult to find a single color scheme that worked for all types of color blindness (and potentially impossible if you consider the rare cases of totally monochromatic vision).\n\nCan it be reduced to a single color set, or does it need to be configurable depending on what type of colorblindness you have?\n\nEveryone's color deficiencies are different, and vary in severity. I suspect coming up with enough schemes that work for everyone isn't feasible.\n. Just to weigh in here from #1278 - it seems like there's going to be suboptimal tradeoffs no matter what you do here. If you limit it to the last two members of a MemberExpression, that would still dump the entire fs module for fs.existsSync(). You could blacklist Node core libs and t, but that starts to feel like whack-a-mole: what if I use fs-extra or mock-fs? You could only show the last member (i.e. filepath on t.context.filepath), but perhaps t.context would be helpful to dump there?\nI'll echo my suggestion in #1278 here - if the magic-assert info could be exposed in a machine readable way, that opens up the door to more userland experimentation with different reporter styles. But based on other issue threads, it sounds like userland reporters is something you intentional don't want to support - if that's the case, then feel free to disregard these comments :wink:.\nFor a bit of background / context: my motivation here is that Denali uses ava for running tests. If ava exposed more machine readable output, Denali could do some really interesting / cool things with that output since it knows much more about app than ava ever could. . Apologies, my mistake, turns out I was on the prior version, thought I was on latest.. Seems like there's a few issues at play here, stemming from these lines in the mini and verbose reporters:\n\nIf the source map's urls are relative, then as mentioned originally, an intermediate build step that moves the compiled files relative to their source before running ava will result in incorrect urls. \nIf the source map's urls are absolute, the reporters will still try to prefix the project's basePath (typically process.cwd()), resulting in two absolute URLs joined together. \n\nBoth cases result in incorrect stacktrace URLs. For the verbose reporter, that seems to be a purely cosmetic problem - the user sees the wrong URLs, but that's all. For the mini reporter, it tries to build a code excerpt from that source URL, so the wrong URL actually crashes ava itself.\nFor absolute URLs, I filed #1267 to try to address this, which should resolve the issue.\nFor relative URLs, this gets trickier. Since ava can't know the what the original paths are relative to for sure, I'd suggest just aiming for better error reporting. At the very least, we could add some guards around the codeExcerpt method so that the mini reporter doesn't throw for non-existent source files, and could return a warning message instead of the code excerpt that could suggest using absolute sourcemap urls. We could go further, and do the same for the verbose reporter (i.e. print a warning that URLs will be incorrect).\nHappy to PR either of those approaches.. @novemberborn thanks for the quick response!\nJust out of curiosity - how do you plan to get the absolute source url for files that have relative sourcemaps? I didn't see a way around that one, so I'm curious what you've come up with.. > That's quite interesting! And difficult, I think. Instead we could better integrate with Node's debugging tools and trigger a breakpoint when an assertion fails, but that too sounds rather difficult.\nYea, a little \"pie-in-the-sky\" to be sure. But personally, I wouldn't find the debugger integration that much more helpful. That's a much heavier process (i.e. you have to start ava, then copy the devtools URL, open that in the browser, then hit \"play\" since you need to pass in --debug-brk, then wait for it to hit the assertion, then look through the Scope panel, ...). The problem here isn't that I need more information - in my examples, the magic-assert had the right info, but it had too much excess info as well.\nIf there was a way to expose magic-assert's info to TAP consumers, or some other kind of reporter that fed the magic-assert info out in a machine readable format, then this kind of thing could be explored in userland.. We can isolate the require statement to a test-only code path (our current workaround), so its not a showstopper for us. But it is surprising that requiring Ava at all results in a hard exit. Is there any supported way to ship test helpers like our use case that doesn't involve conditionally loading Ava?. Is there anything that I could do to help advance this? Or is this in more of the \"need to think through the design / what we want\" stage?. ",
    "ProLoser": "Would love to have a pitfall around the parallel nature of unit tests. I can't tell you how many times I've run into Sinon throwing errors about methods already being wrapped in spies. It's just not something I expected to encounter.\nSpecifically, illustrating the workflow of before, beforeEach, test, afterEach, after and the fact that beforeEach may run multiple times in a row. I realize the answer has been 'make your tests serial' it's just not something I expect to run into. At least illustrating that if you have to mess with singletons or global objects or static properties, that beforeEach can cause issues.\nOne thing I'm still not clear on: if I want to write my tests in a parallel way, are the tests run concurrently, or should I be putting the global modifications and subsequent cleanup into the test itself? In which case, running things like sinon.sandbox.restore() can't be done in an afterEach(). ",
    "paulyoung": "As a workaround I've been doing this:\n`` javascript\nexport const same = (t, actual, expected, message) => {\n  return t.same(actual, expected,\n${message}\n\nActual:\n${JSON.stringify(actual, null, 2).split(\"\\n\").join(\"\\n    \")}\n\nExpected:\n${JSON.stringify(expected, null, 2).split(\"\\n\").join(\"\\n    \")}\n\n`);\n};\n```\nand then using:\njavascript\nsame(t, actual, expected, message);\ninstead of:\njavascript\nt.same(actual, expected, message);\n. ",
    "wenzowski": "Doing something similar to @paulyoung \n``` javascript\nimport chalk from 'chalk';\nimport * as JsDiff from 'diff';\n// warning!\n// t.deepEqual() tests attribute order while prettyDiff() does not\nexport function deepEqual(t, actual, expected) {\n  t.deepEqual(actual, expected, prettyDiff(actual, expected));\n}\nexport function prettyDiff(actual, expected) {\n  const diff = JsDiff.diffJson(expected, actual).map(part => {\n    if (part.added) return chalk.green(part.value.replace(/.+/g, '    - $&'));\n    if (part.removed) return chalk.red(part.value.replace(/.+/g, '    + $&'));\n    return chalk.gray(part.value.replace(/.+/g, '    | $&'));\n  }).join('');\n  return \\n${diff}\\n;\n}\n```\ndiff 2.2.2\nchalk 1.1.3\n. ",
    "calebmer": "Is anyone working on this?\n. Bump. For all my projects, my babel configuration is in a dot file, my eslint config is in a dot file, my editor config is in a dotfile, but my ava config has to be in my package.json.\nThis doesn't seem like a hard feature to implement and pretty much every other major tool in the JS ecosystem allows this style.\n. Bump. For all my projects, my babel configuration is in a dot file, my eslint config is in a dot file, my editor config is in a dotfile, but my ava config has to be in my package.json.\nThis doesn't seem like a hard feature to implement and pretty much every other major tool in the JS ecosystem allows this style.\n. A monorepo where I have multiple packages and the package.json is created dynamically at publish time. I want to use ava to test independently all of the packages in the monorepo which don't have package.jsons.\nOr I'm in a JavaScript environment which abstracts away node (like Meteor).\n. Yeah, that would work, but what about a Meteor-like situation?\n. In #520 @jamestalmage said in regards to an .avarc file:\n\nThis is not going to get implemented until someone can demonstrate an actual need.\nMy point here, is demonstrate some actual need. \"Because I want\" is not sufficient. Neither is \"because project X does it that way\". If not having the option is legitimately preventing you from getting things done, and you demonstrate why, we will happily change our minds.\n\nI think @gajus\u2019s situation qualifies as \u201clegitimately preventing\u201d things getting done.\n. > We always consider user's needs, but you're just one user with a very edge-casy need. If we adapted to every edge-case, AVA would be bloated and hard to use. Creating great simple products requires saying no to thousand things.\nBut this isn't one user's edge-casy need, a couple of issues have been opened asking for a better way to define configuration. Whether that be in the CLI (as this issue requests) or in an .avarc. Ava is already hard to use for all the users that are accustomed to dot file or CLI configurations for the reasons @gajus mentioned above.\nCreating great products does require saying no to some things, but also listening to users to discover the things you should say yes to. See #637, #520, and #782.\n. { \"babel\": \"inherit\" }\n. @novemberborn this, combined with a couple of other factors (including interop with Webpack/Webpack loaders and install size) led me to use another test runner so I don't have a sample project anymore \ud83d\ude10\nI really like ava, but I don't think I can use it yet in my React/Webpack projects.\n. The documentation on the timeout option is not clear and when I've tried using it every test file fails even if the timeout was met.\n. ",
    "iamstarkov": "@sotojuan what was the last idea to fix this?\n. then i got it wrong from the .same docs, sorry.\ndeep-strict-equal doesn't have problems with same object values though:\njs\nvar eq = require(\"deep-strict-equal\");\n//   AssertionError: { a: '1.0.0' } === { a: '1.0.0' }\neq({ a: '1.0.0' }, { a: '1.0.0' }); // true\nAm I doing something wrong?\n. So, its because of sorted-object which im using returns Object with null in prototype\n. @sindresorhus i will in next projects, but current one is tightly coupled with npm dependencies and sorted-object is the package which npm is using for sorting dependencies.\n. while upgrading ava from 0.12 to 0.13 messages like\n1. fs \u203a should reject on empty input\n  undefined undefined undefined\nare not really helpful\n. if i'm in favor of pure string rejection reasons, what should i do?\n. because it is a String reason which is promoted on mdn page. but its fine i can change my code.\nat the same time a lot of devs will still use strings as rejection reasons though, and it a bit uncomfortable if testing framework will dictate how promises should be rejected\n. you are right, but i still think that im not the only one and not the last one who will reject using just strings\n. i would suggest:\n- more descriptive ava error\n- allow to reject strings\n. I like property based testing too and used jsverify as in ava-jsverify by @imranolas. it worked nice but also augments Ava internals (I believe). so integration can be harsh sometimes.. I was thinking about utilising macros for this purpose. do you think it is feasible?. @novemberborn how hard is it to support non-binary snapshots?. @novemberborn it drives people away from Ava to jest in my company =(. @novemberborn they are accustomed to jest snapshots and have routines based on text-based snapshots, so they don't understand why Ava needs snapshots to be binary and quite unhappy about it. @novemberborn like it. ran into the same issue today. @sindresorhus thank you. sounds good. I think I have a fix. Yes, it is a Symbol now. We fixed it by fixing this line https://github.com/concordancejs/concordance/blob/master/lib/formatUtils.js#L6:\n```diff\n-  return fromTheme.open + value + fromTheme.close\n+  return fromTheme.open + value.toString() + fromTheme.close\n```. is it good enough fix?. it will work for all versions of react with Fragment. well, Im sorry. you lost me on registered symbol. \nBut I still think it will be a good idea to defend lib/formatUtils.js@wrap against non string instances. What do you think?. ",
    "creeperyang": "SO no one is working on this?\nLong string diff is really hard to read with ava@0.17.\nJust a pic:\n\n. Great! \ud83d\udc4d . js\nconsole.log('------>', test.error);\nbash\n------> { name: 'AssertionError',\n  actual: false,\n  expected: false,\n  operator: 'fail',\n  message: \n   { name: 'Error',\n     stack: 'Error: ENOENT: no such file or directory, open \\'app.png\\'\\n    at Error (native)',\n     message: 'ENOENT: no such file or directory, open \\'app.png\\'',\n     errno: -2,\n     code: 'ENOENT',\n     syscall: 'open',\n     path: 'app.png' },\n  generatedMessage: false,\n  stack: 'AssertionError: Error: ENOENT: no such file or directory, open \\'app.png\\'\\n    generateSprite.then.then.catch.err (test/sprite.spec.js:103:11)\\n    ' }\ntest.message is an Error object.. It eventually looks like caused by calling t.fail(err). I pass an Error object to t.fail and it seems we should use string here.\nHowever I think it's good to support both error message and error object.\njs\nx.fail = function (msg) {\n        if (msg instanceof Error) {\n            msg = msg.message\n        }\n    msg = msg || 'Test failed via t.fail()';\n    test(false, create(false, false, 'fail', msg, x.fail));\n};. \ud83d\udc4d much better.. ",
    "mayankchd": "Failing on Node.js 0.10 as it doesn't support Set and Map\n. Thanks! I like ava a lot!\n. ",
    "isaacs": "tap's default reporter does use quite a bit of red and green.  Mostly, the difference between red and green is also communicated by the text itself, so it's fine.\nWhere it gets tricky is in the diff coloring, where red and green colors carry a lot of the semantic weight.  I sought out various sorts of less-color-sensitive folks on Twitter, and used Sim Daltonism to verify that the + and - coloring was distinct in every setting.\n\n. We (and by that I mean, mostly I) need to get back to pushing TAP 14 over the finish line.  There were some good bits of feedback from the last round of comment on the PR, and I just haven't gotten around to incorporating them yet.  I should probably spend more time on the spec, and a little bit less on node-tap, I suppose, but when I'm not doing my \"real\" job, I try to just let the spirit guide me :)\nAt the current state of things, the latest and greatest versions of node-tap, prove, and Test::More are pretty much in alignment.\nThe reason why node-tap's output says \"TAP version 13\" is because the TAP 13 spec requires it!  However, TAP is a very \"anything goes\" protocol, so the other stuff like nested tests and such is pretty ambiguous.  Anything that is prefixed by whitespace that isn't yaml should be ignored by a consumer, so that's why child tests work.  If you're not down with them, you just get the roll-up, and it's less informative, but not incorrect.  (Even the idea of whether # 4 tests, 2 pass is a \"comment\" or a \"directive\" is very unclear, but it is clear that consumers should do nothing with it.)\nI'd recommend, especially in the JavaScript world, to follow the not-yet-fully-birthed TAP14 specification.  What you actually print on the version line is mostly irrelevant.\n. ",
    "therealklanni": "Certainly\n. Oh, I added it in assert.js before I saw your edit.\n. Np, guys, thanks for this great tool! :metal: \n. @sotojuan please confirm\n. @sotojuan please confirm\n. According to the spec\n\nIf the whole test file succeeds, the count of skipped tests is included in the generated output.\n\nIt seems like it should always output. (Also see fail above reports 0 rather than not reporting at all).\nSeems legit?\n. Related\n. FYI that's not raw TAP output, that's formatted, so I don't think that image is a good reference. However, the source line I posted seems to indicate it skips printing skips unless there was more than 0.\n. Looks good, thanks for knocking that out @sotojuan :facepunch: :metal: \n. See above. No skip count in the raw TAP output (thereby affecting TAP reporter).\n. I agree that it's the job of the reporter to determine how to interpret the output, so yes, I think AVA should give an accurate output (i.e. report all skips).\n. @sindresorhus I can possibly take a look at this sometime this week.\nIf anyone else wants to give it a go before I get to it, feel free!\n. Oh, so why does it (AVA) print \"TAP version 13\"? ;)\n. I couldn't find anything about v14, can you post a link?\n. Thanks for the link\n. I think we should also take into consideration that if we're going to use a draft specification, the TAP output that AVA produces will likely break most reporters in some way. I would be more comfortable with sticking to v13, personally, because I want to use tools that are available today and not have to wait for things to get updated to v14 (which as @sindresorhus points out has been sitting in discussion for several months). And it stands to reason that v14 will be backwards compatible to v13, but not the other way around (i.e. a TAP 14 reporter can easily/correctly interpret v13, not vice versa).\n. Well, I said that based on the fact that other reporters like faucet seem to get confused in certain situations (maybe it's just an opportunity to get faucet up to v14 spec, then, and any others that can't parse the newer spec output can probably be considered edge cases). I'm fine with whatever the consensus is, just wanted to open the discussion. Thanks for the input. :grinning: \n. That in particular \"works\" (in the sense that it just ignores it and doesn't suppress the line).\n. Thanks @isaacs\n. Yeah, a .finally method would make sense. I had to check the docs last night when I realized .after wasn't working (the way I thought it was meant to) to see if I had missed some other method.\nSo I would assume that the appropriate way to handle it would be to make .finally be the one that runs at the end no matter what. Fair assumption?\n. Seconded @vdemedes's last comment\n. @sindresorhus can we get some help wanted etc labels on this? :smile: \n. No worries, thank you\n. @sindresorhus I started working on this a few weeks ago. Keep meaning to get back around to it, but I'm just swamped lately. Rather than leaving it stagnant any longer I wanted to let you know that I probably won't have time to work on it (certainly not any time soon). I just have too much going on right now, sorry.\n. I haven't started on it yet, FYI. Been busy working on something else. Might be a few more days before I get around to it.\n. Fair enough. I'll update.\n. ",
    "tusharmath": "I guess you are right. Its not skipping, instead its waiting for it to complete till eternity. I guess there should be a default timeout for async tests.\nHere is the link to the never ending build - https://travis-ci.org/tusharmath/react-announce-fetch/builds/101795216\n. Same problem. Had to switch back to version 0.9\n. I am facing the same problem here  \u2014 https://travis-ci.org/tusharmath/Multi-threaded-downloader/builds/117959834\n. Tests run fine locally. I'm not sure what the issue is. Could you elaborate\na little more please?\nOn Wed, Mar 23, 2016, 8:01 PM Mark Wubben notifications@github.com wrote:\n\n@tusharmath https://github.com/tusharmath it doesn't output the\nuncaught exception though. Maybe you're running into the issue discussed\nhere: #604 (comment)\nhttps://github.com/sindresorhus/ava/issues/604#issuecomment-199516941\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/511#issuecomment-200368506\n\nRegards,\nTushar Mathur.\nwww.tusharm.com\n. @novemberborn Some pointers to mitigate the problem would be a great help. I went thru the discussion but I am unable to co-relate it with my issue.\nTake a look at this https://travis-ci.org/tusharmath/Multi-threaded-downloader/builds/118039275\nThis is without coverage and it just works.\n. That din't work either. Infact I tried creating folders and run them one by one also. Did not work.\n. \n",
    "uhoh-itsmaciek": "Oh interesting. Thanks for the clarification @vdemedes.\n. ",
    "0x1mason": "I've implemented a simple solution to this problem, similar to Tape's test::comment. Here's the basic pattern using a wrapper:\n```js\nfunction testWrapper (title, fnc) {\n      return ava (title, function (avaTest) {\n         let _logs = [];\n     // This is a quick and dirty monkey patch to ensure logs print together after\n     // the test has completed. The downside is logs are not printed in real time.\n     avaTest.comment = function () {\n        let msg = \"      # \" + util.format.apply(null, arguments);\n        _logs.push(msg);\n     }\n\n     let test = avaTest._test;\n     test._exit = test.exit.bind(test);\n     test.exit = function () {\n        return Promise.resolve(test._exit())\n           .then(res => {\n              if (_logs.length > 0) {\n                 process.stdout.write(_logs.join(\"\\n\")+\"\\n\");\n              }\n              return res;\n           });\n     }\n     return fnc(avaTest);\n  }\n\n}\n```\nOutput:\nshell\n  \u2714 test_foo \u203a Lorem ipsum dolor sit amet (15.5s)\n        # Sed ut perspiciatis unde omnis iste natus error\n        # Nemo enim ipsam voluptatem quia\n  \u2714 test_bar \u203a Ut enim ad minim veniam (31.4s)\n  \u2714 test_baz \u203a Quis autem vel eum iure reprehenderit (36.7s)\n        # Neque porro quisquam est\n        # At vero eos et accusamus et iusto odio dignissimos\nIt needs full integration into the Ava code  (e.g., lose the wrapper, add to lib/test.js, etc), but I'd be willing to submit this as a proper PR.. Nobody on my team has complained about having to use t.comment instead of console.log, but it's a pretty tiny sample of Ava users. :) We use it everyday as part our CI and have never had any issues. \nI still use console.log sometimes when debugging, but I use it with test.only so asynchrony isn't an issue.\nIt also just occurred to me that for people already using console.log, transpilation/domain magic would change behavior in existing test setups, which might not be desireable in all cases.\n. As far as console.log vs t.comment, I think console.log is unusable for concurrent tests, esp if there are a number of logs. Some of our team use the logging as a sanity check -- you know test passed, but do you know the test passed? ;) They like lots of logs, giving a similar effect to the way Tape prints each assertion. Fine by me.\nIncidentally the code above is essentially the same pattern I used to implement request local logging in an express js server, the difference being the additional use of domains.. log works for me.\nOn Dec 4, 2016 10:33 AM, \"Vadim Demedes\" notifications@github.com wrote:\n\nHow about t.log() instead? Shorter and simpler.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/avajs/ava/issues/420#issuecomment-264710570, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AEX_-1w-3WpCNjJewPEGGRia-137fv7iks5rEt1egaJpZM4HEVoZ\n.\n. \n",
    "kristianmandrup": "@jamestalmage I like your proposal. I think it should be implemented as an extension to ava.\nYou could even add some conventions, like *.test.fork.js for test files where all tests should be forked or even a /*-fork folder?\n. @ivogabe Ava with typings in VSCode!?\nI'm trying to use VSCode as my editor in a project using both ava and mocha for testing.\nI have the following setup:\n{\n  \"compilerOptions\": {\n    \"module\": \"commonjs\",\n    \"target\": \"es2015\",\n    \"moduleResolution\": \"node\",\n    \"allowSyntheticDefaultImports\": true,\n    \"noImplicitAny\": true,\n    \"allowJs\": true,\n    \"types\": [\"mocha\", \"node\"]\n  },\n  \"exclude\": [\n    \"node_modules\"\n  ],\n  \"files\": [\n    \"typings/index.d.ts\"\n  ]\n}\nwith a typings folder containing index.d.ts with the following install via typings binary\nF.ex typings install dt~mocha --global --save\n/// <reference path=\"globals/chai/index.d.ts\" />\n/// <reference path=\"globals/mocha/index.d.ts\" />\nFrom the Typescript section of the ava docs, I'm led to believe it will auto-detect the ava typing files using my tsconfig.json and I would assume that:\njs\n  \"files\": [\n    \"typings/index.d.ts\"\n  ]\nWould somehow make the global typings available for the linter in my files? or only for compile?\nDo I need the weird triple /// syntax in each test file in order for test to not cause a linter error?\n/// <reference path=\"../../typings/globals/ava/index.d.ts\" /> etc.\nThis would be ridiculous to maintain!! Please advice and provide better docs on this aspect. Thanks!\n. Using the following \"match pattern\"\njson\n  \"ava\": {\n    \"files\": [\n      \"test/**/*.test.js\"\n    ],\nava still tries to run files that are not test files and should not be matched, such as test/swagger/_utils/utils.js. What am I missing?\n```\n$ ava test/swagger/_utils/\nTAP version 13\nNo tests found in test/swagger/_utils/utils.js, make sure to import \"ava\" at the top of your test file\nnot ok 1 - No tests found in test/swagger/_utils/utils.js, make sure to import \"ava\" at the top of your test file\n``. With the same settings? Yes, it works only if you runava` without specifying a subfolder of tests to run, otherwise the pattern is matched relative to the path.\n$ ava test/swagger/_utils/\nI believe the pattern should always be applied to the full filename of each file encountered, not the file name relative to the base path, ie. in this case test/swagger/_utils/. Would be nice if there was just a little documentation on how this works. Thanks ;). OK, will do ;). ",
    "lavie": "@novemberborn chiming in. :)\nSo, our use case for this (wonderful idea, btw) is testing various hooking of Node.js standard streams (stdout/err). We do that kind of stuff a lot because we run arbitrary user code in a sort of sandbox, and we need to hook the streams in order to process them and present them to the user. Needless to say, such hooking is really problematic if you try doing it more than once in the same process. So a fork per test would be wonderful. I admit it's not a mainstream use-case, but if any of you guys could think of a way of doing this without actually building it into Ava, then I would be very thankful. Right now we have to resort to \"docker container per test case\" level hacks, and it's not fun. :(\n. ",
    "naptowncode": "Just to clarify, the intent is to lose the AssertionError: false == true line?\n. I tried just stripping the first line of the stack trace but @vdemedes pointed out that it would prevent telling the difference between different types of errors. I see two possibilities:\n1. Just use err.stack without err.message, since the message will be in the first line of the stack trace anyway, or\n2. A special case that strips the first line of the stack trace only for an AssertionError. This isn't my first choice but I see there is already a special case for AssertionError in Api._handleTest().\n. Given the discussion around #565, how should the use of .only() affect the rest of the tests? Should they be counted in stats as skipped, just as if .skip() were used? Or something else altogether?\nI'm just asking what the API should do when reporting stats, regardless of what the reporter decides to do with those stats.\nI would assume that .only() would cause all other tests to be treated as skipped. TAP doesn't seem to have a concept of ONLY, just SKIP and TODO.\n. Given the discussion around #565, how should the use of .only() affect the rest of the tests? Should they be counted in stats as skipped, just as if .skip() were used? Or something else altogether?\nI'm just asking what the API should do when reporting stats, regardless of what the reporter decides to do with those stats.\nI would assume that .only() would cause all other tests to be treated as skipped. TAP doesn't seem to have a concept of ONLY, just SKIP and TODO.\n. But some information about the rest of the tests should be sent to the reporter, even if the reporter chooses not to output it to the user.\nSkipping those tests would feel consistent with the implementation of --match in #477.\n. Ah, I see. Should I leave out the error.message instead? The message will always be in the first line of the stack trace anyway.\n. @sindresorhus Done, though AppVeyor seems to not like it.\n. Looks like this could be done by sending the hasExclusive flag along with testCount in the ava-stats message. Then if api.run() gets any messages with hasExclusive: true it could send a runOnlyExclusive: true flag to the ava-run message. Does that sound right?\n. @sindresorhus @vdemedes @jamestalmage Added unit test. Should be ready to go.\n. @sindresorhus @vdemedes @jamestalmage Added unit test. Should be ready to go.\n. The test uses three files: One file that has only an exclusive test, one file that has an exclusive test and a non-exclusive test, one file that has two non-exclusive test. Total of five tests across three files with two exclusive tests in different files.\n. The test uses three files: One file that has only an exclusive test, one file that has an exclusive test and a non-exclusive test, one file that has two non-exclusive test. Total of five tests across three files with two exclusive tests in different files.\n. I'm not actually sure how it works with --watch. At the very least I should add some documentation for that.\n. @novemberborn I've incorporated those tweaks. If everything looks good, merging this would make it easier to work on #471. Thanks!\n. @novemberborn I've incorporated those tweaks. If everything looks good, merging this would make it easier to work on #471. Thanks!\n. @novemberborn @vdemedes @sindresorhus Updated and rebased. Also added a unit test that should have been there earlier for the .runOnlyExclusive option to Runner.run().\n. Correct, although I'm not super keen on just dropping this info on the floor. I just went for consistency with how non-exclusive tests are dropped within test-collection. I'll add a comment to that effect.\nThat said, I would favor changing the reporting completely as suggested by #471.\n. Yup, same logic. Again, not too keen on it but couldn't think of a more elegant solution. Suggestions welcome!\n. Good call. That's almost certainly a better approach.\n. It does seem redundant but it's to avoid responsibility creep. _handleStats() is concerned with gathering the total testCount before tests run while this method is waiting for each test to emit stats before starting them all. Probably the stats event itself is overloaded by using it for both purposes.\nI'll leave _handleStats() as it is but wrap the tryRun() on test.catch().\n. This options is just being passed along directly to Runner.run() via the IPC handler in index.js. Does that mean Runner.run() should also assume it's always being called with an options object?\n. To keep to test execution concerns contained in tryRun() and separate from the bookkeeping concerns in _handleStats(). But it's not a big hangup for me. I'll go ahead and update tryRun() to use self.hasExclusive.\n. ",
    "pablodenadai": "Cool, thanks for the reply @vdemedes.\nThe test below would break when using Babel.\n~~const favorite = 7;~~\n~~try {~~\n    ~~favorite = 15;~~\n~~} catch (err) {~~\n    ~~t.is(favorite, 7);~~\n~~}~~\n~~//=> Error: \"favorite\" is read-only.~~\nUpdated:\njs\nconst favorite = 7;\nfavorite = 15;\nt.is(favorite, 7);\n//=> Error: \"favorite\" is read-only.\n. Cool, thanks for the reply @vdemedes.\nThe test below would break when using Babel.\n~~const favorite = 7;~~\n~~try {~~\n    ~~favorite = 15;~~\n~~} catch (err) {~~\n    ~~t.is(favorite, 7);~~\n~~}~~\n~~//=> Error: \"favorite\" is read-only.~~\nUpdated:\njs\nconst favorite = 7;\nfavorite = 15;\nt.is(favorite, 7);\n//=> Error: \"favorite\" is read-only.\n. Hi @sindresorhus, I don't believe it's invalid JS - at least not in Node from what I can see. \nTry running the following code in the Node REPL.\njs\nconst foo = 7;\nfoo = 15;\nconsole.log(foo);\n//=> 7;\nNo errors were thrown.\nPs. Maybe this piece of code is only adding complexity to the issue. The original intent for this ticket was the question around disabling Babel.\nThanks!\nEdit: Node v5.3.0\n. Hi @sindresorhus, I don't believe it's invalid JS - at least not in Node from what I can see. \nTry running the following code in the Node REPL.\njs\nconst foo = 7;\nfoo = 15;\nconsole.log(foo);\n//=> 7;\nNo errors were thrown.\nPs. Maybe this piece of code is only adding complexity to the issue. The original intent for this ticket was the question around disabling Babel.\nThanks!\nEdit: Node v5.3.0\n. Great, thanks for taking the time to explain it. Love your work.\n. Any updates on this one @ariporad?\n. ",
    "nexdrew": "Having this same problem. Tests pass with babel-plugin-transform-regenerator@6.3.26, tests fail with babel-plugin-transform-regenerator@6.4.3.\nThe workaround described in the babeljs phabricator link works for me: change const to let within async functions.\n. Having this same problem. Tests pass with babel-plugin-transform-regenerator@6.3.26, tests fail with babel-plugin-transform-regenerator@6.4.3.\nThe workaround described in the babeljs phabricator link works for me: change const to let within async functions.\n. ",
    "ewnd9": "Got it too. It also reproduces on local system after clean npm install \n. ",
    "AlbertoFuente": ":+1: Totally agree\n. ",
    "SimonDegraeve": "No problem, make sense.\n. ",
    "yatki": "I wanted to create an html reporter like mochawesome. I thought maybe i can write an transformer for converting tap output to mocawesome json structure. I searched for a solution all day but the output i get from the tap reporter is really primitive . For instance, I even couldn't find a way to get duration per test which i could easily get with a custom reporter. I'm not talking about getting executed script in the test, or it's context for attaching custom messages to output.\nWhat is bad about letting people to write their own custom reporters? @jamestalmage mentioned a stable reporter API. It looks pretty stable to me :)\nCan't you reconsider to support proper custom reporters?. @novemberborn Thank you very much for the reply. Having NDJSON reports sounds really cool. I'm really happy using AVA so far and totally thankful for your efforts. I still don't understand why you don't want to support more reporters within AVA, but that's ok. I don't want to take more of your time, I'll go with a fork for now. \nBut as far as i'm concerned supporting custom reporters seems not a big effort and this would absolutely give no harm or disadvantage to AVA. Authors of custom reporters are responsible from quality/performance of their reporters, and developers who use those custom reporters can always go back to use native AVA reporters.\nBut anyway. Have a great day. Thanks.. Gentlemen, you had my curiosity\u2026 but now you have my attention.\nHow would you like to proceed to define requirements and when would you like to start? Maybe instead of working on a solution which is gonna work only for me, I can contribute on this? Would you like to proceed with a separate issue ?. @novemberborn I was referring to additional TAP information actually. Because i thought It could be faster to implement. But I could be interested to contribute on NDJSON report specs too. I just don't know how data structure should look like.. @novemberborn I totally agree on that is not an easy task. \nHowever, I'd like to have more info about the tests that we are running not just to have a prettier results but also for statistics. For instance, which tests are constantly taking more time or failing more often. Also we need to integrate the process with monitoring tools to see running tests in real time because we have different projects running different tests suites and console outputs are not enough. These features are not things that I'm making up right now. We really need these and we had them with previous testing frameworks.\nIf there is a way to detailed test info and log it with t.log(), I'm even willing to write my own functions to put additional info to tap output and write a script to parse them (until ava have a better solution). But i couldn't find a way to do it. Or I'll go with the first idea. I'll fork the project and implement my own reporter which is going to bring a bit pain, in terms of keeping ava updated.\nBut to keep it short: ava report outputs have to be improved or ava should support custom reporters and let people to collect whatever information they like. Having a powerful testing framework like ava, but not having control on results is just unacceptable :). ",
    "kasperisager": "No problem!\n. All tests pass on my end when it's removed.\n. ",
    "thisconnect": "One more side note: Since changing to ava I lost the pretty coloring of DEBUG=kit*\n. Same issue after updating AVA (and Node.js at the same time) \nOS X 10.11.5\nNode.js v6.2.0\nAVA ava@0.15.1 \nrm -rf node_modules/\nnpm i\nfixed it\n. ",
    "adriantoine": "To test with webpack loaders, there is this plugin: https://github.com/istarkov/babel-plugin-webpack-loaders, we use it for unit testing and backend rendering of react components using the css loader.\nHas anyone considered using enzyme with ava?\n. @MoOx do you have an example of using ava with expect-jsx? Sounds interesting\n. By the way, unexpected-react doesn't work with react@15 because it relies on react/addons which has been deprecated in react@0.14 and removed in react@15.\n. and @MoOx solution works quite well!\n. @thangngoc89 there is one in the first part \"Setting up Babel\": https://github.com/sindresorhus/ava/pull/747/files#diff-2cb79c7fb78b66228297358846395c3aR7\n. @sindresorhus thanks! I'll have a look at those comments\n. @MoOx https://github.com/istarkov/babel-plugin-webpack-loaders#dynamic-config-path I think it should be added to the babelrc recipe, rather than the react one.\n. @MoOx ah yeah, I'll add an example with an event\n. I fixed the issues raised by @sindresorhus by the way\n. @MoOx I've added an example with an event simulation: https://github.com/sindresorhus/ava/pull/747/commits/a729aeba805caceb588287d80408580b6a4a163d I've used sinon like in the Enzyme example to have some sort of equivalent, let me know what you think\n. So I removed sinon as @MoOx has suggested, fixed \"library\" => \"libraries\" and removed js formatting from the npm install command as @kentcdodds has suggested\n@kentcdodds should I add a reference part with a list of links or should I add it to the intro?\n. @MoOx I agree that the examples are not great, they were roughly copied from several sources and I guess I should have made my own set of examples. I will update the document tonight.\nHowever I think it's better to show very basic examples in this recipe and link to your test file or your repo's documentation for further usage (same for Enzyme) because this is AVA's doc, not Enzyme's doc or jsx-test-helpers doc, we should just show how to integrate AVA with those tools, then you're free to update your jsx-test-helpers documentation to show further examples or use cases of your library.\n. @kevva @sindresorhus about spacing and formatting, is there any rules I should follow? Maybe the ones from eslint-config-xo-react?\n. Ok so, I changed all examples and they are now very basic, the component I am testing is in the example as well, for clarity.\nThe reason why the examples are very basic is that, this is AVA's documentation, so it's good to show that those libraries exist and that they work well with AVA, and I'm showing how to set them up, but it shouldn't be about showing every single use of those external libraries, for that I added a link to their documentation.\nFrom now on, if you're still not happy with one of the example, please provide one to replace it.\n@kevva Both examples are valid on xo and xo-react, except the function called JSX in @MoOx's library, because xo doesn't allow functions to be capitalised.\nCould you all review the recipe again?\n. Ok so, I changed all examples and they are now very basic, the component I am testing is in the example as well, for clarity.\nThe reason why the examples are very basic is that, this is AVA's documentation, so it's good to show that those libraries exist and that they work well with AVA, and I'm showing how to set them up, but it shouldn't be about showing every single use of those external libraries, for that I added a link to their documentation.\nFrom now on, if you're still not happy with one of the example, please provide one to replace it.\n@kevva Both examples are valid on xo and xo-react, except the function called JSX in @MoOx's library, because xo doesn't allow functions to be capitalised.\nCould you all review the recipe again?\n. Thanks @sindresorhus and @forresst, it's all fixed!\n. Thanks @sindresorhus and @forresst, it's all fixed!\n. Another thing is, an update has been made to unexpected-react to make it work with React 15, I've created a repo to try it with AVA. It works well and I quite like the colourful JSX output:\n\ncompared to jsx-test-helpers:\n\nOn the other hand, you have to install unexpected and unexpected-react to your project which are both really big compared to jsx-test-helpers, so it might not suit everyone.\nShould I mention unexpected-react just like expect-jsx has been mentioned? Is it worth an extra section? (considering expect-jsx doesn't have its own section)\n. Another thing is, an update has been made to unexpected-react to make it work with React 15, I've created a repo to try it with AVA. It works well and I quite like the colourful JSX output:\n\ncompared to jsx-test-helpers:\n\nOn the other hand, you have to install unexpected and unexpected-react to your project which are both really big compared to jsx-test-helpers, so it might not suit everyone.\nShould I mention unexpected-react just like expect-jsx has been mentioned? Is it worth an extra section? (considering expect-jsx doesn't have its own section)\n. I've updated the recipe with a new section for \"Other assertion libraries\" where I put expect-jsx and unexpected-react.\nI need another round of review:\nhttps://github.com/adriantoine/ava/blob/master/docs/recipes/react.md\nThanks!\n. Great thanks! \ud83d\ude04 \n. @forresst yep, I'm working on it!\n. @forresst Ah sorry, I assumed I had to do it myself since I made that translation!\n. Ah yeah makes sense, same for the Enzyme example, I will update\n. That was copied from @MoOx example, it's a practice that is documented in eslint-plugin-react's jsx/no-literals rule.\nI'll change the example's anyway, as I said yesterday (didn't have time to do it yesterday evening)\n. ",
    "istarkov": "Babel plugin webpack supports aliases and modulesDirectories.\n. Yes.\n. Yes.\n. Yes babel-plugin-webpack-loader could be used to resolve such imports\nNot sure it supports aliases (as I does not use aliases), but it supports modulesDirectories webpack directive.\nAll aliases in @apedyashev example could be changed on just one modulesDirectories entry.\nmodulesDirectories: [\n  PATHS.app,\n]\n. https://github.com/istarkov/babel-plugin-webpack-loaders/blob/master/README.md#important-note\n. @novemberborn I still use it, and not for testing only, so project is not 100% dead, something like 99% dead. But I have no idea why this issue is here ;-) IMO some misconfiguration or something like this, but without having access to real project I can't say much. \n. ",
    "iam4x": "I adopted another solution using js-dom, enzyme and chai-enzyme:\n- package.json ava config\n- helpers / setup jsdom\n- helpers / mount component\nand tests are written this way:\n- simple concurrent tests\n- serial tests with xhr calls\n(my example has also CircleCI configuration with coverage)\n. ",
    "boutros": "It would also be great to have a recipe of how to use ava with rollup. \n. > Rollup is a module bundler\u2014surely you aren't bundling AVA for production right?\nNo, not for production of course. Anyway I solved it with rollup, so my remark can be ignored. If anyone is interested here is the relevant commit: https://github.com/boutros/koblersken/commit/c00e3a098c035c7a585ab72d1dde91fd2ac47e91\nIt's easy to get lost in the javascript build jungle :)\n. ",
    "backspaces": "TL;DR: If you're using Rollup and no babel in your workflow, building a cjs bundle and requiring it in your tests works fine.\nDetails:\nI've just started using ava and ran into the es6 modules problem. But because my repo uses Rollup to a cjs bundle (to be node friendly) I just require that in the test file. Ava is OK with that.\nHere's an article on the \"modern workflow\" idea where Rollup is used to convert a bleeding edge project into any format you'd like: \nhttps://medium.com/@backspaces/es6-modules-part-1-migration-strategy-a48de0b7f112\n. Would a solution to this issue be something like:\n\nhttps://developers.google.com/web/updates/2017/06/headless-karma-mocha-chai\n\nThus far I've found ava, puppeteer, live-server work (live-server because file:///urls don't work in general). Early days but so far so good. I haven't tried headless-karma-mocha-chai yet. But https://github.com/avajs/ava/issues/1619#issuecomment-356869077 seems to be a good approach for puppeteer.\nI gotta say real in browser testing like headless-karma-mocha-chai suggests, but with ava, would be SOOO abs fab! I'm not clear, however, on what the testing ecology requires to get ava running via karma.. So how do you use ava with puppeteer?\nI would be OK creating a web page for each test file but don't know how to import ava into the browser. Maybe a browser based assertion library? Like http://chaijs.com/guide/installation/#browser. Here's my goal: to somehow start an ava/node repl, in interactive mode. In other words somehow start node/ava, and be left in node so that I can try things within the ava-created environment.\nWhat I mean by that is that I start node, then require('ava.js') and be left in interactive mode to try things just like when using node in interactive mode.\nIf I try: node --interactive --inspect node_modules/ava/profile.js I get Error('Specify a test file').\nOr this node --interactive --inspect node_modules/ava/profile.js test/test.js it runs the tests, printing out\nDebugger listening on ws://127.0.0.1:9229/005415cf-dc16-455b-bcc6-dc76cb96948f\nFor help see https://nodejs.org/en/docs/inspector\nTEST: foo null\nTEST: AS null\nRESULTS: { failCount: 0,\n  knownFailureCount: 0,\n  passCount: 2,\n  skipCount: 0,\n  testCount: 2,\n  todoCount: 0 }\nbut exits, not remaining in the repl.\nHow do I simply start ava/node in such a way it remains in the repl, in interactive mode?. OK we're getting there.  I inserted debugger exactly as above. Then ran the node command:\nnode --interactive --inspect-brk node_modules/ava/profile.js test/test.js\nDebugger listening on ws://127.0.0.1:9229/0a853d55-8246-46ce-b7c8-1d8d9001e3d3\nFor help see https://nodejs.org/en/docs/inspector\nDebugger attached.\nI then opened chrome://inspect which looked like this:\n\nI clicked on the inspect link and this appeared:\n\nBy press continue, do you mean click on the blue debugger continue button?\n\nWhen I do that I get:\n\nThat looks pretty hopeful! When I click on console I get:\n\nBut when I try to require my cjs rollup I get:\n\nIt does appear to be the right path, using others gives errors. And lots of node works, like using the process.cwd() & __dirname give the right values, and fs=require('fs') works fine.\nSo it seems like I'm in the wrong scope or something similar??\nThanks a LOT for your patience, a lot of this is new to me so I'm likely doing something silly.\n. ",
    "ghost": "I note that one of XO's pluses is to pull config out of .foorc and into package.json where it can be more portable and visible. Since latest npm uses flattened module dirs now, it seems like it would be worth just having everything together in package.json in an \"ava\" field, and making ava responsible for having the deps it needs to handle the config it understands.\nThe \"env\" \"ava\" fields in the above .babelrc seems like a good structure for that. (I don't actually feel that strongly about where it goes, either--just that isolating the config to the limit of ava's concern seems prudent.)\n. This didn't work either:\nrequire(\"babel-register\")({\n  optional: [\"es7.decorators\"]\n});\n. @philmill: Does that still happen if you do something like\njs\ntest('parent', (t) => {\n  test('child', (tc) => {\n    tc.fail();\n    t.end();\n  });\n});\n?\n. Sadly, browser-run can't seem to fit ava through its pipe and into electron.\ntest file (test-test.js:\n```\nimport test from 'ava';\ntest('testing works', t => {\n  t.ok(true);\n})\n```\nbrowserify test-test.js - t [babelify --presets [es2015 react ]] | browser-run hangs on that file, despite browserify and babelify in the global namespace.\n. any news about this feature ?. @sindresorhus Any timeframe regarding when #880 are going to be merged? Seems stall atm. 3 months since the PR was created.\n. Ava itself. Es2015 standard now.\n. okay, package.json needs to be around AND it must be a valid JSON as well so an empty file doesn't fit.\n. Would be good if a package.json would be created in the cwd if it is not there\n. unit testing framework. Thanks. A Brilliant Idea\n. @novemberborn , Is there a way to run only the changed tests in watch mode so that all tests are not run again ?\nThis could make it comfortable on the console.. Thanks\n. @vjpr, you can exclude certain tests in a file by adding .skip to the test. For example,\njs\ntest.skip('I will not run', t => {\n  t.fail()\n}). @Briantmorr orr, I would prefer directory.\nAny thoughts on this @novemberborn . :+1:  to @sindresorhus . I don't understand the advantages of using ESM.  I was looking at the docs and it just looks like syntactic sugar.  Am I missing something?. I think we'll find that every developer has their own opinion on testing.  I like the suggestion by @incompl though.  Perhaps we could have a survey?. ",
    "danielhusar": "Any news on this ? It will make testing react components easier :)\n. Any news on this ? It will make testing react components easier :)\n. \ud83d\udc4d would be probably nice to add it to documentation\n. ",
    "davej": "\nAnd incorrect.\n\nDo you mean the lack of a return? Arrow expressions automatically return, so return is not needed.\n\nI think the serial tests are already plenty explicit about their dependencies. They're named serial tests and run serially in the order you specify them.\n\nPerhaps I didn't explain well, the major benefit is that the you can have tests that rely on other tests while still being run in parallel. For example:\n``` js\nimport test from 'ava';\nimport fn from './';\ntest('title', t =>\n  fn('foo').then(data => t.ok(data))\n).then(() =>\n  test('title2', t =>\n    fn('bar').then(data => t.ok(data))\n  )\n);\ntest('title3', t =>\n  fn('fez').then(data => t.ok(data))\n).then(() =>\n  test('title4', t =>\n    fn('baz').then(data => t.ok(data))\n  )\n);\n```\nIn this example title1 and title3 are run at the same time. I'm not sure this is possible with serial tests? It's not really for performance reasons that I'm suggesting this change though. To me it's clearer which tests rely on other tests and makes more sense than a long string of serial tests which aren't necessarily connected.\nEdit: If prettiness is an issue then I guess it might be possible to use async/await for cleaner syntax?\n. > Ideally your tests should be atomic and not have a dependency on other tests. test.serial is mostly there to fill a small niche when you have to modify some global state or something can't handle the concurrency.\nSometimes your lib has side-effects (e.g. writing to the file-system) and you want to test how your lib behaves after the side-effects because it's often a prime spot for edge-case bugs. It's perfectly possible to do it with test.serial (although you would lose concurrency), it's just that I find it's clearer to explicitly express the dependency.\nHere's a real-world example form a module I just started to work on:\njs\ntest('tag folders', t =>\n  fn(__dirname).then(tagged => {\n    t.ok(tagged.dependencies);\n    t.ok(tagged.devDependencies);\n    t.is(tagged.dependencies[0].tag, 'blue');\n    t.is(tagged.devDependencies[0].tag, 'yellow');\n  })\n).then(() =>\n  test('remove tags from folders', t =>\n    fn(__dirname, {clear: true}).then(tagged => {\n      t.ok(tagged.dependencies);\n      t.ok(tagged.devDependencies);\n      t.is(tagged.dependencies[0].tag, 'clear');\n      t.is(tagged.devDependencies[0].tag, 'clear');\n    })\n  )\n);\n. Also seeing this stack trace when used with pify(fs.writeFile). The rejected writeFile promise is an error (err instanceof Error === true), so I'm not quite sure what's going on.\n. ",
    "masaeedu": "The bad thing about serial tests is implicit dependencies, not the fact of having dependencies at all. If you're doing slow integration tests and you can save time by reusing work from another test, it's a good idea to do that.\nI'm not a fan of doing a promise based thing like this, but declaring named dependencies between testcases and expecting the test runner to traverse the graph in dependency order is not unreasonable.. @sindresorhus Might it be a good idea to publish a continuous release on npm? I'm in the same boat, been waiting for a committed feature since August.. @novemberborn Is there already an issue for a proper interactive UI? I had no idea there was a u option since it didn't seem to be documented, and the console UI provides no indication that this shortcut is available.. Is this resolved now that #1489 is merged?. >  It's not necessary for most users.\n@novemberborn It is necessary for me. I'm using TypeScript, which prevents me from using the git URL package reference.. Yes, it happens with ava@next as well. See the repro snippet I linked above; tweaking the version to \"ava\": \"next\", rerunning yarn and yarn test -u still results in all the files getting modified.. @novemberborn I'm not sure I understand the question, sorry. Are you looking for the first commit in the ava codebase that introduces the problem?\nIf you're looking for a commit in masaeedu/fp that reproduces the problem, just copy pasting the snippet I have in the issue should work. All .md and .snap files are modified, even though none should be.. If you think this is a problem with the tests themselves producing nondeterministic output, just run the same snippet on Linux. Neither the .md nor the .snap files will change.. > If you could update snapshots on Linux, commit that\nNothing changes on Linux, so there's nothing to commit. I don't have access to a Windows system at the moment but I'll see if I can do that on a branch tomorrow.. @novemberborn I've added a branch winbug that shows the modifications I get to all the snapshot files if I run the tests with -u. See https://github.com/masaeedu/fp/commit/9ef08bd447ff342422454f7091e6a7e1ee4a84f7. @novemberborn Regarding https://github.com/avajs/ava/issues/1786#issuecomment-387119194, do you just want to do a string substitution of \\ to /? The character \\ is itself a valid character of a POSIX filename, so just replacing \\ -> / might break people who have such files (although it seems very unlikely anyone has such filenames). There may be more path edge cases I'm not thinking of.. ",
    "photonstorm": "Here's a JS Image Diff library which has two Jasmine matchers for unit testing. It uses node-canvas (which in turn uses cairo), so it's pretty 'heavyweight', but definitely works:\nhttps://github.com/HumbleSoftware/js-imagediff\nPerhaps it'd be possible to make use of this somehow?\n. ",
    "wyze": "@mattkrick JSX works on 0.9.x as it reads in the .babelrc, but reading in the .babelrc file was removed in 0.10.\n. Might be related to babel 6.6 release yesterday.\nI noticed that your previous build used 6.5.2 while this one used 6.6.\n. I think the problem lies with JSON.stringifying the serializedTree here: lib/assert#L171\nIf I remove the JSON.stringify and make it just const result = toMatchSnapshot.call(context, serializedTree);, it works every time.. I tried to see if for some reason the macro was causing the problem, and broke out the tests into long form, and it still produced the same result.\nThen, I wanted to check to see if it was jest-snapshot, so I pushed a jest branch up and it does not result in the behavior described above.\nYou can see the jest branch here if needed: https://github.com/wyze/ava-snapshot-issue/tree/jest. Cool, thanks for digging in. It looks like this was fixed with https://github.com/facebook/jest/pull/2482 and we just need to wait for the next release to be cut.. Looks great @vadimdemedes, thanks! I pushed a new branch, wyze/ava-snapshot-issue#ava-1223, in the repo that works without issue now.\nI've also tested this in my real project and it works as well. But it does create snapshots, even if there is no snapshot assertion in that test file. So, in the project with more tests, there are __snapshots__/index.js.snap that just contain {}.. ",
    "benji6": "Can I check whether AVA plans to add support for JSX again in the future or whether I should be keeping all my tests in plain JS?\n. ",
    "corinna000": "I was looking into this and had a couple of questions.\nIt looks like @jamestalmage would like AVA to have an opinion about Promises that fail to reject with Errors. \n- If the conditional logic testing for an Error is removed and we throw fnError, the test still passes if the Promise only rejects with a string. \n- As far as I can tell this would be the only assertion that would have an opinion about the SUT.  How should AVA notify the developer? \n- An Observable converted to a Promises will now be expect the the onError function to throw an Error. Is that right?\n. I was looking into this and had a couple of questions.\nIt looks like @jamestalmage would like AVA to have an opinion about Promises that fail to reject with Errors. \n- If the conditional logic testing for an Error is removed and we throw fnError, the test still passes if the Promise only rejects with a string. \n- As far as I can tell this would be the only assertion that would have an opinion about the SUT.  How should AVA notify the developer? \n- An Observable converted to a Promises will now be expect the the onError function to throw an Error. Is that right?\n. @BarryThePenguin Thank you, noted and updated.\n. Yes. It isn't in the Usage section though, so this is meant to clarify using AVA for users just getting started (like me). The two pull requests I added reflected legitimate bumps I hit while using AVA for the first time. \nThe \"Initialize - Create - Run\" headings under \"Usage\" hint that the steps listed (complete with an example test!) can be followed to get up and running. I believe new users should be able to get through the examples under \"Usage\" and get the results promised in the figure adjacent to \"Run It\". \nI can only see the readme.md with fresh eyes once! :)\n. Yes, that would be perfect. I can make that change.\n. That should update the usage example and remove the unrelated changes that snuck in.\n. ",
    "markelog": "It seems you would need to create a database and user with correct permissions (or use the root one) for every test file if run with serial mode otherwise you would need to create it for every test. And your app have to have the ability to change connection settings dynamically, like with arguments or something.\nThere is also question about selenium like e2e testing. Not sure how ava would fit.\nWith databases it is getting interesting, initially i didn't think it would be applicable... but selenium thing seems like a problem.\nI think though those all should be different recipes - database, mocking, e2e, etc\n. >  I can simplify it and write up a recipe if you guys want\nI would love that\n. ",
    "nfcampos": "for tests that needed do stuff with a mongodb database (with mongoose) I used mockgoose to mock the  database connection (so that the tests aren't potentially changing the same db models) and created a file that populates the db with sample models common to all tests. This way all the tests run concurrently happily without conflicting with each other. I can simplify it and write up a recipe if you guys want\n. I'll see if I can find some time :)\n. one inconvenience with mockgoose is that it currently has a bug that prevents it from working with ava. There's an open PR to fix it but it hasn't been merged in yet. For now I've had manually patch the installed mockgoose to be able to run my tests. (this: https://github.com/mccormicka/Mockgoose/issues/204)\n. @sibelius \nvery generally there's a couple things to be aware of\n- mock mongoose before calling mongoose.connect\n- after mocking (mockgoose(mongoose)) call mongoose.connect\n- after that and before the tests run populate mongoose with some models you'll use in the tests if you need to\n- if some of your tests change the contents of the db you'll need to run all of the tests that use the db serially, otherwise you'll get very confused with some test changing the db while another test is running.\n. @sibelius answered you on the other thread \n. Thanks, btw, it does works if the file is transpiled with babel before running the test,\nso this works babel fail.js -o transpiled.js; ava transpiled.js and this doesn't ava fail.js.\n. I've opened the issue https://github.com/twada/power-assert-runtime/issues/6\n. oops sorry for the duplicate issue. Is there any workaround for now, other than destructuring?\n. oops sorry for the duplicate issue. Is there any workaround for now, other than destructuring?\n. are you sure you're running the benchmark with babel@6.7.7? That was the version that sped up compact mode a lot https://github.com/babel/babel/releases/tag/v6.7.7\n. are you sure you're running the benchmark with babel@6.7.7? That was the version that sped up compact mode a lot https://github.com/babel/babel/releases/tag/v6.7.7\n. and babel-generator and babel-types versions? the release notes mention the speed up being in those two packages\n. and babel-generator and babel-types versions? the release notes mention the speed up being in those two packages\n. I especially don't see how assigning stuff to global could result in a SyntaxError appearing where there was none.\n. @vdemedes Thanks a lot for you investigation of this! based on your findings I think I've traced it the source-map-support module, where they use (typeof window !== 'undefined') && (typeof XMLHttpRequest === 'function') to check whether it's a browser environment. That's why it doesn't fail if you comment either of the two. What I'm not sure is exactly how that plays with ava to produce this weird bug\n. do you guys want a PR setting that option?\n. cool, I'll take care of that today or tomorrow\n. I've opened the PR! #829 \n. ok, I'll remove it\n. I mistakenly committed the failing test together with the fix at first :( I then rewrote the commits but I guess travis never ran the tests with just the failing test. but it does fail :)\n. so should I leave the test in or not?\n. The same reason that makes it worthwhile to pass arguments as an array to the test function without spreading them should make us want to give them as an array to test(), or is it somehow more likely we'll want in the future to introduce a third agumment to the test function than a last argument to test()?\n. sure\n. I've rebased and added the test :)\n. @sindresorhus it should just be removed from the path, like this?\n\n. Is the path separator always /, ie. any way this  .replace(/__tests__\\//, '') can fail?\n. I've added a test, I don't see any way of testing this through the api without changing the current working dir so that's what I did\n. @novemberborn how about I write a unit test for the RunStatus.prototype.prefixTitle function, that should be much clearer?\n. @novemberborn I've added some tests for RunStatus.prototype.prefixTitle. I think this is ready now\n. I'd find this pretty useful \ud83d\udc4d \n. this is the problem I believe https://github.com/avajs/ava/blob/master/cli.js#L168, beacuse of that line this https://github.com/avajs/ava/blob/master/lib/ava-files.js#L34 never gets a chance to work\n. Those defaults should just be removed from cli.js there's no point in having two places with default files\n. @kentcdodds public/test-thing.js not being in the output is because the default pattern that deals with test-*.js only finds files in the top level directory\n. don't you need to add it here https://github.com/avajs/ava/blob/master/lib/enhance-assert.js#L6 ?\n. don't you need to add it here https://github.com/avajs/ava/blob/master/lib/enhance-assert.js#L6 ?\n. done\n. @jamestalmage yes, I'll do that, if I understand correctly it's something like using https://github.com/power-assert-js/empower instead of empower-core in enhance-assert.js and stop using the formatter in run-status.js?\n. @twada yeah you're right\n. yes that note should be removed, that should now work\n. either I'm not understanding how it works or it doesn't seem to work for me, when removing --require babel-register and replacing it with -precompile I'm getting this, which suggests that file is not being run through babel\n\n. clearing the cache didn't help\n. so, it appears that some test files do run correctly, though not all, so it looks like you're finding the dependencies of some test files and missing the dependencies of others\nedit: 21 files do not run because of not transpiled dependencies\n. mine isn't either\n. if any part of the ava or babel config or the file structure helps just ask\n. deps.json.zip\nthis is a json map of files (sources and test files, test files are those that have tests in the path) to array of dependencies (each dependency is an array [string_in_the_import_statement, absolute_path]). does this help?\n. @jamestalmage it's for using css-modules, they're dealt with by https://github.com/css-modules/css-modules-require-hook\nYou would still need to use that .css hook.\n. prior to this PR I had this array in the require entry in the ava config\njs\n\"require\": [\n  \"babel-register\",\n  \"babel-polyfill\",\n  \"css-modules-require-hook/preset\"\n]\nto try out this PR I removed babel-register from that array and left the other two.\nrunning ava --precompile after removing it resulted in some of the source files (.js) being transpiled and some not being transpiled\n. @jamestalmage because it's linked with npm link ava?\n. @jamestalmage alright, I've figured out what the problem is\njs\n// abc.js\nexport const abc = 1\njs\n// def.js\nexport {abc} from './abc'\n``` js\n// pass.js\nimport test from 'ava'\nimport {abc} from './abc'\ntest('abc', t => t.pass())\n```\n``` js\n// fail.js\nimport test from 'ava'\nimport {abc} from './def'\ntest('abc', t => t.pass())\n```\npass.js works, fail.js doesn't, for some reason you're not picking up on re-exports as dependencies\n. @jamestalmage it works now, thanks\ntime with babel-register\nreal    0m48.368s\nuser    2m28.899s\nsys 0m15.420s\ntime with --precompile\nreal    0m32.534s\nuser    1m27.033s\nsys 0m10.058s\n. that screenshot is for npm's verbose mode not ava\u00b4s. you'd have to run npm test -- --verbose to get ava's verbose mode, without the extra -- you're passing the verbose flag to npm itself\n. No need to apologise :) passing flags to npm run is rather confusing\n. for css modules i use css-modules-require-hook (e.g. ava -r css-modules-require-hook/preset) and it works ok\n. good point, i've added that\n. oops sorry, should there be a .npmrc at the root with this?\nsave-prefix='^'\n. @novemberborn to avoid the regex, how about this?\njs\n.replace(this.base, function (match, offset, string) {\n    if (offset === 0) {\n        return ''\n    } else {\n        return match\n    }\n})\nedit: or this \njs\n.replace(this.base, function (match, offset, string) {\n    return offset === 0 ? '' : match\n})\n. I've changed it to what you suggested for this\n. @novemberborn String.prototype.replace replaces only the first occurrence of the searched string (unless you're using a regex with g) so with the base foo/ the path foo/bar/foo/baz.js already worked correctly before this PR \u2014 since base is found at the beginning and replaced the second occurrence of base is never considered. The only base for which this fails is indeed / because it isn't found at the beginning of the path, and thus the next occurrence of / was replaced \n. @novemberborn I can test the base foo/ with the path bar/foo/baz.js but I'll be testing a case that will never be hit in real use. unless I'm missing something the only possible value for base (defined here https://github.com/avajs/ava/blob/master/api.js#L88) that can be not found at the beginning of the path of a test is indeed /\n. forgot to remove this one\n. why do we need TapReporter.prototype.stdout and TapReporter.prototype.stderr if they're never used here?\n. ",
    "sibelius": "@nfcampos could u post an example of how to use ava with mockgoose ?\n. can't I have a different \"database\" for each test?\n. should I mock the filesystem to make this work: \ndotenv-safe do not work with ava\nhttps://stackoverflow.com/questions/39065075/env-do-not-work-with-ava\n. ava configuration\njson\n\"ava\": {\n    \"files\": [\n      \"./**/__tests__/**/*.blue.js\"\n    ],\n    \"babel\": \"inherit\",\n    \"failFast\": true,\n    \"require\": [\n      \"babel-register\",\n      \"babel-polyfill\"\n    ]\n  },\nI'm testing a koajs server using https://github.com/rolodato/dotenv-safe to require .env files\nit is failing in this line of dotenv-safe\nnode_modules/dotenv-safe/index.js:26:42) (https://github.com/rolodato/dotenv-safe/blob/master/index.js#L26)\n. @thangngoc89 this is a repo https://github.com/sibelius/koa-env-ava\nthat reproduces this error\n. fixed here: https://github.com/sibelius/koa-env-ava/pull/1 tks\n. ",
    "cgcgbcbc": "seems that class Sequence must be modified for this feature, I'll try to work on this issue, any advices?\n. Thanks for your advice! I'll try to wrap them as follows:\n1. wrap the final return Sequence: wrap it in a new Sequence followed by after.always as non-bailing if necessary\n2. wrap serialTests, concurrentTests: wrap them in a new Sequence followed by afterEach.always  as non-bailing if necessary\nAs for the chain modification, I'll add a new item always: {always: true} (default false) and then modify the add(https://github.com/sindresorhus/ava/blob/master/lib/test-collection.js#L67) method for adding them to a proper hookset.\n. Hi @jamestalmage , I've opened a pull request #806, however it leads to some failure in other tests, could you have a look and give some advices? Thank you!\n. @jamestalmage I've fixed the mistake, now it should work as desired\n. Hi, I'm thinking about trying this issue, but it seems a little difficult. Seems that I should make the following changes?\n1. add a new failing metadata\n2. modify Runner._addTestResult : keep the metadata in props.\n3. modify Runner._buildStats : add a new count for the expected failing, \n4. seems that in  index.js#test the prop is sent to somewhere for output? But I couldn't determine where to modify yet.\nI'd be appreciate for any advice!\n. For feature 2 seems that reporter must be modified? Is there any better solutions?\n. I wonder if just modifying this will work ? But seems that some tests have failed...\n. Hi @novemberborn I see this issue was closed in the merge request, but it seems feature 2 is still missing, i.e.\n\nWhen it fails, it gets listed in red in the final output (\"expected failure\" or some such message), but the process still exits with 0. This keeps reminding you you've got a problem, but doesn't kill your CI process.\n\nShould another issue be opened for this?\n. > what we still need is to list the expected failing tests separately in the verbose and mini loggers\nDo you have some recommendation of how to display them?\n. @jamestalmage I've added tests verifying that after.always, afterEach.always run if before or beforeEach fails.\n. @jamestalmage Tests added.\nThe after.always and alfterEach.always will always execute after after hook and afterEach hook respectively , no matter the order they are added, since they are implemented by wrapping bail sequence with non-bail one.\n. @jamestalmage Updated according to the comments\n. @jamestalmage should I open an issue in eslint-plugin-ava about this modifier?\n. Just for tracking\n- [x] failing sync test fails - result is passing\n- [x] failing test.cb test fails - result is passing\n- [x] failing test with t.throws(nonThrowingPromise) - result is passing (test async assertions)\n- [x] failing test returns a rejected promise - result is passing\n- [x] failing sync test passes - result is a failure with helpful error message\n- [x] failing test.cb test passes - result is a failure\n- [x] failing test with t.throws(throws) - result is failure (test async assertions)\n- [x] failing test returns a resolved promise - result is failure\n. > Any reason you haven't tackled the last two?\nCause they failed...\n. \ud83d\ude05Aha, not the first time making such embarrassing mistake\n. @jamestalmage , do you think it's necessary to check that the failing modifier can only be used with test and throws error when used with hooks?\n. > Can you open an issue in eslint-plugin-ava about that?\nDo you mean the no-unknown-modifiers rule as I mentioned in sindresorhus/eslint-plugin-ava#107 ?\n. @jamestalmage Thank you for your advice! I've fixed the documents.\n. > You can ignore the issue links part entirely for this PR if you want\nI prefer to ignore this part first \ud83d\ude04 \n. @novemberborn updated.\nAs for the verbose reporter, it seems that it uses the count in runStatus (which in mini reporter it keeps track by itself). https://github.com/sindresorhus/ava/blob/master/lib/reporters/verbose.js#L74-L81\nSo when are the count added to runStatus? As perhaps I need to modify that for counting known failure. \n. seems that I should just modify run-status? But that would influence a lot others?\n. > We want to land this before cutting a new release. Any chance you can finish this up in the next day or two\nOk, I'll try my best to finish this tonight(GMT +8)\n. @novemberborn I've covered the uncovered lines in reporters, but I suppose there must be more tests needed, can you give me some advice?\n. It's strange that ci build failed for my last commit which only remove a comment..\n. \n\n@novemberborn now the output are like this\n. > it would have to be for after.always hooks\nSince hooks are tests in fact, is there an easy way to achieve this ..?\nIn addition, this feature means access test \"history\" in a test, seems that Sequence.run must be modified for passing previous result to hooks run  ?\n. > it would have to be for after.always hooks\nSince hooks are tests in fact, is there an easy way to achieve this ..?\nIn addition, this feature means access test \"history\" in a test, seems that Sequence.run must be modified for passing previous result to hooks run  ?\n. where should I write tests for this line? Seems that there is no test/run-status.js\n. Seems that the results does not have that count, I grep processResults in directory but find nothing, can you tell me where it is called?\n. Seems it is called in api.js, but since it has knownFailures, should I just use its length?\n. Why this assertion failed..\n\n. Indeed, fixed now\n. ",
    "s-a": "I find this very intersting! Are you willing to share your workflow to profile your software in something like markdown documentation? I like to link from my repo to share your work with the community. Just want to say... keep in mind that results might differ from native node but it will in fact deliver insightful results.\n. :+1:  I can not wait to read it . :) @jamestalmage Please ping me here when you finished.\n. @jamestalmage sorry #491 is out of my scope. I feel this could have something to do with \n\ne.g. The setTimeout issue was resolved in a recent commit to devtool, since now we are using true Node.js timers instead of the built-in browser timers.\n\nAs matt metioned in his README.md devTool is in an experimental state.\n\nNote: This tool is still in early stages. So far it has only been tested on a couple of OSX machines. :)\n\nHowever FYI just added https://github.com/s-a/iron-node/blob/master/docs/PROFILE.md. There is a lot more work todo. :) Nice weekend @all :smile: \n. ",
    "wilmoore": "I personally think globs make more sense. Basic string literal searches only get us so far; however, constructing a valid RegExp in the shell feels awkward.\n. > Can you think of any use-cases where a glob might not work?\nOff the top of my head, no; however, I can think of many times where mocha's grep was too limited to the point where I don't even use it so pretty much anything (especially globs) would be a vast improvement.\n. The only other thing I'd suggest is to change --match to --glob to be more explicit about it:\nshell\n$ ava --glob='fo*' --glob='!bar'\n. > Would be awesome if you could provide some examples. That would help make a decision.\nSorry, I don't have any off the top of my head. I only mocha pretty infrequently and when I do I tend to use .only instead of --grep.\n. > You might expect for \"--match 'test group 1: '\" to run all of them but the '/' would throw it off. I suppose \"--match 'test group 1: *'\" would resolve it though.\nYup, agreed. That is the main PITA with globs for sure. Only other flexible option is RegExp which also is no panacea; however, would make it a little easier to get around issues like the one you've presented.\nThat being said, I concede that I have less skin in the game since I've gotten by without this feature entirely using .only or .skip and that's been fine.\n. I'm seeing what @matthewbauer is seeing unfortunately:\n``` js\nvar multimatch = require('multimatch')\nvar matches = multimatch([\n  'test group 1: 4 - 2 = 2',\n  'test group 1: 2 + 2 = 4',\n  'test group 1: 4 / 2 = 2'\n], ['test group 1:**'])\n// => [ 'test group 1: 4 - 2 = 2', 'test group 1: 2 + 2 = 4'  ]\n```\n. LGTM\n. :+1: \n. ",
    "Bearlock": "Is this a solved issue? I tested this with the example code provided here on my Mac's Iterm 2 terminal and I'm not having the same issue. Looks like this:\n\nIs there something else I would have to do to recreate the issue?\n. @sindresorhus Ah, understood. Thanks for the clarification. \n. ",
    "JasonRitchie": "Can I pick this up?. No worries, @novemberborn. Looks good to me!. No worries, @novemberborn. Looks good to me!. I believe I understand the request. I'm also a beginner, but I'd be willing to assist. @monicabhalshankar let me know if there's anything I can do to help.. ",
    "zckrs": "Commits squashed\n- 10ad1a2 Move media into docs/media\n- 076aa09 Use \"docpress\"\n- 3f96378 Auto build/deploy by TravisCI\n\nDocumentations accessible via a web application with a navigation can help some users.\nAnd AVA can be more popular with it.\n. For deployement by TravisCI you should add env variable GITHUB_TOKEN in https://travis-ci.org/sindresorhus/ava/settings\n. @jamestalmage yes I moved media folder into docs because Docpress is based on docs to resolve path.\n. No problem\nYou are right I should open issue but I like coding ^^\n. ",
    "mention-bot": "By analyzing the blame information on this pull request, we identified @jamestalmage, @vdemedes and @sindresorhus to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @jokeyrhyme, @ariporad and @floatdrop to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan to be a potential reviewer\n. By analyzing the blame information on this pull request, we identified @sotojuan, @ariporad and @therealklanni to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan to be a potential reviewer\n. By analyzing the blame information on this pull request, we identified @novemberborn, @ariporad and @sotojuan to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @novemberborn, @ariporad and @jokeyrhyme to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @BarryThePenguin and @twada to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @mdibaiee, @ariporad and @jokeyrhyme to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @jokeyrhyme, @ariporad and @sotojuan to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @charbelrami to be a potential reviewer\n. By analyzing the blame information on this pull request, we identified @forresst and @charbelrami to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @ariporad and @jokeyrhyme to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @novemberborn, @ariporad and @sotojuan to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @novemberborn, @sotojuan and @jokeyrhyme to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @novemberborn and @jokeyrhyme to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @Carnubak and @sohamkamani to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @novemberborn to be a potential reviewer\n. By analyzing the blame information on this pull request, we identified @novemberborn, @sotojuan and @mdibaiee to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @novemberborn, @sotojuan and @ariporad to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @ariporad, @SamVerschueren and @sotojuan to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @charbelrami, @makotot and @ben-eb to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @ivogabe to be a potential reviewer\n. By analyzing the blame information on this pull request, we identified @therealklanni, @sotojuan and @SamVerschueren to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @ariporad and @sotojuan to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @ivogabe to be a potential reviewer\n. By analyzing the blame information on this pull request, we identified @ingro, @sotojuan and @ntwb to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @ingro and @Carnubak to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @bachstatter and @SamVerschueren to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @naptowncode and @ivogabe to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @Carnubak and @BarryThePenguin to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @Carnubak and @jokeyrhyme to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @BarryThePenguin, @sotojuan and @ariporad to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @ben-eb, @sotojuan and @kasperlewau to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @ivogabe, @BarryThePenguin and @jokeyrhyme to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @bachstatter and @SamVerschueren to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @naptowncode, @kasperlewau and @BarryThePenguin to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @BarryThePenguin, @sotojuan and @naptowncode to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @ariporad and @spudly to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @ariporad and @spudly to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @ivogabe and @SamVerschueren to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @BarryThePenguin, @naptowncode and @ivogabe to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @kasperlewau, @SamVerschueren and @sotojuan to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @naptowncode, @ingro and @ariporad to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @BarryThePenguin, @sotojuan and @naptowncode to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @ingro and @kasperlewau to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @floatdrop and @ingro to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @floatdrop and @ingro to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @spudly to be a potential reviewer\n. By analyzing the blame information on this pull request, we identified @spudly to be a potential reviewer\n. By analyzing the blame information on this pull request, we identified @forresst and @SamVerschueren to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @forresst and @SamVerschueren to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @SamVerschueren and @forresst to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @SamVerschueren and @forresst to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @ivogabe and @jokeyrhyme to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @ivogabe and @jokeyrhyme to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @ingro and @naptowncode to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @spudly, @jokeyrhyme and @sotojuan to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @kasperlewau, @sotojuan and @SamVerschueren to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @ariporad, @timoxley and @sotojuan to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @spudly to be a potential reviewer\n. By analyzing the blame information on this pull request, we identified @floatdrop, @ingro and @naptowncode to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @forresst, @ben-eb and @bachstatter to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @kasperlewau, @ariporad and @sotojuan to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @ariporad, @sotojuan and @kasperlewau to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @kasperlewau, @ariporad and @sotojuan to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @kasperlewau, @ariporad and @sotojuan to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @ivogabe and @BarryThePenguin to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @ariporad, @sotojuan and @spudly to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @spudly, @naptowncode and @floatdrop to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @BarryThePenguin and @naptowncode to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @BarryThePenguin, @kasperlewau and @naptowncode to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @spudly, @sotojuan and @ariporad to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @naptowncode and @ariporad to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @spudly, @ivogabe and @jokeyrhyme to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan to be a potential reviewer\n. By analyzing the blame information on this pull request, we identified @uiureo, @sotojuan and @kasperlewau to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @spudly and @Carnubak to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @ariporad, @sotojuan and @Carnubak to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @naptowncode, @kasperlewau and @BarryThePenguin to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @zhaozhiming and @forresst to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @BarryThePenguin, @kasperlewau and @naptowncode to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @ingro and @kasperlewau to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @zhaozhiming and @forresst to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @BarryThePenguin to be a potential reviewer\n. By analyzing the blame information on this pull request, we identified @zhaozhiming and @forresst to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @BarryThePenguin, @sotojuan and @naptowncode to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @uiureo, @kentcdodds and @sotojuan to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @uiureo and @kentcdodds to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @spudly, @ariporad and @ingro to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @forresst to be a potential reviewer\n. By analyzing the blame information on this pull request, we identified @zhaozhiming, @kutyel and @ben-eb to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @BarryThePenguin, @sotojuan and @SamVerschueren to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @BarryThePenguin and @ariporad to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @spudly, @ivogabe and @jokeyrhyme to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan and @BarryThePenguin to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan and @BarryThePenguin to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @forresst and @charbelrami to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @ariporad, @timoxley and @sotojuan to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @mattkrick, @kentcdodds and @kasperlewau to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @ivogabe, @jokeyrhyme and @twada to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @ariporad and @spudly to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @BarryThePenguin, @ivogabe and @ariporad to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @spudly to be a potential reviewer\n. By analyzing the blame information on this pull request, we identified @spudly to be a potential reviewer\n. By analyzing the blame information on this pull request, we identified @BarryThePenguin, @naptowncode and @sotojuan to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @spudly, @sotojuan and @ivogabe to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @ariporad, @ingro and @naptowncode to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @ariporad, @ingro and @naptowncode to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @bachstatter and @forresst to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @ingro, @naptowncode and @jokeyrhyme to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @dcousineau, @SamVerschueren and @ariporad to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @dcousineau, @sotojuan and @jokeyrhyme to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @ariporad, @sotojuan and @BarryThePenguin to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @kasperlewau, @BarryThePenguin and @naptowncode to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @sotojuan, @ariporad and @kentcdodds to be potential reviewers\n. ",
    "jarrettmeyer": "@vdemedes Your recommendation is a change to the supporting text? Are the examples good/bad/necessary?\n. Agreed: 1. If you print to directly to stdout, you can't really suppress it with a minimalist reporter or promote it to an error with eslint.\n. Should this check for duplicate names within a single test file, or across all test files?\n. I can put this on the back-burner. However, I am almost complete with a first pass. My suggestions for this feature.\n1. Add a \"warning\" event type. Similar to \"error\" but with a different name. This gets wired all the way through ava, from the initial emit, through fork, api, logger, and finally to the reporters.\n2. Add a --werror flag to the cli. Stolen from gcc, the --werror flag treats warnings as errors.\n3. A duplicate test title emits a warning event. Currently, this warning is being emitted from TestCollection's add() function, but that only checks that the test title is unique within a single file, not across all files.\n. ",
    "inyono": "I'm not super happy with the NODE_PATH solution myself, but it fits our use case quite well (especially ES6 modules hinder some other viable solutions). We didn't run into problems with mocha though.\nIf it's a beginner-friendly issue, I'd be glad to help.\n. I will give it a shot tomorrow, thanks for the pointers.\n. Yeah, I it works with absolute paths. Considering https://gist.github.com/branneman/8048520#6-the-hack, I looked into _initPaths(), but initPaths() also works with the relative path and does not create an absolute one...\n. Seems like cwd is set here: https://github.com/sindresorhus/ava/blob/master/lib/fork.js#L20\n. See PR. I found a fix that at least works in my minimal example. Because I have failing tests in both my branch and on master, I guess I did something wrong, though ;)\n. I thought so. @ingro has another approach in #531 \n. ",
    "ingro": "I think I've got something, as @jamestalmage suggested inside test-worker.js module:\njavascript\nmodule.constructor._nodeModulePaths = function () {\n    var ret = oldNodeModulesPaths.apply(this, arguments);\n    ret.push(nodeModulesDir);\n    return ret;\n};\nSeems like ret doesn't contain the additional paths specified with the NODE_PATH env variable:\njavascript\n[ 'D:\\\\projects\\\\app\\\\node_modules\\\\babel-core\\\\node_modules',\n  'D:\\\\projects\\\\app\\\\node_modules',\n  'D:\\\\projects\\\\node_modules',\n  'D:\\\\node_modules',\n  'D:\\\\projects\\\\app\\\\node_modules\\\\ava\\\\node_modules' ]\nI tried naively to do this:\n``` javascript\nmodule.constructor._nodeModulePaths = function () {\n    var ret = oldNodeModulesPaths.apply(this, arguments);\n    ret.push(nodeModulesDir);\n    if (process.env.NODE_PATH) {\n        const additionalPath = path.join(process.cwd(), process.env.NODE_PATH);\n        ret.push(additionalPath);\n    }\n    return ret;\n};\n```\nBut the problem is that process.cwd() points to the directory where the test file is, and not on the root of my application, so that didn't work.\nA ugly solution could be to seth NODE_PATH as absolute but that would have a lot of downsides. I did not fully understand how the code inside test-worker.js gets invoked, if someone can clarify it maybe the solution could be at hand.\n. I tried another approach, hope one of them could be merged :)\n. Thanks for your inputs @novemberborn , I made the following changes:\n- removed the ugly process.env.NODE_PATH override in api.js\n- added an option for the forked process (projectRoot) which contains the original path from where ava was called;\n- cycled through possible multiple values of process.env.NODE_PATH (with a simple OS check) and join their path with the one supplied by projectRoot.\n. I added the test and it seems like it's green now, but I surely forgot something else, let me know if something is missing!\n. Thanks @novemberborn for your patience, I hope that if I'll ever have to contribuite again it will be a smoother process, I learned a lot anyway :)\n. Well I will wait to make further changes so, while a better solution is found!\n. Well I think that's cleaner, I will look at it as soon as I have time!\n. The commit has some conflicts, what's the procedure in situation like this?\n. It seems we are back to green!\n. Welp sorry, I blindly copied from one of your comment above.\nThat should fix the wrong assignment:\nenv = objectAssign(env, { NODE_PATH: nodePath });\nAnyway tests still fails, will look into that!\n. I think I found the problem with the test, it was:\njavascript\nvar nodePaths = 'node-paths/modules' + path.delimiter + 'node-paths/deep/nested';\nwhile it should have been:\njavascript\nvar nodePaths = 'fixture/node-paths/modules' + path.delimiter + 'fixture/node-paths/deep/nested';\nThat's because NODE_PATH should be relative to the directory from which ava is running, not to the test itself.\nI've fixed that in my fork, I should create another pull request?\n. Created here https://github.com/sindresorhus/ava/pull/559\n. I didn't want to add another dependency, but if it's fine for you I think too that this would be a better solution.\n. What about parentCwd?\n. Yeah you're right, I'm really bad at this.\n. So I guess I could leave the test as it is (except for the wrong separators).\n. Yep map is surely more elegant!\n. Any suggestion on how to approach this?\n. Cool! So I can get rid of all the env check stuff!\n. There's always to learn something!\n. ",
    "kkemple": "okay, thanks for the response\n. After 2 yrs if you still need this that bad I recommend move to a framework that supports your needs. I opened this issue and have long since left Ava for Jest.. ",
    "mattecapu": "\nWhy? We are not big fans of having an endless amount of meta files in our repos\n\nI am. I prefer separation over concentration, and it's handy to just ctrl-c ctrl-v a dotfile. That said, I think no one is really wrong or right and is just a matter of taste.\nFrom what I can see, to add support for a dotfile there are just a handful of lines to edit in cli.js, profile.js and test/cli.js.\nMy naive implementation would just grab the config object from .avarc instead of package.json. A one-liner. I'd be really happy to make a PR for this (also because I love this project and I'd like to contribute). The only thing to decide is precedence, and I'll leave package.json first for legacy reasons.\nAre there any complications I'm overlooking?\n. I think it would be highly beneficial to support a --max-forks limit to limit the spawning... When more than 1-2 process per core are running the parallelism perf bonus is lost. On my single-core CPU the whole thing simply freeze.\nHappy to help on this.\n. ",
    "diegohaz": "There're some annoying things on putting the configuration in package.json instead of dot files. Besides which was already said here, every time I npm install --save something my babel config will become from this:\njson\n{\n  \"presets\": [\"es2015\", \"react\", \"stage-0\"],\n  \"plugins\": [\n    [\"resolver\", {\"resolveDirs\": [\"app\"]}]\n  ],\n  \"env\": {\n    \"development\": {\n      \"plugins\": [\"react-hot-loader/babel\"]\n    }\n  }\n}\nto something like this due to package.json auto re-indent:\njson\n{\n  \"presets\": [\n    \"es2015\", \n    \"react\", \n    \"stage-0\"\n  ],\n  \"plugins\": [\n    [\n      \"resolver\", {\n        \"resolveDirs\": [\n          \"app\"\n        ]\n      }\n    ]\n  ],\n  \"env\": {\n    \"development\": {\n      \"plugins\": [\n        \"react-hot-loader/babel\"\n      ]\n    }\n  }\n}\nAlso, eslintConfig rules are terribly indented in package.json.\nBecause of this I do prefer to keep it on dotfiles, but I can't do this with ava. \ud83d\ude2d \n. Maybe. I'm using AVA with Mockgoose and for endpoint testing. Without concurrency option set to 1, I've experienced inconsistent errors like that:  src/api/field/field.model.test.js exited due to SIGKILL.\nI've read somewhere that Travis CI has a limit for multi processes, but I can't find it right now.\nThis is the output: https://travis-ci.org/diegohaz/generator-rest/jobs/159648372#L331-L346\n. I solved this by updating the travis.yml file (changing the script to npm test -- --serial). But the problem with multi processes is still an issue.\n. I don't think so. I'm currently creating symlinks instead of copying node_modules. I'll investigate it more, but I would like to see more examples with database mocking like Mockgoose. Creating a MongoDB instance, with all necessary models and indexes for each test (or even test file) seems to be a huge task.\n. I'm actually doing something similar to this: https://github.com/Mockgoose/Mockgoose/issues/3#issuecomment-241894985\n. ",
    "bdentino": "Here's a concrete use-case. I run a bunch of my integration tests using ava inside a docker container orchestrated via docker-compose. My docker image takes significantly longer (~100x) to rebuild when i modify package.json (since it has to re-run npm install). because i have to define ava defaults in this file, any change to those forces a pretty long and usually unnecessary rebuild. this is a relatively minor inconvenience but i definitely feel the pain when i'm setting up a new project, debugging test timeouts, trying to find an optimal concurrency limit, etc. i can of course specify other defaults on the command line or in custom scripts, but at that point i'm being forced to break from the concept of consolidating these defaults in one place anyways so it seems like having a file like .avarc would just be a better alternative. \ni'm not gonna be all torn up inside if this never happens, but given that this project places a high value on developer workflow, productivity, and speed it seemed like a reasonable use-case to share.\n. ",
    "jeffijoe": "\nNeither is \"because project X does it that way\".\n\nThere are a bunch of \"project X\" out there that does have a dotfile for config. It was mentioned that nyc did not, and that was true until not so long ago.. I'd say that \"because project X does it that way\" is a valid argument if there are more than just a few cases.\nI don't see how adding support for dotfiles will ruin the experience for those who prefer package.json.\n. This would still run each test in parallel, right?. Yes, exactly. Helper worker is an even better name for it.\nIt will make AVA more scalable in the sense that I don't have to consolidate a bunch of assertions into a single test case just to avoid the extra overhead.\nSetting up API servers for each test also takes a while; most of the time I want to wait for certain connections to be made before I start listening, adding startup time to each test case. If I could do this once per run, that would cut down test run time dramatically.. ",
    "jtag05": "I was a large proponent of package.json based configs for the exact reasons you've offered here. Once I built a large project with that mentality and had numerous tools and environments to account for it became completely unruly and I quickly changed my tune. Also this is just imposing your thoughts on what is the right way to configure a project rather than allowing a developer to make that choice. . I submitted PR #1627 resolving this issue. It's seriously trivial to implement. Hopefully the maintainers will consider adopting it. I have no interest in maintaining a separate fork for 7 lines of code difference, but find it strange that there has been such a strong pushback against something that has no effect on the library's performance.\n  . I can only speak for myself, but I tend to expect my .rc files to live in the project root and would expect the applications to respond based on current working directory.\nIn regards to the current trend of config.js files I tend to associate that with projects like webpack where the config itself can perform operations. Seeing as AVA currently doesn't operate that way it's hard for me to think of a use case where that would be particularly beneficial, but I also can't say for certain it wouldn't be. \nI think that if you were to take the ava.config.js route, being able to define where that lives becomes more of a conversation. It's easy to parse .rc files as configuration whereas a .js file could have implications for regex based tooling. Perhaps adding a --config option to the cli would be in order to allow developers to define whatever .js file they like as well as where it is stored relative to the working directory.\nAs far as .avarc inheritance, I again can only speak for myself, but I would think that testing configurations are unique enough to not make this a pressing concern.. @novemberborn I'd like to see an argument against implementing 7 lines of code that has no performance issues. I understand that preferences are exactly that, but rather than taking the stance of \"why should we?\", as a utility should you not be approaching this from the perspective of \"Why not?\" as a means of maximizing adoption?\nI think it's fair to argue that whichever route a developer chooses (meta file or package.json) all configuration should be stored the same way. As I have previously stated in the #520 thread, I was a supporter of package.json based configutation, but I've since run into issues on projects where a package.json file get's seriously bloated. Take for instance if you use a custom eslintConfig and store it in package.json, that can easily run 300 lines. \n  . ",
    "danawoodman": "If this is implemented I\u2019d recommend looking at cosmiconfig. . Why not just use a standard tool like cosmiconfig which handles all these decisions and behavior for you?. \u2014config has highest priority. Then it would be whatever config file is present. I don\u2019t know where package.json fits but Cosmiconfig probably documents which is highest priority. The project Prettier just adopted cosmiconfig after a long discussion and it worked out great.  I\u2019m using it too on my own projects and it\u2019s very easy to integrate. \n  . Using a prexisting tool that many projects use is much preferable than a \u201croll your own\u201d approach in my opinion. Convention is good!. @sindresorhus are you sure you\u2019re not willing to reconsider your stance on this? As outlined, there are a lot of good reasons to make this change and the only reason not to is \u201cwe don\u2019t want multiple ways to configure things\u201d. If you use a tool like cosmiconfig you don\u2019t have to make any decisions as the tool does it for you. This means AVA stays simple because it just says \u201cuse cosmiconfig to handle config\u201d. Nearly every major project has a dot file/config file (Babel, Prettier, Mocha, Lerna, nvm, etc.) and I believe it is because there are legitimate reasons to do this. Please reconsider so that users of AVA have options that work for them. Thanks!. @novemberborn, good to know! Would be very excited to see this change. Being able to copy a config file from one project to another sure beats manually editing package.json \ud83d\udd7a Also, I really do think having the config file would improve AVAs marketing because I know, at least for myself, that I\u2019ve discovered lots of great tools by looking at a projects config files! Thanks!. ",
    "pho3nixf1re": "I am cross-posting this from #1627 so it stays with the mainline of discussion.\n@novemberborn there is a perfectly valid argument from @jtag05. The package.json file has become a dumping ground for configuration. In large projects it makes it huge and unwieldy. Digging through a giant JSON file is not a pleasant experience and is reminiscent of the Java world's XML configs. My team prefers the dotfiles as it neatly separates each tool's configuration that is easily explorable. It is not just because \"we are used to it\". Every other tool we use manages to support this trivial feature that we value but AVA. It seems absurd that even in the face of community outcry and the presence of a perfectly fine PR the maintainers here ignore the need.. My use cases follow the same logic that @jtag05 has presented. There isn't much of a use case for a js config file and if there is ever a need for it AVA can implement that at a later date. The current conversation here is to just provide an alternative location with the same implications as the current package.json setup. The MVP implementation requested here is for there to be an option for a co-located rc file that lives next to the package configs. It doesn't need to be more complicated than that as far as inheritance goes. It is a starting point that future changes can build upon if more complicated use cases present themselves. I also like the idea of supporting a --config flag that would allow further customization but it isn't necessary to implement up front.\n  . @novemberborn Although I disagree about the JSON format in an 'rc' file I am fine with the proposal. It solves the need and allows for a lot of future flexibility. Thank you for discussing this with us and coming up with a good compromise on the solution.. Is there any chance of getting a 'quick fix' for this while y'all pull together #1223 ? The current state makes snapshots unusable. Is the issue that jest-snapshot is still waiting for the fix to be merged?. This same behavior happens when using enzyme wrappers. The output is so large it makes the test output useless. I've gotten into the habit of putting the exact test value into a variable before passing it to the ava assertion.. This same behavior happens when using enzyme wrappers. The output is so large it makes the test output useless. I've gotten into the habit of putting the exact test value into a variable before passing it to the ava assertion.. This even fails with a more trivial example...\n```js\nimport test from 'ava';\ntest('renders the component', t => {\n  t.snapshot('change me');\n});\n```. I'll verify and get a quick fix up if it's simple.. @novemberborn there is a perfectly valid argument in #520 that was recently posted. The package.json file has become a dumping ground for configuration. In large projects it makes it huge and unwieldy. Digging through a giant JSON file is not a pleasant experience and is reminiscent of the Java world's XML configs. My team prefers the dotfiles as it neatly separates each tool's configuration that is easily explorable. It is not just because \"we are used to it\". Every other tool we use manages to support this trivial feature that we value but AVA. It seems absurd that even in the face of community outcry and the presence of a perfectly fine PR the maintainers here ignore the need.\n  . ",
    "trusktr": "I know this issue is re-opened, but I'd like to add some points\n\ndemonstrate an actual need.\n\nI decided to try Ava today for the first time, because it looks awesome. However, without so much as a --config option I can't use it. My use case: I store my build procedures and dependencies in a single package that is shared with all my projects, so I need to tell each tool where to read configuration from.\nDespite how awesome Ava looks, I simply can't use it unless I can tell it where to look for configuration (in the shared package), and I want to avoid unwieldy command line arguments.\nFurthermore, when configurations are stored in separate files (particularly in JSON form, or they export a JS object), it makes it easier for projects that use the shared config to be able to extend certain configs for their own needs.\nFreedom of choice is the key here, it allows developer flexibility.\nIn my case, I'm making a setup that allows me to build many JS packages without having to configure each one ever again. Al I have to write in my packages is source with latest language features, and test files. The rest is all abstracted away and shared among all my JS packages. It is only possible (in a clean way) with tools that have configurable config location.\n\n@novemberborn thanks for re-opening!. I know this issue is re-opened, but I'd like to add some points\n\ndemonstrate an actual need.\n\nI decided to try Ava today for the first time, because it looks awesome. However, without so much as a --config option I can't use it. My use case: I store my build procedures and dependencies in a single package that is shared with all my projects, so I need to tell each tool where to read configuration from.\nDespite how awesome Ava looks, I simply can't use it unless I can tell it where to look for configuration (in the shared package), and I want to avoid unwieldy command line arguments.\nFurthermore, when configurations are stored in separate files (particularly in JSON form, or they export a JS object), it makes it easier for projects that use the shared config to be able to extend certain configs for their own needs.\nFreedom of choice is the key here, it allows developer flexibility.\nIn my case, I'm making a setup that allows me to build many JS packages without having to configure each one ever again. Al I have to write in my packages is source with latest language features, and test files. The rest is all abstracted away and shared among all my JS packages. It is only possible (in a clean way) with tools that have configurable config location.\n\n@novemberborn thanks for re-opening!. By the way, hoping this makes it into v1.0.0 so I can come back to try it out later! Mostly everything about Ava seems really awesome, just that the config option is holding me back for now.. By the way, hoping this makes it into v1.0.0 so I can come back to try it out later! Mostly everything about Ava seems really awesome, just that the config option is holding me back for now.. That's true, but means the project has to know about Ava. In my case, projects don't know about the tools, and no configs (even re-export types) need to exist in the project. It's completely decoupled. There's only, at maximum, a few package.json scripts that call generic prod, dev, and test commands on my Builder archetype.. But, true, that could be \"good enough\" for the time being!. ",
    "good-idea": "I'm not sure if this bloated thread needs another +1 comment, but my use cases are:\n\nRunning ava on my src/__tests__ and dist/__tests__ with different configurations. I'd like to test the dist directory without using babel.\nCopying an ava.config.js from one project to another would be ideal.. @novemberborn I'm looking into it right now. I'm a little confused by this line in your comment:\n\n\n\nWhen invoked, AVA walks the file hierarchy looking for a package.json file\nAVA checks whether a sibling ava.config.js file exists\n--->  If this file contains an \"ava\" configuration, and ava.config.js exists, AVA will exit with a warning about ambiguous configuration\nOtherwise the package.json configuration takes precedence over ava.config.js\n\n\nWhen you say \"this file\" are you referring to package.json or ava.config.js? Or, to clarify:\n\nIf package.json contains an \"ava\" config property and ava.config.js exports a configuration, warn about ambiguous configuration.\nIf ava.config.js exists and does not export a configuration, use package.json.\n\nIs that what you're thinking?\n. @novemberborn No hurry, I don't think I'll be able to chip away at this until the end of the week.\nThis is my first contribution to OSS \ud83e\udd13 so any feedback is welcome!. @novemberborn thanks, I'll working on this later today!. @novemberborn I think this just about does it, let me know if you see anything else that should change!. @novemberborn This last update should address the notes in your review -- I didn't look into the fileType issue in babel-conifg yet, though. I updated the initial PR description to include a todo for this.. @novemberborn Just added a new commit with info in the docs. Let me know what you think!\nI've also updated the checklist in the original comment.\nI don't think I'll have time to dive into babel-config.js until next weekend. I'm happy to, though -- but if so could you give me a little direction on how you think it would best be implemented?. @novemberborn I made some changes to address the above -- there another big diff with package-lock.json, so I did the same as before. Let me know if everything is looking OK.. @novemberborn No hurry on my end! I've been using my own fork for now :)\nLooking forward to the next release. Got it!. I haven't looked into why, but if I set up ESM first like so:\njs\n'use strict';\nrequire = require('esm')(module);\nconst path = require('path');\nconsole.log(path);\npath is null.. no, it doesn't seem to. There's a test for this that fails if you don't do the check for __esModule.. \u2705. \u2705. \u2705. esm@3.0.15 was installed, I updated to 3.0.22 and it now works as expected.\nI also moved esm from devDependencies to dependencies, since it's now being used in the main code -- does this make sense?. no problem, \u2705. Ok, better now: from 6,951 to 266. ",
    "smithamax": "At work we made a rule for mocha, no-only.\nPretty self explanatory, prevents you from committing test.only(...) by mistake.\n. ",
    "allensb": "node-tap is in the package.json and I'm seeing the same issue. The t being passed in appears to use node-tap.\n\"tap\": \"^5.0.1\",\n. ",
    "rclanan": "As Allen pointed out, tap is being used. Line reference is here: https://github.com/sindresorhus/ava/blob/master/test/assert.js#L2\nvar test = require('tap').test;\nI understand that Ava is different than tap but since it was referenced (not directly) in the original issue (https://github.com/sindresorhus/ava/issues/507) that changed it from doesNotThrow to notThrows; only seemed appropriate to make it align with notThrow instead of notThrows. \n. Thanks for the advice. \nI created an issue for it. I created the PR in response to the issue in case it was a \"wanted\" change. It's easy enough to close out the PR. As for actual work effort, not much time was lost as it was a pretty simple change. If it wasn't something that was a trivial change, I definitely wouldn't have created the PR to go with the issue without discussion first. \n. ",
    "mastilver": "@hzoo I came to the same conclusion\n\"fix\" for postcss-import: https://github.com/postcss/postcss-import/pull/172\n. For this package: https://github.com/mastilver/nosqwal/tree/master/packages/nosqwal-memory\ninside package.json, I set: ava.files to [\"./node_modules/nosqwal-test/test/*.js\"]\nI believe it's an intended behavior given this test:  https://github.com/avajs/ava/blob/58a2e6803f74fa29d601e019b22f6a0b945c3544/test/api.js#L595\nI would be happy this #1137 being solved :)\nI guess you can close this one. Here's my package.json when using symlink: https://github.com/mastilver/nosqwal/blob/6a5ceaf53ddcc2c1c7747e7f2910def72a4d7963/packages/nosqwal-memory/package.json\nand the failing build: https://travis-ci.org/mastilver/nosqwal/builds/180356647\nSo I've got: ln -s node_modules/nosqwal-test/test ./ as a postinstall script\nI have the default ava config, so I would assume ava to run test inside the test folder. @rnkdev Thank you so much for your work :+1: . Cool, I understand\nI'm closing it then ;). ",
    "gzzhanghao": "@hzoo acorn.input becomes t.is({ // comment }, 1) in your case. Forcing a newline in concise mode at https://github.com/babel/babel/blob/master/packages/babel-generator/src/printer.js#L300 should help.\n. ",
    "KevinMarkVI": "Thank you!\n. Thanks. No rush.\n. ",
    "kl0tl": "Nothing prevents me from using t but enforcing this variable name seemed a bit artificial.\n. For a single assertion yes, but it would be a nicer when multiple assertions are used multiple times in the same test.\n. And while this test is trivial I like how it makes immediately clear what assertions I'm using. \n. I may have been too used to it. Anyway, it means losing the power-assert output while https://github.com/sindresorhus/ava/issues/550 is pending.\n. ",
    "briandipalma": "id-length warnings in ESLint are triggered by t so it would be handy if we could use destructuring as a way to silence those.\n. Any chance we could import the assertion functions from ava so there is no need for the t reference?\n. ",
    "philmill": "I'm also coming from frameworks which use describe and context for organization. I'm wondering if something like Tape's t.test would suffice  although contexts aren't tests.\nI tried nesting a failing test within an empty test which just passes as an organizational experiment.\njavascript\ntest('parent', (t) => {\n  test('child', (tc) => {\n    tc.fail();\n  });\n});\nThe lack of contexts is certainly not a deal breaker as the features presented with AVA are compelling enough for me to switch. But in regards to writing a recipe for Mocha converts, it might be difficult with the current API. Arguably if AVA forces me to write smaller test files which represent more narrowly focused context, that's probably a good thing. \n. Hey thanks for the suggestion @talexand, but the test still passes (results in infinite loop in call back mode). Also requiring calls to end() in callback mode isn't ideal. \nTo be clear, I'm not really suggesting this as a solution to organization. It's just the first thing I tried after looking over the available API and searching for others who have tried similar contextual organization in AVA. I'm thinking this isn't how tests are intended to be structured and possibly not on the road map either (which is fine). \n. @lukechilds I'm digging node-browser-environment which looks to be a convenient DSL for jsdom anyways. Just used it in a React project with ease.  \ud83d\udc4d  for using it in the example here. Nice job with the DSL btw. \n. Yes, that's correct. I called node-browser-environment a DSL since it provides a specific language to use with jsdom (common DOM objects represented as string literals in an array). It's a loose use of DSL since it's not a language to be parsed into a GPL. Maybe API wrapper would be more applicable. \n. ",
    "karlhorky": "@kentcdodds I believe this is the same problem that I was having with the shared ngMocks state in Angular tests.\nThe workaround I came up with was to use after() instead of afterEach(). I've opened kentcdodds/ava-beforeEach-afterEach-bug#1 as an issue in your repo.\n. ",
    "Carnubak": "I made the instantiation clean up at cli.js, but I still would like a few clarifications on the rest.\n. Agreed. This looks much cleaner. I'll update it.\n. Is there any particular reason the Logger needs to know about api? I'm not familiar with the overall design, but it seems to me that Logger only contains api in order to pass it to the different reporters, and does not use it on its own. It's a bit confusing to me.\n. Also, if reporters are created using new, the check on their constructors (e.g. https://github.com/sindresorhus/ava/blob/master/lib/reporters/tap.js#L14-L16) can just throw if used without new, as other modules do.\n. How about this, in cli.js:\nif (cli.flags.api) {\n    reporter = tapReporter(api);\n}\n[...]\nreporter.api = api;\nlogger = new Logger(reporter);\nI think that reads more clearly, and the Logger is no longer manipulating the reporter behind the scenes.\n. ",
    "robertrossmann": "Is there a particular reason why domains cannot be used? Except, of course, that they are \"soft-deprecated\". It seems that simply wrapping all the test implementation functions in a domain.run() should work just fine, as long as each implementation function gets its own domain instance (which is ok).\n. Even an error thrown synchronously from a callback test seems to cause ava to just hang. Note that these errors do not necessarily have to come from assertions or intentionally thrown errors, but also from merely making a mistake while writing the test itself, ie. accidentally making a typo in a function call, causing errors such as undefined is not a function to be completely swallowed, leaving the developer clueless as to what is wrong with the test.\njs\ntest.cb('sync Error in callback-based test causes ava to hang indefinitely', () => {\n  process.chdr(__dirname) // TypeError: process.chdr is not a function (typo)\n})\n. ",
    "loris": "I confirm the bug:\n```\nfunction asyncFn(cb) {\n  cb(null, null);\n}\ntest.cb(t => {\n  asyncFn((err, res) => {\n    t.is(res.bar, 'ok');\n    t.end();\n  });\n});\n```\nThis will cause ava to hang indefinitely instead of displaying an error like TypeError: Cannot read property 'bar' of null\nNot sure why it is tagged as low priority, it makes ava a pain to use to test callback-based methods.\n. ",
    "ivogabe": "I can update the definitions if you notify me of changes. I have submitted a PR: #571\n. @kristianmandrup It looks like you're using both typings and @types? I'd advise to use the latter. You can simply run npm install @types/node @types/mocha, set \"types\": [\"mocha\", \"node\"] (like you already did) and TypeScript will find the type definitions. The type definitions for AVA are automatically found with no additional configuration needed. I'm not familiar with the configuration needed for typings.\n. > Currently it's not possible to use the context object to share context between the beforeEach/afterEach and the test as describe in this section.\n\nBut beware! It's only available in test, beforeEach and afterEach. It's not available in before and after.\n\nWhat do you mean exactly? Should context only be available in test, beforeEach and afterEach?\n. TestContext extends AssertContext, so you cannot write t.skip.skip or t.skip.plan (see b6bc16790db18362ae45db3afab0b8a4caf8543b)\n. @sindresorhus I've addressed your comments. Chaining works because of the namespace declarations.\n. @SamVerschueren Because of your comment I realized that skip had the wrong type :)\n. @sindresorhus Thanks for merging!\n@SamVerschueren You're right about that, will send another PR.\n. :-1:, definition files for TypeScript should not use declare module. When you set module to commonjs (or moduleResolution to node, if you're using babel to transpile modules to commonjs), TypeScript will find these definitions\n. Looks good!\n. \ud83d\udc4d after you've addressed my two minor comments\n. TypeScript users should already know how to use the outDir option, so it should be fine without it too.\n. :+1: \n. If you change the return type of throws to any, \ud83d\udc4d. The return type of notThrows looks correct to me.\n. Indeed, one might write throw \"\".\n. More accurate would be:\ntypescript\nthrows(value: Promise<{}>, error?: ErrorValidator, message?: string): Promise<any>;\nthrows(value: () => void, error?: ErrorValidator, message?: string): any;\nnotThrows<U>(value: Promise<U>, message?: string): Promise<U>;\nnotThrows(value: () => void, message?: string): void;\n. Does todo allow chaining? This definition doesn't allow it.\n. This seems to be related too: Microsoft/TypeScript#9281\n. At the moment we should add every possible order to index.d.ts. If we would really fix this, we should write a script that generates the definition file.\n. \ud83d\udc4d, adding test.always.after should not be to hard but that can be done later too.\n. I've addressed most of the comments and fixed the linting issues. Can you take another look?\n. I usually don't add generated files to a repo, though I had two reasons now: it's still possible to install AVA from GitHub, and you can easily see what changed when you update the generator script.\n. I've added it to .gitignore. postinstall won't work, since the generator script requires NodeJS 6. I think that TS users already know that it's usually not possible to install a package from GitHub. Do you agree?\n. @jamestalmage I would agree, but I'm afraid that the package cannot be published anymore with that change... The generation script uses ES2015, thus needs Node 6, and npm publish is broken on Node 6.\n. That makes life easier then. I've added it in 54a8324f334ec0c74787e5078a0d0fea33cefffd.\n. Sorry, I've been too busy lately and forgot about this. I've added babel/register, I thought it was easiest to do that in package.json, otherwise I would need to add another file that required babel and the generation script.\n. Does babel-node require more configuration? It still fails on node 0.10 and 0.12. The tests are failing on node 4, has anyone an idea why?\n. \ud83c\udf89 \n. You can also set lib to ['es2015'] in TypeScript 2.0 to make those types available, without changing the target.\n. PromiseLike would work, there's no need to add another dependency for it.\n@jamestalmage That only applies when you annotate an async function with a return type. Promise<T> is assignable to PromiseLike<T>, so you can still use async functions after this change.\n. Looks good to me now. @SamVerschueren have you taken a look at this?\n. @SamVerschueren Then all parameters are still typed as any, so that doesn't give any type safety. Using ...args: any[] would then even be better, as it allows an arbitrary amount of arguments. Your example will work. When you give t and obj type annotations, you will get this macro: Macro<ContextualTestContext, { [ key: number ]: string }, {}>. The last position isn't used and defaults to {}. So, macros with less than two arguments work; macros with more than two parameters do not work.\n\nMaybe a bit off-topic, but I think that it might be better to define macros based on partial application/currying like this: (using the same example)\ntypescript\nconst macro = (obj: { [key: number]: string }) => (t: ContextualTestContext) => {\n    for (const key of Object.keys(obj)) {\n        t.is(eval(obj[key]), key);\n    }\n}\ntest('2 + 2 === 4', macro({4: '2 + 2'}));\ntest('2 * 3 === 6', macro({6: '2 * 3'}));\nThat doesn't require a special overload for macros, and can thus be typed correctly. It might be an idea to type the macro overload with ...args: any[] and advise TypeScript users to use currying like in the previous example.\n. Looks good to me\n. I guess that Flow would also need a generator script, since the parameter type depends on the chain and test.cb.cb should be blocked. I think that the current script can be rewritten to generate the two type definition files of both TypeScript and Flow, then both can be kept in sync. However, can't Flow read TS definition files?\n. The TS definitions are indeed stricter; besides test.serial.serial it will also disallow things like test.after.before.\nTS also supports intersection types and generics, so the definition of Flow can be ported to TS with almost the same amount of lines.\n. See Microsoft/TypeScript#8262, fixed in TS2.0\n. @SamVerschueren I thought that .context shouldn't/couldn't be used with beforeEach and afterEach. Was I mistaken with before and after?\n. Then if (!has('beforeEach') && !has('afterEach')) { should be if (!has('before') && !has('after')) {\n. @rcorrear context for beforeEach/afterEach was fixed in #1008. I've got a fix for always in #1025.\n. I see that the following questions have been discussed:\n1. Should silly combinations be disallowed by the runtime?\n2. Should TS block these silly combinations?\nThe only reason against 1 was that it could break other usages of option-chain. Though I think that it would be easy to add it behind an option or something like that. Then other users can opt-in/opt-out of this behavior.\nI think that everyone agrees that if the answer for 1 is yes, then it is also yes for question 2. Otherwise, the TS definitions block constructs that are 'valid' at runtime. Though one could argue that \"2\" * 3 is also 'valid' at runtime, but still blocked by TypeScript. TypeScript also supports private and protected properties. Even though these are available at runtime, the compiler will block access to them outside of the class declaration.\nMy ideal solution would be that AVA disallowed these silly combinations. If the validation logic could be exposed somehow to the TS generation script, then the TS definition will be automatically up to date (most times). The generation script already gets the list of all modifiers from the AVA source code, so when a new modifier is added it will automatically be added to the type definitions.\n. \ud83d\udc4d\n. Yes, rebased\n. Looks good to me, I've tested it on one of my projects and it worked just fine.\nThis does however not work with TypeScript 2.0, 2.1 is required (because of the use of T['skip']). 2.1 was released december last year. But that's probably not an issue since AVA is not 1.0 yet.. I think that's the best explanation of it. I've added it in the comment.\n. Return type is not necessary here, TypeScript will automatically infer it\n. Can you also add outDir? That will compile the sources to a different directory. If you place all files in the same directory, you can easily confuse JS fileswith TS files.\n. I have used lib/test for the TS tests and dist/test for the compiled tests. The options should be \"outDir\": \"dist\", \"rootDir\": \"lib\".\nAVA needs some configuration for that, I started AVA using ava dist/test. It can also be configured in the readme. Though maybe dist/test/*.js could be added as a new test location?\n. This line checks that at least two of them are true, and that's not very easy to do with ||.\n. \ud83d\udc4d \n. As @novemberborn suggested, it would be even better to use a for of loop here\n. Sounds like a good idea, it can be really annoying that the type definitions are not up to date.\n. This doesn't allow multiple macro's (like in the test function in base.d.ts). I would define a type alias, after the definition of Macro in base.d.ts:\ntypescript\ntype Macros<I, E, T> = Macro<I, E, T> | Macro<I, E, T>[];\n. Can you put the macro declarations after the original declarations? Editors will then first show the macro-less declaration, and that is probably the most used one.\n. That would require Microsoft/TypeScript#5453. That's not implemented yet, so for now these are the options:\n- Use ...args: any[] - no type checking at all\n- Support only two arguments (current)\n- Support max two arguments, and mark them optional (?) - this means that the amount of arguments isn't checked; a macro with 2 arguments can be run without arguments, and the other way around\n- Support zero, one or two arguments\nUsing ...args: any[] means that TypeScript doesn't check anything at all, so that's not a good idea in my opinion. The last option means that three interfaces are needed (Macro0, Macro1, Macro2), and 3 x 2 = 6 function overloads for Macros. So that's not ideal, but it might be the best solution. (That option could be extended to any amount of arguments of course. Since the definition is already generated, it wouldn't be very hard to add.)\n. Renaming it to title might be confusing, since you'd then have a function and an argument both named title. I did a quick test and autocompletion doesn't show the argument name when implementing it like this:\n``` typescript\nexport interface Macro {\n    (t: T, input: I, expected: E): void;\n    title? (providedTitle: string, input: I, expected: E): string;\n}\nconst macro: Macro<...> = () => {};\nmacro.title = (/ cursor /) => \"\";\n``\n. These should not be marked optional here, since that would require a user to make them optional as well. Making them optional intestand the generator script is sufficient. (A function with less arguments is assignable to a function type with more arguments.)\n. Addexport type ContextualTest = GenericTest<{ context: any }>;, otherwise this might break some users. Same forContextualCallbackTest. Move this tobase.d.ts. Addexport. Can we think of a better name here? Technically this is not the function that does the tests, it only registers a test. So maybe something likeRegisterBase, and renameTestFunctionRegister`?\nAnd add export here. That's not an issue, base.d.ts is indeed only used by the generation script to write generated.d.ts.. ",
    "alvipeo": "+1\n. ",
    "mciparelli": "Ah, how curious, I was just trying to do the same thing. Thanks for the update.\n. ",
    "Wildhoney": ":clap: :clap:\nFor those wondering, it appears as though this hasn't yet been published to npm \u2014 however a npm i https://github.com/sindresorhus/ava will suffice for the moment.\n. For those who need to parse YAML files:\n``` javascript\nimport { readFileSync } from 'fs';\nimport { safeLoad } from 'js-yaml';\nrequire.extensions['.yml'] = (module, filename) => {\n    module.exports = safeLoad(readFileSync(filename, 'utf8'));\n};\n```\n. @gajus what was your eventual solution?\nI have a similar issue in that I'm using rollup for the build \u2014 which excludes modules in the es2015 plugin \u2014 however for AVA it requires modules, and I therefore logically thought an overwrite in the package.json would have been the solution \u2013 to no avail.. @gajus thanks.\nFor others reading this issue, I implemented it the following way.\nWith my .babelrc file I specified two separate env objects \u2014 one that satisfies Rollup and one that satisfies AVA:\njson\n{\n  \"env\": {\n    \"build\": {\n      \"presets\": [\n        \"stage-3\",\n        [\"es2015\", { \"modules\": false }]\n      ]\n    },\n    \"test\": {\n      \"presets\": [\n        \"stage-3\",\n        \"es2015\"\n      ]\n    }\n  }\n}\nI then had two separate commands for building and testing:\nsh\nBABEL_ENV=build npm run build\nBABEL_ENV=test npm test. ",
    "tleunen": "Can we get a cli option for that as well? I prefer specifying it there instead of adding a config in the package.json\n. Yep, usually it will always be inherit anyway.. Or maybe also --babelrc=[file path]?\n. @sindresorhus When you want to run ava but dont want to pollute package.json with ava specific configs.\n@jamestalmage afaik, the default value is not inherit so having the option make sense to set it.\n. Well.. of course the command will probably be inside a npm script..\nOr at least make ava use the configs property in package.json instead of a \"root property\". I don't think it's the case at the moment\n. https://docs.npmjs.com/files/package.json#config ?\nOk that's a choice you made. But I still think it should be set as a cli option as well... If you don't want, that's fine. I'll just wait for ava to get the default value to inherit in that case.\n. ",
    "StevenLiekens": "@novemberborn link is dead so I'm not sure where to look. Regular user here (with no knowledge of Babel or AVA internals).\nI like having my builds completely separate from my tests. That's why I have a babelrc that transpiles sources+tests to an out-dir first. Then I run AVA on the out-dir with require: 'esm', babel: false and compileEnhancements: false.  This works really well for me. I can even run babel --watch and ava --watch side by side and I haven't had any issues with that so far (but more experimentation is needed).\nedit: getting code coverage to work without polluting the transpiled source files is somewhat of an issue with this setup\nedit2: I decided not to have test coverage because of reasons outlined here https://medium.com/the-node-js-collection/rethinking-javascript-test-coverage-5726fb272949\nedit3: c8 looks like a promising alternative to nyc that doesn't pollute build output, but it's currently broken on my Windows machine. ",
    "opensrcken": "Ah, thanks for the reference.\nThe indentation was changed because I created new top-level modules in the file. Or were you referring to something else?\n. Flagged as [WIP] since this is meant to spark discussion around issues others may be having with typescript.\n. Ah, I wasn't aware that TS had added auto-resolution. Excellent.\n. ",
    "davidhq": "I thought so... but still then update readme to be more clear for beginners?\nAnd I guess it's neccessary for ava to have 300 deps but still I think for me it is unacceptable to have it in each project ... I already gave one project to a friend and he commented that it's installing a lot of deps, I agree. \nStill a great project... \n. Related to this: ava --init command \"hangs\" on my machine.. I mean it updates package.json but I waited and waited, then realized it's not doing anything and exited with ctrl+c. \nI'm on latest OSX\n. @novemberborn how exactly do you use that $(npm bin)/ava ?  I don't understand.\n@sindresorhus thank you for ied! I will definitely see.... \nI understand that slowness is not your problem. but still something is wrong with the default thing, I have to learn more.\n. ",
    "spadin": "Does running $(npm bin)/ava still work? When I ls $(npm bin) I don't see ava in there. I'd like a way to use ava without the global install. Is that possible?\n. Does running $(npm bin)/ava still work? When I ls $(npm bin) I don't see ava in there. I'd like a way to use ava without the global install. Is that possible?\n. Maybe I should open an issue. But having a similar scripts section in my package.json I get the following error. \n``` sh\n$ npm run test\n\nfrontendapp@0.0.1 test /Users/sandropadin/code/spadin/frontendapp\nava\n\nsh: ava: command not found\n```\nBut it works when I have ava installed globally.\nHere's a link to my package.json, also I'm using Node v6.0.0 if that makes a difference.\n. Maybe I should open an issue. But having a similar scripts section in my package.json I get the following error. \n``` sh\n$ npm run test\n\nfrontendapp@0.0.1 test /Users/sandropadin/code/spadin/frontendapp\nava\n\nsh: ava: command not found\n```\nBut it works when I have ava installed globally.\nHere's a link to my package.json, also I'm using Node v6.0.0 if that makes a difference.\n. Hmm. Sorry, please ignore my previous comment. I rm -rf node_modules and npm installed again and it worked. Couldn't reproduce the error message above.\n. Hmm. Sorry, please ignore my previous comment. I rm -rf node_modules and npm installed again and it worked. Couldn't reproduce the error message above.\n. ",
    "QuotableWater7": "Just curious, has there been any movement on this issue?\ncc: @novemberborn . @sindresorhus I'd be happy to help look into a solution for #583.  To speed along the process, do you know which file(s) I should be taking a look at?. ",
    "austinkelleher": "This would be tremendously useful. Is this still something that is planned @novemberborn?. ",
    "vancouverwill": "any update on this now https://github.com/avajs/ava/pull/1722 is merged ? Would be very useful \ud83d\ude04 . @novemberborn hey yeah I would be interested, could you possibly give me some guidelines in terms of code style you would Iike or any goals with it? Thanks . great, i've started to take a look at it, busy with the day job but hopefully I can get more time on the wk \ud83d\ude04 . can you assign me so no one else ends up having to create duplicate/wasted work?. hi @novemberborn thanks for the pointers so far \ud83d\ude04 \nI used the set() and map() like you said and created new tests in test/reporters/verbose.js and these are in the fixtures folder within test/fixture/report/timeoutinmultifiles/... and test/fixture/report/timeoutinsinglefile/... . \nWithin test/helper/report.js I have updated the timeout, this caused me some issues as I was passing in straight ms i.e. 1000 but this was getting converted to a string 1s because of the library ms we use and I didn't notice that straight away but even when I pass in the correct format for that library\ntimeout: type.substring(0,7) === 'timeout' ? \"1000ms\" : undefined, it gets passed through to the api but the api timeout function never completes.\nWithin api.js I have logged out what the given timeout argument is and this is the same in my new tests as what is passed in the original integration/assorted.js tests but for whatever reason the timeout never gets triggered and all the tests end up passing.\nApologies because I have put a lot of in Api.js and the Verbose reporter trying to determine why the timeout never finishes but so far have not been able to track it down. So I wondered if you had any pointers as to why the timeout is not getting triggered? \ud83d\ude15 \nMany thanks, Will \ud83d\ude04 . hi @novemberborn thanks for the pointers so far \ud83d\ude04 \nI used the set() and map() like you said and created new tests in test/reporters/verbose.js and these are in the fixtures folder within test/fixture/report/timeoutinmultifiles/... and test/fixture/report/timeoutinsinglefile/... . \nWithin test/helper/report.js I have updated the timeout, this caused me some issues as I was passing in straight ms i.e. 1000 but this was getting converted to a string 1s because of the library ms we use and I didn't notice that straight away but even when I pass in the correct format for that library\ntimeout: type.substring(0,7) === 'timeout' ? \"1000ms\" : undefined, it gets passed through to the api but the api timeout function never completes.\nWithin api.js I have logged out what the given timeout argument is and this is the same in my new tests as what is passed in the original integration/assorted.js tests but for whatever reason the timeout never gets triggered and all the tests end up passing.\nApologies because I have put a lot of in Api.js and the Verbose reporter trying to determine why the timeout never finishes but so far have not been able to track it down. So I wondered if you had any pointers as to why the timeout is not getting triggered? \ud83d\ude15 \nMany thanks, Will \ud83d\ude04 . Hey @novemberborn thanks for those comments, will look into them. With regard to access it looks like it already is set to that as far as I can see \ud83d\ude04 \n\n. hey @novemberborn I made the changes you suggested and the different files timeout separately with their own respective error messages.\nWasn't sure what you guys prefer to do for commit messages, whether you like to squash them or keep the major ones around for histories sake, they are in there for now but happy to squash them if you prefer.\nThanks\nWill. hey @novemberborn I made the changes you suggested and the different files timeout separately with their own respective error messages.\nWasn't sure what you guys prefer to do for commit messages, whether you like to squash them or keep the major ones around for histories sake, they are in there for now but happy to squash them if you prefer.\nThanks\nWill. ahh that's awesome. thanks for your patience with the back and forth, I know there has been a fair bit of chat but it is the first time me working on the project so there is bound to more questions than there would normally be.\nAnyway cheers for the help, I'm definitely happy to get it out. Our team use ava all the time and being able to see which files have timed out will be really useful\nAnd good  point about \". runningTestsCount is a global count.\", I had assumed originally that timeout was a global state and still had that way of thinking I guess.. ahh that's awesome. thanks for your patience with the back and forth, I know there has been a fair bit of chat but it is the first time me working on the project so there is bound to more questions than there would normally be.\nAnyway cheers for the help, I'm definitely happy to get it out. Our team use ava all the time and being able to see which files have timed out will be really useful\nAnd good  point about \". runningTestsCount is a global count.\", I had assumed originally that timeout was a global state and still had that way of thinking I guess.. ahh true, I was so focused on timeouts but that was part of the orginal\nplan. Should be able to reuse a lot of the same logic for timeouts tho so I\ncan add the code on here or better yet get this out and we can do another\npr for the interrupts?\nOn Mon, Sep 10, 2018 at 9:31 AM Mark Wubben notifications@github.com\nwrote:\n\n@vancouverwill https://github.com/vancouverwill awesome.\nWas about to merge this and then I realized the PR said this would work on\ninterrupts too, and I don't think we're testing that yet. Will have a look\nto see what state this PR is actually in, and then merge it with the\ncorrect commit message \ud83d\ude09\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/avajs/ava/pull/1886#issuecomment-419830885, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AA4ZIyHIM08-DYvCJcutYtajeds0-zVKks5uZiNGgaJpZM4Vu75E\n.\n. ahh true, I was so focused on timeouts but that was part of the orginal\nplan. Should be able to reuse a lot of the same logic for timeouts tho so I\ncan add the code on here or better yet get this out and we can do another\npr for the interrupts?\n\nOn Mon, Sep 10, 2018 at 9:31 AM Mark Wubben notifications@github.com\nwrote:\n\n@vancouverwill https://github.com/vancouverwill awesome.\nWas about to merge this and then I realized the PR said this would work on\ninterrupts too, and I don't think we're testing that yet. Will have a look\nto see what state this PR is actually in, and then merge it with the\ncorrect commit message \ud83d\ude09\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/avajs/ava/pull/1886#issuecomment-419830885, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AA4ZIyHIM08-DYvCJcutYtajeds0-zVKks5uZiNGgaJpZM4Vu75E\n.\n. I can take a look, why I'll I'm at it I wouldn't mind adding the pending tests line writePendingTests() to tap as it is pretty major feature not to show as the main goal of a test suite is to show the tests which have not passed.\n\nWhat do you think @novemberborn ?. yeah good call about separate pr \ud83d\udc4d . ",
    "IssuehuntBot": "@issuehuntfest has funded $80.00 to this issue. See it on IssueHunt. @dflupu has submitted a pull request. See it on IssueHunt. @sindresorhus has rewarded $72.00 to @dflupu. See it on IssueHunt\n\n:moneybag: Total deposit: $80.00\n:tada: Repository reward(0%): $0.00\n:wrench: Service fee(10%): $8.00. @issuehuntfest has funded $80.00 to this issue. See it on IssueHunt. @rororofff has funded $2.00 to this issue.\n\n\n\nSubmit pull request via IssueHunt to receive this reward.\nWant to contribute? Chip in to this issue via IssueHunt.\nCheckout the IssueHunt Issue Explorer to see more funded issues.\n\nNeed help from developers? Add your repository on IssueHunt to raise funds.. @issuehuntfest has funded $80.00 to this issue. See it on IssueHunt. @dflupu has submitted a pull request. See it on IssueHunt. @sindresorhus has rewarded $72.00 to @dflupu. See it on IssueHunt\n\n\n:moneybag: Total deposit: $80.00\n\n:tada: Repository reward(0%): $0.00\n\n:wrench: Service fee(10%): $8.00. @sindresorhus has funded $60.00 to this issue. See it on IssueHunt. @issuehuntfest has funded $200.00 to this issue. See it on IssueHunt. @issuehuntfest has funded $30.00 to this issue. See it on IssueHunt. @sindresorhus has rewarded $27.00 to @itaisteinherz. See it on IssueHunt\n\n\n:moneybag: Total deposit: $30.00\n\n:tada: Repository reward(0%): $0.00\n\n:wrench: Service fee(10%): $3.00. @issuehuntfest has funded $100.00 to this issue. See it on IssueHunt. @dflupu has submitted a pull request. See it on IssueHunt. @sindresorhus has rewarded $90.00 to @dflupu. See it on IssueHunt\n\n\n:moneybag: Total deposit: $100.00\n\n:tada: Repository reward(0%): $0.00\n\n:wrench: Service fee(10%): $10.00. @issuehuntfest has funded $80.00 to this issue. See it on IssueHunt. @dflupu has submitted a pull request. See it on IssueHunt. @sindresorhus has rewarded $72.00 to @dflupu. See it on IssueHunt\n\n\n:moneybag: Total deposit: $80.00\n\n:tada: Repository reward(0%): $0.00\n:wrench: Service fee(10%): $8.00. \n",
    "avivr": "I'd like to give a try\n. ",
    "bachstatter": "Thanks for all the feedback.  English is my second language. Will update as soon as possible \n. This is a quote from the jsdom repo description. \n. :+1: \n. ",
    "avimar": "Is there a reason why context is specifically NOT included in before and after? I'd think you can use it to create a fixture.. but with some dynamic data, that doesn't need a full-blown beforeEach.\nThanks!\n. @SamVerschueren I see you wrote the patch. You're just following spec? I assume there is a reason the spec was written that way...?\nHmm, there's each file is isolated, so I can just use globals I suppose.\n. ... and if you had a promise-based setup, you could either use await or declare the var and assign it during a before hook. Basically, if you need \"global\" context, you don't need anything special for that.\nOK, thanks!\n. ",
    "7flash": "Please remove misleading example from readme:\n\n. 1.0.0-beta.4. ",
    "thangngoc89": "I edited those empty test file to use test.todo and AVA 0.13.0 , same old error \nhttps://travis-ci.org/MoOx/statinamic/jobs/115137586#L794-L800\n. Those failure files contain : sync, async, and todo test. So I assume this happens randomly\n. @novemberborn thanks. I'm setting up a branch for this.\n. @novemberborn Sorry for the delay. We haven't run tests on node 4 for a while so there is some issues I had to be fixed first.\nBut like @ben-eb said. it's still failing https://travis-ci.org/MoOx/statinamic/jobs/117313475#L889-L902\n. @novemberborn this bug is now appear on node 5 too. \nhttps://travis-ci.org/MoOx/statinamic/builds/117380398\nI started to think that this is a concurrency problem since I can't re-produce this locally. Can we limit the concurrency here ? \n. I ran the above command locally and got the process out of memory (node 5)\n```\n\nnode --max-executable-size=10 --max-old-space-size=15 --max-semi-space-size=1 node_modules/.bin/ava --verbose\n\n<--- Last few GCs --->\n3668 ms: Scavenge 13.0 (50.1) -> 13.0 (50.1) MB, 0.2 / 0 ms (+ 37.8 ms in 1 steps since last GC) [allocation failure] [incremental marking delaying mark-sweep].\n3748 ms: Mark-sweep 13.0 (50.1) -> 12.9 (50.1) MB, 80.5 / 0 ms (+ 56.0 ms in 3 steps since start of marking, biggest step 37.8 ms) [last resort gc].\n3815 ms: Mark-sweep 12.9 (50.1) -> 12.9 (50.1) MB, 66.4 / 0 ms [last resort gc].\n\n<--- JS stacktrace --->\n==== JS stack trace =========================================\nSecurity context: 0x1fb24dce3ac1 \n    2: / anonymous /(aka / anonymous /) [vm.js:~52] [pc=0x1d536f0b544] (this=0x1fb24dc04189 ,code=0x22fc581fdc11 ,options=0x11a5c5d57391 )\n    3: _compile [module.js:387] [pc=0x1d536f933d3] (this=0x11a5c5d57401 ,content=0x3b936c004101 ,filename=0x11a5c5d573d9 <Stri...\nFATAL ERROR: CALL_AND_RETRY_LAST Allocation failed - process out of memory\nAborted (core dumped)\n```\nThis is CI build with AVA from sigabrt branch\nhttps://travis-ci.org/MoOx/statinamic/jobs/117540704#L889\n. @ben-eb can I steal your script ?\n. I improved @ben-eb a little bit. You can now choose how many main AVA processes you want to spawn. They will be run concurrency. https://github.com/MoOx/statinamic/blob/3eaf91f6d58fb76eab8236eebbd5b69625182076/scripts/ava-serial.js\nDoes it looks like a concurrency implementation of AVA ? @jamestalmage  @novemberborn \n. babel-preset-react-native doesn't include es2015 or stage-2 preset. You have to include them too.\n. I tried to test using this recipe. But I can't figure out what am I missing.\nHere is my test (with debug info)\n``` js\n// compiler-template.js\nimport template from \"lodash.template\"\nimport { readFile } from \"fs-promise\"\nexport default function(filePath, templateVars) {\n  return readFile(filePath, { encode: \"utf-8\" })\n    .then((content) => template(content)(templateVars))\n}\n```\n``` js\n// test\nimport test from \"ava\"\nimport mockFs from \"mock-fs\"\nimport mountfs from \"mountfs\"\nimport fs from \"fs\"\nimport compiler from \"../compile-template\"\nmountfs.patchInPlace()\ntest(\"compile template with lodash.template\", async (t) => {\n  fs.mount(\"/some/path\", mockFs.fs({\n    \"/template.js\": \"foo <%= bar %>\",\n  }))\nconsole.log(fs.readFileSync(\"/some/path/template.js\").toString())\nconst result = await compiler(\"/some/path/template.js\", { bar: \"bar\" })\nt.is(result, \"foo bar\")\nfs.umount(\"/some/path\")\n})\n```\nLog:\n```\n$ ava src/_utils/service-worker/tests/compile-template.js\nfoo <%= bar %>\n  \u2716 compile template with lodash.template failed with \"ENOENT: no such file or directory, open '/some/path/template.js'\"\n1 test failed\n\ncompile template with lodash.template\n  Error: ENOENT: no such file or directory, open '/some/path/template.js'\n```\n\nAs you can see, I can read /some/path/template.js content using readFileSync. Not quite sure why the compiler can't read it. \n. I tried to test using this recipe. But I can't figure out what am I missing.\nHere is my test (with debug info)\n``` js\n// compiler-template.js\nimport template from \"lodash.template\"\nimport { readFile } from \"fs-promise\"\nexport default function(filePath, templateVars) {\n  return readFile(filePath, { encode: \"utf-8\" })\n    .then((content) => template(content)(templateVars))\n}\n```\n``` js\n// test\nimport test from \"ava\"\nimport mockFs from \"mock-fs\"\nimport mountfs from \"mountfs\"\nimport fs from \"fs\"\nimport compiler from \"../compile-template\"\nmountfs.patchInPlace()\ntest(\"compile template with lodash.template\", async (t) => {\n  fs.mount(\"/some/path\", mockFs.fs({\n    \"/template.js\": \"foo <%= bar %>\",\n  }))\nconsole.log(fs.readFileSync(\"/some/path/template.js\").toString())\nconst result = await compiler(\"/some/path/template.js\", { bar: \"bar\" })\nt.is(result, \"foo bar\")\nfs.umount(\"/some/path\")\n})\n```\nLog:\n```\n$ ava src/_utils/service-worker/tests/compile-template.js\nfoo <%= bar %>\n  \u2716 compile template with lodash.template failed with \"ENOENT: no such file or directory, open '/some/path/template.js'\"\n1 test failed\n\ncompile template with lodash.template\n  Error: ENOENT: no such file or directory, open '/some/path/template.js'\n```\n\nAs you can see, I can read /some/path/template.js content using readFileSync. Not quite sure why the compiler can't read it. \n. Well. After changing the test like you suggested. Test is passed. But when running AVA with nyc. The old issue is still there. \nENOENT: no such file or directory, open '/some/path/template.js'\nMaybe mocking fs with nyc is not a good idea. \n. Well. After changing the test like you suggested. Test is passed. But when running AVA with nyc. The old issue is still there. \nENOENT: no such file or directory, open '/some/path/template.js'\nMaybe mocking fs with nyc is not a good idea. \n. There you go @jamestalmage https://github.com/thangngoc89/nyc-mock-fs\n. There you go @jamestalmage https://github.com/thangngoc89/nyc-mock-fs\n. 1 downside of this recipe is it doesn't work with Windows. \n. 1 downside of this recipe is it doesn't work with Windows. \n. @sibelius could you post your AVA configuration/ test code ? Mocking filesystem is a different problem here\n. You might be interested in this https://github.com/sindresorhus/ava/issues/604#issuecomment-199516941\nI'm testing React (and others memory hungry function). \n. For the babel problem, I found this interesting project https://github.com/hayeah/babel-fast-presets \nLess files to load when boostraping babel\n. I tested this branch. (no dependencies cache on these builds)\n- Node 5, some error about missing .map files (https://travis-ci.org/MoOx/statinamic/jobs/118010809)\n- Node 4, I can see a useful message about exit reason https://travis-ci.org/MoOx/statinamic/jobs/118010810\n. >  Maybe somehow strangely it did cache something?\nYeah. I cleaned all cache. And tests are passing on node 5.\n\nOops hardcoded slashes in the assertions so the tests fail on Windows :(\n\npath.join can help \n. I think you should add a note about adding babel-preset-react to AVA in order to use JSX inside test files\n. @adriantoine sorry. I missed that\n. ",
    "adius": "Oh, sorry. I forgot to update the repo.\nI meant the version with require 'babel-register, which is not working either.\nPlease check out the repo again!\n. ",
    "changbenny": "I just encountered the similar problem and turned out that I forgot the .babelrc file.\nMaybe helpful to you.\n. ",
    "xjamundx": "I looked around a lot in the code, but eventually got lost, so any help would be awesome. Even if it's just a pointer at how to better test.\n. Yeah JSDOM is probably getting us. Power assert is probably looping over the parent nodes or something and it infinite loops. A test timeout might have helped\n. It is only happening for power-assert and it's only happening for failing tests. \nThe furthest I could figure out how to dive into the code was this which was very slow in these cases:\nhttps://github.com/sindresorhus/ava/blob/45f96bd516a91bb0f9cec71b4e2664b712177e12/lib/runner.js#L111\n. I'll try to come up with a better test case. I had repro'd this issue on master as of 2 days ago, but let me check. THANKS for looking very seriously into this.\n. I added a console.time around that line I mentioned earlier and basically in the fast 1 each test takes 1ms and in the slow one each test takes 50+ms. I'll send a more comprehensive test case tonight. Thanks for poking around at this.\n. Things start to compound when it's multiple files. As well. I did this in 10 files and the numbers are like this:\ntest: 119ms\ntest: 132ms\ntest: 130ms\ntest: 127ms\ntest: 132ms\ntest: 97ms\ntest: 96ms\ntest: 110ms\ntest: 101ms\ntest: 102ms\ntest: 75ms\ntest: 78ms\ntest: 86ms\ntest: 88ms\ntest: 92ms\ntest: 79ms\nThat was adding those lines in https://github.com/sindresorhus/ava/blob/45f96bd516a91bb0f9cec71b4e2664b712177e12/lib/runner.js#L111\nconsole.time('test')\n    this.emit('test', props);\n    console.timeEnd('test')\nfull test code (yes it's sloppy):\n```\nvar jsdom = require('jsdom').jsdom;\nglobal.document = jsdom('');\nglobal.window = document.defaultView;\nglobal.navigator = window.navigator;\n// make sure to shim the global Intl if is missing\nif (typeof Intl === 'undefined') {\n        global.Intl = window.Intl = require('intl');\n}\nimport test  from 'ava';\nimport { render } from 'react-dom'\nimport React from 'react';\nfunction CustomComponent({ value }) { return  }\nvar i = 0;\ntest('does something fast' + ++i, t => {\n    const div = document.createElement('div')\n    render(, div)\n    const originInput = div.getElementsByTagName('input')[0]\n    t.same(originInput.value, 33) // assert on a reference to the value\n});\ntest('does something fast' + ++i, t => {\n    const div = document.createElement('div')\n    render(, div)\n    const originInput = div.getElementsByTagName('input')[0]\n    t.same(originInput.value, 33) // assert on a reference to the value\n});\ntest('does something fast' + ++i, t => {\n    const div = document.createElement('div')\n    render(, div)\n    const originInput = div.getElementsByTagName('input')[0]\n    t.same(originInput.value, 33) // assert on a reference to the value\n});\ntest('does something fast' + ++i, t => {\n    const div = document.createElement('div')\n    render(, div)\n    const originInput = div.getElementsByTagName('input')[0]\n    t.same(originInput.value, 33) // assert on a reference to the value\n});\ntest('does something fast' + ++i, t => {\n    const div = document.createElement('div')\n    render(, div)\n    const originInput = div.getElementsByTagName('input')[0]\n    t.same(originInput.value, 33) // assert on a reference to the value\n});\ntest('does something fast' + ++i, t => {\n    const div = document.createElement('div')\n    render(, div)\n    const originInput = div.getElementsByTagName('input')[0]\n    t.same(originInput.value, 33) // assert on a reference to the value\n});\ntest('does something fast' + ++i, t => {\n    const div = document.createElement('div')\n    render(, div)\n    const originInput = div.getElementsByTagName('input')[0]\n    t.same(originInput.value, 33) // assert on a reference to the value\n});\ntest('does something fast' + ++i, t => {\n    const div = document.createElement('div')\n    render(, div)\n    const originInput = div.getElementsByTagName('input')[0]\n    t.same(originInput.value, 33) // assert on a reference to the value\n});\ntest('does something fast' + ++i, t => {\n    const div = document.createElement('div')\n    render(, div)\n    const originInput = div.getElementsByTagName('input')[0]\n    t.same(originInput.value, 33) // assert on a reference to the value\n});\ntest('does something fast' + ++i, t => {\n    const div = document.createElement('div')\n    render(, div)\n    const originInput = div.getElementsByTagName('input')[0]\n    t.same(originInput.value, 33) // assert on a reference to the value\n});\ntest('does something fast' + ++i, t => {\n    const div = document.createElement('div')\n    render(, div)\n    const originInput = div.getElementsByTagName('input')[0]\n    t.same(originInput.value, 33) // assert on a reference to the value\n});\ntest('does something fast' + ++i, t => {\n    const div = document.createElement('div')\n    render(, div)\n    const originInput = div.getElementsByTagName('input')[0]\n    t.same(originInput.value, 33) // assert on a reference to the value\n});\ntest('does something fast' + ++i, t => {\n    const div = document.createElement('div')\n    render(, div)\n    const originInput = div.getElementsByTagName('input')[0]\n    t.same(originInput.value, 33) // assert on a reference to the value\n});\ntest('does something fast' + ++i, t => {\n    const div = document.createElement('div')\n    render(, div)\n    const originInput = div.getElementsByTagName('input')[0]\n    t.same(originInput.value, 33) // assert on a reference to the value\n});\ntest('does something fast' + ++i, t => {\n    const div = document.createElement('div')\n    render(, div)\n    const originInput = div.getElementsByTagName('input')[0]\n    t.same(originInput.value, 33) // assert on a reference to the value\n});\ntest('does something fast' + ++i, t => {\n    const div = document.createElement('div')\n    render(, div)\n    const originInput = div.getElementsByTagName('input')[0]\n    t.same(originInput.value, 33) // assert on a reference to the value\n});\ntest('does something fast' + ++i, t => {\n    const div = document.createElement('div')\n    render(, div)\n    const originInput = div.getElementsByTagName('input')[0]\n    t.same(originInput.value, 33) // assert on a reference to the value\n});\n```\nfull test command:\n./node_modules/.bin/ava --require babel-register test*.js\n. I'll open a repo with just the tests I'm using tonight.\n. Narrowed it down a bit more. This super slowdown only appears when I pull in a massive dependency. Again, it's only slow when the test is failing AND i'm using power assert directly on the DOm element value AND when i'm using a massive dependency (react-intl in this case) in one of the modules that is being tested.\nSo it's clear to me now (Esp. with DEBUG=ava and the requiretime stuff) that the slowness I'm seeing is directly related to require times.\nRequire times when I do this (this is example is just a single file with 2 tests in it):\njs\nt.same(input.value, amount + 2)\nHUGE\njs\nStart time: (2016-03-09 07:09:52 UTC) [treshold=1%] #  module                                 time  %\n 1  babel-types (../....es/lib/index.js)  146ms  \u2587 1%\n 2  ./lib/api/node.js...lib/api/node.js)  264ms  \u2587 2%\n 3  babel-core (../.....l-core/index.js)  265ms  \u2587 2%\n 4  babel-register (....ter/lib/node.js)  307ms  \u2587 2%\n 5  core-js/shim (../...core-js/shim.js)  225ms  \u2587 2%\n 6  babel-polyfill (....ll/lib/index.js)  230ms  \u2587 2%\n 7  cssstyle (../../....eDeclaration.js)  192ms  \u2587 1%\n 8  ../level2/style (...level2/style.js)  206ms  \u2587 2%\n 9  ./jsdom/living (....living/index.js)  334ms  \u2587 3%\n10  jsdom (../../../....om/lib/jsdom.js)  347ms  \u2587 3%\n11  /Users/jamufergus...tup-test-env.js)  899ms  \u2587\u2587\u2587 7%\n12  ./lib/React (../....ct/lib/React.js)  165ms  \u2587 1%\n13  react (../../../..../react/react.js)  165ms  \u2587 1%\n14  react-intl (../.....t-intl/index.js)  261ms  \u2587 2%\n15  ./i18n (../../../...8n/dist/i18n.js)  342ms  \u2587 3%\n16  react-i18n (../.....n/dist/index.js)  342ms  \u2587 3%\n17  ../Slow (../Slow.js)              707ms  \u2587\u2587\u2587 6%\n18  /Users/jamufergus...SlowFields.js)  868ms  \u2587\u2587\u2587 7%\nTotal require(): 4115\nTotal time: 12.9s\nvs. \njs\nconst { value } = input\nt.same(value, amount + 2)\nWhich results in \n``` js\nStart time: (2016-03-09 07:11:03 UTC) [treshold=1%]\nmodule                                 time  %\n1  got (node_modules/got/index.js)        42ms  \u2587 2%\n2  package-json (nod...e-json/index.js)   47ms  \u2587 2%\n3  latest-version (n...ersion/index.js)   47ms  \u2587 2%\n4  update-notifier (...tifier/index.js)  102ms  \u2587\u2587 4%\n5  meow (node_modules/meow/index.js)      27ms  \u2587 1%\n6  bluebird (node_mo...ase/bluebird.js)   39ms  \u2587 1%\n7  ./lib/create (nod...r/lib/create.js)   55ms  \u2587 2%\n8  power-assert-form...matter/index.js)   56ms  \u2587 2%\n9  ./api (node_modules/ava/api.js)        88ms  \u2587\u2587 3%\nTotal require(): 505\nTotal time: 2.7s\n```\nClose to a simplied test case I can upload, but not tonight, sorry..\nThanks again for all of your help\n. It must be fixed in a downstream dependency, because after uninstalling and reinstalling I can't reproduce anymore. Even on 0.9.2. Sorry for the wild goose chase. Previous attempts to install diff versions never uninstalled first, so some of the deps may have been stale.\n:tada:  thanks all for trying super hard to help here\n. Probably related to this is the fact that ava tests/ --tap | tap-xunit > ava-results.xml produces some invalid XML characters that makes jenkins fail.\n<testsuite name=\"calculator ESC[90mESC[2m\u203aESC[22mESC[39m actions ESC[90mESC[2m\u203aESC[22mESC[39m __tests__ ESC[90mESC[2m\u203aESC[22mESC[39m Actions.fetchOriginAmount ESC[90mESC[2m\u203aESC[22mESC[39m with no changes a noop function should be returned\" tests=\"1\" failures=\"0\" errors=\"0\">\nJenkins output looks like this:\nFailed to read test report file /jobvolume/test-test-test/workspace/metrics-ci/ava-results.xml\norg.dom4j.DocumentException: Error on line 3 of document file:///jobvolume/test-test-test/workspace/metrics-ci/ava-results.xml : An invalid XML character (Unicode: 0x{2}) was found in the value of attribute \"{1}\" and element is \"1b\". Nested exception: An invalid XML character (Unicode: 0x{2}) was found in the value of attribute \"{1}\" and element is \"1b\".\n. @sotojuan it needs to be spread across multiple files:\ntest/test.js\ntest/test2.js\nrunning this command\n./node_modules/.bin/ava test/test*.js  --tap | ./node_modules/.bin/tap-xunit\nThe issue (from what I can tell is the little  \u203a arrows)\nxml\n<?xml version=\"1.0\"?>\n<testsuites>\n  <testsuite name=\"test \u203a this is ta thing\" tests=\"1\" failures=\"0\" errors=\"0\">\n    <testcase name=\"#1 test \u203a this is ta thing\"/>\n  </testsuite>\n  <testsuite name=\"test2 \u203a this is ta thing\" tests=\"1\" failures=\"0\" errors=\"0\">\n    <testcase name=\"#2 test2 \u203a this is ta thing\"/>\n  </testsuite>\n</testsuites>\nThe test I'm using is:\njs\nvar test = require('ava')\ntest('this is ta thing', t => t.pass())\n. Thanks for the tip @sindresorhus \n. Thanks for following up. I'll get on the tap-xunit folks about fixing it on their end.\n. Can anyone confirm if the todo output is correct? The tap-xunit reporter is listing any ava todos as failures.\nEdit: ava seems to be correct, filed an issue here:\nhttps://github.com/aghassemi/tap-xunit/issues/8\n. Woohoo!\nOn Fri, Apr 29, 2016 at 11:07 AM Juan notifications@github.com wrote:\n\n@xjamundx https://github.com/xjamundx We fixed the ANSI escape\ncodes\u2014let us know how they are now.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/723#issuecomment-215832985\n. Woohoo!\n\nOn Fri, Apr 29, 2016 at 11:07 AM Juan notifications@github.com wrote:\n\n@xjamundx https://github.com/xjamundx We fixed the ANSI escape\ncodes\u2014let us know how they are now.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/issues/723#issuecomment-215832985\n. My personal favorite power-assert issue from last week was failing the same test over and over again\n\n\n. ",
    "levithomason": "-1 to magic, +1 to defaults or some kind of init.  I ran into a hangup here as well.  The docs seem to indicate if you \"babel\": \"inherit\" in your ava config, then your tests are transpiled same as your source.  However, I also had to \"require\": \"babel-register\" or nothing was transpiled.\nSo, I'd suggest the default be at least the minimal to start testing using your current source babel config:\n\"ava\": {\n  \"babel\": \"inherit\",\n  \"require\": [\n    \"babel-register\"\n  ]\n},\nI have not done a thorough review of this so I may be over looking more.\n. Oh,  \"sources\". I think I understand the point of this whole issue now,  thanks.\n. ",
    "othiym23": "If you want looser constructor checking, take a look at only-shallow, which exists so that tap can follow pretty much exactly @mattkrick's rationale in the original post. tap differentiates between looser structural equality (t.same, using only-shallow), and prototype-chain validation (t.strictSame, using deeper) for those (comparatively rarer cases) in which you want to ensure that the prototype chains match. It turns out that only-shallow has slightly more complicated logic as a result, but for a test library like ava, any performance consequences should be unnoticeable.\nI'd be super amused if somebody found a use for deepest, but probably a terrible idea for ava.\n. FWIW, I wrote deeper, deepest, and only-shallow explicitly because I was horrified by how [very popular package I'm not going to name that isn't deep-equal] handled objects containing circular references. I (much later) patched tap to use deeper (and then, at Isaac's request, wrote only-shallow) entirely because I wanted to compare objects containing circular references.\nThe only dependency that deeper or only-shallow has is that it will use buffertools to do fast Buffer comparison if it can require() buffertools in the current environment (it's not even an optionalDependency). It's not a lot more complicated than deep-equal; it just looks that way because it was important to me that the algorithm be completely documented, because there are many packages that do something like this, but with poorly-specified functionality. As this discussion shows, the set of assumptions the implementer chooses when designing an equality test is important.\n. > Perhaps AVA should add buffertools as a dependency if it's not too heavy. Or at least document somewhere that installing it will speed up buffer comparison.\nbuffertools only offers a significant performance benefit when used with large Buffers, and with versions of Node earlier than Node 0.12, which includes buffer.equals(), which is undoubtedly the fastest method available. Since it includes native code, you don't want to include it if you don't have to.\n. At this point, there's:\n- only-shallow, which I made to match tap's expectations of structural equality\n- deeper, which I made to be like Node's built-in deepEqual / @substack's deep-equal, but with cycle-breaking and more strict prototype checking (largely because it seems absurd to me to have an identity test that says [] and {} are the same)\n- deepest, which I made because it was fun to see how absurdly strict a structural equality test could be\nSo with the exception of deepest, these are purpose-built algorithms designed for specific purposes, which makes tweaking them unappealing (and, at least, a semver-major version bump). I'm not super interested in writing another one, but it's A-OK if you want to use any of my code (which is largely cribbed from elsewhere anyway) as the basis for a new one for ava.\nThis makes me think that it might be interesting to create some kind of identity generator, where you use an options argument and the function constructor to dynamically generate a JITtable equality test that does exactly what a given application requires, but I'm unlikely to have the time to hack on such a thing soon.\n. ",
    "dustinsanders": "I think ideally ava would accept any extension, as long as there is a register hook to get the file to vanilla javascript. I believe this would also fix issue #521.\n. ",
    "azizhk": "\nI guess we shouldn't run the React transform on everything.\n\nExactly my use case. We have a very big repo and we recently shifted to using ES2015 with react.\nIf we enabled babel-core/register, our init would slow down (Even watching and saving to a different location + nodemon was slow). We used babel's only option to transpile only .jsx files. (https://babeljs.io/docs/usage/options/). So now our developers follow this convention of writing ES5 code in .js files but are enabled to write ES2015 code in *.jsx files.\nI wanted to use the same convention in writing test cases, ES2015 code in *.jsx files but ava does not run them \ud83d\ude22 \n. How do .jsx test files run?\n@vdemedes \nDebugged to find this, but unfortunately/luckily this file has been refactored by @jamestalmage \nHoping that its fixed in next release or minor release to patch this is released soon.\n. Ok the code has been moved to https://github.com/avajs/ava/blob/e01ee00/lib/ava-files.js#L248\nWill see if I can account extension from files or source and raise a PR\n. ",
    "DrewML": "\nrunning .jsx without also transpiling React isn't very useful though\n\nand\n\nand only for .jsx test files add the react transform.\n\n@novemberborn Small note on that - it's fairly common to use JSX without React at this point - see this and this\n. I think the main issue here is that import statements (really require statements) do not expect anything but valid JavaScript to be in the files you import.\nIf you're using Babel in your source files (I assume you are since you're using JSX), you might be able to use this Babel plugin to prevent the error.\n. I think the main issue here is that import statements (really require statements) do not expect anything but valid JavaScript to be in the files you import.\nIf you're using Babel in your source files (I assume you are since you're using JSX), you might be able to use this Babel plugin to prevent the error.\n. ",
    "stoffeastrom": "I commented out the path.extname(file) === '.js' and now my dummy ts test file passes\npackage.json\n\"ava\": {\n    \"files\": [\n      \"test/**/*.ts\"\n    ],\n    \"tap\": false,\n    \"require\": [\n      \"ts-node/register\"\n    ]\n  }\nAs other stated, it seems like a simple fix?\n. ",
    "tjoskar": "@stoffeastrom, if I do the same and adds some typescript specific code like an enum, I get a syntax error:\n\n\nSyntaxError\n``` javascript\n1. Uncaught Exception\n   SyntaxError: test.ts: Unexpected token (3:5)\n  1 | import { test } from 'ava';\n  2 |\n\n3 | enum testEnum {\n    |      ^\n  4 |     a = 1\n  5 | }\n  6 |\nat Parser.pp.raise (node_modules/babylon/lib/parser/location.js:22:13)\n...\n```\n\n\nHowever, if I try to run the following typescript test:\n``` javascript\nimport { test } from 'ava';\nlet num = 1;\ntest('should test something', t => {\n    num = '1';\n    t.is(num, '1');\n});\n```\nI get the following error:\ntest.ts (6,5): Type 'string' is not assignable to type 'number'. (2322)\ntest.ts (7,5): The type argument for type parameter 'U' cannot be inferred from the usage. Consider specifying the type arguments explicitly.\nSo it seems to me like babel kicks in first and then the typescript compiler. We need a way to disable babel.\n. @niieani, as you can see in my second example, the typescript compiler gives an error since I try to change type on the variable num so the typescript compiler works fine :) \nMy understand is that ava always runs the test throw babel (https://github.com/avajs/ava#es2015-support) and that babel run before any another requirer hook (like \"ts-node/register\") or I'm wrong?\n@niieani, are you able to run the following (typescript) test case:\n``` javascript\nimport { test } from 'ava';\nenum testEnum {\n    a = 1\n}\ntest('should login', t => {\n    t.true(testEnum.a === 1);\n});\n```\n. @niieani, requiring a typescript file inside a .js file will work since ava only transpile the test files. \nIt feels like we have lost focus from the issue but I want to clarify what I meant.\nLet's say we change the code to accept any file extensions, babel will still try to compile the test. even if we register the ts-node/register hook. eg. source => babel =>  typescript => ava. See: https://github.com/avajs/ava/blob/ca800ebd2bf8a5ba47ec26cdeaf977e99898bc3f/api.js#L50\nThis may not be a problem if you don't use any typescript specific code, like enum or interfaces, in your test files. \nTake a look at this example:\n``` javascript\n// test.ts\nimport { test } from 'ava';\nlet num = 1;\ntest('should test something', t => {\n    t.is(num, 1);\n});\n```\nava => babel (ok) => typescript (ok) => ava (ok)\nThat will work fine since that is valid javascript code.\nNow, look at the following example, where I try to change data type on the num\u00a0variable. \n``` javascript\n// test.ts\nimport { test } from 'ava';\nlet num = 1;\ntest('should test something', t => {\n    num = '1';\n    t.is(num, '1');\n});\n```\nava => babel (ok) => typescript (error)\nBabel will transpile it to ES5 which is fine and then give the source files to typescript which will complain on num = '1' and give the following error: test.ts (6,5): Type 'string' is not assignable to type 'number'. (2322) which is great and a sign that typescript doing its thing.\nNow, look at the following example, where I use typescript specific code.\n``` javascript\n// test.ts\nimport { test } from 'ava';\nenum testEnum {\n    a = 1\n}\ntest('should test something', t => {\n    t.true(testEnum.a === 1);\n});\n```\nava => babel (error)\nBabel will fail here since it doesn't understand enum and don't know what to do with it. \nSo we need a way to disable babel if we want to use another compiler. Maybe by setting babel: false\u00a0as @jamestalmage suggested: https://github.com/avajs/ava/issues/631#issuecomment-198519637. \nI made a dirt quick fix (as a proof of concept) and replaced these lines: https://github.com/avajs/ava/blob/ca800ebd2bf8a5ba47ec26cdeaf977e99898bc3f/lib/caching-precompiler.js#L82-88 with:\njavascript\nreturn function(src, filePath) {\n    fs.writeFileSync(filePath, src);\n};\nand after that I could use ava to run my typescript test :tada:\nSo... We need a way to specify what extension we should use. I think this solutions is great: https://github.com/avajs/ava/issues/631#issuecomment-198973019 and we need to be able to disable babel, without breaking changes.\n. ",
    "LiTiang": "@niieani so at now, we still could not just use \"require\": \"ts-node/register \nwith \"files\": [test.spec.ts], then work easily as magic ?\n. ## How to access window in test file ?\nHi, @jamestalmage , thanks reply\nI read article for mocking the DOM which u present\nbut i have no idea how can i access window object in my AVA test file ?\nIn Node, jQuery need  window to initalize\nlike this var $ = jQuery = require('jquery')(window);\nAVA must know $, \nbecause it really execute that code, \nthat meaning, i must declare $ in test file, \nthen run all test\n. ok, i solve it, \ni decide to use javascript instead of typescript,\nit seems there is no need to use typescript in AVA now,\nbecause i find the way to load typescript module directly in javascript\nanyway, @jamestalmage  thanks again\n. THX, i solve the prob, \ni still use Typescript + jQuery + AVA + Sinon\n. ",
    "dmisdm": "Id really rather not have to get webpack to compile .js files using typescript.\nThis is from ava-files\n// Like in api.js, tests must be .js files and not start with _\nif (path.extname(filePath) !== '.js' || path.basename(filePath)[0] === '_') \n        return false;\n    }\nIs there some rationale behind this that i haven't picked up?\n. ",
    "nkoder": "I see a lot about JSX here. But what about ES6? I mean I have project with both JSX and ES6 files, and I write them in IntelliJ. In IntelliJ you can set one JavaScript language per whole project or (what I prefer) you can hint IDE about language by extension. Therefore .es6 is recognized as EcmaScript 6 and .jsx as JSX. Therefore I do not want to write my AVA tests in ES6 and name them with .js extension.. ",
    "rauschma": "My use case is: transpiling .mjs files via Babel until Node.js supports them natively. Alas, with this config, AVA does not find any files in test/:\njson\n  \"ava\": {\n    \"require\": [\n      \"babel-register\"\n    ],\n    \"files\": [\n      \"test/**/*.mjs\"\n    ],\n    \"babel\": \"inherit\"\n  }\nWhat\u2019s the harm in making .js the default, but allowing arbitrary custom file extensions?. Cool, that would do exactly what I need (via babel.extensions)! Maybe add an explanation for what happened to the property inherit?\nWithout babel.extensions, I can\u2019t currently work with AVA, because I even get errors if I mention files directly:\n```\n\nava \"test/first-module_test.mjs\"\n  \u2716 Couldn't find any files to test\n``\n. Awesome! If you dobabel.extensions, I suspect you need to supportbabel.inherit(or similar), too, given thatbabel: \"inherit\"can\u2019t be specified, anymore.. An easy preliminary solution would be to introduce a propertybabel.valuethat takes over the role thatbabelhad, previously.. Perfect explanation, thanks! Just copy-paste this underneath the code snippet in the readme? Alternatively, it could be the start of a stand-alone file documenting the AVA properties inpackage.json`.. \n",
    "Jamesernator": "I'd like to see this so that @std/esm can be used to run ESM tests e.g.:\n```js\n// test.mjs\nimport test from \"ava\"\nimport foo from \"../esm.mjs\"\ntest(t => {\n    t.is(foo(), 2)\n})\n// esm.mjs\nexport default function foo() {\n    return 2\n}\n// package.json\n{\n    \"ava\": {\n        \"require\": [\"@std/esm\"]\n    }\n}\n```\nGiven that .mjs is going to be the way to distinguish ESM from CommonJS in Node this should be supported even without a more generic way of supporting other extensions.. ",
    "cameron-martin": "This will become even more important once babel 7 is released, which has the ability to parse and remove the type annotations from Typescript files.. ",
    "vanga": "Hello,\nI want to use ava for server-side(Coffee-Script) testing,  I want to write tests also in Coffee. Based on my searching around, I see that there are ways to achieve this. Some posts suggest to use --require, but that seems to have been deprecated.\n\n{\n  \"ava\": {\n    \"require\": \"coffee-script/register\",\n    \"babel\": false,\n    \"files\":  \"test/*/.coffee\"\n  }\n}\n\nIs this still relevant? I tried to do something like this and was unsuccesful.\nCan someone please tell me if what I am trying to do is possible and point me in the right direction if it is possible.\nThanks.\n. Thanks @novemberborn \nI guess I will try having a wrapper that compiles coffee script test files to JS before starting ava tests. That's enough for me if it works.. ",
    "feross": "\n@vanga AVA performs some transpilations on the test files. These assume the test files contain JavaScript. Until those transpilations can be disabled we cannot support arbitrary file extensions, or indeed support non-JavaScript test files.\n\nMakes sense. But why can't .mjs be treated as JavaScript? That extension will always contain JavaScript. ava won't run tests even when specifying an .mjs file explicitly, which is preventing the testing of ES Module code:\n```\n$ ava test/basic.mjs\n1 exception\n\u2716 Couldn't find any files to test\n```\ncc @jdalton. @novemberborn That's great to hear! I was just investigating porting tests from tape to ava for a few minutes, so I'm not invested enough in ava yet to dig into the code to send a PR. Hopefully someone who's more familiar can jump in and try this?. You might want to\u00a0update this PR to point out that .mjs files won't work with ava until https://github.com/avajs/ava/issues/631#issuecomment-357733734 is fixed.\nRunning ava or ava test/**/*.mjs or ava test/my-test.mjs results in:\n```\n  1 exception\n\u2716 Couldn't find any files to test\n```\nThis was surprising to me since the documentation in this PR mentions the .mjs extension explicitly, implying that testing .mjs files would work!. ",
    "Jaden-Giordano": "@novemberborn Do we have an update on this issue?. @novemberborn Do we have an update on this issue?. @novemberborn So really the only issue is that ava is filtering based on the extensions hard coded:\nhttps://github.com/avajs/ava/blob/master/lib/ava-files.js#L60\nWe shouldn't need to filter since the glob already handles that with the comma separated lists, i.e. **/*.{js,jsx}. I say we should just remove this line it should work just fine. Let me know what you think and I'll create the PR if you approve.\nEdit:\nJust tested it out by removing the line and setting the files option to tests/**/*.{js,jsx}; it finds all the files and runs all the tests. Here is the test directory I used:\ntests/\n-a/\n--test-in-folder.js\n--react-test-in-folder.jsx\n-test.js\n-react-test.jsx\nI'm going to make the pull request so we can fine tune it there.. @novemberborn So really the only issue is that ava is filtering based on the extensions hard coded:\nhttps://github.com/avajs/ava/blob/master/lib/ava-files.js#L60\nWe shouldn't need to filter since the glob already handles that with the comma separated lists, i.e. **/*.{js,jsx}. I say we should just remove this line it should work just fine. Let me know what you think and I'll create the PR if you approve.\nEdit:\nJust tested it out by removing the line and setting the files option to tests/**/*.{js,jsx}; it finds all the files and runs all the tests. Here is the test directory I used:\ntests/\n-a/\n--test-in-folder.js\n--react-test-in-folder.jsx\n-test.js\n-react-test.jsx\nI'm going to make the pull request so we can fine tune it there.. So basically ava.babel.extensions should precompile with the CachingPrecompiler and ava.extensions should opt-out of that? Which would mean the writer of the tests utilizing ava.extensions would need to precompile the tests themselves. Is that the intended flow?\nEdit:\nAlso are we allow both extension providers to be used? If so, you mentioned not allowing the same extension in each so we can throw an error in that case.. So basically ava.babel.extensions should precompile with the CachingPrecompiler and ava.extensions should opt-out of that? Which would mean the writer of the tests utilizing ava.extensions would need to precompile the tests themselves. Is that the intended flow?\nEdit:\nAlso are we allow both extension providers to be used? If so, you mentioned not allowing the same extension in each so we can throw an error in that case.. @novemberborn . @novemberborn . Hey @shirtleton I'm working on making it so you can configure ava to use file extensions like .jsx or .ts. It's almost done, were just adding tests and cleaning up a bit of the code written for it, so should be soon. Here is the PR for reference #1746 Thanks!. Referencing #631 . Referencing #631 . Gotcha, I'll make the updates and update this PR. Thanks for the filler!. Gotcha, I'll make the updates and update this PR. Thanks for the filler!. @novemberborn Looking at the code its a bit above my current knowledge with ava.. @novemberborn Looking at the code its a bit above my current knowledge with ava.. @novemberborn that would be wonderful! \nI was looking through the code for awhile and I understand how the pipeline is initiated. I'm just having trouble determining which tests should have the precompile ran and which tests shouldnt.\nEdit:\nI just noticed your comment on the issue. I guess im just trying to think of the best way to pass the extension options to the precompiler.\nEdit 2:\nI was able to figure out a way to pass them along and create deterministic precompilation based on extension parameters. Ill update the pr.. There are currently errors I'm attempting to resolve. I keep getting ResolveError: D:\\Projects\\OpenSource\\ava-test\\.babelrc: Couldn't find preset \"@babel/core\" relative to directory when testing ava in a project.\nEdit:\nThere are other issues as you can see in the CI, I'm still trying to figure out why this is happening if you can help maybe figure that out. After this next steps would be the watcher side of things.. @novemberborn You think you can help me with that?. No worries, I assumed so, thanks for being so helpful so far.. ### Update:\nSo I managed to move past those errors, but now the cli and api tests are not passing, Im assuming this is happening since I added two properties to the config (extensions and babel.extensions), I'll continue to look into what the root cause of these issues are, but if anyone is willing to help who is familiar with the api and cli files help/feedback would be greatly appreciated. \nNext steps after fixes:\nAfter this fix, we should be ready to move on to notifying the watch.. @novemberborn Thank you for the insight it helped immensely! That fixed most of the issues. It is working if I install from my git repo with this current branch and run it with babel.extensions = ['jsx'] too. Maybe you have some idea what is going on in the CI? With that being said I'm going to go ahead and move on to informing the watcher.. Sounds good, take you're time.. Yeah will do!. I think I did the rebase correctly. Will be a couple days until I can finish up your recommendations and write those tests.. I was having trouble figuring out what tests to write without writing a test that did the same thing as other tests, so I only wrote one test that tests if the tests successfully precompiles before being ran then passing if tests in the fixture ran successfully. The test I wanted to write to check if the tests run when neither extensions nor babelExtensions are defined (defaults to then not compile the test files and looks for js files) was basically the same as this: https://github.com/avajs/ava/blob/master/test/api.js#L1020.\nUpdate\nIm actually going to write some better tests that actually check if its being precompiled using the fixture for precompiling.. So the test would just end up precompiling and checking if the test passed but thats what the test I wrote already does so I just left that the same but added a test to ava files to look for files with a specified extension. Also what is the transformSpy issue in the lint, can I fix this or just leave it? It seems to be the only issue.. Whoo! This is exciting!. This was an oops, I was running tap.only to test specific files and forgot to revert this line.. Oh alright I see.. For this im just using concat then filter to filter out any duplicate entries:\njs\nconst allExtensions = doNotCompileExtensions\n    .concat(babelExtensions)\n    .filter((ext, i, self) => self.indexOf(ext) === i);. So elegant!. I'm just going to strip away this code, this shouldn't even be needed with the recommendations you gave for api.js.. Instead of multiple if conditions we could use invalid instead then split them up by doing:\n```js\nlet invalid = false;\ninvalid = !conf || typeof conf !== 'object' || ...;\ninvalid = ...\nif (invalid) {\n    throw ...\n}\n```\nThis should be a bit shorter than a bunch of if statements.. oh I thought i reverted this, sorry about that, yeah.. I placed these into an if statement like you recommended. I think that would work out well and I didn't know it would remove duplicates, that's pretty awesome actually (gotta change stuff in my personal projects).. @novemberborn If you could on this one I can handle the rest, work is a bit busy so I don't have alot of time to work on this. Thanks for all the help!. Oh alright gotcha. ",
    "jamesthurley": "To help those looking for what you need to change now this issue is closed:\nWith AVA version 1.0.0-beta.5.1 I used the following settings to successfully run my .ts tests:\n\"babel\": false,\n    \"extensions\": [\n      \"ts\"\n    ],\n    \"require\": [\n      \"ts-node/register\"\n    ]\nThanks @novemberborn and @Jaden-Giordano for getting this working!. ",
    "bouzuya": "s/void/any/\nNOTE: \nthrows returns thrown object or PromiseLike or void.\nhttps://github.com/sindresorhus/ava/blob/v0.13.0/lib/assert.js#L76-L85\nhttps://github.com/sindresorhus/ava/blob/v0.13.0/lib/assert.js#L95-L106\nnotThrows returns PromiseLike or void.\nhttps://github.com/sindresorhus/ava/blob/v0.13.0/lib/assert.js#L117-L124\ndocument:\nhttps://github.com/sindresorhus/ava#throwsfunctionpromise-error-message\n\nReturns the error thrown by function or the rejection reason of promise.\nE\n. \n",
    "thomaslindstrom": "As pointed out:\n\nI guess you could compare the functionality to before, but before is strictly for test setup, not tests per se.\n\nbefore should be used to prepare data used in the test, not to test.\nOne example I can think of is creating a user \u2013 which on its own is a test, but that also is a prerequisite for latter tests where the user created is the one tested against. In that case, if the required test fails, those tests will always fail, and there's no point in running them.\n. This wouldn't actually be more than syntax sugar, as it doesn't really do more than what's in already. \nBut alright, guys! Thanks for considering. \ud83d\udc19\n. ",
    "jeffreywescott": "FWIW, I've run into this, too. I've done some debugging here, and this is what seems to be happening:\nMy package.json was using the require configuration to do some \"test harness setup\". In that file, I was importing 'ava'. Somehow, this confuses things. I'm not sure why, but I think it's probably related to forking and that variable doesn't stay \"set to true\". Bottom-line:  importing 'ava' inside one of the ava require modules seems to break things.\nAs a workaround, ensure that none of your required modules are importing or requireing 'ava'.\n. ",
    "dawsbot": "Is there a workaround for the time being? Experiencing the same issues as @jacobmendoza.\nsist output:\nOS\nDarwin  - x64  \nnode\nnpm -v: 3.9.5\nnode --version: v6.2.2\nTime created: Tue Jul 05 2016 20:55:11 GMT-0700 (PDT)\n. ",
    "sgnl": "I ran into this and it was because in the README.md it has the line \"require\": ['esm'], so I added that to my configuration file even though the readme didn't say anything about adding that line, I just assumed and did so.\nhttps://github.com/avajs/ava/blame/6d12abfdff4478a1b6f4e87237764b89eb05f18d/readme.md#L308\nremoving this line now removes the \"Tests not found in [file path here]\" error and ava correctly sees my tests and reports are accurate.. ",
    "AlexandreBonaventure": "For every one having the lodash-es issue. \nThis is due to an issue in babel itself (fixed in 7.x) where ignore field is ignored in .babelrc.\nHere's a link for a workaround: https://stackoverflow.com/questions/36857210/using-babel-register-with-ava-and-babelrcs-ignore-false-but-node-modules. ",
    "satya164": "Is there a way to see which tests exactly timed out?\n. ",
    "kutyel": "@sindresorhus Done! :wink:\n. @sindresorhus Done! :wink:\n. ",
    "willsoto": "So the PR should go against TypeScript it seems. Thanks for the\nclarification.\nOn Mar 18, 2016 1:58 PM, \"Sam Verschueren\" notifications@github.com wrote:\n\nCan't seem to find the official announcement anymore. I could find this\nissue however\nMicrosoft/TypeScript#6734\nhttps://github.com/Microsoft/TypeScript/issues/6734\nAnd when running tsc -h, this is the target description\nSpecify ECMAScript target version: 'ES3' (default), 'ES5', or 'ES2015' (experimental)\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/pull/657#issuecomment-198471827\n. So the PR should go against TypeScript it seems. Thanks for the\nclarification.\nOn Mar 18, 2016 1:58 PM, \"Sam Verschueren\" notifications@github.com wrote:\nCan't seem to find the official announcement anymore. I could find this\nissue however\nMicrosoft/TypeScript#6734\nhttps://github.com/Microsoft/TypeScript/issues/6734\nAnd when running tsc -h, this is the target description\nSpecify ECMAScript target version: 'ES3' (default), 'ES5', or 'ES2015' (experimental)\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/sindresorhus/ava/pull/657#issuecomment-198471827\n. \n",
    "StoneCypher": "It's not clear why, but I was redirected to this ticket to discuss that I think it's probably counterproductive for .throws to reject fully valid thrown exceptions merely because a string rather than an Error object is being thrown\nThis is contrary to the language standard, is not mentioned in the documentation, and is not mentioned in the resulting error text. @novemberborn - wsl is still kind of a train wreck (for example, editing things in the wsl area from outside on disk can corrupt the entire image.)\nmost windows node developers use git bash. @simonbuchan - as a word of advice, pretty much everyone who knows that limitation exists also knows why it exists.\ngiven that we can't use windows software with the stuff running on our windows machine, because of a technical limitation not shared by any of the existing solutions, such as cygwin or virtual machines or so on, because just opening the file potentially corrupts an entire drive?\ni'm standing by my evaluation that wsl is currently a technical train wreck. It turns out that I do real work every day.\nIf you choose to ask politely, I'll explain why that won't actually work.  Or you could ask google.\nFailing that, please have a nice day.  . I really don't think we should be excluding valid throws just because a linter rule claims it's bad practice.\nI lost nearly an entire day because I threw a string, which is fully valid, and .throws broke instead of catching it or telling me what it didn't like. I don't really think my commentary belongs on this issue, which isn't about throwing strings in any meaningful way, but a maintainer closed the valid and appropriate issue about that explicitly in favor of this one\nI apologize for what seems to me to be unnecessarily changing the subject. Thanks.  That seems ideal to me.. linters often prevent hurt feelings over formatting rules. like this. Oh.\nMay I please recommend that the error message here point that out as a possibility?  It would not have occurred to me that throwing a string wouldn't be caught by .throws .. And/or should strings thrown be caught by .throws ?. @kevva - sorry, no, throw can and always has legally accepted strings, as the first example on your source shows.. this is not a duplicate of 661; the behavior is entirely different, and does not appear to be defective in presentation\nthis is not a duplicate of 1047, which is about simplifying throws, and barely touches on this in passing\ni wish this had not been closed, and generally it's polite to ask the person opening the ticket before closing\ni retain the belief that throws should accept all valid throws, which it currently does not, without mentioning that in the documentation. Thank you.. ",
    "bitjson": "Looks like this issue hasn't moved in a while. Does anyone have a working example of mocking the filesystem in AVA tests?. @patrick91 (and anyone else coming across this thread) \u2013 I've been getting into Typescript a lot recently, and had some trouble getting a lot of the packages I wanted to use working together. \nI spent a little time starter project which I think it might accomplish what you're looking for: es7-typescript-starter \n\nWrite your library and AVA tests in Typescript\neverything uses tsconfig.json properly (so tooling/build/test settings match)\nsource-mapped code coverage, typedoc API doc generation, etc. \ntype definitions are linked properly for projects depending on yours\nand there are a few examples of how to use both the main (commonjs) and module (es6 module) output formats\n\nI'd love input/feedback on how it could be improved, especially when AVA restarts work on this issue.. If anyone is interested, the solution @adieuadieu mentioned (from @tomdavidson) is what typescript-starter uses right now.. Another option here I'd love to see considered: a standard-feeling node --inspect workflow.\nOne of my favorite aspects about AVA is the apparent lack of \"magic\" \u2013 test files look normal, and testing infrastructure must be imported rather than magically appearing in globals. Even when AVA performs some \"magic\" for Babel compilation, users interact with it like normal javascript, so there's nothing new to learn/configure. I'd love to see this philosophy carry over to debugging.\nIt would seem much more intuitive for me to be able to debug AVA tests using the same configuration as my other work. Rather than configuring my tooling to use an ava --inspect mode, it would be great if I could do what I do everywhere else: node --inspect compiled/test.js. Hopefully this would also require less setup/customization, since a lot of debugging tooling is designed/documented to fit into this workflow.\nE.g. It's pretty difficult to get debugging of AVA tests working in VSCode with TypeScript right now. I'm working from this recipe. And it doesn't seem possible to get \"one-click\" debugging without changes to either AVA or VSCode. (I'm still looking for workarounds in typescript-starter@next.)\nRecommendation\nWhen executing a test outside of the CLI, AVA could look for either:\n\nan environment variable, like AVA_DEBUG_MODE\nif there is an active inspector\n\nIf found, AVA can setup the infrastructure currently in ./profile.js, and allow the user to debug that test file as expected.\nOf course, if a \"debug mode\" isn't detected, AVA can still log the Test files must be run with the AVA CLI message. \nRelated: https://github.com/avajs/ava/issues/1495. To provide an example of where diverging from the the node --inspect workflow causes incompatibility in tooling, here's typescript-starter's .vscode/launch.json: https://github.com/bitjson/typescript-starter/blob/master/.vscode/launch.json#L52. ",
    "lo1tuma": "Instead of monkey patching globals, node core modules or any other third party modules I usually do dependency injection.. I\u2019ve experimented a little bit with implementing sandboxing in ava via the vm module. This seems to be quite tricky because per default you don\u2019t have a node-environment in the context of the vm, which means you don\u2019t have e.g. require.\nI also tried NodeVM from vm2  which unfortunately has a bug which leads to throwing \n a TypeError for every assertion (e.g. t.is()). AFAIK vm2 injects all the commonjs/node globals and the function wrapper manually to the sandbox, so I would assume that it doesn\u2019t work nicely with ES modules.\nI\u2019ve also struggled a little bit with the way how the runner instance is loaded in each test file (i.e. import test from 'ava'). The current approach only works because ava uses a separate process for every test file.. I wonder if Runner is the correct place to implement this. If I remove a test file then the highlighted code in Runner would be never executed, right?. I\u2019ve tried to run this against the test-suite of one of our projects with 1608 tests in total. While ava v1 takes round about 37 seconds, with this changes it seems to be extremely fast (my gut feeling < 5 seconds with the ForkTestPool). The problem is that after 1222 tests have been completed the runner completely hangs and I have to manually abort to process. I see some warnings in the console:\nMaxListenersExceededWarning: Possible EventEmitter memory leak detected. 25 unhandledRejection listeners added. Use emitter.setMaxListeners() to increase limit\nThe same happens when I use the --no-fork option.\nApart from that, the reporting seems to behave strange, does it show a spinner for every process (I can see 8 for fork and 2 for no-fork)? Here is  a screenshot:\n\nAs you can see in the screenshot there is also a warning from react which complains about missing requestAnimationFrame. I don\u2019t get this warning with ava v1. I would guess this is related to sandboxing. In some test files I set global.window to an instance of jsdom.\nAs a follow up feature it would be also nice to provide an option which disables sandboxing in favor of performance.\n. IMHO there is no need for builtin support in ava. We use throat to limit the amount of parallel runs of tests using puppeteer.. @be5invis good point, probably it doesn\u2019t. I have all the puppeteer-based tests in the same file, so that wasn\u2019t an issue.. Sharing the browser/page between tests might cause unexpected issues because all tests will run in parallel. What I usually do in my ava/puppeteer tests is to start a separate browser instance for each test.\n```js\nasync function withBrowserPage(fn) {\n    const browser = await puppeteer.launch();\n    const page = browser.newPage();\n    try {\n        await fn(page);\n    } finally {\n        await page.close();\n        await browser.close();\n    }\n}\ntest('foo', t => {\n    return withBrowserPage(page => {\n         page.goto('https://google.com');\n         t.is(await page.title(), 'Google');\n    });\n});\ntest('bar', t => {\n    return withBrowserPage(page => {\n         page.goto('https://bing.com');\n         t.is(await page.title(), 'Bing');\n    });\n});\n```. ",
    "lucasmotta": "@spudly @jamestalmage that's a good point and def removing the babel step will help a lot. But in the other hand, tape and mocha are also transpiling the source code, but they manage to keep it very fast.\nBut the issue #577 looks very promising thou.\n. ",
    "coodoo": "Huge bump on this, any update?\n. @BarryThePenguin there's a typo in the link, here's the correct one :)\n. ",
    "cbrwizard": "Would love this too.. Why is this closed? Is this feature added?. ",
    "kosmotaur": "I have the exact same use case, trying to use babel-plugin-rewire only in tests.\nextends unfortunately failed, but the env way worked well. Not ideal, since I am trying for the project to be fully environment agnostic, but it unblocked me.\n. Good point, apply the env to the least common tasks to remove redundant bits.\nOne more downside is having to repeat sections of .babelrc, in my case, at the moment:\n{\n  \"presets\": [\n    \"es2015\",\n    \"react\",\n    \"stage-2\"\n  ],\n  \"plugins\": [\n    \"transform-object-rest-spread\"\n  ],\n  \"env\": {\n    \"test\": {\n      \"plugins\": [\n        \"transform-object-rest-spread\",\n        \"rewire\"\n      ]\n    }\n  }\n}\nBut that's just Babel now. Anyhow, things work, testing away! \ud83d\ude02 \n. ",
    "djskinner": "I've found that I have to do the same as @kosmotaur:\n{\n  \"presets\": [\n    [\"env\", {\n        \"loose\": true,\n        \"modules\": false,\n        \"useBuiltIns\": true\n    }],\n    \"stage-0\",\n    \"react\"\n  ],\n  \"plugins\": [],\n  \"env\": {\n      \"test\": {\n        \"presets\": [\n            [\"env\", {\n                \"loose\": true,\n                \"modules\": \"commonjs\",\n                \"useBuiltIns\": true\n            }],\n            \"stage-0\",\n            \"react\"\n        ]\n      }\n  }\n}\nFor my main build I want babel to ignore modules as to allow Webpack 2 tree-shaking but need babel to transpile them in a test context.\nNot matter what I tried in pkg.ava.babel I couldn't get it to work. It seems like the original env preset configuration was still being used.. Yes, you're right about the presets getting replace as I'd expected.\nI ended up doing this:\n```\n// .babelrc\n{\n  \"presets\": [\n    \"./.babelrc.js\"\n  ]\n}\n// .babelrc.js\nconst env = process.env.BABEL_ENV || process.env.NODE_ENV || 'development'\nconst browserlist = [\"last 2 versions\"]\nconst getTargets = (env) => {\n    if (env === 'node') return { \"node\": true };\n    if (env === 'production') return { \"browsers\": browserlist, \"uglify\": true };\n    return { \"browsers\": browserlist }\n}\nconst targets = getTargets(env)\nmodule.exports = {\n    presets: [\n        [\"env\", {\n            targets,\n            \"loose\": true,\n            \"modules\": env === 'modules' ? 'commonjs' : false,\n            \"useBuiltIns\": true\n        }],\n        \"stage-0\",\n        \"react\"\n    ]\n}\n```\n. ",
    "hellopao": "@zhaozhiming nice job\n. ",
    "paulohp": "Solid. Thanks.\n. ",
    "Siilwyn": "Just wanted to throw this in, Ava including babel is only annoying from Node 8 and up. It might be worth to consider dropping Node 4 & 6 once Ava 1.0 is released. Or releasing 1.0 (with babel) and some time after releasing 2.0 (dropping babel) backporting important changes.. @sindresorhus whoops, you're completely right. I mixed up the version numbers. Fixed the comment.\n. Oh that's a bummer. :/. @novemberborn ah my main motivation is making the install size smaller but after reading the issue fully I see it makes Babel an optional dependency, which is great! . @codeslikejaggars did you PR your change? \ud83e\udd14 . Ah I see, needs a more global refactor. Thank you for your reply @codeslikejaggars.. Ah I see, needs a more global refactor. Thank you for your reply @codeslikejaggars.. > Remove Babel as a direct dependency. (from https://github.com/avajs/ava/issues/1908#issuecomment-414130823)\nThis would be delightful for smaller projects, the only reason I'm still choosing tape over ava is the 13 MB install size difference.. It does affect the developer experience though and npm ls ava certainly does not return in my dependency tree for most projects.. @foxbunny could you expand on what you mean with support for binary and unary? To clarify I wouldn't want variadic currying, by exporting it separately this won't be needed.\nSo never do this:\njs\nconst test = require('ava/curried');\ntest('something', t => Promise.resolve('hi').then(result => t.is('hi', 'this is a message', result));\nBecause using the default export already offers a clean way to do this. So providing the ability to write both test(expected, message)(data) and test(expected, message, data) is a source of unneeded complexity.. Yes thanks for the input! Maintenance cost is an understandable concern, in this case it means when changing or adding assertion (arguments) it would also need to be changed in the 'curry wrapper'. Though since ava is already quite mature I don't see this happen often.\nBinding every assertion would be a solution but I'd rather not since it still adds 'noise' as much as passing the argument with an arrow function.. Having a 3rd party project would generate more work (keeping up with versions, maintaining a seperate project, etc.). As I said before this won't add much maintenance time. I understand that this adds complexity like any new feature, and like any new feature the authors can make the call if it is worth adding. It would be very nice to write tests in a functional style. I don't see how this affects the stability of ava.. Hi @novemberborn thanks for weighing in on this. Going a bit off-topic here and I know it's also personal taste: I prefer promises over async / await in this case, it improves the code readability by not having to declare a variable that pops up 'later', a small example doesn't show a big difference but illustrates the point:\njs\n// \ud83d\udc4d \ntest('something', t =>\n  Promise.resolve('hi')\n    .then(result => t.is(result, 'hi')\n);\nvs\njs\n// \ud83e\udd14 \ntest('something', async t =>\n  const result = Promise.resolve('hi');\n  t.is(await result, 'hi');\n);\nRegarding the assertion message I mentioned that earlier, but to clarify it will become the 2nd argument:\njs\nconst is = (expected, [message]) => (value) => {};\nNot sure what you mean with using array functions though? Something like this works:\njs\n['hi', 'hi'].forEach(t.is('hi'));. Ah okay, bummer but understandable. :). ",
    "cjthompson": "I just installed ava to a blank repo and it installed 498 packages totaling 34M.  I think babel should be removed as a dependency but supported as an option if the project is already using it.. ",
    "yamsellem": "Our biggest issue with Babel is that it mess with the debugger \u2014 visual code debugging hardly work with it.\nI don't understand the use nor the interest of Babel when writing tests for nodejs. Since 4.x, nodejs widely supports JavaScript. So bringing source maps, transpiling and all the fuss server side is more complexity for a little gain (or not at all).\nThe tooling around ava is still very poor (no atom / visual code integration, no debugging), and this have way more value than ESnext features, IMO. \n@sindresorhus I may understand the need for you guys, as ava developers (for async/await) but, why forcing the tests to be run with Babel? May you consider removing Babel for node >= 8.0 \u2014 now that async/await is part of it?. Does anyone here as ever succeed to debug an ava test in Atom?\nI've been using a debugger in Atom for a while, but debugging tests has been a headache since.\nThanks.. Sorry about not been cristal clear. \nIt's just the same class. \njavascript\nmodule.exports = class TeaPot {\n    get value() { return 12; }\n}\nTo be complete about it: if every class are required, it's ok; if every class are defined in test file, it's ok; the issue happens when inheritance is part of a require and part of defined in test file.\n. @sindresorhus I'm not using a transpiler, just plain old nodejs (6.3). Does this mean Babel is used by default by Ava? Can I disable it?\n. @sindresorhus nodejs 6.3. If I can disable Babel, this will work I guess ;-)\n. @sindresorhus you're my hero :champagne: take care (ps. ava is a wonder)\n. @alathon Being small does not necessarily means being ascetic.\njavascript\nt.is(array.length, length); // is better with t.truthy(array) before\nt.true(array.every(x => [item, item].includes(x))); // seems cumbersome\nt.true(string.includes(substring)); // could be simpler\nSome basic assertion on strings and arrays seems fair to me \u2014 and don't break the 'no magic' of the thing, IMO.\n. ",
    "bettiolo": "Is this resolved by https://github.com/avajs/ava/issues/791 ?. ",
    "7373Lacym": "I would like to give this a shot.\n. I couldn't find a place where we would test the changes I made.  If there is such a place let me know and I can add a few more commits \n. Updated!  Thanks for the feedback!\n. Sorry Guys,  internship started so I cannot get to this at the moment.  If it's still around in a few weeks I will pick it back up.\n. \ud83d\udc4d . ",
    "ahmednuaman": "Isn't it just about having this in the package.json:\n\"ava\": {\n  \"require\": [\n    \"babel-register\",\n    \"babel-polyfill\"\n  ]\n},\n. ",
    "florianb": "@novemberborn: following Sindre's call to the weapons - i'd like to take this issue if it is still relevant and nobody else is working on it!. @novemberborn - it seems like this issue is invalid, since Array.include() is being part of the standard from ES2016+. Ava seems to support ES2017 out of the box, there is no ES2015-sepecific section in the docs anymore.\nHowever, i might be wrong or i also could still add an ES2015-section to the docs. I'd appreciate any hint. \ud83d\ude04 . Aye - i then will add a proposal for the babelrc. Thanks for clarification.. @novemberborn: Change drafted, i would appreciate any feedback and since this is my first contribution i guess there is room for improvement. Thank you very much in advance.. Thanks for your understanding. \ud83d\ude04\nSo what we would need to say is that the user must ensure he provides the same \"polyfilled\" environment for AVA as he does to the execution environment of his program (in the case he uses more features than defined in AVA's stage-4)?\nIn my opinion this is a more general topic and would better be pointed out at the beginning of the whole file, probably in addition to the default's transpiler behavior section?. Thanks for your understanding. \ud83d\ude04\nSo what we would need to say is that the user must ensure he provides the same \"polyfilled\" environment for AVA as he does to the execution environment of his program (in the case he uses more features than defined in AVA's stage-4)?\nIn my opinion this is a more general topic and would better be pointed out at the beginning of the whole file, probably in addition to the default's transpiler behavior section?. Okay, i guess i'd like to try to clarify this a bit more on the default's transpiler behavior section. If you don't mind, of course. \ud83d\ude04 . Okay, i guess i'd like to try to clarify this a bit more on the default's transpiler behavior section. If you don't mind, of course. \ud83d\ude04 . Okay @novemberborn, i hope i could make the description of the default transpilation behavior a little bit more clear. I tried to become a little bit more explicit, even though i am not sure i got the  meaning of\n\nThis is a great option for small modules where you do not desire a build step to transpile your source before deploying to npm\n\nI think that former sentence implies that AVA allows you using ES-features even in projects you otherwise don't do any transpilation.. Your are welcome @novemberborn - thanks for your help!. Thank you @novemberborn - that's a absolute valid point. I will change that.\n- [x] Remove ES2015-reference. Thank you very much @novemberborn - i somehow assumed it could be wanted to write not too much. \ud83e\udd14 I guess that assumption was just a bit stupid. \ud83d\ude04\n\n\n[x] Elaborate the description. Absolutely, thanks!\n\n\n[x] Remove the polyfill-snippet. @novemberborn I rewrote the description. I think it reads better now (even though my mother-tongue is German, so don't expect too much) but i have still a note on it:\n\n\nThe documentation should probably elaborate on the distinction between the execution environment of tests and the execution environment of the tested code. The current wording may leave that as a question to the beginner.\n. Sorry for that..\n\n\n[x] Fix typo. - [x] Remove itself. - [x] make require-option more explicit. I am somehow torn between using JS over ECMAScript for these reasons:\n\n\nJavaScript sounds just slight and handy\n\nThis paragraph is about the differences between good old vanilla JS and the new (not backward compatible) language features introduced by ES.\n\nI think this topic is read by people who already know the term \"ECMAScript\" and i think in this case it supports understanding (and avoids confusion for less experienced programmers [\"Isn't that feature an ES-feature?\"]).\nBut i am really not insisting on that - so if you'd prefer using JS, i am fine with that.\n. Thanks! \ud83d\ude04 \n\n[x] fix style. Okay - i just thought about the use of \"ES2017\" to explicitly spot on ES-features.\n\nBut as i said, i'm not insisting on it.\n\n[x] Replace ES by JS. You're right, it reads better. However, i rewrote \"...where you don't otherwise use...\" to be a little slighter, i hope you don't mind.. \n",
    "supercrabtree": "Thanks for all your hard work on this, what a pleasant surprise to see in the console this morning. Looks great \ud83d\udc4d \n. ",
    "ejmurra": "Thanks.\n. ",
    "alfredwesterveld": "I read this is fixed in master, but can you also publish that version to npm because 0.14.0 is not yet fixed?\nI kept the code as simple as possible with latest npm module installed:\nenvironment:\n```\n$ lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 14.04.3 LTS\nRelease:    14.04\nCodename:   trusty\n$ node -v\nv4.2.6\n$ ava --version \n0.14.0\n```\nindex.es6:\n```\n\"use strict\";\nmodule.exports = () => {\n    return Promise.resolve(\"bar\")\n}\n```\ntest.js:\n```\nimport test from 'ava'\nconst p = require('./index.es6')\ntest('foo', t => {\n    t.pass();\n});\ntest('bar', async t => {\n    const bar = p()\n    t.is(await bar, 'bar');\n});\n```\n. Btw thanks for the library. I agree with @capaj tha people can think they are leaking memory while ava is doing this. I also understand @jamestalmage standpoint to first dog food, but I have two questions:\n- Isn't it better to mention in README.md about this leak?\n- Should this issue be closed, because when installing via npm it is not yet fixed?\nI understand that you guys are also probably busy. Maybe when I have some time available and I can I will sent a PR\nThanks,\nAlfred Westerveld\n. @sindresorhus thanks for quick reply. I guess you are right about those two points.\n. ",
    "capaj": "I did have the error today as well on 0.14.0. \n. @jamestalmage I know the cause of this error. It is not a problem I worry about. This is only inconvenient for someone new to node.js who could think this error has to do with his own code.\n. funny, I just came here to start the same issue. I am also experiencing performance problems when I have too many test files.\n@yomed you can fix your problem with -c CLI flag.\nBTW I think ava should by default limit the number of concurrent tests to the number of available procesor cores on the current machine. That way optimal performance is achieved on my experience. \n. @novemberborn For me as a user of ava, I'd expect the same level of robustness as I get from jest snapshots. \nI think work done on https://www.npmjs.com/package/pretty-format is quite awesome and I don't think ava comunity needs to reinvent this piece of snapshoting feature. Maybe you could used that instead of doing JSON.parse(JSON.stringify(actual)) ? I know it's not taking ava too far from the current state, but do we really need to? Jest snapshots are easily the most awesome feature of jest.\nAlso when you've listed the features not supported by raw json snapshots, you failed to mention Type support: https://www.npmjs.com/package/pretty-format#type-support\n. I've been using jest and I am used to seeing the snapshots in the git changelog. For our team at @Leaplabs we use those changelogs to showcase data we expect. It's not just an assertion for the test runner-it's for humans as well. Keeping snapshots as binary unfortunately makes snapshots unaproachable for humans.\nokay, we'll stick to jest \ud83d\ude1e. ",
    "cwhenderson20": "I was going to comment on a similar issue, but I found this issue, exactly matching my problem. I'll leave the comment anyway in case it it helps anyone else looking for a solution:\n\nMy tests are all asynchronous (using the test.cb style) and it seems to be that in these tests, thrown errors cause the test to hang indefinitely.\nHere's some example code:\nThis works, as an error is returned (as expected): \n``` js\ntest.cb(\"does not allow extending message visibility past 12 hours\", (t) => {\n  const sqs = new SQS({ params: { QueueUrl, ReceiptHandle: \"fake\", VisibilityTimeout: 43201 }});\nsqs.changeMessageVisibility((err) => {\n    t.truthy(err);\n    t.is(err.code, new InvalidParameterValueError().code);\n    t.end();\n  });\n});\n```\nThis, however, causes the suite to hang:\n``` js\ntest.cb(\"extends a message's visibility and returns an empty object\", (t) => {\n  const sqs = new SQS({ params: { QueueUrl, ReceiptHandle: \"fake\", VisibilityTimeout: 43201 } });\nsqs.changeMessageVisibility((err, data) => {\n    t.falsy(err);\n    t.is(Object.keys(data).length, 0);\n    t.end();\n  });\n});\n```\nI believe the reason for this is that, in the second example, data is undefined, which causes the Object.keys(data) method to throw a TypeError. Clearly this is a contrived example as I know that an error will be returned and am asserting that it should not exist, but it left me confused for a while as 1) AVA doesn't report that an error was thrown (even in verbose mode), and 2) the only way to get the suite to run again (if in watch mode) is to kill and restart the process.\n. ",
    "nonnontrivial": "Hi - I would like to work on this if no one else is already doing so.\n. ",
    "marvinhagemeister": "Thanks, don't know how I could've missed that!\n. ",
    "ulfryk": "So how to tests files that are in helpers directory ? Is there possibility to suppress this ignore ?\n. ",
    "grvcoelho": "As @ulfryk, pointed: is there a way to supress this ignore? I need to test files inside a helpers folder.. I think I might have discovered the problem: \nWe have the following code in this line:  https://github.com/avajs/ava/blob/4b6323e85e509a57f5183a5c7c1385a43d859a30/lib/ava-files.js#L90\njavascript\nconst defaultHelperPatterns = () => [\n    '**/__tests__/helpers/**/*.js',\n    '**/__tests__/**/_*.js',\n    '**/test/helpers/**/*.js',\n    '**/test/**/_*.js'\n];\nThis globs do not cover a case like test/unit/helpers/index.js. If this is the problem it could be easily fixed by adding a **/test/**/helpers/**/*.js glob, like this: \njavascript\nconst defaultHelperPatterns = () => [\n    '**/__tests__/**/helpers/**/*.js',\n    '**/__tests__/**/_*.js',\n    '**/test/**/helpers/**/*.js',\n    '**/test/**/_*.js'\n];\nAm I thinking this right?. Just tested, and my previous comment works! Gonna open the PR :). > I'm not sure about this\u2026 we already have issues with helper globbing performance (#1288). Helper exclusion isn't great either (#909). Perhaps we should solve those problems first.\n@novemberborn: I've read the discussions and I'm not sure if I understood the problem correctly, but the issue with #1288 is because of number of files and not globbing itself, right?\nBecause, ATM in my case, if I rename everything inside my helpers folder to have a _ prefix, it will be considered a helper and be transpiled. So overall, it is the same amount of files, I just don't need to put the _ prefix.\nATM, _ prefixed helpers are transpiled inside subdirectories, but helpers that are inside helpers folder are not. So the behaviors are different if you decide do _-prefix your helpers or to put them inside helpers folder.\n\nWe could then add support for helper patterns, so you can match those and we transpile them.\n\nThat would be great in the future \ud83d\ude42 . > Maybe we should have this recipe recommend building everything to one file, but add a not explaining that all tests will be run in the same process.\n@danny-andrews, @novemberborn: there is a huge downside on this approach: we lose the name of the tests outputted when in verbose mode. Because ava use the name and path of the file to output this on verbose mode.. ",
    "Spy-Seth": "My command to run the test is simple: NODE_ENV='test' ava --require babel-register --require babel-polyfill --require ./tests/helpers/world.js tests/units/**/*\nThe difficulties is that I execute it in a docker container: docker exec -it test-runner npm test\n. Adding -e CI=true works. \nThanks for your help !\n. ",
    "omnidan": "@jamestalmage I just checked again, this is the full code:\njs\ntest.cb('/register with valid POST data should create and login user', t => {\n  supertest\n    .post('/register')\n    .field('email', 'test@user2.com')\n    .field('password', 'test123') // not a good password ;)\n    .expect(200)\n    .end((err, res) => {\n      t.falsy(res.body.error) // shouldn't this fail already???\n      console.log(res.body) // { error: 'Registration failed' }\n      console.log(res.body.user) // undefined\n      t.truthy(res.body.user, 'this should be the error message')\n      t.is(res.body.user.email, 'test@user.com')\n      t.end()\n    })\n})\nBut this results in:\n1. Uncaught Exception\n  TypeError: Cannot read property 'email' of undefined\nI also tried running your code (also in the end handler/callback function):\n1. Uncaught Exception\n  TypeError: Cannot read property 'uhoh' of undefined\nSeems to be an issue related to test.cb or supertest, then.\n. I also tried with 0.13 (and ok/notOk) and it causes the same issue.\nAlthough it does seem to fail at the first assertion when I make sure an exception isn't thrown later, e.g. by doing t.is(res.body.user && res.body.user.email, 'test@user.com'):\nt.falsy(res.body.error)\n                   |\n                   \"Registration failed: \"\n. @novemberborn thanks for the workaround, I only have ES6 features enabled (no async/await), so I tried the following:\njs\ntest('/register with valid POST data should create and login user', t => {\n  (new Promise((resolve, reject) => {\n    supertest\n      .post('/register')\n      .field('email', 'test@user2.com')\n      .field('password', 'test123') // not a good password ;)\n      .expect(200)\n      .end((err, res) => err ? reject(err) : resolve(res))\n  })).then(res => {\n    t.falsy(res.body.error) // shouldn't this fail already???\n    console.log(res.body) // { error: 'Registration failed' }\n    console.log(res.body.user) // undefined\n    t.truthy(res.body.user, 'this should be the error message')\n    t.is(res.body.user.email, 'test@user.com')\n  })\n})\nIt results in:\n2. Unhandled Rejection\n  TypeError: Cannot read property 'email' of undefined\nI'll just wait until test.cb gets fixed as I already got the test to pass anyway :grin: Thanks again!\n. ",
    "bencooling": "\n@bencooling Wont ava still transpile using babel though?\n\n@LinusU I believe Ava runs your tests through Babel not your source code.\n. ",
    "zhaoyao91": "node --harmony node_modules/.bin/ava does not work on windows 10. ",
    "seangenabe": "Try node --harmony node_modules/ava/cli.js -- the contents of the .bin directory are shell scripts and shouldn't be runnable by node.. ",
    "sheerun": "Another approach is to make 1-level grouping obligatory:\n``` js\nava('optional group name', test => {\n  test.beforeEach(t => {\n    t.context = 'foo'\n  });\ntest('is awesome', t => {\n    t.is(t.context, 'awesome');\n  });\ntest('is not fabulous', t => {\n    t.not(t.context, 'fabulous');\n  });\n});\n```\n. Another approach is to make 1-level grouping obligatory:\n``` js\nava('optional group name', test => {\n  test.beforeEach(t => {\n    t.context = 'foo'\n  });\ntest('is awesome', t => {\n    t.is(t.context, 'awesome');\n  });\ntest('is not fabulous', t => {\n    t.not(t.context, 'fabulous');\n  });\n});\n``\n. Because you invoke them on globaltest` object instead of group of test they should modify.\nIt's the same issue you tried to get rid of with ava:\n\nMocha requires you to use implicit globals like describe and it with the default interface\n\nThe same way you need to use t.is for test, you should use test.beforeEach where test is a group.\n. Because you invoke them on global test object instead of group of test they should modify.\nIt's the same issue you tried to get rid of with ava:\n\nMocha requires you to use implicit globals like describe and it with the default interface\n\nThe same way you need to use t.is for test, you should use test.beforeEach where test is a group.\n. I get it, but my point still holds. Plus I like to write all my tests in one file and only after that extract them to another one. With implicit hooks it's impossible to apply hooks only to some tests in one file.\n. ",
    "danilosampaio": "@jamestalmage @sindresorhus  i'm a beginner at AVA, so if nobody has started yet i could do it.\n. @jamestalmage thanks for your feedback.\nI'll take a look at Mocha and Unexpected.js.\n. Mocha holds a color map:\nhttps://github.com/mochajs/mocha/blob/9ae6a85b6ba50ea415affc88456d469d6915a157/lib/reporters/base.js#L52-L72\nthen inserts the ansi codes into the given string:\nhttps://github.com/mochajs/mocha/blob/9ae6a85b6ba50ea415affc88456d469d6915a157/lib/reporters/base.js#L52-L72\nUnexpectedjs uses hexadecimal color palette that seems not to fit into AVA.\nSo i made the suggested changes using chalk. what do you think?\n. @jamestalmage sure!\n. Added some tests.\n. @novemberborn thanks for your feedback. I'll take a look at it.\n. Can i go with this approach?\n- Move logger.js, colors.js, beautify-stack, and enhanced-assertion.js to lib/reporters/helper/\n- Refator references to those files\n. Ok, i'll do it tomorow.\n. @jamestalmage \nFinished the rebase. If you wish, i can squash the commits.\n. @novemberborn thanks for your feedback.\nYou're right, this is not the expected result of this PR. After this PR, the EnhancedAssertion,error output was broken. I'm not sure if it's the best solution, but adding equals to lib/assert.js, we get the output fixed. Using your example, the output looks like:\nEDIT: clarifying: i'm using your example, but calling t.equal instead of assert.equal\n```\n6 failed\n\n\nassert.equal with message\nAssertionError: foo\n+ expected - actual\n-3\n+4\n      +4\n    Test.fn (test.js:4:7)\n    _combinedTickCallback (internal/process/next_tick.js:67:7)\n    process._tickCallback (internal/process/next_tick.js:98:9)\n\n\nassert.equal without message\nAssertionError: 3 == 4\n+ expected - actual\n-3\n+4\n      +4\n    Test.fn (test.js:8:7)\n    _combinedTickCallback (internal/process/next_tick.js:67:7)\n    process._tickCallback (internal/process/next_tick.js:98:9)\n\n\nt.deepEqual with message\nAssertionError: foo\n+ expected - actual\n-3\n+4\n      +4\n    Test.fn (test.js:12:7)\n    _combinedTickCallback (internal/process/next_tick.js:67:7)\n    process._tickCallback (internal/process/next_tick.js:98:9)\n\n\nt.deepEqual without message\nAssertionError: foo\n+ expected - actual\n-3\n+4\n      +4\n    Test.fn (test.js:16:7)\n    _combinedTickCallback (internal/process/next_tick.js:67:7)\n    process._tickCallback (internal/process/next_tick.js:98:9)\n\n\nt.true with message\n   foo \n  t.true(3 === 4, 'foo')\nt.true(3 === 4, 'foo')\n    Test.fn (test.js:20:5)\n    _combinedTickCallback (internal/process/next_tick.js:67:7)\n    process._tickCallback (internal/process/next_tick.js:98:9)\n\n\nt.true without message\n   foo \n  t.true(3 === 4, 'foo')\nt.true(3 === 4, 'foo')\n    Test.fn (test.js:24:5)\n    _combinedTickCallback (internal/process/next_tick.js:67:7)\n    process._tickCallback (internal/process/next_tick.js:98:9)\n\n\n```\n. @sindresorhus feel free to discard this PR. Initially, i thought it was simple, but i'm stuck in it. Maybe, with a little more experience, i can contribute with AVA. Thanks for your help guys.\n. Done. :+1: \n. Done. :+1:\nI assumed not necessary create a new test/run-status.js, since the added tests already check RunStatus.formatAssertionError() output. Right? :smiley: \n. since other improvements will be added in a second moment (like a diff view for larger objects / strings), maybe better option is a individual reporter.\n. Fixed.\n. Fixed.\n. ",
    "willin": "ERROR\njs\ntest('View 200', async(t) => {\n  const response = await server.inject({\n    method: 'GET',\n    url: '/login'\n  });\n  console.log(response.statusCode); // 200\n  t.deepEqual(response.statusCode, 789); // not equal, fail\n});\n```\nava -v\n\u2714 index \u203a Redirect 302\n  \u2714 doc \u203a Redirect 302\n200\n(node) Server.connections property is deprecated. Use Server.getConnections method instead.\n```\nprocess not exit\nSUCCESS\njs\ntest('View 200', async(t) => {\n  const response = await server.inject({\n    method: 'GET',\n    url: '/login'\n  });\n  console.log(response.statusCode); // 200\n  t.deepEqual(response.statusCode, 200); // equal, success\n});\n```\nava -v\n\u2714 index \u203a Redirect 302\n  \u2714 doc \u203a Redirect 302\n200\n  \u2714 login \u203a View 200\n3 tests passed\n\n|\n```\n\nUsing assert success\n``` js\nimport assert from 'assert';\ntest('View 200', async() => {\n  const response = await server.inject({\n    method: 'GET',\n    url: '/login'\n  });\n  assert.equal(response.statusCode, 789);\n});\n```\n```\nava -v\n\u2714 index \u203a Redirect 302\n  \u2714 doc \u203a Redirect 302\n  \u2714 login \u203a Success 200\n  \u2716 login \u203a View 200 200 == 789\n1 test failed\n\nlogin \u203a View 200\n  AssertionError: 200 == 789\n    _callee$ (login.js:9:10)\n    tryCatch (/Users/willin/Documents/wulian/code/hfactoryv1/node_modules/.npminstall/babel-runtime/6.6.1/babel-runtime/regenerator/runtime.\njs:88:40)....\n```\n. ### ERROR\n\njs\ntest('View 200', async(t) => {\n  const response = await server.inject({\n    method: 'GET',\n    url: '/login'\n  });\n  console.log(response.statusCode); // 200\n  t.deepEqual(response.statusCode, 789); // not equal, fail\n});\n```\nava -v\n\u2714 index \u203a Redirect 302\n  \u2714 doc \u203a Redirect 302\n200\n(node) Server.connections property is deprecated. Use Server.getConnections method instead.\n```\nprocess not exit\nSUCCESS\njs\ntest('View 200', async(t) => {\n  const response = await server.inject({\n    method: 'GET',\n    url: '/login'\n  });\n  console.log(response.statusCode); // 200\n  t.deepEqual(response.statusCode, 200); // equal, success\n});\n```\nava -v\n\u2714 index \u203a Redirect 302\n  \u2714 doc \u203a Redirect 302\n200\n  \u2714 login \u203a View 200\n3 tests passed\n\n|\n```\n\nUsing assert success\n``` js\nimport assert from 'assert';\ntest('View 200', async() => {\n  const response = await server.inject({\n    method: 'GET',\n    url: '/login'\n  });\n  assert.equal(response.statusCode, 789);\n});\n```\n```\nava -v\n\u2714 index \u203a Redirect 302\n  \u2714 doc \u203a Redirect 302\n  \u2714 login \u203a Success 200\n  \u2716 login \u203a View 200 200 == 789\n1 test failed\n\nlogin \u203a View 200\n  AssertionError: 200 == 789\n    _callee$ (login.js:9:10)\n    tryCatch (/Users/willin/Documents/wulian/code/hfactoryv1/node_modules/.npminstall/babel-runtime/6.6.1/babel-runtime/regenerator/runtime.\njs:88:40)....\n. js\ntest('View 200', (t) => {\n  server.inject({\n    method: 'GET',\n    url: '/login'\n  }, (response) => {\n    console.log(response.statusCode); // 200\n    t.deepEqual(response.statusCode, 789); // not equal\n  });\n});\n```\n\n```\nava -v\n\u2714 login \u203a View 200\n  \u2714 index \u203a Redirect 302\n  \u2714 login \u203a Success 200\n  \u2714 doc \u203a Redirect 302\n200\n\n|\n```\n\nshould fail\nand tried:\njs\ntest.cb('View 200', (t) => {\n  server.inject({\n    method: 'GET',\n    url: '/login'\n  }, (response) => {\n    console.log(response.statusCode); // 200\n    t.deepEqual(response.statusCode, 789); // not equal\n    t.end(); // you need to call t.end() when using `test.cb`\n  });\n});\n```\nava -v\n\u2714 index \u203a Redirect 302\n  \u2714 doc \u203a Redirect 302\n  \u2714 login \u203a View 200\n200\n(node) Server.connections property is deprecated. Use Server.getConnections method instead.\n. js\ntest('View 200', (t) => {\n  server.inject({\n    method: 'GET',\n    url: '/login'\n  }, (response) => {\n    console.log(response.statusCode); // 200\n    t.deepEqual(response.statusCode, 789); // not equal\n  });\n});\n```\n```\nava -v\n\u2714 login \u203a View 200\n  \u2714 index \u203a Redirect 302\n  \u2714 login \u203a Success 200\n  \u2714 doc \u203a Redirect 302\n200\n\n|\n```\n\nshould fail\nand tried:\njs\ntest.cb('View 200', (t) => {\n  server.inject({\n    method: 'GET',\n    url: '/login'\n  }, (response) => {\n    console.log(response.statusCode); // 200\n    t.deepEqual(response.statusCode, 789); // not equal\n    t.end(); // you need to call t.end() when using `test.cb`\n  });\n});\n```\nava -v\n\u2714 index \u203a Redirect 302\n  \u2714 doc \u203a Redirect 302\n  \u2714 login \u203a View 200\n200\n(node) Server.connections property is deprecated. Use Server.getConnections method instead.\n```\n. server.inject doc: http://hapijs.com/api#serverinjectoptions-callback\nsample server: http://hapijs.com/tutorials\n. ",
    "midnightcodr": "Although this issue has been closed. I found something interesting. The following code outputs the node warning (Server.connections property is deprecated. Use Server.getConnections method instead.) and process hangs\njavascript\ntest.cb('hello', (t)=> {\n    server.inject('/hello', (res) => {\n        t.is(res.statusCode, 300);\n        t.end();\n    });\n});\nBut the following code results in expected result (test fails and process doesn't hang):\njavascript\ntest.cb('hello', (t)=> {\n    server.inject('/hello', (res) => {\n        const code=res.statusCode\n        t.is(code, 300);\n        t.end();\n    });\n});\n. ",
    "atomless": "@sindresorhus @sotojuan - Another issue with the browser testing docs is that if you follow the jsdom usage - and add the browser setup to the required helpers in package.json - the globally declared document and window still do not appear to be available to tests. Just a simple example showing loading a url into jsdom and then getting an attribute of the body element would provide a much better start.\n. I ditched jsdom in the end in favour of zombie. @sindresorhus I'd be happy to make this into a pull request if you wished?\n``` Javascript\nimport test from 'ava';\nimport Browser from 'zombie';\nconst browser = new Browser();\ntest.serial.cb('PAGE REQUEST', t => {\nbrowser.visit('http://example.com/path', e => {\nt.end();\n\n});\n});\ntest.serial('PAGE LOADED', t => {\n  t.plan(2);\nlet status = browser.statusCode;\nt.truthy(browser.success, 'Page Load Failed');\n  t.truthy(status === 200, 'Page Returned Status: ' + status);\n});\n```\n. @sindresorhus here's a very basic test using jsdom...\n``` Javascript\nimport test from 'ava';\nconst jsdom = require('jsdom');\nvar window;\ntest.serial.cb('PAGE REQUEST', t => {\njsdom.env({\n    url: 'http://example.com/path',\n    done: (error, win) => {\n      window = win;\n      t.end();\n    }\n  });\n});\ntest.serial('PAGE LOADED', t => {\n  t.plan(1);\nlet attr = window.document.body.getAttribute('data-some-known-attr');\nt.truthy(attr.length > 0, 'The Body attribute \"data-some-known-attr\" is missing or invalid: ' + attr);\n});\n```\n. @sindresorhus I have now switched to nightmare.js for various reasons and it both fits my needs better and also seems a much better fit for ava. I'll get a simple test script for all 3 and make a pull request asap. WRT using beforeEach, I'm not so sure. I agree unit tests should be atomic but feel browser testing  is often more functional testing and for my use case I will want to perform various actions in serial and have the state maintained between tests.\n. ",
    "petetnt": "@sindresorhus sorry about that! Verbose was a bad example. Consider something not mutually exclusive, like title matching: Say I want to run all the tests that contain COUNTER_ADD in the title with --match\nMy package.json:\n\"scripts\": {\n    \"test\": \"ava | tap-spec\",\n  },\n  \"ava\": {\n    \"files\": [\n      \"src/js/__tests__/*.js\"\n    ],\n    \"source\": [\n      \"**/*.{js,jsx}\",\n      \"!dist/**/*\"\n    ],\n    \"failFast\": true,\n    \"tap\": true,\n    \"require\": [\n      \"babel-register\"\n    ],\n    \"babel\": \"inherit\"\n  },\nnpm test -- --match=\"*COUNTER_ADD*\"\nObviously runs ava | tap-spec \"--match=\"*COUNTER_ADD*\"\" which just runs all the tests as expected. If I could just have the test script as ava and the reporter in the ava options then I could pass whatever options I want to and have it report back to me, without having to do npm test -- --options-here | tap-spec` every time (which like I said before is really not much of problem anyway...).\nIt would get tricky if I had to pass options for the reporter too, so something like reporter: [\"tap-spec\", {\"exclusive-option-for-reporter\": \"true\"}] would be nice too. But then again one could just alias the spec+options command to something shorter / easier to remember so it might be too much of overhead to achieve something in ava that one can work around easily.\nTL;DR: I am lazy and I'd like to run all the things with npm test, with reporters and the ability to pass arguments to the test runner itself too.\n. @novemberborn Fair enough :+1: Closing this.\n. Might be worth it to mention that jsx-test-helpers has react-addons-test-utils as an dependency so you don't have to npm install it yourself. (Or to add it to the install clause regardless if you are going use the TestUtils)\n. ",
    "bcherny": "@novemberborn My tests are asserting equality between long multiline strings, and I'd love to use a custom JSON diff reporter during watch mode. The test output is very hard to read with the built in reporters. Is there a way to configure this?\n. ",
    "flowmemo": "I am confused by ava's deepEqual and node's deepEqual.\nBefore this PR was merged, ava's deepEqual is same as node's deepEqual. But now it is more like deepStrictEqual. Am I right.\nThanks.\n. ",
    "stefanlegg": "Looks to be aligned correctly in master, but not in the 0.14.0 release\n. ",
    "eisisig": "This is actually the first time EVER I add config for a package to the package.json file. In all our repos there is a clear conventions that if anything needs config there is a file for it. .eslintrc, .babelrc, .jscsrc, web.config, cosmic.config, webpack.config, rollup.config and the list goes on... It is really good as most of these are shared or used as templates... We have few repos with configs only. On the other hand the package.json file in each project is strictly for dependencies... We are not even publishing most of the packages...\nFor us the package.json is npm related... Even tho the tools are installed with npm (some git) they are not related or npm should not be a requirement for using them... Most used packages encourage you to use config files (eslint, babel, rollup...)\nEnforcing limited config usage sounds geared towards simple, small projects. And that is ok if that is the goal. I'd really like .avarc - Just my 5c\n. ",
    "jsonmaur": "I also came across this, and found a workaround by simply putting in delete reply.request in the resolved promise. I narrowed it down, the problems seem to be coming from reply.request.route and reply.request.response from the Hapi response. Hoping to see a fix soon :)\n. ",
    "kevinkorngut": "Yes, I am using Babel and that plugin plus the following loader:\n{\n        test: /\\.(gif|png|jpe?g|svg)$/,\n        loaders: [ 'null-loader' ]\n      }\ngot me going again. Thanks.\n. ",
    "AgtLucas": "@DrewML and @kevinkorngut \nI'm having the same problem, I've tried to implement a new webpack config for AVA (using babel-plugin-webpack-loaders) but it's not working.\nHere's what I did:\npackage.json:\n\"scripts: {\n    \"test\": \"AVACONFIG=$(pwd)/webpack.config.ava.js BABEL_DISABLE_CACHE=1 NODE_ENV=AVA ava\",\n}\n...\n\"ava\": {\n    \"files\": [\n      \"src/**/*.test.js\"\n    ],\n    \"babel\": \"inherit\",\n    \"failFast\": true,\n    \"serial\": true,\n    \"tap\": true,\n    \"verbose\": true,\n    \"require\": [\n      \"babel-register\",\n      \"./test/helpers\"\n    ]\n  }\n.babelrc\n{\n  \"presets\": [\"es2015\", \"react\", \"stage-0\"],\n  \"plugins\": [\n    \"transform-object-rest-spread\"\n  ],\n  \"env\": {\n    \"AVA\": {\n      \"plugins\": [\n        [\n          \"babel-plugin-webpack-loaders\",\n          {\n            \"config\": \"${AVACONFIG}\",\n            \"verbose\": false\n          }\n        ]\n      ]\n    }\n  }\n}\nwebpack.config.ava.js\n``` js\n...\nmodule.exports = {\n  devtool: undefined,\n  entry: [\n    path.join(__dirname, 'src/js/app.js')\n  ],\noutput: {\n    path: path.join(__dirname, '/dist/'),\n    filename: '[name].js',\n    libraryTarget: 'commonjs2'\n  },\nmodule: {\n    loaders: [\n      {\n        test: /.(gif|png|jpe?g|svg)$/,\n        loaders: ['null-loader']\n      }\n    ]\n  }\n}\n```\nWhat am I missing?\nThanks!\n. ",
    "bensleveritt": "I've just stumbled into this snag too. Whilst I'm happy to add a plugin, I wonder why we can't exclude the file from AVA's config? Does this only apply to imported files?\n. ",
    "seansean11": "I was able to bypass this issue by copying the \"noop/require.extensions\" section of the react-starter-kit and adding it to my required Ava setup file.  It stops the files from being imported, which was perfectly fine for my testing setup https://github.com/kriasoft/react-starter-kit/blob/master/test/setup.js.\n. ",
    "codeithuman": "How are you getting around this issue today? Are you all still using the answer posted 2 years ago, https://github.com/avajs/ava/issues/802#issuecomment-216262437?\n. ",
    "jantimon": "So better go with jasmine and mocha or tap?\nAva says it's futuristic but debugging with console.log is ancient ;)\n. How are those two issues related?\nIs there anything I can do to support this?\nWhat is the state of #812?\n. Okay thanks very much - I am on the road right now - so I can't promise to much but if I can find some time I'll try to figure out why your two profile tests fail on windows.\n. Wow cool that works perfectly - just tested it with node 4 and iron-node.\nI only had to reload (CMD+R) after iron-node started up\n\n. iron-node node_modules/ava/profile.js test.js\nyou might need to press cmd+r or ctrl+r\nor just switch to https://facebook.github.io/jest/. ",
    "naifen": "thx @fenos for bring this out, I'm facing very similar problem when try to populate/cleanup data with mongoose before/after test. Didn't see an answer on stackoverflow yet, really looking forward to see @sindresorhus or someone could answer it. Thanks in advance \ud83d\ude03 \n. ",
    "jamesdixon": "@fenos did you ever figure out a good solution for this issue? Running into the same thing.\n@sindresorhus any advice?\nThanks, all!\n. @delvedor I ran into this issue as well. I was able to get around it by setting my coverage script to call nyc npm test, which worked perfectly.\n. ",
    "jiananshi": "Since AVA is running each test cases concurrently, any test suites based on same db would may or may not cause unpredictable problems. I'm current running my AVA tests with --serial as reference in document.. ",
    "CImrie": "Hi guys,\nNot much has been said on this recently but I just published a package to make this much easier.\nIt uses the great mongodb-prebuilt library to run in-memory MongoDB servers and you'll get a new test database for each test case.\nCheck it out if you need a solution:\n\nReadme\nNPM\nBlog Post detailing how I set it up with FeathersJS and Mongoose\n\nOne note: in each test case you should probably include a test.after.always function to tear down the DB server and clean up temp files. Your OS should eventually clear them up anyway but it's worth noting.\njavascript\ntest.after.always('destroy db', t => {\n    MongoDBServer.tearDown();\n}). @novemberborn Sure. I'll try to have a look over the common formats for your recipes this week and put something together :). npm is now referenced to the source code and readme :). Does this seem ok now? Indentation on Github seems rather excessive to me, but my editor definitely reports a sensible number of tabs.. \ud83d\ude48  apologies!. I agree with all of your comments. This was written in haste during my lunch hour so thanks for looking over it so thoroughly!\nA big thanks for dropping the const {db} = t.context knowledge-bomb on me. Didn't realise I could destructure an object inline like that. It's not so different to the import syntax but it just never occurred to me! \ud83e\udd47 . Ha, no worries. I'm not sure what has been going on with my grammar in this pull request! \ud83d\ude05 \nIn terms of indentation, what do you normally use for AVA? 4 spaces? \nI tend to stick to tab, so I'll need to change it for the next submission depending on your answer.. ",
    "vjancik": "It seems the extra 20 seconds happen when I change the contents of the test files. So I suspect the culprit is Babel. Is it possible it's transpiling something it shouldn't? What .babelrc do you normally use with Ava?\nNote: my non-test source code isn't using Babel\n. Well, I have a single test file with a single async test function. Atop of that, I'm importing 2 helper function code files written in ES5.\nIn package.json I have just this:\n\"ava\": {\n    \"serial\": true,\n    \"files\": [\n      \"test/**/*.test.js\"\n    ]\n  },\n. Well, it seems rm -rf node_modules; npm install helped ... Not sure what was the problem but it's gone. Thank you for your help!\n. ",
    "Roshanjossey": "@jamestalmage: I know it's been some time since this issue was opened but I'd like to fix this.\nI looked in to #744. I might need some help in understanding where tests should go.\nPlease gimme some information on how to take this forward. @novemberborn, thanks for your reply. I'll start working on this and reach back to you if I need any more information from you.. ",
    "kugtong33": "@novemberborn , hi I'm a beginner and would like to help on this, is this still open?. @novemberborn \nI looked at the instances where the caching precompiler is used and see if there are instances that the options.path is a relative path, for api.js, profile.js, fork.js, hooks.js , all would assign options.path either a unique temp directory name or the ava cache absolute path (usually where node_modules/.cache/ava is located).\nAm I missing something? , because if there is an instance that a relative path would be added on options.path, I would then check first if the map path is absolute before adding it, else retain the relative map path.. @novemberborn \nThese are the steps of what I understood about the problem\n\ncreate the context object before passing it to the before hook\nclone context using Object.create then pass it to the beforeEach hook if there is one, else pass it to the test so that each test will have their own clone of context\n. @novemberborn , i'll give this a try :+1: . @vadimdemedes , will this be covered on your WIP task?\n\n@novemberborn @vadimdemedes , I will still write my thoughts here, on what I understood and if I have the correct context of the problem, to have feedback from you guys as I just started digging up the codes :). @novemberborn @vadimdemedes \nSo I tried all reporters, tap, mini, and verbose\n```javascript\nimport test from 'ava';\ntest('Log an object', (t) => {\n  t.log('sample string log');\n  t.log({ a: 1 });\n  t.is(true, true);\n});\n/ mini only shows logs when it fails /\n  1 failed\nLog an object\n    \u2139 sample string log\n    \u2139 [object Object]\n/github/ava-labs/test/log.test.js:6\n5:   t.log({ a: 1 });\n   6:   t.fail();     \n   7: });               \nTest failed via t.fail()\n/ verbose reporter /\n  \u2714 Log an object\n    \u2139 sample string log\n    \u2139 [object Object]\n1 test passed\n/ tap throws an error as described by the issue /\n```\nLogs are stored in an array as I have seen in here and is directly fed into supertap here which should be an array of strings as its documentation said.\nIf we want it to behave the same as console.log we can format the arguments and append all of them on a single string before pushing it to the logs array here\nDoes it sound right?. @novemberborn , I also found this\n```javascript\nimport test from 'ava';\ntest('Log an object', (t) => {\n  t.log();\n  t.is(true, true);\n});\n/ verbose /\n  \u2714 Log an object\n    \u2139 null\n1 test passed\n/ mini when test fails /\n  Log an object\n    \u2139 null\n/github/ava-labs/test/log.test.js:5\n4:   t.log(); \n   5:   t.fail();\n   6: });\n/ tap /\nTAP version 13\n/github/ava-labs/node_modules/indent-string/index.js:9\n        throw new TypeError(Expected \\input` to be a `string`, got `${typeof str}``);\n        ^\nTypeError: Expected input to be a string, got object\n    at module.exports (/github/ava-labs/node_modules/indent-string/index.js:9:9)\n    at test.logs.forEach.log (/github/ava-labs/node_modules/ava/lib/reporters/tap.js:64:23)\n    at Array.forEach ()\n    at appendLogs (/github/ava-labs/node_modules/ava/lib/reporters/tap.js:63:15)\n    at TapReporter.test (/github/ava-labs/node_modules/ava/lib/reporters/tap.js:83:4)\n    at Logger.test (/github/ava-labs/node_modules/ava/lib/logger.js:27:28)\n    at emitTwo (events.js:126:13)\n    at RunStatus.emit (events.js:214:7)\n    at RunStatus.handleTest (/github/ava-labs/node_modules/ava/lib/run-status.js:105:8)\n    at emitOne (events.js:121:20)\n    at ChildProcess.emit (events.js:211:7)\n    at ChildProcess.ps.on.event (/github/ava-labs/node_modules/ava/lib/fork.js:79:7)\n    at emitTwo (events.js:126:13)\n    at ChildProcess.emit (events.js:214:7)\n    at emit (internal/child_process.js:772:12)\n    at _combinedTickCallback (internal/process/next_tick.js:141:11)\n``. lol, another good for beginner issue, can I get this @novemberborn ?. @novemberborn , code-excerpt dependency updated to version2.1.1. I'll wait for further feedback, before I'll write tests for relative cache directory paths :). @novemberborn , not sure about the failed appveyor node v4 build. @novemberborn , yes I deleted it and then hitnpm installagain, hmm should have just updated the version and let npm do the update itself to the lock file. @novemberborn , if it was that easy then it should have been agood-for-beginnerissue :D, thanks for the detailed review, will work on it. @novemberborn , sorry for the late update, I tried the suggestion above that is to boundcontext.contextvalue, I did it in a simple way, create aContextclass with only one methodclonethen pass the *bound context* to thebeforeEach-test-afterEachsequence, also updated the shared context test to allowt.contextonafter` hook.\nI used lodash.clonedeep since the test uses an array as an example and I think if we only do a shallow clone, objects/arrays would conflict on each test sequence.\nPS: if this gets accepted, can I be the one to update the docs as well? :D. :+1:  for the clear intent, there was a lot going on when I checked on master and didn't want this discussion to get messed up by the new commits so I left it that way, and thanks for the guidance :). @novemberborn , :+1: , will work with the documentation on a separate merge request :). @novemberborn , a lot has happened in the past days when I was gone, I think it's my time to dive into documentation, do you need help with this one?. @novemberborn \nHow does ifError differs from falsy if both checks the value then fails it when value is truthy?. \n\nI hope this gif is helpful, I find it fast myself, lol\nUsing ava master branch as dependency version and vscode 1.20.1\njavascript\n...\n\"devDependencies\": {\n    \"ava\": \"ssh://git@github.com:avajs/ava.git#master\"\n}\n.... :+1: , I was heading on creating a PR for this, should I reference it still? :D , or nah\nhttps://github.com/kugtong33/ava/blob/before-hook-context-document/readme.md. @novemberborn , I think the important part is to determine what values are shared/referenced and what values are independent in each test, and better examples would be fine, as I have added on my side we can give examples for referenced values and primitive values and how they can use contexts correctly.\nWe can also create a recipe for it, migrating declared variables outside of hooks into using contexts. On my side, I am heavily using supertest and I would love to try to use contexts on passing supertest responses from hooks to tests.. \n@novemberborn , apologies if I didn't got it right away (that it needs to be shown in the editor somehow), I think it is better if we use jsdoc. \n\n@novemberborn , my second take on this, missing documentation on t.log, t.skip, t.context and among other things, you can now add your edits :+1: . @novemberborn , absolutely yes, I will look into the new updates and implement it on the flow definitions\nI am using peek for the screen recordings. @novemberborn \nIs it necessary to restructure the flow definitions, that it would closely resemble the typescript definitions?\n. @novemberborn \nRestructured flow definition with what I understood, certain definitions that I directly copied from the ts file doesn't work with flow, but I tried it to be as close as possible.\nWill add screen captures tomorrow\nP.S.\nI see that tests for flow and typescript definitions are only done when there is a need to regress, what if we add dedicated tests, is it possible?. @novemberborn \nWhen you select from the existing methods\n\n\nWhen you write the method name with parenthesis\n\n\n. @novemberborn @Jolo510 , on it. that makes sense! I was trying to find a better way :). @novemberborn , this is quite circular, and gives me an error on XO, I now understand that the contextRef is injected into the Test class then to the ExecutionContext (that is why you pointed me that way), is it necessary that we should create the LateBinding class? Is it possible that we can make the copy() method returns a clone of the class itself?\nInstead, when passing it the beforeEach and afterEach hooks, it would look like this\njavascript\n_buildTestWithHooks(test, contextRef) {\n    ...\n    const context = contextRef ? contextRef.copy() : new ContextRef();\n    ....\n}. If I created the context above and injected it directly, I think it should still have work as intended, but the after hook got a mutated(mutations from afterEach, beforeEach, tests) context when I ran the tests.\nI need to investigate more :D\n```javascript\nconst context = {context: {}};\nconst serialTests = new Sequence(this._buildTests(this.tests.serial, context), this.bail);\nconst concurrentTests = new Concurrent(this._buildTests(this.tests.concurrent, context), this.bail);\nconst allTests = new Sequence([serialTests, concurrentTests]);\n...\nif (this._hasUnskippedTests()) {\n    const beforeHooks = new Sequence(this._buildHooks(this.hooks.before, null, context));\n    const afterHooks = new Sequence(this._buildHooks(this.hooks.after, null, context));\n    finalTests = new Sequence([beforeHooks, allTests, afterHooks], true);\n} else {\n    finalTests = new Sequence([allTests], true);\n}\n```. Using vscode 1.20.1, this is not shown, but I think it is for the better to document the message parameter on each method definition. ",
    "wyvernzora": "Thanks! Is there any ETA as to when this feature will make its way to npm?\n. BTW, to those who encounter the same issue, my current workaround is the following:\n```\ntest.beforeEach(async t => {\n  t.context.db = await mongodb.connect('mongodb://localhost:27017/test');\n  t.context.timeout = setTimeout(() => t.context.db.close(true), 10000);\n});\ntest.afterEach(async t => {\n  clearTimeout(t.context.timeout);\n  await t.context.db.close(true);\n});\n```\nThat is, assuming your tests take less than 10s to run.\n. Updating to the newest version and using the always modifier fixed this issue. Closing.\n. I am still using AVA, and this is no longer an issue as of 0.16.0. I am closing this issue.\nI was able to tell that not all tests finished because the number of passed tests reported by AVA was sometimes a lot less than actual number of tests that I had, while AVA reported that all tests have passed. I am fairly sure it was not because of ignoring files or changing config, since re-running AVA with the same configuration produced different results, sometimes correct sometimes not.\nFor example, test output was 17 passed, and AVA exited with code 0, while I actually had 100+ tests.\n. ",
    "bundacia": "For what it's worth I've moved my cleanup to the before hook for basically the exact reasons @jamestalmage points out here.\n. ",
    "KamuelaFranco": "@sindresorhus Does that work? Otherwise, suggest.\n. ",
    "rhysd": "I want this feature.\n\nThree stars for describing a real world need for the feature!\n\nI'm also currently using ava for E2E testing.\nWhen test failed, I want to take a screenshot of the page before closing browser window.  I'm thinking afterEach.always is suitable but there is no way to know the test result.\n. I think this PR can be merged, right?\n. ",
    "subash-canapathy": "@cgcgbcbc  im not sure about Sequence.run, looking at the runner code I thought we could utilize the report callback to additionally put the status/result of the test in the context so that it still works reliably on concurrent tests (sort of like a thread-local result context)\n. ",
    "frantic1048": "Same question here, I need to log noisy debug information if a test is failing.\nBecause .afterEach() and afterEach.always() hooks behave differently on test failing, I found a workaround:\n```javascript\nimport test from 'ava'\ntest.afterEach('this only runs when test succeed', t => {\n  // set a success status in t.context\n  t.context = 'success'\n})\ntest.afterEach.always('this always runs', t => {\n  if (t.context === 'success') {\n    console.log('this test is succeed')\n  } else {\n    console.log('this test is failed')\n    // log my noisy debug info\n  }\n})\ntest('success', t => {\n  t.is(true, true, '')\n})\ntest('fail', t => { \n  t.is(true, false, '')\n})\n```\nYou can initiate t.context as an Object in test.beforeEach() hook to carry more information. Here I just want to determine whether the test is failing.\nNOTE: This workaround is still not reliable if criterias mentioned #928 are met, try to avoid them:\n\nThere are test failures and --fail-fast is used.\nThere are uncaught exceptions thrown.. @mightyiam How do you get the full object printed in terminal with t.true(isMatch(report, match)) ?\n\nI'm testing tree like structure with same method, getting output like this:\n```plain\n   51:   }                                                            \n   52:   t.true(isMatch(actual, expected), 'should parse two paragraph')\n   53: })                                                               \nshould parse two paragraph\nValue is not true:\nfalse\n\nisMatch(actual,expected)\n  => false\nexpected\n  => Object {\n    ast: [Array],\n  }\nactual\n  => Object {\n    ast: [Array],\n    error: null,\n  }\n```\nThat's painful that I cannot recognize what is failing isMatch(). Versions are:\n\nava@0.19.1\nlodash.ismatch@4.4.0. Finally I wrapped an isMatch() for logging ...\n\n```js\nconst im = require('lodash.ismatch')\nconst chalk = require('chalk')\nconst inspect = require('util').inspect\nfunction isMatch (actual, expected) {\n  const res = im(actual, expected)\n  if (res === false) {\n    console.log(chalk.green('\\nexpected:'))\n    console.log(inspect(expected, { depth: null }))\n    console.log(chalk.red('\\nactual:'))\n    console.log(inspect(actual, { depth: null }))\n    console.log()\n  }\n  return res\n}\n``. @norbertkeri  Which version ofisMatch` are you using? from lodash or the example in https://github.com/avajs/ava/issues/845#issuecomment-312860196\nHere's my usage of doing partial matching:\nhttps://github.com/frantic1048/Est/blob/6f0b24584f54809c197f00f4bd2282eee1c9744f/test/grammar.BulletList.js#L54\nI just wrapped lodash's isMatch a little to generate detailed log. It is not very ergonomic but prints enough data for debugging.. @norbertkeri For example, I deleted one line of input text to parse(), which makes actual lackes one ListItem at https://github.com/frantic1048/Est/blob/6f0b24584f54809c197f00f4bd2282eee1c9744f/test/grammar.BulletList.js#L28\nt.true() generates error info like this(log of tracer.log() is omitted because it's quite long and not related to this issue):\n(expected needs to match 3 ListItem where actual has 2 ListItem)\n```\nexpected:\n{ ast:\n   { T: Symbol(Document),\n     C:\n      [ { T: Symbol(BulletList),\n          C:\n           [ { T: Symbol(ListItem) },\n             { T: Symbol(ListItem) },\n             { T: Symbol(ListItem) } ] } ] } }\nactual:\n{ ast:\n   { ctx: ASTYCtx { ASTYNode: [Function] },\n     ASTy: true,\n     T: Symbol(Document),\n     L: { L: 1, C: 1, O: 0 },\n     A: {},\n     C:\n      [ { ctx: ASTYCtx { ASTYNode: [Function] },\n          ASTy: true,\n          T: Symbol(BulletList),\n          L: { L: 1, C: 1, O: 0 },\n          A: {},\n          C:\n           [ { ctx: ASTYCtx { ASTYNode: [Function] },\n               ASTy: true,\n               T: Symbol(ListItem),\n               L: { L: 1, C: 3, O: 2 },\n               A: {},\n               C:\n                [ { ctx: ASTYCtx { ASTYNode: [Function] },\n                    ASTy: true,\n                    T: Symbol(Paragraph),\n                    L: { L: 1, C: 3, O: 2 },\n                    A: {},\n                    C:\n                     [ { ctx: ASTYCtx { ASTYNode: [Function] },\n                         ASTy: true,\n                         T: Symbol(Text),\n                         L: { L: 1, C: 3, O: 2 },\n                         A: { value: 'item1' },\n                         C: [],\n                         P: [Circular] } ],\n                    P: [Circular] } ],\n               P: [Circular] },\n             { ctx: ASTYCtx { ASTYNode: [Function] },\n               ASTy: true,\n               T: Symbol(ListItem),\n               L: { L: 3, C: 3, O: 11 },\n               A: {},\n               C:\n                [ { ctx: ASTYCtx { ASTYNode: [Function] },\n                    ASTy: true,\n                    T: Symbol(Paragraph),\n                    L: { L: 3, C: 3, O: 11 },\n                    A: {},\n                    C:\n                     [ { ctx: ASTYCtx { ASTYNode: [Function] },\n                         ASTy: true,\n                         T: Symbol(Text),\n                         L: { L: 3, C: 3, O: 11 },\n                         A: { value: 'item2' },\n                         C: [],\n                         P: [Circular] } ],\n                    P: [Circular] } ],\n               P: [Circular] } ],\n          P: [Circular] } ],\n     P: null },\n  error: null }\n```. ",
    "fruch": "I'm also looking for this option, basicly cause I'm trying to write a plugin to capture my app log, and only print it out when test is failing.\nsince ava is runing test in paraller, it's even more importent since logging to a file would be a bit more complex. I have something working, based on  @frantic1048 idea, and that was good enough for me.\nit would be nicer to get the actual status in the test context, so have visibility similar to the test reporter, like test.duration and more.\nI understand most of it is internal, and can easily break between version.\nis afterEach.always being called also for skipped and todo ? (more ideas come in my head, but most of them are in the reporter category). most my tests are serial anyhow, and I do care about their timing.\nIf you think taking timestamp from start of the run till the report, would be better, I'll add that.\nI do care to see how long my tests take, total, and each one. (and if we are it, if there were a way to configure to duration threshold per test would be nice). Now that I've read other thread, I still can't figured why we can report it... \nI care less about precision.\nmy use case is that I want to be able to see the run time of my test suite easily on multi platforms.\nwe put those tests on a precommit hook, which mean we run it quite a lot.\nwe want to be able to track it this number easily.\nand we are a bit spoiled by previous tests runners (for example py.test) that does have this option available.\nyes time( ava ) can do something similar, but some of our developer happen to be working on windows.\nI get is you are trying to keep it small, and not introduce too much features \nbut its shame that I'll have to keep a fork for such small thing.\n. ",
    "lotosotol": "I have a similar issue retrieving the status of the test suite, I am talking about the full test suite's status which I need to pass to BrowserStack (which accepts passed or failed) - would be great to have the ability to retrieve this before closing browser's window either before calling the .quit() method.\nOh, and I am running my tests in Mocha, not AVA and could not find a solution for the issue there as well.. ",
    "gluons": "Agree. I'm new to AVA. I try to play with AVA. After installed, I face the large dependencies. It shocked me. \ud83d\ude31 \nHope AVA be lighter. Regards. \ud83d\ude03 . ",
    "steambap": "I'm using AVA 0.14, which is the lastest.\n. I delete node_module and reinstall everything and it works again. I don't know what happened\n. ",
    "renarsvilnis": "Had the same issue with Node v6.*. Reinstalling node_modules packages didn't help.\nSolved by updating babel-core.\n. ",
    "otakusid": "done :)\nhttps://github.com/chalk/supports-color/issues/37\n. ",
    "simonbuchan": "Obv. this should still be done, but is the OP using TeamCIty 9.1+? Colored output was supported with https://youtrack.jetbrains.com/issue/TW-23760\n. Is there any reason ava --watch leaves the previous errors up? This would be out of scope for this issue, but ideally it would re-write to show only the still failing tests, so if that was done then there is only one timestamp to worry about here.\n. Hmm, I can't reproduce this:\n- git bash 2.9.3.windows.1\n- ava 0.16.0 and 0.15.2\n- node 6.3.1\n- Windows 10 insider build 14905\nI've definitely hit power-assert failing in some cases (for me, when used from a _helper.test.js), but never a difference between cmd, git bash or even new ubuntu on windows.\n. I would suspect different paths for node or global node_modules are causing different versions to run?\n. @StoneCypher There is a reason %LOCALAPPDATA%\\lxss is super-hidden (not visible even with show hidden files), it's not for you. Use /mnt/c like you're meant to. You may as well complain about how editing config files in C:\\Windows with Word breaks your OS means that Windows or Word are \"kind of a train wreck\" (maybe they are, but that sure isn't why)\nSo this isn't a useless reply, Git Bash is the bash that is shipped with the windows version of git (eg git-scm.com), it is a cut-down repackaging of MSYS2 with what is required to run git (which is partially implemented as shell scripts). As it is MSYS2, it is based on cygwin, and requires some nasty hacks internally to get things like fork() to work in user-space, and requires re-compilation. That said, it mostly works fine, but since WSL came out I've completely switched to it and been much happier.\nThat said, I think this issue can be closed as unreproducable?. \ud83d\ude44\nJust in case you are actually interested in doing real work, rather than trying to win points on the internet on a pointless issue on a pointless github issue, put your code in /mnt/c/..., like I said.\nWSL can run windows apps, and windows tools can see your files. Where's the issue? (That's a rhetorical question, I won't actually reply further to this). ",
    "ThomasBem": "As far as I can tell this issue has been made available again because of lack of activity in #1104. So I would like to see if I can finish up what @thinkimlazy started on.. I got it to work for the mini and verbose test reporters already. Gotten quiet familiar with those two from my recent issues / PR`s. Combined with the work you had already done that went really fast. Now I gotta figure out how to stop it from printing color in the tests themselves.\nSo gotta read some more code! :). From what I have read about the TAP standard these additions we are talking about are outside of the specification, so we can call them and format them however we want?\nSo we could use your filepath, line and colum as a starting point, adding this to the output in both error and pass? https://github.com/avajs/ava/blob/master/lib/reporters/tap.js#L35:L51\nDo we actually have access to that information when we are in the reporter though? As far as I know thats not in test.metadata or anything else?. I would like to give this a shot. I am not sure exactly how to go about it yet though, so any pointers are appreciated. \nSindre, for your idea of showing pending tests got any idea how would that work? From what i have seen so far it looks like once a test fails with the --fail-fast flag enabled Ava collects the test results that have been generated so far then starts teardown.. Thanks for the feedback. I'm still interested in seeing if i can figure this out and make a contribution :). So i have implemented some code that should have the test reporter spit out what @mightyiam suggested.\n\n\"--fail-fast is on. Any number of tests may have been skipped\"\n\nI have added a few tests and things seem to be working alright. \nBut is there a way for me to run the Ava code that i now have locally as a test-runner for a separate test project just to verify that everything works as intended all the way through? . So i have implemented some code that should have the test reporter spit out what @mightyiam suggested.\n\n\"--fail-fast is on. Any number of tests may have been skipped\"\n\nI have added a few tests and things seem to be working alright. \nBut is there a way for me to run the Ava code that i now have locally as a test-runner for a separate test project just to verify that everything works as intended all the way through? . I have posted a PR that fixes this issue under #1160. Please let me know what you guys think :). I have posted a PR that fixes this issue under #1160. Please let me know what you guys think :). I have made a PR that fixes #1134 in -> #1160 and since this is kind of similar I would like to take a stab at this too.. After looking at the code its really easy to get this to print out a warning similar to what was done for  the #1134 issue in the #1160 PR. \nI was thinking about changing the logic around counting tests when in .only / exclusive mode. I did some quick testing and I could make it at least count all the tests in files that does NOT contain any .only tests.\nThis would look something like this:\nIf all test files have at least one .only test in them, it logs this:\n''There are tests with the .only() modifier in use. All other tests have been skipped''\nIf there are additional test files that do not contain any .only tests, it logs this:\n\"There are tests with the .only() modifier in use. At least', remainingTests, 'tests were not run.\"\nremainingTests would then be the number of tests in the files without exclusive tests in them.\nAny thoughts around this?. I agree, that wording is much better. \nAny thoughts regarding if there is any point to adding additional details around a number of tests skipped?. I also noticed this while working on issue #1134 today. Here is a screenshot of me trying to run two different files both with a failing test somewhere in the middle.\n\nWhen things are as small as these examples it does not seem like much of a problem. But if you are running a huge test suite for a complex application. Then allowing Ava to keep running tests kind of defeats the purpose of --fail-fast in my opinion. \nSo to keep the --fail-fast option working as intended, i would say shutdown any running processes. But i don\u00b4t know what problems that might cause since you talk about preventing cleanup:\n\nterminate them outright, which prevents any cleanup, even if there are no failures in those processes\n\n. I also noticed this while working on issue #1134 today. Here is a screenshot of me trying to run two different files both with a failing test somewhere in the middle.\n\nWhen things are as small as these examples it does not seem like much of a problem. But if you are running a huge test suite for a complex application. Then allowing Ava to keep running tests kind of defeats the purpose of --fail-fast in my opinion. \nSo to keep the --fail-fast option working as intended, i would say shutdown any running processes. But i don\u00b4t know what problems that might cause since you talk about preventing cleanup:\n\nterminate them outright, which prevents any cleanup, even if there are no failures in those processes\n\n. Thats a much better solution than what i did. Thanks, I'll try and make that change.\nYou mentioned in the issue that it was probably hard or at least not a guarantee that we could  determine the total amount of test to be run by the time failure occurred? So probably out of my league to fix that at this time.\nBut maybe open that as a separate issue / feature request for after we can get this initial functionality in place? . Thats a much better solution than what i did. Thanks, I'll try and make that change.\nYou mentioned in the issue that it was probably hard or at least not a guarantee that we could  determine the total amount of test to be run by the time failure occurred? So probably out of my league to fix that at this time.\nBut maybe open that as a separate issue / feature request for after we can get this initial functionality in place? . Updated PR to reflect change suggested by @novemberborn. FailFast is now passed into RunStatus. . Updated PR with your suggestions. Thanks for the assist \ud83d\udc4d . You mean you think the implementation is wrong? Or do you not think there is any value in showing the remaining count?\nWhat is happening in the case of the screenshots is that there are two test files being run, test.js and test2.js. There is one .only() test in test.js, and none in test2.js.\nSo now you are at least notified of all the tests in test2.js that has not been run because of the .only() tests.\nIf there was at least one .only in test2.js also. It would just report \"all other tests have been skipped\".. Thanks for the review! \ud83d\udc4d \nI am not familiar with template strings, but I think the implementation I have done with regards to  string formatting is in-line / same styles as what was already in place in the reporters.\nI personally don\u00b4t think there is a problem with having the same output strings repeated across the two different reporters. They are two separate pieces that in theory could output different things. It also makes it easy to read what the reporter outputs without having to check some global configuration property.\nBut I am up for making changes if more people think its a better solution :)\n@mightyiam Could you provide an example of how my two lines of output would look using template strings?. @novemberborn updated PR with the changes to remainingCount in run-status.js and added a test for it like you suggested to @jarlehansen in his PR. Had a quick chat with him and he would get on later today to take a look.\nSo you guys can review my suggestion and see what you think :). Yeah, I knew about that behaviour and I tried explaining it in my original PR text. But guess I did not get that through properly, or at least i should have mentioned it more specific :)\n\nIf all test files have at least one .only test in them, it will print out the following message at the end of the report for both mini and verbose.\nThe .only() modifier is used in some tests. All other tests have been skipped.\n\nThats the reason its split into two. Because when .only is present it did not count all the other tests.\nDid not feel comfortable with changing some of the deeper behaviour of how tests where added and removed from collections.\nBut if you think its a good idea to change the behaviour to count all the tests even if there is .only tests present. Then i guess we can simplify the logic you linked to in test-collection.js similar to what was done run-status.js regarding exclusive?\n. @novemberborn I got some results that confused me a bit when trying to fix some failing tests after implementing what I think would be a solution for test-collection.js.\nThen I read this in the description of one of the api.js tests that was failing, https://github.com/avajs/ava/blob/ce42fcb7741039eeb26f0a167b27fd5923936a1b/test/api.js#L22\nIs this correct? That it will still run tests that are NOT tagged with .only() if they are in a file that has at least one .only() test?\nI am still reading up on the code and trying to see if its just me that\u00b4s missing something obvious and thats why things are acting seemingly a bit weird. But figured I would at least ask for a clarification regarding this one thing :). Think I figured out what i was doing wrong on my own :) Was just doing things way too complicated... \nBut this wording is still somewhat confusing to me. Because it does not run any other tests than the .only() as far as i can tell?\n. @novemberborn PR updated with requested changes. It now properly counts all tests, exclusive or not. Also made some changes to the reporters output to be more in line with #1179 that @jarlehansen is working on.\nNow it only reports on .only() / exclusive if there are tests that have not been run.\nAny thoughts on the output text? \n\nThe .only() modifier is used in some tests. 1 test(s) were not run.\n\nA bit clumsy with the \"test(s)\", but i couldn't think of anything better to cover 1 or multiple tests remaining. . PR updated again :). Thanks for the support! \ud83d\udc4d . @sindresorhus commented this in one of the PR`s that was working on this issue previously.\n\nIt also needs to pass the --no-color flag down to the forked process. If I add console.log(require('chalk').red('colorzzzz')); in a test file, it will still log colors even with $ ava --no-colors. I would expect that to prevent all colors.\n\nhttps://github.com/avajs/ava/pull/1104. I have updated the PR with the suggested changes regarding the CLI flag and the for...of loops. \nI need some help with keeping the existing \"default: conf\" in cli.js while also adding the new stuff @sindresorhus mentioned. I could not get it to work together with the new { color: true} so I just removed it for now and its causing a test to fail.\nI don`t understand what this conf thing is and what its doing in default in the first place. It makes no sense to me :P\nI can understand adding { color: true } saying that color should be true if nothing else is set. But just \"conf\" makes no sense to me.\nI also need some support in where to start with passing the flag down to the child processes. That`s one of the things I have yet to understand with how Ava works :). Thanks for the help @novemberborn :)\nI did what you said for the default, and that worked fine. Think I sort of understand whats going on with that now.\nI also made something that works for passing the color flag down to the forked process.\nI am unsure of how / if I can test that small piece of code though. Any suggestions?\n. Looks better now? I used IntelliJ instead of VSCode to edit the markdown file. . Something going on with Travis? First time i edited the readme.md file, travis failed on node 7. \nNow I fixed a small indentation mistake and travis fails on node 4? Even different tests failing.. Seems like things are working much better today. So probably some hiccup with Travis yesterday.. Sure, I can do that :)\nChanges this time was all your small suggestions, let => const, Result => result.. So I have reverted back to babel-config.js change I made, and instead passed down an additional --color flag in execCli where needed.\nTok me quite a while to get my work in after @vadimdemedes big Magic Assert PR. But I think I managed to get everything up-to-date. But please take a look :). I think both of these suggestions are valid and good improvements. \nWe actually had some trouble with duplicated test titles a while ago. It turned out that when we wrote our first test for a new react component. We always ended up calling it 'Render to DOM' by default. None really thought about it and it just happened out of habit.\nThen at some point we where making a new component and our 'Render to DOM' test just kept on failing. It was a real simple test and we could not figure out what was wrong.\nTurned out some other code had broken a previous 'Render to DOM' test in a different component and we just didn't see that it wasn't  the one we where looking at.... I would like to do some work on this unless any of you guys have already started?. > Anyway, I think that a test can only be made clearer by having a title, and that allowing an optional title will incite the lazier users to just not add a title. If you really don't feel like thinking of a title for a single-test function, you're always free to give it a bogus one, like the name of the tested function (test('is-hyper', t => {})), which is not that big of a deal IMO.\nI agree with this. I also think that once people are forced to write a title, in many cases they will actually write something sensible. The difference between just writing 'Test' and writing something like 'Throws if no type is supplied' is pretty negligible once you have to write something.\n@jfmengels I will give a shot then! Think its a good improvement and probably something I can figure out how to do :). Will sync up with @jarlehansen regarding this and update PR.. @novemberborn Sure, I can fix that :). I just re-used the existing code that in other places did this but with \"=true\". But can make changes if you think that`s better.\nhttps://github.com/ThomasBem/ava/blob/7c00c13844da6ab116676027b6482a57cf0f102a/lib/reporters/verbose.js#L9:L11\nhttps://github.com/ThomasBem/ava/blob/7c00c13844da6ab116676027b6482a57cf0f102a/lib/reporters/mini.js#L14:L16. I will double check this. Was stuck on this point for a while trying out different things and reading up on meow and minimist. Got it working on undefined, but maybe I missed something.. @sindresorhus I tried to do what you suggested with default in in meow and that works for my code, \n{\n    color: true\n}\nbut I dont understand whats going on with whats currently in default and how to make it work together with the change you recommended. Tried a few different things, but I keep getting a failing test in cli.js.\nhttps://github.com/avajs/ava/blob/master/lib/cli.js#L75\n. Good catch! Did not even think about that.. Should be good now :). Hmm, ill take a look. It does not look that way for me in IDE or when looking at the readme in my branch. I see even Travis failed on some tests in node7 suddenly, when all i did was updated readme.... You mean adding a MiniReporter.prototype.enabled = function () {} that would enable all the colors?. Ah, oki :) I will commit with updates based on your review, but without doing anything regarding the loop.\nI did a quick try where I changed how we create mini and verbose reporters in their test setup. Making sure to include {colors: true} as an options. That way we can use your suggestion:\nchalk.enabled = this.options.color;\nfor (const key of Object.keys(colors)) {\n    colors[key].enabled = this.options.color;\n}\nIt works for the mini and verbose tests themselves, but it caused this test to fail because chalk and colors did not seem to work as intended, https://github.com/avajs/ava/blob/master/test/cli.js#L73. Sorry, C# habbit :p . I removed it from the actual output, since we removed it from test. But if the problem only exits when running tests, not real Ava. Then we can put it back in the implementation.. I did some testing and it seems like this is only a problem in test? Is this because execCli spawns a new process and the chalk.enabled and colors.enabled stuff is not passed on?\nAs far as I can tell none of the changes I did changed anything related to that behaviour? So why this did work before?\nSince it seems to actually work when running Ava properly, we can always just remove the color from expected in the test? At least we catch that the error is thrown with the correct text. Even though colors and underline will be missing.. ",
    "thinkimlazy": "@ThomasBem great! I think u need to check if main proccess has color flag disabled, then pass it to child test proccesses. And write tests. GL!. @novemberborn Hey, thats not working ideally as you want because of this code in verbose/mini reporters:\nchalk.enabled = true;\nObject.keys(colors).forEach(function (key) {\n    colors[key].enabled = true;\n});\nSo when chalk initialized in reporters, --no-color really disables colors, but this loop enables them. So there is no need to fix anything in chalk. \nAlso we can delete this loops in reporters, then --no-color just works.\n. @sindresorhus sry, I messed up\nI reopened PR \n. @sindresorhus I can't reopen this one because I recreated my branch. Again sorry for spam\n. Colors in test files disabled by default. Without flag\n```\nimport test from 'ava'\nimport chalk from 'chalk'\ntest('no colors here', t => {\n    t.false(chalk.enabled); // passes\n})\n```\n. @vadimdemedes Hello, I'm quite busy right now so I'm not sure that I can fix this any time soon.. Ye that what I meant. Mine test files with no colors, but my terminal supports color. Sindre got colors.\nI'll dig into\n1104  . We don't need arrify because Array.prototype.concat already can concat single values\nArray.prototype.concat docs\n. This turns on colors no matter what.\n. We need to turn on colors no matter what while testing. Also in case that Istanbul turning them off at this condition\n. Force to use colors while testing\nAnd get rid of useless arrify\n. ",
    "aemixdp": "Still doesn't work correctly in 0.18.2:\n```javascript\nimport test from 'ava';\ntest('Test', (t) => {\n    t.is(1, 2);\n});\n```\n$ ava --verbose --no-color > test.log 2>&1\n$ open test.log\n```\n  \u00d7 Test  \n1 test failed [20:29:45]\nTest\n  D:\\Code\\js\\ava-sandbox\\test.js:4\n3: test('Test', (t) => {\n   4:     t.is(1, 2);    \n   5: });                  \nActual:\n [33m1[39m\n\nExpected:\n [33m2[39m\n\n```. ",
    "leewaygroups": "Let me get my hands dirty on this.. First, I agree with the idea of maintaining a simple interface so I think the name like is fine.\nSince lodash is one of the dependencies used, @jfmengels suggestion _.isMatch seem very appropriate. However, @kentcdodds auggestion  containsSubset is what I consider more robust. \nMy take is, adopting a simple name like and building this based on chai-subsets containsSubset.\nDrawback: 'chai-subset' is dependent on chai.\nWhat do you think?.  The pros for Lodash is compelling and green. After previous converstaions went with lodash. I'll checkin the changeset for review soon.\n@mightyiam @novemberborn Thanks for the follow up.. May I take on this as a gentle step into AVA.\nOne question: For uniformity, would it not make sense to apply same check in other sibling functions such truthy, falsy etc?  \nIn that case the check can be centralised as a helper and invoked in all sibling functions where it is needed.. @novemberborn Yeah, some delays from me there. Sorry about that. I'll check-in the test in shortly.. Following your comment, I can think of two possible ways to go about this.\nThrow an exception when msg is anything other than\n\nA string or \nA string or Error object (since for errro type,  the message propoerty value can be used).. @sindresorhus : Great catch! I wonder why I didn't see that.    . I can check if msg exists and not string then throw an error in within the if-statement:\n\nif(msg && typeof msg !== 'string'){\n  throw error \n} \nBut that logic is flawed because 0 will slip through uncaught. Also, inverting the logic as suggested seem flawed as well since an OR condition is short-circuited once first test is truthy.\nI'll look more closely.\nCheers!  . ",
    "norbertkeri": "Did this get resolved? Is there a way to do partial matches? The suggestion by @frantic1048 and @mightyiam both produce erroneous output in the console when used with t.true(). . I can use t.log() for formatting and displaying the error, but I would still need to put an assertion somewhere, and using t.true() is causing the erroneous output. Is there something I'm missing with t.log()?. No, using this code:\njs\n    t.true(isMatch(result, { currentUser: { email: 'hello@world.com' }}));\nProduces this output:\n```\n  1 failed [10:47:10]\ncurrentUser \u203a Current user is returned\n/home/myuser/projects/x/usermanagement/tests/currentUser.test.js:38\n37:         .queryData(\"{ currentUser { id, email }}\");                     \n   38:     t.true(isMatch(result, { currentUser: { email: 'hello@world.com' }}));\n   39: });                                                                       \nValue is not true:\nfalse\nisMatch(result, {\n    currentUser: {\n      email: 'hello@world.com'\n    }\n  })\n  => false\n{\n    currentUser: {\n      email: 'hello@world.com'\n    }\n  }\n  => {\n    currentUser: {\n      email: 'hello@world.com',\n    },\n  }\n{\n    email: 'hello@world.com'\n  }\n  => {\n    email: 'hello@world.com',\n  }\nresult\n  => {\n    currentUser: {\n      email: 'hello@bye.com',\n      id: '1',\n    },\n  }\n``\nWhich all comes from thet.true()` call (from what I assume because of the magic assert feature).. @frantic1048 the example from the comment. Could you show me what output do you get when the test fails (actual doesn't match expected)?. Ok I'm not sure what's going in my code then, the output I get is very different (noted above). Will take a look later, thanks.. Is this going to be fixed before 1.0? I'm evaluating whether to use ava for a new project, it's a bit uncomfortable to to manually edit failing tests to see the stack traces.. Is this going to be fixed before 1.0? I'm evaluating whether to use ava for a new project, it's a bit uncomfortable to to manually edit failing tests to see the stack traces.. ",
    "asafigan": "Could I take this issue on?\n. Thanks!\nI need some more information before I get started. Where do you want the validation tests to be called from? Also are these the only validations that we need?\n. Do you just want me to make the module and test? or do you want me to also integrate it into the other modules?\n. should we prevent skipping hooks?\n. also should we allow only on hooks?\n. So for integration I'm thinking that validation would be done in the runner's _addTest method. It would run the validation and if the result isn't null, it would throw an error with the string from the validation test. Does that seem good?\n. When I was doing the integration I found that 'skip' should work with hooks\n. ok, that's what I assumed after looking into it myself.\nI have made a pull request that fixes this issue. Please take a look at it.\n. Hey, I've never contributed to an open-source project before. It doesn't look like there has been that much progress on this issue. I was hoping I could help out with this issue?\n. I was thinking that a property, like hasBegunRunning, could be set to true in the runner's run method and than checked when the test method was was called. Would that be a good approach?\n. @pinwen Are you still working on this issue?\n. also I didn't know what type of error to throw\n. Ok, I will add that test and also tweak the error message and the second test\n. Ok I'm gone with those changes. Let me know if there is anything else I need to change.\n. thanks!\n. Ok, I changed the property name.\n. Also should we mention in the docs somewhere that you shouldn't have embedded tests or hooks. And also that we have no alternative to describe.\n. woot! first contribution! \ud83c\udf89\nThanks for all of the help on this\n. Thanks for the feed back guys. I'm going to be busy for awhile, but I will try to look into everything.\n. Sorry I have been so slow on this. I will do it this weekend.\n. @vdemedes unless there is any other feedback, I believe it ready to merge\n. So, I need to rewrite the tests in test/runner.js. What about reformatting the error messages? I couldn't tell if there was a discussion with that.\n. I'm getting major issues when I try to test now.. I pull from master and solved the merge conflicts but now there are a bunch of errors when I run npm test or npm run test-win. I went to my master branch and pulled from upstream master and I get the same issues. oh yeah I noticed that. the real issue was that I forgot to rerun npm install. I had to fix some other things, too.\nSorry, it took me so long to circle back on this.. Will do. I didn't name it isRunning because it isn't reset to false after the tests are done running. \n. but I don't like hasBegunRunning either\n. I like that better\n. In the issue, it was discussed that skipping hooks should be allowed. Do you think we shouldn't skip them?\n. @jfmengels I don't know what you meant here.\n. thanks for catching that. ",
    "jvorcak": "I'm sorry, it might be a bit off-topic, but I've tried to integrate nock with ava the same way as @jamestalmage is doing here https://github.com/avajs/ava/issues/849#issuecomment-224667838\nAnd I'm getting \n1. contact \u203a Contact can be created\n  Error: XMLHttpRequest is not supported by your browser\nDo you have any suggestions? I've tried to integrate https://github.com/avajs/ava/blob/master/docs/recipes/browser-testing.md but it's probably a wrong direction.\n. ",
    "morenoh149": "I made a minimal test demoing a problem I've run into https://github.com/morenoh149/avaNockParallel\n@jamestalmage @silvenon @sotojuan what do you advise I do regarding the approach? should I use https://github.com/mmalecki/hock ?. @razor-x in that vien, it'd be nice if ava exposed a unique testId in t.context or similar. I can't read that image on my machine.. this is odd. You had to roll back to a previous version?\n. ",
    "razor-x": "@morenoh149 One approach is just make sure interceptors are unique in tests that run in parallel in the same node process. I realize this may not always be straight forward for all APIs, but it should handle a large set of cases.  At least for rest APIs you can do .get('/widgets/exists') and .get('/widgets/doesnotexists) to test cases in parallel.. My use case is passing a logger that uses t.log to all my injected dependencies:\n```js\nimport test from 'ava'\nimport createLogger from '@private/logger'\nimport Foo from './foo'\n// How it works now\ntest('does bar', t => {\n  const foo = new Foo({log: createLogger({t})})\n  const bar = foo.bar()\n  t.true(bar)\n})\n// How I would like it to work\ntest.beforeEach(t => {\n  t.context.foo = new Foo({log: createLogger({t})})\n})\ntest('does bar', t => {\n  const bar = t.context.foo.bar()\n  t.true(bar)\n})\n// How I have to work around it\ntest.beforeEach(t => {\n  t.context.foo = t => new Foo({log: createLogger({t})})\n})\ntest('does bar', t => {\n  const foo = t.context.foo(t)\n  const bar = foo.bar()\n  t.true(bar)\n})\n```. With the tight babel integration and no support for using a peer version of Babel, is Ava concerned about users who cannot upgrade yet to Babel 6 becoming locked out of support for the next version of Ava? This issue is exacerbated by not using semver as this change would normally force a major version bump leaving the previous version open for PRs to backport security fixes and bug fixes.. That's all reasonable. Mostly want to bring up this issue for an official response so affected users understand what Ava development will support and can plan accordingly.\nI'd say it's dangerous to assume code that passes tests run through Babel 7 but compiled with Babel 6 will run identically in production. Ideally that's not the case, but one cannot guarantee perfect parity between two major versions.. Are you saying Ava does not run relative importin spec files though the bundled Babel version but through the one specified by your package.json?. ",
    "seacloud9": "It would be nice to have a better recipe for actual endpoint testing.  I recently tried this https://github.com/morenoh149/avaNockParallel/ after changing the url and was never able to receive the promise \"message: 'Promise returned by test never resolved'\" everytime.  I have also tried https://gist.github.com/seacloud9/38f79278d62ed8ceb2370f137d6c42ad it would be extremely helpful to have an endpoint recipe that currently runs.  After looking at nock I think I am going to try https://github.com/dareid/chakram does anyone have a chakram test that works with axios or apisauce?  This would be very helpful.  . ",
    "zellwk": "Hey guys, I struggled with endpoint testing (using supertest with Mongomem and Mongoose) according to the docs. Discovered that there's a much much simpler way of doing things. Here's a snippet of what I ended up with: \n```\nimport test from 'ava'\nimport request from 'supertest'\nimport { MongoDBServer } from 'mongomem'\nimport { setupMongoose, setupFixtures, removeFixtures } from '../utils'\ntest.before('start server', async t => await MongoDBServer.start())\ntest.beforeEach(async t => {\n  const db = await setupMongoose()\n  const app = require('../../server')\n  await setupFixtures()\n  t.context.app = app\n  t.context.db = db\n})\ntest.afterEach.always(async t => {\n  const { db } = t.context\n  await removeFixtures()\n  await db.connection.close()\n})\ntest.serial('litmus get test', async t => {\n  const { app } = t.context\n  const res = await request(app)\n    .get('/test1')\n    .send({\n      email: 'testing@gmail.com'\n    })\n  t.is(res.status, 200)\n  t.is(res.body.name, test)\n})\ntest.serial('litmus post test', async t => {\n  const { app } = t.context\n  const res = await request(app)\n    .post('/test2')\n    .send({\n      email: 'test2@gmail.com'\n    })\n  t.is(res.status, 200)\n  t.is(res.body.name, 'oohlala')\n})\ntest.after.always('cleanup', t => MongoDBServer.tearDown())\n```\nWould you like me to write some docs on this? I'd love to! Just let me know. . Edited with a snippet instead of a screenshot. Added PR! :) . Managed to find a way. Awaiting the mods to see if they'll like me to write docs for it. Hence, closing. . @snarfed My process is written in the docs now: https://github.com/avajs/ava/blob/master/docs/recipes/endpoint-testing-with-mongoose.md. @OmgImAlexis Hmm. Would you mind trying again? Recipe looks right to me. Whats in your user.spec.js\n@novemberborn I can edit the file directly, would that work better? Regarding that recipe, I tested it and it doesn't work directly with my app because the Mongoose instances are different, which is why I decided to make this recipe. . Looks right. Can you tell me how you handled /user? . Do you mind trying out a handler like this?\napp.get('/user', (req, res) => {\n  const { email } = req.body\n  User.findOne({email})\n   .then(r => res.json(r))\n  .catch(e => /* handle whatever errors */)\n}). Ah yes. I had the same problem too. Just to confirm. This recipe works fine for you. Yeah? . @OmgImAlexis I have tried using mondob-memory-server with the following code, but was unable to get tests to run in parallel still. \n```\ntest.beforeEach(async t => {\n  const mongod = new MongodbMemoryServer()\n  const uri = await mongod.getConnectionString()\n  await mongoose.connect(uri)\n  await setupFixtures()\n  const app = require('../../server')\nt.context.mongod = mongod\n  t.context.app = app\n  t.context.db = mongoose\n})\ntest.afterEach.always(async t => {\n  const { db, mongod } = t.context\n  await removeFixtures()\n  await db.connection.close()\n  await mongod.stop()\n})\n```\nThe error I get when attempting to run tests in parallel is this: \n```\nlitmus \u203a beforeEach for litmus post test\n  /Users/zellwk/Projects/private-repos/course-server/node_modules/mongoose/lib/connection.js:211\nRejected promise returned by test\nRejection reason:\n[Error: Trying to open unclosed connection.]\n\nNativeConnection.Connection._handleOpenArgs (node_modules/mongoose/lib/connection.js:211:11)\n  NativeConnection.Connection.open (node_modules/mongoose/lib/connection.js:306:37)\n  Mongoose.connect (node_modules/mongoose/lib/index.js:259:47)\n  Test. (test/routes/litmus.spec.js:17:18)\n  step (test/routes/litmus.spec.js:25:191)\n```\nHere, it seems like, without using mongooseInstance = new mongoose.Mongoose() as this recipe suggests, we can't test databases in parallel. But if we do use new mongoose.Mongoose(), we have to change how mongoose connects within the app. (And I have no idea how to do it). If you have any suggestions, I'd be happy to try!  . > Both recipes are very similar to each other. Ideally we have one. If there's some additional use case or nuance we can discuss it in the recipe.\n@novemberborn Sure. We can merge it into one recipe eventually. Can we iron out the issues mentioned above before doing so? That way, we can see if there's really an additional use case or nuance. . @nodkz Thanks for chipping in! I've tried your suggestions, but I still couldn't get ava tests to run in parallel. By parallel, I mean tests within a test file. So far, my test file looks like this: \n```\nimport test from 'ava'\nimport app from '../server'\nimport request from 'supertest'\nimport User from '../models/User'\nimport mongoose from '../handlers/mongoose'\nimport MongodbMemoryServer from 'mongodb-memory-server'\nconst mongod = new MongodbMemoryServer()\ntest.before(async t => {\n  mongoose.connect(await mongod.getConnectionString())\n})\ntest.beforeEach(async t => {\n  const user = new User({email: 'one@example.com', name: 'One'})\n  const user2 = new User({email: 'two@example.com', name: 'Two'})\n  const user3 = new User({email: 'three@example.com', name: 'Three'})\n  await user.save()\n  await user2.save()\n  await user3.save()\n  t.context.app = app\n})\ntest.afterEach.always(async t => {\n  await User.remove()\n})\ntest('litmus get test', async t => {\n  const { app } = t.context\n  const res = await request(app)\n    .get('/litmus')\n    .send({email: 'one@example.com'})\n  t.is(res.status, 200)\n  t.is(res.body.name, 'One')\n})\n// Works if we use test.serial instead of test\ntest('litmus post test', async t => {\n  const { app } = t.context\n  const res = await request(app)\n    .post('/litmus')\n    .send({\n      email: 'new@example.com',\n      name: 'New'\n    })\n  t.is(res.status, 200)\n  t.is(res.body.name, 'New')\n  t.is(200, 200)\n})\ntest.after.always(async t => {\n  mongoose.disconnect()\n  mongod.stop()\n})\n```\nIs this the best we can do? If it helps with debugging, you can find a demo repo at https://github.com/zellwk/ava-mdb-test. . @nodkz, @novemberborn, @OmgImAlexis any suggestions on the demo repo I've sent? If not, I'll go ahead and edit this docs again. . @nodkz \ud83d\udc4d. Thanks so much for all the help! I'm going to update this PR with your latest findings and ping the others. . @OmgImAlexis, @novemberborn and @nodkz, The recipe is updated. Please take a look and let me know if it's good to go. \nI think this recipe should overwrite the original endpoint testing recipe. They're pretty similar. What do you think?. @OmgImAlexis Changed as requested. For the example, should I cede ownership of the repo to AVA? If not, let me know what's the best way. \n@nodkz Thanks Pavel. I'm pretty sure what you've suggested works. Not sure if @OmgImAlexis and @novemberborn would like to be more explicit about test.before, test.beforeEach and test.afterEach.always. . @novemberborn I made the changes you requested. One thing still concerns me regarding 'regenerator-runtime'. I can't get the tests to work without babel-polyfill. I've included the stack trace in the comments, but here it is again: \n```\n/Users/zellwk/Projects/test-repos/ava-mdb-test/node_modules/mongodb-memory-server/lib/MongoMemoryServer.js:10\n  var _ref = _asyncToGenerator(regeneratorRuntime.mark(function _callee(port, dbName) {\n                               ^\nReferenceError: regeneratorRuntime is not defined\n    at /Users/zellwk/Projects/test-repos/ava-mdb-test/node_modules/mongodb-memory-server/lib/MongoMemoryServer.js:10:32\n    at Object. (/Users/zellwk/Projects/test-repos/ava-mdb-test/node_modules/mongodb-memory-server/lib/MongoMemoryServer.js:44:2)\n```\n@nodkz Would you mind checking the above? Let me know if regenerator-runtime is required directly in the tests, or within mongodb-memory-server? \n\nI'm not sure what to do with https://github.com/avajs/ava/blob/master/docs/recipes/isolated-mongodb-integration-tests.md though. Again it's more focused on just MongoDB testing, with a bit of Mongoose added on. It uses a different memory server though. Perhaps it'd be sufficient for these recipes to reference each other.\n\nI've tried the recipe before creating this one. The mongoose version doesn't work. Had a tough time understanding everything, until I hacked stuff together with @nodkz and @OmgImAlexis help in this PR. \nSo, I recommend removing the Mongoose parts, but keep the MongoDB parts for that recipe. . > Thanks @nodkz! @zellwk could you check that's all good with this recipe, so we don't need the polyfill and stuff?\nDone! babel-polyfill is no longer required. Woot! \n\n\nSo, I recommend removing the Mongoose parts, but keep the MongoDB parts for that recipe.\n\nThat's a good idea. Could you add that to this PR? I can fix it up afterwards to if you'd like.\n\nDone too! Removed the Mongoose parts and pointed them to the mongoose docs instead. I'm not sure whether we should change Mongmem to Mongodb-memory server on that recipe. Thoughts, @nodkz? \n@OmgImAlexis and @novemberborn, I also added an explanation on why the user should use test.serial instead of test with Mongoose. LMK if it looks good. . In that case, shall we continue with Mongomem with the mongodb recipe? . > I've also changed the section on using test.serial: I'm pretty sure Mongoose already uses the same connection in the tests and the app. Notably, the connection is made in a test.before() hook, not a test.beforeEach() hook. Thus the real concern is tests interleaving and the database being in an unknown or conflicting state.\n@novemberborn LGTM \ud83d\udc4d . \ud83d\udc4d . \ud83d\udc4d . The 'require' statement configures Mongoose according to the current test (I believe). May not work properly if Mongoose is not set up per test. . Another way is to disconnect mongoose. Works the same, as far as I can tell. Awaiting @nodkz's input on mongodb-memory-server right now. Otherwise, tests can't run in parallel. Should we wait? Or should I edit according to your comments first? . Sure. You're right about this. Let me make the requested changes after confirming if there's a better implementation with @nodkz, as commented below. . Will expand on this! . I tried adding regenerator-runtime, but it didn't work. Looks like regenerator-runtime is required in mongodb-memory-server? \nHere's the stack trace: \n```\n/Users/zellwk/Projects/test-repos/ava-mdb-test/node_modules/mongodb-memory-server/lib/MongoMemoryServer.js:10\n  var _ref = _asyncToGenerator(regeneratorRuntime.mark(function _callee(port, dbName) {\n                               ^\nReferenceError: regeneratorRuntime is not defined\n    at /Users/zellwk/Projects/test-repos/ava-mdb-test/node_modules/mongodb-memory-server/lib/MongoMemoryServer.js:10:32\n    at Object. (/Users/zellwk/Projects/test-repos/ava-mdb-test/node_modules/mongodb-memory-server/lib/MongoMemoryServer.js:44:2)\n```. You're right, babel-register is not required. My bad. Thanks! .  \ud83d\udc4d . \ud83d\udc4d . \ud83d\udc4d . True. Let me omit it. . \ud83d\udc4d . Sure, I'll link out. . Should I link to this PR? Since @nodkz explained in detail, but there's probably no need for others to go into the same depth. . \ud83d\udc4d . Sure, linked. . When you use mongoose, you interact with the database through Mongoose. Since there can only be one Mongoose instance (and that Mongoose instance must always point to the same database), we cant access multiple databases even if we create them. \nDo I make sense? . Ah! I understood @nodkz's explanation (above) wrongly. Although you can create multiple connections with Mongoose, you need to create multiple models. Let me switch the explanation as soon as I find some time to. . @OmgImAlexis updated with a slightly better explanation. Do I make sense in this new one?. I fully understand that AVA runs tests in parallel, and your desire to set it up by default. I would love to do that too. \nHowever, based on @nodkz's explanation, and my personal research over the past month, I strongly believe endpoint testing with Mongoose and AVA is an exception where serial tests would be preferred. \nThe setup for parallel tests changes how a person would create their schemas, and thus their app, too much for the additional benefit of running tests in parallel within each file. Besides, if you create multiple AVA test files, each test file still runs in parallel to other files, which shouldn't slow down testing too much. \nI'll be glad to change the docs if you could show me a good way to run endpoint testing in parallel with Mongoose and Supertest. \n. ",
    "justsml": "I wish AVA kept (or added) to the supported method aliases. \nAppears they were removed before I even knew the project existed. \ud83d\ude3f \n\nIgnore me if many people have already 'complained' about this (and if I just haven't found the Original Issue of record to understand what's going on. )\n\nI see lots of tape compatible tests, all with varying standardization between them (isError vs. error etc.). They have varying levels of babel integration (not-working to no-stacktraces). (I fix it if I can, of course.)\nI avoid messing with it by running it though AVA \u2764\ufe0f .\nI have a 'hack' injector/wrapper for AVA to map function aliases, but I feel silly every time I have to add & include it.\n\nAnyway, thanks @sindresorhus et al. for all the awesome projects.\n\n. ",
    "unional": "Chiming in. Just shape test is not enough:\nhttps://github.com/typings/generator-typings#about-writing-tests-for-typings\n. > It would almost certainly be better than the nothing we have now though!\n:smile: of course. \n. Oh, by the way, if you lobby on this... :smile:\nhttps://github.com/Microsoft/TypeScript/issues/7661\n. An alternative would be using any-promise. Since it includes its typings and does not have any dependencies, it should work too.\nYou can try it out with the changes I made:\n\nI add any-promise and solve the issue:\nhttps://github.com/unional/ava/blob/master/index.d.ts#L1\nhttps://github.com/unional/ava/blob/master/package.json#L80\n. es6-promise would also be possible (and likely a better option) once this is landed:\nhttps://github.com/stefanpenner/es6-promise/pull/211\n. Great! Thanks. \n\nI'm working on ava-fixture in typescript that runs file-based tests (and baseline tests in the future). It would benefit from the updated typings. \nAre you planning to do a release soon?\n. Thanks! Missed that ~~sentence~~ doc completely. \ud83c\udf37 \n. Encounter the same issue. ava@0.17.0\nts\n// generated.d.ts\n        ......\n    export function todo(name: string, implementation: ContextualTest): void;\n    export function todo(implementation: ContextualTest): void;\n    export function todo(name: string, implementation: Macros<ContextualTestContext>, ...args: any[]): void;\n    export function todo(implementation: Macros<ContextualTestContext>, ...args: any[]): void;. @lukescott for watch, you can do this:\nhttps://github.com/unional/color-map/blob/master/scripts/watch.js\n```js\n'use strict';\nconst cp = require('child_process');\nlet ava;\ncp.spawn('tsc', ['-w'], { shell: true })\n  .stdout.on('data', (data) => {\n    if (!ava) {\n      ava = cp.spawn('ava', ['-w'], {\n        stdio: 'inherit',\n        shell: true\n      })\n    }\n    const text = data.toString()\n    process.stdout.write(text)\n    if (/.*Compilation complete/.test(text)) {\n      let lint = cp.spawnSync('npm', ['run', 'lint'], {\n        stdio: 'inherit',\n        shell: true\n      })\n      if (lint.status === 0) {\n        cp.spawnSync('npm', ['run', 'build-commonjs'])\n      }\n    }\n  })\n``. I figure out the problem. It's in theansi-styles. This issue happens on windows andansi-styles` does not produce the correct string in that environment.\nWindows follow this (or something similar, up to Windows 8):\nhttp://www.termsys.demon.co.uk/vtansi.htm\n. This is what's working for me on windows:\nhttps://github.com/unional/aurelia-logging-color/blob/master/src/AnsiBrush.ts#L31\n. Mentioned on the ansi-styles issue. It is not the same.\nTo see the difference: \nsh\nnpm i -g ava\nnpm i chalk ava aurelia-logging aurelia-logging-color\n```js\n// index.js\nvar chalk = require('chalk')\nvar logging = require('aurelia-logging')\nvar color = require('aurelia-logging-color')\nvar test = require('ava')\nlogging.setLevel(logging.logLevel.debug)\nvar log = logging.getLogger('some log')\nlogging.addAppender(new color.ColorAppender())\ntest(_t => {\n  console.log(chalk.bgGreen('suppose to be green'))\n  log.debug('log something')\n})\nsh\nava index.js\n``. Ok. You are correct. I do see thatsupports-colorturnchalkoff inwin32` environment.\nWhen I worked on aurelia-logging-color, I can't use ansi-style because after bundling all dependencies the library become quite huge, and I recall seeing the color not rendered correctly.  That's why I went in and go through the code and the material to figure out how to get it working in my case.\nHowever, right now I can't reproduce the problem. Everything seems to be working fine for chalk, ansi-style and so on. I must be dreaming back then. \ud83d\ude1b \nAs for the OP, I tried chalk.enabled = true to force coloring and it is working. FYI. \ud83c\udf37 \n```js\nvar chalk = require('chalk')\nvar test = require('ava')\nchalk.enabled = true\ntest(_t => {\n  console.log(bgBlue: ${chalk.bgBlue('bgBlue')} and yellow: ${chalk.yellow('yellow')})\n  console.log(chalk.bgCyan('cyan'), chalk.bgGreen('green'))\n})\n``\n![image](https://cloud.githubusercontent.com/assets/3254987/22642601/36c1dc00-ec10-11e6-829e-57fb5c485524.png)\n. Sorry, maybe not unicode, but the color is not showing. What is the proper name of it? Please feel free to change the title.. Sorry, maybe not unicode, but the color is not showing. What is the proper name of it? Please feel free to change the title.. I tried usingshelljs` and use environment variables. But couldn't get it to work.\nThe issue seems like when ava runs, since it runs on a separate process, the environment variable is not picked up.. Thanks, it works wonder!\nFor the record, this work for me:\n```js\n// package.json\n{\n  \"scripts\": {\n    \"coverage\": \"nyc --check-coverage --branches 85 --functions 85 --lines 85 npm test\",\n    \"test\": \"ava && export BROWSER_TEST=true; ava\"\n  },\n  \"ava\": {\n    ...\n    \"require\": [\n      \"./scripts/setup-browser-env.js\"\n    ]\n  }\n}\n// scripts/setup-browser-env.js\nif (process.env.BROWSER_TEST) {\n  require('browser-env')()\n}\n```\n. Thanks. I got it working.\nSeems like the way I use the sourceRoot is a hack.\nI configured it so that the resulting build have the right source map root in the browser:\n\njs\n{\n  \"compilerOptions\": {\n    \"inlineSources\": true,\n    \"sourceMap\": true,\n    \"sourceRoot\": \"/ava-fixture\"\n  }\n}\nThis is the setting I tried to get the source maps play nice on the browser.\nWhen I removed it, the coverage info starts showing up.\nNeed to figure out how to do the source map translation on webpack.... When I Ctrl-C, it reports error like:\nsh\nError: ENOENT: no such file or directory, open 'E:\\hwong\\panda\\module-loader\\node_modules\\systemjs\\src\\instantiate.js'\n    at Object.fs.openSync (fs.js:558:18)\n    at Object.fs.readFileSync (fs.js:468:33)\n    at module.exports (E:\\hwong\\panda\\module-loader\\node_modules\\ava\\lib\\code-excerpt.js:15:20)\n    at runStatus.errors.forEach (E:\\hwong\\panda\\module-loader\\node_modules\\ava\\lib\\reporters\\mini.js:191:22)\n    at Array.forEach (native)\n    at MiniReporter.finish (E:\\hwong\\panda\\module-loader\\node_modules\\ava\\lib\\reporters\\mini.js:178:21)\n    at Logger.finish (E:\\hwong\\panda\\module-loader\\node_modules\\ava\\lib\\logger.js:38:28)\n    at busy.api.run.then.runStatus (E:\\hwong\\panda\\module-loader\\node_modules\\ava\\lib\\watcher.js:112:12)\nFrom previous event:\n    at Watcher.run.specificFiles [as run] (E:\\hwong\\panda\\module-loader\\node_modules\\ava\\lib\\watcher.js:110:68)\n    at Watcher.rerunAll (E:\\hwong\\panda\\module-loader\\node_modules\\ava\\lib\\watcher.js:270:8)\n    at new Watcher (E:\\hwong\\panda\\module-loader\\node_modules\\ava\\lib\\watcher.js:130:8)\n    at Object.exports.run (E:\\hwong\\panda\\module-loader\\node_modules\\ava\\lib\\cli.js:152:20)\n    at Object.<anonymous> (E:\\hwong\\panda\\module-loader\\node_modules\\ava\\cli.js:22:24)\n    at Module._compile (module.js:571:32)\n    at Object.Module._extensions..js (module.js:580:10)\n    at Module.load (module.js:488:32)\n    at tryModuleLoad (module.js:447:12)\n    at Function.Module._load (module.js:439:3)\n    at Module.runMain (module.js:605:10)\n    at run (bootstrap_node.js:420:7)\n    at startup (bootstrap_node.js:139:9)\n    at bootstrap_node.js:535:3\nSeems like ava tries to load file using the sourcemap path?\nsystemjs/src/instantiate.js does not exist. It only has systemjs/dist/... with sourcemap.. Correction: the above case is an issue on the test code. Once the test code is fixed, \"r\" started working.\nHowever, the problem is still valid, and it seems like \"magic-assert\" tries to do something but couldn't, thus crash/block ava from continue to function in watch mode.. Yes, it was an error thrown from systemjs. I have also encountered other cases that when an error is thrown, ava 0.18 just print out \"1 failed\" but didn't print out the thrown error (and \"r\" is not working in some cases), while in 0.17 the error message is printed.\nUPDATE: the error thrown by systemjs is not related to what's printed by ava above.\nIt that case (when I run that test), the error is actually related to relUrl is not defined.\n\nbut the original source is not in the systemjs package.\n\nCorrect.. That one I can't. It's company code. I'll see if I can get the same behavior on one of my open source projects and create a branch for this.. Adding some info:\nIt seems to break the watch mode when an error is thrown during an async test.. You can test this from here: https://github.com/unional/some-issues/tree/ava-0.18-watch\nTo run it:\nsh\ngit clone https://github.com/unional/some-issues.git\ncd some-issues\ngit checkout ava-0.18-watch\nnpm install\nnpm test\nUPDATE: Update branch for this issue.. > @unional could you try #1242? It should crash the watcher when this error occurs.\nForgot to reply on this. Yes I tried it, but it still \"crash\" the watch mode.. Seems like an interesting way. But IMO is confusing and a step too far. \nIt is an assertion that compares two objects. So I think it should just show what's the difference, I.e like git diff, rather then suggesting how to fix the assertion. \nThis is not consistent to t.is() which just throw and say they don't match, instead of saying \"you should change actual to expected. . Tested and seems like it still hang the watch mode.. Async test\nUPDATE: my test are async, I think that makes a difference here.. Thanks.\nI'm thinking of this in the context of BDD, and IMO the benefit of test.each() over macros is that the cases are included within the test.\nSpeaking of each, with the power to power-assert and magic assert, do you think it is possible to generate a markdown spec doc from the JS file?\nUPDATE: this is just an idea.  An alternative is to provide functions allowing ava to be hooked into BDD system such as shouldit. Thank again.\nAfter some more thoughts, it does seem like just relying on ava to do this is not sufficient.\nA layer on top of it is much more desirable.\nThe BDD system I envision is not the BDD syntax in JS test framework such as in mocha, but a system that actually allows Business Analyst, Product Manager, QA, to write acceptance test that they can reason about (similar to fitNesse).\nThese tests can be wired up to the actual tests in ava to determine if they pass or fail.\nThis means the system need to understand before, beforeEach, after, and afterEach code and reuse them.. One thing that is \"lacking\" is the ability to nest test.\nOn the other hand, my understanding is that ava choose to not do nesting.\n. Closing for now as we are in 0.19. FYI on Jest using workers and also spread work based on the previous test duration:\nhttp://facebook.github.io/jest/blog/2016/03/11/javascript-unit-testing-performance.html\n. I'm new to TTY and working on #1533 , what is the use case that TTY is false? . Just want to understand it a bit more, because I'm not aware that Jest distinguishes between TTY and non-TTY mode.. Enter reruns the filtered list of tests. They use a to rerun all tests.. Jest just process.exit(0)\nhttps://github.com/facebook/jest/blob/9c429cd15d639ab42cfe0a9fa7032095af3b675f/packages/jest-cli/src/watch.js#L143\nMy guess is that they put the watcher in the main process?\nI add this PR as a prelude of #1530, so that user can press 'p' and then type in the filter string and 'Enter' to execute.. Once we figure out how to solve the double [Ctrl-C], I can change this PR to use keypress similar to #658 to get the code coverage up.. > To clarify, we're blocked until that's resolved, yes?\nYes. Need to figure out how to handle that. In jest, it just process.exit(0). Is there some special handling in ava?. I think this can be better addressed when the code is refactored. I'll give it a try on that route first before getting back to this. \ud83c\udf37 . I think it might. My idea is to make the test-worker independent from the context thou. i.e. it should contain logic to run test, without knowing it is running in process or vm.\nThen, you can consume it in process or vm by just using an adapter to hook up the events from an event emitter to the process or vm. Sure. How to stop those workers?\nBy the way, I saw in test-worker.js:\njs\nprocess.on('ava-exit', () => { process.exit(0); })\nSo the watcher should fire that even in all sub-processes?. Yes. The code still needs some love. observeStdin is returning a promise that is not rejecting.\nWill fix that.. ",
    "okcoker": "I have no babel section. Unless of course you mean the dependencies in which case I've updated the original post\n. @novemberborn I think even a warning/instructions would be a good first step. I was removing my node_modules and changing a bunch of things back and forth for a while, not knowing what I actually had done wrong or right.\n. ",
    "KeKs0r": "@develar It does not work for me. I am using this version of webstorm:\nWebStorm 2016.2\nBuild #WS-162.1121.31, built on July 9, 2016\nJRE: 1.8.0_76-release-b216 x86_64\nJVM: OpenJDK 64-Bit Server VM by JetBrains s.r.o\nAnd Ava from master, but my debugger just never stops. I was trying with single file debugging (without match):\n\nNot sure why the debugger never stops. It also does not terminate.\n. ",
    "richchurcher": "Just because I couldn't find this anywhere else: when using VS Code as a debugger, you can coerce AVA to behave by using @sindresorhus 's suggestion as part of a launch.json config:\njson\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"type\": \"node2\",\n            \"request\": \"launch\",\n            \"name\": \"Launch Program\",\n            \"program\": \"${workspaceRoot}/tests/index.test.js\",\n            \"cwd\": \"${workspaceRoot}\",\n            \"sourceMaps\": true,\n            \"runtimeArgs\": [\n                \"${workspaceRoot}/node_modules/ava/profile.js\"\n            ]\n        }\n    ]\n}. ",
    "ImTheDeveloper": "Worth mentioning that this setup worked for me but the one bit I had missing was that I forgot to install the webstorm debug helper in chrome https://chrome.google.com/webstore/detail/jetbrains-ide-support/hmhgeddbohgjknpmjagkdomcpobmllji?hl=en-GB. ",
    "dohomi": "@develar @mightyiam did you guys managed to test AVA tests in Webstorm with the current 2017.2? I use the configuration mentioned in this document: https://github.com/avajs/ava/blob/master/docs/recipes/debugging-with-webstorm.md\nIt seems that Webstorm is starting the debugging mode with: --inspect-brk=INT and due to this it results in:\naddress is already in use. @novemberborn yes I think this is better a approach \ud83d\udc4d . Thanks for merging. ",
    "melisoner2006": "I was also having problem with debugging ava unit tests with Webstorm 2017.2. It didn't stop at the break points. Adding inspect-brk to node options fixed the problem. Here's my SO question & answer. Thanks @develar  and @mightyiam  for mentioning the node options.. @novemberborn I'd love to. I'll get on it now.. @Gregoyle Yes, my project was ES6. . @novemberborn --inspect does work for me, but there's risk of using it. --inspect-brk stops before the user's script starts. --inspect flag doesn't guarantee that: if there are breakpoints early in the entry point they might be skipped until the debugger is attached. At least, this is my understanding.. @develar Thanks for pointing out the node version. I updated my statement to include a solution for both 7+ and previous versions.\nI also removed the statement about --inspect-brk=port. There are other flags that can also be used and I thought this recipe is not the place to explain all node debugger flags. . ",
    "Gregoyle": "@dohomi Did you ever get this working? I'm having a hard time getting the WS debugger to work. It could be related to the fact that our project is using babel. @melisoner2006 Was your project ES6 or something else?. ",
    "sveisvei": "Not released yet, any release incoming soon?\n. ",
    "ai": "Wait for release too\n. @pvdlg show your test. We removed Error from CssSyntaxError parents because of Babel error. I can try to add it back.. Done https://github.com/postcss/postcss/commit/0586a53cfc6a92f538b27e2d5c83c8d3c3bb09fa. Released in 7.0.7. ",
    "adjavaherian": "Just noticed what might be a related issue while testing a React component with Enzyme and Ava's native assertions.\nBelow, CustomOption is a child component, and I expect wrapper.contains to return true or false:\n``` jsx\ntest('renders children when passed in', t => {\n    const wrapper = shallow(\n        \n\n\n    );\n    t.true(wrapper.contains());\n});\n```\nBut I end up with this and NPM bails out when the watcher returns 1:\n``` sh\nSyntaxError: Unexpected token (1:24)\n    at Parser.pp.raise (/Users/Amir/Desktop/Nestio/pam/node_modules/acorn/dist/acorn.js:923:13)\n    at Parser.pp.unexpected (/Users/Amir/Desktop/Nestio/pam/node_modules/acorn/dist/acorn.js:1490:8)\n    at Parser.pp.parseExprAtom (/Users/Amir/Desktop/Nestio/pam/node_modules/acorn/dist/acorn.js:333:12)\n    at Parser.parseExprAtom (/Users/Amir/Desktop/Nestio/pam/node_modules/acorn-es7-plugin/acorn-es7-plugin.js:136:30)\n    at Parser.pp.parseExprSubscripts (/Users/Amir/Desktop/Nestio/pam/node_modules/acorn/dist/acorn.js:228:19)\n    at Parser.pp.parseMaybeUnary (/Users/Amir/Desktop/Nestio/pam/node_modules/acorn/dist/acorn.js:207:17)\n    at Parser.pp.parseExprOps (/Users/Amir/Desktop/Nestio/pam/node_modules/acorn/dist/acorn.js:154:19)\n    at Parser.pp.parseMaybeConditional (/Users/Amir/Desktop/Nestio/pam/node_modules/acorn/dist/acorn.js:136:19)\n    at Parser.pp.parseMaybeAssign (/Users/Amir/Desktop/Nestio/pam/node_modules/acorn/dist/acorn.js:112:19)\n    at Parser.pp.parseExprList (/Users/Amir/Desktop/Nestio/pam/node_modules/acorn/dist/acorn.js:660:23)\n    at Parser.pp.parseSubscripts (/Users/Amir/Desktop/Nestio/pam/node_modules/acorn/dist/acorn.js:252:29)\n    at Parser.pp.parseExprSubscripts (/Users/Amir/Desktop/Nestio/pam/node_modules/acorn/dist/acorn.js:231:15)\n    at Parser.pp.parseMaybeUnary (/Users/Amir/Desktop/Nestio/pam/node_modules/acorn/dist/acorn.js:207:17)\n    at Parser.pp.parseExprOps (/Users/Amir/Desktop/Nestio/pam/node_modules/acorn/dist/acorn.js:154:19)\n    at Parser.pp.parseMaybeConditional (/Users/Amir/Desktop/Nestio/pam/node_modules/acorn/dist/acorn.js:136:19)\n    at Parser.pp.parseMaybeAssign (/Users/Amir/Desktop/Nestio/pam/node_modules/acorn/dist/acorn.js:112:19)\n```\nConversely, the following is fine.\n``` jsx\ntest('renders children when passed in', t => {\n    const wrapper = shallow(\n        \n\n\n    );\nconst truthy = wrapper.contains(<CustomOption />);\nt.true(truthy);\n\n});\n```\nCLI\nCONFIG=$(pwd)/webpack/webpack.test.config.js BABEL_DISABLE_CACHE=1 NODE_ENV=ava ava \"--watch\"\nRelevant Links\nhttps://github.com/istarkov/babel-plugin-webpack-loaders\nENV\nAva 0.15.2\nnode 4.4.5\n. ",
    "lukechilds": "Thanks for that example. That just sticks everything on to global though, I still think it would be beneficial to be able to provide an array of properties to be used. That way if your browser lib has no deps you can safely require what you need and be less likely to experience issues with node dev dependencies that may get confused by browser globals (like in that issue). Plus I'd rather not create a gazillion global variables if I don't have to. \ud83d\ude06 \nThere wouldn't be any extra stuff to install compared to the current recipe. Currently it's:\nshell\n$ npm install --save-dev jsdom\ntest/helpers/setup-browser-env.js\njs\nglobal.document = require('jsdom').jsdom('<body></body>');\nglobal.window = document.defaultView;\nglobal.navigator = window.navigator;\nAnd I'm proposing to change it to:\nshell\n$ npm install --save-dev node-browser-environment\ntest/helpers/setup-browser-env.js\njs\nrequire('node-browser-environment')();\nor if you know exactly what you need:\njs\nrequire('node-browser-environment')(['window', 'document', 'navigator']);\nIf you'd rather I just added this to the recipe I'm happy to do this but I thought this seemed like a good Lego block.\n. @vdemedes What do you want me to do with this?\n. @vdemedes Hey, just going through old open issues/PRs. Do you want me to do anything with this or shall I close it?\n. No probs, I'll wait for more feedback before changing anything.\nHave a great holiday! \n. @philmill Awesome, thanks!\nAt the risk of sounding like an idiot, what do you mean by DSL? Domain-specific language?\n. @novemberborn Yeah sure, I'm really busy atm but will try and find some time this weekend to write something up \ud83d\udc4d \n. @novemberborn Yeah sure, I'm really busy atm but will try and find some time this weekend to write something up \ud83d\udc4d \n. @novemberborn Sorry, didn't get a chance at the weekend, just submitted https://github.com/avajs/ava/pull/1054\n. @sindresorhus Yep, I've pinned the version number for jsdom to stop any jsdom breaking changes breaking AVA tests and I've got Greenkeeper set up to notify me of jsdom updates. If you look through the commit history you'll see I normally update this within 24 hours.\nI've got no intention of abandoning this module but I'm happy to add you or anyone else on the AVA team as a maintainer if you want.\nWorking on your requested changes now \ud83d\udc4d \n. @sindresorhus Done requested changes \ud83d\ude42\n. This should be good to go now.\nMight be worth also adding a simpler example just to illustrate the browser globals are available? The current React example hides all the DOM stuff inside React.\n. I'm thinking something along the lines of:\ntest/my.dom.test.js:\n``` js\nimport test from 'ava';\ntest('Insert to DOM', t => {\n  const div = document.createElement('div');\n  document.body.appendChild(div);\nt.is(document.querySelector('div'), div);\n});\n```\nIt could be as well as the current React test or instead of. What do you think?\n. @sindresorhus Done \ud83d\udc4d \n. Regarding the two main points in that link, the docs do recommend only requiring the properties needed to prevent unnecessary globals and I don't think attaching them to global is as big of an issue as it normally would be due to the way AVA runs each test file in a separate process.\nThat said, I'm aware this isn't the cleanest solution in the world. To match the example I think we would have create a new jsdom instance inside every test because they all run async in AVA. It would also require that the module is first built for the browser so it can be injected in via a script tag.\nI feel like the current solution gets you 99% of the way there in one function call and makes sense until AVA is running natively in the browser.\n. I understand your point. I'm not quite sure how you would get the react example working in the current recipe. It loads extra modules for testing which would also need to be built for browsers.\nI don't disagree with what you're saying, I still think this method is ok to easily test simple modules, but maybe it would be better to not recommend browser-env because more complex modules may end up passing tests by using Node.js functionality that wouldn't be available in a sandboxed jsdom environment.\n. I can see your concern. Maybe we should add a disclaimer in the browser recipe that makes it clear that this is not recommended and is a temporary workaround until AVA is supported in the browser.\nI've just added a disclaimer to the browser-env repo linking to the above wiki page: https://github.com/lukechilds/browser-env\n. Oh, the callback one was correct, I just needed to not do an async function to stop the Promise error.\nFor anyone who ends up here, the passing test is:\njs\ntest.cb('Promise resolves with local window', t => {\n  t.plan(1);\n  const config = {\n    html: '',\n    onload: window => domLoaded(window.document).then(() => {\n      t.pass();\n      t.end();\n    })\n  };\n  jsdom.env(config);\n});. Yeah, the background process stayed running and did actually finish eventually and dump the diff to my console. So it wasn't crashing, just took ages. Like literally ~20 minutes.\nThat Google lib does it in <2s.\nNot promising anything but I'll give a PR a bash :). Well that was easier than expected.... Ah, I chose that module as it was recommended in the issue: https://github.com/avajs/ava/issues/1280#issuecomment-282258975\nDidn't realise it was unmaintained, I'll have a look for some others.\n\nAlso, can we swap the usage of 'diff' to the other library, so that we don't have 2 dependencies that basically do the same thing?\n\nOnly if it supports generating a patch. The diff lib was being used to generate a diff for strings and a patch for arrays/objects.. The only other I've found is: https://github.com/liddiard/text-diff\nIt's also pretty outdated and is just a wrapper around the same Google lib that the other project is a clone of: https://code.google.com/p/google-diff-match-patch/. No probs \ud83d\udc4d. @novemberborn Is it not already fixed in https://github.com/avajs/ava/pull/1285/commits/d402277ed8670eec5a4276ff9236d12589046959?. I like to call that squirging. No problem \ud83d\udc4d. No probs, happy to submit a PR but I don't seem to be able to figure out where it's coming from.\nThis is the code for the diff:\n```js\nif (err.actualType === 'string' && err.expectedType === 'string') {\n  const patch = diff.diffChars(err.actual, err.expected);\n  const msg = patch\n    .map(part => {\n      if (part.added) {\n        return chalk.bgGreen.black(part.value);\n      }\n  if (part.removed) {\n    return chalk.bgRed.black(part.value);\n  }\n\n  return part.value;\n})\n.join('');\n\nreturn Difference:\\n\\n${msg}\\n;\n}\n```\nAs you can see the section for the neutral text applies no colouring: return part.value;. So it's weird that it's red the first time it's returned but not the other times. That makes me think that it's inheriting it from a previous unclosed ANSI colour code, however the string is interpolated into the same string that starts with the word Difference::\njs\nreturn `Difference:\\n\\n${msg}\\n`;\nBut Difference: isn't red. Any ideas?. Ok, so that flips the problem round, it's the white text that is incorrect. Instead of the neutral colour being white it should be red.. Hmmn, I'm struggling to think of a clean way to fix this. Obviously a really simple fix would be to just colour the neutral text red, but then if the string pretty-format colour changes then it'll be wrong.\nSo we could check the default pretty-format colour and use that, but then if pretty-format was ran with custom colour options we won't know and again the colour will be wrong.\nIdeally we need to run the diff on an unformatted string, and then run the output through pretty-format. But the diffs for objects and arrays need to be ran through pretty-format first. I think the neatest solution would be to save a copy of the raw value in the err object when it's getting formatted. So inside the diff we can access the raw value. The error would be exposed as:\njs\n{ name: 'AssertionError',\n  actualRawValue: 'foobuzz',\n  actualType: 'string',\n  actual: '\\u001b[31m\"foobuzz\"\\u001b[39m',\n  expectedRawValue: 'foobar',\n  expectedType: 'string',\n  expected: '\\u001b[31m\"foobar\"\\u001b[39m',\n  operator: '===',\n  message: ' ',\n  generatedMessage: false,\n  stack: 'AssertionError:  \\n    Test.t [as fn] (test/visual/console-log.js:4:4)\\n    ',\n  showOutput: true,\n  source:\n   { isDependency: false,\n     isWithinProject: true,\n     file: '/Users/lukechilds/Dev/oss/ava/test/visual/console-log.js',\n     line: 4 } }\nactualRawValue and expectedRawValue are my additions. Thoughts?. If you're ok with hardcoding the red colour in format-assert-error.js then that's all that's needed. It's not necessary to strip any ANSI codes out first.. > I'll take care of it later by creating a \"theme.js\" with colors passed to pretty-format.\nPerfect \ud83d\udc4c\n\nWouldn't it end up with duplicate red color ansi codes?\n\nYes of course sorry, I've just got in after a few \ud83c\udf7b. Sweet, just force pushed the strip-ansi update to the PR \ud83d\udc4d. Fixed test and updated require position \ud83d\udc4d . Thanks :)\nI considered that but decided it was best to leave as is because, while window is definitely the ideal solution, it's only viable if they aren't using 3rd party browser-only modules and if they're happy to make a slight modification to their module. I think that instantly removes a large chunk of the people that will be reading the recipe.\nAlso I think it could cause some confusion to less advanced users. They may try and use window not understanding why they should be using browser-env and get cryptic a is not defined errors from minified browser libs. I think browser-env should be the default because it'll always work and then the users who are able to use window can opt in.. Just put this together to demo the functionality: https://github.com/lukechilds/get-root-module\nNo docs or anything yet but pretty obvious what it's doing from the source.\nI'll submit a PR to demo the functionality in AVA.. @ORESoftware I explained in more detail in the PR #1461\n\nTraverses up the directory tree from process.cwd() (whatever dir ava was executed from on the cli). When a dir with a package.json is found, that dir is required, if no package.json is found module will be undefined.. @ORESoftware \none alternative, is linking your project to itself with npm link\n\nThat works well on your personal installation but isn't portable. Tests will break on CI and on others machines if they git clone && npm install && npm test.\n\nno module can be returned/required, so you'd have to handle the case where nothing can be loaded\n\nThat's what the try/catch does.\n@novemberborn \nYeah, no reason why users can't install get-root-module directly. I just thought the import { test, module } from 'ava'; syntax was really neat, but you raise a valid point about ES modules.\nI think this would be a nice addition but I'm happy using get-root-module directly if you're worried about ES module compatibility.. Heheh, about to head to a festival for the weekend put I can put a simple recipe together when I'm back \ud83d\udc4d. Ok so I've cleaned this up and added docs.\nhttps://github.com/lukechilds/get-root-module\nI also just came across a use case where I needed to require the current module by name to test a dynamic require and came up with a pure JavaScript solution without using npm link.\nhttps://github.com/lukechilds/requireable\nWorks nicely with AVA too: https://github.com/lukechilds/requireable#ava-usage\nSo currently these are both documented in their own repos and should work standalone with AVA or any other test runner. Happy to also submit a PR to AVA with a recipe for how to use both of these solutions with AVA examples if you think that would be helpful.\nAlso, I still think get-root-module would be nice to have exported from AVA. The ES module argument is valid but I personally think it's not a deal breaker because:\n\nIt's not gonna happen instantly, it should be well documented over time and so we'll have plenty of time to implement a fix\nES modules will break a lot of stuff so there will be lots of other people fixing the same problem, we may well be able to drop another fix straight in\nI'm committed to maintaining get-root-module so happy to update that with import support if that's all that's needed. Or submit a PR to AVA if we need to update something on this end too\n\nI just really enjoy how neat and succinct everything is with AVA and I feel like this really fits in well with the way AVA works. Every single test file requires importing at least test and the module you're testing, and often nothing else. Being able to do this with a single import seems super clean to me.\nNote: These packages are gonna be renamed soon\nget-root-module is a pretty verbose name, gonna rename to this so you can import foo from 'this';. @rickyclegg said he's happy for me to use it, just waiting on the transfer.\nAlso just realised requireable is a typo (I'm dyslexic \ud83d\ude11\ud83d\udd2b) requirable seems abandoned, hopefully I can get that.. @novemberborn\n\n\nAVA already knows the root, so we could just expose that instead. (See projectDir here)\n\nIs that useful for your libraries @lukechilds?\n\nNot really because currently they work completely independently from AVA, so they can be used with any other test runner. Could be an option if we decide to integrate it into AVA in the future though.\n@sindresorhus Cool, I'll put a recipe together and see how it goes from there.. @novemberborn I am indeed, it's on my todo list.\nTotally swamped with paid work atm, deadline is next Thursday so by then I'll either be dead from exhaustion or back on OSS full time, one of the two \ud83d\ude43. ```js\nconst conf = pkgConf.sync('ava');\nconst filepath = pkgConf.filepath(conf);\nconst projectDir = filepath === null ? process.cwd() : path.dirname(filepath);\n```\nWouldn't that default to process.cwd() if there's no AVA conf? Which would mean if I was in /project-root/src/helpers and then ran npm test that dir would be required rather than traversing up to /project-root?. Actually I see why this may be a problem. Someone may want to completely overwrite the default @ava/stage-4 with their own presets. So maybe merging isn't the best solution.\nMy original issues are still relevant though, I think it would be better if there was a way to extend the default AVA babel config without having to copy the defaults.. What about an extend option?\nSomething like:\njs\n\"ava\": {\n  \"babel\": {\n    \"extend\": {\n      \"plugins\": [\"transform-react-jsx\"]\n    }\n  }\n}\nThe above config makes it clear I always want the default AVA babel config, but just extend it with \"transform-react-jsx\".\nI'm not keen on that exact example as it's not clear how it should be handled if both ava.babel.extend and ava.babel.preset were set but I think that's kinda in the right direction.\nWhat do you think?\n\nAnother approach may be to explicitly re-export @ava/stage-4 under ava/stage-4, so that you don't feel like you have to install it as a devDependency.\n\nThe issue wasn't so much installing the dev dependency, it was hardcoding stage-4. For example even if you export through AVA and I just set the preset to ava/stage-4, if in the future AVA decides to use a different default preset I'm gonna be stuck on stage-4 or get an error if that file gets deleted.\n. js\n\"ava\": {\n  \"babel\": {\n    \"plugins\": [\"transform-react-jsx\"],\n    \"presets\": [\"ava\"]\n  }\n}\nExpresses the intent perfectly IMO.\nIs it an issue that it can't be used in .babelrc? It's only meant to change the babel config for tests not for the project.\nIf so, what about:\njs\n\"ava\": {\n  \"babel\": {\n    \"extendsDefault\": true,\n    \"plugins\": [\"transform-react-jsx\"]\n  }\n}. > I think it's misleading. Users may be tempted to move it into an external file.\n\ud83d\udc4d \n\nLet's assume extendsDefault is the default. Would setting it to false also disable @ava/transform-test-files? Perhaps we can pick a negated name, e.g. disableAvaPresets.\n\nExtending by default and having a disableAvaPresets sounds like the best solution so far. I just assumed it would extend by default and was pretty stumped why everything broke until I checked the docs. This would be a breaking change though right? Because after this change some people will get @ava/stage-4 who didn't previously.\n\nIf we go this route, perhaps the \"inherit\" setting should be equivalent to {\"disableAvaPresets\": true, \"babelrc\": true}?\n\nThat makes sense to me.. I'm happy to take it on, however I've spent quite a bit of time on OSS recently and ideally need to focus on paid work for a bit so can't guarantee it'll be done soon.. @vadimdemedes You mean so we could have this: https://github.com/avajs/ava/issues/1488#issuecomment-321992658?\nIf the .babelrc issue that @novemberborn raised isn't a concern then yeah, I think that's a great solution.. js\n\"ava\": {\n  \"babel\": {\n    \"plugins\": [\"transform-react-jsx\"],\n    \"presets\": [\"ava/babel\"]\n  }\n}\nis equally beautiful to me \u2764\ufe0f. Just checking, you're saying you want the React section removed, not updated to use ReactJSDOM, right?. Sorry, commits are a bit messy because I used this branch for a PR previously. Squash + merge will do the job.. So maybe 2 on CI? That way we don't hold everything up for one slow test file. Even one CPU should be fine swapping between two threads.\nAnd when not on a CI I'd say go even further. Maybe cap at 8. Majority of the time if this is run locally it'll be on bare metal so os.cpus().length will be safe to use and limit to machine thread count anyway.\nAlthough we'll encounter the same issue on local machines if you're running your tests in docker. But then users are less likely to have a large number of threads (most likely 4 - 8) so that's unlikely to cause issues.\nAlternatively, as this seems mostly related to Docker, we could set the limits with an is-docker check. Otherwise, trust os.cpus().length`.. Done, can be rewritten as\njs\nlet concurrency = Math.min(os.cpus().length, process.env.CI ? 2 : 8);\nbut thought maybe that's too much for one line?. Yeah, done.\nAlso in the docs:\n\nBy default, AVA will use as many processes as there are CPU cores in your machine.\n\nTechnically this should be threads not cores. Do you want me to change that?. Node.js docs are misleading, it's definitely threads:\n```\n$ sysctl -n machdep.cpu.brand_string\nIntel(R) Core(TM) i5-5257U CPU @ 2.70GHz\n$ sysctl -n machdep.cpu.core_count\n2\n$ sysctl -n machdep.cpu.thread_count\n4\n$ node -e \"console.log(require('os').cpus().length)\"\n4\n``. Good idea, done \ud83d\udc4d . @novemberborn Just to clarify, it only caps the default fromos.cpus().length`. You can still manually specify whatever concurrency you want for CI or local.\nBut yeah, like I mentioned in an above comment, it's unlikely to cause issues locally unless they have a really high thread count.\n\nMajority of the time if this is run locally it'll be on bare metal so os.cpus().length will be safe to use and limit to machine thread count anyway.\nAlthough we'll encounter the same issue on local machines if you're running your tests in docker. But then users are less likely to have a large number of threads (most likely 4 - 8) so that's unlikely to cause issues.. @novemberborn was not aware of is-ci. Looks like a more robust solution \ud83d\udc4d. I chose it because I thought that was the exact string most people would search for trying to find something like this.\n\nbrowser-env is already taken, in hind site browser-environment would probably of been better. I'm reluctant to change it now as it's already had a fair amount of downloads and I don't think this can be changed in a backwards compatible way.\n. Good spot on the deprecation, yeah I'll rename it. If you wanna hold off on merging this for now I'll update you when it's sorted \ud83d\udc4d\nQuick Q: Why bump major first? Wouldn't it be better to notify existing users? If they've installed with the default ^ range they'll be expecting to get SemVer minor/patch updates automatically.\n. @sindresorhus node-browser-environment is dead, long live browser-env! \ud83c\udf89\n. \ud83d\udc4c . Woops, sorry running on 2 hours sleep \ud83d\ude2c. ",
    "kripod": "I'm surprised about that, because IntelliSense in VS Code provides great DX while writing tests:\n\n. Press Cmd + Shift + P and then input \"Tasks: Configure Task Runner\" into the box.\nIf the task runner was configured correctly, you can see a file appearing at .vscode/tasks.json with content similar to this:\njson\n{\n  \"version\": \"0.1.0\",\n  \"command\": \"npm\",\n  \"isShellCommand\": true,\n  \"showOutput\": \"always\",\n  \"suppressTaskName\": true,\n  \"tasks\": [\n    {\n      \"taskName\": \"build\",\n      \"args\": [\"run\", \"build\"],\n      \"isBuildCommand\": true\n    },\n    {\n      \"taskName\": \"watch\",\n      \"args\": [\"run\", \"watch\"],\n      \"isWatching\": true\n    },\n    {\n      \"taskName\": \"test\",\n      \"args\": [\"run\", \"test\"],\n      \"isTestCommand\": true\n    }\n  ]\n}\nThe existence of this file lets you quickly build and test projects by pressing Cmd + Shift + B or Cmd + Shift + T while using VS Code. Task output can be viewed in the \"Output\" window.\n. By the way, the --verbose flag does not seem to help...\n. You can also run tests by pressing Cmd + Shift + P and then inputting \"Tasks: Run Test Task\" to the box. That should invoke the npm test command, as specified in .vscode/tasks.json.\n. The --verbose flag still doesn't output anything for me on Windows.\n\n. Thank you very much for your assistance! :smile:\n. It seems that new insider builds of VS Code have fixed this issue. Thanks for the assistance, especially for the tip about --verbose!\n. Thanks for the tip, I saw that project and thought that it would add too much boilerplate to my test suite.\n. I didn't try increasing the timeout, but it seems to me that 6 out of 6 tests have passed, just like in my v5 and v6 Travis builds.\n. Node v6 build output can be found here. The amount of passed tests are the same.\n. I'm not sure that assertion planning will solve my issue. Especially because of the following quote from the readme:\n\nNote that, unlike tap and tape, AVA does not automatically end a test when the planned assertion count is reached.\n. You were most probably right. After setting the test timeout to 30 seconds, my Node v4 build has passed, and it turns out that the npm test command ran for a total of 21.79s. One test could've taken too much time to be run on CI.\n\nThank you for helping in the resolution of this issue! :blush:\n. ",
    "kozzztya": "That can help but promise can't be resolved multiple times instead of callback. That restraint tests. \n. That make sense.\n. Thanks for answers but my before hooks still not work serially. \nI have db clearing in my before hook that's why I can't run my test parallel. \nIt would be great to have one modifier for many tests of for one file. Like:\njs\ntest.serial('all stuff inside of me is serial', t => {\n    test(t => {});\n    test(t => {});\n});\n. ",
    "arings": "Hello. Can I take this and continue, please?\n. I am very sorry for that. AVA is new to me, and I am new to new technologies.  As I understand correctly - old screenshot was sufficient, so I put it instead.\n. 'Optional TAP output' >> 'TAP reporter',\nlinks in 'Why AVA?' updated, \nMini-reporter mentioned that it's the default reporter,\nVerbose reporter mentioned that it can be selected with the --verbose flag,\nReporters section added to the Table of Contents.\n. ",
    "Scrum": "@jfmengels if I take the keys then the order will not be equal\njs\nt.deepEqual(Object.keys({a: {}, b: {}}), Object.keys({b: {}, a: {}}));\n. @jfmengels  Thanks, you right. :+1: \n\nIf you want to compare the list of keys, then you should sort them.\n. \n",
    "jimthedev": "@nfcampos So, quick question about the ava docs and this PR, specifically in the React recipe for JSX helpers there the following note:\n\nNote that you have to use variables like actual and expected because power-assert doesn't handle JSX correctly.\n\nThe text links to this issue which although open (documentation still pending) states that it will be available in ava as soon as this PR (#903) is merged. Since #903 has been merged, do we need to remove that note from the React recipe? \n. ",
    "andreasgrimm": "\nYour best bet is to install from master for now\n\nThat's what I did until today I saw that the typings were gone and now my CI builds fail. Any hint on how to work around this in a quickly fashion?\n. Any chance the above mentioned \"error\" get's fixed in a version that can be installed without any special means ?\n. With the make-ts available now, I guess it won't take too long, will it ;)\n. ",
    "rymohr": "Sounds like you've made up your mind. But for what it's worth, it's not so much better docs on how to structure your project. For me it was the idea of bringing ava in to unit test small chunks of a very large existing project with an established file structure. I like my tests to live side-by-side with the files they are testing and the current helper exclude policy forced me to restructure my project in order to use ava.\nAlso, by renaming the issue you've made it hard for anyone in the same boat to find this issue in the future. Renaming issues for clarity is fine, but please don't use renaming to change the intent of the issue.\n. If it doesn't make sense to relax the exclude rule from helpers to test/helpers then it would definitely be nice to be able to override the ignore.\nHere's an example of where we ran into this problem:\nclient/\n  modules/\n    surveys/\n      helpers/\n        prepareScreen.js\n        prepareScreen.test.js\nIn our case we just renamed helpers to utils to get around the issue for now but I still think it's strange that ava chokes on this by default. The node_modules exclude is a smart default but I feel like the helpers exclude is a special case that should be opt-in.\n. Hard for me to offer any input on whether it's worth it or not since I still don't understand what \"helpers\" are within ava. I prefer to name my tests with an explicit extension so I don't have to worry about where they are within the project (which also avoids the need for the helper exclude).\n. @jamestalmage sounds like a good plan.\n@novemberborn can you explain the original reasoning behind the _ prefix rule? I don't see the obvious connection between this file begins with an underscore and this is not a test file.\n. @novemberborn never mind! Just saw James' earlier comment:\n\nso a _ prefix, or a helpers directory allows you to share that common code, even though it's in a directory where everything else is considered a test\n. \n",
    "Hypercubed": "Here is a gist containing everything discussed: https://gist.github.com/Hypercubed/96db4034dbf80317c3e9d2864265b519\n. @jamestalmage Understood, I had no expectation of fixing this issue when not using --tap --serial but would think the tap reporter could be made consistent.\n@sindresorhus From what I understand it is up to the TAP consumer to decide what to do with extra output. scottcorgan/tap-out treats extra stuff as comments and, therefore, so does zoubin/tap-summary and Hypercubed/tap-markdown.  If the output is stderr then it is not consumed at all.  Other tap producers do not do this.  Again, I'm only considering the tap reporter without concurrency.\nThe reason I'm interested in this because I am writing to console.log when running Chuhai benchmarks then using tap-markdown to generate clean reports (see an example here).\nThis works well, using tape or blue-tape, and would like this to also work in ava.\n. Yes, I think many of the TAP consumers are relying on undocumented behavior.  According to spec the test title is also to be ignored.  What if the output from within the test is a yaml block?  How do we ensure that goes to stout?\n. I would like Chuhai to be compatible with ava and others without custom behavior (i.e. if (ava) ....).  But if I need to I can.  One option is to use a t.comment() if that becomes available in ava, and writes to stdout.\n. ",
    "Alxandr": "Alternatively, stdout and stderr could be hijacked in the test process itself (when running in serial, or probably even with a new cli switch). It could then be included in the event sent to the test runner. Something to the effect of ava --capture-stdio results in tests inside one file running in serial (no problem if multiple test files run in parallel), whenever a test completes, a single event is sent to the testrunner that includes the test data including the stderr and stdout that happened during that test.. Another issue with the TAP output I've had is that if I run it through tap-spec, it produces a \"grouping\" per test. Here is a sample output:\n\nI've tried consuming the TAP output of ava in both tap-parser and tap-out, and tap-out just falls flat on it's head in some cases with AVA output. It parses the exception details wrong IIRC. It also emits two different events for \"test\" and \"assert\" (one being # test name and the other being the ok - assert name). tap-parser also emits an assert event for /^(not )?ok\\b/ lines.\nAnother issue I've had is that I have no way of knowing which file a test is coming from. This is especially bad if I'm using a glob to do tests, but only have 1 test file.\nSo here is my suggestion, split this up into two tasks, one of which is easy and can be done fast (I can probably provide a PR if wanted), the other is likely more work and can be left for later.\n```\ntest name - strip out newlines or similar | relative file path to test file\nok 1 \nnot ok 2 \n    ---\n    name: err.name\n    message: err.message etc, same as errors are reported today\n    stack: \n    ...\nok 3 \n\nerrordata if it failed\n   stdout: content of stdout\n   stderr: content of stderr\n   ...\n```\nThis would require each assert event to be transmitted to the test runner though (which I don't know if it does or not). Alternatively # test name could simply be replaced with # relative file path and only one ok/not ok per test.. @novemberborn The problem with TAP is that for the most part it's largely unspecified. The spec only states that you need to declare the number of ok/not oks, and then you do the actual ok/not oks. That and support for #TODO and #SKIP is about it. The rest is convention people have built on top of TAP. TAP also specifies that anything a tap parser doesn't understand should be ignored.\nThe entire thing seems to me like a confusing half-specified text protocol that people implement their own parsers for that are to a certain extent incompatible, which is bound to lead to problem downstream. IMHO, ava would be much better served by either allowing custom reporters (which have already been dismissed because TAP is apparently supposed to solve all of these problem), or invent a streaming JSON protocol or similar (something that has easy expandable data, and cannot be misinterpreted).\nSimple translation from TAP for instance:\n{\"id\":0,\"file\":\"file.js\",\"name\":\"test name\",\"result\":\"success|skipped|todo|failed\",......}\n{\"id\":1,\"file\":\"other.js\",....}\nThe advantage with this, is that like TAP it's streamed (one line per assert), but you get all the data inline. There is no speculation. No conventions. Sure, different keys might be used, but it should always be valid JSON. And none of the \"anything not understood by a parser is ignored\" bullshit that people abuse to tuck stacktraces in there.\nAnd if we want it to be standard, than create a simple standard for it. Define required, optional, and expansion fields for each assert. Figure out how to deal with grouping, etc. Or find some standard that already solves this. Because as it stands, more than 50 percent of the TAP output the TAP parser produces, is not actually TAP. It's just conventions somebody invented because they tried to shoehorn more data into TAP. It's not standard, nor specified. At least as far as I've been able to find.\nNow, enough of my rant. Regardless of whether we stay on TAP, do something else, or both, I think the reporter should have the option to emit each assert. So that if I wanted to I could format it like this:\n```\nmyTest\n\nassert1\nassert2\n``. Yes, I know there is a performance penalty, but that performance penalty will only be paid by users who use it. Most of the work is already done considering theAssertionErroralready except an externalstack` value. The problem is that is that I don't call ava directly from my assertion functions, I create objects which I then later use to call ava, something like this:\n\njs\nconst assertion = Assert.isDeepEqual(1, 2, \"my message\"); // <- this is where the error should point, stacktrace should be captured inside of `Assert.isDeepEqual`.\ndoAssertion(assert); // here ava is called.\nAlso, instead of passing the stack trace itself, an error object could be passed. This has less performance impact, because then the stack trace is only generated on demand (this might even be something that could improve the performance of your existing async tests).. It might be more accurate to call it the assertion callsite (or something similar). But in effect it would be the stacktrace or some form of stacktrace provider.. This may not work on older node versions btw.. Btw; a bonus of the abstraction of the CallSite class, is that the internals can be changed once standard stacktraces lands in ecmascript/node (see https://github.com/tc39/proposal-error-stacks). It can also be made lazy etc., without the callers having to know.. The CallSite abstraction is just a simple abstraction for just that, call sites (meaning \"where a function was executed\"). There were two points for this, first was being open to the new stacktrace API without having to change the public API, the second was not parsing the stack myself. When you do Error.captureStackTrace(obj), it will insert obj.toString() first into the stacktrace, on its own line, therefore I just made toString() return a replace token. The API is then callSite.getStack(errorMessage), so to get the exact same result you would get from a custom Error object, you do callSite.getStack(myError.toString()).\nThe tap library you linked seems to deal with stack manipulation (I've never seen it), but just because both talk about callsites, doesn't mean they have anything to do with eachother. Callsite is a common name for a data type that refers to where a function was invoked.\nTypically, I would generate code like this in Fable.Ava:\njs\nconst assertion = Assert.isDeepEqual(actual, expected, new CallSite());\nasserter.assert(assertion);\nHowever, since this is F#, I automatically insert the last parameter, so the user doesn't know anything about the callsite unless he calls the overload that explicitly passes one. The F# is still kind of at the experimental stage though (not public anywhere yet).\nIf I was creating this library to use in JS, I would change it to the following:\njs\nconst isDeepEqual = (actual, expected) => ({\n  type: 'deepEqual',\n  actual,\n  expected,\n  callSite: new CallSite(isDeepEqual)\n});\nBut this has proven slightly problematic in F#, but I'm still working at it :).. You're correct. When you call Error.captureStackTrace(obj, fn), it will produce obj.toString()\\n<stack trace>. I simply inserted a token that should not happen during any stacktrace so that it could be replaced at a later time.. Right. Well, that should be easy enough to fix.. That would require putting it as it's own file in the directory root though, next to index and cli, wouldn't it?. Fair enough. I'm in the middle of moving though, so it might be a week or so till I have the time to look at this again. By then maybe there's more feedback :). ",
    "arosenthal-r7": "I have a question @jamestalmage, you said: \n\nIf you are using smart watcher and change a test file, it's the only test that will be rerun anyways, so adding test.only to a test will cause only that file to run.\n\nThat is not the behavior I experience in watch mode. If I modify a source file then it only reruns the corresponding test file, but if I'm working on adding tests or fixing tests on any change it re-runs the whole test suite. This is the AVA config I'm using: \n\"ava\": {\n    \"babel\": \"inherit\",\n    \"concurrency\": 10,\n    \"failFast\": true,\n    \"files\": [\n        \"./test/**/*.js\",\n        \"!./test/helpers/**\",\n        \"!./test/fakeData.js\",\n        \"!./test/testDate.js\"\n    ],\n    \"source\": [\n        \"./src/**/*.js\"\n    ],\n    \"require\": [\n      \"babel-register\",\n      \"babel-polyfill\",\n      \"./test/helpers/setup-browser-environment.js\"\n    ],\n    \"verbose\": true\n  },\nAm I missing something?\n. ok so I ran in DEBUG mode, and it looks like you're right, I see this:\nava:watcher Sources remain that cannot be traced to specific tests. Rerunning all tests +18ms\nis there any way to see which sources AVA could not trace back to tests? there are certainly parts of our codebase that do not yet (or might never) have tests and we use external modules inside some of our test files also (could that be the problem?) Without being able to use the .only (because we have concurrency set) it makes it very tedious to write tests when they get all re-run every time. If anyone has any tips or tricks (other than the obvious of setting the specific path to the file I'm testing), please let me know. Thanks\n. ",
    "catdad": "Just my two cents, since I was welcomed to comment. I have only used ava for one project so far, but I immediately ran into this issue and it confused me very much as a user and avid code tester.\nMy expectation, when splitting my code up into a before, test, and after, is that I am doing build-up and tear-down that needs to happen in order to test a small piece of functionality, but does not otherwise relate to the test code itself. In order for my tests to be predictable, I expect that these blocks always run, no matter what. From an end-user perspective, having an after, after.always, and cleanup is just confusing and redundant. Of course I want to clean up, and of course I want that to always happen. What are the use cases for anything else? (Yes, I understand technical debt and backwards compatibility, but they should only be considerations in design, not driving forces.)\nAlso, I see some bad advice and rhetoric happening here and in #918, suggesting that cleanup should happen before and after, allowing you to re-run tests even in a dirty environment. While I don't disagree that it is a good idea to check that your environment is exactly as desired before a test, there has to be a stick in the ground saying that unit tests must not permanently alter the environment, even if there are other tools in the toolchain (such as .gitignore) which will handle that for you. And to the extent that that is the fault of the test framework, it should be treated as an egregious and urgent bug. As an end-user, I should not have to choose between speed and sane, repeatable tests.\nWith that said, I would support the option of a .cleanup hook that ava transparently runs both before and after (and always run it after), provided that the use cases are fully considered. There might potentially be issues with running unexpected cleanup code before the tests, as that is rather unorthodox among the other test frameworks.\n. I have never seen another test framework do this, and I have used quite a few. I have had code throw errors, both intentional and unintentional, both synchronously and asynchronously, have typos, and flat out be running invalid JavaScript, and the test framework catches that and still runs the after method. While it may not be 100% reliable, as you say, AVA is very far from approaching that percentage, and it seems to be on purpose. I would prefer that my test framework take an extra few milliseconds to clean up after itself (or run the code that I conveniently wrote for it) than to fail as fast as possible and have me need to clean up manually.\nAVA may not understand the before/after code itself, but no environment does. They just understand that they have to run that code. Whether the code does cleanup, just says \"hi\", or does absolutely nothing, it is not up to AVA to decide whether or not to run it. The dev wrote that code in the test for a reason, and has the expectation that the code will be run. If after doesn't always run, then what really is the point of after?\n\nWe should clearly document the intent of the hook. But AVA is not afraid of being unorthodox\n\nI have actually seen this cleanup problem in test suites written by highly active contributors to AVA. \ud83d\ude09  Being unorthodox is great as long as it adds value. In this case, it looks like it's actually taking away value.\n. ",
    "sholladay": "This bit me today. A test failed and temporary files were left all over simply because I had .afterEach instead of .always.afterEach. I don't see the logic in skipping .afterEach when a test fails. Puking on the environment, if this is even a desired feature, should be opt-in.\n\n--fail-fast feature is designed to leave the environment in the state in which the failure occurred\n\nYikes. That seems like a major mixing of concerns. This feature is named --bail in other test frameworks and the thing that is bailed on is tests, not cleanup hooks. The command to do what you mention would be --bail --no-cleanup.. > I think --no-cleanup may be a wrong way to approach this issue.\nI would ask \"why\", but it's going off on a bit of a tangent. I haven't personally needed something like that. But it does exist elsewhere and cleanly solves the \"leave the environment in the state in which the failure occurred\" story, even though I think that is a bad idea 95% of the time. If it needs to be a thing, it can be an option. I don't think it should be default.\n\nafter / afterEach hooks may have assumptions on the tests having passed / reached a certain state\n\nThis sounds hypothetical to me. I would bet money that the vast majority of cleanup hooks are doing the equivalent of rm -rf foo.\nI know AVA likes to pave its own path, and to good effect. But I think Intern really gets this right. It makes strong guarantees that if a test runs, its hooks run.. Is there any chance this could make it in for 1.0.0? Needing to remember .always is among my least favorite things about AVA currently. And removing it is a breaking change.. Okay, that makes sense. The Babel interop is clearly pretty complex and a lot to keep track of. FWIW, though, it's been working pretty well for my own use cases. \ud83d\ude03 \nI'll see if I or someone on my team can contribute to this when you think the time is right. Seems like a good first step is to expose the test pass/fail/error status to the hooks? That doesn't even have to be a breaking change necessarily.\n\nBut don't worry we won't shy away from making breaking changes when necessary, and we can support a deprecation path.\n\nThat is great to hear. \ud83c\udfb9 \ud83d\udc42 . Okay, that makes sense. The Babel interop is clearly pretty complex and a lot to keep track of. FWIW, though, it's been working pretty well for my own use cases. \ud83d\ude03 \nI'll see if I or someone on my team can contribute to this when you think the time is right. Seems like a good first step is to expose the test pass/fail/error status to the hooks? That doesn't even have to be a breaking change necessarily.\n\nBut don't worry we won't shy away from making breaking changes when necessary, and we can support a deprecation path.\n\nThat is great to hear. \ud83c\udfb9 \ud83d\udc42 . @eirslett right now, Intern is the best test framework for WebDriver tests. They have put a ton of effort into making them more reliable with Leadfoot and DigDug (both ship with Intern). I have sponsored some features over there, through SitePen, for a big project that needed extensive WebDriver tests. And I, too, wanted retry because we had flaky tests. Turns out wallpapering over them isn't such a good idea. \ud83d\ude03 \nIf retries were to be added to AVA, I definitely think t.flaky() is better by a long shot. It localizes the effect. And more importantly, it would better equip AVA to help you deal with flakiness.\nt.flaky() could be much more interesting than a simple retry.\n\nCould keep statistics about flakiness and report about them.\nCould temporarily set DEBUG env var and increase log level.\nCould log a reminder to remove .flaky() if it doesn't fail for a long time.\nCould be smart about which errors make sense to retry.\n\nIn short, I don't think t.flaky() should have the goal of getting your test to pass. Quite the contrary, I think it should help you figure out when and how it fails. In fact, perhaps t.flaky() should intentionally run your operation dozens or hundreds of times until it fails and then skip cleanup, or log if it never fails.. I like Sindre's proposal. It would still return the error for extra assertions, right? Let's say I also want to ensure that multi-line error messages never exceed a certain width, for friendly terminal display. Would it look like this?\njs\nconst err = t.throws(() => foo(), {\n    constructor: RangeError,\n    message: /unicorn pasta is/\n});\nt.is(longestLine(err.message), 80);\nI will note that there is a much simpler approach to solve the call site ambiguity problem. Just support only constructors in the second argument, since that's the only type that is non-ambiguous for what it is matching. Delegate everything else to t.is(), t.regex(), etc.. @AlexTes\n\nI'd actually lean against Sindre's suggestion of possibly adding a name matcher. Can I ask, when would a name be preferred over a constructor?\n\nI don't personally need this feature much, but for sake of argument...\nSay myApp() uses a database under the hood and potentially rejects with various DatabaseErrors. You probably want to make assertions about failure cases for myApp() working with the database, but the relevant error constructor does not live in your app and very well may not be exposed by the database driver library (it is uncommon to export all such classes).\nYou still need to distinguish between different types of errors and, without the constructor, name is the preferred way to ID an error (should be more stable than message). Changes to these are arguably semver major, so you should probably test them. This is the best you can do without the constructor.\njs\ntest(async (t) => {\n    const option = {\n        databaseName : 'somethingThatDoesNotExist'\n    };\n    t.throws(myApp(option), {\n        name : 'DatabaseNotFoundError'\n    });\n});. I think I'm using this somewhere. Is it really incorrect? t.throws() returns the error, this being the inverse seems reasonable. What harm does it do?. > Isn't it more useful if you get the Promise's value, so that you can make more assertions on it?\nYeah, exactly.\nHere is at least one place I am using it: https://github.com/sholladay/test-engine/blob/ba56e03eea3a148d431cf021b9624dd2ef2d8406/test.js#L61\nHappy to change if there is a reason this is bad, though.. js\nconst prom = getProm();\nawait t.notThrows(prom);\nt.truthy(await prom);\nt.true(typeof (await prom) === 'string');\nvs\njs\nconst val = await t.notThrows(getProm());\nt.truthy(val);\nt.true(typeof val === 'string');. A simple solution would be to replace this:\nThrew unexpected exception:\n... with something like:\nThrew `TypeError` but expected `RangeError`:. @unional from what I can tell, AVA distinguishes between TTY and non-TTY mode mainly to determine whether it should colorize its output. Same as debug.\nIt is common for most software that If TTY mode is detected, then most likely a human is looking at the output in a terminal, so output should be colorized. On the other hand, if TTY mode is false, then most likely the output is going to a file and should not be colorized, in order to be more friendly to scripts (so they don't have to deal with ANSI styles when parsing the output).\nI'm not sure if AVA does anything else based on TTY mode, but it looks like the problem is simply that the TTY information is not being properly forwarded to test workers. I'm pretty sure test workers are not directly connected to the terminal, meaning that TTY is never actually enabled for them, but the top level ava process collects their output and forwards it to the terminal, so it is kind of a fake TTY. Or, you could say, a TTY tunnel / proxy.\nLooks like the TTY proxy is working correctly for process.stdout, but not for process.stderr.\nI believe Jest uses worker processes as well, so it would presumably have to do the same thing unless it allows its workers to be directly connected to the terminal.. I suspect that this is related to power-assert.\nWithout looking deep into the code, I'm guessing that since t.fail(message) doesn't have anything to diff, it skips all the fancy argument analysis and diffing logic. That also means less colorization and logging to the console, which can be heavy if done a lot.\nBy the way, 4,000,000 / 18.862 seconds is still more than 212,066 operations per second or 212 per millisecond. I would hardly call that slow for any typical workload. \ud83d\ude03 . I would use this a lot and it would have helped me today. \ud83d\ude03 \nAt the moment, transpiling is breaking valid code that runs on LTS versions of Node.\nSee: https://github.com/avajs/ava/commit/37c9122c50722b06039f1cc2306a7c176fd3c786#commitcomment-25662855. What would this do?\njson\n\"ava\": {\n  \"babel\": false,\n  \"compileEnhancements\": true\n}\nWould the false be ignored? I'm wondering if it should imply \"compileEnhancements\" : false.. What would this do?\njson\n\"ava\": {\n  \"babel\": false,\n  \"compileEnhancements\": true\n}\nWould the false be ignored? I'm wondering if it should imply \"compileEnhancements\" : false.. I do use that a lot in my personal projects. It is definitely better than grep. However, there are circumstances where I cannot use it. In particular, projects that use TypeScript or other transpiled languages, which are popular within companies. AVA works perfectly well in these contexts, whereas ESLint often does not.\nFurther, I would argue, AVA's runner should be more robust than static analysis.\n  . > I'm not keen on supporting configuration through environment variables.\nI hear that. I usually go to great lengths to avoid env vars. In this case, though, it would make it easier to have hooks that make no assumptions about the npm test command. I thought about npm test -- --check-skips as an alternative, but that would only work if ava was the last command used. That's usually the case for me, but not always, and making that assumption is going too far IMO.\n\nIf we'd support a JS file to build the configuration though that could be implemented locally.\n\nHmm well to me this isn't really any different than just having my pre-push hook modify package.json momentarily to add \"ava\" : { \"skipChecks\" : true }. The idea of doing that kind of scares me. What would a JS file do better exactly? I guess you were thinking it could do if (isRunningInGitHook) { ... } somehow?. I like that npx @ava/init is clearly its own thing, whereas npx ava-init could be confused for npx ava init or npx ava --init if you read it quickly, as people often do with the install instructions.. I like that npx @ava/init is clearly its own thing, whereas npx ava-init could be confused for npx ava init or npx ava --init if you read it quickly, as people often do with the install instructions.. Tried a bunch of things, but today I cannot reproduce this any more. I checked both beta 5 and 6. Best guess is it was a bug in a subdependency that got fixed (probably in Babel itself). Wish I had more details... \ud83d\ude04 . Hmm this started happening again for me as of beta 7. It's reliable, I can switch between beta 6 and 7 with a nuke rm -rf ~/.npm package-lock.json node_modules in between and beta 7 does it, but not 6.\nSame cryptic message. It doesn't show which plugins or presets are duplicated. And touching package.json still matters. Basically, it always fails the first time, then I have to modify package.json, then I have to run AVA again and let it fail, then put package.json back the way it was originally, run AVA again and finally it will pass.\nI have been trying to isolate if specific parts of package.json are important and I think it is @babel/plugin-syntax-object-rest-spread. If I only remove @babel/plugin-transform-modules-commonjs before the second run, then the third run seems to always fail the same as the first.\nBTW are you sure the below statement is true?\n\n... \"@babel/plugin-syntax-object-rest-spread\"? AVA adds that one directly but it's supposed to check if it's already used\n\nWhen I remove @babel/plugin-syntax-object-rest-spread and run AVA (any version, any number of times), I get:\n```\n  SyntaxError: /Users/sholladay/Code/noirdoor/client/test/albums/reducers.js: Support for the experimental syntax 'objectRestSpread'isn't currently enabled (31:27):\n_class.raise (node_modules/@babel/parser/lib/index.js:3906:15)\n  _class.expectPlugin (node_modules/@babel/parser/lib/index.js:5240:18)\n  _class.parseObj (node_modules/@babel/parser/lib/index.js:6649:14)\n  _class.parseExprAtom (node_modules/@babel/parser/lib/index.js:6281:21)\n  _class.parseExprAtom (node_modules/@babel/parser/lib/index.js:3607:52)\n  _class.parseExprSubscripts (node_modules/@babel/parser/lib/index.js:5911:21)\n  _class.parseMaybeUnary (node_modules/@babel/parser/lib/index.js:5890:21)\n  _class.parseExprOps (node_modules/@babel/parser/lib/index.js:5799:21)\n  _class.parseMaybeConditional (node_modules/@babel/parser/lib/index.js:5771:21)\n  _class.parseMaybeAssign (node_modules/@babel/parser/lib/index.js:5718:21)\n```\nHowever, on the buggy versions of AVA (e.g. beta 7), if I then add the plugin back to my config after letting it fail once, then it will pass.. Hmm this started happening again for me as of beta 7. It's reliable, I can switch between beta 6 and 7 with a nuke rm -rf ~/.npm package-lock.json node_modules in between and beta 7 does it, but not 6.\nSame cryptic message. It doesn't show which plugins or presets are duplicated. And touching package.json still matters. Basically, it always fails the first time, then I have to modify package.json, then I have to run AVA again and let it fail, then put package.json back the way it was originally, run AVA again and finally it will pass.\nI have been trying to isolate if specific parts of package.json are important and I think it is @babel/plugin-syntax-object-rest-spread. If I only remove @babel/plugin-transform-modules-commonjs before the second run, then the third run seems to always fail the same as the first.\nBTW are you sure the below statement is true?\n\n... \"@babel/plugin-syntax-object-rest-spread\"? AVA adds that one directly but it's supposed to check if it's already used\n\nWhen I remove @babel/plugin-syntax-object-rest-spread and run AVA (any version, any number of times), I get:\n```\n  SyntaxError: /Users/sholladay/Code/noirdoor/client/test/albums/reducers.js: Support for the experimental syntax 'objectRestSpread'isn't currently enabled (31:27):\n_class.raise (node_modules/@babel/parser/lib/index.js:3906:15)\n  _class.expectPlugin (node_modules/@babel/parser/lib/index.js:5240:18)\n  _class.parseObj (node_modules/@babel/parser/lib/index.js:6649:14)\n  _class.parseExprAtom (node_modules/@babel/parser/lib/index.js:6281:21)\n  _class.parseExprAtom (node_modules/@babel/parser/lib/index.js:3607:52)\n  _class.parseExprSubscripts (node_modules/@babel/parser/lib/index.js:5911:21)\n  _class.parseMaybeUnary (node_modules/@babel/parser/lib/index.js:5890:21)\n  _class.parseExprOps (node_modules/@babel/parser/lib/index.js:5799:21)\n  _class.parseMaybeConditional (node_modules/@babel/parser/lib/index.js:5771:21)\n  _class.parseMaybeAssign (node_modules/@babel/parser/lib/index.js:5718:21)\n```\nHowever, on the buggy versions of AVA (e.g. beta 7), if I then add the plugin back to my config after letting it fail once, then it will pass.. I am, yes. I'm not seeing any peerdependency warnings related to Babel, but downgrading to 7.0.0-beta.51 seems to fix it. Guess it's my fault, then? Will close since I have a solution. Thanks! \u2764\ufe0f . Okay here's your minimal repro!\n\nInit a project with AVA and Babel:\n    sh\n    mkdir /tmp/repro-ava-1828\n    cd /tmp/repro-ava-1828\n    npm init -y\n    npm install --save-dev --save-exact \\\n        ava@1.0.0-beta.7 \\\n        @babel/plugin-syntax-object-rest-spread@7.0.0-rc.1\nAdd some Babel config to package.json\n    json\n    \"babel\": {\n        \"plugins\": [\n            \"@babel/plugin-syntax-object-rest-spread\"\n        ]\n    }\n\nAdd a simple test so AVA is willing to run\n    ```sh\n    import test from 'ava';\ntest('true is true', () => {\n    t.true(true);\n});\n4. Run AVAsh\nnpx ava\n```\n\n\nThe result, for me, is the same stack trace as I originally posted. I'm currently on Node 8.11.3 and npm 6.3.0, but that doesn't seem to matter in this case.\nI used --save-exact in the example above in the hopes that the steps will remain reproducible in the face of new releases. But it's not what I was doing in practice, so it is not directly relevant.. I wouldn't mind too much either way, but I will say that for me personally, AVA was a nice, practical introduction to ES6 and ESM. There are a lot of blog posts talking about ESM, but it barely has any usage yet outside of projects with a Babel build step. With AVA, it feels native even though it's not, since it's zero-config for test files, and I like that. It feels like the future (and it is).\nI've always found it pretty easy to test CJS modules with AVA. I think testing Babel projects is a bit harder, but that seems expected to me.\nAs for how to reduce the mental burden on users, I have a few ideas:\n1. I think this is a great case for a flow chart. AVA basically has two different setup flows, depending on whether your project needs Babel or not. That lends itself well to a visual explanation in a diagram and I think it's a good way to express that you should make a choice from the beginning.\n2. create-ava should detect my usage of Babel and configure AVA based on that. For example, put @babel/register in ava.require in package.json for me. And output some friendly messages related to Babel setup. Feature create-ava more prominently in the README, recipes, and blog posts. Get people to use it and lean on its automation.\n3. Official Yeoman generators for both Babel and non-Babel \"AVA Hello World\" setups would be super useful to people.. > the only reason I'm still choosing tape over ava is the 13 MB install size difference\nAs long as it is a devDependency, the size won't affect your users except during development. Also, I'm willing to bet that if you do npm ls ava, you'll find it in your dependency tree already. So npm should be able to dedupe it and install only once.. Looks like the CI failures are just flakiness, from what I can tell. Should probably rerun them. Code LGTM!. I don't really understand this. You can already get the filepath of the test (or any other file being run in Node) using __filename (as you mentioned). This is way better since it's not AVA-specific.\nSoon you will be able to use import.meta.url, which is standardized at an even higher level. I believe this one already works but is sitting behind an experimental flag in Babel for now.. I see. I think I didn't catch onto that because you would also have to pass t to the helper, so it doesn't seem any better/worse in that regard.\nWouldn't import { filePath } from 'ava be tricky to implement since it has to be statically defined? Or is AVA compiled per-process? Should be interesting. :). Until there is a robust system to analyze the actual test and source files to figure out what to run, you could get 90% of the way there by using sed or awk in your pre-commit script. Conceptually, what you want to do is map the output of lint-staged and append .test.js to each of the filepths that it outputs. AVA will then run those tests. So if you change foo.js, it will run foo.test.js.\nIt's not perfect, because foo.test.js may not necessarily be the only test that imports (or indirectly relies upon) foo.js. But it's a good start, especially if you can stick to a 1:1 relationship between source files and tests.. This hasn't been a problem for me, but still seems like a strange limitation. Wouldn't it simplify the code to not special-case the configuration? What's actually gained from doing this?\nIf the goal is just to encourage people to use best practices, a more prominent suggestion to use package.json in the docs might be a simpler approach. Or a console warning, rather than actually preventing config on the command line.. Isn't it a bit negative? Like the first thing you want me to think about is that testing is painful? I guess what you're going for is empathizing with the user. But I think it could be done with a more positive tone.\n\nAVA makes testing quick and painless. How about \"chore\"?\nTesting can be a chore. AVA helps you get it done.\n\nChores aren't necessarily fun, but they're something everyone knows they have to do.. ",
    "rhalff": "I've tried this branch with webstorm and for this to work I had to accept another command line option:  --expose-debug-as=v8debug.\nThis is something specific to webstorm, see:  https://github.com/nodejs/node/issues/7102\nI wonder whether the code specific to --debug and --debug-brk is needed at all, in a sense ava does not need to be aware of these parameters, while they are just passed as extra options to childProcess.fork.\nInstead of debug the Api could accept a different option execArgv:\njs\n var api = new Api({\n    execArgv: ...\nThese options will then be passed directly to childProcess.fork.\nIt would be nice if these extra arguments would not have to be specifically specified, this would enable any node parameter to be passed along and let the error handling be handled by the node cli itself.\nTo do this, perhaps make sure all options are covered by the type list (init and no-cache are still missing): https://github.com/avajs/ava/blob/master/cli.js#L81-L107\nAnd then use this list to filter the command line arguments and pass any unknowns using the new execArgv option.\n. @develar I'm using WebStorm 2016.2 (Build #WS-162.1121.31, built on July 9, 2016)\nIt seems webstorm 2016.2 includes --expose-debug-as=v8debug by default without having to specify it,\nwhich doesn't make much difference with .1 where it had to be added manually.\nI've created a branch here: https://github.com/avajs/ava/compare/master...rhalff:exec-argv-increase-debug-port which will increase the port number on each fork and pass any other node command line argument as is.\nWebstorm is configured like this:\n\nWebstorm itself will then detect the debug port for each child process:\n\nI'm not sure this is a universal fix though and whether the automatic detection of the extra debug port is something specific to webstorm only.\ne.g. If I run the same command from the command line only one port is reported.\nn.b. the --verbose option is needed while the spinner behaved weird within the debug console of webstorm. \n. ",
    "SimenB": "Can this be made to work with --inspect in newer nodes as well? No need for node-inspector then\n. Doing a clean install on 6.7 seems to have \"fixed\" it. The last test still hangs as green for almost a full second before exiting, but it does exit cleanly instead of crashing...\nI'll close it, though, as I don't get a crash anymore \ud83d\ude04 \n. Duplicate of itself? I suppose #148 is the one, or #1068.\n. Any news here? I'm guessing upstreaming would mean pulling down changes from upstream as well. Would love to have the latest HtmlElement plugin working inside ava! Or just a clean way of adding extra serialisers.\nI'd be happy to help upstream changes if necessary.\nEDIT: Seems like a small case of NIH hit and you're replacing jest-snapshot/pretty-format entirely: #1341. It'll be interesting to see what comes out of it. Looks cool! \ud83d\ude04 \nEDIT2: Just pretty formatting and passing the string to ava worked out ok, but I lose colors on diff. No biggie, though.\njs\nt.snapshot(prettyFormat(el, { plugins: [prettyFormat.plugins.HTMLElement] }));\nAdding highlight: true gives colors, but that messes colors for the diff. Setting it to 2000 makes no difference. The weird thing is that having unref also makes ava completetly ignore the --timeout option. Seems like doing an unref on a timer messes up some global internal state in ava. We discovered this bug when writing a test that didn't really need the unref, so we just removed it.\nI still find it odd that ava doesn't wait for the returned promise regardless. And that adding a unref-ed timer changes the semantics of the test that much. Is that a quirk (or even a bug) in node itself? A still pending promise is discarded. ",
    "knownasilya": "Would love to see this merged soon \ud83d\udc4d \n. ",
    "tquetano-r7": "Damn, this is actually a result of sinon.test wrapping the async function call ...\n``` javascript\ntest(async function() {\n  const now = await Date.now();\nconsole.log(now);\n});\n// 1466524079966\ntest(sinon.test(async function() {\n  const now = await Date.now();\nconsole.log(now);\n}));\n// 0\n```\nI'll close because I think its now more likely a sinon issue\n. Personally I would not expect deepEqual to care how pointers are used between the compared objects, but they would be considered for how they are used within the same object (for the circular reference reason you describe). That said, the knowledge would only operate insofar as the need to solve circular references and no more ... basically a decycle level of knowledge. Perhaps that is a bit naive, but its what I would expect.. ",
    "tony-kerz": "fwiw, witnessing similar with:\nt.true(wrapper.hasClass('does-not-exist'))\n. @novemberborn oh yeah, my code never has bugs, it's the other devs i'm worried about \ud83d\ude06 \n. like i said, it's just another data point.\nit's possible to write tests like this:\ntest('scam test', () => {\n})\nwhich don't do anything meaningful, yet appear to illustrate testing.\nit's also possible to have 100% code coverage without a single assertion.\nof course, these shoddy testing techniques (let's hope more unintentional than intentional),\ncan be discovered with closer analysis of the test code, but having the ability to display\nan additional \"kpi\" like assertion-count can provide a quick insight without a deep dive.\ni understand your preference for cleanliness tho. \nperhaps consider adding the metric in verbose mode, or even with a distinct flag.\nregards,\ntony.\n. @vdemedes agree. \nif someone was trying to game the system, it wouldn't help.\nin the more likely good-faith case, it is an (easily collected?) metric that could be used as an anecdotal indicator of coverage, and i would use it as such.\ni understand the dark side of metrics too (think lines-of-code), and resultant religious positions on the topic, so if the project owners feel it doesn't add value (or is otherwise evil), i understand.\n. @jantimon can you help me out with the specific command-line/npm-script that you used to invoke iron-node against an ava test? i'm trying the following with ava 0.17.0, node 7.4.0 and iron-node 3.0.17:\niron-node node_modules/ava/profile.js test/ava\nbut getting:\n\n. thanks @jantimon and @sindresorhus, got iron-node working calling out a specific file \ud83d\udc4d \nalso, plain-old babel-node kind of works with this:\nbabel-node --inspect --debug-brk node_modules/ava/profile.js test/ava/my-test.js\nbut dev-tools couldn't resolve the source-map for some reason. \nit showed transpiled code, with this comment included:\n//# sourceMappingURL=../../node_modules/.cache/ava/8777fd944b4d081b32c7a8708f7547a4.js.map\na project for another day, but any insight welcome...\nthanks again!\n. i just timed it at 3m (ouch)...\ni would be happy to instrument for more details if anyone can provide a specific technique to use.\nhere are some relevant file counts:\n\nnode_modules: ~12k\nsrc: 104\ntest/ava: 15\ntest/cuke: 40\n\nif i point to a specific file, it starts almost immediately, e.g:\nava -s test/ava/framework/data/data-test.js\nfwiw, i typically run with the -s flag because some of the tests run against a database and will fail if run concurrently, but i have witnessed similar slowness without the -s flag.... hi mark,\ni had some trouble attempting to use console to output timing (i guess ava hijacks stdout for it's spinner or such), but wrote some data to a file like so:\nfindTestHelpers() {\n        const begin = new Date();\n        const result = handlePaths(defaultHelperPatterns(), ['!**/node_modules/**'], {\n            cwd: this.cwd,\n            includeUnderscoredFiles: true,\n            cache: Object.create(null),\n            statCache: Object.create(null),\n            realpathCache: Object.create(null),\n            symlinks: Object.create(null)\n        });\n        result.then(stuff => {\n            fs.writeFile(\n                './temp-then',\n                `elapsed=${new Date().getTime() - begin.getTime()}, result=${JSON.stringify(stuff, null, 2)}`\n            );\n        });\n        return result;\n    }\nwhich yielded: \nelapsed=756, result=[]\nso looks like findTestHelpers is running fine?\n--concurrency=3 makes the tests start sooner, but it still takes a long time to run all of them.\nas an exercise, i was going to try to run each of the 15 ava files discretely and will report back those results, but open to other suggestions or instrumentations.\n. so, i created a bash script to run the 15 test files separately with timers and indeed each file took about 15s to run which added up to about 3m, but... \ni noticed that the first file ran in about 2 seconds. \nso i commented out running a few of the files, but always the first one to run, runs in about 2 seconds and all subsequent run in about 15s...\nso, i created this simple test:\n```\nimport test from 'ava'\nimport _ from 'lodash'\ntest('', t => {\n  t.truthy(.every([true], elt => elt))\n})\n```\nand ran it twice in the same script, and lo and behold, the first time it runs in 2s and the second time in 15s...!?\nso this script:\n```\n!/bin/bash\nset -o nounset\nset -o errexit\necho \"begin $SECONDS\"\nava test/ava/canary-test.js\necho $SECONDS\nava test/ava/canary-test.js\necho \"end $SECONDS\"\nyields this output:\n~/g/blah (tk/jwt) $ script/ava.bash \nbegin 0\n1 passed\n3\n1 passed\nend 16\n```\ni'm baffled, but does it mean anything to those more traveled?. ",
    "Awk34": "I think the number attempts needs to be configurable. For the generator, I have all the E2E tests retry twice, but the tests for the Sequelize setup is especially flakey, so I've set this to retry thrice. \nedit: Could test.retry(2)(t => {}) be done? (with test.retry(t => {}) defaulting to 2)\n. > ``` js\n\ntest.flakey(t => {}) // retry twice\ntest.veryFlakey(t => {}) // retry thrice\n```\n\nSeems reasonable, but I'm sure you'll get someone who asks for the ability to retry more sometime down the road.\n\nWe could provide some more detailed summaries about flakey assertions as well:\n[...]\n\nI like that a lot. I don't think mocha has the option of showing if a flakey test was retried.\n. > Isn't a test that regularly fails three times in a row basically useless? If the repeated failure is because you are running on an overcrowded CI VM, or because of network issues, aren't you just as likely to fail four times in a row at that point?\nI completely agree; just saying, I imaging people would want it anyway.\n\nPerhaps we could make the number of retries configurable based on context:\n[...]\n\nSeems reasonable. Maybe even just something like this: \njson\n\"ava\": {\n  \"flakey\": 2,\n  \"veryFlakey\": 3\n}\n. Would there be an easy way to set some flakey config variable (in ava config or a CLI flag) from Appveyor instead of introducing some platform-specific code into your AVA config?\n. Yeah, those seem a lot better.\n. If I tell my CI to run npm test and I have my script as \"test\": \"ava somefile.js\", I could easily run npm test --retries=1,2 as well, right?\nI think if anyone ever comes up with the need for env var configuration then it can be discussed, but until then, nah. There's no other use of env vars like this in this project (from what I can tell).\n. Aliasing two correct spellings seems reasonable to me, but if there's a more used spelling, that's the one that should be presented in docs.\n. @sindresorhus I think it's actually 2 tries for my E2E, and 3 tries with Sequelize tests (not retries). Anyhow, the Sequelize tests for my project are too flakey. I don't really use SQL too often, so I've been being lazy about looking into if I can fix those XD.\nAlso as a side note, the reason this story came up is because I'm looking into using AVA for my Yeoman generator, another project you have a hand in. If you're interested: https://github.com/angular-fullstack/generator-angular-fullstack/tree/perf/gen-tests :D\n. ",
    "rozzzly": "@jamestalmage \n\nIsn't a test that regularly fails three times in a row basically useless? \n\nA fantastic point. But, IMHO there really isn't a good reason not to make it configurable... As others have pointed out, there are certainly people out there who have setups which would truly merit the option to configure this. \nCLI flags / package.json fields would be a simple fix to the issue, but what about someone who'd like to have the number of retries differ from test to test? Might I suggest:\nTo get the default no. of retries, flakey can be accessed as a property on the chain like so \nts\nt.flakey.is(foo, bar);  // retry assertion twice\ntest.flakey(t => {\n    // ...\n}) // retry test twice\nBut then it could also a function that accepts an argument like retries?\nts\nt.flakey(7).is(foo, bar); // retry assertion 7 times\nas far as setting a custom retry count for a test, the equivalent syntax might be:\nts\ntest.flakey(7)(t -> {\n    // ...\n}); // retry test 7 times\nBut I think the (nTries: number)(test: ((t:  ContextualTestContext) => void) is kind of ugly when directly in front of test(). Another option might be\nts\ntest.flakey(7, 'testTitle', t => {\n   // ...\n});\nThat however is a departure from the test() syntax; it might be a little confusing. But logically, would work:\n1. if arguments.length === 3 then arguments[0] is (a number for) the retry count, arguments[1] is (a string for)  the title and arguments[2] is the test method (each would be type checked of course)\n2. if arguments.length === 2 then arguments[0] is (a number for) the retry count, arguments[1] is  the test method (all typechecked of course)\n   - the test title would then, if possible, be inferred from the functions name as per usual.\n. A few more thoughts:\n- Do retries happen immediately? or can the delay be configured?\n  - How does that affect other tests in the file? do they run right as it fails and then retest the failed ones? or does it block and wait for the test to pass / limit be exhausted?\n  - what might that syntax look like?\n- I assume retried tests would run in the same process in which they failed the first time around.. We should ask: how does this affect state pollution? I'd imagine beforeEach()/afterEach() would rerun when a flakey test fails?\nNow that's definitely outside of the scope of AVA; something you'd want to an external tool to manage. Such a thing could rerun failed tests when some user defined conditions are met. That could actually be really useful for testing sessions on servers where overtime, the state gets more and more muddled and scenarios , or something where the connection/resource access might be intermittent. \nThese were some questions I asked myself when reading this Issue. I'm not sure what use they might be to the AVA team, but I've always found its better to consider a broad range of possibilities before committing to anything. TBH, I can't think of many cases where I, personally, would be using flakey all that much, but perhaps something I said might be desired by another, or atleast give you all a different perspective. \n. @jamestalmage \nAs far as retrying assertions, You were the first person to mention such a thing :unamused: I'm just following you line of thought my friend:\n\nt.flakey.is(foo, bar); https://github.com/avajs/ava/issues/934#issuecomment-228156646\n\nAnd you're right, it is pretty ridiculous; I would suspect that I'm probably not going to use flakey more than once or twice in my current projects. I merely suggested a syntax/design pattern/what-have-you which would enable users to tweak it as much as one might want in a way that is somewhat contiguous with the rest of the api.\nAVA is pretty damn opinionated, and that's what has made it so successful! I certainly don't begrudge the team this; in fact, I greatly admire such adamancy!! So if you'd rather keep the api minimalistic, I totally respect that. Just throwing out ideas here man. :thought_balloon:  As I described in my second comment, such a thing would probably be better suited as its own tool. Now I'm imagining ava-daemon where you could track/trigger/retry ava tests programmatically. I can think of quite a few cases in which it might be desirable to trigger a test when some condition was met. Might it not be much easier than mocking some complex state? ... (detecting a condition v.s. recreating one)  _Not that I'm suggesting doing away with the core philosophy of AVA test, Just a way to retest something under conditions one to cover more edge cases.\n\nAh good; as it should be! Atomic tests all the way man, better to have the test to blow up :boom:  then write ill-conceived tests.\n. ",
    "taylor1791": "It is unclear to me if there is a consensus on how to implement this feature. If there is a consensus, I would like to take a stab at implementing this next week.. I just noticed that this may be related to #1228.. With that guidance, I put in some console.times around the globbing. time ./node_modules/.bin/ava src/add.spec.js produced 6.74s user 1.25s system 113% cpu 7.042 total. Globbing files reported 13.409ms and globbing helpers reported 6406.377ms. I looked around and I didn't see a way to change the helpers patterns. Assuming I didn't miss it, would this be a feature worth adding?\nFor what it is worth, if there were some debugs in the code with the timings for the globs and I could out find how to configure the default helper patterns, it may have prevented me from reporting this issue.. ",
    "bbiagas": "I am just a random person on the internet, but if I can throw in my two cents I think a configurable number of retries would be great. The opinions against it have merit, but I also think there's not a clear definition of what \"flaky\" means. A flaky test is (in theory) one that passes almost all of the time but occasionally doesn't, for reasons which are not obvious. If a failure turns out to be consistent or obvious for whatever reason then the test isn't flaky; it's just a failing (or poorly written) test.\nActual flaky tests are often symptoms of deep or intermittent issues. The \"flake\" is telling you something useful you should be inspecting, And, sometimes, looking into these tests means you may want, or need, to rerun them many, many times to try to recreate the circumstances which caused it to fail.\nI have a real world example that happened at work recently; I am using Ava to test an Electron app with Spectron (webdriverio). I found that a particular test would fail every now and then for reasons we couldn't explain. It turned out the test was catching a serious (for us) but rare issue our users had reported to us only once or twice before, and a bug in the application code was the culprit. But we needed to run that test hundreds of times and build a better histogram before we could make that connection, determine the magnitude of the problem, and isolate the cause. A convenient way to keep retrying would have been great.\nI might go one step further and say it would actually be handy to retry ANY test a configurable number of times. What better way to ensure an actual flaky test has been fixed than to be able to easily run it lots of times instead of hoping you catch it in CI again. Or test your tests a little more and maybe catch it sooner. This is probably not practical, but I would use it sometimes.\nI would also suggest that the term used is \"retry\" or something similar, and not \"flaky\". There is some baggage among test professionals that comes with the word \"flaky\", the least of which is that it's not immediately clear what it means.\nJust a thought. We're really liking Ava at work so thank you all for your continued hard work and dedication.. I am just a random person on the internet, but if I can throw in my two cents I think a configurable number of retries would be great. The opinions against it have merit, but I also think there's not a clear definition of what \"flaky\" means. A flaky test is (in theory) one that passes almost all of the time but occasionally doesn't, for reasons which are not obvious. If a failure turns out to be consistent or obvious for whatever reason then the test isn't flaky; it's just a failing (or poorly written) test.\nActual flaky tests are often symptoms of deep or intermittent issues. The \"flake\" is telling you something useful you should be inspecting, And, sometimes, looking into these tests means you may want, or need, to rerun them many, many times to try to recreate the circumstances which caused it to fail.\nI have a real world example that happened at work recently; I am using Ava to test an Electron app with Spectron (webdriverio). I found that a particular test would fail every now and then for reasons we couldn't explain. It turned out the test was catching a serious (for us) but rare issue our users had reported to us only once or twice before, and a bug in the application code was the culprit. But we needed to run that test hundreds of times and build a better histogram before we could make that connection, determine the magnitude of the problem, and isolate the cause. A convenient way to keep retrying would have been great.\nI might go one step further and say it would actually be handy to retry ANY test a configurable number of times. What better way to ensure an actual flaky test has been fixed than to be able to easily run it lots of times instead of hoping you catch it in CI again. Or test your tests a little more and maybe catch it sooner. This is probably not practical, but I would use it sometimes.\nI would also suggest that the term used is \"retry\" or something similar, and not \"flaky\". There is some baggage among test professionals that comes with the word \"flaky\", the least of which is that it's not immediately clear what it means.\nJust a thought. We're really liking Ava at work so thank you all for your continued hard work and dedication.. ",
    "eirslett": "Was there any progress on this issue? I'm trying to run selenium tests with ava, and the lack of retry support is painful. (browser integration tests are the most unstable tests...)\nI'm thinking mocha is maybe a better test runner than ava for selenium, since mocha has retry support?\nFrom the thread, it looks like everybody agrees that retry support would be good, but a lot of ideas about how it should be implemented so it covers every use case under the sun... in the spirit of \"don't let perfect be the enemy of good\" it would be useful to just have a bare-bones retry implementation similar to mocha's. Or provide the opportunity for end users to implement their own custom retry logic, like TestNG provides a RetryAnalyzer interface to implement?\nt.retry(3) <-- simple version\nt.retry(data => data.failureCount < parseInt(process.env.RETRY_COUNT)) <-- tailored. ",
    "timurtu": "Hello, I'd like to contribute if anyone else hasn't taken this. By outside package root do you mean calling './(project-root)/node_modules/.bin/ava -w' from a shell (or child_process) isn't working?\n. ",
    "LasaleFamine": "Hi guys! \nCan I take this one? Is there something more I should know? . Just to be sure, I can see a --source flag, not a --sources. Is right?\nAnother thing I can see is that the cli.flags.source is used within the Watcher constructor: \n```javascript\n// lib/cli.js\nif (cli.flags.watch) {\n        try {\n            const watcher = new Watcher(logger, api, files, arrify(cli.flags.source));\n            watcher.observeStdin(process.stdin);\n        } catch (err) {\n...\n```\nWe need to get that source prop from the pacakge.json?. This should be solved with 1.0.0 of stack-utils: tapjs/stack-utils#14 (comment).. Hi there, is this still valid?. > Simplify it to not show the stack trace if it's only the test function.\nCan I take this as the issue?. I have a question. Is this related to reporters? The check should be made inside every reporters?. Well, I need some ideas to a good and \"self-explained\" check. I will make a PR soon though I'm still in WIP.. Dunno why node 4 on Travis timed-out... \ud83d\ude15 . Thanks for the tip. I will be back later today.. I don't know why all those changes on the README. I think now is fixed.\nLet me know if I need to change something more.. > To clarify, you mean test.error.source.file refers to the module's file?\nI this case (that will produce mi first post output), test.error.source.file === 'lol.js' but test.file === 'test.js':\n```javascript \n// test.js\nimport test from 'ava'\nimport fn from './lol'\ntest('asd', t => {\n    fn()\n})\n```\njavascript \n// lol.js\nmodule.exports.testing = () => {\n    throw new Error('lol')\n}\nI understood your point, actually initially I was thinking to check if the extracted stack trace was only one line, but I didn't know if that single line could contain some other info.\nI can proceed in this way if you agree.. Can I assume that there aren't \\n chars within the extracted stack?\nEx: \n```javascript\n// lib/reporters/mini.js finish(runStatus)\n// Check also if the stack isn't only the test Fn (actually check for one-line stack)\nif (test.error.stack && extractStack(test.error.stack).search('\\n') > -1) {\n       status += '\\n' + indentString(colors.errorStack(extractStack(test.error.stack)), 2);\n}\n```\nActually I don't like so much this solution because I'm calling twice the same function and I really need the comment to explain what I'm doing.\nDo you have a better/correct solution for the problem?. I have made the change and also added the check to the verbose reporter. Also test fixed.\nLet me know if I need to change something. . Sorry, I missed it.. ",
    "MeoMix": "@skorlir When I try using your repo I encounter an error:\n\nError: Cannot find module '/C:/Users/Meo/GitHub/jspm-ava-test/jspm_packages/github/jspm/nodelibs-fs@0.2.0-alpha'\n\nNone of the node-libs seem to be loading properly.\nIf I work around it by not applying System.normalizeSync to nodelibs then I encounter another error:\n\nError: Cannot find module '/C:/Users/Meo/GitHub/jspm-ava-test/src/app/test/test.js'\n\nSuspect you guys aren't testing this against Windows paths?\n. @dak Yeah, that appears to work, too -- although using try/catch as part of normal control flow is generally frowned upon.\nAdditionally, there are still issues with Windows paths. I need to have the following:\njspmUri = jspmUri.replace('///', '//'); for it to work properly as Windows looks like file:///C:/... which results in the prefix'ed slash which causes lookup failure.\nThe actual fix should probably mimic how JSPM handles things: https://github.com/jspm/jspm-cli/blob/ff46dbcada4236603649344913aa543011dcfbc1/lib/config/package.js#L30\n. Well, I'm pretty sure that's what JSPM does internally. It's a bit of an odd scenario to interface into JSPM, but then go back to loading through Node after using it.\nI have to wonder if we can just do something similar to how I've handled it with the JSPM css loader: https://github.com/MeoMix/jspm-loader-css/blob/master/src/abstractLoader.js#L58\nbut I'm not sure atm if they just seem similar or if it's actually the same problem being solved\n. ",
    "kdex": "@skorlir @dak sync might be a way to wrap jspm.import in a synchronous way, so people could import their jspm dependencies using the import syntax.\n. Here's my Babel configuration:\njs\n{\n        \"presets\": [\n                [\n                        \"env\", {\n                                \"targets\": {\n                                        \"node\": true\n                                }\n                        }\n                ],\n                \"stage-0\"\n        ],\n        \"plugins\": [\n                [\"module-resolver\", {\n                        \"root\": [\"./\"]\n                }]\n        ]\n}\nBy \"ava worked at 1.0.3\", I was referring to ws-promise@1.0.3. Back in ws-promise@1.0.3, the project had used ava@0.18.2, which didn't throw the error.\nIn other words, if I git reset to ws-promise@1.0.3 and then npm install ava@0.19.0, the error from above arises.\nI use babel-plugin-transform-runtime in ws-promise so that I can ship a transpiled version without polluting the user's global environment.. @novemberborn Ah, that explains everything; thank you. I'll close this issue in favor of novemberborn/hullabaloo-config-manager#10.. ",
    "STRML": "In terms of caching etc., babel-loader does a really good job of that & reading your .babelrc, babel versions, etc., to make a pretty great fast cache that is busted properly every time I've used it.\n. ",
    "michaelgilley": "Hi all! Great work on the project. My team seeing really slow test times along with thrashed cpus when testing > 30 test files in a React app. This is especially the case for those using a VM (Ubuntu on PC). Any updates on this feature or ideas how to mitigate this issue? Thanks!!\n. @jfairbank Yes, have tried that but as noted in #966 it appears to affect the process differently based on what's available including what other things are running in the background. @nickjvm has had issues running ava in his VM. It's even caused active Chrome tabs to crash! It seems like the sort of thing that ought to be baked into ava - to programmatically check what concurrency level it should be running. When running ava in npm scripts and often behind nyc for coverage it becomes awkward for a team to figure out how to individually configure ava to run correctly on their machines.\n. What's the status on this PR? It seems pretty stale. We really need something like this to be done. With > 100 test files this really becomes essential. Any idea when Ava will be updated to support transpiling once, in the main process before tests are run?\nFor now we are setting concurrency to 6 but that means that 400 tests are taking at least several minutes to run.\n. Might I suggest taking a look at how Jest does this? We ended up switching and it's much, much faster. It may be prudent to take a page from their book. \ud83d\ude04 \n. ",
    "jrajav": "I'd like to add to this that even with babel: \"inherit\", the fact that ava still includes some babel plugins (primarily transform-runtime) breaks many repos, including ours, in a very odd way, and has led us away from using ava for the time being. Not just this one case, ava should not impose any extra babel configuration or plugins at all on the source, however convenient it seems - even if it didn't break things, it couples things in a dirty way and goes against the notion of pure composability.\n. @sindresorhus I will try to reproduce it, but it might take time - I'd have to update our large repo with all its new configuration and tests to ava again. I'll definitely open one if I can. ~~But the fact that it pollutes the babel environment with forced plugins is a turnoff in any case, even if it worked. Aside from being more reliable to start from nothing, it's important that we have the same environment for our tests as for our production code.~~ Misunderstanding on my part - this actually does not seem to be the case.\n. ",
    "GramParallelo": "I'm looking to contribute my first PR. Can I have a go at this? (might need a little helping hand :-/ )  Thanks\n. @pinwen Yes, please.  I'm a little lost and also can't put in the time at the moment.  Thanks.\n. ",
    "pinwen": "Can I start looking into this too?\n. ",
    "rbcasperson": "Ah yes, my target was set to 'es5', and the errors went away with target set to 'es6'. I will need to set up multiple tsconfig files in order to send my source to es5 and my tests to es6.\n. ",
    "marcusnielsen": "Yeah, I've dropped the ball on this PR completely.\nI won't give this up, and will get back once I learn more about testing. \nMy way of doing things is by acting on rxjs like a black box, knowing nothing of how to describe schedules and such.\nI'm reading a bit of the work on Symbol.observable interopt, but this is way over my head unless I get the proper resources.\n@blesh do you believe it would be an antipattern to use above pattern for testing subjects? I wanted to contribute to observable testing, not spread bad sample code :-)\n. I had to switch to tape due to the lack of feedback on which test was failing, and thus stopping our TDD progress.\nI wrote this article as a conclusion on the subject. \nhttps://medium.com/@marcus.nielsen82/simplified-testing-of-user-events-in-rxjs-411efa02a341#.2n06fyhwr\nIt still holds true for Ava since it's basically the same API. \nBut I still feel the testing style of not using testScheduler in combination with using Subjects might be a bit controventional (although simple to use) for a larger base of RxJS users. \n. I actually swapped from tape/tap to Ava. But we had issues with tests not reporting where the error happened. It went faster to swap back than to find out what was wrong with Ava.\nThis was much clearer to our team of JS devs (3 persons): \n// Babel-register can runtime parse files after this file\nrequire('babel-register')({\n  sourceMaps: 'inline'\n})\nrequire('../server/test')\nrequire('../common/test')\nrequire('../client/test')\nThe above was better for us compared to the package.json setup with Ava where we didn't know what babel-register even was, what settings it had, and what else Ava was doing behind the doors to transpile our code.\nIt was a bit hard to follow what we had to do to get correct reporting on what row threw the error. We got the transpiled line numbers instead of the ES6 numbers.\n2nd, we dealt with a non-closing Socket.io server which meant that about 3 tests were not completing. \nOne thing I love right now with our setup, is that you just import your test files recursively as if it was normal js-code. (I wish the test runner was like reactDom.render and took a bunch of imported tests.)\nThe recursive imports made it easier to find the Socket.io bug since we could comment out whole parts of the file-tree until the bug went away.\nI wish I had more time to stick with different products, but we have the investor's stress right now. Once we get into a more stable phase with our product I hope we can help the OSS world more.\nI'd love to see Ava succeed, but I'd love to get a more slimmed experience where transpilation isn't a part of the tool. And also get some kind of feedback on what tests are done, and which are pending/timed out.\nI really hope the information above will help in some way. Helping is hard!\n. Cool! I'm happy to give feedback to anyone if it helps.\nGlob patterns is something I just don't benefit from.\nAnd it does add another level of abstraction before it hits your own code.\nWe just want to write code.\nAn aside (but not vital) is that we run docker-compose, and executing special commands is somewhat more complex.\nIf we change our package.json-file, it will download node_modules all over again.\nYou can run your container and then execute standalone commands against your running container, but that is hard for newcomers to reason about.\n. ",
    "skellock": "Hi @shtefanntz ,\nArticle author here.\nThat's strange.  __DEV__ should be available.  I wonder if, perhaps you've got an older version of Node installed?   \nnpm 2.14.7 looks a bit older.  Can you verify your node version too for me?\nI see you're on Windows too.  Any problems running React Native in general?\n. Thx for updating.  Node 6.3.x is out now, might want to try that.\nThe other thing, in your StackOverflow code, your package.json has a few issues with it:\n\nGive that a go.  If that doesn't work, let's hop over to https://github.com/skellock/bajeezus/issues and continue our conversation there.  \nI don't believe this is an AVA issue.  <3\n. I don't have an experience with CircleCI.\nHere's some really bad advice that might work:  If you have an AVA init script, throw a global.__DEV__ = true (or false if that's how you roll) in there and don't look back.  Never look back David.. ",
    "shtefanntz": "Hi Steve!\nI updated npm to version 3.10.5 by running  \n\nnpm i -g npm\n\nand it still doesn't get past that line  in react-native.js. \nThe version on node is 4.4.6.\nI don't remember specific problems running RN in general. Most of the time, they are spawned from my lack of understanding of the whole architecture of ReactNative or mobile development. But it works in the end :)\nI'm looking forward to any suggestions!\n. Indeed it's not an AVA problem, nor a problem of its dependencies. \nApparently, after installing the latest node version (i was using the lts one), deleting the node_modules folder and rerunning \n\nnpm i\n\nthe tests passed. I didn't change anything in package.json. \nThanks a lot for your advice!\nIf you want, post the answer in SO and I'll mark it as resolved :)\n. ",
    "davidosorio": "I have the same problem but when I am running the test in CircleCI, in my local machine are working as expected. Any idea what is happening ?\nI am using node version 7.4.0 and npm version 4.0.5\nThanks.\n. ",
    "jgkim": "It seems like this bug is also caused by #32.\n. ",
    "forivall": "And yet, I have to deal with a vendor that still hasn't moved off of 0.10. I'm aware that 0.10 will leave support soon, which frustrates me that vendor hasn't upgraded. No problem!\nMy workaround is to just do the require in a setTimeout to break the context:\ncall-loose.js\n```\nconst Promise = require('bluebird');\nmodule.exports = function (fn, arg) {\n  return new Promise(function (resolve) {\n    setTimeout(function () {\n      resolve(fn(arg));\n    }, 10);\n  });\n};\n```\nawait callLoose(require, './helpers/ava-bug')\n. ",
    "doug-wade": "```\ndougwade redfin.npm/core-ui \u2039dbw-core-ui-80-percent-coverage\u203a \u00bb time ava -c 10\n116 passed\nava -c 10  113.98s user 7.87s system 590% cpu 20.639 total\ndougwade redfin.npm/core-ui \u2039dbw-core-ui-80-percent-coverage\u203a \u00bb time ava -c 15\n116 passed\nava -c 15  117.13s user 8.00s system 727% cpu 17.197 total\ndougwade redfin.npm/core-ui \u2039dbw-core-ui-80-percent-coverage\u203a \u00bb time ava -c 5\n116 passed\nava -c 5  76.16s user 6.22s system 395% cpu 20.853 total\ndougwade redfin.npm/core-ui \u2039dbw-core-ui-80-percent-coverage\u203a \u00bb time ava\n116 passed\nava  99.11s user 7.69s system 706% cpu 15.107 total\ndougwade redfin.npm/core-ui \u2039dbw-core-ui-80-percent-coverage\u203a \u00bb time ava -c 7\n116 passed\nava -c 7  83.85s user 6.78s system 548% cpu 16.520 total\ndougwade redfin.npm/core-ui \u2039dbw-core-ui-80-percent-coverage\u203a \u00bb time ava -c 6\n116 passed\nava -c 6  79.69s user 6.51s system 522% cpu 16.486 total\n```\nlooks like 5 is a pretty good default; maybe 6 is better? Adding concurrency: 6 to my package.json is ~18% faster\n```\ndougwade redfin.npm/core-ui \u2039dbw-core-ui-80-percent-coverage\u203a \u00bb time ava\n116 passed\nava  94.35s user 7.38s system 715% cpu 14.213 total\ndougwade redfin.npm/core-ui \u2039dbw-core-ui-80-percent-coverage\u203a \u00bb vim package.json\ndougwade redfin.npm/core-ui \u2039dbw-core-ui-80-percent-coverage\u203a \u00bb time ava\n116 passed\nava  79.91s user 6.57s system 534% cpu 16.188 total\n```\nMy intuition is that the best default is likely Math.floor((os.cpus().length || 1) * 1.5)\n. ",
    "jonathanrdelgado": "Is this still being blocked? (#945 was closed). I'm happy to submit a PR to get some sane concurrency defaults.. ",
    "d4goxn": "I put together a better example that excludes Ava, to better confirm that the issue is with the test runner. This works as expected:\n``` js\nconst mongoose = require(\"mongoose\");\nconst db = mongoose.connect(mongodb://localhost/test-asdf, error => {\n    if (error) {\n        return console.error(\"failed to create db\", error);\n    }\nconsole.log(\"created db\");\n\ndb.connection.db.dropDatabase(error => {\n    if (error) {\n        return console.error(\"failed to drop db\", error);\n    }\n\n    console.log(\"dropped db\");\n    db.connection.close();\n});\n\n});\n```\ncreated db\ndropped db\n. ",
    "xpepermint": "Yeah... I saw that but just wasn't sure if that is a safe way :). Thx.\n. ",
    "colutti": "Thank you.\nThe gulp-ava link redirects me to my own question. Cant this be implemented in AVA instead?\n. ",
    "jordanh": "Adding 0.02 here. Our product is using the new CircleCI 2.0 beta. They provide an interface for collecting test timing information and providing statistics. You can use the information to configure CircleCI's test parallelization.\nIf I submitted a PR for timing, even if it's variable, would it be rejected?. Sure! It's a pretty simple. CircleCI expects a directory full of JUnit-formatted XML files (boy, do we ever need a better standard than this, but I'll reserve from making further judgments). The XML looks like this:\nxml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<testsuite name=\"rspec\" tests=\"727\" failures=\"0\" errors=\"0\" time=\"417.957921\" timestamp=\"2017-01-27T14:53:40+00:00\">\n  <properties/>\n  <testcase classname=\"spec.helpers.breadcrumbs.breadcrumbs_helper_spec\" name=\"Breadcrumbs::BreadcrumbsHelper returns a breadcrumbs array\" file=\"./spec/helpers/breadcrumbs/breadcrumbs_helper_spec.rb\" time=\"1.797002\"/>\n...\n</testsuite>\nToday, I get real close to this format by using --tap and emitting XUnit by piping ava to tap-xunit.\nThere are a couple of routes to making this work with ava:\n\n\nAdding XUnit/JUnit output directly to ava, and include: test suite start time, test suite duration, and test case execution duration\n\n\nProvide the same information as 1, but thread the timing information through on the TAP interface in YAML payloads\n\n\nOption (1) is more practical, I think, because there is no standard format for providing timing information in the TAP spec.\n. Could we view this as a simple map/reduce pattern? map() over testcases, reduce() over testsuites.\nMay I look into a patch and come back with real data vs. speculation?. ",
    "hbarcelos": "Any updates here?. ",
    "ChristianMurphy": "// @SamVerschueren\n. Updated with ...args: any[]\n. Done and Done\n. Updated with T as first type param\n. > Should we call this title: string instead of providedTitle: string? \nI would agree with @ivogabe I would lean against renaming, it could be confusing. Additionally providedTitle is how this is documented in the readme.\n\nI did a quick test and autocompletion doesn't show the argument name when implementing it like this\n\nI've noticed autocomplete is not suggesting as well, though the type checker is validating correctly.\nI'm open to suggestions on how to improve the typing to get autocompletion here.\n. Updated to make input and expected optional\n. Good catch, updated\n. ",
    "DDChiang": "The reason behind my goal of exporting tests and including them in another file is so I could start separating out the tests into different sections depending on what I'm testing(\"component behavior\" vs \"component rendering\"). Right now, I'm stuck listing all the tests in one long file. I was hoping to be able to break them up into shorter sections.\n. @vdemedes Could you elaborate more on what you mean by place your tests along with before hooks in MainTest.js?\n. Ahh I see. But if you place the \"before hook\" into the same file as the test, then we can't use the values resolved asynchronously in the \"before hook\" for other tests inside other files. \nPerhaps I should have been more clear. I'm hoping to find a way to allow other test files(that will be exported) to use the resolved values returned from the same \"before hook\"\n. @vdemedes @novemberborn thanks for your input! I ended up using the solution with the \"test.beforeEach\" hook and Object.assign. Did exactly what I wanted it to do in assigning the resolved variables and allowing them to pass into the exported test. \nThanks to you both for your help! \n. ",
    "timwis": "I'm configuring in .babelrc and also experiencing this issue. My source compiles fine, but ava throws an error about Unexpected token ... on ...mapActions([.\nMy package.json has:\njson\n{\n  \"ava\": {\n    \"require\": [\n      \"babel-register\",\n      \"./test/.setup.js\"\n    ],\n    \"source\": [\n      \"**/*.{js,vue}\",\n      \"!dist/**/*\"\n    ],\n    \"babel\": \"inherit\"\n  }\n}\nAnd my .babelrc\njson\n{\n  \"plugins\": [\n    [\"transform-object-rest-spread\", { \"useBuiltIns\": true }]\n  ]\n}. ",
    "kaelzhang": "But it is a string, not a real comment.\n. Yes, I had made it right by changing it to something else. But there is something wrong that is necessary to report.\n. Fixed in master\n/close. @delvedor you're right. The same issue \ud83d\ude22 . The same issue \ud83d\ude22 . json\n\"babel\": false. json\n\"babel\": false. ",
    "eugirdor": "In the Windows command line, you can change your drive letter casing by switching to a different drive and changing directories on the C: drive. (Assuming you have a D: drive, the following commands will work):\n```\nC:>D:\nD:>cd c:\\\nD:>c:\nc:>\n```\n. ",
    "RyanCavanaugh": "Getting off-topic, but TypeScript has private members because people really really really really want private members. It's not enforced at runtime because there's not (in ES3 particularly) a way to do this that doesn't result in weird code mangling, has terrible performance implications, or usually both. Often people want private fields that are \"sometimes\" accessible (e.g. from unit tests, not that I endorse checking private members as part of a test suite) and having the fields be runtime accessible under the normal name while still getting type system enforcement is a very useful compromise.\nIf you read the TypeScript issue tracker you'll see over and over again us refusing to add trivial syntactic sugar because it could conflict with future JS features. The risk is taken quite seriously.\n. ",
    "DanielRosenwasser": "\nOne of the big differences between Flow and TypeScript is that in Flow if you strip the types you have plain JavaScript code and it behaves exactly as expected, we don't add any runtime features, and we don't modify any behavior whatsoever.\n\nTypeScript doesn't do any type-driven emit either. If you don't want to use namespaces or enums, you can feel free not to, the same way that you don't necessarily have to use certain features in Babel.\n\nI'm not sure why TypeScript has added private fields to JavaScript while also not making the runtime enforce them at all\n\nFor the exact same TypeScript has types but doesn't do any dynamic runtime checking. Flow takes a page from that book too. The same way that types are a compile-time only thing, and that's how we view accessibility as well.\n. I disagree with the view that the pattern used here isn't the purpose of a type system. A type system can serve a lot of different purposes. I think it ultimately depends on what the authors wanted, so @sindresorhus and others should probably make that call.\nFor what it's worth, I managed to pretty quickly convert your declarations down to this .d.ts file as well.\n. ",
    "sunny-g": "Possibly related to this? https://github.com/Jam3/devtool/issues/74\n. ",
    "rickmed": "@twada Both print the same thing:\n```\n\nava --verbose\n\n\u00d7 My Test description THIS IS AN ASSERTION MESSAGE\n1 test failed [08:59:39]\n\nMy Test description\n  AssertionError: THIS IS AN ASSERTION MESSAGE\n    Test.t [as fn] (avaTests.js:48:3)\n    _combinedTickCallback (internal/process/next_tick.js:67:7)\n    process._tickCallback (internal/process/next_tick.js:98:9)\n\nnpm ERR! Test failed.  See above for more details.\n```\n. ",
    "ngerritsen": "@forresst But how does your test succeed now?\n. ",
    "DenisCarriere": "Thanks for the feedback, it does sounds like an issue with the Typescript.\nCheers,\n. ",
    "rcorrear": "There's also a few things missing, in test.beforeEach/afterEach the param \"implementation\" (callback) should be as far as I understand a ContextualTest so it provides you with a 'context'; this is preventing me from upgrading to 0.16. Also something that would be nice to have is a before/after[Each] 'always' which is completely missing from the picture and which I guess would have to be added to every namespace that allows it.\nIs anyone still working on this? Is there any way I can contribute?\n. ",
    "kennetpostigo": "@sindresorhus I'm not looking for the verbose output at all \ud83d\ude05\nI'm wondering why this doesn't appear:\n\nAll that appears is two tests passed but doesn't provide the name of it with the checks like above.\n. @sindresorhus hadn't realized that. Looking at the Readme I thought that was default ouput.\n. ",
    "ryyppy": "Will have a look! It's very similar to tape's API anyways\n. @thejameskyle @sindresorhus Looking good! \nSeriously, I really tried to break the flow definitions... instead it safely lead me through the ava API... as a second thought, it really is very different to tape, especially without any nested tests.\nOther than that: Is there a reason why you don't put this in flow-typed instead?\nI thought it would be problematic to track declaration files in the maintainers repositories, because of breaking flow versions etc.?\n. NOTE: I found one case which wasn't really supported by flow:\ntest.after.always('foo', t => {\n  t.is //<-- from here flow cannot tell me what's `t`\n});\n. ",
    "prigara": "Debugging in WebStorm works as described only with #874 \n. Looks good to me. That's for updating the recipe.. ",
    "jasongin": "I'm trying to debug using VS Code. I got the fix above to resolve the EADDRINUSE issue. But now the problem is the debugger only attaches to the parent process, so the 4 child processes sit waiting forever for a debugger that never attaches.\nnode --debug-brk=15326 --nolazy node_modules\\ava\\cli.js --serial --verbose build/test/*.js \nDebugger listening on port 15326\nDebugger listening on port 2027\nDebugger listening on port 2028\nDebugger listening on port 2029\nDebugger listening on port 2030\nWhy does ava launch 4 child processes, even when the --serial flag is passed? I think to successfully debug it would need to run the tests directly in the main process.\n. ",
    "korsmakolnikov": "@jasongin still having this issue in phpstorm. Doesn't share same enging with webstorm?. ",
    "apedyashev": "Thank you, guys for your help. It seems to work with following configs:\nconfig/webpack.config.ava.js\n``` javascript\nconst path = require('path');\nconst PATHS = {\n  app: path.resolve(__dirname, '../src'),\n};\nmodule.exports = {\n   resolve: {\n     modulesDirectories: [\n       __dirname,\n      'node_modules',\n       PATHS.app,\n     ]\n   },\n   module: {\n     loaders: [\n       {\n         test: /.css$/,\n         loader: 'style-loader!css-loader?modules&importLoaders=1!postcss-loader',\n       },\n     ]\n   }\n};\n```\npackage.json\njavascript\n{\n  ....\n  \"scripts\": {\n    \"test\": \"CONFIG=$(pwd)/config/webpack.config.ava.js BABEL_DISABLE_CACHE=1 NODE_ENV=AVA ava\" \n    },\n  ....\n  \"ava\": {\n    \"files\": [\n      \"src/**/*.spec.js\"\n    ],\n    \"require\": [\n      \"babel-register\"\n    ],\n    \"babel\": \"inherit\"\n  }\n}\n.babelrc\njavascript\n{\n  \"presets\": [\"es2015\", \"react\", \"stage-1\", \"stage-0\"],\n  \"env\": {\n    \"AVA\": {\n      \"plugins\": [\n        [\n          \"babel-plugin-webpack-loaders\",\n          {\n            \"config\": \"${CONFIG}\",\n            \"verbose\": false\n          }\n        ]\n      ]\n    }\n  }\n}\n. Btw, I have another solution of my problem (without using webpack):\nenv NODE_PATH=src ava\n. ",
    "Knorcedger": "Tnx @apedyashev !! This should be in a recipe!\n. ",
    "greyepoxy": "I put together a slightly different approach to this problem https://github.com/greyepoxy/webpack-ava-recipe. Where I use webpack as a pre-compile step for my tests before feeding them to Ava.\nCurious what people think, is it better/worse then @apedyashev 's approach? Happy to put a recipe together if people think it is useful.\n. ",
    "StefanoSega": "or is there a way to use AVA with Karma?\n. Hi Sindre, thanks!\nI already posted the issue on StackOverflow:\nhttp://stackoverflow.com/questions/39053566/ava-unit-test-use-gulp-ava-to-test-global-functions\n. ",
    "shaun-stripe": "sorry, wrong repo\n. ",
    "MiguhRuiz": "I've solved it reinstalling ava.\n. ",
    "vikfroberg": "Created an issue instead: https://github.com/avajs/ava/issues/1138. > The option in package.json seams to be working though.\nTurns out this was not true.. > @vikfroberg I suspect you might need to put the --no-power-assert flag before the dist/test.js argument.\n@novemberborn doesn't matter where I put it unfortunately. Turns out ava --no-power-assert --no-cache dist/test.js works, so it seems to be a caching issue.. ",
    "KnisterPeter": "Sorry, already fixed by #1008\n. ",
    "rhubarbselleven": "This is quite straight forward:\n``` js\nimport test from 'ava';\nlet webdriverio = require('webdriverio');\nlet client = webdriverio.remote({\n    // just using a local chromedriver\n    desiredCapabilities: {browserName: 'chrome'}\n});\ntest.before(async t => {\n    await client.init()\n        .url('http://localhost');\n});\ntest.after.always(async t => {\n    await client.end();\n});\ntest('has a body', t => {\n    return client.isExisting('body').then(result => {\n        t.true(result);\n    });\n});\n```\nI've only had success using AVA and webdriverio together when running tests serially \n. ",
    "nreijmersdal": "I did have success running tests in parallel with selenium-webdriver package.\nYou do need to start a Selenium server, for example with the webdriver-manager package.\nHere is my example code:\n``` javascript\nimport test from 'ava';\nimport webdriver from 'selenium-webdriver';\ntest.beforeEach(t => {\n  t.context.driver = new webdriver.Builder()\n                        .forBrowser('chrome')\n                        .usingServer('http://localhost:4444/wd/hub')\n                        .build();\n});\ntest('Search for webdriver', async t => {\n      let driver = t.context.driver;\n      await searchGoogle(driver, 'webdriver')\n      t.is(await driver.getTitle(), \"webdriver - Google Search\");\n      await driver.quit();\n});\ntest('Search for avajs', async t => {\n      let driver = t.context.driver;\n      await searchGoogle(driver, 'avajs')\n      t.is(await driver.getTitle(), \"avajs - Google Search\");\n      await driver.quit();    \n});\ntest('Search for concurrent', async t => {\n      let driver = t.context.driver;\n      await searchGoogle(driver, 'concurrent')\n      t.is(await driver.getTitle(), \"concurrent - Google Search\");\n      await driver.quit();    \n});\nasync function searchGoogle(driver, keyword) {\n      await driver.get('http://www.google.com/ncr');\n      await driver.findElement(webdriver.By.name('q')).sendKeys(keyword);\n      await driver.findElement(webdriver.By.name('btnG')).click();\n      await driver.wait(webdriver.until.titleIs(keyword + ' - Google Search'), 1000);\n}\n```\n. @aptester Sorry no idea. Also I am not using Ava anymore. Running each test in parallel doesn't make a lot of sense anyways. Maybe it is easier to group sets of tests and create a couple of command-line jobs to start the groups in parallel. This is what I used to do when we used Java for Selenium. Goodluck.. ",
    "aptester": "@nreijmersdal After running the above code, the tests are getting executed serially. Getting the below results with 'ava --verbose' \n\u221a Search for webdriver (6.6s)\n  \u221a Search for avajs (10.7s)\n  \u221a Search for concurrent (14.9s)\nI'm running ava@0.22.0, selenium-webdriver@3.5.0 and webdriver-manager@12.0.6 which uses selenium-server-standalone-3.5.3.jar. Any ideas what change is now required to get the tests running in parallel?\n. ",
    "VladimirDev93": "I have found a working demo here.. ",
    "danprince": "Thanks @jamestalmage. Looking at the AVA branch again, I realise I slightly misreported the problem I was having.\n\nWhen a test fails I get the message, but none of the details I'd need to actually start fixing it. Seems like this might just be the behaviour for the default reporter?\nI tried the tap reporter to check and it does report the failing conditions, but it seems like it uses .toString to show the actual and expected values.\n\nFor comparison here's what I would see from the same tests run under tape without a reporter.\n\nIs there a way to enable this kind of reporting with either the default reporter, or the tap one?\n. > Try using t instead of assert as your parameter name (using t should enable power-assert).\nReally?\nIt seems to work, but that's kinda voodoo.\nAll the same, if possible I'd prefer to stick with tap-spec and the output produced with the --tap flag still shows [object Object] even using t rather than assert.\nHappy to make the PR if this just needs a more thought out string converter for the tap reporter.\n. ",
    "smeijer": "\nTry using t instead of assert as your parameter name (using t should enable power-assert).\n\nI had this exact same issue. I guess it's worth a note in the readme, perhaps under FAQ.\nWhy is the power-assert information not shown? \n\nTry using t instead as your parameter name. \nThis is a byproduct of the way power-assert works. It uses a pattern matching scheme that makes it easier for implementors to wrap any assertion library with power-assert goodness without having to understand the ES AST at all\n. \n",
    "ngryman": "Hey @jfmengels,\nIn the algorithm I proposed, all failing tests are re-run, so if you break things you'll know. Let's call this set of tests a scope. If you break tests out of the scope, you'll know it when you fix all tests in the current scope. They end up being run, as the algorithm run all tests when no test is failing in the current scope.\nNot all tests have a direct relationship to each other. Directly related tests are often located in the same file and mostly fail fast if you break something. So there are chances that the first batch of failing tests will be the only one.\nI know this proposal seems weird to read, and my english does not help, but when you think about it, it's exactly what you do manually. You rarely want all your test suite to be run on each file save. You isolate tests you want to focus on. And when they pass, you check that the whole test suite passes and you didn't break anything at a larger scale.\nThat's some kind of local ci, I don't know the term :bowtie:\nAnother way to define my proposal would be to implement an evolutive test scope: test current file first, test failing first.\nLet's illustrate with a typical scenario:\n| Modification | Test 1 | Test 2 | Test 3 | Test 4 | New test |\n| --- | --- | --- | --- | --- | --- |\n| Add test | \u221a | \u221a | \u221a | \u221a | \u2717 |\n| Fix 1 | \u2205 | \u2205 | \u2205 | \u2205 | \u2717 |\n| Fix 2 | \u221a | \u2717 | \u2717 | \u221a | \u221a |\n| Fix 3 | \u2205 | \u2717 | \u221a | \u2205 | \u2205 |\n| Fix 4 | \u221a | \u221a | \u221a | \u221a | \u221a |\n\u221a: Pass\n\u2717: Fail\n\u2205: Skip\n. Thanks for your time and feedback man.\n\nThere's also an issue with the algorithm. You only re-run the tests that failed, even if all previously failed tests pass this time.\n\nFirst I should have precised that while writing the second comment, I slightly changed the algorithm. If all of the tests in your scope pass then all the other tests are also run in the same batch. It avoids to manually get ava to re-run all tests.\nSo your table becomes again:\n| Modification | Test 1 | Test 2 | Test 3 | Test 4 | New test |\n| --- | --- | --- | --- | --- | --- |\n| Add test, all run, test 5 fails | \u221a | \u221a | \u221a | \u221a | \u2717 |\n| While test 5 fails, run test 5 | \u2205 | \u2205 | \u2205 | \u2205 | \u2717 |\n| When test 5 gets fixed, run all, test 2 & 3 fail | \u221a | \u2717 | \u2717 | \u221a | \u221a |\n| When test 3 gets fixed, run 2 & 3, test 2 fail | \u2205 | \u2717 | \u221a | \u2205 | \u2205 |\n| When test 2 gets fixed, all run, all pass | \u221a | \u221a | \u221a | \u221a | \u221a |\n\nYou still won't get the feedback that you broke other tests as long as you don't fix all the previous failing tests.\n\nYes and for me that's the whole point :smiley:! And that's already the case with the current watch implementation anyway, I'm just reducing the scope.\n\nIf I may explain a bit more why I think this feature would be great, instead of going down to the implementation itself.\nI think we should separate test driven development and integration testing. When you add a new feature or fix a bug on your machine, you use watch and you are in tdd mode. You write your test and focus only on making it pass. You make a lot of code changes, want debug, traces, ... That's the first step. That's the 3 laws of Uncle Bob.\nThen when you're satisfied with your implementation and your unit test pass, comes the integration testing phase where you sanity check that your modifications to the codebase did not add regressions. Most of the time it should be ok.\nBack to the old days or for huge projects, integration testing phase was often defered to pre-commit or even only run remotely on a ci server because of compilation time or test suite time. That's the second step.\nThe mode/optin I'm :fist: for is for tdd. It's made precisely to only test what is directly related to your feature/bug and discard on purpose the rest while this precise test doesn't pass.\n. @novemberborn Yes, my proposition was to only re-run already failing tests until they pass. If there are technical limitations on this, I can understand.\nI had 3 requirements to satisfy:\n1. Be able to focus on the current test or set of tests without extra noise.\n2. Optimize the time to first failing test (TTFFT).\n3. Be able to debug easily without having to play with the only modifier manually.\nI guess 3. is optional and quite complex to achieve in the sense that it would require some sort of intelligence. That's probably totally out of scope of a test runner.\nBut I think 1. and 2. may be achieved with minimal modifications. We could prioritize failing tests to run first, then the others. It would allow to reduce drastically TTFFT (2.) and used in fail-fast mode achieve 1..\nA bonus would be to prioritize failing tests by number of failures. If a test fails since 10 runs, it's less important that a test that just broke.\n. @sindresorhus Awesome. Thanks!. ",
    "tw0517tw": "Got same result with\nNode.js v6.5.0\nwin32 10.0.14393\nava 0.16.0\nnpm 3.8.7\n. ",
    "chentsulin": "@sindresorhus @istarkov Any thoughts?\n. @istarkov There is a demo repo for this issue (minimal reproduce example) just keep configuration same as my real project:\n- Github Link\n- Travis CI Link\nMaybe I did something wrong?\nI will remove babel-plugin-webpack-loaders in my tests, but I'm curious why behavior differences between mocha and ava, maybe there is a potential bug here.\n. ",
    "rainydio": "Well, yes:\n- For specific test\n- Initialization and disposal are nearby in code.\n- When you need to dispose external resources which become available during test execution.\n- No need to put data into context, and take it back. Just pass the callback.\nAnother example\n``` js\ntest(\"new repo fields\", async t => {\n  const user = await api.users.create({ username: \"test\" });\n  t.finally(() => api.users.delete(user));\nconst repo = await api.repos.create(user, { name: \"my-repo\" });\n  t.finally(() => api.repos.remove(repo));\n});\ntest.beforeEach(async t => {\n  t.context.user = await api.users.create({ username: \"test\" });\n  t.context.repo = await api.repos.create(user, { name: \"my-repo\" });\n  // ^-- what if exception is raised here\n})\ntest.afterEach.always(async t => {\n  await api.repos.remove(t.context.repo); // then this fails\n  await api.users.remove(t.context.user); // this one isn't freed\n});\n```\nIf you have dangling resources, those start affecting other tests. Like Attempted to wrap load which is already wrapped etc. So I find myself wrapping everything into try { } finally { }. And that's all fine, but resource allocation and disposing are away from each other, create nesting, and require variable outside of scope. So I start grouping them together and hitting the same problem.\nThis is an attempt to solve it.\n. No problem with rejecting that.\n. ",
    "revelt": "I tend to solve string .length problem like Sindre \u2014 convert the string to an array using lodash.toarray, then measure it using Array.length (or perform operations on that array, later .join('')-ing back \ud83d\ude0a).\n. Good spot, it's HyperTerm, I can't recreate the issue on iTerm2 too... Let's close this issue, thanks for clearing this.\n. OK, I see, we can identify which test didn't threw by row number. That's acceptable then.\n\nGood job with 0.19.1 release!. Hi Sindre, guys, I'll take down the PR, but feel free to amend, reuse or replace the text whichever way you want (or drop it completely).. @sindresorhus that's fine, feel free to fix it and publish. It will be more effective, less backwards forwards. Seriously, no hurt feelings. I'll help all I can and keep contributing.. You know Monopoly, it was a popular game but its rules inadvertently alienated the families. It was people winning against people, not a family winning together against imaginary \"Monopoly\". I find GitHub a bit of a similar thing. Documentation editing on OS projects should be like a happy Google Docs editing session: somebody writes up something new, others jump in and everybody edit the damn thing TOGETHER, quickly, happily and in live mode. Like a public Google Doc. If admin/maintainer does not like certain character, he just edits it over and happy days. As opposed to formal \"requests\" and rubbing in to original poster's ego (no offence, I'm talking in general). I know I was not entirely exact on some places, but \"requests\" to amend make it public which hurts. It's like Monopoly, giving away your albeit imaginary money and failing in front of siblings. Also, you know, any change, even to readme, triggers CI on GitHub. They should review the whole documentation concept a bit - how people work together.. For the record, the new .only behaviour is quite annoying and actually impacts my productivity. I often separate the secondary library's functionality into util.js and therefore, unit tests go separately too. When I troubleshoot code, I rely on console.log's and it is especially important that I should be able to isolate that one particular unit test, otherwise, I'll get multiple console.logs from multiple unit tests.\nNow, if we'll end up with functionality as Mark says, where t.only kicks in only after first only is encountered, it's pretty much a futile feature \u2013 I just checked, nearly always my util unit tests run first in the queue and therefore would contaminate the console. We need something more.\nMaybe it's a stupid idea, but why can't we just run a glob against all recognised test files and look for any .onlys in the unit tests? Then, have a think what to do, run .onlys or run everything as it is now? Basically, a one, additional, separate round in the execution chain??? Because we're already globb'ing all test files, we just need to scan them first, before action commences.\nWhat do you think?. Yeah, I'm on 0.25. hi guys! --reset-cache is not documented in readme! raised https://github.com/avajs/ava/issues/1929 about this. hi guys! --reset-cache is not documented in readme! raised https://github.com/avajs/ava/issues/1929 about this. @jdalton the screenshot shows ava was called via npm test script and in my case it's \"sandwitched\" with nyc. There, esm is called in front. This approach seems to work OK.\nI'm talking about calling ava directly from command line, by typing ava. I was expecting esm to pick up the esm config from package.json from ava's config:\njson\n\"ava\": {\n    \"compileEnhancements\": false,\n    \"require\": [\n      \"esm\"\n    ],\n    \"verbose\": true\n  }, \nbut it looks as if esm didn't perform its CJS duties.\nAgain, I'm talking about calling ava through command line by just typing ava.\n\n\nHaving said that, I did some more digging and found very strange things. Sindre was right, \"await\": true, does not affect anything, ava call will work without it in esm config in package.json. Strange thing is, I can remove \"cjs\": true too \u2014 as long as empty key:\njson\n\"esm\": {\n  },\nis present in package.json, ava call will work. But if you remove it, SyntaxError: Unexpected identifier error pops up. That's very strange. Try yourself.\nIt's as if the presence of an empty key esm in package.json enables cjs compat esm.... Thanks a lot! Closing this.. To add my 2p, I never expected ava to transpile my imported files in addition to my test files and documentation told that somewhere. @chrisdothtml you're a rare case :)\nPersonally I found the ava's recipe about bypassing the ESM transpiling a little outdated (talking about Node 8) and a bit hit-and-miss in general. Things are explained but partially. However, I tried out all setups until one worked and settled with that. There's no nyc mentioned there!\nAll my libraries' sources are in ES6 and I use Rollup to transpile to ESM+CJS+UMD and the main reason behind pointing ava towards ESM build rather than CJS is coverage.\nIf transpiled CJS files are used to calculate coverage, all functions that Babel adds are left out and the scores suffer (not to mention CJS code lines differ in Coveralls.io from ESM source, for example).\nIt's ideal when source is ESM and ava runs on ESM build (no transpiling) and coverage is calculated on ESM build.\nSadly, since Babel 7 the usual nyc --require esm --reporter=html --reporter=text ava\" does not work and I had to temporarily remove the coverage completely from all my libraries until I find the way and/or dependencies catch up with Babel 7.\nLooking at higher level, Rollup does the transpiling and it references the Babel config in library's root. I've got @babel/core as dev-dependency. But then, nyc pipes ava through esm \u2014 the whole existence of esm seems questionable to me \u2014 in theory, why can't all the parts of the code use the same @babel/core from node_modules and use @babel/preset-env referenced in library's root? Why do we even need esm? But that's more stones to nyc rather than ava's yard. You guys are doing great and I look forward to proper, semantic 1.0.0 release.. To add my 2p, I never expected ava to transpile my imported files in addition to my test files and documentation told that somewhere. @chrisdothtml you're a rare case :)\nPersonally I found the ava's recipe about bypassing the ESM transpiling a little outdated (talking about Node 8) and a bit hit-and-miss in general. Things are explained but partially. However, I tried out all setups until one worked and settled with that. There's no nyc mentioned there!\nAll my libraries' sources are in ES6 and I use Rollup to transpile to ESM+CJS+UMD and the main reason behind pointing ava towards ESM build rather than CJS is coverage.\nIf transpiled CJS files are used to calculate coverage, all functions that Babel adds are left out and the scores suffer (not to mention CJS code lines differ in Coveralls.io from ESM source, for example).\nIt's ideal when source is ESM and ava runs on ESM build (no transpiling) and coverage is calculated on ESM build.\nSadly, since Babel 7 the usual nyc --require esm --reporter=html --reporter=text ava\" does not work and I had to temporarily remove the coverage completely from all my libraries until I find the way and/or dependencies catch up with Babel 7.\nLooking at higher level, Rollup does the transpiling and it references the Babel config in library's root. I've got @babel/core as dev-dependency. But then, nyc pipes ava through esm \u2014 the whole existence of esm seems questionable to me \u2014 in theory, why can't all the parts of the code use the same @babel/core from node_modules and use @babel/preset-env referenced in library's root? Why do we even need esm? But that's more stones to nyc rather than ava's yard. You guys are doing great and I look forward to proper, semantic 1.0.0 release.. @robertbernardbrown Robert, feel free to pick it up, I'm too busy atm. Thank you. I extended the tick to include the word hook too. Let's do babel-register as a separate commit, let's get nodemon example sorted first. What do you think?. Reworded the title completely. Like Mark noted, it was not accurate actually, we can't say \"AVA's watch\" because it's \"nodemon's watch\". Please review.. Done, good spot. I remember some of my editors had issues in the past recognising console but it's probably an old reflex. Thank you. Please review.. I considered your point, slept over it and considered again. This line is called by .travis.yml. Without it, Travis won't be able to ping Coveralls. We can omit it, but then we should note aside that scripts snippet is incomplete. Some people might think it's complete, paste gung-ho and their Coveralls won't update. Being a lazy person, I like complete snippets, ones without missing bits.\nI'm happy to remove this line, just say no and I'll remove it.. Done. Please review. I tested the separated setup on one of my libraries, seems fine on mine. Thank you, that's a new thing learned. \ud83d\udc4d . Done good spot. Please review.. ",
    "MKRhere": "I ran into this issue while looking for an API. Since it's been two years, was there ever any work on this?. I understand the reasons behind the decision, I think. I strongly needed an API-first framework, so I've built one in the meanwhile. It doesn't do parallelisation yet, of course :)\nThanks a lot for responding! Overall a great job.. ",
    "omerbn": "Hey guys,\nI'd like to do it but need further depiction.. ",
    "yatharthk": "@novemberborn what keys in particular are to be mapped in configuration object? Like the keys we want to be used as plurals. . @novemberborn Thanks. Shall I pick up the task of pulling out the default: conf when calling meow?  Is there a relevant issue or we need to create one? . @novemberborn Thanks for pointing that out for me. \ud83d\udc4d . @novemberborn Would love to. Can you please guide me with proceeding on this? Thanks.. @ThomasBem I'm planning to pick this issue for removing the default: conf from call to meow. Would that be fine or you are doing that?\nAlso, we aren't going to deal with any cli flags here, for now; if I'm not wrong. Please correct my knowledge if I'm wrong over here. @novemberborn . @novemberborn I'm working on this now. Would be great if you could hint me on the right approach to merge cli object with conf. What I can understand is default creates flags on the cli object. Is it something like we merge conf into cli.flags or something? I tried this but I don't think this is something we need to do!. @novemberborn So here's what I think: Instead of passing conf to meow as default, let's keep it separate and override it's properties with those of cli.flags, maybe something like Object.assign(conf, cli.flags) and then we can use conf in place of cli.flags everywhere. If this makes sense, I'll go ahead and make the changes.. Okay I got your point here. The thing that's bit tricky for me here is understanding what you talk about picking default values out of conf. As I understand, it already contains all the default values. Shouldn't we be picking the entire conf? How do I approach the picking of defaults from conf? \ud83e\udd14 . @novemberborn Do we need to work upon this bug?. @vadimdemedes thanks for pointing it out. I actually faced this thing last night and was thinking to open an issue. . @sindresorhus Happy to contribute \ud83d\ude04. Thanks to @novemberborn for all the help!. @sindresorhus I am picking this up. But not really sure if we need to make this improvement in ava-init package or should we do it to write-pkg itself? \ud83d\ude04 . @sindresorhus Cool, then I'll should open an issue and work for indent-preservation on write-pkg.. @novemberborn We need to add a warning over here and that's it or something more is expected? Are new tests required?. @novemberborn I wrote this code:\njavascript\nif (hasFlag('--concurrency') || hasFlag('-c')) {\n        throw new Error(colors.error(figures.cross) + ' The --concurrency and -c flags must be provided the maximum number of test files to run with.');\n}\nwith this test:\njavascript\ntest('bails when config does not contain `--concurrency` and `-c` values', t => {\n    execCli(['test.js'], {dirname: 'fixture/concurrency'}, (err, stdout, stderr) => {\n        t.is(err.code, 1);\n        t.match(stderr, 'The --concurrency and -c flags must be provided the maximum number of test files to run with.');\n        t.end();\n    });\n});\nwhich fails with this error:\nbails when config does not contain `--concurrency` and `-c` values\n  not ok spawn /Users/yatharthk/.node-spawn-wrap-62339-36f9a5f0c8ca/node ENOENT\n    at:\n      line: 1026\n      column: 11\n      file: util.js\n      function: exports._errnoException\n    code: ENOENT\n    errno: ENOENT\n    syscall: spawn /Users/yatharthk/.node-spawn-wrap-62339-36f9a5f0c8ca/node\n    path: /Users/yatharthk/.node-spawn-wrap-62339-36f9a5f0c8ca/node\n    spawnargs:\n      - ../../../cli.js\n      - test.js\n    test: bails when config does not contain `--concurrency` and `-c` values\nCan you help me understand what's going on? Probably because I also want to understand what does {dirname: 'fixture/concurrency'} config does exactly!. @novemberborn Any progress on this?  cc/ @sindresorhus . @sindresorhus @novemberborn Happy to contribute :). Oh, I don't know how I missed out on this. Was stupid! \ud83d\ude1b So what you're suggesting here is something like color: conf.color ? conf.color : true, right?. Sure. \ud83d\udc4d . I get it. Then I guess we can use (conf.color !== undefined) ? conf.color : true. Any thoughts? . Cool. No problem. I wasn't so sure about that. You just enhanced my knowledge. Thank you. :) . I pushed the changes. You can check them now!. @novemberborn This test fails with error below:\nbails when config does not contain `--concurrency` and `-c` values\n  not ok spawn /Users/yatharthk/.node-spawn-wrap-62339-36f9a5f0c8ca/node ENOENT\n    at:\n      line: 1026\n      column: 11\n      file: util.js\n      function: exports._errnoException\n    code: ENOENT\n    errno: ENOENT\n    syscall: spawn /Users/yatharthk/.node-spawn-wrap-62339-36f9a5f0c8ca/node\n    path: /Users/yatharthk/.node-spawn-wrap-62339-36f9a5f0c8ca/node\n    spawnargs:\n      - ../../../cli.js\n      - test.js\n    test: bails when config does not contain `--concurrency` and `-c` values\nI'm not really sure what's expected here. Can you also let me know if {dirname: 'fixture/concurrency'} is correct? I didn't understand how this works!. @sindresorhus I see. In that case, ((hasFlag('--concurrency') || hasFlag('-c')) && !cli.flags.concurrency) should work good, right?. Okay, okay. Thanks for explaining.. @sindresorhus I read the test code in detail to understand how tests are written for cli! Do we need to write it in a similar way as we do for watcher et al? Any specific cases to cover?\ncc/ @novemberborn . @novemberborn This works! Can you tell me what all cases to cover in test/fixture/concurrency/test.js ?. @novemberborn This test doesn't seem to pass. Can you see any  fault in the match()?\n. @novemberborn This tests for success if the value is provided for the flags. Looks good?\n. @novemberborn Thanks. Tests pass now. :)\ncc/ @sindresorhus . ",
    "alextes": "Love this proposal. Eliminating the constructor vs. validator function ambiguity would've saved me a lot of time and confusion (example).\nIs there still something to be done on the 'question' side of things? It's hard to give a helpful push without a clear direction \ud83d\ude04.. @sindresorhus I feel the consensus was around supporting common use-cases for error comparison. In other words, offer shortcuts to common comparisons but keep the API surface small as one can already do any comparison with the returned error. 'The other day' only tells us about recency, how about frequency? I think this decision is a tricky balance between frequency, the cost of doing without, and diff potential. Personally I'm leaning towards leaving it out, like novemberborn.\n@novemberborn I'd actually lean against Sindre's suggestion of possibly adding a name matcher. Can I ask, when would a name be preferred over a constructor? I can't think of anything. That for me makes this an infrequent help at best, which for simplicity's sake I would then leave out.\nOne more detail I'd like to bring to the attention of the jury \ud83d\ude04. Sindre Proposal also adds backwards compatibility for the second argument, when not a matcher object, of type string / regex / constructor.\n\n\nRegex\nMatch on string representation of actual error.\n\n\nConstructor\nMatch constructor function by reference.\n\n\nString\nCurrently unsupported by Node. Is this one to be an exact match of the string representation of the error? Or just the message of the error?\n\n\nValidation Function\nCurrently supported by Node. We drop it. The reasoning being this is again custom and not commonly useful checking. Thus up to the user to do with the returned error.\n\n\nMy fingers are starting to itch to write the implementation on this one \ud83d\ude1b.\n. Thanks @novemberborn and @sholladay! I understand the need and even preference for name now.\nI didn't know Ava's assert has seen some big change. Congrats on the big PR! I read through ava's assert module two weeks back, since then #1302 landed. Forgive me for missing Ava no longer almost directly relies on Node's assert. \nComparing just the message makes a lot of sense to me if we already support the name separately. It would break with Node's behavior. I have no idea how to weigh the two. (I feel your argument to make it simpler is more compelling.)\n\nI didn't even know that's how it worked!\n\nHappy to be able to contribute something small \ud83d\ude04.. ",
    "dashmug": "Thank you for working on this. \nI'm following this thread as I have several macros in my project that I want to reuse in different test files.\n. ",
    "jdalton": "Hiya!\nOk so lodash.isEqual supports comparing:\n\narrays, array buffers, booleans, date objects, error objects, maps, numbers, Object objects, regexes, sets, strings, symbols, and typed arrays. Object objects are compared by their own, not inherited, enumerable properties. Functions and DOM nodes are not supported.\n\nIt is an equivalence method so somethings may be seen as equiv that aren't strictly equal, like 1 and Object(1) (legacy rules). The behavior can be customized with lodash.isEqualWith.\njs\nfunction deepEqual(a, b) {\n  return isEqualWith(a, b, function(a, b) {\n    if (typeof a !== typeof b) {\n      return false;\n    }\n  });\n}\n. > I assume it does strict value equality checks?\nFor primitives yep. For objects it recursively crawls.\n\nDoes it handle circular references?\n\nYes.\n\nI noticed it doesn't handle Node.js buffers? Would you able to add that?\n\nNode buffers fall under the typed array support.\n\nWould you be willing to ensure lodash.isEqual handles these issues:\nhttps://github.com/sotojuan/not-so-shallow/issues ?\n\nI believe it already handles those issues.\n\nMy goal is not having to do that. Would be better if we all agreed what deep equality entails.\n\nYep, I agree. It's something for Lodash v5 for sure. In the meantime this method allows customization for the things you'd like to change and handles the rest.\nIt's a pretty good mix.\n. > I would be explicit about that in the docs. While Node.js buffers inherit from TypedArray, they're not entirely the same thing and not everyone knows buffers inherit from TypedArray.\nC\ud83d\udd76L \nExplicit support is probably better anyways.\nI'll patch it up https://github.com/lodash/lodash/commit/4495e74f7772c4fbe570cceb983a2ddefcca9bca.\n. I haven't cut a release yet.. ~~Ah yep, will do. Look for it some time on the 9th.~~\nUpdated lodash.isequal to v4.5.0.. The relevant bit in Mocha's implementation is here. . @novemberborn \n\nYou should be able to use node -r \"@std/esm\" ./node_modules/.bin/ava\n\nAVA seems to be using its own loader require-precompiled\n```\nexport var value = reset()\n^^^^^^\nSyntaxError: Unexpected token export\n    at createScript (vm.js:74:10)\n    at Object.runInThisContext (vm.js:116:10)\n    at Module._compile (module.js:537:28)\n    at Module._extensions..js (module.js:584:10)\n    at extensions.(anonymous function) (/projects/esm/node_modules/require-precompiled/index.js:16:3)\n    at Object.require.extensions.(anonymous function) [as .js] (/projects/esm/node_modules/ava/lib/process-adapter.js:100:4)\n```\n@mAAdhaTTah\njs\n// Nope\n// Nah\n// Still no\nThat's not the right approach. If the esm loader can be the entry point for all tests then you can do:\njs\nrequire(\"@std/esm\")(module)(\"./testForAll.js\")\nbut there is no way to enable global require overwriting with its programmatic API (that's a feature).\n@novemberborn \nIt looks like AVA sends commands as JSON string arguments to its forked processes. I should be able to sniff them. Is this a standard thing or something AVA cooked up?. It looks like patching @std/esm to sniff stringified params will enable @std/esm to hook in with:\n\"ava\": {\n  \"require\": \"@std/esm\"\n}\n but then it's running into AVA's built-in Babel use (AVA is transpiling the import/export). I tried to disable it with some Babel configs but not having any luck. Any guidance @novemberborn?. @mAAdhaTTah I'm saying once @std/esm is patched to hook in with \"require\": \"@std/esm\" it still won't work because the test files are being handled by Babel via AVA. If there was a way to disable the \"module\":false transform for AVA then it might work. Which is why I'm asking @novemberborn if it's possible.. The problem is that when AVA transpiles the test file\nimport envDelta from '../envDelta';\nwill transpile it to:\nvar _envDelta = require('../envDelta');\nvar _envDelta2 = _interopRequireDefault(_envDelta);\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\nSo because brookjs-cli/src/deltas/envDelta.js is handled by @std/esm its exported ias a regular cjs file without a __esModule property so Babel will treat it as { default: obj } which breaks the tests since envDelta is supposed to be a function not an object as { default: func }.\nUpdate:\nI made the cjs option of @std/esm add the _esModule property to its export and that fixed it with Babel unit tests.. > It is possible to disable this, yes. You'd lose all other transforms though (currently not a problem with Node.js 8, but it'll hurt the compatibility of your tests with earlier versions). \nCan you go into detail. I know some Babel presets like babel-env allow you to disable module transpilation with \"module\": false option. How does that map to AVA?. @novemberborn \n\nBesides this detail, is there anything that prevents @std/esm from working with AVA?\n\nI patched @std/esm so it works with Babel transforms using the \"cjs\":true option and the AVA require option. All good!\n\nAnd has there been consideration for enabling the processing of all files via an environment variable?\n\nThat hasn't been brought up. I'd be concerned that deps of deps of deps using @std/esm would also trigger since they'd notice the environment variable too.. @dacz head over to the @std/esm repo and file the issue there with a simple repro repo and I'll investigate.. It looks like there was complications with ava support for test files.\nSee https://github.com/standard-things/esm/issues/197#issuecomment-353494337.\nPackages using @std/esm still work with ava.\nIt's just that ava handles ESM tightly on test files at the moment. Related to #709.. Let me know if a specific usage pops out for AVA.\nI wouldn't mind making @std/esm detect AVA and applying AVA specific defaults or something.. I'm fine not dwelling on mjs. The recommended config is \"@std/esm\":\"cjs\", which enables cjs compat mode. . The general advice is still solid. Maybe bump the version documented and run with it.. The \"@std/esm\": \"cjs\" could technically be \"@std/esm\": \"js\" since with #1618 it would no longer need the cjs babel interop to be enabled.. Looks great! Let's roll with that configuration and loosen it if needed later.. >  I just want to express appreciation for everyone working on this. Always awesome to see OSS projects come together to support interoperability for users. Thanks y'all\nI was pretty floored by everyone jumping in to help out. It's such a pleasant surprise to have to rein in adjustments. Double thanks to y'all \u2764\ufe0f\n  . Okay, @std/esm v0.19.0 is released \ud83c\udf89  (which supports that shorthand mentioned above)!\n  . Related to https://github.com/standard-things/esm/issues/277.. Related to https://github.com/standard-things/esm/issues/277.. @novemberborn Oh neat!\nUpdate:\nCan this be added to the @std/esm cookbook example. I can see folks reaching for something like this again.. @novemberborn Oh neat!\nUpdate:\nCan this be added to the @std/esm cookbook example. I can see folks reaching for something like this again.. See https://github.com/avajs/ava/blob/master/docs/recipes/es-modules.md.. Maybe the change should be on the disable babel bit. Your way of doing it looks simpler than what is currently documented. Though maybe theirs is more correct in some way?. Ok updated \ud83d\ude0b . Anything else I need to do?. Anything else I need to do?. No problem. Let me know if you ever need anything else.. No problem. Let me know if you ever need anything else.. esm defaults to cjs compat enabled (so should require no configuration). Your current master branch should continue to work if you dropped the cjs:true from the config. I even dropped the await config option since you didn't appear to be using it in the tests.\n\n\n. @revelt \nYou're testing with ava latest which doesn't contain esm support.\nYou'll want to move to the ava beta releases.. Yep! \n\nIt's a basic scenario test (all my AVA scenarios regardless of test will fail with beta5 though).\nFor example this one.. Thanks, the CI=1 workaround does work around it :). Thank you @novemberborn!. Hi @OmgImAlexis!\nThe esm loader disables its cache when run with nyc so we should be okay (unless for some reason our detection missed). As for the Illegal invocation, that looks like an issue with named export projection. Though we have a guard for that. If you could provide a small repro repo it would help me troubleshoot the issue. \n. \ud83d\udc46 I don't believe all those options are needed. Can you try snipping it down to something like:\n\"@std/esm\": \"cjs\". Sweet \ud83c\udf6c !. that enable -> that enables. \ud83d\udc46 So rad!. \ud83d\udc46 is the ... in the json common in recipe docs? It makes a weird error highlight when using ``json. Latest is0.17.0now. Is there a standard way to express a dependency without using a version for the recipe so folks don't lazy copy pasta an out of date version?. Since@std/esmis meant to be a production dependency, as well as a dev dependency, we can't just always overload the globalrequire.extensions*(the Node folks would flip-out, esp. since Lodash v5 will be using@std/esm`)*.\nIf @std/esm is invoked through a CLI then it will hook into require.extensions, otherwise it produces its own loader instance for folks to use as you've done \ud83d\udc46. Whenever AVA requires @std/esm through the --require option the @std/esm loader is in CLI mode so will hook require.extensions. In this case, the thing preventing its use is the precompile check which dead-ends the load chain and prevents it from getting to @std/esm.. \ud83d\udc46 You can shorthand to just \"cjs\" in the .esmrc file. In the next release you can use { \"esm\":\"cjs\" } as a shorthand for the above too.. Ya, kinda gross but it works. You may want to wait for the other shorthand form.. > precompile would only be short circuiting imports for tests, but those will have already used Babel to convert the import statements to require statements.\nThat's the only issue affecting @std/esm (the test files).\n\nI poked around in the @std/esm codebase, and didn't see where you were actually hooking require.extensions (even in \"CLI mode\").\n\nI think @std/esm might be OK with just having ababel: false or precompile: false option. That way it avoids any package specific sniffs and is something that can be applied for other uses/reasons.. > OK, so this is actually only a problem when Babel is configured not to transpile ESM modules.\nCorrect. The user wanted to just use @std/esm which is where I pointed to #709.\n\nwe still apply a few transforms to the tests for power-assert support\n\npower-assert ships with a UMD build (no Babel required). The babel-plugin-throws-helper could always be optional or could be written for acorn+magic-string or other lighter solutions.\n. @std/esm uses acorn+magic-string :yum:. I had one slip into this PR (which is why I rebased and added the .gitignore entry). It's a meta data file on MacOS that's generated automatically when you view a directory. I was surprised AVA didn't have it ignored already.. Ah, yes I can make that more detectable!\nUpdate:\nWe have a symbol namespace we use currently (though not exposed on the exports).\nIt goes something like \"esm\" + ZWJ + \":thing\" so for the exports I could expose a symbol named for \"esm\" + ZWJ, so without the :thing bit. I'll noodle around on it and report back.. Coo cool, no prob. I'll remove it.. Ah gross. I didn't intend to make more work for y'all. Okay. I'll mod it to check both, then y'all can drop the legacy one when you all do a major bump.. Updated \u2728 . ",
    "mmkal": "I could - but that still feels like an unpleasant workaround, and it's probably even more obscure than the one I suggested: it's not immediately obvious (in JavaScript) where that value actually comes from. Is there any reason the options object shouldn't be exposed?\n. @sindresorhus - great, using process.cwd() would be ideal. I'll close this one since #32 is covering my use case. \n. @despairblue @novemberborn Is anyone working on this? I've been looking for something similar and have something working locally.\nI could submit a PR, but before I do the work to get it ready for review, I thought I should check nobody else is working on it.. @despairblue @novemberborn thoughts on the first-cut version? https://github.com/avajs/ava/issues/1298. @novemberborn re whether we could do:\nTypeScript\nfunction contextualize<T>(getContext: () => T): ava.ITest<T> { ... }\nMaybe what I could do is export two types - Test<T> (renamed from ITest) and GenericContextualTest<T> which could be defined by\nTypeScript\nexport type GenericContextualTest<T> = Test<{ context: T }>;\nThe name GenericContextualTest isn't great, but I'll see what I can do about avoiding a clash with ContextualTest when implementing @ivogabe's suggestion.\nAnyway, this way people wouldn't have to constantly repeat the context: ... part. I'll add something along those lines soon and update.. @novemberborn @ivogabe updated with suggested changes, and improved a couple of other things:\nI renamed ITest as suggested, but I am now using it as the interface for all test-like functions - and they are all now optionally generic. Plus, types/generated.d.ts is now about 25% smaller because each test-like function now implements this single interface rather than having its declaration spread over four lines.\nUsage example:\n```TypeScript\nimport * as ava from 'ava'\nfunction contextualize(getContext: () => T): ava.ContextualTestFunction {\n    ava.test.beforeEach(t => {\n        Object.assign(t.context, getContext());\n    });\nreturn ava.test;\n\n}\nconst test = contextualize(() => ({ foo: 'bar' }));\ntest.beforeEach(t => {\n    t.context.foo = 123; // error:  Type '123' is not assignable to type 'string'\n});\ntest.after.always.failing.cb.serial('very long chains are properly typed', t => {\n    t.context.fooo = 'a value'; // error: Property 'fooo' does not exist on type '{ foo: string }'\n});\ntest('an actual test', t => {\n    t.deepEqual(t.context.foo.map(c => c), ['b', 'a', 'r']); // error: Property 'map' does not exist on type 'string'\n});\n```\nIn the above example, contextualize is a generic method, and infers the type of the object passed to it to be { foo: string }. By using the test method returned instead of the vanilla ava one, we can take advantage of type annotations. \nThis would allow people to extend ava with things like their favourite assertion/mocking libraries, and have strongly typed, convenient test setups:\n```TypeScript\nimport MyClass from '../MyClass';\nimport calc = require('npm-calculator');\nimport chai = require('chai');\nimport sinon = require('sinon');\nimport contextualize from './ava-contextualize'; // same contextualize method as above as a module\nconst test = contextualize(() => {\n    const myClass = new MyClass();\n    const sandbox = sinon.sandbox.create();\n    const expect = chai.expect;\n    return { myClass, sandbox, expect };\n});\ntest.afterEach(t => t.sandbox.restore());\ntest('times uses npm-calculator multiply', t => {\n    t.context.sandbox.stub(calc, 'multiply').returns(42);\n    const sixTimesNine = t.context.myClass.times(6, 9);\n    t.context.expect(calc.multiply).to.be.calledOnce;\n    t.context.expect(sixTimesNine).to.equal(42);\n});\n``. Here's a [gist of the generated types file](https://gist.github.com/mmkal/6d99027728d8088174fde481e0b84a36), since it can't be seen on the PR.\n@novemberborn @despairblue @ivogabe I'd love to get this merged as I'm planning to use it in my current project. Sorry about so many small commits - I'd be happy for it to go in as it is now, if it looks OK to you.. Here's a [gist of the generated types file](https://gist.github.com/mmkal/6d99027728d8088174fde481e0b84a36), since it can't be seen on the PR.\n@novemberborn @despairblue @ivogabe I'd love to get this merged as I'm planning to use it in my current project. Sorry about so many small commits - I'd be happy for it to go in as it is now, if it looks OK to you.. @ivogabe updated with theexports and renames you suggested. [Gist](https://gist.github.com/mmkal/6d99027728d8088174fde481e0b84a36) also updated.. @ivogabe updated with theexports and renames you suggested. [Gist](https://gist.github.com/mmkal/6d99027728d8088174fde481e0b84a36) also updated.. @sindresorhus added the above usage example. . @sindresorhus added the above usage example. . Thanks @sindresorhus @novemberborn @ivogabe ! Does it get auto-published to NPM? Or is it done ad-hoc?. Can do - I think my C# is showing \ud83d\ude33. Sure. I didn't want to do that initially because I have other repos that I want.vscodefiles checked in for, but maybe I can use.git/info/excludeinstead.. If I do,base.d.tswill no longer compile becauseContextualTestFunctionandTestFunction(orRegisterandRegisterContextual` once they're renamed) don't exist in base.d.ts. \nOr do you think that doesn't matter so much because base.d.ts isn't used for anything except seeding generated.d.ts?. ",
    "vinz243": "Wow thanks a lot. This solved my issue would have never foind out otherwise :)\n. ",
    "zaaack": "@vinz243 It tooks me a lot of time too because there were not much information in google, you are welcome :-)\n. @novemberborn I think I find out why, it's caused by an undefined property changed to not undefined, or the opposite.\nHere is a minimal reproduce project: https://github.com/zaaack/ava-issue-1433/blob/master/src/Feedback.js#L38\n. ",
    "andrewjrhill": "Is test set as a default export of ava? If not you may need to use \nimport { test } from 'ava';\nI could be completely wrong though.\n. ",
    "carpasse": "@jfmengels thanks for the clarification after thinking it through we will stay with babel-preset-es2015\n. ",
    "Dakuan": "I'm also having this issue on CircleCI...\nmy node_modules folder is 331mb, so you wouldn't need very many of them to trip up the server!. ",
    "darul75": "I confim having ava running tests with Travis in serial mode can fix that kind of issue\nbash\nexited due to SIGKILL\nResolved with\nbash\nava --serial. ",
    "julien-f": "Indeed, it seems that pnpm is doing that for all exported binaries.\n. See rstacruz/pnpm#244\n//cc @zkochan\n. @zkochan I'm afraid that requiring users to configure pnpm for their usage will really hurt pnpm as it will no longer be a drop-in replacement for npm i :unamused: \n. @zkochan I only want what's best for pnpm because it's much nicer to use than npm :)\nAdding a property to ava's package.json looks like the best approach but it's not up to me (@sindresorhus, what's your opinion on this?).\n. ",
    "zkochan": "Using this flag allowed us to install packages to a shared folder. It is unbelievable what amount of disk-space it saves!\nPersonally I did not experience any problems while using the --preserve-symlink flag. If there is a particular reason why it can't be used with ava, maybe you could add a preserveSymlinks: false to your package.json and pnpm will not force it to preserve-symlinks. Probably ava will work anyway from a shared storage. Preserve symlinks is important mainly for packages with peer dependencies.\ncc @pnpm, @iamstarkov, @rstacruz, @andreypopp\n. Well the same we could say about ava, right? Other testing frameworks seem to work with pnpm and the --preserve-symlink flag.\nWhat I suggest is to add a new property to ava's package.json. Users of ava/pnpm won't have to do anything. (I think it would work, I can check later today)\nOr we could add a config to pnpm to disable --preserve-symlinks. But that would disallow the usage of a shared store. Would this work for you @julien-f?\n. Well for pnpm the best would be if ava would just work with --preserve-symlinks :smile: \nIs it somewhere documented why ava cannot do that?\n. I don't know whether this was fixed on ava's end but the latest version of pnpm does not require --preserve-symlinks. It is used by default for now, but you can set the preserve-symlinks config to false by running npm config set preserve-symlinks false and ava should work fine.\nIn case of questions feel free to open an issue on pnpm, or write me on gitter/twitter. ",
    "alathon": "If it were possible to run each test in its own process, instead of each file, then this would be possible as each process would have access to its own copy of sinon. Incidentally not a bad idea, as an option - but I'm not sure its possible without additional syntactic sugar for that specific case, which may be unwanted. @sindresorhus @novemberborn any interest in such a feature, if it should come up as a PR? (and is even feasible, haven't dug too deep into the worker/process stuff)\n. @sindresorhus I've updated the readme to reflect the change, or rather how I'd expect it to work. However, now I'm slightly in doubt as to whether resolveTestsFrom should be implemented in cli.js as it is.\nI'm confused by why api.js:140 (https://github.com/alathon/ava/blob/use-resolveTestsFrom-in-fork/cli.js#L140) is implemented like that. Can someone elaborate on why resolveTestsFrom gets set to process.cwd() if there is non-flag input, and why resolveTestsFrom wasn't made a CLI flag to begin with? Its a bit hard for me to reason about since there are zero tests of resolveTestsFrom.\n. @novemberborn mentions that resolveTestsFrom is used to resolve command-line paths to files. That means I'm misusing it here, and ironically you'd end up with the same problematic behavior, if you ran ava from a sub-folder.\nI've gone ahead and added a package option called pkgDir instead, which does the same thing, and changed the tests accordingly.\n. @novemberborn I've reverted the PR to the previous version where pkgDir is specified in cli.js, and fixed the tests.\n. The Travis CI failures seem to be related to upgrading xo, am I right in this @sindresorhus ?\n. Branch updated with changes from upstream master now (to fix xo errors and make it easier to squash/rebase).\n. I use superagent for testing with AVA, works fine for me. Note that superagent supports generators/promises natively, there is no need for superagent-as-promised.\nF.ex:\n``` javascript\nimport request from 'superagent'\nimport app from 'someKoaApp'\ntest.beforeEach(async(t) => {\n  t.context.agent = request.agent(app.listen())\n})\ntest.serial('client signup: without companyId', async t => {\n  const data = t.context.user\n  let res\nres = await t.context.agent\n    .post('/api/v2/auth/signup/client')\n    .send(data)\n  t.falsy(res.error)\n  t.is(res.status, 200, 'Signup ok')\nres = await t.context.agent\n    .post('/api/v2/auth/login')\n    .send({email: data.email, password: data.password})\n  t.truthy(res.error)\n  t.is(res.status, 400)\n  t.is(res.error.text, 'user not activated')\n})\n```\nWhere request is just an import supertest and app is a koa v2 application.\n. @SuThaw Can you please provide a small example repository that shows this? i.e. a package.json file with the required dependencies, and a single test file to run. \n. I think a very strong point of AVA (and tape) is precisely that there are few assertions by default - this reduces the amount of 'magic' that occurs, and reduces bugs in testing code that occurs because you aren't 100% sure how a particular assertion works. Given that bugs occur almost as frequently in test code as they do in the code they test, I think it would be a mistake to change this.\n. @novemberborn But then you don't get the values for actual and expected, just a true or false value. That makes it (potentially) harder to debug. That depends on what you're testing, though, I suppose -- are you testing an almostEquals function, or that actual and expected are close enough; if its the function, then yes that'd be the way to go.\nI don't see what's wrong with t.is( Math.round(actual_duration), Math.round(expected_duration) ) though, if thats what you're using (Math.round). That seems clear and declarative to me. Anyone looking at it will know exactly what's going on - as opposed to t.almostEquals, which beckons the questions of: Almost equals according to which precision/function? Can it be used on objects/strings? etc... Ah, my bad -- was thinking of tape when it comes to that.. Good point. Removed that part again. Is it relevant to discuss my question above about resolveTestsFrom in a separate issue, or is Gitter more appropriate for that?\n. Ahh yeah, I see what you're saying. The fact that its even possible to create an Api() leading to an undefined pkgDir being given to fork is probably not a good idea. What about doing the following:\n1. Set pkgDir in the same way we do now, but in api.js instead, around https://github.com/alathon/ava/blob/2a65c1295588ebdf36dcc833484c34f3ed956add/api.js#L55\n2. Remove the option from cli.js (making it a flag you can set through the CLI is a decent idea, but probably a separate PR)\nThis would mean the test would work as expected, and test what it says - that if you just create an Api(), then it'll run from the package.json folder by default.\n. Thats a lot more concise, nice :)\n. I changed things to reflect what I suggest here. The commit history is a bit of a mess now though, due to the rebase from master happening before a pull/push. My bad ^^\n. Eh, my editor seems to have done something odd with the tabs here.. Will fix\n. Yes, there is a diff. But it lines up correctly in the terminal, when I execute it now. I'm not sure how to un-diff just that particular part of the file, although I know its possible through git. \n. ",
    "jacobkahn": "Re-added that arg into API - thanks for catching that. Right now the deprecation message mentions the docs, then exits. Do we think this behavior is alright?\n. ",
    "ntwcklng": "Hi, would you mind if i tackle this issue? However, when i run this test i get the following outout:\n``` bash\n\u276f ./cli.js ava.test.js          \n1 failed\nreturns true when happy\n  Error: unicorn\n    Test.t [as fn] (ava.test.js:4:11)\n```\nMy outputTest.t [as fn] (ava.test.js:4:11) vs. sindresorhus output: Test.fn (test.js:5:8)\nI'm using\n\u276f node -v\nv7.1.0\nMaybe the output from the errorstack has changed in node v7?\nResponsible for the output:\n- https://github.com/avajs/ava/blob/master/lib/reporters/verbose.js#L112\n- https://github.com/avajs/ava/blob/master/lib/reporters/mini.js#L184\n. Okay thanks, i will have a look.\n. ",
    "Snugug": "w/r/t #1049, I've just installed this branch and ran against an external macro, and it did not appear to resolve the issue that powerAssert is not applied to those tests. ",
    "tonyeung": "@sindresorhus  right, I followed the recipe but it had different settings then I wanted. I'm just listing what I had to do in order to get it working for me. The impression I got from the recipe was that you had to have certain things in order for everything to work, like having the commonjs module format.\n. @sindresorhus just to be clear, i'm not saying the documentation is wrong. I'm just providing what I needed to do, in order to make it work for me.\n. opened an issue here first https://github.com/avajs/find-cache-dir/issues/4. not sure why this was happening but it works fine without sudo now. okay, so i screwed up the test file name so ava coudn't find anything to test. After fixing the issue, the same errors are happening again. . @ORESoftware  thanks for the advice. I'm going to close these issues since I don't intend on using ava anymore. . ",
    "tugend": "I get the following behavior in typescript. I'd say this is clearly an error.\ntest.todo('my todo'); // compile error\nArgument of type  'my todo' is not assignable to parameter of type 'Macros<ContextualTestContext>'.\ntest.todo('my todo', () => {}); // runtime error\n\u200b\u200bTypeError: `todo` tests are not allowed to have an implementation. Use `test.skip()` for tests with an implementation.\u200b\u200b. happy face, ^_^. ",
    "SuThaw": "@alathon when i change the code as the following. It works. It doesn't work, when i directly use res.status  in the test. I don't know why.\n``` javascript\ntest.only('no email - should respond with errors', async t => {\n    t.plan(1);  \n    const res = await request(app)\n        .post('/users')\n        .send({email: 'ava@rocks.com', password: '123123'});\nconst status = res.status;\n    t.is(status,200);\n});\n```\n. @alathon  I finally know the source of error.  It is because of tap-reporter.\njavascript\n{\n  \"name\": \"ava_tst\",\n  \"version\": \"1.0.0\",\n  \"description\": \"sample test with ava\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"test\": \"node_modules/.bin/ava test/*.js  --tap | tap-spec\"\n  },\n  \"author\": \"SuThaw\",\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"express\": \"^4.14.0\"\n  },\n  \"devDependencies\": {\n    \"ava\": \"^0.16.0\",\n    \"tap-diff\": \"^0.1.1\",\n    \"tap-nyan\": \"0.0.2\",\n    \"tap-spec\": \"^4.1.1\",\n    \"supertest\": \"^2.0.1\"\n  }\n}\nI cannot use the tap-spec as report. If I use tap-nyan or other, it is ok. But when i use tap-spec, i got error.   If you want to test it, here is the repo .Thanks for your time.\n. ",
    "guillaumevincent": "@novemberborn sure, thanks\n. @F1LT3R is right\nava 'src/**/*.test.js' or \"ava \\\"src/**/*.test.js\\\"\" works\nIt's a shell extension transforming the command\ntest.js\n```\n!/usr/bin/env node\nvar args = process.argv\nconsole.log(args)\n```\n```\n$ node ./test.js src/*/.test.js\n[ '/usr/bin/node',\n  '.../test.js',\n  'src/1/1.test.js' ]\n```\nnot an ava issue, closing\nThanks @F1LT3R and @novemberborn. ",
    "dwieeb": "Yes, that works. My tests are working as expected now. \n. @lholznagel I did this, but I think const { Reflect } = global should work, too(?):\ntypescript\nconst g = <any>global;\nconst Reflect = g.Reflect;. ",
    "lholznagel": "@dwieeb hey, just came across the same issue. What did you do for it to work? Adding const { Reflect } = global seems not to work.\nThanks in advance :). Ah thanks!\nWith my code I get this error:\n'Global' has no property 'Reflect' and no string index signature\nReally didn\u00b4t think about adding <any>. When adding <any> it works :+1:\nEnd result looks like this: const { Reflect } = <any> global;\nMy test now works, thanks for your help :+1: . ",
    "jyboudreau": "The documentation of Ava definitively alludes that the output of assertions will be like what the verbose renderer outputs.\nI would vote for switching the renderer to the verbose one.\n. ",
    "domenic": "\nIt would also require that the module is first built for the browser so it can be injected in via a script tag.\n\nYes. That is exactly the point! You need to test the same scenario as you will be deploying to your users.\n. I understand that some people want to abuse jsdom in ways it's not meant to be used and that are not supported. I especially understand if the author of a module geared around doing so wants to do that. But I don't think a widely-used test-runner such as ava should be recommending such practices.\n. ",
    "neverfox": "Do you have an ava config in your package.json? For example,\n\"ava\": {\n    \"files\": [\n      \"test/*.spec.js\"\n    ],\n    \"tap\": true,\n    \"babel\": \"inherit\",\n    \"require\": [\n      \"babel-register\"\n    ]\n  }\n. ",
    "dlebedynskyi": "Have same exact issue  if babel is not specified as inherit - ava fails to compile  . @novemberborn will try to do soon. Thanks for tracking this. @novemberborn \nI tried to override .babelrc with package.json/ava/ babel section. And this is a part that did not work. \nProblem for me was that I expected that override  with no modules to be used. It was not. \nFor now I have a workaround that I do not like - \nhaving .client.babelrc with \"modules: false\" for webpack 2 client build \nand .babelrc without - for ava. \n@mandricore  he is talking about modules false in babelrc file es2015 section. \n. @novemberborn \nstill getting same error \njson\n//package.json \n\"ava\": {\n    \"files\": [\n      \"test/**/*.spec.js\"\n    ],\n    \"source\": [\n      \"src/**/*.{js,jsx}\"\n    ],\n    \"concurrency\": 5,\n    \"require\": [\n      \"babel-core/register\",\n      \"babel-polyfill\",\n      \"ignore-styles\",\n      \"./test/setup.js\"\n    ],\n    \"babel\": {\n      \"babelrc\": true,\n      \"plugins\": [\"transform-es2015-modules-commonjs\"]\n    }\n  },\njson\n//.babelrc is client specific. expected to be merged with ava/babel for ava run\n{\n  \"sourceMaps\": true,\n  \"presets\": [\n    \"react\",\n    \"es2017\",\n    \"es2016\",\n    [\"es2015\", { \"modules\": false }]\n    ],\n  \"plugins\": [\n    \"transform-class-properties\",\n    [\"transform-object-rest-spread\", {\n      \"useBuiltIns\": true\n      }],\n    [\"transform-react-jsx\", {\n      \"useBuiltIns\": true\n      }],\n    [\"transform-regenerator\", {\n      \"async\": false\n      }],\n    [\"babel-plugin-transform-builtin-extend\", {\n        \"globals\": [\"Error\"]\n    }],\n    [\"babel-plugin-transform-runtime\", {\n      \"helpers\": false,\n      \"polyfill\": false,\n      \"regenerator\": true\n      }],\n    \"transform-export-extensions\",\n    \"react-hot-loader/babel\"\n  ]\n}\nalso tried to set \n\"ava\": {\n/// \n\"babel\": {\n      \"babelrc\": true,\n      \"presets\": [\n        \"react\",\n        \"es2017\",\n        \"es2016\",\n        \"es2015\"\n        ]\n      }\n    }, \nonly way i had it working is to have .babelrc targeting ava/nodejs and building client with other config. it looks like babel section in ava is ignored. . @novemberborn working on it. . bug repo https://github.com/dlebedynskyi/ava-issue-1093. @novemberborn Thanks for tracking this. I expected something like this. \nI think I've tried \"transform-es2015-modules-commonjs\" before with same result.\nChecked again - even removing \"babel-core/register\", and adding transform-es2015-modules-commonjs - same error in source files. test files run fine. \n. @ahumphreys87 this is interesting solution. going to try and see how it will work on out setup. @stevenmathews we ended up still having client.babelrc and server.babelrc. Both are using babel-preset-env now. Server one is having test section for Ava like @ahumphreys87 suggested. We also saw problem with babel-register and \"import\" statements in node_modules - like lodash-es. \nAnyway thanks to @ahumphreys87 for idea of solution. Run into same issue. this is blocking (not critical) us from updating to babel.49. Would appreciate the fix \ud83d\ude4f . ",
    "odigity": "I'm in the same boat.  I would like to write some custom assertions for convenience and add them to t in my project.\nThe docs say \"you can use custom assertions\", but provide no information on how.. So... what's the easiest way to extend t with custom assertions?  :). So how do you recommend approaching this particular use case?\nI need to compare two floats with some margin of error.  Because there's no support built-in, and no documented way to add custom assertions, I'm currently doing this:\nt.is( Math.round(actual_duration), Math.round(expected_duration) )\n\n(It's a video processing engine.)\nIt works, but it's ugly compared to, say...\nt.almostEquals( actual_duration, expected_duration )\n\nPerhaps with an options arg to control the error margin.\nalmostEquals is a common term used when dealing with floats in JavaScript.\n\nExhibit A (Yes, I'm thinking about pulling this in to my project.)\n\nNote: I'm not asking you to implement this, only if there exists a sane way by which one such as myself could do so.. Either that, or package it on NPM.  Too busy to factor it out right now, but it's on my todo list.  That would at least provide a central location for discussion on unit-testing with Ava + Knex.. ",
    "epeterson320": "+1 to being able to extend t since we users can get the features we want and the core API can remain small. Jest has a nice API to extend their matchers. Perhaps a similar method would work in AVA.\nFor what it's worth, though, I'd like to see an almostEquals, closeTo, or inDelta matcher in core. Missing instanceOf is easy enough to work around, but comparing floating point numbers with a certain precision gets either verbose or hacky when testing compute-focused code.. ",
    "lewisdiamond": "It's really worth making it easy to provide an implementation of custom tests. Right now using t.true(_.isMatch(... ...)) outputs almost pure gibberish. It should only print the difference so it's really easy to spot.. ",
    "yomed": "@novemberborn Thanks for the suggestions. Splitting the tests up into multiple files seem a bit too hacky/overkill for this, but I can see that manually throttling promises has some... promise. Although even with that, I think I'd have to weigh the extra cost in complexity, and perhaps ava isn't currently well-suited to this type of scenario.\n. ",
    "Kikobeats": "@novemberborn can you do an example using throat for that?. doesn't work because ava tests are triggered at test declaration.\nI wrote something like:\n``js\nconst createCase = targetUrl => {\n    test(targetUrl, t =>\n      limit(async () => {\n        const url =${uri}?url=${targetUrl}`\n        const { body, statusCode } = await fetch(url, { json: true })\n        const {data} = body\n        t.snapshot(omit(data, ['date']))\n        t.is(statusCode, HTTPStatus.OK)\n      })\n    )\n  }\nconst tests = URLS.map(createCase)\n  await Promise.all(tests)\n```\nand works, yes, but CPU 100% is meh. ops it's true \ud83d\ude42. ",
    "Bareus": "@novemberborn I tried out your code in my tests and noticed some problems:\n\nUsing throat with macro's doesn't seems to work when title is programmatically created.\n```\nasync function macro = (t, input, expected) => {/do async test/}\nmacro.title = (providedTitle, input, expected) = { /make test title/ };\n\n// this throws TypeError: Tests must have a title\ntest(limited(async (t, input, expected) => { await macro(t, input, expected)}), input, expected);\n```\nI can re-write above code to make it work like this:\n```\nasync function macro = (t, input, expected) => {\n await limited( async() => { /do async test/ };\n}\nmacro.title = (providedTitle, input, expected) = { /make test title/ };\n// this does not throw TypeError\ntest(macro, input, expected);\n```\nThis code has the problem that I could potentially run into the global timeout even though the actual test didn't start yet. The other problem, which is more of a preference from me, is that I have non-related code in my test cases which I don't really like. \n\nUsing throat limits only the number of concurrent tests itself, but not their related beforeEach/afterEach hooks. As a workaround you could add throat to those hooks too, but IMO this is error prone since you could forget to add throat to a hook. Also I'm not sure how throat and ava handles concurrency internally so maybe a case could happen where all beforeEach hooks are processed first and then the tests. This could lead to a timeout since tests are inactive during the time all those beforeEach hooks are processed. \n\nDo you have any ideas how I can solve this problem? (Btw. I'm using ava 1.0.0-beta3 and node v8.10.0). ",
    "snypelife": "Killing those forEach's in the reporters seems to work nicely, as well as clean things up a bit. But I  noticed that env vars like CI or TEAMCITY_VERSION aren't getting picked up from https://github.com/chalk/supports-color/blob/master/index.js. \n. @novemberborn from my perspective of that conversation in the issue, it looks like both a fix in the supports-color module as well as ava were sought after (ava being the flag needs to be passed down and supports-color needing teamcity <9 support).\n. ",
    "pocesar": "nevermind, you're right. the 't.plan' is making it fail without the rejection, since it's getting caught and swalled in the first catch. so what would be the best way to catch unwanted early rejections? I tried using t.fail.bind(t) in the first then but it doesn't work (fails with the same [object Object])\n. ok, I changed the helper code to:\nes6\nconst valid = (obj, t, shouldfail) => {\n  return validation.validate(obj).then(() => {\n    t.pass()\n  }, (e) => {\n    if (shouldfail) {\n      t.fail(e)\n    } else {\n      throw e\n    }\n  })\n}\n``` es6\ntest('uuid', (t) => {\n  t.plan(3)\nreturn valid({\n    uuid: 'f47ac10b-58cc-x4372-a567-0e02b2c3d479'\n  }, t, true).then(() => valid({\n    uuid: '123e4567-e89b-12d3-a456-426655440000'\n  }, t)).catch(ValidationError, (c) => {\n    t.pass()\n  }).then(() => valid({\n    uuid: 'a'\n  }, t)).catch(ValidationError, (c) => {\n    t.pass()\n  })\n})\n```\nbut now it fails with [object Object] in the list, but on the AssertionError, it shows the ValidationError:\n```\n  \u00d7 uuid [object Object]\n1 test failed [22:49:29]\n\nuuid\n  AssertionError: ValidationError: child \"uuid\" fails because [\"uuid\" with value \"f47ac10b-58cc-x4372-a567-0e02b2c3d479\" fails to match the required pattern: /[0-9A-F]{8}-[0-9A-F]{4}-4[0-9A-F]{3}-[89AB][0-9A-F]{3}-[0-9A-F]{12}/i]\n    index.test.js:9:9\n```\n\nkinda defeats returning a promise from the test\n. thanks @novemberborn that was really enlightning, I'm trying to get my head off the mocha mindset :)\n. I'm using this right now:\n``` es6\ntest('uuid', async (t) => {\n  t.plan(1)\nt.notThrows(valid({ uuid: 'f47ac10b-58cc-x4372-a567-0e02b2c3d479' })) // it's wrong on purpose, on a passing test it would have the f47ac10b-58cc-4372-a567-0e02b2c3d479\n})\n```\nbut got:\n```\n  \u00d7 uuid Got unwanted exception..\n\nuuid\n  AssertionError: Got unwanted exception..\n    From previous event:\n```\n\nshouldn't it be showing what the exception were?\n. oh I assumed it was already implemented, most colorized-by-default libraries comes with --no-color\nalthough each tick of the spinning pipe creates a new line on the stdout, that produces the same (new) line for each step of the progress\nEDIT: using the --tap option makes it all at once, disables progress, and make the output plain and completely readable! I guess I'll just be using it. ",
    "villelahdenvuo": "@novemberborn Do you have a public road map somewhere? Would you be interested in a PR if we can work out the integration?\n. ",
    "SergioMorchon": "@sindresorhus I don't think so.\nOne thing is that in runtime production code you don't anything operated with NaN to result other than a NaN value, too. Even NaN with itself, because it represent an indetermination. And by definition, you can not assure that one undefined value is the same undefined value.\nBut in a test, where you are not operating with values but just comparing them, and most importantly inside a deepEqual, it is really interesting get this handled.\nIn a test I just want to assert that some NaN value is effectively NaN.\nSame happens with the Date object constructed with a NaN.\nAnd because of this, here I have two options:\n\n\nChange my production code to avoid NaN, something I really don't want to.\n\n\nDon't use deepEqual, is etc. and handle the special case manually. Something unacceptable due to the complexity on the nested objects to test.\n\n\nI would be pleased if Ava could handles this.. ",
    "ento": "Correct (I think) link to workaround: https://github.com/avajs/ava/issues/1089#issuecomment-255591680. ",
    "jsatk": "@sindresorhus I'd love the ability to use the node debugger. Mocha has this feature if you run mocha debug. I rely heavily on setting debugger statements in my code and test suite to, well, debug. \nI'd love to contribute to AVA and add this feature in. Is anyone currently working on this? Please let me know if the AVA crew is open to someone adding the ability to use the native node debugger. \nThanks. . @sindresorhus Thank you. Will take a peek and do it if I can. :) . ",
    "jedmao": "@zixia this is awesome work and I'm excited to use it, but can you tell me if it works fine with the --watch flag too? Or is there more work to do for that?. Yep, I've been waiting for the same feature.. Sometimes I add or change a test and it gets appended to the end of the snapshot Markdown file. If I hit u then it refreshes the whole Markdown file and the snapshot that was previously at the end of the file is now somewhere in the middle. This isn't obvious, because there's no messaging that tells me my snapshots need to be updated. Technically, the snapshot itself perhaps doesn't need to be updated, but the Markdown file does (at least). I need some kind of indicator to tell me it's time to hit u so I can ensure my Markdown file won't change with a subsequent test run. In other words, the watch command shouldn't be yielding a different Markdown file than if I were to hit u or do a normal, non-watch test run. Does that make sense?\n. Yeah I realize the Markdown files are not used by AVA, but AVA does produce two different results for the Markdown file, depending on watch vs. normal run, as you mentioned, for performance reasons. That's fine. I just wish there were some notification that it needs a hard re-write (an update) in the watch scenario. You explicitly update snapshots before committing and so do I for this very reason, but if it were more clear, we wouldn't have to be so trigger happy w/ the updates.. Agreed. That would be misleading. Perhaps a message like:\n\nAppended 2 snapshots to Markdown files. Press u to update.\n\nI'm not sure how to be any more concise. The more verbose version would say \"...to update/sync with snapshot order.\". Couldn't you just do this?\njs\ncli.flags.extension = arrify(cli.flags.extension || 'js');. What about .tsx for TypeScript/React?. Could we do the following to cover both .ts and .tsx files?\njs\nif (/^tsx?$/.test(fileExt)). With default settings \u2013 no. But with the --jsx preserve or --jsx react compiler options \u2013 yes. And even test files commonly use the .tsx extension to mock components.. ",
    "patrick91": "Is there any update on this? I'm starting a new TS project and I really would like to use ava without compiling TS scripts first!\n@zixia amazing work so far!. babel has recently merged a PR that allows to parse typescript, so maybe soon enough we will be able to use ava for typescript via babel :)\nhttps://github.com/babel/babylon/pull/523#event-1142533589. ",
    "lukescott": "@novemberborn I know I can do tsc && ava, but that won't work with --watch, will it? Is there a solution to make it work with watch?. Ah, got it. So I need need to run them separately. Tsc will watch and save files, which Ava will pick those up. It's too bad there isn't a way to do a single watcher and have it all happen in memory.. With TypeScript support in babel, wouldn't babel-register (or what ever Ava uses) work? I would imagine babel adding the .ts extension w/ native support. . Babel 7 has support for TypeScript, so all you have to do is use the typescript preset. I've found that enums and namespaces are not supported though. Everything else seems to work great.\nIn the latest betas they've changed from babel-core to @babel/core, and all the packages are now namespaced under @babel. All the plugins with the word \"transform\" have been changed to \"proposal\" and all the config options need to start with @babel/, such as \"@babel/env\" instead of just \"env\".. It just removes the TypeScript specific syntax, similar to what it has already done with Flow. So far it has greatly simplified my build process as I don\u2019t have to pre-process with tsc. I just run TypeScript in Atom for all the type checking. \nI would imagine that the typescript preset would at least allow you to run Ava with TypeScript code. You can use tsc to validate the types are correct. . > Snapshots directory name must be based on the parent directories. If tests use snapshots, else snapshots\nJust a matter of preference - I colocate my tests, so if I have implementation.js, I have a implementation.test.js in the same directory. Instead of putting the snapshots into another directory, I'd prefer to have it also colocated, so, for example, implementation.test.js.snap instead of snapshots/implementation.test.js.snap.. I think an option to exclude specific file(s) from the cache would be a good start. That's what I originally asked for from Jest, but they declined. That's why I came up with the babel-jest wrapper, which ended up working better (it only does the transform if a file is added/removed).\nSo in AVA you could have something like --cache-exclude=path/to/wild.js,..., which would have a similar option to add in a config file / package.json.\nI do encourage custom transforms though. I am currently using TypeScript at work, and it would be awesome to use that for the transformer. Or both. I know there is a workaround that involves tsc && ava, but that involves writing extra files to the filesystem. The beauty behind the custom transform is you can do it all in memory in a single pipeline.\nI would take a different approach from Jest though. If I want to combine Babel and TypeScript, for example, I have to do a third custom transform. I would do something more like webpack's loaders. In fact it would be amazing if AVA was compatible with webpack's existing loaders (not sure if that's technically possible).. If there is another solution, other than doing --no-cache, I'm all for it. My concern with --no-cache is it excludes every file, which for wildcard glob imports is really only needed for one or two files in the project.. I believe it only updates snapshots of tests that were just ran. . ",
    "tomdavidson": "@lukescott you can add   \"compileOnSave\": true to your tsconfig.json if you use an editor that recognizes it. You can also change your watch/dev run script to be something like tsc -w & ava --watch so tsc will watch in the background while you run ava watch.\nI think ava is super interesting but I have not replaced mocha over the typescript issue so the suggestion is not in use for me.. ",
    "sebinsua": "\nThe plan is here, but there hasn't been progress on that lately: https://github.com/avajs/ava/blob/master/docs/specs/001%20-%20Improving%20language%20support.md\n\nHas there been any work on this since then?. @jonahbron As ava uses Babel 7 it should support TypeScript if you use @babel/plugin-transform-typescript. There are a few caveats though so it depends on whether you are using any of the unsupported features.. ",
    "adieuadieu": "I came to this thread/issue while trying to use Ava's --watch feature with TypeScript. @tomdavidson's suggestion of tsc --watch & ava --watch works well enough as an intermediate solution for anyone who wants to watch their TypeScript code for changes and rerun those tests.. ",
    "aendrew": "How difficult would be it be to just add the \"extension\" whitelist option? It seems like that plus ts-node/register in the require config stanza would be sufficient to make this possible.. Re: @lukescott's comment,  according to the babel-plugin-transform-typescript docs, import = and export = aren't supported either. This, combined with lack of enums and namespaces, means I'd really be loathe to use it in place of something like ts-node. \nFWIW, a number of test tools use ts-node already (NYC, Mocha), so it's a pretty well-established workflow when testing using TS.. ",
    "andywer": "I know this is an old issue, but I needed the same thing, since a prior build step can be quite ugly.\nSo I quickly created ava-ts. Give it a try and leave feedback if you want :). @GantMan Thx. Pity, though! Would you mind mentioning ava-ts in your blog post? :)\nBtw, thanks for all the great responses. Feel free to spread the word or \ud83c\udf1f the repo, it feels a bit disregarded \ud83d\ude09. @novemberborn Would you consider linking to ava-ts in the TypeScript recipe for the time being? I think that is where most folks will look for help and it makes using AVA with TS noticeably easier.. ",
    "GantMan": "@andywer - super cool!  That's epic.\nI've already switched though:  https://shift.infinite.red/switching-from-ava-to-jest-for-typescript-a6dac7d1712f\nSorry :(. Added!!!  And call-to action to star the repo added as well.  Great work.  Might see you cool kids again later.  It seems the curse of JavaScript is that I have to switch libs as soon as I feel comfortable with them.   \ud83d\ude06 . ",
    "impaler": "ts-node support sounds like a convenient approach, there are probably other language-node libs that may work the same.\nFor registering ts-node itself, would you use \"require\" in the package.json#ava?\nSince --require was depreciated from the cli options, is it a limiation now that you can't seem scope what to require for what?\nFor example say I didn't want to use ts-node or some global browser env for every test I want to use in ava:\n\"scripts\": {\n  \"test:ts-node\": \"ava --require helpers/ts-node.js src/**/**.ava.ts\",\n  \"test:ts-browser\": \"ava --require helpers/ts-browser-env.js src/**/**.ava-browser.tsx\"\n}. @novemberborn yes and this feature is the only issue I have when using Ava and Typescript. Ideally using sourcemaps to have the snapshot fixtures next to the source *.ts files would be the best solution, developer experience wise.\nI can't speak to the reliability issue with source maps you mention. Currently I am using a basic tsc command to compile all *.ts files in my project along with the ava tests in ts, to one build folder. I can then the node debugger in Intellij Idea to configured to the ava \"profile.js\" to successfully use breakpoints etc on the original typescript source without issue, its fantastic. Its working with the sourcemaps generated from the typescript compiler just fine.\nOn the custom location, I agree one snapshot base directory would make sense for keeping things manageable. Regarding name collisions, maybe a subdirectory of this base path could be derived from the transpiled javascript test __filepath itself?. Ok thanks @novemberborn I'll have a try at implementing this.. @novemberborn in the commit above it made sense to me to add another cli option --snapshot-location think its worth it?\nNo tests yet, looks like I can add another test block to ava/test/assert.js and verify a few of the contexts and add a test sourcemap as you suggest. So far its working with a small test project :)\nI saw the ava has some interesting typescript support upcoming, I guess that this feature is worthwhile not just for typescript though. Any \"compile to js\" language would need this.. @novemberborn no problem having no --snapshot-location cli arg, it was just useful while testing, I doubt there is a more general use case.\nDo you think we should add a package.json#ava config to enable sourcemap detection? This feature will enforce a fs.readFileSync on every test where before there was none. Maybe this is not worth worrying about performance?\nThis is nearly there, when I get some free time next I'll wrap this up in a pull request. Happy if anyone can help test, there maybe some hidden contexts.\n. @mariusGundersen yes in the scenario for testing javascript that is not transpiled with ava. Ava is then running against a transpiled build folder that includes the sourcemaps. Eg I added this as a test fixture https://github.com/impaler/ava/tree/snapshot-location/test/fixture/snapshots/test-sourcemaps.\nSo afaik right now ava has no reason to check and store any existing sourcemaps. In this branch it just manually fs checks if a *.map to know where to put the snapshot. Maybe this would impact thousands of tests? I was thinking it would be fair to instead enable this feature through some config flag in package.json#ava#sourcemaps or similar.. @novemberborn sounds good, its up I linked to an example project I was using to test eveything. I haven't looked into what you mean by load() specifically yet. As it is now I think its meeting all the feature criteria without a sourcemap library. Lets continue in the pull request.. Thanks for the great review @novemberborn :)\nI won't claim that we have complete integration test coverage on this snapshot feature, but it has the main aspects. Otherwise as I said in a comment I think we would need some more utilities to duplicate project folders and run all scenarios like I am testing in https://github.com/impaler/ava-snapshot-location.\nBased on what I can see on appveyor the ci issues seem unrelated.. The windoze build still seems intermittent and unrelated, since the same error is happening in other branches it seems out of scope.\nLet me know if the github squash commits isn't enough, Cheers :)\n. Thanks @novemberborn, yes everything is working as I expected with your changes :)\nAnd looks like appveyor is now working, know if it was it related to changes here?. It seems like a reasonable requirement to prevent false positives. I think an error message for this would need to describe the reasoning and fix, something along the lines of:\n\n\"Snapshot {x} is missing a fixture file. To prevent false posititives, a verified fixture file is required when testing in the CI environment.\". Is that just for constency? in this case everything under ...build/* is directly from the typescript compiler, I usually hesitate to edit compiled files for style only. . Yes this is wrong, I have updated this be a little more dry https://github.com/avajs/ava/pull/1489/commits/12f1ff8d73e0ae47c109eed9737470c9d421fa8e#diff-583d616f4b01bf1fa153e71c0cef7de7R704. If there are more integration tests on projects like this I can think of some better ways to do it, like maybe the project should be first duplicated to a /tmp location and so on. . Yes it does, I don't think there will be any need to run build here for the tests anyway. So I fixed the indentation and removed \"scripts\". . Sure no preference here from me projectDir, I think I saw Dir used somewhere else :). Yes I like it.. Good point, I guess the comment highlighted some of the ambiguitiy, please checkout the latest revision.. Yes, please checkout the latest revision.. Indeed thanks a bunch I am new to that library very cool. Seems to work well with the inline version in my testing https://github.com/impaler/ava-snapshot-location/tree/master/inline-sourcemaps. Thanks I learned a few things here. I guess we wont be able to answer how much more to guard without trying more than just typescript. With the way things are layed out it shouldn't be too hard to expand on this.. :+1: . :+1: . :+1: . :thumbsup:. Sure that will also throw any fs issues, I updated the pull request :). \n",
    "btkostner": "I'm not sure how babel has implimented typescript support, but if it's just removing the syntax annotations then some people will run into issues with type metadata. This could potentionally mess with IoC implementations.\nI would strongly favor using a language-node package to transpile tests.. It's worth mentioning that AVA 1.0 is now out :confetti_ball:. You can use typescript with this recipe. I've been using it in production for a while now, and it works great. This issue can be closed.. The TSError you are getting is probably due to type checking failing. We should make note in the docs that you can require ts-node/register/transpile-only instead, which will pass even if type checking is not perfect.\nRelevent parts of my package.json:\njson\n  \"ava\": {\n    \"babel\": false,\n    \"compileEnhancements\": false,\n    \"extensions\": [\n      \"ts\"\n    ],\n    \"require\": [\n      \"./test/bootstrap.js\",\n      \"ts-node/register/transpile-only\"\n    ]\n  }\nWorks with ava 1.0.0-beta.5.1. My bad. A lot of those edits where me being OCD. I'll revert those tonight when I get a chance.. Alright. It looks good to me. One thing to note, I was unable to get nyc code coverage to work correctly with this setup, but that's not a show stopper for this PR.. Never mind on my last message. Apparently it's working on travis, so something locally is funky with my setup.. I'll take a look at that tonight and see what I can get done!. I added tests for the reporters. They include stack traces because of the errors, but I'm not sure the best way to clear those. Assuming that ts-node doesn't change we should be good though.. While not exactly the same, I noticed in the 1.0 release docs that you can now do helper setup functions. https://github.com/avajs/ava/blob/master/docs/recipes/puppeteer.md. It does, but I felt that import { test } from 'ava'; was more consistent considering that's how it's used later in the docs. Also, a lot of the typescript community (opinionated and no numbers to back this up) recommend against using export default.. These changes were made by the linter. ",
    "jonahbron": "Any recent activity on this?  I'm choosing a test runner for a new application, and good TypeScript support is an important part of that decision.. ",
    "shirtleton": "Any updates on Typescript support?. ",
    "FrancescoBorzi": "Any news about this?. @novemberborn any plan to support *.ts files out of the box?. ",
    "vladimiry": "@ShinDarth it's working with ts-node module installed and config shown here https://github.com/avajs/ava/blob/master/docs/recipes/typescript.md (first TS recipe) . I had to add \"babel\": false, option to the above config, otherwise it gives the following error:\n\u2716 Internal error\n  Error: Cannot apply enhancement-only precompilation, possible bad usage\n\"ava\": \"1.0.0-beta.5.1\",. @novemberborn here it's https://github.com/vladimiry/protonmail-desktop-app/tree/06ac40ee0c537e536b9fa82623e7a8864c98fdef, tests can be executed running  yarn && yarn test:electron:main. I don't use babel for this project, but it comes with ava, would be great ava uses it as an optional dependency if that's possible.\nBesides what I noticed is that output is not informative enough. For example I explicitly cause a TS error, like using nonexistent / not imported type (TS2304):\n\nand I then get the output that doesn't show me what error and on which line is happened, but just the non-zero exit code: 1 thing:\n```\n  TSError: \u2a2f Unable to compile TypeScript:\ncreateTSError (node_modules/ts-node/src/index.ts:250:12)\n  getOutput (node_modules/ts-node/src/index.ts:358:40)\n  Object.compile (node_modules/ts-node/src/index.ts:545:11)\n  Module.m._compile (node_modules/ts-node/src/index.ts:430:43)\n  Object.require.extensions.(anonymous function) [as .ts] (node_modules/ts-node/src/index.ts:433:12)\n\u2716 src/electron/main/ipc-main-api.spec.ts exited with a non-zero exit code: 1\n``. @novemberborn Above mentioned ignoring oferror.message` printing is happening here https://github.com/avajs/ava/blob/1e0e8e84dbc8ed70286d01dd9c8d290f30533aad/lib/reporters/verbose.js#L231\n\n. > We should make note in the docs that you can require ts-node/register/transpile-only instead, which will pass even if type checking is not perfect.\nGetting TSError is totally fine, it's an expected behaviour and I want it to be like that, the issue is that error.message is currently not being displayed by ava, only the error.summary is.. @novemberborn Just tried version from the fixes branch, works well for me withoutbabel: false` option set, thanks.. ",
    "SleeplessByte": "@btkostner ts-node is not a drop-in replacement for tsc, even though many people think so. Running ts-node/register will give most people a similar experience, but each time there is a typescript release, ts-node potentially has compile errors again.\nIMO ava should be able to run this itself.. ",
    "screendriver": "The latest Jest 24 release supports TypeScript via @babel/preset-typescript. Ok. May I ask why? The original indentation was tabs and not spaces. Is it preferred to use tabs?. By the way: fixed.. Oki doki. I fixed that \u263a\ufe0f . ",
    "lithin": "@sindresorhus thanks for such quick feedback!\nFilepath & directory\nI totally agree that we should look at how jest handles this sort of stuff. Is there any way in Ava to see what filename/path the current test is in? \nReporting\nAs for reporting, Jest snapshots generate a message for you:\n\nI'm not sure how easy it would be to change this?\nSnapshots directory\nAt the moment, Jest saves snapshots into the same folder as the current test is in. We were wondering with my colleagues if it wouldn't be better to create a __snapshots__ folder in the root folder. What do you think?\n. Here are a few updates on my progress:\n- We decided to follow jest and have __snapshots__ directory next to the test files\n- Checked code coverage with nyc works as expected\n- Reporting has been slightly improved - please check the screenshot\n\n. @sindresorhus I think I've finished - could you please review?. Builds are failing because jest-snapshot is using const in build files which isn't compatible with node 0.12. Is there any chance of upgrading node on travis?. @sindresorhus Does that mean we need to wait for the upgrade to be made?\nWe just caught one bug so I did a quick fix :) It's again ready for review.. Thanks @vdemedes and @sindresorhus for the feedback - hopefully I didn't miss anything :). @sindresorhus it seems one of my comments got lost in all the changes - how do you propose to use the x.snapshot vs x._snapshot? I wasn't quite sure how to implement that.. @sindresorhus All fixed :)\nAs for the report output, I'm not sure what happened there :x I found out that no matter how I use chalk from within jest or ava, it wouldn't colour anything. I thought it might be cause by the reporter but now I realised it used to work... Very confusing! Any ideas what this might be caused by?\nI haven't seen anywhere that the test would appear twice after updating the snapshot. It could happen if you've changed the name of the test though?. > The error is colored red by our reporter. I'm more confused by how you actually got a diff color there in the first place.\nI'm really confused about that as well. The colouring coming from jest diff used to work fine. Now, even if I remove the test(... message ...) in the snapshot assertion and just do console.log(message), the message isn't coloured.\nI agree it shouldn't be in scope of this pull request though.\n\nShouldn't that remove old tests?\n\nJest identifies tests by their name - if you rename a test, it's as if you were running a new one. It has no way of telling if a test of another name should be identified with the renamed one. In other words, I don't think it can remove old tests from snapshots - it has to be done manually.. @sindresorhus I think you're absolutely right :) Might be a good suggestion to the jest repo but I'm afraid I can't change that through my implementation of it.. Tests passed! :). Hahaha thank you @vadimdemedes ! It was a pleasure to contribute to a project like Ava :)\n@sindresorhus there are definitely things to improve so I might make a few more pull requests if I get around working on them ;). I'm just keeping it in the same style as the rest of the modules - I'd personally throw babel at it and write ES6 :D. thanks will fix!. ...and here too :D. I'm sorry I'm not sure how the internal base method would work?\nHappy to add message :). \ud83d\udc4d . This one isn't js actually - it's just a snap file. \ud83d\udc4d . Had to return this back to make it work with .state. This has been fixed. ",
    "lpil": "Great work, thanks @lithin & @sindresorhus . This file has mixed tabs and spaces.. Why the var x?. Mixed tabs and spaces in this file.. I'm +1 to the ES5, just confused as to why we save module.exports to a var called x. I think it's supposed to be tabs rather than spaces.. ",
    "mymattcarroll": "Another FYI: Having the same issue with flow-bin@0.37.0. ",
    "btipling": "Is this released yet? I still get this problem.. ",
    "yuriy-yarosh": "Nah, once will not work properly, ignore: false is not a good choice because transpiling any node_modules non ES6 stuff will cause an error.\nI'm initializing my babel-register like so nowadays.\n```\nconst packageJson = require('../package.json');\nrequire('babel-register')({\n  // eslint-disable-next-line\n  ignore: /node_modules/!(${Object.keys(packageJson.dependencies).join('|')})/**/*\n});\n```\nPlease keep in mind that babel-register will go for .babelrc config, or embedded package.json babel config, but will completely ignore ava's babel config. So, ava: { \"babel\": \"inherit\" } is a way to go 99% of the time.. Well the trick is that babel-register will not grab a babel config from ava's package.json config. It will go for .babelrc or package.json embedded babel config instead, or it will use defaults and ignore ava's babel config in package.json completely.\nAnd it's VERY frustrating because people are expecting completely the opposite behaviour.. ",
    "dchowitz": "Ups, sorry. Stupid me ;-)\n. Understood. Thanks for the quick reply and clarification!\n. ",
    "asvetliakov": "Any news about PR ? I'd like to adopt ava in my project, but separate TS precompilation (and as i understand, the ava now just doesn't work with file extensions rather than .js) is huge stopper for me. . ",
    "snigdhaAgarwal": "I looked into it, and installed ava using yarn. Can I take up this issue?. ",
    "anoff": "appveyor build seemed to have run into a timeout during npm installation. Not sure what I did to break that?. lol you actually sold me on the idea that this is an annoying task to track. But I'll try to keep an eye out for that easy PR then \ud83d\ude39 . Thanks for the feedback. Updated the readme according to your findings.. ",
    "igl": "Removing flexibility to a static \"one config per package.json\" seems pointless to me. You could still have both and the \"static (pkg) > overwrites (cli)\" model seems more reasonable to me.. 2 package.json files as far as i understood.. ",
    "werme": "Is there any way to have different requirements for different entry points within the same project with the current configuration model?. ",
    "mandricore": "I was  having the same \"problem\" \n.babelrc\n{\n  \"presets\": [\"es2017\"]\n}\npackage.json\njson\n  \"ava\": {\n    \"files\": [\n      \"test/**/*.test.js\"\n    ],\n    \"source\": [\n      \"src/**/*.{js,jsx}\",\n      \"!dist/**/*\"\n    ],\n    \"modules\": false,\n    \"concurrency\": 5,\n    \"failFast\": true,\n    \"tap\": true,\n    \"powerAssert\": true,\n    \"require\": [\n      \"babel-register\"\n    ],\n    \"babel\": \"inherit\"\n  },\n```\n\nava\n\nTAP version 13\n/Users/kristianmandrup/repos/node-libs/rx-aster/test/aster/aster.test.js:1\n(function (exports, require, module, __filename, __dirname) { import _asyncToGenerator from '/Users/kristianmandrup/repos/node-libs/rx-aster/node_modules/babel-runtime/helpers/asyncToGenerator.js';\n                                                              ^^^^^^\nSyntaxError: Unexpected token import\n```\nWorks with another .babelrc setup in another project of mine, I think the transform-runtime plugin is key. \n{\n  \"presets\": [\"es2015\", \"stage-2\"],\n  \"plugins\": [\"transform-runtime\"],\n  \"comments\": false\n}\nHowever adding \"plugins\": [\"transform-runtime\"], to my es2017 preset setup didn't work.\nI guess I will have to go back to es2015!?\nPlease update your docs and recipes for various babel preset setups. Cheers!. Where do I set modules: false !?. This is what I had working:\n\"ava\": \"^0.17.0\",\n    \"babel-core\": \"^6.0.0\",\n    \"babel-eslint\": \"^7.0.0\",\n    \"babel-loader\": \"^6.0.0\",\n    \"babel-plugin-transform-runtime\": \"^6.0.0\",\n    \"babel-preset-es2015\": \"^6.0.0\",\n    \"babel-preset-stage-2\": \"^6.0.0\",\n    \"babel-register\": \"^6.18.0\",\n    \"tap-nyan\": \"^1.1.0\". ",
    "ahumphreys87": "I think the best way to solve this would be the use the env property in the .babelrc and set this when ever running the tests. Working on it now and will post the results here when Im done.. Yeah so this is working for me:\n{\n  \"env\": {\n    \"test\": {\n      \"presets\": [\"es2015-node4\"]\n    }\n  },\n  \"presets\": [\n    [\n      \"env\",\n      {\n        \"targets\": {\n          \"browsers\": [\"> 10%\"]\n        },\n        \"modules\": false\n      }\n    ]\n  ]\n}\nI then run my tests like this: BABEL_ENV=test gulp ava\nIt means I can keep module transpilation disabled for the rollup build and can transpile them when running ava. ",
    "stevenmathews": "I have also had this issue and was solved by following @ahumphreys87 solution - cheers\n@novemberborn mentioned on gitter \"We're working on making it easier to extend a babel config while specializing it for tests etc\". ",
    "MartinMuzatko": "Same here. I have no .babelrc set up (don't need to, I'm using webpack), and want to use ava with es6 modules.\nAfter trying all the above configuration setups, I still get the same error\nUsing a .babelrc, it works. I copied https://github.com/jamestalmage/__ava-0.8-with-babel-6. ",
    "jameskandau": "Make sure your modules are inside the src folder. ",
    "iSanchezDev": ".babelrc\nnpm install babel-preset-stage-0 --save\nAdd to your configuration present list -[..., \"stage-0\"]\nIt worked for me!. ",
    "MartinKristof": "You need add preset \"es2015\" and maybe \"transform-class-properties\" into .babelrc.\n. ",
    "meherett": "reinstall node_modules\n. ",
    "rnkdev": "Hi guys, this is my very first time I actually did something to an open source program. \nSo I followed the hint in the issue tracker and modified api.js so that the hash now uses resolved path as the key. (I hope this is as expected and I'm just not breaking everything) \nI ran npm test before committing and it passed all of the tests. \nI don't want to create pull request because I'm not sure if this is the right way to do it or not. \nhttps://github.com/avajs/ava/commit/7a1a0cdebbe870650ff02956a41a78ea2cc41a34. Hmm it seems that it fails the test for Windows. I'm not sure how to handle the symlink in it. \nEdit: it passes all of the test in Windows with the exception of my new test to check whether the symlink is working or not. . After some research, I think what I could do is I can add check to the test not to be run (or to be automatically passed) if the environment is detected to be Windows. But I'm not sure if it's the right approach considering Windows has no symlink support. . Thanks for adding that support for Windows!\nOn the other hand, there is a new problem, when I added new direct symlink to a test file in the test, it fails to recognise that file exist. \nIt throws AvaError: Couldn't file any files to teststrangely. Normally you would think if it can resolve a symlink to a directory, it should resolve symlink to a file as well. \nThe following commit shows the problem. It stops before even calling precompilation and Api._runFile. So I copied the same tests to the ava master branch. Surprisingly it failed all of the tests including the one that points to directory. \nIn a sense this fix fixes problem #1143 #1137 as the user is trying to do a symlink to a directory containing test files. \nBut then it also uncovers that direct symlink is somehow can't be resolved. (Maybe it has something to do with how files are searched in ava-files) \n. Hi I just added the last commit with the requested change. (My bad for not researching that .js part) Is there anything else I should do to get the PR accepted? Thanks. . ",
    "ccorcos": "Only certain parts need to be run serially, but a lot of things can run in parallel. We get some control over that with before and after. It just seems like with the recursive nature of the way Ava works, this would be a cheap win to get a very flexible way of controlling the parallelism of your tests.\nConsider something like this where we have fine-grained control over what's in parallel and whats serial.\n```js\ntest.test('doug-app init', (t) => {\n  // initialize doug-app\n  shell.cd('~')\n  shell.exec('doug-app init doug-app-test')\n  t.truthy(exists('doug-app-test'))\nt.before('link doug-app', (t) => {\n    // link doug-app\n    shell.cd('cd ~/doug-app-test')\n    shell.exec('npm link doug-app')\n  })\nt.test('doug-app test', (t) => {\n    shell.cd('cd ~/doug-app-test')\n    shell.exec('doug-app test')\n  })\nt.test('doug-app build', (t) => {\nshell.cd('cd ~/doug-app-test')\nshell.exec('doug-app build')\nt.truthy(exists('dist'))\n\nt.before('setup git origin' (t) => {\n  // setup local git origin\n  shell.mkdir('doug-app-origin')\n  shell.cd('doug-app-origin')\n  shell.exec('git init --bare')\n\n  // push initial commit\n  shell.cd('~/doug-app-test')\n  shell.exec([\n    'git init',\n    'git add .',\n    'git config --global user.email \"test@test.com\"',\n    'git config --global user.name \"Doug Test\"',\n    'git commit -m \"doug-app-test\"',\n    'git remote add origin ~/doug-app-origin',\n    'git push origin master',\n  ].join('; '))\n})\n\nt.test('doug-app deploy', (t) => {\n  shell.cd('cd ~/doug-app-test')\n  shell.exec('doug-app deploy')\n  shell.cd('~/doug-app-origin')\n  t.truthy(shell.exec('git branch | grep gh_pages').stdout.trim())\n})\n\n})\n})\n```. > That said, AVA is pretty good with promises, and since by default all tests are started at the same time you could build a library on top of AVA which achieves the sequencing you're looking for.\nI can, and I've made it work by basically just having one giant test. But ideally, I could specify checkpoints in there so AVA can report 5 tests pass rather than just 1 and if there's a test failure, report exactly which test failed...\n. I see. So something like this?\n```js\nconst checkpoint = () => {\n  let resolve = undefined\n  const promise = new Promise((r) => resolve = r)\n  promise.resolve = resolve\n  return promise\n}\nconst build = checkpoint()\ntest('build the project', () => {\n  return buildProject().then(build.resolve)\n})\ntest('deploy the project', () => {\n  return build.then(deployProject)\n})\n```. I like that a lot actually. Thanks @novemberborn!. I like that a lot actually. Thanks @novemberborn!. I'm interested to see where this goes. I'm looking at the following error message right now thinking how it could be better.\nt.deepEqual(result.value, output, result.fail)\n                     |      |\n                     |      [Object{type:\"italic\",children:[#Object#]}]\n                     [Object{type:\"italic\",children:[#Object#]}]\nSo amen to magic assert!\n. Using your current WIP branch:\n```\n Difference:\nArray [\n  Object {\n    children: Array [\n      Object {\n\n\ntype: \"text\",\nvalue: \"hello\",\nchildren: Array [\nObject {\ntype: \"text\",\nvalue: \"hello\",\n},\n],\ntype: \"link\",\nurl: \"world\",\n          },\n        ],\n        type: \"italic\",\n      },\n```\n\nAwww yeah! Thank you so much @vadimdemedes :). One thing to consider is the directionality of the + vs -.\nt.deepEqual(something, value) implies to me that if something has a value that value does not, then that should be a +, not a -.... haha i like all of this. hmm I just realized I was pull the magic assert branch locally so maybe that was being use rather than the global ava. things seem to work now.... ",
    "leebyron": "Taking a look - I'm not 100% sure what's wrong, but the js.flow file seems like it might be double-defining the throws type.. A simpler repro case is simply:\nt.throws(Promise.reject(new Error()))\nWhich yields:\nindex.js.flow:61\n 61:    throws(value: () => void, error?: ErrorValidator, message?: string): mixed;\n                      ^^^^^^^^^^ function type. Callable signature not found in\n 13:   t.throws(Promise.reject(new Error()))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^ Promise. See: test.js.flow:13. Great suggestion - I cleaned up the names of the types.\nAnyone who would like to test this out locally, here's how I'm doing it:\n\nClone AVA with this PR applied (https://github.com/leebyron/ava/tree/fix-flow-types) locally.\nFrom the directory you cloned to, run npm link to have npm use that as the source for ava.\nFrom your project that suffers from #1114, run npm link ava, then run flow check - you should no longer see flow complaining about AVA internals, and only see errors about your own code.\n(npm unlink)\n. Great suggestion - I cleaned up the names of the types.\n\nAnyone who would like to test this out locally, here's how I'm doing it:\n\nClone AVA with this PR applied (https://github.com/leebyron/ava/tree/fix-flow-types) locally.\nFrom the directory you cloned to, run npm link to have npm use that as the source for ava.\nFrom your project that suffers from #1114, run npm link ava, then run flow check - you should no longer see flow complaining about AVA internals, and only see errors about your own code.\n(npm unlink)\n. Thanks for the merge. Feel free to shout my way if you ever need help with anything related to type definitions.. This makes sense to me. The strong type check assumes I'm testing values of type boolean, but that's not a safe assumption. What the type signature should assume is that t.true() will be provided either \"true\" or \"anything that isn't true\", that's not the same as false. The combination of the two values then isn't \"boolean\" but instead \"mixed\" (the supertype of all types). \n\nI don't think it's AVA's place to make judgements on the APIs its testing in this way. Maybe this example is odd, but it exists and would like to be tested. A more common example I see a lot is nullable-boolean where a value is typically true | false but also possibly null | undefined. That would fail the type check as well but seems like a reasonable thing to test.. Updated to have .throws() return Error or Promise. @gajus - that's a good sign, it means the original flow error you encountered was solved so now flow is finding other unrelated issues.\nThis particular ambiguity error you're encountering I sent a PR for a while ago: #1164 - since it was merged it should be in the latest ava release.. I'm not sure that this is a breaking change - you should only be seeing other issues if they were originally masked by this issue - that is this change can bring you from being broken in two ways to being broken in one way, but not from working to being broken.\nSince the annotation errors you're seeing are highlighting lines in the flow files from 0.17.0, could you please make sure that you have 0.18.0 installed? Flow support was broken in 0.17.0. I agree that a patch update where flow goes from passing to failing for otherwise reasonable code would be suboptimal. I don't think this change would have such an effect.. cc @bookercodes @sindresorhus . ",
    "justin-calleja": "Ok thanks @sindresorhus . ",
    "variousauthors": "The link in the above response is broken.. ",
    "eric7578": "Thanks for the reply! But, with Node 7, I can run async/await without any babel plygin. I do run the code with Node(without plugin), and it works. The problem is that I can run with the same code with Node 7, but I cannot run it with avajs.. ",
    "EduardoRFS": "v8 5.4 have async functions when use --harmony-async-await. Ava shebang dont pass flags to node, you need to run using a command node --harmony-async-await ./node_modules/.bin/ava. @novemberborn yep. ",
    "cdyfng": "check this link :\nhttps://github.com/trufflesuite/truffle/pull/146\n. ",
    "xiaomaisu": "I'm sorry that I have another expanded question,I  tried to compare the spent time on mocha and ava, but I can't find a live example. So I  checked the history version of postcss,however,it always made a mistake. Can official support channels provide a live example?. ",
    "sebald": "\nI can't say I understand the reasoning for this. t.true and t.false are assertions for what is essentially a type check.\n\nMaybe the given example is a little bit too much contrived, but I am not testing the type. Rather I am testing if the functions returns the correct outcome. I could also use t.is(isNumber(6), true) instead, but using t.true is easier to read and shorter to write.\nBasically, the current type definitions limit the way you can test, if the thing under tests has (or returns) a union type. The suggested PR gives the power back to the user \ud83d\ude04 \nFurthermore: Writing t.true(<boolean>isNumber(6)); is more error-prone, because some people might think that the returned value is cast to a boolean (which is actually not happening on runtime, the cast is only there to satisfy TypeScript).. ",
    "jarlehansen": "@vadimdemedes all good suggestions for improvements, I have updated the pull request.\nThanks!. Thanks @ThomasBem, this improves on what I created in my pull request. When this is merged I will update #1179 . Thanks for the comments, I will wait on #1177 to be merged and then update this PR.. I have updated this PR with the changes done by @ThomasBem . I have included checking runStatus.failCount, also added/updated the tests.. ",
    "oskarmurand": "In 0.15.2 it works if you cd into the module that contains the tests and then do ../.bin/ava with all of the flas and path that you need.. ",
    "RafaelKr": "@novemberborn Okay, you're right, that would be confusing. I didn't see it like that yet.\nIt's not really a problem for me, because the afterEach doesn't take long. But it could be a problem, if the afterEach would take 5 seconds or more.\nMaybe another solution would be to attach the metadata of the associated test to the t-variable. Then you could check there which type of test was/will be (before/after) called.. ",
    "cncolder": "I'm not sure I fully understand the code logic of lib/cli.js\nOnly change this line? https://github.com/avajs/ava/blob/master/lib/cli.js#L148\njs\n    if (cli.flags.watch && !isCi) {\nThis line https://github.com/avajs/ava/blob/master/lib/cli.js#L97 set conf.watch and conf.tag. Then where it going?\n. @mightyiam I understand what you say. But why not make ava even better?\n\ud83d\ude80 Futuristic JavaScript test runner. ava disable tap reporter in watch mode. And switch to verbose reporter in CI. Why?\nBecause ava is smart enough to correct that.\nUse tap reporter in watch mode is unsuitable or maybe \"wrongly\".\nWe're not encouraging the incorrect use of tap also. So we adjust it.. ",
    "OmgImAlexis": "@sindresorhus this example still doesn't work as it throws an error about importing.\n```\n\u279c  SickRage git:(develop) \u2717 ava test\n/Users/xo/code/SickRage/test/helpers/setup-browser-env.js:1\n(function (exports, require, module, __filename, __dirname) { import browserEnv from 'browser-env';\n                                                              ^^^^^^\nSyntaxError: Unexpected token import\n    at createScript (vm.js:74:10)\n    at Object.runInThisContext (vm.js:116:10)\n    at Module._compile (module.js:533:28)\n    at Object.Module._extensions..js (module.js:580:10)\n    at Module.load (module.js:503:32)\n    at tryModuleLoad (module.js:466:12)\n    at Function.Module._load (module.js:458:3)\n    at Module.require (module.js:513:17)\n    at require (internal/module.js:11:18)\n    at Array.forEach (native)\n    at Object. (/Users/xo/code/SickRage/node_modules/ava/lib/test-worker.js:35:22)\n    at Module._compile (module.js:569:30)\n    at Object.Module._extensions..js (module.js:580:10)\n    at Module.load (module.js:503:32)\n    at tryModuleLoad (module.js:466:12)\n    at Function.Module._load (module.js:458:3)\n1 exception\n\u2716 test/index.js exited with a non-zero exit code: 1\n```\ntest/helpers/setup-browser-env.js\n```\nimport browserEnv from 'browser-env';\nbrowserEnv();\nimport test from 'ava';\n```\ntest/index.js\n```\ntest('Insert to DOM', t => {\n    const div = document.createElement('div');\n    document.body.appendChild(div);\nt.is(document.querySelector('div'), div);\n\n});\n```. @blake-newman using the PR you opened as a base I tried adding this to my current vue app and I'm running into an issue no matter how I have it setup.\n\nError logs\n\n```sh\n\u279c  wvvw.me git:(master) \u2717 yarn ava test/*.spec.js\nyarn ava v0.23.3\n$ \"/Users/xo/code/wvvw.me/node_modules/.bin/ava\" test/post.spec.js\nYou are running Vue in development mode.\nMake sure to turn on production mode when deploying for production.\nSee more tips at https://vuejs.org/guide/deployment.html\n[Vue warn]: Vue is a constructor and should be called with the `new` keyword \n/Users/xo/code/wvvw.me/node_modules/vue/dist/vue.runtime.common.js:3414\n  this._init(options);\n       ^\n\nTypeError: this._init is not a function\n    at Array.Vue$2 (/Users/xo/code/wvvw.me/node_modules/vue/dist/vue.runtime.common.js:3414:8)\n    at hook (/Users/xo/code/wvvw.me/node_modules/require-extension-hooks/hook.js:15:26)\n    at Object.require.extensions.(anonymous function) [as .vue] (/Users/xo/code/wvvw.me/node_modules/ava/lib/process-adapter.js:100:4)\n    at Module.load (module.js:488:32)\n    at tryModuleLoad (module.js:447:12)\n    at Function.Module._load (module.js:439:3)\n    at Module.require (module.js:498:17)\n    at require (internal/module.js:20:19)\n    at Object. (/Users/xo/code/wvvw.me/test/post.spec.js:4:1)\n    at Module._compile (module.js:571:32)\n    at extensions.(anonymous function) (/Users/xo/code/wvvw.me/node_modules/require-precompiled/index.js:13:11)\n    at Object.require.extensions.(anonymous function) [as .js] (/Users/xo/code/wvvw.me/node_modules/ava/lib/process-adapter.js:100:4)\n    at Module.load (module.js:488:32)\n    at tryModuleLoad (module.js:447:12)\n    at Function.Module._load (module.js:439:3)\n    at Module.require (module.js:498:17)\n    at require (internal/module.js:20:19)\n    at Object. (/Users/xo/code/wvvw.me/node_modules/ava/lib/test-worker.js:49:1)\n    at Module._compile (module.js:571:32)\n    at Object.Module._extensions..js (module.js:580:10)\n    at Module.load (module.js:488:32)\n    at tryModuleLoad (module.js:447:12)\n    at Function.Module._load (module.js:439:3)\n    at Module.runMain (module.js:605:10)\n    at run (bootstrap_node.js:423:7)\n    at startup (bootstrap_node.js:147:9)\n    at bootstrap_node.js:538:3\n\n  1 exception\n\n  \u2716 test/post.spec.js exited with a non-zero exit code: 1\n\nerror Command failed with exit code 1.\n```\n\n\n\npost.spec.js\n\n```js\nimport Vue from 'vue';\nimport test from 'ava';\n\nimport Post from '../src/components/Post.vue';\n\ntest(t => {\n    const N = Vue.extend(Post);\n    const vm = new N({\n        propsData: {\n            post: {\n                title: 'Test Post',\n                content: 'This is a test post.',\n                tags: ['test', 'post', 'example']\n            }\n        }\n    });\n    Vue.nextTick(() => {\n        t.is(vm.$el.textContent, 'This is a test post.');\n    });\n});\n```\n\n\n\n./src/components/Post.vue\n\n```vue\n\n\n\n{{post.title}}\n\n\n{{new Date(post.date).toDateString()}}\nby {{owner.name}}\n\n\n\u180e\u180e\n            This post hasn't been published yet.\n\u180e\u180e\u180e\n    \n\n\nimport Vue from 'vue';\nexport default Vue.extend({\n    name: 'post',\n    props: {\n        post: {\n            type: Object\n        },\n        user: {\n            type: Object\n        }\n    },\n    computed: {\n        owner() {\n            var vm = this;\n            var isAnonymous = 'author' in vm.post;\n            return {\n                name: isAnonymous ? vm.post.author.username : 'anonymous',\n                id: isAnonymous ? vm.post.author._id : 'anonymous'\n            };\n        }\n    }\n})\n\n\n```\n\n\n. Here's what I got working. One thing to keep in mind is you can't use Vue.extend(Component); in your testing if you imported Vue anywhere in any of your components as it'll throw an error.\nhttps://github.com/OmgImAlexis/wvvw.me/compare/7d6371062468e6ba617a69dbf44190b93f6ab963...master. Since most people use vue-router or something similar would you be up for adding details on how to use it to https://github.com/avajs/ava/pull/1361? I noticed I can test components fine but trying to test my app.vue spits out errors because my routes field is missing.\nThe code below allows vue-router to work properly.\n```\nimport Vue from 'vue';\nimport VueRouter from 'vue-router';\nimport test from 'ava';\nimport App from '../src/app.vue';\nimport HomeComponent from '../src/components/home.vue';\ntest.only('App should render', t => {\n    Vue.use(VueRouter);\n    const router = new VueRouter({\n        routes: [{\n            name: 'home',\n            path: '/',\n            component: HomeComponent\n        }]\n    });\n    const vm = new Vue({\n        router,\n        render: h => h(App)\n    }).$mount();\n    const tree = {$el: vm.$el.outerHTML};\n    t.snapshot(tree);\n});\n```\nEdit: Although this is working nyc isn't returning anything in the coverage report.\n```\n\u279c  vue git:(feature/add-default-vue-route) \u2717 yarn coverage\nyarn coverage v0.23.4\n$ nyc ava \n1 failed\nApp should render\n  /Users/xo/code/Medusa/vue/test/app.spec.js:21\n20:     const tree = {$el: vm.$el.outerHTML};\n   21:     t.snapshot(tree);                  \n   22: });                                      \nDid not match snapshot\nDifference:\n  Object {\n-   \"$el\": \"<div id=\"app\"><div>Home</div></div>\",\n+   \"$el\": \"<div id=\"app\"><div>\n+         This is the homepage.\n+     </div></div>\",\n  }\n\nTest.fn (test/app.spec.js:21:7)\n  processEmit [as emit] (node_modules/nyc/node_modules/signal-exit/index.js:155:32)\n  processEmit [as emit] (node_modules/nyc/node_modules/signal-exit/index.js:155:32)\n----------|----------|----------|----------|----------|----------------|\nFile      |  % Stmts | % Branch |  % Funcs |  % Lines |Uncovered Lines |\n----------|----------|----------|----------|----------|----------------|\nAll files |  Unknown |  Unknown |  Unknown |  Unknown |                |\n----------|----------|----------|----------|----------|----------------|\nerror Command failed with exit code 1.\n``. Yarn does preserve indentation so that'd be why I never noticed it. Since this and most of your other repos suggest using yarn over npm would you be willing to add it based on that?. Would it not be better to add an auto option towrite-json-file`?. Assuming this is related then.\n```\n$ nyc ava \n1 passed\n  1 rejection\nUnhandled Rejection\n  SyntaxError\n    XMLHttpRequest.open (node_modules/jsdom/lib/jsdom/living/xmlhttprequest.js:486:15)\n    dispatchXhrRequest (node_modules/axios/lib/adapters/xhr.js:45:13)\n    xhrAdapter (node_modules/axios/lib/adapters/xhr.js:12:10)\n    dispatchRequest (node_modules/axios/lib/core/dispatchRequest.js:52:10)\n```. Going off the warning from https://github.com/avajs/ava/commit/c01ac05ef610c87f5c264b1a3eb80f89b4fc0832 I had assumed it was related. If not feel free to remove my comments.. Since xo and ava share a lot of underlying libs this may have some relevance.\nRef: https://github.com/sindresorhus/xo/issues/212. Hopefully this is just a copying error but while testing this in my own project all I manage to get is an error thrown and my tests all fail. \n```\n\u279c  api.wvvw.me git:(master) \u2717 yarn ava -- test/user.spec.js  --verbose\nyarn ava v0.24.6\n$ \"/Users/xo/code/api.wvvw.me/node_modules/.bin/ava\" test/user.spec.js --verbose\n\u2716 start server Rejected promise returned by test\n1 test failed [12:36:40]\nstart server\nRejected promise returned by test\nRejection reason:\n\"Mongod shutting down\"\n\nerror Command failed with exit code 1.\n. Here's my `./user.spec.js`. My `./main.js` exports app as default.js\nimport test from 'ava';\nimport request from 'supertest';\nimport {MongoDBServer} from 'mongomem';\nimport {setupFixtures, removeFixtures, setupMongoose} from './utils';\ntest.before('start server', async () => {\n    await MongoDBServer.start();\n});\ntest.beforeEach(async t => {\n    const db = await setupMongoose();\n    const app = require('../main');\n// Setup any fixtures you need here. This is a placeholder code\nawait setupFixtures();\n\n// Pass app and mongoose into your tests\nt.context.app = app;\nt.context.db = db;\n\n});\ntest.afterEach.always(async t => {\n    const {db} = t.context;\n    // Note: removeFixtures is a placeholder. Write your own\n    await removeFixtures();\n    await db.connection.close();\n});\n// Note the serial tests\ntest.serial('create', async t => {\n    const {app} = t.context;\n    const res = await request(app).post('/user').send({\n        username: 'ava',\n        password: 'avarocks'\n    });\n    t.is(res.status, 201);\n    t.is(res.body.username, 'ava');\n});\ntest.after.always('cleanup', () => MongoDBServer.tearDown());\n```\n./utils/index.js\n```js\nimport mongoose from 'mongoose';\nimport {MongoDBServer} from 'mongomem';\nconst setupFixtures = () => {};\nconst removeFixtures = () => {};\nconst setupMongoose = async () => {\n    await mongoose.connect(await MongoDBServer.getConnectionString());\n    return mongoose;\n};\nexport {\n    setupFixtures,\n    removeFixtures,\n    setupMongoose\n};\n``. In./main.jsI import all my routes and then useapp.use('/user', user);`\n./routes/user.js\n```js\nimport {Router} from 'express';\nimport HTTPError from 'http-errors';\nimport log from '../log';\nimport config from '../config';\nimport {User} from '../models';\nimport {isValidObjectId} from '../middleware';\nconst router = new Router();\nrouter.get(['/', '/:id'], isValidObjectId, async (req, res, next) => {\n    if (req.params.id) {\n        const user = await User.find({\n            _id: req.params.id\n        }).exec().catch(next);\n    return res.send(user);\n}\n\nconst users = await User.find({}).sort({\n    date: -1\n}).exec().catch(next);\n\nres.send(users);\n\n});\nrouter.post('/', (req, res, next) => {\n    if (!config.get('signups.enabled')) {\n        // @TODO: This may not be the best status code for this.\n        return next(new HTTPError.MethodNotAllowed('Signups are currently disabled.'));\n    }\nconst user = new User({\n    username: req.body.username || '',\n    email: req.body.email || '',\n    password: req.body.password\n});\nlog.debug(user);\nuser.save().then(() => {\n    log.debug('User saved.');\n    return res.send({\n        user\n    });\n}).catch(err => {\n    log.debug('User error.');\n    // Duplicate field\n    if (err.code === 11000) {\n        return next(new HTTPError.Conflict('User already exists.'));\n    }\n    return next(err);\n});\n\n});\nexport default router;\n``. Okay this is an issue with mongomem, looks like it's trying to bind to0.0.0.0:27017even though it's usingget-port`.\nRef: https://github.com/CImrie/mongomem/issues/2. Yep after releasing the port from my docker container it's now fine. . As per https://github.com/CImrie/mongomem/issues/2#issuecomment-310947227 maybe this should use mongodb-memory-server instead since ava needs to run all tests in parallel? . @nodkz since you're the dev for mongodb-memory-server would you be able to help here?. For the example I'd suggest linking to something controlled by the avajs org so it can be updated if the recipe changes without a third party needing to do anything.. @codeslikejaggars @novemberborn thank you!. @codeslikejaggars @novemberborn thank you!. Hadn't thought of running it without nyc. Should I report this to nyc's repo?\n```\n\u279c  agenda git:(promise-rewrite) \u2717 yarn nyc ava test/definition.js\nyarn run v1.3.2\n$ /Users/xo/code/agenda/node_modules/.bin/nyc ava test/definition.js\n1 passed\n  1 failed\nreturn self with config\n/Users/xo/code/agenda/lib/definition.js:28\n27:  *  @property {Object} concurrency\n   28:  *  @property {Object} data     \n   29:  *  @property {Promise} fn The job\nError thrown in test:\nTypeError {\n    message: '\"fn\" param is required.',\n  }\nnew Definition (lib/definition.js:28:227)\n  Test.t [as fn] (test/definition.js:20:22)\n  processEmit [as emit] (node_modules/nyc/node_modules/signal-exit/index.js:155:32)\n  processEmit [as emit] (node_modules/nyc/node_modules/signal-exit/index.js:155:32)\n---------------|----------|----------|----------|----------|----------------|\nFile           |  % Stmts | % Branch |  % Funcs |  % Lines |Uncovered Lines |\n---------------|----------|----------|----------|----------|----------------|\nAll files      |       60 |       25 |      100 |       60 |                |\n definition.js |       60 |       25 |      100 |       60 |          36,46 |\n---------------|----------|----------|----------|----------|----------------|\nerror Command failed with exit code 1.\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\n\u279c  agenda git:(promise-rewrite) \u2717 yarn ava test/definition.js \nyarn run v1.3.2\n$ /Users/xo/code/agenda/node_modules/.bin/ava test/definition.js\n1 passed\n  1 failed\nreturn self with config\n/Users/xo/code/agenda/lib/definition.js:34\n33:     if (Object.prototype.toString.call(fn) !== '[object Promise]') {\n   34:       throw new TypeError('\"fn\" param is required.');             \n   35:     }                                                               \nError thrown in test:\nTypeError {\n    message: '\"fn\" param is required.',\n  }\nnew Definition (lib/definition.js:34:13)\n  Test.t [as fn] (test/definition.js:20:22)\nerror Command failed with exit code 1.\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\n``. Updating toava@1.2.0` and disabling babel fixed the issue. Thank you. \u2764\ufe0f  . No worries with closing the issue. Thanks for all the help.\nAny other ideas on where to look and is the Illegal invocation error a bug?\n```console\nroot@Devon:/mnt/disks/code/api# yarn ava --verbose -c 1\nyarn run v1.13.0\n$ /mnt/disks/code/api/node_modules/.bin/ava --verbose -c 1\n\u2714 disks \u203a get-disks \u203a should return no disks\n  \u2714 vms \u203a get-vms \u203a should return no vms\n  \u2714 vars \u203a get-vars \u203a should return all vars\n  \u2714 users \u203a get-users \u203a should return root user\n  \u2714 shares \u203a get-shares \u203a should return no shares\n  \u2714 info \u203a get-info \u203a should return all info (libvirt enabled) (3s)\n  \u2714 disks \u203a id \u203a get-disk \u203a should return 404 when no disk found\n  \u2714 disks \u203a id \u203a get-disk \u203a should return disk based on id\n7 tests passed\n  1 known failure\nvms \u203a get-vms \u203a should return no vms\nDone in 31.47s.\nroot@Devon:/mnt/disks/code/api# yarn nyc ava --verbose -c 1\nyarn run v1.13.0\n$ /mnt/disks/code/api/node_modules/.bin/nyc ava --verbose -c 1\n/mnt/disks/code/api/node_modules/clean-stack/index.js:1\nTypeError: Illegal invocation\n    at Object. (/mnt/disks/code/api/node_modules/clean-stack/index.js:6:20)\n    at Generator.next ()\n    at Object.replacementCompile (/mnt/disks/code/api/node_modules/nyc/node_modules/append-transform/index.js:58:13)\n    at Module._extensions..js (module.js:584:10)\n----------|----------|----------|----------|----------|-------------------|\nFile      |  % Stmts | % Branch |  % Funcs |  % Lines | Uncovered Line #s |\n----------|----------|----------|----------|----------|-------------------|\nAll files |        0 |        0 |        0 |        0 |                   |\n----------|----------|----------|----------|----------|-------------------|\nerror Command failed with exit code 1.\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\nroot@Devon:/mnt/disks/code/api# yarn ava --reset-cache\nyarn run v1.13.0\n$ /mnt/disks/code/api/node_modules/.bin/ava --reset-cache\n\u2714 Removed AVA cache files in /mnt/disks/code/api/node_modules/.cache/ava\nDone in 1.28s.\nroot@Devon:/mnt/disks/code/api# yarn ava --verbose\nyarn run v1.13.0\n$ /mnt/disks/code/api/node_modules/.bin/ava --verbose\n\u2714 disks \u203a get-disks \u203a should return no disks\n  \u2714 disks \u203a id \u203a get-disk \u203a should return 404 when no disk found\n  \u2714 disks \u203a id \u203a get-disk \u203a should return disk based on id\n  \u2714 shares \u203a get-shares \u203a should return no shares\n  \u2714 vars \u203a get-vars \u203a should return all vars\n  \u2714 users \u203a get-users \u203a should return root user\n  \u2714 vms \u203a get-vms \u203a should return no vms\n  \u2714 info \u203a get-info \u203a should return all info (libvirt enabled) (3.1s)\n7 tests passed\n  1 known failure\nvms \u203a get-vms \u203a should return no vms\nDone in 13.31s.\nroot@Devon:/mnt/disks/code/api# yarn ava --verbose\nyarn run v1.13.0\n$ /mnt/disks/code/api/node_modules/.bin/ava --verbose\n\u2714 disks \u203a get-disks \u203a should return no disks\n  \u2714 shares \u203a get-shares \u203a should return no shares\n  \u2714 vars \u203a get-vars \u203a should return all vars\n  \u2714 disks \u203a id \u203a get-disk \u203a should return 404 when no disk found\n  \u2714 disks \u203a id \u203a get-disk \u203a should return disk based on id\n  \u2714 users \u203a get-users \u203a should return root user\n  \u2714 vms \u203a get-vms \u203a should return no vms\n  \u2714 info \u203a get-info \u203a should return all info (libvirt enabled) (3s)\n7 tests passed\n  1 known failure\nvms \u203a get-vms \u203a should return no vms\nDone in 10.63s.\nroot@Devon:/mnt/disks/code/api# yarn ava --verbose -c\nyarn run v1.13.0\n$ /mnt/disks/code/api/node_modules/.bin/ava --verbose -c\n\u2716 The --concurrency or -c flag must be provided with a nonnegative integer.\nerror Command failed with exit code 1.\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\nroot@Devon:/mnt/disks/code/api# yarn ava --verbose -c 1\nyarn run v1.13.0\n$ /mnt/disks/code/api/node_modules/.bin/ava --verbose -c 1\n\u2714 disks \u203a get-disks \u203a should return no disks\n  \u2714 vms \u203a get-vms \u203a should return no vms\n  \u2714 vars \u203a get-vars \u203a should return all vars\n  \u2714 users \u203a get-users \u203a should return root user\n  \u2714 shares \u203a get-shares \u203a should return no shares\n  \u2714 info \u203a get-info \u203a should return all info (libvirt enabled) (3s)\n  \u2714 disks \u203a id \u203a get-disk \u203a should return 404 when no disk found\n  \u2714 disks \u203a id \u203a get-disk \u203a should return disk based on id\n7 tests passed\n  1 known failure\nvms \u203a get-vms \u203a should return no vms\nDone in 27.25s.\nroot@Devon:/mnt/disks/code/api# yarn nyc ava --verbose -c 1\nyarn run v1.13.0\n$ /mnt/disks/code/api/node_modules/.bin/nyc ava --verbose -c 1\n/mnt/disks/code/api/node_modules/clean-stack/index.js:1\nTypeError: Illegal invocation\n    at Object. (/mnt/disks/code/api/node_modules/clean-stack/index.js:6:20)\n    at Generator.next ()\n    at Object.replacementCompile (/mnt/disks/code/api/node_modules/nyc/node_modules/append-transform/index.js:58:13)\n    at Module._extensions..js (module.js:584:10)\n----------|----------|----------|----------|----------|-------------------|\nFile      |  % Stmts | % Branch |  % Funcs |  % Lines | Uncovered Line #s |\n----------|----------|----------|----------|----------|-------------------|\nAll files |        0 |        0 |        0 |        0 |                   |\n----------|----------|----------|----------|----------|-------------------|\nerror Command failed with exit code 1.\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\n``. @jdalton, sorry to bother but any chance you could look at this and give some guidance on where the issue may be coming from?. The await is redundant here since it's in a return statement. . Might want to switch this toexample@example.comjust to be safe that you're not using a real email in an example.. Does this type of import not require webpack? I didn't think this worked out of the box with babel, etc. yet.. This shouldn't be needed since it's part of the default set.. Add js to keep highlighting working.. Add js to keep highlighting working.. @zellwk open an issue with them, it should be added when it's transpiled to ES5 before they publish it.. This really needs a reason why.. Why not just use a new database for each test? That's how we run tests with [agenda](https://github.com/agenda/agenda/pull/506) and we don't have any issues.. Yes you can. http://mongoosejs.com/docs/connections.html#multiple_connections. That does make sense but since ava is meant to be run in parallel it'd be better to explain how to set it up by default instead of expecting tests to run in serial.. This can just be() => {since t isn't used and there's nothing using async.. No need for thet` argument here.\nI'd change it to test.afterEach.always(() => User.remove());. No need for the t argument.. No need for the t argument.. Why was this changed when \"Mongoose\" is know as \"Mongoose\" not \"Mongoose ODM\"?. async and await are redundant here since you're returning a Promise from User.remove().. There's no need for it and anyone using xo will need to remove it if they copy+paste. Better to just remove it and set an example by only shipping code that's linted even if it's only in the docs.. ",
    "tylerjharden": "So this is good to go? And fixes \"warning ava > babel-preset-es2015-node4@2.1.1: Use npmjs.org/babel-preset-env instead, see https://github.com/babel/babel-preset-env\"?\nVery exciting if so.. ",
    "danez": "This PR removed the stage-2 preset and broke my test when updating as I was using object-spread. Should I change the README and remove the mention of stage-2 or was that removed by mistake?. ",
    "leonardovilarinho": "I couldn't make it work :(\nthanks anyway, I'll see if I can find another solution. ",
    "Austio": "@leonardovilarinho I was able to get this working with adding this to my file package.json\n\"ava\": {\n    \"require\": [\n      \"babel-register\"\n    ]\n  },\n  \"babel\": {\n    \"presets\": [\n      \"es2015\"\n    ]\n  }\n. ",
    "leandrooriente": "Guys, do you have any update regarding this request?\nI'm facing the same issue.\nIf I use t.is(resetBehaviour.calledOnce, false); it outputs this:\n```bash\n51:     wrapper.unmount();\n   52:     t.is(resetBehaviour.calledOnce, false);\n   53: });\nDifference:\n\ntrue\nfalse\n\nTest.is (src/common/components/App/App.test.js:52:7)\n  processEmit [as emit] (node_modules/nyc/node_modules/signal-exit/index.js:155:32)\n  processEmit [as emit] (node_modules/nyc/node_modules/signal-exit/index.js:155:32)\n```\nBut if i use t.false(resetBehaviour.calledOnce) it returns this huge output: \n```bash\n51:     wrapper.unmount();\n   52:     t.false(resetBehaviour.calledOnce);\n   53: });\nValue is not false:\ntrue\nresetBehaviour.calledOnce\n  => true\nresetBehaviour\n  => Function proxy {\n    args: [\n      [],\n    ],\n    callCount: 1,\n    callIds: [\n      4,\n    ],\n    called: true,\n    calledOnce: true,\n    calledThrice: false,\n    calledTwice: false,\n    displayName: 'spy',\n    errorsWithCallStack: [\n      Error {\n        message: '',\n      },\n    ],\n    exceptions: [\n      undefined,\n    ],\n    firstCall: {\n      args: [],\n      callId: 4,\n      callback: undefined,\n      errorWithCallStack: Error {\n        message: '',\n      },\n      exception: undefined,\n      lastArg: undefined,\n      proxy: [Circular],\n      returnValue: undefined,\n      thisValue: {\n        resetBehaviour: [Circular],\n      },\n    },\n    id: 'spy#3',\n    instantiateFake: Function create {},\n    isSinonProxy: true,\n    lastCall: {\n      args: [],\n      callId: 4,\n      callback: undefined,\n      errorWithCallStack: Error {\n        message: '',\n      },\n      exception: undefined,\n      lastArg: undefined,\n      proxy: [Circular],\n      returnValue: undefined,\n      thisValue: {\n        resetBehaviour: [Circular],\n      },\n    },\n    notCalled: false,\n    returnValues: [\n      undefined,\n    ],\n    secondCall: null,\n    thirdCall: null,\n    thisValues: [\n      {\n        resetBehaviour: [Circular],\n      },\n    ],\n    toString: Function toString {},\n  }\nTest.t [as fn] (src/common/components/App/App.test.js:52:5)\n  processEmit [as emit] (node_modules/nyc/node_modules/signal-exit/index.js:155:32)\n  processEmit [as emit] (node_modules/nyc/node_modules/signal-exit/index.js:155:32)\n```\nI'm using sinon spies and the log output is completely uselesss in this case.. ",
    "aularon": "Hello!\nI updated to latest master as advised, and now tests are consistent across all nodejs versions (4, 5, 6 and 7!): All tests pass except for these two (Both of which are irrelevant).\nBefore, test results were inconsistent between node versions, and these two failing tests were succeeding across all versions!\nI am going with your approach to support any promise.\nThank you for your time and efforts!. ",
    "hzlmn": "No I don't have any ava configuration inpackage.json. @sindresorhus Hi, did you have a chance to look on it?. ",
    "TheLarkInn": "Thanks for the ping @sindresorhus \ud83d\udc31 \ud83d\udc15 \ud83d\udc13 \ud83d\ude80. Was surprised I didn't know this feature existed!! MFW:\n\n. As someone who didn't know about this feature, I'm asking questions from a first time user standpoint that I think are important to address: \n\nI would love to see an example of what this file looks like. Could a code block be added above to show this? Is it just a list of t.test? Or better yet is there a repo for this?\n\n. ",
    "mlewando": "Hi @novemberborn, thank you for explanation. I read (looks like not carefully enough) the documentation in RFC 001 and understand it and #631 as the extensions parameter will be one of the first things to do. \nAlso I will very appreciate to have some workaround for this extensions problem soon. Thats why I decided to just write one in the meantime as it looks like very simple task. Unfortunately I run into some problems with tests (as you can see in CI statuses) and still have some problems with them. For the sake of training I'm going to make it work on my fork anyway, so there is no lost effort from my side ;)\nOf course when #631 will be ready I'll be glad to help with it as much as I can :)\nBy the way, yours tests runs very slow... after running npm run test-win it takes ages to finish and some error logs are so big that it is impossible to know whats wrong. Do you know any simple fix to run test faster and have better error reporting?. ",
    "bjentsch": "I seem to be suffering the same issue. Force-upgrading to @ava/pretty-format@1.1.0 seems to solve the initial problem, but it generates hundreds of log lines on the console now for my failing test. I tracked it down to this: \n\n\nWhen using t.true or t.false as an assertion, ava tries to show me useful output by showing me all objects involved. In my case, I was doing a [somedomelement].classlist.contains(...) assertion, which resulted in having the console full of the jsonified DOM element and its (prototype) properties.\n\n\nWhen rewriting the test to use t.is, everything works as expected again.. Oh. Well. Yes. Sorry for the noise.. \n\n",
    "lusentis": "Hi,\nI was testing this PR on a project, and found an issue with objects containing undefined values.\nI've created a repo to reproduce this: https://github.com/lusentis/ava-test-1223\n\nLet me know if I can further help,\nthanks!\n. Sure, I didn't explain it very well...\nThe issue is that the snapshot file does not contain the value which is undefined, so t.snapshot() fails.\nThis code:\njs\n        const obj = {\n                a: 123,\n                b: 'bar',\n                c: null,\n                d: undefined\n        }\n        t.snapshot(obj);\nproduces this snapshot when running ava -u:\n```js\n{\n  \"example snapshot of an object with undefined values\": {\n    \"a\": 123,\n    \"b\": \"bar\",\n    \"c\": null\n  }\n}\n``\nwhich is missing the \"d\" key from the Object.\nObiously,t.snapshot()fails when run again because it complains thatd: undefined` is not in the snapshot.\n. Found this: https://github.com/kaelzhang/node-code-stringify it might help (I'm playing with this: https://github.com/lusentis/ava/commit/a39f1be2a48aa7c7e3b2f9a2aa6958a46b2e302b).\nThanks for your work on this! Let me know if I can help testing out.. ",
    "dinoboff": "@novemberborn Assuming it serialise the new value and deserialise it before comparing it to the deserialised old value, it can support toJSON, no?  . ps: when storing a new value, snapshot could warn about any value lost in the process: it could show a diff of the value with the deserialised stored value.. @jdalton to disable module transform you can try with @dinoboff/babel-preset-stage-4 (scoped package for this avajs/babel-preset-stage-4#8):\nnpm i @dinoboff/babel-preset-stage-4\nAnd in package.json:\n{\n  \"ava\": {\n    \"babel\": {\n      \"presets\": [\n        [\n          \"@dinoboff/stage-4\",\n          {\n            \"modules\": false\n          }\n        ]\n      ]\n    }\n  }. ",
    "tzvipm": "One more item for the TODO: \n\n[ ] Allow multiple snapshots per test\n\nCurrently, the snapshots are indexed by test name, it seems; however, when I tested in my project, I received false positives when there were multiple snapshots in a test. The current implementation in master (using jest-snapshot) appends a number after each one.\nFor example, if my test is called \"foo is consistent\", and it has 2 snapshots, We could do something like: \"foo is consistent 1\" and \"foo is consistent 2\".\nWhat I'm currently doing is a bit nicer to read (diff-wise), which is the snapshot a hash with the different snapshots. That way they each have a name associated with them.. On my project, the diffs seem to be backwards. For example, if my test does: t.snapshot({foo: 'foo'}), and then I change the test to t.snapshot({bar: 'bar'}), I think the diff should be:\ndiff\nObject {\n- foo: foo,\n+ bar: bar,\n}\nbut I see the opposite:\ndiff\nObject {\n- bar: bar,\n+ foo: foo,\n}. I tried playing around with non-native objects and it became quite messy quite fast. Perhaps the MVP doesn't need to support anything that can't be serialized / stringify-ed.. We should add something like:\njavascript\nlet hasTests = false;\nfor(var key in obj) {\n  if(obj.hasOwnProperty(key))\n    hasTests = true;\n    break;\n  }\n}\nif (!hasTests) return;\nat the beginning of this function. That way we don't end up making a bunch of empty files/divs.\nWe also need to ask ourselves what we want to do if someone deletes a test. Should we delete the snapshots too? Or leave them there?. FYI, I forked off and tried this out and it works quite well: https://github.com/Omniroot/ava/pull/2/commits/c95962a4de900e7437965d5f7ecb04b6aaf98b9c. ",
    "transcranial": "Removing those lines make no noticeable difference. Changing my package.json config, including removing it, seems to make no difference either. I'm able to reproduce with a new barebones project, but with the problematic subdirectory copied over.\nInterestingly, it seems to be the cumulative effect of all 4 glob patterns, and not any one in particular. Leaving 1 pattern, tests pass but takes 11s. Leaving 2, pass but takes 18s. Leaving 3, pass but takes 35s. With all 4, takes 55s and then exits with the error above. Adding the subdirectory manually to excludePatterns of handlePaths in findTestHelpers solves the issue and tests pass in 1s. Folder maxdepth is 3. Stats:\nsh\n$ find dev/ -type d | wc -l\n17630\n$ find dev/ -type f | wc -l\n107270\nThe folder is already in .gitignore. Would it make sense to automatically exclude those in .gitignore in addition to node_modules?. ",
    "jhnns": "Seems like supports-color reports false on Travis anyway.... do you have a hint how to solve this? I'm already mocking process.stdout.isTTY. > I think the only change that's needed is to require chalk inside the if (!isForked) { body.\nThat is true, but since some of the required dependencies might also require chalk or supports-color (you never know :wink:), I thought it would be better to re-arrange all require statements. But I can change that if you're not ok with that.\n\nThat said we closed #1124 because it seemed fixed. Are you still running into this issue with the latest AVA version?\n\nMy test is failing if my patch is not applied, so I assume that the issue is still there.. > Could you share your test? Where / how specifically is it failing?\nIt's included in the PR. If the patch is not applied, the test fails saying that chalk.enabled is false.\nIn order to finish this PR from my perspective I would need to fix it on Travis. But it's hard to debug it since it works locally ^^. > Seems to already work fine?\nYes, I can confirm that. Since lib/fork.js creates the child process with --color or --no-color depending on the cli.flags, supports-color will report the desired value in the child process.\nI've update my PR and just added tests to test/fork.js to ensure that color support is initialized correctly.\nWhile skimming the source code, I saw that in test/cli.js there's some code that patches chalk. This was necessary because there is a test that checks for colored output while supports-color reports false inside the tap test process because it does not have TTY support. We could set the env flag FORCE_COLOR to enforce colored output, but in that case we would need to update tests in test/api.js in order to check for colored output.\nSo from my POV, you should decide whether you want to:\n\ncheck for colored output. In that case, we should set FORCE_COLOR and update the tests in test/api.js or\nnot check for colored output. In that case, we would need to change the test in test/cli.js to set the --no-color flag and check against uncolored strings.\n\nThe current PR reflects the second choice, but I'm willing to change that since I've already invested some time in it. Just tell me what you want. If you don't care about this and you want me to leave you alone, that's also fine. Then just close this \ud83d\ude01\n. I just love ava as a test runner \ud83d\udc4d \nI'm happy to give something back \ud83d\ude01 . ",
    "GregOnNet": "Yes, totally agreed.\nThanks for your quick response and for providing ava.\nI've been working with it for two weeks now and I love it.\nKinds\nGreg. Thanks for documenting this.\nThis is my current way to extract test data from the t.context. :+1: . ",
    "ochafik": "Awesome, thanks!. ",
    "ryardley": "The Javascript community is moving towards a more functional style and being able to use methods functionally without depending on an object context is becoming quite common. \nI can certainly see this as a low priority change but coming to the API for the first time I think it was something I expected to be able to do but was surprised I could not and I figure it would probably not be a huge change.\nHonestly probably the main reason you want to do this is that it works with tape and would remove friction moving to ava:\n```javascript\n// tape\ntest('timing test', ({plan, equal}) => {\n    plan(2);\n    equal(typeof Date.now, 'function');\n    const start = Date.now();\nsetTimeout(function () {\n  equal(Date.now() - start, 100);\n}, 100);\n\n});\n```\n. You can get around that by using lodash's spread function or ramda's apply. A use case might be some table testing like so:\njavascript\n// eg. using ramda\ntest('test all the things', ({ deepEquals }) => {\n  thingsToTest\n    .map(({ input, expected }) => [ myfunc(input), expected ])\n    .map(apply(deepEquals));\n});\nHowever if there are restrictions in place by power-assert then that prevents this being something quick to implement so perhaps that is reason to shelve the idea as low priority. \n. ",
    "farmasek": "Thank you this works, but i lose great comparison difference in console. \nIs it ok to keep using toJS() or there is a possible problem with it ? . ",
    "subodhpareek18": "Ok yes, I was thinking as much. Any suggestions for a TAP reporter that already does this? I was not able to find much. @novemberborn . Ok thank you. ",
    "caesarsol": "To be verbose, you want to place a PR containing @vadimdemedes commits in Jest upstream repo (without changing their color defaults), then use their version.\nRight?. I think it's just adding possibility of new colors for types, easy to keep the default to a neutral color theme Jest could accept, then switch it to unicorn-colorful in AVA \ud83d\ude09\nI'll see if I find the time!. ",
    "cpojer": "Happy to help! Please send PRs our way and we'll make sure to address them.\n@novemberborn which aspect of the monorepo is giving you trouble?\n. They are still Jest snapshots though, aren't they? If we can, we should keep the header because it looks to me like they are compatible. We can update error messages and the documentation link that it links to, though. Feel free to send PRs to Jest to make the public API more usable as we'll undoubtedly break private APIs in the future.. fyi pretty-format 18 is out :). ",
    "doriandrn": "@forresst Thanks for this!. ",
    "knpwrs": "Hey, guys. @jackmellis commented on my post with his own approach which is rather interesting. He points to a forum post he made as well as two modules he made: require-extension-hooks and require-extension-hooks-vue. I think the implications of either approach should be discussed and the vue community should settle on one. This issue may or may not be the place to discuss that, but I would happily deprecate my package if people like @jackmellis' approach better, or keep it around if there are valid use cases for either approach.. Hey, guys. @jackmellis commented on my post with his own approach which is rather interesting. He points to a forum post he made as well as two modules he made: require-extension-hooks and require-extension-hooks-vue. I think the implications of either approach should be discussed and the vue community should settle on one. This issue may or may not be the place to discuss that, but I would happily deprecate my package if people like @jackmellis' approach better, or keep it around if there are valid use cases for either approach.. Ideally any dependencies you need to mock should be provided via props. When that's the case, mocking is trivial.. I'm going to back up @jackmellis and say that it's probably better for a vue forum, but if you wanted to shoot me at email at ken@kenpowers.net we could discuss it at length.\nKeep in mind that my recommendations are coming from general knowledge of component-based architecture (mostly React). This is by no means the only way to do things, just what I've found to work best for me. Essentially the pattern I would recommend is to pass all external state as props. In your case that would mean passing the accepted state as a prop, and emitting an \"accepted\" event when the user has accepted the policy. The root (entry-point) of your application, save for any other state-management patterns (vuex, etc), should be responsible for getting state from external resources (cookies, local storage, whatever else).\nConsider a component tree where application state is passed unidirectionally from the root component to child components. By having your cookie law component materialize its own state from cookies, you effectively have multiple component tree roots. This isn't necessarily wrong, in fact in some cases it's what would be recommended (see the concept of container vs presentational components in React/Redux), but I would encourage keeping such patterns to a minimum. The idea is to be able to spin up your components in any environment without having to rely on the existence of any particular API in the environment.\nSo now you have a generic component which is easy to test, but now you may be asking how to test the entire application. At this point such a task should be handled by integration/e2e testing, which is a whole different beast. You may also be thinking that it's appropriate to have the cookie law component read state from cookies since that is the component's only purpose, but you should also keep in mind that there isn't actually a \"cookie law\", just a \"user tracking law\" (please note, I am not a lawyer, this is not legal advice). By constructing your component in a way where the accepted state is passed as a prop, your component more reflects the reality of the law, regardless of where the accepted state is actually stored.\nI realize this has been long and rambly, so TL;DR: pass the accepted state as a prop, emit an accepted event when the user accepts. IMHO, realizing that this is not the only, or necessarily the most correct way to do things in all situations, the component should be constructed in such a way that it is not tracking the accepted state, but relying on the application to provide and manage accepted state.. https://github.com/andrepolischuk/approximately-equal. I think it's worth pointing out that Vue has started work on official testing tools. They have an example with AVA available as well.. I don't personally use Vue enough anymore to offer any official recommendations.. I don't personally use Vue enough anymore to offer any official recommendations.. ",
    "blake-newman": "https://github.com/jackmellis/require-extension-hooks-vue/pull/1\nThis adds more support for compiling of templates which should a) improve overall speeds and b) support es2015 features in templates. Will add recipe over next couple of days, should have enough free time to complete. \nRequire.extension.hooks is probably the best solution, most extensible and fastest. vue-node is much slower as each component requeires Webpack to do alot of work. Which adds approx 3-4s to each component import. \nExtension hooks seems to have least impact, and with caching techniques this could be much faster to an already fast solution. Less configuration required aswell. Also mapping of code coverage is very accurate and not yet seen any issues. . @knpwrs Indeed. Will create a PR to link to that example repository. @novemberborn i don't think the PR is relevant. Also there is many examples, via vue-test-utils that enhance what is on offering here. \nSorry for the late reply. Agree, currently you can add more options to babel extension plugin and there will most likely be more work for vue extension plugin to support other languages (TypeScript, Pug ect). Once this is done then we could create this npm module to wrap these options to make it simplier.\nFor now i think this setup is relatively easy to create and at least gives users the ability to test Vue with Ava.js which is a large step forward to Vue.js testing workflow.. On a further note, it should be trivial to add support for this in https://github.com/vuejs-templates/webpack so this could be the first stepping stone. . Hmm perhaps this logic should sit in the ava-vue-setup as mentioned. As we can expose all this functionality with options, and maintainability for users will be reduced. Also i'd rather provide a fully functional solution for the time being. Would cause more issues to provide all that code, especially if there is an edgecase that hasn't yet been thought off, such as dynamic import statements import('a/file.js').. I'm happy for this recipe to be released. Looking at adding a ava-setup-vue package. However due to the flexible nature of Vue files will need to be more customisable so you can select transpilers. Which will take a bit of time. This recipe is good fine for a standard Vue setup but not much more . This is not true. Using the hooks compiles the es6 to es2015. You can use the source components. . Not sure of the point of this example, same as above but es imports. \n@sindresorhus can you confirm @OmgImAlexis is also correct. . Agree with using async and await syntax. . Why would vm.message be a promise? No need to await this. ",
    "jackmellis": "I've been playing with ava this last week with vue. It seems to work fine with require-extension-hooks - although loading babel-core in every process is incredibly slow - I found myself just writing a small script that just replaces all import/export statements instead of using the full force of babel in a node environment.\nOne issue I've come across is that ava will often hang when an assertion fails on a reactive property. I assume it's to do with trying to build a fairly complicated vue-embroiled stack trace?\nExample:\n```javascript\n// where vm.dirty === false\n// This test causes ava to hang until the timeout has elapsed (and even then sometimes it keeps hanging)\ntest('...', t => {\n  t.true(vm.dirty);\n});\n// This test works fine\ntest('...', t => {\n  var dirty = vm.dirty;\n  t.true(dirty); // dirty == false\n});\n```. Nice. I've taken the exact test code and done the same with require-extension-hooks instead of webpack. Goes from 6.8s to 2.4s on my machine...\nhttps://github.com/jackmellis/avoriaz-ava-example. This might be a discussion for the vue forum. I haven't tried it myself and I'm not sure his well they'll play together but I'm assuming require-extension-hooks should work with rewire so that would give you a simple way to mock your required modules.\nAs for transitions; I'm not sure if anyone else has had issues like this? You could always try registering a mock transition component?\nAbd now shameless plugging: You can do component dependency injection with vue-inject and quick quick component instances with mocked properties and props with vuenit :). @OmgImAlexis I've published require-extension-hooks-vue 0.2.2 which may or may not fix your issue...\ud83e\udd1e . Yes I've been thinking about this for a while now. Because Ava starts so many new processes, in-memory caching becomes redundant. I feel this is where the performance is hitting (that and the sheer size of babel-core) I think a possible solution would be to add some persisted caching to the core require-extension-hooks module, so unchanged files are not transpiled over and over. I've not had time to implement it yet though.. I know I wrote the babel extension hook but I've actually stopped using it myself due to the amount of slowdown babel causes.\nI instead went for manually transpiling export/import statements like this as node can already handle pretty much all of ES2015. So unless you're using cutting edge 2017 stuff you'll probably find this much faster than babel (I should probably create a package for it).\nI'm aware the impact of importing babel-core in every ava process is a known issue but I'm not sure about whether people are happy to put up with it or not?. ",
    "marcosmoura": "I could make it work using vue-node.\nIts working great with istanbul for coverage using nyc:\nhttps://github.com/marcosmoura/vue-boilerplate. ",
    "eddyerburgh": "Here's an example repo - https://github.com/eddyerburgh/avoriaz-ava-example. ",
    "AdrianSkierniewski": "Hi, How about mocking dependencies in vue component in this approach?. @knpwrs Thank you for a quick answer to my question. I'm new in Vue.js world and I don't fully understand how I should test Vue components (everything in a single file).\nYesterday I created a simple setup with ava using guides from this issue. \nBefore I found this issue I was looking on Vue.js docs to see how recommended approach looks like, but I only found information about mocha + karma + webpack setup. Then I found this page https://vue-loader.vuejs.org/en/workflow/testing-with-mocks.html \nThe solution suggested in this issue doesn't use vue-loader so I can't mock ES6 modules, right? That's why I asked about other ways to easily mock ES6 modules. There is no constructor dependency mechanism or IoC container in place. \nHere are some files with my approach:\n- test setup\n-  vue.js components\n- test\nFor now, I just created some method to simply return an instance, so I can easily mock it in tests here. I'm curious about how can I use props to pass ES6 module dependency. Isn't props designed to pass some variables from the layout?\nI had one additional problem. I couldn't test mounted component with transition \nHTML\n<template>\n  <transition appear name=\"slideFromBottom\">\nI guess it's a limitation of the approach suggested in this issue, right?\n. @knpwrs Thank you for such detailed answer :) I agree with your point of view, but in this particular application, I only need some small components that don't form any bigger component. So it feels a little overengineered in this particular case. I definitely take a closer look at this approach when I start writing some bigger components.\n@jackmellis Do you have any example code how to use vue-loader with require-extension-hooks? I'm quite new on this topic and I'm not even frontend developer :) Any example would be nice, so I could try to figure out how it works.. ",
    "brokenseal": "This is a fairly old thread and the other mentioned issues are inactive as well.\nI am thinking of dumping ava in favor of some other tools because of this.... Of course, I am using a couple of test.true(objA.name === objB.name), of course it fails but I will never know what objA.name and objB.name values are because the representation, in the terminal, of those two objects is so humongous that it even exhausts the maximum lines terminal capacity, therefore I can't even scroll up enough.\nFor privacy reasons, I can't share a screenshot at the moment.. I understand, I'll see if I can try your advice out then.. ",
    "jakwuh": "@novemberborn thank you for the response. I think having message: String(err) + removing stack property is the best solution here. Isn't it?. @novemberborn here we go, now regardfully listening to any feedback :). @novemberborn here we go \ud83d\ude42 . Thanks  \ud83d\ude0a . Yes, this totally makes sense, still I find fairly to say that errors coming to normalizeError are already serialized (as I can see from the test-worker code).\nAlso thank you a lot for your fast and detailed feedback.. ",
    "simonepri": "@novemberborn still there isn't a way to exclude those large non-tests folders?\nIn my case i have a large folder (4GB) and if the folder is present, ava never starts.\nAlso if I pass a single test file:\nava test/some-test.js\nit hangs forever.. @novemberborn \nI've already tried this (Let's suppose that bin contains lots of subfolders and files that aren't code):\n\"ava\": {\n  \"match\": [\n    \"!bin/**\",\n    \"test/**/*.js\"\n  ]\n}\nBut nothing changes. \nI'll try your suggestion \ud83d\udc4d but I think is pretty the same thing I've already done, am I wrong?. Already tried also with files.\n\"ava\": {\n  \"files\": [\n    \"!bin/**\",\n    \"test/**/*.js\"\n  ],\n  \"source\": [\n    \"index.js\"\n  ]\n}\nI think I've tried any possible configuration \ud83d\ude06\nIt seems that the glob recursively visit the paths also if they are ignored (I think because it allows you to unignore a specific path inside an ignored one)\nE.g:\n\"!bin/**\" \"bin/**/somepath/**\"\nI think we should add some options to filter the paths before passing it to the matcher.\nLike --exclude -e. ",
    "despairblue": "go for it!\nOn Sun, Mar 5, 2017, 12:47 Mark Wubben notifications@github.com wrote:\n\n@mmkal https://github.com/mmkal it's all yours!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/avajs/ava/issues/1291#issuecomment-284222586, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AA4neW60AvpjfrAjbBABXmH3Vu3HLuOwks5riqDpgaJpZM4MOj4P\n.\n-- \nQ: Why is this email five sentences or less?\nA: http://five.sentenc.es\n. @mmkal I'll look into it tomorrow when I'm back at work, I was off for a week.. @mmkal I'll look into it tomorrow when I'm back at work, I was off for a week.. @mmkal Awesome work in this PR. I'm just still struggling to get this to work with async beforeEachs. Changing the type of T in the beforeEach is not picked up by the typescript compiler:\n\n```js\nimport * as ava from 'ava'\nimport * as mongodb from 'mongodb'\nfunction contextualize(getContext: () => T): ava.RegisterContextual {\n  ava.test.beforeEach(async (t) => {\n    Object.assign(t.context, await getContext())\n  })\nreturn ava.test\n}\nconst test = contextualize(async () => {\n  const db = await mongodb.MongoClient.connect('mongodb://localhost:27017')\nreturn { db }\n})\ntest('foo', async (t) => {\n  // fails with \"TS2339: Property 'db' does not exist on type 'Promise<{ db: Db; }>'.\"\n  // console.log(t.context.db.serverConfig)\n// works\n  const context = (await t.context)\n  console.log(context.db.serverConfig)\n// is true since t.context is not a promise\n  console.log(t.context ===  context) // true\n})\n```\nI think it's still ok to make each test async and await the context since it still saves me from defining an interface and casting the context.. Nevermind changing the type-signature to actually reflect what's going on fixes this just fine. The issue is closed. Thank you! \nfunction contextualize<T>(getContext: () =>Promise<T>): ava.RegisterContextual<T> {. Thanks, done.. ",
    "machineghost": "Sorry for not responding sooner. Thanks for the response: --match=\"!*something*\" is perfect, and it was in the docs so I apologize for not finding that on my own.\nThe globbing solution is great too, thanks, although it's not a perfect solution for me personally.  When I run Ava I do it through an NPM script.  We have both front- and back-end tests, in different folders, with frontend/backend test paths hard-coded.  This is great for easily running front/back-end tests only, but it makes it impossible to change the path to target a subset of tests.\nOf course I can always run ava directly from the command line and glob anything I want.  It just would be nice for me personally if there was a way to \"filter\" within whatever glob/path I've already specified so I could continue using my script rather than having to switch between running ava and running npm run test-backend.  But I'm not sure this matters to anyone else.. P.S. For anyone happening on this thread afterwards I should mention that you want to use --match='!*something*' (note the single quotes).  If you use double quotes your command line may very well interpret \"! as something entirely different from what you want.\nThe documentation does have the correct (single) quotes though, so I'm just mentioning it for this thread.. Thanks so much for providing this recipe! By the way, in my version of Ava the path to the Ava executable was in node_modules/ava/cli.js.\nAlso (for anyone else coming after me) if your Node root isn't your Webstorm project root you'll need to update the working directory to point to the Node root.. ",
    "avaly": "I've already added the @ava/stage-4 preset in my babel config (see gist), as instructed by the Babel recipe at: https://github.com/avajs/ava/blob/master/docs/recipes/babelrc.md, so I would expect that would be the only thing needed to configure. Is AVA adding anything else to the babel config in the case where I don't use \"babel\": \"inherit\"?\nThere definitely is some docs missing regarding this, since this can be confusing, especially for new Babel users. . OK, this seems to have been triggered by the transform-regenerator babel plugin which gets included with the es2015 preset.\nClosing this, since it's not an AVA issue.. Maybe I misunderstood the meaning of --concurrency=0. I had assumed it was doing the same as --serial. Using --concurrency=1 works for me. Feel free to close if you believe it's not a bug @novemberborn .. ",
    "shofel": "@sindresorhus I've resolved the conflict.\nAfter some time writing tests for streams I have some thoughts about returning observables)))\ntldr: It looks like the callback syntax is the best for testing values in observables.\nLet's look at the example from ava documentation.\njs\ntest(t => {\n    t.plan(3);\n    return Observable.of(1, 2, 3, 4, 5, 6)\n        .filter(n => n % 2 === 0)\n        .map(() => t.pass());\n});\nUsing .map() for side effects is a hack in FRP and in FP in general. All the side effects should be isolated in subscribers. Thus above beautiful snippet turns into\njs\ntest(t => {\n    t.plan(3);\n    return Observable.of(1, 2, 3, 4, 5, 6)\n        .filter(n => n % 2 === 0)\n        .subscribe({\n            next: () => t.pass()\n        });\n});\nBut now we return not a stream, but a subscription object. Thus we need a var to save the stream:\njs\ntest(t => {\n    t.plan(3);\n    const stream = Observable.of(1, 2, 3, 4, 5, 6)\n        .filter(n => n % 2 === 0);\n    stream.subscribe({\n        next: () => t.pass()\n    });\n    return stream;\n});\nThis is a lot less fruitful. Unexpectedly, it looks like the callback syntax is the most natural and semantically correct in this case:\njs\ntest.cb(t => {\n    t.plan(3);\n    Observable.of(1, 2, 3, 4, 5, 6)\n        .filter(n => n % 2 === 0)\n        .subscribe({\n            next: () => t.pass(),\n            complete: () => t.end()\n        });\n});\nGiven all above, I don't really know if returning an observable from a test has got any advantage.. I've been also thinking on await syntax for asserts\njs\ntest(async t => {\n    const oneTwoThree = xstream.from([1, 2, 3])\n    t.deepEqual(await oneTwoThree, [1, 2, 3])\n});\nAnd it turned out the RxJS team have had a discussion about await-able observables, and the take out was the decision to not have this because there are more caveats and ambiguity than profit.. As for now, I write tests this way:\n```js\nimport {toPromise} from './helpers/observable'\ntest('a stream of 1', async t => {\n    let one = xs.of(1)\n    t.deepEqual(await toPromise(one), [1])\n})\n```\nWhere toPromise is almost the same as observable-to-promise\n``` js\nexport function toPromise (observable) {\n  let values = []\n  let remember = x => { values = values.concat(x) }\nreturn new Promise((resolve, reject) => {\n    observable.subscribe({\n      error: reject,\n      complete: () => resolve(values),\n      next: remember\n    })\n  })\n}\n```\nAs long as we check only values, but not timing, it works well.. Yes, the example from the docs works fine with xstream. Hope it's as good for most and rxjs.. Yes, the example from the docs works fine with xstream. Hope it's as good for most and rxjs.. ",
    "selenium-tester": "Apologies for joining a dead thread; I came looking through the issues for any other discussions about parameterized/data-driven test support in Ava. I found lots of discussion in various tickets, but this seemed the most recent and most directly related to my search so I wanted to add an extra voice to the conversation.\nThe current macro capability is great, but I do agree it would be even nicer if we could just insert parameters into the test function directly as @unional described. I've seen several examples of something similar done for Mocha to simplify its dynamic tests:\nhttps://github.com/mikejsdev/mocha-param\nAnd for QUnit:\nhttps://github.com/AStepaniuk/qunit-parameterize\nAnd another more generic library for multiple test frameworks:\nhttps://github.com/lawrencec/Unroll/blob/master/examples/ava/ava-example.js\nAnd one going in a different direction to emulate TestNG decorators:\nhttps://github.com/alsatian-test/alsatian\nIt's definitely a common use case for easier test design. As you said, you've provided other ways to accomplish this so maybe we don't strictly need it, but it would be a very nice to have.\nIn any event, thank you again for all the great work done on Ava.. True. I listed examples across several frameworks to show that it's very much a common need.\nAs you noted, it's such a common need that there exists, to some extent, modules for many of the popular JS frameworks to accomplish such a thing, but it means yet another dependency to rely on and third party implementations might never be as clean or pleasant to use as Ava supporting it directly.\nComing from other languages (Java/C# specifically) I've found it strange that many JS frameworks don't nicely support this kind of testing out of the box. It would be another stand out feature of Ava, in my opinion.. ",
    "nickjanssen": "@novemberborn thanks, that fixes it! Could you elaborate on what exactly the shell is doing here? I don't understand that.\nRegarding selecting the test files by default, all of my files live in src/ so I'd like ava to look in just that folder. \nEDIT: I see what you mean. Simply passing src/ did the trick. Thanks again!. Got it...lesson learned! Thanks again.. Got it...lesson learned! Thanks again.. ",
    "liubinyi": "missed branch coverage. closing this one.. missed branch coverage. closing this one.. ",
    "axetroy": "I have found it in documentation, I'm curious about why don't work.\nsync work not depend on fail method.\nanyway, Thanks.. I have found it in documentation, I'm curious about why don't work.\nsync work not depend on fail method.\nanyway, Thanks.. ",
    "wmnnd": "I tried the same with fake-fs and unfortunately it also results in the looping error for me.\nThere seems to be no way to test modules that do disk i/o with ava, or did I overlook something?. ",
    "klaussinani": "No worries at all :) \nLove the project \ud83c\udf89 . ",
    "renatorib": "@novemberborn \n\nYou've disabled AVA's default transpilation of test files.\n\nWhere?\nMy custom babel config (next/babel) support import/export and latest es versions.\nAnd when I remove this config:\n\"ava\": {\n  \"babel\": \"inherit\"\n}\nThis error persists.\n\nWhy closed issue, since this not resolved?. ",
    "penge": "Put those to package.json:\n```\n{\n  \"ava\":  {\n    \"require\": [\"babel-register\"]\n  },\n\"babel\": {\n    \"presets\": [\"@ava/stage-4\"]\n  }\n}\n```\nSource: transpiling-sources@AVA\n@avajs, I would suggest making init more clever so those issues are gone.\nTo be \"No extra setup needed.\" :)\n. ",
    "sanderploegsma": "Right, that makes sense, can't see how I missed that. Thanks!. ",
    "neoeno": "Of course @novemberborn \u2014 sorry I didn't do this already!\nHere's a repo with reproduction, output both before and after this PR: https://github.com/neoeno/ava-pr-1313-example\nFor posterity, here's the code:\n```js\nimport test from 'ava';\nconst fn = () => () => doesnotexist;\ntest(\"some test\", t => {\n  fn()();\n});\n```\nHere's the current output (as of 3da8603):\n```\n$ ava test.js\n1 failed\nsome test\n  /Users/kay/Code/libs/tester/test.js:6\n5: test(\"some test\", t => {\n   6:   fn()();\n   7: });\nError:\n[ReferenceError: doesnotexist is not defined]\n\nError thrown in test\n```\nAnd here's the output after this PR:\n```\n$ ava test.js\n1 failed\nsome test\n  /Users/kay/Code/libs/tester/test.js:3\n2:\n   3: const fn = () => () => doesnotexist;\n   4:\nError:\n[ReferenceError: doesnotexist is not defined]\n\nError thrown in test\ntest.js:3:24\n  Test.t [as fn] (test.js:6:3)\n``. I'll fix that formatting error too \ud83d\ude28  . I'll give it a go \u2014 though afaict it won't be as simple as replacingextract-stack.js` with the new module (which is great by the way!).\nThis is because at the moment extractStack accepts a pre-processed stack-trace, of of this form:\nerror message\n  Test.t (test.js:1:1)\nNo ats \u2014 which from reading the code I think the extract-stack module relies upon?\nHere's where my knowledge gets a bit hazy, but it looks like it passes through clean-stack elsewhere \u2014 removing the ats as well as some other stuff \u2014 via paths like this: main.js:37 -> serialize-error.js:52 -> beautify-stack:27 -> clean-stack. There may be one or two more...\nSo the solution would seem to be to do all this stacky work in one place and delegate to extract-stack its due share \u2014 but I've no idea how much work that will be. Might be simple anyway, so I'll give it a shot and get back to you.. Though this is still on my radar I suspect I won't get to it imminently. As such, if anyone is keen to take it on I should certainly give way!. As you prefer!. ",
    "bchapman": "Hit the same issue.  I'm using an async wrapper function inside of a test, and any code inside loses it's context in the traceback.  This would be super helpful to get the error context back.. ",
    "patrickrand": "+1. ",
    "langri-sha": "I'm not that intimate with Webpack, but it makes sense the way you broke it down \ud83d\udc4d (use ava-loader and AvaPlugin). I understand that the ultimate goal would be for Webpack users to be able to seamlessly integrate this into their configuration and receive test results for each compilation?\nLooking at extract-text-webpack-plugin and html-webpack-plugin, it's possible to dynamically create entry chunks. I think what one could do is optimize at the end, extracting all AVA modules from chunks, putting them into a distinct chunk and then run the tests. This would probably let a Webpack user with multiple entry points to run a development server and tests (without having to write an entry point that collects tests from all the other entry points, so they're at the liberty to organize their tests without worrying on how they're going to be discovered and bundled for AVA). . Here's the quickest way to reproduce:\n```\nChange into package directory\ncd ~package/\nUse Yarn, get lock file generated\nyarn install\nGreat! Let's add something\ntouch node_modules/princess.js\nForce lock file change\nyarn install --force\nThe princess has left the castle :|\n```. @novemberborn I don't think you'll have much luck searching their issues, because this with storing something that's not a package alongside other packages has been invented here and I'm not certain why they should be the ones adding support for quirks. \nI'll try my luck at find-cache-dir.. ",
    "Sawtaytoes": "Dynamic Loading of Modules\nUsing Webpack's context, you can dynamically load in modules with the name **/*.test.js on the fly:\nhttps://github.com/Sawtaytoes/Ghadyani-Framework-Webpack-React-Redux/blob/master/src/tests.js#L10\nAVA for Webpack\nI started using AVA on some Node.js projects, and am wanting to switch to AVA from Tape in my React projects. I haven't yet found a way to run it on my Webpack-built code. Since I run Webpack through the Node.js API methods, I can't pipe a config through the CLI to AVA like the Webpack Recipe; I'd have to run two completely separate CLI instances and would lose out on browser debugging.\nThe benefit of running tests in the browser with Webpack is I get easy debugging exactly as if I was running the React app itself.\nIs this still an option that might come in the future?. Is it my understanding this is in now?\nhttps://github.com/avajs/ava/blob/master/docs/recipes/precompiling-with-webpack.md. ",
    "ackerdev": "Results appear to be very similar as long as concurrency is >= the number of logical cores on the machine. And unfortunately no I cannot share the test suite; if it helps any, they're all unit tests running as test.serial, and it's a suite for an HTTP API server. No tests go through the web framework, and there are a few connections to external resources like SQL, Redis, that may be leaking in the tests currently, though they are not testing interactions with those resources directly yet.\n```\n$ time ava -c 8\n110 passed\nava -c 8  85.90s user 7.11s system 644% cpu 14.432 total\n```\n```\n$ time ava -c 36\n110 passed\nava -c 36  91.18s user 7.27s system 628% cpu 15.658 total\n```\nMy thought regarding Bluebird wasn't so much that Bluebird#map was broken or directly responsible, but rather perhaps when concurrency limiting is enabled, event-loop scheduling might be different such that spawning child processes doesn't result in a lockup. Just a guess though, didn't really dig very far down into the depths.. The improvement you're seeing with concurrency is documented here: #1318 \nIf you have any potential information that might help debug that it would be greatly appreciated :). ",
    "pixelass": "having problems here too. Ava has not been doing anything for about 10m.\nI have the same problem with https://github.com/sindresorhus/xo/ \nBoth packages use https://github.com/sindresorhus/globby and I am using globs in both cases.\nfoo/bar/baz/**/spec/*.js, foo/bar/baz/**/*.js\nThe project is HUGE but the folder I am targeting is actually very small and only has one test and about 10 js files.\nI have never encountered the problem but have never used it in a project with so many folders.. Maybe related to several other issues? \nI hope the minimal project I provided helps.\n\nrelated to #1329 \nrelated to #1288 \nrelated to #1228 \nrelated to https://github.com/sindresorhus/xo/issues/212. I added a branch which uses stricter paths and should ignore the other files.\n\nhttps://github.com/pixelass/ava-xo-test/tree/direct-path\nConfig:\nhttps://github.com/pixelass/ava-xo-test/blob/direct-path/package.json#L43-L61\njson\n{\n  \"files\": [\n      \"./one/a/b/c/d/e/f/g/h/i/j/this-is-it/**/tests/**/*.js\"\n    ],\n    \"source\": [\n      \"./one/a/b/c/d/e/f/g/h/i/j/this-is-it/**/*.js\"\n    ],\n}. related to https://github.com/sindresorhus/xo/issues/234. > That would be a sad day, since the principles of this project are spot on IMO\nI love Ava. especially in 'normal' projects. \nIt just works (mostly \ud83e\udd23)\nWe ended up removing Ava from the \"big project\" that had the issue. Most developers in that project are not used to writing tests anyways, so I'm the only one frowning ;(. \nI added Ava to the project because it is so easy to write test and understand the possibilites. \nThere is basically no need to learn an entire suite to write simple tests. More complex things usually require 2 minutes of looking at the docs. Ava is and remains a great API. \nI'm happy to contribute. @novemberborn can you point me to a few or did you mean \"search for\" and then \"summarize\" \ud83d\ude07 ?\n. Performance issues:\n594\n483\n789\n939\n1329\n1318\n1288\n1095\nThese are issues either tagged with the label performance.\nAlso includes isses found via searching:  \"slow\", \"speed\" \nThat's a lot of stuff to read before being able to summarize or sort out actual issues/suggestions and/or examples.\nHow should we continue with this? My attempt would be to evaluate the listed issues and write a new issue that lists the different performance issues.\nI'd extract the gist of the problem (description/example/potential reason) and link the originals.\nThis way those other performance issues could be closed/marked when you're happy with the summary. But I'm actually not sure if thats what you meant.\nSadly I don't have a lot of time myself. The weekend is over so I won't be able to start for another week. I'm also really bad at reading long texts so this may take a while \ud83d\ude1b.\nI skimmed a few of the issues and realized this might be quite a chunk of work.\nAny suggestions on the format are welcome. I'd really hate to spend time on something you can't use.\n. ",
    "tniessen": "\nThis would still run each test in parallel, right?\n\nJust FYI, the vm module has nothing to do with concurrency, and it is not comparable to executing tests in separate processes. There is neither strict isolation nor will any code be executed in parallel, apart from the usual event loop.. ",
    "Magellol": "I'm actually in the need of having different set of configurations.\nI'm using ava to test react components but also code living in the server (proxying requests and handling server side rendering for instance). Although, some parts of my server don't need to be transpiled by babel (node js compliant code) but I couldn't find a way to specify a per directory config set, so ava runs all tests under the same conditions.\njson\n  \"ava\": {\n    \"babel\": \"inherit\",\n    \"require\": [\n      \"babel-register\",\n      \"./test/_init.js\"\n    ]\n  },\nSo I believe this idea would solve these kind of problems.. @novemberborn Oh interesting. Will check it out, thanks!. ",
    "joflashstudios": "This change broke our test helpers - we were using t.notThrows to declare mocked functions that must be called. Now, instead of getting a specific error message, we get a generic \"Test finished, but an assertion is still pending\" instead of the error message passed into t.notThrows(promise, ), or the rejection itself.. @novemberborn The problem is that the t.notThrows() calls need to be non-blocking, because they happen before the promise gets resolved.. ",
    "thisisrai": "@jamestalmage super interested in helping out on this issue, but I'm not sure on where to start. . @novemberborn I haven't had a chance to look it at yet. Been super busy lately. \n@servicelevel Feel free to go for it. . ",
    "undefinedTea": "is this being worked on, or available still?. i'll give @thisisrai a few hours to respond here, and then i'll get it going if its not already in progress/near completion.. alright. on it...\nthe approach @novemberborn suggested above seems reasonable, so i'll go with that.\ni suppose notify() when given the {isGlobal: false} option will still provide the desired prompt, only without the --global flag, so this should be fine.\ni am wondering if there should be an indication as to whether the global or local version of the cli was used, but that may be another discussion all together.. damn - same faith as @thisisrai :/ just 0 time.\ni did 'complete' the actual code, but never managed to get around to test it properly or write tests.\nso please do take it @tdeschryver - and if you want, i can commit my code later (missing tests as mentioned) but its a rather well outlined approach by @novemberborn above, so not sure that would help much.. ",
    "timdeschryver": "@servicelevel Are you working on this issue? \nIf so, do you need some help or can I take this one?. @novemberborn I'm having a small problem with the testing. At test/cli.js you can see I'm deleting the process.env.AVA_LOCAL_CLI property because otherwise its being used in the next tests (if its been set, we just leave it as it is - cli.js) which will then fail. Is there a better way of doing this, this seems a bit dirty to me? I've tried tap.beforeEach and tap.afterEach but without success.... I've tried to solve this issue, but I've hit a point where I need some validation/input from someone with experience.\n- Because we need the value of the promise, is it OK to replace the noop method at lib/assert.js#L177 and use the resolved value to call fail?\n- Is it correct that fail(this, new AssertionError()) is being called or is throw new AssertionError() a better solution?\n- I've tested the solution with the snippet from this issue and the message as you can see below is being shown (I think this is the expected behavior?), but the new test at test/assert.js#L651 is failing because lastFailure is null (because its a promise?). What would a good way to fix this? I'm thinking about creating a failsWith method for promises, but there must be a better way...\nExpected promise to be rejected, but it was resolved instead\nResolved with:\n\"foo\"\n- Is it OK if I'm continuing with the issue?\n. @novemberborn I was trying to figure out how I could test the isGlobal value and to be honest I'm kinda stuck. The only way to test it would be to call lib/cli directly  and stub almost everything (update-notifier, pkg-conf, api,  ...), and put a test inside the .notify() function?  . @novemberborn I don't think that will work (or I am missing something), the problem is we are stubbing resolveCwd with /fixture/empty, thus the lib/cli is never being called. I tried creating a new test where lib/cli is being called, the problem here is that it will test some files and to ignore this we'll have to stub a lot of things I think. \nEDIT: just tried this and the notify function isn't being called as far as I can see.. @novemberborn I discarded my changes, would it help you if I rewrite it?. @sindresorhus is it OK if I'm going to take a look at it?. Bumped update-notifier since yeoman/update-notifier#114 is merged.\n. @novemberborn seems fair to me. I'll remove the 0 check.. @novemberborn would a check on a negative input be useful here? :thinking: . I had to remove the catch because otherwise I couldn't access the AssertionError in the test.\n. Yep, you're right. My bad!. Oh I didn't know that. When I was debugging it, it was a automatically converted to a string.\nIts changed now.. Thanks for the tip and reviews!\nI got it to work now :), but I had to pass the lastFailure to the failsWith function otherwise it would be assigned to null. Is this OK?. That's indeed a cleaner solution.\nChanges are commited!. Oh sorry, I assumed the default had to change, because you wouldn't be able to set it to 0.. Yep! Didn't thought about this case.. No problem! I should be the one saying sorry.... ",
    "timothyjellison": "Would be happy to take this one on if nobody else is.. ",
    "medikoo": "@novemberborn so which features still need to be transpiled when relying on Node 8 ? I thought moving to Node 8 removes the need for babel completely :). > JS is constantly evolving and there will always be features not even available in the latest engines.\n@sindresorhus wouldn't it be better to just use what's available natively, without compilation overhead (?) There's actually quite big cost behind that. Is it worth it?  Isn't what's natively available good enough?. ",
    "cdaringe": "hi all.  love the project--you're doing great work.\ni'd really love to dodge the transpile step for at least my source s.t. debugging is less roundabout:\n\ni can edit my files in the same window during the test (vs closing the transpiled src and nav'ing to my old file)\n...and i can use built in break points!\n\ni think this could be as simple as getting some config to babel to ignore my stuff.  wow-wee that'd be incredible!  is that something you're open to?. good news! its seem as though the system doesn't transpile code if you use pure runtime goodies! woo hoo!  now, i have a native debug experience in VSCode\n\n. @novemberborn, you're an \ud83d\udc7c . can't wait to test this out!. Hmm...\n\ni installed the latest commit directly from github,\nran yarn (observed the request hit GH, though the gif below is too fast moving :))\nconfigured ava\nand ran the test\n\njson5\n// package.json\n  \"ava\": {\n    \"options\": {\n      \"compileEnhancements\": false\n    }\n  } \nsure enough, breakpoints still jump around, leading me to think something is still being transpiled (or built, run, and sourcemapped at least)\n\n. hey @novemberborn, whoops, sorry for that!\neven with\njs\n\"ava\": {\n  \"babel\": false,\n  \"compileEnhancements\": false\n}\ni'm still seeing the jumpiness.  is the file still being meddled with or source mapped somehow?. hey @novemberborn, whoops, sorry for that!\neven with\njs\n\"ava\": {\n  \"babel\": false,\n  \"compileEnhancements\": false\n}\ni'm still seeing the jumpiness.  is the file still being meddled with or source mapped somehow?. hi @novemberborn, thx for the continued support.  indeed, i was using nyc.  after removing nyc, however, the issue persists.  sorry i was late to see this message.  im out for a few days for surgery but will be back soon.  i'll try and reach out to you on gitter then.. hi @novemberborn, thx for the continued support.  indeed, i was using nyc.  after removing nyc, however, the issue persists.  sorry i was late to see this message.  im out for a few days for surgery but will be back soon.  i'll try and reach out to you on gitter then.. OR, here's a teeny tiny reproduction: https://github.com/cdaringe/vscode-ava-no-transpile-party-time\nit's wired up per instruction.  clone, npm i/yarn, then readme steps!. OR, here's a teeny tiny reproduction: https://github.com/cdaringe/vscode-ava-no-transpile-party-time\nit's wired up per instruction.  clone, npm i/yarn, then readme steps!. done, rebased commit. ",
    "alexrussell": "This is good news. On the less-good-news side, I have started a PR for this (as I figured it was kinda simple and worth having a go at) and that's when problems started.\n\nThe tests aren't super simple to work out what's going on due to the use of the failsWith macro, etc. But I kinda worked it out where I could.\nThe tests themselves don't contain much \"everything testing\", at least in the is/not section where I looked. They simply test that t.is('foo', 'foo') and assumes it's all good for the passing case. However, it'd make more sense to include 'all possible' passing cases, such as true, true, null, null, 0, 0, -0, -0, etc. in order to create a good document for the design of is, as well as ensure what is actually expected by the maintainers continues to be true forever. So I added a few of these in, but got a little stuck on things like [], [] and [1, 2], [2, 1] as the formatting is a bit complex, and not really testing what we want - first off, power assert is a bit useless here as the diff between [] and [] is '' (i.e. an empty string), and while I can write a test for this, it's only testing current behaviour rather than some realistic future expectation (that is says that the two arrays are not the same instance and that's why they're not equal despite looking the same, for example).\nI changed is to use Object.is but then I had to change not to use Object.is. No major problem there. But then I see that is and not both use an operator key which is currently '==='. Should I change it to Object.is, Object.is() or just remove it entirely like for deepEqual?\nPresumably the docs should be updated to say that is uses Object.is and not ===. Here's where another issue arises - the macro examples (see lines 635/636 of readme.md) use is, and they assert (in English, not code) that 2 + 2 === 4 because we use is, but now that's not strictly true (it's now Object.is(2 + 2, 4) which is slightly less catchy in the example). If we any examples that use is like this, it could make these parts somewhat more confusing. That said, that section is the only real issue here.\n\nSo, with that in mind, I figured I'd come back here in case the maintainers decide that actually that's too much of a change for the sake of supporting NaN and actually somewhat complicates things when is is used as an example.\nIf we're good to go still, I'm happy to submit the PR and point out the various bits where I made a call that the maintainers may wish to revert.. FWIW my changes (no PR yet) are here: https://github.com/avajs/ava/compare/master...alexrussell:is-uses-object-is?expand=1. @novemberborn Good responses :) I'll have a think if there are a few more test assertions I can add with my limited knowledge of the testing helpers (and I wasn't complaining here, just pointing out that I have added a bunch of assertions and explaining the reasoning) and make the changes to the macro examples.. Okay I have updated my branch to use = in the examples and added a few more test assertions for t.is() I figured it wasn't a big deal to do the same for t.not() as it is literally a negation of t.is(). Arguably the tests should be there to ensure that t.not() doesn't stray from being equivalent to ! t.is() so I can do that if required.. @novemberborn the latest commit addresses all three of these. I have gone for \"the same as\" rather than \"the same value as\" (you used both in your two comments on this) so let me know if you'd prefer \"the same value as\".\nAlso, the 0/-0 thing was my bad - Just adding the - into the test expectation worked, so it appears that the value formatter does indeed include the negative sign. Phew!\nI think when I originally did this work on the tests I was really unclear as to how the AVA test helpers worked with all the formatted values and regexes, etc., so I went for a conservative 0 and simply forgot to go back and see whether it worked as I'd expect.. Woohoo thanks for merging \ud83c\udf89 . My bad. Updated.. ",
    "zs-zs": "I've just ran into the same issue while evaluating Flow. It seems the cause of the problem is that beforeEach and afterEach now has the type TestMethod which can only have a Test body instead of a ContextualTest. Changing its type to ContextualTest fixed the issue - at least for me.\nBut it's strange that there is one automated test which is actually asserting that context should not be there: https://github.com/avajs/ava/blob/master/test/flow-types/regression-1114.js.flow#L29\nI checked the issue #1114 which doesn't seem to be relevant to the fact that beforeEach and afterEach has a context...? So I'm not sure if the failing test is the sign of something wrong or just the unit test is what needs to be revisited.. Okay, I fixed the test by removing it :) Also did some cleanup. Let's see how it works.\nUntil the fix is released, a possible workaround is casting to any which seems to solve the problem:\n```\ntest.beforeEach((t:any) => {\n    t.context = new TestContext(); // no errors\n});\ntest.afterEach.always((t:any) => {\n       ...\n});\n```. I have no idea why it failed for node 4.... Hm I don't know. I only removed \"dead\" types, so I hope it won't break anything.. I thought about keeping the beforeEach, but then I thought it has nothing to with the issue #1114. But maybe it has, I didn't dig deeper.. BTW, how to trigger a rebuild? I didn't find that, never used Appveyor before.. ",
    "cevek": "ava version: 0.18.2\nnode: 7.4.0\nSo, yeah, that's example doesn't crash, cause output it is not very much :)\njs\ntest(t => {\n  const document = require('jsdom').jsdom('', {});\n  const node = document.createElement('div');\n  // because every dom linked with each other\n  for (var i = 0; i<1000; i++) node.appendChild(document.createElement('div'));\n  t.is({node, a:1},{a: 1})\n}). ",
    "exogen": "\nThere have been security releases since 4.5 even so ideally nobody is using such old versions in production.\n\nI agree. Unfortunately pre-4.5 seems to be the default on CircleCI, which is how I discovered this (all my projects using AVA were failing). I added a circle.yml to force a newer version. AVA can't fix that directly, but debugging would have been quicker if npm had warned about engines (it doesn't warn about hullabaloo-config-manager, but would warn about ava since it's a direct dep). I can open a PR to update the version range this evening. :). > Do you think AVA should flat out refuse to start if it detects an outdated Node.js version?\nI'd probably give people's environment the benefit of the doubt (like maybe they patched it up/polyfilled it to work somehow?), but print a warning when AVA starts \u2013 this would be more immediately visible than npm's warning (the user may have installed a long time ago before running ava in non-CI cases).. ",
    "southpolesteve": "@novemberborn Just came across this bug too. Wanted to chime in and say that AWS Lambda is still uses 4.3.2 as their \"Node 4\" version. We're working on getting stuff over to node 6 but it was only released recently. I would imagine plenty of Lambda users still using 4.3.2 in production including ourselves.. Yeah I think you are probably right. Just sucks that this happened at all. I've also sent a note to AWS telling them to upgrade their node 4 version :). ",
    "reconbot": "Node 4 is lts you know? I think https://github.com/feross/safe-buffer would do what's needed here.. I think it's a worthwhile game as long as node4 is in LTS (until 2018-04-01) there are a lot of people still on node 4.3.2 because of lambda (and probably older because of Linux distros) and it's worth trying to support them if we can. I've submitted a patch to hullabaloo-config-manager https://github.com/novemberborn/hullabaloo-config-manager/pull/15 \nLTS isn't forever. Lambda is a major holdout, I did accidentally get the program manger in touch with someone from Node who works on LTS, but I doubt we'll see much movement there for a while. I found out that only 4.2+ is considered LTS which still lower that 4.5. Since you do control it and it's the only package blocking <4.5 do you mind holding out a little longer?. ",
    "pendar747": "I seem to get a similar error whenever I change fs.readFileSync!\n```\n   Uncaught Exception\n  TypeError: Cannot read property 'sections' of undefined\n    new SourceMapConsumer (node_modules/source-map/lib/source-map-consumer.js:20:19)\n    mapSourcePosition (node_modules/source-map-support/source-map-support.js:175:14)\n    wrapCallSite (node_modules/source-map-support/source-map-support.js:343:20)\n    node_modules/source-map-support/source-map-support.js:378:26\n    Function.prepareStackTrace (node_modules/source-map-support/source-map-support.js:377:24)\n\u00d7 Test results were not received from dist\\test\\current\\asset.spec.js\n```. ",
    "Brzda": "We have cca 30 files with tests. I tried run ava with --serial and --concurrency option, but nothing change.. Oh, very thanks. It helped.. ",
    "uncleGena": "I've tried. my npm is latest - v4.5.0 and nodejs is latest to v.7.9.0 \nAlso I've cleared cache and reinstalled node_modules and use command npm rebuild node-sass. \nThat did not help.. ",
    "sdd": "Is this not effectively the same as the following example in the t.throws() docs here?\n```javascript\nconst promise = Promise.reject(new TypeError('\ud83e\udd84'));\ntest('rejects', async t => {\n    const error = await t.throws(promise);\n    t.is(error.message, '\ud83e\udd84');\n});\n```. ",
    "justinhelmer": "A valid use-case for this design pattern in case you are on the fence:\n```js\nconst someTruthy = true;\nconst someAsyncThing = await () => {\n  if (someTruthy) {\n    throw new TypeError('\ud83e\udd84');\n  }\nawait asyncStuff();\n  await moreAsyncStuff();\n}\n// currently fails with unhandled rejection\ntest('rejects', async t => {\n  await t.throws(someAsyncThing, '\ud83e\udd84');\n});\n```\nIMO the above design pattern is much nicer than:\njs\n// passes\ntest('rejects', async t => {\n  await someAsyncThing()\n    .catch(err => {\n      t.is(err.message, '\ud83e\udd84');\n    });\n});\nAlso, it took me a while to discover this is actually functioning as designed, since it seems very similar to the use-case documented here: https://github.com/avajs/ava/#throwsfunctionpromise-error-message. ",
    "hildjj": "Agree that this would be a lovely feature.  As a note to my future self (since I keep making this mistake), a work-around is to use an IIFE to turn the promise-returning function into a promise:\ntest(async t => {\n    await t.throws((async () => {\n        throw new Error();\n    })());\n});\n. ",
    "parro-it": "As a simple workaround, I remove notThrows altogether:\njs\ntest(`Convert accelerator`, t => {\n    toKeyEvent(accelerator);\n    t.is('silly', 'silly');\n}));\nI still not got the stack trace, but at least I can see the point in my code that throwed:\n\nThe \"silly\" part is to avoid Test finished without running any assertions check.\n. ",
    "mo": "The global timeout documentation makes it sound like the timer starts when the actual testcase starts, and also that the timer is reset whenever a new testcase begins (I think this sounds very sensible):\nhttps://github.com/avajs/ava#global-timeout\nIn general, I think it's important that it's possible to:\n detect bugs where an expected callback is never invoked\n detect bugs where an expected callback is invoked two or more times\n* detect bugs where a synchronous function accidentally ends up in an infinite loop\nUsing Mocha I was able to cover all three use cases but I ended up adding an explicit timeout for each TC which has both pros and cons.\nit('is working', function (done) {\n  this.timeout(10000);\n\n  util.asyncCallbackTrimLowThatAccidentallyCallsCallbackTwiceWhenBuggy('   Foo   ', actual => {\n    assert.equal(actual, 'foo');\n    done();\n  });\n});\n\nI also implemented similar testcases in tape and qunit. Mocha and tape both make it look a bit like the testcase passed when the callback is invoked the first time. When the the extra, unexpected, call comes in it correctly fails the TC though, like this:\nhttp://temp.minimum.se/screenshot_20170507_162344.png\nThe corresponding fail output from QUnit was less ambiguous:\nhttp://temp.minimum.se/screenshot_20170507_162557.png\nTo catch all cases with ava I had to resort to a quite ugly hack:\n```js\ntest.cb('util test async callback function that forgets to callback', t => {\n  t.plan(1);\n  util.asyncCallbackTrimLowThatForgetsToCallbackWhenBuggy('   Foo   ', actual => {\n    t.is(actual, 'foo');\n  });\n  // Ugly hack to make sure \"missing / double callback\" bugs are detected\n  setTimeout(t.end, 500);\n});\ntest.cb('util test async callback function that accidentally calls callback twice', t => {\n  t.plan(1);\n  util.asyncCallbackTrimLowThatAccidentallyCallsCallbackTwiceWhenBuggy('   Foo   ', actual => {\n    t.is(actual, 'foo');\n  });\n  // Ugly hack to make sure \"missing / double callback\" bugs are detected\n  setTimeout(t.end, 500);\n});\n```\nIt would be really sweet if there was a way to catch all three bug cases without resorting to the ugly hack.. Yay, thanks for fixing!. ",
    "tp": "Just saw that a similar concern was already raised in #1341.\nGlad to see you are aware!\nNot sure though if this will definitely be fixed by the aforementioned ticket, as it was just mentioned as the trigger, not necessarily the solution.\nIf it is included for sure, feel free to close this one.. ",
    "mmmveggies": "For those who end up here looking for a fix, you can delete the corresponding entry in the __snapshots__/ folder. You lose your snapshot history but it will fix the bug. ",
    "sudo-suhas": "@novemberborn I could be wrong, but my understanding is, it is precompiling source files. Only indirectly. So in my case, I had many independent test files. And had to use babel-register for running the tests in node 4. This proves to be very resource intensive as you might notice from the test durations. So the suggested approach is to precompile test files. Which would in turn precompile the source files because they are being referenced. One more thing I would like to add here is that if we were to indeed directly precompile src files, we would need to reference _build in test files. I think that is less elegant.. @novemberborn If I wanted to output multiple files, how would I do it? Take the basename for each file? Also, if it performs well, do we care if the tests run in the same process?. The following webpack config outputs a file for each entry:\n```js\n'use strict';\nconst path = require('path');\nconst glob = require('glob');\nconst nodeExternals = require('webpack-node-externals');\nconst testFiles = glob.sync('./test/*/.js');\nconst entryObj = {};\nfor (let idx = 0; idx < testFiles.length; idx++) {\n    const file = testFiles[idx];\n    entryObj[path.basename(file, path.extname(file))] = file;\n}\nmodule.exports = {\n    target: 'node',\n    entry: entryObj,\n    output: {\n        path: path.resolve(__dirname, '_build'),\n        filename: '[name].js'\n    },\n    externals: [nodeExternals()],\n    module: {\n        rules: [\n            {\n                test: /.(js|jsx)$/,\n                use: {\n                    loader: 'babel-loader',\n                    // Adding this saved ~9 seconds for me\n                    options: {\n                        cacheDirectory: true\n                    }\n                }\n            }\n        ]\n    }\n};\n```\nBut this negates any performance benefit because each entry has its own copy of the src. CommonsChunkPlugin cannot be used because it is only meant to be used with the browser and not when the target is node.\nIf I add the commons plugin:\njs\n    plugins: [\n        new webpack.optimize.CommonsChunkPlugin({\n            name: 'commons',\n            filename: 'commons.js',\n            minChunks: 3\n        })\n    ]\nEvery test file fails:\n```\n\ncross-env NODE_ENV=test nyc --cache ava _build --concurrency 3\n\n125 exceptions\n\u00d7 No tests found in _build\\aggregation.test.js, make sure to import \"ava\" at the top of your test file\n\u00d7 No tests found in _build\\avg-agg.test.js, make sure to import \"ava\" at the top of your test file\n\u00d7 No tests found in _build\\avg-bucket-agg.test.js, make sure to import \"ava\" at the top of your test file\n```\nSpecifying entry for src and using that in the plugin doesn't help either.. Not entirely sure which part of the setup is responsible for this but there is a huge difference in execution time between 1st(no cache) and subsequent runs.\nHere's the test run timings:\n|test|time|\n|----|----|\n|single entry, 1st run|0m44.255s|\n|single entry, next run|0m23.667s|\n|multiple entry, 1st run|17m32.462s|\n|multiple entry, next run|1m59.068s|\n\nWebpack output(single entry)\n\n```\nHash: 8898e462dc944831f19e\nVersion: webpack 2.5.1\nTime: 12228ms\n   Asset     Size  Chunks                    Chunk Names\ntests.js  3.63 MB       0  [emitted]  [big]  main\n   [5] ./test/_macros.js 21 kB {0} [built]\n [146] ./test/queries-test/match-phrase-prefix-query.test.js 418 bytes {0} [built]\n [170] ./test/queries-test/span-multi-term-query.test.js 698 bytes {0} [built]\n [171] ./test/queries-test/span-near-query.test.js 783 bytes {0} [built]\n [172] ./test/queries-test/span-not-query.test.js 680 bytes {0} [built]\n [173] ./test/queries-test/span-or-query.test.js 685 bytes {0} [built]\n [174] ./test/queries-test/span-term-query.test.js 780 bytes {0} [built]\n [175] ./test/queries-test/span-within-query.test.js 245 bytes {0} [built]\n [176] ./test/queries-test/term-query.test.js 693 bytes {0} [built]\n [177] ./test/queries-test/terms-query.test.js 1.97 kB {0} [built]\n [178] ./test/queries-test/type-query.test.js 419 bytes {0} [built]\n [179] ./test/queries-test/weight-score-func.test.js 346 bytes {0} [built]\n [180] ./test/queries-test/wildcard-query.test.js 664 bytes {0} [built]\n [181] ./test/recipes.test.js 8.62 kB {0} [built]\n [292] multi ./test/_macros.js ./test/aggregations-test/avg-agg.test.js ./test/aggregations-test/avg-bucket-agg.test.js ./test/aggregations-test/bucket-agg-base.test.js ./test/aggregations-test/bucket-script-agg.test.js ./test/aggregations-test/bucket-selector-agg.test.js ./test/aggregations-test/cardinality-agg.test.js ./test/aggregations-test/children-agg.test.js ./test/aggregations-test/cumulative-sum-agg.test.js ./test/aggregations-test/date-histogram-agg.test.js ./test/aggregations-test/date-range-agg.test.js ./test/aggregations-test/derivative-agg.test.js ./test/aggregations-test/diversified-sampler-agg.test.js ./test/aggregations-test/extended-stats-agg.test.js ./test/aggregations-test/extended-stats-bucket-agg.test.js ./test/aggregations-test/filter-agg.test.js ./test/aggregations-test/filters-agg.test.js ./test/aggregations-test/geo-bounds-agg.test.js ./test/aggregations-test/geo-centroid-agg.test.js ./test/aggregations-test/geo-distance-agg.test.js ./test/aggregations-test/geo-hash-grid-agg.test.js./test/aggregations-test/global-agg.test.js ./test/aggregations-test/histogram-agg-base.test.js ./test/aggregations-test/histogram-agg.test.js ./test/aggregations-test/ip-range-agg.test.js ./test/aggregations-test/matrix-stats-agg.test.js ./test/aggregations-test/max-agg.test.js ./test/aggregations-test/max-bucket-agg.test.js ./test/aggregations-test/metrics-agg-base.test.js ./test/aggregations-test/min-agg.test.js ./test/aggregations-test/min-bucket-agg.test.js ./test/aggregations-test/missing-agg.test.js./test/aggregations-test/moving-average-agg.test.js ./test/aggregations-test/nested-agg.test.js ./test/aggregations-test/percentile-ranks-agg.test.js ./test/aggregations-test/percentiles-agg.test.js ./test/aggregations-test/percentiles-bucket-agg.test.js ./test/aggregations-test/pipeline-agg-base.test.js ./test/aggregations-test/range-agg-base.test.js ./test/aggregations-test/range-agg.test.js ./test/aggregations-test/reverse-nested-agg.test.js ./test/aggregations-test/sampler-agg.test.js ./test/aggregations-test/scripted-metric-agg.test.js ./test/aggregations-test/serial-differencing-agg.test.js ./test/aggregations-test/significant-terms-agg.test.js ./test/aggregations-test/stats-agg.test.js ./test/aggregations-test/stats-bucket-agg.test.js ./test/aggregations-test/sum-agg.test.js ./test/aggregations-test/sum-bucket-agg.test.js ./test/aggregations-test/terms-agg-base.test.js ./test/aggregations-test/terms-agg.test.js ./test/aggregations-test/top-hits-agg.test.js ./test/aggregations-test/value-count-agg.test.js ./test/core-test/aggregation.test.js ./test/core-test/geo-point.test.js ./test/core-test/geo-shape.test.js ./test/core-test/highlight.test.js ./test/core-test/indexed-shape.test.js ./test/core-test/inner-hits.test.js ./test/core-test/query.test.js ./test/core-test/request-body-search.test.js ./test/core-test/rescore.test.js ./test/core-test/script.test.js ./test/core-test/sort.test.js ./test/core-test/util.test.js ./test/index.test.js ./test/queries-test/bool-query.test.js ./test/queries-test/boosting-query.test.js ./test/queries-test/common-terms-query.test.js ./test/queries-test/constant-score-query.test.js ./test/queries-test/decay-score-func.test.js ./test/queries-test/dis-max-query.test.js ./test/queries-test/exists-query.test.js ./test/queries-test/field-value-factor-func.test.js ./test/queries-test/full-text-query-base.test.js ./test/queries-test/function-score-query.test.js ./test/queries-test/fuzzy-query.test.js ./test/queries-test/geo-bounding-box-query.test.js ./test/queries-test/geo-distance-query.test.js ./test/queries-test/geo-polygon-query.test.js ./test/queries-test/geo-query-base.test.js ./test/queries-test/geo-shape-query.test.js ./test/queries-test/has-child-query.test.js ./test/queries-test/has-parent-query.test.js ./test/queries-test/ids-query.test.js ./test/queries-test/joining-query-base.test.js ./test/queries-test/match-all-query.test.js ./test/queries-test/match-none-query.test.js ./test/queries-test/match-phrase-prefix-query.test.js ./test/queries-test/match-phrase-query-base.test.js ./test/queries-test/match-phrase-query.test.js ./test/queries-test/match-query.test.js ./test/queries-test/mono-field-query-base.test.js ./test/queries-test/more-like-this-query.test.js ./test/queries-test/multi-match-query.test.js ./test/queries-test/nested-query.test.js ./test/queries-test/parent-id-query.test.js ./test/queries-test/percolate-query.test.js ./test/queries-test/prefix-query.test.js ./test/queries-test/query-string-query-base.test.js ./test/queries-test/query-string-query.test.js ./test/queries-test/random-score-func.test.js ./test/queries-test/range-query.test.js ./test/queries-test/regexp-query.test.js ./test/queries-test/score-func.test.js ./test/queries-test/script-query.test.js ./test/queries-test/script-score-func.test.js ./test/queries-test/simple-query-string-query.test.js ./test/queries-test/span-containing-query.test.js ./test/queries-test/span-field-masking-query.test.js ./test/queries-test/span-first-query.test.js ./test/queries-test/span-little-big-query-base.test.js ./test/queries-test/span-multi-term-query.test.js ./test/queries-test/span-near-query.test.js ./test/queries-test/span-not-query.test.js ./test/queries-test/span-or-query.test.js ./test/queries-test/span-term-query.test.js ./test/queries-test/span-within-query.test.js ./test/queries-test/term-query.test.js ./test/queries-test/terms-query.test.js ./test/queries-test/type-query.test.js ./test/queries-test/weight-score-func.test.js ./test/queries-test/wildcard-query.test.js ./test/recipes.test.js 1.5 kB {0} [built]\n    + 278 hidden modules\n```\n\n\nWebpack output(multiple entries)\n\n```\nHash: 40ba01d7a31287b335bc\nVersion: webpack 2.5.1\nTime: 10753ms\n                             Asset     Size    Chunks                    Chunk Names\n   serial-differencing-agg.test.js  3.29 MB   62, 123  [emitted]  [big]  serial-differencing-agg.test\n        common-terms-query.test.js  3.29 MB    0, 123  [emitted]  [big]  common-terms-query.test\n                 geo-point.test.js  3.29 MB    2, 123  [emitted]  [big]  geo-point.test\n               filters-agg.test.js  3.29 MB    3, 123  [emitted]  [big]  filters-agg.test\n                   recipes.test.js   3.3 MB    4, 123  [emitted]  [big]  recipes.test\n            wildcard-query.test.js  3.29 MB    5, 123  [emitted]  [big]  wildcard-query.test\n               terms-query.test.js  3.29 MB    6, 123  [emitted]  [big]  terms-query.test\n             span-or-query.test.js  3.29 MB    7, 123  [emitted]  [big]  span-or-query.test\n            span-not-query.test.js  3.29 MB    8, 123  [emitted]  [big]  span-not-query.test\n           span-near-query.test.js  3.29 MB    9, 123  [emitted]  [big]  span-near-query.test\n     span-multi-term-query.test.js  3.29 MB   10, 123  [emitted]  [big]  span-multi-term-query.test\nspan-little-big-query-base.test.js  3.29 MB   11, 123  [emitted]  [big]  span-little-big-query-base.test\n          span-first-query.test.js  3.29 MB   12, 123  [emitted]  [big]  span-first-query.test\n  span-field-masking-query.test.js  3.29 MB   13, 123  [emitted]  [big]  span-field-masking-query.test\n simple-query-string-query.test.js  3.29 MB   14, 123  [emitted]  [big]  simple-query-string-query.test\n         script-score-func.test.js  3.29 MB   15, 123  [emitted]  [big]  script-score-func.test\n              script-query.test.js  3.29 MB   16, 123  [emitted]  [big]  script-query.test\n                score-func.test.js  3.29 MB   17, 123  [emitted]  [big]  score-func.test\n              regexp-query.test.js  3.29 MB   18, 123  [emitted]  [big]  regexp-query.test\n               range-query.test.js  3.29 MB   19, 123  [emitted]  [big]  range-query.test\n         random-score-func.test.js  3.29 MB   20, 123  [emitted]  [big]  random-score-func.test\n        query-string-query.test.js  3.29 MB   21, 123  [emitted]  [big]  query-string-query.test\n              prefix-query.test.js  3.29 MB   22, 123  [emitted]  [big]  prefix-query.test\n           percolate-query.test.js  3.29 MB   23, 123  [emitted]  [big]  percolate-query.test\n           parent-id-query.test.js  3.29 MB   24, 123  [emitted]  [big]  parent-id-query.test\n              nested-query.test.js  3.29 MB   25, 123  [emitted]  [big]  nested-query.test\n         multi-match-query.test.js  3.29 MB   26, 123  [emitted]  [big]  multi-match-query.test\n      more-like-this-query.test.js  3.29 MB   27, 123  [emitted]  [big]  more-like-this-query.test\n               match-query.test.js  3.29 MB   28, 123  [emitted]  [big]  match-query.test\n match-phrase-prefix-query.test.js  3.29 MB   29, 123  [emitted]  [big]  match-phrase-prefix-query.test\n        joining-query-base.test.js  3.29 MB   30, 123  [emitted]  [big]  joining-query-base.test\n                 ids-query.test.js  3.29 MB   31, 123  [emitted]  [big]  ids-query.test\n          has-parent-query.test.js  3.29 MB   32, 123  [emitted]  [big]  has-parent-query.test\n           has-child-query.test.js  3.29 MB   33, 123  [emitted]  [big]  has-child-query.test\n           geo-shape-query.test.js  3.29 MB   34, 123  [emitted]  [big]  geo-shape-query.test\n         geo-polygon-query.test.js  3.29 MB   35, 123  [emitted]  [big]  geo-polygon-query.test\n        geo-distance-query.test.js  3.29 MB   36, 123  [emitted]  [big]  geo-distance-query.test\n    geo-bounding-box-query.test.js  3.29 MB   37, 123  [emitted]  [big]  geo-bounding-box-query.test\n               fuzzy-query.test.js  3.29 MB   38, 123  [emitted]  [big]  fuzzy-query.test\n      function-score-query.test.js  3.29 MB   39, 123  [emitted]  [big]  function-score-query.test\n   field-value-factor-func.test.js  3.29 MB   40, 123  [emitted]  [big]  field-value-factor-func.test\n             dis-max-query.test.js  3.29 MB   41, 123  [emitted]  [big]  dis-max-query.test\n          decay-score-func.test.js  3.29 MB   42, 123  [emitted]  [big]  decay-score-func.test\n      constant-score-query.test.js  3.29 MB   43, 123  [emitted]  [big]  constant-score-query.test\n            boosting-query.test.js  3.29 MB   44, 123  [emitted]  [big]  boosting-query.test\n                bool-query.test.js  3.29 MB   45, 123  [emitted]  [big]  bool-query.test\n                      sort.test.js  3.29 MB   46, 123  [emitted]  [big]  sort.test\n                   rescore.test.js  3.29 MB   47, 123  [emitted]  [big]  rescore.test\n       request-body-search.test.js   3.3 MB   48, 123  [emitted]  [big]  request-body-search.test\n                inner-hits.test.js  3.29 MB   49, 123  [emitted]  [big]  inner-hits.test\n             indexed-shape.test.js  3.29 MB   50, 123  [emitted]  [big]  indexed-shape.test\n                 highlight.test.js   3.3 MB   51, 123  [emitted]  [big]  highlight.test\n                 geo-shape.test.js  3.29 MB   52, 123  [emitted]  [big]  geo-shape.test\n               aggregation.test.js  3.29 MB   53, 123  [emitted]  [big]  aggregation.test\n           value-count-agg.test.js  3.29 MB   54, 123  [emitted]  [big]  value-count-agg.test\n              top-hits-agg.test.js  3.29 MB   55, 123  [emitted]  [big]  top-hits-agg.test\n                 terms-agg.test.js  3.29 MB   56, 123  [emitted]  [big]  terms-agg.test\n            sum-bucket-agg.test.js  3.29 MB   57, 123  [emitted]  [big]  sum-bucket-agg.test\n                   sum-agg.test.js  3.29 MB   58, 123  [emitted]  [big]  sum-agg.test\n          stats-bucket-agg.test.js  3.29 MB   59, 123  [emitted]  [big]  stats-bucket-agg.test\n                 stats-agg.test.js  3.29 MB   60, 123  [emitted]  [big]  stats-agg.test\n     significant-terms-agg.test.js  3.29 MB   61, 123  [emitted]  [big]  significant-terms-agg.test\n                    script.test.js  3.29 MB    1, 123  [emitted]  [big]  script.test\n       scripted-metric-agg.test.js  3.29 MB   63, 123  [emitted]  [big]  scripted-metric-agg.test\n               sampler-agg.test.js  3.29 MB   64, 123  [emitted]  [big]  sampler-agg.test\n        reverse-nested-agg.test.js  3.29 MB   65, 123  [emitted]  [big]  reverse-nested-agg.test\n    percentiles-bucket-agg.test.js  3.29 MB   66, 123  [emitted]  [big]  percentiles-bucket-agg.test\n           percentiles-agg.test.js  3.29 MB   67, 123  [emitted]  [big]  percentiles-agg.test\n      percentile-ranks-agg.test.js  3.29 MB   68, 123  [emitted]  [big]  percentile-ranks-agg.test\n                nested-agg.test.js  3.29 MB   69, 123  [emitted]  [big]  nested-agg.test\n        moving-average-agg.test.js  3.29 MB   70, 123  [emitted]  [big]  moving-average-agg.test\n               missing-agg.test.js  3.29 MB   71, 123  [emitted]  [big]  missing-agg.test\n            min-bucket-agg.test.js  3.29 MB   72, 123  [emitted]  [big]  min-bucket-agg.test\n                   min-agg.test.js  3.29 MB   73, 123  [emitted]  [big]  min-agg.test\n          metrics-agg-base.test.js  3.29 MB   74, 123  [emitted]  [big]  metrics-agg-base.test\n            max-bucket-agg.test.js  3.29 MB   75, 123  [emitted]  [big]  max-bucket-agg.test\n                   max-agg.test.js  3.29 MB   76, 123  [emitted]  [big]  max-agg.test\n          matrix-stats-agg.test.js  3.29 MB   77, 123  [emitted]  [big]  matrix-stats-agg.test\n              ip-range-agg.test.js  3.29 MB   78, 123  [emitted]  [big]  ip-range-agg.test\n             histogram-agg.test.js  3.29 MB   79, 123  [emitted]  [big]  histogram-agg.test\n                global-agg.test.js  3.29 MB   80, 123  [emitted]  [big]  global-agg.test\n         geo-hash-grid-agg.test.js  3.29 MB   81, 123  [emitted]  [big]  geo-hash-grid-agg.test\n          geo-distance-agg.test.js  3.29 MB   82, 123  [emitted]  [big]  geo-distance-agg.test\n          geo-centroid-agg.test.js  3.29 MB   83, 123  [emitted]  [big]  geo-centroid-agg.test\n            geo-bounds-agg.test.js  3.29 MB   84, 123  [emitted]  [big]  geo-bounds-agg.test\n                filter-agg.test.js  3.29 MB   85, 123  [emitted]  [big]  filter-agg.test\n extended-stats-bucket-agg.test.js  3.29 MB   86, 123  [emitted]  [big]  extended-stats-bucket-agg.test\n        extended-stats-agg.test.js  3.29 MB   87, 123  [emitted]  [big]  extended-stats-agg.test\n   diversified-sampler-agg.test.js  3.29 MB   88, 123  [emitted]  [big]  diversified-sampler-agg.test\n            derivative-agg.test.js  3.29 MB   89, 123  [emitted]  [big]  derivative-agg.test\n        date-histogram-agg.test.js  3.29 MB   90, 123  [emitted]  [big]  date-histogram-agg.test\n        cumulative-sum-agg.test.js  3.29 MB   91, 123  [emitted]  [big]  cumulative-sum-agg.test\n              children-agg.test.js  3.29 MB   92, 123  [emitted]  [big]  children-agg.test\n           cardinality-agg.test.js  3.29 MB   93, 123  [emitted]  [big]  cardinality-agg.test\n       bucket-selector-agg.test.js  3.29 MB   94, 123  [emitted]  [big]  bucket-selector-agg.test\n         bucket-script-agg.test.js  3.29 MB   95, 123  [emitted]  [big]  bucket-script-agg.test\n           bucket-agg-base.test.js  3.29 MB   96, 123  [emitted]  [big]  bucket-agg-base.test\n            avg-bucket-agg.test.js  3.29 MB   97, 123  [emitted]  [big]  avg-bucket-agg.test\n                   avg-agg.test.js  3.29 MB   98, 123  [emitted]  [big]  avg-agg.test\n         weight-score-func.test.js  3.27 MB        99  [emitted]  [big]  weight-score-func.test\n                type-query.test.js  3.27 MB       100  [emitted]  [big]  type-query.test\n                term-query.test.js  3.27 MB       101  [emitted]  [big]  term-query.test\n         span-within-query.test.js  3.27 MB       102  [emitted]  [big]  span-within-query.test\n           span-term-query.test.js  3.27 MB       103  [emitted]  [big]  span-term-query.test\n     span-containing-query.test.js  3.27 MB       104  [emitted]  [big]  span-containing-query.test\n        match-phrase-query.test.js  3.27 MB       105  [emitted]  [big]  match-phrase-query.test\n          match-none-query.test.js  3.27 MB       106  [emitted]  [big]  match-none-query.test\n           match-all-query.test.js  3.27 MB       107  [emitted]  [big]  match-all-query.test\n              exists-query.test.js  3.27 MB       108  [emitted]  [big]  exists-query.test\n                     index.test.js  3.31 MB       109  [emitted]  [big]  index.test\n                 range-agg.test.js  3.27 MB       110  [emitted]  [big]  range-agg.test\n            date-range-agg.test.js  3.27 MB       111  [emitted]  [big]  date-range-agg.test\n            terms-agg-base.test.js  1.21 MB  112, 123  [emitted]  [big]  terms-agg-base.test\n            range-agg-base.test.js  1.22 MB  113, 123  [emitted]  [big]  range-agg-base.test\n        histogram-agg-base.test.js  1.22 MB  114, 123  [emitted]  [big]  histogram-agg-base.test\n         pipeline-agg-base.test.js   883 kB  115, 123  [emitted]  [big]  pipeline-agg-base.test\n   query-string-query-base.test.js  1.07 MB  116, 123  [emitted]  [big]  query-string-query-base.test\n   match-phrase-query-base.test.js  1.07 MB  117, 123  [emitted]  [big]  match-phrase-query-base.test\n      full-text-query-base.test.js  1.07 MB  118, 123  [emitted]  [big]  full-text-query-base.test\n     mono-field-query-base.test.js  1.05 MB       119  [emitted]  [big]  mono-field-query-base.test\n            geo-query-base.test.js   821 kB  120, 123  [emitted]  [big]  geo-query-base.test\n                     query.test.js   687 kB  121, 123  [emitted]  [big]  query.test\n                      util.test.js   665 kB       122  [emitted]  [big]  util.test\n                        _macros.js  71.8 kB       123  [emitted]         _macros\n [181] ./test/aggregations-test/extended-stats-agg.test.js 807 bytes {87} [built]\n [244] ./test/queries-test/fuzzy-query.test.js 529 bytes {38} [built]\n [245] ./test/queries-test/geo-bounding-box-query.test.js 1.44 kB {37} [built]\n [246] ./test/queries-test/geo-distance-query.test.js 1.07 kB {36} [built]\n [247] ./test/queries-test/geo-polygon-query.test.js 508 bytes {35} [built]\n [248] ./test/queries-test/geo-query-base.test.js 891 bytes {120} [built]\n [249] ./test/queries-test/geo-shape-query.test.js 1.19 kB {34} [built]\n [283] ./test/queries-test/span-or-query.test.js 685 bytes {7} [built]\n [284] ./test/queries-test/span-term-query.test.js 780 bytes {103} [built]\n [285] ./test/queries-test/span-within-query.test.js 245 bytes {102} [built]\n [286] ./test/queries-test/term-query.test.js 693 bytes {101} [built]\n [287] ./test/queries-test/terms-query.test.js 1.97 kB {6} [built]\n [288] ./test/queries-test/type-query.test.js 419 bytes {100} [built]\n [289] ./test/queries-test/weight-score-func.test.js 346 bytes {99} [built]\n [291] ./test/recipes.test.js 8.62 kB {4} [built]\n    + 277 hidden modules\n```\n\n\n. @novemberborn I am a little confused. What do you mean by AVA precompiles the test file? Isn't that what we are doing before feeding it to AVA?. From the README\n\nWe use our own bundled Babel with our @ava/stage-4 preset, as well as custom transforms for test and helper files.\nNote that AVA will always apply a few internal plugins regardless of configuration, but they should not impact the behavior of your code.\n\nI don't think there is a way to tell AVA that there is no need to transpile again even if we were to apply those transforms in out precompile step. ~17m is not acceptable even though it'll be once. \nAre there any other options we can consider?. I have a solution for this but it is not pretty. Let me know what you think. Comments inline.\njs\n// npm scripts\n{\n  \"scripts\": {\n    // Use the babel cli to compile the src files but preserve file structure\n    \"precompile-src\": \"cross-env NODE_ENV=test babel src --out-dir _src\",\n    // We still need to compile tests to change the require path for src files\n    \"precompile-tests\": \"cross-env NODE_ENV=test webpack --config webpack.config.test.js\",\n    \"pretest\": \"npm run precompile-src && npm run precompile-tests\",\n    \"test\": \"cross-env NODE_ENV=test nyc --cache ava _build --concurrency 3\"\n  }\n}\nWebpack Externals Docs - https://webpack.js.org/configuration/externals/#function\nwebpack.config.test.js\n```js\n'use strict';\nconst path = require('path');\nconst glob = require('glob');\nconst nodeExternals = require('webpack-node-externals');\nconst testFiles = glob.sync('./test/*/.js');\nconst entryObj = {};\nfor (let idx = 0; idx < testFiles.length; idx++) {\n    const file = testFiles[idx];\n    entryObj[path.basename(file, path.extname(file))] = file;\n}\nmodule.exports = {\n    target: 'node',\n    entry: entryObj,\n    output: {\n        path: path.resolve(__dirname, '_build'),\n        filename: '[name].js'\n    },\n    externals: [\n        nodeExternals(),\n        // Rewrite the require paths to use _src\n        (context, request, callback) => {\n            // This is a little messy because tests are not output in original file structure\n            // test/index.test.js -> _build/index.test.js \n            // => ../src -> ../_src\n            // test/aggregations-test/avg-agg.test.js -> _build/avg-agg.test.js \n            // => ../../src -> ../_src\n            if (request.includes('/src')) {\n                const requestReqwrite = request\n                    .replace('/src', '/_src')\n                    .replace('../../_src', '../_src');\n                return callback(null, commonjs ${requestReqwrite});\n            }\n            callback();\n        }\n    ]\n};\n```\n\nWebpack output\n\n```\n> cross-env NODE_ENV=test webpack --config webpack.config.test.js\n\nHash: 97f887db4e4364259826\nVersion: webpack 2.5.1\nTime: 2646ms\n                             Asset     Size    Chunks             Chunk Names\n              top-hits-agg.test.js  29.3 kB   62, 108  [emitted]  top-hits-agg.test\nspan-little-big-query-base.test.js  26.3 kB    0, 108  [emitted]  span-little-big-query-base.test\n        joining-query-base.test.js  27.1 kB    2, 108  [emitted]  joining-query-base.test\n        common-terms-query.test.js  28.1 kB    3, 108  [emitted]  common-terms-query.test\n                    script.test.js    28 kB    4, 108  [emitted]  script.test\n                 geo-point.test.js  28.5 kB    5, 108  [emitted]  geo-point.test\n               aggregation.test.js    29 kB    6, 108  [emitted]  aggregation.test\n          metrics-agg-base.test.js    27 kB    7, 108  [emitted]  metrics-agg-base.test\n               filters-agg.test.js  29.7 kB    8, 108  [emitted]  filters-agg.test\n           bucket-agg-base.test.js  26.8 kB    9, 108  [emitted]  bucket-agg-base.test\n                   recipes.test.js  32.9 kB   10, 108  [emitted]  recipes.test\n            wildcard-query.test.js  25.9 kB   11, 108  [emitted]  wildcard-query.test\n               terms-query.test.js  27.7 kB   12, 108  [emitted]  terms-query.test\n             span-or-query.test.js    26 kB   13, 108  [emitted]  span-or-query.test\n            span-not-query.test.js  26.2 kB   14, 108  [emitted]  span-not-query.test\n           span-near-query.test.js  26.2 kB   15, 108  [emitted]  span-near-query.test\n     span-multi-term-query.test.js    26 kB   16, 108  [emitted]  span-multi-term-query.test\n          span-first-query.test.js  26.2 kB   17, 108  [emitted]  span-first-query.test\n  span-field-masking-query.test.js  26.1 kB   18, 108  [emitted]  span-field-masking-query.test\n simple-query-string-query.test.js  25.5 kB   19, 108  [emitted]  simple-query-string-query.test\n         script-score-func.test.js  26.5 kB   20, 108  [emitted]  script-score-func.test\n              script-query.test.js  26.1 kB   21, 108  [emitted]  script-query.test\n              regexp-query.test.js    26 kB   22, 108  [emitted]  regexp-query.test\n               range-query.test.js  26.6 kB   23, 108  [emitted]  range-query.test\n         random-score-func.test.js  25.4 kB   24, 108  [emitted]  random-score-func.test\n        query-string-query.test.js  27.5 kB   25, 108  [emitted]  query-string-query.test\n   query-string-query-base.test.js  26.9 kB   26, 108  [emitted]  query-string-query-base.test\n              prefix-query.test.js  25.8 kB   27, 108  [emitted]  prefix-query.test\n           percolate-query.test.js  26.5 kB   28, 108  [emitted]  percolate-query.test\n           parent-id-query.test.js  26.1 kB   29, 108  [emitted]  parent-id-query.test\n              nested-query.test.js    26 kB   30, 108  [emitted]  nested-query.test\n         multi-match-query.test.js  28.8 kB   31, 108  [emitted]  multi-match-query.test\n      more-like-this-query.test.js  29.4 kB   32, 108  [emitted]  more-like-this-query.test\n               match-query.test.js  27.1 kB   33, 108  [emitted]  match-query.test\n   match-phrase-query-base.test.js  25.9 kB   34, 108  [emitted]  match-phrase-query-base.test\n match-phrase-prefix-query.test.js  25.5 kB   35, 108  [emitted]  match-phrase-prefix-query.test\n                 ids-query.test.js  26.2 kB   36, 108  [emitted]  ids-query.test\n          has-parent-query.test.js  26.4 kB   37, 108  [emitted]  has-parent-query.test\n           has-child-query.test.js  26.3 kB   38, 108  [emitted]  has-child-query.test\n           geo-shape-query.test.js  26.9 kB   39, 108  [emitted]  geo-shape-query.test\n            geo-query-base.test.js  26.3 kB   40, 108  [emitted]  geo-query-base.test\n         geo-polygon-query.test.js  25.7 kB   41, 108  [emitted]  geo-polygon-query.test\n        geo-distance-query.test.js  26.4 kB   42, 108  [emitted]  geo-distance-query.test\n    geo-bounding-box-query.test.js  27.4 kB   43, 108  [emitted]  geo-bounding-box-query.test\n               fuzzy-query.test.js  25.8 kB   44, 108  [emitted]  fuzzy-query.test\n      function-score-query.test.js  27.8 kB   45, 108  [emitted]  function-score-query.test\n      full-text-query-base.test.js  26.2 kB   46, 108  [emitted]  full-text-query-base.test\n   field-value-factor-func.test.js  26.2 kB   47, 108  [emitted]  field-value-factor-func.test\n             dis-max-query.test.js  26.4 kB   48, 108  [emitted]  dis-max-query.test\n          decay-score-func.test.js  27.2 kB   49, 108  [emitted]  decay-score-func.test\n      constant-score-query.test.js  26.5 kB   50, 108  [emitted]  constant-score-query.test\n            boosting-query.test.js  26.9 kB   51, 108  [emitted]  boosting-query.test\n                bool-query.test.js  28.4 kB   52, 108  [emitted]  bool-query.test\n                      sort.test.js  28.9 kB   53, 108  [emitted]  sort.test\n                   rescore.test.js    27 kB   54, 108  [emitted]  rescore.test\n       request-body-search.test.js  35.6 kB   55, 108  [emitted]  request-body-search.test\n                     query.test.js  25.9 kB   56, 108  [emitted]  query.test\n                inner-hits.test.js  28.4 kB   57, 108  [emitted]  inner-hits.test\n             indexed-shape.test.js  25.9 kB   58, 108  [emitted]  indexed-shape.test\n                 highlight.test.js  35.4 kB   59, 108  [emitted]  highlight.test\n                 geo-shape.test.js  27.6 kB   60, 108  [emitted]  geo-shape.test\n           value-count-agg.test.js  25.8 kB   61, 108  [emitted]  value-count-agg.test\n                score-func.test.js  26.4 kB    1, 108  [emitted]  score-func.test\n                 terms-agg.test.js  28.2 kB   63, 108  [emitted]  terms-agg.test\n            terms-agg-base.test.js  26.5 kB   64, 108  [emitted]  terms-agg-base.test\n            sum-bucket-agg.test.js  25.8 kB   65, 108  [emitted]  sum-bucket-agg.test\n                   sum-agg.test.js  25.6 kB   66, 108  [emitted]  sum-agg.test\n          stats-bucket-agg.test.js  25.8 kB   67, 108  [emitted]  stats-bucket-agg.test\n                 stats-agg.test.js  25.6 kB   68, 108  [emitted]  stats-agg.test\n     significant-terms-agg.test.js  27.4 kB   69, 108  [emitted]  significant-terms-agg.test\n   serial-differencing-agg.test.js    26 kB   70, 108  [emitted]  serial-differencing-agg.test\n       scripted-metric-agg.test.js    27 kB   71, 108  [emitted]  scripted-metric-agg.test\n               sampler-agg.test.js    26 kB   72, 108  [emitted]  sampler-agg.test\n        reverse-nested-agg.test.js  26.4 kB   73, 108  [emitted]  reverse-nested-agg.test\n            range-agg-base.test.js  27.7 kB   74, 108  [emitted]  range-agg-base.test\n         pipeline-agg-base.test.js  26.4 kB   75, 108  [emitted]  pipeline-agg-base.test\n    percentiles-bucket-agg.test.js  26.2 kB   76, 108  [emitted]  percentiles-bucket-agg.test\n           percentiles-agg.test.js  26.8 kB   77, 108  [emitted]  percentiles-agg.test\n      percentile-ranks-agg.test.js  27.1 kB   78, 108  [emitted]  percentile-ranks-agg.test\n                nested-agg.test.js  26.3 kB   79, 108  [emitted]  nested-agg.test\n        moving-average-agg.test.js  26.7 kB   80, 108  [emitted]  moving-average-agg.test\n               missing-agg.test.js  25.8 kB   81, 108  [emitted]  missing-agg.test\n            min-bucket-agg.test.js  25.6 kB   82, 108  [emitted]  min-bucket-agg.test\n                   min-agg.test.js  25.6 kB   83, 108  [emitted]  min-agg.test\n            max-bucket-agg.test.js  25.6 kB   84, 108  [emitted]  max-bucket-agg.test\n                   max-agg.test.js  25.6 kB   85, 108  [emitted]  max-agg.test\n          matrix-stats-agg.test.js  26.5 kB   86, 108  [emitted]  matrix-stats-agg.test\n              ip-range-agg.test.js  26.7 kB   87, 108  [emitted]  ip-range-agg.test\n             histogram-agg.test.js  25.6 kB   88, 108  [emitted]  histogram-agg.test\n        histogram-agg-base.test.js  28.5 kB   89, 108  [emitted]  histogram-agg-base.test\n                global-agg.test.js  25.6 kB   90, 108  [emitted]  global-agg.test\n         geo-hash-grid-agg.test.js  26.8 kB   91, 108  [emitted]  geo-hash-grid-agg.test\n          geo-distance-agg.test.js  27.2 kB   92, 108  [emitted]  geo-distance-agg.test\n          geo-centroid-agg.test.js  25.8 kB   93, 108  [emitted]  geo-centroid-agg.test\n            geo-bounds-agg.test.js  26.3 kB   94, 108  [emitted]  geo-bounds-agg.test\n                filter-agg.test.js  26.4 kB   95, 108  [emitted]  filter-agg.test\n extended-stats-bucket-agg.test.js  26.2 kB   96, 108  [emitted]  extended-stats-bucket-agg.test\n        extended-stats-agg.test.js  26.1 kB   97, 108  [emitted]  extended-stats-agg.test\n   diversified-sampler-agg.test.js  26.5 kB   98, 108  [emitted]  diversified-sampler-agg.test\n            derivative-agg.test.js    26 kB   99, 108  [emitted]  derivative-agg.test\n        date-histogram-agg.test.js  26.1 kB  100, 108  [emitted]  date-histogram-agg.test\n        cumulative-sum-agg.test.js  25.8 kB  101, 108  [emitted]  cumulative-sum-agg.test\n              children-agg.test.js  26.1 kB  102, 108  [emitted]  children-agg.test\n           cardinality-agg.test.js  26.3 kB  103, 108  [emitted]  cardinality-agg.test\n       bucket-selector-agg.test.js  26.6 kB  104, 108  [emitted]  bucket-selector-agg.test\n         bucket-script-agg.test.js  26.4 kB  105, 108  [emitted]  bucket-script-agg.test\n            avg-bucket-agg.test.js  25.7 kB  106, 108  [emitted]  avg-bucket-agg.test\n                   avg-agg.test.js  25.6 kB  107, 108  [emitted]  avg-agg.test\n                        _macros.js    24 kB       108  [emitted]  _macros\n         weight-score-func.test.js  3.77 kB       109  [emitted]  weight-score-func.test\n                type-query.test.js  3.89 kB       110  [emitted]  type-query.test\n                term-query.test.js  4.12 kB       111  [emitted]  term-query.test\n         span-within-query.test.js  3.63 kB       112  [emitted]  span-within-query.test\n           span-term-query.test.js  4.32 kB       113  [emitted]  span-term-query.test\n     span-containing-query.test.js  3.64 kB       114  [emitted]  span-containing-query.test\n     mono-field-query-base.test.js  4.25 kB       115  [emitted]  mono-field-query-base.test\n        match-phrase-query.test.js  3.71 kB       116  [emitted]  match-phrase-query.test\n          match-none-query.test.js  3.62 kB       117  [emitted]  match-none-query.test\n           match-all-query.test.js  3.62 kB       118  [emitted]  match-all-query.test\n              exists-query.test.js   3.8 kB       119  [emitted]  exists-query.test\n                     index.test.js  46.3 kB       120  [emitted]  index.test\n                      util.test.js  3.66 kB       121  [emitted]  util.test\n                 range-agg.test.js   3.8 kB       122  [emitted]  range-agg.test\n            date-range-agg.test.js  4.54 kB       123  [emitted]  date-range-agg.test\n   [2] ./test/_macros.js 21 kB {0} {1} {2} {3} {4} {5} {6} {7} {8} {9} {10} {11} {12} {13} {14} {15} {16} {17} {18} {19} {20} {21} {22} {23} {24} {25} {26} {27} {28} {29} {30} {31} {32} {33} {34} {35} {36} {37} {38} {39} {40} {41} {42} {43} {44} {45} {46} {47} {48} {49} {50} {51} {52} {53} {54} {55} {56} {57} {58} {59} {60} {61} {62} {63} {64} {65} {66} {67} {68} {69} {70} {71} {72} {73} {74} {75} {76} {77} {78} {79} {80} {81} {82} {83} {84} {85} {86} {87} {88} {89} {90} {91} {92} {93} {94} {95} {96} {97} {98} {99} {100} {101} {102} {103} {104} {105} {106} {107} {108} [built]\n  [70] ./test/core-test/geo-point.test.js 2.3 kB {5} [built]\n [127] ./test/queries-test/span-little-big-query-base.test.js 655 bytes {0} [built]\n [128] ./test/queries-test/span-multi-term-query.test.js 698 bytes {16} [built]\n [129] ./test/queries-test/span-near-query.test.js 783 bytes {15} [built]\n [130] ./test/queries-test/span-not-query.test.js 680 bytes {14} [built]\n [131] ./test/queries-test/span-or-query.test.js 685 bytes {13} [built]\n [132] ./test/queries-test/span-term-query.test.js 780 bytes {113} [built]\n [133] ./test/queries-test/span-within-query.test.js 245 bytes {112} [built]\n [134] ./test/queries-test/term-query.test.js 693 bytes {111} [built]\n [135] ./test/queries-test/terms-query.test.js 1.97 kB {12} [built]\n [136] ./test/queries-test/type-query.test.js 419 bytes {110} [built]\n [137] ./test/queries-test/weight-score-func.test.js 346 bytes {109} [built]\n [138] ./test/queries-test/wildcard-query.test.js 664 bytes {11} [built]\n [139] ./test/recipes.test.js 8.62 kB {10} [built]\n    + 125 hidden modules\n```\n\n\n\nCompiled test file sample\n\n##### Original test file\n```js\nimport test from 'ava';\nimport { AvgAggregation } from '../../src';\nimport { setsAggType } from '../_macros';\n\ntest(setsAggType, AvgAggregation, 'avg');\n\ntest('constructor sets field', t => {\n    const value = new AvgAggregation('my_agg', 'my_field').toJSON();\n    const expected = {\n        my_agg: {\n            avg: {\n                field: 'my_field'\n            }\n        }\n    };\n    t.deepEqual(value, expected);\n});\n\n```\n\n##### Compiled file\n```js\n/******/ (function(modules) { // webpackBootstrap\n/******/    // The module cache\n/******/    var installedModules = {};\n/******/\n/******/    // The require function\n/******/    function __webpack_require__(moduleId) {\n/******/\n/******/        // Check if module is in cache\n/******/        if(installedModules[moduleId]) {\n/******/            return installedModules[moduleId].exports;\n/******/        }\n/******/        // Create a new module (and put it into the cache)\n/******/        var module = installedModules[moduleId] = {\n/******/            i: moduleId,\n/******/            l: false,\n/******/            exports: {}\n/******/        };\n/******/\n/******/        // Execute the module function\n/******/        modules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n/******/\n/******/        // Flag the module as loaded\n/******/        module.l = true;\n/******/\n/******/        // Return the exports of the module\n/******/        return module.exports;\n/******/    }\n/******/\n/******/\n/******/    // expose the modules object (__webpack_modules__)\n/******/    __webpack_require__.m = modules;\n/******/\n/******/    // expose the module cache\n/******/    __webpack_require__.c = installedModules;\n/******/\n/******/    // identity function for calling harmony imports with the correct context\n/******/    __webpack_require__.i = function(value) { return value; };\n/******/\n/******/    // define getter function for harmony exports\n/******/    __webpack_require__.d = function(exports, name, getter) {\n/******/        if(!__webpack_require__.o(exports, name)) {\n/******/            Object.defineProperty(exports, name, {\n/******/                configurable: false,\n/******/                enumerable: true,\n/******/                get: getter\n/******/            });\n/******/        }\n/******/    };\n/******/\n/******/    // getDefaultExport function for compatibility with non-harmony modules\n/******/    __webpack_require__.n = function(module) {\n/******/        var getter = module && module.__esModule ?\n/******/            function getDefault() { return module['default']; } :\n/******/            function getModuleExports() { return module; };\n/******/        __webpack_require__.d(getter, 'a', getter);\n/******/        return getter;\n/******/    };\n/******/\n/******/    // Object.prototype.hasOwnProperty.call\n/******/    __webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n/******/\n/******/    // __webpack_public_path__\n/******/    __webpack_require__.p = \"\";\n/******/\n/******/    // Load entry module and return exports\n/******/    return __webpack_require__(__webpack_require__.s = 15);\n/******/ })\n/************************************************************************/\n/******/ ([\n/* 0 */\n/***/ (function(module, exports) {\n\nmodule.exports = require(\"ava\");\n\n/***/ }),\n/* 1 */\n/***/ (function(module, exports) {\n\nmodule.exports = require(\"../_src\");\n\n/***/ }),\n/* 2 */\n/***/ (function(module, __webpack_exports__, __webpack_require__) {\n\n\"use strict\";\nObject.defineProperty(__webpack_exports__, \"__esModule\", { value: true });\n/* harmony export (immutable) */ __webpack_exports__[\"setsAggType\"] = setsAggType;\n/* harmony export (immutable) */ __webpack_exports__[\"illegalCall\"] = illegalCall;\n/* harmony export (immutable) */ __webpack_exports__[\"illegalParamType\"] = illegalParamType;\n/* harmony export (immutable) */ __webpack_exports__[\"validatedCorrectly\"] = validatedCorrectly;\n/* harmony export (immutable) */ __webpack_exports__[\"simpleExpect\"] = simpleExpect;\n/* harmony export (immutable) */ __webpack_exports__[\"aggsExpectStrategy\"] = aggsExpectStrategy;\n/* harmony export (immutable) */ __webpack_exports__[\"nameExpectStrategy\"] = nameExpectStrategy;\n/* harmony export (immutable) */ __webpack_exports__[\"nameFieldExpectStrategy\"] = nameFieldExpectStrategy;\n/* harmony export (immutable) */ __webpack_exports__[\"makeSetsOptionMacro\"] = makeSetsOptionMacro;\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_0_lodash__ = __webpack_require__(4);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_0_lodash___default = __webpack_require__.n(__WEBPACK_IMPORTED_MODULE_0_lodash__);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_1__src_core_util__ = __webpack_require__(3);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_1__src_core_util___default = __webpack_require__.n(__WEBPACK_IMPORTED_MODULE_1__src_core_util__);\n\n\n\nconst ILLEGAL_PARAM = Object.create(null);\n\n/**\n * Macro for testing that aggregation type is set as expected.\n *\n * @param {*} t\n * @param {function} Cls\n * @param {string} aggType\n * @param {Object=} defaultDef\n */\nfunction setsAggType(t, Cls, aggType, defaultDef) {\n    const value = new Cls('my_agg').toJSON();\n    const expected = {\n        my_agg: {\n            [aggType]: Object.assign({}, defaultDef)\n        }\n    };\n    t.deepEqual(value, expected);\n}\n\nsetsAggType.title = (ignore, Cls, aggType) => `sets type as ${aggType}`;\n\n/**\n * Macro for checking method cannot be called on the instance\n *\n * @param {*} t\n * @param {*} Cls constructor class\n * @param {string} propKey method name\n */\nfunction illegalCall(t, Cls, propKey, ...args) {\n    const err = t.throws(() => new Cls(...args)[propKey](), Error);\n    t.is(err.message, `${propKey} is not supported in ${Cls.name}`);\n}\n\nillegalCall.title = (ignore, Cls, propKey) => `${__WEBPACK_IMPORTED_MODULE_0_lodash___default.a.snakeCase(propKey)} cannot be set`;\n\n/**\n * Check that calling method on instance with illegal param type throws error\n *\n * @param {*} t\n * @param {*} instance\n * @param {string} method\n * @param {string} clsName\n */\nfunction illegalParamType(t, instance, method, clsName) {\n    let err = t.throws(() => instance[method](null), TypeError);\n    t.is(err.message, `Argument must be an instance of ${clsName}`);\n\n    err = t.throws(() => instance[method](ILLEGAL_PARAM), TypeError);\n    t.is(err.message, `Argument must be an instance of ${clsName}`);\n}\n\nillegalParamType.title = (ignore, instance, method, clsName) => `checks ${clsName} class`;\n\n/**\n * Macro for testing method validation\n *\n * @param {*} t\n * @param {function} getInstance\n * @param {string} method\n * @param {Array} validValues\n * @param {boolean=} toggleCase\n */\nfunction validatedCorrectly(t, getInstance, method, validValues, toggleCase = true) {\n    __WEBPACK_IMPORTED_MODULE_0_lodash___default.a.forEach(validValues, val => {\n        t.notThrows(() => getInstance()[method](val));\n\n        if (toggleCase) {\n            t.notThrows(() => getInstance()[method](val.toLowerCase()));\n            t.notThrows(() => getInstance()[method](val.toUpperCase()));\n        }\n    });\n\n    t.throws(() => getInstance()[method](null));\n    t.throws(() => getInstance()[method](`invalid_${__WEBPACK_IMPORTED_MODULE_0_lodash___default.a.snakeCase(method)}`));\n}\n\nvalidatedCorrectly.title = (ignore, getInstance, method) =>\n    `${__WEBPACK_IMPORTED_MODULE_0_lodash___default.a.snakeCase(method)} correctly validated`;\n\n/**\n * Simple strategy for checking option is set for use with `makeSetsOptionMacro`\n *\n * @param {string} keyName\n * @param {*} propValue\n * @returns {function}\n */\nfunction simpleExpect(keyName, propValue) {\n    return { [keyName]: propValue };\n}\n\n/**\n * Expect strategy for use with `makeSetsOptionMacro` for aggregations\n *\n * @param {string} name\n * @param {string} type\n * @param {Object} defaultDef\n * @returns {function}\n */\nfunction aggsExpectStrategy(name, type, defaultDef) {\n    return (keyName, propValue) => ({\n        [name]: {\n            [type]: Object.assign({ [keyName]: propValue }, defaultDef)\n        }\n    });\n}\n\n/**\n * Expect strategy for use with `makeSetsOptionMacro` for queries, score functions\n *\n * @param {string} name\n * @param {Object=} defaultDef\n * @returns {function}\n */\nfunction nameExpectStrategy(name, defaultDef) {\n    return (keyName, propValue) => ({\n        [name]: Object.assign({}, defaultDef, { [keyName]: propValue })\n    });\n}\n\n/**\n * Expect strategy for use with `makeSetsOptionMacro` for full text queries\n *\n * @param {string} name\n * @param {Object=} defaultDef\n * @returns {function}\n */\nfunction nameFieldExpectStrategy(name, defaultDef) {\n    return (keyName, propValue) => ({\n        [name]: {\n            my_field: Object.assign({}, defaultDef, { [keyName]: propValue })\n        }\n    });\n}\n\n/**\n * Make macro for checking property is set.\n *\n * @param {function} getInstance\n * @param {function=} getExpected Set to `simpleExpect` by default\n * @returns {function}\n */\nfunction makeSetsOptionMacro(getInstance, getExpected = simpleExpect) {\n    /**\n     * Macro for testing that property is being set\n     *\n     * @param {*} t\n     * @param {string} methodName\n     * @param {Object} options\n     * @param {*} options.param\n     * @param {*=} options.propValue Optional argument for use when value passed is not the value set\n     * @param {boolean=} options.spread If array is passed, to control spread\n     * @param {string=} options.keyName Optional override argument, default is `_.snakeCase(methodName)`\n     */\n    function setsOption(\n        t,\n        methodName,\n        { param, propValue = param, spread = true, keyName = __WEBPACK_IMPORTED_MODULE_0_lodash___default.a.snakeCase(methodName) }\n    ) {\n        const value = Array.isArray(param) && spread\n            ? getInstance()[methodName](...param).toJSON()\n            : getInstance()[methodName](param).toJSON();\n        const expected = getExpected(keyName, __webpack_require__.i(__WEBPACK_IMPORTED_MODULE_1__src_core_util__[\"recursiveToJSON\"])(propValue));\n        t.deepEqual(value, expected);\n    }\n\n    setsOption.title = (providedTitle, methodName) =>\n        !__WEBPACK_IMPORTED_MODULE_0_lodash___default.a.isEmpty(providedTitle) ? providedTitle : `sets ${__WEBPACK_IMPORTED_MODULE_0_lodash___default.a.snakeCase(methodName)} option`;\n\n    return setsOption;\n}\n\n\n/***/ }),\n/* 3 */\n/***/ (function(module, exports) {\n\nmodule.exports = require(\"../_src/core/util\");\n\n/***/ }),\n/* 4 */\n/***/ (function(module, exports) {\n\nmodule.exports = require(\"lodash\");\n\n/***/ }),\n/* 5 */,\n/* 6 */,\n/* 7 */,\n/* 8 */,\n/* 9 */,\n/* 10 */,\n/* 11 */,\n/* 12 */,\n/* 13 */,\n/* 14 */,\n/* 15 */\n/***/ (function(module, __webpack_exports__, __webpack_require__) {\n\n\"use strict\";\nObject.defineProperty(__webpack_exports__, \"__esModule\", { value: true });\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_0_ava__ = __webpack_require__(0);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_0_ava___default = __webpack_require__.n(__WEBPACK_IMPORTED_MODULE_0_ava__);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_1__src__ = __webpack_require__(1);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_1__src___default = __webpack_require__.n(__WEBPACK_IMPORTED_MODULE_1__src__);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_2__macros__ = __webpack_require__(2);\n\n\n\n\n__WEBPACK_IMPORTED_MODULE_0_ava___default()(__WEBPACK_IMPORTED_MODULE_2__macros__[\"setsAggType\"], __WEBPACK_IMPORTED_MODULE_1__src__[\"AvgAggregation\"], 'avg');\n\n__WEBPACK_IMPORTED_MODULE_0_ava___default()('constructor sets field', t => {\n    const value = new __WEBPACK_IMPORTED_MODULE_1__src__[\"AvgAggregation\"]('my_agg', 'my_field').toJSON();\n    const expected = {\n        my_agg: {\n            avg: {\n                field: 'my_field'\n            }\n        }\n    };\n    t.deepEqual(value, expected);\n});\n\n\n/***/ })\n/******/ ]);\n```\n\n1st run time - 3m18.812s. Next run - 2m31.209s\nPerformance wise, single entry test file is the best.\nAlthough this is too complicated to setup and introduces too many restrictions on file structure, \nI think this could be integrated into AVA, enabled with an optional flag.\nEssentially, AVA has to run the src files through babel to some other path, \nmaybe into the cache dir(?) and replace src with the new path.. > @danny-andrews that makes sense. @sudo-suhas?\n@novemberborn Initially I did agree with the suggestion. However, I am not so sure now.\nAll the various approaches we have discussed come with their own but. However, this discussion itself can be very useful for somebody trying to setup precompilation. So why not add a paragraph summarising the discussion and linking back to this thread? . I have updated the PR but it might be a bit too long. Let me know if you think we can drop any of the discussed approaches or trim anything. Markdown link. @novemberborn Thanks for taking the time to review.. This issue does not occur on my system anymore. Did not do any changes. I already had updated the limit for maximum files that can be watched:\n\u279c tail -2 /etc/sysctl.conf  \nfs.inotify.max_user_watches=524288\nI will update here if I face the issue again.. According to the webpack docs, the target directory for all output files must be an absolute path (use the Node.js path module). You can see this in the code example here - https://webpack.js.org/configuration/ . >  I'm not sure why the entry here is src/tests.js\nPerhaps the entry should be changed to tests/index.js? \n\nHow are you using this?\n\nI am using this in elastic-builder. You can see the improved test duration here - https://travis-ci.org/sudo-suhas/elastic-builder/builds.\n\nPerhaps the globbing approach can be mentioned in the paragraph that follows the example code, with a link to the Stack Overflow post?\n\nI wasn't entirely sure about how to present it. I am okay with this. Shall I push a commit for this?. Not exactly.. It is the combined time for execution in 3 node envs(in parallel) .. But the test duration did go down from nearly 13 mins of 100% resource usage to under 3 mins in one run. \nDo you think it would be more appropriate for this recipe to discuss multiple test files transpilation  by default? I understand it can also be used for cases where you have aliases too. But to me it seems like the former is the more prominent use case. . > This seems distracting:\nThought it would be better to acknowledge the difficulty of precompiling and mention that a better permanent solution is in the works. Would you prefer I reword it or remove it?\n\nAnd the discussion URL should have https://github.com in front of it.\n\nI thought it might be better to have relative links. And that / would go down to the root. I'll just change this to use full URL. There is also the option of using avajs/ava#1385. Let me know if you'd prefer it.. Although there is a single file that's not the same thing as having a single test. I don't mind changing this though. . > Why _src here versus _build above?\nI was thinking _build for tests and _src for source. \n\nHow would users do this precompilation?\n\nI was thinking babel-cli.. I'll change this to mention babel-cli watch mode instead of webpack. Sound good?. I too used reduce initially but felt the for method was easier to understand. The reduce returns the entryObj and not the testFiles though. I'll change it to use reduce.. Let's go with test.js. Sold on the readme.. ",
    "zhengqingxin": "@novemberborn Got it.Thanks for your guide.. ",
    "yasuf": "I'm also interested in unit testing knex + ava, will let you guys know if/when I put something together and link it here. ",
    "2rhop": "Hello\nI'm starting a new project for automating the seeding on multiples db with javascript and knex.js, please have a look at this and let me hear your recommends.\nI need some help to add more unit tests that include mock access to a db. thanks\nhere is a code example:\njavascript\n//creating and seeding process\nks.createAndSeed(userTableModel, 10).then(() => {\n    //creating the table automatically & seeding ...\n    ks.createAndSeed_close(roleTableModel, 10, (table) => { //closes the connection after process\n        //creating the table with knex.js fn & and seeding ...\n        table.increments(),\n            table.string('name'),\n            table.string('category'),\n            table.timestamps(true, true)\n    }).then(()) =>{\n        //do something here...\n  }\n})\nOne code to seed them all!\n . ",
    "monicabhalshankar": "Hi, I am new to contributing to here\nI would like to work on this bug \nI have got an idea of the issue that is present currently, but I might need some help testing the code after I modify it. @novemberborn can you please provide some info.. @novemberborn so far I went through the 2 files that were linked above\nMy understanding is we can modify the test.js file to generate the stack trace only when plancount and assert count are equal.\nApart from this I did not find if any other modification can be done\nIs this what is expected ? Or I should be considering some other scenarios as well?. Sorry have been busy for a while, so could not work on the issue.\nPlease feel free to give it a go\n. ",
    "Sharan-ram": "@novemberborn  i'm a beginner to open source, is this issue still open to work on?would like to make a contribution.. ",
    "javiroberts": "Hey is this already done? May I give it a go?. ",
    "sainalshah": "@novemberborn I'm a beginner, and I'd like to work on this issue.. @Jolo510 yeah sure, but how u wanna collaborate?. @Jolo510 we can have this chat on hangouts. ",
    "peterr101": "Hi, I am new to open source, but I would like to contribute here if possible?. ",
    "Ullauri": "Safe to say no one is working on this?. Sounds good. I think I understand what this request is asking for. Either way, ill open up a PR for it when I'm done.. I was looking at this when including that condition.. Agreed.. That would mean getting rid of getStack() no?. ",
    "KzjLe": "Hi @novemberborn, i'm looking forward to contribute to AVA, as of my first contribution I start work on this issue.. ",
    "duartealexf": "Is this going to be merged anytime soon? Please, I really need to pass some arguments to my tests :). ",
    "TheeRFG": "I realize this has been closed for a while. I have opened a stack overflow question, as I have come across more free time and am once again interesting in getting ava working. \nI am currently trying to get get Ava working with an angular/cli built project as a replacement for the default karma/jasmine test runner with relative ease. The steps outlined in that document do not currently work for me in my attempt to get anything working as I am greeted with \n```\nCouldn't find any files to test\nnot ok 1 - Couldn't find any files to test\n1..0\ntests 0\npass 0\nfail 1\n```\nNo matter what I put in my ava files directive in my package.json. \nIt's also not clear how to split up the tests into multiple files or how to adapt ava to the currently standard Angular practice of running multiple *.spec.ts test files scattered throughout the project.  \nIf you can assist me in setting this up, I would be happy to add to the documentation, but as of currently all of my efforts to implement Ava are failing and I'm not sure how to get them to succeed. . > \n\n@TheeRFG @ShinDarth I know it's been a while since the question was asked, but I updated SO with the Typescript setup I've been successful with: https://stackoverflow.com/a/52806849/2989405\n\ninteresting! I know this is quite old, but thank you, I may look at this!. ",
    "snarfed": "hey @zellwk what was your solution to this? i have the same problem with ava + express + supertest, or at least similar. it's fine when tests pass, but when there's an uncaught exception in async code, it hangs forever.. ",
    "afenton90": "I'm also finding AVA way too slow on larger projects.\nEspecially when combined with Babel.\nAlthough a lot of AVA features are awesome, the performance is crippling on larger projects. The lack of feedback on the progress of https://github.com/avajs/ava/projects/1 is also pretty frustrating and until then these issues (relating to performance) will keep cropping up.\nThere needs to be one issue where the main performance issues are made clear, at the moment, there seems to be no clear direction or strategy for resolving these issues.. ",
    "coderas": "\ud83d\udc4d Considering we initially adopted AVA for the speed benefits, these problems are a real concern for us - we may even have to migrate our tests back to mocha https://github.com/jfmengels/mocha-vs-ava-performance\nThat would be a sad day, since the principles of this project are spot on IMO. ",
    "tomheadifen": "@wmertens To confirm this fixed the issue for me too. Went from about 8 seconds to run my tests to around 500ms.. ",
    "dhershman1": "Food for thought in the meantime, I swapped out the babel-register module in favor of the esm module and it dropped my time from about a minute and a half to 20-25 seconds. \nMy library is a heavy ESM module (import/export everywhere) with ~280 unit tests, this is a tremendous win for me in terms of run time on tests.. ",
    "CMCDragonkai": "The ./test/.babelrc still doesn't work.\nIt also doesn't work, if I just specify babel settings inside package.json like:\n\"ava\": {\n    \"require\": [\n      \"babel-register\",\n      \"babel-polyfill\"\n    ],\n    \"babel\": {\n      \"presets\": [\"env\"]\n    }\n  },\nAlthough this time it complains that it doesn't understand the token import.\nNo matter what, it only works when I have /.babelrc at project root.. Main issue is that I want to isolate the babel configuration for tests\ndifferent from my build and dev. For building, I use rollup and it allows\nme to specify babel config in rollup.config.js. I can bring in babel\nplugins for resolving and compiling and finally bundling the output. So\nwhat's the best way to have an ava specific babel config that also brings\nin babel register and babel polyfill?\nOn 25 Jun 2017 7:06 PM, \"Mark Wubben\" notifications@github.com wrote:\n\nThere's two separate issues at play here. AVA needs to compile the tests.\nSince you're requiring babel-register that will be used to compile any\nsources you may require.\nAVA resolves its Babel config starting at package.json#ava.babel.\nSeparately, babel-register resolves it from .babelrc (or\npackage.json#babel if the .babelrc file is missing).\nThe error in your original post came from AVA not being able to resolve\nits Babel config. Your current problem is probably babel-register not\ndoing any compilation since it doesn't have any config.\n\nThis is pretty hard to resolve without seeing your actual project though.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/avajs/ava/issues/1419#issuecomment-310891397, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAnHHaXVLc9T9yw1r5vvLEU_yYGp0wQNks5sHiMEgaJpZM4OB42U\n.\n. Ok, what about the extends property, it's still not finding the alternative location? Or is it because babel-register doesn't use the extends property and still looks for a .babelrc at project root?. \n",
    "nodkz": "@OmgImAlexis @zellwk \nSorry guys for the late response. \nYou may test mongoose in several ways:\n- As described in README for JEST example\n- Or something complex via mocking mongoose module. Like it done in graphql-compose-mongoose:\n  - mock mongoose just mock connect method and adds callback on disconnect for shutdowning mongo server.\n  - user model used mocked mongoose module to define schema/model\n  - test1 - beforeAll(() => UserModel.base.connect()); calls only once mocked connect method per test file, and afterAll(() => UserModel.base.disconnect()); for disconnecting mongoose and shutdowning server and freeing event loop for correct test exit\n  - test2 - same operations.\nThis tests works in parallel. Each test file has own mongodb server.\n\nAlso it should work for your case, where you use a dedicated database for every test case in one test file. If not, please provide test repo i try to figure out what is the problem.\n\nI see following disadvantages in your example:\n- low performance it's not good to use for every test case dedicated server. It's ok to use one server per one test file. If you need for every test case different db:\n  - spin up one server before all tests const mongoServer = new MongodbMemoryServer();\n  - and for every test case  use different dbs via await mongoServer.getConnectionString('someDbName').\n- also it may be some internal mongoose problems if AVA runs test in event loop, not in separate processes. Cause mongoose package use globals inside and it may produce some errors when different test cases set different connection strings and tries to connect.\n- mongomem in parallel tests will download 70MB files first time for every test (it's bad for CI envs). mongodb-memory-server has internal lock file for such cases and should download binaries only once. Also you may provide required version of mongo server via new MongodbMemoryServer({ binary: { version: '3.4.4' }}); mongomem does not allow you to choose mongod version.\n. @zellwk miss your prev message. Now I'm looking your repo. Write about results today later. . Tests do not work on node 7.2.1\n\nGot following error:\n```\n$ ava --watch\n/Users/nod/www/_sandbox/ava-mdb-test/server.js:16\napp.get('/litmus', async (req, res) => {\n                         ^\nSyntaxError: Unexpected token (\n    at Object.exports.runInThisContext (vm.js:78:16)\n    at Module._compile (module.js:543:28)\n    at loader (/Users/nod/www/_sandbox/ava-mdb-test/node_modules/babel-register/lib/node.js:144:5)\n    at require.extensions.(anonymous function) (/Users/nod/www/_sandbox/ava-mdb-test/node_modules/babel-register/lib/node.js:154:7)\n    at extensions.(anonymous function) (/Users/nod/www/_sandbox/ava-mdb-test/node_modules/require-precompiled/index.js:16:3)\n    at Object.require.extensions.(anonymous function) [as .js] (/Users/nod/www/_sandbox/ava-mdb-test/node_modules/ava/lib/process-adapter.js:100:4)\n    at Module.load (module.js:488:32)\n    at tryModuleLoad (module.js:447:12)\nat Function.Module._load (module.js:439:3)\nat Module.require (module.js:498:17)\nat require (internal/module.js:20:19)\nat Object.<anonymous> (/Users/nod/www/_sandbox/ava-mdb-test/test/litmus.spec.js:2:1)\nat Module._compile (module.js:571:32)\nat extensions.(anonymous function) (/Users/nod/www/_sandbox/ava-mdb-test/node_modules/require-precompiled/index.js:13:11)\nat Object.require.extensions.(anonymous function) [as .js] (/Users/nod/www/_sandbox/ava-mdb-test/node_modules/ava/lib/process-adapter.js:100:4)\nat Module.load (module.js:488:32)\nat tryModuleLoad (module.js:447:12)\nat Function.Module._load (module.js:439:3)\nat Module.require (module.js:498:17)\nat require (internal/module.js:20:19)\nat Object.<anonymous> (/Users/nod/www/_sandbox/ava-mdb-test/node_modules/ava/lib/test-worker.js:61:1)\nat Module._compile (module.js:571:32)\nat Object.Module._extensions..js (module.js:580:10)\nat Module.load (module.js:488:32)\nat tryModuleLoad (module.js:447:12)\nat Function.Module._load (module.js:439:3)\nat Module.runMain (module.js:605:10)\nat run (bootstrap_node.js:420:7)\nat startup (bootstrap_node.js:139:9)\nat bootstrap_node.js:535:3\n\n```\nOn 8.2.0 works perfectly. I think that NOTE is required in docs about minimal node version.\nFix mongoose deprecationWarning: open()\nFollowing console warning:\n(node:5017) DeprecationWarning: `open()` is deprecated in mongoose >= 4.11.0, use `openUri()` instead, or set the `useMongoClient` option if using `connect()`\n or `createConnection()`. See http://mongoosejs.com/docs/connections.html#use-mongo-client\nWas fixed in the beggining of this week (needs to install the last mongoose version 4.11.3). Also it's required small config changes:\n```diff\n// handlers/mongoose.js\n- mongoose.connect = async (uri) => {\n+ mongoose.connect = async (uri, opts) => {\n-   originalConnect.bind(mongoose)(uri)\n+   originalConnect.bind(mongoose)(uri, opts)\n// test/litmus.spec.js\n- mongoose.connect(await mongod.getConnectionString())\n+ mongoose.connect(await mongod.getConnectionString(), { useMongoClient: true })\n```\nFor more info you may read docs and may see this issues #5423, huge #5399\nParallel tests inside one file do not work with one mongoose model. And never will not work.\nTests cases inside one file when runs in parallel uses the same proccess (checked it with console.log(process.pid)). So it means that all test cases work via EventLoop and on top of context used THE SAME mongoose module (THE SAME mongoose model USER). So in current setup it is impossible to test mongoose in parallel with different connections. Connection will be one and the same. So you got a dubplicate key error 11000.\nHow to solve it:\n1. DO NOT USE parallel tests inside one file [RECOMMENDED]. You always may split your tests in several files for which ava will use different process for every file (tested with your repo and it works perfectly). So you will not meet with mongoose globals restrictions.\n\nCreate one more mock for User model. If you want to ran in parallel one mongoose schema with different connections you must create two mongoose models:\n```js\nimport mongoose from 'mongoose';\nimport MongodbMemoryServer from 'mongodb-memory-server';\n\nmongoose.Promise = Promise;\nconst mongoServer = new MongodbMemoryServer();\nconst connections = {\n  conn1: mongoose.createConnection(),\n  conn2: mongoose.createConnection(),\n  ...\n  ...\n};\nconst mongooseOpts = {\n  server: {\n    promiseLibrary = Promise;\n    auto_reconnect: true,\n    reconnectTries: Number.MAX_VALUE,\n    reconnectInterval: 1000,\n  },\n};\nmongoServer1.getConnectionString('server1_db1').then((mongoUri) => {\n  connections.conn1.open(mongoUri, mongooseOpts);\n  connection.once('open', () => {\n    console.log(MongoDB successfully connected to ${mongoUri});\n  });\n});\nmongoServer1.getConnectionString('server1_db2').then((mongoUri) => {\n  connections.conn2.open(mongoUri, mongooseOpts);\n  connection.once('open', () => {\n    console.log(MongoDB successfully connected to ${mongoUri});\n  });\n});\n...\n...\nexport default connections;\n// somewhere in other file\nimport { Schema } from 'mongoose';\nimport { conn1, conn2 } from './file_above';\nconst userSchema = new Schema({\n  name: String,\n});\nexport default {\n  UserForConnection1: conn1.model('user', userSchema),\n  UserForConnection2: conn2.model('user', userSchema),\n  ...\n}\n``\nIt can be simplifed. But stays still complex for test. You should somehow mockUserthat for first test case it gotUserForConnection1, and for second test caseUserForConnection2, for third test...` and so one. I'm crying.\nSo my decision: DO NOT USE PARALLEL TESTS INSIDE ONE FILE FOR ONE MONGOOSE MODEL. You may split huge test into several files.. @zellwk good write up \ud83d\udc4d\n```js\nimport {before, beforeEach, afterEach, after} from './utils'\ntest.before(before)\ntest.beforeEach(beforeEach)\ntest.afterEach.always(afterEach)\n```\nTry to reduce this code a little bit more in such manner:\n```js\nimport { prepareMongoose } from './utils';\nprepareMongoose(test);\n// ------utils.js--------\nprepareMongoose(test) {\n  test.before(before)\n  test.beforeEach(beforeEach)\n  test.afterEach.always(afterEach)\n}\n```\nI do not sure will it work in AVA, but if so it makes tests nicer.. @zellwk try new mongodb-memory-server@1.4.0 it imports regenerator-runtime explicitly in its code, so I suppose now tests should work without babel-polyfill.. In mongodb-memory-server@1.4.1 ship some changes with babel-polyfill. \nNow your tests may work without babel-polyfill and regenerator.\nhttps://github.com/nodkz/mongodb-memory-server/issues/9#issuecomment-323044701. Firstly I'm using mongomem too, but when met with its downsides was writtenmongodb-memory-server.\nmongomem downsides:\n- does not have lock for downloading binary (so when you start in parallel several tests from scratch, bins also will be downloaded several times).\n- does not allow to choose mongodb version and bunch of other options.\nFull list of options for mongodb-memory-server:\njs\nconst mongod = new MongodbMemoryServer({\n  instance?: {\n    port?: ?number, // by default choose any free port\n    dbPath?: string, // by default create in temp directory\n    storageEngine?: string, // by default `ephemeralForTest`\n    debug?: boolean, // by default false\n  },\n  binary?: {\n    version?: string, // by default '3.4.4'\n    downloadDir?: string, // by default %HOME/.mongodb-prebuilt\n    platform?: string, // by default os.platform()\n    arch?: string, // by default os.arch()\n    http?: any, // see mongodb-download package\n    debug?: boolean, // by default false\n  },\n  debug?: boolean, // by default false\n  autoStart?: boolean, // by default true\n});. See User and UserOnServer2 models constructed on one schema with different connections in the following example:\nhttps://github.com/nodkz/mongodb-memory-server#several-mongoose-connections-simultaneously\n. ",
    "cyberwombat": "Hi all. I've written a module to handle parallel testing with Mongo. It's in early test but please check it out. https://github.com/cyberwombat/mongoprime. ",
    "kayceeingram": "Yeahh I noticed that as soon as I sent the pull\nRequest. My bad.\nOn Mon, Jun 26, 2017 at 1:05 AM Kaycee Ingram kayceeingram33@gmail.com\nwrote:\n\nOn Mon, Jun 26, 2017 at 1:03 AM Vadim Demedes notifications@github.com\nwrote:\n\n@vadimdemedes commented on this pull request.\nIn readme.md\nhttps://github.com/avajs/ava/pull/1422#discussion_r123932226:\n\n@@ -376,7 +376,7 @@ test(t => {\n\n### Running tests serially\n-By default tests are run concurrently, which is awesome. Sometimes though you have to write tests that cannot run concurrently.\n+Tests are run concurrently by default, however, you sometimes have to write tests that cannot run concurrently.\nyou sometimes => sometimes you?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/avajs/ava/pull/1422#pullrequestreview-46178650, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AYfEG5J85DuCJe4_BoIiq5ogQ0oISs38ks5sH0nNgaJpZM4OEufr\n.\n\n\n. \n",
    "ajtorres9": "Hey @sindresorhus I looked into the issue. The errors are occurring due to the introduction of strict checking of function call arity in 'flow-bin@0.47.0'. The issue should be resolved with the pull request that I have submitted.. Hey @novemberborn I updated the feature branch to accommodate your requested changes. Thanks for the feedback. \ud83d\udc4d . Hey @novemberborn when you say we should run test on 4.3, should that be in addition to running tests on the latest 4.x release (which is what Travis CI currently does) or should the .travis.yml file be modified to specify 4.3 as opposed to 4?. Glad I could help @novemberborn \ud83d\ude04 . ",
    "norama": "Almost solved the problem, but gave a ReferenceError: regeneratorRuntime is not defined error so I had to use this as well:\nhttps://stackoverflow.com/questions/33527653/babel-6-regeneratorruntime-is-not-defined \nI ended up with this config (plus the necessary dev-deps according to the above post):\n\"ava\": {\n    \"require\": [\n      \"ignore-styles\"\n    ],\n    \"babel\": {\n      \"babelrc\": false,\n      \"presets\": [\n        \"es2015\",\n        \"stage-0\",\n        \"react\"\n      ],\n      \"plugins\": [\n        [ \"transform-runtime\", {\n           \"polyfill\": false,\n           \"regenerator\": true\n          } \n        ]\n      ]\n    }\n  }. ",
    "davidnagli": "It didn't work with watch mode. . ",
    "oba2cat3": "That might be so - issue #1382 is indeed releated to this. But this is not the issue I refered to.\nThe documentation is wrong at the moment - while I agree that the Team cannot resolve this from a Code/Technical angle - updating the Documentation can help users of Ava and IntliJ IDEs.\nThis is especialy true when you google \"webstorm ava debug\" - and that brings you to a misleading documentation.\nFix the documentation with a simple solution, until InteliJ fixes cause on thier side.. ",
    "suchmaske": "@novemberborn I recently have changed my workflow to vue-test-utils. I will take the feedback and try to rework the PR in the next days. . @novemberborn I recently have changed my workflow to vue-test-utils. I will take the feedback and try to rework the PR in the next days. . ",
    "nowells": "@novemberborn thanks for the review! I will:\n\n[x] Switch from comment to log\n[x] Add log output to mini reporter on failure\n[x] Add tests for t.log\n\nI should hopefully have this up to you for review by the end of the day. Cheers!. @novemberborn let me know if my rework looks good, and if you think there are other types of tests that I should add. Thanks!. > Can you document it in the readme?\nAbsolutely! Done.\n\nHow would that work? The comments would need context.\n\nThe way I implemented it, in mini reporter the log messages show up in failure cases (arguably the only place you really care to see them in that context) but in success cases I do not display them.. The tests that are failing appear to be failing on master as well?. thanks for the great feedback and guiding me through this @novemberborn! . @sindresorhus thanks for making such a wonderful testing system!!! I look forward to helping out on other features as well. :heart:. This whole stream of changes looks more noisy than it is. This is simply switching from an immediate return approach to appending to a lines.push approach so that we can concatinate comments after the test summary.. Good catch! I'm going to make it so that it will result in something like this, where I add the i figure to the first line.\n\u2714 file-one \u203a passing test\n    \u2139 a comment for passing test\n      with a newline\n    \u2139 a comment for passing test without a newline. Done. @novemberborn the reason I didn't do that was I only wanted to i symbol to show up once per log line (to indicate where a log message starts)\n* this is a message that will have multiple lines, but there only the first line\n      will have the `*` to indicate a log message, and the indent of the other lines\n      is at the indent level of the first log message (after the `*`)\n    * Some other log message\nDoes that make sense? Or would you rather it be\n* this is a message that will have multiple lines, but there only the first line\n    * will have the `*` to indicate a log message, and the indent of the other lines\n    * is at the indent level of the first log message (after the `*`)\n    * Some other log message, but how do I know that.. ",
    "jottr": "There's a bunch of tools specific to API documentation, e.g. swagger and the likes. \nI'd love to help improve the documentation. \n@sindresorhus do you have any preference? \n. ",
    "nklayman": "What about vuepress? It supports markdown -> HTML, is open source, has a zero-config build step, and can be deployed anywhere.. What about vuepress? It supports markdown -> HTML, is open source, has a zero-config build step, and can be deployed anywhere.. If you want, I'd be happy to convert the docs into separate pages for each h2 using vuepress and submit a PR.. If you want, I'd be happy to convert the docs into separate pages for each h2 using vuepress and submit a PR.. I converted docs to vuepress in my fork. just run npm run docs:dev to start dev server (w/ hot reload). I still need to configure the deploy command to handle versioning (and actually deploy somewhere). I assume you want to use github pages? I wanted to make sure you liked the look/style before setting up the deploy pipeline. There are also a few locations where I'm not sure what to write (home and recipes index).. What about publishing under a different url every time, ie docs.ava.io/v1/, docs.ava.io/v2? You could then create a custom drop-down that would redirect to the correct page. Each url would be its own vuepress app, so search would work fine. There would need to be a way to retrieve new versions from an old page, however.. Does anyone know how to get just the release tag? https://api.github.com/repos/avajs/ava/releases gives you a lot of unnecessary info.. How should I authenticate with the API?. Since the graphql api forces use of an authentication token, maybe we should just host a json file with the release data ourselves, as part of the docs site.. Then your access key would be publicly available on the site. I can use a postversion script to automate updating the json file, so it won't be an issue.\n. Updated my fork to support versioning. It requests the releases.json, located in the public folder. It creates a dropdown called \"versions\" and links each version. The link redirects to /${version}. I also added a \"version\" script which adds the latest release to the releases.json. All that's left is to publish each version of the docs to /${version}.. https://github.com/avajs/ava/pull/2051. Those help, but a simple helper function makes them even quicker to use:\n```javascript\nimport test from 'ava'\nfunction macro(t, input, expected) {\n  t.is(eval(input), expected)\n}\ntest('2 + 2 = 4 (macro)', macro, '2 + 2', 4)\ntest('2 * 3 = 6 (macro)', macro, '2 * 3', 6)\nfunction each1(scenarios, name, testFn) {\n  scenarios.forEach(params => {\n    let testName = name\n    params.forEach(param => {\n      // TODO: proper interpolation\n      testName = testName.replace('%s', param)\n    })\n    test(testName, testFn, ...params)\n  })\n}\neach1([['2 + 2', 4], ['2 * 3', 6]], '%s = %s (each1)', macro)\nfunction each2(scenarios) {\n  return function(name, macro) {\n    scenarios.forEach(params => {\n      let testName = name\n      params.forEach(param => {\n        // TODO: proper interpolation\n        testName = testName.replace('%s', param)\n      })\n      test(testName, macro, ...params)\n    })\n  }\n}\neach2([['2 + 2', 4], ['2 * 3', 6]])('%s = %s (each2)', macro)\noutput:\nyarn run v1.13.0\n$ /home/noah/Documents/HTML/ava-test/node_modules/.bin/ava -v\n\u2714 2 + 2 = 4 (macro)\n  \u2714 2 * 3 = 6 (macro)\n  \u2714 2 + 2 = 4 (each1)\n  \u2714 2 * 3 = 6 (each1)\n  \u2714 2 + 2 = 4 (each2)\n  \u2714 2 * 3 = 6 (each2)\n6 tests passed\nDone in 0.82s.\n```\nWhat do you think about including one of these?. It might need some updated to accept different types of args.. The idea would be to add one of these functions as test.each. The goal is to make it really easy to test similar scenarios, so you aren't using array.forEach, writing multiple tests, or something similar.\nWithout test.each:\n```js\ntest('2 + 2 = 4', macro, '2 + 2', 4)\ntest('2 * 3 = 6', macro, '2 * 3', 6)\ntest('2 - 2 = 0', macro, '2 - 2', 0)\ntest('4 / 2 = 2', macro, '4 / 2', 2)\ntest('2 * 16 = 32', macro, '2 * 16', 32)\ntest('2 + 8 * 2 = 18', macro, '2 + 8 * 2', 18)\n```\nwith test.each: \njs\ntest.each(\n  ['2 + 2', 4],\n  ['2 * 3', 6],\n  ['2 - 2', 0],\n  ['4 / 2', 2],\n  ['2 * 16', 32],\n  ['2 + 8 * 2', 18]\n)('%s = %s', macro). It also has a version script that will update the releases.json automatically.. ",
    "hiroppy": "@novemberborn Thank you for your feedback. I saw ava's coverage.(using Istanbul)\nI got the following results on my PC.(console output is the same)\nThis is master branch.\n\nIt's strange.\ud83e\udd14\n. @novemberborn hmmm... ok, I'll close this PR. Thanks!. @novemberborn ok, I'll modify the test code;). The feature/add-test-for-improper-usage-messages branch was force-pushed or recreated.\nso, I can not reopen this PR.\nSorry, I'll create new PR.... @novemberborn PTAL. @kevva fixed;). ",
    "edouard-lopez": "@rijkvanzanten could you describe how you solved this ?. Fixed it with:\nnpm uninstall ava\nnpm install --save-dev ava.\n",
    "mariusGundersen": "I just ran into this same issue, and want to +1 either option (having it find the original .ts file, or having a snapshot-location configuration option). . Does its need to read the sourcemap file on every test? Isn't it using the same sourcemap that the transpiled test is using? \nI'm OK with a cli arg (as long as it's also a package.json config) . ",
    "ivikash": "Thanks for your response, in my case I have an object and within the object, this is happening for a certain property. \nUsage\njavascript\nt.deepEqual(results, [\n    {\n      ...state,\n      start: {clientX: 10, clientY: 15},\n      move: {clientX: 15, clientY: 15},\n      carouselX: 0,\n      selected: 0,\n      direction: Direction.NONE\n    }\n  ])\nAnd depending upon the direction we have to do certain computations. In such a scenario carouselX gets computed to -0. So using t.true(-0 === 0) is not possible\nAlso, this was working in the earlier version --> 0.19.1\n. @novemberborn  Its completely meaningful because that's used to decide the direction for carousel and screen and also the animation. Also this was working in the previous version. ",
    "DonNicoJs": "We can easily remove yarn dependency by switching to npm, but sure a blog post is cool, I will try to scavenge some time to make it (not sure how but will try). Hi Mark,\nSure I do not mind, I will write the blog post when I have some time! and\npost it here!\nOn Thu, Oct 26, 2017 at 4:33 PM Mark Wubben notifications@github.com\nwrote:\n\nClosed #1468 https://github.com/avajs/ava/pull/1468.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/avajs/ava/pull/1468#event-1312096766, or mute the\nthread\nhttps://github.com/notifications/unsubscribe-auth/AFngdn7ngF0mdRv-7OQrwd6jwF9E3Roxks5swJhXgaJpZM4Oilgn\n.\n. Hi Mark,\n\nSure I do not mind, I will write the blog post when I have some time! and\npost it here!\nOn Thu, Oct 26, 2017 at 4:33 PM Mark Wubben notifications@github.com\nwrote:\n\nClosed #1468 https://github.com/avajs/ava/pull/1468.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/avajs/ava/pull/1468#event-1312096766, or mute the\nthread\nhttps://github.com/notifications/unsubscribe-auth/AFngdn7ngF0mdRv-7OQrwd6jwF9E3Roxks5swJhXgaJpZM4Oilgn\n.\n. \n",
    "AJamesPhillips": "The prior art I can find is:\n\nJest: \"Jest will automatically define NODE_ENV as test\"\nLab: \"value to set the NODE_ENV environment variable to, defaults to 'test'.\"\n\n\"Actively\" not set for:\n\nMocha\n\nCould not find for:\n\nJasmine\nKarma\nQUnit\nEnzyme\nCucumber\ntape\n\nNot a testing framework but a framework for building front end web apps:\n\ncreate-react-app: \"By default you will have NODE_ENV defined for you...when you run npm test it is always equal to 'test'\"\n\nSo it's mixed.  I'm going to close this for now but happy to have a discussion and consider this proposal if you'd like to take it forwards.. Hey @dashersw sorry to hear you've obviously encountered hassle with this change.  We did look at prior art to see what others were doing and why.  From looking at the implementation of #1523 it would be possible to set process.env.NODE_ENV = undefined before ava and this should / might result in the existing behaviour?  Does that help?. > Object.assign, which doesn't work the way you suggested.\nObject.assign({NODE_ENV: 'test'}, {NODE_ENV:undefined}) returns {NODE_ENV:undefined}, not sure what you're seeing?\n\nso NODE_ENV= ava would simply work, which is not the case now.\n\nRun yarn add ava, make a test.js file with: \nimport test from 'ava';\n\ntest(t => {\n    console.log('NODE_ENV... \"' + process.env.NODE_ENV + '\"');\n    t.deepEqual([1, 2], [1, 2]);\n});\n\nRunning: NODE_ENV= yarn run ava gives:\nyarn run v0.27.5\nwarning package.json: No license field\n$ \"/Users/ajp/projects/fluid_dynamics/node_modules/.bin/ava\"\nNODE_ENV... \"\"\n\n  1 passed\nDone in 3.28s.\n\nSo that should be fine.  Shout if you have any problems with that @dashersw . Got it.  You can use this:\nprocess.env.NODE_ENV = undefined\nimport test from 'ava';\n\ntest(t => {\n    console.log('NODE_ENV... \"' + process.env.NODE_ENV + '\"'); // logs out:  NODE_ENV... \"undefined\"\n    t.deepEqual([1, 2], [1, 2]);\n});\n\nDoes that let log-suppress work?  Alternatively submit a fix for log-suppress.  Actually the docs also show:\nprocess.env.NODE_ENV = 'test'\nrequire(\"log-suppress\").init(console, 'test');\n\nSo it seems you should use:  require(\"log-suppress\").init(console, process.env.NODE_ENV); though you should then set NODE_ENV explicitly using NODE_ENV=whatever ava or make sure ava is imported and run before log-suppress.. Thanks all for the excellent contributions to date, much appreciated!\nAny further thoughts on this?  ~I wasn't clear on how Ava would tell it was running in a CI environment.~ You use is-ci which uses https://github.com/watson/ci-info/blob/master/index.js Got it.\n~Perhaps this behaviour of erring if no snapshot was found could be enabled by default, and to generate the new snapshot files you would have to pass the -u flag?  This would be a breaking change and would not score great on the \"self documenting options\" but I can't think of anything better right this moment.  Anyone considering better ideas?~. Hi @novemberborn , I had a quick look at the code.  I would prefer to add a flag which if set, will fail the tests if a snapshot can not be located, as opposed to only failing if on a ci environment and a snapshot can not be located.\nI just had a case now where our tests passed when they should have failed.  In this case they were run locally inside a docker container.  This would not be caught by the is-ci check proposed.  Any thoughts on this approach (they're not mutually exclusive so we could do both at some point)?  Thanks.. Yes apologies that wasn't clear @novemberborn .  Locally we normally run and update our tests outside of docker.  On our jenkins ci server we build a docker image and run the ava tests inside the container.  In this case I was running the ci build and tests locally to check it was working correctly.  It wasn't because the build step did not copy the snapshots to the right file name and location for ava to see them, so ava then generated new snapshots which would have masked a subsequent real error.\n\nif we implement the CI flag then you could set CI=1 in your container\n\nThat would work, though setting CI=1 when running a container locally is strange.  Not saying I object, just saying I think it wouldn't be obviously for others.  In our case it would be reasonable as it would sit in the script file we call on the ci server to setup, build and run the tests in the container.  So setting CI=1 inside a file like test-ci.sh would not be unreasonable.. ",
    "maxgallo": "We're using NODE_ENV=test for the tests in my team to use different .babelrc configurations. So it's a \ud83d\udc4d . I'm happy re-using the same configuration option, but I personally would avoid the */ since it's too much related with the Glob syntax.\nWhat about something more explicit like TEST_FILE_FOLDER (with some prefix/suffix), used in the same way you proposed? . ",
    "dashersw": "This is very heart-breaking. NODE_ENV is such a crucial piece that ava taking such a bold decision is simply unfathomable. There are so many use cases that change their behavior based on NODE_ENV and even on its absence, they all now break.\nI think the worst part is we can't force ava to not set it, deliberately leaving NODE_ENV empty. This breaks log-suppress, and probably many others.. @AJamesPhillips how do you mean, before ava? The implementation uses Object.assign, which doesn't work the way you suggested. If you set NODE_ENV to undefined assign will overwrite it. 'NODE_ENV' in process.env would be a better check for the existence of the key, even if it's undefined, so NODE_ENV= ava would simply work, which is not the case now.\nBut apart from these, why the need for such a feature? Apparently ava itself doesn't make use of this feature. Why was this approach with many implications chosen over simply typing NODE_ENV=test ava instead of ava? Setting such an important environment variable without actually using it is a very interesting choice.. That unfortunately sets NODE_ENV to an empty string, such that process.env.NODE_ENV === '' is true. But this is different than NODE_ENV === undefined, so in this case, log-suppress still doesn't work.. ",
    "dwqs": "I maybe fault, I put the config into package.json and it work.\n// package.json\n\"ava\": {\n    \"babel\": {\n      \"presets\": [\n        \"es2015\",\n        \"stage-0\"\n      ],\n      \"plugins\": [\n        \"transform-runtime\",\n        \"transform-decorators-legacy\"\n      ]\n    }\n  },\nI close it.. ",
    "mliou8": "hey @novemberborn . I read through the ticket and I'd like to try and make an attempt at it if that's all right. . @novemberborn I'd like to take a try at this if that's all right.. hi @novemberborn ! I think i'm pretty close, but I'm having trouble getting the assertion to pass correctly. Part of the problem is I'm unsure of how to actually test what the error I'm throwing is. \nConsole.log and process write seem to be disabled, is there another way to find out what I'm throwing as an error? . @novemberborn thanks, that was helpful. I opened a PR here: https://github.com/avajs/ava/pull/1514. @novemberborn Hey Mark - this is what I've got so far for the integration tests. I'm not sure why there's so many commits? Hopefully this is what you meant in the issue discussion.. @novemberborn  Hey mark thanks for the comments! I've pushed up some changes, please let me know if there's anything else I should change.. ",
    "russel": "It's the npm that comes as standard with Debian Sid. Bug report duly entered. It is bizarre that they have nodejs 6.11 but npm 1.4.21 instead of at least 3.10.10.\nnpm install -g npm\n\npulls in 5.3.0. Ava duly installed.. @sindresorhus @ORESoftware OK, I believe I have a reasonable version of Node (the Debian nodejs package is 6.11, but nodejs reports itself as version 7.10.0) and a reasonable npm using the bootstrap from the ancient Debian version, as indicated earluer. I installed Ava globally, i.e.\nnpm install -g ava\n\nand that seems to have worked. However the test test:\n```\nimport test from 'ava';\ntest('something', t => {\n    t.true(true);\n});\n```\nleads to:\nError: Cannot find module 'ava'\n\nso I am guessing that globally installed modules are not on the default search path.\n(Yes, as you can tell I am a total beginner at JavaScript, ES6, Node, etc. Sorry.). I had the file something.js in the directory test, and typed ava in the directory containing test.. @novemberborn This would seem to imply that global installs are actually useless: if the global location is not on the package search path, only the per project location is, what is the point of the global location.\n(You'll have to excuse what is clearly a \"newbie\" Node/JavaScript/NPM question \u2013 it's because I am a \"newbie\" at this coming from years with Python, D, C++, Java, Kotlin, etc.). ",
    "jugglinmike": "Sure. Here's an example test for a \"Todo list\" application:\n```js\ntest.afterEach(t => { t.context._passed = true; });\ntest.afterEach.always(function(t) {\n    if (t.context._passed) {\n      return;\n    }\nreturn t.context.driver.saveScreenshot(t.title);\n\n});\ntest('filtering', async (t) => {\n    var driver = t.context.driver;\nawait driver.create('first');\nawait driver.create('second');\nawait driver.create('twenty-third'); // intentional test bug\nawait driver.complete(1);\n\nt.deepEqual(await driver.readItems(), ['first', 'second', 'third']);\n\nawait driver.filter('Active');\nt.deepEqual(await driver.readItems(), ['first', 'third']);\n\nawait driver.filter('Completed');\nt.deepEqual(await driver.readItems(), ['second']);\n\n});\n```\n(I've omitted details about creating the selenium-webdriver instance since\nthey do not seem relevant to the demonstration.)\nThe first assertion is violated due to an intentional test bug. Under Ava's\ncurrent behavior, the test continues to execute, driving the application\nthrough more state changes. It ultimately completes after activating the\n\"Completed\" filter. To be clear: the test is correctly reported as \"failing.\"\nThe issue here is that the web browser under test is no longer in a state that\ndemonstrates the test failure, so the screenshot captured in the\nafterEach.always hook is not useful.\nBy changing the three invocations of t.deepEqual to Node.js's\nassert.deepEqual (for example), the first violation throws an exception, the\nflow of the test is interrupted, and the afterEach.always hook is executed\nwhile the browser is still in a state that is relevant to the failure.. > Would it help if the --fail-fast flag made AVA blow up the specific test\n\nthat failed with a thrown exception? The downside is that you'd only get one\ntest failure in your CI run (#1158 notwithstanding). \n\nMy goal is to give reviewers a clear picture of the entire problem. While this\nserves that goal in one way, as you say, it detracts from it in another. I'm\nnot sure the trade-off would make that alternative preferable.\n\nAnother approach might be to decorate test() and wrap the t object,\nassuming we expose the test state on it. That way you can build your desired\nbehavior on top of AVA.\n\nI initially prototyped something like this, but I'm not comfortable maintaining\nsuch a tight coupling with Ava's API. If Ava provided a more formal hook (e.g.\nemitting an event for every assertion made), then I could implement this\nfunctionality on my own without worrying about problems between major\nreleases... But that feature seems much more complex and specific to my use\ncase than simply throwing, so I wouldn't recommend it.\n\nFor the time being I'd want to avoid adding an option to make these\nassertions throw.\n\nI might be able to help more if I understood your reluctance. For the time\nbeing, I'll use a dedicated assertion library and try to avoid this problem\ninformally by requesting that my teammates avoid the Ava-provided API.. > Having the option will tempt people to enable it, even though they have no\n\nuse for it. Indeed we want to run all assertions and provide an log that is\neven more complete than we have now.\n\nDue to that specific deficiency in Ava's logging implementation, I expect many\nconsumers are factoring their assertions to only share tests in cases where\nthe assertions are inter-related. Otherwise, in the event of failure, they will\nonly receive a partial view of the extent of the problem. So it may be that\nconsumers are already using the assertions as if they threw, and that by\nextending the logs, Ava would only be giving them redundant information:\ndetails about subsequent assertions that are already understood to be\ninterdependent. In that case, the logging fix would tend to motivate more\nrequests for \"throwing\" behavior.\nBut that's clearly conjecture on my part. I'm pointing it out just in case it\nhelps you weigh the relative merits of these solutions.\n\nYou wouldn't even have to wrap each assertion method, you'd just need to wrap\nthe test implementations to intercept the t object.\n\nThis satisfies my use case. Compared to wrapping assertion methods, it also\nseems like a much more maintainable way to interface with this library. I would\nstill prefer the ability to make violation throw errors (and as mentioned,\nother users might find that useful later on), but I certainly won't complain if\nthis solution is implemented. Sorry to say that I don't have the bandwidth to\nhelp with that, though.. Sounds good to me!. > Could you have a look at why tests are failing though?\nSure thing. This was a tricky issue, but I think I have it fixed over at https://github.com/avajs/ava/pull/1524. Okay, I've pushed up another commit to remove that check. Since it reflects an intentional but subjective change, I'd prefer if we could include it in a standalone commit. That will help future contributors understand that the change in behavior was not a regression.\nThere have been some recent commits to master that cause merge conflicts in this changeset. I've taken the liberty to rebase and force-push the result. In the interest of transparency, I've made the original version of this branch available here:\nhttps://github.com/avajs/ava/compare/master...bocoup:include-anonymous-functions-in-stacktraces-orig-2. No worries; I know how things get with maintaining big FOSS projects like this. Take your time, and thanks for keeping me in the loop!. Great! And thank you!. > Sounds like there is an unexpected breaking change in perhaps chalk? If we\n\nupdate our version of chalk would that help? How does #1401 impact this\nproblem?\n\nI think I'd call it a bug fix because as I understand it, it disables colors\nwhen a TTY is not present, which generally seems expected. I don't believe that\nthe referenced issue about the --color flag is directly related because the\ntest process is being created by tap, not Ava itself.\nWith the linting problem out of the way, I can see some Windows-specific test\nfailures. It looks like there are some complexities in Chalk that need to be\nre-created in the new test helper. I submitted another \"fixup\" commit to\nimplement that. In some ways, it's unfortunate for the test logic to grow in\ncomplexity like this. In other ways, this is functionality that consumers\ndepend on, so it's probably for the best that it is explicitly guaranteed by\nthis project's tests.. My pleasure!. Yeah, this seemed a little hinky to me, too. I implemented that to satisfy an existing test which looks pretty intentional. It was introduced here: https://github.com/avajs/ava/pull/206 . Maybe @jamestalmage can give us some perspective.. Sure thing. The test demonstrates a case where a function value is thrown and not caught. Removing the test and this condition would prevent Ava from displaying the source of the function in such cases.\nGranted, it seems extremely rare for a function value to be thrown in the first place. I just want to make sure we're on the same page before proceeding. Can you verify?. ",
    "edbrannin": "I'm not sure if this is the right issue, but #1560 points here, so I'd like to add another use-case for caught assertions:\nI just wrote some business-logic assertions and unit-tests for them, but the only way I can find to say \"This input should fail its assertion\" is to mark the test as failing.  That seems against the \"temporary\" spirit of test.failing, and clutters the test output slightly (counting & naming perfectly OK tests).\nTrivial example:\n``javascript\nconst urlShouldBe = function(t, observed, expectedHost, expectedPath) {\n    t.is(observed,${expectedHost}/${expectedPath}`);\n}\ntest('urlShouldBe should fail when domain is wrong', (t) => {\n    try {\n        urlShouldBe(t, 'google.com/test', 'example.com', 'test');\n        t.fail('Should have failed this test!');\n    } catch {\n        // OK\n    }\n});\n```\n(t.throws() also didn't work as a replacement for that try-catch)\n(Having a way to add functions to the test instance t would be really helpful, too.). @kugtong33 ...and we've come full-circle to the error that prompted this issue. :). ",
    "karimsa": "This also makes tests more difficult to debug using logs. In my case, I've got all events being logged (such as jobs being executed) which continue to run even after an assertion fails. So in order to figure out which logs actually led up to the failure, I have to insert a throw or return to forcefully stop the test right after the failure - which is quite annoying.. ",
    "emilio-martinez": "@TheeRFG @ShinDarth I know it's been a while since the question was asked, but I updated SO with the Typescript setup I've been successful with: https://stackoverflow.com/a/52806849/2989405. ",
    "thakkardharmik": "I would like to take this up. @novemberborn Can you help me with getting started?. @novemberborn can you point me to some reference for writing a test?\n. ",
    "anshulwadhawan": "Remove Bluebird.longStackTraces() issue (#1492)  solved. Can someone help me with these 2 tests i.e. the Travis CI build and AppVeyor build that take place.\nWhat all do these tests depend on ?. I am new to Open Source.Can you guys help me with some of my queries:\nIs my PR merged ? or you want me to use your commit.\nPlease help me get my first PR merged !\n@novemberborn @forresst \n. ",
    "philippotto": "Sure thing, done.. ",
    "codeslikejaggars": "Yep, #1574\nOn Sun, Jan 28, 2018 at 11:52 PM Selwyn notifications@github.com wrote:\n\n@codeslikejaggars https://github.com/codeslikejaggars did you PR your\nchange? \ud83e\udd14\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/avajs/ava/issues/1501#issuecomment-361165115, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABfmhVx9z3EWn0tIe8yMziFECz1fzYEuks5tPXi7gaJpZM4PI3Ff\n.\n-- \n- Nick\n. Yep, #1574\n\nOn Sun, Jan 28, 2018 at 11:52 PM Selwyn notifications@github.com wrote:\n\n@codeslikejaggars https://github.com/codeslikejaggars did you PR your\nchange? \ud83e\udd14\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/avajs/ava/issues/1501#issuecomment-361165115, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABfmhVx9z3EWn0tIe8yMziFECz1fzYEuks5tPXi7gaJpZM4PI3Ff\n.\n-- \n- Nick\n. The same behavior affects any infix operator. Empty strings are also rendered invisible:\n\njavascript\nconst val = 'some value';\ntest('fails with multiple === expressions', t => {\n    t.false(val === '' && '' === val);\n});\n```\n  1 failed\nfails with multiple === expressions\n/Users/path/ava/test/fixture/enhanced-assertion-spaces.js:5\n4: test('fails with multiple === expressions', t => {\n   5:   t.true(val === '' && '' === val);\n   6: });\nValue is not true:\nfalse\nval===&&===val\n  => false\nval===\n  => false\nval\n  => 'some value'\n```\nOne way to resolve this would be to use babel-generator in enhance-assert.js to regenerate the code from the AST, instead of concatenating the token values.\n. Thanks @novemberborn, I added a few tests that update the node version and validate the output.. \ud83d\ude2c yep.. should probably actually execute the tests. \ud83d\ude2c yep.. should probably actually execute the tests. \ud83d\udc4d Looks good to me, thanks @novemberborn . \ud83d\udc4d Looks good to me, thanks @novemberborn . Updated the readme and added a test for after.always. . Makes sense @novemberborn . I don't think that would work. If it's an empty sequence, the skipped tests won't be reported in the runner output.. Not sure I can, see response to the comment below.. ",
    "rhendric": "So shall it be!. @novemberborn, ready to merge?. Huh. Looking at lib/test-worker.js, I do see that installSourceMapSupport is called before opts.require is handled. And yet, this fix works for my project\u2014with vanilla master AVA, I get the wrong line numbers in stack traces and error line highlighting, but with this patch, I get the right ones. I'll need some time to look into what exactly is going on here.\nsource-map-support is already in the business of tinkering with Node.js internals, so I'm fine with pushing a fix through there. But then that PR plus more PRs in babel-require (and also @babel/require, I guess\u2014huh, I should add support for that in this patch, shouldn't I?) and here to bump source-map-support version all have to land before this is fixed in AVA, and there's no guarantee that source-map-support maintainer(s) will see this as their responsibility. Can I pursue that in parallel (I agree it's the better long-term solution), and submit a patch to upgrade source-map-support and revert this patch if and when it lands?. I'm okay with that simplification, but you should know that it does change behavior\u2014that would essentially treat a titleless test the same as a test with an empty string title, so -m '*' would truly include all tests, even the titleless ones, and there would no longer be an escape hatch to go back to the old behavior. I was trying to lean towards making this PR as conservative as possible, but I can't think of a real reason to want to distinguish between anonymous tests and empty title tests, so if you're fine with that then I'll make the change.. ",
    "edenworky": "I didn't know about macros! That's cool.\nHowever, it doesn't solve my issue. I want to inherit tests. A macro allows a shorter way to define a test. I'm looking for polymorphism, not simply reuse. This would require me to register every superclass test for every derivative.. ",
    "sabrehagen": "I came here to make the very same request. If the parent ava process is passed --inspect=host:port it could pass it to the child process test runners. I use remote debugging which requires --inspect=0.0.0.0:9229 but the docs only support --inspect.. We'd have to change the concurrency to 1 when a specific port is provided\nYes, this sounds ideal. Can you reference the location in the code where this change would need to be made and one of us open source contributors can get to work on it?. ",
    "krisnye": "This feature is really a necessity. When launched with --inspect it should run tests one at a time and break on the first debugger statement.. Maybe, you'll take a pull request with it?. ",
    "FinnFrotscher": "whats the status of this? \n. I have switched to Mocha to get around this problem, but the solution is not satisfactory since it comes with another set of drawbacks.\nCould you point me to the right place in Ava? I have not looked much under the hood yet.. ",
    "darthgera123": "is this issue still open or is it closed?. ",
    "JavierPons": "Hi there,\nI'm interesting in help in this issue. It will be my first intervention. (Im a beginner). \nWhat concretely do you need? Where I can help in this? Or if you need help in an other issue as well Im ready to participate.. @novemberborn Im looking for some issue with code that I can make better for example or if it is a bug that I can resolve. (Maybe its very high level for me :p ) But I would like to try.. ",
    "qaraluch": "I guess, It will be handled by commit: https://github.com/Collinslenjo/ava/pull/3/commits/7a28c595a5cfe06254359629be96f181795a0937. After reinstalling npm to v. 5.4.0 it works. Thanks. . ",
    "mfainshtein2": "I would like to take a crack at this issue. Where should I start looking to find the cause of this issue?. I would like to take a crack at this issue. Where should I start looking to find the cause of this issue?. @novemberborn, Your proposed solution seems to have worked, only the test that had its .snap file deleted got rerun but it does show the following message if the other test has a mismatch. Is this intentional?\n\n. ",
    "itaisteinherz": "Since #1751 wasn't merged, can I take a shot at fixing this?. ",
    "P-Seebauer": "I think, I'm done.\nSorry about the commit/build spam, I wasn't aware that github links the branch this way.\nI'm not too happy about the Documentation part. Do you have any suggestions what could be done better?. Yeah looks much better.. ",
    "rilut": "I'm interested to work on this. Since ava always clears its console, where to add the message?. @novemberborn I'm not sure if what I did (asserting this) is the right way to do this. ",
    "KompKK": "Can I work on that?. I don't know why node 8 on windows fails.. Have you got any ideas?. ",
    "Smilebags": "I am also experiencing this issue with a rather simple example.. ",
    "johhansantana": "hey @novemberborn is there a way to run the tests file by file (I don't mind the order). Reason is, I need to be authenticated to do some tests and the authenticated user needs to be an admin. But each time I test something like this I have to clean up the user (remove it from admin role).\nThis conflicts if I have multiple tests doing the same thing ( I imagine, haven't tested yet ).\nI use test.serial in a file and it works perfect (they get executed in order) but running multiple test files, it will try to test every file at the same time.. @novemberborn wow, that's perfect, thanks!. ",
    "piercus": "@novemberborn I would need to run tests in a specific order.\nReason is : I'm tracking an issue, which is occuring sometimes (but not every time) when running my test suite with ava.\nI think it might be related to the order of test, like a test run A is corrupting another test run B.\nProblem is : i have 37 tests and even if i know test B (because it is failing) it is very hard to track test run A if the order of test run is changing.. ",
    "nordqvist": "For anybody looking for a solution for this, how i came around it was looping through the files and running the tests individually.\nfor file in {your test folder}/*; do echo '\\n Testing' $file;  ava $file || break; done\nThe ||\u00a0break is important, otherwise it will just go to the next test if you try to quit.. ",
    "Martin-Kuca": "@ORESoftware Thank you. I've looked the issue up and found a solution for my problem\n```\ntest.afterEach('this only runs when test succeeds', async t => {\n  // this is here since my eslint does not allow object mutation\n  // eslint-disable-next-line mutation/no-mutation\n  t.context = 'success'\n})\ntest.afterEach.always('clean up', async t => {\n  if (t.context !== 'success') {\n    await browser.saveScreenshot(./screenshot-${Date.now()})\n  }\n// the rest of the code runs regardless of the test succeeding\n// more of my clean up code\n})\n```\nfor anyone wondering, the issue this is discussed in is #840 . @sindresorhus Yeah, I just realized this while making coffee right after posting this. I am sorry for a probably completely unnecessary suggestion and a very long issue. \nThank you for an extremely quick response! :-). ",
    "samhatoum": "Hi\nSorry, I left this so long. I'm the author of Chimp, and I'm currently adding support for AVA in the new version. You can see this here\nOne aspect of running end-to-end tests is that they take a while. In particular, one activity that keeps a test open is debugging, where a user would use the directive await driver.debug and this opens up a console REPL interface for the user to inspect the browser context.\nIf a test framework sets the timeout to be low, then this won't work. Being able to set this at a global level would allow users to set it once and forget it. In Chimp, I'd like to set this on users behalf if they are in debug mode so they can get on with debugging without the burden of configuring tools (the purpose of Chimp).\nSome frameworks like Mocha allow this to be set using a command line\u00a0option, like --timeout 100000 and others like Jasmine allow you to set jasmine.DEFAULT_TIMEOUT_INTERVAL = 100000. I was hoping for something similar in AVA.. Many thanks @novemberborn, that helps. I like the idea of letting users know so they can deal with it\n. ",
    "Lifeuser": "@novemberborn I have done a PR for this. Check it out please. - #1568 . Label sounds good for me!\nI used it because of \"it's\". Linting style doesn't allow doublequotes.\nSo I changed your label a bit for this reason (they're ->> they are) and used regular string.\nI also moved assignments up, so we don't run .describe second time in deeply equal case.). ",
    "nesbocaj": "I have to agree with @wmertens, this is counter intuitive.\nReason being that Ava has this nifty feature $ ava --update-snapshots which at least in my mind is analogous to git commit or git push.\nWith this in mind I find it more intuitive that the diff shows what will happen if the snapshot is updated.\nWhich means the original should be denoted by a minus and in red, while the new (updated) value should be denoted by a plus and in green.\nBut that's just my opinion.\nEdit:\nLooking at the jest snapshot testing documentation it seems they're using a compromise:\n\nThe symbol (minus or plus) indicates the history of the value, minus being the original value, and plus being the new (updated) one\nThe color (red or green) indicates the (assumed) correctness of the value, red being incorrect, and green, correct\n\nThey also explicitly state which is which to avoid any confusion.\nI think this may be the best approach.. @novemberborn I'm having a hard time figuring out what the current order of the values are, so just to avoid any confusion I'm gonna lay it out below:\n- the snapshot value, colored in green\n+ the new (updated) value, colored in red\nDoing it this way will make Ava more approachable to developers coming from Jest and vice versa, as well as work towards establishing some kind of standard in the matter.. @novemberborn It is worth noting however that this way of doing it is still different from what most developers will be familiar with from eg. git diffs:\ndiff\n- the snapshot value\n+ the new (updated) value\nSo I think it would be beneficial to have some kind of introduction to the diff syntax, akin to how Jest does it:\n\nOf course the lines\n```\nReceived value does not match stored snapshot 1.\n\nSnapshot\nReceived\n```\n\ndo take up quite a bit of screen real estate and it's up to the Ava developers as to whether or not they want to make this compromise.\nMaybe it should only be shown to new developers, eg. the first 5 or so times a failed snapshot test is encountered, but I'm not sure if this is even possible.. @arteniioleg Jest should only be confusing if you assume the colors have the same purpose, they don't.\nIn Git the colors signifies changes, with green signifying added/new, and red, removed/old. Whereas in Jest the colors signify (the assumed) correctness of the value, with green signifying correct, and red, incorrect.\nIn both instances the preceding symbol signifies changes, with + signifying added/new, and -, removed/old.\nIt's also worth noting that Jest goes quite far in explaining (or at least trying to explain) this concept to the user.. ",
    "rzec-r7": "@novemberborn any update on this? I have been tripped up by the + / - difference with AVA so many times.\nI think what @nesbocaj said makes sense. For me the colors are not as important as the - / + and since snapshots are tests that are just diffing 2 values, it make the most sense to have the - be the existing value in the snapshot and the + be the new value from the test (I have not opinion on the colors, probably makes to to just match what jest does).. I found it a bit surprising (at least for the weird star that I have no idea why it is there, the line feed one at least makes sense on why it would be there).\nAs for effecting the test, no, it does not really impact it all that much (it is seems weird as I usually don't see those characters), just wanted to point this out as I was not sure if it was expected or not. Since it is intentional, I am good with it.. ",
    "01e9": "Git and markdown ```diff ...``` shows new changes with green + and deleted with red -\nJest inverted diff colors is confusing.. ",
    "maxrimue": "Not 100% relating, but I just spent so much time looking into my Unit Tests because I didn't realize what each symbol stood for in this output for a deepEqual:\n```\nDifference:\n[\n\n\nNaN,\nNaN,\n12,\n30,\n    ]\n```\n\nWhy don't we add a simple\n- received\n  + expected\nbelow? Only the symbols are not completely intuitive to me.. ",
    "motin": "Quick note: The proposed workaround does not work when testing observables, but using the timeout operator works fine.. ",
    "ahmadawais": "@sindresorhus Also let me know if you'd like me to fix it throughout your other Git repositories. \nLooking forward!. @sindresorhus Also let me know if you'd like me to fix it throughout your other Git repositories. \nLooking forward!. ",
    "loganfsmyth": "Another way to phase this is,  how do we move forward without Ava being tied aggressively to Babel 6.x, peerDep or otherwise? Once we release 7.x I'd presume that most users will start migrating to it, and Ava will likely be a big sticking point since it will try to use Babel 6, and other parts of user's build process will try to use Babel 7.\nOn the Babel side of things, we're planning to release new major versions of all of our main integration packages with\n\"peerDependencies\": {\n  \"@babel/core\": \"7.x\"\n}\nThe downside for ava being that users who don't care about Babel would get a peerDep warning about it when they install, but maybe that is alright?\n. Fair enough. As people switch to v7 you'll likely start seeing issues from users who use \"babel\": \"inherit\" since that will try to load Babel 7 plugins into Ava's Babel 6 core.\n\nWe'll have to update our config resolution to support the new .js files.\n\nI don't think I realized Ava has its own implementation of our config resolution. I'd love to talk eventually to see if any of those performance improvements could be built into Babel's core itself to avoid you needing the separate implementation.\n\nWill babel-core@latest see a v7 release, or will this solely be @babel/core?\n\nThe plan at the moment is to have @babel be Babel's new home. The only thing I've proposed, which isn't guaranteed, is that we could technically make a babel-core package that had its own peerDep on @babel/core as kind of a bridge so packages could write a peerDep on \"babel-core\": \"6.x | 7.0-bridge\" rather than making packages introduce a breaking change to remove babel-core with @babel/core, but not decided on that.. Unless I'm misunderstanding, the conflict only comes up when users use inherits for their config. Maybe at Ava could check if the version installed in the user's project is the same one used by Ava, and warn them?. @nervetattoo If you can put together a reproduction repository example, I can take a look. Definitely not what I'm expecting to happen.. I should clarify what \"works with Babel 7\" means in this context. At the end of the day, Ava has 100%  tied itself to Babel 6, so any config you pass to Ava needs to be config for Babel 6.x.\nYou could install Babel 7 locally and build your own code with it if you want, but Ava itself doesn't know how to handle Babel 7 stuff. It looks like you've either passed babel: 'inherits' or explicitly passed @babel/plugin-proposal-optional-chaining as a plugin for Ava to load, which won't work. You'd have to use something like @babel/cli before Ava runs.. @dacz It looks like that plugin is using an undocumented API from Babel 6 that was removed in v7. It looks like a fix has landed in https://github.com/power-assert-js/babel-plugin-espower/commit/e56edd188ffd4c9ceef4f77b59b3c39962355eb5#diff-b75a7345fb93215f800539fa526b9d23 but there's no version on npm with that available. Maybe worth asking them what their plan is.. @novemberborn @dacz We've definitely tried to prevent breakage. There's one issue with babel-plugin-istanbul in https://github.com/istanbuljs/istanbuljs/issues/92 that I know of that they'll have to fix. That said, if you do run into plugins that are failing, do feel free to report it to us. I have purposefully removed a few things, but I've tried to keep it to things that I honestly hoped no-one was using, like that .parse function.. ",
    "nervetattoo": "Judging by this thread, I get the impression that using babel7 should just work because ava brings its own babel dep. However, I made an attempt to use babel 7 in a project that uses ava for testing of react components, but even when using just the @ava/stage-4 babel preset and no inherits of babelrc it fails due to loose option being wrong somewhere. I didn't look further into it as I just wanted to test out babel7, and wasn't ready to invest time in issues since its still in beta. Just chiming in that it certainly didn't work out of the box at least :). ",
    "vjpr": "@loganfsmyth Here is the loose error:\nTypeError: Cannot read property 'loose' of undefined\n    at _default (/app/babel7-test/node_modules/.registry.npmjs.org/@babel/plugin-proposal-optional-chaining/7.0.0-beta.32/node_modules/@babel/plugin-proposal-optional-chaining/lib/index.js:13:32)\n    at Function.memoisePluginContainer (/app/node_modules/.registry.npmjs.org/babel-core/6.26.0/node_modules/babel-core/lib/transformation/file/options/option-manager.js:113:13)\n    at Function.normalisePlugin (/app/node_modules/.registry.npmjs.org/babel-core/6.26.0/node_modules/babel-core/lib/transformation/file/options/option-manager.js:146:32)\n    at /app/node_modules/.registry.npmjs.org/babel-core/6.26.0/node_modules/babel-core/lib/transformation/file/options/option-manager.js:184:30\n    at Array.map (<anonymous>)\n    at Function.normalisePlugins (/app/node_modules/.registry.npmjs.org/babel-core/6.26.0/node_modules/babel-core/lib/transformation/file/options/option-manager.js:158:20)\n    at OptionManager.mergeOptions (/app/node_modules/.registry.npmjs.org/babel-core/6.26.0/node_modules/babel-core/lib/transformation/file/options/option-manager.js:234:36)\n    at OptionManager.init (/app/node_modules/.registry.npmjs.org/babel-core/6.26.0/node_modules/babel-core/lib/transformation/file/options/option-manager.js:368:12)\n    at File.initOptions (/app/node_modules/.registry.npmjs.org/babel-core/6.26.0/node_modules/babel-core/lib/transformation/file/index.js:212:65)\n    at new File (/app/node_modules/.registry.npmjs.org/babel-core/6.26.0/node_modules/babel-core/lib/transformation/file/index.js:135:24)\n    at Pipeline.transform (/app/node_modules/.registry.npmjs.org/babel-core/6.26.0/node_modules/babel-core/lib/transformation/pipeline.js:46:16)\n    at CachingPrecompiler._transform (/app/node_modules/.registry.npmjs.org/ava/0.22.0/node_modules/ava/lib/caching-precompiler.js:58:24)\n    at transform (/app/node_modules/.registry.npmjs.org/caching-transform/1.0.1/node_modules/caching-transform/index.js:43:10)\n    at CachingPrecompiler.transform (/app/node_modules/.registry.npmjs.org/caching-transform/1.0.1/node_modules/caching-transform/index.js:60:17)\n    at CachingPrecompiler.precompileFile (/app/node_modules/.registry.npmjs.org/ava/0.22.0/node_modules/ava/lib/caching-precompiler.js:39:9)\n    at Api._runFile (/app/node_modules/.registry.npmjs.org/ava/0.22.0/node_modules/ava/api.js:58:33)\nFrom previous event:\n    at Api._runWithPool (/app/node_modules/.registry.npmjs.org/ava/0.22.0/node_modules/ava/api.js:243:5)\n    at _setupPrecompiler.then.then (/app/node_modules/.registry.npmjs.org/ava/0.22.0/node_modules/ava/api.js:174:17)\n    at <anonymous>. Its unfortunate that ava is so closely tied with babel and babel@6. Especially with its own config resolver. I feel its way too complex. Ava should remove all dependencies from babel, and add them to a separate package called ava-babel-6 and ava-babel-7.\nI'd really like to use the new existential soak and ava is the only blocker in my codebase.. So I got ava to run with babel@7 + @babel/plugin-proposal-optional-chaining quite easily.\nUsing {babel: inherits}...\n\nReplaced babel-core with @babel/core\n\nava/lib/caching-precompiler.js:48\nthis.babel = require('@babel/core');\n//this.babel = require('babel-core');\n\nCommented out ava/lib/caching-precompiler.js:63\n\n//inputSourceMap: getSourceMap(filePath, code),\n. ",
    "dacz": "to @vjpr solution (thanks for it, because I'm AVA man): it works but it seems to be more complex (and more babel6 wired). I wanted to use t.true and got\nUncaught Exception: utils/objectTemplateCheck.test.js\n  undefined\n  TypeError: file.parse is not a function\n    at doParse (/Users/dacz/development/sea/node_modules/babel-plugin-espower/lib/babel-assertion-visitor.js:164:27)\nFortunately t.is(got, indefined) works.. @loganfsmyth I think you are right that this is not a problem with babel, but AVA with Babel 6 too much under it's skin. I hope that AVA will sort this out, it's excellent tool.. @novemberborn @loganfsmyth thanks for info. I appreciate and I'm grateful for amazing work on Ava.\nI saw branch babel7 in repo. Is there npm beta package we can try and test or only via clone?. It doesn't work for me. I use flow so my .babelrc:\n{\n  \"presets\": [\"@babel/preset-flow\"]\n}\nand package.json\n...\n\"@std/esm\": { \"esm\": \"all\", \"cjs\": true },\n  \"ava\": {\n    \"files\": [\"**/**/*.test.js\"],\n    \"require\": [\"@std/esm\", \"@babel/register\"],\n    \"babel\": {\n      \"babelrc\": true\n    }\n  },\n...\nI tried all possible combinations in require section of ava and in @std/esm settings.\nOutside ava `@std/esm works like a charm. And w/o @std/esm ava works (with some limitations based on babel7).\n. ",
    "Droogans": "If ava keeps a cache, would it be possible to write to it and flag warnings/fail tests if a user attempts to write to the same file location twice?. ",
    "HamedFathi": "@novemberborn \nI am not familiar with ts-node/register Do you have a sample for this problem?. ",
    "yangbin1994": "in my multiple package project, each subpackage depend on the project under the root directory of ava, subpackage often appear this script: \"test\": \"../../node_modules/.bin/nyc ../../node_modules/.bin/ava --verbose --tap test/**/*.test.js\", but gives me a headache, can't seem to like babel-cli as long as in the root directory to create a new .babelrc can be applied to all child package, to make the configuration of the ava, I seem to be in all the subpackage write clear configuration of content.. ",
    "telekosmos": "Forgot to say I'm obviously using browser-env in order to mock the browser environment just as described in https://github.com/avajs/ava/blob/master/docs/recipes/browser-testing.md.. Sorry for the late view and thanks for the reply. What you tell will be subject of further discussions in our department ;-)\n. ",
    "reddysridhar53": "Can I work on this?\n  . ",
    "irahulcse": "How to do the work on this project?. ",
    "motss": "\ud83c\udf89 Thanks for merging the PR. Well done all! \ud83d\udc4f . @jdalton Sure.. ",
    "walshe": "I could press ctrl+c if I was running in my own terminal sure, but what about an automated build by circleci for example ?. not following.. I have a --timeout in there already, but I just the the exceptions that I put in the original comment above. Im still not understanding - so what is the fix/flag I need to use?. ",
    "citycide": "@novemberborn was about to try it out but it looks like babel-preset-transform-test-files#babel7  needs its deps updated because babel-plugin-espower#babel7 was merged to master and removed (https://github.com/power-assert-js/babel-plugin-espower/commit/71e1a50fb44d8308e9c4abc04b6d286a2ea3e4e3)\nUntil then npm will fail with a very unhelpful message:\nshell\nnpm ERR! code 1\nnpm ERR! Command failed: /usr/bin/git checkout babel7\nnpm ERR! error: pathspec 'babel7' did not match any file(s) known to git.\nwhile yarn is a bit more useful since it actually gave me some kind of idea where the problem was:\nshell\nyarn install v1.3.2\ninfo No lockfile found.\n[1/5] Validating package.json...\n[2/5] Resolving packages...\nerror Couldn't find match for \"babel7\" in \"_c\" for \"https://github.com/power-assert-js/babel-plugin-espower.git\".\ninfo Visit https://yarnpkg.com/en/docs/cli/install for documentation about this command.. All seems to work so far! Was up and running using new @babel plugins in tests. .babelrc.js support is something I'm looking forward to.. @novemberborn would it be premature to publish these at @next tags? That'd make it more accessible for testing so is that the plan now at some point?. @novemberborn it works :raised_hands:\ntested it on:\nnode v8.9.3, v6.12.2\nnpm v5.6.0, v3.10.10\nbabel v7.0.0-beta.34 w/ @babel/preset-stage-0 ( used pipeline and nullish coalescer in test :yum: )\nFor others who want to test, yarn didn't install the hullabaloo-config-manager dependency correctly. Once I used npm it worked fine. . ",
    "pvdlg": "That would be possible but that would make it impossible to use ava directly during development, for example to select which test to run during with ava <files_to_test>. \nI found a workaround that kind of works in the current version:\nWith the following:\n.\n+-- test/\n|   +-- integration\n|       +-- index.test.js\n|       +-- scenario-1.test.js\n|       +-- scenario-2.test.js\n```js\n// test/integration/index.text.js\nimport test from 'ava';\nimport delay from 'delay';\nimport requireGlob from 'require-glob';\ntest.before(async () => {\n  console.log('Setup start');\n  await delay(1000);\n  console.log('Setup complete');\n});\nrequireGlob(['./_*.test.js']);\ntest.after(async () => {\n  console.log('Tear down start');\n  await delay(1000);\n  console.log('Tear down complete');\n});\njs\n// test/integration/scenario-1.text.js\nimport test from 'ava';\nimport delay from 'delay';\ntest('Scenario 1 - Test 1', async t => {\n  await delay(400);\n  t.is(true, true);\n});\ntest('Scenario 1 - Test 2', async t => {\n  await delay(100);\n  t.is(true, true);\n});\njs\n// test/integration/scenario-2.text.js\nimport test from 'ava';\nimport delay from 'delay';\ntest('Scenario 2 - Test 1', async t => {\n  await delay(200);\n  t.is(true, true);\n});\ntest('Scenario 2 - Test 2', async t => {\n  await delay(300);\n  t.is(true, true);\n});\nOn `ava -v` I obtain:bash\nSetup start\nSetup complete\n  \u2714 integration \u203a index \u203a Scenario 1 - Test 2 (104ms)\n  \u2714 integration \u203a index \u203a Scenario 2 - Test 1 (200ms)\n  \u2714 integration \u203a index \u203a Scenario 2 - Test 2 (300ms)\n  \u2714 integration \u203a index \u203a Scenario 1 - Test 1 (404ms)\nTear down start\nTear down complete\n4 tests passed\n```\nif I use t.test.serial it also work as expected.\nThe problem is that if define t.before, t.after, t.beforeEach or t.afterEach hooks they are executed within the context of index.test.js. That mean a t.beforeEach defined in scenario-1.test.js will be executed before each test in both scenario-1.test.js and scenario-2.test.js. But as long as all the scenario share the same t.before, t.after, t.beforeEach or t.afterEach hooks its ok.\nSo, yes there is workarounds that are not too bad. But having such feature in the core would still be nice, even in a simpler than what I proposed.\nBut feel free to close if you think the improvement of having that in the core vs a workaround doesn't worth the effort/extra code/extra maintenance.. Thanks for the very fast answer! And the podcast!\nWhat you described is exactly what I'm doing and what I observe as a result!\nIndeed destructuring my stub out of the t.context, prevent to log the rest of the t.context.\nI'm closing, as there is no issue. Thanks again!. Have you checked cosmiconfig? It would fulfill a lot of the requirements, and the one missing might be implement over there.. I encountered the problem with https://github.com/semantic-release/npm/blob/master/test/integration.test.js\nHere is a reduced test case: https://gist.github.com/pvdlg/171b84eecd08b22da547c15c55a6163b. Analyzing a bit further I think it's related to the cwd set by AVA when calling clearModule.\nSee this other reduced case: https://gist.github.com/pvdlg/6bd01c2d66350e8bb2f25aba64ad8859\nIt's similar to the previous, but instead of having the test.js file in the test directory it's at the root of the project, so clear-module is called with clearModule('.'). In that scenario clear-module throw the error Cannot find module \\'.\\'.\nFor some reasons in the first scenario clearModule('..') doesn't throw but I'm guessing it doesn't clear the cache the module I'm testing.\n. With the second example, in the clear-module package, on this line, callerPath():\n- returns <project_root>/test.js in ava@0.25.0\n- returns <project_root>/node_modules/ava/lib/test.js in ava@1.0.1\nI can't figure out why though....\n. Thanks, that works!\nI think we can close this issue then, unless you see a reason to keep it opened?. #1841 was closed, so I opened this one. Does it means this won't be addressed?\nAny workaround to suggest when your code depends on a library on which you don't have control that throws non Error object?. That makes sense and in the case of a popular and well maintained library like postcss it's definitely possible to fix. However it's not always the case. So it might worth it to includes an option in ava to avoid \"punishing\" ava user for something they are not really responsible for.. @ai this is this test: https://github.com/pvdlg/karma-postcss-preprocessor/blob/a6ce488420ed3e7a414cb27a7687039a384b4760/test/unit.test.js#L153\nLet me know if you want a reduced test case.. Thanks a lot!. ",
    "Fmajor": "I tried the requireGlob workaround, but got \nError: All tests and hooks must be declared synchronously in your test file, and cannot be nested within other tests or hooks.\nIs there any other working workaround for the global before and  after feature?. I just use the chokidar package to watch file changes and concat all test files into a single one.\nThat's really ugly but work for me.\nHope to have the global before and after feature.... thanks, i have write anoter functiontestInitDone and put it into the before hook\nanyway, i saw the delay option in other packages with the watch feature, maybe there are conditions that people must use this feature. ",
    "ajafff": "I'm looking for a similar feature.\nMy tests are located at test/api/*.spec.js\nI already have a snapshot directory called baselines that's also used by other tests.\nNow I want to save my snapshots there using --snapshot-dir baselines. Unfortunately that creates baselines/test/api/*.spec.js.snap while I would prefer baselines/api/*.spec.js.snap.\nLong story short: I'd like to be able to replace test/ with baselines/ and leave everything else the same.\nI see that this is very hard to do right for everyone while still maintaining an understandable configuration. I just wanted to share my use case.. AFAICT this only applies to t.fail. Everything else, e.g. t.is, returns void in the success case.\nI don't know any flow, so I can't tell if there's a similar concept like typescript's never. . > Shouldn't we type everything to have never return value? Perhaps I'm missing something with when never should be used.\nThe return type never means the function has no reachable endpoint. Either because of an endless loop or by throwing an exception. That's only the case with t.fail() as it will always throw.\nA function that has a normal completion in at least one case (e.g. t.is()) but doesn't return anything of interest has the return type void.\nTooling can for example detect function calls that return never and mark everything following this statement as dead/unreachable code. That's true for t.fail() as it makes all following statements unreachable. That's not the case with t.is() and friends.. I see. I thought it would behave similar to assert.fail in other testing libraries.\nI'm closing this as the changes in this PR are not valid. Thank you for your time.. ",
    "ppatel221": "Hello, Im a student in an Open-Source development class and looking to contribute. Can i take care of this?. Ive requested a PR https://github.com/avajs/ava/pull/1625. ",
    "jgdev": "Just changed to mongodb-memory-server and it works for me.. ",
    "aknuds1": "@sindresorhus Thanks for the quick reply! The unadulterated stack trace looks like this after your suggested change:\nError: waiting failed: timeout 1000ms exceeded\n      at Timeout.WaitTask._timeoutTimer.setTimeout (/Users/arve/ava-stacktrace/bc2f21662821c5cc423e2383dd9b63b6-95c510e2794b843c50e0ec5f488559090844da3d/node_modules/puppeteer/lib/FrameManager.js:593:58)\n      at ontimeout (timers.js:466:11)\n      at tryOnTimeout (timers.js:304:5)\n      at Timer.listOnTimeout (timers.js:264:5)\nI guess the extra stack frames are coming from Node.js and should as such be filtered out? What we want, and which is missing from the trace, is the original call site (within the test)?\nGod jul! ;). Alright, thanks Sindre, I will try to figure out why no other context is captured in the stack trace. I guess it has to do with intricacies of async JS. . Also thanks for enlightening me as to the logic of Ava's stack cleaning, it's helpful to know! @sindresorhus . @backspaces I use ava as a framework for writing Node.js test scripts, it's not running in the browser. I load my web pages with Puppeteer and use its API to manipulate and query them. You can also use the Puppeteer API to execute code in the browser and obtain the results.. ",
    "coopy": "Yep, quoting the pattern solves the issue. \ud83e\udd26\u200d\u2642\ufe0f Thanks, @novemberborn! I'll add quoting globs in scripts to my list of best practices\u2026\nThere is a curious difference apart from just the file matches, though. When I quote the glob (or just run ava --verbose without a pattern), it lists the full directory structure:\n```shell\n$ ava 'src/*/.test.js' --verbose\n\u2714 modules \u203a deeper \u203a add \u203a should add two numbers\n  \u2714 lib \u203a dep \u203a should return configuration\n2 tests passed\n```\nWhen I run without the glob quoted, I just get the test description (for the test in the one matched path):\n```shell\n$ ava src/*/.test.js --verbose\n\u2714 should return configuration\n1 test passed\n```\nEither way, my issue is resolved.. ",
    "psirenny": "To add to what @pho3nixf1re said, I prefer dotfiles because they can easily be added to a .npmignore file. Consumers of my packages don't care about various config choices and don't need to download bloated package.json file.. ",
    "james-s-turner": "OK - I can take a hint ;-) I've already had a brief squirrel around the code.\nThe auto-generation code is on my laptop at the moment. It takes manual intervention to generate benchmarks. I'll get it working automatically as part of the PR.. ",
    "troysandal": "@novemberborn great point Mark, I added a new section under Debug that addresses this, thank you.. Changes look great @novemberborn and @sindresorhus thank you.. ",
    "willnode": "Oh wait, sorry I put the wrong stacktrace. That's was my experiment when leaving babel presets empty \"presets\": []. The correct stacktrace points to export default like what you get.\nMy main problem here is that I thought I can put babel config inside ava:\n\"ava\": {\n    \"require\": \"babel-register\",\n    \"babel\": {\n      \"presets\": [\n        \"@ava/stage-4\"\n      ]\n    }\n  }\nI believe that because I saw the example from this documentation. (below is what I assumed to work so far)\n\"ava\": {\n    \"babel\": {\n      \"presets\": [\n        \"@ava/stage-4\"\n      ]\n    }\n  }\nBut it still does not work, then I scroll until I found that I miss \"require\": \"babel-register\".\nTo my surprise, it still does not work. Frankly I've spend hours figuring why until I found that I just put my babel settings in the wrong place.\n\nAnyway I new here so I really sorry if my story feels weird for you. I'm here to suggest whether it's possible to put a specific babel configuration for ava (using scenario above). \n. > That section only applies to AVA's compilation of test files. Indeed it starts with the words You can override the default Babel configuration AVA uses for test transpilation in package.json.\nWhoa, really? Thanks for that. It makes sense now.\nI think we can prevent futher confusion by mentioning that in the documentation. \n. All fixed. Sorry for that.. @novemberborn looks better to me.. Done.. ",
    "jrgleason": "@novemberborn Nope...\n```\nnode --experimental-modules ./node_modules/.bin/ava ./src/test/js/webdriver/tests/sampleTest.mjs \n(node:1742) ExperimentalWarning: The ESM module loader is experimental.\n\u2716 Couldn't find any files to test\nnode --experimental-modules ./node_modules/.bin/ava ./src/test/js/webdriver/tests/sampleTest.js \n(node:1744) ExperimentalWarning: The ESM module loader is experimental.\n\u2839 (node:1745) ExperimentalWarning: The ESM module loader is experimental.\n1 test passed\n```\n. ",
    "gisderdube": "Unfortunately, to require a module, the given module has to be a commonJS module (module.exports), since commonJS modules are loaded synchronously, whereas ES6 modules are loaded asynchronously. Since the MongoDBMemoryServer is declared as a ES6 module, it is not possible to just require() it.\nThe reason why I'm writing that here is that the guide in this repo explicitly recommends using the MongoDBMemory Server. I am just wondering how the people wh o wrote that guide handled the situation.\nA more general thing: When transpiling scripts with babel, the stack traces are correct (when configured correctly /w sourcemaps). Here I don't know why the stack trace is off.. Thanks a lot. Having this insight solved my issue. For anyone having this issue:\n```\nconst MemoryServer = require('mongodb-memory-server').MongoMemoryServer\nconst mongod = new MemoryServer()\n```\nsolved it. Just doing \n```\nconst MemoryServer = require('mongodb-memory-server')\nconst mongod = new MemoryServer()\n```\nthrew an error. Thanks a lot for you help @novemberborn . ",
    "CodeOtter": "Yes, I am currently solving this problem with a dedicated test/index.js file that Ava requires, which contains that require hook:\ntest/index.js\nrequire('flow-remove-types/register')({\n  all: true,\n  pretty: true,\n  sourcemaps: true\n})\npackage.json\n{\n  \"ava\": {\n    \"files\": [\n      \"test/**/*.{js,jsx}\",\n      \"!test/index.js\"\n    ],\n    \"source\": [\n      \"src/**/*.{js,jsx}\"\n    ],\n    \"require\": [\n      \"./test\"\n    ]\n  }\n}\nGiven JavaScript's established and, apparently, never-ending proclivity to invent new DSLs/dialects/parsers/AST opinions-per-project, this PR is designed to enhance Ava's future proofing, as far as that interest is relevant. :P. ",
    "okyantoro": "sure @novemberborn \nI'll try to write the test but I need guide where to start to add the test\nThank you. Thank you @novemberborn \nI have added a test. Could you review this and give some some suggestions about what I do wrong and what should it be with this.\nThank you. Thank you @novemberborn \nI learn a lot from here.. Hello, I am new here :smile: \nNeed suggestions about this pull request \nThank you. Broken on node 4 and 6. > Instead we should check the return value of fn and if it's a promise or an observable wait for it to resolve, like we already do when you pass a promise or observable directly.\n@novemberborn So we need to try to execute the fn inside a try {} catch(e) {}, right?. @novemberborn I realize that calling fn() twice is dangerous, especially when the fn() it self modify outer states.\nI try different approach like what you suggested and call the fn() only once.. hi @novemberborn I'll continue this on saturday or sunday. Thank you for the suggestions and help. hello @novemberborn \nIn notThrows(), the fn() is executed inside coreAssert.doesNotThrow() directly. Is it okay to wrap the fn() by using arrow function so we can check the returned value of fn()?. @novemberborn when I try to add function signature for function that return promise, I get the following error:\n``\nError: test/flow-types/regression-1148.js.flow:6\n  6:    t.throws(() => { throw new Error(); });\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ call of methodthrows. Method cannot be called on any member of intersection type\n  6:    t.throws(() => { throw new Error(); });\n        ^ intersection\n  Member 1:\n   91: type ContextualTestContext         = TestContext & { context: any; };\n                                            ^^^^^^^^^^^ TestContext. See: index.js.flow:91\n  Error:\n    6:  t.throws(() => { throw new Error(); });\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ call of methodthrows. Method cannot be called on any member of intersection type\n   91: type ContextualTestContext         = TestContext & { context: any; };\n                                            ^^^^^^^^^^^ intersection. See: index.js.flow:91\n    Member 1:\n     85: type TestContext = AssertContext & {\n                            ^^^^^^^^^^^^^ AssertContext. See: index.js.flow:85\n    Error:\n      6:    t.throws(() => { throw new Error(); });\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ call of methodthrows. Could not decide which case to select\n      6:    t.throws(() => { throw new Error(); });\n            ^^^^^^^^ callable object type\n      Case 2 may work:\n       62:      (value: () => PromiseLike<mixed>, error?: ErrorValidator, message?: string): Promise<any>;\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ function type. See: index.js.flow:62\n      But if it doesn't, case 3 looks promising too:\n       63:      (value: () => mixed, error?: ErrorValidator, message?: string): any;\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ function type. See: index.js.flow:63\n      Please provide additional annotation(s) to determine whether case 2 works (or consider merging it with case 3):\n        6:  t.throws(() => { throw new Error(); });\n                          ^ return\n    Member 2:\n                                            v\n     85: type TestContext = AssertContext & {\n     86:    title: string;\n     87:    plan(count: number): void;\n    ...:\n     90: };\n         ^ object type. See: index.js.flow:85\n    Error:\n      6:    t.throws(() => { throw new Error(); });\n              ^^^^^^ propertythrows. Property not found in\n                                            v\n     85: type TestContext = AssertContext & {\n     86:    title: string;\n     87:    plan(count: number): void;\n    ...:\n     90: };\n         ^ object type. See: index.js.flow:85\n  Member 2:\n   91: type ContextualTestContext         = TestContext & { context: any; };\n                                                          ^^^^^^^^^^^^^^^^^ object type. See: index.js.flow:91\n  Error:\n    6:  t.throws(() => { throw new Error(); });\n          ^^^^^^ propertythrows`. Property not found in\n   91: type ContextualTestContext         = TestContext & { context: any; };\n                                                          ^^^^^^^^^^^^^^^^^ object type. See: index.js.flow:91\nError: test/flow-types/regression-1148.js.flow:9\n  9:    t.notThrows(() => { return; });\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ call of method notThrows. Method cannot be called on any member of intersection type\n  9:    t.notThrows(() => { return; });\n        ^ intersection\n  Member 1:\n   91: type ContextualTestContext         = TestContext & { context: any; };\n                                            ^^^^^^^^^^^ TestContext. See: index.js.flow:91\n  Error:\n    9:  t.notThrows(() => { return; });\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ call of method notThrows. Method cannot be called on any member of intersection type\n   91: type ContextualTestContext         = TestContext & { context: any; };\n                                            ^^^^^^^^^^^ intersection. See: index.js.flow:91\n    Member 1:\n     85: type TestContext = AssertContext & {\n                            ^^^^^^^^^^^^^ AssertContext. See: index.js.flow:85\n    Error:\n      9:    t.notThrows(() => { return; });\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ call of method notThrows. Could not decide which case to select\n      9:    t.notThrows(() => { return; });\n            ^^^^^^^^^^^ callable object type\n      Case 2 may work:\n       68:      (value: () => PromiseLike, message?: string): Promise;\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ function type. See: index.js.flow:68\n      But if it doesn't, case 3 looks promising too:\n       69:      (value: () => mixed, message?: string): void;\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ function type. See: index.js.flow:69\n      Please provide additional annotation(s) to determine whether case 2 works (or consider merging it with case 3):\n        9:  t.notThrows(() => { return; });\n                             ^ return\n    Member 2:\n                                            v\n     85: type TestContext = AssertContext & {\n     86:    title: string;\n     87:    plan(count: number): void;\n    ...:\n     90: };\n         ^ object type. See: index.js.flow:85\n    Error:\n      9:    t.notThrows(() => { return; });\n              ^^^^^^^^^ property notThrows. Property not found in\n                                            v\n     85: type TestContext = AssertContext & {\n     86:    title: string;\n     87:    plan(count: number): void;\n    ...:\n     90: };\n         ^ object type. See: index.js.flow:85\n  Member 2:\n   91: type ContextualTestContext         = TestContext & { context: any; };\n                                                          ^^^^^^^^^^^^^^^^^ object type. See: index.js.flow:91\n  Error:\n    9:  t.notThrows(() => { return; });\n          ^^^^^^^^^ property notThrows. Property not found in\n   91: type ContextualTestContext         = TestContext & { context: any; };\n                                                          ^^^^^^^^^^^^^^^^^ object type. See: index.js.flow:91\nError: test/flow-types/regression-1148.js.flow:12\n 12:    const error = t.throws(() => { throw new Error(); });\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ call of method throws. Method cannot be called on any member of intersection type\n 12:    const error = t.throws(() => { throw new Error(); });\n                      ^ intersection\n  Member 1:\n   91: type ContextualTestContext         = TestContext & { context: any; };\n                                            ^^^^^^^^^^^ TestContext. See: index.js.flow:91\n  Error:\n   12:  const error = t.throws(() => { throw new Error(); });\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ call of method throws. Method cannot be called on any member of intersection type\n   91: type ContextualTestContext         = TestContext & { context: any; };\n                                            ^^^^^^^^^^^ intersection. See: index.js.flow:91\n    Member 1:\n     85: type TestContext = AssertContext & {\n                            ^^^^^^^^^^^^^ AssertContext. See: index.js.flow:85\n    Error:\n     12:    const error = t.throws(() => { throw new Error(); });\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ call of method throws. Could not decide which case to select\n     12:    const error = t.throws(() => { throw new Error(); });\n                          ^^^^^^^^ callable object type\n      Case 2 may work:\n       62:      (value: () => PromiseLike, error?: ErrorValidator, message?: string): Promise;\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ function type. See: index.js.flow:62\n      But if it doesn't, case 3 looks promising too:\n       63:      (value: () => mixed, error?: ErrorValidator, message?: string): any;\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ function type. See: index.js.flow:63\n      Please provide additional annotation(s) to determine whether case 2 works (or consider merging it with case 3):\n       12:  const error = t.throws(() => { throw new Error(); });\n                                        ^ return\n    Member 2:\n                                            v\n     85: type TestContext = AssertContext & {\n     86:    title: string;\n     87:    plan(count: number): void;\n    ...:\n     90: };\n         ^ object type. See: index.js.flow:85\n    Error:\n     12:    const error = t.throws(() => { throw new Error(); });\n                            ^^^^^^ property throws. Property not found in\n                                            v\n     85: type TestContext = AssertContext & {\n     86:    title: string;\n     87:    plan(count: number): void;\n    ...:\n     90: };\n         ^ object type. See: index.js.flow:85\n  Member 2:\n   91: type ContextualTestContext         = TestContext & { context: any; };\n                                                          ^^^^^^^^^^^^^^^^^ object type. See: index.js.flow:91\n  Error:\n   12:  const error = t.throws(() => { throw new Error(); });\n                        ^^^^^^ property throws. Property not found in\n   91: type ContextualTestContext         = TestContext & { context: any; };\n                                                          ^^^^^^^^^^^^^^^^^ object type. See: index.js.flow:91\n```\nor maybe I did something wrong when added the function signature.. hi @novemberborn \nThank you for reviewing\nI think flow will not allow if we have two function signatures with subset type like:\njs\n(value: () => PromiseLike<mixed>, error?: ErrorValidator, message?: string): Promise<any>;\n(value: () => mixed, error?: ErrorValidator, message?: string): any;\ncorrect me if I am wrong.\n\nDon't forget to add pass and failure cases to https://github.com/avajs/ava/blob/a051d3e18dba92c893fddb08490a8627f586c231/test/assert.js#L624:L659 and https://github.com/avajs/ava/blob/master/test/assert.js#L701:L727.\n\nisn't it already covered by these lines:\n```js\ntest('.throws() returns the rejection reason of promise', t => {\n    const expected = new Error();\nreturn assertions.throws(Promise.reject(expected)).then(actual => {\n    t.is(actual, expected);\n    t.end();\n});\n\n});\ntest('.throws() returns the rejection reason of a promise returned by the function', t => {\n    const expected = new Error();\nreturn assertions.throws(() => {\n    return Promise.reject(expected);\n}).then(actual => {\n    t.is(actual, expected);\n    t.end();\n});\n\n});\nandjs\ntest('.notThrows() returns undefined for a fulfilled promise', t => {\n    return assertions.notThrows(Promise.resolve(Symbol(''))).then(actual => {\n        t.is(actual, undefined);\n    });\n});\ntest('.notThrows() returns undefined for a fulfilled promise returned by the function', t => {\n    return assertions.notThrows(() => {\n        return Promise.resolve(Symbol(''));\n    }).then(actual => {\n        t.is(actual, undefined);\n    });\n});\n```\n?\nThank you. @novemberborn Oh I see. Updated. Is it correct?. Thank you @novemberborn . Thank you @novemberborn . Hello @novemberborn I am interested in working on this issue.\nMy question is, how should we format the output?\n```\nTAP version 13\npassing\nok 1 - passing 30 ms\n1..1\ntests 1\npass 1\nfail 0\n```\nis it okay?. @novemberborn that spec didn't mention about time duration but I think it is reasonable to put the time duration information in test description.\nI think it is okay to add duration options in supertap.test()\nwe will have something like this\njs\nsupertab.test('Test Foo()', { ..., duration: test.duration, ... });\nbut if we concat the test title with test duration inside ava, we don't need to modify the supertab behavior, we also already have pretty-ms as dependency to format time duration.. It is harder than it seem, directly passing the title and duration together like\n```\nTAP version 13\npassing 30ms\nok 1 - passing 30ms\n1..1\ntests 1\npass 1\nfail 0\nThe above output will not work at least if we pipe those output with `tap-spec` and `tap-summary`.\nWhen we try to format those output with `tap-summary`, for example `$ cat test.txt | npx tap-summary` the result would be:\nTests\n\u2714 unnamed test [pass: 1, fail: 0, duration: 4ms]\nSummary\nduration: 4ms\nplanned: 1\nassertions: 1\npass: 1\nfail: 0\n``\nor maybe we can put the time duration in supertapcomment` option?. I browse the TAP spec repo and found https://github.com/TestAnything/Specification/issues/16 discussed about time directive.\nUsing time directive looks nicer to me, but this is not the standard way as the spec just allow two directives:\n\nDirective The test point may include a directive, following a hash on the test line. There are currently two directives allowed: TODO and SKIP.\nhttp://testanything.org/tap-version-13-specification.html\n```\nTAP version 13\nok 1 - passing # time=5ms\n\n1..1\ntests 1\npass 1\nfail 0\n``node-tap` uses time directive for SubTest:\n\nA test point associated with a Subtest can also have a # time=... directive indicating how long the subtest took to run.\nhttp://www.node-tap.org/tap-format/\n\nbut I think ava test reporting doesn't work this way.\nanother option\nwe put the time duration inside yaml diagnostic \n```\nTAP version 13\nok 1 - passing\nDiagnostic\n\ntime: 3ms\n   ...\n1..1\ntests 1\npass 1\nfail 0\n``\n. @novemberborn\nAs far as I know, innode-tapthe assertion treat one asertion as one test. Innode-tap,  when we need to group certain assertions into one, these assertions will be treated as subtest. Ava's assertions is not treated as test like innode-tap. Also in ava we cannot createtest()insidetest()` so ava doesn't have subtest contained in it. Correct me if I am wrong.\nI haven't checked the source code of node-tap yet, but I have read an example in http://www.node-tap.org/basics/\nThe example mentions subtest that has no test contained in it. Here is the example:\n```\nTAP version 13\nSubtest: some async stuff\nok 1 - should match pattern provided\n1..1\n\nok 1 - some async stuff # time=9.647ms\nSubtest: this waits until after\n1..0\n\nok 2 - this waits until after # time=6ms\n1..2\ntime=36.53ms\n```\nThis example is taken from http://www.node-tap.org/basics/\nThen for ava, I think we can do something like:\n```\nTAP version 13\nSubtest: test foo\n1..0\n\nok 1 - test foo # time=3ms\n1..1\n``\nBasically, test foo has no subtest contained (pseudo subtest \ud83d\ude04 ), at least it match the spec mentioned in http://www.node-tap.org/tap-format/ \ud83d\ude04 but as I mentioned in https://github.com/avajs/ava/issues/1668#issuecomment-364721169 , the Test Anything Protocol didn't mention about time directive. Any thought about it?. sure @novemberborn . Whoa, it is an interesting issue. I would like to follow the discussion since I want to understand the high level overview of how ava work.. Whoa, it is an interesting issue. I would like to follow the discussion since I want to understand the high level overview of how ava work.. as far as I know, ava extract first line of the error thrown, where this line is the location where thethrowhappened. I think extracting the calling graph of the error message started from thetest filewill be useful.. Good idea @novemberborn. It will give more information about returned value byt.throws()andt.throwsAsync()` assertions. Exciting issue to work on.. Hello @novemberborn \nCan I work on this?. Sure @novemberborn :). Thank you @novemberborn @ronen \nI created a pull request in https://github.com/avajs/ava/pull/1902. Let me know if I do something wrong with the unit test.\nThank you. Sure @sindresorhus . Hello @novemberborn \nCan I work on this issue?. You are right. Thank you for pointing this out.\nI made change to the code.\nThanks. updated. sure, we can use already provided library zen-observable in package.json. maybe we can make it simple like:\n\nWhen testing an async function you must also wait for the assertion to complete:\n\n@sindresorhus is it better?. ",
    "majgis": "I added the following to the package.json\njs\n  \"ava\": {\n    \"files\": [\n      \"runtimes/node_modules/*.test.js\"\n    ],\n    \"tap\": true\n  }\nWhich gives this output\n```\n$ ava\nTAP version 13\nCouldn't find any files to test\nnot ok 1 - Couldn't find any files to test\n1..0\ntests 0\npass 0\nfail 1\nDespite the fact that there is one test file present:\n$ ls runtimes/node_modules/*.test.js\nruntimes/node_modules/getCachePath.test.js\n```\nI'm expecting that by being explicit, I'd override the default ignores, but it doesn't seem to be the case.  Let me know if I got the syntax wrong.. ",
    "jy95": "Less text is more simple :) \nFor the FR translation it should be : \n\nC'est un probl\u00e8me connu. Vous devez mettre vos tests dans un dossier nomm\u00e9 test ou __tests__\n. @novemberborn I push the PR right now : https://github.com/avajs/ava-docs/pull/45 . No problem for any change, you are free to change whatever you want ^^ . \n",
    "BusbyActual": "Claimed : ^). Are there tests to run against? I'm not sure how to confirm each case is working as intended.. For \n\n\n[ ] Testing Vue.js components I don't see any changes needed. Require extension hooks appears to be a bit different than @babel/register.. I'll look into the Vue docs shortly. Slipped my mind : ). @novemberborn I like the idea of separate PR's for smaller changes to be reviewed. Hmm there's conflicts in docs\n\n\nregistration documentation is different on npm's vs babel's site? Do we want to honor npm?\n\npreset documentation is different from npm's  vs babel's site? Do we want to honor npm?. Updated pr with doc changes and pulled latest\n. That looks quite a bit simpler. Do we want to integrate the babel changes into the new docs instead?. @novemberborn there was a comment on the French translation I updated. . Made requested changes. The commit spacing looks strange however in my local it's fine?\n\n\n. I think I didn't see babel-loader was in beta in that version. . Updated prs and removed duplicate commits from prs. Merged changes ; ^). can do\n. Sounds good to me! I've addressed the requested changes : ^). It's not necessary for ava to run ~ it was in the examples in babels docs. I can remove it for brevity.. Sure thing : ^). ",
    "NickHurst": "@novemberborn After struggling to get the test suite added to my new nuxt.js app, I can confirm the current recipe does not work, but after a few hours of receiving this error:\n```bash\n\nyarn test\nyarn run v1.5.1\n$ ava\n\n2 exceptions\nUncaught exception in tests/foo.spec.js\nError: Requires Babel \"^7.0.0-0\", but was loaded with \"6.26.0\". If you are\nsure you have a compatible version of @babel/core, it is likely that something\nin your build process is loading the wrong version. Inspect the stack trace of\nthis error to look for the first entry that doesn't mention \"@babel/core\" or\n\"babel-core\" to see what is calling Babel.\n```\nI did eventually get it working with a few modifications to the current recipe. Running the test suite actually works, that is, figuring out coverage is the next thing on my todo list. Once I get that working, I'd be more than happy to submit a pull request updating it. I'm far from a babel expert though, so I fear it may only be working out of sheer luck. So, I'll just post my setup here and hopefully someone else can confirm it works, and/or point out better way to do something while I attempt to get the coverage working.\npackage.json\njson\n\"ava\": {\n  \"files\": [\n    \"tests/**/*.spec.js\"\n  ],\n  \"sources\": [\n    \"**/*.{js,vue}\"\n  ],\n  \"require\": \"./tests/helpers/setup.js\"\n}\ntests/helpers/setup.js\njavascript\nrequire(\"jsdom-global\")();\n// Pretty sure require(\"browser-env\")(); still works here though\nconst hooks = require(\"require-extension-hooks\");\nhooks(\"vue\").plugin(\"vue\").push();\nrequire(\"@babel/register\")({\n  extensions: [\".vue\", \".js\"],\n});\nI used jsdom-global here instead of browser-env just because the vue-test-utils guide recommends it\n.babelrc\njson\n{\n  \"presets\": [\n    \"@babel/preset-env\",\n    \"babel-preset-vue\"\n  ],\n  \"plugins\": [\n    \"@babel/plugin-transform-runtime\",\n    \"@babel/plugin-proposal-object-rest-spread\",\n    \"@babel/plugin-proposal-pipeline-operator\",\n    \"@babel/plugin-syntax-dynamic-import\",\n    \"@babel/plugin-transform-modules-commonjs\"\n  ]\n}\nEdit for completeness, the Foo.vue and foo.spec.js files\ncomponents/Foo.vue\n```vue\n\nhello world\n\n\nexport default {\n  props: {\n    bar: { default: false },\n  },\n  methods: {\n    baz() {\n      return true;\n    },\n  },\n};\n\n```\ntests/foo.spec.js\n```javascript\nimport test from \"ava\";\nimport { shallow } from \"@vue/test-utils\";\nimport { createRenderer } from \"vue-server-renderer\";\nimport Foo from \"../components/Foo.vue\";\ntest(\"Foo text is 'hello world'\", t => {\n  const wrapper = shallow(Foo);\n  t.is(wrapper.text(), \"hello world\");\n});\ntest(\"Foo prop bar returns true\", t => {\n  const wrapper = shallow(Foo, { propsData: { bar: true } });\n  t.true(wrapper.props().bar);\n});\ntest(\"Foo.baz() returns true\", t => {\n  const wrapper = shallow(Foo);\n  t.true(wrapper.vm.baz());\n});\n// The SSR way of rendering a component wrapper to an html\n// string, it's \"formatted\", but this will also add the attribute\n//     data-server-rendered=\"true\"\n// to the rendered tags\ntest(\"Foo SSR snapshot\", async t => {\n  const wrapper = shallow(Foo);\n  const renderer = createRenderer();\n  t.snapshot(await renderer.renderToString(wrapper.vm));\n});\n// If you just want a string of html for the snapshot\ntest(\"Foo snapshot unformatted\", t => {\n  const wrapper = shallow(Foo);\n  t.snapshot(wrapper.html());\n});\n// Or if you want the snapshot html to be formatted\nconst beautify = require(\"js-beautify\").html;\nconst beautifyOpts = {\n  unformatted: [],\n  indent_size: 2,\n  preserve_newlines: true,\n  wrap_attributes: \"force-aligned\",\n};\ntest(\"Foo snapshot formatted\", t => {\n  const wrapper = shallow(Foo);\n  t.snapshot(beautify(wrapper.html(), beautifyOpts));\n});\n// This would save a snapshot as:\n// \n//   hello world\n// \n// Rather than the single line string wrapper.html() returns:\n// hello world\n// Which makes the diffs easier to read\n```\nThe suite this works on is only a few basic tests done using @vue/test-utils, so there may still be something that won't work with this setup. Snapshot testing does work though, but that was the only thing out side of pretty much t.true(1 === 1) I've tried writing.\nEdit: And fortunately there weren't any issues setting up coverage, yarn add -D nyc and changing \"test\": \"ava\" to \"test\": \"nyc ava\" in the package.json was all I needed to get that working.. ",
    "lili21": "babelrc: false not working.\nmy .babelrc config.\njson\n{\n  \"presets\": [\n    [\"env\", {\n      \"modules\": false,\n      \"targets\": { \"browsers\": [\"Android >= 4\", \"iOS >= 9\"] }\n      }\n    ],\n    \"stage-2\"\n  ],\n  \"plugins\": [\n    \"lodash\",\n    \"transform-decorators-legacy\",\n    \"transform-runtime\"\n  ]\n}\nmy ava config.\njson\n{\n  \"ava\": {\n    \"files\": [\n      \"packages/**/__tests__/**/*.js\",\n      \"!packages/**/dist/**/*\",\n      \"!packages/**/node_modules/**/*\"\n    ],\n    \"require\": [\"@babel/register\"],\n    \"babel\": {\n      \"testOptions\": {\n        \"babelrc\": false,\n        \"presets\": [\n          \"@babel/env\",\n          \"@babel/stage-2\"\n        ]\n      }\n    }\n  }\n}\nThe error message\n\nit still use babel-preset-stage-2 instead of @babel/preset-stage-2\nI'm using ava@1.0.0-beta.3. @novemberborn  #1767 . I guess the problem is @babel/register.   @babel/register will read the config from .babelrc unless You specify options like below\njs\nrequire('@babel/register')({\n  babelrc: false\n}). confirmed\nupdate ava config\njson\n\"ava\": {\n    \"require\": [\n      \"./setup.js\"\n    ],\n    \"babel\": {\n      \"testOptions\": {\n        \"babelrc\": false\n      }\n    }\n  }\nsetup.js\njs\nrequire('@babel/register')({\n  babelrc: false,\n  presets: ['@babel/env']\n})\nit works.\n. just read the docs again. looks like this is the expected behavior? . . kind of weird to me, that test file and source file using different way to handle babel config.. any updates?. ",
    "Jolo510": "Hey @novemberborn, I'd like to take on this task! . @novemberborn To remove the ifError assertion, is it just removing it from the assert.js file? \nhttps://github.com/avajs/ava/blob/master/lib/assert.js#L464\nAnd potentially fixing any tests that uses the ifError?\nAlso, since it's a breaking change, what's the rules for versioning AVA?. @novemberborn Gotcha! @kugtong33 you get first dibs!. ",
    "amslv": "Hey, @novemberborn, I'd like to work on this task. ",
    "kelvinman": "I'm interested to work on this. ",
    "liewrichmond": "Hey just checking in on this issue. Is there any way I could help on this?. @novemberborn ok thanks! I'm really excited to work on this. It's my first real open source contribution I'm still trying to get my bearings with ava and the issue itself but I'll start on this as soon as I can. @novemberborn Do you have any suggestions on testing the potential bug fix/ steps to reproduce the bug?. ",
    "hekod777": "Wrong place for this post I guess.. Wrong place for this post I guess.. ",
    "JayAndCatchFire": "I was actually thinking in terms of tests that aren't pure equality.  For example, a function that should return a value within a certain range of known good values.  But if the return value starts to slip too close to the edges of what's acceptable, throw a warning.  Of course, I realize that my example is stretching the definition of unit testing.. ",
    "jviotti": "I sent a PR with my diff in case you think this is something we can add to Ava: https://github.com/avajs/ava/pull/1683. @novemberborn Thanks for the response!\n\nNote that you could write a wrapper script that uses Inquirer. If you then load ava/cli it should behave the same as if you invoked ava directly.\n\nNot sure I get it. ava/cli is CLI entry point of this library, right? Can you elaborate on how wrapping such same entry point would make a difference? I guess I can execute Inquirer.js on the wrapper script, but how would the wrapper script pass the response back to the running Ava tests?. No worries! I was able to get it working with a wrapper script + IPC between the wrapper and the children.. ",
    "ianwalter": "Very recently I created webdriver-helper, a simple AVA helper to provide a WebdriverIO browser instance in a test. It works well, and I prefer writing AVA tests instead of using Mocha or Jasmine (which WebdriverIO officially integrates with), but one issue is that await browser.debug doesn't work properly. browser.debug is cool because it provides a REPL interface to WebdriverIO that lets you enter commands and try to figure out why your test isn't working as expected. Commands being typed in aren't making it to WebdriverIO though and I don't really understand how or why.\n\nThere's just too many ways in which users may expect to use AVA that your proposal is not compatible with.\n\nI feel like my use case is different in that a proposed interface could be made available through a manual process and not by default when using -s? I don't know what the proposed interface looks like but the discussed workaround seems very complicated so I'm asking that this feature be reconsidered.. @novemberborn Absolutely, will try it out, thanks for your help.. I have the same issue. Build failure can be found here: https://github.com/ianwalter/vue-component-reset/pull/3/checks?check_run_id=67824620\nEdit: Regenerating the yarn.lock file fixed the issue.. ",
    "friday": "It was introduced in 0.18 and happens in babel transpilation with Node.js older than v8.3. There might be more conditions involved but this is what I was able to narrow it down to.\nI'm guessing it's just the babel presets that needs tweaking. AVA changed the babel conf (among other things) at 0.18 according to the changelog.\nAlso see: https://github.com/babel/babel/issues/6084\n``\n  Uncaught Exception\n  SyntaxError: /Users/albin/src/eslint-plugin-lodash-fp/test/consistent-name.js: Unexpected token (58:8)\nSyntaxError: /Users/albin/src/eslint-plugin-lodash-fp/test/consistent-name.js: Unexpected token (58:8)\n  56 |       code:var lodash = require('lodash');`,\n  57 |       errors: [Object.assign({\n\n58 |         ...error, message: 'Lodash should be imported as _'\n     |         ^\n  59 |       }, error)]\n  60 |     }\n  61 |   ]\n    at Parser.pp$5.raise (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/babylon/lib/index.js:4454:13)\n    at Parser.pp.unexpected (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/babylon/lib/index.js:1761:8)\n    at Parser.pp$3.parseIdentifier (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/babylon/lib/index.js:4332:10)\n    at Parser.pp$3.parsePropertyName (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/babylon/lib/index.js:4156:96)\n    at Parser.pp$3.parseObj (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/babylon/lib/index.js:4045:12)\n    at Parser.pp$3.parseExprAtom (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/babylon/lib/index.js:3719:19)\n    at Parser.pp$3.parseExprSubscripts (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/babylon/lib/index.js:3494:19)\n    at Parser.pp$3.parseMaybeUnary (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/babylon/lib/index.js:3474:19)\n    at Parser.pp$3.parseExprOps (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/babylon/lib/index.js:3404:19)\n    at Parser.pp$3.parseMaybeConditional (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/babylon/lib/index.js:3381:19)\n    at Parser.pp$3.parseMaybeAssign (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/babylon/lib/index.js:3344:19)\n    at Parser.pp$3.parseExprListItem (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/babylon/lib/index.js:4312:16)\n    at Parser.pp$3.parseCallExpressionArguments (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/babylon/lib/index.js:3573:20)\n    at Parser.pp$3.parseSubscripts (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/babylon/lib/index.js:3533:31)\n    at Parser.pp$3.parseExprSubscripts (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/babylon/lib/index.js:3504:15)\n    at Parser.pp$3.parseMaybeUnary (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/babylon/lib/index.js:3474:19)\nFrom previous event:\n    at Api._runWithoutPool (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/ava/api.js:234:5)\n    at /Users/albin/src/eslint-plugin-lodash-fp/node_modules/ava/api.js:161:23\nFrom previous event:\n    at Api._run (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/ava/api.js:150:5)\n    at /Users/albin/src/eslint-plugin-lodash-fp/node_modules/ava/api.js:75:24\nFrom previous event:\n    at Api.run (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/ava/api.js:75:5)\n    at Object.exports.run.api.on.api.run.then [as run] (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/ava/lib/cli.js:165:7)\n    at Object. (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/ava/cli.js:22:24)\n    at Module._compile (module.js:409:26)\n    at Module.replacementCompile (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/nyc/node_modules/append-transform/index.js:58:13)\n    at module.exports (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/nyc/node_modules/default-require-extensions/js.js:8:9)\n    at Object. (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/nyc/node_modules/append-transform/index.js:62:4)\n    at Module.load (module.js:343:32)\n    at Function.Module._load (module.js:300:12)\n    at Function.Module.runMain (module.js:441:10)\n    at runMain (/Users/albin/.node-spawn-wrap-62817-995987306568/node:68:10)\n    at Function. (/Users/albin/.node-spawn-wrap-62817-995987306568/node:171:5)\n    at Object. (/Users/albin/src/eslint-plugin-lodash-fp/node_modules/nyc/bin/wrap.js:23:4)\n    at Module._compile (module.js:409:26)\n    at Object.Module._extensions..js (module.js:416:10)\n    at Module.load (module.js:343:32)\n    at Function.Module._load (module.js:300:12)\n    at Function.Module.runMain (module.js:441:10)\n    at /Users/albin/.node-spawn-wrap-62817-995987306568/node:178:8\n```. \n",
    "rinu": "I'm using ava 1.0.0 beta 8 and still getting that SyntaxError. I tried adding babel-plugin-syntax-object-rest-spread as well, still nothing.\nI'm trying to add unit tests to a project that uses spread syntax all over the place. Apart from this limitation, ava is the first thing that actually works.. I'm using ava 1.0.0 beta 8 and still getting that SyntaxError. I tried adding babel-plugin-syntax-object-rest-spread as well, still nothing.\nI'm trying to add unit tests to a project that uses spread syntax all over the place. Apart from this limitation, ava is the first thing that actually works.. I was following a badly out of date example project (https://github.com/eddyerburgh/vue-test-utils-ava-example) which I think does attempt to compile sources as well. Anyway, thanks @novemberborn for getting me on the right track!. I was following a badly out of date example project (https://github.com/eddyerburgh/vue-test-utils-ava-example) which I think does attempt to compile sources as well. Anyway, thanks @novemberborn for getting me on the right track!. ",
    "ToniChaz": "Hi @novemberborn I have tha same problem with ava@1.0.0-beta.8 and Node@10.3.0\n\n\n\u00bfAny solution?\n. ",
    "MrZhouChiBang": "thx\nCould you modify the document to avoid the mistake? . ",
    "danr": "Any further comments on this? \nIs it likely that this would be accepted upstreams? \nIf so can anyone give me some pointers or how to implement what @novemberborn and I have discussed?. ",
    "dashuser33": "I arrived at this issue searching for a way to retry assertions until they succeed, similar to how Scala Specs2 does it https://github.com/etorreborre/specs2/blob/master/tests/src/test/scala/org/specs2/matcher/EventuallyMatchersSpec.scala\nin pseudo-code:\ntest('random < 0.1', t => {\n     t.eventually(\n    t.true(Math.random() < 0.1)\n     );\n});\nIn spec2, by default it retries 40 times with a delay of 100ms, but both are configurable and have in the past needed to use with delay of 0ms and delay of 1000ms. One example of where this is interesting. This allows you to avoid adding sleep for testing slow async operations. It will keep retrying until ready or max retries is reached.\nIs there any wait do that today?. To help anyone wanting the same feature, here is how I implemented it:\n```\nconst test = require('ava');\nfunction sleep(ms) {\n  return new Promise(resolve => {\n    setTimeout(resolve, ms);\n  });\n}\nasync function retry(t, fn) {\n  for (let i = 0; i < 10; i++) {\n    if (fn()) {\n      t.pass();\n      return;\n    }\n    await sleep(1000);\n  }\nt.fail('failed after 10 attempts');\n}\ntest('random < 0.1', async t => {\n  await retry(t, () => Math.random() < 0.1);\n});\n```\nOf course this has the drawback of only supporting true assertion. I might try to generalize this at some later point.\nUPDATE: this is what I did: https://github.com/dashuser33/darkcoin-client/blob/master/src/lib/async.ts#L20. ",
    "qlonik": "@iamstarkov There is a work in progress solution sitting in the #1947. I've been using it on my project and it works for now. There will be changes before it will get merged into master, but if you really would like to, you can check it out at that specific commit bde2be1087c7c37f49a63e9082caa9abe0ab18de.\nI also use jsverify but I modified the connector between ava and jsverify: https://gist.github.com/qlonik/9a297285284d71f7da47022f120ef4ad. The modification handles the logging slightly better, provides typings, and works with try-commit/discard pull request. It is written to work in a way similar to the original ava-jsverify. Maybe later on, it could be modified to be a macro.. > how Test and assertions work\nYea, it was a bit difficult to follow. There are functions being bound to instance of Test all over the place.\nI was thinking that it would be cool to refactor assertions as a class with whatever fields they require, and Test to extend that assertions class with snapshot management code. Additionally, ExecutionContext might be extending the Test class as well with log(), plan(), and try() assertions (or others as needed).. I would like to rebase this branch on top of master. Anything I should worry about?. Its okay if it takes time. Its holidays after the next week anyway, or something :)\nI've been using snapshot of ava forced on commit that includes these changes. It worked for me quite well. It will be cool when it makes it into ava :). With regards to concurrent attempts. I think it would be nice to allow attempt to be concurrent. From my use case, I'm passing async function to the library, and I have no control whether that is called concurrently or not, so I might have to do some magic outside of passed function to ensure that attempts run sequentially. It might affect performance in some cases. \nAdditionally, if we choose to do one test at a time or multiple tests at a time, switching to the other one will probably be a breaking change. Since people might rely on one or another behavior. Also, ava runs tests in parallel by default, so maybe attempts should also be run by default in parallel?\nI would probably prefer concurrent attempts, unless we figure out that something breaks if it runs concurrently.\n\nI'm still pretty confused with snapshots and test titles. It seems that if title of the attempt is changed, then the snapshot will be bound to that title. I'm not sure if it depends whether attempt is committed or discarded. The attempts should have the same title, unless the order of them is changed. Maybe this will be influenced by the fact there are concurrent attempts or not. \nLast thing that I'm hesitant to change is about setting { failing: false, callback: false } vs setting some value to be { inline: true } and handling it specifically. There are various functions in various places that get bound to the instance of test, which I do not know about. I'm thinking that adding special behavior for { inline: true }, which will override some of the behaviors of failing and callback, might cause bugs where we wouldn't know or forget to change something related to failing/callback. \nEDIT: It might be nicer to have special mode for attempts via { inline: true }, but I dont think I'm familiar enough with the code base to make that change.. > Changing the test titles will definitely have unintended consequences.\nBut, the code as it is now does not change the title of the running test. It creates new test with a different title. And as long as order of attempts is the same, the attempt test title will correspond. However, if they are reordered, snapshots will fail in those tests, just if they would when reordering snapshots themselves.\nI'm going to add the test case with the current behavior. > You're assuming that the number of attempts required is deterministic.\nOh yea. I haven't considered that use case.\nEDIT:\nShould that be something that is configurable via options for example? We would default to one behavior where snapshots from different attempts would conflict with each other based on the order of snapshots in each attempt. But somehow it would be possible to switch to another behavior, where snapshots do not conflict with each other. Does this behavior even need to exist? Maybe just one of the two options is acceptable.\nThinking about it a bit more. Maybe it does make sense to have only one concurrent attempt running. However, I would advocate that ava forces those attempts to be sequential internally, so that there is no difference between await t.try(); await t.try(); and await Promise.all([t.try(), t.try()]);.. Oh, I had an idea yesterday. If we allow to have macro passed as the attempt function to t.try(), that macro might have the title generating function. We can have the behavior that if that function is not provided, then tests will be in 'conflicting' mode, where you can only have one snapshot which will be committed to. However, if the title function is provided, and whatever it generates does not conflict with the title of the running test or previous attempt, then it will be in the 'non-conflicting' mode, like it is now. Additionally, if the attempt title is generated but still conflicting with the test title or with previous attempt, it might be in the 'conflicting' mode as well.\nDoes it even make sense to have this feature?. > Sorry, still haven't gotten round to this PR :cry:\nYeaa, me neither sorry. The only thing is I looked at assert.js file very carefully in order to understand which fields it depends on from Test class. That is so I could refactor test.js if needed.\nAs an attempt, I refactored assert.js and test.js in such a way that assertions dont need to be bound to the test instance. Instead ExecutionContext is extending the Assert class, and Test exposes needed functions for ExecutionContext. Would this refactor be useful to submit? Maybe as a separate pull request.\nAlso, as a passing question: is there interest in porting ava to TypeScript?. @novemberborn While working on typing the try function, I stumbled upon the solution to #1970. I found that we can get rid of that clever crafty extends and infers and conditional types. It appears that TS didn't like the fact there was no definition for macro without the title. When I added that, it worked without problems. Would you like me to file PR with that refactoring?\nWhen I apply those conditional types to try fn, it does not work very well, as it forces developer to define all types explicitly. But it would be nice to not force that. Hey.\nI stumbled across this pr and I thought it was interesting.\nI tried simply moving definition which only has macro and args (without title) to be above the one with title. However, in that case, the outlined problem shifts to other function call. So that does not work well as solution.\nI gave it a shot though. It is possible to merge those two definitions. I ended up having the following. Also, props to tycho01/typical for bunch of awesome TS types. Tail here is his definition: https://github.com/tycho01/typical/blob/65b015307eb4fb35f3771494be09a9ee00dd285d/src/array/Tail.ts\n```typescript\nexport type Tail =\n    ((...args: T) => any) extends ((head: any, ...tail: infer R) => any) ? R : never;\nexport interface TestInterface {\n    /* Declare a concurrent test. /\n    (title: string, implementation: Implementation): void;\n/** Declare a concurrent test that uses one or more macros. Additional arguments are passed to the macro. */\n<\n    OneOrMore extends OneOrMoreMacros<Context>,\n    X extends string | OneOrMore,\n    Y extends (\n        X extends string ? OneOrMore :\n        X extends OneOrMore ? InferArgs<X>[0] :\n        never\n    )\n>(\n    titleOrMacro: X,\n    macroOrArg: Y,\n    ...rest:\n        Y extends OneOrMore ? InferArgs<Y> :\n        X extends OneOrMore ? Tail<InferArgs<X>> :\n        never\n): void;\n\n// replace both definitions of macro based fn with the definition above.\n// after, afterEach and so on\n\n}\n```\nWhile it is possible, this solution has problems.\n1. It works the best when title is passed. It can detect if not proper amount of parameters is passed. It gets confused however when zero parameters is passed. It says that Macro is not assignable to Implementation. However, if you pass at least one additional argument, then it will say 'expected x, but got y<x'\n2. When title is not passed, it can detect improper amount of parameters passed, but then you need to pass at least 2 parameters. If you pass zero parameters, it says 'needs at least 2 params', if only one parameter is passed, it says 'macro is not assignable to string'\n3. There is weirdness when macro has zero parameters to it. I think macro has to have at least one parameter? if it has zero parameters, it is actually just a test implementation. Maybe test implementation could be generalized somehow?\nOne more important thing I should add. I was exploring at some point these rather complicated types that could be done with TS. TS has a lot of power, and a lot can be done. You can actually put TS compiler into infinite loop, or even make it exceed the stack. It is possible to be careful with types and not let TS explode like this. However, all of this power comes at a usability price. TS will take time to determine what are the parameters to the function supposed to be, so that while user is typing, the text editor might not keep up and show the appropriate type information while typing. It seems that the implementation is only using extends and ?: and infer, which should be handled well by TS compiler, and there are no loops/recursions, which is good. But the complexity of types is something that should be kept in mind.\nP.S. I had some fun doing this haha :smile: . It looks good. I have 2 comments:\n\n\nIn the case of overloads, TS requires to have most precise/specific type on the top followed by less specific types. I'm not sure if the function with 3 arguments is more specific than the one with 2 arguments or vice versa. Because of this, functions might need to be reordered in the TestInterface (and others)\n\n\nSince in TS types are matched based on structure, it seems like it does not make distinction between Implementation<Context> and Macro<[], Context>. This is evident from the case of test.after(). \nIf in the index.d.ts you reorder such that macro definition above implementation definition (without titles), then call test.after() with either test implementation or macro with no parameters, and if you ctrl-click in VS Code on the .after part, then both of them will link to the same type declaration, regardless of whether its macro or implementation. It seems that because of this TS does not make a distinction of empty macro and test implementation. I think it might be good to merge those two somehow, so that there are no unneeded type declarations in the TestInterface.\nSimilarly, the declaration (title: string, fn: Implementation<Context>): void should be equivalent to (titleOrMacro: ToM, macroOrArgs: MoA, ...rest: Stuff): void, when macro is empty, and TS might not be making any distinction between two types. \nThe same test works - have the type definition with implementation above the big test definition, and pass empty macro to test with title, (e.g. test('title, macro)) and then ctrl-click test, the VS Code will link to test declaration with title and implementation and not to the big type declaration.. I was trying to see if Macro can be merged with Implementation somehow.\n\n\nI tried declaring Macro as following (notice default declaration to Args as empty tuple). It becomes equivalent to Implementation and could be used interchangeably.\n```ts\ninterface Macro {\n    (t: ExecutionContext, ...args: Args): ImplementationResult;\n/**\n * Implement this function to generate a test (or hook) title whenever this macro is used. `providedTitle` contains\n * the title provided when the test or hook was declared. Also receives the remaining test arguments.\n */\ntitle?: (providedTitle: string | undefined, ...args: Args) => string;\n\n}\n{\n  const testImpl: Macro = t => t.pass()\n  test('test title', testImpl)\n}\n```\nEDIT Sorry, I also tried to keep only one type declaration (titleOrMacro, ...) and remove all others. The following is happening in that case.\nHowever, I noticed that it is slightly awkward, that this case:\nts\ntest('title', t => {\n  t.pass()\n})\nResults in t being of type any, which is not acceptable.\nI tried few more things, but it ends up being awkward that implementation passed to test need to have t typed as ExecutionContext. It is very hard to merge those two together (EDIT merge those two together and keep one type declaration in the TestInterface), ~it might be easier to make sure that Macro has at least one value passed to it, and Implementation always has zero values passed to it.~\nEDIT: It is possible to merge Macro and Implementation, but then there should be all three type declarations kept: just with empty macro, with title and empty macro, and the last one which covers parameters.\nI discovered another problematic case:\nts\n    test('string', (t, a: string, b: number) => t.pass(), '', 0);\n    test((t, a: string, b: number) => t.pass(), '', 0);\nI'm not sure if someone might use it like this.. About the order - okay, I think that it might matter for the case of interfaces. But sure, this is good. If it will become a problem, we will see then.\nFor Implementation, I was thinking about duplicate definitions (one with implementation and one with empty macro). I was worried that they are unnecessary and might come in the way. Again, if they will become a problem, we will see then.\nWhat about the case of untyped macros functions being passed to the test? As shown in my  last message. Those do not pass type check. So what should be the recommendation, for people using test function like that?\n\nts\ntest('string', (t, a: string, b: number) => t.pass(), '', 0);\ntest((t, a: string, b: number) => t.pass(), '', 0);\n. Could you provide minimal reproduction of the problem?\n\nMy impression was that allowJs is used during the transition period when porting the project from javascript to typescript and because of that it is not so common. Is it the case?. I dont understand why is it that it did not fail in CI on the first commit (looking at detailed results, it passed for node on linux for 11, 10 and other versions). It fails on my machine when I'm running test/assertions.js without the fix.\nI also dont understand why after the fix is pushed, windows is failing in integration test.\nHow  are all of those issues are related?. Here, the pendingAttemptCounteris decremented, which means that it is required to call either 'commit() or discard() for every attempt. Instead it could be decremented as part of addPendingAttemptAssertion() and then we will require at least one call to commit() and no need to have discard() and no need to explicitly call either commit or discard on every attempt.. That's because the t.try() might be discarded before it is resolved. I wasn't sure what to do in this case and decided to resolve with null.. :+1: . Maybe duration and metadata are not.\nHowever, logs and title are helpful for the case when t.try is used inside the macro provided by third party. (This is described in more detail in parent comment). :+1:. I added this so that there is no duplicate code between commit() and discard() functions returned to the user. The user dont have access to call this function directly, it is proxied. I will remove it though, for clarity between commit() and discard() functions.. :+1:. It could be. I'm not sure how it would be better. I was trying to be explicit so that there is not more commit()/discard() calls than needed.\nAlso, while trying the solution, it was possible to have discard be implicitly default behavior. So that t.try() would transparently be not an assertion, until commit() is called. This way discard() is not needed at all. I'm not sure if this way is better, or if it should be more explicit like it is now.. Could you elaborate?. :+1:. they come from the ret value.. So should it not throw an error and simply return?. This way was simpler. I'll try to change. I was adjusting the title in order to prevent any conflicts related to snapshots. Isn't it the case that if we adjust the title, then there are no problems about number of concurrent t.try() and both may contain snapshots?. Should it be Attempt is complete, but the test has already finished?. Should it be the same as above?. :+1:. Decided to have no implicit discard(). There has to be either the call to commit() or discard().. Currently run() only returns one error, which I think is the first recorded error.\nWe can modify saveFirstError(error) to save array of errors, rather than one error.. This value is not returned from the attempt. It can be obtained from assertCount private field of the attempt instance. Is it acceptable?. Sounds good. We should pick what can be exposed in AssertionError. ",
    "MagixInTheAir": "I would have to skip the beforeEach call only in this single test then ? Is there a way to do this ?\nThis looks like a sensible work-around, but maybe a better solution will be found :). ",
    "shellscape": "\nt.throws() now fails if the exception is not an error. Fixes #1440.\n\nThis is causing problems migrating an older module for me. PostCSS@5.x rejects with a CssSyntaxError that does not inherit from Error. Silly, but it's there. Would be keen on an option to tell this to allow unexpected behavior, and at the very least, and instance of a particular non-Error constructor.. ",
    "leegee": "Many thanks. Thought it might relate to #73. Many thanks. Thought it might relate to #73. > This feature is available as of 1.0.0-beta.1\nDo you have a road map for when it might leave beta?\nTIA\nLee. > This feature is available as of 1.0.0-beta.1\nDo you have a road map for when it might leave beta?\nTIA\nLee. ",
    "samuelli": "@novemberborn I filed #1723 with more details and a minimal repro. It appears to only be an issues when using strict mode in typescript.. That looks good @novemberborn! Thanks!. ",
    "TotomInc": "Thanks for putting me in the right way @sindresorhus, I finally fixed my tests:\n// install browser-env and canvas-prebuilt\n// (https://github.com/jsdom/jsdom#loading-subresources)\nyarn add browser-env canvas-prebuilt -D\njs\n/**\n * test/helpers/setup-browser-env.js\n *\n * using the new API of browser-env, you need to specify the `resources: 'usable'`\n * config to allow browser-env + jsdom to load subresources like images, scripts,\n * stylesheets, ... (https://github.com/jsdom/jsdom#loading-subresources)\n * the 3-lines below make the `window`, `document` and `Image` objects global\n */\nrequire('browser-env')(['window', 'document', 'Image'], {\n  resources: 'usable',\n});\n```js\n/\n * test/test.js\n \n * load an image and return a resolved or rejected promise.\n /\nimport test from 'ava';\nfunction imagePromise() {\n  return new Promise((resolve, reject) => {\n    const img = new Image();\n    img.addEventListener('load', () => resolve(true));\n    img.addEventListener('error', () => reject(false));\n    // an heavy 4k wallpaper to check if promise is resolved after a long time\n    img.src = 'https://www.wallpaperup.com/wallpaper/download/991808/8000/5224';\n  });\n}\ntest('load image with promise', (t) => {\n  return imagePromise().then((result) => {\n    t.is(result, true);\n  });\n});\n```. ",
    "stringbeans": "@novemberborn thanks for your response! im a little new to how tap outputs and tap reporting works, but i'll keep digging. i took a peek at how tap is handled by ava and from what i can guess at, it looks like ava DOES at least include the strack trace.... @jamestalmage thanks for the tips. using a SQLite instance wont work for us as there are some slight differences in syntax and behaviour between mariadb/mysql (our db) and sqllite. \nbecause of including the db in our integration tests we actually make sure we only run our integration tests serially to avoid the shared global state. ",
    "gmahe": "Oh I see!\nI didn't realised it was a beta feature!\nThanks. ",
    "martypdx": "My .02:\nYes, for removing global install and yes for npx ava --init, but no for running tests. \nFor required project dev dependency I would advocate purely installing locally. I only npm -g for utilities and tools that are optionally used in a project.\nSince --init is only run once, it makes sense to use npx ava --init.\nFor ongoing scripts (test running) I would advocate creating script test because it's needed by CI and as cli flags getting added beyond ava it provides a documented, standard way you know how to test the project.. @novemberborn updated the PR to just provide local install instructions. I don't use yarn, so I think from research that is right way to run --init, but haven't tried it myself. ",
    "Seiyial": "Hi, I've recently used t.context in the .before() / .after() and .beforeEach() / .afterEach() hooks in my project successfully and would love to help with this. I'm rather new to open source contributing so any feedback/advice would be appreciated. Thanks!. @novemberborn Sorry for the delay. Please let me know how you think about it, thanks!. Hi @novemberborn , sorry I missed the notification.\nBy assertions do you mean t.deepEqual() etc? I personally thought of using console.log() because the readers may not yet be familiarised with ava's syntax. But come to think of it, the assert syntax looks quite foolproof so I guess using assert statements might be better.\nOh I see, that sounds important. I'll guess I should highlight that in too.. Hi @novemberborn , sorry for the late reply again. I have been busy and will be busier in the coming weeks, so it may be considerable if someone else is free to do it faster. That being said, I've updated the recipe in docs/recipes/context.md based on your comments in this thread, including changing console.log to assertions and adding test.afterEach.always() to the diagram. The point on shallow cloning is also highlighted with examples.\nThanks for looking through it!\nJust in case the diagram still needs editing and someone else is taking it, the raw version is here: https://bit.ly/2IWsSbL. @sindresorhus Sure! Am freer now. Please let me know if it needs any more changes, thanks!. @sindresorhus so sorry I got busy halfway! As discussed at #1730, @mh81 has picked this up and is working on it now :). ",
    "mh81": "I have some free time this weekend, I'd be happy to build on @Seiyial's work in his PR and update it with @novemberborn's PR feedback.  Any objection?. ",
    "ivan-kleshnin": "Maybe relevant. I'm getting the following error:\nPlugin or preset file '/Users/username/Sandboxes/projectname/node_modules/babel-preset-es2015/lib/index.js' \ndid not export a function\nat $ ava tests with ava@1.0 while the same environment works just fine with ava@0.25.\npackage.json\n\"ava\": {\n    \"require\": [\n      \"@std/esm\"\n    ]\n  },\n\"@std/esm\": \"^0.16.0\",\n  \"babel-core\": \"^6.26.0\",\n  \"babel-preset-es2015\": \"^6.24.1\",. Maybe relevant. I'm getting the following error:\nPlugin or preset file '/Users/username/Sandboxes/projectname/node_modules/babel-preset-es2015/lib/index.js' \ndid not export a function\nat $ ava tests with ava@1.0 while the same environment works just fine with ava@0.25.\npackage.json\n\"ava\": {\n    \"require\": [\n      \"@std/esm\"\n    ]\n  },\n\"@std/esm\": \"^0.16.0\",\n  \"babel-core\": \"^6.26.0\",\n  \"babel-preset-es2015\": \"^6.24.1\",. ",
    "gauntface": "Just hit this issue again and it looks like a stack trace was display that helped be dig into the problem so thank you.\nIs there any way the message could include the specific tests or test files that should be running but aren't? I'm assuming Ava knows what they are.. ",
    "sshetty": "I get \"t.context is not available in before tests\" even after updating AVA. I tried versions 1.0.0-beta.2 and 1.0.0-beta.3. ",
    "git-jiby-me": "@novemberborn any idea when is there gonna be a next release with this changes ?. ",
    "brandonweiss": "Huh, OK now I'm not so sure. It seemed like I was very reliably able to stop the re-running by naming the file javascript.js and then start it by naming it js.js, but I've moved onto another feature and even though the file is name javascript.js I'm still encountering the endless re-running. \ud83e\udd14. Huh, OK now I'm not so sure. It seemed like I was very reliably able to stop the re-running by naming the file javascript.js and then start it by naming it js.js, but I've moved onto another feature and even though the file is name javascript.js I'm still encountering the endless re-running. \ud83e\udd14. Ah, thanks for the debugging tip!\nOK, so the filename being js.js was a total red herring. Sorry about that! What was happening is my package needs to create files on the filesystem and then the tests assert the files are there or have changed in some way. I was writing those files to tmp/tests, and it wasn't a problem because the files were all .css, .html, or .html.jsx files. But when I started testing .js files, creating a .js file in the test triggered another re-run. \ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0f\nSo the solution is I just needed to exclude tmp from the source files being watched in my package.json like so:\njson\n{\n  \"ava\": {\n    \"source\": [\n      \"!tmp/**/*\"\n    ]\n  }\n}\nAgain, sorry about that! Thanks for your help!. Ah, thanks for the debugging tip!\nOK, so the filename being js.js was a total red herring. Sorry about that! What was happening is my package needs to create files on the filesystem and then the tests assert the files are there or have changed in some way. I was writing those files to tmp/tests, and it wasn't a problem because the files were all .css, .html, or .html.jsx files. But when I started testing .js files, creating a .js file in the test triggered another re-run. \ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0f\nSo the solution is I just needed to exclude tmp from the source files being watched in my package.json like so:\njson\n{\n  \"ava\": {\n    \"source\": [\n      \"!tmp/**/*\"\n    ]\n  }\n}\nAgain, sorry about that! Thanks for your help!. ",
    "Briantmorr": "I'm just getting into the open source community, and I'd love to work on this!. Thanks @novemberborn ! I submitted the pull request late Tuesday Night, but I just realized I should also post here for visibility. Please let me know if my changes were what you had in mind! . I pushed again with updated changes. Minor semantic detail, I changed the word 'folder' to directory for clarity. . Thanks for taking the time to explain @novemberborn . I greatly appreciate it! I just updated with your suggested changes!\n. Thanks! Will do! . ",
    "okmanl": "Hello, I'm new to open source contribution. Would it be possible for me to work on this issue? Thank you.. ",
    "TheDancingCode": "@okmanl \nJust checking in: are you still working on this? Otherwise I'd like to have a go.. I took this on because I thought it was a rather easy issue, but I'm afraid it's a bit over my head. I don't understand the structure of AVA well enough and thus lack what it takes to be \"a litte more creative\". Maybe we should close this and let someone else try.\nSorry for that.. I guess so.\nI'll close this for now. Who knows, if inspiration strikes, I'll get back to this again. ;). Thank you for the pointers. I don't manage to update the logs however. Maybe it's a Windows thing, but I'm getting: 'UPDATE_REPORTER_LOG' is not recognized as an internal or external command,\noperable program or batch file..\nEDIT: never mind, I found a solution.. Should be good now. :). ",
    "F1LT3R": "Thanks @novemberborn - I've whittled things down a bit, and created a test case for you:\nAVA iTerm2 - No 1337 IMG Support\nhttps://github.com/F1LT3R/ava-iterm2-no-1337-img\nI'm happy to try #1722 if you can give me instructions on how to do this? If it's complicated, perhaps you might find it easier to run my test case than explaining it to me. :)\n. Thanks, @novemberborn - makes sense.\nGood to know about t.log, I will try that out.. The things that makes this really tricky @novemberborn - I'm writing an image diff library to run in AVA, so you get image diffs in the CLI as colored ANSI output. Logging via t requires passing t to my library, which feels really weird. I wouldn't want the user to have to pass t.log into the function that calls my library just so they could see the correct visual output.\nHope that makes sense.\nWhat works is if I pass in t and then call t.log from within my library. But what fails is pass t.log and calling that function, because I get:\nTypeError {\n    message: 'Cannot read property \\'addLog\\' of undefined',\n}\nThis also makes my test look like this...\njs\ntest('Scorecard grid', async t => {\n    const img1 = 'fixtures/green-circle.jpg'\n    const img2 = 'fixtures/green-circle.png'\n    const opts = {\n        grid: {\n            columns: 8,\n            rows: 8\n        },\n        tolerance: {\n            hue: 1,\n            sat: 20,\n            lum: 0.2,\n            alp: 0\n        },\n        display: {\n            images: 32\n        },\n        $MODE: 'CLI',\n        // Can't add T here (merging kills the function)\n        t\n    }\n    // I'm forced to pass t in as a param\n    //            passing in t here -----------\\ /\n    const result = await fuzi(img1, img2, opts, t)\n    t.true(result.fail)\n})\nThen inside my library:\njs\n    if (t) {\n        t.log(ansiImgDiff)\n    }\nThis is far from ideal. I would have to ask the user to pass t into every call to the library.\nThis is output that should only be being seen in CI. Putting some kind of \"if it fails, then t.log(all the image diff info)\" into each test, would be so cumbersome.\nAt the moment this is making me wish I'd stuck with Mocha, but I'm hoping I can find a workaround because I prefer AVA.\n. > \"I am tempted to make all worker->AVA communication go over IPC, rather than having some go over stdout / stderr, but no guarantees on whether that'll actually happen.\"\nIs this something I could do? Can you point me in the right direction @novemberborn ?. Ok thanks @novemberborn . In case this helps anyone:\n\nCalling t.log() from within my test file test('x', async t) logs long output strings correctly\nCalling t.log() directly from within an imported library still seems to cause long output strings to get corrupted\n\nThe least painful workaround I have for now is to add a reporter option to my tests:\n```js\ntest('My img diff test', async t => {\n    const img2 = 'fixtures/green-circle.jpg'\n    const img1 = 'fixtures/green-circle.png'\nconst opts = {\n    tolerance: {\n        hue: 1,\n        sat: 20,\n        lum: 0.2,\n        alp: 0\n    },\n\n    // PASS IN A REPORTER CALLBACK OPTION HERE\n    reporter: err => {\n        t.log(err)\n    }\n}\nconst result = await fuzi(img1, img2, opts)\nt.true(result.fail)\n\n```\nThen I can pass this back into the context of test(), which then gives the inline output correctly when tests fail:\n\nIt appears that t.log() is already using IPC? It definitely feels a lot slower using t.log() than console.log(), but at least the output is correct.. Is there a trick to get t.log to work in CI?\nEven though t.log helps locally, things still look ugly remote:\nhttps://travis-ci.org/F1LT3R/fuzi/builds/368486782\n\nLocally with t.log this looks like:\n\n. It's not images being output. It's ANSI characters.\nFrom what I'm seeing, IPC might not be doing what you expect on Linux. (Assuming you would expect 0 corruptions over IPC.)\nNote: even if I remove all ANSI escape sequences from the t.log calls, I still see corruptions in CI.\n. Have you tried putting the glob in quotes? \nbash\n\"test\": \"ava \\\"src/**/*.test.js\\\"\"\nIt is possible your shell might not be passing the pattern in the way you expect.\n   . ",
    "malimccalla": "@novemberborn I am using VS Code v 1.22.1. This is what Im getting before changes for all assertions\n\nAnd this afterwards \n\n. \ud83d\ude05. ",
    "szmarczak": "@novemberborn You're right. Sorry, I haven't studied p-queue enough :P. > I don't think that would work for asynchronous tests: multiple tests would be running concurrently so there wouldn't be any one currentTest.\nHmm, that's true, unfortunately.\n\nBesides the code sample, could you elaborate on your use case?\n\nYou just need to pass the test instance every time, which is quite irritating. I often did compare(entries, ...) instead of compare(t, entries, ...).. @novemberborn That whould be the right thing :). ",
    "harrysarson": "ah I see, so the whole pronouciation should have been wihin the code block?\nLike:  /\u02c8e\u026av\u0259/ ay-v\u0259\nOr should the second / be at the end too?. @boneskull they are one step ahead of you it seems: #1760. @alexfqc If you strip out everything to do with Enzyme do you still get this error message? . I had a look at https://github.com/hughsk/path-sort which is the dependancy added in this Pr.\nThere is a pull request there that has been open for a year without any response so I dunno if the package is maintained at all.\nJust thought I should flag this up, I like the idea of split tests though :) . ",
    "nothingismagick": "Have you tried to use the babel bridge repo?\nhttps://github.com/babel/babel-bridge\nyarn add babel-core@^7.0.0-0. ",
    "xxxxxMiss": "I also experienced it. I tried many times and i can confirm the config\n\"babel\": {\n      \"testOptions\": {\n        \"babelrc\": false\n      }\n    } \ndon't  working.\nThan i use .babelrc and installed babel-core@^7.0.0-0 and run test, it throws an error:\nError: Requires Babel \"^7.0.0-0\", but was loaded with \"6.26.3\". If you are sure you have a compatible version of @babel/core, it is likely that something in your build process is loading the wrong version. Inspect the stack trace of this error to look for the first entry that doesn't mention \"@babel/core\" or \"babel-core\" to see what is calling Babel.. @novemberborn Sorry that i view issues to it and commented BTW. About my that issue, i give a demo link here. @novemberborn OK, no problem.. ",
    "ashimagarwal": "Hi Ava Team,\nI would love to update the correct register module based on how source files are compiled.\nCan you please guide me on the process. (New to the OSS community).\n. ",
    "Isikiyski": "Well, yes I need the lines which are marked in red in the terminal (using Mac) where the assertion has failed. \nI need those particular lines written in a file.. Yes, good idea, just will you please provide me with a link to the documentation/repo?\nI googled but got confused of the different tools that appeared.. ahhh..... sorry for my absent mindness and the spam... This is part from the AVA itself...\nThanks a lot buddy!. ",
    "hallettj": "\nAre you saying that if we drop interoperability with Flow v0.69 we can support .then(null, func)? I'd rather be more fully compatible with how people write JavaScript than support older Flow versions.\n\nYes - but the PromiseLike interface in the AVA type definitions serves a narrow role. I argue that it is not important for that interface to support all valid use cases for promises.\nPromiseLike is one of the types that AVA allows to be returned from a test functions, before and after functions, etc. AVA accepts PromiseLike values, but does not produce them. There is no case where a user would be given a value where they expect to be able to call .then(null, func) but it does not work. I think that makes PromiseLike a good candidate for a lowest-common-denominator interface, such as the one that I suggested in this pull request.. I can reproduce the error with Flow v0.71.0 and both AVA v0.25.0 and AVA v1.0.0-beta.4. But the problem is fixed by #1778.. By the way, it is possible to back-port the fix from #1778 to v0.25.0 by making nearly the same change:\n```diff\ndiff --git a/index.js.flow b/index.js.flow\nindex d214706..76d588f 100644\n--- a/index.js.flow\n+++ b/index.js.flow\n@@ -4,9 +4,9 @@\n  * Misc Setup Types\n  */\n-type PromiseLike = {\n+export interface PromiseLike {\n    then(\n-       onFulfill?: (value: R) => Promise | U,\n+       onFulfill: (value: R) => Promise | U,\n        onReject?: (error: any) => Promise | U\n    ): Promise;\n }\n``. tl;dr: I think that the simplest option is to splitthrowsinto multiple assertions:throwsfor functions that should throw a synchronous error,rejectsfor functions that should return a rejected promise, andemitsErrorfor functions that return an observable that should emit an error. The assertions for a rejected promise and an error-emitting observable could reasonably be combined into one. But I don't see a good way to overloadthrowsto handle both synchronous and asynchronous behavior. Another option is to changethrowsso that it always returns the error in aPromise`.\nThe root of the issue is overloading a function signature in a way that requires Flow to look ahead to determine which signature to select. In theory Flow could figure this out if it considered all possible signatures, and backtracked as necessary. In practice the Flow team have found that too much backtracking leads to unacceptably slow type checking. So they limit the number of steps Flow will look ahead before it must identify the correct \"path\" to choose. At that point if Flow cannot unambiguously pick a signature then it reports an error.\nThe type for the signatures of t.throws looks like this (with most comments removed):\n```js\nexport interface ThrowsAssertion {\n    (fn: () => ObservableLike, expectations?: null, message?: string): Promise;\n    (fn: () => ObservableLike, constructor: Constructor, message?: string): Promise;\n    (fn: () => ObservableLike, regex: RegExp, message?: string): Promise;\n    (fn: () => ObservableLike, errorMessage: string, message?: string): Promise;\n    (fn: () => ObservableLike, expectations: ThrowsExpectation, message?: string): Promise;\n    (fn: () => PromiseLike, expectations?: null, message?: string): Promise;\n    (fn: () => PromiseLike, constructor: Constructor, message?: string): Promise;\n    (fn: () => PromiseLike, regex: RegExp, message?: string): Promise;\n    (fn: () => PromiseLike, errorMessage: string, message?: string): Promise;\n    (fn: () => PromiseLike, expectations: ThrowsExpectation, message?: string): Promise;\n    (fn: () => any, expectations?: null, message?: string): any;\n    (fn: () => any, constructor: Constructor, message?: string): any;\n    (fn: () => any, regex: RegExp, message?: string): any;\n    (fn: () => any, errorMessage: string, message?: string): any;\n    (fn: () => any, expectations: ThrowsExpectation, message?: string): any;\n    (promise: ObservableLike, expectations?: null, message?: string): Promise;\n    (promise: ObservableLike, constructor: Constructor, message?: string): Promise;\n    (promise: ObservableLike, regex: RegExp, message?: string): Promise;\n    (promise: ObservableLike, errorMessage: string, message?: string): Promise;\n    (promise: ObservableLike, expectations: ThrowsExpectation, message?: string): Promise;\n    (promise: PromiseLike, expectations?: null, message?: string): Promise;\n    (promise: PromiseLike, constructor: Constructor, message?: string): Promise;\n    (promise: PromiseLike, regex: RegExp, message?: string): Promise;\n    (promise: PromiseLike, errorMessage: string, message?: string): Promise;\n    (promise: PromiseLike, expectations: ThrowsExpectation, message?: string): Promise;\n/** Skip this assertion. */\nskip(thrower: any, expectations?: any, message?: string): void;\n\n}\n```\nThere are only two possible return types; so a lot of these signatures can be combined without losing \nprecision, which does make things somewhat easier for Flow. We just need to express that if the first argument is a function that returns a PromiseLike or ObservableLike then the return type is a Promise; otherwise the return type is not a Promise (which is expressed here as any but perhaps could be given as ?Error); and there are some requirements on what may be given for the optional second and third arguments:\n```js\ntype ErrorExpectations = Constructor | RegExp | ThrowsExpectation | string\nexport interface ThrowsAssertion {\n    (\n        fn: (() => PromiseLike) | (() => ObservableLike),\n        expectations?: null | ErrorExpectations,\n        message?: string,\n    ): Promise;\n    (\n        fn: () => any,\n        expectations?: null | ErrorExpectations,\n        message?: string,\n    ): any;\n/** Skip this assertion. */\nskip(thrower: any, expectations?: any, message?: string): void;\n\n}\n```\nBut that still does not solve the problem of #1790! There are still two signatures to choose between, and the second signature will always be valid if the first signature is valid. If we change throws to behave only according to the second signature, and create a new assertion called rejects with the behavior of the first signature that would solve the problem.\nBut a nice feature of async / await is that it makes dealing with async exceptions pretty similar to dealing with sync exceptions, and it would be unfortunate if the test framework could not accommodate that analogy. Another option is to change throws so that it returns a Promise even on synchronous exceptions:\n```js\nexport interface ThrowsAssertion {\n    (\n        fn: () => any,\n        expectations?: null | ErrorExpectations,\n        message?: string,\n    ): Promise;\n/** Skip this assertion. */\nskip(thrower: any, expectations?: any, message?: string): void;\n\n}\n```\nIt could be slightly annoying to have to use async code to wait for synchronous\nexceptions in cases where the test needs to reference the exception that was\nthrown. But it would make the type ambiguity problem go away.. > Similarly, we support tests that return thenables, rather than promises. Typing these \"promise-like\" objects is also more difficult than expected (#1778). At the very least we could change the typings to expect a Promise.\nI think this is reasonable. The majority of cases will be async test functions, which return a native promise. Third-party promise implementations will likely have an option to convert a third-party promise into a native one.\nUnfortunately Flow does not provide a promise interface - the definition for the native Promise is a class, because promises can be constructed, and Promise has static methods. It would be nice if they shipped a Thenable interface compatible with the native promise definition.\nOn the other hand I think that the change I made in #1778 is a reasonable solution. I did see the note that you pointed out from a76d462:\n\nThis seems to have broken using async-await\n\nThe whole point of the change was to fix uses of async-await, and it works in my testing. I don't know what is going on in @jamiebuilds' case.\n\nI don't quite think it's worthwhile enforcing tests to return proper Promise instances rather than thenables, and properly typed promise implementations should match the Promise interface anyhow.\n\nI tend to agree; but I think that both options are reasonable.. > We could refine this to Error, however if the assertion fails it returns undefined. I may have typed it as any so you can cast it to an Error if appropriate without really having to worry about type safety in case of an assertion failure.\nSo it could be Error | void to indicate that the value may or may not be a defined error.\n\nAn added complication is that when used with a promise / observable, you must await the assertion. It'd be weird to not mandate that for synchronous exceptions.\n\nI think it would work to fail the test synchronously if an error is thrown synchronously, but to require the user to wait on a promise to examine the error that was thrown regardless. In any case that would make the types work out.\n\nWould Flow's Promise class match say a Bluebird promise? I think that'd be sufficient.\n\nYes, because Bluebird's promise is a subclass of the native Promise. Or at least it is declared to be a subclass in the type definitions on flow-typed.\n\nTo recap, @hallettj are you saying that if we had an t.errors() that always returns a promise, we can overload that interface to accept observable-like things, streams, async iterators, regular functions, and promises? And then we'd have t.throws() always be synchronous?\n\nYes, exactly - if we can limit the assertion type to a single signature then that will fix the problem. If the return type is always the same for a given assertion then we can use one signature with type unions for argument positions to accept various observable-like arguments, various different matchers, etc.. >> Similarly, would it be a terrible bother if our typings were stricter on promises?\n\nI would be fine with this if we used a Object.prototype.toString.call(promise) (this would allow libs to set Symbol.toStringTag) type check and not instanceof.\n\nFWIW I think that the permissive PromiseLike interface as of #1778 is the way to go. It describes the minimal shape of a promise, and matches up with the runtime checks that the is-promise package performs.\nI think that trouble in front of us is not due to descriptions of promises, but is due to Flow not having enough information to choose between multiple function signatures in some cases.\n\n\nIf you're using observables or generators, how would you feel if we'd remove first-class support?\n\nI don't use either, but I've seen a lot of people use the observable support in their tests implementation. I'm fine with removing observable support from t.throws though. It always felt out of place. And I think we should definitely remove support for generator functions in the test implementation. It's really just a leftover from the early days when generators were a popular alternative to async/await.\n\nThis all seems sensible to me. Especially the bit about generators.\nI use a lot of reactive streams that are in theory compatible with the Observable interface. But the Observable standard is not in a solid place right now. Every project has to create its own version of the interface. I am more likely to use custom assertions that apply to the particular stream implementation that I am working with.\n. >> I think that trouble in front of us is not due to descriptions of promises, but is due to Flow not having enough information to choose between multiple function signatures in some cases.\n\n@hallettj does this problem go away if we remove ObservableLike?\n\nNo; even if there are only two options, a function that returns a promise (() => PromiseLike<any>), or a function that throws an error synchronously (() => any), Flow does not have enough information to choose one signature because the second option will be applicable in every case where the first is applicable. What we would need to disambiguate would be a type like () => any type that is not a promise. Unfortunately Flow does not have a way to express that.\n. >> even if there are only two options, a function that returns a promise (() => PromiseLike), or a function that throws an error synchronously (() => any), Flow does not have enough information to choose one signature because the second option will be applicable in every case where the first is applicable. What we would need to disambiguate would be a type like () => any type that is not a promise. Unfortunately Flow does not have a way to express that.\n\n@hallettj oh so you're saying the problem is the any return type?\n\nI'm sorry; the problem is not the return type of t.throws() - it is the return type of the function that is given to t.throws(). If that is restricted to something other than any or mixed then users would get errors that they probably do not expect.\n. ",
    "emilingerslev": "Would it be possible to create a new beta release with this? That would be awesome because the current release is getting a bit old and this issue breaks stuff if you go async.. ",
    "cortopy": "Makes sense. I've changed to serial, and that did the trick for now at least. Thanks!. ",
    "lunelson": "I just ran in to this, and was rather hoping that snapshots were separate within a single file, so only the --match'd test snapshots would update and not the others.\nThe thing is, I'm kinda hacking AVA to test Sass code: my tests are in separate .scss files but collected and run, each test named for its source filename, by a single JS test file; so I use --match to focus on one .scss file at a time.\nMaybe there's a better way of collecting and running Sass tests and keeping them as separate tests, without writing both .scss and .js files for each test?. Thanks @novemberborn I've tried to summarize it in the Spectrum chat. ",
    "erikkemperman": "From reading earlier discussion I thought I might ping @develar -- hope that's not too presumptuous of me.. Thanks for the quick follow-up!\nYes, I would prefer to keep AVA config in package.json, and only there. And, conversely, I would prefer to not have any WebStorm-specific stuff in my package.json.\nBut the way I understand things currently, I can either follow the \"Setup using Node.js\" section in the recipe, which doesn't use NPM (and thus doesn't look at package.json, requiring me to duplicate AVA config in my IDE), or otherwise I can follow the \"Setup using NPM\" section in the recipe which appears is not applicable for latest WS -- and to make it work I have to add IDE-specific stuff to package.json.\nPerhaps there is no better way to do this, but this situation isn't ideal. If I'm missing something obvious that'd be great. Either way, I think the recipe could be improved (slightly or significantly, depending on what I find out).\nI guess I'll wait to see if any WebStorm expert chimes in here, and take it from there? As mentioned, there isn't much urgency for me: I can continue developing just fine, it's just not very pretty as it is now.\n. I will try that but expect it won\u2019t make a difference \u2014 I don\u2019t think webstorm even looks at package.json at all in this \u201crun/debug with node\u201d mode, as opposed to the npm alternative? I would prefer to use the npm variant and keep everything neatly in package.json but as far as I can tell that requires me to use a pretty ugly way of invoking AVA (with a WS specific environment variable).. All right, thanks for the suggestion! I've tried with all AVA config in toplevel object in package.json, but it makes no difference unfortunately.. > which itself then looks at the package.json#ava field for its options.\nAh! I see, hadn't thought that far ahead, thanks for clearing that up. Perhaps I should have been more explicit though in my original post here, the issue isn't that it doesn't run AVA -- it does -- but that it doesn't break at my breakpoints unless I change\njson\n\"scripts\": { \"test\": \"ava\" }\nTo\njson\n\"scripts\": { \"test\": \"node $NODE_DEBUG_OPTION node_modules/.bin/ava\" }\nThis works but it's ugly, imho. But given that AVA itself inspects the package.json I suppose I could try to somehow hide this ugly $NODE_DEBUG_OPTION somewhere in the AVA config, assuming I can affect the execArgv that spawned processes get?. Thanks again for your quick reply @novemberborn.\nIt has to be node $NODE_DEBUG_OPTION ava, or at least according to my experiments thus far.\nI did notice the logic you mention, was actually thinking of making a PR on top of that to merge in stuff from this env var if present. But I hesitate, on the one hand because I don't really grok all that is going on there (or more accurately I would have thought it should work differently than what I see there).\nAnd on the other hand it'd be addressing this issue for ava specifically when actually it affects every npm script I might want to debug from WebStorm. I am currently thinking about patching my npm-cli locally to merge $NODE_DEBUG_OPTION into execArgv when spawning subprocesses.. Yeah, so I guess the point there is that some of the proces.argv of the main ava thread should be mapped to execArgv of the workers.\nRegarding WebStorm I\u2019m inclined to give up \u2014 they apparently make a point of fixing their own debug port and make it available via that $NODE_DEBUG_OPTION. Which might be well and good but I don\u2019t see a pretty way of injecting that in the \u201cDebug via NPM\u201d variant in the recipe.\nI found two methods but both have to be done separately for each package script I might wish to debug (the one I mentioned earlier which is to insert the env var into package.json and another which hacks the shebang line in .bin/ava.\nI think the other issue you linked won\u2019t (can\u2019t) help with having to figure out which port WS picked for a given run.. I\u2019ll give updating the recipe a try, sure. The debug port forwarding is not going to work \u2014 if I understand things properly. But maybe I don\u2019t :-). Sorry, this completely fell off my radar, and I am swamped for the time being. If this isn\u2019t actually appearing to bother many, perhaps simply close the issue?. ",
    "stavalfi": "Any progress?. @novemberborn thanks, I will do that.\n\nIf this isn\u2019t actually appearing to bother many, perhaps simply close the issue?\n\n@erikkemperman it's a bug. Don't close this issue, even when only a single person suffers from it.. ",
    "maritz": "It would fix my specific issue, for sure. Not sure it's the best long-term solution for everyone though.\nI am curious though: Why would printing an error and ignoring potentially cause breakage in your opinion?. >you're left wondering why helper compilation stopped working.\nWell, it would log out a warning about the inaccessible directories. But if the user makes the directory that contains his helpers inaccessible (or something else does), it's not gonna work anymore anyways, right?!. Oh. that makes sense. \ud83d\udc4d . ",
    "xtx1130": "I wrote test cases with es modules, If I don't add babel-register, it will case error. \nI have deleted source-map-support and update ava@0.25.0, nyc@11.7.3. \nThere still has the same performance. \nWhen I use ava -v with ava@0.25.0, the error stack is:\nshell\nUnhandled Rejection: packages/tb-lavas-core/test/unit/config-reader/development.test.js\n  TypeError: Cannot read property 'children' of undefined\n  packages/tb-lavas-core/core/utils/router.js:83:37\nBut when I use nyc ava -v with ava@0.25.0, the error stack is:\nshell\nUnhandled Rejection: packages/tb-lavas-core/test/unit/config-reader/development.test.js\n  TypeError: Cannot read property 'children' of undefined\n  packages/tb-lavas-core/core/utils/router.js:10:913\nWhen I use ava -v with ava@0.18.0, the error stack is:\nshell\nUnhandled Rejection: packages/tb-lavas-core/test/unit/config-reader/development.test.js\n  TypeError: Cannot read property 'children' of undefined\n    packages/tb-lavas-core/core/utils/router.js:49:41\n\nOnly ava -v with ava@0.18.0 give me the right error stack.. The same issue. @sindresorhus https://github.com/avajs/ava/issues/1800. ",
    "dmitriz": "Thanks, that is awesome!. ",
    "isnifer": "I will try to create reproducible demo. Did some debug and found \u2014 this line https://github.com/avajs/ava/blob/1e0e8e84dbc8ed70286d01dd9c8d290f30533aad/lib/worker/precompiler-hook.js#L19 returns null as a string.\nIt brokes log.\nIf I change locally to:\n```js\nif (sourceMapCache.has(source)) {\n  const map = fs.readFileSync(sourceMapCache.get(source), 'utf8')\nif (map !== 'null') {\n    return {\n      url: source,\n      map: map\n    };\n  }\n}\n``\nThen there is no problem. @novemberborn yes, I will. Just need some free time. \nAlso, I will check it withava@next`. Thank you for your response.. @novemberborn \n\nDo you happen to have source maps disabled in your Babel options?\n\nNo. ",
    "bailer": "To fill in for @iamstarkov: Coming from Jest, the feature I miss the most is the interactive mode that Jest has. It really makes updating one snapshot at a time a breeze though that has nothing to do with storing the snapshot as a binary. \nThough a problem I found with binary snapshots is when you update a snapshot in two different branches which comes from the same origin branch. Git can diff and merge the markdown files fine but there is always a merge conflict with the binary. What is your thoughts on this?. ",
    "chocolateboy": "@novemberborn I don't think t.expr (assert that its argument is an expression?) is clearer or more descriptive than t.assert. What other test frameworks use that?\n(P.S. I think this is a feature/enhancement request rather than (or as well as) a question :-). > As a general rule we don't alias assertions, so since we already have t.truthy() we wouldn't introduce t.assert().\nI think there are already aliases, e.g. t.true is effectively an alias for t.is(value, true). But if this rule is really non-negotiable, why not just rename t.truthy to the standard t.assert (and t.falsy to e.g. t.refute)?\n\nHowever if we were to restrict power-assert to a single assertion, then perhaps t.expr() communicates \"assert that the expression in the assertion call gives truthy, or else we'll print lots of detail about what was in the expression\", and I think that would be sufficiently different from t.truthy() that we could introduce it.\nwill add such labels when we come to a satisfactory conclusion on this.\n\nJust to clarify a few things:\n1) There are two parts to this proposal: a more convenient API (t(...)) as well as a more explicit and descriptive API (t.assert(...)). They are (of course) the same function, so the parts are linked, but I believe both are important as far as improving the readability, usability, and discoverability of power assertions in AVA is concerned.\n2) I don't agree that t.expr is a better name for these assertions than t.assert. In fact, I think it's even less clear and less descriptive than t.true or t.truthy. Perhaps the t.expr discussion could be moved to a separate issue (e.g. a question) so that this proposal can be considered on its own merits as an enhancement?. > I was just raising the possibility of restricting power-assert to a particular assertion.\nI don't think that's incompatible with this proposal.\nBefore\njavascript\nt.true(...) // power assert\nt.truthy(...) // power assert\nt.false(...) // power assert\nt.falsy(...) // power assert\nAfter\njavascript\nt.assert(...) // power assert\nt(...) // power assert\nThe implicit power-assertion reporting can be uncoupled from t.truthy etc. if there's an explicit function.. > I think that would make power-assert easier to understand, and provides a better argument for adding an additional assertion.\n\nWithout that restriction I'm not sold on the need to make any changes\n\nSince assert does what the other functions do (and more), I think it makes sense to make it explicit and convenient first and then to (re-)consider the utility of the other functions, rather than ruling it out on the grounds that other, redundant/overlapping functions already exist.. > We're adding a new assertion that is power-assert enabled. I think t.assert() is sufficient here. It should behave like t.truthy().\nSounds good!\n\nI think we'll want to keep t an object. That way it's easier to compose behavior on top of it without having to forward calls to t itself.\n\nNot sure I follow this. Do you have a particular extension/plugin in mind this change would make things harder for?\nAIUI, extensions wouldn't need to concern themselves with (or even know about) the fact that t === t.assert. They'd just reference t.assert.\nA function is still an (extensible) object, and this duality appears to be standard for assert, e.g. in node, commonjs-assert and power-assert itself, which describes the simplicity and convenience of this usage as a \"core value\":\n\nThough power-assert is fully compatible with standard assert interface, all you need to remember is just an assert(any_expression) function in most cases.\nThe core value of power-assert is absolute simplicity and stability. Especially, power-assert sticks to the simplest form of testing, assert(any_expression).\nAre you interested in working on this?\n\nI'll take a look if I have time, but I'm hoping someone will beat me to it :-). > Syntax like {...t, additional () {}} wouldn't work\nWhy wouldn't that still work?\njavascript\nlet t = () => {}\nObject.assign(t, { assign () { }, truthy () { } })\nlet t2 = { ...t, additional: 42 }\nconsole.log(Object.keys(t2)) // [ 'assign', 'truthy', 'additional' ]\n\nAlso t() is pretty obscure, compared to assert().\n\nWell, that's a consequence of AVA requiring the name to be t (currently). How hard would it be to (also) allow it to be assert?. > t2() would fail with a t2 is not a function.\nIf extensions are using a documented API to extend the t object, I expect AVA would automatically ensure the resulting object is a function.\nIf they're constructing new t-like objects on the fly without AVA's help, it could still easily expose whatever mechanism it uses to make the object callable e.g.:\n```javascript\nimport { callable } from 'ava'\nfunction extend (t) {\n    let t2 = { ...t, additional () { ... } }\n    return callable(t2)\n}\n```\nIn addition to the implementations listed above, there's also the apply trap of ES6 proxies, which are fully supported (without a flag) in AVA's oldest supported Node.js version (currently v6.14.2):\n``javascript\nfunction callable (value, method = 'assert') {\n    // dummy target as theapply` trap requires the\n    // target to be callable\n    const fn = () => {}\nreturn new Proxy(fn, {\n    apply (target, ctx, args) {\n        return value[method].apply(value, args)\n    },\n\n    get (target, name) {\n        return value[name]\n    }\n})\n\n}\nconst value = { assert () { return [ \"assert called!\" ] }, foo: 42 }\nconst proxy = callable(value)\nconsole.log(proxy.foo) // 42\nconsole.log(proxy())   // [ 'assert called!' ]\n```. To match the original test, shouldn't this be:\njs\nconst foo = 'bar';\nt.assert(!foo);. ",
    "eemed": "Hi! I would like to take a look at this. This is my first time contributing not only to this project but to any project. I will probably need some help at some point.. @novemberborn So it almost works, but I have 3 tests that won't pass and I can't figure it out. Should we discuss them here or should I make a pull request?. Updated tests and used .assert() in the test case. PR for babel-preset-transform-files is here.. I can take a look. Do I just commit doc updates to this pr aswell? . I'm not sure if there's more updating to be done. I tried to look through all the files but didn't find anything else.. I can take a look at it next weekend. . I passed in a string because the original test passed a string also.. ",
    "mgibson91": "Ahh great thanks. ",
    "PhilT": "Would it also require the restriction to only process *.js files to be relaxed?. ",
    "jim-king-2000": "Hi novemberborn, thank you very much for the reply. The --node-args CLI flag or the corresponding config object is exactly what we need. I'm OK that AVA only processes .js file. But I do need AVA support the statement like \"import xxx from './yyy.mjs'\" natively. Because it's a lot of extra work to introduce babel in test.. By the way, the esm package works well. Thank you again.. @novemberborn Fantastic! I tried the latest beta5 AVA and it loaded my test_.mjs successfully. My config file is like the following. No babel any more.\n\"ava\": {\n    \"require\": [\n      \"esm\"\n    ],\n    \"babel\": false,\n    \"extensions\": [\n      \"mjs\"\n    ]\n  },. @novemberborn I don't know how to implement it, however, I'd really like to digest \"test_.mjs\" with empty config file or with a little bit more characters as the following:\n\"ava\": {\n  \"node-args\": [ \"--experimental-modules\" ]\n}\n. It seems that AVA has it's own module load mechanism. So, when es module becomes the fully supported feature by node and can be used without the \"--experimental-modules\" argument, AVA still requires \"esm\" to be installed. Is it right?. @novemberborn Great! I think it would be soon for node to support es6 module officially. Then AVA would work without installing \"esm\" module. Thank you for the help all along.. Hi @novemberborn , long time no see. Shall we let AVA run .mjs without \"esm\" plug-in now? The \"esm\" plug-in can work, but the error reported by AVA is based to the transpiled code which is hard for us to find the true culprit in our source code.. What bothers us mostly is the line of the errors reported by AVA is NOT the line in our source code when using -r esm. How can we (or AVA) circumvent this issue? Shall we add some more plug-ins (which I don't know) to \"magically\" map the transpiled line to the original line in the error reports? Or should AVA add an experimental flags as nodejs does (usage becomes this: ava --experimental-modules ./test/*.mjs)?. Hi @novemberborn , I'm so sorry that I haven't made a minimal repro for the issue I reported. Now when I'm trying to do it, I find that it is not reproduced. So, it is almost perfect of AVA at this time. I'm still looking forward to the experimental flag(s), which would make it absolutely perfect.. Better if it could print the time consumption of every test cases.. throwsAsync is added.. throwsAsync is added.. ",
    "bradennapier": "Your system breaks if using @babel/preset-stage-*  - doesnt matter if i turn off anything or change any settings - it will not work. Fixed - babel false and compileEnhancements false - yall should fix that - sucks to conflict with your setup just to run tests and all that.\n. ",
    "goooseman": "I'm working with ava and typescript using following config:\njs\nexport default {\n  compileEnhancements: false,\n  extensions: [\"ts\"],\n  files: [\"test/**/*.ts\", \"src/**/*.spec.ts\"],\n  require: [\"ts-node/register\", \"./test/helpers/enzyme.js\", \"./test/helpers/module-alias.js\"],\n};\nAnd just launch ava from ava binary (npx ava) works great.\nnpx ava test/app.spec.ts - works ok.\nBut when I launch it through profile.js, it does not work:\nnode node_modules/ava/profile.js test/app.spec.ts\nOutputs:\nSyntaxError: /Users/goooseman/dev/myProjects/boodka-desktop/test/app.spec.ts: Unexpected token (6:18)\n    at Parser.raise (/Users/goooseman/dev/myProjects/boodka-desktop/node_modules/@babel/parser/lib/index.js:3938:15)\n    at Parser.unexpected (/Users/goooseman/dev/myProjects/boodka-desktop/node_modules/@babel/parser/lib/index.js:5247:16)\n    at Parser.parseVar (/Users/goooseman/dev/myProjects/boodka-desktop/node_modules/@babel/parser/lib/index.js:7768:18)\n    at Parser.parseVarStatement (/Users/goooseman/dev/myProjects/boodka-desktop/node_modules/@babel/parser/lib/index.js:7594:10)\n    at Parser.parseStatementContent (/Users/goooseman/dev/myProjects/boodka-desktop/node_modules/@babel/parser/lib/index.js:7194:21)\n    at Parser.parseStatement (/Users/goooseman/dev/myProjects/boodka-desktop/node_modules/@babel/parser/lib/index.js:7144:17)\n    at Parser.parseBlockOrModuleBlockBody (/Users/goooseman/dev/myProjects/boodka-desktop/node_modules/@babel/parser/lib/index.js:7695:23)\n    at Parser.parseBlockBody (/Users/goooseman/dev/myProjects/boodka-desktop/node_modules/@babel/parser/lib/index.js:7682:10)\n    at Parser.parseTopLevel (/Users/goooseman/dev/myProjects/boodka-desktop/node_modules/@babel/parser/lib/index.js:7109:10)\n    at Parser.parse (/Users/goooseman/dev/myProjects/boodka-desktop/node_modules/@babel/parser/lib/index.js:8495:17)\n    at parse (/Users/goooseman/dev/myProjects/boodka-desktop/node_modules/@babel/parser/lib/index.js:10448:38)\n    at parser (/Users/goooseman/dev/myProjects/boodka-desktop/node_modules/@babel/core/lib/transformation/normalize-file.js:170:34)\n    at normalizeFile (/Users/goooseman/dev/myProjects/boodka-desktop/node_modules/@babel/core/lib/transformation/normalize-file.js:138:11)\n    at runSync (/Users/goooseman/dev/myProjects/boodka-desktop/node_modules/@babel/core/lib/transformation/index.js:44:43)\n    at Object.transformSync (/Users/goooseman/dev/myProjects/boodka-desktop/node_modules/@babel/core/lib/transform.js:43:38)\n    at filename (/Users/goooseman/dev/myProjects/boodka-desktop/node_modules/ava/lib/babel-pipeline.js:201:29)\n. @novemberborn wrote a little PR to fix that. ",
    "cantremember": "on a possibly related note ... i nearly had had to downgrade my package.json's 'esm' configuration from { mode: \"strict\" } to \"auto\" because\n- (at least according to the documentation I have read) there is no ava --config <CONFIG-FILE> CLI option,\n- the file must be named 'ava.config.js'\n- \"strict\" enforces an '.mjs' extension on files which use ESM syntax\nif a CSJ export was supported, this wouldn't be an issue.  fortunately i have the fallback to use the { ava } configuration option in package.json -- and that allows me to keep \"strict\"\non a related but tangential note, i have (so far) had no success in telling ava to glob for 'test/ava/*.mjs'\n[x] Couldn't find any files to test\ni have just started to integrate ava, so i could be making foolish mistakes / not RTFM properly. +1\nin my case, as in my comment against #1820, i can't use esm + { mode: \"strict\" } if 'ava.config.js' doesn't have an .mjs extension (because currently the file must use ESM export syntax)\nava --config ava.config.mjs or some equivalent would be very helpful.  until then, i am relegated to using { ava } in package.json. ",
    "ehmicky": "\n@novemberborn \nLet's use this issue to see if there are any problems and then we can add support for module.exports = {}.\n\nI am running into the following problem related to not being able to use module.exports:\nI need to use an ava.config.js. It has some logic in it, i.e. I cannot use an ava property in package.json instead. My codebase is written with CJS. This leads me to run ESLint on my repositories JavaScript files with the sourceType option set to script (not module).\nThis results in the following problem: ava.config.js cannot be parsed by ESLint because it's written with ES modules while the other files are using CJS. My current workaround is to fire ESLint twice with different options, but this is not great and complicates my setup quite a bit when coupled with files watching.\nAllowing the full compatibility mode of esm would solve this problem.. Thanks! This works.. Sorry! I did search the issues twice but I must have used the wrong keywords. Thanks for the quick reply.. Actually the solution outlined in #1857 might not work for my use case because the package.json is looked up based on the current directory :(\nThis would require me to add an ava property in all my repositories, which is going back to the original problem.. Oh great then! From this comment:\n\nHaving to specify the --config argument would be bothersome. So I'm \ud83d\udc4d on a package.json#ava.config option\n\nI thought the --config CLI option was not part of the solution, but if it is then this completely solves this issue.. ",
    "minhchu": "@novemberborn thanks for your answer\nAfter adding sources to ava config it now works as expected. ",
    "Adriang71": "Thank for your replay, it is configured ava this way because I have application and test setup in two different repositories.. ",
    "coreyfarrell": "Thanks for replying.  I'm not ignoring you it's just taking me a bit to digest 1692.  I'm not sure yet how that would allow me to accomplish my goals which is to manipulate the reporting functionality of t.snapshot (both the console output and the test.js.md report).\nThat said this idea of what I need has had some time on the back-burner so I'm starting to think of it a bit differently.  I'm thinking for the test.js.md output I could probably reformat the base64 png images so they contain the proper markdown:\njs\nt.snapshot(`![](data:image/png;base64,${imageBase64}]`);\nIf I need to look at two versions I can just copy the md to another file before updating it, render both and view in the browser.  But this still means that upon failure I'll get two giant base64 dump's to the console output.. @novemberborn Thanks for pointing me to the right place, was a pretty easy fix.  I'm not sure how/if this could be tested?. Yes that seems like a better place for it to be.. @sholladay __filename is only correct from the test file itself.  The point of this bug is to be able to know the test path from helper sources - without needing to pass __filename to helper functions.  For the same reason import.meta.url will not solve this issue.. @sholladay I've submitted a patch for the functionality in #1977, was pretty straight forward.. @novemberborn have you had a chance to consider my latest patch?. @novemberborn how/where do you think this feature should be documented?. > We'll need to add this to the TypeScript and Flow definitions as well.\nI've added it.  Up front I don't know TypeScript or Flow at all.  This didn't break existing tests but I haven't added any new TS or flow tests. . My hope is for the final report to report to state that tests for unavailable browsers to be 'skipped'.  Specifically when a specific browser cannot be run it should not report pass or fail for the associated tests.\nBTW I've edited my example code to show that you would return from the test after calling t.skip().  I'm open to different naming, maybe t.skipped() or t.dependencyMissing()?. Agreed that it should be mandatory to return after calling this function.  One hesitation about t.warn is that you have t.log.  I've always felt that t.warn should exist and be similar to console.warn the same way t.log is like console.log.  Not sure if that could cause confusion?. t.cancel definitely seems better than t.warn.\n\nVery interesting, I didn't know it was possible to defer the first registration to ava.  This might be a solution to my specific problem, though honestly something like t.cancel would be easier.. I've tweaked the way my tests are declared so they do not get created until after the browser is successfully started, though this does have one drawback.  An example test declaration is:\n```js\npage('check-text.html', async t => {\n    const {selenium, checkText} = t.context;\n    const ele = await selenium.findElement({id: 'test'});\nawait checkText(ele, 'This is a test!');\n\n});\n```\nThe page function adds the title and callback to an internal array, then after the browser is successfully started it performs a bunch of calls to test() or test.skip().  If this results in an exception due to duplicate test name then the backtrace is wrong, so having page directly call test() with an implementation that conditionally calls t.cancel() would be preferable for the purpose of duplicate title error reporting.  This is a minor issue though.. So in my use case t.cancel() would be called before any t assertions and be followed by an immediate return:\n```js\ntest('test with async resolvable external dependency', async t => {\n  let externalSystem;\n  try {\n    externalSystem = await startExternalSystem();\n  } catch (error) {\n    t.cancel();\n    return;\n  }\n// perform testing with externalSystem.\n  t.is(typeof externalSystem, 'object');\n});\n```\nSo the questions I have - how should ava react if t.cancel() and t assertions are both run by the same test?  In that case should order matter - t.true(true);t.cancel(); vs t.cancel();t.true(true);?\nAlso should the promise returned by the test matter to how t.cancel() is interpreted?  In the example above the async function returns Promise.resolve(), which I think should cause the test to be reported as 'skipped'.  What if t.cancel();return; were replaced with t.cancel();throw error;?  Should the rejected promise be treated as an expected failure like t.failing()?  Same question for a uncaught throw after t.cancel().\nSorry to bombard with questions, just trying to understand the edge cases.. So I can change this to:\njs\nthis.chain.meta = {file: options.file};\nOr maybe the defaults argument of createChain should expect a meta property, pass it that way?  I'm about to go vote, I can update this when I get back.\nFollow-up: Do we want meta to include additional properties or even functions?  One thing I was thinking of (not in this PR) is artifactDir(type).  So test.meta.artifactDir('snapshots') would return the snapshots path for the current test.  Admittedly the snapshotDir configuration option is a wrinkle in this idea.. Done.  I've also swapped the arguments of t.is in the new tests since __filename is the constant we're testing against.. Done. ",
    "Sosomqk": "Ok, thank you. \nThis is what i asked. I achueved this by simply running all folders in parallel with bash script, but I wanted to know if this can be handled by AVA.\nCheers!. Ok, thank you!\n. ",
    "dewdad": "Can't this be managed with streams? Use one stream specifically for AVAs output and pipe it at the end to stdout or a file.. ",
    "ceremcem": "My 2C: I'm using JSON Diff Patch to generate such diffs in my test environment (I'm not using ava, I'm here to consider it). \n1. Its output is very clear and compact once you learn how to decode it \n2. It's possible to generate a link to the webpage for inspecting large diffs.. ",
    "vinsonchuong": "I was able to work around this error by casting to any first:\n```js\nimport untypedTest from 'ava'\nconst test: TestInterface<{ directory: string }> = (untypedTest: any)\n```\nI'm using Flow v0.77.0 and played around for a while with the type definition, but haven't yet been able to find a fix.. ",
    "SassNinja": "Ah ok, then it's not a mistake it's ava.config.js but I just need to add export default to the example from the Configuration section.\nThanks for the quick reply!. ",
    "bigslycat": "@sindresorhus sry, I forgot about this. I'll finish it tomorrow.. ",
    "bichikim": "I need this also!\n. Maybe we will run into a problem which a project root folder has so many xxx.config.js files \nso I mostly move those things into /config\nowing to Ava is not supporting ava --config xxx the Ava config remains in a project root folder.\nit is so sad \ud83d\ude2d\nmy opinion\n- Yes it Should override or ignore all the config silently \n- no because it means I don't want to use package.json#ava.config or projectRoot/ava.config.js. ",
    "bookmoons": "Would also appreciate this. I'd like to make the config file a dotfile. Try to put them all in dotfiles to shunt them out of view.. >Should this silently override config from package.json#ava / ava.config.js?\nSeems sensible. If someone explicitly specifies a custom config file, probably not surprising if that becomes the sole config.\n\nShould we allow a package.json#ava.config value so you don't need to provide the argument?\n\nThe command line argument would work fine for me. I set up an npm script to run ava, so that's a fine place to put the arg. I do JSDoc that way - one config file, one arg in the script.\nAnother benefit to this would be easy setup of separate test suites. Just make a config file for each, and an npm script for each. Clean all around.\nnpm run unit-test\nnpm run integration-test\nnpm run network-test\njson\n\"scripts\": {\n    \"unit-test\": \"ava -C .unittest.conf.js\",\n    \"integration-test\": \"ava -C .integrationtest.conf.js\",\n    \"network-test\": \"ava -C .networktest.conf.js\"\n}. ",
    "christroutner": "I'm using node v8.11.3.\nHere is a snippet of the code I'm trying to use:\n``\n    // Create a file to output results.\n    const test1Log = fs.createWriteStream(${LOG_DIR}${TEST1_LOG}`);\n// Pipe the output to the log file and do not block.\nconst test1Child = execa.shell(RUN_HAPPY_PATH_PATENT);\ntest1Child.stdout.pipe(test1Log);\ntest1Child.stderr.pipe(test1Log);\n\n```\nSo I'm actually trying to pipe the output from execa, which is the output from and SSH command, which is the output from Ava.\nSo @novemberborn you think all I need to do is upgrade to node v10? I'll give that a go.. I upgraded to node v10 and I got the same output.. Those were good ideas. I tried both. \nUsing execa.shell(RUN_HAPPY_PATH_PATENT, {stdout: test1Log, stderr: test1Log}) didn't work at all. I included the error output at the bottom of this comment.\nUsing the |cat hack didn't change the output at all. The log file is still showing the same output, as illustrated at the top of this Issue. I think the TTY behavior is getting lost along the chain of output. I believe the source of problem is the output from Ava, not the output of execa.\nIs there any way to force the isTTY setting that Ava detects?\nHere's the error I got when trying  execa.shell(RUN_HAPPY_PATH_PATENT, {stdout: test1Log, stderr: test1Log}):\nypeError [ERR_INVALID_OPT_VALUE]: The value \"WriteStream {\n  _writableState:\n   WritableState {\n     objectMode: false,\n     highWaterMark: 16384,\n     finalCalled: false,\n     needDrain: false,\n     ending: false,\n     ended: false,\n     finished: false,\n     destroyed: false,\n     decodeStrings: true,\n     defaultEncoding: 'utf8',\n     length: 33,\n     writing: true,\n     corked: 0,\n     sync: false,\n     bufferProcessing: false,\n     onwrite: [Function: bound onwrite],\n     writecb: [Function: nop],\n     writelen: 33,\n     bufferedRequest: null,\n     lastBufferedRequest: null,\n     pendingcb: 1,\n     prefinished: false,\n     errorEmitted: false,\n     emitClose: false,\n     bufferedRequestCount: 0,\n     corkedRequestsFree:\n      { next: null,\n        entry: null,\n        finish: [Function: bound onCorkedFinish] } },\n  writable: true,\n  _events:\n   { open: { [Function: bound onceWrapper] listener: [Function] } },\n  _eventsCount: 1,\n  _maxListeners: undefined,\n  path: '/var/www/html/happy-path-patent-output.txt',\n  fd: null,\n  flags: 'w',\n  mode: 438,\n  start: undefined,\n  autoClose: true,\n  pos: undefined,\n  bytesWritten: 0,\n  closed: false }\" is invalid for option \"stdio\"\n    at internal/child_process.js:936:13\n    at Array.reduce (<anonymous>)\n    at _validateStdio (internal/child_process.js:863:17)\n    at ChildProcess.spawn (internal/child_process.js:300:11)\n    at Object.spawn (child_process.js:528:9)\n    at module.exports (/home/trout/welo-docker-images/images/pantheon-ubuntu/bvt/src/runtests/node_modules/execa/index.js:204:26)\n    at handleShell (/home/trout/welo-docker-images/images/pantheon-ubuntu/bvt/src/runtests/node_modules/execa/index.js:119:9)\n    at Function.module.exports.shell (/home/trout/welo-docker-images/images/pantheon-ubuntu/bvt/src/runtests/node_modules/execa/index.js:327:39)\n    at runTests (/home/trout/welo-docker-images/images/pantheon-ubuntu/bvt/src/runtests/index.js:145:30)\n(node:697) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). (rejection id: 1)\n(node:697) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.\nI tried various tweeks around setting stdio for the file write stream, but couldn't improve the situation. I'm just not very familiar with the inner workings of this low-level behavior.. I tried forcing process.stdout.isTTY = false at the top of the ava test file, but that didn't have any affect on the command line animation.. I just noticed the ava test-file.js > output.log produces an empty file. Hmm... \nI was thinking that I could pipe the ava output to a file and then just retrieve the file, rather than trying to read the output over SSH.. I figured it out! I just used the --tap command line argument with ava. That fixed all the log file output issues.. ",
    "sanchitbansal10": "I would like to fix this..Thanks. ",
    "SidneyNemzer": "Unfortunately same behavior with Node 10. Might be worth noting that the output was correctly cleared when I first started this project about two weeks ago. As I added more tests it eventually stopped clearing the output, and it got worse as I added more tests.. Correct, I also added more test files too, I've got about 33 test files now. I'll try to narrow it down to one file (although I may not have time to do so until this weekend).\nI'm a bit surprised I'm the only one experiencing this, but I suppose in that case it's a not a super high priority.. Progress: Commenting out a console.warn in firebase-functions greatly reduces the number of stuck lines. I suspect there's another console call somewhere, the problem is that it's probably in a library and I don't see it in the terminal. Sometimes, the console.warn from firebase-functions would show up, sometimes multiple times, which is what motivated me to comment it out.\n(I commented that line out inside of the package in node_modules)\nThoughts?. ",
    "sh7dm": "Your new error message is OK, thanks for your help!. I'm sorry, I have closed this PR by mistake.. Yes, it will be awesome to add some more examples. Thank you very much!. ",
    "chrisdarroch": "If an error gets thrown during teardown it\u2019s likely you want to fail the test. Teardown is theoretically cleaning up side-effects to put the suite back in to a known state. an error during cleanup is a warning that subsequent tests will run in an unknown state. if you ignore those, subsequent test failures may be cryptic. . ",
    "havenchyk": "@jamiebuilds are you still interested in implementing this? \ud83d\ude04 . No-no, it will take ages for me to implement this @jamiebuilds \nI just found this idea very cool and useful and I wanted to check was there any activity after the last comment at this issue. @novemberborn I have the same problem - my directory is /dest/blabal/blabla/helpers/filename.js and this folder just skipped even if I specify files section at config to check exactly this helpers folder. So the problem is in helpers pattern.\nCan I help you somehow? . @novemberborn I have the same problem - my directory is /dest/blabal/blabla/helpers/filename.js and this folder just skipped even if I specify files section at config to check exactly this helpers folder. So the problem is in helpers pattern.\nCan I help you somehow? . helpers are filtered out by default, consider another name of the folder. Maybe also try another pattern like tests/*.test.ts. @wmik would you like to create a PR?. @novemberborn please, take a look. @mesqueeb it would be nice if you could create a repository that will reproduce the issue. @natzcam I don't know the answer except using Promise.all, but the best place for asking such questions is https://spectrum.chat/ava or stackoverflow.\nBtw, if you could split this logic into 2 tests it would be easier to track/manipulate from the first sight.. Hey, @Isikiyski, the best place to ask such questions is https://spectrum.chat/ava or stackoverflow.. @novemberborn is right, the root cause of this problem is not in AVA itself, but in ts-node nature.\ntl;dr TypeScript only \"understands\" aliases, but it doesn't convert them to what we would like to have (real path).\nYou certainly can run tests with aliases, but you will have to use another tool or script that will convert aliases to normal paths in your js code. I used module-alias, but you can choose any of packages or there is a babel plugin for it - it's up to you.\nIf you're still interested in working tests, I believe it would be enough to install https://www.npmjs.com/package/tsconfig-paths and add tsconfig-paths/register to require section of ava config. \nYou can check the link https://github.com/TypeStrong/ts-node#loading-tsconfigjson, ts-node recommends tsconfig-paths for such case.\n@jackTheRipper you wrote that aliases are resolved correctly. What do you mean by this? Because if I have import * as something from '~src/mypath it's converted to ...require('~src/mypath').. @lucaperret I just tried your example with circular dependencies and you know, after tsc node also complains about the same, so it's not a problem of ava.. @lucaperret maybe you could also provide your tsconfig?. @lucaperret so, I created a new project, put there a.ts, b.ts and index.ts, index.ts looks like \n```\nimport a from './a'\nconsole.log(a())\n```\nthen I run tsc with your config and got \n\"use strict\";\nvar __importDefault = (this && this.__importDefault) || function (mod) {\n    return (mod && mod.__esModule) ? mod : { \"default\": mod };\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nconst a_1 = __importDefault(require(\"./a\"));\nconsole.log(a_1.default());\n//# sourceMappingURL=index.js.map\nfor index.js. I tried to run node dist/index.js and received this error\n```\nif (a_1.default()) {\n               ^\nTypeError: a_1.default is not a function\n    at Object. (/Users/havenchyk/projects/typeorm-test-docker/dist/b.js:8:16)\n    at Module._compile (module.js:653:30)\n    at Object.Module._extensions..js (module.js:664:10)\n    at Module.load (module.js:566:32)\n    at tryModuleLoad (module.js:506:12)\n    at Function.Module._load (module.js:498:3)\n    at Module.require (module.js:597:17)\n    at require (internal/module.js:11:18)\n    at Object. (/Users/havenchyk/projects/typeorm-test-docker/dist/a.js:6:29)\n    at Module._compile (module.js:653:30)\n``. Thanks for the repo. @lucaperret, how to make sure it works? I donpm i,npm run build,node ./dist/index.js` -> \n```\nava-ts-test-fails/dist/b.js:8\nif (a_1.default()) {\n               ^\nTypeError: a_1.default is not a function\n```\nand after npx ts-node ./src/index.ts I got a_1.default is not a function\nWhat're your results?\nUpdate\nI see now, the problem is not in building (with tsc), but in running the compiled code. Have you tried to run this code?. @lucaperret I think that the problem is with typescript that doesn't handle circular dependencies well.\nSo maybe try to avoid using circular dependencies ;). Seems to be a feature request, what do you think @novemberborn?. @Druotic could you please try ava@next? I have the same case (integration tests with serial: true) and if one test fails, ava stops executing.. @novemberborn isn't it better to allow overriding by cli?. ",
    "godu": "\n\n\nThis is the bsb -make-world output? I think that's fine. Perhaps though don't ignore the directories themselves, but add a README.md which explains why they're there, and nest a .gitignore that ignores all other files inside.\n\n\n\nI try to put .gitignore and readme.md inside lib/bs andlib/js, butbsb -clean completely removes this folders.\nI fallback on add lib/.gitignore and lib/readme.md and explain this folders presence in readme.md.\n\n\n\nIt'll definitely do the same on publish. Perhaps because lib is in package.json#files it overrides. I think you could add !lib/js and that should fix it.\n\n\n\nNPM's documentation recommends to use .npmignore (and .gitignore as fallback) instead of negative glob files entry.\n\n\"The consequences are undefined\" if you try to negate any of the files entries (that is, \"!foo.js\"). Please don't. Use .npmignore.\n\n\n\n\nIs the bsconfig.json necessary for publication? If so, is it odd that the test/bs-types files are not published?\n\n\n\nYou're right, it's very odd. I create a second bsconfig.json in test/bucklescript folder. It's look like better.\n\nI'll fixed the implementation to follow the new API.\nThank a lot for you help.\n\n@novemberborn Did you notice comment sections in Sync.re, Async.re and Promise.re ? I followed ava and flow definitions and the both allow the following unimplemented APIs: \n- test.serial.failing.only\n- test.serial.failing.skip\n- test.serial.only.cb\n- test.serial.failing.only.cb\n- test.serial.skip.cb\n- test.serial.failing.skip.cb. @novemberborn I'm inspired by bs-mocha. They split sync, promise and callback worlds.\nWith ava, i create 3 namespaces: Ava.Sync Ava.Async and Ava.Promise. I pick all assertion that i consider in its place. I tried to implement obvious assertions and skip the weird ones. I will be happy to discussion about my choice and the next step for reasonml's ava.. \n\n\nIs the bsconfig.json necessary for publication? If so, is it odd that the test/bs-types files are not published?\n\nYou're right, it's very odd. I create a second bsconfig.json in test/bucklescript folder. It's look like better.\n\nI had some issues with my solution. I rollback to my initial solution. I have no idea how to correctly setup as you suggest.. @sindresorhus @novemberborn I agree with a separate package.\nI updated @godu/bs-ava. We could move this repo as ava/bucklescript.\n```reason\nopen AvaBucklescript.Sync;\ntest(\"passed\", t => \n  t.pass\n);\n```. ReasonML returns last statement of function. It's like following in javascript\n``js\ntest(\"function returns a fulfilled promise\", t => {\n  return t.notThrowsAsync(() => Promise.resolve(true));\n});\n````.end` is a reserved word in ocaml. I try to find something similar. I'm open to discussion.. ",
    "rishavsharan": "Hi folks.\nI am from MS Azure DevOps and this issue is very interesting to us as well. \nWe would also love it if\n\nAva had native support for junit output via a config variable\nAva allowed us to set the test config via an environment variable like \"AVA_OPTS\" (as pytest does). This will allow us to get xml output from every CI/CD project using Ava without the user having to update their config manually.\n\nI am not in favor of TAP (where the metadata is not standardized) or a completely new format (with low adoption). JUnit XML is an industry standard and is used by most of the CI/CD platforms. \nLet me know what you guys think. If there is consensus on this, we would love to pitch in with contributions as required.. Hey Jamie\nZAP seems like a cool new format. Streaming real time results is something we are interested in as it enables the fast failing philosophy. I will be keeping an eye on ZAP and try to increase its visibility internally within my team.\nBut our focus is right now on serving the current developer need and for that being able to consume any-Unit XML is critical. Having Ava output the xml report will help the projects using Ava, build and release on DevOps Pipelines with minimum friction.. Oh, I did see @novemberborn 's reply and decided to contribute to this conversation by bringing in our perspective as a CI/CD platform.  \ud83d\udc4d . ",
    "jimzhan": "Revert back to \n\nava v0.25\nbabel v6.26\n\nwith the following settings rocks again:\njson\n  \"ava\": {\n    \"files\": [\n      \"!api/**/*.test.js\",\n      \"ui/**/*.test.js\"\n    ],\n    \"require\": [\n      \"babel-register\",\n      \"./setup-env.js\"\n    ],\n    \"babel\": \"inherit\"\n  },\n.babelrc\njson\n{\n  \"presets\": [\"env\", \"react\"]\n}. @novemberborn tks for the comment, do you have any working example that I can ref. to?. @novemberborn I think this is mainly caused by babel + monorepo, after switching to jest, and place exact same .babelrc in each package did solve the issue. Now I'm having a homemade package esnext-scripts to help :-). ",
    "midgethoen": "To be honest I was trying out several testing frameworks to see which appeals most to me. So I\u2019m not even sure yet if this will be ava. However I have been wanting to contribute to open source for some time and the issue seems pretty straight forward. \nI\u2019ll give it a go. Although I\u2019m a bit occupied atm. So it might be in a litte while.. I have some time tonight. Let's see how far I get :). > @midgethoen oh no, I stepped on your toes!\nAh, That's ok \ud83d\ude01 \nThe fix works like a charm for me! . ",
    "denis-sokolov": "For future readers, my watcher initially only watched for changes in test files, but not in imported source files. I had to manually configure the sources key in my configuration:\njson\n\"files\": [\n  \"src/**/*.test.ts\"\n],\n\"sources\": [\n  \"src/**/*.ts\"\n],. ",
    "ronen": "hi @novemberborn i'm happy to report something to @babel/register, but i'm not certain what to report to them! :)   i don't know what it is or isn't doing that it shouldn't or should, so i don't know how to isolate the problem/generate a minimal reproduction for them.  clearly, i'm not a babel maven.  (and i feel like if i reported only \"it behaves weird with AVA\" they'd say \"well, take it up with AVA\"...)   any hints...?. @novemberborn Thanks, I followed that tip (though note tweak #1889) and that fixes the filename.   \nIn the minimal reproduction that seems to clear everything up.  But in my actual app, although the filename is fixed, the source map seems to still be having trouble: the reporter lists the last line of the file as the location of the exception, and still doesn't show the source code.   Unfortunately I'm not likely to have time in the next while to pare it down to another minimal reproduction.. @novemberborn thanks for the fix!   FYI confirming that it's working for me.\nBut see #1892\n. > I'm closing this issue for housekeeping purposes\n...yes this source map stuff seems like quite a messy tangle, housekeeping is a good idea :)  \nThanks for looking into it and for the update.  I'll keep my eye on #974. @okyantoro go for it!\n(I'm happy to complain but not actually fix anything myself :). Thanks @okyantoro from me too!. Thanks @okyantoro from me too!. ",
    "jagoda": "@sindresorhus I made the change you requested. The build failures seem unrelated to my changes and rather seem to have been introduced in 8663028de2d7a7be88c4a101dafb873c5af87937. Please correct me if I'm wrong and I will happily fix.. @novemberborn thanks for the feedback! I believe that I have addressed all of your concerns. I also added some extra testing around the different possible TTY states. Please let me know if you have any further concerns.. Thanks for the feedback @novemberborn! I believe that I have addressed everything that you commented on. Please let me know if I missed anything.. Oh, very cool. I hadn't found that feature yet\u2014thanks for the tip!. That makes sense to me. I included the stdout and stderr objects here to maintain consistency with the original code which checks both the numeric file descriptor and the stream object. If the tests pass then I'm happy to drop the stream object comparison.. ",
    "sharkykh": "@novemberborn \nI'm not happy with what I came up with, mainly because of the tap-snapshots folder, but also the how I had to test the .snap file.\nIt would be very helpful if you could maybe point me in the right direction, as I couldn't find a similar test to base the new test upon.. From what I can tell, snapFile only ever holds the snap filename.\nhttps://github.com/avajs/ava/blob/f7fce6a81cf8ee77d0e4823e4ff8961d5097da17/lib/snapshot-manager.js#L392-L393. This is also problematic: It's using Windows paths since I'm on Windows.. ",
    "rhysburnie": "Can confirm other team member also getting same issue, so its not something thats gone wrong in my local directory. Can confirm other team member also getting same issue, so its not something thats gone wrong in my local directory. I have a pretty basic setup, not specifying files they match default globs.\none is in src/helpers/validation/dob.validatior.test.js for example.\ntried also specifying just that path, same error.\n\"ava\": \"^0.25.0\"\n\"ava\": {\n    \"require\": [\n      \"babel-register\"\n    ],\n    \"babel\": \"inherit\"\n  }\nLast week I had the tests passing and haven't changed anything in regards to tests since.\nI could understand if maybe somethings gone wrong with my local machine, but a coworker who has never run the test command before also tried and also got the error.\nhere's a test file sample - tho the problem is its not being found\n```\nimport { test } from 'ava';\nimport dob from './dob.validator';\ntest('dob() creates a dob validator', t => {\n  t.is(typeof dob, 'function');\n  const validator = dob();\n  t.is(typeof validator, 'function');\n});\ntest('validator will only allow valid yyyy-mm-dd strings', t => {\n  const validator = dob();\n  t.true(validator(null));\n  t.true(validator(''));\n  t.false(validator(0));\n  t.false(validator(new Date()));\n  t.false(validator('1st of January 1970'));\n  t.true(validator('1970-01-01'));\n});\ntest('validator will only allow valid days', t => {\n  const validator = dob();\n  t.false(validator('2000-01-00'));\n  t.false(validator('2000-01-32'));\n  t.true(validator('2000-01-31'));\n  t.false(validator('2000-04-31'));\n  t.true(validator('2000-04-30'));\n  t.true(validator('2018-02-28'));\n  t.true(validator('2020-02-29'));\n});\ntest('validator will only allow valid months', t => {\n  const validator = dob();\n  t.false(validator('2000-00-01'));\n  t.false(validator('2000-13-13'));\n  t.true(validator('2000-12-01'));\n});\ntest('validator will only allow dates with years >= minYear if specified', t => {\n  const validator = dob('1900');\n  t.false(validator('1899-12-31'));\n  t.false(validator('0000-01-01'));\n  t.true(validator('1980-05-20'));\n});\ntest('validator will only allow dates with years <= maxYear if specified', t => {\n  const validator = dob(null, '2000');\n  t.false(validator('2001-01-01'));\n  t.true(validator('1999-12-31'));\n  // obviously it's best to also use minYear\n  // most of the time unless you want 2000 year olds etc.\n  t.true(validator('0000-01-01'));\n});\n```\nI can ls dir structure from same root as the test command and see .test files:\n\n. I have a pretty basic setup, not specifying files they match default globs.\none is in src/helpers/validation/dob.validatior.test.js for example.\ntried also specifying just that path, same error.\n\"ava\": \"^0.25.0\"\n\"ava\": {\n    \"require\": [\n      \"babel-register\"\n    ],\n    \"babel\": \"inherit\"\n  }\nLast week I had the tests passing and haven't changed anything in regards to tests since.\nI could understand if maybe somethings gone wrong with my local machine, but a coworker who has never run the test command before also tried and also got the error.\nhere's a test file sample - tho the problem is its not being found\n```\nimport { test } from 'ava';\nimport dob from './dob.validator';\ntest('dob() creates a dob validator', t => {\n  t.is(typeof dob, 'function');\n  const validator = dob();\n  t.is(typeof validator, 'function');\n});\ntest('validator will only allow valid yyyy-mm-dd strings', t => {\n  const validator = dob();\n  t.true(validator(null));\n  t.true(validator(''));\n  t.false(validator(0));\n  t.false(validator(new Date()));\n  t.false(validator('1st of January 1970'));\n  t.true(validator('1970-01-01'));\n});\ntest('validator will only allow valid days', t => {\n  const validator = dob();\n  t.false(validator('2000-01-00'));\n  t.false(validator('2000-01-32'));\n  t.true(validator('2000-01-31'));\n  t.false(validator('2000-04-31'));\n  t.true(validator('2000-04-30'));\n  t.true(validator('2018-02-28'));\n  t.true(validator('2020-02-29'));\n});\ntest('validator will only allow valid months', t => {\n  const validator = dob();\n  t.false(validator('2000-00-01'));\n  t.false(validator('2000-13-13'));\n  t.true(validator('2000-12-01'));\n});\ntest('validator will only allow dates with years >= minYear if specified', t => {\n  const validator = dob('1900');\n  t.false(validator('1899-12-31'));\n  t.false(validator('0000-01-01'));\n  t.true(validator('1980-05-20'));\n});\ntest('validator will only allow dates with years <= maxYear if specified', t => {\n  const validator = dob(null, '2000');\n  t.false(validator('2001-01-01'));\n  t.true(validator('1999-12-31'));\n  // obviously it's best to also use minYear\n  // most of the time unless you want 2000 year olds etc.\n  t.true(validator('0000-01-01'));\n});\n```\nI can ls dir structure from same root as the test command and see .test files:\n\n. I don't see how the import would make a difference, if its no longer { test } then I would just get errors when i try to use tests, ava is simply not finding *.test.js files anywhere in my project.\nI even throw an error in a *.test.js file and get nothing - surely if is was found there would be something in the console?\nI see you have node >=8.9.4 listed in engines - which is what Im using (I usually use newer but the other dev has set it up with unrelated packages that need 8.9.4)\nI did try import test from 'ava' but like I said its more like the test files themselves are never found even tho Im leaving the default glob patterns (I only have \"test\": \"ava\" in my package.json). update: \nI updated ava and used import test from 'ava' and created a test at same level as package.json\nimport test from 'ava';\ntest(t => t.fail());\nThat works... however it still doesn't reach any other tests for example in my src/ folder...\nBut according to your docs one of the patterns in the default globs is: **/*.test.js shouldnt that find *.test.js files in my src/ folder and any folder in my entire project?. Hi it must be that it is in the project helpers/ directory.\nHowever, it was working with files in the helpers/ directory fine.\nIs there ways around that? The exisiting project already had a src/helpers/ directory\nEven if its because helpers/ is ignored why did it work previously? I made no changes and they have always been in helpers. Hi it must be that it is in the project helpers/ directory.\nHowever, it was working with files in the helpers/ directory fine.\nIs there ways around that? The exisiting project already had a src/helpers/ directory\nEven if its because helpers/ is ignored why did it work previously? I made no changes and they have always been in helpers. ",
    "ericmorand": "@novemberborn, out of curiosity, what is the reasoning that led to helpers folder being excluded?. @novemberborn, out of curiosity, what is the reasoning that led to helpers folder being excluded?. > Albeit with prettier separator characters.\nI didn't notice the light gray caret character between \"headers\" and the name of the test. \nDoes it mean that the name of the file in this case is headers.js?\nIf I have sub-folders, for example foo/bar/test.js, will AVA print something like the following?\nfoo > bar > [name of first test]\nfoo > bar > [name of second test]. Amazing! I think it should make the deal. And will actually force me to have smaller (aka more focused) and better organized test files.\nThanks a lot.. ",
    "wmik": "I'm also experiencing a similar issue while trying to setup testing with\n- ava v0.25.0\n- ts-node v7.0.1\n- typescript v3.0.3\non a 64-bit windows 10 machine working from WSL.\nThe error message is exactly the same and attempting to specify files like so npm test src/app.test.ts  or ava src/app.test.ts doesn't change anything.\nCurrently I've resolved to building the project first before testing but it'd be nice to skip that step. \ud83d\ude04 \nHere's my setup\ntsconfig.json\njson\n{\n  \"compilerOptions\": {\n    \"target\": \"es6\",\n    \"module\": \"commonjs\",\n    \"esModuleInterop\": true,\n    \"moduleResolution\": \"node\",\n    \"sourceMap\": true,\n    \"baseUrl\": \".\",\n    \"outDir\": \"dist\",\n    \"noImplicitAny\": true,\n    \"experimentalDecorators\": true,\n    \"emitDecoratorMetadata\": true\n  },\n  \"include\": [\n    \"src/**/*\"\n  ]\n}\npackage.json\njson\n\"scripts\": {\n    \"test\": \"ava\"\n}\n\"ava\": {\n    \"files\": [\n      \"src/**/*.test.ts\"\n    ],\n    \"compileEnhancements\": false,\n    \"extensions\": [\n      \"ts\"\n    ],\n    \"require\": [\n      \"ts-node/register\"\n    ]\n  }\nAny help would really be appreciated \ud83d\ude42 \nPS: Omitting the files option from package.json also doesn't change a thing.\n-----------------------------------------------------UPDATE-------------------------------------------------------\nThanks @novemberborn . It works with the latest beta at the time of writing v1.0.0-beta.8 . . I'm also experiencing a similar issue while trying to setup testing with\n- ava v0.25.0\n- ts-node v7.0.1\n- typescript v3.0.3\non a 64-bit windows 10 machine working from WSL.\nThe error message is exactly the same and attempting to specify files like so npm test src/app.test.ts  or ava src/app.test.ts doesn't change anything.\nCurrently I've resolved to building the project first before testing but it'd be nice to skip that step. \ud83d\ude04 \nHere's my setup\ntsconfig.json\njson\n{\n  \"compilerOptions\": {\n    \"target\": \"es6\",\n    \"module\": \"commonjs\",\n    \"esModuleInterop\": true,\n    \"moduleResolution\": \"node\",\n    \"sourceMap\": true,\n    \"baseUrl\": \".\",\n    \"outDir\": \"dist\",\n    \"noImplicitAny\": true,\n    \"experimentalDecorators\": true,\n    \"emitDecoratorMetadata\": true\n  },\n  \"include\": [\n    \"src/**/*\"\n  ]\n}\npackage.json\njson\n\"scripts\": {\n    \"test\": \"ava\"\n}\n\"ava\": {\n    \"files\": [\n      \"src/**/*.test.ts\"\n    ],\n    \"compileEnhancements\": false,\n    \"extensions\": [\n      \"ts\"\n    ],\n    \"require\": [\n      \"ts-node/register\"\n    ]\n  }\nAny help would really be appreciated \ud83d\ude42 \nPS: Omitting the files option from package.json also doesn't change a thing.\n-----------------------------------------------------UPDATE-------------------------------------------------------\nThanks @novemberborn . It works with the latest beta at the time of writing v1.0.0-beta.8 . . @Christilut\nTry putting the ava configuration object at the top level in the ava.config.js\njs\nexport default {\n  \"compileEnhancements\": false,\n  \"extensions\": [\n    \"ts\"\n  ],\n  \"require\": [\n    \"ts-node/register\"\n  ],\n  \"files\": [\n    \"tests/**/*.test.ts\"\n  ]\n}. Thanks for the review @novemberborn I suppose .hasOwnProperty was overkill \ud83d\ude05 Otherwise the updates look great. Nothing more to add on my side.. ",
    "Christilut": "Same problem here but with beta 8\nava: 1.0.0-beta.8\nts-node: 7.0.1\ntypescript: 3.0.3\nSays Couldn't find any files to test regardless of the files pattern.\ntsconfig:\n{\n  \"compilerOptions\": {\n    \"target\": \"es2017\",\n    \"outDir\": \"dist\",\n    \"rootDir\": \"\",\n    \"moduleResolution\": \"node\",\n    \"module\": \"commonjs\",\n    \"sourceMap\": true,\n    \"declaration\": true,\n    \"importHelpers\": true,\n    \"listFiles\": false,\n    \"traceResolution\": false,\n    \"pretty\": true,\n    \"emitDecoratorMetadata\": true,\n    \"experimentalDecorators\": true,\n    \"lib\": [\n      \"es2017\",\n      \"dom\",\n      \"esnext\"\n    ],\n    \"types\": [\n      \"node\"\n    ],\n    \"baseUrl\": \".\"\n  },\n  \"include\": [\n    \"**/*.ts\"\n  ],\n  \"exclude\": [\n    \"node_modules/\"\n  ],\n  \"compileOnSave\": false\n}\nava.config.js:\nexport default {\n  \"ava\": {\n    \"compileEnhancements\": false,\n    \"extensions\": [\n      \"ts\"\n    ],\n    \"require\": [\n      \"ts-node/register\"\n    ],\n    \"files\": [\n      \"tests/**/*.test.ts\"\n    ]\n  }\n}\nMy project looks like this:\nserver/...\ntests/\n-- user.test.ts\ntests/helpers\n    some helper files here . My helpers folder doesn't include any tests, that should't prevent ava from finding my test files right?\ntests/*.test.ts didn't find anything either.\nBut it seems like it runs the compiled files in dist/ automatically (I noticed because they were leftover files from my old compiling typescript method of running the tests).\nIf I delete dist/ then it won't find any files to test. The only place I have dist is in the tsconfig file. Maybe ava checks the tsconfig file and tries to look for files in the outDir folder but won't find any cause I deleted dist/ ?. Ah thanks! Missed that during the upgrade :). I'll pick this up tomorrow. Planning to use AVA for a long time, may as well get to know it's inner workings :). Okay so every file has its own mongoose instance. Is it then possible to have all files use the same instance of https://github.com/nodkz/mongodb-memory-server ? Since it would launch a new one for every worker instance, I'm not sure how to re-use an existing one in different workers.. I agree but in this case I want all my tests to run simultaneously on the\nsame database since the real world database should handle similar scenarios.\nBut I can achieve that result by just running the tests on a local\ndatabase.\nOn Mon, 1 Oct 2018, 13:30 Mark Wubben, notifications@github.com wrote:\n\nIs it then possible to have all files use the same instance of\nhttps://github.com/nodkz/mongodb-memory-server ?\nNo. But then ideally your test files are independent of each other, so\nthat shouldn't matter.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/avajs/ava/issues/1944#issuecomment-425874948, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ADT4AxUSLFIOmi8d_oU4OJUmbOTwGGwkks5ugfzegaJpZM4XBCgn\n.\n. Okay, can this be a feature request? I love power assert :). Ah okay. I thought that all the options here were both ava.config.js options and CLI options.\n\nI think I was confused by this line: Arguments passed to the CLI will always take precedence over the configuration in package.json. which is followed by the ava.config.js options.\nI'll use the debug flag and see why sources is ignored when adding match.. Thanks, but it's not quite the same. That site looks nice but I don't feel that it works that well. Search is slow and doesn't work intuitively.\nPersonally I'd go either 1 giant markdown file or if you want something more fancy maybe Gitbook, which is free for open source anyway.. ",
    "SHITianhao": "same here. same here. ",
    "bcomnes": "I do not, this is an Uncompiled node app.  . I do not, this is an Uncompiled node app.  . See the runnable gist for a repeatable example. . See the runnable gist for a repeatable example. . Currently rewriting on a private repo unfortunately.  I can next week.\n...Just retesting my example again:\nhah, what was not working yesterday, is working again today.  Smells a lot like a broken transient dep.  Let me run some sanity checks and decide what to do with this issue.. Currently rewriting on a private repo unfortunately.  I can next week.\n...Just retesting my example again:\nhah, what was not working yesterday, is working again today.  Smells a lot like a broken transient dep.  Let me run some sanity checks and decide what to do with this issue.. Seem to be working now.  Closing.  Feel free to re-open if you can reproduce again.. Seem to be working now.  Closing.  Feel free to re-open if you can reproduce again.. ",
    "onexdata": "I've tried everything in this to no avail.  I've googled for 2 hours now to no avail.\nTried...\nthis\nthis\nthis\nthis\n...and many others. Nothing seems to do the trick.  Would really like to try out Ava... seems incredibly complex to just use ES6 basic syntax...\n. I've tried everything in this to no avail.  I've googled for 2 hours now to no avail.\nTried...\nthis\nthis\nthis\nthis\n...and many others. Nothing seems to do the trick.  Would really like to try out Ava... seems incredibly complex to just use ES6 basic syntax...\n. Well I'm trying to use AVA for node only, and Node 8.x supports ES6 / import out of the box, so there is no compilation step or need for Babel.\nI'm a little puzzled what is going on since the test itself uses the import statement, yet the code the test is testing also uses it, but fails.  Is there some kind of directive that tells AVA not to use ES6 that I can bypass?\nThis is only for server/Node/back end code and has nothing to do with the front-end and never will.. Well I'm trying to use AVA for node only, and Node 8.x supports ES6 / import out of the box, so there is no compilation step or need for Babel.\nI'm a little puzzled what is going on since the test itself uses the import statement, yet the code the test is testing also uses it, but fails.  Is there some kind of directive that tells AVA not to use ES6 that I can bypass?\nThis is only for server/Node/back end code and has nothing to do with the front-end and never will.. The link you gave has a link to disable AVA's use of Babel all together and here:\nhttps://github.com/avajs/ava/blob/v1.0.0-beta.7/docs/recipes/babel.md#preserve-es-module-syntax\nIt suggests putting this in package.json:\n\"ava\": {\n    \"babel\": false,\n    \"compileEnhancements\": false\n  },\nBut when I do that I get this:\n```\n  \u00d7 Unexpected Babel configuration for AVA. See https://github.com/avajs/ava#es2017-support for allowed values.\nnpm ERR! code ELIFECYCLE\nnpm ERR! errno 1\nnpm ERR! microservice-template@1.0.0 test:ava --verbose`\nnpm ERR! Exit status 1\nnpm ERR!\nnpm ERR! Failed at the microservice-template@1.0.0 test script.\nnpm ERR! This is probably not a problem with npm. There is likely additional logging output above.\nnpm ERR! A complete log of this run can be found in:\nnpm ERR!     C:\\Users\\nsteele\\AppData\\Roaming\\npm-cache_logs\\2018-08-14T21_58_24_743Z-debug.log\n````\nThe URL the error suggests (https://github.com/avajs/ava#es2017-support) doesn't exist (the anchor doesn't exist so it just goes to the main page)\n. The link you gave has a link to disable AVA's use of Babel all together and here:\nhttps://github.com/avajs/ava/blob/v1.0.0-beta.7/docs/recipes/babel.md#preserve-es-module-syntax\nIt suggests putting this in package.json:\n\"ava\": {\n    \"babel\": false,\n    \"compileEnhancements\": false\n  },\nBut when I do that I get this:\n```\n  \u00d7 Unexpected Babel configuration for AVA. See https://github.com/avajs/ava#es2017-support for allowed values.\nnpm ERR! code ELIFECYCLE\nnpm ERR! errno 1\nnpm ERR! microservice-template@1.0.0 test:ava --verbose`\nnpm ERR! Exit status 1\nnpm ERR!\nnpm ERR! Failed at the microservice-template@1.0.0 test script.\nnpm ERR! This is probably not a problem with npm. There is likely additional logging output above.\nnpm ERR! A complete log of this run can be found in:\nnpm ERR!     C:\\Users\\nsteele\\AppData\\Roaming\\npm-cache_logs\\2018-08-14T21_58_24_743Z-debug.log\n````\nThe URL the error suggests (https://github.com/avajs/ava#es2017-support) doesn't exist (the anchor doesn't exist so it just goes to the main page)\n. Oh my GOSH Node doesn't even support import yet \ud83d\ude27  ...\nThanks.\nsmacks forehead. Oh my GOSH Node doesn't even support import yet \ud83d\ude27  ...\nThanks.\nsmacks forehead. ",
    "chrisdothtml": "I've been trying out ava for the first time today, and can confirm that I totally expected it to transpile my imported files in addition to my test files. I'd be fine with it either transpiling all by default, or none by default (with a single option to enable transpilation of all) as long as it's consistent.\nIn the interim, though, this worked perfectly fine:\npackage.json\n\"ava\": {\n  \"require\": \"esm\"\n}\nIt might be a good idea to explicitly point out in the readme under Transpiling imported modules that this is the simplest solution (until a change is made for v2). Until I found this issue, it wasn't very clear what I needed to do to fix the problem. ",
    "mellster2012": "Agreed. Really like ava, but when trying to bypass transpiling (node.js v8+) none of the recipes and answers really worked here, only a \"mix-and-match\" combo. In the (eventually) working solution test and source files (using es6 import/export) have to have the .mjs extension, esm must be installed as part of the dev-dependencies, no ava config file or .babelrc file is present, the ava config in package.json must require esm, the babel/extensions need to include js and mjs and module compilation needs to be disabled for testOptions/presets:\n\"ava\": {\n    \"require\": \"esm\",\n    \"babel\": {\n      \"extensions\": [\n                \"js\",\n                \"mjs\"\n      ],\n      \"testOptions\": {\n                \"presets\": [\n                    [\"module:ava/stage-4\", {\"modules\": false}]\n                ]\n            }\n        }\n  }\nIt would be great to clean up/clarify the documentation and add specific (verified working) sample skeleton test projects for the most important es6 settings/scenarios for the 1.0.0 final release.. ",
    "PM5544": "Our setup uses Rollup to bundle ECMAScript Modules into ESM and ES5 (for unsupported browsers), and we want our tests to also be written in ESM.\nAfter a lot of trial and error the only solution that worked was the one in this comment https://github.com/avajs/ava/issues/1908#issuecomment-422235027. ",
    "afoninsky": "No, I'm looking for a way how can I ensure if plan executed OR timeout achieved in particular test. Sometimes (often, actually :) I have to test async code without clean interceptors or mocks. So I want to have something like:\n```\n// just an example\ntest('some test', async t => {\n  t.plan(1) <- set test spaln\n  somethingAsync.on('event', () => {\n    t.pass()\n  })\n  somethingAsync.start()\nawait resolveWhenTestPlanAchievedOrRejectAfterTimeout() <- this\n})\n```\nSure it is a dummy example, but I hope I was able to convey the idea.. ",
    "Dianoga": "@novemberborn not a problem, I understand things being easier. I'm just glad it's being updated. . ",
    "robertbernardbrown": "I can pick this up. Correct me if I'm wrong, but from reading the Babel Recipe and the mentioned PR it looks like --reset-flag should only be used when Babel config files are changed. Otherwise, AVA automatically detects changes to test files and recaches them. Is this more or less correct?. Cool - just submitted a PR (#1937 ).\nLet me know if you see anything that needs changing. Hope it's alright I re-used a little of the explanations offered in the #1796 and the referenced Babel Recipe.. Of course! Happy to help. ",
    "alanshaw": "Apologies - this works in 1.0 beta. Apologies - this works in 1.0 beta. ",
    "Akiyamka": "Still not work for me.\nnode -v \n10.6.0\nava.config.js\njs\nexport default {\n  \"ava\": {\n    \"babel\": false,\n    \"compileEnhancements\": false\n  }\n}\nnpx ava --reset-cache\n\u2714 Removed AVA cache files in .../node_modules/.cache/ava\ntest.js\n```js\nimport test from 'ava';\ntest('bar', async t => {\n    const bar = Promise.resolve('bar');\n    t.is(await bar, 'bar');\n});\nCrush with error\n   6:                                    \n   7: test('bar', async t => {           \n   8:   const bar = Promise.resolve('bar');\nReferenceError: regeneratorRuntime is not defined\n```\npackage.json\njs\n\"ava\": \"1.0.0-beta.8\",\nEnv\nNode.js v10.6.0\nlinux 4.14.67-1-MANJARO\nnpm: 6.1.0. Still not work for me.\nnode -v \n10.6.0\nava.config.js\njs\nexport default {\n  \"ava\": {\n    \"babel\": false,\n    \"compileEnhancements\": false\n  }\n}\nnpx ava --reset-cache\n\u2714 Removed AVA cache files in .../node_modules/.cache/ava\ntest.js\n```js\nimport test from 'ava';\ntest('bar', async t => {\n    const bar = Promise.resolve('bar');\n    t.is(await bar, 'bar');\n});\nCrush with error\n   6:                                    \n   7: test('bar', async t => {           \n   8:   const bar = Promise.resolve('bar');\nReferenceError: regeneratorRuntime is not defined\n```\npackage.json\njs\n\"ava\": \"1.0.0-beta.8\",\nEnv\nNode.js v10.6.0\nlinux 4.14.67-1-MANJARO\nnpm: 6.1.0. import \"@babel/polyfill\";\nIn every test file help temporary fix this error, but not fix problem. I still want to turn off the babel, since async/await works great in node 10 without babel.. import \"@babel/polyfill\";\nIn every test file help temporary fix this error, but not fix problem. I still want to turn off the babel, since async/await works great in node 10 without babel.. @nonnontrivial you right, this really solved the problem. I was confused by the examples in the documentation, and ava did not report an error (usually trow error about invalid options in config). @nonnontrivial you right, this really solved the problem. I was confused by the examples in the documentation, and ava did not report an error (usually trow error about invalid options in config). ",
    "JerryC8080": "@Akiyamka  \nI came across the same problem in ava-1.2.1\uff0cand below is how i slove the problem:\nin package.json\njson\n \"ava\": {\n    \"require\": [\n      \"@babel/register\",\n      \"@babel/polyfill\"\n    ]\n },. ",
    "grant37": "I'd like to work on this if that's alright. Messing around locally and trying to follow the guidelines here I had success (I think) with: \n1. Checking if the fn argument to throws() returns a promise (and adding the no-op catch here)\n2. If it does, calling fail() like it's called on line 314 with a message about asynch functions\n3. Wrapping all this in a try/catch (no-op) because I got errors on the promise check if fn was synchronous (probably there's a better method though?)\nThe result is a test failed message that refers to the line await t.throws(asynch () => { in a test file (from #1931), which is all I tested against so far. Would this be the correct behavior/do you see any issues?. The check fails for Node.js:6 with\n```\n/assert.js:718:\n    assertions.throws(async () => {\n                    ^\nSyntaxError: Unexpected token (\n```\nBut using the arrow syntax seems to be required. Any thoughts on how to work around this? . Makes sense to me, thanks @novemberborn!. Thanks for the feedback! I can work on making these changes this week.. ",
    "miguelsolano": "Sweet! I'll get to work on this. Appreciate the feedback as well. I'm super excited to get this wrapped up \ud83c\udf89. Oops... That shouldn't be there \ud83d\ude05. Absolutely I'll remove that. Right now it's just a command so that I don't have to parse all test output. That being said I'll run all tests before I push an update. . ",
    "mesqueeb": "@novemberborn however the weird thing is that other commonjs files are properly imported even with import in ava, just this one cjs file that gives me problems. : S\nAbout your workaround: use require instead. \nThis is not possible because import merge from 'merge-anything' is happening in one of my source files. Not in the ava test file. In the source file it works perfectly like so, but importing that source file gives me the bug. \nI know it\u2019s confusing sorry for the bad explanation.\n\nAva test: import a.ts\nA.ts: import merge\n. I don\u2019t use babel yes. Only TypeScript. \nThe problem only occurs in ava for me. . @novemberborn \nIt's reproducable if you clone this:\nhttps://github.com/mesqueeb/vuex-easy-firestore/tree/rewrite-tests\nnotice tree/rewrite-tests\nThen just yarn and yarn test ./test/utils/setDefaultvalues.js\n\nAs you can see in the top two lines:\njs\n// import setDefaultValues from '../../src/utils/setDefaultValues'\nimport setDefaultValues from '../helpers/utils/setDefaultValues'\nThe first one is the TS file from src, the second one is the CJS file compiled by rollup which uses a typescript compiler plugin. No babel is used for this compilation and I have a TS config file set up.\nWhen using the top line from the TS file it will give me the error\n'merge_anything_1.default is not a function'\nWhen using the second line with the CJS file it works\nAny ideas for me?\nThakns a lot!. macOS with iTerm2 and fish shell. @novemberborn @havenchyk Trying to replicate this, I had moved it to a separate test file and it worked.\nBut when in the same test file as other tests this breaks.\nAfter some tinkering I found it that AVA replaces my console logs with the name of the other test instead of what I wrote in the console log.\nLook at this screenshot below: top is a test file with only wait, the second is a test file with wait and another test. I'm not console.logging anything in the other test, but it starts to log the other test's name!\n\nIt's reproducable if you clone this:\nhttps://github.com/mesqueeb/vuex-easy-firestore/tree/rewrite-tests\nnotice tree/rewrite-tests\nThen just yarn and yarn test ./test/mutations.js\nComment out the SET_PATHVARS test and it will work.. I tried npm update but still have the same error.. @novemberborn \nthanks.\n\nI removed these from package.json:\n@ava/babel-preset-stage-4\n@babel/plugin-proposal-object-rest-spread\n\n@babel/preset-env\n\n\nand then deleted node_modules and package-lock.json\n\n\nthen reinstalled everything with npm i\n\n\nand it works again \ud83c\udf89. ",
    "zachleat": "Just ran into this, thanks for the --verbose tip workaround!. ",
    "p1pchenk0": "same is for ava: 1.0.0-rc.1. reproduction link: https://github.com/p1pchenk0/nuxt-render-route-hangs. @novemberborn is there any way to let ava know to wait for nuxt build? I suppose, it has to be done inside setup file. ",
    "natzcam": "Tried above, but the test would not complete because connection is initiated on the constructor and await pEvent(alice, 'connect'); would block new Bob(). Changing the order did not help either, because 'connect' might have been fired already.\nconst alice = new Alice();\n  const bob = new Bob();\n  await pEvent(alice, 'connect');\n  await pEvent(bob, 'connect');\n  t.pass();\nUsing Promise.all would solve it though, but it would circle back to my original question. :)\nawait Promise.all([pEvent(alice, 'connect'), pEvent(bob, 'connect')]);\n  t.pass();\nIt would be nice, for callbacks, there is a 'countdown' functionality. Wherein, the test would end when the countdown = 0\n```\ntest.cb('2 callbacks', t => {\n    t.plan(2);\n    t.countdown(2);\nconst alice = new Alice();\nalice.on('connect', () => {\n    t.pass();\n    t.end(1); //decrement 1\n});\n\nconst bob = new Bob();\nbob.on('connect', () => {\n    t.pass();\n    t.end(1); //decrement 1\n});\n\n});\n```. ",
    "sarneeh": "@novemberborn Uhm, well, I guess I misunderstood how it works. When I put a second thought on this topic it totally makes sense that ava's babel config property has nothing to do with what you put in require (because... why? \ud83d\ude04). Thanks for clarifying this and sorry for taking your time unnecessarily \ud83d\ude04 . ",
    "makepost": "Please do export utilities. What's worth testing is probably a functional unit others would like to reuse. Modules and IIFEs with inaccessible stuff inside are often a pain, when I need just some of their behavior but have to launch something with dependencies the size of my app simply to get a way to execute it.\nTests serve great as up-to-date usage examples, so it's convenient for a library end user to focus on a separate file without getting distracted by implementation details. Contrarily, library authors, who are looking for what usage of a specific method their changes can affect, benefit from having the tests by hand. Sort of a compromise is placing Unit.js and Unit.test.js in the same dir.\nOther point is about tests that are roughly the size of what they check, or bigger, and have their own additional dependencies. They get in the way when you're navigating inside the file, and search of unused imports which might have side effects is probably more resource-intensive that stripping test calls.\nWhen these are not a problem, it's alright I think. Interested in seeing code bases with inline Ava tests, checking out how efficiently minification and hot module replacement escape these, and whether it reminds of CommonJS tests.. ",
    "ratiotile": "Updated with minimal reproduction\nedit: This seems to be a regression. AVA 0.25.0 doesn't have this behavior.\nedit2: AVA 0.25.0 shows the correct line for a failing assert, but the wrong line in the code under test.. I followed the docs and tried to configure @babel/register\n1. ignore: ['src/test_*'] doesn't work\n2. ignore: ['src/test_parameter_convert.js'] works. Maybe wildcards are broken?\n3. ignore: [/src\\/test_.*\\.js/] the ignore items must be regex.\nSeems that the docs are wrong about how to configure @babel/register. Would you accept a PR on the docs?. Errors in the source file are still reported on the wrong line (a failing assert in the test file is correctly reported). I tried disabling sourceMaps in babel/register, but it still doesn't work.. The workaround I've found is to disable AVA's builtin babel transpilation with compileEnhancements: false, babel: false and rely on babel/register to do it by with the configuration:\njs\nrequire('@babel/register')({\n  ignore: [\n    /node_modules/,\n    /build\\//,\n    /coverage\\//,\n  ],\n  inputSourceMap: true,\n  plugins: [\n    [\n      'istanbul',\n      {\n        all: true,\n        cache: true,\n        exclude: ['**/test_*.js', 'src/functional_tests/*'],\n        include: ['src/**/*.js'],\n      },\n    ],\n  ],\n  presets: ['@ava/transform-test-files'],\n})\n@babel/register requires that the ignore paths are regex expressions and not strings.\nNow the reported line numbers of errors are correct both in test files and source files.. ",
    "lucaperret": "Hi @havenchyk \nHere's the tsconfig.json\njson\n{\n  \"compilerOptions\": {\n    \"target\": \"esnext\",\n    \"module\": \"commonjs\",\n    \"sourceMap\": true,\n    \"outDir\": \"dist\",\n    \"declarationDir\": \"dist/types\",\n    \"declaration\": true,\n    \"esModuleInterop\": true,\n    \"strict\": true,\n    \"noUnusedParameters\": true,\n    \"noImplicitReturns\": true,\n    \"noUnusedLocals\": true,\n    \"noImplicitAny\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\", \"dist\"]\n}. It happen on AVA, but not on building with tsc.\nHere's the repository: https://github.com/lucaperret/ava-ts-test-fails. Also have this issue when running with node.... Thanks for your help @havenchyk, I'll fix that by creating a module who exports both method.. ",
    "RedHatter": "My bad. You can get the config like so.\nimport { get as getConfig } from 'ava/lib/worker/options.js';\n\ntest('...', (t, page) => {\n  getConfig();\n  ...\n});\n\n. I'm building some until functions to create visual snapshots for regression testing using puppeteer. I need access to the config object so I can get the snapshotDir and whether ava was run with --update-snapshots. Exposing on the meta options seems perfect.\nYou could expose the raw command line arguments that were passed to ava. That way if you remove the --update-snapshots option from the snapshot work flow any test or functions that depend on it would still work. In addition it would have the added benefit of allowing me to specify --update-visual-snapshots if I want.. Ah, okay, was not aware of that.\n. ",
    "momocow": "I have come up with 2 ideas worthy to have a try:\n1. Filter those files from lint-staged whose filename is ended with .test.js.\n2. How about trying --match flag provided by ava command, which can filter the title of tests?\n  (FYI: https://github.com/avajs/ava/blob/master/docs/05-command-line.md#running-tests-with-matching-titles)\nDon't blame me if not working ;)\nI just happened to read this issue and thought that these ideas might help.. ",
    "NathanielHill": "@momocow Thanks for the thoughts, I don't think either of those really work.\n1) Using lint-staged I can specifically only apply actions to test files, but this means these actions are run when the test files themselves change, not the files the tests apply to. Not really what I need.\n2) This might work, if I put the filename of the tests in each test. However, lint-staged is passing complete pathnames, so this would require an additional step to strip the paths down to a relative path. Also, seems terribly fragile - for example, how does this work for integration tests where I may be importing components or modules from multiple files?\n@havenchyk I'm new to ava, but I guess this is a feature request then. Seems like a pretty critical one IMHO, because the workflow has always been so straightforward for me and just makes sense. Running related tests before every code commit is great DX, and good practice.\n. Doesn't seem too complicated. If given a list of test files and a list of staged files, all that needs to be done is determine which if any test files import from the staged file list (passed as command line arguments). My vote would be to keep/use the files config for the former list.. ",
    "AgentBurgundy": "\nUntil there is a robust system to analyze the actual test and source files to figure out what to run, you could get 90% of the way there by using sed or awk in your pre-commit script. Conceptually, what you want to do is map the output of lint-staged and append .test.js to each of the filepths that it outputs. AVA will then run those tests. So if you change foo.js, it will run foo.test.js.\nIt's not perfect, because foo.test.js may not necessarily be the only test that imports (or indirectly relies upon) foo.js. But it's a good start, especially if you can stick to a 1:1 relationship between source files and tests.\n\nI'm new to this repository, so my solution is a little rough haha, but after reading what @sholladay said, I figured it would be simple to implement. I got it working by adding a flag to the cli\njs\nprioritize: {\n    type: 'string',\n    default: []\n}\nWhich I later use in cli.js where the test files are normally assigned to a constant.\\\n``js\nlet files = '';\nif (cli.flags.prioritize.length > 0) {\n    console.log(You're prioritizing ${cli.flags.prioritize}`);\n    files = cli.flags.prioritize.split(' ').forEach(file => {\n        let temp = file;\n    file = file.replace('.js', '.test.js');\n\n    console.log(`${temp} is now ${file}`);\n});\n\n} else {\n    files = cli.input.length > 0 ? cli.input : arrify(conf.files);\n}\n```\nOf course there are some issues here, we would probably want to pull the desired test suffix out of the ava config file for one.. ",
    "Druotic": "@havenchyk Nice! ava@next worked like a charm, thanks! Looking forward to the next release \ud83d\udc4d . ",
    "itskolli": "@novemberborn I have updated the docs to clarify the usage of CLI options along with options in package.json. Please take a look into this PR #1993 . ",
    "FreddyFY": "I don't know if it'll help you but I had a similar problem.\nChanging@babel/plugin-syntax-dynamic-import to dynamic-import-node worked for me.. ",
    "foxbunny": "It probably won't be as simple as what you describe as it would need to support both binary and unary forms. It'll probably end up being not worth it. Partially applying t.is is probably simplest here:\n```\ntest('something', t => Promise.resolve('hi').then(t.is.bind(t, 'hi'));. Oh, ok, I get what you mean. Still, there's an increased maintenance cost doing that where partial application is already available natively and can solve the issue. Of course, I'm not the maintainer, so it's not my problem. Just pointing it out.. Personally, I'd rather that ava developers have an easier time maintaining the library because that means it will be more stable for me. I'll trade stable for ergonomics every time, especially since this is a test framework -- it's supposed to be fool-proof. Why not just start a 3rd party project for the curried version?. @Christilut Don't know if it helps, but github has a search bar at the top which can be used to search within the repository. You then have a filter to select only Markdown files.. I agree it's not the same. \nIf the ava devs are fine with introducing some Python dependencies just for documentation, I could look into preparing a MR that ports the existing docs to MkDocs. It has the ability to create search indices that are strictly client-side, and a single command to publish on GH pages.\nThemes can be customized to taste, although I normally don't bother and just go with the ReadTheDocs theme you see on the link I've provided.. > Search is slow and doesn't work intuitively.\nI've just checked the MkDocs site, the search doesn't work at all for me. Though, I've set it up for Imba earlier, and the search is both quick and finds what you need.. ",
    "pearofducks": "@sindresorhus - fixed per your suggestions. Let me know if anything could use further improvement.\nTravis and code coverage seem to be randomly failing - unrelated to the changes in this PR.\ne.g. Travis fails in 1b8a5ba, but then codecov fails in 3b5fb9d.. @novemberborn - looks good, thanks for helping get this in shape!. ",
    "alanosman": "Actually, I figured out that there is a flag I can provide at runtime ava --serial which does what I need, so closing this. . ",
    "ijjk": "@novemberborn okay thanks for the reply! Yeah, the changes started to add up. I'm still sorting out some test cases, and then hopefully it will be good to go.. @lo1tuma thanks for trying out the changes! Yeah, there are still some kinks to work out. I actually just pushed up a change where I removed the use of vm2 since I had to use require in the host context which pretty much eliminated the sandboxing. \nI think one of the main reasons it was so much faster when you tried it and then it stalled was because the modules are being cached in memory in require.cache between tests. This caching seems to be causing problems so the current solution I've found is to wipe require.cache. ~~Currently wiping require.cache has a pretty heavy cost on performance so if you have any ideas on more optimized ways of doing this I'd appreciate them.~~\nI briefly tried using import-fresh since it seems like a better idea than wiping the entire cache at the start of every test file. I still need to investigate more why it was failing when I implemented it though. Also, is the project with the test suite you ran it against public by any chance? \nEdit: nevermind on the require.cache question I ended up implementing a custom vm set up.. @novemberborn \n\nWhat do you mean by having to use \"`require\" in the host context\"?\n\nvm2's NodeVM has the option to require in the sandbox or in the host context. When using the sandbox require mode, modules that need to modify the module.constructor._extensions like @babel/register fail since vm2 doesn't expose a full module instance. \nYou can use the host require mode and it works but this implementation breaks pretty much any sandboxing since any changes made globally or to the module on require are done in the host and not the sandbox. \nSo for example, in host require mode, if you require a module let's call it browser-setup.js with below contents, the changes will be made in the host context and not the sandbox pretty much breaking the isolation.\njs\nconst { JSDOM } = require('jsdom')\nconst { window, document } = new JSDOM(`<!DOCTYPE html><p>Hello world</p>`)\nglobal.window = window\nglobal.document = document\nglobal.fetch = require('node-fetch')\nI couldn't find any existing modules that handle this.\n\n\nvm2 has above problem\n\n\nisolated-vm runs in a fork and according to the README mainly works with node >= 8.11.2\n\n\nnapajs runs on seperate threads and doesn't support all built-in node_modules\n\n\nThis is why I started working out a bare minimum custom implementation of vm. . @novemberborn WorkerThreads look interesting although still experimental. I re-worked my branch to support a non-isolated single process mode, the current mode of creating a new fork for each test, and a shared fork mode which reuses the forks and distributes tests to them as they become available. \nThe single process mode and shared fork modes are opt-in currently. I could try out a WorkerThread implementation that's opt-in too since it's still in the experimental stage. Was this re-work what you had in mind with the previous comment?\nEdit: Not sure why 1 test fails on the mini reporter and 1 on verbose only on Windows in travis-ci. If you have an idea why I'd appreciate it.. @novemberborn I added the option to use Worker Threads instead of forking. The API was pretty similar to child_process.fork so I was able to implement it using ./lib/fork.\nIt's also set up so that Worker Threads can be shared in the same way that forks can with the --share-forks flag since creating new Worker Threads can be expensive on performance similar to creating new forks. \nOn one project's test suite that has around 632 tests across 59 files, that is using AVA, I found on github, the suite had below run times. Note, by no-cache I mean the cache under node_modules/.cache has been cleared for the project.\n| Mode | Run duration in seconds |\n| ---- | ----------------------- |\n| ForkTestPool no cache | 48.67s |\n| ForkTestPool with cache | 38.34s |\n| SharedForkTestPool no cache | 22.09s |\n| SharedForkTestPool with cache | 14.91s |\n| WorkerThreads no cache | 48.43s |\n| WorkerThreads with cache | 35.76s |\n| SharedWorkerThreads no cache | 21.51s |\n| SharedWorkerThreads with cache | 15.60s |\n| SingleProcessTestPool no cache | ~~Suite can't run in single process~~ 50.44s |\n| SingleProcessTestPool with cache | 41.13s |\nI also ran this on styled-jsx's test-suite which is around 174 tests across 16 files and got below run times.\n| Mode | Run duration in seconds |\n| ---- | ----------------------- |\n| ForkTestPool no cache | 10.73s |\n| ForkTestPool with cache | 6.34s |\n| SharedForkTestPool no cache | 8.80s |\n| SharedForkTestPool with cache | 6.30s |\n| WorkerThreads no cache | 10.84s |\n| WorkerThreads with cache | 6.96s |\n| SharedWorkerThreads no cache | 9.17s |\n| SharedWorkerThreads with cache | 6.37s |\n| SingleProcessTestPool no cache | 8.11s |\n| SingleProcessTestPool with cache | 4.36s |\nI didn't run these under a super controlled environment so they might not be perfect estimates but thought it might be helpful to compare on actual projects using AVA.. @SneakyMax thanks for trying out the branch! If you have a chance could you try running them again with the latest changes? \nI'm not able to reproduce the --worker-threads not being able to find tests error so any more info would be appreciated. I was able to reproduce the hanging and running out of memory errors and it appears there was a memory leak in the way I was clearing require.cache between test files which should be fixed now. \nYou might need to clear the cache under node_modules/.cache/ava before trying again.. @novemberborn I understand it is a lot. I started looking over the checklist and tried addressing some of the points below.\n\nReview documentation (probably needs a dedicated section / recipe)\n\nI can add more verbose info to the documentation just have been waiting until everythings been agreed on. \n\nBikeshed CLI flags\n\nI was thinking it might be good to instead of having cli flags for each test pool to just have a test pool flag e.g. --test-pool singleProcess or --test-pool sharedForks. With the test pool flag we could also allow passing in a path to a custom test pool to achieve something like #1451 I'm open to thoughts on this though.\n\n\"Shared fork\" reusing fork leads to complicated code paths, both in the pool code and in the Fork class, the latter since it's used both in shared and non-shared modes\n\nAre you thinking it would be better to have a seperate fork module for shared forks? I was mainly trying to avoid duplicating code. \n\nConsider possible code re-use between subprocess code and single process test pool\n\nI just pushed up some updates and was able to re-use the code in subprocess with single process test pool.\n. @SneakyMax Interesting \ud83e\udd14on the node project you might be right about having a memory leak in your require cache clearing also. I had to make sure to remove the module from the parent module's children to get it to free up. It's weird that the shared forks froze up faster than the single process though since the shared forks should have more memory available unless your environment has restricted memory so the shared forks hit it faster maybe.  \nThe Vue errors have me curious, I wonder if something is being modified globally that I'm not catching somehow. Is either of these able to be shared by any chance or possibly a minimum example of the set up so I can try investigating a little more? . @SneakyMax I understand. I added an option cacheRequire that defaults to true which allows you to either require options.require everytime before new tests or to only require them once before all tests in that process. \nNote: this is only used in sharedForks and singleProcess mode. This might help increase performance a bit if your tests are set up to be able to allow cacheRequire to be enabled.. This was left over from some previous debugging sorry. I was thinking it could be removed also. I removed it and updated the docs with the new command.. After re-working the branch, I don't think run-status need updating anymore. Do you think it still does?. This ended up getting removed, but good to know. . Good catch, I actually updated it to use nowAndTimers.setImmediate since it just needs to run after runStatus.observeWorker has been called.. This is mostly for mocking during testing. I thought it would be easier only mock here instead of each test-pool that needs Fork. Yeah that's better \ud83d\udc4d . Yup, updated. Updated. Yeah that makes sense, combining them can add a lot of complexity. Updated. Updated. This was named promise to match the current fork.promise. We can rename it just we would also have to update it in ./api.js and everywhere else. . Yeah that makes sense, updated. Removed all except those related to worker_threads, since I can't test them on unsupported node versions it brings down overall code coverage. This has been updated now that I'm re-using subprocess for SingleProcessTestPool. Not really, mainly there to be verbose. Removed.. I updated this to only be used in SingleProcess mode to fix an issue with esm causing the getRunner export not to be set here.. ",
    "SneakyMax": "I wanted to try out this branch, so If you'd like some more anecdotal numbers (Node 11.9.0):\nVue Project\nWe have a Vue project that has 180 tests over 101 files. We use the t.snapshot feature a lot:\n\nava - 1:36, all tests pass\nava --worker-threads - Cannot find any tests in any of the files.\nava --share-forks - Hangs after 0:25 with 166 tests passed, 2 failed. Error in the Vue runtime file.\nava --single-process - Exits after 0:28 with 68 tests passed, 0 failed. Same error as before, except this time it quits the whole process.\n\nNode Project\nWe have a GraphQL API that extensively clears the require cache during its tests, and all tests run serially in each file because they use global state. It has 1447 tests over 96 files:\n\nava - 2:52, all tests pass\nava --worker-threads - Cannot find any tests in any of the files\nava --share-forks - Froze my VM\nava --single-process - JavaScript heap out of memory after 333 tests\n. Sure thing @ijjk!\n\nVue Project\n\nava - 1:37, all tests pass\nava --worker-threads - 1:49, all tests pass. Slower :(\nava --share-forks - Hangs after 169/180 tests pass. A couple tests errored (Error in nextTick: \"TypeError: Failed to execute 'appendChild' on 'Node': parameter 1 is not of type 'Node'.\") (jsdom?)\nava --single-process - Error after 68 tests pass: node_modules/vue/dist/vue.runtime.common.js:7005\n  var transitionDelays = styles[transitionProp + 'Delay'].split(', ');\n\n--share-forks is significantly slower than the previous time I tested, unfortunately. Whatever cache changes you made seem to have undone any performance benefit.\nLooks like you fixed --worker-threads somehow though.\nNode Project\n\nava - 2:43, all tests pass.\nava --worker-threads - 3:14, all tests pass. Slower.\nava --share-forks - VM froze after 196 tests passed.\nava --single-process - Heap out of memory after 333 tests.\n\nThe out of memory problems might actually be that whatever require cache busting I'm using has a memory leak, and it never happens during normal ava because the worker processes shut down, taking the memory leaks with them. Global state is so annoying in Node.\n. @ijjk sorry about the late reply - unfortunately I can't share either of the projects :(, I'm happy to keep testing the branch though. I hope we can get the speedup I first experienced, without any errors.. ",
    "dflupu": "Hi @novemberborn, thanks a lot for the quick feedback! I've tried to address everything you've mentioned. Please look over the updated changes when you find the time.. I'm rather curious why you'd go through the trouble of checking for Timer#refresh. Not that I have anything against it.. My thought process was along the lines of: whenever an error is thrown during an ava test, the stack trace always shows where the error was constructed, with the assertion being at the bottom of the trace. If an error is thrown inside a .notThrows, I think that it is fair to treat it as any other unexpected error. Same with errors of different types inside a .throws. \nDo you disagree? Is there any situation in which both traces would be required?. I've updated the PR. The output should be closer to what you had in mind.\nLet me know if you see a cleaner way of implementing this. Note that the trace created in the AssertionError constructor doesn't always contain the frame where the assertion was made.. @sindresorhus No worries, I'll be sure to submit it. I prefer to wait until the PR is accepted/merged because it feels like bad manners to submit before. Might be mostly because of the useless bot post when you do submit.. Can't get that test working when ran during npm test. tap --no-cov test/integration/assorted.js works fine. I assume the event loop is being blocked by something else.. @novemberborn Can you take a look at the replies above? I don't know if you get notifications for them so I figured I'd poke you in a separate comment. :). Good catch. I'll make it exit(1) if ran without the watcher as well.. It seemed a bit long, repetitive and unclear.\nExited - not always true\nno new tests - why the new?\n10000ms - do I have to pull out a calculator if its longer duration? do I actually not know AND care about what the duration was?\nof inactivity - repetitive. I'll look into it and revert back if I can't find a way to work around the race condition you mentioned. Wanted to avoid duplicating the code in the reporters.\nEDIT: Could you explain why having the code inside the reporter helped with race conditions? I'm not as familiar with the codebase.. I've removed this test altogether since it didn't work during npm test. For reference, here it was:\n```\ntest('interrupt', t => {\n    const proc = execCli(['long-running.js'], (_, stdout) => {\n        t.match(stdout, /SIGINT/);\n        t.end();\n    });\nsetTimeout(() => {\n    proc.kill('SIGINT');\n}, 2000);\n\n});\n```. I've fixed the naming and added some checks. The reset shouldn't create problems. It's basically the same implementation as before, except its in RunStatus instead of the reporter. I think that it's more fitting for it to be in RunStatus, and it spares us of some code duplication.\nTake a look at the diffs and if you still want it reverted, it's no big deal either way.. ",
    "CrispusDH": "@novemberborn or @sindresorhus , could you take a look at it? It's first PR in AVA so I'm not sure what are next steps. @novemberborn thanks for review, commit with requested change has been done :). ",
    "niktekusho": "\nAh I thought I saw the PR, but then it didn't show up in the links here. Turns out the reference to this issue was incorrect (I've since fixed that). Thanks @niktekusho!\n\nYeah, sorry about that! Glad this was merged quickly! \ud83c\udf89. ",
    "Chrisyee22": "Hi. If this issue is still open I would be happy to work on it.. @MackenzieBerliner-Glasser I am still working on this issue. Got delayed by a flu. \u2639\ufe0f . ",
    "MackenzieBerliner-Glasser": "@Chrisyee22 hey are you still working on this issue? If not I would be happy to work on it :). . @Chrisyee22 oh no :/. I hope you are feeling better! . ",
    "mikob": "@qlonik allowJs is also used when you import plain js in typescript (eg. from libraries that don't have types). I don't have a minimal reproduction, unfortunately, only a complex one.. ",
    "bongofury": "As stated in the PR, it seems to be ok, thank you!\n. Tested, it's working on my large project (~1400 spec files).\nI think this could be merged.\nThanks a lot, @novemberborn !. ",
    "fotonmoton": "@novemberborn I found workaround. After requiring @babel/register you can set \nrequire.extensions['css'] = () => {}\nrequire.extensions['jpg'] = () => {}\nand this imports will return nothing.\nava.config.js:\njs\nexport default {\n  files: [\n    'src/**/*spec.{js,jsx}',\n  ],\n  sources: [\n    'src/**/*.{js,jsx}',\n  ],\n  cache: true,\n  concurrency: 5,\n  failFast: true,\n  failWithoutAssertions: true,\n  verbose: false,\n  compileEnhancements: false,\n  require: [\n    './config/jsdom.setup.js',\n    './config/babel.register.js',\n  ],\n  babel: {\n    extensions: ['jsx'],\n  },\n};\n./config/babel.register.js:\n```\nrequire('@babel/register');\nrequire.extensions['.css'] = () => {};\nrequire.extensions['.jpg'] = () => {};\n``\nFull code on branchsolution` in this repo.\nI think this should be somewhere in docs, or better add support for something like ignoreExtensions to configuration so that AVAs internal babel pipeline can ignore some modules (because this solution works only with @babel/register).. So if I don't use @babel/register I can't rewrite require() calls for specific modules? How can I solve this problem without compiling sources on the fly then? Solution above works for me well but, I think, there should be less \"hacky\" way. Or maybe importing non-js modules is not a good way to compose React components and this problem can be solved outside AVAs context? . Yeah, I see argument behind testing final bundle itself - it will contain \"final\" version of sources and tests for compiled modules will be less \"flaky\" in some edge cases. I will try to setup Webpack to produce compiled components and test them instead of compiling on the fly. And, as you say, by default AVA doesn't follow imports so there is no way to tackle this problem on AVAs side.. I think title is misleading, I wrote about module imports inside sources not tests, but I don't know how describe problem more precisely.. You can check out this issue, I have same frustration. Here working repo with TypeScript support for AVA.. ",
    "oldj": "I had the same issue, then I executed the npm update command and everything worked fine.. ",
    "be5invis": "@sindresorhus \nt.resource would return a resource manager of a given resource kind (i.e., memory of DB connection), which allocates resource with a given amount.\n```typescript\ninterface TestContext {\n    resource(id: string): IResourceManager ;\n}\ninterface IResourceManager {\n    // Acquire resource with given amount\n    acquire(amount: 1000): Promise;\n}\ninterface IResourceHanlde {\n    release(): void;\n}\n```\nNote that the memory resource works pretty like a semaphore: it does not really do malloc, only ensures that there are not too many tests running in parallel.. @sindresorhus \n1000 is simply a number, a number representing the resource quantity that the test claimed it will use. It has no link to the actual memory usage. Ah I should name it stress-test-capacity instead of memory.\nResource managers will take this number, and decide whether to resolve the promise: resolve, wait or throw a error (unable to allocate).. @novemberborn Yes, coordinate resources across tests.\nI have a lib with some stress tests, may take a lot of memory or whatever, so I want to limit the parallelism among them -- they are distributes in multiple files, belonging to multiple sub modules.. @lo1tuma\nDoes Throat work across Ava's worker processes?. Well, my problem is slightly different: #1366 is mostly about preparing some resource before a test run, and my proposal is mainly about parallelism control.\n\n\u53d1\u4ef6\u4eba: Mark Wubben notifications@github.com\n\u53d1\u9001\u65f6\u95f4: \u661f\u671f\u4e8c, \u4e8c\u6708 26, 2019 01:18\n\u6536\u4ef6\u4eba: avajs/ava\n\u6284\u9001: Belleve Invis; Mention\n\u4e3b\u9898: Re: [avajs/ava] Mutexes / external resources (#2048)\nI think this is a duplicate of #1366https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Favajs%2Fava%2Fissues%2F1366&data=02%7C01%7C%7C7deaf904623045af571208d69b452fb8%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636867118788726192&sdata=Y3iah80CG3v13zv3%2Fk54wLpdqbu6J9%2FMP%2FS%2FaVbKUDY%3D&reserved=0. @be5invishttps://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fbe5invis&data=02%7C01%7C%7C7deaf904623045af571208d69b452fb8%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636867118788736196&sdata=KkHwxSVWiglkg7trfGC96L9QhXf3cx4RBTsAEH121mw%3D&reserved=0 what do you feel about moving the discussion there?\nIn order to make progress on this we need to flesh out use cases further, and make the argument for why this can't be done by other modules building on top of AVA. Our proposal process is lacking at the moment, so for now bullet points in GitHub comments will suffice.\n\u2015\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Favajs%2Fava%2Fissues%2F2048%23issuecomment-467097491&data=02%7C01%7C%7C7deaf904623045af571208d69b452fb8%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636867118788746201&sdata=G9WYM4DjatbKL0UGmb3MOVT%2FjS9UMBNVDftp3wI3IJg%3D&reserved=0, or mute the threadhttps://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAAOp21OT5do4KQeG6P39KDGs5mHUa0uxks5vRBrEgaJpZM4bI8KX&data=02%7C01%7C%7C7deaf904623045af571208d69b452fb8%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636867118788756212&sdata=c2doenp5BrcHabLtu9Juoi1MQbN7hq4XpKymF31%2FZEU%3D&reserved=0.\n. @novemberborn \nA resource manager could do both, but in general this issue and #1366 are two different aspects. I recommend handle them separately.. ",
    "aparajita": "You're missing a bunch of config to make aliases work, ava won't read Nuxt's webpack config:\nhttps://github.com/avajs/ava/blob/master/docs/recipes/babel.md#webpack-aliases\nhttps://github.com/shortminds/babel-plugin-webpack-alias-7\nHowever, it still won't work even with the documented config. The ava Babel pipeline unfortunately calls babel-plugin-webpack-alias-7 before transforming your source code. The alias plugin is looking for require statements, and since your ES6 imports haven't yet been transformed to require calls, no alias resolution takes place.\nOne option might be to disable ava's Babel pipeline entirely and configure it manually. Haven't tried it (I already lost half a day tracking down the ava bug), we ended up just switching to mocha and got aliases working no problem.. I really wanted ava to work, it looks promising. But we lost a full day trying to get this to work and had to give up. mochapack worked immediately.\nI've been an open source maintainer myself so I admire what you're doing. Keep up the good work! And remember that every hour spent on better documentation is 5 hours less of support. \ud83d\ude01. ",
    "IgnusG": "I was using jest in a previous project and it has a watchAll option which disables its \"smart\" change tracking and reruns all tests anytime anything changes.\nI was quite happy with this behavior. Used it quite often for smaller test suites when I just want to see all green or a complete list of every failure from every test.\nMaybe ava could make use of something similar?. This works really well: npx sane ava\nsane is just a file watcher I picked randomly from the registry . Yeah, knew that. It's just 2 more key presses than I would prefer :). It was just a problem I had when I followed this recipe. \nFor this project I used webpack to bundle my code, but I don't use babel. Because of that, I rely on webpack to resolve the import/export paths from my files, but that's obviously missing when ava is running these tests.\nWhen I followed this recipe, using the esm extension helped compile the imports/exports via node but it still didn't allow custom module paths - only relative ones.\nYou can achieve this by using babel and ditching the esm extension, or some other magic with webpack integration (pre-compiling sources etc.), but if you're using esm and still want module paths - this is the simplest way.\nI found that package in a discussion on a different repo - took a while so I thought I'd include it as a remark here :)\nMaybe it could be just a side-note instead? Something like\n\nIf you want to use custom module paths, you can do so by including the module-alias package after the esm package in your register config field.. \n",
    "skycult": "Yes I solved putting the AVA absolute path.\nThank you. ",
    "arve0": "Uncaught exceptions will crash your tests, ~~possibly~~ preventing .always() hooks from running?\n. I'm not familiar with the internals, but are there cases where uncaught exceptions will allow .after to run?\n. ",
    "geowarin": "I think if you don't merge this with the babelConfig already defined, this will implicitly be merged with the .babelrc.\nWhich is totally cool if the babel option is set to \"inherit\". Otherwise, metadata generation might fail.\njs\nvar config = objectAssign({}, this.babelConfig, {\n   plugins: this.babelConfig.plugins.concat(detective)\n})\n. See this repo failing with this config\n. ",
    "gurpreetatwal": "I fully agree, that was just some blind copy and pasting from the comment on my part. Should I still keep the link to the issue for anyone who wants to know more?\n. Ah okay, did not know npm did that. I'll change those back.  There are two other json snippets (L664 and L680) that are currently indented with tabs, should I change those as well? \n. ",
    "sheepsteak": "@lithin this needs to be snapshotState.get.bind(snapshotState) or snapshotState.get.bind(this) as it uses this internally and I keep getting the error: Error: Cannot read property 'state' of undefined.\nI'm not sure which is more correct but both seem to work and stop the error. . ",
    "chaucerbao": "The entry: ['src/tests.js'] can be a list of t.test or, better yet, require or import statements pulling in the tests you want to run, because you probably don't want all your tests in one file, unless your project is tiny. As the name suggests, it's the entry point that Webpack will look at to determine what to compile.\nThe output.filename will contain the compiled code.. ",
    "efegurkan": "that explains the failing tests indeed \ud83d\udc4d . for jest it updates only for running tests. \n. oops \ud83e\udd37\u200d. ",
    "willmendesneto": "There's no --inspect-brk. Could you update t=your regex removing that validation, please?. ",
    "ace-n": "Agreed and done - though this does burrow a bit into the TestCollection, I feel like adding such a loop at the top of buildStats() (or exit(), if we were to rename it) is a cleaner way to do this.. Done.. I imagine people want to be able to identify any non-passing tests with --fail-fast. That's doable without this PR if all tests are serial (--> any tests after the failed one are pending), but harder to reason about if test order is nondeterministic or involves multiple files executing concurrently.\nThoughts?. Done.. Done. I missed this originally - good catch!. See follow-up above.. The teardown process is initiated by the SIGINT handler added to lib/main.js - which then calls the exit() function in that same file (which I'm relying on to handle any required tear-down tasks).\nIf you disable the SIGINT handler in lib/main.js while keeping this one, AVA (from what I can see through the CLI) basically ignores SIGINTs. If you disable this SIGINT handler however, lib/main.js doesn't have enough time to gracefully exit.\nThis was largely based on experimenting with the code though - so it's definitely possible that I'm initiating the wrong teardown call/event.. I misspoke - sorry about that \ud83d\ude04 s/non-passing/pending/g\nThat being said - I see two distinct use cases for --fail-fast:\n1) as a simple yes/no \"Do my tests pass?\" check? In this case, I'd agree that people don't really care about which tests are pending.\n2) iterative development/debugging. Personally, I'd find knowing which tests are pending useful in this case - especially if I'm trying to troubleshoot why said tests are pending.\nIf Option 1 is more important, I have no objection to hiding pending tests on --fail-fast - but I'd vote to keep things the way they are if we're prioritizing Option 2.. There are spurious repeated results in some tests otherwise - namely here. This is a bit of a band-aid fix though, so let me know if further investigation or a more proper fix is required.. Done - any files that haven't started running when a SIGINT is received will trigger an AvaError stating that they weren't run. I assume that works?. ",
    "Navomi-Jaden": "Thanks for this catch!. Gotcha. Alright I removed that.. I'll add another function that checks the keys and return a boolean for whether the keys are valid or not and place it in that checks place.. Good idea moving this to be checked here https://github.com/avajs/ava/blob/master/api.js#L134 . ",
    "tacooper": "I think you meant require('fs') here.. ",
    "iwilson-r7": "I believe this should use nowAndTimers.setTimeout, otherwise use of sinon.useFakeTimers and forgetting to restore the original timer functions in a test can cause the test run to either hang inexplicably or exit without completing (a difficult to debug problem as I discovered!). "
}
{
    "Winslett": "Does vulcand read etcd at load and run without re-reading the etcd for changes?  Or, does it poll etcd for changes?\n. @tvb I'm indecisive how this should work.\nMy core issue with moving this way is solving the \"what if the etcd cluster goes away?\" problem.  I need to create another issue for that problem, and probably reference this problem.  If we relied on etcd state for leader / follower on haproxy_status.sh, and etcd had a maintenance window, crashed, or had a network partition, then the Postgres cluster would go down.  With the current behavior, etcd going away would cause governor.py to throw an urllib error, which would stop PostgreSQL.  In a perfect scenario, if etcd is unavailable to a running cluster, the cluster should maintain the current Primary if possible, but not failover.  @jberkus and I chatted about this scenario.  If etcd is unaccessible by the leader (network partition, etcd outage, or maintenance), a leader governor should expect a majority of follower governors to provide heartbeats to the leader.  If follower heartbeats are not providing enough votes, the leader governor would go read-only and the cluster would wait for etcd to return.  I would start the process by modifying the decision tree.\n[update: created the issue at https://github.com/compose/governor/issues/7]\nIn the interim of solving that problem\u2026\nThe more I think about this, the more I think governor.py should handle the state responses to haproxy.  Thus, removing the haproxy_status.sh files and moving the HTTP port cofiguration to the postgres*.yml files.\nFor people who know Python better than I do, is there a sensible way to run governor.py with a looping runner and a HTTP listener?\n. TL;DR: it solves the problem of an unknown leader.\nWhen a stopped PostgreSQL with files in the data directory is started, it is started as a secondary without an accessible leader.  I chose 169.0.0.1 because I was thinking it was the \"Automatic Private IP Addressing\" space , but I was wrong -- looks like I should have chosen 169.254.x.y.\nI originally made this change for the scenario of an initialized deployment with no running Postgres members and the etcd TTL had expired.  In that scenario, the best situation I could come up with was:\n1. governor starts PostgreSQL as a secondary\n2. governor queries all known PostgreSQL members to determine healthiest member\n3. governor only tries to take TTL if it is the healthiest member\nUsing this method, a deployment can recover from all hosts going down.\nIn this governor project, there is one check I did not implement between 2 and 3 above.  It could be a \"is reasonably healthy\".  With this, store the last known xlog position for the leader in a governor for each HA loop.  Then, 2.5 would be \"confirm this host is within some megabyte size of the last known leader xlog position.\"  This additional check would prevent a stale PostgreSQL from taking over as leader if it is the only member running.\nAfter I solved for the scenario above, I never had issues with PostgreSQL coming online as a secondary.  When the HA loop would run, govern would point the secondary to the proper leader.  Thus, it wasn't an issue.\n. @scalp42, All changes are welcomed.  Thank you for the fixes.  I'm looking forward to the others too.  Cheers.\n. @bookest thank you.\n. @pires \"load balance\" was not the correct term.  I should have used \"properly connect to the master\".  I will update the README.  For load balancing reads / writes, you are correct that a different solution is needed.\nAs for high availability, in my testing, haproxy was the only tool which worked reliably with multiple role changes.  PGPool stopped connecting to the leader properly after a few failovers between hosts or disconnects.\nDo you have a solution that would make the proxying a better experience?\n. @feikesteenbergen I was wondering why this is closed?  Are you not interested in pursuing it?\n. I'm thinking we should add rows for pg_hba.conf to the postgresX.yml files.  As part of the initialization process, Governor should create the proper pg_hba.conf records.  That would allow people to customize the cluster based on their needs.\npostgres:\n  pg_hba:\n    - type: local\n      database: all\n      user: all\n      method: trust\n    - type: host    \n      database: all\n      user: all\n      address: 127.0.0.1/32\n      method: trust\n    - type: host    \n      database: all\n      user: all\n      address:  \"::1/128\"\n      method: trust\n    - type: hostssl\n      database: all\n      user: all\n      address: 0.0.0.0/0\n      type: md5\nThe replication user should automatically be added when creating the pg_hba.conf.\n. @tvb, Good catch!  This will probably be a catalyst for a test suite, which I've been pondering on.\n. Actually, I reverted that change because it wasn't a typo, it was just confusing code.\nThe call to last_operation at https://github.com/compose/governor/blob/master/helpers/etcd.py#L85 is calling the last_operation method on the state_handler(), which retrieves an xlog location from Postgres.\nThe last_leader_operation method at https://github.com/compose/governor/blob/master/helpers/etcd.py#L90 is a call to etcd, which retrieves the last recorded xlog location stored in etcd.\nThe method names should be refactored to avoid confusion.\n. Create a new master branch, and see if the problem persists. \ngit fetch origin && git checkout origin/master -b new_master \nI force pushed to erase the change. I'll quit doing that. \n. That method has existed since 3 days ago: https://github.com/compose/governor/blob/master/helpers/postgresql.py#L210\n. Typically, if you are starting a new test with governor, it is best to stop etcd and any governor processes.  Then, run rm -rf data/* from the governor directory.  Then, restart etcd + the governor processes.\n. This should never happen unless you are running different clusters trying to talk to each other.  For example, you initialize clusters A and B separately.  Then, you try to attach a follower from cluster B to cluster A.\nThis could happen in Governor if you have two clusters sharing a namespace.\n. See #18\n. :+1: merged\n. :+1: \n. This happens when you do a pg_basebackup from a different Postgres cluster than you are trying to stream replication from.  This can also happen if you run initdb for the member instead of running pg_basebackup from the cluster's leader.\nI'm assuming you ran initdb for this Postgres directory.  If you did, remove the data directory for that Postgres, and let governor run initialize the empty directory from the cluster's leader.  Basically, you should start governor with Postgres uninitialized.\n. Fencing: take a look at the setting and code around maximum_lag_on_failover. Each time the loop runs, we log the last XLOG position for the primary Postgres.  With maximum_lag_on_failover, you can tailor the maximum bytes between the recorded value and a follower deemed healthy enough to failover.\nQuorum: take a look at this method https://github.com/compose/governor/blob/master/helpers/etcd.py#L81.  With it, we use the built in prevExist=false, it is a check and set action built into etcd.  We rely on it to etcd's RAFT and prevExit to ensure there are no ties.\n. Great questions.\nHaproxy status checks rely on etcd as a single source of truth.  If a rogue node can take over the leader key, it would start receiving reads and writes.  The maximum_lag_on_failover is one convention used to prevent that -- particularly when all nodes are shut down and nodes are started one at a time.  Take a look at the blocker method is_healthiest_node (https://github.com/compose/governor/blob/master/helpers/postgresql.py#L125).  The is_healthiest_node method is called before any node tries to take over as a leader.\nIf an etcd namespace is shared between Postgres Governor clusters, Governor doesn't prevent the cluster from comparing themselves to nodes of other clusters.  It'd be awesome if you could code something up to prevent rogue clusters from trying to overthrow the current cluster's etcd namespace.\nTake a look at the code and throw some scenarios at the code base.  If you have a scenario of a rogue takeover that can be recreated, we'd love to solve for that situation.\n. @blelump I was head deep in some different code and just saw your request for information.\nThe follow_no_leader on a returning member allows Postgres to run health checks on any member coming online before participating in the HA process.  This returning member starts as a follower, and thus can request and apply any WAL logs and validate it is in a consistent state before participating.\n. Excellent addition.\nSince Postgres socket location can change based on the database configurations, I'd like to design a configuration setting to enable connections no matter the Postgres configuration.  Instead of a boolean configuration setting use_unix_socket, is it possible to set a unix_socket_path: /tmp/.s.PGSQL.5432.\nAnother possibility would be to keep the boolean, but rebuild the Unix socket path based on the PG configuration settings.\nThoughts?\n. :+1: nice addition.\nI'm going to merge this.  I'll extract the connection string generator into it's own method to cleanup the functionality of the cursor.\n. Will you update the README.md to match the changes?\n. Specifically regarding ssl.SSLError raised from the touch_member method:\nThese errors should be handled differently based on where the error happens.  If the error happens at initialization, we should rescue from it, but we should notify about the flakiness of the SSL attempt.\nDuring the HA loop, we should probably re-attempt this command, but if it fails, then we should gracefully degrade Governor's capabilities until etcd access has been restored.\nLong story short, it's not just rescuing this additional error, but doing it properly in context.\n. This is a scenario this template chose not to address. pg_rewind is effectively deleting data.  Every HA deployment has a different best-outcome for the times when pg_rewind would be used.  Some want to save the data from the mis-fork, some are okay with overwriting the errors, some need STONITH more than pg_rewind.\nIn certain scenarios, I would argue that it's better for a database to be down than to have mis-forked data.\n. So\u2026this is a much deeper issue than it appears at the surface.  There needs to be an extensive amount of warning around the extension of timeouts and re-attempts. Governor relies on the expectation that ETCD has all the information about the environment.  The extension of timeouts and connection reattempts reduces the trust that Governor puts on ETCD, which introduces more potential for split brain clusters.  This is \"okay\" for people who can trade off consistency for availability.  Scenario:\netcd:\n  timeout: 60\n  attempts: 5\nThe above scenario would mean that a Governor acting as a leader, if losing connection to ETCD, would hold the state of primary for 5 minutes before going read-only.  In the mean time, during that 5 minutes, other members of the cluster would take over the primary role.  This behavior introduces manual cleanup because of split brain, and may be a serious issue for people who require higher consistency.. What error is raised with this?  What happens when this error is triggered?\n. ",
    "pires": "AFAIK, vulcand subscribes (long-polling) for changes in a number of keys and reloads configuration every time you add/change/remove frontends, backends, etc.\n. confd is just another moving part that exists only because haproxy doesn't allow for frontend/backend management without reloading configuration file. I'm against adding other moving parts as it makes the solution more complex to setup and maintain.\nI'm currently working on having TCP support for vulcand.\n. True. Working on that.\nOn Jun 11, 2015 16:22, \"Kapil Thangavelu\" notifications@github.com wrote:\n\nvulcand has significantly worse performance, upstream development also\nseems to have paused.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/compose/governor/issues/1#issuecomment-111171267.\n. @Winslett well, no. For proper load-balancing to happen we'd need to actually jump into the pipe and understand the query itself for breaking into read and write operations. pgpool does act as the middleman (proxy) and handles that for you, namely with Streaming-Replication.\nWhile using Streaming replication and Hot Standby, it is important to determine which query can be sent to the primary or the standby, and which one should not be sent to the standby. pgpool-II's Streaming Replication mode carefully takes care of this.\n\nBut unfortunately, they're scarce on details.\nI know a similar solution is kind of far-fetched but definitely needed.\n. @schmitch we're discussing different issues here. Slaves can't write but should be able to read in order to scale. Right now, that doesn't seem possible with current implementation [this project].\n. @schmitch I totally feel your pain. But it would be awesome! Otherwise, we just have failover and no throughput scaling. \n. @schmitch correct. So, should I close this issue?\n. @schmitch don't take me wrong but doing it at the app layer kinda feels wrong. To me, both as a developer and an enabler - in the sense of enabling developers to do all sorts of crazy stuff without actually knowing details of PostgreSQL or whatever -, we (Compose, me, whatever) need the proxy behavior. I know it's hard. I know it's error prone. But it's a huge feature to have. Not a must, but a really-awesome-to-have feature.\n\nBtw.: I did not use your project\n\nWhich project do you refer to?\nOh, and I'm eager to contribute to this project. Can we go the go way? :-D\n. @schmitch to be clear, I'm just part of the community and am not working on this project as a member of Compose.\n. ",
    "PierreKircher": "thats something confd can do ... first of all vulvand does not support tcp lbing .. so thats not possilble anyway .. nginx could .. yet haproxy is the far better solution\n. ",
    "kapilt": "vulcand has significantly worse performance, upstream development also seems to have paused.\n. ",
    "tvb": "Hi @Winslett any plans to implement this any time soon?\n. > In a perfect scenario, if etcd is unavailable to a running cluster, the cluster should maintain the current Primary if possible, but not failover\nThis is tricky as there would be no way for the primary to check its status.\n. @Winslett You said \"properly connect to the master.\" but how is this accomplished? haproxy has no knowledge of which pgsql node is master. right?\n. I altered haproxy_status.sh to let haproxy put down the server that is read_only so no queries will be sent  to it:\n``` bash\n!/bin/bash\nwhile true\ndo {\n        response=$(echo \"SHOW transaction_read_only; SELECT pg_is_in_recovery();\" | psql -t postgres --port $2 --host $1 2> /dev/null | tr -d \"\\n\" | tr -d \" \")\n    if [ \"${response}\" == \"offf\" ]\n    then\n            echo \"HTTP/1.1 200 OK\"\n            echo \"X-XLOG-POSITION: $(echo \"SELECT pg_current_xlog_location();\" | psql -t postgres --port $2 --host $1 2> /dev/null | tr -d ' ' | head -n 1)\"\n    else\n            echo \"HTTP/1.1 503 Service unavailable\"\n            echo \"X-XLOG-POSITION: $(echo \"SELECT pg_last_xlog_replay_location();\" | psql -t postgres --port $2 --host $1 2> /dev/null | tr -d ' ' | head -n 1)\"\n    fi\n\n} | nc -l $3; done\n```\n. I think it is best to go readonly so replication from the new elected master can continue.\n. @feikesteenbergen can you post a screenshot? I'm curious to see how it looks like!\n. @Winslett yes that would be a nice start.\n. I think we need to (re)write the postgresql.conf as well for this to work:\n\nNote: Remote TCP/IP connections will not be possible unless the server is started with an appropriate value for the listen_addresses configuration parameter, since the default behavior is to listen for TCP/IP connections only on the local loopback address localhost.\n. It is not really a bug. I have it working now on non-localhost ip addresses. I had to change the helpers/postgresql.py file so it will write the correct  listen_addresses to postgresql.conf and had to move the write_pg_hba() en the extra write_postgresql_conf() function before the pg_ctl start function together with the changes in the first post to get postgres to start correctly.\n\npython\ndef initialize(self):\n        if os.system(\"initdb -D %s\" % self.data_dir) == 0:\n            # start Postgres without options to setup replication user indepedent of other system settings\n            self.write_postgresql_conf()\n            self.write_pg_hba()\n            os.system(\"pg_ctl start -w -D %s\" % self.data_dir)\nthe postgresql_conf function itself:\npython\n    def write_postgresql_conf(self):\n        f = open(\"%s/postgresql.conf\" % self.data_dir, \"a\")\n        f.write(\"listen_addresses = '%s'\" % self.host )\n        f.close()\n. @wkielas sorry. I do not :(\n. Hmmm then I have another issue:\nTraceback (most recent call last):\n  File \"./governor.py\", line 61, in <module>\n    logging.info(ha.run_cycle())\n  File \"/var/lib/postgresql/governor/helpers/ha.py\", line 70, in run_cycle\n    self.update_lock()\n  File \"/var/lib/postgresql/governor/helpers/ha.py\", line 25, in update_lock\n    return self.etcd.update_leader(self.state_handler)\n  File \"/var/lib/postgresql/governor/helpers/etcd.py\", line 85, in update_leader\n    self.put_client_path(\"/optime/leader\", {\"value\": state_handler.last_operation()})\nAttributeError: Postgresql instance has no attribute 'last_operation'\nLOG:  received fast shutdown request\nwaiting for server to shut down....LOG:  aborting any active transactions\nLOG:  autovacuum launcher shutting down\nLOG:  shutting down\nLOG:  database system is shut down\n done\nserver stopped\nChanged that line and now I get:\nTraceback (most recent call last):\n  File \"./governor.py\", line 57, in <module>\n    postgresql.follow_no_leader()\nAttributeError: Postgresql instance has no attribute 'follow_no_leader'\npg_ctl: PID file \"data/postgres/postmaster.pid\" does not exist\nIs server running?\n. I have not pulled the latest changes you made today. \n. Creating the data directory manually gives a new error:\n$ mkdir -p data/postgres\npostgres@sql1:~/governor$ ./governor.py postgresql.yml\n^CTraceback (most recent call last):\n  File \"./governor.py\", line 48, in <module>\n    time.sleep(5)\nKeyboardInterrupt\npg_ctl: directory \"data/postgres\" is not a database cluster directory\n. So after adding some more debugging I am actually stuck at \nwhile not synced_from_leader:\n            logging.info(\"I am not in sync\")\n            leader = etcd.current_leader()\n            print(leader)\n            if not leader:\n                logging.info(\"I am not the leader, waiting 5 seconds\")\n                time.sleep(5)\n                continue\nbecause my leader is None. Not sure how to recover from that..\n. Actually on sql1 node:\njson\ncurl http://127.0.0.1:2379/v2/stats/leader\n{\"leader\":\"f3a45927640b6da1\",\"followers\":{\"8d32f0c7cf61d86f\":{\"latency\":{\"current\":0.005404,\"average\":0.005804184899796387,\"standardDeviation\":0.004551149296781203,\"minimum\":0,\"maximum\":0.453886},\"counts\":{\"fail\":0,\"success\":1084939}},\"c351a9659cc1ca65\":{\"latency\":{\"current\":0.004392,\"average\":0.004885503809053086,\"standardDeviation\":0.008000549516033555,\"minimum\":0,\"maximum\":6.972503},\"counts\":{\"fail\":0,\"success\":1085703}}}}\nOn the other two nodes:\ncurl http://127.0.0.1:2379/v2/stats/leader\n{\"message\":\"not current leader\"}\nSo the leader is f3a45927640b6da1 which is the sql1 node..\n. Ok, I had to clear/rm the initialize key as there was probably a previous value defined?\n```\npostgres@sql2:~/governor$ etcdctl -o extended get /service/batman/initialize\nKey: /service/batman/initialize\nCreated-Index: 8\nModified-Index: 8\nTTL: 0\nEtcd-Index: 54\nRaft-Index: 520353\nRaft-Term: 9\nsql1\n```\n. Hmmm well I have not seen it again since I reinstalled the cluster. So lets not spent anymore time on this.\n. @Winslett When will you pull this in?\n. Ok removed the files from pg_xlog/, base/ and global/. Governor is now trying to start again.\n```\nLOG:  database system was shut down in recovery at 2015-08-27 12:21:31 CEST\nWARNING:  recovery command file \"recovery.conf\" specified neither primary_conninfo nor restore_command\nHINT:  The database server will regularly poll the pg_xlog subdirectory to check for files placed there.\nLOG:  entering standby mode\nFATAL:  the database system is starting up\nFATAL:  the database system is starting up\nFATAL:  the database system is starting up\nFATAL:  the database system is starting up\nFATAL:  the database system is starting up\nFATAL:  the database system is starting up\n2015-08-27 12:24:18,544 ERROR: Error communicating with Postgresql.  Will try again.\n2015-08-27 12:24:18,545 INFO: None\nFATAL:  the database system is starting up\nFATAL:  the database system is starting up\nFATAL:  the database system is starting up\nFATAL:  the database system is starting up\nFATAL:  the database system is starting up\nFATAL:  the database system is starting up\nTraceback (most recent call last):\n  File \"./governor.py\", line 64, in \n    if postgresql.is_leader():\n  File \"/var/lib/postgresql/governor/helpers/postgresql.py\", line 84, in is_leader\n    return not self.query(\"SELECT pg_is_in_recovery();\").fetchone()[0]\n  File \"/var/lib/postgresql/governor/helpers/postgresql.py\", line 49, in query\n    raise e\npsycopg2.OperationalError: FATAL:  the database system is starting up\nLOG:  received fast shutdown request\nwaiting for server to shut down....LOG:  shutting down\nLOG:  database system is shut down\n done\nserver stopped\n```\n. Ok bacially I did a clean restart by cleaning up as you recommended in https://github.com/compose/governor/issues/13#issuecomment-107049640\nBut now I get:\n2015-08-27 13:17:39,940 INFO: Lock owner: sql2; I am sql3\n2015-08-27 13:17:39,941 INFO: does not have lock\n2015-08-27 13:17:39,946 INFO: no action.  i am a secondary and i am following a leader\nFATAL:  database system identifier differs between the primary and standby\nDETAIL:  The primary's identifier is 6187648254339472088, the standby's identifier is 6150520899791371405.\nFATAL:  database system identifier differs between the primary and standby\n. ",
    "jberkus": "\" If etcd is unaccessible by the leader (network partition, etcd outage, or maintenance), a leader governor should expect a majority of follower governors to provide heartbeats to the leader. If follower heartbeats are not providing enough votes, the leader governor would go read-only and the cluster would wait for etcd to return. I would start the process by modifying the decision tree.\"\nMy thinking was this:\n- if etcd is not accessible to any follower db, it should remain as it is;\n- if etcd is not accessible to the leader db, it should restart in read-only mode in case it is no longer the leader\n- if etcd is not accessible to HAproxy, it should make no changes except for disabling failed nodes.\nThe last reason is a good reason, IMHO, for HAProxy to be doing direct checks against each node as well as etcd, via this logic:\nIs etcd responding?\n    Is node marked leader in etcd?\n        Is node responding?\n            enable node\n        else:\n            disable node\n    else:\n        disable node\nElse:\n    Is node responding?\n        leave node enabled/disabled\n    else:\n        disable node\nOne problem with the above logic is that this doesn't support ever load-balancing connections to the read replica.  However, that seems to be a limitation with any HAProxy-based design if we want automated connection switching, due to an inability to add new backends to HAproxy without restarting.  FYI, I plan to instead use Kubernetes networking to handle the load-balancing case.\nOne thing I don't understand is why we need to have an HTTP daemon for HAProxy auth. Isn't there some way it can check the postgres port?  I'm pretty sure there is something for HAProxy; we really want a check based on pg_isready.  This is a serious issue if you want to use Postgres in containers, because we really don't want a container listening on two ports.\nAlso, if we can do the check via the postgres port, then we can implement whatever logic we want on the backend, including checks against etcd and internal postgres status.\nParathetically: At first, the idea of implementing a custom worker for Postgres which implements the leader election portion of RAFT is appealing.  However, this does not work with binary replication, because without etcd we have nowhere to store status information.  And if we're using etcd anyway, we might as well rely on it as a source of truth.  Therefore: let's keep governor/etcd.\n. Still seems like a heavy-duty work-around to do something which Kubernetes does as a built-in feature.\n. Well, any approach which requires polling has two major drawbacks:\n\n\nrequires an additional container, or an additional daemon inside the Governor/Postgres container, and I thought one goal of this new approach was to reduce external requriements?\n\n\nadds latency between the time failover happens and the orchestration system learns that failover has happened, thus adding additional downtime.. Just make the requirement that the external script has to be in a certain directory and owned by the same user as governor.  There's no real point in restricting it further; if someone can shell in as the user who owns the governor process, they can do anything they want anyway.. So at a minimum, we'd need to pass the external script the result of the election, which would be one of three things:\n\n\nleader\n\nfollower\n\ndown (as in, no longer part of a valid cluster). Discussion: do we want two different failure states?\n\n\nCan't join cluster\n\nLocal instance is inoperable (postgres won't start, etc.)\n\nJust thinking that the system might react differently to those two different situations.. Relevant to this is that in failure state (1), the node probably can't message the orchestration system anyway.   The only time that state would really be relevant is if cluster networking is messed up, or if you have a \"no valid lead\" situation.. Hmmm.  Given that mess of states, I'm starting to think that we just want two failure states:\n\nFail: any condition in which somehow Governor is up, but Postgres isn't taking connections, and\nOrphan: any condition in which the node isn't part of a cluster, but is up for read-only access.\n\nReally only those two states matter in terms of the orchestration system; \"Fail\" status should be taken out of any connection pool, and \"Orphan\" would be according to local policy (some might want orphans in a pool, some might not).\nFor any other status refinements, I think it makes sense to just poll the HTTP status from the individual node, which would tell us all of the above other information.. ",
    "bjoernbessert": "@jberkus \n\"One problem with the above logic is that this doesn't support ever load-balancing connections to the read replica. However, that seems to be a limitation with any HAProxy-based design if we want automated connection switching, due to an inability to add new backends to HAproxy without restarting. FYI, I plan to instead use Kubernetes networking to handle the load-balancing case\"\nYou can add new backends (modify HAProxy config) with zero-downtime. Reload HAProxy with a little bit of help from iptables. We're using this with great success: https://medium.com/@Drew_Stokes/actual-zero-downtime-with-haproxy-18318578fde6\n. In my opinion, i would prefer to keep the current Primary and do not take any \"decision\" when you loose the \"brain\". In general, i try to avoid the situation that the HA software/layer itself is able to bring down the protected application (and the protected application itself haven't any problems at all).\nThe Postgres cluster can go read-only as an additional safety measure, but it's not a must-have.\n. ",
    "scalp42": "Thanks for the answer @Winslett ! About to port all this into Chef and it's definitely useful.\n. LGMT\n. Noticed this one :crystal_ball: \n. ",
    "schmitch": "there could be an easy solution with https://github.com/skynetservices/skydns, where you could put a postgresql service which will hold the ip of the master.\nThis has some major gains, since you could just use older applications with your HA postgres\n. @pires this is not done easily especially if you make full usage of sql.\nCurrently we have made a router inside our application to do this.\nMostly its really hard to do a custom implementation on a loadbalancer, etc. since you would need to have a fully fledged sql parser that looks at every query and routes them correctly.\nI know there are some solutions to do it, especially when using pgpool-II, but they are really error prone (we did that already), after we found out that it is way easier to scale reads, when tuning your application codebase.\n. @pires awesome yes, but there are several points against it.\nFirst if you want \"true\" scale your master should never be used for reads if you have enough read slaves.\nSecond it is really hard due to the fact that there are some SELECT queries which also writes.\nThird Performance... If you need to parse your SQL queries the whole thing will be WAY slower than just routing them to the correct database. ;)\n. @pires it would be better to give some examples, these could either be done via skydns, where people put their read slaves and master into skydns with a low ttl or as low as governor provides, then on a failover it could decomission the slave or change the master ip.\nthis would be one example.\nthe other one would be about application sql routing and failover directly through etcd.\nhowever which way people choose, the application MUST support it. Connection Pooler needs to autoretry connections and or the application should do this also and should not fail hard.\nBtw.: I did not use your project, but I've seen this project on HN and seen the draft and it is closely the thing what we do. Before we had the setup with pgpool-II which were really bad, we also had something setup with pgbouncer were we could only read / write from the master and every master change we would reload the pgbouncer but that wasn't as good as we thought since it didn't scaled reads. \nHowever on top of that to get the most benefits people should change their applications to support read only queries and a way to failover via etcd, since that brings the most benefits. design the database access as netflix did it with hystrix.\n. @pires i know it feels wrong but as said i would not prefer it to somebody only as a exception if you are planning to change that legacy code.\n\nWhich project do you refer to?\n\nAs said we use our own project which is close to that what you do. The only thing we do not as you do is the initalization.\n. ",
    "miketonks": "Another solution is to have postgres server listening on different ports for master / slave connections.  This way haproxy default behaviour will suffice without any special health-check.\nAs a nice extra, we get a read-only pool from the slaves that can be used for scaling.\n. I added a pull request for this: https://github.com/compose/governor/pull/21\n. I submitted a pull request for this change: https://github.com/compose/governor/pull/20\n. ",
    "s200999900": "Thank for interesting HA solution!\nI tried to implement Postgres HA solution with governor not on localhost and that is not implement, there are many errors, fault's and etc (\nI think bug need to prioritize. \n. ",
    "wkielas": "@tvb, do you have the non-localhost code available somewhere, like a fork or a branch? I'd love to take a look at it ;-)\n. ",
    "titanous": "Check out Joyent's Manatee state machine for an example of how to do an HA Postgres system that is safe. It actually requires a ton of work (and the use of sync replication) to eliminate all of the data-mangling failure cases.\n. ",
    "shashankmjain": "What is the impact if we just catch this exception and retry for few times instead of just letting postgres crash on first attempt?\n. ",
    "nstott": "lgtm\n. ",
    "asmartin": "Thanks for the fast reply!\nFencing: The maximum_lag_on_failover parameter makes sense in terms of preventing the failed node from becoming a candidate for failover, but what prevents a rogue node from coming back at some point and haproxy starting to send traffic to it? The stonith story is a good illustration of why the fencing mechanism needs to ensure that any rogue node is reset to a known clean state (e.g with a reboot via IPMI)\nQuorum: okay, so it sounds like etcd's RAFT ensures that a race condition between multiple nodes cannot end in a tie or multiple concurrent masters?\n. ",
    "ctriegg": "You're right, governor should consider other socket configurations. \nMy thoughts: \nIt's not possible to set a unix_socket_path, just to the path to the socket file. The socket is always named s.PGSQL.nnnn where nnnn is the server's port number, You can just change the directory where to look for the socket. \nSolutions:\n1. We could check if in parameters the option unix_socket_directories got configured and use the first specified directory in this string. Additional we need to check if the listen port of postgresql has reconfigured (cause this affects the name of the socket)\n2. Alternatively we could add the unix_socket_path variable as suggested (but just configuring the path). But then we need something like unix_socket_port if the port is changed. \nI would go with the first solution. \n. Look at the new commit and let me know what you think. \n. Maybe it helps if I explain what problems we're encouter in some of our environments. We had some problems with etcd not responding on a request. We were not able to debug this but installed some monitor scripts to see the availability of etcd. The check runs every 5 seconds to check for an etcd response. After serving one repsonse with return code 500 it continued to respond with 200. But one failed request to etcd leads to a postgresql shutdown which terminates all existing connections. In the next loop it recovers and the old master promotes himself to the master again. \nThis leads to an unexpected downtime of postgres because one request to etcd failed. A second check attempt helps to solve this problem. \nYour example scenario can lead to a split brain, thats right. \nMaybe we could ensure that the absolute retry time for etcd is lower than the etcd TTL. But we have to consider the loop_wait too. \nif config[\"etcd\"][\"timeout\"] * (config[\"etcd\"][\"attempts]\" - 1) >= (config[\"etcd\"][\"ttl\"] - config[\"loop_wait\"])\n  logger.warning(\"Failed to start governor: absolute retry time for etcd requests is higher than the lock TTL\")\n  sys.exit(1)\n(code snippet not tested, just for an example). After thinking about this, the compare to the TTL doesn't fix the split-brain issue either. \nI'll need to have a look when read requests are sent and when the current master is updated and the TTL gets refreshed. \nMaybe I can find a solution this way. . I fixed the problem with a more robust etcd setup. \nI will close this PR. Thank you for your time! . ",
    "ovidiucp": "Great stuff, thanks for packing it! Any updates on the status of this PR?\n. ",
    "odedlaz": "@Winslett sorry for the delay! I just saw your comment :smile: \n. ",
    "doanerock": "I believe you would have a VIP on the primary HA proxy server, if the primary where to fail the VIP would be brought up on the second HA proxy server.  That way the application only has to know about one IP/Hostname.\n. Just because you use a VIP does not make it publicly accessible.  It is all in how you set it up, just like another IP.  While a VIP can be transferred between machines as needed.\n. ",
    "Veeresh-Patel": "@doanerock  Thanks for your reply. Sorry I was little caught up with something so not able to reply. As I am exposing PostgreSQL as a service to Cloud Foundry, I don't want to have access as a VIP. As, I guess, having a VIP will make the database publicly accessible. Isn't it? Is there any other way we can achieve this?\n. ",
    "debackerl": "Thank you for you fast response!\nEven if you STONITH the ex-leader, if you used async replication, you may still have several megabytes in the replication queue when the ex-leader is finally shot. You will have no issue to use the new leader, but when you want to \"recycle\" the ex leader and bring it back online, I have read that either you wipe clean its data directory or use pg_rewind, otherwise there would be inconsistencies.\nI guess you don't need pg_rewind if you use sync replication, because you know the secondary server will never lag behind.\n. ",
    "joeljacobs": "I am having the same issue. I see that it has something to do with how HAproxy interacts with status_handler.py, but haven't yet found a solution. Have you?\n. ",
    "doodles526": "Yes. That's the plan - in the next couple weeks once we consider the branch stable and running it in production we're planning on moving it to it's own repo\n. Hmm. Interesting problem @jberkus . I agree I don't think I want to add an \"execute arbitrary code\" bit to the main codebase. Currently you can poll the /fsm/leader endpoint from the API and check for changes - so that's a possible stop-gap for now.\nExample output from that is \n```\n\ncurl http://localhost:5000/fsm/leader | json_pp                      golang-custom-raft [75c3f42] modified untracked\n{\n   \"data\" : {\n      \"exists\" : true,\n      \"leader\" : {\n         \"connection_string\" : \"postgres://replication:rep-pass@127.0.0.1:5432/postgres\",\n         \"name\" : \"postgresql0\"\n      }\n   }\n}\n```\n\nIn the long-term, we might look to implement something in the API similar to etcd's Watchers https://coreos.com/etcd/docs/latest/api.html#waiting-for-a-change\nBut in general I can't think of a good way to have the core code execute something on an event without bloating it beyond what I'm comfortable with\n. My reasoning is that I want governor to be a tool to provide auto failover - and give endpoints for external processes to query the state. Those processes can use that information to what they wish\nI'm always rather cautious of giving a process the ability to say \"When an event happens, fork and execute this user-provided string\". I know buffer overflows are supposed to be impossible with standard Go code. But... There's been exploits published where you can exploit certain data race conditions, garbage collector, and the internals of a Slice to induce a malicious buffer overflow. So I'm a little hesitant to expose Governor to the possibility of an arbitrary code execution exploit\n. @jberkus Agreed with the argument against polling. I don't like polling either. Since adding the ability to run a script on failover is quite a bit easier than implementing a \"watchable\" API endpoint I'll add the ability to run a script on failover. But it may be deprecated in the future if we add that endpoint. I'm probably too cautious with security for my own good. Sounds good. I'm on an on-call shift this Sunday, so I'll probably shell this out to pass the time during that. Expect a release probably Sunday evening . Cool. That fits with what I was going to do. There's a few reasons a failover could occur (dead man switch, governor recognizes underlying PG is unrecoverable for some reason, loss of quorum). I wasn't planning on revealing this to the script. But just passing the script the current state(follower, leader, no-leader available). Good point. Although can't join cluster could mean 2 things:\n Network issues for the node\n Loss of quorum due to other nodes dropping out on their own(for whatever reason)\nAs I'm looking through the possible state changes there are actually 5 possible states in a failover. Right now there's \n I am the new leader\n I am a new follower - This only happens when the governor process launches.\n Read only following no leader - When a leader step-down occurs but this particular node's WAL log isn't progressed enough to claim the leader position(set in the maximum_lag_on_failover flag). The node is waiting for a new leader to be elected to follow.\n Read only following no leader - When there is no quorum so no leader can exist until the cluster regains quorum(caused by network issues or other nodes dying)\n* PG has become unhealthy - Currently if PG isn't healthy and something with pg_ctl errors when attempting to fix it then Governor gets killed. There are a couple dangerous situations with outcomes like this I need to fix and I'll create issues for that\nThat being said I see 3 failure states I think are worth reporting:\n maximum_lag_on_failover condition not met when trying to gain leadership(this will auto-resolve after another node declares leadership)\n Quorum issues with the cluster\n* Unhealthy PG. Agreed. I found an issue with how it handled the maximum_lag_on_failover flag that I'm working on correcting. After that then I'll get back to working on script execution. @oneness totally fine! We're planning on creating a new repo from the golang-custom-raft branch ourselves. But we're happy to have folks forking this project. If someone can create something better than we have then that's awesome and welcome that! \n@CyberDem0n yes those are some drawbacks. This solution worked quite well for our particular use case at compose. \n A couple things on those compromise though. First, while you need 3 nodes participating in a raft cluster for fail over, not every node must be running governor(see lifeboat in github.com/compose/canoe). How we run the new governor internally, we have 2 nodes running a governor process, and 1 node running lifeboat. Lifeboat is essentially a process which runs as a member of a raft cluster, but doesn't retain state itself beyond the raft log. It acts as a quorum pad. This way we have a 3 node cluster, but not have the overhead of running 3 pg processes. \nAlso, while you don't have a central store for cluster information, you can query any governor node in a cluster for information on the state of the cluster using the Governor API\nFinally, yes. The new governor isn't designed for huge clusters. A thought I have in that though... And shooting from the hip on this totally untested. But what you might be able to do with that is have a 3 node governor cluster. That way you have 3 nodes for a failover scenario. But then you might could query the Governor API to find the leader connection string, then configure a read only pg node as a hot_standby. It would never be able to become the leader, and it wouldn't have a replication slot on the leader to ensure it can keep up. But could be something to test out . It throws a urllib2.URLError It gets caught - and spin-locks with sleep until it can contact etcd again. Then it starts the leader race/decision making again and comes to a stable state\n. urlopen uses the global timeout settings by default. But this way we can use the value from the config to limit it so we aren't hanging on trying to contact what should be a local etcd for forever\n. Should also mention, the node goes into read-only mode on timeout here too\n. ",
    "codepope": "Why not an option to run a script built into the code so that it's just an extension point for integration? (Serious question - just wondering what the rationale is for not doing it)\nDj\n\nOn 18 Nov 2016, at 05:39, Joshua Deare notifications@github.com wrote:\nHmm. Interesting problem @jberkus . I agree I don't think I want to add an \"execute arbitrary code\" bit to the main codebase. Currently you can poll the /fsm/leader endpoint from the API and check for changes - so that's a possible stop-gap for now.\nExample output from that is\n\ncurl http://localhost:5000/fsm/leader | json_pp                      golang-custom-raft [75c3f42] modified untracked\n{\n   \"data\" : {\n      \"exists\" : true,\n      \"leader\" : {\n         \"connection_string\" : \"postgres://replication:rep-pass@127.0.0.1:5432/postgres\",\n         \"name\" : \"postgresql0\"\n      }\n   }\n}\nIn the long-term, we might look to implement something in the API similar to etcd's Watchers https://coreos.com/etcd/docs/latest/api.html#waiting-for-a-change\n\nBut in general I can't think of a good way to have the core code execute something on an event without bloating it beyond what I'm comfortable with\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "CyberDem0n": "There are two major drawbacks of using raft protocol on your own:\n * You have to run at least 3 nodes for auto-failover.\n * Raft protocol does not recommend to have more than 9 nodes (no big clusters for read-only queries).\nAlso such approach brings a lot of other inconveniences, like:\n * Previously it was possible to have a central place, where you can find all your clusters (and nodes)\n * You have to configure every single cluster (and actually every node) uniquely.\nI was thinking about implementing more or less the same thing in Patroni with using one of the python raft implementations, but above thoughts stopped me. Now I'm thinking about implementing support of Kubernetes API in addition to Etcd, ZooKeeper and Consul.. ",
    "bradnicholson": "Governor has been deprecated, we are no longer maintaining it.... "
}
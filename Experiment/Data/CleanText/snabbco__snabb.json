{
    "lukego": "Here are additional things you need to do in order to run snabbswitch and reproduce the problem:\n- Reboot Linux with a kernel parameter to reserve physical memory for Snabb Switch DMA: memmap=16M$0x10000000\n- Unbind your NIC from the OS: echo 0000:00:04.0 > '/sys/bus/pci/devices/0000:00:04.0/subsystem/drivers/e1000e/unbind'\nIn the likely event that the NIC you want to test with does not have PCI address 0000:00:04.0 then you need to substitute the actual address in the command above, and in src/selftest.lua. Use lspci -v to find the right card.\n. Here is an update on how to experiment, written in response to the first interested user!\nThe Snabb Lab: I have two EX6 servers colocated at hetzner.de that each have one spare Intel ethernet port and have a cross-cable connecting them together. The hostnames are arbon.snabb.co and bern.snabb.co. I'm currently developing inside a VM on arbon and occasionally running tcpdump/dstat/ifconfig/etc on bern to check what's being output.\nTo connect to the development VM (running Ubuntu v12 \"cloud image\") on arbon use:\n  ssh -p 54322 $user@arbon.snabb.co\nand to connect to bern use:\n  ssh $user@snabb.co\nwhere $user is an account that I have created for you (just leave a comment here with your ssh public key if you want to have one for trying out the switch).\nGiven today's selftest workload I feel the NIC is only in use about 1% of the time so this lab setup should be able to scale up to a few people. Here's an important tip to avoid multiple people running snabbswitch at the same time (which would be very confusing): instead of \"./snabbswitch\" always run \"flock -x /tmp/snabb.lock ./snabbswitch\". This way only one process will run at a time and any extra ones will automatically block until they have a turn.\nPlease leave the servers as you found them! no wild global configuration changes etc. do whatever you want in your home directory and feel free to sudo and install software you need etc. Mail luke@snabb.co if something crashes or needs rebooting, no stress :)\n. rahul@serverstack.info commented by mail that the Intel e1000 driver in Linux is very well-debugged and high-quality code. definitely a good resource for comparing to understand where the snabb switch bug is.\n. Great!\nI see that changes I made to the selftest procedure (now calling selftest2() instead of selftest() in intel.lua) means this Issue doesn't reproduce out of the box. Sorry about that. I will try to make a fix now so that running the switch reproduces the problem again. Update to follow.\nDoes the code compile and run for you btw?\n. Wow cool that it runs! :-) You are the second person after me to run the switch!\nLooks like I have broken selftest() quite a bit with recent hacking. I will now extend selftest2() to also support receive and then we can try to reproduce the problem with that.\nDoes the code make any sense btw? I am still learning Lua and I think especially the way I'm doing object-oriented programming - lots of \"M.\" prefixes - is a bit clunky and can be better.\n. OK! The updated selftest is checked in now with commit b3867caff51e261769822bb6d55de6a37947884d.\nNow selftest2() is extended to also handle RX and is renamed to selftest() replacing the old one.\nThe problem that shows up now is that the transmit+receive+loopback test drops most of the packets. Do you see this too? I don't know why that is but it's a bug that would be good to fix. Welcome to have a look :). Probably best to create a new Issue.\nThe original problem from this issue doesn't seem to be reproducible now? Could be that it was fixed by changes to the logic that says when descriptor rings are full/empty (I think I fixed stuff there last week), or could be that it still exists and I'm just not seeing it.\nbtw: another interesting but larger thing to hack on in this source file is the add_txbuf_tso() function that is currently just a stub. The goal is to use the TCP segmentation offload hardware features so we would have a test case transmits really big packets (~64K) and then (by loopback) receives the same data back in more smaller packets. This would be a major step towards implementing STT in the future (possibly being the first open source implementation...)\nDinner time over here! :-)\n. Great! Thanks!\nSo I'm a Github newbie and I'm curious to see how it works. Do you think you could send that fix over as a \"Pull request\" so that we can test the workflow?\n. Great, it worked fine! :-D\n. Congratulations you are the first contributor of a patch :-)\n. Yes.\n. Closed because there is no practical demand to support this chip yet.\n. Done: intel10g.lua.\n. Implemented based on Linux sysfs, the /sys/ file systems. See commit 491a08922063884383eb35ea5f0830900f4d6e65 and supporting code in snabb.c.\nNo interrupt support today but that's not currently needed.\n. Simple solution implemented in commit f3834fc3b3ee6bd54bd2fc2df657087add05005b:\nMap a user-specified physical address range into the switch to use as DMA-friendly buffers. This requires memory to be reserved \"somehow\" e.g. with the Linux boot parameter:\nmemmap=16M$0x10000000\nto reserve 16MB of memory in the physical range 0x10000000 - 0x11000000 for use by Snabb Switch\nMajor downside to require booting Linux with special parameters to reserve memory for each switch instance.\n. Duplicate of #11.\n. Added in commit 8530acb443cd4152aa4083fef2c24c87b06987f5.\n. Added basic vhost_net support in virtio.lua.\n. Experiment done by hacking a new networking backend into QEMU/KVM.\nHere is the backend code: https://github.com/SnabbCo/QEMU/compare/master...shm.\nThe Snabb Switch code has been removed from the repository, as is the fate of throw-away prototypes.\n. Howdy!\nCool to be doing FDB hacking.\nHere is a braindump of what I have thought about re: FDB previously:\n1. There's a cool trick that might work for aging old entries. Long\n   version: http://blog.lukego.com/blog/2013/02/04/cute-code/\n2. I would like to choose a max number of FDB entries to support and have\n   well-defined behavior if this is exceeded. For example, 1M entries\n   supported at a time, old entries aged out after 1 minute, overflow handled\n   by flooding.\n3. Cool if Lua dictionaries work as the data structure. Definitely the\n   right way to start. If we later discover that e.g. they slow down the GC\n   when they are large then we can switch to e.g. Judy array.\nCool cool :-)\nI am a little stuck right now on the vnet/virtio stuff - fighting the\nkernel - but expecting a breakthrough any day now :)\nOn 31 January 2013 14:58, rahul-mr notifications@github.com wrote:\n\nI made a simple FDB prototype :\nhttps://github.com/rahul-mr/snabbswitch/blob/fdb/src/fdb.lua\n(Disclaimer: it is not very efficient and may not be what you are looking\nfor. ;-) )\nIt also tries to address issue #13https://github.com/SnabbCo/snabbswitch/issues/13- whenever an address (key) is looked up in the FDB dictionary, it will\nreturn the corresponding port number if the entry's expiry time has not\npassed. Otherwise, it will set the key's value to nil (and hopefully the\nold value (a dictionary) gets GC'd )\nHere is the sample output when running the (modified) selftest :\nFDB test\nDBG: selftest(): starting\nDBG: i=1\n F[00:00:00:00:00:00] = 0\n F[01:01:01:01:01:01] = 1\n F[02:02:02:02:02:02] = 2\nkey = 01:01:01:01:01:01 ; value = { port = 1, expiry = 277447711470485 }\n key = 00:00:00:00:00:00 ; value = { port = 0, expiry = 277445711465500 }\n key = 02:02:02:02:02:02 ; value = { port = 2, expiry = 277449711471322 }\nDBG: sleeping 2 seconds\nDBG: i=2\n F[00:00:00:00:00:00] = nil\n F[01:01:01:01:01:01] = 1\n F[02:02:02:02:02:02] = 2\nkey = 01:01:01:01:01:01 ; value = { port = 1, expiry = 277447711470485 }\n key = 02:02:02:02:02:02 ; value = { port = 2, expiry = 277449711471322 }\nDBG: sleeping 2 seconds\nDBG: i=3\n F[00:00:00:00:00:00] = nil\n F[01:01:01:01:01:01] = nil\n F[02:02:02:02:02:02] = 2\nkey = 02:02:02:02:02:02 ; value = { port = 2, expiry = 277449711471322 }\nDBG: sleeping 2 seconds\nDBG: i=4\n F[00:00:00:00:00:00] = nil\n F[01:01:01:01:01:01] = nil\n F[02:02:02:02:02:02] = nil\nDBG: sleeping 2 seconds\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/issues/12#issuecomment-12943303.\n. Thanks!\n\nOn 6 February 2013 15:41, rahul-mr notifications@github.com wrote:\n\nThanks for the pointers, Luke. I'll look into it.\nGood luck with the virtio stuff! :-)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/issues/12#issuecomment-13184696.\n. Right!\n\nYes Pete it's so simple as that my mental model is \"FDB entries need to\ntime out\" but hadn't gotten to the level of detail of how much time. Thanks\nfor referencing the appropriate spec!\nOn 9 February 2013 08:52, rahul-mr notifications@github.com wrote:\n\nPete, if you were referring to the 2 second aging time in the \"throwaway\"\nprototype, it was meant for testing purposes only - I wanted a quick,\nsimple test (hence maximum entries = 5, timeout = 2 sec) for the data\nstructure proposed by Luke.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/issues/12#issuecomment-13327795..\n. I added the 802.1D Bridge standard PDF to snabb.reddit.com for future\nreference.\n\nOn 9 February 2013 09:10, Luke Gorrie lukego@gmail.com wrote:\n\nRight!\nYes Pete it's so simple as that my mental model is \"FDB entries need to\ntime out\" but hadn't gotten to the level of detail of how much time. Thanks\nfor referencing the appropriate spec!\nOn 9 February 2013 08:52, rahul-mr notifications@github.com wrote:\n\nPete, if you were referring to the 2 second aging time in the \"throwaway\"\nprototype, it was meant for testing purposes only - I wanted a quick,\nsimple test (hence maximum entries = 5, timeout = 2 sec) for the data\nstructure proposed by Luke.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/issues/12#issuecomment-13327795..\n. Fixed by commit f8869df9ca57e491a8788dfa1275c80457f9ee66. Now all mapped memory can be used for DMA.\n. Fixed by 859e977f6e6e603dd094ebd81fb6bbf2a1396b4f.\n. Makefile creates a statically linked file called \"snabbswitch\" that's currently 532KB.\n\n\nThe file is statically linked with LuaJIT, luafilesystem, and all Lua source and C header definitions. See here: https://github.com/SnabbCo/snabbswitch/blob/805b159b9616f7c6604fb0a3f7f49cffaec21909/Makefile\nCurrently does not statically link with Libc. I was convinced that this is not worth the effort. See LuaJIT mailing list thread: http://www.freelists.org/post/luajit/Creating-a-statically-linked-executable-for-a-LuaJITC-program\n. The first throw-away ethernet switch prototype is visible on the tag prototype1.\nHere are some features of interest:\n- switch.lua is the forwarding logic.\n- shm.lua is the Lua side of the new KVM networking backend that I added (also a throw-away prototype).\n- medium.lua is a Lua abstraction for network ports based on either tuntap, snabb_shm, or 'null'.\n- report.lua reads Lua scripts from a ramdisk directory, executes them, and writes out the output. This is a kind of RPC into the switch for diagnostic purposes and potentially the basis of a status reporting mechanism.\n- src/config/ contains config files that are Lua source and define the switch (add ports, feed test traffic, etc). These are a bit like ~/.emacs files.\nBlack box testing based on creating traffic test vectors, running them through the switch and recording the outputs, then checking the outputs for logic errors.\n- maketraces.lua generates PCAP files containing traffic test vectors for input into the switch.\n- replay.lua feeds test vectors (or other traffic dumps) into the switch and records the switch behavior in output PCAP files.\n- checktrace.lua checks recorded PCAP traces from the switch for logic errors: dropped packets, packets looping back to ingress port, multicasts not flooding, etc.\nThis was a fun experiment!\n. Here is a promising design based on discussions with KVM gurus Stefan Hajnoczi and Michael Tsirkin at KVM Summit in Barcelona and freenode #kvm.\nBackground: Today KVM has an efficient ethernet I/O towards the Linux kernel called VhostNet. They start with a TAP device and then map a hardware-style virtio shared memory ring between userspace and the kernel for directly sharing packets. The guest is able to transmit with zero-copy because the kernel can access memory within the guest. See QEMU source file vhost_net.c to see the QEMU side of this.\nForeground: Snabb Switch needs a KVM extension to do exactly the same thing but towards another userspace process instead of the kernel. That is: we should implement a virtio ring in common with KVM, exactly as vhost_net does, except with a more suitable interface than ioctl()ing towards the kernel. Snabb Switch also needs to be able to map memory from within guests and pin this to physical pages for DMA - heavy duty stuff but I have a reasonable expectation that all the necessary OS interfaces to do this already exist.\nThis needs to be implemented in both KVM and Snabb Switch. The code needs to be upstreamed into KVM. Stefan has indicated that it should not be a problem to upstream a good implementation of this feature.\n. First throw-away is done on the prototype1 branch.\nreport.lua that finds Lua source files in a special directory, executes them, and writes the output back to a result file. This is essentially an RPC / remote-eval mechanism based on ramdisk files. (OK, probably sockets would have been much simpler.)\nreport.lua also defines a command to output the forwarding database to a file. I played with the idea of fork()ing before doing this so that reports can consume a bunch of resources in a lower-priority child process while allowing the main switch to continue processing in the realtime-priority parent process. That's an idea that needs more thought and experimentation.\n. Here is a super-simple mechanism to use markdown to format program listings: https://gist.github.com/3945964\nHere's an example of formatting some very early Snabb code as a Leanpub book: http://fresh.homeunix.net/~luke/snabb-preview.pdf\nThis looks promising. Markdown, Pandoc, Leanpub is the right tool chain to use perhaps?\n. Done!\n. Fixed the wiki page, thanks!\nCool to see that you are ready to roll when the NIC arrives :-)\n. Worked great! Congrats on being a close second to send a pull request (first one merged via Github web interface :-))\n. True! I will prepare a bit of test case to show what I have in mind and let\nyou know when it's checked in.\nCurrently no IRC channel. I find asynchronous communication most convenient\nat the moment - operating in batch mode a bit - but don't let that stop you\ncreating one :).\nOn 11 January 2013 18:55, rahul-mr notifications@github.com wrote:\n\n@lukego https://github.com/lukego Regarding TSO: I've had a look at the\nrelevant documentation in the datasheet and looked at what you've done so\nfar in intel.lua . I was wondering if you can elaborate on that test case\nyou've described (64Kpacket and read it in smaller packets). I haven't seen\nany ffi cast of IP packet/TCP segment structs in the device driver code,\nhave you implemented it somewhere else or are you planning to use some\nother lib?\nBTW do we have an IRC channel for snabbswitch?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/issues/33#issuecomment-12155655.\n. The commit above adds a failing test case. I will hack some code to construct a TCP/IP header next and then hand back to you Rahul.\n. Cool I'll leave it for now then.\n\nGithub does take some getting used to. I'm impressed with the email integration.\n. IOMMU hardware is definitely cool. And it seems to work: I believe the KVM\nprocess on arbon.snabb.co is using the IOMMU to make the \"physical\"\naddresses in the VM running snabbswitch make sense to the hardware. This\nseems like by far the neatest solution to the whole memory mapping issue\ni.e. simply share the snabbswitch process's virtual address space with the\nhardware using IOMMU and then we don't need to translate addresses anymore.\nQuestion is how to enable this? I haven't looked in to that. If there is an\neasy way to do it that's compatible with commonly running Linux kernels\nthen it could be a great solution.\nIf you look into this then do write a note about it somewhere!\nOn 12 January 2013 12:13, rahul-mr notifications@github.com wrote:\n\nOn 01/12/2013 04:19 PM, Luke Gorrie wrote:\n\nCool I'll leave it for now then.\nGithub does take some getting used to. I'm impressed with the email\nintegration.\nGithub does a lot of impressive stuff. Apart from this minor quirk, it\nhas worked quite well for me (I guess I'm used to being asked for\nconfirmation before I delete stuff).\n\nSince you are working on memory mapping stuff, have you looked at IOMMU\n? It seems many processors support it nowadays and it looks like a good\nfit for snabbswitch.\nRegards,\nRahul\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/issues/33#issuecomment-12176698.\n. Good direct workaround to the problems with constructing a TCP/IP header in code :)\n. Took this for a spin now! Here is the output from my run:\n\nselftest: TCP Segmentation Offload (TSO)\nwaiting for old traffic to die out ...\nadding tso test buffer...\nDBG: add_txbuf_tso: start\nDBG: add_txbuf_tso: stop\nwaiting for packet transmission...\nsize    mss     txtcp   txeth   txhw\n4       1500    1       1       0\nExpected 1 packet(s) transmitted but measured 0\n. This is exciting!\nsize    mss     txtcp   txeth   txhw\n4096    1500    1       3       3\nLooks like TSO in action i.e. hardware sending multiple physical frames for one large software frame?\n. This is awesome :)\n. This is totally awesome :-) works well for me on bern too!\nMore soon :) just back from a short trip.\nOn 1 February 2013 06:55, rahul-mr notifications@github.com wrote:\n\nUpdate: I've upated the TSO selftests:\nhttps://github.com/rahul-mr/snabbswitch/commits/mem_high_issue33\nLooks like {TCP,UDP)/{IPv4,IPv6} segmentation is working :-)\nHere's the output of the current testcases:\nNIC tx tso test - defaults (TCP, IPv4, size=4, mss=1442)\nselftest: TCP Segmentation Offload (TSO)\nWaiting for linkup............... ok\n[Before]\nStatistics for PCI device 0000:05:00.0:\n[After]\nStatistics for PCI device 0000:05:00.0:\n                   1 GPTC       Good Packets Transmitted Count\n                  64 GOTCL      Good Octets Transmitted Count\n                  64 TOTL       Total Octets Transmitted (Low)\n                   1 TPT        Total Packets Transmitted\n                   1 PTC64      Packets Transmitted [64 Bytes] Count\nsize    mss     txtcp   txeth   txhw\n4       1442    1       1       1\nNIC tx tso test - TCP, IPv4, size=4096, mss=1442\nselftest: TCP Segmentation Offload (TSO)\nWaiting for linkup.............. ok\n[Before]\nStatistics for PCI device 0000:05:00.0:\n[After]\nStatistics for PCI device 0000:05:00.0:\n                   3 GPTC       Good Packets Transmitted Count\n               4,216 GOTCL      Good Octets Transmitted Count\n               4,216 TOTL       Total Octets Transmitted (Low)\n                   3 TPT        Total Packets Transmitted\n                   3 PTC1522    Packets Transmitted [Greater than 1024 Bytes] Count\n                   1 TSCTC      TCP Segmentation Context Transmitted Count\nsize    mss     txtcp   txeth   txhw\n4096    1442    1       3       3\nNIC tx+rx loopback tso test - TCP, IPv4, size=4096, mss=1442\nselftest: TCP Segmentation Offload (TSO)\nWaiting for linkup.............. ok\n[Before]\nStatistics for PCI device 0000:05:00.0:\n[After]\nStatistics for PCI device 0000:05:00.0:\n                   3 PRC1522    Packets Received [1024 to Max Bytes] Count\n                   3 GPRC       Good Packets Received Count\n                   3 GPTC       Good Packets Transmitted Count\n               4,216 GORCL      Good Octets Received Count\n               4,216 GOTCL      Good Octets Transmitted Count\n               4,216 TORL       Total Octets Received (Low)\n               4,216 TOTL       Total Octets Transmitted (Low)\n                   3 TPR        Total Packets Received\n                   3 TPT        Total Packets Transmitted\n                   3 PTC1522    Packets Transmitted [Greater than 1024 Bytes] Count\n                   1 TSCTC      TCP Segmentation Context Transmitted Count\nsize    mss     txtcp   txeth   txhw\n4096    1442    1       3       3\nNIC tx tso test - UDP, IPv4, size=4096, mss=1454\nselftest: TCP Segmentation Offload (TSO)\nWaiting for linkup.............. ok\n[Before]\nStatistics for PCI device 0000:05:00.0:\n[After]\nStatistics for PCI device 0000:05:00.0:\n                   3 GPTC       Good Packets Transmitted Count\n               4,192 GOTCL      Good Octets Transmitted Count\n               4,192 TOTL       Total Octets Transmitted (Low)\n                   3 TPT        Total Packets Transmitted\n                   3 PTC1522    Packets Transmitted [Greater than 1024 Bytes] Count\n                   1 TSCTC      TCP Segmentation Context Transmitted Count\nsize    mss     txtcp   txeth   txhw\n4096    1454    1       3       3\nNIC tx+rx loopback tso test - UDP, IPv4, size=4096, mss=1454\nselftest: TCP Segmentation Offload (TSO)\nWaiting for linkup............ ok\n[Before]\nStatistics for PCI device 0000:05:00.0:\n[After]\nStatistics for PCI device 0000:05:00.0:\n                   3 PRC1522    Packets Received [1024 to Max Bytes] Count\n                   3 GPRC       Good Packets Received Count\n                   3 GPTC       Good Packets Transmitted Count\n               4,192 GORCL      Good Octets Received Count\n               4,192 GOTCL      Good Octets Transmitted Count\n               4,192 TORL       Total Octets Received (Low)\n               4,192 TOTL       Total Octets Transmitted (Low)\n                   3 TPR        Total Packets Received\n                   3 TPT        Total Packets Transmitted\n                   3 PTC1522    Packets Transmitted [Greater than 1024 Bytes] Count\n                   1 TSCTC      TCP Segmentation Context Transmitted Count\nsize    mss     txtcp   txeth   txhw\n4096    1454    1       3       3\nNIC tx tso test - TCP, IPv6, size=4096, mss=1422\nselftest: TCP Segmentation Offload (TSO)\nWaiting for linkup.............. ok\n[Before]\nStatistics for PCI device 0000:05:00.0:\n[After]\nStatistics for PCI device 0000:05:00.0:\n                   3 GPTC       Good Packets Transmitted Count\n               4,256 GOTCL      Good Octets Transmitted Count\n               4,256 TOTL       Total Octets Transmitted (Low)\n                   3 TPT        Total Packets Transmitted\n                   3 PTC1522    Packets Transmitted [Greater than 1024 Bytes] Count\n                   1 TSCTC      TCP Segmentation Context Transmitted Count\nsize    mss     txtcp   txeth   txhw\n4096    1422    1       3       3\nNIC tx+rx loopback tso test - TCP, IPv6, size=4096, mss=1422\nselftest: TCP Segmentation Offload (TSO)\nWaiting for linkup............ ok\n[Before]\nStatistics for PCI device 0000:05:00.0:\n[After]\nStatistics for PCI device 0000:05:00.0:\n                   3 PRC1522    Packets Received [1024 to Max Bytes] Count\n                   3 GPRC       Good Packets Received Count\n                   3 GPTC       Good Packets Transmitted Count\n               4,256 GORCL      Good Octets Received Count\n               4,256 GOTCL      Good Octets Transmitted Count\n               4,256 TORL       Total Octets Received (Low)\n               4,256 TOTL       Total Octets Transmitted (Low)\n                   3 TPR        Total Packets Received\n                   3 TPT        Total Packets Transmitted\n                   3 PTC1522    Packets Transmitted [Greater than 1024 Bytes] Count\n                   1 TSCTC      TCP Segmentation Context Transmitted Count\nsize    mss     txtcp   txeth   txhw\n4096    1422    1       3       3\nNIC tx tso test - UDP, IPv6, size=4096, mss=1434\nselftest: TCP Segmentation Offload (TSO)\nWaiting for linkup............ ok\n[Before]\nStatistics for PCI device 0000:05:00.0:\n[After]\nStatistics for PCI device 0000:05:00.0:\n                   3 GPTC       Good Packets Transmitted Count\n               4,232 GOTCL      Good Octets Transmitted Count\n               4,232 TOTL       Total Octets Transmitted (Low)\n                   3 TPT        Total Packets Transmitted\n                   3 PTC1522    Packets Transmitted [Greater than 1024 Bytes] Count\n                   1 TSCTC      TCP Segmentation Context Transmitted Count\nsize    mss     txtcp   txeth   txhw\n4096    1434    1       3       3\nNIC tx+rx loopback tso test - UDP, IPv6, size=4096, mss=1434\nselftest: TCP Segmentation Offload (TSO)\nWaiting for linkup.............. ok\n[Before]\nStatistics for PCI device 0000:05:00.0:\n[After]\nStatistics for PCI device 0000:05:00.0:\n                   3 PRC1522    Packets Received [1024 to Max Bytes] Count\n                   3 GPRC       Good Packets Received Count\n                   3 GPTC       Good Packets Transmitted Count\n               4,232 GORCL      Good Octets Received Count\n               4,232 GOTCL      Good Octets Transmitted Count\n               4,232 TORL       Total Octets Received (Low)\n               4,232 TOTL       Total Octets Transmitted (Low)\n                   3 TPR        Total Packets Received\n                   3 TPT        Total Packets Transmitted\n                   3 PTC1522    Packets Transmitted [Greater than 1024 Bytes] Count\n                   1 TSCTC      TCP Segmentation Context Transmitted Count\nsize    mss     txtcp   txeth   txhw\n4096    1434    1       3       3\n\u0097\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/issues/33#issuecomment-12982961.\n. Seriously cool :-)\n\nOn 12 February 2013 04:04, rahul-mr notifications@github.com wrote:\n\nUpdate: added support for multiple descriptors and vlan tagging :\nhttps://github.com/rahul-mr/snabbswitch/commits/stt\n(new branch)\nAlso, a boatload of selftests to go with the added features ;-)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/issues/33#issuecomment-13416129.\n. Cool :-)\n\nThe first step is probably to recognize your card by its PCI vendor/device id numbers (pci.lua). Then if you are really lucky it might just work. Otherwise you will need to find out what significant differences there are between your controller and the supported one. The Intel 'igb' driver in Linux is one place to look for this information and the data sheets should be good for reference. You can also use 'ethtool -d' to get a register dump from the card when the OS driver is controlling it to compare values. And of course feedback from github issues is always at the ready :-)\nDriver hacking is interesting. You could be lucky and the card works really quickly. Or you could spend a really long time banging your head against what turns out to be a maddeningly obvious problem. I spent about a month banging my head against making DMA work on the ethernet chip before finally realizing it was turned off at the PCI level :).\n. btw one thing that strikes me about the output you get from snabbswitch is that it's so much \"-\" in the table. I had expected it would recognise you have a NIC called e.g. \"eth0\" even if this isn't usable by the switch. Could be that the code in pci.lua for scanning /sys/bus/pci/devices/* isn't working as expected on your machine. (Have I overestimated how widely supported that sysfs directory is, for example?)\n. Yes, phy_read() and phy_write() should really be using them.\nOn 13 January 2013 09:03, pkazmier notifications@github.com wrote:\n\nQuick question: I noticed phy_lock() and phy_unlock() functions, but they\nare never used. Are they supposed to be?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/issues/34#issuecomment-12190840.\n. Pete you have these user accounts now to access the existing test lab for reference:\n\nssh -p 54322 pkazmier@arbon.snabb.co # development KVM instance\nssh pkazmier@bern.snabb.co # has eth1 cabled to the development instance, can be useful for tcpdump etc\npassword \"snabbswitch\" but please change that (not that these machines are sensitive in any way).\nalso when you run snabbswitch please do it like this:\nflock -x /tmp/snabb.lock ./snabbswitch ...\nto avoid colliding with me and Rahul on the same machine :)\n. Good stuff!\nI created issue #37 for the static linking problem, good catch.\nTime to go snowboarding here.. 9:30am in Grindelwald, Switzerland.\n. Pete, regarding PCIe setup for DMA, the key thing to check is that \"lspci -v\" shows 'bus master' in 'Flags'. That's what the PCIe config setup code is there to ensure.\nIt would sure be handy if ^C would generate a Lua backtrace eh!\n. Howdy Pete!\nI need a distraction from fighting the kernel on memory access and HugeTLBs. Is there something useful I can do to assist on the 82571GB? (do you have a dump_status() that I could stare at with you for example?)\n. Hey awesome work! That code looks great. Cool also that Github makes it so easy to browse branches and their changes!\nThe best for me would be initially a single commit that adds support using low-tech if statements and then we can have a separate ongoing process of trying to find the best way to factor the code. I think there is a lot of room to experiment there e.g. PHY register array could be an object, each register could have its own C type laying out its bits as an anonymous struct, etc. I have not been fully satisfied with any of my experiments in this direction yet but I'm sure we'll find some really nice solutions over time :). (Think what a luxury of high-level tools we have compared with e.g. Thompson and Ritchie!)\n. Fantastic!!\nI merged but haven't actually tried it myself since bern and arbon-VM are\nboth currently stuck in \"DMA not working\" state -- and maybe it's handy to\nhave them so until we figure that out.\nI am right now wrapping my head around virtio/vnet which is a DMA-like\ninterface for packet I/O both into the host Linux kernel and directly into\nKVM guest VMs.\nRahul if you would find it useful to reboot bern/arbon/VM please feel free,\nthey should reboot cleanly (tell me if you need the KVM command line for\narbon - should be in root's shell history and directory ~/vm).\nOn 24 January 2013 05:12, pkazmier notifications@github.com wrote:\n\nSee pull request #48 https://github.com/SnabbCo/snabbswitch/issues/48.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/issues/35#issuecomment-12636416.\n. I am also fine with replacing my dubiously chosen magic numbers with more\njustifiable ones :)\n\nOn 10 February 2013 16:30, pkazmier notifications@github.com wrote:\n\nAgreed.\nOn Feb 10, 2013, at 4:22 AM, rahul-mr notifications@github.com wrote:\n\nOn 02/10/2013 12:52 AM, pkazmier wrote:\n\nRahul, based on your patch, it looks like you solved the MPCs on\nbern/arbon by choosing a value of 2 for PTHRESH, HTHRESH, and WTHRESH. In\naddition, you set the timers RDTR to 10 and RADV to 1. How did you come to\nchoose these values?\n\nTrial and Error FTW ;-) Based on the info from the 82574L DS, I tried a\nset of values that seemed sensible - the DS gives some general\nguidelines for choosing the threshold values (and ofcourse, the range of\nvalues). The values were then tweaked based on feedback from the\ntransmit+receive loopback selftest.\nBTW I guess setting the threshold values to the e1000e driver defaults\nshouldn't be a problem. Given that ATM we only have a tx+rx loopback\nselftest running for a second, I wouldn't expect any drastic change in\nthe results. But in the future, I expect these values to be tweaked\nbased on actual load experienced by the NIC.\nRegards,\nRahul\n\u2014\nReply to this email directly or view it on GitHub..\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/issues/35#issuecomment-13351357..\n. Closed by commit 6842a9b0d1340903e3cf20dfa90c494ceec924dc.\n. So thinking about the workflow I see these steps:\n1. Hack-and-slash on Intel.lua to make it target 82571 instead of 82574.\n2. Once that works, decide what the minimum important differences between the two controllers is.\n3. Create an updated intel.lua that supports both controllers, assuming the differences turn out to be relatively minor.\n\nIn this case perhaps it's easiest to do the first steps on a branch and then merge onto master at the end? That way we don't need to use the short-term code for supporting two independent versions of intel.lua in the same branch from this pull request.\nWhat do you think?\nHow are your first steps with Lua treating you? :-)\n. Here is what I see on bern.snabb.co:\n- This problem is happening on the host.\n- This problem is not happening within a KVM using pcipassthrough.\nI have tried stopping the KVM, restarting the KVM, and running on the host both with and without the KVM running. In these tests it has always worked in the KVM and never on the host.\nCould be that the DMA-enable code in pci.lua works for QEMU's PCI bridge but not physical ones?\n. The issue seems to be that the PCI card is not able to do DMA with the HugeTLB memory that snabbswitch has allocated. I see these messages repeatedly in dmesg:\n[709668.764714] DRHD: handling fault status reg 3\n[709668.764741] DMAR:[DMA Read] Request device [05:00.0] fault addr d8800000 \n[709668.764743] DMAR:[fault reason 06] PTE Read access is not set\nLooks like this memory needs to be enabled for DMA (in the IOMMU?) somehow. I'm looking for a straightforward way to do this from userspace.\n. Booting with intel_iommu=off does not help.\n. Dinner time here! Will investigate more tomorrow. Hope we can find a\nsolution that doesn't need reboot and hacking grub (but that would be good\nenough to start with).\nOn 15 January 2013 18:01, rahul-mr notifications@github.com wrote:\n\nOn 01/15/2013 10:28 PM, Luke Gorrie wrote:\n\nBooting with intel_iommu=off does not help.\n\ndunno if this helps, but can u try booting kernel with\niommu=force,memaper=3 (taken from here:\nhttp://whiteboard.ping.se/Linux/IOMMU)\ncheck this also:\nhttp://www.mjmwired.net/kernel/Documentation/x86/x86_64/boot-options.txt#230\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/issues/39#issuecomment-12277488.\n. The problem seems to be that the HugeTLB allocated memory doesn't reliably work for DMA. The situation is better when using the old mmap() of /dev/mem memory allocation scheme.\n\nI will create a workaround based on ye olde memmap=16M\\$0x10000000 Grub parameter and then look for a way to make the HugeTLB memory work. I cross my fingers that we can find a reliable solution that needs neither a boot parameter nor a helper kernel module.\n. The workaround is done now: the commit above (+ the one after). This works on bern.snabb.co.\nGoing to look more into why HugeTLB pages are not working next. One more theory to explore is that memory.map() is not finding the right physical address for them and so we would pass the wrong addresses to the NIC.\n. I am continuing to look into this. There are plenty of exciting /proc files to check and cross-check on the hunt for where memory really is.\n. Interesting. This fix is working for me on bern.snabb.co. I see 0x00000003\nin the TDBAH/RDBAH registers suggesting that the huge pages have been\nallocated from the high part of the 16GB of ram.\nCan you help me reproduce a failure with that commit applied?\nOn 20 January 2013 13:19, rahul-mr notifications@github.com wrote:\n\nHmm, it seems the latest commit f23d5cbhttps://github.com/SnabbCo/snabbswitch/commit/f23d5cb464ec46cb7de5c423b89c4ec491ec633edoesn't fix this issue. It is interesting to note that during testing with\nthe current master, the 'high' 32 bits of the rx/tx descriptor base\naddresses always remained 0x00000000\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/issues/39#issuecomment-12469851.\n. I wonder if there are some new tools we could add to our toolbox for this\nsituation?\n\nI saw an old LWN article about tracing MMIO operations at the PCI level:\nhttp://lwn.net/Articles/270939/\nCould it be that new Linux tracing infrastructure can give us more\nvisibility into what's going on?\nI once had a similar problem while writing a driver for a USB device and\ndiscovered to my delight that tcpdump could record USB traffic (\nhttp://lukego.livejournal.com/22799.html)\nOn 22 January 2013 10:51, rahul-mr notifications@github.com wrote:\n\nUpdate (on bern):\n1) I have tried using memory.install() to use the memmap-ed address space\nfor the {rx,tx}desc/buffers but still couldn't get the NIC to print\nstatistics.\n2) Tried to bind the pci device (0000:05:00.0) to the e1000e device driver:\necho -n \"0000:05:00.0\" > /sys/bus/pci/drivers/e1000e/bind\ndmesg | tail -n 10\n[503057.724204] e1000e 0000:05:00.0: Disabling ASPM L0s L1\n[503057.724224] e1000e 0000:05:00.0: PCI INT A -> GSI 18 (level, low) -> IRQ 18\n[503057.724249] e1000e 0000:05:00.0: setting latency timer to 64\n[503057.724533] e1000e 0000:05:00.0: irq 56 for MSI/MSI-X\n[503057.724540] e1000e 0000:05:00.0: irq 57 for MSI/MSI-X\n[503057.724546] e1000e 0000:05:00.0: irq 58 for MSI/MSI-X\n[503057.999025] e1000e 0000:05:00.0: eth1: (PCI Express:2.5GT/s:Width x1) 30:85:a9:a3:c5:38\n[503057.999030] e1000e 0000:05:00.0: eth1: Intel(R) PRO/1000 Network Connection\n[503057.999109] e1000e 0000:05:00.0: eth1: MAC: 3, PHY: 8, PBA No: FFFFFF-0FF\n3) Ran the ethtool on eth1:\nroot@bern:~# ethtool -t eth1\nThe test result is FAIL\nThe test extra info:\nRegister test  (offline)         0\nEeprom test    (offline)         0\nInterrupt test (offline)         0\nLoopback test  (offline)         13\nLink test   (on/offline)         0\nroot@bern:~# ethtool -t eth1 online\nThe test result is PASS\nThe test extra info:\nRegister test  (offline)         0\nEeprom test    (offline)         0\nInterrupt test (offline)         0\nLoopback test  (offline)         0\nLink test   (on/offline)         0\n4) Unbind the device:\necho -n \"0000:05:00.0\" > /sys/bus/pci/drivers/e1000e/unbind\n5) Ran selftest again and still couldn't get the NIC to print any\nstatistic.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/issues/39#issuecomment-12537140.\n. I have also written some code (uncommitted) to decode the kpageflags file\nto see information about physical pages (e.g. does the kernel agree that\nthey are part of Huge pages, etc). I put this code down for the moment when\nit seemed like everything was working :-)\n\nOne idea with Arbon: Can we have broken the VM by running something (e.g.\nsnabbswitch or e1000e bind) on the host? I don't think I have done that\nsince the last reboot but I can imagine it may mess up the IOMMU trickery\nthat makes PCIe bypass work.\nMore tomorrow!\nOn 22 January 2013 19:29, rahul-mr notifications@github.com wrote:\n\n@pkazmier https://github.com/pkazmier:\nWeird, I still don't see it on my machine. I ran the selftest 2,997 times\nover night and it's still working. Rahul, are you executing a different\npart of the code than I am? I'm only running the selftest as defined in\nselftest.lua. I'll leave my loop going all day long while I'm off at work\nto see if I can get it to happen.\nI'm working on issue #33https://github.com/SnabbCo/snabbswitch/issues/33ATM (branch I'm working on:\nhttps://github.com/rahul-mr/snabbswitch/commits/mem_high_issue33 ). The\nselftest.lua I'm using includes selftest_tso() which ends up calling\nadd_txbuf_tso(). I'm reviewing the code I've written to see if I ended up\ndoing something stupid. Will you be able to test it in your hardware\n(disclaimer: alpha quality code ;-) ) to see if that triggers this issue?\nBut IIRC the issue got triggered y'day in arbon while running the master\nbranch.\nMy concern is that whatever triggered the DMA access problem, we should\nstill be able to get the NIC to work without a reboot.\n@lukego https://github.com/lukego:\nI wonder if there are some new tools we could add to our toolbox for this\nsituation?\nThough it isn't much, I've added the definitions of the rx/tx diagnostics\nregisters to help with the debugging.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/issues/39#issuecomment-12558650.\n. arbon-VM seems in the DMA-stuck state. Do you know a \"cure\" for this that doesn't require a reboot? I tried a replacement reset() above and your @rahul-mr/issue33 branch but can't get the counters moving.\n. I did a reboot on arbon.\n\nHere's the procedure I use after reboot to get the devel VM back up, in\ncase you want to do it yourself any time:\necho 0000:05:00.0 > /sys/bus/pci/devices/0000\\:05\\:00.0/driver/unbind\nscreen -S kvm # create a stable screen session\n/root/kvm.sh\nOn 27 January 2013 12:44, rahul-mr notifications@github.com wrote:\n\nI'm thinking of trying to directly write to the PCIe Power Management\nControl/Status Register (PMCSR) [page 266, 82574L DS] to do a D0 -> D3 ->\nD0 power state transition and see if that helps.\nTried it - doesn't seem to fix the problem. arbon needs a reboot.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/issues/39#issuecomment-12753040.\n. I believe this was an issue with truncating 64-bit pointers to 32-bits so that we would give mangled pointers to the NIC.\n. Good point. Yes it is intended as bytes. This is possibly confusing!\n\nOn 16 January 2013 18:22, pkazmier notifications@github.com wrote:\n\nWhat is that third argument to protected supposed to be? When I read the\ncode, I took it to imply the total number of bytes for the memory you\nwanted to protect. In your example, I guess I would have thought that the\nthird argument in your example should have been a 2 like this:\nlocal total_len = protected(\"uint16_t\", context, 14+2, 2)\ntotal_len[0]\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/issues/40#issuecomment-12329534.\n. The protected() function is actually (re)written by Mike Pall the author of LuaJIT. Here is the historical context: http://permalink.gmane.org/gmane.comp.lang.lua.luajit/1171\n. So on consideration I think your fix is good. Makes sense for the bound to be in the same units as the index.\n\nCan you make a pull request that also updates intel.init_dma_memory() to use the new interface? That function should really be allocating descriptors in terms of num_descriptors * sizeof(a descriptor) rather than raw bytes. (Perhaps also avoid merge commits in the history if they are not needed?)\nThe reason for the index being in bytes is really historical. To start with I had assigned fixed memory addresses like 0x1010000 to each DMA structure (read descriptors, write descriptors, etc) and so it seemed natural to think in bytes.\n. Fixed by merge of Rahul's pull request #42.\n. This also uncomments the write RXDCTL by including commit 1286653cc5b63dd9d727271e3199d945e9fd8d07. Intended?\n. Fun :-). I'm not such an expert with Git so I am clicking and playing around a bit to try and understand how the master history looks after taking in such a pull request.\n. OK I'm working on understanding the git/github workflow.\nThe pull request contains:\n1 fix that is already merged.\n6 merge commits\n1 fix that is not yet merged.\nIf I git merge this branch onto my master I see this new history:\n```\ncommit 86fbae001bfcdcce8a5effc15dd668e1ab4dcdf7\nAuthor: Rahul rahul@serverstack.info\nDate:   Wed Jan 16 16:59:18 2013 +0000\nFix for issue#40: change how bound is calculated\n\ncommit a1ceae1842f24764a2c625fdeeaf829d10fceb71\nMerge: f1065ec 6417d5c\nAuthor: Rahul rahul@serverstack.info\nDate:   Wed Jan 16 16:52:22 2013 +0000\nMerge branch 'master' of https://github.com/SnabbCo/snabbswitch\n\n```\nIs this as it should be or does it make sense to eliminate the merge commit somehow? (would you do that or would I?)\n. Awesome! You guys are right and I misread the data sheet here.\n82574 DS Section 4.6.1 (Interrupts During Initialization) recommends disabling interrupts both before and after initialization so I will also update the other IMC write at the end of init().\n. (Oh I see you already fixed both writes to IMC. Getting the hang of github, I confused the line being commented on for the whole patch. Cool!)\n. Great! Yes this was annoying me too.\n. Thanks Pete! Looks great and no missed packets (output below).\nI really appreciate this work of checking the data sheet thoroughly and questioning suspicious magic values. It's really valuable and necessary to avoid weird problems. I think the inline comments are good too. Great stuff!\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:05:00.0:\n           1,247,545 GPTC       Good Packets Transmitted Count\n          79,843,712 GOTCL      Good Octets Transmitted Count\n          79,847,040 TOTL       Total Octets Transmitted (Low)\n           1,247,621 TPT        Total Packets Transmitted\n           1,247,626 PTC64      Packets Transmitted [64 Bytes] Count\nNIC transmit+receive loopback test\nintel selftest: pciaddr=0000:05:00.0 secs=1 receive=true loopback=true\nWaiting for linkup.............. ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:05:00.0:\n           1,153,626 PRC64      Packets Received [64 Bytes] Count\n           1,153,649 GPRC       Good Packets Received Count\n           1,153,658 GPTC       Good Packets Transmitted Count\n          73,834,368 GORCL      Good Octets Received Count\n          73,834,752 GOTCL      Good Octets Transmitted Count\n          73,843,520 TORL       Total Octets Received (Low)\n          73,843,904 TOTL       Total Octets Transmitted (Low)\n           1,153,818 TPR        Total Packets Received\n           1,153,823 TPT        Total Packets Transmitted\n           1,153,825 PTC64      Packets Transmitted [64 Bytes] Count\n. Thanks Simon! You are now the fourth person to hack code into Snabb Switch :-)\n. Interesting!\nWhere do you see the kernel API? I do see linking in a 3rd party event loop as a strong negative.\nProof of concept integration with QEMU/KVM I already did around August last year. Details in Issue #10. This worked fine but had a couple of fundamental performance problems:\n- Packets are copied with memcpy between snabbswitch and QEMU.\n- snabbswitch talks to the QEMU hypervisor code rather than the guest directly. That means the KVM kernel module has to return back to userspace on every I/O, like a reverse system call.\nNow what I want is a real solid production implementation to move forward with.\nI sat down to hack this a few times, but never made it happen. Partly I just don't know how to write it in such a way that the QEMU upstream will want to take it. Which system calls to use, how to refactor the QEMU sources.\nI traveled to the KVM Summit and hung out in freenode #kvmri. The people are friendly and helpful, but nobody wanted to write this code and contribute it for fun.\nNow more recently I decided \"enough is enough\" and found an impressive freelance hacker with vhost_net experience to do the KVM development. I set aside for a budget to pay for this in my little company. But before starting off I suddenly had this idea that maybe we should solve the issue in Snabb Switch using FUSE instead of getting snabb switch support hacked into QEMU.\nLong story! I would love to sort this out.\n. Beautiful!\nJFYI here is roughly what I needed to apt-get on a fresh ubuntu (chur) to 'make book':\napt-get install pandoc texlive-xetex texlive-luatex texlive-latex-recommended texlive-latex-extra texlive-fonts-extra\n. Beautiful! And a great use of page space.\nFootnotes - How do we write them in the source code? How will we render them in other formats (HTML, Kindle)?\nHere is some braindump on the priorities in my mind for readable code:\n1. Easy and natural source code in Emacs. Easy to read (minimal markup) and easy to write (minimal worrying about how it will typeset).\n2. Readable on various mediums: Emacs, PDF viewer, web browser, Kindle, paper.\n3. Beautiful in one or more of the mediums.\nI see a risk that if we make the typesetting \"just so\" it will be a barrier to contribution because people will worry about making the typesetting ugly. I think that's why Knuth's stuff never caught on.\n. I do think the footnotes will work out very well btw. I do relate to the use case of \"I want to write a short comment to clarify what this function is doing\". Should be easy enough to make a style guide with such tips.\n. This typesetting is quite inspiring :-)\nI am running around restructuring the code now. Instead of a chapter structure like \"Startup, Linux support, Device support, ...\" I would prefer \"Memory, PCI, Hardware Ethernet, Software Ethernet\", etc. So mostly I plan to split up snabb.c into separate components independent components that can be digested separately.\nSound sane?\n. I created a branch called bookediting and tried to improve the overall readability of a coupe of chapters.\nHowever I get a pandoc/LaTeX error when trying to make the book:\n```\n$ make book\npandoc --template=doc/template.latex --latex-engine=lualatex -V fontsize=10pt -V monofont=droidsansmono -V monoscale=.70 -V verbatimspacing=.85 -V mainfont=droidserif -V sansfont=droidsans -V documentclass:book -V geometry:top=1.0in -V geometry:bottom=0.75in -S --toc --chapters  -o doc/snabbswitch.pdf doc/snabbswitch.md\npandoc: Error producing PDF from TeX source.\n! Undefined control sequence.\n\\enit@endenumerate ->\\enit@after \n                                 \\endlist \\ifx \\enit@series \\relax \\else \\if...\nl.481 \\end{enumerate}\nmake: *** [doc/snabbswitch.pdf] Error 43\n```\nAny idea how to debug this? :-)\n. Thanks! I did not realize you could close() file descriptors and keep using the mmap()s created with them.\n. Is this needed?\nFor me everything builds on ubuntu without -lpthread, and I don't think we are using pthreads anywhere in the code.\n. Ah! Okay merged now.\nI looked briefly at Musl but it seems like the practical gain:pain ratio didn't look so good at the time. could be worth revisiting when we actually want to start distributing binaries in the future.\n. Cool! Would you mind \"squashing\" the history a bit? I would be happy to merge this.\nI really like this style of object-oriented programming: creating a fresh metatable for each kind of object with exactly the appropriate entries. I had previously tried factoring RO/RW/RC as different kinds of objects but I used inheritance-style (RW extends RW, etc) based on PiL examples. That felt like opening a can of worms and soon needing to worry about Design Patterns so I switched to if-conditions. Thanks for the Lua style tip!\nRe: optimization I reckon we need to get a reproducible performance bottleneck (e.g. Issue #56) and then do profiler-driven optimizations. I tried to make register access more efficient in intel.lua (which is pre register.lua) but this seems to have been unnecessary because the hardware part of register access (going over the PCIe bus) is so slow that I don't think we can ever read registers in a tight loop anyway. Caching often-used registers e.g. RDH seemed to have a huge impact on performance. If we typically cache register access for (say) 1 millisecond then a few cycles extra probably won't be a measurable cost.\nIdeally I would like to write the code \"as inefficiently as possible\" (\"Joe Armstrong style\") to begin with, get good at profiling to identify exactly which code impacts system performance, and do targeted optimizations on those.\nI have been doing some premature optimization e.g. using FFI structs instead of Lua objects in buffer.lua mostly out of fear that the code will drift too far away from C and be hard to bring back if/when we need to. I do want to make overall system performance as good as if we did everything in C. I do think this is premature though, the code base is small enough that it will be no biggie to do major rewrites if needed.\n\n. btw I am a bit of a Git newbie. Feel free to educate me with links about merging patch sets and suchlike. I reviewed Linus's tips today.\n. True. FFI metatypes are lovely and I very much like the way you use them in the ljsyscall code. There is also much to be said for keeping Snabb Switch data structures on the \"hardware\" rather than \"Lua\" level of abstraction e.g. being explicit about memory layouts.\n. (How come Github doesn't close this pull request automatically? Seems like I have pushed the commit to master.)\n. (merged manually.)\n. Good question! You are right that this is an important issue. Here's how we handle it.\nEach time the device driver code needs to have memory access synchronized it executes a memory barrier operation. For example, intel10g.sync_transmit:145 calls C.full_memory_barrier after storing packets and descriptors in memory but before telling hardware to process it by writing to the TDT register. The full_memory_barrier() function is a one-liner at the bottom of lib.c that uses the GCC builtin __sync_synchronize() function.\nI admit I don't know exactly how __sync_synchronize() behaves on x86. I would be curious to know. I believe this usage is safe, though.\nHaving said all of that, there is a separate but related issue that I do think is a current bug, but testing hasn't turned up yet. Julian Stecklina warned me privately that we should prevent the Intel NIC from writing to memory in a very relaxed/random/bad way by disabling some bits in the CTRL_EXT, DCA_[RT]XCTRL registers. Meaning to look into that ...\n. The official Intel CPU manuals are here btw: http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html. Can also buy hard copies for cheap from lulu.com.\n. You guys have got me curious now too :-). Here's what I see when I disassemble the snabbswitch full_memory_barrier function on chur (dev server):\n$ objdump -d obj/core/lib_c.o\n...\n0000000000000057 <full_memory_barrier>:\n  57:   55                      push   %rbp\n  58:   48 89 e5                mov    %rsp,%rbp\n  5b:   0f ae f0                mfence \n  5e:   5d                      pop    %rbp\n  5f:   c3                      retq\n. btw the current design doesn't execute a fence on every packet. instead we internally process about 100-1000 packets (transmit/receive functions) and then synchronize (sync_transmit/sync_receive functions) to let hardware process them. So the memory barrier is not really in the \"inner loop\" and instead executes every (say) 10+ microseconds (inner loop typically runs in around 100ns on the simple test programs we have now).\n. Howdy!\nThis came with 11 individual commits - did you mean to push a different branch?\n. On 27 November 2013 18:40, Javier notifications@github.com wrote:\n\nhum.... the branch is right; maybe i should do the merge to trunk and send\nthat as the pull request?\nIf you have done a rebase you might need to push the branch to github with\n\"--force\" (because you are overwriting history) ?\n\n(is the branch right on you local copy or also on github?)\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/pull/78#issuecomment-29404292\n.\n. Oh! just assumed it was a mistake, my bad, will check out the 11 now :-))\n\nOn 27 November 2013 18:48, Javier notifications@github.com wrote:\n\nyes, it's the same on github as on local. i condensed a little over 20\ncommits to those 11 ones that i feel are more readable.\nmaybe another round with rebase -i to distill a little more...\ndo you want it just tighter, or only the merge?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/pull/78#issuecomment-29404938\n.\n. On 27 November 2013 18:50, Luke Gorrie lukego@gmail.com wrote:\nOh! just assumed it was a mistake, my bad, will check out the 11 now :-))\n\ns/now/in the morning/ ... time flies when having fun :)\n. Howdy!\nI merged this now.\nJavier, could you take a look at make test? I suspect that some failing cases only lack some simple vfio initialization somewhere:\nERROR     testlog/core.memory\nERROR     testlog/apps.intel.intel_app\nERROR     testlog/apps.ipv6.ipv6\nERROR     testlog/apps.vhost.vhost\nERROR     testlog/apps.vhost.vhost_apps\n(I have not dug in but those seemed to work before merging the vfio patch)\n. Hey I started fixing one test and ... before I knew it they are all running :)\nI did a couple of funny things:\n1. Sprinkle require(\"lib.hardware.bus\") in funny places to avoid crashes when allocating memory.\n2. Hard-coded intel_apps to use vfio (and it's already hard-coded to use a specific PCI device on chur).\nIf you see a better way then please fire off a pull request, for now I am really happy to see all tests passing :)\nPush coming...\n. Howdy Jamie!\nCool that you got it to compile :-)\nI suspect there will be some more problems than this ... it maybe doesn't help that within the Lua code we are also using C datatypes and likely depending on 64-bit.\nIf you would like an account on chur.snabb.co (dev server) to run the test suite on a 32bit-compile just post an ssh pub key :)\n. (I am inclined to let the codebase and testing infrastructure mature some more before merging in portability code but I like the idea of having a Gitub branch showing how it's done for when the time comes.)\n. Closing now: Could be reopened as a pull request.\n. Hi Jeff,\nYou should be good to go with \"ssh jeffl@chur.snabb.co\" now!\nsudo should work too.\nLet me know if you have any trouble logging in with that key.\n. I pushed a workaround to master (1349d98152101a458944c9fc0f4db927aa9c7da9). Let's think about whether that's the right solution between now and the April 1 release.\n. Thanks for the explanation. Later we need to improve the error message to avoid confusion.\nI see in dmesg errors like this:\n[6751892.413163] dmar: DRHD: handling fault status reg 502\n[6751892.422642] dmar: DMAR:[DMA Read] Request device [01:00.0] fault addr 0 \n[6751892.422642] DMAR:[fault reason 02] Present bit in context entry is clear\n[6751902.067847] dmar: DRHD: handling fault status reg 602\n[6751902.077539] dmar: DMAR:[DMA Read] Request device [01:00.0] fault addr 0 \n[6751902.077539] DMAR:[fault reason 02] Present bit in context entry is clear\n[6752047.896885] dmar: DRHD: handling fault status reg 702\n[6752047.906410] dmar: DMAR:[DMA Read] Request device [01:00.0] fault addr 0 \n[6752047.906410] DMAR:[fault reason 02] Present bit in context entry is clear\nwhich looks like the IOMMU blocking access to address 0 i.e. CTRL device control register. any ideas?\n. (My bad. I unbound from Linux but didn't setup_vfio(). will try some more..)\n. Works for me!\n. The coding standard encourages experimentation and creative freedom when creating new apps, so adding a new OO framework is totally in the spirit of this.\nThis is the best pull request yet: adding useful functionality, beautifully documented, carefully structured into logical commits. I look forward to reading in more detail and helping to get it running fast :-)\n. I'd like to start by understanding the problem that Alex is solving, how he has structured his solution, and how we can help make his project a success. Then we can collectively also work on improving the \"eventual consistency\" of the code base.\n. It's still early in the project's life. Let's make a mess that's worth cleaning up :)\n. @agladysh I hope we will rewrite every line at least once in the next couple of years. \"The best writing is rewriting\" :-)\n. (wow my Macbook is amazingly determined to auto-correct the words encap->encamp and decap->recap! probably time to turn that feature off...)\n. Hi again!\nI'd like to get this code merged in so that it can be maintained together with the rest of the repository (it's an undue extra maintenance burden on you when it's out-of-tree and the other code is changing).\nCould you possibly pull the class/datagram/etc code into the app/ directory? I'd like to make the core/ and lib/ trees small and conservative for now and keep higher-level frameworks inside the apps. This will mean more code in app/ directories in the short term but it means app authors can experiment with any programming style they want without standing on each others' toes.\n. On reflection I agree that the structure you have now with code in lib/ is better than having everything in app/. I would like to merge this code soon. Does this pull request contain the latest?\n. Could you perhaps move class.lua somewhere suitable (in lib/ ?) and send a new pull request?\n. The easy fix is to pass a string and then parse it as Lua. That's what the ipv6 app does.\nThe benefit of moving to strings is that it will be possible to load new app networks into a running system with minimal disruption. (The app object won't be recreated if its configuration string is the same.)\n. Yes, that sounds reasonable.\n(I'm also not really sure if the current core/lib/app/design directory layout really makes sense, would be interesting to think about better alternatives.)\n. I'm ready to merge if you can:\n- Tidy up the commit history (git rebase -i).\n- Get \"make test\" passing on chur.\n- Leave out the intel10g MTU change from this pull request (save it for after we get in Javier's new code).\n. Howdy!\nTo clarify what I meant when requesting to git rebase -i: I'd like the commit list to please reflect the final result and not the intermediate steps. So commits that all update the same bit of code - e.g. first draft, debug, optimize, etc - can be squashed into a single commit for merging.\noptional bonus points to split logically separate features into separate commits: socket I/O, vhost changes, etc but if that's too much work then just squash everything into one commit (minus the intel driver changes please because that's going to conflict with Javier's big driver changes that are also coming in now).\n. Here's a possibly handy link: https://help.github.com/articles/interactive-rebase\n. Thanks for making the effort!\nBy the way: This is the document that made Git 'click' for me, after years of confusion and annoyance: http://ftp.newartisans.com/pub/git.from.bottom.up.pdf\n. Your key is installed on chur now. You should be able to ssh gall@chur.snabb.co and have sudo access.\n. Change merged manually into authorized_keys.\n. > After crash we may only restart SnS processe.\n\nMay be it is the simplest solution for changed configuration also?\n\nI agree this is a great design, but there are some reasons I don't want to adopt it now in SnS:\n1. I want to support the rate of one config change per second without disrupting operation.\n2. Stateful apps (e.g. NAT, VRRP, ...) may be hard to write if they can be restarted continuously.\n. Ubuntu 13.10. That's what the chur.snabb.co shared development server is running.\nSupporting other Linux distro's should require:\n1. Relatively simple porting e.g. importing a couple of header files.\n2. Setting up a way to detect breakages in new changes e.g. via continuous integration system.\n. You should now be able to ssh krisk84@chur.snabb.co. Let me know if you have trouble!\n. Howdy!\nYep: to decapsulate you can remove the preamble, probably by hacking iovec.offset. To encapsulate you can add the preamble, probably by inserting a new iovec at index 0.\nI'll see what more information I can get about integration testing possibilities, but that might have to come a little later. I'd say that a good testing roadmap is something like:\n1. Testing using Wireshark and PcapReader/PcapWriter i.e. read packets from a file, encap/decap them, write results to another file. Load the traces in Wireshark and see if the contents look right. (I'm not sure if Wireshark understands this protocol yet -- may have to read it as hex.)\n2. Testing using the app with itself and PcapReader/PcapWriter i.e. same test but do both encapsulation and decapsulation and make sure your output looks the same as your input.\n3. Testing with an external system. I'll need to identify a good one. My first guess is Linux L2TPv3 but I'm not sure if that's implementing the same internet draft as we're talking about (the author of that blog post is involved in the same project as us though...).\nI'd say only do the validation that's required by the standard.\nI believe that Alex Gall managed to factor out the IPv6 control messages (neighbor discovery / etc) as a separate app. You might want to ignore that aspect to begin with since we can probably reuse his code.\nDoes that help? (next questions?)\n. Hi Alex,\nSetting to all ones sounds like the right interpretation to me to.\nI'm not working on getting a router config dump + an interoperability testing setup. I expect this will come some time after we merge the first version of this code though so we can revise the code as needed when that is available.\n. Hi Nathan,\nNo. Currently we are compatible with Icehouse and require a small number of patches to Nova and Neutron. OpenStack is gradually taking in NFV related features and we are being patient and maintaining our code downstream.\nThe holdup is that NFV functionality in general and vhost-user in particular is taking time to be accepted by OpenStack even though it is already in QEMU and Libvirt. We submitted patches to Juno but those slipped. Intel have kindly taken over and resubmitted this to Kilo and hopefully that will go in.\nThe good news is that we have reduced our total OpenStack changes down to about 200 lines of simple code that we can maintain downstream for as long as is needed.\nHere are our latest Git trees for nova/neutron/qemu: https://github.com/SnabbCo/snabb-nfv\nAnd here is the upstreaming effort now being lead by Intel: http://specs.openstack.org/openstack/nova-specs/specs/kilo/approved/libvirt_vif_vhostuser.html\n. The first step is to find the appropriate data sheet for the NIC. This is probably a thousand-page PDF from Intel. Then either write a driver from scratch or adapt the existing intel.lua driver. (Alas, intel.lua has some bit rot at the moment and needs to be revived.)\nQuestion: Can VMware Fusion provide a VMnet device instead of the Intel? That is potentially more interesting: there could be some juicy features supported by the hypervisor underneath. (I think that's the VMware equivalent of Virtio-net i.e. a paravirtualized device.)\nbtw if this is too much pain you can use the vhost app to do I/O via the Linux kernel instead of a built-in device driver.\n. So what kind of \"Hello, world!\" do you prefer, to get Ethernet packets in/out in a boring way, or to poke a virtual hardware device's registers without making it do much?\nFor Ethernet in and out there are three options:\n1. Tap device (see tuntap). That should work \"out of the box\" with the Vhost app. It's a fancy implementation that's accelerated by vhost architecture.\n2. Raw socket. That exists on Alex Gall's vpn branch.\nOne bit of development that needs to be done is to extend the Vhost app to let you choose between the tap device (creating a new virtual interface in Linux) or a raw socket (attaching to an existing interface). Both can use the vhost architecture acceleration.\nIs that useful? (If not especially then please give more hints about what would be interesting for you :-))\nIf you want to try writing a driver for VMware vmxnet (or writing the first little bit of such a driver) then I can say a few words about that too.\n. Regarding the crash: This seems to be a bug related to https://github.com/SnabbCo/snabbswitch/issues/84. Sorry, I lack the brain cycles to fix it today.\n. The test you quoted creates an \"app network\" with this topology:\nSource -> TapVhost -> Sink\nThe arrows show the flow of packets. Source is generating traffic in a loop and making it available to TapVhost. (If you run ifconfig snabb0 up and tcpdump -ni snabb0 you should see this garbage traffic arriving on Linux.) If TapVhost receives packets from Linux then it will forward them on to Sink, which will discard them. If you want to send packets from Linux into TapVhost you could do something like:\nifconfig snabb0 192.168.100.1/24 up\nping -b 192.168.100.255\nthen when app.report() runs it will tell you how many packets were transferred over each link in this network i.e. how many packets sent from Source to TapVhost and how many from TapVhost to Sink.\nYou might want to replace Sink with a PcapWriter in order to save received traffic to a file. You can see an example of how that's used in the ipv6 or vhost_user apps.\nThat help?\n. You might also want to make the loop run forever and simply run app.report() on a timer every 1 second or so. There are examples of that in other selftest() methods in the system.\n. Rumour has it I introduced some terrible breakage with commit ef376fa75c3a58e9b4d85619b377918de1f77523. Maybe try backing that out? I will try to fix and clean up my mess on Monday.\n. This looks more promising. Before your transmit queue was stuck at 40K packets. I think that was 8K packets queued on the struct link_ring between snabb apps plus 32K packets that TapVhost was trying to deliver to the kernel without success/progress.\nNow you've had about a million packets transmitted from Source to TapVhost and likely delivered to the Linux kernel. If you would do if snabb0 up && tcpdump -ni snabb0 during the test then you would probably see a bunch of garbage packets arriving to Linux (coming from Source).\nTo see packets on the tapvhost.tx->sink.in link you need TapVhost to have something to transmit. This will be whatever it receives from Linux on the snabb0 tap interface. One way to generate something would be ifconfig snabb0 192.168.100.1/24 up && ping -i 0.1 -b 192.168.100.255 which should cause Linux to transmit an ICMP packet into the snabb0 interface every 100ms. Those packets should egress from tapvhost.tx within Snabb Switch and then be thrown away at Sink.\nFor reference, the fundamental operation of TapVhost is based on standard Linux tuntap tap device semantics.\nIt would be neat if Linux supported an ioctl to put a tap device into loopback mode, so that snabbswitch could blast in packets and receive them back again, but I don't think they have that feature.\n. To be consistent with normal OpenStack operation I believe we need to lock the VMs to only use the SMAC that has been assigned to them. I don't think we need SMAC/DMAC filtering beyond that.\n. Looking good :-) I made some requests as line comments.\n. Thanks for pointing that out. I added this issue to the Milestone for the April 1st release.\n(Maybe we will have instructions for running non-root by then instead - we'll see...)\n. I think we should have as few files as we possibly can in the package for snabbswitch. The snabbswitch binary, a manpage, maybe an example config... can we get away with so little?\n. (OK, maybe we'll want an init.d script, but we don't have that yet...)\n. That sounds good :)\n. JFYI Here is a mailing list thread with somebody else trying to build a patched version of Neutron for an Ubuntu PPA: http://www.mail-archive.com/openstack-dev@lists.openstack.org/msg20902.html\n. (Obsoleted by #131)\n. The requirements seem to be well captured. Thanks!\nQuestions:\n1. For the first iteration, can we use Travis-CI as-is?\n2. To add more machines to the build, could we have the Travis-CI script first check permissions (great idea) and then remotely start tests on all appropriate hosts? (So CI only runs on Travis-CI and the other machines running tests stay \"dumb\".)\nI'd like to have a solution on that level of simplicity, if this can work out in practice.\n. Are you running 32-bit (i386)? Realistically I'd say that we only support x86_64 at the moment.\n. Hi Jay. You should be able to ssh root@chur.snabb.co with that key and then useradd an account for yourself. Have fun.\n. Strange. How about jfenton@chur.snabb.co ?\ngrep jfenton ~jfenton/.ssh/authorized_keys\nssh-rsa\nAAAAB3NzaC1yc2EAAAABIwAAAQEAvqo1Xk7r6rb/B2ghTvKTcJHRcnsy7+mYosIjQ+mwOFqdaM+CysAxyIAKlOHtliR5fnAsrttYATDcrzauhzXW2tinBohhiUjLW1QdvO2mt2IqHrz/wYTzaJ0YqKJ13ngqj8OTbkV0Q4etCQqkF58BuVant8NC0owYuVSnRwJ4PWHTxTVXDfJaVO5BUQaImpfF/tLQcGN6pKyog57Hh8RYVUae9pcxKkhheoIoQi7dTyh11lwWrwfsqIETy3j0Mew3v27/xYREzpVSawZVDtuF9/mDPc+anY32ODZ0WlgFeWuGMxopazsTmOlKmbDv5g0R7E3ZWz8xdiMJJg5yyR+uiQ==\njfenton@lap0.na.nu\nOn 23 March 2014 14:14, Jay Fenton notifications@github.com wrote:\n\nHey Luke, still asks for password. Can you double-check it's updated your\nside?\n\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/pull/126#issuecomment-38382053\n.\n. Awesome!\n. Great hacking!! I'll give more specific feedback on the line comments.\n. Thanks for diving in and attacking the problem. I would say that the most important things to conditionalize are the PCI address for the 10G NIC (because most machines have no such PCI card and others will have it in different slots) and the filename for the vhost_user socket file. Those parameters should be mandatory and the tests should be skipped if they are not supplied (with a message saying why).\n\nI don't think we need to conditionalize the proc/sys files because (afaik) we can reasonably expect them to have stable paths between servers (?). It could be nice to fail with a good error message if we don't find one of those files though.\n. (Sorry about the slow feedback. I'll try to make the time to understand the packaging issues in more depth this week.)\n. Thank you for all of this hard work on packaging.\nOne very useful discovery from this process for me is to understand that Ubuntu packages are a really heavyweight way to distribute patches to OpenStack. This is a bigger can of worms than I had anticipated and it has potential to become much more difficult e.g. if the end-user wants patches from other people too and we are all trying to distribute our own ubuntu packages.\nI would like to suspend this work on ubuntu packages for the moment and resume it later when the software is more mature. In the meantime I think we're better off using devstack instead of PPAs and being proactive in getting our changes upstream into standard software packages.\nCan you please keep the various Git repos up for when it's time to revisit this topic?\n. Super duper ultra cool! Works for me too. Code looks nice and well thought out.\nI would like the diff to be much smaller though. I don't want to add 1400 lines of code because that would consume 14% of the lifetime code budget of 10KLOC. So let's please trim it down a bit before merging.\nI don't think it makes sense to have two separate drivers for 82599. Can you merge these features into intel10g.lua instead of creating a new driver? (And preserve the existing code / coding style where reasonably possible?)\nCan we eliminate multiqueue.lua in order to reduce both new concepts and lines of code? (What would be the consequence?)\n. Great! Works for me too.\nI have only some small requests before merging:\n1. Tidy up selftest() output (or make it more self-explanatory)\n2. Use \"function foo()\" syntax at least in files that are already doing that (e.g. lib.lua)?\n3. Squash into fewer commits.\n. Could you squash a bit more? I see three commits but they don't seem to be logically separate. It's a first cut of the driver, then a merge from master, then a new version of the driver. I'd be happy to take the whole thing as one commit or (optional bonus points) with the mac address / hex dump features split into separate commits.\nalso please do make the \"function foo()\" change where it's needed for consistency (I can be more explicit using line comments).\n. Merged now!\n(Tests aren't passing for lib.core btw but hey we spent enough time in this Pull Request window now :-))\n. Fixed by converting all branches into tags.\n. Great! and nicely rebased :-)\n. Great hacking!\nWould it be reasonable for you to send these changes as separate pull requests (topic branches)? It's good to have the split into separate commits but I think it's justified to take a step further because I see these as unrelated changes.\n. I am worried that everything will be way too confusing when there are PRs depending on other PRs. Already the reason I'm taking time to merge changes is that PRs are too complex and including both changes that I want to merge and others that I don't. Please help me out and keep it simple.\n. Alexander, Sorry I haven't managed to give the Travis-CI patches priority to get them merged before Javier's new testing campaign. They do at least clearly demonstrate the requirement on testing to take things like PCI addresses as configuration items and automatically skip tests that don't have necessary resources available (e.g. when running in a Travis VM).\nLet's see what Javier cooks up and then revisit the patches if that's okay for you.\n. welcome!\nyou should now be able to:\nssh andycjw@chur.snabb.co\nand use sudo too.\n. (This code is merged.)\n. Sorry guys. There is a lot going on in the testing code right now. Let's please drop this line of development for now and perhaps reopen it later.\n. Thanks, awesome, sorry about the long merge process!\n. Does this pull request really solve a problem? make book is still not working and I'm not sure how you are checking the effect of these changes?\n. I'd be glad to merge a change that successfully creates a snabbswitch.pdf again.\nThis one isn't working for me. I did a clean checkout, make at the top level, then make in src/ and it fails for me:\nawk '/^ *[^-]{3}/ {if (last~/^ *---/) printf(\"\\n\")} {print} {last=$0}' < lib/hardware/pci.lua | sed -E -e 's/^/    /g' -e 's/^    --- ?//g' > doc/obj/lib/hardware/pci.lua.md\n/bin/sh: 1: cannot create doc/obj/lib/hardware/pci.lua.md: Directory nonexistent\nmake: *** [doc/obj/lib/hardware/pci.lua.md] Error 2\nGenerally it looks to me like genbook.sh is rotten and referencing old filenames that don't exist anymore e.g. hub2.lua but perhaps those errors are survivable?\n. Could you please send pull requests with master as the base (rather than other pull requests)?\nIf I read the diff or click \"Merge\" on this pull request then I'm getting all the changes for documentation, test framework, etc and in my mind these are all logically separate.\n. This pull request contains unrelated changes. I'd like to pull in the fix for broken multi-iovec packets but I don't think it belongs with all the new utility functions in packet.lua and buffer.lua.\n. Interesting. The utility code appears in the diff as being totally new and not a fix to existing code.\n. Awesome!\n. The referenced commit seems to have solved this.\n. Possible. I'm not certain of anything yet.\n. That does sound bad. Better would be to always call push() at least one per breathe() for all links that are not empty?\n. This raises a question for the API: What happens when push() does not consume all input packets?\n1. Called again in the same breathe() loop.\n2. Called in the next breathe() loop.\n3. Called after the next time a new input packet is received.\nWhat do you think?\n. (Aside: Is this actually related to the Issue that we are commenting on? If so, how?)\n. I agree 2 is best.\nCan you file a bug? (Or send a patch?)\nLet's move the discussion off this issue. I don't think it's related. The failing test for this issue involves only a small number of packets (less than 200) so no links should be full.\n. It's for building boxes that process packets. Yes, it would be a reasonable starting point if you wanted to build your own traffic shaping box instead of buying one from Bluecoat / Sandvine / Cisco / etc.\n. Great!\nRequests:\nCould you add more detail to the pull request so that it can serve as release notes? e.g. title \"Intel10G: Added support for multi-iovec packets and jumbo frames\" and text saying if any configuration is needed etc?\nReading the code, the main change I see is adding support for stop()ing VFs. That is not mentioned in the pull request. Can you split that out?\n. Looks great!\nHow about the Travis-CI failing test?\n. It would also be nice to have the detailed description as the commit message so that we see it in \"Git log\". It seems like a bother to have to duplicate the text for both Commit and Pull Request. I wonder how people usually deal with this.\n. One more request: Could you update the title to say that these changes relate to the Intel 10G driver?\n. Thanks!\nCould you put \"intel_app:\" in the Pull Request title? Then the PR will be specific enough to serve as a Release Notes entry.\n. Great!\nCan you add \"Intel10G:\" to the Pull Request title? Then it should serve as a good Release Notes entry.\n. (Merged -- title can surely be chagned after the merge anyway.)\n. Obsoleted by #166.\n. Looks like this may be needed even with latest LuaJIT.\n. Done!\nYou should be able to:\nssh jxta@chur.snabb.co\nand have sudo rights.\n. Cool stuff :-).\nThe Snabb Switch way to handle library dependencies is to import them into deps/ and statically link them. I suspect doing this with the whole of libpcap would blow the budget for compile time (and perhaps object size). Is there an easy way to build only the packet filtering support?\nI agree that pcap-filter is a very convenient way to filter packets.\nbtw: notes on performance of various BPF executation engines here http://carnivore.it/2011/12/28/bpf_performance\n. Maybe something here? http://blog.cloudflare.com/bpf-the-forgotten-bytecode\nOn Friday, May 23, 2014, Alexander Gall notifications@github.com wrote:\n\nNope, doesn't work. It still depends on libpcap for the bpf compiler.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/pull/171#issuecomment-43995022\n.\n. re: @sleinen's comment on Travis-CI: you can hack snabbswitch/.travis.yml to update the tests and include that change in the pull request. The test triggered by Github should run with the test spec you are pushing.\n. I like that idea very much.\n\nI discovered something annoying though: one is not allowed to link GPL code into an Apache licensed program. Those implementations are all GPL.\nSo it looks like we can't have everything. I'm tempted in that case to compromise in the direction of convenience, which would be to dynamically link with libpcap as your patch already does. Then over time we could look for another solution e.g. factoring out the BPF bits of libpcap so that we could link them in (and not have to worry about inconsistent snabbswitch behaviour depending on what libpcap version is installed).\nWhat do you think?\n. On 23 May 2014 17:40, Alexander Gall notifications@github.com wrote:\n\nAh, the subtleties of free software :) So we're lucky that libpcap is BSD.\nIndeed :).\nI'm fine with that. Can I rebase my code to the current master and submit\na new pull request?\nYep. Please include a change to snabbswitch/.travis.yml with Simon's\nsuggestion to include libpcap so that the CI tests should pass.\n\nbtw: I'm excited about the idea of seeing how much work we can offload to\npcap filters in Snabb Switch. I've had good experiences with this in the\npast actually. You can even do funky stuff like simple traffic hashing with\npcap expressions.\nI have also been burned by libpcap bugs and confusing version-dependent\nbehaviour in the past and vowed not to use it again, but hey it does seem\nlike a darned convenient solution to the general packet-filtering problem\nright now :-).\n. pcap-filtering update:\nlibpcap is practical now but for the long term I'd love to have good filtering built into Snabb Switch.\nI posted a challenge on Twitter to see if anybody would attempt to make a highly optimized pure LuaJIT implementation of pcap-filter/BPF. Andy Wingo (@andywingo) and his colleagues at Igalia took the bait and are going to hack this over the coming month.\nTheir working tree is here: https://github.com/Igalia/pflua. I'm looking forward to seeing what they come up with - Andy is a compiler-hacker whose work I have admired for some years.\n. Great!\n@alexandergall how does this look for you? (building on your framework.)\n@javierguerragiraldez does this look like something you can use to help with intel10g testing / bug-finding?\n. @alexandergall Is it okay for you if I merge this first and you rebase your commits onto this if there is a conflict? (Or what would you suggest?)\n. You are my hero! :-)\n. Fixed by 18a81552\n. You are right: my bad for missing that on the merge.\n. Nice catch!\n. I don't immediately see a workaround.\nCurrently we focus on x86 as the target. This may be a mistake but it is the state of affairs today. Can you say something about the virtues of this platform?\n. Note: The platform restriction is in LuaJIT, which we depend on, and not in Snabb Switch itself.\n. @agladysh I would like to hear about the intended application even if the intended hardware is not supported by LuaJIT.\n. So, two issues:\n1. Snabb Switch is using LuaJIT 2.1 and Mike seems to have dropped PPC E500v2 support. This could potentially be worked around by back-porting Snabb Switch onto LuaJIT 2.0.\n2. Our Intel 82574L driver has bit-rot and needs to be updated to work. The internal Snabb Switch API has changed since that driver was written, particularly with introduction of struct packet instead of passing raw buffers around.\nSo some hacking would be required to make this work.\n. Can't wait to take it for a spin! :-)\n. Could you split this into two pull requests please?\nI'd like to merge the virtio optimizations directly.\nThe other optimisations: I'd be really happy if we could make all of our debug code run fast so that we can keep it enabled in production. That is such a wonderful luxury that can save weekends and late nights. So I would like spend some energy trying to reduce the amount of debug = false optimization in the code base by making the debug code run faster.\n. Thanks! This should be really practical, even given the caveats. It's quite a luxury to be able to restart the snabb process at any time e.g. when doing integration testing with another box and deploying a series of small code tweaks to make it happy.\n. I have no plan at this stage :-).\nHow do you think we should do it?\n. On further thought: it seems best to run the CI inside a Vagrant to me. devstack/tempest have so many side-effects on the host OS (apache / iptables / ebtables / etc) that it would be messy to really host it on chur in a stable way. Could perhaps use a passthrough-mode Intel NIC (or a fake/software NIC perhaps).\n. Can you explain your approach? I may not understand it properly.\nMy line of thought is basically: if we can live without a Python agent on the compute host then that makes the design simpler. (I think I got this idea from OpenDaylight driver?)\nThe disadvantages that I see in having a Python agent running on the compute nodes is:\n1. One more process that we need to test, debug, and troubleshoot.\n2. Makes basic operation of Snabb NFV depend on RabbitMQ.\n3. More code that we need to upstream into OpenStack instead of developing independently.\n4. More code in a language that's foreign to Snabb Switch i.e. Python.\nThere may be advantages that I am overlooking though?\n. You are right. I hacked this in an ugly way and have not found a better solution. Here is what I did: https://github.com/lukego/neutron/commit/31d6d0657aeae9fd97a63e4d53da34fb86be92f7. It seemed like the problem was caused by the DHCP agent, and we don't necessarily want to run the DHCP agent at all in Snabb NFV, so I was hoping we can work around it somehow.\nI would ideally like to not have an agent. Does ODL do that successfully? What would be needed to make this work? If you can analyze the situation I can discuss it with the OpenStack hackers e.g. via code review.\n. Does OpenDaylight have an agent? If not then we shouldn't need one either. If they do then perhaps I will accept that we do need one after all :-).\nI dislike the RabbitMQ infrastructure connecting our own components together. I would be happier if we had a \"perimeter\" around our code where all the message queue infrastructure stops. Then we only need to troubleshoot MQ for talking with OpenStack and not for internal communications in our own code.\nI don't know how #191 should report on errors/statistics but I am not imagining this as something on the critical path for being able to boot a VM, like the other agent stuff we are talking about.\n. If you send a pull request with a tidied up version of your Snabb Agent then we can look seriously at merging that. I would like to take that discussion 100% on pull-request / mailing list without referring to code that's only on Bitbucket please.\n. That sounds like a good start. If there would good ways to report on (say) errors detected by traffic processes, traffic usage statistics, etc, then that could be neat too.\nThe snabbswitch_neutron_agent.py is not currently in the SnabbCo repositories so the next step for reviewing that would be to submit a pull request that adds it.\n. Here is the current architecture: Snabb NFV Architecture.\nOverall we have worked to make the code small and conservative. Small: there is no agent anymore. Conservative: we do all API extensions via binding:vif_profile key/value options instead of relying on API extensions like QoS. This makes it easy for us to retarget between OpenStack versions and limits the impact of upstream activity/non-activity (e.g. QoS API not being merged for 2+ years).\nCode for Neutron (based on Icehouse):\nhttps://github.com/SnabbCo/neutron/commits/nfv/icehouse\nCode for Nova (based on Icehouse):\nhttps://github.com/SnabbCo/nova/commits/nfv/icehouse\n. See Neutron API Extensions. Hosts are being selected with default Nova logic but then the specific 10G port on that host is chosen with bandwidth-awareness.\n. OK. You should be able to:\nssh root@chur.snabb.co\nand create a user account for yourself (with group sudo).\n. Great!!\n. Good catch! Fix submitted in #199. Too bad tests don't currently cover descriptor-ring wrap-around. (The follow-on version of the loadgen benchmark will cover this case when we have the VM transmitting as well as receiving.)\n. Looks promising!\nHow much is the total speed up? Do all these changes measurably contribute to the speed up, or are there some we can safely skip?\nSkipping the zero-fill on freed packets is a bit too aggressive. For example, the length field won't be reset. You could skip this change and let me do something here: there is some cruft in packet.h (unused fields) that I need to remove at the same time.\nTravis-CI build failed. Can you see why? (Does \"make test\" work for you?)\nCan you please rebase this to master so that there is no merge commit in the pull request?\n. See also snabb-devel thread on whether we could automate the process of measuring the performance impact of new changes.\n. Superseded by #203.\n. There is also an unstated assumption in this code that huge page are huge-page-aligned i.e. that a 2MB page will be aligned to a 2MB address. I think this is true but [citation needed].\n. Superseded by #205\n. I have learned to love Git and I wonder how I ever survived without it now.\nYou could actually have used rebase -i to remove commit d696a40 from history instead of having to add a new commit to revert it. However I am too impatient to merge the optimizations to request that :-)\n. I'm working on the \"Fast TLB\" problem for now.\n. 5% doesn't sound like much, though maybe these changes will become more interesting once we have some other optimisations out of the way.\nHey, I realised that some optimizations that looked promising in the past are not currently on master. Do you want to take a look? description here: https://groups.google.com/d/msg/snabb-devel/ZhIa4iSHj1Y/XGEOVkNoOg8J\nIIRC it made a substantial difference to cut out the dTLB misses by allocating 'struct buffer' and 'struct packet' with memory.dma_alloc() (i.e. from 2MB huge pages) instead of from malloc(). This likely requires a change in packet.lua to allocate individual structs instead of one big array (because the array will be too big for a huge page and dma_alloc() can't serve that.)\nSorry I never got around to hacking the faster packet clear. 'fuel' struct can be deleted, length needs to be zero'd, that should be about it. (I had an experimental change to make refcount initialise to 0 instead of 1 though I don't know if that makes any measurable speedup on packet freeing.)\n !\nI will attack TLB lookup today.\n. Checked off TLB.\n. @llelf, I'd like to check off the dTLB, alignment, and packet-clearing items tomorrow. Let me know if your schedule permits you to hack them, otherwise I can do.\nLooks like there will be more LuaJIT optimization needed. On longer runs I am seeing GC kick in and that is really not wanted so we probably need to read the IR and eliminate the allocations somehow.\nalso some other wild ideas like trying to reduce the total number of instructions in the inner loops -- possibly useful or possibly not. can discuss :).\n. @llelf that's awesome! I have also had at least one run with 4.5 Mpps. That is exciting!\nThe latest fresh challenges I see now are:\n- Keep performance consistent over longer runs (100M+ packets)\n- Keep performance good with larger packet size (e.g. 256 byte packets)\n- Keep performance consistent between one run and the next\nDo you have time to look into these too? If so I'd suggest testing with an ~/bench_conf.sh like:\nPCAP=/opt/bench/256.pcap\nNFV_PACKETS=100e6\nI also posted my idea for cutting dTLB misses at pull request #214 though I am not sure if it's actually valuable from my quick tests.\n. So this afternoon it looks to me like \"the big problem\" is performance degradation during longer runs e.g. 100M packets or more. This is more apparent when profiling is enabled, but I do see it otherwise too. The profiler points to 10% garbage collection overhead, which may or may not be the root cause. I don't know where the garbage would be generated: could be in the tight push/pull loops or in the app.lua main loop that runs every 100us or so.\nDo you have time to check this out and try to make 100Mpacket runs go as fast as 10Mpacket runs?\nIt looks like we will have sufficient performance with that problem solved.\nbtw: I had a thought about the dTLB. Could be that this isn't such a problem for us now because we don't allocate all that many buffer objects. The virtio-net interfaces have 8K queue length for TX/RX compared with hardware that supports 32K. So could be that we don't suffer from dTLB thrashing on 4KB pages due to having fewer buffers in use. (Or that could be totally wrong :-)).\n. Howdy! any new insights on the \"100M is slower than 10M\" / GC problem?\n. That is progress!\nCan I test the minimum-4.0 version somehow?\nI'd ideally like to eliminate all of the allocations. (Do you know what causes them?)\n. If you can send a pull request with the minimum needed changes to get 4.0 Mpps on 100e6 that would be great. I would like to merge that and lock it in. That is really excellent and practical performance.\nI suppose the \"where\" of the object allocations will be IR nodes that represent allocations somewhere in the hot traces? (I wonder if we can eliminate all of those?)\n. I see ~ 4.1 when testing with a single port but only ~ 3.7 when testing with 6 ports. The latter is what I'm using for a benchmark. (Difference is probably due to the Xeon TurboBoost feature scaling the CPU frequency depending on how many cores are in use... couldn't find a way to disable that in the BIOS.)\nDo you think we can get the same performance on 100e6 as on 10e6? That would be ideal.\nWild speculation: If the issue is allocations due to boxings that \"allocation sinking\" doesn't eliminate, perhaps we could pre-box whatever is the problem? For example, link.receive() could have an optional argument that is a \"struct packet[1]*\" and it would store the pointer there instead of creating/boxing a new packet pointer in Lua. (know what I mean? could be way off base of course, but I think Alex Gall has done something like this in his latest optimisations.)\n. Howdy!\nI would love to have 10e6 performance on 100e6 (and 1000e6, etc). Ideally this week :-). Do you see any light at the end of the tunnel?\n. awesome!\n. Added. Currently we are reinstalling our main development server (chur). We should be able to make your account accessible next week.\n. Great stuff :-)\nI made one extra commit to replace -lpcap at compile-time with ffi.load(\"pcap\",true) at run-time. So you don't need to compile with libpcap, but the filter module will only load at runtime if you have it available. This seems kinda neat to me. I think that behind-the-scenes ffi.cdef() is resolving all of the symbols with dlsym(3).\nHope it didn't break anything. The test case passes.\n. Awesome!\n. Hi James, you'll get an account on chur when we get it back online, which could be a week or so :).\n. Could you please use git rebase -i to reduce the number of commits?\nThis is a wonderful feature of Git that you can use to reorganise commits into the \"the way it should have been\" from the way it actually was.\nOr if you are not in the mood to geek out on Git you could squash everything into one big commit and I will merge that.\n. On 28 August 2014 13:22, Max Rottenkolber notifications@github.com wrote:\n\nThat is wrong indeed, it works if you swap x and y in the second loop but\nmaybe it should just compare '#x == #y`?\nThe Lua # operator returns creative results when applied to something that\nis not an array indexed by 1,2,3,4...\n. It would be really wonderful to have a way to test and measure these optimizations. Do you have any ideas?\n\nNow we have a built-in benchmarking setup so if we could write a benchmark that captured this optimization we could avoid regressions in the future.\nGenerally with fancy optimizations it would also be great to find a way for people in the future to know:\n- Is this still needed? (e.g. after LuaJIT upgrade)\n- Is this needed in other places too? (How do you check if it helps?)\n. The automatic performance testing that Max implemented does actually run a benchmark multiple times and look at the distribution of results. So if we had a benchmark that behaves less inconsistently after the optimization then the CI might be able to give us visibility of that.\n@eugeneia what do you think?\n. Sorry about the slow turnaround on this one.\nCould you rebase onto master? There is a conflict in M_sf:transmit() with Alex Gall's recent optimization there.\nAlso: I think we should modify the design of the driver so that linkup is detected asynchronously. If the link is down then the NIC will be idle and if the link is up it will TX/RX packets, but it will never block. I will open an issue to track that and put your name on if that is okay :-).\n. Great! Looks like a good implementation with good unit tests.\nCould you rebase onto master? Github says there is a conflict to resolve.\n. @eugeneia could you please pull this into your tree, make yourself happy with the way neutron2snabb does packet filtering, and push onward to me?\n. Sorry to be slow about that!\n. There is a related feature that we should add, but I suspect it belongs in neutron-sync-agent and not directly in neutron2snabb:\nThe new configuration file for a traffic process should be created:\n- Only if the configuration has changed. i.e. if the new file and the old file are the same then we should not bother the traffic process with an update.\n- Atomically, so that the traffic process will never try to load a partly-written file.\nI suspect both could be handled by updating the neutron-sync-agent script to generate files into a temporary location, then diff to see if there are changes compared with the running configuration, and if so do an atomic update with mv.\n. I have a favorite O(1) expiry algorithm that might be interesting:\n- Have two tables, old and new.\n- If lookup in new fails then check if the entry exists in old: if so copy it into new.\n- Every (half-of-timeout-interval) do old, new = new, empty() i.e. discard the previous old table, make the previous new table old, and create an empty new.\nTurns out I have blogged about this once upon a time: http://lukego.github.io/blog/2013/02/04/cute-code/\n. Great stuff! reading :-)\n. First question: How should we unit test this? (Basic functionality, performance, and graceful behaviour when tables fill up.)\n. I think this design is good :).\nI think we need a unit test that stresses it with lots of semi-randomized traffic e.g. seeing when the table is large or overflowing. Like with 100 addresses, 1000 addresses, all packets having a different address, etc.\nThen we see if any parts overheat and if so find a simple replacement.\n. Hi ba1020!\nYour github profile is very anonymous. Who are you? :-)\n. Welcome! You should be able to chur with ssh -p 2020 root@lab1.snabb.co and create yourself an account based on the Wiki instructions. Let me know with a comment here if you have any trouble.\nPardon the wait. Your online persona is the most anonymous that has asked for an account so far and so I had to recalibrate my mental model a bit. You are most welcome to join in the hacking :).\n. NB: The SnabbBot test failure was separately fixed by #254.\n. Fixed. Intel selftest now automatically detects failures. See #373 and earlier changes.\n. This is also to make it easier to work on the PCI device API. More commits to follow.\n. The PCI device locking is what exposed the bug in engine.configure(). (intel_app's selftest function got stuck because the new app started before the old one stopped.) That's why these commits are grouped together on the same branch / pull request.\n. Welcome! You should be able to chur with ssh -p 2020 root@lab1.snabb.co and create yourself an account based on the Wiki instructions. Let me know with a comment here if you have any trouble.\n. To my mind we are looking for a design that is simple, reasonable, and predictable.\nQuestions:\n- Does the driver/app need to know the link state? Why?\n- When should we force the link up always, never, after a timeout, or according to config? Why?\nOne alternative would be to always force the link up (set FLU bit) and never care about the actual physical link state. This is simple: less code even than we have today. This may be predictable too: I expect that the DMA engine will process descriptors continuously and drop anything that can't be delivered (?). Is this behaviour also reasonable? I am not sure about the last point: maybe this makes it harder to write e.g. selftest code that expects the app to buffer packets sent before the link is physically up (?).\nIf that would be a real problem then a second alternative would be to have a configuration option to force the link up. This is still relatively simple. The behaviour may be predictable: I would expect the DMA engine to process descriptors only when the link is up, either physically or when forced by configuration (?). This would seem fairly reasonable, and potentially make selftest code easier to write (?), at the expense of being less simple (user has to decide whether to force the link).\nI am cautious about introducing our own new logic in the driver for monitoring the link state and deciding whether the DMA queues should be processed. If this behaviour already exists in hardware then I would prefer to let that do the job. If we duplicate functionality in software then it means more code and the risk of falling out of sync (e.g. getting bugs where hardware thinks the link is up and software thinks the link is down). However, this might be justified if the simpler implementations do not turn out to have reasonable and predictable behaviour.\n. I am closing this issue because currently we do not have any applications that need to dynamically provision physical 10G ports under time constraints. (We do dynamically provision virtual VMDq ports but that is fast.)\n. The reason to use pcap-ng is that it supports more metadata, in particular you can say which link you have picked up each packet on. (So if you see the same packet 10 times in the trace you can work out what path it took through the app network.)\nHowever, now that we start talking about exposing link structs in shared memory, I wonder if it will be better to run such tracing code externally in its own process that maps all of the links and \"races\" to record what is going on while the traffic is being processed?\n. Resolved on this mailing list thread:\nhttps://groups.google.com/forum/#!topic/snabb-devel/MrzImre1gbM\n. Welcome! Your key is added to chur. You should be able to ssh -p 2020 root@lab1.snabb.co and create yourself an account as described on the Snabb Lab wiki page.\n. Welcome! Your key is added to chur. You should be able to ssh -p 2020 root@lab1.snabb.co and create yourself an account as described on the Snabb Lab wiki page.\n. Welcome :-)\nYour key is added to chur. You should be able to ssh -p 2020 root@lab1.snabb.co and create yourself an account as described on the Snabb Lab wiki page.\n. ... Related: If the intel10g driver has a minimum buffer size requirement, what should happen if the rx_buffer_freelist in intel_app contains buffer(s) that are too small? (For example due to a virtual machine supplying unsuitable buffers via Virtio-net.)\nCould be that this should cause intel_app to automatically stop using the rx_freelist and falling back to generic memory.\n. There is no guarantee of the buffer size. The virtual machine can give us weird buffers if it wants to and we need to protect against this.\nI like the idea of making the driver's buffer size configurable. Then I think intel_app needs to check whether each buffer it gets from the rx_freelist is big enough. If there is ever a buffer that is too small then the VM is not compatible with the zero-copy optimisation and we should stop using the rx_freelist and fall back to general memory. (The virtio app will then copy the data into the weird buffers provided by the VM.)\nmake sense?\n. SnabbBot CI says the lib.nfv.config module selftest fails. Is there a bug there?\n. That test should also run on other machines (e.g. chur) if you have define SNABB_TEST_INTEL10G_PCIDEVA to be the PCI-ID of a 10G port.\nBut @eugeneia can you tell us if the new nd_light works for your NFV test cases now?\n. This could be an opportunity to experiment with some advanced git-fu :-)\nMax, how about if you pull Alex's branch (from this pull request), add a commit that makes L2TPv3 app add the ethernet header (for overwrite), then send me a pull request from that branch? Then I think I can press Merge and both this pull request and that one will be closed (because all the commits they contain will be merged into master).\n. There is a problem with checking the freelist at init time: the freelist can be populated with different buffers over time so even if the contents were OK to start with this does not guarantee they will be OK in the future. (And in practice the freelist is probably empty at initialisation time and later becomes populated when buffers are retrieved from VMs.)\nGood news though. The virtio app should be happy to accept both zero-copy and non-zero-copy buffers (or even a mix for different iovecs of the same packet). So I think it would work to initially accept the rx_freelist and then stop using it if we are ever given an unsuitable buffer.\nCan we rename rx_buffsize to rx_buffersize? Otherwise I know I am going to confuse abbreviations buf vs buff sooner or later and when in doubt I think it's best not to abbreviate.\nIs the rx_buffsize now a mandatory argument when creating the intel_app? (Have to update the code base for this? The SnabbBot CI test is failing on lib.nfv.config case -- is this the reason? does make test work for you?)\n. :warning: NEW IDEA WARNING :-) :warning:\nHow about if intel10g automatically learns the minimum buffer size being used and reprograms the SRRCTL register for each hardware queue when needed?\nOne simple scheme could be to start with the maximum buffer size (16KB) and then for add_receive_buffer to detect when a receive buffer is too small and automatically reprogram SRRCTL with a safe value before adding the receive descriptor.\nThen intel_app should still detect buffers that are too small and drop the freelist, but only for the extreme case when a buffer is so small that it can never be used by the NIC i.e. < 1KB.\nWhat do you think? :+1: or :-1:? (Sorry never tried Github emoticons before...)\n. Will pull req 286 fix this?\nhttps://github.com/SnabbCo/snabbswitch/pull/286\nOn Monday, October 13, 2014, Max Rottenkolber notifications@github.com\nwrote:\n\nI didn't know how long it would take to find the bug that's why I\nsuggested reverting this PR for the time being but I don't mind how the\nfix gets into master. Just make sure master is \"broken\" as short as\npossible.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/SnabbCo/snabbswitch/pull/274#issuecomment-58900660.\n. Sounds like progress :-). I will be really happy to merge this feature.\n\nDo the statistics counters tell you anything interesting? (For example, if the checksum is being set wrong on transmit perhaps the NIC is discarding the packets on receive?)\nYet another idea for keeping the code simple/minimal btw... could the NIC automatically drop RX packets with bad checksums so that the code can always mark the checksum correct for packets actually received? (I don't think we have any important use case for processing packets with invalid checksums.)\n. Fantastic!! (fast work!)\nHow about testing this with Max's new functional test suite (https://github.com/eugeneia/snabbswitch/blob/lib_nfv-selftest/src/lib/nfv/README-selftest.md)?\nHere is what I think needs to be done:\n- Add flag C.VIRTIO_NET_F_CSUM to supported_features in https://github.com/SnabbCo/snabbswitch/blob/master/src/lib/virtio/net_device.lua#L53\n- Check that this makes the test case CHECKSUM pass i.e. ethtool in the VM will report that checksum offloading is enabled.\n- Check that real traffic works e.g. with the IPERF test case\n- (Bonus points) use tcpdump -vv in both guests to confirm that they send packets with an invalid checksum but receive them with a valid checksum.\n. Sorry: I merged this too eagerly and will need to revert.\nWe should change the logic for deciding what and where to checksum:\nInstead of looking in the packet payload we should look in the packet metadata (struct packet.info field). That is, checksum offload is only used when explicitly requested in the metadata of the packet. In the NFV case it is the task of the virtual machine to fill this in for us (it does this when it would otherwise calculate the checksum in software).\nIf no checksum is requested in the packet.info field that we should not do any checksum offload.\nMake sense?\n. Welcome Andy! You should be able to ssh to chur with ssh -p 2020 root@lab1.snabb.co and then create yourself an account as described on the Snabb Lab wiki page.\n. Hi Diego,\nYou should also be able to ssh into chur as root now and setup a private account.\n. Welcome Tim! You should be able to ssh -p 2020 root@lab1.snabb.co and then create a personal account as described on the Snabb Lab wiki page. Happy hacking.\n. welcome! you should be able to ssh -p 2020 root@lab1.snabb.co and create yourself an account as described on the Snabb Lab wiki page.\n. Ten thousand apologies Nikolay for taking so long to merge this.\nThe trouble was that SnabbBot tests had failed. Today I finally dug in and found the reason, which is kind of random. (See 8ae4ccb096d429989eeed7e09ff6217c776b7db0 if interested.)\n. Merged. Can you please help with the effort to make NFV selftest.sh cases pass for any that are relevant to this code?\n. Cool :)\nFor transmit checksum offloading, I think this pull request contains more functionality than we really need. I see two separate but related features:\n1. Inspect the packet payload to determine the position/length/offset where checksum is required.\n2. Create a context descriptor to communicate the checksum requirements to the NIC.\nCurrently I think we only need feature #2 i.e. creating the context descriptor when already knowing where the checksum should be. Instead of looking at the packet contents to determine where the checksum is needed (feature #1) we should look at the packet.info struct where this should already be known. Specifically, packet.info.csum_start and packet.info.csum_offset should contain the information that we need to create the context descriptor. (More info about those fields in section 5.1.6.2 of the Virtio spec. Basically, the kernel in the virtual machine has populated these checksum info fields at the time that it would have calculated the checksum but instead decided to offload it.\nThe code for determining checksum location from packet contents will surely be handy one day but I prefer not to merge that until we need it.\nmake sense? (entirely possible that I have missed some important detail.)\n. I see what you mean. I checked the reference Intel ixgbe driver and it is also inspecting the packet to determine how to write the context descriptor: http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c#n6622\nSo next request :-)\nCould you complete the pull request by getting coverage in the lib/nfv/selftest.sh? would need to:\n1. Enable the test_checksum tests (line 150-151).\n2. Make them pass.\n3. Make sure the other traffic tests continue to pass when using checksum offload e.g. iperf.\nbtw how much additional work does TSO (segmentation offload) look like? :-)\n. This is wonderful!!\nJust have to get the tests passing for tunnelled traffic :-)\nI have a simple idea. How about if packet.want_modify() would force checksum computation on the CPU. (That is, we assume it's too hard to make the checksum-offload request survive further modifications to the packet, so we calculate the checksum in software and clear the offload request.)\nThe checksum should be simple to compute based on my understanding of Virtio-net: Just to calculate the checksum from the given start offset until the end of the packet, and then store the result in the given value offset. You shouldn't have to worry about pseudo-header-checksums and all of that: IIUC the guest has already taken care of this and stored the necessary preliminary value in the packet's checksum field where it will be automatically picked up.\n. @eugeneia Great catch. I'm taking a look on chur.\n. I rebooted chur with 1GB large pages in the grub config. Does this help? (I am not sure that is the root cause, but I had sneakily changed that to 2MB recently while testing the straight-line branch, and 1GB is the preferable setting for the NFV application.)\nThe error on Grindelwald is the same for you? (Failing to resolve a physical address?) 1GB pages are already enabled there (see /proc/meminfo).\n. Can you strace the snabb process?\n. I suppose that we should be automatically unbinding the NICs from the OS driver before we use them.\nJavier, do the NFV tests work for you on the \"good\" NIC? That is the most interesting question from my perspective. (Should I replace some cables on davos in case it is a physical connectivity issue?)\n. Great detective work, Javier!\nQuestion on my mind is whether the straightline branch will work well enough to merge within the next week or two. If so then this bug will probably disappear during the Great Simplification. If not then we will have to fix it. Sorry about this limbo situation - has to be short lived - really seems worth it if we can make the merge.\n. Going to need a revision after the straightline merge! Sorry about that :-)\n. (Oops should not have closed, only commented.)\n. (Damned how did I accidentally close this twice. Fat fingers!)\n. @eugeneia Can you see what the problem is with NFV selftest?\n. Looking forward to merging this! The NFV selftest we can perhaps troubleshoot on master. (This is not the only PR affected.)\n. Great hacking! :beer: \n. Hold on! :-)\nLet us make sure that the right thing is happening here. It would be easy for a problem with verifying the checksum to hide a problem with setting the checksum. I am thinking especially of the case with L2TPv3 tunnelling enabled.\nLet's take an example packet that the VM has requested checksum offload for and then Snabb Switch has added encapsulation to. The headers marked with (*) are the ones that don't have valid checksums yet and need to have this fixed before transmission.\nEthernet (outer)\nIPv6 (outer)\nEthernet (inner)\n*IPv4 (inner)\n*TCP (inner)\nI don't see from the code how this would happen today. The hardware needs to know to checksum the inner headers instead of the outer ones. Or, if hardware can't do that, then it has to be done in software by Snabb Switch.\nHere is the code that I am looking for but don't find:\n1. The Virtio code to set the packet.csum_start and packet.csum_offset fields in the packet sent by the VM so that we know where the not-yet-checksummed header is.\n2. The L2TPv3 app to update these offsets when adding a prefix to the packet (e.g. in packet.prepend())\n3. The Intel10G driver to use these offsets to checksum the right header (the inner one) e.g. by telling the NIC that the ethernet header is really big in order to skip over the outer headers.\nand then we also have to be very careful not to over-interpret the checksum validation done by hardware. Just because hardware says that the outer header's checksum is OK does not mean that the inner one is also OK. So perhaps the L2TPv3 app needs to clear the PACKET_CSUM_VALID flag when it removes the encapsulation too.\nDoes this make sense?\n. This makes a lot of sense.\nFor the transmit path I am looking forward to the hack to checksum the inner packet :-)\nFor the receive path I like the idea of letting the guest do the checksum if it was not done in hardware. So for non-tunnelled traffic the hardware will checksum the TCP header and we will report that to the guest (PACKET_CSUM_VALID) but for tunnelled traffic the inner TCP header won't have been automatically checksummed and so the guest will need to check it (PACKET_NEEDS_CSUM).\nThe issue still in my head though is: do the packets actually get checksummed on TX and RX with the code on this pull request? Or is checksum skipped in both directions and that is what makes the tunnel test case pass? If the latter then it is too early to merge because it is only passing the test cases on a technicality (limitation on the test suite). I could be missing something obvious in the code though :)\n. BTW: Is the \"tell the NIC there is a large ethernet header so that it skips the inner header\" trick based on the packet.csum_start field and initialized from Virtio? (That seems like the correct way, or?)\n. Aside: Some friends kindly open sourced their CPU checksum implementation. In trivial testing at least it seems way faster than e.g. the DPDK one. It doesn't sound like we desperately need this right now but, anyway, the code is in #391 for reference.\n. Victory at last :-). This is a very impressive piece of programming! Celebration is in order :beers: :+1:\nI have an second idea. (Uh-oh.) How about if we measure the benefit of hardware checksum offload before we commit to it?\nSpecifically I start to wonder about the \"straightline\" tradeoff here. Hardware offload turns out to be quite complex and the justification for this is to get a major performance boost. Now that we have an implementation that we can benchmark, why not check whether a more generic CPU-based checksum can be competitive?\nEven more specifically I am imagining this experiment:\n- Enable checksum offload at the Virtio level.\n- VM to network: Calculate checksum using the SIMD checksum function from #391 instead of offloading it onto the NIC.\n- network to VM: Keep it simple for proof of concept experiment - same as we have today, using hardware checksum if available and otherwise letting the guest do it.\nWorth testing?\nMy thinking here is that the NIC offers a pretty crumby checksum offload interface, really. Ideally we would simply say \"checksum from START to END and store the result at OFFSET\" and the rest would be taken care of in software. Instead they have us parsing packets and explaining the data to the NIC in terms of a really limited set of protocols. The next Intel NIC is better in some respects (more protocols) and worse in others (complexity explosion while trying to simultaneously offload inner and outer checksums). God knows how nasty it will be making this nice across more vendor NICs.\nSo a reasonable person may ask, what is the payoff compared with implementing the checksum primitive that we actually want on the CPU? Especially if this can be offloaded onto the SIMD unit that has exponentially increasing throughput across contemporary CPU models?\nI also have a slightly uneasy feeling about embracing special cases to make hardware happy. There are a lot of situations where we could either do everything well, slowly, or brokenly, and it's not obvious which. For example, all of these rhetorical questions would require some head scratching to answer in a satisfying way:\n- Can we use VLANs on the outer headers? QinQ? MPLS? GTP?\n- How about on the inner headers?\n- Can the guest add its own encapsulations and ask us to offload its inner headers?\n- How about if we and the guest both add encapsulations and wind up with three levels?\n- How is the relative performance of different encapsulations (GRE, L2TPv3, Geneve, ...) on different hardware? Do users have to care about this when they are choosing protocols and/or buying hardware? (What do you do if the NIC only offloads VXLAN and the Cisco router only offloads L2TPv3? and whose fault is it if you find yourself in that situation?)\nThese problems would all seem to disappear if we could \"do a straightline\" and have a fast generic checksum that we can use guilt-free. (There are also cool userspace bonus points we could have a really fast SIMD checksum since the kernel does not allow itself to use SIMD in the network stack. Not that the kernel's life is not already hard enough these days.)\nHaving said all of that, I will have a beer to celebrate the fantastic driver hack you have got working :-)\n. I know what you mean about much effort and little code. But I like it best that way really. Even better when the final result is to delete code :-). Great to have the fun of writing the tricky code and then find a simpler solution that is easy to maintain :-).\nI am not sure exactly how the checksum flags work but I can tell you how I hope they work:\n- Guest calculates all checksums except one. For example, guest computes IPv4 checksum and TCP pseudo-header checksum, but not TCP data checksum.\n- Guest requests offload of this last checksum. The offload is one dumb ones-complement checksum only. The checksum location is given in the struct virtio_net_hdr fields: from csum_start until the end of the packet, with the result stored at csum_offset.\n- In the case of TCP, the guest will prepopulate the TCP checksum field with the pseudo-header checksum and that means only a dumb ones-complement checksum is needed.\n- This would work for all protocols, including ones that are not invented yet, provided the guest tells us the right bytes to checksum. (The Snabb Switch code does not care what the protocol is -- none of this packet-parsing that the NIC makes us do.)\nCould this be close to reality?\nIf so then the offload should be basically one function call to update_checksum() in either the virtio module or the intel10g module like e.g.\nchar *buffer = packet.data + net_hdr.csum_start;\nint len = packet.length - net_hdr.csum_start;\nint initial = 0; // assume pseudo-header is already in the checksum field\nint csum_pos = net_hdr.csum_offset; // xxx is this relative to start of packet or to csum_start?\nupdate_checksum(buffer, len, initial, csum_pos);\nalthough there might be an incompatibility in update_checksum() because it is setting the checksum field value to 0 before doing the calculation, but if that is there the pseudo-header checksum is then it should not be cleared (and so you would need to comment out the line *cksum_ptr = 0).\nMake any sense? Sound possibly like reality?? (I know that in the case of the NIC the actual programming interface turned out different than I hoped :-))\n. Closing because the software checksum seems like a big success. (Great to also have this NIC offload code for future reference.)\n. Awesome!\nCould you follow up with an NFV test case too please? For this we will need to enable the rate limiter app in the NFV app network. That policing setting is fixed here. (Separately, we need to make that configurable per port in neutron2snabb e.g. to have a vif_details.rx_policing=true option, I will discuss this with @eugeneia.)\n. Great hacking! This looks like a solid, bold improvement :-)\n. Good time to rebase and merge?\n. @eugeneia do you know why SnabbBot hasn't posted test results for this pull request (and #296)?\n. Interesting! SnabbBot reports a performance regression: 0.75x performance on basic1-100e6 benchmark. (Could be worth making that error message more prominent?)\n@eugeneia do you have a recommendation for how @clopez should investigate this?\n. @eugeneia right on :)\n. Looks like a false-negative test result from SnabbBot. Tested manually on chur with success.\n. welcome :)\n. It seems like a complex task to keep two related global data structures in sync: the app state and the timer state. It feels like a step in the wrong direction to expose the internals of the app data structure to the timer module and to have to generalise the app-restart code into a separate module.\nBear with me if you are kind, but can we think about the simplest thing that could possibly work for our use cases, and then relate the solution to that?\nThe simplest thing that I can think of would be to delete the timer module and simply have an app:ticktock() method that is automatically called at some reasonable frequency, e.g. 10ms. Apps that have timing-dependent behaviour (e.g. ARP resolution) would implement this callback in some simple way (if engine.now() > self.last_stamp + 1.0 then ... end). Timers would then be closely tied to apps (and could use the same error handling behaviour) and would be based on the same familiar polling model of callbacks that we already use for push and pull.\nIf an app required high-resolution timers then it could implement them in pull() instead e.g. the rate limiter that wants to refresh its token bucket at ~microsecond intervals. If an app needed a really large number of timers, e.g. to track timeouts for a million sockets, then we could reintroduce the timer module as a local data structure to use within such apps' ticktock() methods.\nIs keeping the timer module and making its implementation larger really justified over the \"so simple it hurts\" alternative? (Is there an important use case where it works better?)\n. Sounds interesting and indeed getting simpler...\nSo would we simply implement timers by having the pull method poll engine.now() to see if it's time to take an action? And to wrap that up in a really simple data structure in lib.timer?\nThen in the future if performance became an issue, due to too many apps or too many timers in a given app, we can solve that when the time comes.\n. Were you able to test this? (I'm not sure if it is covered by the make test or not.)\n. Thanks!\n. Thanks for testing this! I have pulled your branch and done some tests too. Looks like the rate limiter is working as expected. (I was confused for a while before I realised that the second iperf run goes in the other direction and so that is why the other bandwidth limit applies :-))\nI can't merge the pull request because it would replace the basic test case with a rate limited one, instead of adding a new test case, but I am pretty satisfied with the rater limiter just by testing your branch. So I close this for now.\n. I merged #293 now.\n. This is an exciting development :)\n. Great, Javier!\nI can do the rebase. I would like to merge #311 first because I have added a better way to specify when rate limits should be enforced (per port instead of globally/hardcoded).\n. This looks really promising!\nOn Grindelwald I see around 3.4 Mpps per port:\nOn 0000:01:00.0 got 3.396\nOn 0000:03:00.0 got 3.403\nOn 0000:05:00.0 got 3.399\nOn 0000:82:00.0 got 3.449\nOn 0000:84:00.0 got 3.404\nOn 0000:88:00.0 got 3.374\nRate(Mpps):     20.425\nwhich is less than the ~ 4.5 Mpps we have become accustomed to but this is likely related to recent changes in the vhost-user code.\n. Thanks Max! Could you please give a cheat sheet for running this on chur? (Of particular interest to @dpino).\n. @eugeneia Can you take a look at ~luke/bench_conf.sh on grindelwald and see if you can replicate that on chur? This uses an optimized guest that returns all traffic received from the load generator. (More info in bench_env/README.DPDK.) This is a good benchmark because snabbswitch is the bottleneck when using small packets.\n. Could be that we need to simplify the setup of this benchmark. I don't think we are all talking about the same thing at the moment.\nThe benchmark that I have in mind is:\n- One load generator, connected to N 10G ports.\n- N identical guest VMs, each connected to one 10G port (cabled to a load generator port).\n- Guests run DPDK l2_forwarder application to quickly resend all packets received.\nSo we have a collection of VMs each connected by a Snabb Switch to a load generator. The load generator blasts traffic and the Snabb Switches loop packets through the VMs (network->VM->network) as quickly as they can. The main scenario we focus on is having 6x10G blasting 256-byte packets and we measure the aggregate packets-per-second.\nHere is how a run of that looks based on grindelwald:~luke/bench_env.conf:\nConnect to guests with:\ntelnet localhost 5500\ntelnet localhost 5501\ntelnet localhost 5502\ntelnet localhost 5503\nOn 0000:05:00.0 got 3.361\nOn 0000:82:00.0 got 3.237\nOn 0000:84:00.0 got 3.198\nOn 0000:88:00.0 got 3.217\nRate(Mpps):     13.013\n@dpino I created an account for you on Grindelwald. Do you want to try to run this there? ssh -p 2010 dpino@lab1.snabb.co.\nNB: grindelwald ports 0000:01:00.0 and 0000:03:00.0 are acting up at the moment. I will put on my lab-tech hat and take a look at the cabling.\n. @dpino Great that it's working. This is a benchmark in which snabbswitch is the CPU bottleneck and not the guest and so it is very interesting to optimize. It should be representative of situations where packet-forwarding appliances are hosted inside VMs (routers, firewalls, NATs, etc). These appliances tend to perform very little processing (perhaps only touching the header so the packets) but at high packet rates. This kind of workload is a weakness for the Linux kernel and intended to be a comparative strength for snabbswitch.\nThese activities would all be very valuable:\n- improving our score. We had ~4.5Mpps per port (max w/ 256-byte packets on 10G) before the new virtio work around October, and we need to claw our way back. (@llelf is looking into this too but has limited availability right now.)\n- checking the scores for more NFV configurations eg. rate limiting, filtering, tunneling (which would require preparing input traffic that is L2TPv3 encapsulated).\n- checking what performance we can get with the zero copy optimization disabled (lib.nfv.config to not pass the rx_buffer_freelist from virtio app to Intel app).\n- checking other packet size mixes eg. mean size 256 instead of all packets 256 bytes.\nWelcome to dig into anything juicy there :-)\nAlso please feel free to create accounts for your colleagues.\n. @eugeneia it would be great to be able to run this setup on chur. Please do if you have the chance. I'd love to get snabb bot onto it too for peer regression tests :)\n. @dpino btw the guest is not using th Linux kernel virtio driver but rathe is running a userspace driver (Intel dpdk). This helps the guest not be a bottleneck. I don't think this guest can handle jumbo frames though so we probably can't test with packets bigger than ~1514 bytes.\n. @eugeneia I really would have liked to send my commit as a pull request to you on your branch, but I couldn't work out how to make Github do that. Do you think my change is OK? (Good to merge along with your commits?)\n. Handled in commit 8c1d2b3005bb03c2230217f32c9efbad8d4eb331\n. Interesting!\nI wonder exactly what limiting factor the parallel tests are working around.\nQuestion: Do you see any suspicious errors on the interface counters (ifconfig or netstat -s)? I have had a report from one site of iperf results being low due to corrupted packets (causing TCP congestion control to throttle the per-socket transmission rate). I have not been able to reproduce this myself but it could plausibly be a limiting factor that might be affected by parallel testing.\n. (How is the bandwidth when testing in UDP mode?)\n. This is extremely interesting :-). I have no immediate theories for this pattern of MTU/performance but it is encouraging to see a pattern emerging.\nGenerally the net_device/virtq code is not very optimized at the moment. We had really tuned it at the LuaJIT level to get the loadgen benchmark up to ~4.5Mpps but since then we have added more features and performance has regressed. This is a big problem right now.\nSee the back-and-forth between features and performance in the Git history here: https://github.com/SnabbCo/snabbswitch/commits/master/src/lib/virtio/net_device.lua with Nikolay and I adding features / fixing bugs and llelf optimizing the LuaJIT behaviour. The code was optimised well in July until October when we rewrote much of the fast path to support more Virtio features.\nMax reports seeing only 1.7Mpps in his latest benchmark (see #307) and this may also be due to re-optimisation needed in the virtio code, because that should really run around 4.5 Mpps per snabbswitch process (and scale linearly with more parallel processes).\n. This part is interesting by the way:\n133 times recovered from packet loss by selective acknowledgements\n    170 fast retransmits\nThis shows packet loss (or corruption) that will tend to throttle the TCP transmission speed due to congestion-control logic. I would be curious to know if this happens only with heavy parallelism or also with a single iperf thread, and whether ifconfig on the other VM shows any receive errors.\nPeople have been reporting low TCP throughput due to packet loss/corruption on the mailing list lately and I would love to be able to reproduce and fix this.\n. Howdy!\nJust wanted to mention that there is some progress in the pipeline for getting high speeds with MTU=1500. Here is a recent run from Grindelwald transferring between two VMs on the same Snabb Switch and 10G NIC (i.e. each packet being processed twice as in previous tests):\n```\niperf -i 1 -t 20 -c fe80::5054:ff:fe00:1%eth0 -V\n\nClient connecting to fe80::5054:ff:fe00:1%eth0, TCP port 5001\nTCP window size: 85.0 KByte (default)\n\n[  3] local fe80::5054:ff:fe00:0 port 49500 connected with fe80::5054:ff:fe00:1 port 5001\n[ ID] Interval       Transfer     Bandwidth\n[  3]  0.0- 1.0 sec   904 MBytes  7.59 Gbits/sec\n[  3]  1.0- 2.0 sec   940 MBytes  7.88 Gbits/sec\n[  3]  2.0- 3.0 sec   955 MBytes  8.01 Gbits/sec\n[  3]  3.0- 4.0 sec   950 MBytes  7.97 Gbits/sec\n[  3]  4.0- 5.0 sec   957 MBytes  8.03 Gbits/sec\n[  3]  5.0- 6.0 sec   957 MBytes  8.03 Gbits/sec\n[  3]  6.0- 7.0 sec   937 MBytes  7.86 Gbits/sec\n[  3]  7.0- 8.0 sec   967 MBytes  8.11 Gbits/sec\n[  3]  8.0- 9.0 sec   960 MBytes  8.06 Gbits/sec\n[  3]  9.0-10.0 sec   964 MBytes  8.08 Gbits/sec\n[  3] 10.0-11.0 sec   962 MBytes  8.07 Gbits/sec\n[  3] 11.0-12.0 sec   938 MBytes  7.86 Gbits/sec\n[  3] 12.0-13.0 sec   938 MBytes  7.87 Gbits/sec\n[  3] 13.0-14.0 sec   954 MBytes  8.00 Gbits/sec\n[  3] 14.0-15.0 sec   928 MBytes  7.79 Gbits/sec\n[  3] 15.0-16.0 sec   916 MBytes  7.68 Gbits/sec\n[  3] 16.0-17.0 sec   914 MBytes  7.66 Gbits/sec\n[  3] 17.0-18.0 sec   911 MBytes  7.64 Gbits/sec\n[  3] 18.0-19.0 sec   916 MBytes  7.68 Gbits/sec\n[  3] 19.0-20.0 sec   907 MBytes  7.61 Gbits/sec\n[  3]  0.0-20.0 sec  18.3 GBytes  7.87 Gbits/sec\n```\nThe notable differences compared to previous tests are:\n- Providing hardware checksum offload to the guests (from @javierguerragiraldez's offload branch)\n- Setting shorter maximum packet ring lengths (~512 vs ~8192)\n- NIC/Virtio zero-copy mode disabled (to explore our limits without it)\nSo we are well on the way to providing the full hardware bandwidth to basic untuned Linux guests.\n. Great! Could you please rebase?\n. Could it also be that we should add some more functionality to intel10g:close() for a full clean shutdown? I'm thinking of e.g.:\n- Deref all packets on the transmit queue.\n- Free all buffers in the RX queue.\n- Put the DMA memory for TX/RX descriptor queues onto a freelist for use by the next instance.\n. Are we sure about the root problem?\nCould it be that the rx_buffer_freelist for the new app has no buffers available, because the old closed one failed to free them?\n. Hopefully close/open on a VF will be a cheap operation and we can do that instead of explicitly updating all of the individual parameters.\nClose/open on the PF is more heavy duty and I don't think we want to do that ever if we can avoid it?\n. Can you describe the root problem with VLAN changing in simple words? I can be a second pair of eyes comparing our code with the data sheet and ixgbe driver code.\n. (Is this the relevant part of ixgbe? http://lxr.free-electrons.com/source/drivers/net/ethernet/intel/ixgbe/ixgbe_common.c#L2996)\n. This should be fixed by #373.\n. This code is sensitive because it's the main performance hotspot for the NFV application. Changes here need to be checked for performance implications on the \"NFV loadgen benchmark\" before we can merge them. @eugeneia has a new version of the benchmark (#307) that I will merge now. Then we can get the key benchmark running nicely on chur and perhaps hook it into the CI too.\n. @dpino You already ran the loadgen benchmark on Grindelwald and found that it does not affect performance, right?\n. Thanks for the patch!\n. Sorry for the slow response! Reading the diff puts timers into a different perspective and I have been pondering this in the back of my head.\nSeems like we really have two kinds of timer use cases: system-wide timers and app-local timers.\nSystem-wide timers are things like:\n- Print a status report every 1 second.\n- Poll for a new config file every 10 milliseconds.\nand app-local timers are things like:\n- Perform IPv6 neighbor solicitation every one second.\n- Time out old table entries every one minute.\nThere are likely to be tens of system-wide timers and only a couple of timers per app.\nDoes this sound reasonable?\nIf so I almost wonder (ducking) if the old core.lib.timer() function would be suitable for app-local timers and the old timer module suitable for system-wide timers. (I realise that I am sounding fickle here...)\n. ... the refactored \"timer module as data structure\" will then be needed as soon as there is an app requiring a large number of timers, but until then is overkill for the minimal requirements of the apps we have today.\n. Late reply...\nAgree that apps should have their own simple timer mechanism separate from the timer module.\nI like the idea of a repeating lib.timer(). Is the mode argument actually a bit awkward? should we have separate timer and repeating_timer functions instead?\nHow should we represent time in Snabb Switch? integer nanoseconds or floating-point seconds?\n. This is actually tricky.\nWill our time value be represented as a native Lua number or an FFI number?\nNative Lua numbers are convenient because you can do arithmetic on them without creating garbage and you can use them as table keys. On the other hand, native Lua numbers are double floats, and those can only count up to 2^53 before losing precision. That is limiting because 2^53 nanoseconds is only a few months. So if we want to use native Lua numbers as integers we might have to choose microseconds as the unit instead (2^53us ~= 285 years).\nFFI number could be a uint64_t. That is precise enough to use for nanoseconds (2^64ns ~= 584 years). The downside is that we will have to be careful with time values: if you do arithmetic on time values inside an inner loop and LuaJIT's allocation sinking optimization doesn't apply for any reason then you will invoke the garbage collector. That kind of issue is a pain to debug.\nThe third possibility would be to use seconds as a double-float if it could be determined that the floating point behaviour is sane and easy to describe. Hypothetically, if time we measured time in double-float seconds since 1970 and we knew that precision would be within 10ns for the next 100 years then that might be something we could live with. (Hopefully the IEEE would develop quadruple floats before the year 2115?)\nThe Wikipedia page for double floats has some nice information but it would be nice to find a detailed discussion of exactly this problem.\nOtherwise: integer microseconds represented as a double-float? and hope that quad-floats are invented before the year 2355 or so? :-)\n. Closing this PR because I believe that a replacement version is coming.\n. Let's take a look at this again later.\nThere is another relevant development that is the idea of running a separate Lua VM for each app as @javierguerragiraldez is exploring. If we do this then it will change the constraints - it would be more awkward for timers to be global and shared.\n. (Merged other pull request)\n. Welcome to the Snabb Lab! You should be able to ssh -p 2020 root@lab1.snabb.co to create yourself an account on chur.\n. Welcome to the Snabb Lab! You should be able to ssh -p 2020 root@lab1.snabb.co to create yourself an account on chur.\n. Is a patch needed? It sounds like we get best results with 1KB buffers but that the 1KB buffer size is already automatically selected. So a patch would not make a difference?\nHere is some back story.\nThe Intel driver would like to use as much of the available buffers as it can (up to 16KB) but its hand is forced by the actual size of the buffers received. If we are using memory allocated by Snabb Switch buffer.allocate() then it will be comfortable 4KB chunks. But in the NFV application we are typically using buffers supplied by a virtual machine and we don't get to choose their size: it depends on the driver in the VM, which depends on the operating system / kernel version / DPDK / etc in use.\nSo the Intel driver does the dynamic size reduction (details in #274) to use the largest possible buffer size (N*1024) that does not risk having DMA overwrite the end of a buffer. This effectively sets the DMA size to 1024 bytes when using Virtio with a recent Linux kernel inside the guest (supplying us with crappy small buffers).\nI suspect the bad performance cases you see here are due to memory corruption. If you force the DMA size to be larger than the buffers available then you will wind up overwriting the ends of the buffers, which will probably kill throughput amongst other things.\nThe reason this is tricky, by the way, is that we are attempting to DMA directly into guests. The alternative would be to DMA into Snabb Switch's own memory (e.g. 4KB chunks) and then copy into the guest buffers when needed. If we could make this performant then it would be a major simplification of our code because the zero-copy is design extremely intricate. The simplest way to try non-zero-copy mode should be to modify lib.nfv.config to skip the set_rx_buffer_freelist() call that tells the Intel driver to allocate its buffers from a dedicated freelist that it gets from the virtio app (containing buffers that are already inside the guest).\nLong reply. That make any sense to you? :-)\n. Couple more notes:\nThe small packets (arp, etc) issue should be a red herring: these are empty buffers being allocated to receive yet-unknown packets into, so they could reasonably have a uniform size.\nThe choice of buffer size by Linux-based virtual machines seems a little mysterious and moving-targetly to me. I would much prefer to have larger buffers. If it were the case that they would (say) start with small buffers but then switch to larger ones then it would be worth extending our Intel driver to also scale up the DMA size when this can safely be done i.e. to a value that we know is <= the size of all buffers in the receive DMA queue at a given time.\n. Great catch!\nI really wish the kernel would consistently give us buffers of at least 2060 bytes. Then we could DMA 2048 bytes and still have space for the 12-bytes of Virtio metadata. There are many factors that make it hard to do zero-copy with Virtio (i.e. hardware DMA directly into the buffers obtained over Virtio) and it would be wonderful to interest the community in making this work smoother.\n. Good idea, thanks for the patch!\n. Welcome to the Snabb Lab! You should be able to ssh -p 2020 root@lab1.snabb.co to create yourself an account on chur.\n. The NICs are dual port cards and the ports of each card are\ncabled together. So PCI address 0000:xx:00.{0,1} are connected to each\nother.\n(The NICs also have a hardware loop back mode that ca be handy for testing\n\"real\" traffic even with only one port and unknown cabling.)\nOn Monday, December 1, 2014, Kristian Larsson notifications@github.com\nwrote:\n\nThanks. Are the NICs wired in some certain topology? Couldn't find info on\nit at the wiki or elsewhere. How do you typically run tests on chur?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/SnabbCo/snabbswitch/pull/326#issuecomment-65080602.\n. This is very good!\n\n(Did I really miss that you opened this a month ago? Sorry!)\n. Fantastic and awe-inspiring hacking :-)\nGreat documentation too!\nJust one question: Do we have some more places in the code that need to be aware of the per-iovec copy-on-write semantics?\nSnabbBot's complaint is spurious. Don't worry. (See https://groups.google.com/forum/#!topic/snabb-devel/3wvmBph3EAA.)\n. SnabbBot test failure?\n. Great start :-)\nError handling seems a little inconsistent:\n`\nSnabb> unbound\nSnabb> =unbound\nError in [string \"return unbound\"]:1: variable 'unbound' is not declared\nwould be nice to get a backtrace in both cases.\nI wonder if we could also run the REPL \"in the background\" e.g. on a timer that polls stdin for input every 10ms? (If we didn't print the prompt until after the first command then you wouldn't even know the REPL was there until you started to use it by typing. Simple way to make it always available?)\nCool stuff :)\n```\n. How do you measure the performance penalty? I would like to fix the root problem with dead-app-detection if possible rather than conditionalize it out. (I see it as an important basic feature: I want to use it and I don't want to pay a significant cost to do so.)\nThe timer change looks great. Could send that separately?\n. I take your point and definitely agree with merging fixes that avoid JIT problems.\nI would suggest not modifying the options table that is passed as an argument, just in case the caller wants to use it for something else too and is surprised when it changes.\n. Great analysis!!\n. Looks like a valuable activity to me. I'm really happy to see some editing to gradually bring our varied and experimental coding styles closer together.\n@alexandergall what do you think? you wrote most of the affected code.\n. Yeah. It would sure be nice to do away with those require lines entirely. Perhaps we could cook up a simple convention for giving the app name as a string and for config.lua to infer an app name.\nOne step at a time.. :-)\n. Welcome! You should be able to access chur in the lab now with ssh -p 2020 root@lab1.snabb.co and then create your own user account.\n. SnabbBot has caught a real bug here: undeclared global variable.\n. @eugeneia SnabbBot has failed this and all other open pull requests. How can we resolve this and get the working changes merged?\n. NB: We already have a function lib.hardware.pci.unbind_device_from_linux() that will free a device from the ixgbe driver. We should call that from some appropriate place so that it happens for the Intel app.\n. @eugeneia Yes, I think the intel_app should do the unbinding, if not the pci library. I don't think we removed this for any very good reason. Could have been collateral damage when I removed the VFIO code.\n. This is great work! both the code itself and the Git-fu to do a multi-person topic branch :-)\nThe only problem I have is that this pull request adds a bunch of lines of commented-out code without explanation. Can we fix that somehow e.g. remove those lines?\n. To me there should be an additional step after swinging code around with trial and error, etc, which is to say \"okay, now that it works, what are the minimum changes needed to keep it working?\". Then we work out what is essential to the solution and should be kept vs. what is incidental and should be removed (or taken separately).\nI don't want to feel like I am pulling code that is just the contents of the working directory at the time something started working. Then when something else stops working it will be really hard to diagnose from the history.\n. Awesome!\n@n-nikolaev is the one field-testing the changes related to the Neutron code. We should probably find a nice workflow for him to test and review these changes and push them to me. (Currently I merge them onto master and then they land in his lap by surprise when he does a git pull :)).\n. This is a very pleasingly short implementation :-)\nTests covering the interesting cases (including filling up tables) would be great.\nI will think about whether the Neutron front-end would have any more requirements on the API. I don't think of anything immediately.\n. @eugeneia Do you have an idea for how we should integrate this into the Neutron config? Goal is to be able to selectively enable stateful filtering e.g. on a per-port basis.\n. @eugeneia That is probably hard compared with extending one of the JSON fields. (Risk is that we have to update the OpenStack database schema, get mixed into their ORM, and slog for years to sync with upstream.)\n. @eugeneia what test coverage do you think we need for this in the NFV context?\nI'm thinking something like:\n1. State table initially empty - connections can be made with e.g. telnet/ssh/iperf.\n2. Fill up the connection table e.g. with nmap.\n3. New connections cannot be made anymore.\n4. Existing connections keep working.\nand to get some performance visibility with iperf both with table quite full and table quite empty.\n. Sorry, my explanation is not a good match for the code, and maybe I am really suggesting requirements for the implementation that I should have written down earlier :-). My bad!\nQuestions:\n- How many connections can the table track in parallel?\n- How are connections beyond that limit treated?\n- How does performance scale as more connections are added, up to and above the limit?\nA reasonable answer might be: The table tracks up to 1000 connections. Once the table is full, no new connections are allowed until an existing entry times out. Performance is similar for all table sizes: you always get >= 9 Gbps in iperf. A denial of service attack spamming random ports will temporarily block new connections but will not consume especially much CPU or memory.\nA problematic answer would be: The table size has no fixed limit. Connections are added without bound. Performance of the Snabb Switch process will degrade as the Lua garbage collector is invoked and swap memory is needed. A denial of service attack will cause the Linux \"Out Of Memory Killer\" to be invoked.\nFor current purposes it is more important to be stable and predictable than to be very fast or scalable. The main use case for stateful filtering at the moment is on legacy management ports that may be carelessly exposing themselves to attack in some unpredictable way. (Like when chur was hacked via the Supermicro management port that was reachable via the internet and woefully insecure.)\n. Short term it seems like the simplest solution would be to stick with Lua tables but have a counter to keep them small? And have a test case for simple DoS?\n. Agree that sooner or later we will want an FFI table that handles large datasets gracefully. I've used Judy arrays in the past but they are not very Snabbish (big and complex). I don't think we need this acutely yet though.\n. Current requirement seems modest to me: between 100 - 10,000 entries would be adequate for protecting a management port. Overload behavior (if spammed with legitimate connection requests) would be to deny new connections without slowing down the process (i.e. affecting other ports or previously accepted connections).\nFuture requirements will be tougher. If we were talking about NAT/firewall for all traffic on an ISP we might be talking more about more like 1M sessions per gigabit of traffic. I'd prefer to cross that bridge when we come to it though rather than trying to anticipate future needs without a realistic test case.\n. (Tracking only outgoing might be reasonable in this case. I do think we need predicable overload behavior in that case too though e.g. in case somebody innocently runs nmap.)\nGenerally for NFV I reckon that stateless filtering is more practical. The main value of stateful filtering now is to support deploying e.g. management ports that you have zero confidence in security-wise e.g. could have a telnet server running in the ephemeral port range or something nuts like that.\nMake sense?\n. I would like to get a test case up and running that shows us where we stand e.g. run background traffic with randomized addresses and see what happens.\nI would not be surprised if the lookup operation itself will be a bottleneck due to allocating interned strings for hashtable keys. (I'd wager a beer that LuaJIT will not sink those allocations.) Then we might need to switch to an FFI table immediately. But speculating about performance is a dangerous habit...\n. Nice one!\nThe SnabbBot CI system says that tests failed but this seems to have been an error on the CI. make and make test work for me on chur.\nI made one comment: would you like to change that or merge as-is?\n. Merged!\nHistorically I have asked for commits to be rebased before merge with fixups squashed. However, I spent this weekend deeply geeking out on Git workflows and I am changing my mind. On the one hand a rebase/squash will make the history shorter but on the other hand it won't actually be the history any more and we lose context such as this discussion. (I'll post to snabb-devel with latest Git workflow ideas for Snabb Switch.)\n. I am flexible. If somebody prefers to close a pull request, rebase onto a new branch, and send a new cleaner pull request, that is absolutely fine with me.\nOn the other hand, rebasing does destroy context, and an authentic history will contain lots of mistakes.\nHow about if we try to model our history on the Linux kernel and try to follow Linus's tips:\nhttp://www.mail-archive.com/dri-devel@lists.sourceforge.net/msg39091.html\n. @n-nikolaev is it correct that you can leave both loadgen and the guest running and only restart the Snabb Switch process? (I mean: it will be happy to keep connecting to the same loadgen and guest?)\nI have been restarting the guest in my tests but that may be overkill.\n. Straightline branch was successfully optimized to the level of master before merge.\n. FWIW: To me it seems entirely possible that the link name issue is really nothing to do with link names but something more nebulous e.g. use of uninitialized memory that is subtly influenced. I would not be surprised if there are 100 different things that can be done to provoke the good/bad behaviour. (I had previously seen significantly lower performance when enabling profiling: could be that this was the same issue and actually nothing to do with profiling directly.)\n. FYI: I created accounts for you guys on interlaken. I don't see any urgent need to move from grindelwald but it is there if you want to use it.\nThe accounts should be equivalent to your grindelwald accounts. If you want to create an account for a colleague then please go ahead. To login ssh -p 2030 lab1.snabb.co.\n. @dpino is this issue resolved now? (That is: does it represent something we need to fix on master?)\n. Thanks for measuring!\n. This looks completely awesome :-)\n. I am a little scared of locks and especially fine-grained locks. The model we have now keeps the traffic process independent of the monitoring tools. Locks would allow the monitors to screw up the traffic.\nHow about if:\n- Traffic process is independent of monitoring tools (never knows if they are running or not).\n- Monitoring tools can always access counters instantly.\n- Monitoring tools see a consistent snapshot that is refresh only when they want.\nThat would be a great solution for our current use cases, right?\nThis could be done via the filesystem. The traffic process updates counters in its own private address space and then periodically writes snapshots to the filesystem (e.g. ramdisk). Each snapshot atomically replaces the old one (mv counter.dat.new counter.dat). Monitoring tools open a counter.dat snapshot at any time and can read it for as long as they want. Monitoring tools refresh counters by closing the file descriptor they have and opening the latest file (on the same filename).\nWhat do you think?\n. I'd shoot for a 1ms update interval, and settle for 10ms or 100ms if that turned out to make a difference.\nTo write a snapshot should be quite fast: open(), write(), close(), link(). This is comparable to the work that Apache does to serve a HTTP request.\n. NB: I suppose I mean rename() rather than link().\n. FYI: Quick benchmark suggests that the stat file on tmpfs can be rewritten around 10K times per second. That is 100us mean time and with uncertain variance. That seems a little too expensive to do on the thread that is processing packets, and to introduce a background thread seems excessively complex.\nHere is the test for posterity anyway:\n```\nluke@interlaken:~$ cat filetest.c\ninclude \ninclude \ninclude \ninclude \ninclude \ninclude \nifndef RUNS\ndefine RUNS 10000\nendif\nifndef BYTES\ndefine BYTES 8192\nendif\nint main(int argc, char argv) {\n  int i;\n  char data;\n  data = (char)malloc(BYTES);\n  assert(data != NULL);\n  for (i = 0; i < RUNS; i++) {\n    int fd;\n    fd = open(\"/tmp/stat.new\", O_CREAT|O_WRONLY);\n    assert(fd >= 0);\n    assert(write(fd, data, BYTES) == BYTES);\n    close(fd);\n    assert(rename(\"/tmp/stat.new\", \"/tmp/stat\") == 0);\n  }\n}\nluke@interlaken:~$ gcc -o filetest filetest.c && sudo time ./filetest\n0.02user 0.49system 0:01.02elapsed 50%CPU (0avgtext+0avgdata 460maxresident)k\n0inputs+160000outputs (0major+155minor)pagefaults 0swaps\n``\n. Here is another idea based on Javier's double-buffer-with-counter idea above:\n- Counter file has auint64_t serialnofield initialised to 0.\n- Traffic process uses a double-buffer: writes counters in private memory and periodically syncs to shared memory withmemcpy()`.\n- Sync is in three steps: increment serial number; memcpy; increment serial number.\nThe consumer can then choose to do either a simple non-atomic read or a more complex atomic read.\natomic read algorithm:\n- Wait until serial number is even. (No update in progress.)\n- Read value(s).\n- Check if serial number is the same: if yes then success, if no then -EAGAIN.\n. If none of these schemes really seems ideal than I would agree that it is better to keep it simple and forget about atomic reads for now :).\n. ARGH!\nWho knew that /tmp is not mounted on tmpfs on ubuntu? The measurement of 100us update time above is for a normal file system on a spinning disk.\nTesting on /run/ instead (which is tmpfs) looks much nicer: I can do 150K updates in one second i.e. 6.7us per update. That sounds like something that one could realistically do e.g. once every 10ms.\nLet me humbly repurpose that solution for your consideration :-)\n. I confess that I have gotten carried away :-).\nHow about if we keep the code as you have it without having atomic reads of compound datatypes if you don't need that.\nIf we need atomic reads in the future we have a plausible simple solution with tmpfs snapshot files.\n. Merged. Even if there is an issue with the counter interface we can fix that when we add code that uses it.\n. Sorry to keep you waiting! you are in.\n. Howdy!\nSorry about the wait, I had thought I added your key long ago.\nYou should be good to go now: ssh -p 2020 root@lab1.snabb.co and then create an account as described on the [[Snabb Lab]] wiki page.\n. This seems to be a client-side issue. Can you try googling the error message and looking for a solution that suits your setup?\nHere is one relevant link: https://help.github.com/articles/error-agent-admitted-failure-to-sign/\n. This should be fixed by #373.\n. Howdy! Sorry about the SLOW response! Could you please rebase (or merge from master) so that I can merge it cleanly? Thanks !\n. Looks promising! Do you see why SnabbBot failed this on the Intel selftest?\n. Test case is looking really good with so many iterations!\nI tested this branch (on interlaken) though and I see the same error as SnabbBot.\n. Merged to master. Let's see what feedback there is when putting this to work in more programs.\n. This is awesome work! Let me kick the straightline branch a bit before digging into this :)\n. Newbie question: In the brave new world of YANG are the IF-MIB objects still used? I mean: should we expect to support SNMP and YANG with the same backend or will they be separate? cc @plajjan \n. Looks like the YANG models are taking pains to be compatible with the SNMP models, and so we can reasonably expect our SNMP support to gracefully extend to YANG in the future?\nFor example, the instrumentation that this pull requests adds to the driver would supply the statistics information to make both models happy? (I ask for the sake of my own education. I am perfectly happy to merge SNMP and YANG support separately.)\nI have more questions for you guys :-). My experience with SNMP etc has mostly been implementing MIBs and I have always suspected that most objects that I lovingly populate will never be used by anybody. Please enlighten me to what is actually interesting and important as a network operator from:\n1. Physical network interface statistics.\n2. Sub-network interface statistics (SR-IOV, VMDq, VRF, VLAN, etc).\n3. Virtual network interface statistics (Virtio-net interfaces to virtual machines).\n4. Relationships/heirarchy between network interfaces (ifStackTable)\n5. Snabb Switch app network internal interface statistics (links)\n6. Other common MIB-II objects\n7. Other objects entirely?\n. Thanks for the great input!\n. Alex, what do you think about moving this SNMP instrumentation from the intel10g object into the link object?\nThe link object would need to track more state (admin/operational status) and the apps/drivers would have to supply this. The benefit would be that we could monitor any link with SNMP.\n(I'd like to add this SNMP monitoring support to the Virtio-net interfaces for NFV too.)\n. Relevant news: Cisco have released Tail-f ConfD for free-as-in-beer now. (Looks like the free version excludes SNMP.)\nI have used ConfD in the past. You can write a YANG model and ConfD will handle all CLI/SNMP/NETCONF/etc for you. There are multiple ways to integrate the management plane with the traffic plane, including a really simple batch import/export one that I like.\nSomething for the future?\n. I will ponder too. Can be that it is the wrong solution and that it is better to explicitly register interfaces. I'll look at the most straightforward way to tie in Virtio interfaces.\n. I have pushed substantial updates and I hope to merge this onto master soon. (The changes touch so many files that it is a challenge to keep this branch compatible with master. I did a large and necessary merge from master today.)\nPlease review @eugeneia and anybody else who is willing and interested! Here is what is new today:\nThe whole designs/ directory is gone. The scripts have all been refactored into modules in programs/. (This includes the Neutron shell scripts!) See commit 49cadd6.\nMakefile now automatically embeds files named *.inc into the snabb binary. (The Neutron shell scripts are bundled this way and so are the READMEs containing usage information.) See 3b8d459.\nNFV related files have been refactored to live under programs/snabbnfv/ instead of more general locations (lib/nfv/ and test_fixtures/).\nThe vhost-user client app has been removed. This was bit-rotten and its C declarations of Linux ioctl data structures clash with ljsyscall. The app should be revived on top of ljsyscall. See commit e49a292.\nDocumentation structure is modified. As before, the public API code (core lib apps) is documented for Lua developers. However, the application code (program.nfv program.snsh ...) is now documented for end-users, in the style of man pages. This is proposed as policy: the programs are for non-programmer end-users while the APIs are for programmers writing programs. This implies that we should revise the program syntax (e.g. NFV config file) to be less Lua-centric (mea culpa). (I may have also lost some useful documentation e.g. from the NFV modules by not finding a suitable place to put it under this model.)\nQuestion and comments welcome! (we can also throw this whole branch away if it is really a bad idea.)\n. Thank you for the great question!\nYou should now be able to decide how generic your binary is. The extremes are packetblaster that does one narrow task and snsh that is the same full-blown Lua scripting interface that we have always had. You could ship snsh and do the rest with scripts if you want to. You could also create something medium-generic e.g. a new front-end that reads a YANG-based config file and translates to Lua internally. (That is kind-of how the snabbnfv command works except with YANG.)\nYou can still run your old scripts/designs with snsh <script> or snabb snsh <script> so there is no loss of functionality in this new style. The command-line syntax is not backwards compatible though. (Maybe we should deprecate the snabb <script> syntax instead of removing it? I'm not sure.)\n. Here is another idea:\nWe could automatically increase the kernel shmmax setting and print a message to say that we are doing so. (Kind of like the way we automatically provision new huge pages.) If we are worried about users disliking this then we could add a --no-change-kernel-settings argument to print an error in these cases instead of automatically correcting them.\nI note that on Ubuntu 15.04 the limit seems to be increased to essentially infinity:\n$ sudo sysctl -a | grep shmmax\nkernel.shmmax = 18446744073692774399\n. @eugeneia I think this test failure is due to SnabbBot running an older version of LuaJIT than the one reference in our deps/luajit submodule. Can you check? (Is there a special step needed for SnabbBot and developers in general to pick up the latest LuaJIT? If so I wonder if we can solve that in the Makefile?)\nThis is the tell-tale error that should mean out-of-date LuaJIT:\nInvalid DMA address: 0x500438aedf00\ncore/memory.lua:95: DMA address tag check failed\n. Guys, how is intel10g reliability for you now on master?\nI am seeing diverse errors and warnings when I run in a loop: never got link up, sq_sq: missing packets, mq_sq: wrong proprtion, transmit counter did not increase. So to me it seems like we currently have a high-priority bug that NIC initialization is not dependable?\n. Here is output of 100 runs on master for me, with the new Interlake server: https://gist.github.com/lukego/bda37923f504e0c4f71f\nCould be that I broke something in merge too.. whatever the case, we need to put this problem behind us quickly.\n. Closing this issue because the fix from Javier works very well for me. On interlaken I tested each of the four NICs 20 times and had no failures. (Previously I had many.)\nAwesome if that is the end of it!\nPlease report any suspicious NIC failures ocuring after commit 0f98bd6. cc @javierguerragiraldez @n-nikolaev @dpino @eugeneia \n. done! you shuold be able to login to chur with ssh -p 2020 root@lab1.snabb.co and create yourself an account.\n. Great work!\nI ran this in a loop for over an hour on interlaken, cycling between NICs, and it worked every time.\nOne day when we have spare time I would love to review our init vs. the Intel ixgbe driver and also try to optimize the selftest down to ~ 10 seconds. However, reliability is what really matters. Great work!\n. Great summary!\nThe Neutron rules are actually stateful even though they don't spell this out. When they talk about \"inbound traffic\" they are talking about two-way connections that originate with an inbound packet.\nI agree that sticking as close to the standard Neutron API as possible makes sense. So the default behaviour should be stateful filtering. Then we should add a vif_details option to request stateless filtering. (And we could keep it in our minds that we probably want to add the ability to use pcap/pflua filter expressions instead of security group rules in the future.)\nThe three use cases we want to cover are:\n1. No packet filtering.\n2. Neutron security groups without stateful filtering.\n3. Neutron security groups with stateful filtering.\nBeyond that it is more important to keep the Neutron interface simple than to expose additional capabilities of our packet filter.\nAside: I am bothered that Neutron default port settings will enable stateful security groups since this is usually not what you want in NFV. (Taking a high-throughput router/firewall/NAT and wrapping it in an additional layer of connection tracking would be madness.) However, I think I need to live with this and that it is better to explain our recommended configuration methods instead of changing the Neutron API semantics.\n@n-nikolaev has written an end-to-end OpenStack test suite for our API extensions that will need to be updated too.\n. This sounds like the right model for us: one state table per virtio port i.e. per pair of ingress/egress packet filter apps.\n(Our Neutron docs will have to spell out the fact that state is tracked individually per virtio port, just in case there is ever a situation where that presents a potential problem for somebody.)\n. I added a Stateless Packet Filtering section to our Neutron API Extensions document.\n@eugeneia Does this look okay to you? (If yes then can you hack neutron2snabb and nfvconfig etc to make it so?)\n. Closing this issue as resolved.\nThe Neutron API Extensions page documents our packet filtering behavior. The default behavior is stateful filtering and no special explanation of that is required because it is the same as the normal Neutron behavior (not an extension). The stateless mode is optional and is documented as an extension.\n. Closing this one.\nThe patch to redirect snabbnfv traffic output is ill-conceived:\n1. The documentation says that the output will be logged to SNABB_LOG0 and the patch makes this false (logging to a hardcoded location instead).\n2. Consequence of the above: NFV selftest fails to locate the traffic log file and that causes it to think that the tunnel test case fails (because it doesn't find the message indicating success).\n. Cool :-)\nYes, it seems we don't really have a nice synthetic benchmarking setup for taking meaningful measurements of app performance in isolation. I would like to expand programs/snabbmark to serve this goal one day.\nOn the other hand, microbenchmarks are frequently misleading anyway. The best way to benchmark this could be to use iperf between virtual machines and check for an impact on end-to-end throughput. The numbers will be preliminary anyway because this code is based on SSE instructions and we may be able to get double speed by porting it to AVX2 for Haswell (interlaken) and then double again porting it to AVX512 for Intel's next CPU.\nRegarding the API, yes I think these routines are not exactly matched to the way Virtio-net does checksum offload. The routine expects any pseudo-checksum to be passed as a parameter (initial) but Virtio-net does this implicitly by storing the value in the checksum field of the packet (instead of 0). The effect is the same. This should be resolved in \"some nice way\". (To start with it should be enough to change the routine to not clear the checksum field.)\nZooming out a little bit, now is the time to decide and document the exact semantics of checksum offload metadata in struct packet. We have two flags (PACKET_NEEDS_CSUM and PACKET_CSUM_VALID) and two numbers (csum_start and csum_offset). What do they mean and what do Snabb Switch users (people building app networks, people developing apps, etc) need to know about them?\nHere is a suggestion for PACKET_NEEDS_CSUM:\n- PACKET_NEEDS_CSUM means that one checksum operation is needed before transmission: the ones-complement checksum from csum_start until the end of the packet needs to be calculated and then stored at csum_offset.\n- All apps that transmit packets externally need to take this flag into account. The simplest way to do this is to call a finish_packet() library function that finishes the checksum with SIMD if needed. The alternative is to use some offload that is available to that specific app e.g. based on a NIC or Virtio-net.\nand for PACKET_CSUM_VALID:\n- PACKET_CSUM_VALID is an optional hint to say that certain checksums are already known to be correct.\n- The headers this refers to would initially be defined as exactly the same ones that Virtio-net uses. The exact details need to be checked, but approximately the outermost IP and TCP/UDP headers.\n- All apps that modify packet payload need to be aware of this flag. The simplest way is to clear the PACKET_CSUM_VALID flag when you modify the packet. The alternative is to make sure that this flag is true when set e.g. by calling checksum_update_incremental_32() to incrementally update the checksum when modifying one field of packet payload.\nIn both cases the concept of checksum offload would be \"under the hood\". You need to think about it when writing an app to make sure you respect the flags. You don't need to think about it when writing an app network because you depend on all apps to handle this internally. So the code from the Checksum app would be refactored into a library routine that is called by all apps that need it.\n. Here is yet another possible design for checksum offload in Snabb Switch:\nSuppose that we removed all of the checksum offload metadata from struct packet. That is: packets moving around in Snabb Switch should already be checksummed and there is no native concept of offloading this.\nThen apps don't have to think about checksum offload, but they do have to respect checksums when modifying packet data.\nThe Virtio-net app could still provide checksum offload from the VM perspective. Packets from the VM could be checksummed by the Virtio-net app before being sent onto the app network. Packets going to the VM could also be checksummed by the Virtio-net app and the result would be provided to the VM in the virtio_net_hdr struct.\nThis seems like an appealingly simple design that makes our struct packet even more streamlined. The question is whether this would require more checksum work and how expensive that would really be. Hard to say when it would be practical: Always? with AVX2 on Haswell? with AVX512 on future CPUs?\n. For the sake of experiment I made an AVX2 version of the checksum routine and did a simple microbenchmark of cycles-per-packet for various \"packet sizes\" (really just an array). I compared the AVX2 code with the SSE code and with a reference implementation (from DPDK).\nHere are the very preliminary results in cycles per packet:\n| PKTSIZE | REF | SSE | AVX2 |\n| --- | --- | --- | --- |\n| 64 | 26 | 37 | 39 |\n| 100 | 39 | 49 | 47 |\n| 256 | 83 | 65 | 53 |\n| 300 | 128 | 82 | 67 |\n| 512 | 182 | 123 | 76 |\n| 600 | 212 | 130 | 100 |\n| 1000 | 347 | 210 | 135 |\n| 1500 | 529 | 296 | 189 |\nThis was run on Interlaken (Haswell) and compiled with gcc-4.9 -O2 with cycles measured by perf stat.\nThoughts:\n- The cost of checksum seems to be significant. It's not trivial but it's not massive either.\n- Checksum may be realistic to do on the CPU. Especially so if this code can be further optimized and if Intel SIMD performance continues to improve as expected.\n- (Checksum offload is not free either because the hardware makes us parse the packet, create an extra descriptor, jump through hoops.)\n- If bulk checksum is faster we could consider using segmentation offload to move larger chunks of data between VMs. This could potentially also be done in software and kept isolated in the Virtio-net code. (I don't suggest this now, but only observe that we will still have plenty of opportunities for further optimization within the simple design.)\nI'll clean up the AVX2 port and submit it.\nStandard disclaimer: This was a quick microbenchmark so the quality of the results is not very certain. However, I think that end-to-end benchmarks like iperf and packetblaster are more interesting for us anyway.\n. You know @javierguerragiraldez, I am really warming up to the idea of deleting the checksum offload flags/fields struct packet and doing the whole thing locally in the Virtio-net module. If the guest sends us a packet that needs offload then we do that before putting it onto the app network. On the receive path we could either let the guest do the checksum or we could do that in the Virtio-net module and pass them the result.\nThis means one software checksum calculation on the VM->network path and potentially one on the network->VM path. So we have to measure the performance and potentially optimize the checksum code more.\nThe checksum code will be a bit tricky but it will make struct packet even simpler and that should help app developers.\nWhat do you think, worth a try?\n. (And of course if the performance is not competitive with the hardware checksum then we can stick with that. This would be an experiment that might simplify the code a lot or might be a dead-end.)\n. I pushed an update that automatically selects between x86, SSE2, AVX2 based on the running CPU. For best results test on Interlaken which is the only lab server with the AVX2 CPU feature.\nThe API could possibly be improved e.g. to take a parameter saying where to store the checksum as the original did. also tip: if checksum doesn't work then check the endianness :).\n. Cool!\nLet's focus on 1500-byte frames for now. That is the more important case. (Slow code can easily look fast with jumbo frames.)\n. Code looks very good! I hope that approach pans out :-)\nIs the current bottleneck Snabb or the guest? (Guest has to calculate its own checksums on received packets on this branch. Curious to know whether throughput is better if you tell the guest not to do that work by setting PACKET_CSUM_VALID.)\n. @eugeneia SnabbBot failure?\n. Cool! Let us get to the bottom of this.\nI am guessing the problem will ultimately be failure to allocate a HugeTLB page. Snabb Switch and QEMU both need these and they are allocated from per-NUMA-node pools.\nDo you have any of this information?\n1. QEMU or Snabb Switch is the process causing test to fail?\n2. How many huge pages are available on each node? (Should be in a /proc or /sys file.)\n3. Does the Snabb core.memory selftest pass on both nodes (selecting with taskset)? That will allocate huge pages.\n4. If the Snabb process is failing, does strace show a system call causing this? (Likely a shm call from memory.c)\nOr I could be barking up the wrong tree entirely and maybe it is about the relationship between the node of Snabb Switch and the node of the NIC and/or QEMU.\n. Cool problem.\nCould be NIC related but the evidence seems weak to me. \"PING failed\" is also what you see if the VM fails to boot, right? Also, we have tested various NUMA combinations between NIC/Snabb/QEMU and never seen any bugs of this kind before, the worst case has been ~33% performance impact. So it is possible but I would like to audit logs and eliminate other possible causes too.\nThinking of the future: nice if SnabbBot would attach more log files to the gist. I would quite like to see the output of the snabb process, the two QEMU processes on the host, and the ping/iperf/etc processes inside the VMs.\nI still suspect the issue is related to hugetlb allocation. Here are more thoughts:\n1. Which processes does the taskset apply to? Is it forcing everything (Snabb Switch + both QEMU) to run on a single core? (If so that is probably not what we want... Could use numactl -N instead to pick a node instead of a core, but then probably need to remove isolcpus kernel parameter because I don't think numactl plays well with that.)\n2. How many HugeTLB pages are available on each node? (How many are reserved by grub kernel parameter? How many are actually allocated now?)\n3. How many HugeTLB pages are required for the test? In the strace above it looks like 2MB page size is used and that Snabb Switch is allocating around 16 pages. The QEMU VMs will use HugeTLB pages for guest memory and if that is (say) 1GB then the VMs would need 512 pages each. So estimate of our HugeTLB requirement is around 1100 pages based on those assumptions.\n. Check this out:\n$ cat /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages\n1024\n$ cat /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages\n1024\nLooks like each NUMA node has 1024 hugepages available. QEMU 1 wants 512, QEMU 2 wants 512, Snabb Switch wants ~16... there won't be enough if all three processs are running on the same node.\nCan you try this?\necho 8192 | sudo tee /proc/sys/vm/nr_hugepages\nThen hopefully we have 4096 huge pages (=8GB) available on each node.\nCould also edit the grub config to add this kernel parameter:\nhugepages=8192\nso that it happens automatically on boot. (Can be that the kernel will fail to allocate hugepages long after boot due to fragmentation and not being able to find enough contiguous regions.)\n. Thanks for checking on the hugepages. I'm surprised that was not the issue.\nOn interlaken I can run snabbnfv on a different node to the NIC  and guests can connect to that switch and ping each other. So it is not a black-and-white issue of the traffic process having to be on the same node as the NIC.\nHow about the intel_app selftest, does that work on both nodes for the same NIC? (Can we reproduce this problem in a simpler way?)\nI'll see if I wake up with new ideas..\n. Awesome :-)\nCan you show me how a \"NUMA failure\" looks at this verbosity level?\n. (If QEMU would crash would we see the output anywhere?)\n. Nice catch! :-)\n. Thanks for reporting! I have made a new issue (#405) to track this.\n. @n-nikolaev How do these changes look to you?\n. Great!\nre: rebase/squash, I actually would prefer to preserve the history in this case. There is a lot of interesting code buried in there from the more complex pre-straightline versions and maybe somebody will find it valuable to go digging through that one day when implementing something new for this NIC.\nThe PCI module is becoming more useful now that we have multiple NIC apps that are interchangable at least for certain applications.\n@eugeneia what do you think about the integration to the pci module and the SnabbBot fail? and what if any changes do you think we need to make in other layers of the code e.g. documentation?\n. The CI is running make test (really: make test_ci) with an environment that makes an Intel NIC available.\nYou can do for example:\n```\nexport SNABB_TEST_INTEL10G_PCIDEVA=0000:XX:00.0\nexport SNABB_TEST_INTEL10G_PCIDEVB=0000:XX:00.1\nmake test\n```\nwith XX replaced with the ID of an Intel NIC.\n. Better to run them locally. Can you get that working? If those pass then SnabbBot will also be happy.\nSnabbBot will eventually test newly pushed changes but @eugeneia slaps us on the wrist if we try to use it as a substitute for local private testing :-).\n. My own views on rebasing are evolving. Broadly speaking, I used to be for rebasing and now I am against it.\nLinus said it best: clean history should be (a) clean and (b) history.\nIf we rebase routinely then we create a couple of problems:\n1. We have a fictionalized history that does not show when work was done, what version(s) it was tested against, and so on.\n2. We can't pull from each others' trees without making a mess and so we end up with a centralized workflow instead of a distributed one.\nI suspect that we should stop rebasing branches after we publish them on github/PR. (To keep history clean we should consider rebasing them immediately before publishing them, to make sure all the changes make sense and have excellent commit messages.) If something is really screwed up then we can create a new branch and rebase onto that, but this should be the exception. If the changes are just messy then, well, that is history, and we should either live with it or adjust our development habits.\nIt seems to me like the reason that rebasing is solving test problems is that we have had regressions on our default branch that people base changes on. I see a couple of potential solutions to this. First to be more careful to avoid letting in regressions, which we are already doing. Second we could perhaps use a separate branch for integrating and debugging changes and keep the default/master branch more stable.\n(End braindump.)\n. Let me see if I understand the options. (Git workflows are complex with long-lived branches.)\n- Squashing. This makes things easy to review by compressing all the history into one \"Added Solarflare driver\" commit. It also discards information that only exists in history, for example code that drives the Solarflare scatter-gather I/O interface that was only used in pre-straightline ancestors of the current code.\n- Rebasing: This eliminates the merge commits so the history will only contain code directly related to adding the Solarflare driver. The history could still be available for browsing in git log but you could not check it out and run it anymore because the older versions of the driver really do depend on older versions of master (e.g. pre-straightline, pre-program).\n- Keeping: If we keep the history as it is now then it is more work to review and merge, and it will look more convoluted in git log, but you could check out the older versions of the driver and they would compile and run the same way as when they were developed.\nIs that a reasonable overview of how these things work with git?\nGenerally we need to work out some more details of our suggested git workflow:\n- When should I pull from other branches?\n- When should I rebase?\n- When should I send a pull request and to which branch?\nI want to have a truly distributed workflow like the Linux kernel has. I want master to be \"just another branch\" and not a global bottleneck for all review and merge work. This stuff is hard :).\n. @plajjan I see what you mean!\nSo we could squash together several sub-series of commits to shorten the history into fewer better-defined points and still expect them to behave the same as they always did.\nQuestion then is which series to squash? And do we have to be careful with the merge commits somehow or can we squash those at will too?\nWould a reasonable starting point be to plan on squashing the commits between merges from upstream? Then those could be split up a little if it makes logical sense and if we are lucky we could squash away some unnecessary merges (e.g. when there are two consecutive merge commits)?\n. @plajjan @eugeneia Further thought: we are all saying \"rebase\" but we are talking about different things, right? (\"Rebase\" is a very versatile verb.. like to \"emacs\" a branch it can mean almost anything.)\nKristian is talking about reducing the number of commits on this branch by coalescing them. The effect of this is a shorter history, which mostly affects how things will look in git log.\nMax is talking about eliminating the merges so that the commits on this branch all target the same version of master. The effect of this is to make it look like the code was all written for the current version of master, which will make the older commits look nonsensical e.g. code that uses APIs that don't exist anymore. If we take this route then it seems like the history will be a negative thing to have around (\"how the heck did this ever work?\" - it didn't) and we would be better off squashing the history and discarding the code that would never have existed in our new historical narrative.\nI am sure I am missing multiple important things. Improvements to my world view always welcome :).\n. (Incidentally my favourite source of history in the world is novels by Robert Graves and Gore Vidal so I do appreciate the value of a spiced up historical narrative with no more of the truth than is needed :-))\n. Let us merge this change in two steps:\nHans -> Max -> Luke\nMax has already clearly said what is needed to get into his tree.\nI will accept squashed, rebased, or fixed-in-a-new-merge-commit.\n. The basic solarflare test/benchmark could live in programs/snabbmark and be called from a selftest.sh.\nI like the idea of the snabbmark growing a collection of benchmarks that we could track and check for regressions. (This is the successor to the bench/ scripts directory that we had previously.)\n. @eugeneia here is the story on memcpy:\n- GCC will compile memcpy to a dependency on GLIBC >= 2.14 by default.\n- __asm__(\".symver memcpy,memcpy@GLIBC_2.2.5\"); declaration forces a more conservative dependency.\n- This declaration exists in snabbswitch/gcc-preinclude.h.\n- GCC can be forced to use this with gcc -include gcc-preinclude.h ....\n- This should always be done when we compile C code.\nCould be that you need to make clean at the top-level e.g. if your libluajit.a is compiled with the old memcpy?\n. Superceded by #409.\n. Fixed by #439.\n. zone_gbps is not needed by the traffic process in the current implementation.\n. Looks like the Solarflare selftest needs to only run when hardware is available?\nAnd we need to acquire another suitable Solarflare NIC for davos for use by SnabbBot CI.\n. Current thinking is to install the Solarflare library separately.\nThe Solarflare setup requires additional steps e.g. to have the correct kernel driver and firmware. Hans is working on a HOWTO for this. So to submodule the shared library would only solve part of the problem.\n. SnabbBot didn't test?\n. Resolved by #411.\n. How is the state of this branch at the moment? (It would help to add a comment to the PR when pushing new commits so that I know when/what to test.)\nI did some light testing locally. Observations:\n1. iperf performance seems to be good.\n2. Fishy experiment: commenting out the finish_packet() call does not break iperf, but I think it should cause all packets to be dropped on the receiving VM due to bad checksum.\n3. Fishy experiment: adding counter/printout it seems like verify_packet() is returning false around 1/3 of the time, but I expect checksums to always be verifiably correct in practice with local VM TCP/IPv4 traffic.\nDoes this make any sense to you?\n. Interesting! That makes sense.\nCan you on this branch pick either host or network byte order, fix the functions to be consistent, and make the checksum module unit test ensure this holds?\n. Awesome! Works well and runs fast for me. (Sorry about the endian bug, I should have covered initial value argument in the original unit tests!)\n. How come SnabbBot is unhappy?\n. @eugeneia Among other things this should remove an artificial ceiling in snabbmark bench1 of 2.5 Mpps (i.e. 256 packets per 100us).\n. Rejected by SnabbBot, but I don't see an error in the log?\n. @eugeneia How does this look to you?\nThe idea is that Neutron has a strange behavior (bug?) where the binding:profile field is sometimes cleared and so the notion is for our code in Neutron to copy the value into vif_details and for neutron2snabb to access it there instead.\n. @eugeneia I think it's worth making the snabbnfv traffic config file documentation stand-alone, like a manpage, so that users can write configurations without referring to the Lua API documentation for PacketFilter and so on.\n. OK. Let's revisit this in the future. Requires some reflection. Perhaps we will want to target this as part of a larger management interface revamp (YANG, etc).\nOverall I feel like we want to encourage power users to build tools with Snabb Switch using Lua, but the resulting tools should be accessible to everybody (just like tcpdump, iperf, netcat, etc). So refering to Lua in the documentation under programs/ should perhaps be the exception (e.g. snsh) rather than the rule.\n. Further thought to documentation refactoring:\nAnother solution to the \"too many packet filtering APIs to document\" is to replace the current PacketFilter app with a pflua based one. Then we shorten our Lua documentation by throwing away the current PacketFilter API and replacing it with a reference to pcap_filter(7). The original SecurityGroup-style rules={...} syntax can then be documented in the snabbnfv traffic manpage without being a duplicate of anything. (The nfvconfig module would be responsible for translating high-level rules into low-level pcap filter expressions.)\ncc @andywingo @dpino \n. Added some follow-on changes for improving the messages printed to stdout.\n. Couldn't resist: I added fpGbps to estimate how much traffic is being processed in terms of on-the-wire physical resources. fpGbps = 10 would mean the total traffic corresponds to full utilization of one 10GbE link in one direction.\nExample from an iperf run between two VMs connected via a local NIC. The VMs are showing 10.7 Gbps goodput in this example and one Snabb Switch process is serving both VMs:\nload: time: 1.00s  fps: 2,114,697 fpGbps: 22.570 fpb: 293 bpp: 1325 sleep: 0   us\nHere fpGbps of 22.5 makes sense for a 10GbE interface that is being driven in both directions at more than 100% of line rate (possible here because the 82599 NIC can forward between local VMs at more than 10Gbps).\n. I start to wonder whether frees is really a useful metric instead of simply counting ingress and egress.\n. Merged because this feels like a big step in the right direction.\n. SnabbBot struck by the Github DDoS?\n. Oh it's luajit.org that is offline. I mailed Mike. Perhaps we should mirror luajit on Github.\n. Turns out the Snabb Lab IP address has exceeded its quota for git requests to luajit.org. See #436 for an attempted fix.\n. @javierguerragiraldez Could you merge or rebase onto master to pick up the change from #436? SnabbBot needs this change because the Snabb Lab is blocked currently from luajit.org.\n. Superseded by #444 if all goes to plan.\n. This looks very well done.\nHowever, I would really like the \"Hello world\" experience to be simpler than this, and I think that requires some revision of our APIs.\nHow about we spend some brain cycles in the next release on making it more trivial to write a first app and then revise and merge this then?\n. I think that it is the dash, yes. I have been substituting underscores, but maybe there is a way to make the dash work too.\n. I somehow missed the text on this PR and only saw the code. Awesome with friendly prose! Let me read...\n. I would have known if you had pushed it as a new commit instead of discarding the history with a rebase :-)\n. @eugeneia Wow! I love the new getting started guide :-) Great job!\nLogistic question. Will it be awkward to have the example programs upstream in the repository? In #470 I am making snabb --help print all the available programs, and it might be confusing if this included ones that are meant as sample programs rather than end-user tools.\nGenerally it may be a reasonable idea to support writing apps/applications in separate repositories rather than forks of the main repo. Have the Makefile pick them up somehow? But this is also something we can worry about later if it is not needed now.\n. If only I could rebase my Sent Mail folder so that I never recommended git rebase :-)\nI take your point. There does not seem to be much harm in including a couple of examples, even positive if they would morph into real programs some time. I also think it makes a lot of sense to keep the Getting Started guide based on the current API because we do want to support that for a reasonable length of time and who knows when a new one will really land (it would be a shame to churn the API prematurely).\n. So time to merge?\nThat would allow us to close #330 and #405 and welcome new scripters with welcome arms. @plajjan and @jeffbrl might have some feedback but we could take that on new Issues/PRs rather than holding up this one.\n. Howdy! Sorry for slow response. I have had family visiting from Australia around Fred's first birthday :). Back to normal now.\nGreat to be getting strict about MTU and to relate MTU mismatches to standard counters like ifOutDiscards.\nQuestion: how much do we want to do in software vs. hardware? I suppose the simplest and most flexible is to use software for enforcing the MTU and tracking all counters. Then we could simplify the driver by using fixed values of MAXFRS and SRRCTL based on the known size of struct packet.data. I suppose the downside is potential overhead of checking MTU, checking multicast/broadcast to bump the right counters, etc... not obvious whether that would be significant?\n(Note to self: in both cases I think we can remove set_rx_buffersize() and the dynamic tuning of receive buffer size SRRCTL now that the straightline merge made our buffers all uniform sizes.)\nI will check on lab machine connectivity when the baby sleeping on my lap awakens :)\n.  (Sorry if I am veering of topic after joining the thread late and still starting to think about applying the MIB-counter-goodness to other parts of the code base e.g. Virtio-net and Solarflare interfaces.)\n. Crazy idea that should perhaps be discussed separately: Would it make sense to create an Interface app that enforces MTU, collects ifTable statistics, and otherwise supports externally visible logical interfaces?\nThis would allow app network designers to introduce logical interfaces at appropriate points and it would also free people writing I/O apps from worrying about tracking statistics and enforcing MTUs. Using a standard app would also give us consistent behaviour between different I/O interfaces (Intel10G, Intel1G, Virtio-net, RawSocket, etc).\nWorth considering? Technical challenges?\n. > Personally, I have a strong tendency to get rid of as much hardware assistance as possible.\nI agree. CPUs are amazing these days. NIC offloads feel messy and old-fashioned lately.\n\nI think it would be better to devise our own system of counters and statistics and generate all external representations\n\nSounds reasonable to me. I also like the shared memory files and the idea of persistence.\nDreaming of the future I also ponder having the app network fork() multiple processes, including both realtime traffic processes and non-realtime management processes that could conceivably run apps that implement SNMP, NETCONF, and so on. Just dreamin'.. in the short term it's very convenient to farm out all of the northbound interfaces to off-the-shelf software but I can imagine that we will prefer to have our own one day.\n\nIf I'm correct, this will demonstrate that the current setup for the snabbnfv selftest is flawed, probably because the VMs use an MTU that is too large.\n\nDo I understand correctly that you mean the NIC is configured with too small of an MTU compared with the VMs, but that the selftest does not detect this because the NIC is not enforcing MTU for locally switched packets?\n. Let us integrate this code after the May release is tagged. Just because it makes substantial additions to an important module (intel10g) and so it would be nice to have it tested on master for a bit before shipping.\n. @alexandergall Looks like we can merge this now and then follow-up with the MTU fix, which is an independent problem. Agree?\n. Great catch, @SnabbBot! This seemingly innocent Makefile change breaks runtime compatibility with GLIBC < 2.15 when compiled on Ubuntu.\n. The problem here is toolchain voodoo: when compiled with -O2 the select() call in apps/socket/io.c becomes dependent on GLIBC >= 2.15:\nsnabbswitch/src$ ./selftest.sh\nselftest: ./snabb binary portability\nScanning for symbols requiring GLIBC > 2.7\nGLIBC_2.15 __fdelt_chk\n^^^ Error ^^^\nSo our choices are:\n1. Leave things as they are (Makefile issues warnings due to overlapping rules).\n2. Require GLIBC >= 2.15 instead of >= 2.7.\n3. Rewrite io.c to use a voodoo-free alternative to select().\n4. Find a simpler solution.\nI close this PR because I have not found a simpler solution yet and leaving things as they are for nwo seems the most reasonable alternative.\n. I think that we see things the same way, just that the text in my PR is a bit bombastic :)\nThe meat of this change is to replace the syntax for end-users to configure packet filtering in the snabbnfv traffic program.\nInstead of human users writing verbose configurations like this:\ningress_filter = { rules = { { ethertype='ipv6',\n                                   protocol='tcp',\n                                   dest_port_min=24,\n                                   dest_port_max=25 } } },\nwe would write short ones like this:\ningress_filter = \"ip6 and tcp and dst portrange 24-25\"\nand that this change would have non-trivial implications for the rest of the code base that have to be worked out on this branch.\nI do take your point that we must not force developers to shoe-horn new problems into filter expressions. There also needs to be a fully general Lua API and we need to be making that more friendly to use over time e.g. with abstractions like Alex's datagram library. I have spent more than enough of my life writing code to translate abstract problems into elaborate iptables configurations and so on :) and I don't want to see Snabb Switch users having to do that stuff either.\nbtw: In the YANG universe are there some standard schemas for defining packet filters? (ACLs, etc?) That would be interesting to see as context.\n. @alexandergall Yeah this branch is incomplete. I'll try to push a working version this week. (Let me know if you want this urgently and I will push a work in progress; only issue is that I have one small change to pflua itself that I need to work on what to do with.)\n. The code is all there now but it is not properly tested yet.\n- snabbnfv traffic config only accepts filter expressions now.\n- Neutron Security Groups are translated into filter expressions.\n- PacketFilter is replaced by the pflua-based PcapFilter.\nI don't claim that filter expressions are the universal solution for all applications, but I do think that they are an upgrade from Security Groups and that translation is a reasonable way to stay compatible with Neutron. I especially like the fact that we can easily add new concepts, for example VLAN-aware filter rules for trunk ports, without waiting for OpenStack to reach consensus on an API.\n@eugeneia are you interested in helping to test and document this? :-)\n. @alexandergall woohoo! 20% speedup in a real application simply by switching libpcap for pflua sounds promising indeed. Great work, pflua hackers!\n. @eugeneia Thanks! I had indeed missed that :)\n. Groovy. Let's get some more experience with this code :).\n. Welcome! You should now be able to connect to chur with ssh -p 2020 root@lab1.snabb.co and create a private account for yourself.\n. Handled in #446.\n. Thanks for sharing this!\nLet me put another interesting NAT-related problem on your radar: lwAFTR (lightweight IPv4/IPv6 Address Family Transition Router).\nThis is a stateless form of NAT for IPv6 networks to communicate with IPv4 networks. For example, Deutsche Telekom's TeraStream network is natively IPv6 and applies lwAFTR to all internet traffic. That is an important network element.\nThere is no production-quality open source lwAFTR implementaton today, and only one commercial one to my knowledge (from A10 Networks). If you guys are ever interested in prototyping an lwAFTR I would gladly put you in touch with Ian Farrer at Deutsche Telekom who lives and breathes this stuff (and is an author of the IETF draft linked above).\nCarrier Grade NAT is another important application with additional requirements for ISPs e.g. being able to tell the police who was using an address/port at a given time.\n. Hoppsan! Nice catch :-)\n. @eugeneia SnabbBot reports a performance regression but I suspect this is spurious?\n. Sorry @darius. I fat fingered your name on the Assignee.\n@eugeneia I am not honestly sure if this is a good idea or not. I suppose at the minimum it may require us to first improve our error messages so that they are useful even without the backtrace for context.\n. Thanks for the review, Max!\nLet me update this one and we can think about merging it onto next early in the next release cycle. That way we will have a few weeks to try it out and improve error messages and potentially revert the change before shipping it.\n. I would like to give this a little more thought. Could be that it is better to introduce a separate function for exiting with a fatal error message. Then genuine uncaught error() messages would be bugs and it would make sense to include the backtrace. Closing this issue for now.\n. Closed due to lack of overwhelming support. Soon we will make the May release and then it will be more disruptive to change the name.\nThe name PcapFilter at least has the virtue of being named after the pcap-filter(7) man page.\n. Additional thoughts:\nThe kernel behavior seems to be changing over time. If I recall correctly, on Ubuntu 13.04 the IOMMU was disabled by default, and if it was enabled then Snabb Switch could not perform DMA (and you could see a syslog message saying that it had been blocked). In Ubuntu 14.04 the IOMMU is now enabled by default, and Snabb Switch actually does seem to work to a first approximation, but we have seen major performance disruption specifically when the IOMMU is enabled (at least with the Solarflare NIC: also Intel I think).\nSo for me it is not yet clear what is the intended behavior of the kernel on recent Ubuntu releases. Is everything supposed to Just Work even with the IOMMU enabled? Or is it a bug that it ever works at all when not using VFIO? This could be worth chasing up and potentially even helping to fix things in the kernel if the interface is supposed to work for our current usage.\n. Thank you for bringing this up Justin!\nI know that Sandy Bridge has performance issues in the IOMMU but I had not considered that this could be the root cause of the problems that we have seen. This has to be checked. I will enable the IOMMU on our new Haswell server (interlaken) and see if I can reproduce the performance problem there.\n. Ho ho ho...\nTurns out that the IOMMU is already enabled on interlaken and apparently always has been. There has been significant optimization work done on this server, both by me and @alexandergall, and it has seemed to behave very well indeed throughout.\nI suppose that we can close this issue by updating our recommendation of disabling the IOMMU to only apply to Sandy Bridge servers, and to post a note asking people to please report any problems with IOMMU on other platforms?\n. @justincormack any idea what changed so that enabled IOMMU blocks us on Ubuntu 13.04 (iirc) but not 14.04?\n. @eugeneia Do you think this is a real error or a spurious one?\n. I'll wait for a recheck before merge just to be safe.\n. CI failing because I had not pushed the latest pflua commits to our SnabbCo fork/mirror.\nI will take the opportunity to close this and resubmit the two commits as separate PRs.\n. The snabb --help referred to in this README is now implemented in #470.\n. Currently directly referencing the Getting Started guide on Max's branch for #439.\n. This would be a big change. What do we think (a) of the idea of a unified snabb module and (b) of this draft API?\n. Thanks @sleinen! You are the target user for this change in many ways i.e. the idea is to create a friendly and stable interface for people to write their first app.\nI mean \"stable\" in the sense that we would only make backwards-compatible changes to the snabb module. If we need to make incompatible changes then we do that by creating a new module like snabb2 and snabb3. The older modules would be deprecated and then removed in a thoughtful and controlled way.\nSo the user module might actually start with:\nlua\nlocal snabb = require(\"snabb1\") -- get warning/error if deprecated/removed\nThe implementation cost seems minimal thus far in that the snabb module is a thin layer over the top of the actual implementation. The module would need to become more complex over time if it implements an older API version that is less in sync with the actual implementation (e.g. if we have a snabb0 that supported the iovec API in some clever way even though it does not exist in the actual packet module).\nI am experimenting with how the Getting Started apps would look in this scheme. I will push progress on those experiments onto this branch.\n. In commit b44f510 I wrote an experimental new version of the Getting Started sprayer app/program. For comparison the version that works on master is in #439.\nThis is a style experiment: if I could write this code using any of the programming styles currently under discussion on snabb-devel then how would I want it to look? This code is an attempt to make the code simple and more like plain scripts rather than parts of a big framework. (The code does not actually run yet.)\nSee the commit message and code for full details.\nQuestion: Is this the coding style that we want to adopt and explain to new users? What is good and what is bad? What details do I miss?\n. Commit 40bc269 uses short method names and commit eb36db5 uses long ones.\nThe trade off seems to be between redundancy when the variable name conveys type information:\nengine:configure(config)      -- short and sweet\nengine:engine_configure(conf) -- long and redundant\nverses ambiguity when the name does not clearly tell you what the type is:\ninput:is_empty()      -- short and ambiguous\ninput:link_is_empty() -- long and clear\nJust at the time I am writing this I prefer the long names. That frees you up from having to think about whether you are conveying type information in your variable names. You could call a config object config, conf, old, new, or c and nobody need get confused or ask you to change it on a github review.\n. > Do we want people to explicitly specify an API version in each of their files?\nGood question.\nSuppose we say yes. Could we then support multiple apps each choosing which API to use? That is, some apps already using the latest and greatest, and others using older APIs. This would allow us to introduce a new and incompatible API with a new module name (e.g. snabb3) and immediately make it available to everybody but without forcing them to adopt it.\nThat would mean, for example, that Alex could pull from master into his VPN branch and get radically new features, but decide for himself when to adopt them.\nThe goal that is developing in my mind is to support rapid improvement on the master branch (API revisions, app sandboxes, parallel execution, etc) and also for application developers to feel that they can safely pull from master and have the chance to adopt new features on their own schedule. That is unlike the situation now where pulling a feature like the straightline API means you have to immediately update all of your code before you can run again.\n. Question: is that the right goal? and what is the best way to achieve it?\n. @eugeneia I suppose the reason for the local snabb = ... is that it seems like a really simple mechanism that does not involve any new concepts. How else would we indicate API version? I think it would be more complex to do it in the Makefile for example.\nIf we simply made all of the APIs globally accessible then that would be simple too, but then we would be writing function calls like snabb2.config() instead of snabb.config().\n. Speaking of the Makefile...\nIf we supported multiple API versions then a very nice test case would be to run the latest Snabb Switch core with the older apps based on the previous API. Then we would have full test coverage on the original API instead of gradually less as fewer modules are using it.\nHow could we do that? I mean, run core from master but apps from an older branch.\nThis thread becomes long :-). That is good: we can use this PR mostly for discussing ideas. I will resubmit the code on a new PR once we know how it should look.\n. @eugeneia If we have a global snabb module then how does it work when we introduce new and incompatible versions? For example if we have snabb1, snabb2, and snabb3 then how does each app get the right one bound do its snabb variable?\nI am totally on board with avoiding boilerplate. I am not sure that is what we are talking about here though. The local snabb = require(\"snabb2\") seems like important semantic information that is being put in the most directly relevant place. It tells both the reader and the compiler \"This file has to be linked with API version 2.\" I don't immediately see a good way to handle this otherwise.\nThe whole scheme could be naive of course. For example we may end up with a mess of modules based on different versions calling each other and passing incompatible objects. I am not sure what is the best precedent for a scheme like this.\n. Here is one more idea:\nIf we would go ahead with the design to sandbox each app in a private Lua VM then we could also have one API version valid in each VM. That way each app could depend on a specific API version and we would not have the issue of calling modules with different APIs (each app can only load one API).\n. This can also be that I am drifting into fantasy land a bit.\nConcretely the problem to solve is to be able to redefine core APIs and deprecate rather than immediately remove older ones.\nCould be a simple idea to have a single snabb module that should implement the union of all supported API versions?\n. > I just meant that an app would use the latest and greatest by default\nThe scenario that I want to support is that a long-lived branch supporting a specific application (for example Alex's VPN device) can safely pull from master. The expected effect is to automatically get fixes and optimizations but that API changes do not cause breakage and can be adopted separately.\nThat is, to stop doing what we have been doing recently, where pulling straightline means your branch is broken until you update all your apps, and pulling program means the scripts to run your programs are all broken, etc. I think it is time to start being more responsible about this so that people will continue to track master closely.\n. > To be honest to me the multi-api-version scenario sounds a bit wonky to me\nI am coming around to this way of thinking at the moment. :-)\n\nkeeping radical API revisions in a long lived feature branch until we are happy with it\n\nWould two weeks be \"long lived\"? :-) I would love to land a new API in the June release so that we can bring new people into the project with a warm and fuzzy getting started experience.\n. The next step on this branch for me is to flesh out the proposed API more and make it actually work and then we can see whether it gets a thumbs up or thumbs down.\n. The code currently on this branch is actually both radically new and backwards compatible. The documentation would tell people to use the snabb module but that is only a front-end on the code that already exists.\nIsolating apps in sandboxes might be done only when the app class is given as a string so that \"classic\" apps can continue to share a Lua VM and operate exactly as they always did. (People might want to wait for an excellent implementation of parallelism before bothering to update their apps to the new scheme to take advantage of it.)\n. > I feel like keeping radical API revisions in a long lived feature branch until we are happy with it might be the simpler and more failure proof approach\nThat is actually a really excellent point. It is a perfectly valid idea to stick with the API that we have today, that we want to support for some time to come, and that by now have good documentation for. Then the new features (APIv2, sandboxes, parallelism) could develop at their own pace on their own branches and be merged onto master when it makes sense for everybody. (Or not at all in the case of ideas that lead to dead ends.)\nThere is more than enough to get on with this month just in terms of shaving off rough edges and fixing open bugs.\n. Closing because the goal of making an experiment and getting feedback is accomplished.\n. Welcome!\n. Awesome! Can't wait to read and understand :-)\nQuick answer: Yes, it makes senes that private freelists would be a problem indeed. I suspect that a Source app would be allocating infinitely many packets and they would always end up on the private freelist of the Sink app. I like the idea of making the freelist into a shared FFI object in some simple way. (I'm not sure the simplest way to create shared FFI state: a C global variable perhaps?) This should work fine.\nOr the quick workaround would be to test with a new app network that has the same app both allocating and freeing the packets. Like a SourceSink app. Then the packets should be correctly recycled. However, this wouldn't work for any other app networks, so maybe you want to skip this small step.\n. I am not sure what you are benchmarking exactly but 1.4 Mpps does not sound fast.\nThe snabb snabbmark bench1 100e6 is supposed to be handy for exactly this scenario. That is, to feed packets through a simple app network and see what the throughput is. I would expect >= 20 Mpps as the baseline. For the multiple states it would be interesting to be able to make the app network longer e.g. to pass packets through N trivial app instances between the Source and the Sink. Then we would get a feel for whether the extra Lua states are slowing us down by chewing up cache space.\n. also: Awesome that it is actually up and running so quickly!\nSorry to fixate immediately on the packet rate :-) reflex!\n. Closed this PR now that we have successfully completed the proof-of-concept work.\n. Fixed by #493.\n. Cool! Fanastic! Awesome!\nI have this running now:\n```\n Kfrees/s        freeGbytes/s    breaths/s    \n 1775.38         2.14            5496           \nLinks (rx/tx/txdrop in Mpps)    rx      tx      rxGb    txGb    txdrop \n B_Virtio.tx -> B_NIC.rx         0.19    0.19    0.01    0.01    0.00 \n B_NIC.tx -> B_Virtio.rx         0.70    0.70    1.06    1.06    0.00 \n A_Virtio.tx -> A_NIC.rx         0.70    0.70    1.06    1.06    0.00 \n A_NIC.tx -> A_Virtio.rx         0.19    0.19    0.01    0.01    0.00 \n```\nI would like to request more detailed usage information and more verbose information while trying to connect to an instance please! My initial attempts to run looked like simply:\n./snabb top \nConnecting...\nand it was not until I ran with strace that I realized it was a permissions issue and I needed to run with sudo to be able to open the counter files.\n. Bug report: On this branch when I start a second snabbnfv traffic process the first one crashes with a SIGBUS. I am not sure if that is expected to work or not at this stage or not.\n. I merged this onto next to test for the next release. I hope that we will be able to use #513 to more easily expose state from the core and the apps to SnabbTop but we will see.\n. Hoi!\nHow is SnabbTop feeling nowadays? Should I merge this branch? (How come SnabbBot is unhappy?)\n. Hi @eugeneia!\nThanks for merging. Sorry about the slow response: latest problem I need to solve is how to stay on top of Github notifications :-).\nSo yes I reckon the process is working:\n- [x] Pull Request is created and we decide you are the one in charge.\n- [x] You accept the change by merging it into one of your well-loved branches.\n- [ ] As soon as you are ready you send a Pull Request from your branch to next saying that all the included changes should go into the next release.\nSo already now we can close this issue in the knowledge that the change has safely landed in your documentation-fixes branch and from there it will be PR'd to next and from there to master.\nDoes that make sense at all? :-)\nLet us see how practice matches up with theory!\n. Could be that cpuset(7) is a good solution. There is much discussion of this as a potential successor to isolcpus on the interwebs and it sounds like it can cover our use case of excluding other processes (and kernel threads and interrupt handlers) from using \"our\" cores.\n. The kernel documentation: Documentation/cgroups/cpusets.txt.\n. I reckon the Snabb NFV wiki pages should move into src/program/snabbnfv/doc/.\n. This is awesome!\nI have attempted to update the Snabb Switch and Snabb NFV wikis to remove unneeded pages and leave some forwarding pointers to the repo. (Currently they point to the next branch but could be updated when this documentation lands on master.)\nThere is some general information embedded in the Scenario tests page that we need to migrate into its own file i.e. how to configure the Neutron driver and tell it about the network setup. The challenge here is that currently the Neutron driver supports a very specific kind of network that is quite unlike what most people have. In the future it needs to be extended very slightly to support traditional vlan and flat network models too. Then the user will not have to fuss about this extra configuration.\nDo you think more action is needed right now for this PR?\n. Device Driver Programming should die, but upon reading it I realized that I would like to start writing small introductory texts like that in the future, as simple entry points into the code. I was distracted by nostalgia and did not press Delete :).\nScenario Tests... That is alive because currently it is the only place where I can point to the description of the config file format for our Neutron ML2 plugin. That information needs to exist in some good place. The rest can go... it was a draft write-up while we were preparing the end-to-end tests for our OpenStack setup (which should also be documented Somewhere Else).\n. For a root directory one idea would be to look at the Filesystem Heirarchy Standard and see which standard location best matches our requirements. For example, if we would like a ramdisk then /var/run/ could be suitable.\nHow about the naming scheme? Subdirectories named by pid, for example?\n. I like this scheme.\nPerhaps we could even have an optional name that would create/update a symlink? So you could write snabb top snabbnfv-traffic-01:00.0 to attach to the latest incarnation of the named process. (symlink like /var/run/snabbnfv-traffic-0 -> 90210).\nCleanup is interesting. Yes, we need to clean up old files to avoid using unbounded space, particularly on a RAM disk. On the other hand, the state of a given process might actually be especially interesting after it has crashed/terminated. So it might be nice to have a scheme that would somehow expire old directories to keep the space usage within bounds but without being too eager to delete data?\n. Here is another idea:\nSuppose that we used a symbolic name instead of a PID. Then we could avoid leakage by using a reasonably small set of names. The decision then would be what happens when we start and the directory already exists: do we delete it, archive it, or use it?\n. There is a precedent for keeping it simple and leaving such details to others: the init scripts. Snabb NFV is not pushing people to use standard startup scripts and instead leaving that to people doing packaging for distributions. They are the experts on how they want things done (sysv, upstart, systemd, logging, file ownership, etc) and we don't care so much at the moment. The best approach seems to be to let people take care of this downstream and then pull good solutions upstream in the future if that makes sense.\nI am not sure about this case though. As a user I want a simple and uniform way to monitor and control the Snabb Switch processes that are running. The processes can be started in a bunch of different ways:\n1. Throw-away instance started in an interactive shell.\n2. Persistent instance started in a shell running under tmux or screen.\n3. Started by script e.g. init.d or bench_env.\nIf we push work onto the code that is starting Snabb Switch processes that it would seem to complicate all of those scenarios and make it harder to control Snabb Switches in a consistent way that is independent of how they started. That sounds negative to me.\nI would also like to drastically simplify the bench_env scripts. They are complex and a lot of that comes from boring details like keeping track of which processes have been started and making sure they are terminated together. So if we would do more in the Snabb Switch core it could potentially make these scripts simpler too.\nThe notion I have in mind is a bit like the Erlang -sname foo argument that assigns a name to a process that is unique to the local machine. That seems to work fairly well in practice.\n. Here is something I did not really consider: it could be that it is hard to automatically choose a suitable name.\nFor example, if the traffic process would self-assign itself the name snabbnfv-traffic-01:00.0 then that seems simple on the surface but how will the name change if we have one process handling multiple ports or one port shared by multiple processes? Then we are potentially making life more complicated than if the startup scripts would manually assign a name that suits them (e.g. nfv-1).\nSo I start to come around to your way of thinking.\nI wonder if we could have a command like snabb gc that would clean up old directories in a good way and could be easily called from scripts or cronjobs that want to adopt a default behavior?\n. I suppose also that over time snabb top could  be made quite flexible about how it chooses which instance to monitor. For example instead of always having to be told the pid or symlink name perhaps it could do a simple search of the available directories. Then naming becomes less of a big deal if a user can say snabb top nfv 01:00.0 and it finds the right instance heuristically. That may not be the best example but at least I see now that we are not so heavily dependent on a symbolic name or lack thereof.\n. Closing this issue. I think you have all the context to decide what you think is the best way to do this for snabb top.\n. Resolved by #489.\n. Anybody want to volunteer for this? If so then what kind of changes are you interested in testing? (Check out the Pull Requests lists for inspiration on what kind of changes come down the pipeline.)\n. > Why would a tester merge a PR onto their own branch?\nThis is to adopt the distributed model from the Linux kernel workflow.\nThe idea is that we treat our branches like IP routers and our commits like packets. Each branch (router) makes sure that the commit (packet) looks okay and then forwards it (pull request) to the next hop. The rules are all applied locally and independently of the size of the network. The chain of merges that a commit passes through on its way to master will read like a traceroute of the path that it took and who tested it.\nIt works for Linux and for the internet so I hope it will work for us too :-)\n. To clarify:\nReviews posted on Github issues are still important in the same way that they have always been. The difference is that each PR will have a definite next-hop branch and the owner of that branch will be the one weighing the comments and deciding when they are satisfied.\nGreat that you ask @plajjan! I am still working on really understanding the kernel git workflow but I feel that there is something deeply right about it.\n. @plajjan This is an interesting discussion :-)\nLet me concede that:\n- There are many excellent workflows in the world.\n- This one is relatively uncommon.\n- This may not be the best option.\nHowever, I am personally optimistic about it and I do want to try it out seriously as an experiment.\nSo, coming back on topic, what I need is an enthusiastic volunteer or two. For example I would love to persuade @eugeneia to verify code cleanups and documentation improvements. Perhaps @alexandergall would be willing to verify extensions of shm/counter and snmp/mtu related changes too? If so I would be happy to explain the idea of this workflow in however much detail is desired and start setting Assignee on the relevant Pull Requests.\n. Oh, hey, @eugeneia did volunteer the last time I suggested this on the mailing list!\nMax, let me belatedly take you up on that offer, and include code cleanups and basic usability enhancements in addition to documentation fixups :-)\nLet me also revise the idea for how to dispatch the Pull Requests. Instead of sending them directly to your branch on your repository, let us try keeping the PRs open on the main repository but making you the Assignee. This is to indicate that the PR will first be accepted by you onto one of your branches (merge) and then that branch (containing one or many approved fixes) will be PR'd by you for merge to master.\nClear as mud? :-)\n. @plajjan Good reference here: Distributed Workflows.\nThe idea is to keep it simple and scalable and flexible.\n- Simple: no core team, no voting, no special processes for reaching consensus/quorum/etc. Code lands upstream when a short chain of people individually approve it. That chain is defined socially by who likes to pull what kind of changes from who.\n- Scalable: as the community grows we only need a small number of people to become subsystem maintainers by collecting good changes on branches. This process should hold up at least until we reach the size of the Linux kernel community :-)\n- Flexible: anybody can have a branch that they consider to be the master e.g. one for the NFV application, one for the VPN, one for snsh, one for long term maintenance, etc, and they synchronize on their own terms. The SnabbCo/master branch is not special and may not even end up being the most important branch.\nThe programs/ structure actually allows us to scale up Snabb Switch development massively over time. Perhaps in the years to come we will have multiple major network elements upstreaming onto the master branch but each working primarily in their own downstream branches and controlling their own releases cycles, etc. This is already happening with both the NFV and the VPN applications that have control over when they will pull changes from master and when they want to maintain downstream extensions.\nI really appreciate the autonomy that this structure grants to every developer. If I am developing on a project then I want to be maintaining the version that works for me and merging that with upstream in a controlled way. I don't want to be forced to work directly on the upstream master branch, I don't want anybody to have veto of what code I choose to deploy, and I don't want to lose sync with development and be left behind either. This distributed workflow is the process that I see as solving these problems.\nThe trouble with a centralized workflow for me is that it works well for some people and badly for others and runs more risk of becoming political. OpenStack is perhaps the most advanced centralized workflow in the world today and from my perspective it is the least pleasant project to contribute to for the reasons sketched above.\nI close this as an issue but will very willingly continue this discussion here :-)\n. @plajjan Concretely I hope this is a solution for cases like #483. The distributed model means that people can cooperatively develop new features fairly independently of how ready other people are to adopt them.\nI really hope this will work out and we will have an active musl-compatible branch. Then it would make it easy for the community to include people who need things that cannot land on master overnight. More extreme examples would be a Windows port, a DPDK I/O backend, a branch that links GPL code, etc. These ideas should be given a chance to attract users and gain momentum.\nHow do you deal with these issues in a centralized model?\nI admit that this is experimental and the distributed model has not actually solved any practical problems for us yet. I hope to soon be able to say otherwise :-)\n. This is very interesting work! I am keen to make our Git workflow support simple ports like this. Like, working out what we need to do to land them on master, and making their development and maintenance smooth while they are living on branches.\nMore soon!\n. I would like to bring compatibility with musl, 32-bit Linux, and nongnu tools onto master. There are some issues we will need to deal with along the way though and I think this effort should initially live on a long-lived branch (per #485) before landing on master.\nI would be glad if you want to maintain this branch! In that case please send a PR to add it to src/doc/branches.md.\nMusl users would benefit from having a branch that is known to be musl-compatible. The easy short-term solution seems to be a dedicated branch whose maintainer regularly pushes from master with any required fixes for musl. Then people can run the code with musl and over time we can look at the branch to see what the compatibility issues are and how much interest it gathers.\nThe better long term solution would then be to make sure master is musl-compatible. This would require some new programming discipline and extended CI infrastructure. This might be awkward to adopt right this moment, but it does seem to fit in with future themes like replacing C code with Lua code and verifying support for more OS distributions and kernel versions.\nHow does that sound? (This is an early attempt at setting expectations for somebody who is submitting a PR for work that is valuable but cannot immediately land on master, please let me know how I am doing :-))\n. Interesting. I build this branch with GNU tools and the ./snabb exits with status 1. I have seen this in the past when an error occurs very early during startup and we don't have any code to catch and report it.\nI suspect the issue is this: we are using luajit -bg to compile Lua sources into .o files containing bytecode. Then when we require() these modules at runtime LuaJIT will expect to find them with dlsym(). Could it be that dlsym() does not work in this build for whatever reason?\n@justincormack might know more.\n. Right, scratch that. If I put a print(\"foo\") io.flush() in core/main.lua then it prints. So Lua code is executing but perhaps hitting an error before we setup an error handler.\nCould be a job for bisect? I can't dig into this right now.\n. The root problem is core.main failing to load a Lua module that it depends on. This happens early before an error handler installed. If you merge #502 you should get a proper error message.\n. These changes would be a very valuable starting point to a port away from GNU dependencies. However, somebody needs to follow them through all the way. Closing the PR for now.\n. Currently we always build statically.\nWe have considered moving away from this model (see e.g. here) towards dynamically linking or even breaking Snabb Switch into separate LuaRocks components, but to me it seems like the static linking model is more practical for our current needs.\nOne major challenge with using system-supplied versions of LuaJIT and pflua is how to get a compatible version? There is no published stable version of either that we can nominate and in practice it has been important to be able to pull specific fixes at least from LuaJIT. I also like the idea that we could extend these projects and immediately make use of those extensions before they land upstream.\nSo submodules seem very practical for the immediate future, at least until LuaJIT starts making releases from its v2.1 development branch that we could depend on.\n. The DMA address tag check failed issue highlights the way we depend on LuaJIT.\nWe added some code that depended on bringing in a specific bug fix in LuaJIT. This broke Snabb Switch for anybody using an earlier version of LuaJIT, including everybody who had not updated and rebuilt the luajit submodule in our repository. The solution we adopted was both to update our LuaJIT submodule to a known-good version and also to update src/Makefile to check that this is actually built.\nThis has worked okay so far as I know. People have since stopped reporting problems caused by an out-of-date LuaJIT. It does mean that we have a hard dependency on the specific submodule versions that we link to though.\nOne piece of the puzzle that is missing is a plan for when to pull in updates to our submodules (luajit, ljsyscall, pflua). Currently we do this when we need something new but not as a matter of routine.\n. Can you easily include patches in your packaging? If so I would suggest initially working on a branch that updates the Makefile to suit your needs. Once you are happy with it we can look for the best way to make the master branch support this use case and reduce/avoid the maintenance burden of maintaining a branch.\nSpace optimization is interesting. I would like to start working on shrinking the snabb executable back to its target size of 1MB from the ~1.5MB that it is today. I am not sure that dynamically linking with LuaJIT will turn out to be a good trade-off though due to the \"I know what I am doing\" assumption that the user will always have a suitable version of LuaJIT installed at all times. There may be simpler ways to shave a few hundred KB off the object size.\n. I suppose there is also the risk of Snabb Switch packaged for a particular distro diverging too much from the mainline. The best solution I can think of for this would be to publish the branch in src/doc/branches.md so that the current state of the package is transparent and we can see what changes are currently needed compared with the master repo.\n. I don't think it makes sense for the master branch to offer to use the system-installed LuaJIT at this time. If LuaJIT were making releases from the v2.1 branch and we could refer to a well-known minimum version then this might make sense. However, LuaJIT v2.1 is not making releases and I would not be confident to run Snabb Switch with a version older than what is on master at any given time.\nIf anybody really wants to do this they can of course modify the Makefile to stop statically linking LuaJIT. That seems to me like roughly the right level of difficulty to discourage people from doing this lightly and potentially creating problems for themselves.\nClosing this issue for now.\n. I will mail snabb-devel with more about this idea soon...\n. This fix is tested in the OpenStack environment and makes our end-to-end test suite there pass 100%. That is currently 18 different test cases of booting VMs, adding/removing tunnels, etc. Woohoo :-)\nI merged this into the nfv tree now and intend to synchronize that with master regularly (at least once for each release on master).\n(The reason this issue closed automatically seems to be that I accidentally used the magic 'fix account/repo#issue' syntax when pushing this to another branch on github. I had not considered that things are so interlinked!)\n. Merged onto nfv branch.\n. Merged onto nfv branch.\n. Merged onto nfv branch.\n. Merged onto nfv branch.\n. I have merged this into the nfv branch for wider testing now. (Surprised Github doesn' show that? You can see it when examining the individual commits at least.)\n. Thanks. I would like to have SnabbBot runs on the nfv branch too please. I will PR this to master to trigger for now.\n. Merged into nfv branch. Will be included on a future PR from there to master.\n. Closing because this has been merged onto the fixes branch.\n. This API change makes sense to me.\nThe code looks nice too. I like the way you simplified the generic checksum routine. Is there any meaningful performance impact on that? If not then great. But we should remove the comment at the top of the file saying that the generic checksum is based on the DPDK one (http://dpdk.org/browse/dpdk/tree/lib/librte_net/rte_ip.h) because it will not be anymore.\n. > But how would one make sure that there is room for the padding bytes after the data?\nThis seems doable in the special case that we are checksumming a struct packet: we could define a system-wide maximum packet data size and ensure there is some excess in the size of the struct packet.data array. Then every packet always has trailing padding. This would add complexity though so we wouldn't want to do it unless there was a meaningful payoff.\n. This change is merged onto the next branch now so I close this PR.\nI made a small change to the comment in the merge (cc0ebb9). If you would like to rephrase that you can just push a new commit on this branch and reopen the PR.\n. Boy do we need something like this :-)\nCouple of questions about intended usage:\n1. How often can you reasonably call logger:log()? Is once per packet too often? Once per push? What is a practical limit to suggest?\n2. How careful do you have to be about log messages that are expensive to compute? (Is it going to be a problem to use things like .. string concatenation or string.format() even in cases where the message is suppressed?)\n. Commit 979af8c3 is an example of a case where we had a log message during traffic processing and ended up hitting the pathological case where pretty much every packet causes a log message to be printed (and that message to be a computed string no less).\nIt would be awesome to have a generic logging capability that could be used in that situation without wrecking performance.\n. Thinking aloud...\nHow about if we had separately a generic token bucket object and a function for printing well-formatted log messages?\nSo then we could write:\nlua\nlocal logger = lib.tokenbucket(...)\n...\nif logger:take() then lib.log(...) end\nwith the nice benefits that:\n1. The take() operation could be fast enough to run on every packet.\n2. The rate limiter app could adopt this token bucket too (save on code).\n3. The log message would only be constructed if there is a token so it would be okay to use expensive operations even on the traffic path (since we know it will seldom run).\n. Good point. Logging the number of suppressed messages is important.\nOne compromise idea: How about if the logger took the same arguments as string.format? So you could write logger:log(\"Packet too small: %d bytes\", packet.length\") and the string would only be constructed if there is a token available? That might be nice if it meant you could expect good performance even when calling log() on every packet. (On the other hand maybe that varargs call is a LuaJIT NYI and would not be efficient anyway..)\n. Cool :). This is merged onto the next branch for testing for the next release.\nI am very keen to improve our logging and counter usage across the board so that we can troubleshoot production systems more easily.\n. Merged onto next for inclusion in the next release. Closing PR.\n. Hah! I also addressed this in #481. \nLooks like I did two things wrong to obscure that change:\n1. Failed to reference the bug Issue in the Pull Request, so that Github would cross reference.\n2. Closed the PR after having merged it onto the nfv branch (but not yet onto master).\nIs there one that we should keep or should we merge them together?\n. Good point.\nIn a perfect world it could be nice to refuse to operate on ports that the kernel is currently using and insist that the user do ifconfig ethxx down. That would avoid really annoying problems like wiping out networking on the machine by accidentally using the wrong PCI address.\nHowever, this would not be backwards compatible behavior for snabbnfv traffic and so I suspect we would do more harm than good by introducing this now. I close this PR for now.\n. I am pushing an update that expands the content and revises the language a bit.\n. Here are a few commands for seeing what is coming in the next release by inspecting the next branch:\n```\nFirst make sure we have the latest changes on all the branches\n$ git fetch origin\n```\nWho wrote the code being tested on next?\n$ git shortlog -sn --no-merges master..next                                                    \n15  Max Rottenkolber\n 7  Luke Gorrie\n 3  Alexander Gall\n 1  Edward Hope-Morley\nAnd what are the changes exactly?\n```\n$ git shortlog --no-merges master..next\nAlexander Gall (3):\n  Modify checksum API\n  Apply new IP checksum API\n  Fix type casts for commit 5cc2c1f\nEdward Hope-Morley (1):\n  Fix neutron2snabb parser db schema\nLuke Gorrie (7):\n  neutron2snabb: Fix for security group -> pflua translation\n  neutron2snabb: Added automatic database schema detection\n  neutron2snabb: Fix selftest for security group translation\n  snabbnfv traffic: Check PCI device exists and is supported\n  neutron_sync_agent.sh: Fix to use qualified paths\n  neutron_sync_master: Include *.sql table definitions\n  neutron2snabb: Fix schema selftest to find database files\nMax Rottenkolber (15):\n  Import \"Documentation Guide\" from Wiki.\n  [documentation guide] Mention different target audiences for     documentation.\n  [neutron-sync-master] Only amend commit when SQL dump actually     changes.\n  [neutron-sync-] Don't print command outputs and print timestamped log     messages instead.\n  [neutron-sync-agent] Don't try to generate configurations if repository     coild not be successfully cloned.\n  [neutron-sync-agent] Add TMP_DIR and SYNC_PORT options.\n  [neutron-sync-agent] Do not treat \"$TMP_DIR/\" as file in $TMP_DIR' is     empty.\n  [neutron-sync-agent] Only try updating confs whengit diff'     return 1 as opposed to failing with other error codes (>1).\n  [neutron2snabb] Recover selftest.sh (should have really been in     7e9477)\n  [neutron-sync-] Do not hide output of neutron2snabb and mysqldump.\n  [neutron_sync_master] Update default tables in Lua wrapper.\n  [neutron-sync-master] Remove the '-t' that prevents the table     info from being stored in .sql. Add '--skip-dump-date' so that     these files don't include a timestamp that makes it look like     something changes between runs.\n  [neutron-sync-agent] Less verbose git clone' andgit fetch'.\n  [snabbnfv] Import documentation from snabb-nfv Wiki.\n  [snabbnfv] Deleted outdated src/program/snabbnfv/doc/neutron-configuration.md.\n```\nAnd which branches are the Pull Requests being merged into on the way to master? (Some directly onto next and others collected on nfv and then merged from there.)\n$ git log --oneline --merges --graph next\n*   affd604 Merge PR #501 into next\n|\\  \n| * 3607df8 Merge commit 'pr/499' into nfv\n* | e750506 Merge PR #479 into next\n* | cc0ebb9 Merge commit 'pr/494' into next\n* |   183d9c5 Merge branch 'nfv' into next (tag: v2015.05.nfv1)\n|\\ \\  \n| |/  \n| * 3a71bac Merge commit 'refs/pull/upstream/492' into nfv\n| * 8ccc900 Merge commit 'refs/pull/upstream/491' into nfv\n| * 7cc3f9c Merge commit 'refs/pull/upstream/490' into nfv\n| * 133f855 Merge commit 'refs/pull/upstream/487' into nfv\n| * 5e9369c Merge commit 'refs/pull/upstream/489' into nfv\n| * 3d6d8c8 Merge commit 'refs/pull/upstream/488' into nfv\n| * 7c8a58d Merge commit 'refs/pull/upstream/486' into nfv\n| * 39a8f76 Merge PR #486 into nfv\n* | ce5142d Merge PR #485: branches.md\n|/  \n* 9dae83a Merge pull request #477 from lukego/64bit-registers-merge\n| * 188bc06 Merge PR #475 (Intel 10G 64-bit counters w/ SNMP support)\n|/  \n* b6a4f8e Merge pull request #473 from lukego/README2\nWe could adapt our git usage to make the output of such commands nicer e.g. I can try to include the most relevant information on the merge commit messages.\n. @eugeneia Thanks for catching that. I had locally merged that onto next for testing, and had not meant to push it to the repository.\nNow we are quite close to release time. Is it better to merge the follow-on changes or to revert the change and then reapply immediately after the June release?\n. @eugeneia I think revert is best right now. This is a bit of an uncomfortably large change to pull in before a release, and I had not actually meant to do it now. Could be that e.g. we break startup on certain platforms due to directory names/permissions etc and I would like a chance to debug that before releasing.\nThis will also give me a chance to test my new understanding of the workflow for reverting and then reintroducing merges :-)\nBased on my reading of the undo merges howto the steps will be:\n1. Merge code too early.\n2. Revert the merge of partially finished code.\n3. Make the new release.\n4. Introduce the latest code by first reverting the revert (get back to the partial state) and then pull the remainder from the topic branch.\n. Hoi :).\nYeah, somewhere in the Git workflow discussion I got it into my head that we should make weekly maintenance releases and just started acting as if this was something we had actually discussed. Excuse me :).\nThe idea is just to reduce the maximum latency of small but important fixes landing on master. If we only update master once per month then dumb bugs may end up bothering people for weeks longer than necessary. For example, all of our tutorial material says to make -j but that often doesn't work because of a race condition in Makefile. I don't want to regress into the habit of pushing individual fixes like this directly onto master and so \"merge fixes onto master on mondays if it happens to contain new commits\" seems like a reasonable compromise between \"merge right now\" or \"merge next month\".\nWhat do you think?\n. I am not sure what is the optimal process for bugfix releases.\nOne idea would be:\n1. If fixes contains no new commits, then skip.\n2. Merge fixes onto master.\n3. Tag as vYYYY.MM.N with the first available N.\n4. Create Github release page with the shortlog.\nCould be that we could automate the release process over the longer term so that we humans focus only on having the branches in the right state at the right time. Having humans pushing to master is a bit scary. I actually very briefly screwed up master last week by accidentally pushing a couple of commits and then quickly rebased them away again. I would love to have a process that doesn't permit such goofs.\n. To me it seems that fixes does contain new commits: the ones shown above on this PR.\nAdmittedly this is only one small fix to Makefile (ab563cc4098792e6f7bc699cdb0c3a0405fc5467 ) plus some excessively noisy merge commits. However, that Makefile fix does seem to be needed to make the Getting Started instructions work reliably i.e. for make -j to not fail.\nOr do I have a mistake somewhere here?\n(Too bad that the lab is offline and so SnabbBot can't run tests.)\n. I should have merged fixes onto next before the release but goofed. have not mastered the multibranch workflow yet.\n. These landed on next and will be included in the July release.\n. No problem :).\nYou are welcome to mirror your branch on the SnabbCo repo if/when you want to promote it for other people to check out. Just send a PR to add it to src/doc/branches.md.\n. This is impressively simple on a few different levels.\nI really like how simple basic apps like sink have become.\nI also really like the way the Lua API is designed for VMs to be managed by C but then the C FFI is so powerful that you can even more easily control them from Lua :-).\nLooking forward to taking it for a spin. Looks like the file core/stats.lua is missing at the moment.\n. Thanks Javier! This runs now:\n$ sudo ./snabb snsh -t apps.basic.test\nlink report:\n          34,917,405 sent on source.output -> sink.input (loss rate: 0%)\nI would like to spend some time with this and aim to merge it for next month's release.\nI suppose there is nothing stopping you from attacking N:M threading now :-)\n. This is very interesting code to read. I am quite fascinated that engine.configure() does not need to know that some apps are now in their own sandboxes, because behind the scenes config.app() is creating backwards-compatible app \"classes\" that automatically create a sandboxed object when instantiated with new() and forward method calls to pull(), push(), etc. I had expected the meat of this change to be in engine.configure() but that code did not actually have to be touched at all. Clever.\nI will come back and read through sandbox_loader.lua carefully. That code is quite intimidating at first glance :-)\nI am also reflecting on the increasingly many use cases that we have for shared memory these days: sharing memory between Lua states, sharing memory between threads (potentially), sharing memory with other Snabb processes (SnabbTop), sharing memory with external application (Alex's NetSNMP plugin). I wonder if there is a simple way that we could consolidate the code for these.\n. @javierguerragiraldez This branch seems to be in quite good shape now. How about if you move your new development onto a new branch based on this one and shoot for a 10x speedup from parallelism e.g. running many copies of this same app network?\nI would like to spend some time working on core/ to make sure that we can bring in new developments like sandboxes and SnabbTop's statistics changes while keeping the code very compact and consistent. So it would help me to have this branch in a stable place so that I can hack on top of these commits while making everything fit together in the way that makes best sense to me.\n. @javierguerragiraldez I would like to merge this code onto a branch of my own and work on integrating it in the smoothest way with core.shm (#513) and SnabbTop (#472). I would close this PR and open a new PR from my branch that includes these commits. I am hoping there will be opportunities for cross-module simplifications. Would that be okay with you? (You should be able to keep doing the parallel hackery based on this same version and we can do a merge later when all parts are ready.)\n. Closing this PR because we have had a productive review and I think if/when/how we merge sandboxing will depend on the parallelism model we choose. (Can reopen if my understanding here is dodgy.)\n. Can you give some more information please?\n1. What platform are you on? (OS/distro/version)\n2. Can you paste the output when it fails?\n. Please reopen if you want to follow up on this with more information about your environment.\nCurrently we are depending on a GNU toolchain (where basename -a exists) and this is the first I have heard about http:// not being a suitable method for accessing Github.\n. Thank you for reporting this. It seems from browsing Debian manpages that basename -a is a new feature that arrived in Debian Jessie. I had assumed it had been around forever.\nI will look into a fix.\nI suppose this is a good motivation to support efforts like #483 that are generally reducing our dependence on GNUisms.\n. Does #510 fix this for you?\nThe easy way to check should be to checkout the next branch where I have merged this along with other features for the next release:\n$ git checkout next\n$ make\n. @eugeneia Could SnabbBot please retest this PR and also #507? I had not pushed the update of the submodule into our SnabbCo git mirrors.\n. @eugeneia Can we try again please? I pushed the new v2.0.4 tag now too.\n. Closing until compatibility issue with latest LuaJIT is resolved.\n. We are doing this now.\n. Could we use atexit?\nljsyscall has a Lua interface to to this I believe, @n-nikolaev showed me how to use it once to call a Lua function on shutdown. However, I don't see it available right at this moment (see justincormack/ljsyscall#180).\n. Looks like I misunderstood atexit(): it is only for orderly shutdowns.\nThought: it is not possible to handle all kinds of errors in the process itself. For example, heap corruption, kill -9, out-of-memory killer can all break any in-process cleanup logic. So there needs to be a separate mechanism to cover those cases. If we are going to have a separate cleanup mechanism anyway, why do anything on process termination at all? That would seem to only be an optimization to make cleanup happen faster.\nJoe Armstrong argues persuasively that error handling should always be done outside of the process that is crashing. If interested see \"Error Handling Philosophy: Let some other process fix the error\" in his thesis Making reliable distributed systems in the presence of software errors.\n. Other thought: what are the use cases for this mechanism? (are there things we need to do for which signal-driven cleanup is the best solution?)\n. Crazy idea that may or may not have a precedent...\nCould we fork() a process that will detect when the parent process dies and perform the cleanup? That would seem to be immune to most kind of errors that I can think of.\n. Sounds good. I reckon we will be able to handle the DMA case too. (Though on that specific case maybe we will want to do it in the main process so that it happens synchronously before the kernel has the chance to reuse memory that will be DMA'd.)\n. (Nevermind: This is already bundled in #474.)\n. My motivation here is that I would like to shrink the size of the core code.\nThis is growing a bit now in that SnabbTop is using lib.ipc from the core.app module and the sandbox branch is also doing significant work to pass objects between VMs. I would be really happy if we could have a second look at the whole topic of shared memory IPC and find a simpler solution that works for everybody.\n. I have pushed a redesign based on the early feedback so far. How does it look?\n@javierguerragiraldez I like your interface! I have rewritten the code to be similar. I kept the explicit unmap() and unlink() operations because I think that doing these things on a garbage collection hook is scary and will create problems that are hard to debug. (I think the unlink() is done too early in your example btw and that each call would allocate and return a different object.)\n@alexandergall Thanks for the initial assessment, I am interested to hear your follow on thoughts :-). If this code would simplify the SNMP plugin then that would be the icing on the cake. The main motivation though is to support new use cases for which lib.ipc.shmem would be overkill e.g. for multiple Lua VMs to have a shared freelist for packets, shared link structs, and so on. This would allow us to remove code from the SnabbTop and SandboxVM branches. (I would also be happy to avoid having core.* calling lib.* as on the SnabbTop branch because that feels like a layering violation.)\n. The reason I dropped storing the type with the object is that this was an awkward feature. You don't need to store the type in simple cases, and you probably don't want to store it in that exact way in complex cases. So now it is out of the way and the caller can deal with types however it wants e.g. hard-code them or store them in suitable metadata file(s).\n. I have extended the syntax for object names. Does the idea come across clearly? Is this better or worse?\nThe intention is to make it easier to name and organize objects. Particularly, the engine could set shm.path = app.name when switching between apps and then it would be easy to create app-local objects e.g. shm.map(\"counter/drops\") creating /var/run/snabb/1234/app/myapp/counter/drops. Then we would have no excuse not to define multiple counters for each and every app & find a way to conveniently monitor them.\nThis also hopefully brings the code and directory layout more into line with SnabbTop on #472.\n. @alexandergall Cool. Great that the low-level shared memory objects and the higher-level abstractions like well-defined SNMP MIBs can be independent layers. I suspect we will hit some new challenges on the higher levels soon e.g. to aggregate counters split across multiple processes/VMs. One step at a time though :).\n. I added a core.counter module intended to make counters so incredibly easy to work with that literally every module in Snabb Switch can use them. This is a replacement for lib.ipc.shmem.counter which is roughly my previous attempt at facilitating ubiquitous counters that never really took off.\nSo now we have three layers in terms of counters:\n1. Generic shared memory objects: core.shm.\n2. Counters as uint64_t shared memory objects for ad-hoc use: core.counter.\n3. Curated collections of objects intended for external applications to consume: lib.ipc.shmem.mib providing data for a NetSNMP agent.\nThree layers is a lot but I feel like this is moving in a reasonable direction. The counter module is not really an abstraction but rather a few convenience functions.\nHow does this look?\nIf this direction still looks promising then the future steps to gradually take would be:\n- Use core.shm for internal objects e.g. freelists, links, etc so that they are available.\n- Take advantage of this in the VM sandbox branch.\n- Use core.counter to start tracking all manner of interesting events inside Snabb Switch.\n- Take advantage of this in SnabbTop to give more visibility into Snabb Switch processes.\n- Update the MIB code to use core.shm and core.counter in whatever way makes the most sense.\n. This PR was based on v2015.05. I updated it to v2015.06 with git merge master. Seems to have work out neatly! Guess we don't need to rebase for this case.\n. Ouch, no, I goofed that. I should have used git merge origin/master because my local master branch was not clean. So I have a couple of unwanted submodule updates bundled onto this branch now. I will live with that for now because a rebase would probably be too destructive. Have to resolve somehow before merging this branch.\n. Yuck... I made a mess, fixed it with rebase, and hopefully this does not have unintended consequences. Seems like in this case rebase did not completely screw up the pull request when I rebased back onto an earlier commit of this branch. This was risky and probably was not a good idea.\nThe net result is that I have merged v2015.06 into this branch cleanly now whereas before I had merged a non-pristine version of master that introduced some regressions with submodules.\n. @eugeneia I am happy if you want to charge ahead on that and make use of this code. This might be a good time to merge this PR onto next to make it easier to build on for you and @javierguerragiraldez?\nI have an app that I want to write to explore some of the new ideas we have been talking about lately so I would happily context-switch to that.\n. Shm objects are all initialized to 0 by ftruncate (). Should be documented\n. Value defaulting to 1 sounds eminently sane \n. I merged this branch onto next.\n@eugeneia and @javierguerragiraldez if you want to PR code that depends on core.shm you could make next the PR target instead of master and then it should be less noisy i.e. not show these core.shm changes that you merge when they are already on the target branch.\n. Merged onto next. Closing PR.\n. Interesting test failure. Looks like luajit and pflua upgrades each worked individually (#506 and #507 passed CI) but seem to have trouble when combined together and with the latest Snabb release? I will take a look at this tomorrow.\n. Interesting. SnabbBot is failing the current build with program.snabbnfv.nfvconfig triggering an error in pflua (log). However, this is not reproducible on my development machine.\nI wonder if recent LuaJIT versions have introduce changes that break pflua? That seemed to happen with issue Igalia/pflua#225 which reproducibly happens only with the LuaJIT update that we have on this branch.\n. Interesting idea to use the stable LuaJIT v2.0.x. I am not sure if that would be realistic or not. Snabb Switch has used the v2.1 branch from the beginning and there are at least some new features of interest, like the profiler and 64-bit bitops.\nSo var v2.1 has been quite good to use and I do think it's supposed to be fairly stable. I think that Cloudflare run v2.1 in production on billions of HTTP requests per day. Could be an idea to ask them how they handle updates.\n. Seems like the LuaJIT+pflua upgrade problem is caused by bug(s) in the latest LuaJIT. So with hindsight it would have been better to revert LuaJIT upgrade instead of pflua.\nHopefully LuaJIT will have a fix shortly and we can merge that. Otherwise we could revert the LuaJIT upgrade and un-revert the pflua upgrade for this month. Let's give it a few days and see.\n. @eugeneia any insight into this test failure? Seems like the tests passed by performance regressed to 0? :-)\n. Awesome work @kbara and the pflua team! I prepared PR #520 to reintroduce the pflua upgrade together with the compatible LuaJIT and will merge that here once it passes CI.\nYour hard work to create minimal test cases is a real contribution to LuaJIT and Mike's to collection of reproducible test cases for the future.\n. @eugeneia Merged as requested. I am ready for release if you/SnabbBot don't see problems.\n. This looks good to me.\nIncidentally I recently read a way to globally disable tabs in Emacs:\n(setq-default indent-tabs-mode nil)\n... whereas I have been doing this on mode-specific hooks for untold years.\n@alexandergall what editor do you use? can you easily switch from tabs to spaces?\n. It is very interesting to consider what is the best way to merge cleanups like this.\nThere is a bunch of development happening in parallel, mostly based on master, and changes that touch many files have potential to cause merge conflicts. So how should we deal with that?\nIf there are conflicts then it seems best to deal with them all at once and for this to be done by somebody who understands the context of what is going on. It would be bad if lots of people get surprised by conflicts when running git pull master and have to work out what is going on.\nHow about if we merge this onto next towards the end of the monthly release cycle? That way we have a chance to merge all of the currently open PRs ahead of this change without conflicts. Then we can merge this (resolving any conflicts from the merged PRs) and quickly release it onto master. The only conflicts for contributors will be for PRs that don't merge on this release: they might see a conflict when they git pull master next month to sync with the latest release.\nIn this case we would leave this PR as-is for now and in a week or two I will ask you to please git merge nextonto this PR branch and resolve any conflicts and then resubmit the PR with branch target next.\nThanks for this initiative Javier. It's great for a few reasons: cleans up the code, helps us work out a cleanup workflow, and perhaps opens the door for more nice cleanups. There are a couple of other cleanups that I have been considering but did not do because I was too lazy to think through the implications with respect to conflicts.\n. btw: another interesting topic is how to choose the right target branch for a PR.\nThe safe choice is master and that should always be fine when in doubt. PRs sent to master can easily be merged onto any other branch (fixes, next, etc) because it is based on a common ancestor.\nThe exception is when you are building on other changes that are not on master yet. Then it is better to send the PR to a branch that includes those changes (or to wait until they land on master). So for example code that depends on core.shm or the new logging facility that Alex wrote can be sent to next since those changes won't land on master until next month.\nCleanups like this are a grey area. On the one hand it does not explicitly depend on any other changes but on the other hand it does implicitly depend on every other change that touches the same code. So I think the above strategy of sending it to next at a strategic point in time makes sense.\nI am still enjoying the Git workflow geekout... :-)\n. also: once more thanks Javier for taking the initiative on this. This kind of \"gardening\" to keep the project neat and tidy is very valuable.\nI like the idea of making cleanups and consistency improvements into a pleasantly meditative background process that we can all do when we feel like it. That seems much more pleasant than trying to enforce strict rules at all times e.g. having a CI that automatically rejects changes containing a tab character or a line over 80 columns, or making code reviewers into style police.\nFurther thought: one day somebody could maintain a cleanups branch. Then when people send in cleanups to master at random times they could be collected on the cleanups branch and then merged cleanly with next towards the end of the release cycle. The way to volunteer for all such work as this would be to send a PR advertising a branch to branches.md.\n. @alexandergall I am also bothered by that. I am occasionally running over 80 columns as a consequence. I wonder whether we should adapt our programming style or look into customizing/extending lua-mode. Do you have some particular examples to point to? I don't have any in mind right now.\n. Merged onto next.\nLooks like there is even a command git blame -w that will skip over whitespace changes so that should make the impact on our Git history more manageable.\n. Closing this PR. Expecting these commits to merge in a PR together with SnabbTop.\n. Hallelujah!\nThis is really important. Seems like every time I hack my bench_env config I end up burning half a day to make it work again. It would be wonderful to make this much easier to run.\n(Aside: I also pondered our e.g. Intel NIC selftests. Could be that they could automatically find a safe NIC to test and use VMDq or hardware loopback mode instead of depending on two ports cabled together. Would be neat if those tests would also run with zero setup on any machine that has suitable hardware not already in use.)\n. I have setup the port forward now.\n. Can you please put a tag like [draft] into the PR title until you are happy to see it merged into other branches? That will be more obvious than the note in the PR text because it is visible also when browsing the PR list.\n. Exciting!\nI would like to host the assets somewhere other than the lab. The lab is not always online and its uplink is much slower than its downlink.\nI wonder what is the easiest way to host them on a CDN? (Dropbox? S3? Google Drive?)\n. I disagree. The lab is a pain compared with hosted services. It's much more prone to connection outages, hardware failures, accidents at the root shell, getting hacked, etc. It has no redundancy, no RAID, no backups, it gets powered off during storms, and it doesn't always have a pair of hands available to fix it.\nI would like to work on diversifying. I think our core infrastructure should all be on professional hosted platforms (Github, etc) and that our supplementary infrastructure is more spread out (e.g. servers with interesting hardware to run tests on).\n(Just as I write this there are two lab servers offline, chur and jura, and the server hosting snabb.co also went offline today. Solving these problems with unix sysadmin'ery feels very old fashioned to me.)\n. One very important test case that is often hard to setup is the benchmark pushing packets through a DPDK virtual machine. The benchmark should run N parallel instances and measure how many packets/second can be forwarded through them. This has been the key benchmark in our NFV optimization efforts and it is really a pity that we don't have it hooked into the CI yet.\nCan test_env help with that?\n. This is a great step forward, Max :). The NFV selftest is much easier to run now. Thoughts follow!\nRhetorical question: Who should use test_env and why? The README defines it as a replacement for bench_env but most people will not know what that is. Is test_env a component of the snabbnfv test suite? When would you use it directly instead of via program/snabbnfv/selftest.sh?\nCould tmux simplify test_env? I mean to take care of spawning processes, logging their output, terminating them as a group, and being able to attach to and manually restart individual processes when troubleshooting. That works quite well with the OpenStack devstack test script (using screen). Managing processes manually is less intuitive for me... e.g. to terminate the processes am I supposed to kill the shell that sourced test_env so that the shutdown hook runs?\nCould this be the basis for a \"Getting Started with snabbnfv\" guide? That is sorely lacking in snabbswitch now: a really trivial runnable reference example of how to boot up a VM inside snabbnfv. (Have to think harder about the mailing list threads where people have struggled with qemu version, qemu args, iommu settings, etc.)\nShould we move this under program/snabbnfv? Is it usable for other programs? (@alexandergall, did you end up using bench_env for your VPN? How?)\nCan we delete bench_env once we merge test_env? If not then why not?\n. Incidentally this is how I ended up running test_env:\nsudo TESTPCI=0000:01:00.0 TELNET_PORT0=5000 TELNET_PORT1=5001 program/snabbnfv/selftest.sh\nand it looks like the test suite ran OK. Slightly ambiguous because I do see errors scrolling by and have been assuming those are expected e.g. this timeout:\nTrying ::1...\nConnected to localhost.\nEscape character is '^]'.\nnc -w 1 -q 1 -v  fe80::5054:ff:fe00:0000%eth0 12340\nnc: connect to fe80::5054:ff:fe00:0000%eth0 port 12340 (tcp) timed out: Operation now in progress\nroot@vma:~# \nroot@vma:~# Connection closed by foreign host.\nFILTER succeded.\n. @eugeneia Recently I have been running the DPDK-based test manually. Here is what I do: I start a loadgen process on one port and a qemu process running the DPDK guest+kernel and I leave those running. Then when I want a test result I run snabbnfv and it connects everything together until I kill it. I find it really convenient to leave the loadgen and guest running because it means I can take a performance measurement in under a minute.\nIt is a long time since I ran the whole shebang under bench_env. That is very nice when it works but I always get tangled when commenting/uncommenting bits of my bench_env.conf to switch between test scenarios.\nSo I don't have a fully working reference setup at the moment.\n. Can we host the test assets on Github? They could have their own repo and we could even link that as a submodule e.g. program/snabbnfv/test-assets. I have the impression that Git has some extensions for dealing with large files and that Github supports this use case up to a point, but I am not sure what point that is :-).\nSounds like test_env can be considered a part of snabbnfv. Let's move it under program/snabbnfv/. If other programs adopt it in the future then we could move it somewhere more common again.\ntmux is basically a scripted multi-window terminal app. You can use commands/scripts to create a new window (terminal/shell) and you can send input to the window and log its output and so on. The killer feature from my perspective is that you can drive it both manually and with scripts. So even if test_env has everything highly automated you could attach your terminal to any of the processes to see their output directly, hit them with an ^C, etc. It is a bit like if everything were in Emacs and you could switch to the e.g. the *snabbnfv* buffer and poke the process directly when you want to.\nCould even be that tmux would simplify sending commands to VMs via their stdio but I don't know if that would really be an improvement.\nCheck it out and see what you think?\n. also a thought: is it important for test_env to be based on shell functions that are sourced, or could it also be based on stand-alone scripts e.g. to start QEMU with the right arguments? Separate scripts could be handy for ad-hoc reuse in other contexts. Generally I am a lot more comfortable with adding a directory to my $PATH to access some handy scripts like snabbnfv-qemu than with sourcing a library of shell functions.\n. @kbara Good point. It would be neat to have more flexibility about which app instances are used for an application. For example, the NFV application could just as well connect a VM with a PCAP file, or a raw socket, or whatever, as to a VMDq queue of a NIC. Likely we could even write a simple load generation app that has equivalent behaviour to a hardware NIC (in terms of cache behavior etc) and do a bunch of testing on laptops with that.\nCould be that we could build generic Input and Output apps that take a URL-like string to say what they should attach to (raw:eth0, pci:01:00.0, pcap:foo.cap, etc?) This ties in a little bit with topic #522 and whether it makes most sense to have discrete Input and Output apps vs combined IO apps. That feels like a pretty fundamental design crossroads.\n. @eugeneia I love the new test_env :). I am testing the tmux branch and I really like being able to flip between the processes (snabb, qemu0, qemu1) and see them running. Running nested tmux instances even seems to be quite straightforward (hit escape key once for outer tmux, twice for inner tmux).\nIf we are using tmux then we don't need to track $snabb_pid anymore, right? We can just kill the tmux session and it should take down all the child processes with it? (Looks like that is how you are doing it already?) So maybe shell scripts instead of shell functions is easier to adopt in this model too?\nIt is easy to imagine generalizing this to other use cases like @kbara's too, e.g. if you want to automatically start two QEMU guests with well-known vhost-user socket names, but then be able to start/restart the snabbswitch instance manually. But if the qemu startup is a stand-alone script then perhaps you just call that manually instead of using the framework to start the tmux.\nI am keen to merge this :).\n. OK. Maybe I am pushing to do too much at the same time. Could be better to keep it simple and merge test_env as a basic replacement for the old NFV selftest. That is an important thing to have and it does already do this now.\nCould be that manual testing should be done in a different way instead of trying to reuse parts of test_env e.g. for starting QEMU processes. This is something we could look at separately.\nI definitely think it's more important to keep these scripts simple than to try and cover lots of new use cases from the start.\nWhich of the branches are you most happy with?\n. Basically yes. The same_vlan.ports should work fine for example. The only caveat is that if loadgen is blasting garbage packets then you need to be careful about things like packet filters that could drop all the traffic.\n. Merged onto next. Closing PR. Please create a new one for any follow-on changes.\n. Hi Pavel,\nCan you explain a bit more context about how your software works today and how you want to change it?\nYou are using ixgbe? with PF_RING? and VMDq or SR-IOV? The problem you want to solve is lack of an ioctl/ethtool trick to enable mirroring between queues in this situation? Or you want to do a Snabb-based prototype of FastNetMon?\n. So if I understand correctly you are looking for a way to capture all packets sent/received on the NIC without interfering with the normal traffic processing by e.g. the Linux kernel. The mechanism you have in mind is to provision new hardware RX queues in \"mirror\" mode that your application can attach to. This is supported in hardware, but the problem is that the standard Linux driver has no way to enable it. So you are looking for the simplest way to enable this.\nIs that correct?\nI am interested in this topic: How to make multiple applications play nice when they all want to access the NIC hardware directly. In principle SR-IOV could be a nice way to take care of this but in practice Intel's implementation is pretty limited. So it seems like a mechanism operating directly on TX/RX queues makes sense.\nThe full solution would have a few components:\n1. Driver to initialize the NIC. For example, ixgbe, or a derivative like the one from PF_RING/DNA, netmap, or Intel's bifurcated driver.\n2. Interface to setup queues the way we want them e.g. provision new mirror ports. For example, ethtool talking with a kernel driver.\n3. Applications that independently attach to individual TX/RX queues. For example, PF_RING, netmap, Snabb Switch, etc.\nThis would be a neat ecosystem. Then Snabb Switch could have \"apps\" to attach to an RX or TX queue and they would not really care how the queue was provisioned: by our own NIC driver or by ethtool/ixgbe on the host. It should work for your application too.\nSo what is missing? If it is only an ethtool command to provision a mirror queue then the best solution might be to implement that and submit it to ethtool/ixgbe. Even if Intel do not have a roadmap to implement this themselves it could still be the best solution.\nIf you really need something in a hurry there should be multiple possibilities for a quick hack, including using the Snabb Switch driver to add new mirror queues after ixgbe has setup the NIC. This kind of hack usually works to a first approximation but is fragile and I would not especially recommend it.\nI would be curious to hear the perspective of Intel engineers who are working with the bifurcated driver and related areas. Let us see if we can attract their attention and get their feedback.\n. ... actually perhaps the direct hack would not be so bad with SR-IOV VFs when you already have a queue dedicated to your application and you only need to enable mirror mode on it. Let me ponder..\n. Does your application already support operating on a VF instead of a full NIC? (I am not up-to-date on the latest internals of netmap and PF_RING to know whether they support this.)\n. If you are using netmap then you already have a modified ixgbe driver. Could be straightforward to add mirroring support directly to the netmap ixgbe? Then you would have the whole solution in one place.\n. I would suggest asking the netmap and/or ixgbe community about that. If that turns out to be a dead-end for some reason then we can look into how to help you with Snabb Switch. However, it seems like overkill to use Snabb Switch just to poke a register on a device controlled by a different driver.\nAre you familiar with the 82599 data sheet? That is a really nice document, once you get to know it, and there you can find details of how to enable mirroring in the driver.\n. Here is our code to enable mirroring: https://github.com/SnabbCo/snabbswitch/blob/master/src/apps/intel/intel10g.lua#L987.\nI don't think this particular feature has test coverage so beware in case there is a bug. The code probably only makes sense with reference to the data sheet.\n. I have not tested performance of this feature. We implemented it in the driver but do not currently use it in any applications.\nI would imagine the PCIe bus will the the bottleneck. Packets will have to be DMA'd twice. The PCIe bus has around 16 Gbps of bandwidth but there is additional overhead to be counted in that. I would guesstimate you should see ballpark 2/3 of line rate.\n. Good luck and let us know how you go :-)\n. Closing issue since there is no action pending on Snabb Switch.\n. Merged onto next.\n. NB: The I350 driver code linked above is very quirky. I am just playing around with ideas there for now.\n. Playing around with this idea a bit. Here is a diagram of a hypothetical design for an Ethernet switch showing two different ways that each port could be implemented: with separate TX and RX apps feeding into parallel paths of the app network (\"Separate RX and TX\") vs with a series of apps each handling bidirectional traffic (\"Unified RX+TX\"). I imagined that each port would use two processes (cores/hyperthreads) and I used a distinct color for apps that would run together in the same process.\n\nI suppose it would be hard to argue that the new-style \"Separate RX and TX\" design is fundamentally better. It has more apps and that probably makes it both more parallelisable and also more complex. The extra parallel opportunities don't seem that exciting because the more traditional design seems to have plenty too. So at first glance it seems like more complexity with no really certain benefit.\nSo for the moment this leads me to think we may be better off sticking much closer to the existing intel_app that we have. It seems like we could pretty straightforwardly extend that to cover all scenarios we are talking about:\n- Option to attach multiple apps to the same NIC from different processes.\n- Option to send RSS.\n- Option for each app to use 0..N RX queues and 0..M TX queues.\nThe next radical possibility, bouncing even further back in the other direction, is that we could make links in the app network bidirectional. That would make them more like physical ethernet links and so constructing app networks would be more closely related to constructing real networks. Just a thought.\nThis was also an interesting exercise in using Graphviz to sketch app networks. It was kind of a pain but maybe that gets better with practice. I have some arrows pointing the wrong way because it was a pain to try to fix. Here is the source for dot file - tips welcome:\ndigraph labswitch {\n  graph [style=dotted ranksep=0.3]\n  node [style=filled]\n  subgraph cluster_port1 {\n    label=\"Separate RX and TX\"\n    node [color=lightblue]  RX1 -> RXFilter1 -> RXRatelimit1 -> Switch1\n    node [color=tomato]     TX1 -> TXFilter1 -> TXRatelimit1 -> Switch1\n  }\n  subgraph cluster_port2 {\n    label=\"Unified RX+TX\"\n    node [color=violet] NIC2 Ratelimit2\n    node [color=green2] Switch2 Filter2\n    Switch2 -> Filter2 -> Ratelimit2 -> NIC2\n    edge [constraint=false]\n    NIC2 -> Ratelimit2 -> Filter2 -> Switch2\n  }\n  rankdir=LR;\n  Switch1 -> Switch2\n  Switch2 -> Switch1\n}\n. Gonna close this for now. I will experiment with the labswitch program.\n. (I'm not sure why this failed CI. It works when I cherry-pick it onto the vpn branch where the root problem is found. Some revision likely needed anyway.)\n. It would be neat to have a benchmark that is CPU bound and does only simple packet forwarding. That could be used to test optimizations of the basic I/O facilities.\nIn the case of DCA I believe that this is legacy technology from older CPUs that had an external PCIe controller (before Sandy Bridge). I believe that the successor DDIO works automatically provided that the CPU processing traffic is the same one that the NIC is attached to (now that each CPU has a private PCIe controller). Anecdotally we have seen ~30% performance difference between using the CPU that the NIC is attached to vs. a second CPU, and this may well be the effect of DDIO.\nI would actually love to optimize the \"wrong NUMA\" setup and have a benchmark for that too. I am sure that it will be common for NUMA affinity to be misconfigured in practice e.g. when working with complicated cloud computing middleware.\nClosing this issue now in favor of #524.\n. Merged onto next. Closing PR.\n. Merged onto next.\n. Sorry to be boring but closing this one because I think we need a different portability strategy than POSIX compliance, for example rewriting all the C code with ljsyscall.\n. Thanks Justin! Would you please patiently help me to understand some portability issues from first principles? :-)\nI'm thinking that we need two things:\n1. Definition of which platforms we support (hypothesis).\n2. CI test coverage of multiple platforms (attempt to disprove hypothesis).\nSo for example suppose we say that today the master branch supports \"Compile on Linux/x86-64 with GNU toolchain, deploy on Linux with GLIBC >= 2.7\". On the one hand that is quite narrow compared with all the interesting platforms in the world (POSIX, musl, NetBSD, Windows, etc). On the other hand it is quite broad compared with what our CI is testing (only Ubuntu 14.04 LTS w/ check for linkage to newer GLIBC symbols).\nSo we have multiple related problems to work on here:\n1. Maintain a correct definition of the platforms that we support.\n2. Shrink that definition when we are able to demonstrate non-working platforms.\n3. Expand that definition when we are able to demonstrate more working platforms.\nDoes that make sense?\nIf we run with this mental framework then it seems like the portability-related changes that we want to land on the master branch are those that directly contribute to one of these areas. That is, we want to merge changes that improve test coverage, fix problems with already supported platforms, or add support for new platforms.\nComing slowly back on topic :-)\nI think it makes most sense for portability code to be developed on a branch and then merged onto master when it reaches the point where we can say \"This branch adds support for platform FOO, we know it works because CI TESTS, and the branch history shows that the maintenance workload is modest and people are willing to do it.\"\nThe risk I see of merging piecemeal changes onto master is that on the one hand a port only benefits users when it is complete, and on the other hand it immediately makes general maintenance harder because there will be a bunch of stuff that works fine and passes tests but is vaguely taboo e.g. using features of GNU make, bash, GNU awk, and so on.\nComing even closer to being on topic :-)\nI really don't want to chase away people who are interested in other platforms. I just want to make sure that portability work is done in such a way that it is a net benefit to the community.\nHow about we do one of two things with patches like this:\n1. Find a way to describe and test how it improves our platform support. For example, could these small fixes be bundled together as a way to switch to using stricter CFLAGS?\n2. Maintain the patches on a branch (e.g. posix) if it will take some time to collect enough changes to really improve support for a platform, have tests to show that it works, and be confident that the community will maintain it.\nIs this making any sense?\nI hope that I am not chasing away people who are interested in portability. It is a very important topic. I just want to make sure we do it in a good way. \n. To me it seems like we need somebody to take charge of portability by maintaining a branch that is more portable. You seem very well qualified to be that person, having a lot of relevant experience and strong opinions. Are you interested?\nI am happy to merge portability code onto my branches when it adds support for some identifiable and relevant new platform (whole ports, not partial ones), when it is maintainable (has tests that will catch regressions), and when it is actively maintained (people seem to care about it enough to keep it working).\nTo me a new \"platform\" could be as simple as a different CFLAGS that would cause gcc to reject code without this fixes, provided there is a reasonable justification for why those CFLAGS are better.\n(You know, what I really want to do is rewrite all of this C code in Lua with ljsyscall instead of spending our time on porting it. To me it is not yet clear whether substantial ports of the C code will be possible without making dealbreaker changes like introducing autoconf. I see a risk that the traditional ways of making Unix software portable are not going to be palatable for Snabb Switch.)\n. Can you tell me exactly how to reproduce this at the shell on davos? I'll take a look.\n. I am rebooting davos with the IOMMU disabled in order to eliminate that as a potential cause. The default behavior of Linux when the IOMMU is disabled has been inconsistent at best.\n. Er, right :).\n@eugeneia  I have disabled the IOMMU and the problem is not immediately reproducible for me. I am not sure if it is actually fixed though. Can you reproduce it now?\n. Is this fixed by #531 or by the IOMMU change? (If the IOMMU is believed to be innocent then I would like to re-enable it so that we don't falsely blame it for voodoo problems.)\nNB: For some reason it seems like the default kernel in grub for davos is not bootable. I brought it up manually last time. So at the moment if you reboot it and it doesn't come up then that is probably why :-)\n. On Ubuntu 12.04 the IOMMU was disabled by default and when you enabled it then DMA would be blocked with messages like the one you show above. Quite predictable.\nOn Ubuntu 14.04 the IOMMU was now enabled by default and the behavior has been much less clear cut. Often everything works fine and occasionally we see strange behavior such as >50% loss of throughput or these test failures you saw.\nI would like to identify the right kernel hacker / mailing list to follow this up with. Trouble is, I believe the kernel people would all suggest using the vfio mechanism to access PCI devices, but I don't feel that this kernel feature is mature enough for us yet. (That could also be something to discuss.)\nThere are actually three modes to the IOMMU: on, off, and passthrough. I wonder if passthrough is really okay? That would be convenient because then ideally it should not affect Snabb Switch but QEMU could still use the IOMMU to make whole PCI devices available to virtual machines (an important use case).\n. (See also #455.)\n. @eugeneia Looks like SnabbBot is not running this packetblaster benchmark. I don't see the result in the log. Something missing? (Can fix in next release / bugfix release.)\n. Howdy! Sorry I didn't dig into this yet. Here are some quick ideas:\nYes, davos is cabled. (Every 10G port in the lab is cabled to itself except for some quad-port Broadcom NICs that we haven't done anything with.)\nIs the destination MAC address in the traffic you are sending 00:00:00:00:00:00 like in the config file? That is required to make the NIC send them to that VM's RX queue. If you have 00:00:00:00:00:00 everywhere already then maybe there is something special/reserved about that address. I'd recommend replaying a trace that sends packets to FF:FF:FF:FF:FF:FF which is broadcast to all queues. (I believe Nikolay has been testing with packets containing all 0xFF -- should be able to find those files on grindelwald even.)\nTry running snabbnfv traffic -D 10 to get a debug report every 10 seconds? The statistics registers in the NIC often provide a hint to what is happening.\n. Fixed by #539.\n. (Accidental duplicate.)\n. @eugeneia Could I tempt you to take a look at this one and the related #537 and #535?\n. @eugeneia I'd like to release this as v2015.07.1. There are good fixes here that will reduce user annoyance. Should I just do that or would you like to?\n. @eugeneia Actually, I would be quite happy to do this one myself. OK with you? with both of us doing releases (e.g. you feature releases and me fix releases) then we can build on each others' tricks and workflows.\n. (I imagine that this time next year the actual releases will be done by cronjobs and we will only be responsible for having the RC branch in shape when the time comes around. However, I don't know how we would come to that point, or if that is really how it will work out :-))\n. This time I generated the release notes automatically with Git commands. The hand-written version could still be better for larger feature releases though. I don't think we have an urgent need for more automation, it is just something in the back of my head.\n. I missed merging #548 before the release. Oh well - next week.\n. @eugeneia I have nothing more to sneak into this release so over to you :).\n. @eugeneia Merged! \n. Thanks!\nIs this fixing a bug that will annoy users? (What happens?) I can merge it onto the fixes branch for quicker release in that case.\n. OK. Generally it would help me a lot for the commit message to give the \"why\" of bug fixes i.e. how the problem affects the user. That helps me work out how urgent it is to release the fix.\nCurrently I'm not sure whether this is a \"snabb doesn't start due to file not found error\" problem or a \"directory is mysteriously missing when you look for it in ls\" problem. I can test that myself this time :).\n. @n-nikolaev Can you explain more context please? I am testing on the master branch and if I remove /var/run/snabb and then run make test it all passes and I do have a /var/run/snabb directory created afterwards. How are you testing?\n. No problem. Thanks for the follow up.\nThere could conceivably also be a code path somewhere that is not creating the directory properly since we have not ported everything onto core.shm yet, but if there is I don't immediately know what it would be.\n. I have seen this problem now when running packetblaster after a reboot:\n$ sudo ./snabb packetblaster replay ./program/packetblaster/64.pcap 0000:01:00.1\nlib/ipc/shmem/shmem.lua:196: /var/run/snabb/3644/core-stats: No such file or directory\nstack traceback:\n        core/main.lua:118: in function <core/main.lua:116>\n        [C]: in function 'assert'\n        lib/ipc/shmem/shmem.lua:196: in function 'init'\n        lib/ipc/shmem/shmem.lua:218: in function 'new'\n        lib/ipc/shmem/top.lua:23: in function 'new'\nThanks for the fix. I will merge. I really took in the SnabbTop branch too early. This release we need to bring consistency to our counters and shared memory objects and so on.\n. Great! I will read through and give detailed feedback.\nLet me braindump a little on how a documentation update like this will make it into a release, based on the new Git workflow that we have recently started using. (I write this for the benefit of everybody involved in the process so that we will all be on the same page - important since we are still bootstrapping this model.)\nFrom your perspective as the author of the change it should look pretty straightforward:\n- Submit update in PR.\n- Get feedback and make revisions.\n- Change is merged upstream.\nThe wrinkle then is that \"upstream\" doesn't mean landing directly on the master branch but rather starting on the most appropriate well-known branch and then migrating onto master via a short series of hops. For a documentation change the path to master is likely to be topic->documentation_fixes->next->master.\nIn more detail:\n- @eugeneia (Max) merges the change onto his documentation_fixes branch where he is integrating all changes to the documentation. He might make some edits himself on this branch e.g. for the global consistency of our documentation.\n- @eugeneia sends a Pull Request to merge all changes on documentation_fixes upstream.\n- @lukego (me) merges these documentation fixes onto the next branch to queue it for the next release.\n- @eugeneia merges the next branch onto master at the start of a calendar month to make a release.\nThis sounds complicated, and it is to some extent, but the value is that it will make sure a short chain of people (@mwiget, @eugeneia, @lukego) are all happy with the change and that it hits the master branch as part of a coordinated release. Meanwhile everybody in the community is able to give feedback on the changes at every step in the process. Github makes it pretty easy to ask specific people for their feedback by mentioning their name (e.g. @lukego) too.\n. We are consciously adopting the Linux kernel workflow instead of the Gerrit one.\nFortunately the short version \"Submit PR to master, respond to feedback, relax once it is merge onto any upstream branch\" is all that's needed for contributing improvements.\nI spelled out the long version mostly for the sake of getting me and Max onto the same page since this will be the first time a documentation addition is handled under this workflow. Maybe this was gratuitously confusing to do on the PR containing the change :-). This tends to happen since the project is young and a bunch of stuff needs to be worked out as we go along.\nThe backstory on the workflow can be seen in e.g. #482. This was definitely a situation where many people have different strong preferences and it's not possible to make everybody happy :-).\n. Your blog entry also prompted me to consider another issue: latency. Your ping times in the screenshot are often >1ms. Snabb Switch generally has consistent low latency (~100us) except that under low traffic we make usleep() calls to reduce CPU utilization, but the kernel does not seem to honor the requested sleeping time particularly well. You might ask for a 1us sleep and actually get 2,000us. This becomes visible in ping.\nSo the out-of-the-box setup of snabbnfv is not really ideal for latency testing. Could be that we should add a common-line argument to busy-wait instead of sleeping, which does provide consistent low latency in my tests. Even better would be a way to make the kernel do the right thing with usleep if that is possible e.g. with nice process priorities but I am not sure how much we dare to depend on that.\n. This guide looks really excellent to me, Marcel. Thank you for writing it.\nThe main change I would like to make is to not have to tell people to disable the IOMMU or tune their NUMA. However, this is not a problem with the guide, but rather a problem with the software that we don't yet handle these things automatically. It is great to have an explicit guide that can inspire us to make the software simpler to deploy. (Otherwise it's easy to forget all these little steps that will be a pain in the backside to new users.)\n. Cool thought. The labswitch really does have potential here as a \"sponge\" to soak up reusable functionality (i.e. apps) built as part of more special-purpose applications (e.g. alex and nfv).\n. @eugeneia as editor-in-chief please merge this one onto one of your branches, make any edits you need, and then PR again to master for me to merge. (I don't want to merge documentation update PRs myself because then you would not have an opportunity to edit for global consistency etc.)\n. @kbara does #552 help?\n. @eugeneia @javierguerragiraldez what do you think?\n. @eugeneia yeah that makes perfect sense to me. @alexandergall was way ahead once again with already supporting the split horizon groups in the bridge app :) I hadn't made that connection yet.\n. There is probably a bunch of optimization to do. I would suggest defining a standardized benchmark that we can use for reference. Then we can define a meaningful performance target and gradually work on making the application reach it.\nOne idea would be to blast 64-byte packets through the switch and aim for ~30 million packet per second per process capacity (~15M from NIC + ~15M from other switch apps). That is the performance we would expect from a hardware switch.\nThe lab switch will also need to be very accountable for any packets that are dropped, even if the are dropped by the NIC rather than the snabb process. Otherwise it would be very confusing to involve in performance tests of other applications. This is basically the same requirement as Alex's SNMP monitoring support.\nGetting to the right performance may be a long process. To start with I think the important thing is to have an easy way to measure performance and evaluate optimizations. Perhaps it would even be meaningful to have multiple related benchmarks e.g. separately exercising the NIC and the L2 switch app.\nIs that useful feedback?\n. Note: we have often started from 10x less performance than we want but progress has been fast once we set a target and have a common benchmark. So don't be discouraged in any way by whatever the starting point is.\n. Curious, what topology would STP be likely to settle on for this network? (Would it chain all the L2 switch apps together in series? Or put one in the middle as a hub? Or arrange them in a tree?) My mental model is that STP will operate by selecting a set of links to deactivate i.e. by applying an eraser to the diagrams.\nIf we had a bridge app with STP support already then it would be interesting to test. I do tend to agree the static setup should be simple/fast/robust though.\n. > Oh we are using scanners now? I love scanners!\nI am starting to love them too :-). I switched now from taking a photo of the diagram with my phone to a shiny new USB scanner. (Diagram above updated.)\nGood to know that the high DPI mode is there if I really need to do justice to my crayon artistry too:\n\nGoodbye whiteboard and goodbye Sketchbook Pro w/ drawing tablet?\n. I wonder if another interesting variant to compare would be using a Lua table for the forwarding table and representing MAC address keys as Lua numbers (48-bit is okay as a double-float). That would probably have the same cache behavior as the bloom filter but execute as a LuaJIT builtin lookup primitive. Could help to isolate where the cost is coming from. (I have a hard time being really confident in interpreting perf counters based on my past experience.)\n. 30 Mpps would be valuable because then the switch should not be a limiting factor in any of our benchmarking activities. Each port could handle all packet sizes on both ingress and egress. Then we could hook up all of the lab ports to the switch and do all performance tests confident in the knowledge that packet drops will be due to the system under test and not the switch in the middle.\nToday we sometimes do performance tests with packetblaster spamming 64-byte packets. This is handy when looking for applications' absolute limits on packet rate. It would be a pity if the lab switch prevented this kind of testing.\nThere are still quite a few tricks in are arsenal if needed...\n- Optimize the hell out of the bottlenecks e.g. rewrite in assembler or get help from Mike Pall :-)\n- Use two Snabb processes per port e.g. NIC-I/O and L2-Bridge running on separate hyperthreads.\n- Require a fast CPU e.g. E5-2690V2 that has 10 x 3GHz cores.\n- Hardware tricks e.g. offload switching onto the NIC or use multiple TX queues so that each process can transmit directly to any output port.\nI see a lot of value in the exercise of seeing what performance we can achieve and what kind of tricks we have to use to get there. This experience should also help to inform other people considering Snabb based projects about what performance targets are easy / hard / impossible. I also think the lab switch would be a really cool application for other people to use too :).\n. I wrote a really quick script to sanity-check parallelizing the labswitch benchmark:\n``` bash\n!/bin/bash\nfor core in \"$@\"; do\n    sudo taskset -c $core  ./snabb snabbmark labswitch 10e6 64 &\ndone\nfor core in \"$@\"; do\n    wait\ndone\n```\nThe results I see are:\n- 6.6 seconds running one process.\n- 6.6 seconds running two processes on different cores.\n- 13.6 seconds running two processes on a pair of hyperthreads.\nSo on this specific benchmark (max's lab-switch branch @ 7a2db8e8) there doesn't seem to be any value in hyperthreading. That could easily change in the future though e.g. if one process would be stalled on cache misses while the other could make progress. I would love to be better with ocperf to understand from very specific performance counters what work the CPU is bottlenecked on.\n. Indeed. I wonder if we will end up adopting programming practices like un-nesting our loops e.g. having separate loops over the available packets to learn their MAC addresses, lookup their output ports, and transmit them. I am warming up for another round of LuaJIT and Xeon investigations :).\n. I really like the \"learn from every Nth packet\" sampling optimization. I don't know what the worst-case band will be on overflooding but I suspect it is very modest.\nI agree with @alexandergall that the truncated MAC method sounds very problematic. If you have two hosts on different ports with colliding MAC addresses then service will be badly disrupted for one or both.\nGenerally it is okay to sometimes send packets to too many ports but not to too few ports or to the wrong port.\n. > A test on real hardware (two packetblasters, two NICs, one labswitch instance with one intel app per NIC) gives me 7 Mpps. I don't think that's too bad is it? Unsure.\nPerformance is never really good or bad in isolation. I think it's only meaningful to discuss in the context of a particular application.\nFor a lab switch that would connect the servers with each other and a 10 Gbps internet uplink, perhaps running heavy traffic (e.g. distributed filesystem like CEPH), I would estimate that 7 Mpps is okay. That could be a nice initial application for the lab switch.\nIn the future we can take on use cases that would require more performance e.g. being in the middle between 64-byte load generators and systems under test.\n. @eugeneia This is way too much voodoo. People want to be able to account for every packet when they are troubleshooting networking problems. If the argument for this optimization is so complex then people will have to think really hard about it every time they are troubleshooting any packet loss problem anywhere. People don't want to have to worry about this kind of stuff in their networks.\nAlso: Is there not a case where two hosts A and B have colliding MAC addresses, neither is actively transmitting, and one of them will receive all the incoming packets while the other receives none?\n. So I suppose that we have separate questions of syntax and semantics here, i.e. what different kinds of scope should exist and how should we specify the right one.\nFor syntax one idea would be to specify the scope with separate functions map_global(), map_process(), map_local(), etc. That would be an alternative to encoding scope into the object name.\nFor semantics it seems worthwhile to support only a minimum number of simple kinds of objects. Do we really want to have global objects that are not owned by any one process? How are we going to synchronize access to them? Specifically, would it be simpler for each process to allocate memory from a private DMA pool instead of sharing that?\nHow would we solve these problems if Snabb processes were each started separately as peers instead of by forking children from a common parent? (Could we support both models i.e. forked children and links between separate processes? Could we make our implementation simpler in the process e.g. by minimizing shared state that will have to be synchronized?)\n. The whole inner loop for the example program is only 12 instructions with no branch except for the loop exit condition. That is looping over the receive descriptor queue, passing new packets to the callback, having the callback bump a counter, resetting the descriptor, continuing until no packets left.\nI like the way this design made it possible to inline all of that C code into one place. Like what LuaJIT does automatically with its tracing JIT :-).\nCool to browse the disassembly.\nFunction entry and loop setup:\n00000000000007d0 <firehose_callback_v1>:\n 7d0:   49 63 c0                movslq %r8d,%rax\n 7d3:   48 c1 e0 04             shl    $0x4,%rax\n 7d7:   48 8d 3c 02             lea    (%rdx,%rax,1),%rdi\n 7db:   f6 47 0c 01             testb  $0x1,0xc(%rdi)\n 7df:   74 41                   je     822 <firehose_callback_v1+0x52>\n 7e1:   8b 05 5d 08 20 00       mov    0x20085d(%rip),%eax        # 201044 <packets>\n 7e7:   83 e9 01                sub    $0x1,%ecx\n 7ea:   44 8d 48 01             lea    0x1(%rax),%r9d\n 7ee:   66 90                   xchg   %ax,%ax\nInner loop:\n7f0:   41 83 c0 01             add    $0x1,%r8d\n 7f4:   41 21 c8                and    %ecx,%r8d\n 7f7:   49 63 c0                movslq %r8d,%rax\n 7fa:   4c 8b 14 c6             mov    (%rsi,%rax,8),%r10\n 7fe:   48 c1 e0 04             shl    $0x4,%rax\n 802:   41 0f 18 0a             prefetcht0 (%r10)\n 806:   c6 47 0c 00             movb   $0x0,0xc(%rdi)\n 80a:   48 8d 3c 02             lea    (%rdx,%rax,1),%rdi\n 80e:   45 89 ca                mov    %r9d,%r10d\n 811:   41 83 c1 01             add    $0x1,%r9d\n 815:   f6 47 0c 01             testb  $0x1,0xc(%rdi)\n 819:   75 d5                   jne    7f0 <firehose_callback_v1+0x20>\nFunction return:\n81b:   44 89 15 22 08 20 00    mov    %r10d,0x200822(%rip)        # 201044 <packets>\n 822:   44 89 c0                mov    %r8d,%eax\n 825:   c3                      retq   \n 826:   66 2e 0f 1f 84 00 00    nopw   %cs:0x0(%rax,%rax,1)\n 82d:   00 00 00\n. I had a quick look at this today. I haven't found the problem yet. I can reproduce though and I am suitably impressed that our performance regression tests are doing their job :).\n. @javierguerragiraldez That was my first thought too i.e. that the slow down comes from function call overhead in an inner loop. However, I am not so sure. Caching the counter functions locally doesn't seem to improve performance. Also, digging into the LuaJIT -jdump trace it looks like the machine code is quite streamlined in the code that is increasing link counters:\n0bcae77a  mov r13, [r12+0x810]\n0bcae782  mov r14, [r13+0x0]\n0bcae786  add r14, +0x01\n0bcae78a  mov [r13+0x0], r14   ; bump link packets counter\n0bcae78e  mov r14, [r12+0x800]\n0bcae796  add r15, [r14]\n0bcae799  mov [r14], r15       ; bump link bytes counter\n... but I need to come back and check this more thoroughly and it can be that I am misinterpreting or not looking at the most relevant trace.\nCurrently I am using the LuaJIT profiler patch from Mike show which traces use the most CPU and then generating the JIT dump with:\n./snabb snsh -jdump=+rs,dump.txt -jp=v \\\n             -e 'require(\"program.snabbmark.snabbmark\").run({\"basic1\", \"10e6\"})'\nProbably we should improve the snsh command-line to make it easier to run arbitrary Snabb programs with customized environments e.g. profiler enabled.\n. @eugeneia Looks like we also need to create /var/run/snabb with more liberal permissions:\n$ ./snabb snsh --help\ncore/shm.lua:86: shm open error (17174/engine/breaths):Permission denied\n$ ls -ld /var/run/snabb\ndrwx------ 169 root root 3380 Jul 17 05:29 /var/run/snabb\n. Slight digression, forgive me. It's kind of fascinating to look at microbenchmarks with modern technology like LuaJIT and x86. (Leaving aside for this moment the practical relevance of such benchmarks.)\nHere is a tiny program that bumps a counter in a billion-iteration loop:\nlocal c = counter.open(\"test\")\nfor i = 1, 1e9 do\n   counter.add(c, 1)\nend\nand here are the 5 instructions that LuaJIT generates for the inner loop that I slightly annotated:\n->LOOP:\n0bcaf930  add rbp, +0x01  ; bump counter value\n0bcaf934  mov [rcx], rbp  ; store counter to memory\n0bcaf937  add ebx, +0x01  ; bump loop counter\n0bcaf93a  cmp ebx, eax    ; check for loop exit condition\n0bcaf93c  jle 0x0bcaf930        ->LOOP\nand amazingly here is a Haswell CPU being able to execute all five instructions in one cycle:\nPerformance counter stats for './snabb snsh -jdump=+rs script.lua':\n     1,018,356,818      cycles                    #    2.211 GHz                    \n     5,021,774,051      instructions              #    4.93  insns per cycle\nThis really makes me want to spend more time with my new copy of CSAPP3 :-)\n. That sounds reasonable to me (without wearing any kind of unix guru hat).\n. This is really interesting. I have been digging a bit more and for the moment it looks like CPU voodoo rather than LuaJIT voodoo.\nThe performance drop seems to be directly linked to making struct link contain pointers to counters instead of counter values directly. If you remove the * characters from this line of link.h then the performance is high again:\nstruct counter *txbytes, *rxbytes, *txpackets, *rxpackets, *txdrop;\nSo for some reason it seems that the performance penalty comes from the indirection of struct link having pointers to counters (shm objects) instead of having them inline.\nI had imagined this would be annoying LuaJIT somehow but that does not immediately seem to be the case. I ran snabbmark basic1 and compared the machine code for the trace that the profiler says is most used. The immediate difference I see besides a few different register names is just an extra pointer indirection. I have the full pretty diff (including some noisy register name differences) and here is an example of what I am interpreting as bumping a counter (left is with indirection removed, right is  code on this branch):\n$ diff -W 80 -y fast slow\n  ...\n  mov [rsi+0x828], r14d               |   mov [rdx+0x828], r14d\n  mov r14, [rsi+0x818]                |   mov r13, [rdx+0x818]\n                                      >   mov r14, [r13+0x0]\n  add r14, +0x01                          add r14, +0x01\n  mov [rsi+0x818], r14                |   mov [r13+0x0], r14\n  ...\nSo now I am tempted to formulate a hypothesis for why this could annoy the CPU and then look for a way to disprove that e.g. relevant perf performance counters. I should also consider the possibility that I am looking in the wrong trace but I don't think so -- there were only three showing up in the profile and this one at the top.\nFun. :-)\n. I have been playing around with the performance regression. The basic approach for now is to use ocperf to look for interestingly different CPU performance counters on the slow code vs the fast one. The next step is to try in-between changes, but for the moment I am struggling with that. So here is a preliminary braindump.\nI didn't find anything interesting in the standard perf stat counters but maybe some ideas in the extended ocperf ones.\nHere is the profile for the slowed down code on this branch:\n```\nluke@interlaken:~/git/snabbswitch/src$ make && sudo ~/git/pmu-tools/ocperf.py stat -e cycles,instructions,cycle_activity.stalls_l1d_pending,cycle_activity.stalls_l2_pending,cycle_activity.stalls_ldm_pending,resource_stalls.any,resource_stalls.rob,resource_stalls.rs,resource_stalls.sb,icache.ifetch_stall ./snabb snabbmark basic1 1e9\nmake: 'snabb' is up to date.\nperf stat -e cycles,instructions,cpu/event=0xa3,umask=0xc,cmask=12,name=cycle_activity_stalls_l1d_pending/,cpu/event=0xa3,umask=0x5,cmask=5,name=cycle_activity_stalls_l2_pending/,cpu/event=0xa3,umask=0x6,cmask=6,name=cycle_activity_stalls_ldm_pending/,cpu/event=0xa2,umask=0x1,name=resource_stalls_any/,cpu/event=0xa2,umask=0x10,name=resource_stalls_rob/,cpu/event=0xa2,umask=0x4,name=resource_stalls_rs/,cpu/event=0xa2,umask=0x8,name=resource_stalls_sb/,cpu/event=0x80,umask=0x4,name=icache_ifetch_stall/ ./snabb snabbmark basic1 1e9\nProcessed 1000.2 million packets in 57.44 seconds (rate: 17.4 Mpps).\nPerformance counter stats for './snabb snabbmark basic1 1e9':\n137,610,684,367      cycles                    [40.00%]\n   265,654,638,390      instructions              #    1.93  insns per cycle         [50.01%]\n                 0      cycle_activity_stalls_l1d_pending                            [50.01%]\n       577,030,750      cycle_activity_stalls_l2_pending                             [50.01%]\n    15,187,350,549      cycle_activity_stalls_ldm_pending                            [50.01%]\n    65,936,797,294      resource_stalls_any                                          [50.01%]\n        22,569,426      resource_stalls_rob                                          [40.00%]\n    39,972,536,873      resource_stalls_rs                                           [39.99%]\n    26,848,268,009      resource_stalls_sb                                           [39.99%]\n       152,017,985      icache_ifetch_stall                                          [40.00%]\n  57.457470008 seconds time elapsed\n\n```\nand here is when I remove the * characters in the counter declarations of struct link:\n```\nProcessed 1000.1 million packets in 49.22 seconds (rate: 20.3 Mpps).\nPerformance counter stats for './snabb snabbmark basic1 1e9':\n117,936,355,494      cycles                    [40.00%]\n   252,311,482,143      instructions              #    2.14  insns per cycle         [50.00%]\n                 0      cycle_activity_stalls_l1d_pending                            [50.00%]\n       902,218,405      cycle_activity_stalls_l2_pending                             [50.00%]\n    11,441,697,088      cycle_activity_stalls_ldm_pending                            [50.00%]\n    50,650,953,527      resource_stalls_any                                          [50.00%]\n        29,498,053      resource_stalls_rob                                          [40.01%]\n    21,189,725,460      resource_stalls_rs                                           [40.01%]\n    30,318,684,145      resource_stalls_sb                                           [40.00%]\n       177,455,398      icache_ifetch_stall                                          [40.00%]\n  49.241633824 seconds time elapsed\n\n```\nThe test is with one billion packets so it is easy to convert these big numbers into per-packet values. The fast one takes 117 cycles per packet (+17% for slow), 252 instructions (+5% for slow), loses 50 cycles to CPU resource stalls (+30% for slow) and 12 cycles stalled on pending L2 or memory accesses (+27% for slow).\nI am not really sure how to interpret these counters. The Haswell CPU has many execution units: are the stalls when one execution unit can't run or when none can run? etc. Some background digging is needed there, and for now I don't claim my interpretation is any more meaningful than reading tea leaves.\nThe main theory I have is that the performance penalty comes from the address patterns of the counter objects in either one of two ways:\n1. Counters allocated as shm objects will be on different cache lines but inline counters will be on the same. This potentially allows the CPU to coalesce memory operations better for the inline counters.\n2. Counters allocated as shm objects may have page-aligned addresses i.e. starting on a 4096-byte boundary. (This is due to the way we allocate each counter with a separate mmap()ed file.) This can cause cache collisions because the cache is looking only at the low bits of the address and assigning them to the same bucket (example).\nand the next experiment that I want to compare is with counters still on separate cache lines but with more variation in the low bits of the addresses.\n. Looks like it is problem 2: Counter are colliding in L1/L2 cache because they are allocated at 4096-byte aligned addresses.\nTested by adding some random padding at the start of the counters to unalign their addresses:\n``` diff\ndiff --git a/src/core/shm.lua b/src/core/shm.lua\nindex 3daeabc..650a455 100644\n--- a/src/core/shm.lua\n+++ b/src/core/shm.lua\n@@ -85,11 +85,12 @@ function map (name, type,  readonly)\n    local fd, err = S.open(root..'/'..path, \"creat, rdwr\", \"rwxu\")\n    if not fd then error(\"shm open error (\"..path..\"):\"..tostring(err)) end\n    assert(fd:ftruncate(size), \"shm: ftruncate failed\")\n-   local mem, err = S.mmap(nil, size, mapmode, \"shared\", fd, 0)\n+   local r = math.random(100)16\n+   local mem, err = S.mmap(nil, r+size, mapmode, \"shared\", fd, 0)\n    fd:close()\n    if mem == nil then error(\"mmap failed: \" .. tostring(err)) end\n    mappings[pointer_to_number(mem)] = size\n-   return ffi.cast(ffi.typeof(\"$&\", ctype), mem)\n+   return ffi.cast(ffi.typeof(\"$&\", ctype), ffi.cast(\"char\",mem)+r)\n end\nfunction resolve (name)\n```\nThe result is that performance comes back to normal:\n```\nProcessed 1000.1 million packets in 48.88 seconds (rate: 20.5 Mpps).\nPerformance counter stats for './snabb snabbmark basic1 1e9':\n117,094,002,854      cycles                    [40.00%]\n   265,802,027,480      instructions              #    2.27  insns per cycle         [50.01%]\n                 0      cycle_activity_stalls_l1d_pending                            [50.01%]\n       544,000,698      cycle_activity_stalls_l2_pending                             [50.02%]\n    10,019,546,691      cycle_activity_stalls_ldm_pending                            [50.02%]\n    45,995,283,277      resource_stalls_any                                          [50.01%]\n        19,196,410      resource_stalls_rob                                          [40.00%]\n    17,526,932,629      resource_stalls_rs                                           [39.99%]\n    28,985,851,873      resource_stalls_sb                                           [39.98%]\n       130,095,179      icache_ifetch_stall                                          [40.00%]\n  48.900446887 seconds time elapsed\n\n```\nThis is an interesting class of problem since you could read the machine code for the inner loop forever and never see the problem: it's not in the code but rather in the data. The only sensible way to find such problems that I can think of is via performance counters, which I have used quite inexpertly above. (The reason I found this relatively quickly now is that @n-nikolaev and I were already bitten by the same issue last year when packet buffers were being allocated on 4096-byte boundaries and touching payload became very expensive.)\nHow to fix this for real? That is tricky. My hack above has the side-effect of adding padding bytes to the counter files on disk, which mucks up their nice representation as one counter per file. I don't immediately see a better solution that is fast and preserves the same file format but let us think about that for a bit.\n. @javierguerragiraldez I like having a separate file for each counter and shm object. This way we outsource work to the file system instead of having to invent our own directory lookup mechanism. (The initial implementation of counters in Snabb Switch did keep them in a common shared memory blob with a separate index.) I don't want to give up on that right away if we don't have to.\nI am tempted to try a simple double-buffer. Mirror the counters into ordinary FFI objects and then commit them to the shared memory locations once per breath cycle. This could potentially also help us support atomic reads of shm objects if we need to in the future.\nI also think we could probably drop most of the counters from the link objects and instead derive them once per breath from the deltas of the ring indexes. However, the issue will remain for all the other counters.\nQuestion is also whether this cache-associativity performance problem will apply to other shm objects such as links or whether that will be okay because we don't operate on so many of them at the same time.\n. Got it! Your key is installed on chur now and you should be able to login and create an account with ssh -p 2020 root@lab1.snabb.co.\n. Howdy! Getting up to speed here... btw, the vhost-user protocol spec could be handy as a reference.\nJust responding based on referring to the information in the issue and related specs for now:\n1. virtq id out of bounds.\n2. vhost_user unrecognized request: 0.\n3. map_from_guest() failed.\nvirtq out of bounds\nIt does indeed sound plausible that an integer datatype is being mismatched. I wonder why. The vhost-user protocol says that the argument is expected to be vring_state i.e. 32-bit ring index and 32-bit number. On the Snabb side I see a couple of mildly suspicious things: our struct vhostu_vring_state is not declared with __attribute__((packed)) and we are using types like unsigned int instead of uint32_t where the vhost-user spec specifically calls for a 32-bit number. I am not sure if this is a problem but it does seem conceivable that the compiler would give us a different memory layout than we expect and so I am testing a small fix to tighten up these types.\n(If I recall correctly the Linux kernel vhost ABI does not nail down the exact memory layout as vhost-user does and rather depends on C compiler behavior for the target platform. It is easy to imagine how issues like this could lead to subtle bugs when some QEMU internal functions are common between vhost and vhost-user.)\nIt could be interesting to run with strace -e sendmsg,recvmsg to see the payload of the vhost-user protocol messages i.e. is an invalid value arriving over the wire (bug elsewhere that we need to report and workaround) or is the wire value correct and only our interpretation wrong (bug we need to fix).\nunrecognized request: 0\nThis seems to be the Snabb vhost-user app complaining that it received a vhost-user protocol message with id 0. The spec has no message with this id. I think that the same strace command would let us see what is happening here too.\nmapping to host address failed\nThe error seems to be saying that we failed to translate the address of a packet buffer from guest address space into our own address space. I am not sure why this would happen but here are a few quick ideas:\n1. Race condition. Are we somehow seeing the value before it is correctly written? This happened once before due to an obscure a missing memory barrier and that took around an hour of iperf to trigger. Snabb Switch could potentially trigger race condition bugs in guest drivers since we are polling their vring at a high frequency instead of waiting for them to signal an interrupt.\n2. Memory corruption. Could something be writing a bad value to the vring somehow?\n3. Surprising value on the vring. The pointer hex printout is truncated to 32-bits but at least if this were Linux I would say it looks like a kernel-space virtual address with all of those ff values, but unless I am mistaken we are expecting a guest physical address here. (I may well be mistaken on that score.)\nidea\nCould be interesting to get an strace for fixing the first two and then move on to the third?\n. Interesting. I suspect QEMU is the place to track this down, either reading the source or catching that message send in gdb.\nCould also try a newer QEMU and see if it's reproducible.\n. @virtualopensystems-nnikolaev Do you think this change makes sense? Could otherwise drop it.\n. @virtualopensystems-nnikolaev Thanks! Since you approve I have merged it onto next.\n. Great topic, Kristian. I have pondered a bit and I actually think there are multiple good ways we could bring the firehose-style optimization into the app network.\nOne would be to have a firehose app that takes a callback function (C or Lua) and uses that to decide whether to output a packet vs. leave the buffer in the NIC RX ring for reuse.\nRelated ideas could be to optimize the hell out of the \"slow path\" in general or even add the callback capability described above to the standard Intel10G app.\nI think we could do all of these things using standard apps and without needing the firehose program front-end. That could be left as a special-purpose interface for people who are not interested in the app network and only want a simple way to deliver packets to a C function.\nbtw did you try the pcap filtering since we merged pflua? Alex Gall saw a big speedup compared with the older implementation that called out to libpcap.\n. I reckon we could find a suitable way to have a preprocessing hook with the same performance that Pavel is seeing with firehose. Question is whether it should be a new app or a feature of the standard driver.\nIt would also be neat to create a simple-but-realistic snabbmark benchmark to represent your application. Then we could track the performance for optimization and avoiding regressions. I wonder if in the future we will have 20 small benchmarks in snabbmark and each will demonstrate a recommend programming style and reference performance for a certain kind of application? That could be neat.\nOne step at a time...\n. Yeah. Welcome to me and @alexandergall's world :-). Optimizing the Lua code is still a black art but we are getting incrementally better at it. I would like to build up a good collection of reference examples (so that you always know approximately what performance you should be getting) and better diagnostics so that it is much easier to find these tiny details that annoy the JIT.\nI am optimistic that we will make this process much easier but it will take some time.\nCongratulations on your successful optimization campaign now anyway :-)\n. @plajjan btw check out LuaJIT 2 Optimization Guide.\nThe trouble for me is that I hate taking optimization advice that I don't understand and it is taking me some time to really understand what is going on under the hood... probably everything they say is right though :).\n. @plajjan Interesting :). Yes, I am also really interesting in turning optimization of our Lua code from an art into a science. This will take some thought :). Good to be gathering more experience in the community.\nIndeed chur is a pretty slow machine: 2.0 GHz Sandy Bridge. Fast-clocked i7 CPUs do seem to really beat it. Generally the strength of the higher-end Xeon E5 chips is to have many cores and if you don't need that you are probably much better off with an i7.\nI am really curious to see the Skylake CPUs hit the market in the coming months and see what new SIMD capabilities each model has from the AVX512 feature family.\n. Exciting progress :-)\n. So it looks like this PR includes solutions to most of the big problems in parallelizing:\n1. Sharing link objects between processes: make them into shm objects.\n2. Sharing packet mappings between processes: allocate DMA memory before fork and allocate from a shared pool. The pool is a shm and access is synchronized by an flock mutex on the shm file inside dma_alloc().\n3. Returning packet buffers to the freelists of the processes that need to allocate them: the link struct is extended so that free packets can be chained backwards to where they came from. This part I don't understand yet and have to read more closely.\n4. Spawning multiple processes: the app network defines which processes will exist and the engine will fork the children that are needed.\nOne part that seems to be missing is incremental updates of the app network. Currently it looks like the engine restarts all child processes every time the app network changes. That would be too drastic. Instead each child should independently inspect the new app network and update its apps as needed (restarting them, reconfiguring them, or leaving them alone).\nRestarting after a configuration change is a valid design choice for an individual app but we can't give that luxury to the engine. Imagine if you tweak a small obscure config item and suddenly 20 cores worth of traffic processes all restart, re-discover their gateway address with ARP, reset their connection tracking state, etc... that would be no fun. It's very valuable for apps to be able to expect to stay up and running and be allowed to keep some simple state in local variables.\nCool to see so many pieces of the puzzle together in one place :-). Are there some other parts of the puzzle that still need to be filled in that I haven't thought of?\n. btw really impressive work on streamlining the design and eliminating tricky cases. I really like the fact that this code makes sense on its own without needing to refer to really heavy-weight concepts e.g. computer science papers about lockless data structures, fine details of the x86 memory model, and so on. It is hard work making software simple :).\n. ... and getting the diff down to 500 lines! That is also an impressive and important feat :).\n. So the radical idea that keeps coming back to me is that the code might become a lot simpler if we allowed each process to be started independently instead of assuming they are forked.\nThe forking is convenient at first but then it gets things tangled up. For example the app network being in copy-on-write memory and awkward to update across all processes; the parent needing to wait/synchronize on terminating child processes; every module having to think about whether it is \"fork-safe\" with prefork/postfork methods.\nHow about if we switch the model to allowing processes to be started separately? We could still fork() the processes when creating an app network but this would be a convenience and not a requirement: you could also run genuinely separate app networks and still connect them with cross-process links.\nI have a few ideas that would help that I need to flesh out a bit more:\n1. A way to privately allocated DMA memory and then map it into new processes on-demand as they receive packets. (Hint: catch SIGSEGV and mmap() the mapping.)\n2. A way to pass packets between different processes over ordinary links and still recover them to the right freelist -- even when processes crash and restart. (Hint: Garbage collect packets.)\nI don't have any idea yet for sending updates app network descriptions between processes. I suppose we could put each new configuration into a well-known location (e.g. file or shm object called config.47) and expect that everybody will find it sooner or later?\nI will write these up ASAP and you guys can shoot them down :).\n. > i think we still need some way to associate all the intervening process into a group\nI would be interested to understand how fundamental this is. Could we make things simpler and more orthogonal by having a \"flat\" design that potentially connects every process running on the machine?\nJust wondering whether group-shared objects are potentially a \"code smell\" indicating bits of state that have complex ownership rules and may end up being difficult to synchronize access to. For example having a common DMA pool seems more complex than having separate ones, all other things being equal.\n(I suppose we may find entirely other reasons to separate objects into groups e.g. for running multiple versions of Snabb Switch that each expect shared memory files to be in a different format. But that is perhaps a separate problem.)\n. Wow! Great effort. That is really crystal clear now. I see now that the idea is very simple and neat.\nIs the difference between link and interlink now so small that we could merge them together? e.g. simply have a flag on struct link saying whether you need to put back a free packet when you receive a full one? That would also potentially shrink the code-and-concepts footprint of the diff.\nOne nagging feeling is that LuaJIT may make us regret introducing unpredictable control flow on link.transmit(). I am thinking that the interlink will become full much more frequently (since both sides have to free up resources) and that this will execute a while loop that will fail to unroll. This seems likely to create a whole bunch of side-traces in every app that might make them harder to profile. I don't have an immediate solution.\n. I do have one idea to simplify the control flow actually.\nSuppose that transmit() on an interlink always frees exactly one packet. Then full() returns a flag saying whether there is a packet to free or not. The loops in app pull and push methods would always be successfully sending one packet and freeing one packet, until the link fills up, which is a termination condition for the loop anyway.\n. How do I run these benchmarks?\n. (Sounds like what you need is a second pair of eyes to try it out and ask dumb questions. Happy to volunteer.)\n. Should move read and write indexes into different cache lines. Don't want two cores writing to the same cache line.\n. Transmit and Receive apps is an interesting idea. I suppose really that this is the simpler design and that making all links interprocess friendly is mostly a (premature) optimization. If you think the implementation can be simplifed with separate apps then I would say go for it and let's see what we can learn from that.\n. I have finally taken this for a spin. However, now I need to get perftools installed on NixOS so that I can get visibility into the performance counters :). On it... See also NixOS/nixpkgs#9095.\n. @javierguerragiraldez would the memory situation be simplified if I implement #571? (Or what would be useful assistance with memory.lua hacking?)\n. Howdy! Sorry about the slow feedback.\nThis is now a very impressively short change! The idea of using dedicates apps to transport packets between processes seems to be extremely good. I really like the way it will not impact the runtime behavior of existing applications that don't use it, and that it takes so extremely little code.\nI think we have a winning design for the initial multiprocessing support :). It is also nice for the future that multiple such solutions could co-exist which makes experimentation and refinement easy in case we have new ideas that we want to try later.\nSuper duper cool! and to think we came this close to entering pthreads hell. Great save :-)\nI have a few mundane housekeeping feedback before merging:\nCan we have another pass at shrinking the diff? Even though it is small it seems like there are some changes that are not required to be bundled with the multiprocessing: particularly the new FFI metatypes in packet and link modules seem like an unrelated change (?). I do think that FFI metatypes are the right facade for our API, after your previous benchmarking experiments, but I suspect we can take that separately.\nCan we have a long comment or README.md explaining the interproc app design? (So that people will know what the app is for and what ret_packets is and why it works to pass pointers between processes.)\nCan we also have a comment explaining how the extended type syntax in freelist works? (Or remove that change if it is not needed...?)\nCan we have a README.md and some in-file comments explaining the spoon program too?\nJust amazingly cool. This is exactly how I hoped the multiprocessing support topic would play out. i.e. that we consider and prototype all of the most extreme options and then eventually settle on something extremely simple. Great hacking :).\n. oh one more thought: perhaps rename app.lua in interprocess/ to something else? I really like to have globally unique filenames so that it is easy to find the right Emacs buffer :) and app.lua is already taken (though that is overdue to be renamed engine.lua...)\n. Further work on this feature is being done on #568.\n. Superceded by #568.\nThis integration branch is based on next and the new PR is sent directly there instead of master. (The list of changes looks very long on this PR because it includes unrelated stuff from next.)\n. The double buffering change causes some problems (as we see in SnabbBot tests):\n1. Counters are not ordinary shm objects anymore. You can't perform shm operations directly on objects you get from counter.open(). This is breaking link.free() that attempts to delete counters directly via shm operations. Could be solved by adding a counter.delete() function and using that instead of shm calls.\n2. Counters will lose updates if you map them read/write more than once (whether in the same process or different ones). Have to avoid this situation. One idea would be to use flock to raise an error if a counter is opened read/write more than once. Another idea would be for counter.publish() to only update the public shm counter when the private value has been changed.\n. You probably need #576 \n. Great fix, Max! I was wondering what the problem could be.\nAdvanced Github-fu to send a PR to a PR too :-)\n. @dpino That looks like a failed test to me. Packets are being sent but not received. Do you have the two ports cabled together? I believe this is expected by the current selftest function.\nIn your tests with VMs are you sending packets over the wire (physical cable) or are both VMs attached to the same NIC? If the latter then the test might work even if the NIC is not able to transmit/receive on the physical link.\n. So do I interpret correctly that:\n1. NFV selftest passes and card is successfully passing traffic between locally attached VMs.\n2. NIC selftest is failing and card is not successfuly passing traffic between physical ports.\nIn this case perhaps the most appropriate thing would be to add support but print a loud warning?\n. I think it would be better to have a more explicit error message saying that the T3 works for loopback traffic but not physical traffic. Then users will know what is expected to work and what is not. Could even print this in the intel10g device initialization perhaps?\nAlso: Please add commits to the PR instead of rebasing. I know, I have asked people to rebase in the past, but I am taking that back because I find that it makes Github based code review really hard and punishes people who merge PRs into their own trees for testing (which is a behavior we want to encourage).  See github-workflow.md for latest humble requests of contributors :).\nWe should also fix the driver. @javierguerragiraldez didn't you have this card working at some point? (IIRC it just needed a little time to initialize or something?)\n. I am reluctant to merge this. Based on Javier's comments it seems like we could add full support for the card without much more effort than marking it as broken/experimental.\n@javierguerragiraldez Could we possibly persuade you to send a new PR that adds support for this card? Based on your comments it sounds to me like the NIC does work but the selftest needs to pause before testing for the link to be usable? However, there are a lot more code changes than this on your 10GbT branch so it would really help if you can strip this down to the minimum needed.\n. One idea would be that the selftest could have a \"warm up\" period. That is, if the link is not actually usable when it reports \"link up\" then the first step of selftest could be to loop until a packet is actually delivered (or timeout) before proceeding to the real tests. Reasonable?\n. @dpino I \"did a Max\" and sent a PR to your PR branch. Could you consider merging those changes please?\nGenerally this implementation seems good enough but I am still a little unclear. @javierguerragiraldez Is the T3 NIC reporting \"link up\" before it is able to send traffic? and another 2 seconds are needed? is the SNMP object that reports link status going to have a sane value? or is the issue that we are not trying hard enough to detect when the link is up before running the test?\nThe 2 second sleep seems fragile and likely to break in the future e.g. when testing other variants of the Intel NIC or with optical transceivers installed in the SFP+ NICs. This seems manageable though: it is only the selftest function that is affected and not the driver itself.\nGenerally I don't think it is part of the driver contract to be able to pass packets immediately upon startup (and in real life e.g. if we are cabled to a switch running Spanning Tree Protocol then it could take tens of seconds after link-up to actually be able to send traffic). So the selftest requirement of avoiding attempts to send before the link is ready is somewhat unusual.\n. @eugeneia That is a fair point. I see no urgent need to make this change now. We could revisit this idea in the future if/when we are trying to squeeze more performance out of the basic app network and see if it really helps.\nI close this PR for now.\n. Hard core :-).\n. @virtualopensystems-nnikolaev Understood. This sounds related to issues that @mwiget has seen recently too. You will need to accept a Github Invitation that I sent before I can assign the issue to you.\nInteresting development to see corporate-sponsored usernames on Github. I wonder if I could rename to coke-redbull-lukego and rake in some sponsorship bucks ;-)\n. @virtualopensystems-nnikolaev btw: you guys could consider creating a branch called e.g. vosys-nfv and registering it in src/doc/branches.md. Then you could collect fixes that you want to submit on that branch and PR them to master when you are ready to release them. This would give the vosys team proper credit for contributions because the vosys-nfv branch would be visible in both PRs and git merge commits. It would also allow Snabb Switch users to discover your branch via the SnabbCo repo.\nI definitely see it as a positive for contributors to get proper credit (both as individuals and companies) so that positive contributions to the upstream repo will be a valuable marketing tool.\n. @virtualopensystems-nnikolaev This is a very good Issue you have written :). I am also interested in fixing this. Can I help somehow?\n. Here is a relevant qemu-devel thread suggesting that the intended vhost-user behavior in QEMU is not fully locked down with respect to guest reboot vs. guest driver unload/reload. Could be related to the issue we are seeing.\n@eugeneia is working on adding a CI test case where the guest reloads the virtio_net kernel module to see if that triggers this problem or a related one. (@virtualopensystems-nnikolaev do you have any tips on bootstrapping that kernel/guest? do we need to add an additional make modules_install compared with the old instructions from bench_env README? I suppose @eugeneia can shout directly if he needs tips.)\n. @eugeneia OK. Looks like an unrelated bug. I think we should cover this in the CI tests. I think we should also Dockerize the bootstrapping of kernels and images: I don't think we should ever be doing this stuff by hand anymore. You interested in handling that?\nThere is a Docker feature called \"volumes\" that seems relevant: one container could build the artifacts (kernel and guest image) and then export them as a \"volume\" that another container (SnabbBot) could access. This should make it possible to mix-and-match the same SnabbBot with many different kernel/image combos. (Could also be that this is the wrong solution.)\n. I merged this onto next even though the integration is a little messy and we will need to find a workflow for keeping it in sync with upstream. For now it seems okay because it is a small amount of code that is not changing much.\n. Does that make the test case pass? I have been trying this kind of thing without success so far.\n. I also think that fix should go inside the pseudo checksum function in order to be consistent with the comments in checksum.lua\n. @eugeneia SnabbBot not testing this change?\n. @javierguerragiraldez Oh! You are right! Looks like I outsmarted myself: I broke the test case when I moved it from an ad-hoc script file into selftest(). I suspect the code on this branch now is correct since 84b497a and that I need to verify this on the real test environment and then correct the selftest function.\n.  This does seem to resolve the issue. Great. @javierguerragiraldez thanks!\n@eugeneia I reckon I understand why this was not detected by the SnabbBot CI iperf performance regression test. This problem causes performance to suffer only if the guest VM becomes the limiting factor in performance. SnabbBot tests with two guests attached to one Snabb Switch and in that case the limit is the Snabb Switch (not surprising especially since CI is running without AVX2 CPU). The setup where the problem has been seen is when each guest is attached to a separate Snabb Switch instance and in that case this checksum issue causes the bottleneck to move from the Snabb Switch into the guest that is receiving packets (iperf server).\n. (davos up again now. The Ubuntu install there is a bit sad.. default GRUB entry does not boot. Bear with me.)\n. I very much like the idea of treating network traffic as an infinite stream of bytes.\nI feel like networking has traditionally been regarded as a system programming problem (interrupts, spinlocks, etc) but on modern x86 it is more of a High Performance Computing problem (don't let the processing pipeline stall).\nTo process traffic as a sequential stream of bytes - like unix tools processing strings - would seem like a very interesting paradigm and extremely well suited to modern CPUs.\nIs that at all in line with how you are thinking?\n. Cool stuff, Max!\nThoughts:\nIs there really one \"full Snabb Switch test environment\"? This all looks pretty specific to the NFV application so far. I would expect to find this Dockerfile somewhere under program/snabbnfv/. I would expect other applications to have their own separate Dockerfiles for CI tests e.g. for ALEx VPN that will install NetSNMP with its perl extension and so on. I think it makes sense to split this up and make it easy for individuals to create and run only the tests that they care about.\nI also think that for Snabb NFV we need to scale up in terms of the number of guests we support. Suppose that we want to run the CI with every Ubuntu release since 12.04 and with every DPDK release since 1.7. And that we also want to test every major version of important binary-only network appliances from Juniper and other companies. Then we will need a framework that makes it easy for a motivated person to go and create a new test case that can be executed by the CI e.g. like Marcel is Dockerizing the Juniper vMX router.\nThis makes me imagine a filesystem layout like:\nprogram/\n snabbnfv/\n  test/\n   vm/\n    ubuntu/\n      12.04/\n      14.04/\n      ...\n    dpdk/\n      1.7\n      1.8\n      ...\n    juniper/\n     vMX/\nand then to have a Dockerfile in each place that automates the process of creating the VM? (This could be bootstrapped from the ground or downloaded from a well-known location e.g. official VM image distributed by Ubuntu or Juniper and then with some configuration applied.) The actual tests to be executed would have to depend on the VM under test too e.g. whether it is a Linux box, a router between two Linux boxes, etc.\n@eugeneia How impractical am I being here?\nOne more passing thought... if it is logistically difficult to bootstrap VMs then could we use off-the-shelf ones and install software in them? (even use Docker to get iperf/dpdk/etc installed inside the VMs? Just a thought..)\n. also re: size I see the command dd if=/dev/zero of=$OUT/qemu.img bs=1MiB count=2048 that is creating a 2GB file. That may be excessive.\nI suppose the case could also be made for a \"jumbo\" guest VM that can be booted with many kernel versions and includes the software needed for many test cases (e.g. iperf, ping, and every relevant version of DPDK). That would look very much like the asset-creation script. I am still interested in making it easy for people to run tests with other VMs e.g. binary releases from vendors but I suppose that this could be a separate problem. (Indeed perhaps you would even want to test these by putting Linux VMs on either side and driving traffic through with iperf/ping/dpdk/etc -- then the test cases would look very much like the ones we have now?)\n. Great hacking!\nThis looks very neat and tidy :-).\nLet me see if I understand...\n1. src/Makefile builds an NFV test environment: compile QEMU, compile guest kernel, create guest image. It is easy to define new variations (software versions and build parameters). You can run this directly on your development machine for testing.\n2. Docker can be used to \"snapshot\" ready-made test environments. Each environment would include both a base container OS for executing the tests in (e.g. Ubuntu 14.04) and also specific qemu/kernel/guest versions (built as above). This would make it possible to execute a test directly from dockerhub without compiling any software. [This part is not done yet.]\n3. Guest OS is created in two steps: first use Docker to create a container with all the software that we want and second snapshot this into a QEMU disk image file. This means that we use the Docker toolchain/ecosystem to create guest images instead of distro-specific tools like debootstrap or off-the-shelf VM images. [This seems like a cool idea that will make it easier when we want to install non-trivial software inside guests e.g. DPDK.]\nI really like this direction! I can easily imagine using this framework to test more things that I am interested in e.g. all recent QEMU versions and all recent guest kernel versions.\nCouple of questions:\n1. Will it be easy to take ready-made test environment from Dockerhub and run it with a new version of Snabb Switch? (I mean e.g. if we have 20 test environments defined and the CI wants to run each of them to test a PR.)\n2. Can we use this for ad-hoc testing? For example, would I be able to quickly drop into an interactive tmux session in a container with various interesting assets available and ready to run (e.g. a specific QEMU and a Juniper vMX guest image that I would start manually)?\nalso thinking long-term, maybe best not to worry about right now, we seem to have a few things mixed together that in my mind are logically separate:\n1. Does building QEMU and virtual machines really belong in src/Makefile? Or is it specific to the NFV application and better contained in src/program/nfv where it will not confuse/distract/annoy people who don't care about NFV?\n2. How should the \"user interface\" to individual test cases really look? For example selftest of a NIC. Is it better to provide arguments as environment variables, command-line arguments, or Lua scripts? I mean that even if we like SNABB_PCI0 and so on for running the entire CI test suite we might prefer a different mechanism for interactively running individual test cases. (This comment comes specifically from seeing snabbmark move one of its parameters from the command line into an environment variable and wondering whether this is the right trend.)\nLastly: if the size of qemu.img becomes an issue then you might want to try compressing it with e.g. gzip. I believe that Linux file systems use clever tricks for keeping track of which parts of files do not really contain valid data and not bothering to store them (so you can have large image files that are mostly empty space) but that this breaks down when you use the files in other ways e.g. upload to Dockerhub.\n. This surprises me. The initial proof-of-concept Dockerfile was successfully running test_env inside the container and that should have been compiling this feature of QEMU from source.\nIs there a hint to be found there? https://gist.github.com/lukego/850a037c7d9ecdf594af\n. Agree! This sounds wonderful.\nI also agree that we want to do as much as possible inside containers. Assume that the host will be some horrendously crippled incomprehensible distribution like NixOS with no software installed :). This will make it practical to administrate the lab and keep everything repeatable.\n(The fine tweaks you are doing on Grindelwald are probably breaking the fine tweaks that @nnikolaev-virtualopensystems did previously to get OpenStack running with customized Ubuntu packages. That stuff all has to be moved into containers too so that we can run it on all servers. One step at a time though...)\n. Can we check if these packet drops are related to #592 somehow?\nGenerally it takes quite some head-scratching to work out the reason for packet drops in test environments where the input load is above the capacity of the system under test. Often it depends more on the order in which each queue is processed than on raw performance. (Guest dropping packets doesn't necessarily mean the guest has slowed down: can also mean we are processing its receive queue faster than its transmit queue and so it can't always send the packets it receives.)\n. I would like to switch to an upstream DPDK. Ideal would actually be the latest release.\nThe customized one should only have been needed for earlier experiments when we were planning to send patches to QEMU and DPDK to increase the sizes of the vrings, but we dropped that change and should not require any patches nowadays.\nRight @nnikolaev-virtualopensystems?\n. This was the least disgusting implementation I could come up with when browsing kernel sources etc. The shm syscalls are really annoying and non-interoperable with other kernel features, but we seem to be stuck with them because mremap() doesn't work with hugetlb from mmap(). C'est la vie.\nWeekend!\n. btw: this version is going to leak huge pages because it doesn't unlink the shm IDs anymore. If you start running low on huge pages then you can use the command line tools ipcs to list the allocated ones and ipcrm to delete them.\n. @eugeneia Good point. Have to think about potential impact of running these rough draft patches in the CI environment. (I don't think it will have been a problem here: much more likely it was the reboot. QEMU depends on a bunch of hugepages to be preallocated whereas Snabb Switch allocates them on demand.)\nI wonder if Docker-fication will help with this kind of problem. That is, if rough draft code makes a mess in /var/run and /dev/shm and so on, that this would be discarded with the one-shot container that runs the tests. Or indeed that the test suite could detect such messes since it knows that the tests had started running in a pristine environment.\n. @eugeneia Interesting. I suppose the problem is that I \"innocently\" pushed a change with a resource leak on memory allocation, but then the CI is running zillions of tests (booting VMs, etc) with that leak enabled. So whereas I imagined that one or two pages would leak in fact this is happening a lot.\nHere is a command-line to clean it up. Perhaps we want to run this in a loop for the time being:\nfor m in $(sudo ipcs -m | awk 'NF == 6 {print $2}'); do sudo ipcrm -m $m; done\nIt is very unlikely to cause a problem for the CI. The master Snabb Switch is only allocating the shared memory object for a millisecond or so in order to map it into memory.\nDocker is using Linux Containers (cgroups). The kernel hackers have been busy and added support for multiple namespaces for filesystem / processes / sockets / routing table / etc.\n. Counters are a different kind of shared memory. The ipc commands are only operating on \"System V\" shared memory objects created by shmget(). That is how we allocate DMA memory (due to obscure Linux kernel limitations) but core.shm and counters are done differently (we mmap() a file instead).\nHowever my command is pretty brutal in that it is deleting all of the root user's shared memory objects system-wide. So maybe it will crash some other process installed on the server. I suppose we will find out.\nI do actually think that Docker/containers is a good solution to this problem. That should also protect against other interesting kinds of problems e.g. I tried to write the shm module test suite carefully to avoid potentially doing an rm -rf / if there would be a bug in the path resolving code. That would not be a problem if the test runs inside a container but it would be a major headache if it nuked davos' root file system.\n. Closing PR: I am not actively developing this branch.\n. Great that you will maintain a branch! This seems like the key to scaling development to me. You and I can gradually thrash out a working/testing/reviewing/etc style that works for us and then you can do most of the review of the changes on the branch before PR'ing them to master. Very good :). Let's get started!\nGenerally I need commit messages to explain why changes are being made. If the change is a bug fix then how is the bug triggered and how does it affect users? If the change is a performance optimization then what is the impact and how can it be measured? This will help every interested person to evaluate and test the change and potentially improve on it in the future. When this is not explained in the commit messages then I need to ask a bunch of questions on the PR and then other people will have to find that discussion in the future.\n. The level of corporate branding on this PR seems excessive to me. I would like to find a better way to make sure that both individuals and companies are recognized for their contributions.\nMaybe I started that with all of the \"Snabb\" names and logos. The intention of that is really to be a brand for the project - like \"Linux\" and its penguin - and not a corporate thing of mine. However I am using the same brand for my professional services, mostly for historical reasons, and that may not be appropriate anymore. I am open to feedback about what would make people more comfortable.\nTo be clear the reason I say it seems excessive is that the virtualopensystems brand is in the PR title, the branch name, the Github usernames of all the contributions, and the company logo is being used for all of the user avatars. If everybody would brand their contributions like this then it could dehumanize the upstream project and make everything feel corporate.\nCan you explain a bit about what is important to you guys and perhaps we can find a better solution for giving credit where credit is due?\n. @eugeneia ah! we will fix this for all lab servers forever once we have a unified nixos grub configuration stored in Git :-).\n. > We would like to keep the names and avatars like they are now.\n@virtualopensystems-nnikolaev Could you say a few words to help make me more comfortable with this? For example, are there other projects on Github that taking branded contributions like this and where that is working out well? I have never seen this practice before so I am curious.\n.  re: branding a few more practical concerns about how it interacts with the way Github works:\n1. The names are really long. \"@virtualopensystems-nnikolaev\" is 28 characters. Completion is only a partial help because Github doesn't always have it available and the prefix is not unique anyway.\n2. This PR includes commits from multiple people but this is completely invisible on the main PR page (just see thumbnails of the vosys logo).\n3. Even if you look at the detailed commit list it is hard to notice that there are multiple authors. The user avatar images are exactly the same (company logo) and so are the first 19 characters of the usernames.\nThese could probably be solved with some tweaks e.g. naming nnikolaev-vosys and using unique images (e.g. photo with company logo added, etc).\nThe ideal though from my perspective would be to have a productive discussion about how to give people and companies proper credit for the work that they do. This is an important problem and worth thinking about. I don't really know what people are concerned about and what I can do to help.\n. @virtualopensystems-nnikolaev They are your accounts and you can do as you please. My feedback is that I find this confusing.\n. Great catch!\n@eugeneia has recently added a performance regression on the CI that I would expect to catch this. It is running the classic DPDK packet forwarding benchmark (loadgen->snabbnfv->vm->snabbnfv). However, this test may have been enabled for the first time after the regression was introduced.\n@eugeneia can your regression test detect the performance difference between v2015.07 and v2015.08? That would be very cool :-).\n. @eugeneia also is there a simple instruction for how others (e.g. me) can run that test case manually?\n. > which is self documenting\nPoint taken :-).\nThe challenge to quick testing now is that I am on nixos and test-env assumes that quite a lot of diverse software is installed and available e.g. everything needed to compile QEMU. I suppose that this is the problem that the Dockerized test environment will solve.\n. In the near future this kind of problem should be the perfect use case for Docker based testing (#588). If the test environment is based on Docker then it should be a one-liner for anybody to run and reproduce the problem. I would just type docker run --privileged vosys/vm2vmbench and the rest is automatic.\nThat would be wonderful because otherwise it can take anything from an hour to a day to setup a test environment by hand when dealing with complex software e.g. installing DPDK inside the guests and so on.\n. @nnikolaev-virtualopensystems Great detective work. I also don't immediately see why this specific commit would be the issue. Let me braindump a little.\nThere has previously been a performance regression related to snabbtop that we tried to resolve with #568. That was an extremely hard to profile issue: we were accessing many counters on 4096-byte aligned addresses and CPU cache could not handle that. The backstory of finding that problem is on #558.\nOur solution was to use a double-buffer to access the counters less frequently. However, we still write them once every 100 breathe() loops and maybe that is too often. Could be worth changing app.breathe() to call counter.commit() less often e.g. every 10000 loops and see if that affects the results.\n@eugeneia has added a standard DPDK-based packet forwarding benchmark on the master branch now. I have tried to reproduce the issue with that but haven't seen it yet (but can be that I am making a git mistake when backporting it onto 8ecc0c0). It would be very useful if we could reproduce the problem using that test since it is the easiest setup for multiple people to run and also it is running automatically on the CI. If you can see the problem using that benchmark, or an adaptation of that benchmark to be more like your environment (one-way packet send instead of two-way), then that could help me to reproduce it over here (just one idea).\n(Hopefully once we move our test environments over to Docker images it will be easier to run the test with an arbitrary version of the software. Currently the test scripts and the code are living in the same directory tree and it is not so easy to run a new test with older software from before the test was added. I know, I know, I am expecting Docker to solve all of our problems at once, we will see :))\nHere is how that test runs:\n```\n$ sudo TESTPCI0=0000:01:00.0 TESTPCI1=0000:01:00.1 TELNET_PORT=5000 program/snabbnfv/packetblaster_bench.sh\n...\nload: time: 1.00s  fps: 4,756,445 fpGbps: 2.778 fpb: 218 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,935,922 fpGbps: 5.803 fpb: 256 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,862,646 fpGbps: 5.760 fpb: 255 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,944,067 fpGbps: 5.807 fpb: 255 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,957,900 fpGbps: 5.815 fpb: 256 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,932,911 fpGbps: 5.801 fpb: 256 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,922,261 fpGbps: 5.795 fpb: 256 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,924,639 fpGbps: 5.796 fpb: 256 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,919,184 fpGbps: 5.793 fpb: 256 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,941,215 fpGbps: 5.806 fpb: 256 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,945,911 fpGbps: 5.808 fpb: 255 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,508,429 fpGbps: 5.553 fpb: 248 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,984,065 fpGbps: 5.831 fpb: 256 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,972,250 fpGbps: 5.824 fpb: 256 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,949,254 fpGbps: 5.810 fpb: 256 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,983,303 fpGbps: 5.830 fpb: 256 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,204,803 fpGbps: 5.376 fpb: 242 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,355,580 fpGbps: 5.464 fpb: 242 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,196,086 fpGbps: 5.371 fpb: 249 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 9,951,668 fpGbps: 5.812 fpb: 256 bpp: 64   sleep: 0   us\nload: time: 0.91s  fps: 9,988,466 fpGbps: 5.833 fpb: 256 bpp: 64   sleep: 0   us\nProcessed 100.0 million packets in 20.40 seconds (6400007744 bytes; 2.51 Gbps)\nMade 890,951 breaths: 112.24 packets per breath; 22.90us per breath\nRate(Mpps):     4.901\n``\n. For profiling withsnsh` here is one idea:\nsnabb snsh -jp=z -p snabbnfv ...\nwhich should tell you how time is spent in \"zones\" (z) which is separately for each app i.e. which part is getting slower of vhost-user vs intel_app vs engine.\nYou could also profile with perf stat e.g.\nperf stat -e instructions,cycles,L1-dcache-misses,cache-misses,dTLB-load-misses,dTLB-store-misses,iTLB-load-misses\nwhich may lead to some interesting information. For example if you see many more instructions for the same work then it might point to the JIT. If you see lower instructions per cycle it might point to the CPU.\nI have been using a personal blog lately to explore low-level profiling issues around both LuaJIT and Xeon E5. I am starting at the most basic and working my way up to Snabb Switch. If you are interested there are four entries starting with lukego/blog#5. Could make LuaJIT slightly less mysterious :).\n. Looks like the test failure here is an operational issue (hugepage availability on the CI host). That is no biggie: the next merge will trigger a rerun anyway.\nDoes raise a couple of interesting thoughts though:\n1. How come the hugepage is failing to allocate? In the log we see that Snabb Switch makes three attempts to provision a new huge page in the kernel and then allocate that. I wonder why those attempts are all failing and whether we could improve that algorithm to make it more robust.\n2. I see that it would be valuable to run the CI on multiple hosts so that CI operational issues are easier to distinguish. If the CI result were that tests passed on 4/5 CI servers then it would be easier to estimate confidence in the code before digging into the root cause. Once more I see docker (Issue #588) as the great hope to make it easy to operate a CI fleet.\n. Roger!\n. Humble request: Could you put the release name in the first line of the commit message when you merge onto master? Just so that it is immediately visible in one-line summaries e.g. git log --oneline.\nFor example\nMerged PR #593 (v2015.09 release) onto master\n. Cool! Let us do a Git/Github workflow geek out :-)\nI think that 874e8b should be submitted in its own PR based on master for test and review. I don't want to pull it immediately because I don't know whether it is the right thing or not. Commit message only says that it attempts to solve a problem but not that it actually does :-).\nThe branch vosys-nfv has the stated purpose in branches.md that it is for sending changes upstream. In this case I think that only \"known good\" changes should be pushed onto this branch after they are tested and reviewed. There is no formal definition of how test and review should be done: I only ask that you push changes that work and that don't contain things that will make me complain :-). The easiest way to do that today would be to post the changes as individual PRs on Github and get feedback from anybody who is interested before merging them onto vosys-nfv.\nalso: Once we add vosys-nfv to branches.md it will be mirrored automatically onto the SnabbCo repo and anybody should be able to write git checkout vosys-nfv to run it. The mirroring script currently does not allow rebasing: if you would rebase the branch then it will stop synchronizing until you rebase it back again. This is meant as a feature: the branches are for publishing code for other people to merge and nobody wants to merge from a branch that is being rebased onto a long-lived branch.\nnote: PR branches are also not supposed to be rebased. If I merge a branch onto fixes or next and then it gets rebased then I have a major headache on my hands. Likewise everybody else maintaining a long lived branch. I would actually like to find a way for Github to disallow rebasing PRs. The intention of branches being PR'd should either be that they will eventually be merged or that they are dead-ends that will be replaced by a new (rebased) branch later. Branches that are dead-ends should be marked as \"draft\" and people should not merge them into long-lived branches like next. (More in git-workflow.md).\nHow does this sound? make sense? something we should change? we don't have so much experience with this workflow yet but it has been working well so far and seems to match the kernel process AFAIK.\n. btw: another nice use for the vosys-nfv branch is that it is a mechanism for you guys to take the lead on integrating changes if you want to. For example when somebody submits an NFV-related PR that you are interested in we could assign that to you and you could work on test/review to bring it into vosys-nfv and then pushing it onwards to next from there.\nThis how I see the evolution towards the Linux kernel subsystem maintainer model where each change is initially targeted to a specific person and branch and then moves upstream after it has been accepted there.\n. ... one more idea would be to make vosys-nfv the best branch for snabbnfv users in general. For example by being the fastest to merge good fixes/features from PRs and also doing extra testing when merging new releases from master e.g. to deal with regressions like #592 before it is pushed onto your branch. You could also do your own releases off that branch, etc, if you want to. Just an idea. It would certainly make it easy for me to pull from that branch onto next if you guys are treating it as production code. (Kind of like the way Redhat &co have their own branch of the kernel that is probably a better choice for many users than Linus's upstream tree.)\n. > Then I am not sure why not just merge the PRs themselves instead of moving the patches to vosys-nfv and merge from there?\nYou are right. If the only intention is to push code upstream then it is fine to PR it from a topic branch. We don't need a separate vosys-nfv branch for that purpose.\nThe main reason for a separate branch is if you want to have a branch that you control and publish that to all users, who can either run it directly or merge it into their own branches. For example if somebody is supporting a Snabb based application then they might want to have their own branch for that where they can control what is merged and when, and users might want to see that branch mirrored into the main repository where it is easy to compare it with upstream e.g. via the Github branch list.\n\nCan you clarify what \"rebasing\" means here\n\nI mean that because we now use a merge-based workflow it is not okay to git push --force to published branches (including Pull Requests). This is so that everybody can safely merge from PRs and other branches into their own branches (e.g. as I do with next). If people do force pushes then it causes problems and conflicts for people who already merged the previous version. I think the Git DMZ Flow document explained this idea best.\n\nbtw nice git-workflow.md\n\nThanks! :-)\n\nIs there a way that I get subscribed to receive mail notifications once new PR appears? It will be easier to monitor an pick whatever is important to us.\n\nGreat question! Here are the tips that I am picking up for staying on top of Github activity - working well for me now:\n1. If you have the repository \"Watched\" (not just Starred) then you should get email notifications about issues and PRs.\n2. If you go to the Github front page you should see a list of the latest activity for all the projects you are tracking. In the beginning this was extremely noisy for me but I solved that by not following so many Github users. Now it is very relevant.\n3. I use the iOctocat iPhone app that makes it very easy to browse notifications, Issues, PRs, etc.\nThere is probably a blog somewhere with much better tips than this. Originally I found Github very hard to communicate with but nowadays I use it more than Gmail.\n. I have been geeking out deeply on CPU performance counters. The way this branch is headed is to bypass the kernel and use raw CPU instructions to precisely track counters during a short piece of code execution e.g. the push() method of an app that might execute around one thousand cycles.\nThis would then be used much like wall-clock time for computing the number of events (e.g. cache misses, branch misses) and interesting ratios (e.g. instructions per cycle, cycles per packet, packets per cache miss). Could be that we hook this deeply into the breathe() loop for computing per-app metrics or could be that we use it manually as an optimization tool.\nI see this as a separate feature to the whole software toolchain around performance counters: Linux kernel support including multiplexing many logical counters onto the hardware slots available, perf tools building various analysis tools on top of the kernel perf_event_open(2) interface, and pmu-tools extension layer on top of perf. That stuff is extremely valuable for system-wide analysis and it would be awesome to make LuaJIT play better with that. However, for self-profiling software I found the raw hardware interface more appealing (taking a leaf from Agner Fog's book).\nThe missing code right now is a lib.pmu module to tie it all together and let you execute a function with a named list of performance events being counted for the duration. not finished yet...\n. Phew! The whole PMU API is in place and seems to be working. It is really easy to use.\nTrivial example:\n$ sudo taskset -c 0 ./snabb snsh -i\nSnabb> pmu = require(\"lib.pmu\")\nSnabb> pmu.profile(function() for i = 0, 10000 do end end)\nEVENT                                   TOTAL\ninstructions                           45,702\ncycles                                 45,130\nref-cycles                             90,264\n. @alexandergall whaddayareckon?\naside: I reflect that one day it would be neat to have a notation for generating synthetic traffic. (Or maybe just a really massively mixed pcap file that we could filter to extract the kind of packets we care about.)\n. Sorry about the slow response on this. Newborn baby. I am really looking forward to thinking this lovely little diff through more carefully and merging the code.\nThis change also stimulates a lot of ideas for me e.g. related to memory allocation and whether we could make that code simpler (must be too hairy if even you are not comfortable with it :)). However, this is stuff to experiment with in the future and not to hold up getting this code merged.\n. Sorry to leave this unmerged for so long. I meant to merge it at the start of this month but that didn't happen.\nI have a couple of questions niggling me a bit:\n1. Is catching SIGSEGV this way going to cause problems? I do think we have at least one bug where a genuine segfault will turn into an infinite loop instead of terminating the process.\n2. The snabb --help output is becoming less helpful for end-users every time we add a new program that is really just an internal test case or example program. I would like to avoid this problem and make snabb --help only offer to run stuff that is actually useful to users. However, this should probably be taken as a separate issue rather than blocking this PR.\nI will try to merge this at the start of the next release cycle. It would be very helpful if somebody wants to review the segfault handling code and make sure it is not introducing overly exciting new failure modes.\n. The program name spoon also bothers me because it sounds intriguing and people will probably be tempted to dig in and see what the program does, only to be disappointed that it is an internal test case :).\n. ... also I am wondering more broadly about the whole core.shm mechanism. This has been a useful abstraction for exploring different multiprocessing models but all of the remaining use cases we have for it actually seem fairly awkward to me. I have been meaning to look into whether we could delete that whole concept and cover the few remaining use cases in a simpler way.\nThat is another whole can of worms though and should be taken separately..\n. I am also planning to use this \"in anger\" for 100G support in the immediate future. More on that to follow. (This was the reason that Javier and I decided to pursue the multiprocessing support last summer - to prepare a clear path for applications running 100G+ of traffic.)\n. Looks good to me. Thanks!\n. Currently the performance I see with perf is 11 cycles per packet to run a Source and Sink app and recycle packets between them via freelist.\nI have been reading CSAPP3 and so I can see some obviously bad patterns in my coding style like this series of instructions where each depends on the result of the previous one and probably sucks for ILP:\n-- Write onto output link                                                                                                             \n|  mov rax, output->write\n|  add rax, rcx\n|  and rax, 255\n|  mov [output + rax * 8], packet\nCPUs have changed a lot since the last time I wrote assembler code in anger (Amiga 500...) and so for now I am giving myself permission to suck :-).\n. ... I will try to implement the Tee app to make the program equivalent to basic1 before getting carried away with tweaking the instructions.\n. See also The Unofficial Dynasm Documentation that I only discovered recently. Really great! Note that this is based on the standard C-based dynasm while Snabb Switch is now using the extended Lua-based dynasm.\n. The code is still extremely rough but I added a Tee app so that it implements the same topology as snabbmark basic1.\nEach packet is allocated by a Source from a freelist and lightly initialized, transmitted to a Tee which makes a full copy and transmits on two output links, on to a Sink that puts the packets back onto the freelist. This is roughly equivalent to the work done by the snabbmark basic1 benchmark. I have cut some corners but hopefully not important ones.\nCurrent performance is around 23 cycles per packet. On Interlaken this is 107 Mpps and that is about 5x faster than snabbmark basic1. I suspect that both the asm code and the Lua code could be optimized substantially.\nNote: The reason we have not dived into optimizing these basic operations in the past is that in real applications the performance bottlenecks have been elsewhere i.e. inside the main loops of the apps and not in the machinery that connects the apps together.\nSo why is this interesting at all? Well: I do think that sooner or later we will be in the situation of wanting to squeeze maximum performance out of the app network. There are already some apps that bypass the app network for performance (packetblaster and firehose) and this may actually turn out to be unnecessary. Likely we will also want to write an app that dispatches a 100G traffic stream across different links in an app network and it would be valuable to understand what the maximum performance we could expect from that would be (and maybe this is an app that we really would want to write in JIT'd assembler).\nI also have a basic feeling that our machinery link link.transmit is not sufficiently minimalist but I have not been able to quantify this either by measuring performance or by showing all of the instructions. However, a 5x potential performance gain would make it easier to motivate work like #570 and speeding up the basic1 benchmark.\nHere is a cute feature of the code: copying a 64-byte packet using SIMD. This version uses 128-bit registers:\n|  movdqu xmm0, [packet];    movdqu [packet2], xmm0\n|  movdqu xmm0, [packet+16]; movdqu [packet2+16], xmm0\n|  movdqu xmm0, [packet+32]; movdqu [packet2+32], xmm0\n|  movdqu xmm0, [packet+48]; movdqu [packet2+48], xmm0\nand on AVX2 (Haswell) you would only need half as many instructions (pardon my fudged opcodes):\n|  mov.qu ymm0, [packet];    movdqu [packet2], ymm0\n|  mov.qu ymm0, [packet+32]; movdqu [packet2+32], ymm0\nand on AVX512 (Skylake) you can actually fit a whole 64-byte packet in a register:\n|  mov.qu zmm0, [packet];    mov.qu [packet2], zmm0\nFunny to think about \"packet copies\" that will take only 2 instructions and execute in 1 cycle! (Of course not all packets are 64-bytes but small packets are usually where performance hurts.)\n. Oh and if you want to actually run it then you can do this:\nsudo perf stat ./snabb snsh -l program.snabbnfv.snabbmark_asm\nand it will print the link statistics and CPU performance counters after running 1 billion packets from the source through the app network:\n```\ntee1.read       1000000000ULL   tee1.write      1000000000ULL\ntee2.read       1000000000ULL   tee2.write      1000000000ULL\nfl  .read       2000000000ULL   fl  .write      2000000255ULL\ns2t .read       0ULL    s2t .write      1000000000ULL\ndone\nPerformance counter stats for './snabb snsh -l program.snabbmark.snabbmark_asm':\n64,488,027,290 instructions              #    2.90  insns per cycle\n22,243,300,039 cycles\n\n   9.276217798 seconds time elapsed\n\n```\n. Just having a little more fun here: I ported the assembler code back to Lua with the same simplifications (i.e. no real breathe loop, etc). The notion is that the exercise of writing the code in assembler can help to write an efficient Lua version due to fresh mechanical sympathy.\nHere is how it runs:\n| version | cycles/packet | cycles/step |\n| --- | --- | --- |\n| asm | 23 | 4.6 |\n| asmlua | 51 | 10.2 |\n| basic1 | 116 | 23 |\nSo more-or-less the Lua port of the asm code is twice as fast as the original basic1 benchmark and then the actual asm code is twice as fast again.\nI added \"per step\" values that take the average number of cycles for each of the five \"steps\" in processing a packet. The app network looks like this:\n.------.     \n    Source ---> Tee      Sink    \n                  `------'\nand so the five processing \"steps\" are:\n1. Source app allocates and transmits a packet.\n2. Tee app receives the packet and transmits a duplicate.\n3. Tee app transmits the original.\n4. Sink app receives the duplicate and frees.\n5. Sink app receives the original and frees.\n4.6 cycles per step must be really approaching the limit of what is possible, particularly since one of the processing steps is making a copy of the packet (albeit with only 64 bytes of payload).\nOverall the impression I am forming is that:\n1. The app network abstraction is fundamentally pretty cheap.\n2. The implementation we have can possibly be tuned to double or quadruple speed.\n3. We should remember this if/when we have an app that is limited by basic app network operations e.g. an app that splits up a 100Gbps traffic stream and has to perform link.transmit() at a ferocious rate.\n. > Nice way to get big, impressive numbers\nThis is a problem and it could be that we need to be more disciplined in our use of metrics.\nGenerally the term \"Mpps\" can refer to any code that is operating on packets but we might want to consider reserving this for benchmarks that are actually performing I/O. Then we would need to find a different metric for tests like this one and also the multiprocessor tests that are moving packets between rings in memory rather than physical ethernet links.\nI also think we should move towards metrics based on cycles rather than seconds for synthetic benchmarks. Most benchmark scores are directly proportional to CPU frequency and so if you tested with (say) both a 2GHz and 4GHz then you should expect the same cycles/packet on both but a factor of 2 difference in Mpps. To me this says that cycles/packet is the more useful metric for talking about the performance of a given bit of code.\nThis is the thinking also that lead me to cycles/step in the description above. That is supposed to tell the average cost of processing a packet per app and be somewhat independent of how complex the app network is.\nI actually have a fantasy of making a benchmarking environment that will take any app and generate a \"data sheet\" that says how it performs under various configurations and workloads and also how dependent it is on cache behavior e.g. test separately with packets that fit in L2 cache, in L3 cache, or in DRAM. Just a notion at this stage though.\n. > I don't see the 'original' code that's being compared\nThe reference code is running snabb snabbmark 1e9:\n```\n$ sudo ./snabb snabbmark basic1 1e9\nProcessed 1000.1 million packets in 36.75 seconds (rate: 27.2 Mpps).\n```\nThis is a standard benchmark of the app network with simple Source/Sink/Tee apps. SnabbBot CI is running this benchmark on every PR and will report a test failure if the score regresses. This means that when we optimize the app network we should see an improvement in the snabbmark score and the CI will defend this improvement in the future.\nIdeally I would like to have performance regression tests in place for all applications and only merge optimizations that can be shown to improve the benchmark scores. This is the optimization equivalent to requiring bug fixes to be submitted with a test case to show that it really works and to ensure that it will keep working in the future.\n. Closing PR: I am not actively developing this branch.\n. This seems like a valuable feature to me. Likewise pure VLAN dispatch.\nI wonder whether we should restrict snabbnfv config to only having one port in this setup? If we support multiple ports with a mix of VMDq/non-VMDq then we would have to document how this is expected to behave and keep that consistent across current/future NICs that we support. Could be simpler at least immediately to report an error if this feature is used in a config file that contains multiple ports.\nSide-thought: Looking ahead I think it is likely that we will end up replacing VMDq with a software solution instead. The hardware offloads always seem to become limiting and fragile at a certain point. Based on experiments with #603 I am even becoming hopeful that we could write a software traffic dispatcher for 100G ethernet ports. However, VMDq is the solution we have today and it does work fine up to this point.\n. @javierguerragiraldez I am experimenting with the \"split driver\" approach on #561 (Intel I350/1G) now. I will resubmit that when the design is more mature. Meanwhile I added some comments this morning to explain the design a bit. Basic concept is to move decisions from happening automatically inside the app (e.g. assignment of TX/RX queues) to being explicitly configured in the app network. So when you create an intel1g app you tell it which TX queue to use (if any) and which RX queue to use (if any) and whether to initialize the NIC (or assume that somebody else does this). It seems promising to me and it should make basic I/O orthogonal to queue setup (VMDq / RSS / etc) as you say. If this design makes sense we could even add 82599 support to that driver?\nThe downside though of hardware offloads like VMDq and RSS is that they are not universal. Marcel mentions being interested in more exotic protocols and we can't really depend on commodity NICs to handle dispatching on PPPoE, MPLS, GTP, and so on. So I like the idea of making the hardware capabilities available, and for some applications they will be exactly the right thing, but I am anticipating that people will prefer well-optimized and flexible software dispatching in the future. Juho Snellman made a similar remark in a great recent presentation: Mobile TCP Optimization - lessons learned in production. The product that Juho is describing is actually an ancestor of Snabb Switch: I worked on that with Juho immediately before starting Snabb Switch and that was the first time we ditched vendor libraries and wrote our own drivers.\n. @javierguerragiraldez Cool. I am in favor of this API style since your benchmarking campaign and I like the idea of using it pervasively.\nI wonder whether it is better to do this piecemeal or in a \"big bang\" that defines a coherent API? I started that in #465 (Create a snabb module for core API) but never completed it. There are still some other interesting issues to consider in addition to function/method dispatching e.g. is it bad for packet.receive() to return an FFI pointer that may need to be heap-allocated and garbage collected?\nre: performance implications we really need a performance regression test suite that can quantify these matters so that we don't have to make guesses in pull requests. That is actually the idea behind the snabbmark basic1 benchmark that runs simple source/sink/tee apps and SnabbBot will benchmark that on every PR and show whether the speed increases or decreases (latter is flagged as an error). So if part of the motivation for this change is to improve performance then I think it makes sense to also update the snabbmark basic1 benchmark so that SnabbBot can show us the performance impact.\n. Pondering this very interesting topic a bit more...\nOverall I feel like it would be great to revise our core API and make it very efficient. Doing this well seems to require a fairly deep understanding of the limits of LuaJIT and the CPU. I am working on this in the background via my blogging, writing apps in assembler by hand (#603), improving the profiler to be able to compare our JIT'd code with the hand-written code (#611), and thinking about a unified facade for the API (#465).\nI hope the result of these explorations will be a simple API that is hard to misuse and also some practical optimization and profiling tips to make it easy to get good and predictable performance.\nThere are a few specific issues that I want to address in the API:\n1. Use efficient function/method dispatch e.g. the FFI metatype style of this pull request.\n2. Keep API functions on-trace: avoid small branches/loops that can lead to side traces. (The \"straightline\" design seems great here: no looping over iovecs in the basic functions.)\n3. Keep API functions garbage-free: avoid allocations that might not be sunk e.g. returning freshly created FFI pointers from functions like link.receive() and packet.allocate().\n4. Make API performance robust to different compilation contexts e.g. fast for apps that loop in a single trace (able to \"hoist\" most work) and also fast for apps that are jumping between traces a lot (having to repreat a lot of work).\nThis is all probably quite simple once you have a strong enough mental model of LuaJIT but I feel like I am only around half of the way there.\nMeanwhile it is hard to evaluate changes like this PR.\nOn the one hand it makes sense to merge code quickly if it is really having a measurable impact on a benchmark that we care about e.g. NFV packet forwarding. Is that the case? If so it would be interesting to know.\nOn the other hand I am cautious about merging piecemeal changes to the core API. Should everybody switch to the new syntax? Should all existing code be changed? Are we going to change this again in a short while when we understand LuaJIT better? These questions would all be easier to answer with a unified API update that comprehensively solves all the known problems of the existing API, rather than with individual extensions like adding a second way to call functions on packets and links.\nDoes this make sense?\nPerhaps we need an Issue to start tracking the requirements for an updated API. I think that a bunch of us are making parallel investigations of LuaJIT behavior and coming up with theories of how the API and programming style should be adapted for better performance.\n. I don't think that master is the right place to land this PR. I see this as a step in the right direction towards a nicer core API but I would like to merge a whole API onto master and not only bits of it.\nThe trouble with the piecemeal approach is that in the short term it means more code, less consistency, and no more functionality.\nCould be that we need a snabb-api-2.0 branch to collect API improvements and then actually apply them to the codebase before merging onto master? (kinda like we did with straight-line?) I would like to do this but don't have the time right now.\n. Howdy!\nSorry for the slow response.\nSo do I understand correctly that the two main changes that need to be integrated here are:\n1. Makefile builds libsnabb.so instead of snabb. (Should have option to build either/both?)\n2. Firehose expects to find the callback functions directly without loading a separate shared library (need to support both modes or just this one?)\nI also had another thought: would it be interesting to use snabbswitch as a separate program instead of a .so? I mean so that you could run firehose to initialize a NIC and put the DMA buffers in a shared memory file, then fastnetmon could separately use a small library to map that file and access the packets. Could simplify matters potentially, or?\n. Howdy!\nSo wondering what the most expedient way is to support this on the master branch.\nCould use e.g. make EXTRACFLAGS=-fPIC and a separate target to build the shared library?\nI am also tempted to make it libfirehose.so instead of libsnabb.so. The reason being that this is still quite a rough first cut at integration. If we wanted to be a really neat shared library then I suspect we should change a bunch of things (more conservative use of the C symbol namespace, document/rethink any potential conflicts e.g. calls to library functions that may be non-reentrant, etc). Right?\n. Can you explain more definitely what support you need here Pavel? (Do you actually need this upstream for your PoC? what is your plan?)\n. So! This is actually a good test case for the Snabb Switch git workflow. Bear with me here please :-)\nJust now you seem to be stuck waiting to synchronize with upstream but actually that is not supposed to happen. Instead application developers should be able to hack up exactly what they need, ship their application, and then sync with upstream as a background process over time. This should be good for everybody because it means that application developers don't get blocked and also code landing upstream has already had some battle-testing and is not so experimental.\nWIth me so far? :-)\nSo my suggestion would be to create a branch called e.g. firehose_fastnetmon and add a patch that builds libfirehose_fastnetmon.so instead of snabb. Then you can use this to build and ship your application from. Then you are self-sufficient and have complete control over what you ship to your users.\nIf you want to stay in sync with upstream then you only have to git merge master once per month after a new release is made. If you want to give visibility into your changes you can also publish your branch in branches.md where the rest of us can easily see them (everybody would get your branch when they clone the upstream repo).\nOver time we can work out what is the best integration with upstream. The answer to this could change in the coming weeks/months e.g. we have #589 landing soon that makes it possible to transparently share all DMA memory between any processes on the machine without having to link them together in any way. That could make it possible to have the same tight integration between fastnetmon and firehose but without any object code linkage. That would potentially be a lot easier to maintain.\nI feel like the optimal solution to linking processes together is emerging now but we don't have the complete picture yet.\nWhat do you think? Crazy talk? :-)\n. @kbara Agree. Really the problem we want to solve is to increase our mastery of and contribution to LuaJIT and other dependencies. Stripping out features is only one idea to that end and potentially a really dumb one, particularly since anybody could want to port snabbswitch to ARM or make a Windows port of SnabbNFV for HyperV instead of KVM.\n609 is a more low-key alternative formulation of this idea. (I close this PR now in favor of that one.)\n. @justincormack Sounds good. Thanks for the words of experience.\n@eugeneia @SnabbBot not testing?\n. Superceded by #617.\n. Thanks!\nThe next step I am working on is to separately track the \"loop\" part of a trace from the \"straight\" part. The idea being that it is very interesting to know whether a trace is spending its time staying inside the loop (probably efficient with lots of work \"hoisted\" out of the way) or in the straight part (probably expensively being re-entered from a side-trace). Really rough patch here: https://gist.github.com/lukego/d801cc7ad5fa68d884fb\nExample output on rate_limiter:\n97  TRACE  17:LOOP (13/4)   ->13       rate_limiter.lua:82\n   55  TRACE  10:LOOP          ->loop     basic_apps.lua:26\n   54  TRACE  13               ->loop     rate_limiter.lua:73\n   11  TRACE  39:LOOP (13/17)  ->15       link.lua:70\n    9  TRACE  22:LOOP (16/2)   ->10       timer.lua:46\n    8  TRACE  10               ->loop     basic_apps.lua:26\nHowever I am not satisfied with the code yet. I \"back tested\" it on the example from lukego/blog#8 and expected to see three lines reported (Trace A, Trace A:LOOP, and Side Trace B) but actually I see only this:\n409  TRACE   3               ->loop     s2.lua:5\nSo there is something wrong in my mental model. I have been digging into how the profiler works and what I see is that there is not a 1:1 relationship between a SIGPROF and a callback to the profiler. Instead multiple samples are being coalesced together into a smaller number of callbacks. I suspect this is costing resolution i.e. all samples are being counted as the same code location in this example even though multiple traces are executing. (Could be that the results for rate_limiter are also misleading for the same reason but that it is less obvious.)\nI am pursuing an idea for improving the situation :).\n. This is a really fantastic investigation and write-up :). More thoughts to follow..\n. I have not reproduced this setup yet but initial thoughts anyway:\nThis is a great write-up! I think that we really do need to go through the exercise of following the compiler in minute detail a few times to make sure that we understand what is going on. Thank you for digging into this and sharing what you figured out.\nI have been doing a little related tracing in lukego/blog#8. I hope that once we have done this a few times we will be able to automate the analysis e.g. with profiler extensions like #621 and perhaps automatic generation of graphviz trace diagrams with hotspots indicated.\nHaving app instances interfere with each other seems unfortunate. It would be nice for an app to always behave in the same way independent of the rest of the app network. (This is already hard due to e.g. shared caches but it still seems like a good goal to strive towards.) Here we are solving one instance of this problem with a more careful programming style. I wonder if a reasonable alternative would be to somehow tell LuaJIT to always have separate traces for each app instance? (Could perhaps be accomplished by loading the app module fresh for each instance instead of using require() that caches and reuses the module definition on each call.) This would be moving back in the direction of \"sandboxes\" where we have done a lot of experiments already.\nGenerally I would love to get to the point where it is straightforward to optimize an app, understand its performance under \"lab conditions\", and accurately predict its performance in production and in combination with other apps. It feels like this is a pretty big problem to solve but that we are already making a lot of progress.\n. I reproduced this now. It works quite well to have a little snsh script in an issue/PR.\nI confirmed that the alternative trick of loading a separate copy of the Lua code for each app instances will also solve the problem. This way I believe that each app has a separate trace and so there is no need for the instances to behave in a compatible way.\nI did this in a really kludgy way - removing the modules from the cache that require() looks in - but if we wanted to adopt this behavior for all apps then we should be able to find a better solution from amongst @javierguerragiraldez's many experiments with sandboxing possibilities.\nHere is the code:\nlua\nlocal Intel82599_1 = require(\"apps.intel.intel_app\").Intel82599\n-- Load a second copy of the Intel driver modules                                                                                     \npackage.loaded [\"apps.intel.intel_app\"] = nil\n_G.apps.intel.intel_app = nil\npackage.loaded [\"apps.intel.intel10g\"] = nil\n_G.apps.intel.intel10g = nil\nlocal Intel82599_2 = require(\"apps.intel.intel_app\").Intel82599\nlocal c = config.new()\nconfig.app(c, \"if1\", Intel82599_1, { pciaddr = \"0000:01:00.0\" })\nconfig.app(c, \"if2\", Intel82599_2, { pciaddr = \"0000:02:00.0\" })\nconfig.link(c, \"if1.tx -> if2.rx\")\nconfig.link(c, \"if2.tx -> if1.rx\")\nengine.configure(c)\nengine.busywait = true\nrequire(\"jit.dump\").start('+rs', 'dump')\nengine.main({ duration = 10, no_timers = false, report = { showlinks = true, showload = true } })\n. btw: This PR would close automatically when it merges onto master in a release. (I think it's okay to leave PRs open before they are released because it gives people a window of time to discuss the change and potentially suggest reverting it before release.)\n. no worries :)\n. I was reminded of this PR when I came across this note in the LuaJIT FFI API documentation today:\n\nPlease note that an anonymous struct declaration implicitly creates a new and distinguished ctype every time you use it for ffi.new(). This is probably not what you want, especially if you create more than one cdata object. Different anonymous structs are not considered assignment-compatible by the C standard, even though they may have the same fields! Also, they are considered different types by the JIT-compiler, which may cause an excessive number of traces. It's strongly suggested to either declare a named struct or typedef with ffi.cdef() or to create a single ctype object for an anonymous struct with ffi.typeof().\n. There does seem to be a long delay before these NICs reach the \"link up\" state. I don't know why exactly but we made our selftest routine accept this lag.\n\nCan you start actually passing traffic more quickly with the kernel driver? Could be that we can accelerate the init somehow.\n. Closing PR: I am not actively developing this branch.\n. Closing PR: I am not actively developing this branch.\n. The version of LuaJIT that I imported is the vanilla upstream v2.1-beta1. This is missing some features that we have added on the Snabb version of LuaJIT. I reintroduce these features in #619, #620, and #621.\n. Good to go I reckon :)\n. Oh actually it looks like one queue is enough when descriptor prefetch is enabled. Here is a preliminary patch that does the job:\ndiff\ndiff --git a/src/apps/intel/intel10g.lua b/src/apps/intel/intel10g.lua                                                                \nindex a3b1584..437082d 100644                                                                                                         \n--- a/src/apps/intel/intel10g.lua                                                                                                     \n+++ b/src/apps/intel/intel10g.lua                                                                                                     \n@@ -421,6 +421,7 @@ function M_sf:init_transmit ()\n    self.r.HLREG0:set(bits{TXCRCEN=0})                                                                                                \n    self:set_transmit_descriptors()                                                                                                   \n    self.r.DMATXCTL:set(bits{TE=0})                                                                                                   \n+   self.r.TXDCTL:set(bor(bits{Enable=25, SWFLSH=26, hthresh=8}, 32))                                                                 \n    return self                                                                                                                       \n end\nthen I see 14.878 Mpps with just a source feeding one Intel app.\nHave to verify this and check out the receive side too.\n. @alexandergall Sorry if I am duplicating work that you are already doing. I know you mentioned wanting to solve this mystery and that kind of captured my imagination too :).\n. I pushed a fix that includes the linkage between traces now too. Report looks like this on lukego/blog#8 example:\ntraceprof report (recorded 10572/10572 samples):\n 63% TRACE   3               ->loop    counter2.lua:5\n 25% TRACE   4      (3/5)    ->3       counter2.lua:9\n 11% TRACE   3:LOOP          ->loop    counter2.lua:5\nI am really happy with how closely that matches the analysis that I did by hand. Could be that the next time I have this situation I can diagnose the problem by looking at traceprof output and without opening the JIT dump file.\n. I had a somewhat epic Twitter discussion with a couple of LuaJIT internals gurus about this.\nOverall this has reinforced my feeling that it makes sense to maintain traceprof. The problem I want to solve is making the trace-level compilation behavior made by LuaJIT transparent to the application programmer. The JIT behavior to observe is very subtle and complex and in my mind this calls for a very predictable and simple tool. Traceprof is such a tool: it simply logs the instruction pointer at regular intervals and then summarizes those values in terms of traces.\nThe standard jit.p uses a more complex method of sampling: it dynamically instruments the bytecode/IR to cause trace exits that call into the profiler. The exact points of these exits are a bit subtle and it can happen that many profiler events are coalesced together because execution did not reach an exit to the profiler in a reasonable timeframe. (That seems to be the reason that even with 10 billion loop iterations jit.p is only seeing one of the traces in the example above.)\nThis may also be the reason that upstream LuaJIT jit.p module does not offer per-trace reports. The code we are using for this was written by Mike but I don't believe he ever actually adopted it himself. This could be because he understood that it was not necessarily well matched for the sampling method that is being used.\nI do still see jit.p as the right solution for basic source-level profiling and especially when you want to see call trees. However, I prefer the traceprof approach for more low-level trace-oriented profiling like in #612 and lukego/blog#8.\nIncidentally in the tweet stream there is also a reference to ll_prof.py which is a tool that the V8 (Google/Javascript) developers use. They are using perftools as the backend to record data and then writing tools to analyze the raw dumps afterwards. This is an interesting idea. For now I prefer the method of doing everything from first principles (e.g. lib.pmu) and within our own process (with access to all internal data structres e.g. bytecode/IR/mcode). However, maybe the trade-offs will look different in the future e.g. if we want to support many CPU architectures with different PMU hardware.\n. Thought of the morning: Could it be a problem with mapping the vring memory in the first place?\nThat is, rather than this guest starting from nonzero indexes on its used/avail rings, could this simply be garbage memory and not a real vring? I am testing with our QEMU v2.1 at the moment but I believe some fixes have landed in newer QEMU versions for vhost-user memory mappings. I will rebase the reconnect feature onto a newer release and see if that makes a difference.\n@nnikolaev-virtualopensystems Thanks for the ideas! I am testing with an A10 vThunder virtual appliance.\n@javierguerragiraldez Good thought! I think it is not acceptable to do garbage DMA. That would potentially mean transmit/receive of arbitrary bits of guest memory (same as if we would put garbage DMA descriptors with random addresses into a physical NIC). I would be less concerned about retransmitting valid packets than this personally.\nIncidentally here is the error message that triggered this whole investigation:\nSet features 0x10020000\nVIRTIO_NET_F_CTRL_VQ VIRTIO_RING_F_INDIRECT_DESC\nrxavail = 0 rxused = 0\nrxavail = 8738 rxused = 8738\nvhost_user: Connected and initialized: /root/vhost-sock/B.socket\nload: time: 1.00s  fps: 56,797    fpGbps: 0.025 fpb: 0   bpp: 0    sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nlib/virtio/net_device.lua:357: mapping to host address failedcdata<void *>: 0x7fe091030cb8\nstack traceback:\n        core/main.lua:116: in function <core/main.lua:114>\n        [C]: in function 'error'\n        lib/virtio/net_device.lua:357: in function 'map_from_guest'\n        lib/virtio/net_device.lua:129: in function 'packet_start'\n        lib/virtio/virtq.lua:67: in function 'get_buffers'\n        lib/virtio/net_device.lua:122: in function 'receive_packets_from_vm'\n        lib/virtio/net_device.lua:108: in function 'poll_vring_receive'\n        apps/vhost/vhost_user.lua:76: in function 'method'\n        core/app.lua:75: in function 'with_restart'\n        core/app.lua:276: in function 'breathe'\n        core/app.lua:231: in function 'main'\n        program/snabbnfv/traffic/traffic.lua:85: in function 'traffic'\n        program/snabbnfv/traffic/traffic.lua:61: in function 'run'\n        program/snabbnfv/snabbnfv.lua:15: in function 'run'\n        core/main.lua:56: in function <core/main.lua:32>\n        [C]: in function 'xpcall'\n        core/main.lua:121: in main chunk\n        [C]: at 0x0044dd60\n        [C]: in function 'pcall'\n        core/startup.lua:1: in main chunk\n        [C]: in function 'require'\n        [string \"require \"core.startup\"\"]:1: in main chunk\nThat is an assertion failure when we can't translate the address of a vring descriptor (consistent with the address value being garbage).\n. This seems to be a QEMU issue. Everything looks much more normal when testing with the latest QEMU v2.4.0. I suspect the issue is a problem of memory mapping i.e. we are not seeing the real vrings in the cases that fail. More testing is needed to confirm.\nI have rebased the one patch we have that is not upstream and pushed a v2.4.0-snabb branch to the SnabbCo/qemu repo. Perhaps we should start recommending this version to users instead? (Except for people have specific requirements e.g. want to backport features/fixes onto a specific version to match a distro.)\nLink: https://github.com/SnabbCo/qemu/tree/v2.4.0-snabb\n. Closing: This is assumed to be a QEMU issue and that the existing Snabb behavior is good.\n. What a lot of work! This is going to be awesome :-)\nI am taking this for a spin on NixOS (chur). Apologies in advance :). Having a different distro inside and outside the containers is probably going to uncover some obscure bugs but I hope that is a valuable exercise.\nThe test scripts (e.g. make benchmarks) don't run on NixOS. bash is not called /bin/bash and instead you need #!/usr/bin/env bash. No biggie: running tests inside docker should avoid this.\nThe snabb binary compiled on NixOS does not run inside the container. I think that it is looking for glibc in the wrong place (interesting data point re: Linux binary portability). No biggie: building the binary inside docker should avoid this.\nBuilding and running inside the container with sudo scripts/dock.sh bash -i does seem to basically work! I did not get success on all of the benchmarks though and I am not sure how to troubleshoot:\n```\nSNABB_PCI0=0000:03:00.0 SNABB_PCI1=0000:03:00.1 make benchmarks\nbasic1-100e6 20.4\npacketblaster-64 9.47\nsnabbnfv-iperf-1500 0\nsnabbnfv-iperf-jumbo 0\nTerminated\nsnabbnfv-loadgen-dpdk 0\n```\nI don't really understand how to build the assets e.g. a custom kernel? Looks like these live in a separate repository called eugeneia/snabbswitch-docker but cloning that and running the Makefile didn't work:\n$ make\nMKDIR   assets\nCOPY    QEMU SnabbCo\nMKDIR   context\nCOMPILE Linux ubuntu-trusty\nmake[1]: Entering directory `/home/luke/git/snabbswitch-docker/kernel/ubuntu-trusty'\ncat: debian.master/config/config.common.ubuntu: No such file or directory\ncat: debian.master/config/amd64/config.common.amd64: No such file or directory\ncat: debian.master/config/amd64/config.flavour.generic: No such file or directory\nmake[2]: Entering directory `/home/luke/git/snabbswitch-docker/kernel/ubuntu-trusty/ubuntu-trusty'\nmake[2]: *** No rule to make target `bzImage'.  Stop.\nmake[2]: Leaving directory `/home/luke/git/snabbswitch-docker/kernel/ubuntu-trusty/ubuntu-trusty'\nmake[1]: *** [ubuntu-trusty/arch/x86/boot/bzImage] Error 2\nmake[1]: Leaving directory `/home/luke/git/snabbswitch-docker/kernel/ubuntu-trusty'\nmake: *** [assets/bzImage] Error 2\nSo some clarification of the workflow for that would be very helpful. Maybe another README that explains the \"big picture\" like how to bootstrap everything and how to know what versions of stuff you are really running?\n. One more drive-by testing thought:\nThe original intention of make test was to be a quick and broad test to run before committing code. That only works when it takes around 30 seconds to run. This broke down when we added longer running tests e.g. rate limiter benchmark, extended NIC selftest, NFV, etc. I don't think anybody routinely runs the test suite locally anymore because it takes too long.\nIt would be very good to strip make test back down to 30 seconds or less. For example, skip the NFV tests, run 5 iterations of the NIC tests instead of 100, and so on. Then we could have a separate way to run the full test suite e.g. in CI context.\n. @eugeneia Good point and well argued with data :).\n. In the short term I reckon it makes sense to focus on running the tests inside containers where we can safely assume that all the right software is installed where we expect it. Then we can consider to what extent it is worth porting the test infrastructure to run \"natively\" in other environments e.g. NixOS.\n. Still getting the hang of this... :)\nAny tips for debugging?\n[luke@chur:~/git/snabbswitch/src]$ sudo SNABB_PCI0=0000:01:00.0 SNABB_PCI1=0000:01:00.1 scripts/dock.sh program/snabbnfv/packetblaster_bench.sh | tee log.txt\nDefaulting to SNABB_TELNET0=5000\nDefaulting to PACKETS=100e6\nDefaulting to CAPFILE=64\nDefaulting to MAC=52:54:00:00:00:\nDefaulting to IP=fe80::5054:ff:fe00:\nDefaulting to GUEST_MEM=512\nDefaulting to HUGETLBFS=/hugetlbfs\nDefaulting to QUEUES=1\nsnabbnfv traffic starting (benchmark mode)\nLoading program/snabbnfv/test_fixtures/nfvconfig/test_functions/snabbnfv-bench1.port\nengine: start app B_NIC\nengine: start app B_Virtio\nGet features 0x18428001\n VIRTIO_F_ANY_LAYOUT VIRTIO_NET_F_MQ VIRTIO_NET_F_CTRL_VQ VIRTIO_NET_F_MRG_RXBUF VIRTIO_RING_F_INDIRECT_DESC VIRTIO_NET_F_CSUM\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\n...\nlooks like no traffic is processed.\n. I'm a bit lost here: I can't connect to a VM on port 5000 or 5001 and I can't find the tmux session to attach to either. Can you take a look and see if you can run the tests on chur please?\n. Yep. I modified dock.sh to run docker -t so that I can run the container interactively.\nThe whole configuration of chur is on Github and you should have an account defined here:\nhttps://github.com/SnabbCo/snabblab-nixos/blob/master/chur-configuration.nix#L170\n. How did you find that error message? (Any refresher on troubleshooting problems during test?)\nI believe the problem is that DPDK was compiled on a newer CPU than what chur has. DPDK is tricky to deploy: they choose a specific CPU microarchitecture at compile-time and then if you the binary on a different CPU you will get an error, a crash, or bad performance.\nCould be that we can update the scripts for bootstrapping the VM to compile with more conservative hardware expectations. I'll take the new README on bootstrapping the assets for a spin.\n. The kernel takes a very long time to clone. Maybe instead of a submodule we should use a git clone --depth=1 command?\n. are you running docker inside docker? I did not expect the scripts/dock.sh on the second line..?\n. More recursive dockery...\nmake image doesn't run on NixOS. should I be running that inside docker? (any particular requirements?) The problem I hit was bc not being available in PATH.\n. I think we need to automate the asset building with a Dockerfile instead of a Makefile. I'm testing on NixOS and the first few dependencies were minor (bc and openssl) but now it is trying to run dpkg-gencontrol that looks Debian/Ubuntu specific and at this point it seems like I am bumping into the same problem that docker is supposed to solve.\n. I think I need to look more closely at the scripts and draw some pictures of the whole container workflow. I am still in the naive docker-newbie world where every problem can be solved by one more application of containers (and where any other solution e.g. manual package installation is taboo). I don't really appreciate the limitations of docker build etc yet.\nReally what I want to accomplish now are:\n1. Work out how to run the benchmarks/tests on chur and attach to the tmux to see all of the processes running.\n2. Try rebuilding the guest with different DPDK build options so that it will run on chur's CPU.\n. The kernel's build dependencies are actually more than I expected. bc, openssl, dpkg, and now some missing perl modules. I start to feel some sympathy for the people who have struggled to compile Snabb Switch on different environments like 32-bit or musl-libc.\n. (I will keep thinking aloud as I work this out. Please bear with for a while :). I am excited to be able to run the CI suite on the rest of the lab servers starting with chur.)\n. The scripts on this PR all look really nice and simple and neat :).\nI am struggling with running things a bit though. I am still not sure how you were able to see the error message from DPDK but I didn't find it. It might make life easier if there is an easy way to attach to the tmux and see the processes?\nI tried building the assets inside an Ubuntu container with a Dockerfile like this:\nFROM ubuntu:15.04\nRUN apt-get update\nRUN apt-get install -y build-essential gcc pkg-config glib-2.0 libglib2.0-dev libsdl1.2-dev libaio-dev libcap-dev libattr1-dev libpixman-1-dev libncurses5 libncurses5-dev git telnet tmux numactl wget\nRUN git clone https://github.com/eugeneia/snabbswitch-docker\nbut for some reason I can't check stuff out:\n```\ngit submodule update --init --depth=1\n...\nfatal: reference is not a tree: c9cea8f431c929f70a9371f4b379ab66c15c5293\nUnable to checkout '9692d47896b508d94991ec9dfa75fd6bddd73981' in submodule path 'kernel/ubuntu-trusty/ubuntu-trusty'\nUnable to checkout 'c9cea8f431c929f70a9371f4b379ab66c15c5293' in submodule path 'qemu/SnabbCo/qemu'\n```\nand I know you had something similar once: do you know the solution?\nGeneral observation: running commands partly on the host and partly in a container seems to be asking for trouble. I see this on NixOS but I guess this would also bite e.g. with different versions of Ubuntu inside and outside.\n. Zooming out a bit and thinking holistically about the whole set of use cases and whole technology stack and how it relates to this PR...\nUse cases:\n1. Compile and run Snabb Switch.\n2. Deploy a Snabb Lab server.\n3. Quickly and repeatably run CI tests on any machine.\n4. Quickly experiment with new CI-like tests on any machine.\n5. Publish new tests that are easy to share and repeatable.\nCompile and run Snabb Switch: currently we don't define a controlled build environment and we just hope that the build infrastructure is simple enough that the user will succeed. They do need a suitable gcc, GNU make, GNU shellutils, etc. This seems to be working basically okay but does have lapses e.g. somebody reported failure to build on a recent Debian release and it was a find argument that turned out to be very recently added. The overall situation here seems basically satisfactory to me anyway (if anything I am tempted to move towards making distro-portable binary releases instead of expecting users to compile).\nDeploy a Snabb Lab server. I am very happy with NixOS from an administrator point of view. Makes it easy to use config files in a Github repo to manage details like kernel version, IOMMU setting, hugetlb preallocation, user accounts on each machine, etc. Huge improvement over the situation with Ubuntu and a bunch of people independently making well-meaning changes in the root shell. NixOS is a weird user-space environment but containers are the solution here: just run something like 'tmux docker --privileged -t -i ubuntu' and you can forget about NixOS. (This should work fine: I'm writing this on a laptop that runs a full graphical Ubuntu inside a container on ChromeOS.) NixOS is essentially a hypervisor for containers (same idea as CoreOS, etc). On reflection we shold really not bother about stuff like #!/usr/bin/env bash to make NixOS happy because that is a layering violation - NixOS is the container hypervisor and not the runtime environment for running the CI test suite.\nQuickly and repeatably run CI tests on any machine. Covered most excellently by this PR. Containerized test environment is obtained from dockerhub and then an arbitrary version of Snabb Switch can be tested inside. Nits: there is a learning curve for troubleshooting the test environment is a bit steep and also some risks of container-related hygenie problems e.g. tests are likely to fail if you compile snabbswitch on the host and then try to run it inside the container (at least on a NixOS host). For example if you compile snabb on NixOS and then run inside an Ubuntu container you will get an error like snabb: File not found which is really complaining that libc isn't at the path the linker expects.\nQuickly experiment with new CI-like tests on any machine. Like: fire up a tmux session containing two VMs with vhost-user interfaces, or with packetblaster attached to a couple of interfaces, and then be able to manually run a Snabb instance to process traffic. For example the test case in #612 (shunt script) could reuse a \"blasting 64-byte packets through bump in the wire\" test harness instead of the user having to manually work out how to run packetblaster in the right topology. I believe the last time we discussed this we decided to make it explicitly out of scope for the CI but this still seems like a pity to me because we have all the \"bits\" inside test_env and seem like they could make life easier. I know that people are regularly fighting with understanding QEMU's incomprehensible command-line syntax, as seen in e.g. the Snabb NFV Getting Started Guide, and it would be lovely if test_env provided reusable convenience scripts that could be used instead (topic for the future..)\nPublish new tests that are easy to share and repeatable. This PR seems like a huge step forwards in this respect: have a common set of env variables that can be reused when defining tests, have a way to create new versions of the test \"assets\" like VM images, and have everything hosted on Github and Dockerhub. (Incidentally I noticed that Github has added support for large file storage now.)\n... Sorry, mega braindump I know ...\nSo coming back on topic all we really need to solve with this PR are:\n1. Run CI test suite on any docker host (including chur).\n2. Make sure mere mortals like me can successfully bootstrap a custom test environment.\nthen relatedly I want to install NixOS and SnabbBot on all lab servers.\nWhile I am already ranting...\nI also think we need to stop manually entering PCI addresses. This seems fine from the SnabbBot perspective since you can pick an address and stick to it. However it is terrible from the perspective of sharing development servers between multiple people because everybody is manually guessing addresses to try and we end up either with very low hardware utilization or with people frequently colliding with each others' tests. We need to fix this somehow. This is challenging because different people have different needs at different times e.g. sometimes you want to run a 1-minute test with any pair of ports, other times you want to run a long test with 6 ports on the same NUMA node, etc. I am not sure how to solve this. Could even be worth throwing hardware at the problem e.g. moving towards having more smaller lab servers each for use by small groups instead of a few beefy ones. NixOS might make this realistic to administrate and though the cost per core+port would look bigger on paper it might be less in practice if it increases utilization i.e. density of people who can work in the lab before it feels crowded.\nEnd mega braindump ! Sorry to hijack this PR thread :)\n. The code is really getting pleasingly short and orthogonal :). Maybe we could make it more user-friendly though?\nSome details are hidden but could be helpful to make more transparent to the user:\n- How the docker container is launched. Hidden in dock.sh but could be simplified enough to run manually? (also: quietly using a global --name that will collide when two instances are started using the same unix account.)\n- The software versions that will be used for test. These seem to be quietly stored in ~/.test_env within the container. Could be simpler for the user if the software and images were stored in more obvious places (e.g. QEMU installed in $PATH and test assets given unique names)? Then one could conceivably even run the test scripts independently of the docker env.\n- Usage of tmux for managing processes. Could be more obvious if the default behavior would start the tmux session interactively so that the user could easily page between the windows (and potentially see output of commands that have failed unexpectedly in them)?\n- Generally there is a lot of indirection: the test scripts don't really spell out what commands they are running but rather call shell functions that make a lot of substitutions. It is necessary to read the shell libraries pretty carefully to work out details like how tmux is used and what the QEMU image filenames will be. I might find the code easier to read and troubleshoot if it were built from more independent parts e.g. a shell script with a documented getopt-style command-line syntax for starting QEMU and perhaps \"inlining\" the tmux commands instead of hiding them in the library?\nThere are also some details that the user supplies explicitly that I would love to somehow eliminate:\n- PCI addresses. Generally speaking we don't have a solution to the \"how do I decide which PCI address to supply?\" problem. Could be that this needs to be automated somehow e.g. always take the lowest-numbered cards or probe for a card that is available. See #634. (This might be a separate problem: perhaps we need another method by which the PCI address env vars will be already defined so that the user doesn't have to do it.)\n- Telnet ports. This just seems like a leak of an internal detail of the scripts. Can we avoid this? Looks like QEMU supports using a named pipe or unix socket (filename) instead of a tcp socket (port number). Then we would need a telnet/netcat-like program that can talk to a unix socket or named pipe.\n- Pcap file for packetblaster. Really we should extend packetblaster to generate packets of a chosen size instead of needing pcap files.\nI don't really now how good and relevant all of these ideas are but this is what is coming to mind while poking around and trying to develop a mental model :)\n. > This whole docker/qemu business feels a ot like this movie, Inception I think was the title. ;)\nYeah :). It is surprisingly complicated. Is there a simpler way we could do all of this?\n. Howdy!\nI could not resist having another look at methods for storing and sharing big files like VM images in a controlled way. I had a quick read about:\n- Github Large File Support (new)\n- Dockerhub\n- Dropbox\n- Bittorrent Sync\n- Syncthing\n- Tarsnap\n- git-annex\nI actually think that git-annex looks very interesting:\n- Lets us add big files anywhere e.g. in program/snabbnfv/test/vmimages.\n- New images can be sent in PRs (together with corresponding bootstrapping script).\n- Github only actually transfers/stores the filename and SHA hash of the image (not content).\n- Separate commands for syncing content e.g. directly between machines or via third-party hosts.\n- Can refer to non-open-source VMs by SHA hash (have to obtain VM file separately).\n- Compatible with third party hosting (e.g. for backups) but not dependent on it.\nI wonder if that would potentially simplify the workflow somehow?\n. This is a fascinating odyssey through modern and classic build tools. My head does hurt :).\nLet's look again at the problems that we want to solve (and apologies for freely revising this list on a whim...):\n1. Make it easy (\"one-liner\") to faithfully reproduce a test case on a new machine.\n2. Make it easy to package up a test case for somebody else to run.\n3. Simplify test scripts: let them assume they are running in a suitable environment.\n4. Make it possible to define many different test environments (qemu, guest, libvirt, openstack, etc).\n5. Make it easy to deploy a CI to a Snabb Lab server.\n6. Make it easy to deploy a CI as part of a different environment e.g. GitLab/Jenkins.\n7. Make everybody comfortable to do day-to-day hacking in the Snabb Lab.\nHow are we doing? which problems are easy vs hard, and which tools match which problems?\n(1) does seem like a good match for Docker. Docker does deployment well if you have the test environment already uploaded to Dockerhub.\n(2) seems to depend on the problem. You could share a test case as a shell script, a snsh script, a Dockerfile, etc, depending on how much of its environment it has to capture. I do think that Docker is a valuable addition to this toolbox e.g. if you want to demonstrate that Snabb Switch fails to build on a distro/version, or performs badly with a distro's libc, or you want to run some other random software as part of your test case and not require the user to install it separately.\n(3) test script simplification seems to be working out well? In particular the \"separation of concerns\" of test case vs. test environment has simplified the scripts in snabbnfv/test_env because they don't have to manually build/install QEMU, guest image, etc. This is only possible because we now have separate means to run with a well-defined test environment (e.g. docker container).\n(4) is still a problem. We want our test infrastructure to be \"parameterized\" with many different software versions e.g. hypervisor software (QEMU, Libvirt, OpenStack, etc) and guest kernel (e.g. different Linux versions with different virtio_net behavior) and guest userspace (e.g. different DPDK versions that have to be compiled to match both the running kernel and running CPU). Docker does not seem to solve this problem especially well: its \"build\" step is annoyingly restricted, Dockerfiles don't compose in any nice way, and generally it does not provide a solution to this problem (only a sandbox where we can fight with distro-specific toolchains like debootstrap). This is still a hard problem.\n(5) making it easy to deploy CI to a new Snabb Lab server. Have we solved this? This would be cool to actually do. I suppose that the CI could run either via Docker, NixOS, or simply a shell script. I suppose that step one would be to run a CI that only tests on the host environment that it finds (e.g. QEMU found in $PATH) and second step would be to parameterize it to test multiple environments (once we solve that problem to our satisfaction). Or?\n(6) making the CI deployable to third party environments e.g. somebody's private GitLab/Jenkins setup or OpenStack 3rd-party CI, etc. Could be that we can keep it simple here and just use solution (1) i.e. the CI would execute a one-liner that runs the tests in a Docker container. If that were too hard we could alternatively use a dedicated machine with the same software as the Snabb Lab and have CI run tests in that via SSH.\n(7) making the Snabb Lab comfy for day-to-day development. Docker seems to not be the solution to this actually because it's just a bit awkward to use a docker container as your login shell and if you complain then people will say \"don't use Docker like that\". Could be that we should just make the default NixOS configuration more friendly e.g. include all editors, compilers, tools, optimal QEMU version, etc that people want to work with? People are also free to use their own favorite container- or VM- based tools to make themselves at home, if they happen to already have one.\n. Obstacle to merge: CI is failing on this PR. Can resolve?\n. re: image files, I am fine with leaving them out of scope for now and expecting to find them in a directory.\nI am skeptical of Dropbox. The client software is awkward and it has no revision control to speak of. If we would keep an extra backup of the image files and track their SHA hashes with Git (keeping old images available to repeat tests, writing commit messages to describe new images, etc) then we would just be reinventing git-annex. However: let us argue about this later :).\n. @eugeneia OK. Please just tell me what actions I should take (e.g. merge this PR onto next) and when :).\n. I don't understand what you mean. The next branch is ready for release from my perspective. Do you want to include this PR in the v2015.11 release? If so let me know and I will merge.\nI am not sure what you mean to take care of the SnabbBot merge. I would like to avoid e.g. ad-hoc merges onto master that are not part of releases. Is anything like that needed?\nbtw please release with a nice commit message like in 6c0545d that shows the version number on the first line. This is very handy to understand what release you are on from one of git's one-line summaries.\n. Good question. Have to think.\nMerging the new master commits (git pull master) would be preferred to rebase. Could even be that the CI should test not the exact submitted change but the master branch with the PR merged? Could be interesting to see what other CIs do e.g. the Rust one.\n. Pushed to next.\n. This is an interesting topic :-).\nThere are several different things the CI could tell us:\n1. Does the PR branch work?\n2. Does the target branch (e.g. master) work with the PR merged?\n3. Does an integration branch (e.g. next) work with the PR merged?\nThese are actually all interesting questions. (1) tells us whether the change works in isolation. (2) tells us whether the author needs to update the PR by pulling new changes from the target branch and resolving a conflict of some kind. (3) tells us whether maintainers of integration branches (e.g. me with next) need to do some manual work before merging.\nNaive question: should we test all scenarios? If not why not?\n. Sounds reasonable to me!\n. Good observation that (1) and (3) can both be handled by people testing locally. This is safe because (2) will prevent a badly tested change from landing on master.\n. > (3) is even done \u201cautomatically\u201d by the CI since the \u201cnext PR\u201d will trigger a test of master with next merged.\nIndeed: though this happens after I have pushed to next. So if somebody is running next, to be helpful and give more user testing before we make a release, then they are not protected from getting broken changes that don't pass CI. However with the test suite being easy to run it seems reasonable for me to be responsible for testing before pushing instead of relying on SnabbBot.\n. @eugeneia Can we just ignore the \"PR base branch\" and use the latest release (master) instead?\nI am imagining that SnabbBot could prepare the code for test like this:\ngit checkout master\ngit merge pr/xxx\nor alternatively/equivalently:\ngit checkout pr/xxx\ngit rebase master\nThen contributors would only need to manually merge master onto their PR branch in cases where this failed with a conflict.\n. This seems important to me. Every release does invalidate the CI results if the CI results are supposed to say \"will master work after this change is merged\". If we don't handle this automatically then people will have to manually update their PRs on every merge to master (including e.g. quick bugfix releases).\nPerhaps we can find an easy formulation for this logic? SnabbBot just needs to know whether a given commit-id is already tested, where that commit-id is not literally the one on the PR branch but rather the commit-id that it gets after doing a \"git merge\"?\n. Just tacking another note onto this (closed/merged) PR in order to get it out of my head:\nNix has a built-in solution to the \"git-annex\" problem i.e. for keeping, sharing, and backing up assets like VM images that are difficult to replace (hard to download or time consuming to build). We would define Nix expressions to create the assets including the location and sha256 hash of all dependencies. These dependencies could be software (e.g. kernel, iperf, etc) or simply binaries on downloaded from some URL (e.g. specific version of a Juniper vMX image). In either case Nix would download them and store the resulting assets in its binary store. Then there are commands for synchronizing the stores e.g. to send copies to other machines, export them as tarballs, etc. So in principle we should be able to refer to assets based on their primary URL but automatically fetch them from a local mirror and also be able to make reliable backups.\nThere seems to be a lot to like about Nix. I suppose we will have to find out the hard way what there is to dislike :).\n. Generally git fetch in the submodule directory should pick up missed commits too? (Surely not the official solution.)\n. If submodules are a pain could consider a shell command to git clone instead.\n. Progress at least...\nThe packetblaster benchmark should score 14.8 (Mpps) with 64-byte packets if all is well. That would be more like +55%. I suspect the issue is that packetblaster is sensitive to the other end of the connection and we need something there that is actually up and quickly receiving packets -- but not certain about that.\n. ... or really should that be with 60 byte packets in the input file? (the other 4 bytes being the automatic/implicit ethernet CRC rather than part of the packetblaster config.)\n. Encouraging: this seems to be a working example running in userspace on Linux: https://github.com/pyrovski/powertools/blob/master/msr_pebs.c\n. Closing PR: I am not actively developing this branch.\n. Here is one idea for a compromise between simplicity and fanciness.\nSuppose we had a script hw ...command... that would reserve hardware resources (cores and NICs) for the duration of a command. The command would automatically execute with affinity to the right cores (implicit NUMA affinity if the cores are chosen from the same node) and the PCI addresses would be supplied in environment variables like $PCI0. If the hardware is busy then the script would wait until it is available.\nExample that reserves four NICs and three cores, all belonging to NUMA node 0:\n```\n!/bin/sh\nif [ $# == 0 ]; then\n  echo \"Usage: hw \"\n  echo\n  echo \"Run a command with hardware (NICs and CPU cores) exclusively reserved.\"\n  echo \"The command will automatically have affinity to the CPU cores.\"\n  echo \"The NICs will be supplied with NIC0..NICn environment vars.\"\n  exit 1\nfi\n(\n  flock -n 9 || exit 1\n  NIC0=0000:01:00.0 NIC1=0000:01:00.1 \\\n  NIC2=0000:02:00.0 NIC3=0000:02:00.1 \\\n  taskset -c 0,1,2 \\\n    \"$@\"\n) 9>/var/tmp/hwlock\n```\nExample usage:\n$ ./hw bash -c 'echo NIC0 = $NIC0; echo affinity:; taskset -p $$'                                                        \nNIC0 = 0000:01:00.0\naffinity:\npid 2590's current affinity mask: 7\nThe idea is that we would have several such scripts on each server that would be able to reserve different kinds of hardware. You would choose which script to use based on your hardware needs: do you want to use the hardware that is reserved for CI? do you want one pair of NICs? two pairs? four pairs? the whole machine?\nThe user running tests would then have to choose a server (e.g. grindelwald) and a hardware profile (e.g. script that reserves 2 NICs and 2 cores) but the rest would be automatic. The code they run could depend on already having CPU and NUMA affinity (unless deliberately testing on a whole machine or multinode hardware profile) and would always choose NICs based on env variables instead of magic numbers.\nFew more potential usages and extensions:\n1. Reserve resources for one hour of interactive testing: timeout 3600 hw bash -i.\n2. Scripts that reserve a small amount of resources could probe a few possibilities for availability (nonblocking flock).\n3. Scripts could log some usage information for calculating a \"load average\" i.e. summary of how busy each hardware profile of each server has been lately.\nJust an idea.\n. Could also be that Docker is not right for all the use cases I have in mind e.g. running your text editor and compiler etc. Then we would need an alternative solution for day-to-day hacking e.g. a NixOS cheat sheet.\nPlease feel free to rant here for and against Docker, NixOS, and other container-like software (chroot, LXD, etc) that could be relevant for day to day hacking on chur :).\n(I was surprised to discover that Docker hard codes its escape key to Control-P. That does not make it an attractive environment to run Emacs inside all day long.)\n. > The delayed commit method allows me to keep the code modular and still avoid a performance impact by moving the packet around three times in a row.\nMea culpa: the way we decided to update lib.protocol for straightline clearly turned out to be less than practical.\nI have an idea for an alternative to the delayed commit that might solve a wider class of problems including efficiently adding/removing the virtio-net header in #645. Observation: the operations causing the copies in all of these cases are packet.shiftleft() and packet.shiftright(). How about if we made a modest extension to struct packet so that those operations could be implemented without a copy?\nSketch code:\n```\nstruct packet {\n  char buffer[10240+512];  // buffer where packet data can be stored\n  char *data;              // pointer to start of packet within the buffer\n  int length;\n}\nfunction packet.allocate (p)\n   ...\n   p.data = p.buffer + 256 -- new packets get 256 bytes of \"headroom\" before the data\n   return p\nend\nfunction packet.shiftright (p, n)\n   p.data = p.data - n\n   assert(p.data >= p.buffer)\nend\nfunction packet.shiftleft (p, n)\n   p.data = p.data + n\n   assert(p.data + p.length <= ffi.sizeof(p.buffer))\nend\n```\nThis would have a few implications:\n1. Packet shift operations would become cheap constant-time operations. That would seem to make life easier for datagram and virtio work.\n2. Packet data would not have a fixed alignment in memory. This may be okay for SIMD: newer CPUs seem to care less about alignment and they are the ones we need to optimize for. Have to make sure it would also be okay for other purposes like DMA. (I believe the 82599 is happy with any even-numbered address i.e. 2-byte alignment and that should be quite okay.)\n3. The struct packet would grow and be less minimal. This seems justified if it will simplify the code that is working with packets. (I also still daydream about the GC for packets in #572 that would require a couple of additional fields in the struct too.)\nMake any sense?\n. (Haven't read the exciting code yet - more feedback to follow :-))\nGreat idea to make benchmarks for regression testing!\nI see a lot of potential for interesting work on traffic matching/dispatching mechanisms over time and it will be excellent to have enough tests for people to evaluate whether new mechiansms are suitable to succeed/replace existing ones (without breaking important performance characteristics of somebody's product).\nFrom a software maintenance perspective I think the most important purpose of the benchmarks is to compare the relative performance of two software versions and to cover enough scenarios to represent the realistic performance characteristics that people care about in programs that are using the apps. This would make it possible to rewrite dispatching code over time and be confident when new versions are suitable/unsuitable to merge onto master. (That would also make the optimization work accessible to people who are masters of CPUs and data structures but don't have a background in networking and want to treat the benchmark setup as a black box that takes in code and outputs a score.)\n. This code is super interesting! I have still not been all the way through it but you have implemented multiple ideas here that I have been imagining somebody would tackle one day:\n1. To write an efficient lookup table for an application that is lookup-intensive. This seems extremely valuable as a reference benchmark. Then people can experiment with on the one hand optimizations to further increase performance and on the other hand trying to cut code while keeping performance steady. Fun times ahead! I am sure that our table lookup use cases will be growing over time :).\n2. The programming technique of using FFI calls to hide small inner loops from the JIT so that they don't impact trace behavior. I have also been anticipating that this will be important if you have a loop in a trace and then want to introduce (say) a small hash code calculation that would need to iterate. (In a sense we are already doing this with memcpy.)\n3. The programming technique of processing packets in two steps: first sorting them into a queue (flood, unicast, drop) and then using branch-free code to process each queue. I can really imagine LuaJIT being happy with the branch-free code and perhaps that benefits the CPU itself too. (Hard to know when you are contorting yourself to satisfy a cruel JIT compiler vs fundamentally matching your problem to modern CPU architecture! ref lukego/blog#5).\nVery cool.\nThis also makes me think it would be really valuable to have a comprehensive benchmark in the style of SPECint that benchmarks the entire code base and says whether it is improving or degrading. The CI benchmarks are moving in this direction but I wonder if we need to make it easier to grow them somehow. Just thinking how wonderful it would be if the benchmarks were sufficient that people could independently work on problems like \"Can I make this code run faster?\" or \"Can I halve the size of this module without reducing performance?\" without actually having to understand the applications themselves (treat them as black boxes that either speed up or slow down as a result of changes to the source code).\nLooking forward to digging a bit deeper into the implementation to see what more I can learn from your investigations :)\n. Merged onto next.\nHow are you measuring performance of this code today? Can we make a performance regression test out of that somehow?\n. @alexandergall Ad-hock code sounds fine. The purpose of this test case will be to estimate whether a new change is going to make you will be happy/sad/indifferent. So it only needs to test whatever you are already testing.\nI wonder if this could be a nice basic template for making regression tests:\nperf stat -e cycles ./snabb snsh -e 'require(\"mymodule\").selfbenchmark()'\nwhere selfbenchmark() does a fixed amount of work (e.g. 100M hashtable operations) and then returns.\nI suppose that time would work instead of perf stat but that might be a little less dependable if \"Turbo Boost\" is enabled on the server (CPU clock speed will change based on activity on other cores).\n. aside:\nI would love to make it as easy as possible to write performance regression tests.\nOne idea is to support a selfbenchmark() method that runs performance regression unit tests in the same streamlined way as selftest(). Then we could have a make benchmark target that outputs a CSV file with all the unit benchmarks for a software version. This could be used for testing and tracking the performance of individual subsystems over time.\nOne other idea would be to stop throwing out new ideas when we already have a working mechanism and only need to use it more... :-)\n. Great to revisit the topic of naming!\nCouple of ideas:\nDo we want to allow versions like 1.0-alpha2? The regex in this PR would disallow the alphabetic characters and the hyphen. Could be worth considering something more accepting like -[0-9].*?\nDoes our current method of program invocation making naming awkward and could we fix that? Suppose you are deciding whether to call your program foo or snabb-foo. On the one hand snabb foo is a neat invocation but foo may clash with something else on your computer. On the other hand snabb snabb-foo is a bit too wordy while snabb-foo is pretty okay. Could it be worth adding logic so that if argv[0] is snabb-foo then we treat the program name as foo? That way every program could be neatly invoked either via the snabb name or via a dedicated name.\nThinking of this last point in terms of our existing programs I would be happy to install packetblaster as /usr/bin/packetblaster but I would prefer to install the gc cleanup program as /usr/bin/snabb-gc than simply /usr/bin/gc.\nI am not sure if this makes things too complicated and creates too many choices though. The requirements are not completely straight in my head. One aspect that I think is really important is that each snabb program should be able to be distributed independently i.e. no requirement that your snabbnfv and packetblaster and other programs are built from the same source.\n. This looks excellent to me. Especially appreciate the test cases that really spell out the intended behavior.\nThere is a bug in one usage:\n$ ./snabb\n...\ncore/main.lua:82: variable 'statis' is not declared\nstack traceback:\n    core/main.lua:126: in function <core/main.lua:124>\n    [C]: in function 'error'\n    lib/lua/strict.lua:37: in function '__index'\n    core/main.lua:82: in function 'usage'\n    core/main.lua:45: in function <core/main.lua:32>\n    [C]: in function 'xpcall'\n    core/main.lua:155: in main chunk\n    [C]: at 0x0044f770\n    [C]: in function 'pcall'\n    core/startup.lua:1: in main chunk\n    [C]: in function 'require'\n    [string \"require \"core.startup\"\"]:1: in main chunk\n. Great initiative, Diego! We definitely need a shorter way to refer to PCI devices.\nZooming out to the overall idea rather than the code, I see two alternative ways to implement this:\n1. Using Lua patterns to match PCI addresses.\n2. Exactly defining a short/canonical form for PCI addresses.\nLua patterns are tempting. We are already using them in packetblaster too. Packetblaster accepts a pattern to name multiple devices, so you can write 0[123]:00.[01] to potentially match all of 0000:01:00.0 0000:01:00.1 0000:02:00.0 0000:02:00.1 0000:03:00.0 0000:03:00.1. Or you could simply write . to match all available devices. The downside I see is that this is a leaky abstraction (exposing Lua quirks to end-users of Snabb based applications) and it can be surprising too e.g. that . will be treated as a wildcard rather than a literal period (awkward since PCI addresses have a significant period character in them).\nI feel like the principle of least surprise takes precedence here. I have come to think of the packetblaster command-line as a misfeature.\nThe alternative is to define (or find and reuse) a canonical shortening of PCI addresses. Like the way IPv4 and IPv6 addresses have lang/short forms via pton and ntop.\nHere is an idea for some isomorphic address transformations:\n1 <-> 01 <-> 01.0 <-> 01:00.0 <-> 0000:01:00.0\nand the canonical representation could be the full address but omitting any leading 0000:.\nWhat do you think? Would that work for lwAFTR too?\n. @kbara The full syntax is DOMAIN:BUS:DEVICE.FUNCTION. Here is one documented convention.\nOn reflection I also like the idea of having a canonical form that simply omits any leading 0000:. So we would write 01:00.0 and expect to see PCI addresses printed in that same shortened format. If the domain is non-zero then the long format would be used. End of story.\n. I agree with @kbara. @dpino, I humbly apologise for suggesting the more complex scheme. The extra code and explanations are really not justified by the convenience (which in itself might even lead to confusing inconsistency in usage).\n. Key added via SnabbCo/snabblab-nixos#1.\n. This is fantastic stuff!\nHaving a Virtio-net driver is extremely valuable and will make a lot of VM-based deployment scenarios easy.\nI would like to please see more documentation e.g. a README.md that explains the exact behavior of the app and includes references to relevant specifications (e.g. the new Virtio-net standard from OASIS).\n. Generally also: I would like to keep core modules like packet fairly minimal. In the past the packet module in particular had become a bit of a kitchen sink for utility functions and it was very satisfying the strip this away as part of the \"straightline\" refactoring. I think that PRs adding code to core modules like packet and link need to be accompanied by a strong rationale for factoring the code in that way.\nIn the code on this PR I can imagine a case being made for packet.physical() being a fundamental thing that we want but I am not so sure about adding packet.dump() when we already have a seemingly equivalent lib.hexdump().\n. This looks it is is going to be a really popular feature! Lots of people interested in reviewing the code :-)\nRegarding rebasing and respinning: I would suggest that the optimal way to do that would be to rebase onto a new branch called e.g. virtio_net_v2 and submit a new PR from that (referencing this one).\nThe alternative would be to force-push a rebase onto this branch but that would make a mess of this discussion on Github.\nSince there is so much interest in this feature it would seem especially valuable to be verbose in documentation, comments, commit messages, and PR text so that everybody gets a common understanding.\n. @dpino Yes the benchmarking setup I mean is to have snabbnfv on the hypervisor side with a setup like the one described in the Snabb NFV Getting Started Guide.\nThis setup with kernel-bypass on the hypervisor should be almost identical from the guest perspective. The difference is within QEMU that it will use a unix domain socket to setup vring memory mappings and interrupts (rather than /dev/vhost-net towards the kernel). You should still have a Virtio-net PCI device available in the guest and it should behave in the same way because this is implemented with the same code in QEMU in both cases. The difference should just be that other software is processing the descriptors you put into the vrings because the memory mapping is exported to a Snabb Switch process instead of the kernel.\nI'd recommend checking lspci to see if you have a Virtio-net PCI device. I am not sure exactly what presence/absence in /sys/class/net means (may say more about the guest kernel than the guest hardware?)\n. I would reuse the performance test we already have and replace DPDK in the guest with Snabb Switch. Check out Max's new CI infra!\n. > It definitely still is an expensive operation\nFor what value of \"it\" though?\nI do realize that calling ffi.copy() is expensive. I don't think it follows that copying is an expensive operation though. Sounds more like there is a bug in our memcpy that we are spending a lot of effort to work around and should fix instead. (This would benefit the reworked lib.protocol too since that is still doing a move at the end.)\nOne reference data point is #603 where I ported some app code to assembler and see an average of 4.6 cycles per processing step (one of which includes a 64-byte copy to duplicate a packet). This code is a bit artificial but I don't immediately see why our normal packet copies can't be in that performance ballpark?\n. (Me talking too much and not giving other people much room to chime in... but hard to resist...)\nI wonder what hypotheses we have for why copies take surprisingly much CPU time today and how we could test them?\nHere are a few hypotheses:\n1. The actual data movement instructions take a long time to execute. (Could try to reproduce with hand-written assembler routine?)\n2. The GLIBC memcpy code itself is expensive e.g. incurs branch misses when checking edge cases. (Could benchmark it with pure C code and/or profile it with perf?)\n3. The ffi.copy causes a compiler load/store barrier that forces the JIT to generate a bunch of extra instructions to move data from registers into memory and back again. (Could read LuaJIT source or inspect generated code?)\n4. The ffi.copy makes a C function call with an expensive calling convention that forces the JIT to generate a bunch of extra instructions to save registers on the stack and restore them. Especially SIMD registers that LuaJIT does use and that are all callee-save. (Could inspect the generated code to see?)\n5. Some kind of cache effect. Could be that the packet data is not in L2 cache and we have to take an L3 access before the copy instructions can be retired? (Could profile with pmu module and look for L2 cache misses?)\nThat's everything that I can think of right now. I suppose it has to be one or more of those reasons -- or what else could it be?\n. @kbara Yeah, I understand your perspective. I acknowledge there could be a good reason to avoid copies, I just don't think that we have identified it yet and so I still hope it does not exist.\nI also see potential for us to solve this problem in ways that are not available to others. For example we could round up the size of our copies to a 64-byte boundary because we do have ample space available in the buffer. Having a small collection of special-case copy routines like \"Copy left in 64 byte blocks\" could potentially be a neater solution than avoiding copies all together.\nGoing down this rabbit hole could potentially pay other dividends too. For example if the performance cost turned out to be due to this kind of alignment issue then it could also be affecting our SIMD checksum routines. Those could also be \"rounded up\" to 64-byte blocks just by using one vmovdqu instruction to zero the unused bytes in the last word of data. (Tony Rogvall suggested that idea to me once, he is the guy who wrote the original SSE checksum routine that we are using.)\n. > type X of copy can really go fast, type X' which is similar is quite a bit slower\n@kbara That is an interesting observation and it reminds me of when I benchmarked the rte_memcpy routine from DPDK.\nI saw a major speedup in our standard NFV benchmark when I tested with C.rte_memcpy() instead of ffi.copy() for moving data on virtio on a workload of 256-byte packets. However, the speedup seemed to disappear when I tested a workload that had average size of 256 bytes instead of each packet being exactly that size.\nThis seemed undesirable to me: I don't want to have better performance in benchmarks than in real life. However, perhaps with a trick like rounding up the copy size to a 16-byte boundary we could have that performance on all workloads? (I admit I didn't dig too deeply into this at the time but noted it as something to think about later.)\n. One more thought:\nffi.copy() is an intrinsic in LuaJIT that can be compiled specially (eg for structs or constant-size objects) and only compiles into a call to memcpy if no such case matches. Potentially we could add more cases there, even with an extra 'mode' argument, if we wanted control over things like barriers and calling conventions and roundings-up.\n. Two more thoughts that occurred, one theoretical and one more practical:\nTheoretical: Copying 550 byte packets should in the best possible case take 18 cycles with AVX2 (32 bytes / cycle) or 9 cycles on a future CPU with AVX512 and expanded cache bandwidth. If one would do a triple-copy to incrementally add headers then that would take 54 cycles on AVX2 or 27 cycles on AVX512. Hypothetically if you would want to process 10 Mpps with one 2.0 GHz core then your total budget would be 200 cycles per packet. The triple-copy would then burn 27% of the total processing budget with AVX2 and 13.5% on AVX512. That would definitely be a hotspot worth optimizing: especially on AVX2 which is the best we will have for a couple of years (and that's not even thinking about older CPUs).\nThis seems like a reasonable case against the idea treating packet copies as \"too cheap to care about\" at least for a certain class of interesting applications especially in the short term.\nThis also seems like a strong motivation for the lib.protocol overhaul (#637) to make fewer copies. Sorry, it was my bright idea of \"let's pretend copies are free and see what happens\" and it seems the effect is to limit uptake of lib.protocol which is unfortunate and needs to be fixed.\nI said that I have a practical idea too: Let me write that on the PR for #637 instead though.\n. Good question about cache behaviour!\nI don't know the answers with absolute certainty. I can explain my mental model. I would love to do some experiments with the PMU to verify/falsify/correct this.\nLet us refer to my favourite diagram in the whole world: I/O architecture of a dual-processor Xeon server:\n\nPackets are being DMA'd over the PCIe links (red arrows at the bottom). Each PCI link is connected between one NIC and one CPU (NUMA node). The hardware feature that performs the DMA is Intel DDIO. DDIO transfers the data directly between the NIC and the L3 cache of the specific processor that the NIC is attached to.\nThis means that a process running on the same CPU as the NIC will access the packet data via the local L3 cache while a process running on the other CPU will access packet data remotely via the QPI link. (That long trip across the QPI link is where the \"bad NUMA penalty\" comes from AFAIK.)\n(Reading in the Intel DDIO FAQ that I linked above) it seems that if the working set can be kept within L3 cache then the packet data need never be written out to DRAM. I also read there that Intel are planning to improve DDIO so that you don't have to worry about NUMA affinity. That would sure be nice: I bet it happens right after everybody is done expensively making all of their software NUMA-aware :).\n. @andywingo We have seen substantial variations in throughput on snabbnfv by tweaking how many packets the intel10g app introduces in each breath. (Diego has looked into this too.) 100 packets seems pretty optimal and 200 is measurably slower. My theory is that we are looking for the sweet spot between too many context switches (too few packets per breath) vs the workload spilling out of L2 cache during processing (too many packets per breath). Could be an L3 cache issue instead however.\nThe ultimate solution to this kind of issue is supposed to be #616 which will simply report the number of L1/L2/L3 cache misses per packet for each app so that we don't have to guess. I have a few too many things competing for my attention at the moment to really finish that.\n. > I think NUMA affinity will be with us for a long time to come.\nYeah. NUMA will also surely affect a lot of different applications in different ways.\nOn Snabb Switch we are careful about NUMA because in practice we see ~1/3 loss of performance when processing packets from a \"remote\" NIC. I currently attribute this to additional latency when loading packet data from remote L3 cache vs local L3 cache. I am not really sure that this assessment is correct or complete though.\nIf that is the whole story though then Snabb Switch may not have to worry about NUMA affinity in the future. Intel's DDIO FAQ (linked above) says in Q6 that they are going to support DDIO DMA into the remote L3 cache in future processors. Then it should not matter which socket the NIC is wired to if the only performance issue is/was L3 cache latency.\nLife would become simpler and simpler if the CPUs provide more robust performance and free us from details like NUMA affinity, SIMD alignment, and so on.\nI am not sure whether we will ever really need to worry about bandwidth on any of these links. Currently it seems to me like HPC workloads are an order of magnitude heavier than networking ones and so we have a free ride there.\n. Just noodling around: I sketched some assembler code for how a special-cased memcpy for packet.shiftleft() might look. This is not compiled or tested let alone benchmarked. Here is it anyway:\n``` lua\n-- API: void packet_copy_backward_64(void dst, void src, int size):\n--\n--   Copy data backward in memory in 64-byte blocks.\n--   Size will be rounded up to a multiple of 64.\n--   Intended for packet-sized data.\n-- \n-- The block size of 64 bytes is intended to be reasonable for all the\n-- current/next/previous generations x86 SIMD (AVX, AVX2, AVX512).\n-- Haswell implementation based on AVX2 with 32-byte registers:\nlocal function asm_haswell_packet_copy_backward_64 (Dst)\n   |  -- Setup registers like this:\n   |  --   rcx = 0        (counter)\n   |  --   rdx = bytes    (limit for counter)\n   |  --   rdi = dst      (base destination address\n   |  --   rsi = src      (base source address)\n   |  -- (x86-64 calling convention does most of the work in this case.)\n   |  xor rcx, rcx\n   |\n   |  -- 64-byte copy loop:\n   |1:\n   |  -- copy first 32 bytes\n   |  vmovdqu ymm0, [rsi+rcx]\n   |  vmovdqu [rdi+rcx], ymm0\n   |  -- copy second 32 bytes\n   |  vmovdqu ymm0, [rsi+rcx+32]\n   |  vmovdqu [rdi+rcx+32], ymm0\n   |  -- test loop exit condition\n   |  add rcx, 64\n   |  cmp rcx, rdx\n   |  jb <1\n   |  ret\nend\n```\nQuestion: How should we evaluate a function like this to decide whether it is good/bad/neutral?\n. @gonzopancho Thanks for sharing! I wonder how they decided to use this instead of memcpy?\nGotta say that I find it really hard to guess the effectiveness of this code. So many variables that depend on compiler and CPU behavior that may be obvious to some people but not to me:\n1. If that compiles into 8-byte load/store operations (movq) then what throughput/cycle will Haswell achieve? (Just 8 bytes/cycle or will the processor combine the operations and get full 32 bytes/cycle?)\n2. Can GCC vectorize those moves as written? How about if you changed the inner loop to memcpy(dst, src, 64) that it will treat as a builtin/intrinsic (small constant size copy)? If so would you expect people to compile with -mnative to enable optimizations specific to their CPU microarchitecture (e.g. Haswell)?\n3. How much can you depend on compiler behavior? (Got to worry about consistent results for various versions of GCC, CLANG, and ICC?)\nI know that an expert C system programmer will probably know the answer to all of these questions but it makes my head hurt :). LuaJIT optimization also makes my head hurt of course...\n. Just another passing thought re: testing:\nFor test purposes it would be really interesting to have some standardized distributions of packet sizes for the various averages that we want to test.\nFor example in practice I would say that 500 bytes mean packet size is quite representative of consumer internet traffic, but it is likely to be a bimodal distribution with many large packets (1500 byte carrying e.g. HTTP/TCP payload) and many small packets (ACKs, DNS req/res, etc). This could be significant. Looking at the excerpted netmap code they use a completely different memcpy routine for packets over 1024 bytes and a naive benchmark setup could miss that code path completely (and such separate code paths can also exist internally in e.g. GLIBC memcpy).\nIn the past working as a vendor we have always had some test vectors based on real traffic from the networks we are targeting that we have obtained with permission. I am sure there are research projects that have taken representative traces and anonymized them and made them available to share freely. Anybody have a reference?\n. @gonzopancho Thank you for taking the time to follow different projects and share ideas between them. I am really glad that so many projects are able to explore different kinds of networking software designs (Linux, BSD, netmap, DPDK, Snabb Switch, ...). This seems much more productive than everybody doing things the same way, at least if we are able to get the benefit of each others' experiments and experiences.\nI reckon we need to expand our performance regression test suite to be able to resolve all questions about the relative merits of this sort of code. Then we merge all changes that simplify the code while preserving performance and we merge only those optimizations that demonstrably improve performance without excessive complexity. I have some new ideas on this that I will write up separately.\n. @gonzopancho Thanks for the links! cc @eugeneia.\nWe are also investing a bunch of effort in end-to-end performance tests on our CI e.g. booting VMs and driving traffic between them and a physical network. These repeatable benchmarks have allowed us to retire a boatload of complex optimizations already. I love those kind of activities :).\n\nI think it's likely that the finding will be that the copy is cheap, but its effect on the cache (pollution of same) may be quite expensive.\n\nThis would be a valuable and actionable result. The copies that are controversial in our code base right now have their source and destination addresses in the same cache lines. They are moving data in-place while modifying packets (e.g. adding and removing encapsulations).\nThis kind of copy can simplify our code base, in particular allowing us to represent packets in a canonical form, but they can also create complexity elsewhere if people feel the need to code around these operations because they are not fast enough. If that happens then we need to either optimize them more or create a new mechanism e.g. adding an indirection so that the start address of a packet can be modified without touching the data.\n. Interesting. Packetblaster is segfaulting on the CI. Anybody know why?\n. Interesting! Thanks for digging. Have to get to the bottom of that.\n. Agree. Yes, please do.\n. Passing thought:\nI wonder if we could test many/all apps for this at the same time? Seems like any app that suffers from this class of error - leak of a scarce resource - could be tested by a loop that cycles back and forth between engine.configure() with an empty app network vs with an instance of the app under test.\nThis would seem to have caught the resource leaks in the intel10g driver that we once upon a time wrote tests for and fixed, and likewise the same resource likes in the draft intel1g driver that has no test coverage for this today.\nT'would be quite awesome to have an app \"stress tester\" based on e.g. fuzzing or quickcheck.\n. ... yet another thought:\nThe raw socket code should be rewritten in Lua based on ljsyscall. This would be shorter code and simpler because it doesn't cross programming language barriers. (It would also make the file descriptors automatically close when garbage collected - though I don't think we would want to depend on that.)\n. @alexandergall do you have strong feelings on this patch (affecting vpws.lua)?\n. Great hacking, @dpino! I love PRs that reduce code size while preserving functionality.\nOne more opportunity to simplify if you fancy: the code in dev.lua could be merged straight into raw.lua. The separation between I/O primitives and apps does not seem very important these days. That was the way we wrote code back when the \"app network\" concept was new and experimental but these days it seems to be well established. (That is one perspective anyway.)\n. This is a great initiative and the new ljsyscall code looks very crisp and clean compared with having to call out to C.\nSorry to be so slow on a detailed review. I have spotted a couple of small bugs that are worthy of fixing before merge, details below.\n. Thanks Diego! Sorry to take so long merging these recent excellent PRs. Merged to next now.\nSide-topic: The intention of our git workflow is to never force-push to a branch that is the source of a pull request. This should not ever be necessary: if it seems to be then please say why so we can improve the workflow docs :-).\nFor the case where you want to incorporate new changes that have landed on master into a PR branch then the solution should be git merge master to bring them in with a merge commit. I know that many people hate merge commits but the Torvalds school of thought says it is valuable history. (If you don't actually care about the latest changes on master then you should also be able to ignore them and SnabbBot will be smart enough to automatically (re)test the right version i.e. latest master with the PR branch merged.)\nThe main reason I want to avoid force commits on PR branches is that it will make the master branch into a central bottleneck. If force pushes are allowed then no sane person will merge PRs into their own branches and everybody will wait for the \"real\" version to land on master before merging fixes and features. This is backwards for our distributed workflow: the master branch should be the most conservative one trailing behind the other branches that are eagerly incorporating and testing early versions of PRs.\n. The idea that makes most immediate sense to me would be for the basic behavior of apps to be (1) and for fancy algorithms like (3) to be implemented as their own apps. Flow control can easily have unintended consequences: better to tune the sizes of the buffers and the amount of work that is accepted in each breath so that packets are dropped when capacity is exceeded or links are out of service.\nIf we adopted that position we would need to eliminate some flow control from our codebase i.e. cases where an app checks its output capability before reading packets off its input link. Instead the transmit function would detect lack of capacity and drop.\nReasonable people may have a different point of view :).\n. Have been pondering this. My theory is that the simplest and neatest solution is to never apply flow control within the app network. That is: apps will always receive and process all of their available packets and never check whether the output has capacity beforehand. During congestion packets will be dropped at the bottleneck point.\nTo put this into practice we would need to do a couple of things:\n1. Put some \"burst\" limiting on apps that bring packets into the app network. For example limit each app to introducing 100 packets per breath. (The intel10g app is already doing this.)\n2. Increase the size of our buffers so that they will only overflow due to actual congestion (packet arrival rate exceeding packet processing rate) and not due to burstiness (e.g. Join app combining several input links that could reasonably overflow its output link or the next app in a single breath). In particular we would need to consider increasing the size of struct link and NIC transmit descriptor rings to avoid premature overflows.\nThe benefit of this activity would be more predictable behavior under overload and less code (hallelujah) since apps would always \"blindly\" receive and transmit and depend on the transmit function to drop packets that are over capacity.\n. @plajjan Good input! This is indeed a subtle topic. I will make the case for a fully asynchronous design but I am willing to be convinced otherwise too.\nHOLB\nYour VlanMux example does seem like a case where you will be burned by head of line blocking as a consequence of the Tee app being blocking (synchronous / using flow control / using back-pressure). If one of the outputs is full then no input will be processed and that will also block the outputs that do have capacity.\nI see this as a big problem. I think that providing both blocking and non-blocking apps makes it unreasonably hard for users to predict the behavior of their app networks. On the whole it would be simpler and better for apps to be exclusively non-blocking even if this requires us to write test rigs in a different style. cc @wingo.\nQoS\nQuality of Service is an interesting example. If we had a QoS app then it would need to do two things: detect how much capacity is available and then use a deliberate policy to choose which packets to drop. Backpressure would be one way to detect overload that might be neat in simple setups (QoS->NIC) but still problematic in complex ones (QoS->Tee->NIC). I would suggest that a simple QoS app could use a statically configured capacity (like RateLimiter) and a fancy QoS app might dynamically measure capacity e.g. via a shm object. In all cases the purpose of the QoS app would be to enforce a policy without buffering and causing delay.\nRealistic? Are there more examples that make the case for synchronous/blocking/backpressure model?\n. > What reasons are there for a link being full? What's the queue depth of a link? What is the normal size of a breath?\nMy suggestion is that the engine regulates the rate of incoming packets to around 100 per breath, links can hold up to 1,000 packets (10:1 safety margin), and links are expected to overflow under a diverse range of exceptional circumstances but not in normal operation.\nHere are some examples of where a link might fill up and cause packets to drop:\n- Receiving app is out of service and not processing packets. For example it could have crashed with an exception and be waiting for a scheduled restart by the engine or it could be a NIC whose link is down.\n- Receiving app is running slowly e.g. the link is across processes and the receiving process cannot keep up with the workload.\n- Extreme load of some kind e.g. very many input sources all converging on the same link and the chosen buffer size being insufficient.\nI hope that this behavior would be predictable and graceful in overload situations. That is: use the capacity you have, drop the excess without introducing delay, make the situation observable (via counters) to the people responsible for capacity planning in the network.\nTo be a little more vivid: imagine you are an ISP experiencing extreme load e.g. your customers are all streaming the world cup in HD for the first time, you are being DDoS'd, and a data center outage has wiped out half of your capacity. This will push all of your network elements beyond their specifications. How should they cope? I would hope that they focus on using the capacity they have as efficiently as possible and providing quantitative information about how load is impacting their operation. This is not the time you want to be triggering bugs and edge-cases in exotic code paths.\n@eugeneia I am not immediately sure how to take a step towards common ground. I agree with your sentiment but not the details. If a system is overloaded then the last thing I want it to do is report assertion failures and restart. Mixing both lossy+lossless is complex, like eager+lazy evaluation in a programming language, and risks kicking this problem down the road until it surprise)s somebody in production. On the internet the default behavior to overload/congestion is to drop the excess as quickly as possible and then continue: for an app to have something clever to do (apply QoS rules; send a TCP Explicit Congestion Notification; send an ICMP Source Quench) will be the exception rather than the rule. Saying the current Snabb behavior here \"emerged naturally\" seems an overly generous way of saying that the person writing the code used the first idea that came to their mind. The asynchronous networking model is the one on the right side of history i.e. simple and predictable design that keeps on killing more complex alternatives (and their successors) when looking at how the internet has killed off competitors like traditional telco networks.\nCould be that my views are too extreme. I would be curious to know @alexandergall and @sleinen's perspectives on this issue.\n. Reflection: This Issue should perhaps have been a Pull Request from the beginning so that we would be talking about concrete code changes and could reach a conclusion with a merge or refusal to merge.\n. @plajjan Today there is a tiny little breathe() function that implements this logic:\n1. Call pull() on every app that defines this method. That is, every input source can bring in packets on each breath.\n2. Call push() on every app that received new traffic on an input link.\n3. Repeat step 2 until idle.\nThe original implementation - still mostly in place - is that on pull() each app tries to fill up its output links. This is max 256 packets per link. However, the intel_app has an artificial input limit to burst maximum 128 packets per breath. This seems empirically to be a sweet-spot and I think we should generalize this mechanism to control the total number of packets that we pull() into each breath. There are a few ways that could be implemented, I am not sure which is best.\n. Awesome !\n. @eugeneia I believe it is hardware dependent, yes. The network cards we are working with are pretty simple e.g. one ethernet controller chip with two ports attached to it. There could be others that e.g. combine multiple ethernet chips multiplexed behind a PCIe switching chip. Companies like [Silicom](http://silicom-usa.com/] make a lot of interesting cards in this direction. I don't know exactly how the PCIe addresses will end up looking but I wouldn't want to make too many assumptions based on the cards we have in the lab today.\n@dpino I am surprised that the virtio-net driver doesn't identify the device by a PCI address exactly like the Intel10G one? (Why does /sys/class/net come into the picture? Can discuss this on that PR branch though.)\n. Great info @nnikolaev-virtualopensystems, thanks! cc @domenkozar.\n. Update: OpenStack Liberty can now be automatically deployed with NixOS. The out-of-the-box networking uses the ML2 LinuxBridge driver. The next step is to add Snabb NFV and run our test suite.\nSee snabb-openstack-testing repo for details.\n. This fixes a bug so I am merging it.\nHowever, I disagree on the plan for handling this class of error. My position is that timers in apps should be implemented locally inside the app and not with a global timer whose lifetime is independent of the app instance.\nThis is the timer implementation that makes sense to me, potentially with some library support:\nlocal deadline\nfunction push ()\n   if deadline == nil or deadline <= engine.now() then\n      ... timer code ...\n      deadline = (deadline or engine.now()) + conf.interval\n   end\n   ... other push method app logic ...\nend\nThis is simple and local and has no risk of the timer callback outliving the app.\nOr?\n. @eugeneia Could you please update the issue text to say exactly what we know about the differences between the fast and slow test environments and how to run both tests? (In the discussion on the other issue I lose track a bit of exactly what code is being tested and what its results are.)\nStandard question whenever a mysterious 30% performance difference appears: Can the NUMA affinity have changed? (Could check e.g. in htop which node's cores are being used in each run of the benchmark.)\n. Can you tell me how to reproduce please?\nWe should not apply any patches to DPDK.\n9000 byte is what I believe won't work in the older DPDK.\n. Great detective work, Max!\nThe Docker workflow really does make it easy to reproduce tests! I ran this on chur and the results seem consistent when taking into account the slower CPU:\n```\n[luke@chur:~/git/snabbswitch/src]$ scripts/dock.sh \"(cd ..; make -j)\"\n...\n[luke@chur:~/git/snabbswitch/src]$ SNABB_TEST_IMAGE=eugeneia/snabb-nfv-test-legacy   SNABB_PCI_INTEL0=0000:01:00.0   SNABB_PCI_INTEL1=0000:01:00.1   scripts/dock.sh program/snabbnfv/packetblaster_bench.sh\n...\nProcessed 100.0 million packets in 23.90 seconds (6400003840 bytes; 2.14 Gbps)\nMade 891,403 breaths: 112.18 packets per breath; 26.81us per breath\nRate(Mpps): 4.184\n```\nNext I would really like to get in control of the patches. Specifically I would like to migrate over to testing with the latest releases of QEMU and DPDK without any patches applied. That is the intended target software environment. If the performance does not match expectations then we would dig in to look for the root cause and try to address this in the snabb code rather than with adding/reviving patches. Is this easy to setup?\nPatches can really take on a life of their own :-). I have a little bit of Nix envy here: Nix makes the exact versions/patches being tested completely transparent, even down to kernels and libc both on the host and inside the VMs, whereas Docker seems to make this very opaque. Dockerhub makes it extremely easy to reproduce the test environment but the summary page doesn't provide any insight into what code is actually in the container.\n(There is actually one QEMU patch that I do still recommend applying, \"f393aea Add G_IO_HUP handler for socket chardev\", but that should not be needed for the DPDK benchmark. It's only to allow the snabb process to restart without restarting QEMU.)\n. Awesome, thanks! I am really impressed with the Docker workflow you have cooked up, I think that the effort has already paid off in terms of time saved on manually reproducing test environments.\nHere is a braindump on the general theme of compatibility and performance with different software versions from our upstream Snabb Switch perspective:\nThe main thing is to make the latest upstream versions work and prevent them from breaking/regressing in the future. Then over time we build up a long trail of compatibility with consecutive versions. Supporting older and/or patched versions is less important unless specially required for some reason.\nWe want to take responsibility for making all the components work well together: Snabb, QEMU, DPDK/Linux guests, etc. If there is a problem then we need to find a solution even if that involves creating workarounds in Snabb Switch, making temporary patches to QEMU/DPDK, and working with upstream communities to merge fixes so that we can drop patches. In this sense it is better when a problem is caused by Snabb Switch, where it should be easy to fix, rather than a change in another project that we need to somehow deal with.\nOn performance issues like packet drops it can be that we need to take a \"holistic\" view of the relationships between all of the components rather than looking at one in isolation. For example if a new QEMU version leads to packet loss then this doesn't necessarily mean that there is a bug in QEMU but rather that the interactions between the components has changed. Particularly, QEMU is not involved in packet processing at all (this is done directly in shared memory between Snabb & DPDK) so it cannot be a direct processing bottleneck, but it is involved in negotiating the sizes and features of the shared memory rings and this can indirectly affect performance.\nLikewise even between the processing components it is hard to point a finger at one and say that it is the problem. If the DPDK guest is dropping packets then all we know is that it is receiving faster than it is transmitting (and the difference is the dropped packets). This could be due to local DPDK behavior, or subtle Virtio-net behavior that gives the receive ring larger capacity than the transmit ring, or subtle Snabb Switch behavior that bursts packets onto the receive ring faster than it takes them off the transmit ring, and so on.\nThe most practical method I know of for diagnosing holistic performance problems is to bisect i.e. to identify two software versions that are as close as possible but with one being \"good\" and one being \"bad\". In this example it seems like we have \"good\" behavior from the fully patched QEMU 2.1 and \"bad\" behavior from the unpatched QEMU 2.4.1 and the next problem is to isolate this more narrowly e.g. does the problem appear when we drop one of our QEMU patches (like the vring size increase) or did it appear in QEMU 2.2 or 2.3 or 2.4 and so on.\nI have a fantasty that our build infrastructure could make this easy to generate a test matrix e.g. by running commands like:\nfor snabb in v2015.08 v2015.09 v2015.10 v2015.11; do\n  for qemu in 2.1-snabb 2.1 2.2 2.3 2.4; do\n    for dpdk in 1.7-snabb 1.8 1.9 2.0 2.1; do\n      run-test > result-$snabb-$qemu-$dpdk.txt\n    done\n  done\ndone\nI am not sure if this is practical with the Docker-based test bootstrapping? If so that would be interesting!\nI believe that this is a built-in feature of Hydra where build/test parameters can be specified as e.g. enums and all combinations can be tested (seen in a blog post on Hydra). I am keen to dig into this on the side in case it would be a nice solution for us in the future.\n. @eugeneia Running the \"vanilla\" test I don't see traffic passing so it seems like we have a compatibility issue between Snabb master + QEMU 2.4.1 + DPDK 2.1. Do you agree? If so I can create a separate ticket for that.\n. Is there an easy way to confirm that based on the Docker workflow? This seems worth confirming before investing serious time in making a fix.\n. @eugeneia do you have a quick tip for how I could run the snabbnfv in benchmark mode (-B) and see the output/result from the docker container? (The docker containers seem opaque to me, I never remember how to see what is going on inside. Sorry, I think you have explained before :))\n. My goal is to run the full benchmark (packetblaster+qemu+snabb) but to control the snabbnfv traffic arguments and see its output. Does one of those commands let me do that? (How?)\n. Have been thinking about indirect descriptors a bit more. I am starting to think that they are an expensive feature that should be avoided.\nDirect descriptors only require the device/hypervisor/snabbnfv to make one L3 cache access to access packet payload.\nIndirect descriptors require two L3 cache accesses: first to resolve the address of the payload and second to actually access it. These L3 accesses are dependent on each other so the CPU won't be able to parallelize them (second can't start until the address is provided by the first).\nThis makes me think that indirect descriptors will generally have higher per-packet overhead than direct descriptors. This would be visible in the DPDK l2fwd benchmark (high packet rate) but not with Linux kernel VMs (low packet rate, bottleneck is checksum offload).\nCould be that this can be resolved with clever assembler code in #719 to access multiple packets in parallel but I am not sure. (Could also be that I am mis-analysing the situation entirely.)\nJust flagging to @nnikolaev @dpino @wingo that you may be able to expect better efficiency with direct descriptors rather than indirect ones but I am not sure yet. Ideas/input/data welcome. (My understanding is that indirect descriptors are mostly useful for working around the impractically small vring size that is hard-coded in QEMU but the existing CI benchmarks show that it is possible to achieve good performance even with such small vrings.)\n. See also the excellent test suite walkthrough that @eugeneia wrote. Down the bottom you see the much higher results when testing with older version of DPDK l2fwd that did not use indirect descriptors. (Hope we can improve the situation for both.)\n. This seems sensible but it does highlight a limitation of the packetblaster UI i.e. the lack of a simple method to describe the packet(s) that should be sent. I suppose this could be done either inside packetblaster or with a utility to generate the pcap files programatically rather than as opaque binaries. (See also Scapy).\nI suppose that improving packetblaster is a whole topic of its own though. One more feature that we are sorely lacking is to perform basic handshaking with a router (e.g. ARP or IPv6 neighbor discovery) so that we can generate load when there is a router between packetblaster and the system under test. (Have needed this in the past and worked around e.g. with a static configuration on the router.)\nLots of unrealized potential in the packetblaster program...\n. @eugeneia I pushed a couple of last PRs. If this passes CI then I reckon we are good.\n. Thanks! Looks like I manage to sneak this one into the v2015.12 release. Indeed I had probably indirectly initialized the PMU with perf before running tests.\nbtw: I met an Intel engineer at a conference recently and he reminded me of their own stand-alone pmu software effort (separate from kernel perf framework): https://software.intel.com/en-us/articles/intel-performance-counter-monitor\n. My overall impression is that there is a fair amount of variation between what PMUs support from one CPU model to the next.\nIntel publish some CSV files that list all of the counters for each CPU model (identified with CPUID instruction) and Snabb Switch has a script to translate these tables into Lua and lookup what is supported on the running machine.\nHere is the Lua file containing the per-CPU definitions and the script that generates it:\n- lib/pmu_cpu.lua\n- scripts/generate-pmu.sh\nThe CPU-ID string is created with these lines in pmu_x86.dasl using the CPUID instruction.\n. (Probably we should have a simple way to print all available counters for the current CPU...)\n. Oh I think I see the problem here.\nThe CPU can only track a limited number of general purpose event counters at a time: four when hyperthreads are enabled or eight when disabled. Marcel's examples show four general purpose counters so if he has hyperthreads enabled (BIOS) then the others are likely being skipped due to lack of hardware resources.\nWe should print a warning about this! The pmu.setup() function does return the number of counters that were dropped due to lack of resources but this is undocumented and should really become a warning with higher-level functions that call setup().\nThe Linux perf framework avoids this problem with sampling. It rotates the logical counters across the available physical counters and then also reports the % of time that the counter was active. I am not sure whether something like this would make sense for us and if so whether it should be done inside the pmu module or above in some higher-level code e.g. the engine.\n. :+1: \n. Somewhat zoomed-out thoughts stimulated by the PR:\n- I am glad to drop the dependency on lib.macaddress. Could be that it is worth removing that module entirely to further our goal of having as little code and as few abstractions as possible? (Or is it important? Maybe we need to decide more broadly what qualifies for an abstract data type.)\n- Network namespaces seem like a good solution for selftest.sh to avoid interfering with host networking. This does highlight the general problem that we don't have a clean/well-defined/isolated environment where the tests are executed. I wonder: should we define one and if so how?\n- Promiscuous mode worth supporting, even as the default? To avoid the \"ohhhhh, right, the packets were dropped because I forgot to put it into promiscuous mode\" troubleshooting phenomenon :-)\n. @petebristow On reflection I don't have a use case for promiscuous mode. I suppose it makes sense for raw sockets and not for tap devices. If promiscous mode is needed on the tap device, e.g. because it is put into a bridge with brctl, then this should be done by the kernel and not the snabb process.\n. @petebristow Great! This is merged onto the next branch for the next release now.\nPlease revise PRs by pushing new commits instead of rebasing. This is to make it safe for people to merge the PR branches into their own long-lived branches without waiting for a final version. (I also find it easier to follow what is happening on the branch when changes are made with new commits vs rewriting history but I realize that many wise people are the opposite.) More in git-workflow.md. If the workflow is suboptimal then people could also send a PR proposing changes to that document :-)\n. Interesting. This was tested to be working in luajit/luajit#95. Did we miss a commit on the merge and/or is this a failure of that specific machine code?\n. Great! Merged onto next, sorry about the wait.\nWhat do you think about a future policy of suppressing stacktraces for well-defined detected errors? (Showing them only for unexpected errors caused by bugs.) For example with a main.fatal(msg...) function to print and terminate. That would seem friendly to end-users.\n. iPXE and nixrdb are potentially interesting links too.\n. I read somewhere - can't find where now - that @wingo said this virtio-net implementation has now been verified to meet the performance targets of the snabb-lwaftr application? In this case a hearty congratulations to all concerned!\n. Super cool.\nI would love to have a CI benchmark that is equivalent to the DPDK-VM one we already have but running Snabb with Virtio-net inside the VM. The entrypoint to that code is packetblaster_bench.sh if you guys are interested. That way we could keep track of VirtioNet performance both to catch regressions and also to see whether it can take advantage of optimizations that we make on the host underneath.\n. Once more: This is a great piece of work!\nJust reading the code now on the max-next branch and really enjoying it. Really nice touch with the diagram of the vrings :-)\n. Thinking about the different ways we could approach musl support in Snabb Switch:\n1. Import musl as a subtree and make the master branch always build a statically-linked executable.\n2. Support musl as our second first-class platform with full test coverage (including performance regression tests compared with glibc).\n3. Support musl as a second-class platform with only basic test coverage e.g. CI uses docker to check that Snabb Switch compiles on Alpine Linux.\n4. Just merge patches that look like they would help musl but without doing any upstream testing.\nHow about we celebrate the new year by being wild and crazy with option 1?\nThat would be to add musl as our fourth dependency (alongside luajit, ljsyscall, and pflua) and truly ship a statically linked executable that will run on any Linux/x86-64 machine.\nThe first step would be to send a PR that adds musl as a subtree and uses it for the build. This will probably trigger test failures e.g. I'm expecting the NFV application to have a performance regression because the inner loop depends on the libc memcpy to be optimized for packet copy workload (glibc memcpy is, musl memcpy is not). This probably should be resolved in our own code i.e. by having a suitable packet-copy routine of our own instead of depending on non-standard properties of the libc one.\nI have had a quick look at musl. I appreciate the coding style i.e. really focusing on short and simple code. Compilation time is about 1 minute serially but only 5 seconds in parallel (measured on chur) so we would still hit our compile time targets if we assume make -j.\nI believe there will be more issues e.g. that we can't use dlsym with a static link and this will break LuaJIT's FFI? This is something we would need to resolve before merge too.\nThoughts for/against? anybody willing to take the lead on such an integration e.g. to create a branch that compiles a static executable with musl imported as a git subtree?\n. Sounds like a plan! Merged this PR onto next.\nHere is some data about the packet copy routine for us:\n1. Primarily copying ~ 32 - 1500 bytes that is in cache.\n2. Safe to round up copies to whole cache lines (either guarded with a predictable branch or actually guaranteed).\n3. May have nice alignment. (Can potentially ensure this for struct packet but less certain for virtio-net buffers allocated by VMs.)\n4. Caller will know whether src/dst overlap in memory and whether forwards or backwards copy is needed.\nQuestion then is whether we should write a memcpy() or write one or more special purpose packetcopy() routines.\nI have browsed memcpy implementations around the internet (dpdk, libc, musl, etc). The shortest path to a relatively simple and efficient one that matches our needs seems to be to take Agner Fog's design and remove the special cases that are not relevant for us. He seems to have struct a balance between performance and simplicity (well, I say that not having benchmarked it for our use case).\nSee also memcpy odyssey threads on snabb-devel and #648 (Packet copies: Expensive or cheap?).\n. Good news re: memcpy is that we do have CI coverage for this. A bad memcpy should impact the performance regression tests for forwarding packets through a DPDK VM. So if that test passes then we probably have a winner.\n(One weakness of that test is that it is using fixed-size power-of-2 packets. To be more confident of the memcpy we would want to vary the packet sizes. I have done this manually a few times with glibc memcpy and it seems to behave well.)\n. LuaJIT only uses its own memcpy for fixed-size objects e.g. structs or constant-size arrays. Otherwise it will emit a call to libc memcpy. (This is much like GCC behavior.) The calls that we have to libc memcpy today are actually ffi.copy() with variable size data in the Lua sources.\n. Great work, Rolf! Merged onto next.\nHow are you finding the driver hacking experience? :-)\nThere is a bunch more that we can do with Intel drivers. I would like to use this simple 1G driver as the base to generalize up to the new features that we will want. Thinking along the lines of:\n1. Support RSS to spread traffic between queues.\n2. Support VMDq to spread traffic between queues (already works on intel10g driver).\n3. Support X710/i40e (10G/40G) once available in the lab.\n4. Support FM10K (100G) once available in the lab.\n5. Support VF (SR-IOV) for multiple NIC families.\nbut one step at a time :-) and having I350 support is really great now! Supporting 1G for I/O is a hard requirement for many applications and even at this speed it is important to have kernel-bypass drivers.\nInteresting to see some test results on another x86 platform than Xeon too...\n. Sounds great! and sounds like an authentic driver-hacking experience :).\nI think it is fine to focus on the immediate needs of 1G in that module. I would like to converge our various Intel drivers in the near future, and perhaps deprecate the existing intel10g one in favour of another one more like this 1G driver, but I think it is fine to write them as separate drivers to start with (1G, 10G, 40G, 100G) and unify them when the similarities emerge.\nThe similarities I have in mind are that each Intel NIC will do transmit/receive in basically the same way (TDT, RDT, etc), have  RX and TX queues that can be used independently, and have some means of dispatching packets between queues (e.g. RSS and VMDq). So we should be able to provide a fairly consistent interface to all the drivers and also share some code (e.g. transmit/receive but not PHY init).\nI also think that reusable test suites will be really key. If the drivers have a uniform interface then we could reuse the test suites that check for basic TX/RX, PHY init (e.g. 100 times in a loop to catch timing bugs), RSS/VMDq dispatching onto the right queus, etc.\n. > Is is worth to spend a few cycles on trying to apply some of my lessons learned to the intel10g app\nIt would be interesting to know what you have in mind. Perhaps propose via an Issue or draft PR anyway?\nOverall though I like the idea of evolving a new driver style separately and then once this is mature we can reconcile it with the intel10g driver. I will open an issue to talk about another proposed new requirement on I/O drivers that may be hard to retrofit onto the existing intel10g driver.\n. This branch seems to have lost the documentation that I had on mine. Can you reintroduce it please?\n. I wrote up the idea of requiring these I/O drivers to support multiple Snabb processes separately attaching to transmit/receive queues on the same device (with one process being responsible for initialization) in #687. What do you think: doable?\n. @eugeneia do you see why the CI is failing on this latest commit?\n. @hb9cwp What do you think about #689? Is that design realistic? I mean for two different processes to attach to the same card, and somehow initialize it correctly, without undue synchronization (e.g. avoiding complicated dependency on which process starts first or IPC or ...).\nI am wondering how to neatly handle the case when for example one process attaches to a TX queue on the NIC and then another process reset the NIC afterwards. Can the TX process cleanly detect the reset and restore the queue for example? I am not sure if this is realistic. It would surely be nice and simple.\n. Great picture!\nYes. To make it more complicated too:\n- Can be that you have many instances of those triangles e.g. if the NIC supports 128 transmit and receive queues then you may have  processes each wanting to attach to  queues.\n- Can be that setting up the queues in the right way (e.g. telling the NIC to dispatch a specific MAC address to a specific queue) requires some synchronization (can't have two processes doing this at once because they would be touching the same registers).\nI think that initialization is the hard part, at least for Intel NICs, because once that is done they do have a separate copy of all the TX/RX registers (TDH, TDT, etc) for each queue at a different address and so processes can access them directly/separately/safely.\n. I suppose that I didn't really state the problem...\nWe do already have a solid CI system including performance regression tests. This checks every PR before merge and defends against regressions.\nThis idea is about exploring the universe of possible tests see what we can discover. It may actually run separately from our existing CI-for-PRs, as a background process that generates actionable data.\nJust to quantify a bit, suppose we were interested in testing with all of these aspects:\n- 10 different test cases.\n- 5 versions of QEMU.\n- 10 different guest VMs (Linux and DPDK).\n- 16 combinations of Virtio-net options.\n- 2 NUMA setups (\"good\" and \"bad\")\n- 2 polling modes (engine \"busy loop\" and sleep/backoff)\n- 2 error recovery modes (engine supervising apps vs process restart)\n- 2 C libraries (glibc and musl)\n- 3 CPUs (Sandy Bridge, Haswell, Skylake)\nThen if we would enumerate all of the tests we would have 10 * 5 * 10 * 16 * 2 * 2 * 2 * 2 * 3 = 384,000 test scenarios.\nIf each test would take one minute then it would take more than six months to run every scenario, and this would increase exponentially as we added more interesting variables to the tests (IOMMU setting, NIC vendor, ...).\nAlternatively if we could sample the test universe in some suitable way then we could be adding over a thousand results to our database each day and use these to answer interesting questions:\n- How sensitive is each benchmark to CPU microarchitecture?\n- ... to NUMA (non-)affinity?\n- ... to particular software version combinations?\n- which engine features should we enable by default?\n- do we have any option-dependent bad cases in our virtio-net performance?\n- on which kernel versions do we observe problems with iommu enabled?\n- how much variation is there in test results that we can't currently account for?\nClever people could answer such questions definitively just by looking at the CSV files that we provide -- or at least tell us what additional data we need to collect to provide an answer.\n. Here is an example of the kind of analysis that can be done using R to process the CSV data: Exploratory Multivariate Analysis. This kind of analysis is a simple everyday thing for many people and it does not necessarily even require much information about what the data means or how it is generated. Just a matter of finding relationships between variables e.g. what combinations predict high performance, low performance, failed tests, etc.\n. @petebristow Cool!\nGreat that you are working on factoring out test and benchmark code to make it easier to use systematically. This is really valuable stuff and AFAIK nobody else is working on that now. I really relate to the questions you have formulated and the parameters to explore e.g. to see the impact of engine parameters. I would love to be able to simplify the engine e.g. by removing configuration knobs where performance tests can demonstrate that there is one setting that always works well (e.g. with engine.Hz).\nRelated activities:\nFirst we have Continuous Integration for Snabb Switch i.e. SnabbBot that has been diligently benchmarking every PR that is submitted to Github. SnabbBot has an archive of more than a thousand test results stored in Gists. You can see the performance test results at the start of these logs:\nChecking for performance regressions:\nBENCH basic1-100e6 -> 1.0055 of 14.54 (SD: 0.185472 )\nBENCH packetblaster-64 -> 1.00019 of 10.584 (SD: 0.0162481 )\nBENCH snabbnfv-iperf-1500 -> 0.906008 of 5.426 (SD: 0.471703 )\nBENCH snabbnfv-iperf-jumbo -> 0.974238 of 6.366 (SD: 0.239633 )\nBENCH snabbnfv-loadgen-dpdk -> 1.00709 of 2.652 (SD: 0.0147919 )\nThis system is available today, it's easy to add more benchmarks to, and each benchmark will automatically be checked for regressions and its results stored on Github. This is the base that we are building on.\nThe main part that is not covered today is to define a large number of parameters and to have the CI explore them in a systematic way. This is complicated for applications like NFV that have complex dependencies that can be large/slow to build. The test framework is not specific to NFV/VMs but it does have to support that application.\nNow moving forwards with testing more combinations on more servers I am keen to take some time to explore references like Setting up a Hydra build cluster for continuous integration and testing to see if there are some kindred spirits who have already developed the kind of tooling that we need. The Nix community particularly seem to have excellent taste and to have invested a lot of creativity in the problem of \"build and test\". I am talking with @domenkozar about bringing up a Hydra instance for Snabb Switch that we can experiment with.\n. @domenkozar Good description. I'd say that you are more-or-less describing the CI that we already have. This is effective for catching regressions on a small set of tests with one reference platform.\nNow I want to take the next stop beyond this and start searching for problems in a larger space of configurations and dependency versions (\"scientific testing\" until someone points out a better name). Goal is to be able to show, based on data, how well a given Snabb Switch release works with different CPUs, network cards, Linux distributions, virtual machines, and so on. \nThe same mechanism may work for both: CI that tests a 2-tuple of snabb+environment. The difference would be that for basic regression tests there would be one environment whereas for scientific tests there would be (say) a thousand environments chosen pseudo-randomly from a space of a billion possibilities. Could be that we setup a Hydra that has two Git repos as input and that another process (SnabbBot) is creating random permuations of environments and pushing them into Git for testing, for example?\nZooming out a bit...\nFor analogy suppose that we were developing a web browser like Firefox and we were taking these two approaches to testing:\nFor regression tests we would test very specific things: for example load the same content from the same apache version and simulate the same UI events and assert that the browser products the expected image (screenshot). Or feed a hundred programs to our JavaScript engine and check that they all produce the expected result. And so on. These tests would tell us that our changes are not breaking obvious things.\nThe scientific tests would be to tell us when the tests are breaking subtle things in the real world. This requires that the test environment has a comparable amount of variation to the deployment environment. For example, in searching for problems we would want to test with many different independent variables like:\n1. Website to load: chosen from one of the million most popular ones.\n2. Operating system: Linux, OSX, Windows, iOS, Android plus various sub-variables (corporate/OEM additions, localization, virus scanners, etc).\n3. IPv4/IPv6/dual-stack.\n4. Geographic region.\nThis kind of test coverage would be needed to ensure that we can ship a major new release to millions of people with confidence that it will have fewer problems than the previous one. People who really do build browsers can perhaps gather this data in semi-automated ways, e.g. by having a thousand people download a nightly build that is instrumented to report back to upstream, but for network equipment we don't really have that luxury and I think we need to generate the variation ourselves (or else we will be forever on the back foot reacting to bug reports without context).\nComing back to topic...\nToday our CI tells us that the software is working and performing as expected with simple configurations and running on a reference platform modelled on the latest Ubuntu LTS. This means that we can ship new releases and people can deploy them on that platform. However, we need to do much better than this. There are a tremendous number of configurations, platforms, VMs, etc that people can use. I want the CI to keep track of how well we are working with all of these. I want to avoid the situation where most users have a bad experience because they deploy with software different than our CI.\n. Just another thought to throw onto the pile, following @plajjan's train of thought...\nIt would be neat if we could run these tests in \"SETI@HOME\" style. That is: if we have 10-20 servers in our lab we could have them detect when they are idle and automatically start running tests e.g. in a container that can be instantly aborted if a developer starts working on that machine.\nalso wondering whether we can split this whole problem into several independent parts:\n1. Define a test environment with many variables (software versions, configurations, etc) and run it.\n2. Archive test results in a convenient place and with enough metadata for analysis.\n3. Execute bulk tests in a continuous way with automatically chosen variable settings.\n. @xrme Looks like the right direction to me :).\nI start to feel like a DynASM caveman myself.. I have not been using the variable registers like Rq(i) in my code. It does seem like there is quite some flexibility in how to factor assembler code with DynASM.\nI have also been admiring the DynASM AES crypto code in eugeneia/snabbswitch#8. First time I have seen higher-order functions used in an assembler program :-). Seems like it's possible to express things like loop unrolling in fairly short and neat ways with DynASM.\nI like this kind of programming :-). The last assembler that I really used in anger myself was AsmOne and that was really a different era!\n. Great initiative!\n\nThis provides go like benchmark functions\n\nI am not familiar with how benchmarking is done in Go. Could you write a README that summarizes the idea and links to relevant references e.g. to the Go feature?\nI like the idea of including engine parameters in the config object. Really we should unify all of our configuration - engine, programs, apps, links - into a tree structure that can be modelled in YANG. Then we could have quite complete external representations e.g. load/store fragments as JSON. (I'll file a separate issue with that idea.)\n. Thanks for writing this up!\nI am definitely sold on having a low-ceremony way to define benchmarks that can be run in a generic way and produce data in a uniform format that we can track.\nCan we try the simplest thing that could possibly work though? How about if the benchmark functions take no parameters and return no values, but just do some work and we record how long it takes? If they need to run an app network or tweak engine parameters then they can do that directly (or via library functions).\n. This is looking very much like the kind of benchmarking that I want to do!\nCan we define a really simple interface between the benchmark definitions and the framework that is running them? The benchmark definitions could be as simple and minimalist as possible (so that people will write them) and the benchmarking framework could have multiple implementations (depending on what people need e.g. simple wall-clock time, or count cycles, or track with PMU, or track with external profiler, or collect JSON, or ...).\nIn terms of minimalist definitions I mean something like what we do with the selftest framework. This is really trivial. To be testable a module defines a function selftest() that returns on success (otherwise raises exception or terminates process). The \"framework\" that runs this is actually just one line of code in snsh that does require(moduletotest).selftest(). This has been pretty successful in terms of adoption and I think the simplicity of the framework is a part of that.\nThen we also have a more elaborate bit of framework, like a Makefile target that detects what selftests exist and runs them all and logs results, but nobody really has to care about that because it is separate.\nSo could we make the interface for this test framework similarly minimal?\nHere is one idea. Suppose that a module that is benchmarkable would have a table called selfbenchmarks like this:\n``` lua\nselfbenchmarks = {}\n-- Benchmark how long it takes to execute an empty loop 1 billion times\nfunction selfbenchmarks.loop_1e9 ()\n   for i = 1, 1e9 do end\nend\n-- Benchmark how long it takes to execute nested loops 1 billion times\nfunction selfbenchmarks.loop_1e6_1e3 ()\n   for i = 1, 1e6 do\n      for j = 1, 1e3 do end\n   end\nend\n```\nand then a simple test framework could execute these with something like:\nlua\nfunction benchmark (modulename)\n   for name, fn in pairs(require(modulename).selfbenchmarks) do\n      local start = now()\n      fn() -- run the benchmark function\n      local finish = now()\n      print(name, finish-start)\n   end\nend\n... and then this would be the starting point for more fancy extensions e.g. fancier test frameworks that measure and report in more elaborate ways, libraries that can be called to generate interesting test data, ways to setup engine parameters, etc.\nWhat do you think?\n. @andywingo I would like to resolve this by saying that the individual selfbenchmark functions (e.g. loop_1e9) are responsible for executing the code that should be measured. If they fail to do this then it's a bug in the selfbenchmark function.\nIn practice I would expect selfbenchmark functions to have to be written very carefully depending on exactly what they intend to measure. If the compiler eliminating important work is a concern they may need to pass certain values to C functions or store them in FFI structures and force a compiler barrier; if the cache is a concern they may need to choose their workload size and access pattern very carefully; if they are sensitive to the alignment of data in memory then they need to allocate objects deliberately; etc. There are so many of these concerns that I don't think we can outsource them to the framework above us on the stack very effectively.\nI may be overstating the case against return values from test functions, but there it is anyway :).\nThere is actually one value that I would like to return but did not mention for simplicity :-) and that is the \"auxiliary\" table used in the PMU. For example then you declare some metrics like how many iterations you executed and this can be used to normalize the time spent e.g. into nanoseconds per iteration. This would be useful for interpreting data e.g. on a checksum routine I would be interested in comparing the cycles-per-byte metric for various conditions such as input size and alignment. For example see the /breath and /packet columns in the lib.pmu example.\nLuaJIT does support multiple return values so we could in principle make both of us happy :-).\n. @petebristow There is a lot going on in your snippet and I am probably missing the forest for the trees. The coding style is unfamiliar to me and my brain is asking things like: what is b? what does the \"j\" in jbenchmarks mean? is the camelCase significant? It would help me to have a little more commentary or to break the code into more distinct bits and say what the interfaces between them are.\nI do start to come around to your way of thinking though. My example is weak in terms of not allowing expensive setup or easily running many variations in a loop (without awkwardly factoring them into separate named functions). Could be nicer to have one function that makes a series of calls to e.g. start(name) and stop() for the parts that it wants to profile more like your example.\nThe reason I like the idea of returning the table of \"aux\" metadata (which I left out of the initial proposal because it is really a non-essential added detail) is not specific to the PMU but simply because it provides some ways to normalize the data e.g. to convert from \"total time to run the benchmark\" to \"nanoseconds per packet\" that may be easier to interpret and compare. Like if the benchmark performs a loop N times then it can tell the framework the value of N to help with reporting on per-loop instead of total-for-all-loops.\n. @petebristow okay, reading the code closer I see that you have this stuff covered with fields in the b struct. I'd still like to find the simplest possible way to write the code in the modules being benchmarked, with the simplest interface to the testing framework that is worrying about JSON etc (or not, depending on what the user is interested in e.g. what arguments passed to snsh/snabbmark).\n. @petebristow So what do you think of this example that is moving in the direction of the one you posted:\nlua\n-- Run a series of checksum benchmarks with different alignment, size,\n-- and architecture-specific code.\nfunction selfbenchmark (b)\n   for _,align in ipairs({0, 1, 2, 3, 4, 5, 6, 7, 8}) do\n      for _, size in ipairs({32, 64, 100, 128, 200, 256, 500, 512, 1000, 1024, 9000}) do\n         for _, code in ipairs({\"generic\", \"sse\", \"avx\"}) do\n            local name = 'checksum-align:'..align..'-size:'..size..\"code:\"..code\n            local data = allocate_packets(size, align, 1e3)\n            -- Start benchmark, or skip if b:start() returns false for this benchmark name\n            if b:start(name, {checksum=1e6, byte=1e6*size}) then\n               -- Perform 1e6 checksums (1e3 loops over 1e3 packets)\n               for i = 1, 1e3 do\n                  for j = 1, #data do\n                     functions[code](data[j])\n                  end\n               end\n               b:stop()\n            end\n         end\n      end\n   end\nend\nResults would look like:\nchecksum-align:0-size:32-code:generic = 312312312ns (312ns/checksum, 10ns/byte)\nchecksum-align:0-size:32-code:sse = ...\nThis one has a simple b object with two methods:\n- start(name, aux) => boolean return true if the benchmark called name should be executed. Start recording and remember the aux values for later reporting.\n- stop() stop recording.\nThis interface is small but hopefully generic so that programs like snabbmark could implement arbitrarily complex profiling and reporting in the b argument that they pass.\n. @petebristow Daydreaming about orthogonality...\nPerhaps we could have a global bench module instead of the local b callback parameter?\nSo any code could run a benchmark like:\nbench.start(\"my-benchmark-1000\", {...properties...})\n... work ...\nbench.stop()\nand fancy features like JSON capture, PMU recording, etc could be handled separately e.g. with:\nbench.set_mode('pmu')\nbench.set_json_output('/tmp/%s.json')\netc. Then the selfbenchmark() function would work exactly like selftest() i.e. simply a function that gets called.\n(This starts to sound a lot like the existing interface to the jit.p profiler in LuaJIT.)\n. @petebristow So we are edging closer together :)\nOne issue I have with using functions is that Emacs will indent your code much more \"aggressively\" with default settings.\nHand-indented example:\nbench.run(\"simpletest\", function()\n   b.process_with(foo)\nend)\nEmacs default settings:\nbench.run(\"simpletest\", function()\n                           b.process_with(foo)\nend)\nwhich is a lousy objection but it is annoying to fight the tooling.\n. Here is another idea that I will just throw out there:\nlua\n-- Return a table of benchmark functions that can be run.\n-- The functions take no arguments, consume time that should be measured, and return metadata.\nfunction selfbenchmarks ()\n   do_expensive_setup() -- not counted\n   local benchmarks = {} -- table that we will add our benchmark functions to (and return)\n   -- Simple benchmark\n   function benchmarks.simpletest ()\n      do_work()\n      return { iterations = 1000 }\n   end\n   -- Complex collection of related benchmarks\n   for _,align in ipairs({0, 1, 2, 3, 4, 5, 6, 7, 8}) do\n      for _, size in ipairs({32, 64, 100, 128, 200, 256, 500, 512, 1000, 1024, 9000}) do\n         for _, code in ipairs(instructions) do\n            local name = 'checksum-align:'..align..'-size:'..size..\"code:\"..code\n            local data = allocate_packets(size, align, 1e3)\n            -- Create a closure that runs this benchmark variant\n            local fn = function () for i = 1, 1e3 do functions[code](data[i]) end\n            -- Add it to the benchmarks table with a name\n            benchmarks[name] = fn\n         end\n      end\n   end\n   return benchmarks\nend\n. Added here: https://github.com/SnabbCo/snabblab-nixos/commit/0f809f44782113b356e8bd334cdcb58ba2f7ace6\nYou should be able to login to chur with ssh -p 2020 petebristow@lab1.snabb.co and have sudo access there.\nYour account will be on the new lab servers too: I'll post an update on those to the list tomorrow.\n. Closed: duplicate of #679.\n. @andywingo Great thoughts! Nodding vigorously while reading.\n. @plajjan So, fishing for more practical perspective here... :)\nCan you tell me from your perspective how much you care about each of these things from a device you are deploying:\n- YANG/NETCONF interface for provisioning a cluster of devices as a single unit.\n- YANG/NETCONF interface for provisioning one device.\n- YANG/NETCONF interface exposing the internal workings of a device (e.g. Snabb app network).\n- YANG/NETCONF interface exposing \"generic\" information that is not application-specific e.g. hard disk utilization, management network (Linux kernel) details like interface names / addresses / routing tables, etc.\n. @plajjan @andywingo The \"build vs reuse\" trade-off is an interesting one that will indeed require some digging into what is available. Generally finding a suitably Snabbish solution requires some care. It would be lovely to avoid linking third party code into our process (e.g. C library) and to avoid dealing with unnecessarily complicated tech like XML.\nCould be neat if the interface between the Snabb process and the management system is really simple. For example if the management system (NETCONF daemon, or NixOS module, or ...) would output configurations in a simple format (e.g. JSON file corresponding to YANG model) and import operational data in some simple way too (e.g. JSON file or direct access to shm files).\nOn the other hand we would really want to avoid linking third party code into our process to accomplish this e.g. a C library providing a DOM-like API. That's not Snabb style.\nOne more topic (alluded to in previous comment) is that there may be some standard YANG objects that are not part of our applications but that operators want to be able to manage for the device. For example to change the IP address on the eth0 management port or to monitor whether the /log filesystem is running out of space. Maybe people will want to make these changes atomically together with application-specific changes like snabbnfv parameters. Question then is whether such aspects can compose in a nice way e.g. via a modular NETCONF daemon that has all the base functionality but that we can also plug our extensions into.\nGenerally it could be nice to think of NETCONF and NixOS as two alternative ways to accomplish the same thing i.e. define new configurations, force rollbacks, etc. NixOS will accomplish such things by taking an abstract model (module definition) and generating application configuration files and systemd process definitions. This seems like a simple and down-to-earth model for other management tool integrations to follow (compared with e.g. linking a C library that provides us with a DOM-like API to access our configuration or some other such abomination :-)).\n. One of the few times I have somewhat regretted building instead of reusing was adding support for SNMP MIB-II (RFC 1213) to a device. MIB-II defines a bunch of objects to basically explain the Linux kernel setup for the machine that the application is running on.\nOn the one hand I can see the value of having a baseline of management objects available on all devices and why operators would flag \"must implement MIB-II objects\" as a basic requirement for all devices.\nOn the other hand this is boring from the application developer perspective because it is not talking about the problem that they care about.\nI am guessing that @alexandergall is tackling this problem by reusing all of these objects from NetSNMP and then plugging in the application-specific objects that he cares about (with developer hat on) as an addition. This would seem like NetSNMP providing value i.e. taking care of the bits that are not application specific. (Same way Linux provides value to Snabb because we don't care about e.g. how filesystems are implemented, and BIOS provides values to Linux because it doesn't care how the memory controller is initialized, etc.)\n(The reason I didn't do this way 10+ years ago when faced with the same problem was that we were building on the Erlang SNMP daemon which didn't provide these objects out of the box and I preferred to add them in rather than somehow integrate NetSNMP. It was not actually that much work to implement the objects anyway but it felt a bit futile e.g. is anybody ever going to actually query the ifStackTable to see how I have lovingly explained the mapping between physical interfaces and bonded management interfaces, etc.)\n. Sorry, much braindump this morning, perhaps this topic can be spun out into a separate issue...\nI am thinking there are basically three ways to manage network equipment (Snabb or otherwise):\n- \"Classic telco\" style. Configure manually (e.g. as you would a home router) and monitor with SNMP (e.g. Cacti) to track metrics of interest (packet drops, busy hour traffic volume, etc). The way smaller ISPs like to operate.\n- \"Big telco modern\" style. Configure automatically with NETCONF and YANG. Monitor with fancy tools attached to the YANG model. The way larger ISPs want to operate.\n- \"devops\" style. Provision network equipment the same way as servers. For example with Puppet/Chef/Ansible/NixOS. Relatively new idea (?) pioneered with considerable success by Cumulus Networks who make a \"normal\" Linux distribution for installing on hardware switches.\nThe right choice will depend on the user and Snabb applications will ideally accommodate all options in a nice way.\nMake any sense?\n. @plajjan How about SNMP? Do you care about it? Do you get it from these same tools that do NETCONF?\n. @plajjan Hm. I suppose generally there is a risk of a mismatch between Snabb / NC agent / NMS. For example because one component is lazy and doesn't validate or because the wrong YANG file has been loaded somewhere. If a mismatch is detected then it seems like the best we can do is log an error and skip that value.\nQuestion is who is responsible for doing this: does Snabb do it internally? or does Snabb do it externally via the NC daemon with a DOM-like API that will raise an exception? Or does Snabb ship data and rely on the NC daemon to detect/skip/log on error?\nI have a feeling that the most important aspect will be having test coverage. The CI should be generating the data and validating it against the YANG model. In production it is important that these problems are detected, and that they are rare, but less important which software component detects them.\nMake any sense?\n. @alexandergall Curious for your viewpoint: if a device offers you NETCONF and you have a decent open source client then how much do you care about SNMP?\n. Great braindump, @mwiget!\n. > I think you should go talk to the sysrepo people about your concerns.\nGood idea. This can be the next step after establishing our prejudices in this thread :-).\n\nwhat is okay on this front? Lua libs is ok? C is not?\n\nGenerally I would like a solution that is small and has few moving parts. If they provide a library it would ideally be as \"rewritable software\" (lukego/blog#12) i.e. simple enough that it doesn't really matter whether we use their implementation or make our own.\nI feel like the ideal interface towards an external NETCONF daemon would be based on a small number of interactions like:\n1. Snabb->NETCONF: Here is the YANG model we are using.\n2. NETCONF->Snabb: Here is your new configuration tree.\n3. Snabb->NETCONF: Here are the latest operational counter values.\nThese are coarse-grained high-level interactions that keep the systems at arms-length from each other. We make no assumptions about where these configuration updates are coming from and they make no assumptions about how we are representing and processing them in our application.\nI imagine this protocol would be implemented in some really simple way. For example tree data would be represented in a simple external format like JSON and interactions would be done with a simple mechanism like a socket or a file. (@alexandergall already defined an interface towards NetSNMP along these lines I believe.)\nIf the Snabb process needs to have a relatively deep understanding of the YANG models then I imagine that we would parse the YANG files ourselves and make our own \"smart\" representation. This idea is quite appealing to me i.e. that we would define our own configuration schemas directly with YANG files.\nThe main part I would like to avoid is fine-grained interactions, for example that the native data structure we operate on is a third-party library that e.g. requires adding a third-party YANG compiler into our build toolchain, or depends on calling out to an external library for things like diff'ing old/new configuration trees, or expects to be able to frequently RPC the daemon for support, and so on. I feel like most users are happy with this kind of library support but that it is decidedly un-Snabbly.\n@andywingo How does this match up with your thoughts?\n. @mwiget I'm thinking that our ideas are quite in sync here e.g. that in your diagram the vMX is feeding the Snabb process with a file that contains the current complete configuration tree. Much like the way our OpenStack driver provisions the Snabb traffic process by creating a new configuration file for it.\nI like this kind of arms-length distance. In the OpenStack case we could also for example have made a ZeroMQ endpoint out of snabbnfv traffic so that OpenStack could send it RPCs but that seems like much too tight coupling for my taste.\n. @plajjan wow amazing list!\n. @plajjan Great list of pitfalls. Can the separate NETCONF daemon shield us from these problems?\n(NOTE: I wrote the below without reading @andywingo's comments above in sufficient detail. Leaving for posterity :))\nHere is one idea for an interface that a Snabb application could expose to a daemon. Simple case would be:\nsnabb-lwaftr config --load foo.yangdata\n... which could use a generic library to update our internal YANG configuration tree and propagate changes everywhere they need to go (e.g. app network).\nThis could also be extended for different programs with variants like:\nsnabb-lwaftr config --load-compiled-binding-table foo.dat\nto handle exceptional data that has to be treated specially somehow.\nThis could also implement a validation hook that an external daemon could call while processing NETCONF requests:\nsnabb-lwaftr config --validate foo.yangdata\n... though this is a half-baked thought and @andywingo is already thinking ahead there in talking about supporting things like \"candidate\" configurations. (I am not familiar enough with NETCONF to have any bright ideas in that area yet.)\n. Thanks! Having a nonstandard make install behavior is unfortunate.\nOne more alternative we should consider: should we drop our install target? I do like the idea of Snabb Switch being a single file that could be (but needn't be) distributed as a binary. If we would want to create symlinks, manpages, etc then this could perhaps be done by a command snabb install [dest] rather than a Makefile.\n. The fundamental issue here is that Snabb Switch only supports Linux/x86-64. That is, i486 is not a supported platform.\nAre you interested in porting to i486? This should be reasonably straightforward and I can sketch the work likely involved. More challenging if your target CPU does not support huge pages.\n(Curious: what are you working on here?)\n. More specifically to this PR:\nThe top-level Makefile is cherry picking the relevant modules from ljsyscall. It takes x64 (supported platform) but not x86 (unsupported).\n. (Sorry @justincormack for doing violence to ljsyscall in our downstream packaging like this. Perhaps we should change that to avoid creating support problems for you. And we should really print a message at startup to complain about running on an unsupported platform instead of hitting a random error.)\n. Can you say some words about your packaging work? I am confused about why you want to package up software for platforms that it does not support :).\n. @comotion Thanks for the feedback! Indeed we have done a terrible job of setting expectations with regards to portability. I have attempted to address this in #712. Feedback welcome!\n. Great initiative!\nGenerally I believe that Github doesn't let people self-assign issues and it is a bit locked down. I am not an expert on Github permissions (have previously set them conservatively mostly to avoid e.g. accidental force pushes to important branches like master). Let me know if there is something I should check or change.\n\"Open\" and \"Closed\" are interesting. I see the meaning for bugs: open means action needed to investigate/resolve. Likewise with PRs: open means the creator wants the commits to be pulled. Questions perhaps too: is the creator satisfied that they received an answer. How about ideas though? Do we just leave those open? Or close e.g. when somebody creates a new one that restates the idea?\nCould also be nice to keep track of which issues could be fixed in some direct way (e.g. a reproducible bug) and ones that are causing problems for users (e.g. the IOMMU setting issue that has been haunting us for much too long now but requires some detective work to understand and fix/document.)\n. @eugeneia we could also define some interesting categories of Issue as Github search terms e.g. encoded into a link like open bugs or open ideas. These would be actionable in different ways e.g. it could be a priority to close all of the open bugs but harmless to leave the ideas open unless/until they are adopted or rejected.\n. This is most excellent!\nSo the role of next is redefined i.e. instead of merging individual changes it will now merge collections of changes from integration branches, of which max-next is the first.\nThis is a long anticipated step forward towards our prophesised Git workflow. It should also help with the slower rate at which changes have been pulled onto next since my second child was born :).\n. @eugeneia we need to think about what is the workflow between you and me for reviewing these changes on max-next and landing them on next. I suppose the first step is for me to look for a good way to review changes en masse (likely not this Github page) and see what help I need from you :).\n. For reference here is what a Pull Request looks like in the Linux world when a batch of accepted changes are sent from a subsystem maintainer upstream:\nhttps://lkml.org/lkml/2013/2/8/410\nThat is a lot neater than what we see on this Github page, but I think that we have all the information and that I only need to learn how to query it.\nFirst example:\n[luke@chur:~/git/snabbswitch]$ git log --oneline --merges --graph master..origin/max-next\n* 1d3a1c3 Merge PR #717 (Fixed a bug where strings could not be app arguments) into max-next\n*   dfbc5f9 Merge PR #681 (implementation of virtio-net driver) into max-next\n|\\  \n| * 7a17d0a Merge branch 'master' into virtio-net\n* dd0f939 Merge PR #683 (intel1g: Intel 1G driver) into max-next\n| * 004ac13 sync with intel1g_rs_i210, e.g. remote master (v2016.1)\n| * 295e5d4 sync with remote (v2016.01), resolve conflicts\n| * 68980a8 Merge pull request #1 from hb9cwp/intel1g_rs_snabbmark\n* fe1a893 Merge PR #633 (core.engine: enable selftest() method) into max-next\n* 337071c Merge PR #699 (make PREFIX work like expected) into max-next\nThat makes it reasonably clear what high-level changes have landed in max-next as context for scanning through or diving into the diff.\n. @eugeneia Hey I think this setup is going to work!\nI am happy to pull the max-next branch onto next when it passes CI. Generally I think it will make a nice flow if you open a PR from max-next to next each time you feel it is in a state to be merged and that I try to do that without undue delay.\nIf I were a really advanced Git/Github user I could probably identify a prefix of the tree that is safe to merge now, before the CI error comes along, but I don't see one. Could be an idea for SnabbBot to automatically test all top-level merge commits on maintainers' branches? (Just a thought for one day.)\n. On closer inspection I have some things I would like to fix on max-next before merging onto next. Can we try to resolve those quickly and do this merge before taking new code onto max-next? I will try to be responsive to avoid blockage of the flow.\n. @eugeneia Can you please retarget this PR to next and open/reopen it every time you want me to merge some new changes? (The branch should only contain versions that you really want me to pull when the PR is open, if you are integrating stuff then better to close the PR or do that on a separate branch until ready.)\n. The compile error is due to luajit/luajit#126. The workaround is to make clean (or git clean with suitable args). Just have to accept this. (Perhaps our Makefile could somehow suggest a make clean if the build of a dependency fails.)\nLooks like @eugeneia already did the git subtree pull and I needn't have bothered with #715.\n. @eugeneia next step is you merge this onto max-next?\n. @nnikolaev-virtualopensystems The normal operation of packetblaster is to create transmit descriptors and then reuse them in a loop. This works on Intel NICs. Hopefully works on Mellanox NICs too. I suspect it would also work on Virtio-net devices by changing the vring used and avail indexes. If it works for Virtio-net vrings then it should also work for a tap device if you use /dev/vhost-net to access that with a vring.\nFailing all of that, it would also be possible for packetblaster to have a mode where it doesn't use the \"DMA reset\" trick and simply transmits packets. That should work for any I/O device but would likely become CPU bound if you use many NICs. (packetblaster does 100G - 10x10G - without breaking a sweat and that is because of the trick.)\n. Added a link to Mellanox firmware release notes. Interesting reading. Gives some insight into the line between hardware/firmware on the cards i.e. which bugs are fixed and features added by firmware upgrades.\n. I have pushed a major update of the ConnectX-4 driver in commit 7659eb61fcf01cc9db342dfdc040644c23b45186.\nI have been able to initialize the card and transmit and receive packets now. I am reformulating the code more cleanly now. The initialization side of this is pushed and next I am doing clean transmit and receive.  I also need to provide a suitable API for multiprocess operation and for setting up the Flow Tables and hashing in useful ways.\nI squashed the history on that branch before I pushed to github. I will be fetching some draft code from the more complete history at lukego/mellanox-refactor branch in the short term. Part of the reason to squash is that I had included a relatively large log file from the Linux mlx5 driver in the repo and I'd like to avoid bloating the snabbco/snabb repo with that.\nI also started filing issues for things that I am following up with Mellanox support. See issues with tag 'mellanox'.\n. I pushed a big update to the Mellanox driver with commit 21d0dc36d1f3dd1c022d19c3777339f52c3609b2. There is still work to do but most things are in place now.\nThe driver is now designed for multiprocess operation for use with #1021. The design is to have one ConnectX4 app for each NIC that performs initialization of all the queues, then to have any number of IO apps that attach to queue-pairs and can run in other Snabb processes.\nExample:\nlua\n-- App to setup the NIC\nconfig.app(c, 'nic', ConnectX4, {pciaddress = '01:00.0', \n                                 queues = {'a', 'b', 'c'}})\n-- Apps to perform I/O (can be in other processes)\nconfig.app(c, 'io-a', IO, {pciaddress = '01:00.0', queue = 'a'})\nconfig.app(c, 'io-b', IO, {pciaddress = '01:00.0', queue = 'b'})\nconfig.app(c, 'io-c', IO, {pciaddress = '01:00.0', queue = 'c'})\nCurrently all queues are setup for hashing (RSS).\nThere is more to do:\n- [ ] Setup a Hydra job to run the selftests.\n- [ ] Create a truly torturous selftest with multiqueue, restarts, etc. (Could be part of IO app #1043?)\n- [ ] Support L2 switching in addition to hashing (same \"Flow Tables\" mechanism).\n- [ ] Get \"self-loopback\" under control (currently broadcasts go onto the wire and back to the NIC).\n- [ ] Cleanup & document interface (incl. better names for the apps).\n- [ ] Expose configurations options for VLAN insert/remove, MTU, etc.\nCurrent basic selftest output with sending/receiving packets between\ntwo NICs. (Here we see the apparent issue with the NIC duplicating\nbroadcast packets i.e. sending onto the wire and also back onto local\nRX.)\n```\nselftest: waiting for both links up\nLinks up. Sending 10,000,000 packets.\nNIC0\n2,000,000,000 rx_bcast_octets\n  20,000,000 rx_bcast_packets\n           0 rx_error_octets\n           0 rx_error_packets\n           0 rx_mcast_octets\n           0 rx_mcast_packets\n           0 rx_ucast_octets\n           0 rx_ucast_packets\n1,000,000,000 tx_bcast_octets\n  10,000,000 tx_bcast_packets\n           0 tx_error_octets\n           0 tx_error_packets\n           0 tx_mcast_octets\n           0 tx_mcast_packets\n           0 tx_ucast_octets\n           0 tx_ucast_packets\nNIC1\n2,000,000,000 rx_bcast_octets\n  20,000,000 rx_bcast_packets\n           0 rx_error_octets\n           0 rx_error_packets\n           0 rx_mcast_octets\n           0 rx_mcast_packets\n           0 rx_ucast_octets\n           0 rx_ucast_packets\n1,000,000,000 tx_bcast_octets\n  10,000,000 tx_bcast_packets\n           0 tx_error_octets\n           0 tx_error_packets\n           0 tx_mcast_octets\n           0 tx_mcast_packets\n           0 tx_ucast_octets\n           0 tx_ucast_packets\nselftest: complete\n```\n. Hi @tsuraan. Yes, I am actually planning to loop back this month and try to get the driver branch ready for upstream.\nThere is a quite complete driver here: connectx_4.lua. This basically works but seems to often exercise some bad cases in the firmware (sometimes the NIC gets wedged and requires a cold boot server power cycle to recover.) The more recent firmwares are also lacking some important information from their release notes (definitions of new error codes that are appearing sometimes.) It's a bit of a slow and frustrating process to resolve these issues but I have some new leads that I plan to follow up shortly.. There has been a performance regression observed when using indirect descriptors (especially) and mergeable RX buffers. The root cause is not known yet but it does not seem to be anything \"obvious\". (Could be additional L3 cache misses due to touching multiple guest memory locations?). You might want to test without indirect descriptors if this is convenient. See #665.\n. @wingo Thank you for suffering through these long winded workflow discussions :).\nI am in favor of bringing in this change to support merge of the lwaftr. Merge from @eugeneia would make it official. I am impressed at your compiler skills to make this change in LuaJIT :-).\n. @eugeneia I don't think we need to single out this patch. There is a diff between our luajit and the upstream one and some of those changes will probably never e accepted. Good idea to try and minimize the diff over time but sometimes we will just agree to disagree with them.\n. One day we can have a \"luajit\" subsystem maintainer who will volunteer to worry about this stuff...\n. @eugeneia Good question. The quickest way to get an overview seems to be:\n[luke@lugano-1:~/git/snabbswitch]$ git log --oneline --graph lib/luajit\n* 8dd093b Synchronize lib/luajit with LuaJIT/LuaJIT:v2.1\n* 782e9b3 Add pointer to int casting of T->mcode in setintfield call\n* ef49da4 jit.util.traceinfo(): Include mcode, mcloop, szmcode\n*-.   863854c Merge PRs #619 #620 #621 (LuaJIT extensions) into next\n|\\ \\  \n| | * a212176 jit.p: Enhance 'v' mode of profiler\n| | * 065979c jit.p: Add support for profiling at trace granularity\n| |/  \n|/|   \n| * 53aedce jit.p: Add CPU performance counters as profiler source\n|/  \n* 2d8ba7c Merge commit '532f9f4dcb1a8fbccb3c766be96f72b995834b03' as 'lib/luajit'\nand for full details it seems to be necessary to export the subtree with git subtree split or git subtree push e.g. into a repository of its own. However, those commands don't seem to actually work when I try them for the first time:\n[luke@lugano-1:~/git/snabbswitch]$ git subtree split --prefix lib/luajit\nfatal: bad object 55c3b29f7b20f3801848e0ab71e9de1d22207b95\n[luke@lugano-1:~/git/snabbswitch]$ git subtree push --prefix=lib/luajit ~/tmp master\ngit push using:  /home/luke/tmp master\nfatal: bad object 55c3b29f7b20f3801848e0ab71e9de1d22207b95\nand I am not immediately sure if that is due to user error, software error, repository error, etc.\n. Here is at least a middle ground for exporting the changes as individual patch files:\n[luke@lugano-1:~/git/snabbswitch]$ git format-patch 2d8ba7c lib/luajit/\n0001-jit.p-Add-support-for-profiling-at-trace-granularity.patch\n0002-jit.p-Add-CPU-performance-counters-as-profiler-sourc.patch\n0003-jit.p-Enhance-v-mode-of-profiler.patch\n0004-jit.util.traceinfo-Include-mcode-mcloop-szmcode.patch\n0005-Add-pointer-to-int-casting-of-T-mcode-in-setintfield.patch\nwhich look like this:\n``` diff\n[luke@lugano-1:~/git/snabbswitch]$ cat 0004-jit.util.traceinfo-Include-mcode-mcloop-szmcode.patch \nFrom ef49da4219100a39e8f285609f7e6645fb317651 Mon Sep 17 00:00:00 2001\nFrom: Luke Gorrie luke@snabb.co\nDate: Wed, 23 Sep 2015 13:25:27 +0200\nSubject: [PATCH 4/5] jit.util.traceinfo(): Include mcode, mcloop, szmcode\n\nlib/luajit/src/lib_jit.c | 3 +++\n 1 file changed, 3 insertions(+)\ndiff --git a/lib/luajit/src/lib_jit.c b/lib/luajit/src/lib_jit.c\nindex 9e0e01d..975063c 100644\n--- a/lib/luajit/src/lib_jit.c\n+++ b/lib/luajit/src/lib_jit.c\n@@ -299,6 +299,9 @@ LJLIB_CF(jit_util_traceinfo)\n     setintfield(L, t, \"nk\", REF_BIAS - (int32_t)T->nk);\n     setintfield(L, t, \"link\", T->link);\n     setintfield(L, t, \"nexit\", T->nsnap);\n+    setintfield(L, t, \"szmcode\", T->szmcode);\n+    setintfield(L, t, \"mcode\", T->mcode);\n+    setintfield(L, t, \"mcloop\", T->mcloop);\n     setstrV(L, L->top++, lj_str_newz(L, jit_trlinkname[T->linktype]));\n     lua_setfield(L, -2, \"linktype\");\n     / There are many more fields. Add them only when needed. /\n-- \n2.7.0\n```\n. > fatal: bad object 55c3b29f7b20f3801848e0ab71e9de1d22207b95\nOn reflection I am pretty sure this is user error. I haven't added LuaJIT upstream as a remote and fetched its commits, so there is no way that git subtree could reconstruct the history based on the squashed version in the snabbswitch repo.\n. Looks like the LuaJIT Makefile has a bug that it misses some dependencies that need to be rebuilt after a DynASM upgrade? (If I'm reading the CI error correctly.)\n. @eugeneia @capr: The \"make clean\" compile issue seems to be a bug in upstream LuaJIT. I filed an issue as luajit/luajit#126. I suggest we merge this code but also pull any fix that happens to materialize before the release.\n. Closed: action has moved to #719.\n. One vague point in the document is: when is a port a \"net benefit\" and desirable to merge onto the main branch?\nI imagine that we would want to weight the trade-offs. Some ports are more desirable than others.\nGood things might be:\n- Bring more of our target users (network operators of all shapes and sizes) into the community.\n- Bring more developers into the community to work with.\n- Rewrite ugly non-portable C code as neat portable Lua/ljsyscall code.\n- Better define interfaces towards primitive functions (e.g. PCI device access and DMA memory allocation).\n- Make new applications possible with great hardware.\nNeutral things might be:\n- Support platforms that aren't relevant to network operators (e.g. Raspberry Pi, OSX laptop).\nBad things might be:\n- Create many more lines of code to maintain.\n- Globally complicate our programming style: make everybody spend brain cycles thinking about POSIX correctness, race conditions due to relaxed/weak cache coherence protocols, additional memory alignment restrictions, etc.\n- Create technical/practical/social barriers to writing awesome optimizations for important platforms (e.g. Xeon).\n- Create pressure to replace simple/snabby tools (like DynASM) with complex/unsnabby tools (like LLVM).\n- Make our code uglier e.g. #ifdef.\n- Make our build process uglier e.g. autoconf.\nQuite a list, eh?\nThe example to follow would seem to be LuaJIT itself. LuaJIT has managed to retain its character and style while supporting very many platforms: Windows, Linux, BSD, OSX, Playstation, Xbox, i386, x86-64, ARM, PPC, E500, MIPS. This has required extreme dedication to the maintenance -- for each architecture Mike has written an assembler and used that to write the virtual machine in assembly language by hand. This, in my mind, is the kind of work that we are signing up for when we start accepting new platform ports into Snabb Switch.\n. Great feedback!\nI rewrote porting.md to be less wishy-washy and defend our lovely x86-64 geekout fest. How does it look now?\n. @wingo Good idea to error rather than warn on unsupported platforms. Adopted.\nI am happy with this branch now and would like to start upstreaming it now please.\n. > For me, LGTM, I'm happy to merge into wingo-next.\nThanks! I set you as Assignee as a proof-of-concept to see that this is really possible.\n\nProcedural question: you OK with me resolving the merge conflicts?\n\nYep. I'm thinking that merges need to be resolved at the point they occur. I imagine that mentionbot will likely poke everybody involved in the conflict when you PR your branch to the next hop and so we will have a chance to check the resolution if we like.\n. @eugeneia your max-next branch is now automatically being synced to the snabbco repo so that it is visible in the branch list.\nThis is done by a shell script of mine. That's a bit ad-hoc. Can be that we could push to these branches directly instead of having them indirectly sync'd now that Github supports \"protected branches\" that block force pushes. (The main reason I setup the script to synchronize is so that people can rely on these well-known branches to not be rebased without notice.)\n. Yep!\n. Closing in favor of #705.\n. @eugeneia You are right! I had misunderstood Mike's response as saying that we are stuck with this problem, but really yes he did add a new fix that we could pull from the luajit v2.1 branch: https://github.com/LuaJIT/LuaJIT/commit/2f6b2967c7312d867890df158fe6e0988fda3854.\n. > The CI only has two states: success / failure.\nYes - but - there can be multiple CIs and the admin can say which ones are important enough to block merges. Ref: Github: Protected branches and required status checks.\nJust clarifying for future discussions, not meaning to push this separate-build-upgrade-CI idea now.\n. YES!\nSee also luajit/luajit#115 which has a bounty attached.\n. Thinking about this a bit more...\nCan be that the snabbco/luajit#snabb repo is a mistake and that it is better to pull changes directly into the snabbswitch repo with git subtree pull. That way the changes will pass CI before they are merged. (If we did have a snabbco/luajit#snabb branch it could be strictly a downstream export of the code in the snabbswitch repo so that it is available to others.)\n. Closing: see that we have a working solution in #724 with using git subtree pull from multiple upstreams directly.\n. @alexandergall what would it take for you to adopt this for the learning bridge (#638) as a step towards consolidating our table structures?\n. (See also #718)\n. @eugeneia @andywingo Github Pull Requests are unfortunately ambiguous. People open a PR with a specific intention, but what is that intention? (To get code accepted upstream ASAP? To get early feedback on a design before going forward? Just to be social and show the code they are working on?). So it is easy to rub each other the wrong way e.g. don't bother to merge something that you assume is a draft, or give \"I can't merge this until X,Y,Z\" feedback on code that is not prepared for merge.\nI'm not sure how to reduce this. I tend to put hints in my PR titles like [draft] to mean \"I'm probably going to throw this away and rewrite it\" or [wip] to mean \"here's something I am working on that I don't want to be merged yet\".\n. @andywingo @alexandergall you guys might want to talk cycles because I think you're testing with much different CPU clock speedw\n. @wingo Thank you for doing this work and sending it upstream!\nThe \"need a big table for FFI keys/values\" problem has been with us since pretty much the beginning of Snabb Switch, way back when @plajjan did his DDoS protection prototype. Since then we have tried a lot of different things but never found a really general solution: until now it would seem.\nI think the code is extremely stylish btw :-) both on this PR and also the asm code for searching the table.  I would not have guessed it would be possible to have a sophisticated optimized hashtable in so few lines of simple code.\nFor my part I had experimented with bindings to a few off-the-shelf libraries (\"Judy arrays in 1250 lines\", khash, a patricia trie) but was not satisfied with any of those and gradually coming to the conclusion that what we really wanted was something almost exactly like what you have written here.\nIn the back of my mind I wonder if some day we could unify our hashtables with the builtin table support in LuaJIT -- without mixing up FFI data with the Lua heap -- but I have no idea if that is practical and it seems like something for later if at all.\n. re: lib.ctable.ctable this makes me crazy too. There is actually a standard Lua idiom for avoiding this i.e. when you require(\"foo\") it first looks for foo.lua and then for foo/init.lua so that you can transparently implement a module in several files. However, I don't believe this nice feature is available in LuaJIT when working with Lua modules that are linked as object code. I hope that we can clean this up in #734.\n@wingo Rebasing open PRs creates problems for me. I can't really follow the review on Github, e.g. Kristian's comments have disappeared, and it would also burn me if I had merged this branch onto one of my own branches and now wanted to pull in the latest updates. This is why git-workflow.md requests that branches not be rebased after the Pull Request is made. (I do think we should make an exception for PRs named with the prefix [sketch] though... perhaps all of these tips/requests should be migrated into CONTRIBUTING.md?)\n. @plajjan This seems like the \"centralized rebase-oriented workflow\" vs \"distributed merge-based workflow\" debate. If people want to switch Snabb to a rebase-oriented workflow then a PR to git-workflow.md would be the way to propose it.\nHowever, I would ask people to accept that I am trying to establish a specific workflow here, the one that has been very successful in the Linux kernel world, and to help me by embracing this and figuring out how to make it work well for us. I am sure we will want to make a lot of adjustments over time but they should all be in the same spirit.\nJust a few notes based on my mental model of the merge-based workflow:\n\nWhy would we want Snabb git history to reflect spelling mistakes.\n\nThe same reason you would want your bank statement to show if you paid the wrong amount for something and then corrected that with additional transaction(s). In a merge-based workflow the git history is a ledger keeping track of which commits are in each branch and git uses this information directly when you ask it queries like \"do I have all the changes from branch FOO?\" or run merge operations like \"catch me up with the latest changes in branch FOO\".\nThere should be ways to elide this information when you are not interested in it e.g. run git log with the --merges option to compress the history into a higher-level view. If people want to see the Snabb history in a different way then I am certainly up for helping to find the right git incantations.\n\nI don't think you should base your work on random branches on the Internet.\n\nRight, you need to know what to expect from the branch. I expect that branches PR'd to the main Github repo will be developed with a consistent forward-moving history (unless clearly marked otherwise). Being able to rely on this is powerful, for example Alex could have merged this ctable into his ALX branch and shipped it in a product without waiting for consensus on the API and for documentation to be available.\n\nwhere it's obviously just to get things in order to quickly merge on master\n\nThis is a red flag to me. There should not be any urgency to landing changes on master, that would be a bottleneck and a scalability problem. Changes should already be available to users directly from the feature branches where they are maintained. The master branch represents the \"trailing edge\" of changes that are mature enough to propagate to everybody.\n\nPublishing a branch for a PR is no contract... :)\n\nPeople need to know what to expect from each other. I have tried to spell out the expectations for the master branch in git-workflow.md. These may need to be written better, or migrated into a CONTRIBUTING.md, but I certainly expect downstream branches to carefully consider the expectations of their upstream branches in order to make things run smoothly.\n. @plajjan ... oh now I can see your comments. I had clicked on the \"outdated diff\" link but didn't realize that the text of your comment was there if I scroll down below the diff. That is nice i.e. information is not being lost.\nCan be that rebasing makes sense in these situations but I still want to understand how it fits into the broader merge-based workflow so that the master branch does not become a bottleneck.\n. @wingo @plajjan Coming back on topic, here is the sort of Git workflow that I would ideally like to have for ctable:\n\nThat is:\n- The ctable feature is developed on a feature branch that is maintained while the first version of the code is being developed.\n- The applications that care about this functionality, for example lwaftr and ALX, merge the change directly as soon as it is working well enough for their use cases. This is when testing is done and optimizations/fixes are made.\n- Once the code has been battle-tested sufficiently a subsystem maintainer merges it onto the data-structures branch for broader integration. This is when the code is fit into the rest of the system e.g. replacing existing code that is rendered obsolete such as mac_table.c, adopting ctable to backend the conntrack module instead of Lua tables, etc. This is when we cleanup the code and make sure the net result is simplification (less special-purpose table code, clearer solution for the next person who needs a table, etc) rather than obfuscation (yet another hashtable that lives in parallel with the existing ones, confusing picture for the next person who may decide to play it safe and write their own again, etc).\n- Once the code base has been updated sufficiently to have \"internalized\" the ctable feature then this can be merged from the data-structures branch onto next for general release.\nI see a lot to like about this picture. The missing pieces I see now are (a) we don't have a data-structures branch that somebody is lovingly maintaining and (b) it is not clear whether it is safe for an application like ALX to merge this ctable branch i.e. whether this is a stable branch that will be maintained for the lifetime of the initial version of the feature or whether it is a throw-away branch that will be rebased at will.\nEDIT: I drew only next in the diagram but max-next could serve just as well or better.\n. @wingo oh sorry for flaming you for my own lack of understanding of the Github PR UI :-).\n. @wingo Maybe it is time to take the discussion: Could we merge the whole lwAFTR branch onto master? \nYou would have the program/lwaftr/ folder for keeping all code that you currently consider private to your application e.g. load/save of tables to disk and dynasm lookup routines. If other people want to use this code they could make a PR to \"promote\" it up to core/ or lib/ e.g. when refactoring the code so that they can use these libraries in their own application. You could chime in there with your feedback (\"Awesome! Great to see it adopted!\") or (\"We want to keep this private! We have lots more application-specific things we want to try out!\").\nI don't think this would require much changes to the maintenance workflow that you are already using today. One thing is that you would need to keep application-specific code in program/lwaftr/ (including stuff you just can't be bothered arguing in review about at the moment) and another thing is that people could propose changes to the program/lwaftr/ code that is on master (e.g. as part of a major refactoring effort or just to add a feature) and the maintenance workflow would need to make sure you sign off on these before they are accepted.\nThe benefits I see are:\n- Get lwaftr development closely in sync with the master branch.\n- Put lwaftr code in plain sight where people can borrow ideas, propose promoting certain code, update lwaftr code as part of sweeping refactorings (like the \"straight line\" change we did a year ago).\n- Take these \"upstreaming\" discussions in the context of refactoring (move code from lwaftr/ up to lib/) rather than \"from scratch\" based on code that is in limbo on a feature branch that nobody has merged.\n- Make it easier for the upstream CI to run functional and performance regression tests for the lwaftr application.\n. > At one early point I had a vision of landing the thing nicely, in a dozen commits or so, all of which making sense on their own: but maybe that is an unattainable thing :)\nI know what you mean. For what it is worth, this is not so important from my perspective. I find it most productive to think of Git as a tool for keeping track of content and the commit history as a means towards this end. I suspect that this is the way Linus intended it (\"git - the stupid content tracker\"). So if the first version of lwaftr would come as 500 commits, or as one squashed commit, etc, that is not a big deal for me personally. I have accepted that I will need to learn to query the git history like a graph database in order to make sense of what is going on anyway :-).\nThe LuaJIT patch is a very interesting one. I would be tempted to bring that onto master because it's written and maintained by people who understand compilers, it is directly needed by one application (so far), and because the LuaJIT upstream situation is still in flux and some degree of self-sufficiency in the Snabb community seems pragmatic. The main cost I see is that if we want to report bugs to LuaJIT upstream we probably need to separately make sure they are reproducible without our own changes (but creating separate minimal test cases seems to be normal practice for reporting bugs upstream anyway).\nI do see that there will be situations where an application genuinely does need to adopt and ship changes that are not immediately acceptable to the master branch. This will disrupt the merge workflow. I am not sure to what extent we can anticipate and avoid these problems vs deal with them as they arise. So long as landing code on the master branch is not on the critical path for applications to make their releases I feel like this should be manageable.\n. @plajjan Thank you for patiently sharing your perspective and stimulating discussions on the Git workflow :). I do want to observe that in the Linux community there is actually a lot of rebasing done at the edges of the network before changes are accepted by the first hop and so it may be that we will have more rebasing in the Snabb future too. I mostly want to avoid rushing into this too early in case it disrupts building a distributed workflow (which seems like a pretty delicate operation to begin with).\n. @plajjan @wingo Surely LuaJIT must have some nice hashing functions in the C code somewhere. I wonder if we could call those via FFI as a fall-back for cases that don't have a hand-written hash?\n. @wingo makes sense, thanks for clearing that up!\n. @wingo I would like to try using ctable to backend the packet_filter.conntrack module.\nThere are two challenges. One is that it mixes two sizes of keys in the same table (IPv4 vs IPv6) but it should be straightforward to resolve this by using separate tables. The other is that the keys are larger (37 bytes for IPv6 connections and 13 bytes for IPv4 connections) and I am not sure the most straightforward way to hash those? (The hash is actually over multiple separate fields in the packet header but the code currently copies those into a linear structure and then hashes that like a string.)\nThese are the definitions of the keys:\n``` lua\nffi.cdef [[\n   typedef struct {\n      uint32_t src_ip, dst_ip;\n      uint16_t src_port, dst_port;\n      uint8_t protocol;\n   } attribute((packed)) conn_spec_ipv4;\ntypedef struct {\n      uint64_t a, b;\n   } attribute((packed)) ipv6_addr;\ntypedef struct {\n      ipv6_addr src_ip, dst_ip;\n      uint16_t src_port, dst_port;\n      uint8_t protocol;\n   } attribute((packed)) conn_spec_ipv6;\n]]\n```\nThere is some very basic performance test available that could be sanity-checked for impact on lookup speeds i.e. how many packets are processed in one second:\n[luke@lugano-1:~/git/snabbswitch/src]$ sudo ./snabb snsh -t apps.packet_filter.pcap_filter\nselftest: pcap_filter\nRun for 1 second (stateful = false)...\nlink report:\n             757,532 sent on pcap_filter.output -> sink.input (loss rate: 0%)\n          20,327,070 sent on repeater.output -> pcap_filter.input (loss rate: 0%)\n                 161 sent on source.output -> repeater.input (loss rate: 0%)\nok: accepted 3.7267% of inputs (within tolerance)\nRun for 1 second (stateful = true)...\nlink report:\n             724,611 sent on pcap_filter.output -> sink.input (loss rate: 0%)\n           9,721,875 sent on repeater.output -> pcap_filter.input (loss rate: 0%)\n                 161 sent on source.output -> repeater.input (loss rate: 0%)\nok: accepted 7.4534% of inputs (within tolerance)\nselftest: ok\nThat code uses Lua dictionaries with string keys and I don't think that will ever provide such robust performance as ctable (but maybe competitive when tested with few different keys).\n. @eugeneia I see it as a high priority to bring programs onto the master branch as much as possible. This is something that would be done voluntarily by people developing those programs to make their own lives easier. It is our job as upstream to \"sell\" this concept so that people actually decide to do it.\nThe reason this is important to me is that I think it is very powerful to have a common code base. This makes it possible to make \"global\" changes to the software like the \"straightline\" redesign or rewriting the standard hashtable or ... lots of interesting things to keep the software moving forward. I see lots of this kind of work on the horizon e.g. making programs work with any I/O interface instead of depending on an 82599 NIC, making programs parallelize in consistent ways, and so on. I would much prefer to tackle these problems with all of the relevant code \"on the table\" rather than tucked away in branches and without upstream test coverage.\nNow, if this concept is going to succeed, people will need to actually want to host their software on the master branch. This will mean staying in control of their code and not having to worry about undue interference from gatekeepers (i.e. us) between themselves and their users. This will require establishing definite expectations about who is responsible for what.\nI have discussed this privately with @alexandergall in the past in the context of his VPWS/ALX application. The basic model that we came up with was for applications to have their own private directory trees (program/foo/) where they have a large degree of autonomy. There they can make their own rules: use camelCase, ALL_CAPS_MODULE_NAMES, comments in Chinese, etc. They only need to avoid negatively impacting the build e.g. blowing up the size of the binary or the compilation time (which could also be solved by creating separate Makefile targets for them).\nThe expectation is that the code outside those directory trees would be shared between all the programs and we would maintain that together in accordance with the evolving community standards.\n. @eugeneia really we are running the major risk of blowing that whole scheme here. Andy is trying to submit some valuable code and I am holding it up by talking about elaborate Git workflows and you are holding it up by cataloguing mistakes in the README. If the downstream branches see engaging with upstream as having net-negative value then people will not bother sending us code and we will become irrelevant.\nI think that we need to re-style ourselves as humble servants rather than gatekeepers :-)\n. This ctable change is looking extremely polished to me. I am looking forward to getting it on a feed to next :).\n. @wingo You have already done so much. Somebody should update the conntrack module but I think we can take this asynchronously and it need not be you if you don't feel inspired to do so.\nI'm assuming that this change is targeted to max-next and then from there to next but that's really up to you guys.\n. Glad to have this change land on next. Note that there are infinitely many opportunities for everybody to further improve this code and documentation with follow-on PRs.\n. What systems are these? See also #712.\n. The master branch does not accept this kind of change. There need to be separate posix, arm, ppc branches where somebody is maintaining ports. If ports work well and have users then we can plan for merging them onto master.\nThere could also be a branch for collecting general portability patches but again this is not master.\nMore on #712.\n. I agree that the magic number is bad and it would be better to avoid that.\nI am not wild about using a GCC extension to do this though. I want to rewrite as much of the C code we have as possible in Lua based on ljsyscall and adding new C code that is potentially hard to rewrite seems like a step in the opposite direction.\n. @nnikolaev-virtualopensystems Thank you for your understanding :). I hope that we will have a portability branch in the future for collecting these changes (and that once ports are working well we could also merge them onto master).\nIn the short term I'd like to come up with a tagging scheme on Github to make these portability fixes easy to find for the day when somebody decides to collect and integrate them on a branch. There have been about half a dozen such changes submitted to master. Working this out on #725.\n. > I think this patch is worth merging, for what it might be worth.\n@kbara My beef about merging this as an improvement for x86-64 is: shouldn't we rewrite this C code in Lua? I want to use C very conservatively so that Snabb hackers need only basic C skills. So I am not enthusiastic with replacing a magic number (okay, lame) with a new GCC trick that I don't really understand using an advanced compiler feature that I have never seen before.\n. @kbara Historical context is that most of the C code in Snabb Switch should never have been written: we should have written it in Lua with ljsyscall. Unfortunately, we didn't use ljsyscall back in the beginning so every function that made non-trivial use of system calls or the standard C library had to be written in C and called via FFI. This C code then has a natural tendency to spread e.g. because an extension is needed and it is inconvenient to do in Lua because of the FFI barrier so we add it to C instead.\nI really want to weed out this code. I don't feel that there is a place in Snabb Switch for C code that could have been written in Lua. I dislike the way the smattering of C code that we have is consuming brain cycles on talking about POSIX compliance and so on. I'd much prefer to work with tools that don't require such a level of language-lawyering to use effectively.\n. Great hacking @eugeneia and @capr! This was a complex piece of Git'ery and a truly unprecedented level of interest in a DynASM update :-)\n. Sorry to write this idea up with so many words!\n. > One more concrete question: Is the intention for all commits to go through max-next? I have been making my PRs against next. I'm happy to change, though I wonder if we're not putting too much of a burden on @eugeneia to have to be the sole reviewer for all things Snabb.\nWe are bootstrapping a graph/tree structure for merges (like the Linux kernel has).\nHere is how it should be:\n- PRs are \"sharded\" across target branches based on their subject matter (driver change, doc change, portability change, collection of changes already accepted by a particular person, etc).\n- Target branch has one responsible person who reviews/tweaks/merges changes and then sends them in a new PR to the next hop upstream (e.g. next).\n- The graph scales up by adding more nodes i.e. branches that are being maintained by somebody. These could be leaf nodes (where new PRs are sent directly) or internal nodes (like next that exists to combine downstream nodes).\nHere is a picture of how this could look in practice one day:\n\nIn that case new PRs are entering the flow on the right-hand side and flowing with the current towards the master branch.\nHowever, we are not there yet and we need help from people who will volunteer to maintain such a branch!\nLast year we had:\nmaster <- next\ni.e. just me deciding what to merge and when. This kind-of worked before my second child was born :) and then I became overloaded and releases did not flow so smoothly.\nThe struct as of this month is:\nmaster <- next <- max-next\nwhich is one step forwards. On the one hand the same bottleneck exists (but it's Max instead of me) but on the other hand there are two of us approving all of the changes now and that will hopefully decrease the work we each do (we'll see...)\nHowever: we really do need more maintainers who are willing to have changes on a particular topic PR'd to them that they will review/merge and then push onwards to the next hop (e.g. next or max-next).\nHelp wanted :-)\n. ... well another nice aspect of having both next and max-next is that allows the graph to grow in more ways i.e. somebody could agree with me to push changes to next or they could agree with Max to push changes to max-next. And once those agreements are made then it can continue to grow with people individually making these arrangements.\nThis is my understanding of how the kernel (and also e.g. QEMU) scales its development.\n. @eugeneia @wingo re \"targetting\" there are two independent issues:\n1. Which branch do you base your work on. This should be master if possible i.e. a stable common ancestor of all other up-to-date branches.\n2. Which branch you send your Pull Request to. This seems more social than technical i.e. signals who you are asking to review and merge your changes (which are based on master, not that person's own branch, except when exceptions are needed).\n. Great pointed questions, @wingo!\n. @wingo some responses. and note that I am learning as we go along i.e. we are incrementally adopting practices from the Linux kernel workflow as we need them to scale up.\nThoughts:\nThe \"where the next release is being debugged\" branch is next. Changes should land here quickly for testing and integration with each other. That has not been the case recently and this is the acute problem we need to fix now.\nFor many people I think it makes life easier to be able to base their changes on the master branch (a stable base) and have somebody else worry about merging it with other changes happening in parallel. That's why I recommend it as the default behaviour for people adding features.\nI believe Linus also asks people not to pull the tip of his tree into their development branches and to prefer building on a stable release tag. The master branch serves this role for Snabb i.e. you can assume that whatever you pull from master is the latest release that is recommended as the basis for new work.\n\nnow it seems we have serialized the review process through first {Max then you, topic branch then next}, instead of parallelizing\n\nI see this as N-way parallelism where N=2 and one node is reserved for coordination :-). I would like to solve this by increasing N i.e. having more maintainers.\nI also hope that we have pipelined the merge process already. Max is reviewing/merging changes more quickly than I did, which is fantastic, and I still get an editorial voice on \"Snabbyness\", which I think is important at this stage of the project's evolution. So I hope that the max-next -> next -> master workflow will mean changes moving forward more smoothly than last year. However, there is a learning curve now to adjust to the new workflow of dealing with changes in batches.\nI do think that this works well once the working habits are established. In the Linux kernel community it seems like most of the time when somebody says \"Please pull these 20 new features from my new-networking-features branch\" the reply is \"Applied, thanks.\" The other cases where discussion is needed are the ones where people are getting into sync on their communication styles / expectations / etc.\n\nI am happy to maintain a topic branch\n\nThis would be awesome!\nSo now we are three volunteer maintainers: @lukego, @eugeneia, @wingo (and maybe somebody else will raise their hand).\n\nbut I don't see how my work would avoid conflicting with Max's\n\nThe basic solution I see is sharding i.e. quickly assigning each incoming PR to one maintainer who will be the immediate \"upstream\". (Then we also need a default upstream for PRs that don't match somebody specific: that used to be me but now it's Max.)\nThe kernel resolves this by having the MAINTAINERS file that divides up the source tree into subsystems with their own maintainers. Changes to a subsystem are sent to the subsystem maintainer, who is the best qualified person to evaluate and merge it, and they then send it upstream to their next hop. (The kernel seems to have around a thousand such subsystems defined.)\nThis could work for us too? i.e. to define an unambiguous dispatching procedure to decide who of us will be upstream for an incoming PR and then have that person review/merge it and send it onwards?\nI would be happy for my next branch to be the upstream for both max-next and wingo-branch-that-is-yet-to-be-named. I think that will keep me more than occupied and I would prefer not to be direct upstream for incoming PRs. (I will still check them out and comment on them, like everybody else in the community is welcome to, but I'd prefer for changes to pass through a subsystem maintainer hop before I merge them.)\n. Here is the list of all pull requests. I wonder what would be a useful way to start slicing them into separate topics?\nOne idea: new features, extensions to existing features, bug fixes.\nI'd really like to have a \"safe bug fixes only\" branch being maintained. This would have extra advantages:\n- Could be merged directly onto master (bypassing next) to ship bugfix releases like 2016.01.1.\n- Could be browsed and cherry-picked by people maintaining long running branches that don't sync so closely with master e.g. LTS release of a Snabb based application.\n. @wingo one more point in this diatribe :-)\n\nI don't know how to test my changes to make sure they are correct\n\nThis is the job of CI. PRs can include unit tests, functional tests, and performance tests. These are all automatically checked for regressions on every hop from branch to branch.\n. The idea is to build a robust distributed workflow, like the internet. Each of us will be like a router that takes a packet (feature branch), checks it according to our own local rules/ACLs/etc (review), and forwards it to the appropriate next-hop (pull request).\nIf a router is overloaded or misconfigured or misbehaving then we work out a solution and workaround. Some sites may not be connected due to lack of peering agreements (e.g. master and partial-port-to-platform-foo) but this will improve over time.\nThe result should be a large system that behaves well in the presence of errors of all kinds.\n. @wingo Could be that we can avoid the feeling of artificial ownership in some ways...\nI believe that Rust have a system of randomly assigning PRs to upstream reviewers. There's also the @mentionsbots that automatically links in other reviews.\nHowever could just be that the term \"owner\" is a misnomer here. Really we are talking a person who \"does the review process\" including reaching out to people whose feedback is important, making sure there is enough (but not too much) time for comments from the community, etc. The usual open source maintenance work.\n\"Ownership\" could also be reduced by dispatching on something other than filename e.g. fix vs. enhancement vs new feature.\n. @wingo @eugeneia one last thought for the day:\nWe could also steal an idea from the Rust community and have SnabbBot randomly assign a reviewer from a pool when a PR is submitted to the master branch. This would avoid the ownership issues and make it easy to become a reviewer. This could increase reviewer diversity, both in having more people doing reviews and also having each person reviewing more of the code base. I definitely agree with @wingo that we want to be as inclusive as possible and draw more people into the maintenance process.\nIf reviewers are newbies then their next hop could be to a more experience maintainer who can help them out.\nThis also fits the internet/router analogy :-). It's like ECMP (Equal Cost Multi-Path) where a router has multiple equally good next-hops and it spreads the load across them.\n. > I really do lament taking up your time on these procedural things. \n@wingo Not at all! Thank you for engaging in this discussion. We need to bootstrap a network of maintainers who work well together and know what to expect of each other.\nLast month there was one upstream (@lukego), now there are two (@lukego and @eugeneia), and likely there will need to be 5-10 before it is obvious to everybody how the whole system works.\n\nI like this last idea. Say, a reviewer will be randomly assigned for new tickets if no one is @-tagged in the PR description. They can be assigned by @-mention or by the assignee field; either way. Of course an assignee can pass the buck to someone they feel is more appropriate. I would be happy to maintain such a branch.\n\nI like this idea too. If you want to create a branch and maintain it in this way then I think that would be cool (and would volunteer to be a reviewer :)). This should be no problem given our decentralized workflow with independent branches maintained in their own ways. The only question would be how to route changes in and out of that branch. We could start small and then grow.\nThe more I think about it the more I see that we are building a network like the internet.\nAdding a branch is like adding a router. Making a branch feed into another branch is like running a cable between two routers. Deciding how changes should flow between branches is routing. Deciding exactly how to perform review is like deciding how to engineer one of the routers (which is a black box to most people).\n\nknow that when merged that my code would have a particular effect or not? I especially wouldn't want the person doing the merge to have to fix things up, because they might make a mistake. I would prefer them to reject the merge in that case!\n\nLet me paint a picture of how I see this working in the Linux world...\nSuppose that I am shipping a networking product based on my own branch of the Linux kernel. I decide to optimize the Intel device driver by changing the descriptor prefetch options (like #628). I test this change with my product and ship it. I also send the change to be reviewed and merged upstream:\n1. I send a PR to the intel-driver branch. This is reviewed by one or more people who are familiar with this specific hardware. They may find a very specific bug e.g. relevant errata that I had overlooked. With luck they accept the change into their branch and take responsibility for pushing it upstream. Now my job is done: other people (kernel subsystem maintainers) will take it from here.\n2. The intel-driver branch is PR'd to the broader ethernet-drivers branch. The maintainer here may spot something relevant: should other branches consider making this change too e.g. for Mellanox and Broadcom NICs?\n3. The ethernet-drivers branch is PR'd to the networking branch. The maintainer here may spot something too: maybe I am making an unsafe assumption about how much memory I can allocate for my descriptors and more checking is needed.\n4. The networking branch is PR'd to the master (Linus) branch. Linus may spot something e.g. that the change is likely only to be beneficial on a subset of the CPU architectures that this card is available for and that more testing may be needed before globally changing the default behavior.\n5. Linus merges the code onto his branch and then it will propagate to the rest of the world: Redhat, Debian, Ubuntu, etc.\nThis workflow seems reasonable to me. I see value in having a chain of people who specialize in one kind of review. I see each maintainer in the chain adding value. If I had the option to bypass this chain and push my patch directly into Linus's tree I would not choose to do that instead.\nOne important aspect here is that I already shipped my product long before the change landed on the master branch. I also only really cared about the feedback from the first hop i.e. the NIC expert who tells me whether I have overlooked some crucial detail. Ideally I want that interaction to be fast. Beyond that I am interested in upstream for the purpose of \"eventual consistency\" i.e. to minimize the changes that I have to maintain on my long-running product branch.\nIf I were actually dependent on the feature to be merged by Linus before my customers could use it then I would be pretty frustrated -- but that is not the model. (Other people do have this problem though e.g. NIC hardware vendors who have to push drivers upstream into Linux before the hardware is debugged so that users will have a driver installed when they purchase the cards. I don't envy them.)\nHey, reflection, we could talk about this much more concretely in terms of the branch that you Igalians are already maintaining i.e. lwaftr. This branch has a well-defined purpose (developing the snabb-lwaftr program) and a smooth workflow where PRs are being created, reviewed, merged. This branch does effectively have ownership of that application i.e. this is the branch you would send a PR to extend snabb-lwaftr and this is the only branch that people would want to pull snabb-lwaftr from. The only thing missing is connectivity: there is no uplink to send changes from this branch out into the world and towards master.\nHow about \"connecting this up to the network\" and sending Pull Requests to some suitable place like next or max-next?\n. @wingo Yes, we will need to deal with all of these things. I think that we should pay close attention to how the Linux kernel handles them because that is the model for the Snabb git workflow. This will be a continuous process as the Snabb universe expands and our maintenance processes need to keep up with expanding activity across more products/branches.\nPlease also remember that it is always frustrating to switch between tools and to find that the way you are used to solving a problem doesn't work anymore. The first impression of everybody adopting a new workflow is going to be \"well, this sucks\" and this in itself doesn't tell us much about how well it will work over the long term :).\n(Same way I am now cursing Nix/NixOS every time I struggle to do something that I usually do with apt-get or ./configure && make install but now requires a different approach.)\n. @wingo commenting on the specific example you mention: git revert is useful here. This backs out a change by applying an inverse patch. The merge commits can actually help here: when reverting you can decide on the granularity of the revert (just one commit? or the whole feature-branch merge that introduced it into the code? or one of the parent merges that propagated the change?)\nThis is messy to be sure but it is doable.\nFor example here is a case where I temporarily reverted a premature upgrade of pflua that revealed a bug in LuaJIT: https://github.com/lukego/snabbswitch/commit/9f5f1ad5cd9d45c9676ea7405f8c2eeaecb0ba36. Once LuaJIT fixed the bug and we pulled that into our repo I restored the pflua upgrade by reverting the revert: e7c8d6d1baccc1be7f6966e98d0b1a3f16e683c9.\n. @wingo QA and release engineering is a really interesting open topic btw.\nI have a couple of different models in my head for how it could work:\nCentral: Snabb Switch is distributed as one snabb binary that includes all applications: packetblaster, lwaftr, nfv, alx, etc. These are all released at the same time e.g. monthly. The developers of these applications work very closely together to ensure that a new release never ships with changes that break one of the applications.\nDistributed: Snabb Switch is distributed as separate binaries (snabb-nfv, snabb-lwaftr, snabb-alx, etc) that are each built from their own branch and according to their own QA processes and release schedules. The master branch exists to help application developers cooperate i.e. to synchronize their source trees and run standardized tests.\nThe second option seems appealing to me i.e. that each application developer is responsible for the QA and release schedule of their software, and has veto of every change that they will ship, but that we all benefit from keeping development focused on the master branch because 90% of our interests are in common. In the simplest case there would be no changes on the application branch compared with master but the owner would still decide when to make a release, what to call the release, and what QA to do beyond the upstream CI.\nThis workflow could also give application developers flexibility e.g. if Alex wanted to ship an ARM port of his VPLS application before other people were comfortable with merging that architecture support onto their own branches.\nI am not sure how this plays out in the kernel world. I have the feeling that people don't generally consider Linus's releases to be ready for installation. I assume that people shipping products downstream (e.g. Google with Android) are doing additional QA and engaging with upstream on any problems that they find. (I am out of touch... I remember that in the old days Redhat maintained their own kernel tree and often rejected changes that Linus had accepted but maybe nowadays they are so involved in upstream that they just ship that. I'll have to have lunch with a kernel hacker some time soon to ask about how this all works nowadays.)\n. @wingo Tangentially...\nI have found it fascinating to participate in the OpenStack upstream community for a couple of years. They use a very high-tech centralized workflow, based on Gerrit and so on, and this has become a train-wreck at scale.\nThe main problem I have seen in the OpenStack world is that there are many different special interest groups inside the community who are all standing on each others' toes. There is also a lot of politics (literally elections for each subsystem) and the outcome of this politicking decides who is able to ship a product to their customers.\nFor example in the time I was working upstream I had open hostility from the gatekeepers because I was associated with an unpopular interest group (telco) who were being pushed out by the people in charge (enterprise). If I have any problem the answer is always \"my god, you telco people, you just don't understand open source, I already wasted the whole of last week talking with big telco vendors\".\nThe general situation was that telco-motivated changes could not be merged anywhere because they were not welcome on the master branch and... no other branches exist because it is a centralized workflow. You can't even git merge the feature branches because Gerrit requires them to be constantly rebased against master while they sit in review-limbo. So there was no meaningful cooperation between any of the people with a mutual interest in working on telco because the workflow does not allow them to collectively drift out of sync for a year or two to develop their ideas and then rejoin the mainline.\nI believe there was a similar situation in the Linux world with Android.  The Android people put a lot of resources into development but they were very firmly focused on getting a product to market to compete with iOS. They made a bunch of changes that the upstream project refused to merge. However, it all worked out: they drifted out of sync, shipped a competitive product, and then once they were successful they put in the work to reconcile their changes with upstream and make everybody happy.\nThere is a related situation in the Snabb world now on a small scale. People are sending portability patches and I am declining to upstream them onto the master branch. However, I see the workflow supporting this in the same style as with Android: people who want portability can develop that on a branch, doing whatever makes sense for their immediate objectives, and then after the port is working well we can figure out how to sync it with master and other branches. This way nobody is blocking anybody else and we are all operating asynchronously.\n. Great feature!\nOften we have wanted to test with a different packet size, or specific mixes of packet sizes (e.g. to have variation but maintain a definite average size), and it is really primitive to do this by creating new pcap files all the time. Much better to be able to do it on the command line with a series of packet sizes.\nThe Synth selftest method looks like it may depend on the app outputting exactly the same number of packets when executed for a very short time period? That seems like it could break in one of two ways: first that we could change the size of struct link so that it would output a different number of packets per breath, and second that the engine might run the app for multiple breaths. Can we make this more robust somehow? (Just to avoid confusing the next person who changes the size of a link or the engine event loop behavior and sees this test start failing.)\n. @eugeneia I would prefer to see max-next passing tests so that I can merge the changes that are already there. I'm not sure what the optimal workflow is for releases but I feel like I should be merging max-next onto next at least once per week or so. Then we would be using next as a place to land changes as quickly as they can be reviewed by a subsystem maintainer and also by me and other people could test them there.\n. @eugeneia oh it is passing :-) Cool! I merged max-next into next now. Sure, merging this onto max-next seems okay to me, though it seems likely we will forget about fixing the tests if we don't do it now.\n. Howdy @hannibalhuang!\nCool if somebody here in the community is also involved in these projects and can cooperate with you.\nGenerally the Snabb community is quite remote from ETSI, OPNFV, etc. I think technical discussions will work best if we keep the vocabulary familiar e.g. in terms of Snabb Switch code or specifications from more familiar organizations e.g. IETF RFCs.\nPersonally I am not active in any of these projects like OpenStack, OPNFV, OpenDaylight, ETSI NFV, etc but I think it would be awesome if people who are could use Snabb Switch for interesting things.\n. Thanks @wingo and @nnikolaev-virtualopensystems for raising this discussion. Thanks @wingo also for the link, I have read it and it seems very level-headed to me.\nThoughts...\nLicense\nI agree that it is crucial for everybody to understand that Snabb Switch is an apache-licensed open source project, and that contributed code is also apache licensed unless clearly marked otherwise. This is what makes Snabb Switch open source and gives us all the right to use each others' code. The only reason it is open source is that we say it is when we contribute code, and so it is important to be crystal clear and avoid misunderstandings.\nCurrently we spell out these license terms in the top-level file COPYING. It seems reasonable to also point this out in a contributing.md file (that Github would automatically show when creating a PR) and perhaps in source files.\nI can imagine for example having a standard two-line header at the start of source files along the lines that Andy suggested:\n-- This is the <foo> module that does <bar>.\n-- Use of this source code is governed by the Apache 2.0 license; see COPYING.\nin order to further minimize the risk of misunderstandings. I am not really convinced that longer per-file boilerplate is worth the fuss (see Andy's link).\nCopyright\nCopyright notices seem less fundamental to me. We are all using the code under the terms of the apache license and it doesn't make much practical difference who owns the copyright to each line of code. Copyright is not actually assigned by writing a note in a file anyway, and the copyright status can change after the note is written e.g. you might sell your copyright, or you are sued and your copyright is awarded to somebody else as damages, or a former employer challenges your right to the copyright in the first place, etc.\nIt is possible that one day copyright ownership will become relevant, e.g. because we want to unanimously relicense the code under some different open source license, but this will be a huge administrative mess independent of whether comments are available in the source files.\nHaving said that, if people want to make notes about copyright assignment that it would be nice to have an optional mechanism to do so. One way would be to have a top-level COPYRIGHT file where people can write notes like Joe Blogg's contributions in 2016-2018 are copyright Big Co Ltd. It could also be reasonable to put notes directly into source files but, like all source code comments, these will need to be maintained and so it would be nice to write them in a way that makes this easy.\nOne idea could be:\n-- Portions of this file are copyright Big Co Ltd.\nwhich seems easier to maintain when the file is further developed by the author and extended by others than the more traditional:\n-- Copyright 2016 Big Co Ltd.\n. > companies want to put their name out there\nHere are some suggestions for perfectly fine ways for people and companies to be visible in the community:\n- Maintain valuable subsystem branches on a MyCompany/snabbswitch Github fork.\n- Give public presentations about Snabb Switch.\n- Blog about Snabb Switch.\n- Represent Snabb Switch in other communities (OpenStack, OPNFV, ETSI, FOSDEM, etc).\n- Sponsor work on a project that is related to Snabb Switch (LuaJIT, NixOS, etc).\n- Donate important equipment to the lab.\n- Buy Google Adwords for \"snabb switch\" searches.\nThese all seem like valuable/appropriate activities that can be very naturally associated with the person doing them and their employer.\nHowever, I agree that it is negative for the project to inject direct advertising into the development process e.g. by trying to maximize the number of times that other developers type your company name or look at your company logo and so on. This comes across as antisocial to me, like if my neighbour puts up a Coca Cola billboard that I have to look at from my living room window. There are better ways to promote a brand.\nJust a personal opinion:\nI think it is also a reality of open source projects that individuals get a large share of the credit for their work, more so than in less transparent communities where nobody knows who is \"making the sausage.\" This is empowering to individual developers and companies need to accept that.\nOver time as Snabb Switch grows the experienced developers will have better and better opportunities available to them and their employers will need to work harder and harder to keep them. For example, a senior Linux kernel hacker can pick and choose between any number of employers large or small. I see this as a good thing: a virtuous circle. I don't think it would reflect well on a company to be seen as fighting this by trying to transfer excessive credit from its employees to its corporate entity.\n. @nnikolaev: Sorry but I am going to stop typing the \"-virtualopensystems\" suffix on your handle since no reasonable justification for requiring me to was ever given. I think your company should embrace the image of \"home of famous Snabb Switch hacker Nikolay Nikolaev\" as their brand instead of these other funny things.\n. @wingo Thanks again for starting this discussion.\nThe next action I see for closing this issue is for somebody to PR a CONTRIBUTING.md file that captures the outcome of this discussion into actionable advice for people sending contributions.\nTo me it seems like we have rough consensus that adding this line to source files would be valuable:\n-- Use of this source code is governed by the Apache 2.0 license; see COPYING.\nand that we could create a src/COPYRIGHT file to record notes about copyright assignment. The exact purpose and contents of this file is not entirely clear and would need to be spelled out in CONTRIBUTING.md. I imagine that the COPYRIGHT file would be used to write \"notes to our future selves\" in case one day we need to establish the current copyright status of the code e.g. for relicensing, etc. In this case it could be useful to get feedback on the contents from people who have done such things. (I don't think the purpose of COPYRIGHT is to be a proxy for measuring contributions to Snabb Switch and if that is the only reason people want it then perhaps we don't need it at all.)\nGenerally discussions about how best to promote Snabb Switch products and services is a very welcome topic. Everybody who would like to discuss this and help establish community norms is very welcome to create a new Issue dedicated to that subject.\n. @eugeneia Merged! I suppose that we can think of the current contents of next as a release candidate for v2016.02?\n. I am not 100% sure but I think this is just another OEM packaging of the same ethernet controller that we are testing. I would be comfortable merging the patch and considering the card to be supported.\nIf we can establish that it really is significantly different to the NICs we already have then I would be happy to acquire one for the CI.\n. Great! Awesome! Fantastic!\nIf I read this correctly then the proposal is for wingo-next to feed upstream to next and therefore run in parallel to max-next giving us this graph:\n\nI am happy for the next branch to be upstream for both max-next and wingo-next and to work with you guys to make this workflow produce good results that everybody is happy with.\nFirst request: how do we define the \"downstream\" branches that feed into max-next and wingo-next? If both branches will be processing PRs sent to the main Github repo then what is the \"sharding function\" (\"routing table\") for unambiguously deciding whether a given change is for max-next or wingo-next?\nThe Linux kernel has the MAINTAINERS file that defines the \"first hop\" for new incoming changes. We would seem to need some equivalent. I would like contributors to always know what is their next-hop and I would prefer not to pull the same changes from both max-next and wingo-next.\nOne suggestion would be to choose a more specific subject area for one or both of the branches, and rename the branch to reflect the changes that it accepts, but any solution that is unambiguous and agreeable to everybody should be fine.\n. > I was going to initially suggest that the first few wingo-next merges go through max-next\nIf this suits Max then it suits me very well too.\nThen we would have the simple flow:\nmaster <- next <- max <- wingo\nand you two could decide on a good criteria for choosing the \"first hop\" for incoming PRs that makes it clear to everybody.\n\nI think if the committer wants a particular path they can choose to @-tag one of us. Otherwise ideally we get SnabbBot to @-tag one of us at random. Until we have that kind of bot integration, I think the first person that comments on the ticket and says \"I'll take this one\" gets it.\n\nCan we develop these workflows locally within branches?\nI still see us building a network based on the internet here. Each branch is a router, each agreement to pull changes between branches is a patch cable, and each commit being propagated is a packet. The system scales by treating the routers/branches are predictable black boxes that take packets/commits one step closer to their destination. The processing decisions made by the routers/branches are made locally whenever possible.\nI am very comfortable with each branch inventing its own maintenance style, for example one branch being operated by a cooperating group who are assisted by a bot. This is like building a new router that the rest of the world can see as a black box with observable throughput/latency/drops/support/etc. If it works really well then lots of traffic can be delegated to it and other routers could borrow its design.\nI am more cautious about inventing new protocols for coordinating branches. Then the analogy is to more complex mechanisms like OSPF, BGP, VRRP, etc that multiple routers need to support and interoperate with. I feel like it is too early to be dealing with this kind of complexity in the small network that we have for the immediate future (less than 10 nodes compared with thousands for Linux or millions for the internet).\nSo I would request that we focus on innovative workflows within branches but make the interactions between branches as boring as possible. Reasonable?\nI also note that there will be more kinds of downstream branches in the future and a solution designed for handling individual features PR'd to Github won't cover everything. For example program-specific branches for NFV, lwAFTR, ALX, etc could send large PRs to push their latest releases onto the main branch and it may require a different skill set to process these. (I will resist the temptation to make an analogy to routers in this instance as a courtesy to fatigued readers :-))\n. Here are some interesting links from Linux subsystem maintainers:\n- Linux Kernel Subsystem Maintenance\n- Ask a kernel developer: maintainer workflow\nThe picture this paints for me is individuals or groups taking responsibility for maintaining some aspect of the software. Their inputs are simple (e.g. all changes related to GPIO drivers) and their outputs are simple (e.g. publish three branches feeding changes to the testing/unstable/stable upstream branches). The part in the middle is their own domain where they apply thought and creativity.\n. ... having said all of that :-)\nI suppose that the rate of PRs is fairly modest for now and so a \"routing function\" that requires some social interaction may still be efficient enough. Could also be that one cooperatively-maintained branch will turn out to have really good performance and be able to take pretty much all the individual changes that are sent to Github. I may be mis-predicting what will be important for scaling up in the future.\n. Sorry to keep you waiting. Your wingo-next branch is live now. You can see it mirrored on the SnabbCo repo so that everybody should pick it up by default. Note: the mirroring script is not picking up rebases (doesn't force-push) so let me know if it gets stuck and intervention is needed.\n. You should also have access to Issues e.g. to set yourself as Assignee or apply tags.\n. To be clear: you can just push to your own branch and send PRs from there. The mirror of wingo-next on SnabbCo is just a convenient read-only mirror of your branch that is updated each ~1minute.\nI will shift my attention now from writing long blah blah blah diatribes to working on high throughput low latency merges with an acceptable rate of mistakes :)\n. > As for \u201crouting\u201d, I think its too early to have a good answer. There may be some obvious ownership (Snabb NFV, lwaftr) but I hope that we will be able to self-assign on a \u201cwhatever seems reasonable\u201d basis.\nI wrote a braindump on what I think the downstream people submitting changes are expecting: https://github.com/SnabbCo/snabbswitch/pull/737#issuecomment-179727491\nDoes that seem reasonable? If so then every currently open PR should be assigned to either @wingo or @eugeneia as the upstream who will engage with the submitter? (How will we make that happen?)\n. It will be interesting to see a workflow develop between individual contributors and the upstream maintainers :-). Tags on issues are another tool in the maintainer-toolbox that might come in handy.\n. I really need a good way to consume documentation. How do other people do it?\nI really enjoy having everything on Github when I want to send a link to it e.g. on the mailing list. However, I don't browse documentation on Github: I find it very laggy and the search too limited.\nCurrently I am most often digging up documentation in a text editor but this is not optimal when I have to open a separate readme file and read in markdown syntax.\n. Having an up-to-date easy-to-find single-file manual sounds fantastic to me.\nIdeally this would be built by the CI and trivially easy to refer to so that it is obvious to everybody whether their new change is keeping the documentation consistent or not. Could SnabbBot perhaps make doc/snabbswitch.html and post the result on the Gist?\nI think that the best thing we can do for documentation quality is to attract people to actually use it on a daily basis. I will make it a personal goal to start doing this and see what is necessary to make it stick :).\n. > Are we OK in the long term with the existence of both core.lib module and the lib module tree?\nI hope that with #734 we can present our API to application developers in a coherent way that is independent of the source code layout and Lua module names.\nThen the issue of lib vs core.lib would at least be less significant i.e. we would be talking about the internal factoring of the implementation.\nI have an overall sense that our API is too big. We have originally written code quite freely, with all functions externally visible from all modules, and then we have documented the result. The step we are missing is \"recoil in horror at the haphazard API and revise, revise, revise.\" You may already be onto that step @wingo but I am not quite there yet... I don't immediately have access to an update-to-date copy of the documentation (getting one requires connecting to a suitable development host, installing the necessary dependencies, building the HTML manual, copying it somewhere that my browser can find it. I already did that this morning but on another machine :).)\n. btw there are some handy Git tricks for making it easy to checkout/merge/diff PR branches from your working copy. Here's a link to my favorite but you can also do a one-liner version without any config e.g. git fetch origin pr/nnn/head && git merge --no-ff FETCH_HEAD.\n. (Comment above was meant for #737.)\n. @eugeneia I see two different use cases here for making the docs accessible:\n1. Make the documentation for the latest release available at a canonical URL.\n2. Make the documentation for each feature branch available during review.\nThe first one seems reasonable to solve with a file in a well-known location. Could perhaps host the full snabbswitch.html and snabbswitch.md in its own Github Pages repo? Then we could also use the Github interface to view diffs in the markdown between releases which might be convenient.\nHow about the second one? The best idea I have is for SnabbBot to make doc/snabbswitch.html and store the result as an attachment file on the Gist where it posts its results.\nThis is something we could also in principle farm out to a Hydra server. They have the notion that every build generates well-defined artifacts (tarball, manual, test logs, etc) that are archived and served up. For example: build with code coverage report and build with release tarball. Hydra might be a viable way to avoid excessive feature-creep in SnabbBot.\n. @kbara it would be awesome if you take the lead on this!\n. @wingo The main value I see in this activity is to ask ourselves, \"How should Snabb application code really look?\"\nConsider a really simple example like example_spray.lua:\n``` lua\nmodule(..., package.seeall)\nlocal pcap = require(\"apps.pcap.pcap\")\nlocal sprayer = require(\"program.example_spray.sprayer\")\nfunction run (parameters)\n   if not (#parameters == 2) then\n      print(\"Usage: example_spray  \")\n      main.exit(1)\n   end\n   local input = parameters[1]\n   local output = parameters[2]\nlocal c = config.new()\n   config.app(c, \"capture\", pcap.PcapReader, input)\n   config.app(c, \"spray_app\", sprayer.Sprayer)\n   config.app(c, \"output_file\", pcap.PcapWriter, output)\nconfig.link(c, \"capture.output -> spray_app.input\")\n   config.link(c, \"spray_app.output -> output_file.input\")\nengine.configure(c)\n   engine.main({duration=1, report = {showlinks=true}})\nend\n```\nThere are a bunch of aspects to this module that we may (or may not) want to reconsider:\n- Using module(). Better way?\n- Using require(). Useful or senseless boilerplate?\n- Cache modules with local. Important or not? (Less verbose alternative available?)\n- Calling functions (config.app(c, ...)) rather than methods (c:app(...) or c:config_app(...)). Is this the way that we prefer? (See also #740).\nI think it would be a valuable exercise to review these kind of issues periodically and see which ones we have agreement on, which ones are still contentious, and to capture this in a well-versioned API definition that provides some basic levels of forwards/backwards compatibility (semver).\nI frame this as creating a snabb module but it needn't be so. However, let me just sketch for the sake of illustration another way that the example above could be written:\n``` lua\nif not (#snabb.parameters == 2) then\n   print(\"Usage: example_spray  \")\n   main.exit(1)\nend\nlocal input = snabb.parameters[1]\nlocal output = snabb.parameters[2]\nlocal c = snabb.config()\nc:app(\"capture\", snabb.app.pcap_reader, {path = input})\nc:app(\"spray_app\", snabb.app.sprayer)\nc:app(\"output_file\", snabb.app.pcap_writer, {path = output})\nc:link(\"capture.output -> spray_app.input\")\nc:link(\"spray_app.output -> output_file.input\")\nsnabb.engine:configure(c)\nsnabb.engine:main({duration=1})\n```\n... and I am not arguing that the second example is better, only that the first example showing current practice represents a lot of choices that were made in isolation and never systematically revised.\nEDIT: Shortened second example for better effect.\nEDIT2: Adopted snabb.app.foo syntax for referring to app classes.\n. Torch is another LuaJIT application similar to Snabb Switch. Their API documentation is worth checking out, for example the Tensor object. IIRC the torch module was the inspiration for having a snabb module.\nQuite nice that they also think about, and document, the printed representations of their core objects and make everything REPL friendly.\n. @sleinen Yeah :).\nI did ask their advice (via mailing list) before adopting the \"busybox\" structure of a unified executable that includes everything. They went the opposite direction and decomposed their software into libraries that are installed separately by the \"luarocks\" framework perhaps via the OS distribution. I don't think that approach would have worked for us though, too much putting our fate into the hands of others.\n. @eugeneia On the one hand I agree that the overall structure may be fine and having \"more than one way to do it\" for libraries is healthy. Even so I would like to consider coming up with a neater way to refer to the libraries and apps that we already have.\nFor example I would much prefer to write this:\nsnabb.app.vhost_user\nthan this:\nlocal vhost_user = require(\"apps.vhost.vhost_user\")\nvhost_user.VhostUser\nwhere in the second example I have typed the word \"vhost\" five times in order to refer to a common app.\nLikewise if all other things were equal I would much prefer to write this:\nsnabb.link.transmit(...)\nthan this:\nlocal link = require(\"core.link\")\nlocal link_transmit = link.transmit\nlink_transmit(...)\nwhere again I typed the word \"link\" five times in order to make one function call.\nThis kind of thing seems to me worth fixing and also best resolved with a \"How do we really want the code to look?\" survey.\n(Side-benefit: shorter and less redundant source would make the REPL much more convenient to me. Even just typing ffi = require(\"ffi\") every time I run snsh -i makes me crazy.)\n. Further thought:\nThe OO convention documented on #740 could also help to relieve verbosity.\nProcedural code accessing a snabb module might end up more verbose even without the require() calls:\nlocal link = snabb.link.new()\nsnabb.link.transmit(link, p)\nsnabb.link.receive(link)\nand this might be better with object-oriented code:\nlocal link = snabb.link()\nlink:transmit(p)\nlink:receive()\n... but these are surely not the only options available.\n. An extreme example of what Andy mentions is lukego/blog#8.\n. @mraleph hinted that there may be a simple way to avoid the overhead of side-traces rejoining the parent trace. I posted a bounty on LuaJIT via BountySource in case anybody is inspired to make that work. See https://github.com/LuaJIT/LuaJIT/issues/114.\n. @capr Short answer: that ship has already sailed :).\nLong answer:\nI thought seriously about this approach a year ago, see snabb-devel post. I understand that it makes sense for other projects. I vastly prefer the approach that we have now though where Snabb is an atomic application that tracks all of its dependencies very carefully. This eliminates large classes of errors and support problems.\nSometimes Snabb requires a really recent version of LuaJIT e.g. a specific fix that was made only days ago (example). It is very powerful to be able to resolve those dependencies once-and-for-all in the main repo rather than dealing with support requests due to changes made downstream (\"I ran Snabb with LuaJIT 2.0 and <...completely predictable problem that should never have happened...>\").\nStill: we could spin off parts of the code into separate libraries and track them as submodules, the same way that we use ljsyscall, pflua, etc. Certainly I think it makes sense that the Igalians wrote pflua as its own library rather than exclusively a part of Snabb.\nThere have also been some related ideas like compiling a libsnabb.so for use in C++ applications (#606) but this has not taken off and is a lot of work to do right (e.g. audit all code for thread-safety/reentrancy even though this is not relevant to the Snabb application itself.)\nMy own thoughts are moving in the opposite direction. I want to add musl-libc as a submodule and use fully static linking to break our dependency on Glibc.\n. @capr well that is my off-the-cuff response anyway. Since this PR is all about looking at things with fresh eyes we can see if other people see things differently too.\n. > what a can of worms I opened :) I understand the need for the metatype early-binding hack now, although TBH I would still cache on an as-needed basis, close-to-the-loop and documenting why I'm doing it in each case.\nThis is very hard to predict though. In practice we will either do it too often or too seldom. Code reviews will haggle about \"is it really worth caching this?\" or \"are you sure you shouldn't cache that?\". I would be much happer if we could find a simple and consistent solution that doesn't require too many tiny choices. (That even seems to bother Mike Pall).\nJavier once went through the code and replaced a bunch of tables (e.g. packet, link, etc) with FFI metatables (early bound). He observed ~5% speedup on some non-trivial benchmarks. This suggests to me that we are paying a significant price of unused late-binding today (see snabb-devel post).\n. Alternative idea: we could put all of our modules directly into the global namespace instead of a snabb module.\nHere is how the example above might look in that case:\n``` lua\nif not (#args == 2) then\n   print(\"Usage: example_spray  \")\n   main.exit(1)\nend\nlocal input, output = unpack(args)\nlocal c = config()\nc:app(\"capture\", app.pcap_reader, {path = input})\nc:app(\"spray_app\", app.sprayer)\nc:app(\"output_file\", app.pcap_writer, {path = output})\nc:link(\"capture.output -> spray_app.input\")\nc:link(\"spray_app.output -> output_file.input\")\nengine:configure(c)\nengine:main({duration=1})\n```\nThis would be in the style of Lua's own native model where the basic APIs are globally available (string, table, math, etc) and don't need to be required. The extra brevity could also make us more REPL-friendly.\n. (Note: The core modules are already in the global namespace today, e.g. packet, but people don't use them that way for fear of expensive lookups when referencing the module on every call.)\n. cc @xrme\n. Nikolay: True - this will be even more critical in the Snabb internal parallelism context. @xrme is working on this aspect right now i.e. taking the multiprocess design and doing a low-level implementation geared towards 100G with software dispatching. Could be for example that a software packet dispatcher needs to inspect payload using cache-bypassing instructions (MOVNTDQA and so on) in order to avoid populating its own L1 cache with data that will be immediately accessed by another core.\n@xrme Yes. False sharing is an interesting case: I suppose that we would expect to have that today if two cores would access the same struct link since each one is updating different fields (read vs write cursor) that happen to be on the same cache line and so they might frequently block in synchronous RFO (Read For Ownership) requests to each other to take over the cache line. Likely we will need to add some padding so that each core updates a different cache line. (This is what Virtio-net does with its shared memory ring buffers IIRC.)\nOverall I feel like the only hope to make sense of this stuff is with profiling e.g. using the PMU to isolate exactly what is happening and take deliberate (non-voodoo :-)) corrective measures. Could be that it is not really so difficult to optimize these aspects once we have the right mental model and the right tools. The PMU is able to count events like RFO requests on the cache coherence protocol and so on.\n. @wingo Interesting observation. Overall it seems like there are two ways to deal with latency: minimize/shorten it or parallelize/amortize it. Can be that both are needed if there are fundamental bounds on what is possible with each approach.\n. Neat :).\nI appreciate that you have made a simple solution for choosing between different I/O sources (tap, virtio, 10G) in select_nic_driver(). This is a nice baby-step towards solving that problem more generally in the future i.e. making Snabb programs more flexible about their I/O sources.\nAside: I would really like to have a variant of our standard CI benchmark that uses this l2fwd instead of the DPDK one. Then we could make sure that we have the same performance with both guest Virtio-net drivers. @eugeneia would be the person to talk with about how to set this up.\n. @domenkozar does this look sensible to you?\n. Aside: I wonder if there is any toolkit for writing \"shell scripts\" directly in Lua. This could conceivably make the code more homogeneous and avoid obscure dependencies.\nI see a major problem with both C and shell that using them \"professionally\" requires having a sixth sense for when you might be using something that is non-POSIX or otherwise taboo. I much prefer to use tools that are more WYSIWYG. These days using ljsyscall seems vastly preferable to writing system code in C... is there a candidate for something vastly preferable to shell too?\n. @wingo @eugeneia Can you tell me which of you two is the \"next hop\" who I am negotiating with to merge this PR? (I understand that everybody in the community is welcome to review this but I hope that one person will be assigned to merge it when they determine that it is ready according to their own criteria.)\n. SnabbBot is flagged a CI error but this looks spurious. It reports a performance regression on the NFV iperf benchmark that I can't account for based on the content of this PR.\n. Here is what I would ideally like as a downstream contributor:\n1. I can create a feature branch based on master and send a Pull Request to SnabbCo:master branch.\n2. Responsible \"upstream\" maintainer is identified i.e. the person who will merge this (or not) onto a branch that feeds to master.\n3. The maintainer guides me through the review process e.g. tells me what I need to fix now, tells me what I should do differently next time, tells me if feedback from some subject-matter expert is needed, etc.\n4. The maintainer merges the change onto their branch. Now from my perspective the rest of the upstreaming flow is \"somebody else's problem\" at least to a first approximation. (I still may want to watch the progress, see what edits are made upstream, be available to help with problems e.g. that might cause the change to be reverted, etc).\nI would hope that any confusion and misunderstandings in this process will at least contribute towards making things go smoother the next time i.e. I am developing a useful functioning relationship with my upstream maintainer(s).\nAnd yes please I would like to send my changes through the same upstreaming process as everybody else. I see this as a value-adding process and that skipping it by merging directly onto next I would be missing out on that.\n. > If /bin/sh also works on guix, we could use that. But unfortunately that will break if we use bash features and folks have a different shell set.\nThanks for clarifying. This would be a bigger change i.e. porting from bash to vanilla bourne shell and I am not motivated to do that on this branch.\n. > OK, to be clear in the future for this commit I should have said \"LGTM, I will merge onto wingo-next.\"\nGithub can also help with this process btw. Here is how I try to involve it when accepting changes:\n1. Make a non-fast-forward merge (git merge --no-ff origin/pr/nnn) into my branch with a commit message summary line of the form Merged PR #nnn (very short summary) into my-branch.\n2. Push the change to my branch e.g. lukego/next.\nThen Github will automatically add a cross-reference to this PR page that links to the merge commit so that I can see whether the change has landed. See for example the note on (random example) #694.\nCould also make sense to close PR branches once they have been merged onto their upstream. I am not sure. My habit has been to leave PRs open until they land on master so that when you list the PRs on Github you see the code that has not landed yet (and is perhaps extra useful to review). That practice is perhaps a crutch that I used because without changes passing through multiple maintainers I was not always sure that I am doing the right thing by landing them on next.\n. @wingo Roger that. Thanks for the info! I am a happy downstream contributor :).\n. Closing this PR because I see that it has landed upstream on wingo-next. Thanks @wingo!\n. I would be interested to know what people consider good/bad role models for open source project brands. Particularly ones that are administrated in a simple way.\n\"Linux\", \"GNU\", and \"Apache\" are all good strong open source brands for me. The \"Linux\" brand has been administrated with very low ceremony before there was ever a Linux Foundation.\n\"OpenStack\" not so much. The upstream community has used the brand as a stick to wave at their contributors to make them run hither or thither for fear of being de-branded. This has created a lot of animosity, politicing, and gaming of the system.\nI do have a Snabb trademark here in Switzerland. I should subscribe to a trademark monitoring service to be notified if somebody would apply for a confusing trademark elsewhere in the world and perhaps also register in a couple of important places. Practical tips on how to do this in a quick and simple way would be welcome if somebody knows a good service.\n. > We should prohibit Snabb in company names (besides yours, if you decide to keep it)\nJust to be clear: My company (Snabb GmbH in Switzerland) is serving as a bootstrapping vehicle for the Snabb project. In the beginning it has been practical to have a well-known entity that can sign contracts to provide services, sponsor relevant work on Snabb and related projects, buy and operate lab equipment, etc. Over time these activities are spreading within the community: other people and companies are selling reputable Snabb services, SWITCH is hosting and maintaining the new lab, Intel and Mellanox and other sponsors are providing equipment, and so on. So the community dependence on that one company is decreasing all the time and eventually I imagine that it will fade into the background and discretely stop trading under the name \"Snabb\".\nThis is fine with me. I have no desire to exercise unfair/asymmetric control over the \"Snabb\" brand: it is something for the community to share. Snabb GmbH is safeguarding it while we work out how we want to do things in the long term.\nFurther notes:\n- Snabb GmbH has no other shareholders or other obligations that I could imagine creating a conflict of interest during the bootstrapping era. Nor does it own any intellectual property besides the trademark (not even the copyright to the code that I write).\n- In the very beginning of the project there were other paths available too e.g. to follow Hwaci (SQLite), Apache, Mozilla, BerkeleyDB, MySQL, etc. I consulted with some of the people who created those projects to get help in working out a good path for Snabb. I am very satisfied with the track we are on and the progress we are making.\n. Great! So we need volunteers to do these things:\n1. [ ] PR to update \"Snabb Switch\" to \"Snabb\" in the source tree.\n2. [x] Make a plan for renaming snabbswitch to snabb in whatever is the smoothest way.\n3. [ ] Propose simple and unambiguous rules for using the Snabb brand (name/logo).\n4. [ ] Create a project homepage e.g. landing page that explains what \"Snabb\" means and links to Snabb-branded projects and professional services.\nI also need to come up with a simple and unconfusing plan for rebranding my own professional activities.\n. Just moving forward here a little bit...\nGithub seems to handle repository renames very smoothly. Existing checkouts will work, the old name and URL can still be used, website will redirect quietly. So it seems we can simply rename the repository from snabbswitch to snabb once we merge a PR to update the content.\nIn the background I am setting up a separate brand for my own professional activities and I plan to complete that this year. This will free up the name Snabb and the domain snabb.co to refer specifically to the open source project. Then we can all use the Snabb brand on the same fair and equitable terms that we will thrash out here on Github. I imagine that I will continue to hold and administrate the trademark for the foreseeable future. Happy to discuss any concerns, I have no sinister plans.\nHow should we define the rules for using the Snabb name and logo? Generally my feeling is that a Snabb product or service is one closely connected with the upstream community here on Github. This is a grassroots project so it should be easy for well-intentioned people to create projects, sell services, organize events, start user groups, sell t-shirts, etc. Specifically though I think we need to be prepared to evolve the details: we could start with a simple statement like the PostgreSQL one and if needed then evolve toward something more elaborate like the Apache one over the coming years/decades. I would suggest we maintain a TRADEMARKS.md file where we keep track of all the uses that we have defined as okay or not-okay i.e. maintain the trademark policy the same way we maintain everything else.\n. Too bad the filename extension is .md.src instead of .md. This seems to suppress Github's quite awesome capability to show diffs in the rendered versions of markdown files.\n. Removing [wip] tag. This branch is supposed to be a straightforward removal of dead code and shortening of long-winded documentation. I'd like to land it upstream.\n. @eugeneia Small rant about keeping documentation build artefacts in the source tree, which you are free to ignore:\n- I accidentally pushed a change to README.md.src without updating README.md. Seems easy mistake to make and easy to miss.\n- Running make markdown has not worked on lab servers ever since they switched to NixOS. That is due to the hashbang issue (#737). This seems like a sign that people are not routinely working with updating the docs.\n- Running make markdown regenerated the PNG image files and git shows these as changed when I don't believe that they really are, i.e. ditaa seems to be non-deterministic. I simply ignored adding these files on the assumption that the changes are not significant.\n- To install ditaa on my laptop required a 120MB download of Java, etc. This is one of the most \"unsnabby\" things that I can imagine i.e. downloading a massive piece of software from Oracle in order to update a readme file. (I think somebody has actually ported Ditaa to Go because elsewhere in the world people also hate this dependency like I do.)\n- I don't know if I have the same version of Ditaa as you do anyway so the images may be changing randomly depending on who builds and commits them i.e. this is an uncontrolled dependency in the software environment for maintaining our source code which seems to me kind of extreme.\nThank you for listening :). I feel that we are quite far from nirvana in terms of the documentation maintenance/build process but I don't have any actionable ideas right now.\nI am the person who introduced Ditaa into the build chain in the first place so I feel that I an entitled to rant about it like this :).\n. Good discussion!\nI would like to flag this whole topic as controversial. It is recommending a coding style that is unlike the core modules e.g. packet and link. This needs to be addressed somehow.\nThe broader discussion could take place under the umbrella of #734 including e.g. should we update the packet and link modules to follow this example too?\nI will advocate for Occam's Razor i.e. the technically simplest and most minimal solution that actually covers our requirements. If that requires procedures, classes, subclasses, class methods, multimethods, predicate dispatch, etc, then so be it. But I would presume everything to be unnecessary complexity until proven otherwise.\nThe risk I see is that we will introduce concepts that make one part of the community comfortable at the expense of another part of the community, and if we do this inconsistently over time then everybody will end up uncomfortable. To understand the whole Snabb source tree should you already be familiar with Java, Python, Smalltalk, CLOS, etc and recognize ideas that are borrowed from these languages? That is the kind of transitive complexity that I want to avoid.\nI would also like to be explicit about where the rules apply and where they don't. For example documented APIs may be expected to strictly follow the recommendations, and core modules too, but not internal code within apps or programs where we defer to the author's creative impulses instead.\nI observe that the lib.protocol code has been controversial and I think it's because the first version PR'd already used classes and subclasses and people didn't have the chance to \"see the working out\" and understand what value those concepts are bringing to the table.\n. > I'd be curious how people who don't like this approach would have written that code.\nI observe that when people have dug into that code they tend to develop a better appreciation for it. It is very carefully and thoughtfully written. I can believe that it would be awkward to factor that code in a more \"cave man\" style and that the object-orientation is really solving a problem.\nI also like reuse. My fear with class.lua is that we will casually create object-oriented class hierarchies as one does in other languages like Java even when they are not required. That would not seem idiomatic for Snabb (\"Snabby\") to me.\n\nat least the intel10g app uses inheritance internally and I actually find that code hard to read because the hierarchy is not obvious\n\nI struggle with that code and am sometimes surprised about which functions are really being called in different configurations and which registers are really available. This leads to awkwardness in changes like 9c8b1025f049d3803b7aa03dc4ed2b754a4459d5.\nWith the benefit of hindsight I feel that inheritance complicates the code in intel10g for me and that is why in the I350 driver I factored everything into one app and used if on configuration parameters to decide e.g. whether to initialize the device or not. That driver has fewer features and so I may be cheating a bit, I am not sure yet. I really enjoy the creative process of looking for ways to eliminate clever ideas and make things more low-brow/cave-man/down-to-earth. This is not always possible though... \"as simple as possible, but not simpler\" as the saying goes.\nIn the meta-sense though I am also really glad that we have not prematurely standardized all of this stuff and so we have an interesting assortment of programming techniques in our repository that we can talk about. I think we need to preserve the feeling that the person actually solving the problem has a lot of latitude to make design decisions.\n. > However I would like to go further, and to advocate globally for preferring method syntax where it's obvious what the data types are.\nGood!\nLet's decide on a canonical style that works for everybody. This will require some due diligence on JIT efficiency to make people comfortable but hopefully we understand things well enough to do that now.\n. Interesting! I thought we had already adopted this behavior but looking at the diff it doesn't seem so :).\n. @someonelse thank you for letting me borrow your name for the purpose of that example :-)\n. cc @domenkozar @eugeneia \n. @sleinen Cool! This chart shines a light on the data in a pretty interesting way.\nFirst that there is quite a bit of variation in the \"basic1\" benchmark results. The highest and lowest values differ by around 20% and there is quite a spread in between. It would be interesting to account for this (CPU? OS? JIT?) and control it (eliminate with isolcpus? reduce with longer runs? control by averaging multiple runs?). For the moment the basic1 benchmark seems not that practical for evaluating the performance impact of code changes if indeed the result can vary by 20% due to random chance.\nSecond that the packetblaster benchmark seems to have around a 3% failure rate. That is high! This may well be caused by an important bug in our software and warrants investigation.\nI am surprised that there is so much low hanging fruit in looking at the data. And this is even when the DPDK benchmark results seem to be missing from your visualization, and those are the only ones that I expected to be potentially interesting :).\nHow do you make charts like that in Google Drive? How might I have made it less bothersome to generate CSV?\n. > packetblaster benchmark seems to have around a 3% failure rate. That is high!\n(Out of curiosity I just ran the packetblaster benchmark manually 500 times and didn't see any failures. I wonder what happened in the experimental run. Suggests that we should preserve logs/output!)\n. @sleinen Oh now I see the \"Explore\" button on the spreadsheet. That is super convenient!\n. I also did a Google Sheets import and this time included the DPDK results: https://docs.google.com/spreadsheets/d/165a9OKs5Q3yHxUX15nSrsGvTeLwvwK-U1AyQ7rgskDE/edit?usp=sharing\nAnybody knows how to get Google to tell us what is going on there?\nThe output was a bit of a pain to massage into CSV. I used an Emacs macro but would prefer to use e.g. AWK. I wonder if it would make sense to capture all output, have everything addressable to screen-scraping, but also try to make known-interesting values easy to pick up e.g. with a convention like @parameter foo 42 and @result mybenchmark 96 to make it easy to cherry-pick values for CSV/JSON/etc. Could be handy to include a bunch of \"extra\" information e.g. syslog entries during each test, etc...\n. So many different tools available: R, Torch, IPython, gnuplot+awk/guile/perl...\nCoolest link I found today is Agile Visualization and its suite of visualization tools based on Pharo Smalltalk. That looks like potentially a convenient way to import/massage data, and do numerical charting like we are here, and also visualize things like LuaJIT trace structures and profiler results. See demo video.\nIn the past I have started writing bits of code for analyzing LuaJIT trace dumps in Emacs Lisp but that just feels wrong in this day and age :). Still: these are all exotic tools so I am sure it is important to keep our options open by generating simple formats e.g. CSV/JSON/text.\n. @eugeneia mentioned on #746 that mention-bot is annoying him enough that he blocks it. If this is a common sentiment then we can do away with it. Please vent here :).\n. @wingo @eugeneia Yes, now I agree too, mentionbot is mostly a distraction. Disabled now.\nThanks all for bearing with that experiment.\n. I would be curious to know what @eugeneia, @alexandergall, @capr, @wingo think of this plan: both the general idea of merging applications upstream and the specific working relationships suggested in the diagram.\nOne reflection is that it will probably be important to focus on the content of the merges more than the history. The command git merge combines two parallel repository histories together and these can be long: each one is the life story of the birth and development of an application. I suspect it will be practical to embrace the rich tapestry of our project history rather than spend a lot of effort on rewriting history with rebases and so on.\nSo when reviewing the merge of an application branch the main things I would think to look at are:\n- Is it clear what the application is? what does it do, how do you run it, where do you find out more, how do you contribute?\n- Is there a src/program/foo subdirectory? Let this be: application developers are in charge of this code except in exceptional cases (e.g. license conflicts, build complications, etc).\n- Are there changes elsewhere e.g. in core, lib, and apps? This should be subject to the same review and revision other code submitted upstream. The review could either be done directly on the application branch or the changes could be spun out and submitted separately, whatever is most practical. If it takes too long to reach agreement on these changes then they could be moved into src/program/foo and then refactored later once we see eye to eye.\n. Thought for the day: \"Bootstrapping a distributed merge network is as easy as riding a bike.\"\n\nHopefully it will be easier once we are moving... :-)\n. This is starting to work!\nToday I pulled from both max-next and wingo-next onto next. This is the first turn of the pedals on our metaphorical tandem bicycle.\nWe are still fumbling around and not yet really working in unison but I think this will improve and we will gather momentum with every merge.\nLet me paint a picture of how I see upstream contribution working:\nContributor opens a PR and one person is unambiguously assigned as the \"upstream\". The upstream either merges the change directly or engages with the contributor to tell them exactly what they can do to get the change merged. The interaction should be something crisp like:\n- Merged, thanks!\n- This needs a README and a selftest and then I will merge it.\n- I need an ack from [persons] before I am confident enough to merge this.\n- Have you considered doing FOO? Would you like to revise or do you prefer that I merge as-is?\nThat's it! This process is then applied recursively from the first upstream to the next upstreams until the change lands on master.\nEverybody in the community is welcome to give feedback on all pull requests, of course! However the contributor is free to evaluate the merits of this advice for themselves unless their upstream decides that it is important and says that it is a blocker.\nGreat work @eugeneia and @wingo thank you for your perseverance while we are working this out!\nAnybody else want to volunteer to be an upstream, btw? :-)\n. @kbara awesome!!! :smile:\nSo how about this workflow as the next step based on what seems to be working now:\n\nI reckon I can handle up to around 5 downstream branches so we should be okay without rebalancing for a while.\nOne challenge now will be dispatching PRs send to master between more upstream maintainers. How about if a maintainer self-nominates as the upstream for a given change via the Assignee field on the PR? Then this maintainer is the one engaging with the submitter according to the process sketched above (i.e. focused on getting the change merged) whereas everybody else is just providing supporting comments and ideas.\n@kbara I added you to the \"maintainers\" group on Github so that you can manipulate issues (e.g. set assignee and apply tags). I think the last step towards making it official is to register a branch in branches.md.\nN=4! log5(N)=0.86! Onward!\n. Good question.\nThere are several tools available for putting metadata on PRs:\n- Open / Closed\n- Assignee\n- Tags\nSo question is: how should we use these states? what does it mean for a PR to be \"open\" - that it is not released on master yet or that no upstream maintainer has merged it yet?\nand is there other information that we should be keeping track of: waiting for a maintainer to be assigned, waiting for feedback, waiting for a revision to address problems, ...?\n(not to suggest that we need to answer all of this at the same time, just thinking of the broader context.)\n. I understand that you need a way to search for issues that require action. I wonder if open/closed is the best search criteria though? How about applying a tag upstream and using an advanced search to filter those out when looking for work to do?\nJust reflecting that Open/Closed is only one bit of information so it is a fairly limited way to encode the status of pull requests. If somebody is going to have to use more sophisticated queries it could make sense for that to be maintainers rather than e.g. application developers or casual users.\n. I feel like it would be particularly useful for application developers to see PRs for code that is accepted by a maintainer but not released yet. Then they would have the chance to adopt it early and provide feedback based on their testing.\nFor example if there is a LuaJIT update that is accepted upstream but not released yet then it could be productive for an application developer to merge that for testing and provide early feedback about whether it causes them any problems or not.\nFor general users and contributors it could also be handy for the PR tab to show \"what's going on this month?\" as a window into the work that is going on and showing what might be valuable to test and review (even if it is accepted by the first hop it may still need update/revert on the way to master).\nOr could be that I am overcomplicating things :).\n. I can create a label by clicking Issues -> Labels -> New Label. You too? If not, which labels do you want?\nGithub permissions are confusing. I actually think we already have it setup too loosely e.g. too many of us can accidentally push to master or other public branches with a mistake on the command line. Should work out what is the optimal solution some time.\n. I merged this because the target is one of my branches (next) and the content looks good to me (no blockers).\n. (Thanks! :beers: for the first changes from wingo-next landing on next :-))\n. > Apparently app:report() on intel apps also needs vmdq enabled to work: Igalia#260\nOuch. This is the kind of issue I was alluding to in https://github.com/SnabbCo/snabbswitch/issues/740#issuecomment-182327203. I think the PF/SF/VF factoring of the code is error prone (inheriting methods between objects that have quite different state e.g. available register definitions) and that we should try refactoring in terms of INIT/TX/RX like in the newer I350 driver (see also #687 \"I/O 2.0\"). See commentary in intel1g.lua on master for more on that idea.\nThe next step that I have planned in that direction is to finish my Intel FM10K (100G) driver make a nice integration between that an Snabb NFV.\n. Thanks! Merged onto next.\nI respect your decision to merge #700 although I am opposed to the content of that change. Snabb Switch does not actually work on 32-bit platforms and so I consider it a feature if compilation fails and a misfeature of compilation succeeds (deferring the inevitable problem to runtime). I realize that I am the minority here though so I will try to propose an alternative solution in the future rather than pushing back on this PR.\nOnward!\n. NB: I think it will work better next time if you make next the target of the PR instead of master. That way it will only show the commits that are not already on next and Github will detect when I have merged and close the PR.\n. I believe that is the majority view. I am the minority who sees this as using C language-lawyering to land a broken untested port to i386.\n. (I am glad that you merged it. That puts pressure on me to propose an alternative solution rather than just complaining and gatekeepering.)\n. @kbara I will concede that it would be a good change to land on an integration branch with a goal such as \"port Snabb to i386\" or \"make Snabb independent of machine word size.\" Such a branch could then be merged onto master if it achieves its objective without undue complexity and is maintainable (e.g. has test case based on strict compiler warnings or running Snabb inside a 32-bit QEMU VM.)\nThe master branch is not the place to accumulate and integrate such changes piecemeal though. This makes work for the upstream maintainers and it doesn't benefit users. (It may actually harm users if it is motivated by helping distro maintainers to do things that upstream don't want e.g. to ship broken i386 binaries or to link with a LuaJIT that is missing critical fixes or ... etc.)\nTo me this is the same as if e.g. we had some Guile code and somebody wanted to make it more R5RS compliant by rewriting a few functions to not depend on the order of procedure argument evaluation. This is \"more correct\" with respect to some well-known specification but it is creating work for maintainers without utility for users (unless taken all the way and actually made portable to other schemes).\n. I suppose the unstated assumptions in my position are:\n1. That it is correct to cast between uint64_t and void* in a program that assumes sizeof(uint64_t) == sizeof(void*).\n2. That we do make this assumption on today on the master branch and this is a legitimate design decision.\nThis could be debated on #752.\n. @kbara How do you feel about the \"compromise\" of rewriting the code in Lua (#751)?\nPersonally I like explicit stable ABIs. For example the Intel NIC always represents addresses as 64-bit values whether the host is running in 32-bit or 64-bit mode. Just zero the bits you are not using. I appreciate the simplicity of this and wish that more interfaces were so simple.\nI dislike making ABIs implicitly dependent on compiler settings particularly in the Snabb context where we are doing things like mapping memory from virtual machines that may not even have the same word size as the host.\n. I do think that the code only looks \"gratuitously\" unportable to a serious C hacker who is accustomed to writing portable code. I have the opposite reaction: intptr_t seems like a gratuitously high-brow concept to introduce into a program that only targets one architecture.\nOn a higher-level though the gratuitous thing here is using C at all when this code can so easily be written in Lua. I am much more willing to talk about what makes good Lua programming style for manipulating FFI pointers because that seems much more Snabby to me.\nThe C code should really be expunged from the codebase IMO. If we had a time machine we would have adopted ljsyscall from the beginning and we could have avoided antagonising the more sophisticated C hackers in the community.\n. > My general heuristic is that unportable code is more likely to have various subtle bugs in it that porting makes obvious but which are active bugs even on the main platform\nGive me some time and I will come around to your way of thinking on this. I have already had the realization that it is worth inserting \"NOP\" memory barrier operations with a rationale for why they do not apply to x86 rather than simply omitting them.\nMy excuse for digging in my heels on this pointer size issue is that the PR is coming from somebody who is attempting to create a snabb-i386 package for a Linux distribution and in this situation the most important thing is to clearly communicate that this will not work and should not be done before a genuine port is implemented. I am trying to avoid making it easier for porters to shoot themselves and their users in their feet here.\n. One last comment in this volley :-)\nI also want to acknowledge that it is actually really hard to write portable code without the benefit of test suites and so on. I feel much more inhibited in my programming when I feel like I need to cross-reference everything I write with abstract specifications.\nIn support of this thesis I submit into evidence Exhibit A: The time when I had to debug a portability problem in a Lisp program written by Guy Steele where he had illegally depended on the order of elements returned by PAIRLIS. GLS is the editor of the ANSI standard that this violates and probably the person least likely to make such a mistake in the whole world... and yet it happens to him too.\nI prefer to use subsets of these languages that have well-defined behavior.\n. @xrme There is some code here you might be interested in btw: an optional (unused) persistent argument that will leave DMA/packet memory with its file backing so that another process could map it (e.g. via SEGV handler).\nThis is a bit simpler than with the POSIX share memory implementation because you can directly calculate the right filename from the pointer value, it's something like:\n/var/run/snabb/dma.0000deadbeef\nwhere the hex number is the physical address of the memory (which you can get from the virtual address by removing the tag).\n. Thanks for the tips! I have adopted @xrme's suggestion and it does indeed seem to work fine. I'd like to land this code upstream.\n. I still feel like the documentation is irrelevant unless/until people are actually using it on a daily basis and I wonder to what extent they are.\n. @kbara great to hear! How do you consume it -- via Github, build HTML manual, or read the markdown source locally?\n. I would find it valuable to cite/link parts of the documentation that we especially like and want to emulate in the future. Then these rules could be used more as a reference and to fill in details.\nI find it noteworthy that @kbara is the number one fan of the documentation and that she is consuming it in source format in her text editor. This reinforces my feeling that we have an accessibility problem for the formatted documentation.\nI wonder how snabb doc man packet or snabb doc apropos append commands might be implemented? (Compile the docs into the executable and print them to the console? Or compile in an index and link to the online docs? Or just not a good idea in the first place?)\n. I reflect that we are all giving our $0.02 here but we have not identified an upstream maintainer to actually guide this change towards merge. How to solve that?\n. I still view the branches in the kernel / subsystem model. For me we should have a documentation branch, this branch should have an owner, that person should decide what changes are merged and how much review is enough. If we want the documentation done differently then we would send in PRs as input to that person (who would still hold the editorial control).\nOn the other hand I don't want every change to have to pass through the documentation branch so all maintainers need to be basically on the same page about what is expected and what we should ask downstream contributors to provide.\nThe model we are using now of maintainers volunteering to take changes on a case-by-case basis seems unlikely to scale to me, but I am prepared to be proven wrong.\n. I reflect that I am bikeshedding on this PR so please take my feedback with a grain of salt. On a meta-level I feel like we will have more of a positive effect on documentation by increasing the rate of contributions (merges) in this area. This can all be changed in follow-on PRs after it lands.\nI mark @kbara as the Assignee here based on the discussion above. Seems that @kbara has signaled that she is willing to merge so now it's up to @eugeneia to signal how much more revision/feedback he wants to do before that.\n. (@kbara I can't assign you. Github team invite stuck in your Spam folder?)\n. @eugeneia Generally I am happy to pull documentation improvements from you directly when you decide they have had enough feedback and revision. Like I mentioned above I do want to move towards having subject matter experts (individuals or small groups) who consistently take the lead on upstreaming changes in specific areas (subsystems). I hope that at the moment we are in the process of discovering where upstream people's interests lie.\n. @wingo do you have some code for measuring the latency of the breathe loop that could come upstream for testing here?\n. @eugeneia @wingo @kbara I also adopted an experimental documentation style here: to write real documentation in markdown syntax but include it inline at the top of the source file. The Makefile could extract this for inclusion in a manual. I still think the key to high-quality documentation is to put it somewhere that people will see it every day. Flame on! :-)\n. Here is an idea: How about if the \"black box\" is literally a core dump file (or equivalent) produced either on termination of on request (e.g. signal requesting a background fork-and-dump).\nThis way analysis would be done via inspection of a snapshot of the heap i.e. actually reading the LuaJIT internal data structures. This would be done with the moral equivalent of GDB scripts to extract the state you are interested in e.g. locate the Lua VM via a C global variable and extract all of the bytecode/IR/mcode for the traces and then parse that as binary data for analysis. If ephemeral information is also needed - e.g. information about aborted trace recordings - then that would need to be stored somewhere in the heap e.g. a ring buffer.\nThe advantages that I see to this approach over explicitly logging messages like trace dumps and profiler reports are:\n1. Have all information that exists available for analysis. Less burden of anticipating what information will be needed and logging that explicitly. (Avoid \"It crashed in production? Okay, please deploy this new version that will also crash but provide more debug information that we missed the last time.\")\n2. Simple software interface between runtime and analysis: Just write down the ABI and access everything directly. Compare with e.g. extending the LuaJIT runtime system to export more information, writing Lua code to dump that into an external format, writing a parser for that format. (For example #623 traceprof could be rewritten as an external tool and we could drop the patches I made to export more information about traces from the LuaJIT runtime system.)\n3. Less frequent fork-and-dump required. If the complete core dump can be created reliability, even from a process that is crashing hard, then we can save the black box \"just in time\" rather than as a continuous background process.\nbtw: I am exploring using the Moose toolkit from the university of Bern to partly automate the analysis I usually do on LuaJIT trace files e.g. cross referencing with profiler information to see which traces are relevant, checking those for red flags like unsunk allocations, looking for problems like lukego/blog#8, etc. Could be useful as a tool for exploring and prototyping ways to inspect the black box. Currently I am parsing the textual trace dumps but I imagine that operating on core files would work too. Early days yet.\nThoughts? cc @wingo @kbara @xrme @mwiget.\n. @kbara Wow! That is a small world indeed :). How did you like it? Can I ask you tech questions? :-)\n. Closing this PR for now. I will reopen when I have something new to show.\n. @petebristow what do you think about this in the context of #692?\nI am thinking that this PR basically steals that idea but keeps everything local inside snabbmark and so defers the discussion about a more generalized benchmarking library/framework until the future when we have more code and experience and are thinking about how to refactor everything based on lessons learned.\nI can imagine having additional functions for benchmarking apps and app networks alongside this one for benchmarking byte-oriented functions. (This one can also be improved a bit e.g. to recognize the checksum tests ignore the destination arguments and so prune those out of the permutation space.) These could initially share code via subroutines in snabbmark.lua.\n. Baby steps...\nI asked my mum about this kind of data analysis (she's good with statistics) and the keyword she gave me was SST (Total Sum of Squares). This lead me to the Kahn Academy videos on Analysis of Variance (Inferential Statistics). Looks promising.\n\"A little knowledge is a dangerous thing\"... early days yet. Please pipe up if you are interested in these things :).\n. Closing this PR for now. I will reopen when I have something new to show.\n. My mum tells me that she did an analysis of variance with a few variables and it says that the benchmarks are not sensitive to the alignment of the source operand but they are sensitive to the combination of operation (cksum, cksumavx2, memcpy) and displacement (cache level).\nHere is a picture of the latter:\n\nThis seems to be saying that the base checksum is always slow, the AVX2 checksum varies by a factor of 2 depending on L1/L2/L3/DRAM, and the memcpy operation is dramatically faster in L1 cache (up to 12KB working set size).\nI find it encouraging that this kind of thing can be detected purely numerically without any explanation of what the data actually represents.\nEverything with a grain of salt at this stage, but still... feels like progress.\n. So, seriously, is R the coolest thing in the universe? Probably.\nI very easily loaded the CSV file into R:\n```\n\nd <- read.csv(file='data4.csv', sep=';')\ndf <- data.frame(d$byte.cyc, d$disp, d$lendist, d$l1.hit, d$srcalign, d$lenalign, d$name)\nd$disp <- as.factor(d$disp)\n```\n\nand then, completely by magic, it is able to just tell me the answers to all of these big questions I have been wondering about. Like:\n```\n\nsummary(aov(d.byte.cyc ~ d.srcalign * d.lenalign * d.disp, data=df))\n                                Df Sum Sq Mean Sq  F value Pr(>F)  \nd.srcalign                       1     86      86    4.364 0.0367 \nd.lenalign                       1    125     125    6.343 0.0118 \nd.disp                           4 139577   34894 1773.645 <2e-16 ***\nd.srcalign:d.lenalign            1      0       0    0.003 0.9551  \nd.srcalign:d.disp                4     78      20    0.993 0.4098  \nd.lenalign:d.disp                4     20       5    0.250 0.9095  \nd.srcalign:d.lenalign:d.disp     4      0       0    0.004 1.0000  \nResiduals                    46060 906175      20                    \n\n\nSignif. codes:  0 \u2018\u2019 0.001 \u2018\u2019 0.01 \u2018\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n```\nwhich says to me that:\n1. The maximum displacement of data in memory (disp) makes a big difference. This is effectively the working set size i.e. which level of the memory hierarchy will be serving the requests.\n2. The source and destination data alignments don't matter: not individually, not in combination with each other, and not in combination with the memory hierarchy. (This is expected on Haswell: it would be interesting to compare with a Sandy Bridge machine where various SIMD instructions carry penalties for unaligned access.)\nHoly smokes! Imagine taking hours of benchmarking numbers and getting a detailed analysis of them in one second. Mind: blown.\nThis seems like exactly the tool that I need to move forward with micro-optimizations like the asm blitter in #719 where I want to understand how robust the optimization is to different workloads.\nEDIT: Actually it is saying that the alignments do matter a little bit... I said they don't matter based on the apparently much smaller effect than for disp.\n. ahem :)\nI think the conclusion above is okay but the details are wrong: I didn't properly declare that some more of the numeric columns in the CSV file are \"factors\". Correcting that we get a similar table:\n```\n\nsummary(aov(d.byte.cyc ~ d.srcalign * d.disp * d.lenalign, data=df))\n                                Df Sum Sq Mean Sq  F value Pr(>F)  \nd.srcalign                       7    134      19    0.965  0.455  \nd.disp                           4 139577   34894 1762.442 <2e-16 ***\nd.lenalign                       7    167      24    1.207  0.295  \nd.srcalign:d.disp               28    146       5    0.263  1.000  \nd.srcalign:d.lenalign           49      0       0    0.000  1.000  \nd.disp:d.lenalign               28     41       1    0.073  1.000  \nd.srcalign:d.disp:d.lenalign   196      1       0    0.000  1.000  \nResiduals                    45760 905995      20                    \n\n\nSignif. codes:  0 \u2018\u2019 0.001 \u2018\u2019 0.01 \u2018\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n```\nwhere column Df (degrees of freedom) is now consistent with the number of possible values. This analysis took more like a minute to run on my little Chromebook.\nGrain, salt, etc.\n. If anybody wants to try then here is the full script:\n``` R\n!/usr/bin/env Rscript\nprint('loading data4.csv file')\nd <- read.csv(file='data4.csv', sep=';')\nColumns that are \"factors\" in the experiment.\n(R would auto-detect if the values were non-numeric.)\nd$srcalign <- as.factor(d$srcalign)\nd$lenalign <- as.factor(d$lenalign)\nd$disp     <- as.factor(d$disp)\nCreate a \"data frame\" with some columns to look at\ndf <- data.frame(d$byte.cyc, d$disp, d$srcalign, d$lenalign, d$name)\nrun the analysis of variance\nprint('running analysis of variance')\nsummary(aov(d.byte.cyc ~ d.srcalign * d.disp * d.lenalign, data=df))\n```\nwhich expects to find data4.csv in the same directory.\nThe expected output:\n```\n[luke@lugano-1:~/git/r]$ ./byteops \n[1] \"loading data4.csv file\"\n[1] \"running analysis of variance\"\n                                Df Sum Sq Mean Sq  F value Pr(>F)  \nd.srcalign                       7    134      19    0.965  0.455  \nd.disp                           4 139577   34894 1762.442 <2e-16 ***\nd.lenalign                       7    167      24    1.207  0.295  \nd.srcalign:d.disp               28    146       5    0.263  1.000  \nd.srcalign:d.lenalign           49      0       0    0.000  1.000  \nd.disp:d.lenalign               28     41       1    0.073  1.000  \nd.srcalign:d.disp:d.lenalign   196      1       0    0.000  1.000  \nResiduals                    45760 905995      20                    \n\nSignif. codes:  0 \u2018\u2019 0.001 \u2018\u2019 0.01 \u2018\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n```\n. Handy link: R Tutorial that was the best one I found.\n. Here is a Google Doc summarizing some more thoughts that my mum shared about this data set. I haven't fully digested them yet.\n. I have not done any testing myself. I suspect that LuaJIT upstream is the best place to discuss this work.\n. Closing this PR. The intention was simply to advertise the fact that proof-of-concept intrinsics support exists for future reference.\n. On a high level I reckon we should start to separate app networks into two parts: the application logic that runs on a given core (in the middle) and the I/O connectivity (at the edges). Ideally the application logic part could be equally happy when connected to many different I/O mechanisms (intel1g, intel10g, mellanox, tuntap, vhost-user, etc). See also #720.\nOn a low level each application needs to decide how it wants the traffic sharded between processes and how to accomplish that.\nFor example, do you want the outside world to see a single 100G lwAFTR (single L2/L3 address) or 10x10G ones (separate addresses)? If the former then you want traffic to be hashed across your instances e.g. using the hardware feature RSS with one process handling e.g. ARP/IPv6-ND and other traffic hashed across all instances. If you want the latter then you want packets to be dispatched between instances based on a key e.g. destination MAC address and then VMDq is more suited.\nThese hardware features all have limitations and differences between cards/vendors. For this reason I think it is important to have a comprehensive optimized traffic mux/demux capability in software (#691) and to view hardware mechanisms only as optional offloads to be used when application and NIC happen to match.\nThe intel1g driver makes it optional to initialize the NIC or not. The idea is that if you run 10 processes then process 0 could be responsible for NIC init while 0-9 would merely attach to TX/RX queues. I want to migrate all current/future drivers towards this model if indeed it works out in real life. (If it is really too constrained for only one process to touch the common registers on the NIC then we could introduce a mutex as a shm object or something like that... we have options.)\nThat any help?\n. cc @xrme\n. Check out RSS. If you are lucky it can hash packets in a suitable way and ensure that ARP/ND are handled neatly too (e.g. always sent to the same process if that is important to you). It is a much simpler hardware feature than VMDq. If you hit a dead-end there then you need to talk with @xrme about being an early adopter of software mux/demux (#691).\nI suspect that RSS will match the lwaftr well. I would be less confident with a stateful NAT because then you have to be stricter about hashing e.g. that the same customer is always hashed onto the same process in both directions. Your application is probably okay with a bit of asymmetry?\n. Looking ahead a bit...\nI also quite like the idea of having N+1 process structures i.e. N busy-looping traffic processes + one coordinator process that is not processing heavy traffic but could be \"punted\" certain packets e.g. ARP/ND and perhaps act as a non-realtime-constrained front-end for e.g. configuration changes.\n. Just an artifact. Just make process 0 attach to RX queue 0 and TX queue 0, etc, and poke the RSS register with a suitable hashing configuration (e.g. on L3/L4 header). Should work fine. I have some past experience with RSS on the 82599 from before Snabb Switch.\n. @kbara @xrme was also talking about some ideas where we could define the overall app network in the \"+1\" management process and it could fork/spawn all of the worker processes. That could be neat.\n. @wingo I reckon you will be able to extend the intel10g module easily enough for this use case. The PF object is basically what you want to initialize the NIC and define how packets should be dispatched across queues; the VF object is basically what you want to come along later and attach an app to one TX queue and one RX queue. You just need a variant (config option or new object) that does this with RSS instead of VMDq and this should be straightforward because RSS is a simpler mechanism.\nThis is assuming you don't find any blocking problems with RSS when looking at its capabilities in the data sheet.\n. (and you need to deal with the fact that the PF and VF objects will be instantiated in different processes but that should be okay. If you have a fixed number of processes then you should be able to \"set and forget\" the RSS settings to dispatch across queues 0..n. This is more tricky for VMDq in the NFV application where each process will want to dynamically provision new queues for new MAC addresses.)\n. ... oh and then there is the question how \"How do you do RSS on Virtio-net?\"\nThere is multiqueue support for Virtio-net but the details of dispatching seem underspecified to me compared with hardware mechanisms like RSS and VMDq. The basic strategy I have in mind is to be flexible on the NFV application and try to support extensions for whatever kind of hashing the VMs really need. Still: as an application developer I would want a software mux/demux as insurance to make sure the application can be deployed on diverse I/O infrastructure.\n. @eugeneia Fine: you reject all these ideas as invalid.\nHowever: the documentation does need more enthusiastic users and contributors. How do we make that happen?\n. Can also be that I simply don't understand how documentation is supposed to be consumed. Is there a canonical recommended way to consume the documentation?\nI know that I can read markdown sources in Emacs, or that I can browse online via Github, or that I can manually build a HTML file and copy that somewhere my browser can access it. Which of these am I expected to do? (Or should I be doing something else?)\n. @kbara None of them work to my satisfaction and I would like to know which is the most important to fix.\n. (Historical note: Snabb Switch was originally a kind of literate program that formatted comments in source files as markdown and interspersed them with code.)\n. @eugeneia How about this for a more concrete plan:\n1. Define what is the canonical manual for Snabb. This could be a local directory in the source tree, a Github URL, a HTML file, a PDF file, etc but we should decide which of those things is primary.\n2. Define how to access it e.g. a canonical URL for finding a relevant and well-defined version.\n3. Treat any defects in this format of the manual as bugs and fix them.\nThis would make me happy: I could identify the manual, I could refer to it easily, and I could contribute improvements to it.\nSo question for you @eugeneia: does this sound like a reasonable plan and do you see a way forward to lead us down?\n. Great! I can totally get on board with having a stand-alone HTML manual as the primary documentation that we give loving attention to and prioritize above all other mediums. I think you are right in your comment https://github.com/SnabbCo/snabbswitch/pull/764#issuecomment-187191216 that we can still keep the markdown sources Github-friendly anyway.\nHow about Github Pages for hosting? I am not in favor of using a lab server in my basement as a production webserver for the project. I also see davos as long overdue for migration to NixOS... but that is a separate topic :-)\n. Merged!\nI would prefer to have shorter summary lines on the commits so that they don't get elided/truncated when browsing history. See the ... markers on the commit list above. This is to keep the history easy to browse with limited columns and wanting to include other info e.g. date, author, etc.\n. I agree that we want a CONTRIBUTING.md but I think it should be a half-page of helpful information to make contributors understand the process of landing code in Snabb.\nLet's please avoid lots of rules and policies that need active policing. My view is that consistency breeds consistency with the code and documentation. If we start sending PRs that improve consistency then this will become a pattern and the results will speak for themselves. A picture says a thousand words, etc.\n. Generally when somebody contributes code to Snabb I think it would be productive to assume that:\n1. They have done something valuable that we can benefit from as a community.\n2. They are smarter than we are.\n3. They have other important things to attend to.\nOn the other hand we know the Snabb code and workflow better than they do. We can help people understand how their code will be merged, make technical suggestions that we think people will appreciate, flag important issues that should be addressed before the code is merged, and make unsurprising corrections ourselves (\"I removed the copyright line and added a license line as described in CONTRIBTING.md\").\nI think we need to be very careful to respect the time and intelligence of the contributors to the project and so e.g. not insist that they read the essay \"L. Gorrie: Treatise on the nature of simplicity in software development\" before they can be considered qualified to contribute to Snabb.\nI need to spend some time reflecting on whether I am following these guidelines in practice...\n. ... putting my money where my mouth is: I swept through and added license references to the entire code base on #729. I hope that doing this consistently will establish the idiom i.e. people will notice that every existing file has such a notice and tend to follow suit.\n. I noticed this while doing some hacking on my space-constrained Chromebook CB3-111 laptop. Interesting that storage space for source tree is a metric that we overlooked in the beginning of the project.\n. First observation: a table of contents is needed to navigate this much content.\n. Could also move each library into a dedicated lib/foo/ subdirectory with its own README. Then on Github the directory listing would act as a table of contents. Could also hack require() to make short names like require(\"lib.ctable\") work.\nHowever: I really think we need to nail down which user experience we are trying to optimize foremost. On the one hand if we focus on a stand-alone HTML/PDF/EPUB manual created with pandoc then we presumably have a lot of options for structures and showing chapter contents and so on. On the other hand if focus on reading via Github then the file layout is important and we have to make a trade-off between clutter vs latency of clicking around in subdirectory links.\nSo I think we need to resolve that question over on #758 first i.e. which documentation user experience are we primarily optimizing?\n. @wingo Fair question. My $0.02 is that the Github UI is too laggy. In practice I am not willing to click around there looking for documentation: I read the sources in my editor instead (like @kbara). I see this as a negative thing because it makes me lose touch with the \"real\" documentation and not be motivated to improve it.\n. I hope that having a browser tab with the complete Snabb manual open and easy to navigate will fix that situation. (Or if not already open then at least auto-completing as a URL in my browser.)\n. @capr do you have a suggestion for how we could make require(\"foo\") automatically consider require(\"foo.foo\") as a fallback? I know that ordinarily the Lua path can be used to make an init.lua load but I don't think LuaJIT supports anything like that when loading modules that are compiled into C objects. So we would need to make some kind of extension e.g. wrapper function around require or hack the LuaJIT C code or new function in package.loaders or ... something?\n. > Modules are APIs and APIs are not hierarchical in nature, the API dependency is a graph. \nThe hierarchy is a way of constraining the dependency graph to provide useful structure. Core does not depend on lib, lib does not depend on apps, apps do not depend on programs. The lower layers can be understood completely without reference to the layers above.\nThis is the same as e.g. a Unix system: the kernel does not depend on libc, libc does not depend on Firefox, etc. This layering seems very typical in my experience i.e. trying to avoid cycles in the dependency graph both between nodes and groups of nodes.\nThis is practical. For example it allows us to be quite strict in core (refuse duplication, ensure consistency, etc) and a bit more liberal in lib (provide alternative implementations to see which becomes the most popular, introduce new programming styles that may or may not catch on, etc), and downright libertarian in other areas (e.g. allow programs to do whatever they like provided it doesn't mess up the build). Good fences make good neighbours.\n. @wingo I would say that any dependency from core to lib is a bug. The whole core.lib module is really a \"where should this go, anyway?\" miscellaneous dustbin. Refactoring this to be consistent with the layering model would be valuable.\n. @wingo Having said that, there is no big prize for absolute perfect consistency, so it is not an acute problem for a small number of dependencies to break the rules. The main motivation to fix this is the Fix Broken Windows principle.\n. @capr So we are talking about two things: the structure of the code - dependency graph - and the presentation to the user - API.\nI have described the structure above. This seems sound to me: I don't recognize any objections to this aspect.\nI agree that the API is haphazard and that is why I created issue #734. We can work out a better way once @kbara is available to take the lead on that.\nReasonable?\n. @capr Relating the structure vs API idea to Unix too: I see this as like when as a user you call an API function like clock_gettime(). Are you calling the kernel? Or the C library? Or the kernel via the C library? You don't really care and the answer may be implementation dependent e.g. the C library may access one time source directly from the CPU and another via the kernel.\n. The filesystem layout is another question. I care much less about that aspect personally.\n. @eugeneia I don't really feel good about releasing v2016.03 with the documentation style in lib/ becoming less consistent. I am not sure if we can still fix that. The only solution I see is what both @wingo and @capr have suggested i.e. separate README files in the same directory for each module (lib/pmu.md, lib/ctable.md, etc). That would avoid the whole mess of moving things into subdirectories and making require() still work in the same way. What do you think?\n. Obsoleted by #790.\n. Thanks for the quick feedback! Great that we are starting to successfully churn code in lib and share code between applications.\n. Depends on (includes commits from) #766 to make the engine create the counters locally for each app instance.\n. @eugeneia Cool idea :)\n. Great catch, SnabbBot! Have to think about correctly managing the lifetime of those counter objects to avoid errors and/or memory mapping leaks when recreating app instances.\n. (Added [wip] because of bug that needs to be fixed. Possibly overkill since SnabbBot has already marked this change as failing tests.)\n. Thinking about how to manage the lifecycle of counter objects. How about if we leave that to the engine?\nFor example we could define a new app callback:\nfunction myapp:counters ()\n   return {'drop_bad_checksum', 'drop_bad_cookie', ...} \nend\nand then the engine could create these on app init (and delete them on app stop) and make them accessible via:\ncounter.add(self.counter.drop_bad_checksum, 1)\nReasonable? I am not sure what a cleaner alternative would be that is not error-prone.\n. Exciting stuff!\nThis is the first time we undertake to merge a long-lived application branch onto master.\n. @eugeneia will you be upstream for this? That would be great if so :) Just mark yourself as Assignee.\n. @ nikolay @wingo On reflection that nice structured workflow with topic branches, reviews, etc, is a great benefit of working with upstream and maybe in the future it would make sense for new programs to be part of that workflow from the very beginning. We are still feeling our way around in terms of seeing how to successfully \"incubate\" new Snabb programs with their own development teams.\n. @capr @eugeneia This PR seems to be stuck in limbo. Let's get it unstuck and merged!\nHow about quickly putting the branch into ExtremeNormalForm, then merging it upstream, then continuing development with topic branches and pull requests?\nHere are some concrete ideas for addressing the \"TODO\" items above so that they are not merge blockers:\n- IPsec support is not finished. Fine. Just delete that code from the branch so that it doesn't block the merge. The code can be reintroduced on a topic branch that can be merged when it is finished.\n- More tests and benchmarks are needed. Fine. Just create Issues and/or Pull Requests for these after the code is merged upstream.\nGenerally the best strategy for new applications seems to be to merge something simple and clean as quickly as possible so that they can benefit from the upstream workflow.\nThe pitfall I see is to try to do too much before bringing the code upstream e.g. treating every unfinished feature or missing regression test as a blocker. These may be blocking something but surely not participation in the upstream community :).\n. I notice that @wingo \"squashed\" the lwaftr history in https://github.com/eugeneia/snabbswitch/commit/8364729b1cc4975e1c03e389de162e7e1bd7f3ed before merging to simplify the history. Could do the same here as the very last thing before merge if we want to shorten the history.\nStill: content is king :-)\nLooking ahead it seems like new applications would do well to get into the upstream development process earlier e.g. from the very beginning. Even if an application's branch is not actually merging onto master it can still be registered in branches.md and developed with PRs via the main repo.\n. @capr thanks for the input. @eugeneia as upstream maintainer can you define the shortest path to merging this code?\n. On reflection the idea of squashing changes seems like quite a can of worms compared with merging.\nMerging is simple: branches can bidirectionally merge each other and each time they will pick up any commits that they were missing. If this history seems messy then you need to use more sophisticated query arguments to git log e.g. --first-parent and so on.\nSquashing: How does this really work? Is this a one time thing where the application developer discards the history of their application and then continues working based on the upstream history? Or would the application developer do a dance to maintain their own private history in parallel with the upstream project, akin to git subtree?\nI don't think I can recommend squashing as an option until I understand it better in this context. I wonder how this is going to be managed with the lwaftr and whether we might end up merging its complete history in the future in order to simplify maintenance.\n. @wingo ah thanks for clarifying :). yes that will work and I suppose the consequence is just that if you need to search back into pre-march-2016 history you need to remember to look in another repository for that. not the end of the world. okay, so I am fine with both alternatives again, modulo whatever details I am still failing to understand :)\n. Ping...\nCan we land both IPsec and Lisper upstream now? Or what is blocking?\n. Short and sweet!\nI would like to tweak the COPYRIGHT section so that we are not encouraging people to register their copyrights but merely telling them how they can do so. For example:\n\nIf you want to record a copyright notice with your contribution then you can optionally do this in the file src/COPYRIGHT. (Copyright notices inside source files will be rejected or removed because that is not our style.)\n. Long term the goal is to bootstrap us into the Linux model where every change has an unambiguous upstream branch based on its subject matter (documentation vs intel drivers vs virtio drivers, etc). The actions we take should be leading us in that direction. But how? :-)\n\nMore immediately: if maintainers are getting frustrated that nobody merges their patches then it may be a general process issue with assigning upstreams. In that case it could be counter-productive to create an exception mechanism that only works for maintainers since everybody else will be stuck.\nShould we experiment with a different way of assigning upstreams? I would be willing to do this manually based on my own perception of who should review a change and how much they are backlogged if we want to try that.\n. (Or maybe the process today works once we declare that PRs to master should be assigned an upstream to merge them, even when submitted by maintainers.)\n. ... blathering a bit ...\nCould also define a tag needs-upstream that any of us can apply simply to flag when a change risks being reviewed with no clear path forward e.g. immediately after a PR is opened. (This state is perhaps implicit by looking for PRs that are not marked [sketch] and have no upstream but maybe explicit is better than implicit, I dunno.)\n. > The only things that are ever freelisted are packets. It therefore might make sense to remove the type parameterization as an unnecessary abstraction.\nThis sounds like a good idea in light of @wingo's observation.\nLuaJIT generates code that is very type-specialized and every non-trivial call to ffi.typeof() returns a new type from the JIT perspective. This means that hypothetically if a function were using multiple freelist objects then the JIT would see them all as distinct datatypes and the type guards in generated code would often fail and cause branches off the fast path.\nThis can be avoided by hoisting the ffi.typeof() call up to the module level.\nI don't think this would actually happen today with the code as written - we're only working on one freelist - but if you were already thinking of simplifying the type then this would be another reason to do so.\nThis whole \"FFI ctype diversity\" JIT issue is something that @alexandergall discovered relatively recently in #612.\n. @wingo will you be upstream for this one?\n. > The apparent performance regression is bothering me.\nThis is bothering me too. I want a well-defined way to reach definitive conclusions on these issues. So that we could say with certainty what is the impact of the FFI freelist and of different possible alignments. I am feeling around for steps we could take in this direction.\nThe basic1 benchmark should be the most sensitive to freelist performance. It does something like 100M packet freelist operations per second. (The iperf-1500 benchmark is actually dominated by totally different factors e.g. SIMD checksum routine performance which is more reason to doubt the significance of that result.)\nI compared 100 runs on this branch with 100 runs of the master branch on lugano-1 and collected the results in a spreadsheet. I see 0.9% difference. I have not done an analysis of variance to determine whether this is more likely due to background variation (significant) vs the code change, I am not quite at this level of sophisticated yet, but I would suggest that the difference seems very small indeed and not likely something we should worry about here and now.\nI want to start doing more thorough A/B performance evaluations of release candidates on the next branch so that we can look at the sum total impact of the changes we have merged and then go digging in more detail if we find significant regressions. It's probably overkill to do this much analysis on individual PRs.\n. For an exercise I made a CSV file from Nikolay's data:\n$ cat nn.csv\nbase,patched\n27.6,28.3\n29.5,30.0\n29.4,29.9\n29.4,29.8\n29.0,30.0\n29.2,29.8\n29.4,29.3\n29.4,30.0\n29.3,30.0\n28.5,29.9\nand wrote an R one-liner to summarize that:\n```\n$ R\n\nsummary(read.csv(file='nn.csv'))\n      base          patched\n Min.   :27.60   Min.   :28.3\n 1st Qu.:29.05   1st Qu.:29.8\n Median :29.35   Median :29.9\n Mean   :29.07   Mean   :29.7\n 3rd Qu.:29.40   3rd Qu.:30.0\n Max.   :29.50   Max.   :30.0\n```\n\nand then I reformatted the data into another format, which I am sure could have been done within the R script instead:\n$ head -3 nikolay.csv \ncode,Mpps\nbase,27.6\nbase,29.5\nso that I can easily ask R for a post-hoc analysis using Tukey's test:\n```\n$ R\n\nnn <- read.csv(file='nikolay.csv')\ndf <- data.frame(code=nn$code, Mpps=nn$Mpps)\nTukeyHSD(aov(Mpps ~ code, data=df))\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Mpps ~ code, data = df)\n$code\n             diff        lwr      upr     p adj\npatched-base 0.63 0.09845798 1.161542 0.0227726\n```\nwhich tells us that the measured average difference from patched to base is +0.63 Mpps and we are 95% confident that the real difference is between +0.1 Mpps and +1.16 Mpps. (lwr and upr are the lower and upper bounds that we are 95% confident of.)\nThis says to me that the change looks promising but it would be interesting to run with more measurements so that we can say with 95% confidence that the difference is more than 0.1%.\nNikolay if you want to submit that change on its own PR I would be happy to poke around at it as an exercise in data analysis.\n. This all sounds spot-on to me.\nHaving a global freelist protected by a spinlock seems quite neat to me when we are allocating and freeing packets in batches to amortize any cost of the lock. This is very much in the spirit with the rest of Snabb i.e. doing something simple (like polling) that would be too expensive once-per-packet but is okay once-per-hundred-packets.\nOne more topic: How do we recover from the failure (crash) of one or more of the processes?\n. (The idea that I came up with in the past while pondering recovery from process failure was to garbage collect packets: #572. I am not sure if that really makes sense or not. Could also be that having a distinct \"director\" process permits a simpler solution than operating peer-to-peer.)\n. IOMMU will become interesting if/when it is ubiquitously available i.e. when we can assume that a reliable hardware and software implementation is available on machines where Snabb will run.\nI don't think that we are there yet. Sandy Bridge CPUs have known performance problems with IOMMU and in practice people are running performance tests on these machines today. Linux VFIO support is also relatively new, and buggy as of at least a few years ago, so I am not sure when we can depend on having reliable kernel support available e.g. on LTS releases of popular Linux distros.\nI can imagine that in 2017-2018 timeframe we could assume that IOMMU is available on Linux/x86-64 machines and potentially exploit this, but a lot can happen in that timeframe too e.g. supporting other platforms that don't have IOMMU.\n(The reason we removed the VFIO code before is that we needed to maintain it in parallel with the non-VFIO code and that created rather than reduced complexity. Having said that we absolutely do need to fix our problem of unpredictable behavior on Linux when booted with iommu=on and this may involve some calls to VFIO or UIO modules.)\n. (It is also possible that VFIO/IOMMU will never be interesting due to quirks in the implementation. For example they have this concept of groups of devices that need to share the same memory map that does not necessarily match application requirements. I think the main use case for IOMMU is to map whole PCI devices directly into virtual machines and the assumption is that people who need to do that are willing to deal with the complications involved.)\n. > I can imagine that in 2017-2018 timeframe we could assume that IOMMU is available on Linux/x86-64 \nOn reflection I may be too conservative here. It's possible that adopting IOMMU/VFIO today would make the code simpler and easier to deploy. I am sure we could live with the Sandy Bridge performance ceiling (~10Mpps/socket) and the kernel dependency may not be any worse than our intel_iommu=off dance today. If somebody wants to try switching us over to VFIO then would be cool to take as a separate issue/PR. This may even be fairly easy to do via ljsyscall nowadays.\nI see this as mostly orthogonal to multiprocessing support so we should move the discussion off this issue. We would need to somehow cook up a consistent memory map to share between PCI devices and multiple processes but presumably this would be possible.\n. @kbara The best is the enemy of the good. I have been pondering for several years how to make a script like this with finer granularity and never arrived at a solution.\nI see this script as potentially boosting the useful capacity of the lab by an order of magnitude. That is because I believe that our servers are actually used less than 10% of the time (while at least one snabb process is running) while they are nominally used nearly 100% of the time (while somebody has their name associated with them).\nThis is also part of my thinking with acquiring more smaller servers for the lab. If we have 10 machines then taking a machine-wide mutex is not such a big deal compared with e.g. everybody working on chur at the same time.\n. Further thought: We could use isolcpus to reserve the majority of the CPU cores and insist that people only use these cores while they are holding the lock. Then background processes (emacs, make, git, nix-env, etc) should not disrupt ongoing test runs.\n. (Can be that this script really belongs in the snabblab/snabblab-nixos repository together with machine init code to create /var/lock/snabb with suitable permissions. I could send this script there in a new PR if nobody shoots it down here first.)\n. @eugeneia I would suggest that we consider this as a future extension if and when it becomes a practical problem that people are waiting too long for their tests to acquire locks. Then we can consider making the locking more sophisticated vs other alternatives like acquiring more hardware.\nGoal here is to solve the problem of everybody wanting to work on a separate machine for fear of colliding with each others' tests. I suspect this is costing us most of our effective lab capacity.\n. Note: These days our new servers (lugano model) only have a small number of CPU cores and PCI cards. On these it is not so wasteful to lock a whole machine. This is the environment I have in mind for most people's day-to-day development and testing.\nYou might say that the new servers have been chosen to make this script easy to write and use. (Or in other words to make the lab resources easier to partition and share.)\nThe situation is more complex e.g. on a large server with a lot of resources e.g. 24 cores and 20x10G on Grindelwald. However if we would reserve these beasts for testing highly parallel workloads then again it becomes reasonable to use machine-wide locks on them.\n@eugeneia I suppose you are thinking in terms of davos and chur that have about 10 different models of network cards installed. These machines could indeed support many people doing independent driver development at the same time with fine-grained locks. Perhaps we will want to improve on this script's workflow for those machines.\nFor now I would be happy with the modest goal of optimizing utilization of the lugano servers.\n. Closing. I reckon this script belongs in the snabblab repo and hopefully @domenkozar and cherry pick the source from here.\n. I was imaging keeping it unscheduled i.e. we do it when we discover a problem that is likely to cause people pain and we want to fast-track a patch. For example could even be on the day after a release if we discover that we missed a Makefile dependency that will cause a compilation error after git pull as nearly happened recently: not much value in keeping people waiting for a fix to such a problem.\n. ... but I suppose that could be an exception and maybe it does make sense to try to release generally useful but non-critical fixes around the middle of the cycle.\n. @eugeneia I want to thoroughly benchmark the next branch against the current master. The challenge is that I have never been able to get all of the make benchmarks running on the NixOS lab servers. The nearest I have come is #744 which runs everything but the iperf tests.\nCan you possibly have a look and see if you can get them running on e.g. one of the lugano servers? @domenkozar can help with NixOS questions but I think it is more of a docker/test-env challenge. Probably there is some obvious problem and I simply failed to find the relevant log message.\nI wrote up a related issue on snabblab/snabblab-nixos#13.\n. ... I suppose that I can run in the fully dockerized environment to avoid this problem. That is probably the best solution for now. I will try it.\n. Thus spake the wise man:\n\nA little knowledge is a dangerous thing.\n\nI have taken @sleinen's bait and started learning R for making sense of test results. Bear with me, I am surely doing it all wrong. Here is a first visualization of around 200 test results from lugano-1: comparing master and next by running the DPDK packet forwarding benchmark with a DPDK 1.7 guest and three different packet sizes (64, 128, 256):\n\nThis seems to be telling us an encouraging message: that 256-byte consistently  hits line rate (~4.4 Mpps), that 128 and 64 byte both exceed this, that variance is low, and that master and next are much the same. If this is indeed the message then this seems like a valuable kind of test to run (and particularly on release candidates).\nThere is one outlier with next and 64-byte packets: it would be interesting to make that dot into a hyperlink to the relevant log file (which I know can be done in R using SVG output).\nI also found the function aov (analysis of variance) and that seems to basically work:\n```\n\nsummary(aov(dat17.mpps ~ dat17.branch * dat17.packetsize, data=df17))\n                               Df Sum Sq Mean Sq   F value Pr(>F)  \ndat17.branch                    1   0.00    0.00     3.244 0.0732 .\ndat17.packetsize                2  75.22   37.61 33379.059 <2e-16 ***\ndat17.branch:dat17.packetsize   2   0.00    0.00     1.469 0.2328  \nResiduals                     195   0.22    0.00                     \n\n\nSignif. codes:  0 \u2018\u2019 0.001 \u2018\u2019 0.01 \u2018\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n```\nIf I am reading this correctly (massive \"if\") then it is saying that different packet sizes have a major impact on performance (as expected) but different branches do not (as desired i.e. same perf from head and next).\nI like the fact that with R this is all scripted and so analysis like this could easily be integrated into the CI.\nEnd braindump. Can be that we look back on this post and chuckle at how I managed to completely misinterpret every aspect of R on my first day trying to use it. But that is then and this is now so I am feeling quite proud for the moment :).\n. (The cool thing about the analysis of variance is that it can tell you both how much effect a variable has and how likely that is to really be caused by random background noise. I am really keen to explore looking for relationships between multiple factors in the context of #755 e.g. how sensitive are memcpy/checksum to alignment and how much does that depend on which level of the cache hierarchy they are hitting.)\n. cc @domenkozar could be that we could have Hydra run these kinds of analysis and make the results available for every PR.\n. Here is a picture of the same test above but with the DPDK 2.1 version of l2fwd:\n\nOn the one hand it is good for the v2016.03 release that master and next have consistent performance. On the other hand we see both lower performance overall (#665) and surprisingly lower results for 128-byte packets compared with 256-byte packets. Performance is sensitive to guest behavior in ways that we don't understand yet.\nI am excited to press ahead with parallel efforts of measurement and optimization. These are both really valuable and mutually beneficial activities. There is a lot that can be done in both areas and for more applications. (I am focusing on the NFV application here only because it is the one that has upstream performance tests available: going forward I see this activity as encompassing all programs.)\nOnward! One step at a time...\n. @mwiget mentioned seeing lower performance (~10%) when leaving an application idle for a while before starting to process traffic. This is likely some JIT behavior that we need to understand and correct (following @wingo's fine example with #707). The way forward that I see would be to define a test case where the waiting time before processing traffic can be controlled so that we can measure its impact and see which code changes successfully reduce variance.\nThis is all extremely interesting from my perspective :-)\n. Hey wow cool awesome!\nI asked R to do an analysis of variance with the branch, the packet size, and the container (DPDK version) and here is what it says:\n```\n\nsummary(aov(dat.mpps ~ dat.branch * dat.packetsize * dat.container, data=df))\n                                         Df Sum Sq Mean Sq   F value Pr(>F)  \ndat.branch                                1   0.01    0.01     1.857  0.174  \ndat.packetsize                            2  54.59   27.30  7321.203 <2e-16 \ndat.container                             1 271.85  271.85 72912.023 <2e-16 \ndat.branch:dat.packetsize                 2   0.01    0.00     0.734  0.481  \ndat.branch:dat.container                  1   0.00    0.00     0.001  0.970  \ndat.packetsize:dat.container              2  29.06   14.53  3897.513 <2e-16 ***\ndat.branch:dat.packetsize:dat.container   2   0.00    0.00     0.037  0.964  \nResiduals                               390   1.45    0.00                     \n\n\nSignif. codes:  0 \u2018\u2019 0.001 \u2018\u2019 0.01 \u2018\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n```\nand if I am interpreting this correctly then it is really providing valuable information that is consistent with the graphs above:\n1. Switching branches (master/next) does not make a difference.\n2. Switching packet size does make a difference.\n3. Switching containers also makes a difference.\n4. The differences caused by the packet size and the container are dependent on each other: packet size impacts each container differently (128-byte is faster with DPDK 1.7 and slower with DPDK 2.1).\nThis starts to look like exactly the thing I was looking for in #755 as a means to search for interactions between data alignment (src/dst/len) and memory hierarchy for different functions. And we already have the data in a CSV file to try this out with!\n. Charging ahead with R and surely making a generous helping of mistakes...\nPost hoc analysis with Tukey's test is another way to use the ANOVA results. This can be easier to interpret because it tells us how much the average changes between groups of results: the best estimate and the +/- range that can be predicted with 95% confidence.\nHere is what Tukey's test says about the effect on Mpps of branch\n(master vs next):\n```\n\nTukeyHSD(aov(mpps ~ branch, data=df), ordered=TRUE)\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n    factor levels have been ordered\n\nFit: aov(formula = mpps ~ branch, data = df)\n$branch\n                   diff        lwr       upr     p adj\nmaster-next 0.008300208 -0.1769718 0.1935722 0.9298626\n```\nThis says that the estimated difference is 0.0083 Mpps (i.e. very small) and we predict with 95% confidence that if we reran the test the effect would be within -0.177 and +0.194 of that value (i.e. not clear that there is a positive or negative difference overall).\nHere is the same analysis but with instead comparing the DPDK/L2FWD version in the guest:\n```\n\nTukeyHSD(aov(mpps ~ dpdk, data=df), ordered=TRUE)\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n    factor levels have been ordered\n\nFit: aov(formula = mpps ~ dpdk, data = df)\n$dpdk\n                    diff      lwr     upr p adj\ndpdk1.7-dpdk2.1 1.644667 1.554203 1.73513     0\n```\nThis estimates that DPDK 1.7 shows +1.64 Mpps in this dataset and we can say with 95% confidence that the real difference is between +1.55 and +1.74 Mpps.\nCool information!\nJust for another angle, maybe pointlessly fancy, we can also look at the effect of branch and DPDK version in combination:\n```\n\nTukeyHSD(aov(mpps ~ branch * dpdk, data=df), ordered=TRUE)\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n    factor levels have been ordered\n\nFit: aov(formula = mpps ~ branch * dpdk, data = df)\n[... clipped redundant info ...]\n$branch:dpdk\n                                     diff        lwr       upr     p adj\nmaster:dpdk2.1-next:dpdk2.1   0.008071301 -0.1602539 0.1763965 0.9993226\nnext:dpdk1.7-next:dpdk2.1     1.644434343  1.4748576 1.8140111 0.0000000\nmaster:dpdk1.7-next:dpdk2.1   1.652963458  1.4846382 1.8212887 0.0000000\nnext:dpdk1.7-master:dpdk2.1   1.636363042  1.4680378 1.8046883 0.0000000\nmaster:dpdk1.7-master:dpdk2.1 1.644892157  1.4778278 1.8119565 0.0000000\nmaster:dpdk1.7-next:dpdk1.7   0.008529115 -0.1597961 0.1768543 0.9992013\n```\nwhich makes it look like the effects are independent: same results with same DPDK version, consistent 1.6Mpps boost with DPDK 1.7 vs 2.1.\n. (FYI: My mum tells me that this use and interpretation of Tukey's test is right.)\n. @eugeneia ship'able from my perspective.\n. I believe the issue is a bug in the selftest function (not in the driver). See also this thread. Basically the selftest function assumes that the NIC will come to \"link up\" very quickly but the copper 10G NICs take a second or two.\nThe selftest function in the bug description is sending packets to one port and expecting half of them to arrive on the other port (but none do because the link is not up yet).\nFix would probably be to make selftest wait for linkup before sending packets (e.g. polling the driver to ask if the link is up or with a model-specific delay. Currently we don't really have a framework for scheduling events e.g. \"wait for event  from app  and then do \" but maybe that would be handy. I believe that Scratch has a simple model that we could consider borrowing.\nHarish, what is your goal here? If you want to run an application then I think you are okay to ignore this selftest failure. If you want to fix the selftest bug we can help you work out how to do that :).\n.  @harishiitd Good question. I need to make time to check into this. \n. That sounds like a great catch!!\nCan you send this change in a Pull Request? You can put [wip] at the start of the title if the fix is not complete and you want some more time before it is merged.\n. @eugeneia do you see any blockers here that we should start looking at in parallel with the work that @wingo has flagged?\n. Small request: next time can you please make next the target branch for the PR? Then Github will only show the commits that I have not already merged.\n. Merged, thanks! See #786.\n. Background question: Is there an application where it is important to disable promiscuous mode on a single-function NIC? (What's the \"why\" of this change?)\n. The reason we lack a function for MAC address lookup is historical: we have used VMDq with MAC addresses assigned by QEMU and haven't had a use for the one programmed in EEPROM.\nMy mind is on the \"straight NIC\" concept (#801) at the moment so I am viewing every new feature on the intel10g driver as a step forward on the one hand (more functionality available) and a step back on the other (further from feature parity between drivers). Still: that is a background consideration since the whole do-it-all-in-software concept is not proven yet.\n. The real problem in my eyes is lack of an immediate way to measure the impact of the assertions on real applications via end-to-end performance measurements. Removing assertions is only an optimization if it makes something important run faster: otherwise it's a bug.\n. @eugeneia On the one hand that sounds like an entirely reasonable position and I tend to agree. On the other hand contrast with the \"No hand holding!\" statement in the LuaJIT FFI documentation that sets a very different tone and is describing the level of abstraction that we are operating at.\n. In a perfect world I would like to only support safe variants and to have sufficient performance tests upstream to demonstrate that the advantage of removing assertions is negligible.\nThe historical picture is that we have started with making things safe and then people have removed the assertions during optimization campaigns.\nI am not sure how I feel about supporting both safe and unsafe variants. People will all end up using the unsafe ones in practice if they believe that they are faster. Could make things worse if one person writes the code using safe routines and then somebody else comes along and switches them when they are trying to shave cycles.\nI do think that clearly documenting the preconditions that these functions depend on is an unambiguously excellent idea.\n. Note: Checking overflow/underflow like this should be effectively free for the CPU. If there is any performance overhead it is surely due to the way the test is implemented e.g. implications for the JIT when the tests are done in the middle of long traces.\nThis suggests to me that if the assertions are too expensive then we should find a new implementation strategy rather than removing them. For example between breaths we could have a small loop that scans through packets and ensures that all freed packets had a valid length and/or that known-good \"canary\" values are found to be valid leading/trailing the packet struct. This would not detect errors quite so immediately but it may be a reasonable compromise.\n. Just wanted to summarize a bit more reflections since this will be a recurring theme in the future.\nOn the one hand:\n- I am firmly in the camp of wanting safety features built-in and always available. I am reminded of the computer science classic The Emperor's Old Clothes.\n- I see much more benefit in chasing 1000% speedups like multicore vs 1% speedups by removing valuable safety features like bounds-checks.\nOn the other hand:\n- Wearing my upstream hat I don't want to stand between people and their application-level performance targets. If assertions in the packet module prevent an application from reaching a performance target then I am inclined to accept a patch that removes them.\n- I am not in favor of safe/unsafe variants of functions and expecting application developers to toggle back-and-forth based on benchmarks. This is a complicated interface and in practice I would expect to see Pull Requests asking for every safe call to be replaced with an unsafe one as byproduct of people's product optimization efforts.\nOverall I think the onus is on people who want safety, including myself, to find implementation strategies that have universally acceptable costs. If this requires moving checks out of the inner loops of apps, or adopting compiler intrinsics, or using hardware breakpoints, etc, then so be it.\nI do see the statistically-significant benchmarking as one defense against overdoing the dangerous optimizations. Then we can at least avoid removing safety features when they don't actually cause problems but people are tempted to remove them anyway due to gut feeling or on the general principle that they care more about performance than safety.\n. @wingo I believe the basic way forward is for upstream to do performance tests that are much more sophisticated than people would bother to do themselves. This would incentivise people to upstream their tests (or reasonable proxies thereof) which would be invaluable for performance optimization efforts.\nI believe that I do now have the tools to run an A/B test of a change for any reproducible benchmark to test a hypothesis such as \"Performance impact is less than 0.5% with 99% confidence.\" I am happy - even eager - to do this with tests that people contribute to the project.\nI don't want to get into the habit of waving sticks at contributors. I do however feel that expecting upstream to defend invariants that are ill-defined and non-reproducible is heaping technical debt onto the project and is not the path to nirvana.\n. @wingo also note: Safety on those interfaces was there from the beginning but has been removed. The benchmarks that lead to this removal are not immediately reproducible and that is an example of technical debt i.e. we can't check whether there was a good reason to remove them in the first place nor whether that reason still holds now e.g. after LuaJIT upgrades and so on.\nI was conscious of this when I merged the PRs to remove the checks and I would be much happier if there were (say) a completely reproducible Hydra build showing the test results that the decision was informed by.\n. > Performance impact is less than 0.5% with 99% confidence\nCould also tests slightly different hypotheses like \"99% confidence that there is no performance impact.\" I am sure it will be an interesting process to define our tests in meaningful terms that lead the code in the right direction (and are computationally feasible).\n. Could remove the README prefix. The idea was mostly to keep the docs separate in directory listing and command-line completion. If somebody else says they want that then I will do it before merge :).\n. (The part that bothers me here is how user-hatingly bad our behavior is when somebody downloads Snabb, compiles it, and runs it as a non-root user i.e. print an obscure error message and dump a stack trace at them.)\n. @eugeneia You are being lucky there. Some earlier instance has done privileged operations like creating /var/run/snabb and mounting /var/run/snabb/hugetlbfs and so on. Generally speaking we don't work reliably as non-root and often print horrendous error messages.\n. @eugeneia I disagree. Our behavior is not well defined when not run as root. Better to tell users to run as root unless/until we truly nail that down.\n. I see related issues here:\n1. Identifying when - if ever - it is okay to run as non-root.\n2. Printing a clear error message when running with insufficient privileges.\nI suspect that (1) is a really narrow use case at the moment e.g. running --help if even that.\nFor (2) we already have a solution which is to call core.lib.root_check() before executing a privileged operation.\nStrawman first step: Simply call check_root() on startup and never run as a non-superuser? (Then relax once we identify well-defined functional subsets that will work reliably?)\n. @eugeneia In my mind we have never supported non-root operation and so depending on root access is not a bug (except insofar as it is not checked). IIRC there was some push-back when I tried to make root a hard requirement and so I added the check_root() instead which has now become a maintenance burden.\n. ... could also be that we really do depend on certain commands like neutron2snabb to be available without root access. This is at best inadequately documented though. I am in favor of making root a hard-requirement and then adding exceptions when they are well-defined and well-motivated on an application-by-application basis.\nCould be a good change to introduce early in the next release cycle so that people have a chance to see whether requiring root on next will cause them issues and have a chance to send in patches before that is released.\n. Sorry to be late to the party here...\nThe big question: Do we rely on flow control (back pressure) in the Snabb app network or not?\nIf we do then this change makes sense (rate limiting via app composition) but if we don't then #782 makes sense instead (rate limiting as a feature of synthetic packet source apps).\nMy mental model is that we don't want to use back pressure in the app network (#656) and in that case the original PR #782 makes more sense. This PR seems like a good test case for whether there is agreement on that idea or not.\n. @wingo Reasonable. I can accept that making all the upstream apps consistent in their buffering behavior is a separate piece of work (potentially containing some surprises) and until that is done it should not block adding new functionality like this.\nI am also skeptical of the defaults. The throttle rate seems naturally like a mandatory parameter to me too. I would also expect the \"leaky\" behavior to be the default for two reasons: it's more consistent with the plan to avoid back-pressure in apps (i.e. config option is to switch to the unusual behavior) and I also think it is what you normally want in production (where early and deliberate packet drops tend to have better end-to-end behavior than nebulous attempts at link-layer reliability at least in my experience.)\n. FWIW I have no objection to a default bucket size.\nI do think that defaults should favor production environments rather than test environments, or else we should have entirely separate apps like in the other PR (which I personally liked better).\nI may be overly sensitive because I have spent several years of my life compensating for the side-effects of well-meaning attempts to prevent packet loss on links carrying ethernet traffic, particularly side-effects introduced by ARQ in mobile networks.\n(Lots of other people are likely sensitive because of experience with buffer bloat too. I am probably over-reacting in this case but I do think one could paraphrase jwz: \"A programmer had a problem with packet loss. I know, he said, I will eliminate this by making the links buffer packets. Now he had two problems.\")\n. @wingo Understandable :).\nThis discussion has been valuable in terms of coming towards a common understanding of how to handle default parameters and ratification of the \"forward-or-drop-but-don't-hold\" app behavior.\n. @wingo Could be that I should have waited with my input for when Max sent the change upstream to me. Then I could have haggled with him about the changes that I want e.g. grumbled about introducing backpressure into the app network and asking him to push back on that in the future.\nI chimed in early because I thought you might want the chance to address the feedback earlier since you have previously expressed concern about people changing your code after the initial merge. On the other hand, that is also something that could be addressed with a follow-on PR to next if you wanted to propose undoing that.\nStill working all this stuff out. Sorry to meddle.\n. (I am sure that it is a newbie maintainer mistake to over-estimate how interested people will be in your feedback. Forgive me these small missteps :))\n. @wingo Here are the invariants I am ham-fistedly trying to protect:\n- Backpressure is not used in the Snabb app network. (That is a legacy misfeature since #656).\n- Apps designed for live traffic should not have defaults chosen for test traffic.\n- Defaults should only be provided when they are broadly applicable e.g. for the bucket size (who cares) but not for the maximim traffic rate (epsilon chance that the default is appropriate).\nSorry that I have evidently not done this very well.\n. s/protect/establish/\n. > @lukego is backpressure never appropriate for test generators?\nThat is an important question that can only be answered through discussions like this :).\nI would argue that the core simplicity of never using backpressure more than compensates for putting rate limiting into synthetic load generation apps. (End-to-end principle: simplify the core at the expense of the edges.) However, I feel that I need to introduce a PR to pitch that idea in terms of the existing codebase before I push too hard on this with new apps.\n.  @eugeneia Good catch to flag the difference between comments here vs documentation in the rate limiter readme. Certainly the docs should reflect whatever we really think.\nRequiring the bucket size as a parameter does seem like a poor user interface to me. My intuition is that, as @wingo says, there will be a sensible default value (either a constant or a function of the rate) and it would be better for the app to know/calculate this than expect the user to do so. The reason it is a required parameter is probably that the person writing the code was not confident of the answer and so punted the issue to the user (reasonable, and certainly better than choosing a bad default, but not optimal).\nI would intuitively imagine that around 1 millisecond worth of bucket capacity would be adequate but [citation needed].\n. > bear in mind that the future of Snabb includes a RateLimitingPcapReader and a RateLimitingAndRepeatingPcapReader :)\n@wingo :-)\nOn reflection I see this \"leaky vs non-leaky apps\" question as like \"eager vs lazy functions\" in a programming language. Lazy functions do permit some really lovely formulations of certain problems, but it can also have surprises consequences (e.g. if you try to sum an infinite list), and if you mix both kinds of functions then it's a whole new can of worms about what happens when lazy and eager functions call each other.\nSo with that analogy it seems like in Snabb we started off without committing to lazy vs eager, then we wrote a bunch of code that is lazy, then we got a vaguely uncomfortable feeling that we should probably pick one way or the other (probably eager), and now on this PR we start to add library code that takes a parameter saying whether to behave eager or lazy.\nThis seems to open a whole can of worms where users have to understand both evaluation models and the consequences of mixing things together. \"How come my router is showing egress packet drops after the latest update?\" \"Well, your RateLimiter app is now applying backpressure to your Intel10G app, which is causing the NIC to run out of receive buffers, which is causing it to send Ethernet PAUSE frames, which is engaging flow-control on your router port, which is causing the router to drop packets the packets that your rate limiter doesn't want.\" with addendum: \"Be careful not to add a leaky app between the NIC and the rate limiter, like an L2 switch, because then the backpressure will stop there and your router behavior will change again.\"\nToo hard. Better to just pick eager evaluation for its predictability and live with the limitations.\nThe PR that I am imagining formulating btw is to replace this \"lazy\" idiom:\nfor i = 1, link.nwritable(l) do ... end\nwith this \"eager\" alternative:\nfor i = 1, link.burst(l) do ... end\nand for a simple example link.burst() could return a constant e.g. 1/10th of the size of the ring buffer for the link (e.g. bump link capacity up to 1000 elements and burst 100 packets). That way on a breath each app would burst up to 100 packets (by convention), these would all be processed by the app network before the next pull(), and no links should overflow because the ring buffers are generously dimensioned.\nTaking this a step further we could have a related idiom for any app that generates synthetic load:\nfor i = 1, ratelimiter:clamp(l) do ... end\nwhich does not seem so bad. Kind of like how in an eager language every function that returns a list needs to take a length argument saying how many elements you want.\n.  ... So when RateLimitedPcapReader comes along then hopefully we will see a clear pattern and be able to simply add a rate parameter to PcapReader and every other app that generates packets.\nThis could also happen asynchronously with follow-on PRs if it starts to bother us that we have too many RateLimitedFoo apps in the code.\n. @wingo So next step for me is to put my code where my mouth is and propose my design in a PR, if I can really formulate it, and see if it gets shot down. Sounds good :).\nMusing and bikeshedding are a major potential problem. My mindset here is that the foreground discussion is between the submitter (@wingo) and the upstream (@eugeneia) and everything else is parenthetical background discussion that you can cherry-pick ideas from but otherwise ignore. If this is hard in practice, because the chatter is distracting, then we could declare that general musings are off-topic for incoming PRs and should be raised separately e.g. as their own PRs.\nSuccess here is if we land a steady stream of improvements upstream on master. If musings end up derailing this then they should be curbed somehow e.g. expressing ideas as [sketch] PRs rather than comments on other work.\n. \"hierarchical\", \"brevity\", \"artefact\", \"bureaucracy\"... damn these words :)\n. I should consider whether this helps solve the problem with #767.\n. @darius Yeah. I am not sure what is right for these test runs. I start this script running and expect it to continue for hours/days and provide me with a symmetric / orthogonal data set. I think I would prefer to have errors clearly logged rather than actually stopping the test run (and leaving the machine idle).\n. Reflection: There is a problem with the test execution script in that background activity on the server could impact one branch's tests more than another's. The statistical tests assume that any such background activity is randomly distributed between the tests.\nThe trouble is this structure to the shell script:\nshell\nfor commit in next no-indirect no-mrg neither; do\n    for i in $(seq $n); do\n        for dpdk in 2.1 1.7; do\n            for pktsize in 64 128 256; do\n               ...\n            done\n        done\n    done\ndone\nwhich does the testing very sequentially: run all tests for one branch, then all tests for the next branch, etc. This is a problem because if there would be some background activity on the server some of the time, e.g. some independent activity by somebody else, then that might have a big impact on one of the branches but not the others. This would lead the analysis to report significant differences when in fact there are none.\nOne solution would be to perform the tests in a random order with something like:\n``` shell\nCreate a list of test cases\ntrue > tests.txt\nfor commit in next no-indirect no-mrg neither; do\n    for i in $(seq $n); do\n        for dpdk in 2.1 1.7; do\n            for pktsize in 64 128 256; do\n                echo \"$commit $dpdk $pktsize\" >> tests.txt\n            done\n        done\n    done\ndone\nShuffle the cases into a random order\nshuffle tests.txt\nExecute the tests\nwhile read commit dpdk pkgsize; do\n    ...\nend < tests.txt\n```\nThis way the impact of background activities should be randomly distributed between the test cases. This would satisfy an assumption of the statistical analysis. Tests with background activity would still be valid and the analysis would take the variation into account when reporting its confidence level about the results (e.g. lots of background activity leads to larger +/- bounds around the results).\n. @domenkozar I am starting to dream about how to hook this into Nix. I have an idea now for how Hydra could automatically execute a test campaign across all available lab servers and publish the result. Please shoot it down and tell me what I get wrong :)\nFirst is that we would define one Nix expression for each individual measurement. The names of the expressions would be something like:\npacketforward-snabb2016.03-dpdk2.1-64byte-1\npacketforward-snabb2016.03-dpdk2.1-64byte-2\npacketforward-snabb2016.03-dpdk2.1-64byte-3\npacketforward-snabb2016.03-dpdk2.1-128byte-1\npacketforward-snabb2016.03-dpdk2.1-128byte-2\npacketforward-snabb2016.03-dpdk2.1-128byte-3\npacketforward-snabb2016.03-dpdk2.1-256byte-1\n...\nand so for a simple experiment the number of entries could be something like 2 (branches) * 2 (VM versions) * 3 (packet sizes) * 10 (measurements per scenario) = 120 Nix expressions. (More complex scenarios could have a thousand expressions or more.)\nThen we would ask Hydra to build all of these expressions. So once we start this experiment there would be 120 new items in the Hydra build queue and it would distribute them across all available slaves that meet the requirements (e.g. is one of the lugano servers that all have equivalent hardware).\nOnce that completes we will want to aggregate the results. This could be done with a new Nix expression that depends on all of those, so that they are accessible on the file system, and skims through the logs to create a CSV file and generate a report.\nCould even be that we submit just one job to Hydra - the report expression - and the test execution is done automatically when Hydra detects the expressions as requirements?\nThis sounds utopian to me:\n- Hydra would execute benchmarks across all available servers, so a 10h benchmark could be completed in 1h on 10 servers.\n- Each individual test scenario would be addressable with Nix: you could build it from scratch with nix-build (execute test), get it from a binary cache (fetch result from an earlier run), or submit it to Hydra to build somewhere / some time later.\n- The complete software environment for every test would be well-defined and permanently reproducible.\nWhat's not to like?\n. Sounds neat.\nI am thinking that the jobs would also need to run with the lock script (#773) so that they would be synchronized with other activities on the lab servers e.g. manual testing. This way we could run a Hydra slave on most/all lab servers and share them between interactive and CI use. (If needed we could extend the lock script so that interactive work would preempt/abort Hydra jobs and make those automatically loop/backoff until they manage to complete without preemption.)\n. Great! I would really love to have this. I definitely have that uneasy feeling when using table parameters that are not validated for typos etc. Commonly do make mistakes on parameters to engine.run() for example.\n@eugeneia Good question. To me core.lib is a bit of a disorganized mess and I shudder a bit when I read the documentation and see how much stuff we have just thrown in there and thus made part of our API. So I agree that putting this into core.lib would be consistent with normal practice but I am not sure how much I want to promote that. Up to you guys.\n. Merged, thanks!\nI note that sooner or later we need to address some consistency issues in src/core:\n- Documentation: in source files or in READMEs? Histogram docs are in the source file. Once the manual is online hopefully we can migrate this all into READMEs. (Until then, meh, I hate reading markdown source so I don't complain about having it in the source files instead.)\n- OO/metatables: Having histogram use one API style and counter use another would seem fine to me in lib or apps but not in core. Perhaps in the future we can unify those modules (and perhaps that will be done with some form of OO...). Something to ponder for later.\n- Source formatting. The histogram module lacks spaces before function argument lists. This is a trivial detail but it does strike me in the eyes when reading code in src/core and I think one day we should do a PR to make this subtree consistent.\nRegarding PR names I am not too fussy. I personally think of releases in terms of numbers (v2016.04) and not names (uvilla) so please don't depend on me remembering which release is called what. I also think it will not always be clear which release I ultimately merge a PR into e.g. if the PR comes late in the release cycle and changes code that I have spent a lot of time testing/benchmarking but most of the time we will probably have a common idea on this.\nOne way to provide a symbolic name to a PR would be to use a short-lived branch for the PR that ships a ready-to-merge prefix of your integration branch. You could create the branch with git checkout -b uvilla-1 max-next and then PR from that. This would also make it possible to continue merging new changes onto max-next while you wait for me to merge. (The Linux world does something similar with PRs that say \"Please pull branch foo at tag bar\" but I don't think tags and Github PRs would play well, hence the idea to use short-lived branches instead.)\nJust ideas.\n. @wingo I have separate thoughts on the local level (#740) and the global level (#734).\nLocally it all looks fine. The histogram module exports a well-defined API in a style that is familiar from core Lua modules like string.\nGlobally the change does not really fit it with the surrounding code. Core modules like engine, config, packet, link, and counter all export a function-calling API. I would like these modules to all be consistent, whether than is with a function-calling API, or with an OOP API, or with the dual-mode option.\nI would like to pick a canonical style and adopt that for the whole project. Meanwhile I would prefer not to introduce more diverse styles in the subset of our codebase that is already homogenous i.e. core/.\n. I have also been feeling conflicted in exactly the way @plajjan describes. On the one hand the pure software implementation is compellingly simple but on the other hand it feels too optimistic to assume this will always be sufficient.\nOn reflection now I feel that neither extreme is right - all hardware or all software - and I have a proposal for a compromise. Let me review the extremes and then spell out the proposal for a compromise.\nHardware-oriented approach\nThe fundamental trouble with the all-hardware approach is dealing with the variability between NICs. How do you write drivers with a consistent interface when the hardware they are controlling has different behavior?\nFor example, here are some of the questions that have different answers from one NIC to the next:\n- How can we dispatch traffic - L2, L3, L4 header? \n- Which L3/L4 protocols are recognized for dispatching?\n- Can we do exact matches and hashing at the same time - if so how does this impact the number of queues available?\n- Can we do \"hairpin turns\" on packets and does this bypass any other dispatching features?\n- Which standard SNMP/YANG counters can be sourced \"for free\" from hardware registers?\nOne way to deal with this is to say that drivers will each have a unique interface that describes the card they support. This punts the problem up to the application developer e.g. to pick one card to target and deal with its pecularities. This is basically the situation today e.g. the NFV application has targeted the Intel 82599 specifically and when we added Solarflare support we worked with them to extend their firmware for feature parity.\nHowever, in the big picture this is not so satisfactory. One problem is that it does not help applications to support many NICs and still benefting from the hardware that is available. Another problem is that hardware sometimes lets you down later in the lifecycle -- for example you get a new requirement to support a transport protocol that the NIC does not understand (e.g. GTP, L2TP, MPLS, etc) and that could screw up your application by hashing all your input into the same bucket -- forcing you to redesign the application or to new hardware.\nSoftware-oriented approach\nThe all-software approach has the virtue of uniformity and predictability. This is huge, as described above. The primary problem is uncertainty about what this costs: how much CPU must you reserve for dispatching? does it create a bottleneck that limits your peak packet rate? Then once you have these answers the secondary problem is: Is it worth it? for which applications?\nI would love to reach the point where we can say that software dispatching has modest overhead and is the right choice for all but the most extreme applications. However, in practice it is a lot of work to understand the possibilities, and there is no guarantee that the results will represent the right trade-off for application developers e.g. that the NFV application would really be better off switching from VMDq to a software switch+tag app.\nProposed compromise\nI think we need to make a pragmatic compromise that takes advantage of hardware features when available, software-emulates hardware features that are not available, and provides well-defined interfaces to application developers.\nI have a proposal for how we could do this and it is one of those \"turtles all the way down\" ideas.\nThe idea is that we could define abstract apps that become concrete when you instantiate them. The new() method of an abstract app would return an app network fragment (i.e. a config object) instead of a single app instance. This fragment could be generated dynamically based on the config supplied to the abstract app: for example if the config specifies a PCI address then the app could check the device ID and return the driver that is appropriate. If the driver does not support all of the necessary features then the app network fragment could include some supplementary software apps.\nThat is the whole idea. Let me give an example to be more concrete.\nSuppose we developed a switched_nic app:\n\nThis app can have multiple ports attached to it, can dispatch packets based on DMAC and/or VLAN, can be configured to automatically insert/remove 802.1Q VLAN tags, and can optionally provide all of the interface counters required by SNMP MIB-II objects.\nSuppose that you instantiate the switched_nic app and provide the PCI address of an Intel 82599 NIC. In this case the new() method would check the PCI device ID and recognize that all of the processing can be offloaded onto hardware. It would return a config object that specifies a suitable intel_app to splice into the app network as the implementation of the switched_nic:\n\nNow suppose that you instantiate the switched_nic with a tap device. In this case the new() method would know that emulation is needed and return an app network that includes both a tap device for I/O and a vlan_switch for emulating VMDq:\n\nIf the config required SNMP counters to be provided then we may also need to include a software app that inspects the packets and updates the appropriate tallies (unicast packets, broadcast packets, etc).\nThis would mean that applications depending on VMDq-like functionality could be deployed on every I/O medium that the switched_nic app supports.\nThe overall effect would seem to be to make life easy for driver developers (just implement what the card supports), and to make life easy for application developers (just pick the abstraction that suits your application), but difficult for people writing the abstract apps (the switched_nic app would not be trivial and would require extensive tests and documentation to give users confidence and understanding).\nThis would also allow the hardware vs software battle to continue in the background. Vendors can add powerful new features to NICs, hackers can write optimized software alternatives, and over time we will see if these keep each other in balance or whether the wind blows one way or the other.\n. I am on board with this vision! The code on #897 looks very good to me. I would be happy to bring in an 82599 driver in this style.\nHave you considered writing an X710/XL710 driver rather than 82599 next? This would punt the compatibility issues down the road a little and also give us a working driver for cool new hardware that is abundant in the lab :-) e.g. lugano-1 and lugano-2.\nVMDq could be supported in the same way as RSS here, no?\n. Cool just checking :).\nI am hopeful that RSS and VMDq can work in combination in a simple way but we shall see.\n. > let's say we need an urgent fix to something in core and have no time to talk about it with upstream. What do we do?\nI would imagine these steps:\n1. Create a topic branch for the core change, based on master, and PR that to master. This makes the change available for discussion / testing / adoption by anybody in the community.\n2. Immediately merge that change onto lwaftr or any other branches where you need it. This lets you ship the code without waiting for anybody. (Anybody who wants the change quickly can do this.)\n3. Asynchronously reach an agreement with upstream about whether the core change is the right thing or whether there is a better solution. This would need to be resolved in order to land the latest lwaftr branch on master.\nThis way the code is available to everybody from the beginning, you can ship it without synchronizing with anybody else, and the code lands upstream when everybody is happy with it. Hopefully we will have a regular cadence of merges from lwaftr onto master, e.g. monthly, and aiming to keep this rhythm will apply gentle pressure on us all to agree on the change in a suitable timeframe.\nReasonable?\nLooking ahead, who knows. Could be that the Igalians have multiple lwaftr branches (e.g. latest release vs long-term stable), and @mwiget also has a branch he is maintaining, and open source contributors are sending in changes that need to land on an integration branch before you are ready to merge them into the branches that you ship from, etc. For now having one lwaftr branch and aiming to merge that with master every month seems like one decent strategy.\nThe pitfall that I would suggest avoiding is to delay sending the core change upstream e.g. if the first time we see it is within a big lwaftr pull request rather than via its own branch and pull request.\nShall I merge this PR directly onto next? That seems like a reasonable thing to do with branch registrations to me but I can wait for somebody else to upstream it first if that is preferred.\n. I upstreamed this. Seems reasonable to land branch registrations directly on next. (Give a shout if you disagree.)\n. Sounds good!\n. You can see lwaftr on the Github branches list now. This is automatically mirrored from the branch URL that you registered. This makes it easy to see how far ahead/behind each branch is compared with master. It also means that anybody can git checkout lwaftr after cloning the SnabbCo repo.\n. > until now it's been rare that you can make the change directly on master and also test it.\nIt will really help the upstreaming process to include test coverage with changes to the Snabb core e.g. in selftest methods. If we can't be confident that the code works without separately testing it on the lwaftr then it will be hard to bring upstream.\n. The upstream branch for this change is multiproc and I have already merged it there. No action from other maintainers required here.\nThe whole multiproc branch will be submitted upstream when it makes sense to merge towards master.\n. Interesting! Please push that crude sleep somewhere e.g. your mp-ring branch so that it will show up over on #813. Seems simple if we keep using that branch and PR for discussion. I will merge it up to multiproc whenever that makes sense.\nI wonder if movnt would also avoid this ping-pong overhead. Curious to look at the PMU counters for this when I have a chance to understand the exact MESIF interaction.\n. I think it's because there is no open Pull Request from your branch at the moment. Github automatically closed #813 when I completed the merge. Should be able to start a new pull request from the same branch to send further changes.\n. @eugeneia Great work!\nCan you please make a plan with @domenkozar to migrate all of this infrastructure into the nixops-managed snabblab universe? I would like to phase out running infrastructure on single-point-of-failure servers and with out-of-tree scripts e.g. cron jobs. This is fragile and we are lucky to have gotten away with it for as long as we have.\nI would then see what solution you guys cook up and adopt that for the little script that mirrors registered branches on the SnabbCo repo :).\n. @domenkozar @eugeneia I believe we have a dependency here on snabblab/snabblab-nixos#13. That issue is closed but I am not sure if we have verified it working.\nI also think we need to be conscious of which Github credentials we install on a shared machine like eiger. This should not include write access to the Snabb repository for example. We should either lock down the Github permissions to a safe level or host them somewhere more locked-down.\nRelatedly: is it a good or a bad idea to make Hydra execute the builds and tests and for SnabbBot to check the results and report to Github? (Why?)\n. Awesome!\nI like simple black-on-white style with a floating table-of-contents. That is how I read most documentation.\nI do think that programmer friends don't let programmer friends write their own CSS though. Consider finding a popular pandoc template in this style?\n. @eugeneia I definitely see a floating ToC as a big step in the right direction. I will gladly merge.\nOn a meta-level I think we need to decide how we want to present the documentation and perhaps make a follow-on PR based on that. Some possibilities:\n1. DIY-retro-programmer style. In this case we should write our own CSS.\n2. Generic contemporary style. In this case we should pick a popular template.\n3. Clever and novel style. In this case we should hire a designer.\nI lean towards the middle option and would be happy with a generic template like the one on readthedocs. I would hope that somewhere there is a gallery of 20 popular pandoc themes that we could choose from.\nI have some specific beefs with the CSS on this PR, particularly that the left-hand-side feels cramped (horizontal + vertical scrollbars on the ToC) while the right-hand-side is empty space. However, unless we specifically want the DIY-retro-programmer style then I don't think it makes sense to work out the design via haggling on Github PRs.\nSorry to poke on what is acknowledged to be an emotional topic.\n. @eugeneia I start to come around to your point of view. I googled around for pandoc templates and to me it seems like everybody else is cooking up their own. I don't find any generic off-the-shelf template that I can recommend. On closer inspection the readthedocs CSS does look like an eyesore to me too.\nI will seek some feedback from the in-house designer at Snabb HQ :-) and we can see what we think about that.\n. @eugeneia Okay! Let's open this \"programmers talking about layout\" can of worms after all :-).\nI like the overall style on this PR. I feel like some tweaks could improve it. I asked Ann for suggestions and she whipped up an idea for some tweaks in Photoshop.\nBefore:\n\nAfter:\n\nWhat do you think?\nThe main change is to add more whitespace and then also to make H1/H2/H3 headers more distinct with shading and spacing.\n. @eugeneia Sorry, I don't know how to write CSS. The \"after\" image is only a visual suggestion, it is literally a Photoshopped version of the \"before\" image.\n. Sorry if this is a distraction on this PR but I also generated the PDF manual for the first time in ages: snabbswitch.pdf.\nJust in case we would become frustrated with HTML/CSS one backup option would be declaring the PDF manual canonical. This way we already have the formatting that Pete Kazmier did years ago and the reader software (Evince, Preview, iBooks, etc) can take care of making the table of contents accessible.\n. Sounds good.\nI do fear that maintaining raw CSS for the HTML manual will be laborious, since people will be reading this on Safari/Firefox/Chrome/IE and Desktop/Laptop/Tablet/Phone and complaining about perceived shortcomings, but let us see how that plays out. I don't have much to offer in this area.\nI can volunteer to give the PDF manual some love as a secondary format. Personally I read most reference material in PDF e.g. datasheets, Intel CPU manuals, Agner's optimization manuals, etc. This works really well for me in Preview on the Mac and in iBooks on the iPad. (evince on Linux is at least tolerable.) I also have a little shelf of hardcopies from Lulu and would love to include the Snabb manual there :).\nLikely 95% of what people do to improve one format will carry through to the other formats anyway since content is king. I do like the idea of having one format as canonical, and never shipping a release where we screw that one up, and am totally fine with this being the HTML edition once we have that in the shape we want it.\n. @eugeneia How about The Unofficial DynASM Documentation as tasteful CSS that we could reuse as a base?\n. @capr The intention of this PR is to register the lisper branch while we are still working on preparing it for merge upstream, right?\nIn that case I think you want to create a new topic-branch on top of master and cherry-pick the branch registration onto that (i.e. commit cc0f867). That way we can merge the branch registration right away and continue working out the upstreaming of the branch itself.\n. cc @xrme @kbara in case this is handy as a reference one day.\n. I ran this with sudo taskset -c 0 ./snabb snsh ./snabbmark_pmu.lua and actually running this in the lab depends on https://github.com/snabblab/snabblab-nixos/issues/23. I did a sneaky way to enable the msr module for PMU on lugano-1 like this:\n$ nix-env -i module-init-tools\n$ sudo insmod /nix/store/gxk7qq6s8hx2c20dch634xf2bh5hx6l9-linux-4.3.3/lib/modules/4.3.3/kernel/arch/x86/kernel/msr.ko\n. Finger memory! Looking forward to merging and then following the new recommendations from you and @kbara :)\n. I braindumped some thoughts on lukego/blog#15. Basically I am wondering whether the PMU can work as a wireshark for catching expensive MESIF interactions (or at least a netstat -s). Seems to me like all performance issues around multicore are likely to be interactions with the L3 cache.\n. Reflection from lukego/blog#15: I suspect that L3 cache access is the key thing to optimize for any multiprocess application (including Snabb+VM over virtio-net). The L3 cache is \"the network\" that connects the cores together and has much higher latency than anything they do locally.\nHere is a side-by-side comparison of some L3 PMU counters for the mp-ring benchmark (#804 #809) comparing single-process operation (left) with multi-process operation (right):\n[luke@lugano-1:~/git/snabbswitch/src]$ pr -w 160 -m -t a b                                                                                                                         \nBenchmark configuration:                                                        Benchmark configuration:\n       burst: 100                                                                      burst: 100\n  writebytes: 0                                                                   writebytes: 0\n   processes: 1                                                                    processes: 2\n   readbytes: 0                                                                    readbytes: 0\n     packets: 10000000                                                               packets: 10000000\n        mode: basic                                                                     mode: basic\n   pmuevents: mem_load_uops_l3                                                     pmuevents: mem_load_uops_l3\n  69.20 Mpps ring throughput per process                                           5.26 Mpps ring throughput per process\nPMU report for child #0:                                                        PMU report for child #0:\nEVENT                                             TOTAL     /packet             EVENT                                             TOTAL     /packet\ncycles                                      500,751,679      50.075             cycles                                    6,651,599,204     665.160\nref_cycles                                            0       0.000             ref_cycles                                            0       0.000\ninstructions                                562,015,029      56.202             instructions                                593,294,818      59.329\nmem_load_uops_l3_hit_retired.xsnp_hit               447       0.000             mem_load_uops_l3_hit_retired.xsnp_hit           432,740       0.043\nmem_load_uops_l3_hit_retired.xsnp_hitm                4       0.000             mem_load_uops_l3_hit_retired.xsnp_hitm        9,410,279       0.941\nmem_load_uops_l3_hit_retired.xsnp_miss              280       0.000             mem_load_uops_l3_hit_retired.xsnp_miss            9,189       0.001\nmem_load_uops_l3_hit_retired.xsnp_none              567       0.000             mem_load_uops_l3_hit_retired.xsnp_none           11,856       0.001\nmem_load_uops_l3_miss_retired.local_dram              0       0.000             mem_load_uops_l3_miss_retired.local_dram              0       0.000\nmem_load_uops_l3_miss_retired.remote_dram             0       0.000             mem_load_uops_l3_miss_retired.remote_dram             0       0.000\nmem_load_uops_l3_miss_retired.remote_fwd              0       0.000             mem_load_uops_l3_miss_retired.remote_fwd              0       0.000\nmem_load_uops_l3_miss_retired.remote_hitm             0       0.000             mem_load_uops_l3_miss_retired.remote_hitm             0       0.000\npacket                                       10,000,000       1.000             packet                                       10,000,000       1.000\n(also as a gist in case that is easier to read.)\nI see two interesting and likely related things here:\n1. The single-process version is executing about 10x faster in terms of instructions-per-cycle.\n2. The single-process version has no L3 access while the multi-process version has one xsnp_hitm event per packet. I believe this event means that the L3 cache served a hit from a line that was _m_odified in another core.\nFrom the paper cited in #735 I would expect an L3 hitm to have >100 cycles of latency and that could partly explain the whopping 665 cycles/packet performance that the benchmark is reporting. So I predict that if somebody is able to identify that hitm and reduce its frequency then that will significantly increase the performance of this benchmark.\nNow if we had working PEBS support (#631) hooked into a profiler we could also have a report telling us which instruction is triggering that hitm and which data structure it is accessing at the time. This is firmly on my wishlist for the future.\n. cc @xrme @kbara this is hopefully a useful development tool.\n. Last touch for now: I created a new gist that runs the same test but in a different order, so that the results for 1/2/3 processes are shown together i.e. easier to look at how certain counters changed between the 1 process and 2 process scenarios.\n. Thanks! Merged to next.\nRequests for next time:\n- Please write a brief description for the pull request explaining the reason for the change (above it says No description provided.)\n- Please read the diff to make sure every change is intentional (some unexplained whitespace changes are included on this PR).\n- Please don't be annoyed by upstreaming nitpicks :-)\n. I don't see a problem with the specific whitespace change here. I also think it is reasonable to make whitespace cleanups while you are already touching the file anyway. (Git even has a -w option to ignore leading/trailing whitespace when looking for diffs.)\nI did have to check a couple of things though: this is a pre section of a markdown file and those are indicated by consecutive lines that have four leading spaces (seems to render okay) and I am also processing this file with awk to mirror the branches (still understands the file okay).\nGenerally though the important thing is for contributors to know exactly what changes they are sending and to account for them with a well-written commit message. The first thing upstream will do is read the diff and compare it with the commit message and life is easier if everything is consistent. If there are surprises then it looks like the PR has been prepared carelessly.\nI would recommend that all contributors read the diff before sending a Pull Request and make sure that every change is intentional and accounted for in the commit message and/or PR text. If something has slipped in by accident, e.g. a debug printout or source reformatting by the editor, then this should either be explained in the commit message or removed with a rebase before sending the PR.\nMake sense?\n. @kbara Oh I didn't realize that change was there.\nCurrently I see on the branches list that kbara-next is fairly out of sync with upstream: 49 commits \"behind\" master (changes released but not yet merged into kbara-next) and 13 commits \"ahead\" of master (changes landed on kbara-next but not yet released on master). I think we should sync them more closely so that we can take any merge conflicts early.\nConcrete suggestion is that you:\n1. Merge latest master onto kbara-next and resolve any conflicts.\n2. Pull request kbara-next to next.\nThen I should be able to resolve the branches.md conflict and complete the merge. If for some reason I dodn't feel confident to merge kbara-next into next then I might ask you to please do that for me by merging next downstream into kbara-next.\nGenerally I think it might make sense to try and merge the various *-next branches onto next roughly once per week (when there are changes to merge). That seems like a reasonable cadence that keeps the workload reasonable and also deals with merge conflicts sooner rather than later.\nApologies if you already have a pull request outstanding that I have missed. I am trying to work out a routine for keeping track of my upstream work e.g. to poll daily for pull requests from *-next branches and poll weekly for PRs to master that may have fallen between the cracks. See #812.\n. The can of worms here is Github permissions. They are confusing. Historically at least I don't think it is possible to give somebody access to issue tags without general read/write access to the whole repo. I am not an expert though and this is a moving target lately as Github adds features.\nThe reason I suggest putting [wip] and [sketch] in the PR title is that this is under the control of the contributor. Labels are not: only maintainers can manipulate those AFAIK. If there would be a mechanism for contributors to add/remove the sketch and wip labels from their own PRs then that would be neat but I am not aware of such a possibility.\n\nI would like to replace the [sketch] and [wip] tags with labels so that they can be queried.\n\nNote: This can already be queried today e.g. [wip] in:title. Does that solve the problem anyway?\n. Interesting!\nI ran this with PMU enabled. (This should also work on lugano-4 if you give it a reboot, see snabblab/snabblab-nixos#23.)\n[luke@lugano-1:~/git/snabbswitch/src]$ make && sudo ./snabb snabbmark mp-ring --mode ff --processes 2 --events mem_load_uops_l3_hit_retired\nmake: 'snabb' is up to date.\nBenchmark configuration:\n       burst: 100\n  writebytes: 0\n   processes: 2\n   readbytes: 0\n     packets: 100000000\n        mode: ff\n   pmuevents: mem_load_uops_l3_hit_retired\n   8.30 Mpps ring throughput per process\nPMU report for child #0:\nEVENT                                             TOTAL     /packet\ncycles                                   42,160,813,384     421.608\nref_cycles                                            0       0.000\ninstructions                              3,543,164,335      35.432\nmem_load_uops_l3_hit_retired.xsnp_hit         2,788,733       0.028\nmem_load_uops_l3_hit_retired.xsnp_hitm       88,033,111       0.880\nmem_load_uops_l3_hit_retired.xsnp_miss           82,130       0.001\nmem_load_uops_l3_hit_retired.xsnp_none           26,158       0.000\npacket                                      100,000,000       1.000\nCouple of interesting things jump out here compared with https://github.com/SnabbCo/snabbswitch/issues/808#issuecomment-194239854:\n1. L3 cache access seems to have decreased but only slightly. Here we see around 0.91 total L3 hits per packet compared with 0.98.\n2. Instructions per packet decreased from 60 to 35. This seems like a good thing: it should improve performance once the cache bottlenecks are out of the way. (And make the assembler code in traces easier to read, too.) This is probably from skipping the counters? I have wanted to minimize those for a long time but haven't been able to land that in the past, see #570. (Counters also became a pointer-chasing indirection with the whole counter_t business that bothers me quite a lot, too.)\nI would have expected a bigger effect from moving the read and write cursors onto separate cache lines since that seems like classic \"false sharing\" that would ping-pong the cache line back and forth between cores that need to update it. Given this data it looks like that is not the biggest problem though.\nI suppose another \"L3 hitm\" situation is accessing the pointers in the link ring. One core is writing pointers and another is reading them: second core will have to request them from the first core. Here are a few dubious and overlapping ideas that may be useful for reducing L3 synchronization there:\n- Write a whole cache line worth of pointers before bumping the write pointer. On x86-64 I suppose this means sending bursts of 8 packets (64 byte cache line / 64 bit pointer = 8 pointers per cache line).\n- Write to the link using cache-bypassing stores (MOVNT). I am curious about the behavior of these on Haswell: will the effect be to bypass L1/L2 and update L3 (if already populated there)?\n- Drop cache lines with CLFLUSH once written. Curious about this on Haswell too: can we drop from L1/L2 but keep in L3?\nGenerally I think a good way forward would be to take all the code on this PR and make it a local part of the ff mode of snabbmark. That is, the ff would use its own private definition of struct link instead of the standard one. This style would allow us to merge a bunch of different implementations into mp-ring and continue to tweak and evaluate them over time. Even if we find the optimal design now the answer might change with the next generation of CPU and we may have a new bright idea that we want to try so I think it is important to keep and maintain these benchmarks over the long term.\nOne more note about mp-ring: it could be a problem that the packets are always being accessed in the same order that they were allocated. This could provide an unrealistic consistent \"stride\" for the memory prefetcher. If we start to see worryingly high performance then we may want to mix that up a bit e.g. shuffle the freelist before allocating the packets.\n. One more idea:\nThere is a potential cache \"conflict miss\" due to the struct link objects being allocated with shm that puts everything on a 4096-byte boundary. This seems very likely to be a problem in future benchmarks where one core is accessing a lot of links. One quick solution would be to allocate the links with memory.dma_alloc() that happens to allocate shared memory with less extreme alignment (128 bytes, a fairly arbitrary number).\nSee #566 for where we discovered this conflict-miss problem and made a complicated workaround for counter objects.\n. Yet one more idea:\nI wonder whether the CPU \"store buffer\" introduces meaningful latency on writes i.e. whether the CPU will hold onto the stores for a while before pushing them out to the cache. If so it seems conceivable that one core could keep using a stale value (e.g. of the write cursor position) for a while. This might be avoided if the writer would execute an SFENCE instruction to flush the store into cache.\nI don't immediately see how this could account for the low performance when taking the PMU reports into account. If cores were continuing to access old data then I would expect to see both high cycles per packet while waiting for the stores to propagate and also high instructions per packet while spinning around in a busy-loop waiting for data to arrive. The PMU shows the former but not the latter.\n. One more reflection is that we should really celebrate the progress on this PR :-). This seems to have wiped 200 cycles per packet off the benchmark. If we do that twice more then we are in good shape :-).\n@xrme I mark myself as upstream for this PR. I have one request which is to localize the changes into the benchmark e.g. to add a commit that moves the global changes from struct link into a local definition of e.g. struct fflink for this benchmark. Then I will merge this branch onto the multiproc integration branch where we can keep hammering away until we are satisfied and ready to push multiprocess support upstream.\n(To me it seems quite luxurious to have integration branches where features can be developed at their own pace and following their own priorities. This fancy distributed Git workflow definitely takes some getting used to be I do enjoy the advantages it offers.)\n. Thanks! I have merged this onto the multiproc branch now. Github has automatically closed this PR now that the changes are merged but you can just reopen it if you want to push more changes from the same branch.\nI removed a few lines of dead code in order to proactively minimize the diff between multiproc and master (particularly the new functions in link.lua that we don't need after your new refactoring). You might want to git fetch origin && git merge origin/multiproc to pull your changes into this branch.\nOrdinarily people are left in peace while they hack on their [wip] branches but since the multiprocessing code is interesting to so many people I want to pull it upstream onto multiproc in case other people want to contribute. Sorry for the extra fetching and merging that this entails.\n. @xrme Interesting that these variants all have similar performance. I wonder what the bottleneck is? I agree that dropping down to assembler is a good direction so that we will have every detail on the table.\nI am not confident of my interpretation of the PMU events yet. I think we need to check the documentation and errata for the events much more carefully. For example some counters are precise (give an accurate per-packet value) while other are sampling (more nebulous to interpret). I have a hard time reconciling the L2 and L3 event counts that I see on some benchmarks and it is possible that they are actually irreconcilable. Probably there is one set of counters that is way more interesting than all the others but I am not certain which it is yet.\n. Hi Harish,\nThis is supported and the reason you are confused is a documentation problem.\nOn the snabbnfv command-line you can specify the socket path like this:\n/somewhere/%s.socket\nand the %s will be replaced by the id for the port in the configuration file. Then you can start your VMs with different paths and the Snabb process will connect to both.\nDoes that help?\n. Great!\nL2 switching, VLAN tag insert/remove, and MAC anti-spoofing are all being done in the NIC by VMDq. The rest is done in software.\n. Closing and marking as [sketch]. I originally thought this was a simple isolated change and only later realized that it depends on PEBS.\n. Merged, thanks!\nI fixed up branches.md in 95ac5b522e6c8e5925505f1d5cb5fad9d8779cae.\n. I have undone this mistake now with git reset and git push --force to rewind the next branch to the point before I pushed changes unintentionally.\nIf you pulled from next between 08:00-09:00 CET today then let me know and I will help you clean up.\nThe reason I resolved this with git reset to rewind the branch instead of git revert to undo the commit is that it seemed unlikely to me that anybody had pulled the branch. So I made an exception to the \"don't rebase public branches\" rule. I hope this was the right call.\nWhat happened\nThe problem is that I meant to merge kbara-next but I actually merged one of the multiproc branches. The commands I ran were:\ngit checkout next\ngit fetch origin pull/813/head\ngit merge --no-ff FETCH_HEAD\n... write commit message saying I am merging kbara-next ...\ngit push lukego\nand the mistake is the ref that I fetched that should have been #816 and not #813. I didn't catch that during the merge and since I wrote the merge commit message by hand that was misleading.\nHow to avoid this next time\nOne idea for avoiding this in the future would be to always refer to branches by name and preserve any useful information that is automatically added to the commit message:\ngit checkout next\ngit fetch origin\ngit merge --no-ff origin/kbara-next\n... keep default commit message saying which branch I merge ...\ngit push lukego\nEnd\nAny feedback on how to handle this better? Questions from people concerned about making similar mistakes? Can keep this Issue open for a little while to invite comment.\n. Great to have this in upstream review!\n\nI would like to see this merged as an initial version that can be further iterated upon, hoping that having this in the tree for all eyes to see will stimulate peer review and uncover further bugs etc.\n\nCounter-proposal: How about if this code lives on this feature branch until it is adopted by a real application e.g. lisper? The upstream flow could be esp -> lisper -> max-next -> next -> master for example.\nI feel like it would be a nice invariant if all of the upstream code is really in use by at least one application. Application developers are probably in a better position to declare when the code works than passive reviewers. It might surprise people if they find code upstream that has not really been used and tested, too.\n. > I agree with the reasoning. Should we add a named esp branch on upstream then, that\u2014presumably\u2014I will maintain?\nSounds like a good idea. Much like I have done with Multiproc and Mellanox branches.\nGoing forward it will be interesting to see whether in practice these turn out to be short-lived integration branches that are used to reach \"1.0\" on a feature or whether they continue to be maintained as the first-hop for new changes to those subsystems.\nI do start to think that it will work well to have our collection of fairly general-purposes integration branches close to master (next, max-next, wingo-next, kbara-next) and then for more topic-specific branches to spring up downstream (esp, multiproc, mellanox, etc). Then people maintaining -next branches could all have a reasonably consistent overall view of the project while people working on more specific stuff downstream could focus more on their own subject matter. So, I am glad that you all pushed back on me when I suggested that we start with topic-specific subsystem branches directly downstream from next :).\n. I think having the esp branch upstream will help for collaboration too. For example, I am keen to experiment with switching from aligned- to unaligned- instructions in the assembler code and I would prefer to send such a PR to the SnabbCo repo (with target: esp) rather than onto your private fork that is not being watched by the community and SnabbBot and so on.\n. Merged, thanks!\n:beers: fantastic to complete the lwaftr merge!!!!! Great work everybody! :fireworks:\n. cc @capr @eugeneia @wingo @kbara. This is intended as a radioactively controversial PR to stimulate discussion :).\n. > What is the \u201cZen of Snabb\u201d?\nI was imagining a short summary of Snabbiness. I was particularly thinking of the summary on lwaftr slide 3 \"Tao of Snabb\":\n- Simple > Complex\n- Small > Large\n- Commodity > Proprietary\nwhich seems to put a lot of design decisions in Snabb into context e.g. why our struct packet is quite simple and bare whereas other projects like DPDK have adopted all the complex Intel-proprietary offload metadata and so on. Our one is simpler, smaller, and more commodity - and still practical.\nTao of DPDK seems to be Benchmarks > Everything and this seems like an important contrast to me.\nThis is also along the lines of the Zen of Python which looks like a joke but is/was actually very useful documentation for newbies to orient themselves with the community (and, at the time, to contrast their values with the perl community).\n\nMore importantly we have to define the module naming scheme that this ToC uses\n\nGood point! Yes, one useful thing about documentation is that it can drive code changes when some things are just too damn ugly to document. The many and varied ways that we have for referring to classes of apps is one such thing. I suspect that we should create a new namespace for apps so that they can have simple string names like source that are less dependent on the module that implements them. (And that we should be consistent about how we write names e.g. lowercase_with_underscores.)\n\nI disagree with use formatting (fixed width fonts) in headers, but that is a detail.\n\nYeah. Just now I am optimizing for the PDF manual and this needs to be reconciled with the HTML edition before merge onto master.\n. @eugeneia Thanks for the fast feedback and thumbs-up on the basic structure btw! I can create an integration branch to collect the contributions that I want to make to documentation this month and then push that upstream to you.\n. The immediate upstream for this change is pdf-manual. Further review can be done when that branch is sent upstream to documentation-fixes.\n. I like that idea for showing return values!\nThanks for clarifying re: history. Some fine day I think we should consider switching to a more Lua-esque format. I find that easier to parse and it seems like there are not so many different kinds of symbols to differentiate between. (Unlike e.g. Lisp universe where you need to carefully distinguish between function / variable / constant / macro / symbol-macro / special-form / compiler-macro / type / ... and where I suspect we picked up this convention from).\n. (Can also be that I underestimate the value of the type specifier e.g. for spelling out configuration items, counters, etc. I only say that we should consider switching if we don't have a good reason to be different.)\n. I am not aware of any reason that LuaJIT would limit the amount of guest memory that can be mapped. Citation needed, Nikolay?\nLuaJIT does limit the amount of address space that can be used for garbage-collected Lua objects but that would not apply here.\n. Thanks for PNG. That will likely make this easier to include in the manual alongside the other documentation, eventually.\n. @eugeneia Thanks for catching the snabb dependency. That was me being lazy about making sure the directories in obj existed when they were needed for writing markdown output. Pushed commit 64ee8af with a better solution.\n. @domenkozar thanks for feedback, incorporated with latest commits.\n\nBy the way, I have a nix expression for SnabbBot lying around, should I PR that as well?\n\n@eugeneia Yes!\nI think we need a subsystem maintainer for reviewing and merging nix expressions. @domenkozar are you up for that? There is a hot-off-the-press description of the process for becoming a subsystem maintainer in #835 so you could also test that and give feedback :).\n. I did the patchShebangs wrong the first time. See 6a2d652 for the fix.\nGenerally I have a slightly tricky hacking situation with many documentation-related topic branches that are somewhat interdependent and hard to test in isolation. I am integrating them on pdf-manual and plan to send that upstream during this release cycle. So note to upstream maintainers: if there are some bugs in my latest documentation-related scripts then it is not the end of the world for those to slip upstream because we can fix/integrate them properly on next.\nThe main reason I am opening separate PRs is to get feedback and understand how easy/hard it will be to upstream these changes when they are all ready. I am also happy to use pdf-manual as the upstream branch for these changes provided that @eugeneia and others are happy with the general direction of these changes and will accept them upstream when ready.\n. Nikolay, you would make an awesome upstream maintainer for changes related to QEMU. Interested? #835 :-)\n. @eugeneia Good point. How about if we start maintaining a map of the current branches and then refer to that where appropriate?\nHere is a first attempt and using a little creative license in terms of content:\n\nSource for ditaa:\n+--lisper\n                   +--max next<----+--documentation<--pdf manual\n                   |\n       fixes       |\n         |         |\nmaster<--+--next<--+--kbara next<--+--nix\n                   |               +--mellanox\n                   |\n                   |\n                   +--wingo next<--+--lwaftr\n                                   +--multiproc\n. Thanks @domenkozar @eugeneia for the feedback. I have the feeling that the first redraft I submitted was not explaining things very clearly. I have pushed another partial rewrite now (half complete, half missing). Are there some parts that are less clearly explained than others now?\nI also know that I have a tendency to overuse analogies when trying to elucidate the Git workflow (sorry @wingo...) but I can't look at our branch structure now without thinking of master as a river and the subsystem branches as tributary streams, like in this map from the Wikipedia page about the source of the Rhine river. Is this worth talking about in the text or would it just be distracting?\n. I pushed a new section about how to be an upstream maintainer. I would really like feedback/ack/nack on this from the existing maintainers. cc @eugeneia @wingo @kbara.\n. @kbara Thanks for the feedback! I have written and pushed the next-hop section now. I am sure it is missing a lot of information. Can you tell me how it looks and what is still not clear?\nGenerally I think that having a well-defined branch tree is the key to making upstreaming work smoothly. This is hard now in the early days because we have so few branches and maintainers. Going forward it seems like if new features are first merged onto an appropriate subsystem branch (e.g. nix, mellanox, pdf-manual, etc) and then make their way up the tree (being sanity-checked and conflict-resolved at each step) then it should be easy to coordinate development.\nThe hope is that this maintenance structure will be scalable i.e. that we can get more useful work done by adding more maintainers. The barrier of entry to becoming a maintainer should be fairly low because the tree above acts as a safety net and mentoring framework. This seems to be how it works in the Linux kernel world.\nOne aspect I have not touched on in the documentation is having additional upstream communities around another Github fork e.g. the one on igalia/snabbswitch. This seems like an important mechanism for long term scalability since the rate of notifications from snabbco/snabbswitch is likely to become overwhelming at some point and it will be better to localize specific subsystems to their own repositories. I see a parallel where the Linux Kernel Mailing List was enough in the early days but can't keep up with all the discussions anymore and so these days there are around a hundred more specific mailing lists.\nHowever, I feel that it would be premature to promote wide adoption of that model just yet. I tried creating a separate snabbnfv-goodies repository for the NFV application and that seems to have been counter-productive compared with working directly on this upstream repository because it reduced visibility of the development without commensurate benefits in that particular case.\n. Ready for upstreaming, please!\n. Have merged master and resolved conflicts now. The overall diff seems to reflect only the intended changes against current master.\n. Merged, thanks! I added a note that the upstream branch is kbara-next following her ack.\n. @domenkozar your nix branch should automatically started being mirrored on the SnabbCo repo in a minute or two i.e. show up on the branches list.\n. @domenkozar How come you don't like the idea of putting all nix expressions under src/nix from the original #831? Seems a bit funny to sprinkle them around to me, even to have a src/nixos directory that only contains nixos modules and not other kinds of nix expressions.\nHow do other projects organize their in-tree nix expressions? can you link an example or two?\n. @domenkozar Thanks for the tips!\n. @eugeneia will you be upstream for this?\nwhat do you think about renaming documentation-fixes to simply documentation and using that as the next-hop for general documentation changes that are not part of a specific program? (I suppose it would be a bit funny for you to be your own next-hop with documentation -> max-next and a simple solution would be for documentation to feed directly into next instead.)\n. (Could also be that maintaining both max-next and documentation separately is too much extra work. In that case another alternative would be to \"optimize away\" the merge to documentation and merge directly onto max-next instead, which I suppose is roughly the current practice. Even in this case I think it is valuable to have the specific branches well-defined and written up in branches.md because it does at least tell us which person is responsible for each area even if they may take a short-cut in terms of where the merge things. I feel like we are still feeling around to see how this will play out longer term.)\n. > I would be interested in what others think about the \u201cContributor Covenant\u201d. I don't think the text is trivial.\nThanks for reading and thinking about it. My take is that the Contributor Covenant rules seem sensible. They pass my basic litmus test of accepting basic Snabb upstream behavior and rejecting basic Linux kernel upstream behavior. This is important to me because I am often explaining that we use the same workflow as the Linux kernel but I don't want to imply that we should conduct ourselves like Linus Torvalds does.\n\nAre we binding ourselves to follow it literally or is it more of a non-authoritative guideline?\n\nThis would be binding and enforced, just like other project policies and related rules that we are bound by such as the Github Terms of Service.\n. > I think replacing documentation-fixes with documentation ... is a good move\nDone on next: 06c27c655bf565d83a2234f2617cd6add46587dd.\n. Merged, thanks!\nI dropped the QEMU command line change following comment from @nnikolaev-virtualopensystems. I have not verified but I don't think that part of #833 is needed. Our CI test scripts don't use unique names for the QEMU chardevs.\nThis seems to me like the upstreaming process working well.\n. @eugeneia Thanks for the review. Fair point. I will close this PR now. The branch will be available for somebody to merge in the future if they write some more substantial test code and need a place to put it.\n. Sorry about the slow response!\nThis change looks good to me. I also played around a bit to see if it would be neater to pass only the expanded_keys array rather than the whole gcm_data (which feels a bit like \"type punning\" as you say) but on reflection this does not seem important to me. The gcm_data struct has the array we need in the position we expect it and so the function prototypes like reasonable to me.\n. Looks like solid optimization work, and awesome performance :-).\n. @plajjan Good question.\nFirst off, the whole intention here is really to be permissive, not restrictive, and to secure the right for our whole community to use the Snabb brand (name, logo, color scheme, etc). That would include the right for big equipment vendors to ship Snabb-branded products under the terms above too. It would also include the right for people to create Snabb-branded blogs, user groups, etc, on their own initiative.\nThen the scope of these rules would actually be really narrow. Implicitly this policy would only cover uses Snabb that are protected by trademark rules e.g. that appear to be representing this project. That would include using the name + logo + color scheme for branding networking products but exclude the millions of everyday uses of the word Snabb (\"Cisco 1000V: Snabb som tusen!\" etc.)\nThe intention of registering trademarks in a few jurisdictions is also defensively to secure our own right to use the name. I would like to avoid boring trolling situations e.g. where somebody files a trademark and then demands that we pay them royalties to continue using the name ourselves (as happened to Linux). An ounce of prevention is better than a pound of cure.\nI am also a relative newbie to trademark issues and just hoping to get the brushstrokes right here and expand the set of people who are comfortable with using the Snabb branding for their activities.\n. One interesting new case for the Snabb brand is the Youtube channel that I have created, Luke's Snabb braindumps. This trademark policy says that it is fine for me or anybody else to do these Snabb-branded activities and use the logo and so on.\nI originally called it simply \"Snabb braindumps.\" That would also seem to be fine. I changed it to be more distinctive and in keeping with the raw and ad-hoc theme of the videos.\nSeems like a reasonable example of how people could use the project name and logo?\nIf I had called the channel \"Linux Braindumps\" then I believe I would rather need to contact the Linux Marks Institute and buy a sublicense for the name \"Linux\" for $5000. This has the downside of costing time and money but the upside that I could then presumably be sure that nobody else will use that name and so invest in building up the \"Linux Braindumps\" brand as my own. I am not sure that this system is worth the fuss... for us it seems to me like being permissive with the Snabb brand provides the best results for the least effort. However, IANAL...\n. Missed a reference to Agner Fog's instruction tables which seems to tell us that the instruction RDTSCP is high-latency (takes a while to produce a result), not pipelined (can't execute multiple RDTSCP instructions in parallel), and does not consume execution units (won't slow down execution of other instructions while waiting for the result). This is why it seems like the selftest producing a log message every 36 cycles is fairly reasonable and also why the cost may be less in real usage.\n. Quick braindump on latest details I am working out:\n- Logging every event all the time would produce too much data to keep. So how to manage that?\n- Events could be logged into ring buffers that wrap around. Each category of event (e.g. system, engine, app instance, ...) could have its own buffer containing e.g. the latest 1MB of messages in that category. These buffers could be shared memory (core.shm objects) and snabb-top could attach to \"tail -f\" and write them to disk if you want to store the full stream.\n- Breaths could be sampled. Perhaps you record SYSTEM and ENGINE events all of the time but for really detailed app events you only record 1% of breaths. This way you would still have the complete set of events for a subset of the breaths. (This would also reduce the overhead from e.g. 1% to e.g. 0.01% which should make it a no-brainer to enable in production.)\n- Sampling that works across processes/cores should be doable. This could be synchronized via the common clock (TSC) e.g. fully record the next 10 breaths each time the TSC passes a multiple of 1^9.\n- PEBS is awkward: if it randomly samples hardware events then it will randomly supply information about some breaths and not others. Could be that it doesn't fit into the timeline picture so well (even if the timeline could be useful for statistically analyzing PEBS events).\nI am now aiming to quickly make this into something we can merge and then talk about enabling by default. The initial use cases and goals that I have in mind now are:\n- Log into shm files that will be persistent after process termination/crash.\n- Have the CI keep these files and make them available for analysis.\n- Ensure that overhead on throughput and latency is immeasurably small\nSo far in simple tests it seems like the basic overhead of calling an assembler routine is extremely low but I am not sure at what rate it is still efficient to call RDTSCP. Could be that efficiency also demands sampling rather than logging every event of every breath.\n. I added a timeline.md readme file explaining the idea and how the interface has evolved. Feedback would be welcome!\nI have a much updated implementation but still in the process of making that match up with the API.\n. Thanks for the feedback, @sleinen! I hope this is clearer now with the latest pushes?\nThe current version should be basically feature-complete now. I plan to do one more round of bug-fixes and then re-introduce the branch as something that people can use and experiment with.\nThe module now includes a really simple dumper that prints messages in reverse-chronological order with the timestamp deltas:\nselftest: save and dump\n  cycles category         message\n       0 selftest         event1 a(0) b(0) c(0) d(0)\n      96 selftest         event3\n     100 selftest         event2 a(0)\n      84 selftest         event1 a(0) b(0) c(0) d(0)\n      80 selftest         event3\n     100 selftest         event2 a(0)\n      88 selftest         event1 a(0) b(0) c(0) d(0)\n      76 selftest         event3\n     104 selftest         event2 a(0)\n      84 selftest         event1 a(0) b(0) c(0) d(0)\n      80 selftest         event3\nThis is meant mostly as an example and I hope that we will evolve multiple interesting ways to process the timeline files. (I like the way even this simple dumper is sufficient to show that logging performance in this version is a bit irregular and slower than in earlier versions i.e. performance issue to resolve. Hopefully it will be able to illustrate bits of apps that slow down in the same way!)\n. I fixed the issue with arguments being 0-valued (was a string matching bug) and too many cycles between messages (was because I had made the loop that calls the logger more complicated).\nI also added a nice feature that the core ID and NUMA node are stored for each log message. This information turns out to be available \"for free\" because the instruction that grabs the timestamp (RDTSCP) also loads this into a register. See http://stackoverflow.com/questions/22310028/is-there-an-x86-instruction-to-tell-which-core-the-instruction-is-being-run-on.\nHere is how the selftest output looks now:\nselftest: timeline\nmedian logging time: 40 cycles\nselftest: save and dump\nnuma core  -cycles category         message\n0    4      <last> selftest         event with no args\n0    4          60 selftest         event with one arg: i(1048575)\n0    4          40 selftest         event with four args: i(1048575), 2i(2097150), 3i(3145725), and 4i(4194300)\n0    4          40 selftest         event with no args\n0    4          36 selftest         event with one arg: i(1048574)\n0    4          40 selftest         event with four args: i(1048574), 2i(2097148), 3i(3145722), and 4i(4194296)\n0    4          40 selftest         event with no args\n0    4          36 selftest         event with one arg: i(1048573)\n0    4          40 selftest         event with four args: i(1048573), 2i(2097146), 3i(3145719), and 4i(4194292)\n0    4          40 selftest         event with no args\n0    4          36 selftest         event with one arg: i(1048572)\n0    4          36 selftest         event with four args: i(1048572), 2i(2097144), 3i(3145716), and 4i(4194288)\n0    4          44 selftest         event with no args\n0    4          36 selftest         event with one arg: i(1048571)\n0    4          36 selftest         event with four args: i(1048571), 2i(2097142), 3i(3145713), and 4i(4194284)\n0    4          40 selftest         event with no args\nselftest: ok\n. I implemented the priority() function now. This makes it possible to selectively enable the level of detail that you want. I am imagining that in the engine we could randomly choose to enable higher levels of detail for certain breaths so that on the one hand we always have interesting samples in the timeline but on the other hand it does not wrap around too quickly.\nI also renamed the priorities in a hopefully more helpful way. The main ones are:\n- trace for occasional messages.\n- app for per-breath messages e.g. messages that you print in your pull and push functions.\n- packet for per-packet messages e.g. messages that you print inside loops in your pull and push functions.\n- library for messages inside subroutines that could potentially be called very often e.g. checksum routine.\nI hope this makes it easy to choose the right priority when defining messages and deciding how much logging to enable.\n. I added a few \"log a million priority-disabled events in a loop\" to the selftest to estimate the cost of calling an event. Looks like around five cycles per event in total (1m event loop in 5m cycles). See 9b659f9.\nCould be that you could even reasonably estimate the cost of the JIT compilation (interpretation, tracing, optimization, code generation) on the first iteration of the event logging loop:\nnuma core      -cycles category         message\n0    3         5014856 selftest         invoked many events(1000000)\n0    3         5025592 selftest         invoked many events(1000000)\n0    3         5176004 selftest         invoked many events(1000000)\nthe first iteration takes around 150K cycles (~42 microseconds on this machine) longer than the next two. That strikes me as pretty fast compilation! Though some experience with timeline will be needed to know how safely such conclusion can be drawn from the logs.\n. Idea for the future: I notice that we have room for a 40-bit Performance Counter in the log entry. It could be really need to log the number of instructions in addition to the number of cycles. Then you could see the rate of instructions-per-cycle for every section of code. This could be handy as a heuristic to indicate whether or not you are really suffering from things like cache misses and branch mispredictions e.g. if instructions-per-cycle (IPC) is 1-2 that might be fine, more than 2 might be really good, less than 1 not so great. Or in other words if you want to optimize code the IPC hints you whether you should be trying to execute fewer instructions (e.g. making the JIT happier) or making your instructions run faster (e.g. trying harder to keep things in cache or keep branches predictable).\n. Done!\nNow you should please publish your branches by pushing directly to the snabbco repository. These are all \"protected branches\" which means (a) they don't accept force pushes and (b) each branch has a list of users who are allowed to push to it (I made the assignments that seemed obvious to me, let me know if you have trouble!)\nNote that the mirroring script is disabled now so if you push to your own fork then that will not automatically arrive on the snabbco repository.\nIf you accidentally screw up a public branch and need to force push then please raise an issue in the spirit of #817 and we can work it out together. Github does not seem to have any option to allow force-pushes to protected branches, which is harsh on maintainers (difficult to undo mistakes) but easy on users (won't get their merges messed up by an upstream rebase).\nQuestions welcome!\ncc subsystem maintainers @capr @eugeneia @kbara @wingo @domenkozar\n. Oops: Chose master as target branch. will recreate PR to next.\n. Very interesting! Great idea to write an even more isolated benchmark to establish where the performance boundaries are. Interesting that the pipeline test is giving much better results than the mp-ring benchmark.\nI wonder what results would make us confident of the approach? First thought that comes to my mind is 50-100 Mpps on the \"fan\" benchmark with 10-20 output queues. If that is possible using round-robin on a microbenchmark then it seems plausible that software can be competitive with hardware dispatch.\n. Snabb is probably more low-level than what you are looking for. I would suggest checking out OpenResty.\n. Upstream needed - @kbara or @wingo?\n. Merged, thanks! Sorry that I missed this PR for a while somehow.\n. Thanks for the poke @lperkov. I have updated the wiki page. Is it telling you what you need now?\n@domenkozar I would like to delete this wiki page but I think we need two things: some kind of dashboard showing people what machines are available and what PCI devices they have and documentation in the snabb repo for using the lab (getting an account, using lock, etc). Something for you?\n. My experience has been that Snabb \"just works\" inside Docker provided that you use docker run --privileged to get root access.\n. Background: There is an incoming PR (#835) with documentation about the current maintenance model. This suggests maintaining applications within a unified source tree (and git repo) with clearly established maintainer roles (e.g. @plajjan decides what changes to merge in program/ddos).\nThis is indeed tighter coupling than many other projects use. This is bound to create some tension but hopefully also cohesion. I think this model is the Linux kernel one fairly purely. Snabb applications are like major Linux features such as bridge, router, iptables, etc that are all maintained in one unified tree.\nSo @plajjan here is what I would suggest for SnabbDDoS:\n1. We create a ddos branch on this repo that only you can push to. You can push the latest development version of SnabbDDoS here.\n2. People can open Pull Requests for SnabbDDoS here. You are the \"upstream\" (Assignee) for those and take the lead on review and decide when to merge. (You can even ask people to rebase if you like, you are the upstream :-)).\n3. You can \"release\" SnabbDDoS by opening a Pull Request from ddos to a next-hop branch where it will swiftly land on master and be included in the monthly releases.\nThen SnabbDDoS is being regularly released together with Snabb. You can still decide whether to make your own releases separately from other branches (e.g. if you want to have an LTS release, etc, and we haven't worked out those details on the upstream repo yet).\nThis way the snabbco/snabb repository acts as a central point for maintaining all of the applications together. This is like the Linux Kernel Mailing List (LKML) in the earlier days i.e. a central place to send patches where the appropriate maintainer will process them.\nThis model can be generalized and excepted in the future:\n- If you prefer for Issues and PRs to be submitted to a separate fork e.g. plajjan/snabb then we can inform people that this is the process. This may not be necessary in the beginning but it is your call. The LKML has divided up into many topic-specific mailing lists over time and Snabb likely will too. (This seems to be how lwaftr is operating for example.)\n- If you prefer to have a separate Git repository e.g. plajjan/ddos then we could invent a simple mechanism to link Snabb as a Git submodule or subtree. This would provide you with more separation between your application and the Snabb upstream community if you prefer that.\nI will continue to push for the kernel model - unified git repo, well-defined maintainer structure, applications developed in-tree - as the operating model for the snabbco/snabb repo and try to help people climb onboard and become maintainers of applications/modules/documents/aspects. I like the social side of this and find it practical even though it is not without its frustrations. This is not the only game in town though and application developers absolutely get to opt-in and opt-out whenever they like.\n. Coming back to testing:\nIf we have 100 programs in the tree then they should each have their own test cases developed under program/foo. The CI is flexible and will try to accommodate those tests independently of how they are written. Today if you would just put a selftest.sh function in a directory it will automatically be executed by CI and can do anything that it wants. Over time we can teach the CI more fancy tricks e.g. if a performance test returns a CSV file then we could generate a comparison between releases.\nSnabbmark is not a central point that needs to test all applications. Rather it is one convenient place to put reasonably generic benchmarks to make them easy to run.\nIf people would like to make the test framework more sophisticated then a good start would be to try a new way locally for an application, e.g. under program/ddos/, and if it works well then suggest that other people adopt it too. This way everybody can try their own styles and the convenient ones can catch on. (The simple selftest model we have now is borrowed from Sun Openboot / Openfirmware btw.)\n. @plajjan I added you to the @snabbco/maintainers group on Github. If you want you can push your DDoS code upstream like this:\ngit remote add snabbco https://github.com/snabbco/snabb\ngit push snabbco ddos\nand then SnabbDDoS will appear on the branches list and I can mark that branch as protected so that you own it.\n. > I can rewrite the top level README but that will likely give me a headache every time I pull in the latest master.\nGood point!\nHow about if the first section of the README would be links to the subdirectories of the in-tree applications? (ddos, packetblaster, nfv, lwaftr, etc.) This is much more important than the blah blah blah parts for people looking for the code to some application.\n. @plajjan Yes code ownership becomes tricky :).\nThe VLAN code you changed was in apps/ which is a directory tree for shared code that is supposed to be collaboratively developed and extended and adopted by more applications. The LWAFTR developers have hopefully chosen to put that app here to benefit from community work exactly like what you have done. If the intention were for this to be a private \"hands off\" app then the implementation could be in program/lwaftr/ instead.\nIt is also possible that this change will break something for them. The solution I see is that they will ship the lwaftr to users from their own branch(es) for production use. The master branch is then more of a synchronization point with other developers. They can avoid breakages in merges from master by reviewing the changes they merge and by contributing tests to the upstream CI.\nThis is still the kernel model. Redhat don't ship Linus's tree to their customers verbatim but they do use it for synchronizing their kernels with development from everybody else in the Linux universe.\nYou are right though that we are still working out the details for all these issues. I quite like the idea of people distributing binaries like snabb-ddos and snabb-lwaftr that they have built from their own production/release branches. Our CI could also build releases of individual applications from their own release branches rather than from master. We will have to see what works well in practice.\n. I saw that. This seems like a small and helpful thing to do i.e. updating the users of an app/library after changing its interface. I see being able to do this as a big benefit to having all the code in one repository with common CI coverage. Can certainly be worth @mentioning the lwaftr team so that it doesn't surprise them though.\nIf it were a more substantial change then I would expect to get their sign-off.\nThis kind of issue is what I am responsible for considering when @eugeneia sends your VLAN change upstream to me in a Pull Request to next. The code passes a few steps on the way to master and everybody uses their best judgement along the way and hopefully we don't screw up too often.\n. Great reference to the Google monorepo! That does resonate with me and seems like basically what I am trying to describe. Thanks for posting that link!\nI do also see this as a continuum with Snabb perhaps somewhere between the kernel and the Google/Facebook repos. Over time it will be interesting to see the evoluation of what functionality we want to have in-tree and what we want to interface externally to.\n. Good catch.\nI would really like for us to rewrite the SIMD checksum code with DynASM rather than GCC SIMD intrinsics. This way we could build with older GCC versions and trust that we get the right code (generated by the DynASM in our tree rather than an uncontrolled version of GCC).\nOne step at a time... DynASM did not support the AVX2 instruction set back when we introduced this code.\n. Good move!\n. Yes! Consistent naming has been on my list forever too. I agree that input and output are better names than rx and tx too.\nThe only alternative I see worth considering is anonymous ports accessed by index. Then the name is irrelevant. This is already implemented but I know that you recently noticed problems with the implementation (can't distinguish between the first port or a port named 1) so I would be okay with removing that anonymous-ports feature and always using names instead.\n. Woohoo! Goodbye more C glue code :-)\nI think the basic protocol for changing the API like this is to update the documentation and any callers that we have in the repo. I like this monorepo concept description that @kbara linked to!\nMaybe @kbara or @wingo would like to be upstream for this one?\n. Here is a quick screenshot from some tooling I am doing around timeline. This is a sample taken during the NFV benchmark to forward packets to a DPDK VM. I defined a metric Lag as the time between a log message and the previous one - treating this loosely as a duration.\n\nThis is based on the Agile Visualization software stack (Pharo) and I am experimenting with some visualizations. Here is a quick screenshot where each event is drawn as a rectangle, sized based on time \"lag\" (area = cycles), and with each message being assigned a distinct color. (This is the same data as in the table above.)\n\nEarly days but the timeline does seem able to provide interesting data to go digging in.\n. Here is a summary of the diagram above that shows the sum total cycles for each kind of event in the log and draws edges between events that follow each other:\n\nThe big square is Virtio-net, the next is Intel10G, etc. On the top-left there are two different events that can precede the start of a breath: the end of the previous breath (top-left) or commit on the counter objects to shared memory (on the right).\nThis visualization is a bit more like what you see with a traditional profiler i.e. overall proportion of time spent on each activity.\n. Hey, cool, there is actually no mess here and everybody is doing the right thing :) we are exercising a new edge in our collaboration graph here :-).\nThe reason we see many commits here is that downstream branch nix has merged a bunch of commits from the v2016.04 release on master but these commits are not yet in kbara-next. So Github is  showing that the consequence of merging nix onto kbara-next would be to get all of these commits i.e. the v2016.04 release in addition to Domen's own change a357165.\nThe next steps could be either of these things:\n1. kbara-next merges from master and then this PR should automatically update to show only the commits that are still missing i.e. a357165. Then this could be merged.\n2. kbara-next merges from nix which in one fell swoop would bring in the v2016.04 release and also Domen's change i.e. all the commits we see listed above.\nSo @kbara it is really up to you which you prefer.\nIn a perfect world we might all merge master into our long-lived branches immediately when a new release is made, but this demand-driven merge may well be fine too. The main thing is to transfer all the commits between all the branches and git is good at keeping track of what is missing (provided that we don't rebase and change the commit IDs.)\n. Here is one git incantation to summarize the changes more succinctly:\n$ git log --oneline --first-parent origin/kbara-next..origin/nix\na357165 Add Nix expressions to build Snabb project\n03f5958 Merge PR #855 (v2016.04 release) onto master\nToo bad there is no button on the Github UI to see --first-parent :)\n. @kbara You will need to push your branch directly to the snabbco repo now e.g. with git push snabbco kbara-next if you have a remote called snabbco. The mirroring-from-our-forks hack is gone now - see #850.\n. Interesting, yep. I see that you have pushed:\n$ git log -1 --oneline origin/kbara-next\nd5d355a Merge remote-tracking branch 'upstream/master' into kbara-next\n$ git log --oneline origin/kbara-next..origin/nix\na357165 Add Nix expressions to build Snabb project\nbut I suppose that the Github UI does not automagically refresh in this case. Oh well, probably not a big deal.\n. One related programming pattern that keeps popping into my head is to process packets stepwise, like (loose example):\nlua\n-- Receive packets\nfor i = 0, max do\n   packets[i] = link.input(in)\nend\n-- Lookup state records based on packet fields (could be streaming etc here)\nfor i = 0, max do\n   state[i] = table_lookup(packets[i])\nend\n-- Classify forward vs drop\nfor i = 0, max do\n   if should_forward(packets[i], state[i]) then\n      table.insert(forward, i)\n   else\n      table.insert(drop, i)\n   end\nend\n-- Execute forwardings\nfor i = 0, #forward do\n   forward_packet(packets[forward[i]])\nend\n-- Execute drops\nfor i = 0, #drop do\n   drop_packet(packets[drop[i]])\nend\nI see this as having a few potential advantages that admittedly need to be fleshed out:\n1. Divide-and-conquer processing functions into smaller pieces that can be optimized separately.\n2. Give LuaJIT what it wants: small loops with consistent control flow that it can aggressively optimize.\n3. Timeline (#873) could show how many CPU cycles are consumed at each step.\n4. Could also integrate with more advanced profiling methods e.g. to see the exact number of cache accesses in each processing step.\nThis is a bit inspired by @alexandergall's learning bridge code which in turns is probably influenced by his examination of JIT behavior.\nI am developing the timeline tooling a bit in the background, once I have a demo that may help to motivate this kind of structure.\n. Thanks @plajjan - closing this PR.\n. Interesting that this fails CI. The test that fails is nfvconfig that is cycling through very many app network configurations. The failure seems to be a resource leak. I merged #884 to bring in the assertion that the packet freelist is overflowing.\nNext step is to work out where the leak is, possibly a case where the intel_app is not returning receive descriptors to the freelist and that triggers more easily now that each instance allocates more buffers.\n. Interesting. This is actually not a leak but a resource exhaustion. The nfvconfig selftest creates 64 VFs in the same app network and the hard-coded limit on packets in core.packet is too limited. The limit is 100,000 packets but with 64 VFs and 2,048 buffers per receive queue we need 131,072 buffers.\nOne solution would be to increase the maximum number of allocated buffers. The reason I see against this is memory consumption. Each buffer is 10KB and so 100,000 buffers is already 1GB of RAM.\nOne alternative would be some dynamic sharing. For example for I/O sources to allocate buffers based on their recent activity (is the queue busy or idle?) and system resources (is there a scarcity of packet buffers?). Anybody have a good algorithm for this?\n. Reasonable idea.\nTrouble is that 1GB is not enough if we merge this PR and allocate 2048 buffers per VF. How do you feel about bumping the limit to 10GB?\n. The allocation is dynamic - pay for what you use - and the limit is currently a hard assertion i.e. PANIC the process on exhaustion. The intention of the limit was to be so high that it only triggers on resource leak bugs but I evidently underestimated our appetite for packet buffers.\nYou are right that 1.3GB should be enough to make this test pass but with such a strict limit it would be good to have more safety margin on the KISS approach.\n. I suppose also that it is important to allocate resources for the practical worst-case. This seems like a mark in favor of the KISS approach where you always allocate everything you need rather than moving the requirements based on workload (that maybe bites you on the bottom at the worst possible time).\n. I merged #890 which should make CI pass for this branch. If that change is rejected then I will revert it here and merge in whatever replaces it.\nThese PRs need an upstream, please!\n. Soft limit for detecting resource leaks -- potentially even slow leaks from unusual code paths -- does sound neat. I don't plan to implement that on this PR though.\nMy feeling now is that it is reasonable to have a hard-limit on what should be \"enough packets for anybody\" (now 10GB of packets per process) and also for application test suites to check the application designs against these limitations. This may not be optimal but it does seem fairly practical.\nKudos to @eugeneia for writing a test case for the NFV application that exercises loading a configuration file with the maximum number of network interfaces defined. This has correctly detected that using 4 x more receive descriptors per NIC would cause resource exhaustion with an NFV application configuration that is supposed to be supported (64 receive queues / virtio-net NICs). Great practice to test these boundary-case configurations.\nIf we had neither this test case nor the hard limit then we may totally lose touch with how much memory the NFV application requires e.g. whether for a large configuration it would try to allocate a 100G+ of packet buffers.\n. This is exciting to me as a big step in the direction of supporting standard RFC 7223 network interface statistics. This is the raw information that network operators need in order to monitor links in their networks. Having these counters available at all major points where packets ingress/egress from Snabb (e.g. NIC, VM, kernel, important tunnel, ...) would make it much easier to produce applications that are easy to monitor.\nThis is really only indirectly related to YANG i.e. the counters in RFC 7223 are spec'd as YANG objects and so to implement that you implicitly need to have some kind of mapping.\nFiguring out the right way to connect this information with the various \"northbound\" interfaces - snabbtop, netsnmpd, netconf, etc - is the next discussion that I think @wingo has just started :-) and of course @alexandergall already has one such solution in tree here for intel10g and netsnmp.\nHave only skimmed the code but looks good to me so far! I am really curious to know whether we will see a significant performance impact from things like counting unicast vs multicast packets in software when hardware counters are not available.\n. Could consider s/YANG/netmod interface counters/ in the title.\nI see this branch as primarily developing support for keeping track of counter values that are needed for populating some standard YANG models. This is kind of a big deal because it can involve inspecting the packet stream in apps that would otherwise be payload-agnostic e.g. the vhost_user app needing to check payload to decide whether to bump the unicast or multicast counter. Have to see what the overhead is here - and if it is high then whether any compromise makes sense.\nThese values will then be fed into the YANG framework that you are hacking based on #696. Meanwhile snabbtop is providing an interim interface for dumping the counters during development. This all has to be integrated together (and with existing SNMP support) once all the bits are there.\nMake sense @eugeneia @wingo?\n. Just a related background point:\nCurrently we are allocating each shm object (e.g. counter) on a 4096-byte aligned address. This is not a valid design for x86. The problem is that the low 12 bits of the address will always be zero and this leads to \"conflict misses\" for Intel's set-associative cache. The CPU cache simply cannot contain many objects that have the same values in their low bits. See e.g. http://danluu.com/3c-conflict/.\nThis has already bitten us in practice. The workaround we made for now is to double-buffer counters so that we access a local cache of the value and then periodically commit that to the 4096-byte aligned shared memory address. This is complex and lacks generality.\nSo we need to come up with a new allocation scheme and ideally make the objects cheap to access by having more entropy in the low bits of their addresses. Simplest to implement might be to assign random padding to the beginning of the objects. However we may alternatively prefer a scheme that allocates many  objects on the same page instead.\n. Great work, SnabbBot !!! I will investigate now.\n. @eugeneia I am having trouble running this benchmark in the Docker environment on Interlaken:\n[luke@interlaken:~/git/snabb/src]$ scripts/dock.sh 'sudo SNABB_PCI0=01:00.0 program/snabbnfv/selftest.sh bench'                                                                               \nDefaulting to SNABB_TELNET0=5000\nDefaulting to SNABB_TELNET1=5001\nUSING program/snabbnfv/test_fixtures/nfvconfig/test_functions/other_vlan.ports\nDefaulting to MAC=52:54:00:00:00:\nDefaulting to IP=fe80::5054:ff:fe00:\nDefaulting to GUEST_MEM=512\nDefaulting to HUGETLBFS=/hugetlbfs\nDefaulting to QUEUES=1\nDefaulting to QEMU=/root/.test_env/qemu/obj/x86_64-softmmu/qemu-system-x86_64\nfailed to connect to server\nfailed to connect to server\nWaiting for VM listening on telnet port 5000 to get ready...^C^C^C [TIMEOUT]\nAny hints what might be wrong? I compiled Snabb in dock.sh. This is on branch next.\n. oh hm seem to be two issues. First I used the wrong PCI address. Second seems that I need to fully qualify the PCI address for some reason.\nThis now does work on interlaken:\nscripts/dock.sh 'sudo SNABB_PCI0=0000:81:00.0 program/snabbnfv/selftest.sh bench'\nGroovy. I have a test env for debugging the performance issue now.\n. @eugeneia Good catch on s/2015/2016/ in the title of the PR, but to me it feels like the PR title is owned by the submitter and it seems intrusive for somebody else to change it directly rather than point out the problem.\n. Current theory is that the performance regression sneaked in on #882 (increase descriptors from 512 to 2048). This branch passed CI but only barely: the log shows loss of around 10% on the iperf test. Digging in...\n. @eugeneia I think our tests on interlaken and lugano-1 are colliding at the moment. I was naughty and did not use lock but am now. Can you hop onto #lab anyway so that we can coordinate?\n. ... or maybe I am mistakenly thinking they are yours because I see eugeneia in docker ps but that is only talking about who created the image and not who is running the container :)\n. Sorry about the wait. Currently I am concerned about the lower results we are seeing on the iperf-1500 benchmark on davos and I would like to resolve or at least understand that before releasing from this branch.\nI see two related issues really:\n- new: This branch seems to have lower average performance than master.\n- old: Both branches have high variability in benchmark scores.\nThese issues interact: the variability makes it harder to compare results between branches.\nI am now approaching this in the \"scientific testing\" style by collecting more performance samples and comparing them using R scripts. I am becoming incrementally less ad-hoc in my R scripting with support from @domenkozar and http://hydra.snabb.co/. Here is what I have so far:\n- Report written in Rmarkdown and then built and published by Hydra.\n- Source to the report that lives on a Github gist. There are two files: data.csv with the data available so far and rmd.nix that builds the report.\n- Hydra Jobset that builds and archives this report every time I update the gist. If you click your way through then you will come to the HTML report.\nThe immediate next step is to experiment with a few changes to next and bring them into this report to see if they restore performance.\nGoing forward I am very interested to have the CI measure variability and reduce this over time. I cannot account for this at the moment. I am keen to make the timeline viewer into the tool of choice for post-mortem analysis of runs with unexpected results e.g. the slowest of the samples here.\n. So based on the idea that performance decreased because https://github.com/snabbco/snabb/pull/896/commits/382827d8aa6537f1567675836c0635d68405446b increased the NIC descriptor ring size from 512 packets to 2048 packets I tried a compromise by patching next to use 1024 packets instead.\nSee Report for full details. Overview below.\nUsing 1024 packets in the descriptor ring seems to partly resolve the performance regression:\n\nThe average is still lower but Tukey's Test tells us that this is not statistically significant. Specifically, with the data we have the best we can say with 95% confidence is that the next1024 performance is between 0.5 Gbps slower and 0.09 Gbps faster than master.\nSeen in this table excerpt where we have the average difference (diff) and the lower bound on the confidence interval (lwr) and the upper bound on the confidence interval (`upr):\n```\ndiff        lwr         upr     p adj\nnext1024-master -0.2042000 -0.5054358  0.09703584 0.2466440\n```\nSo it is easy to believe that there is still a modest performance reduction on this benchmark but the noise from uncontrolled variation means that a lot of testing would be required to confirm this. This is the point where I start to be more interested in understanding the variation itself.\nI also briefly checked packetblaster to see if it still works okay once we reduce the default descriptor rings size from 2048 to 1024 and it seems basically okay to me. Excerpt from davos with two ports:\n03:00.0 TXDGPC (TX packets)     14,880,151      GOTCL (TX octets)       952,305,792\n03:00.1 TXDGPC (TX packets)     14,880,119      GOTCL (TX octets)       952,353,472\n03:00.0 TXDGPC (TX packets)     14,878,160      GOTCL (TX octets)       952,131,840\n03:00.1 TXDGPC (TX packets)     14,877,169      GOTCL (TX octets)       952,112,704\n03:00.0 TXDGPC (TX packets)     14,880,168      GOTCL (TX octets)       952,328,704\n03:00.1 TXDGPC (TX packets)     14,880,166      GOTCL (TX octets)       952,314,944\n03:00.0 TXDGPC (TX packets)     14,880,096      GOTCL (TX octets)       952,339,648\n03:00.1 TXDGPC (TX packets)     14,880,561      GOTCL (TX octets)       952,360,768\n03:00.0 TXDGPC (TX packets)     14,880,600      GOTCL (TX octets)       952,359,488\n03:00.1 TXDGPC (TX packets)     14,880,186      GOTCL (TX octets)       952,363,392\n03:00.0 TXDGPC (TX packets)     14,879,073      GOTCL (TX octets)       952,247,616\n03:00.1 TXDGPC (TX packets)     14,879,371      GOTCL (TX octets)       952,212,800\n03:00.0 TXDGPC (TX packets)     14,880,556      GOTCL (TX octets)       952,372,800\n03:00.1 TXDGPC (TX packets)     14,880,554      GOTCL (TX octets)       952,375,296\nSo the action plan that comes to my mind is:\n1. I push the num_descriptors=1024 default value as a compromise and we ship that in v2016.05 if SnabbBot and @eugeneia are okay with that.\n2. Over the 2016.06 development cycle we start using R+Hydra to measure the variation in CI results.\n3. We make an ongoing effort to both minimize variation and increase average performance.\nThough this is negotiable if other people have strong opinions.\n. Relatedly:\nSnabbBot reports low scores for the iperf benchmark mainly for three reasons:\n- CPU is low frequency (1.8 GHz)\n- CPU does not support AVX2 so falls back to SSE2 checksum.\n- SSE checksum is slower in hardware and also not entirely optimized in our software.\nMerging the new checksum routine from #899 may well provide a boost to all platforms including these pre-Haswell machines that lack AVX2. Could be something for v2016.06.\n. I did a little more R hacking. I am really excited to be starting to \"productionize\" some of this performance analysis to become part of the release process. This time it is only the iperf-1500 benchmark but this should be easy to extend to cover all benchmarks and run them all automatically.\nThe new report has updated visualizations, wikipedia links explaining how to interpret each plot, and my own interpretation of the results. The Hydra build publishes everything needed to reproduce the report.\nTLDR pretty picture:\n\n. @domenkozar Good question. I would like to understand what the outliers mean first. Is it that we have uncontrolled background activities on the test server and need to improve our scheduling? Is it that we have a problem in our test scripts that fails to control something important? Is it that we have a bug in our software that causes the JIT to generate the wrong code in certain instances? It's possible that the outliers are boring but it is also possible that they are the most important points of all.\nThe best strategy I have in mind would be if each blue dot becomes a hyperlink to a Hydra build for that one specific test case. Then we can click on data points to zoom in on the best/worst/middle results and understand what is different between the runs.\nCan we setup Hydra to publish each individual test run on its own page? R will be able to do the hyperlinking if we include some identifying information in the CSV file and use the SVG backend for the graph.\nI am imagining the CSV file we want Hydra to produce for R will be something like...\nBRANCH, COMMIT,  BENCHMARK,  RUN, SCORE, SERVER, URL\nmaster, adfa4b7, iperf-1500, 1,   5.0,   davos,  https://hydra.snabb.co/build/445\nMeanwhile... I added a section to the report with some numeric statistics including median and quartiles that are not too sensitive to outliers:\n``````\nbranch: master\nbranch        gbps\nmaster  :50   Min.   :2.310\nnext    : 0   1st Qu.:4.883\nnext1024: 0   Median :5.295\nMean   :5.089\n3rd Qu.:5.577\nMax.   :6.020\n--------------------------------------------------------\nbranch: next\nbranch        gbps\nmaster  : 0   Min.   :2.100\nnext    :46   1st Qu.:4.340\nnext1024: 0   Median :4.555\nMean   :4.519\n3rd Qu.:4.755\nMax.   :5.370\n--------------------------------------------------------\nbranch: next1024\nbranch        gbps\nmaster  : 0   Min.   :3.050\nnext    : 0   1st Qu.:4.543\nnext1024:50   Median :4.915\nMean   :4.884\n3rd Qu.:5.170\nMax.   :5.940```\n`````\n. ... one interesting observation from the numeric table above is that there are only 46 values available for the branchnext`. This is also an anomaly to explain... did I grab the log file too early? or were there some errors (and if so how should we detect and present those?)\n. ... even better would be to use VMs and actually run a part of the test suite that includes PCI device access. Then we would get compatibility testing with diverse important kernel versions.\n. Nice!\nI had a play around with this and I see that it works, I can reproduce the problem from #867 (failure to build on CentOS 6 due to older GCC):\n``` nix\nwith import  {};\nwith vmTools;\nlet snabb = stdenv.mkDerivation rec {\n  name = \"snabb-${version}\";\n  version = \"2016.04\";\n  src = fetchurl {\n    url = \"https://github.com/SnabbCo/snabbswitch/archive/v${version}.tar.gz\";\n    sha256 = \"1agrcd19jf28cmar8452qm42f53hxzbvrjzq1a4rm5gv1rhpb3yd\";\n  };\n}; in\nrunInLinuxImage (snabb // { diskImage = diskImages.centos65x86_64; })\n```\nThat boots a CentOS 6.5 VM and gets a compilation failure:\n```\n$ nix-build -j 20 ./test.nix\n...\nFormatting '/tmp/nix-build-snabb-2016.04.drv-0/disk-image.qcow2', fmt=qcow2 size=4294967296 backing_file=/nix/store/vlsa5ikv51cc2qbmpr6vpcz68qabfs2w-centos-6.5-x86_64/disk-image.qcow2 encryp\ntion=off cluster_size=65536 lazy_refcounts=off refcount_bits=16\nloading kernel modules...\n[    0.360209] EXT4-fs (vda): couldn't mount as ext3 due to feature incompatibilities\nmounting Nix store...\nmounting host's temporary directory...\nstarting stage 2 (/nix/store/y64r5c1m6khhy042qm3jmlv12ykq5871-vm-run-stage2)\nunpacking sources\nunpacking source archive /nix/store/qziji771avy740jpg9w4lin27sy78nrq-v2016.04.tar.gz\nsource root is snabb-2016.04\n...\nC(AVX2)   obj/arch/avx2_c.o\ncc1: error: unrecognized command line option \"-mavx2\"\nmake[1]:  [obj/arch/avx2_c.o] Error 1\nmake[1]: Leaving directory `/tmp/snabb-2016.04/src'\nmake:  [all] Error 2\n[   20.505023] reboot: Power down\n```\nThis should make it possible for us to test Snabb with both the userspace and kernel of all the major distros including relatively archaic versions. This would be quite awesome. Since Snabb is a very low-level program it is extremely interesting to test with diverse host kernels and that is not something we get with e.g. Docker containers.\nSeems like your Nix code for our OpenStack test suite has the right ingredient for passing in PCI devices: https://github.com/domenkozar/snabb-openstack-testing/blob/master/tests.nix#L28-L30. How would I add that to the script above?\n. Seems that if I use runInLinuxImage to runInLinuxVM then it will still run inside a VM with the distro of my choice but it will build the software using Nix instead of the host tools?\nThis would seem like a nice way to separately test running a standard binary vs one built on the distro under test.\n. Is Hydra active at the moment?\nI defined a Hydra job to build on all known Linux distros - knowing that most will fail - but it does not seem to be executing. https://hydra.snabb.co/jobset/snabb/compile-distros\n. oh, actually now it did execute, but seemingly half an hour after I requested the build. and seems to have stopped on the first error while I was hoping it would separately test each platform. Any ideas?\n. Just an observation that now you can open the counters any number of times but the first close will invalidate all of them. Could be error-prone? Would be helpful to see how this is used.\n. Thanks for explaining!\n. Merged, thanks!\nSorry about not merging master into next already :-)\nMight be better to use your own Github fork for short-term branches like wingo-next-06-04-2016 rather than pushing them to snabbco just to keep the public branches list lean. (I have surrendered to branch clutter in my lukego fork for now...)\n. Merged, thanks!\nIf the performance regression is real then it will be flagged on #888 and we can resolve it on next.\nI tried to manually run that benchmark on lugano-1 but the Docker gods are not smiling on me today. @domenkozar is there an easy way to run the NFV tests via nix-build atm?\n. Cool! Pioneering a new design for multiprocess drivers :-)\nPersonally I find the plumbing rather stylish with using flock to become the master, the NIC software-software semaphore to protect global register updates, and automatically provisioning queues that are used. This keeps the interface simple and the code short.\ncc @hb9cwp who is surely affected by this driver rewrite and likely has feedback too?\n. Fixed the bug with 4993c37. This code works fine up to 128KB inputs in casual testing. That limitation seems okay to me i.e. not worth writing more code to increase it because packets are not that big.\nThe next step is to integrate and test/benchmark more extensively both with synthetic benchmarks and end-to-end tests (offloading checksums from QEMU VMs).\n. Added snabbmark checksum benchmark:\n$ taskset -c 1 sudo ./snabb snabbmark checksum\nVARIANT          BYTES/PACKET    BYTES/CYCLE  CYCLES/PACKET\nbase                  631.331          0.326       1935.081\nasm                   631.331          4.244        148.770\navx2                  631.331          2.743        230.180\nsse2                  631.331          2.318        272.319\nThis is intended to be fairly harsh and realistic. The input sizes, contents, and alignments are randomized. I draw the input sizes from a log-uniform distribution which is my current favorite for packet sizes (mostly small but also including large and jumbo sizes). I also update the assembler routine to have the same interface as the others (added the initial argument).\nThe assembler routine shows the best results by far. The older AVX routine is likely suffering from the logic that selectively falls back on the generic routine on small inputs (both for the unpredictable branch and because the current implementation of generic checksum is beyond awful -- should fix that as a matter of principle even if we are using the SIMD one in practice.)\nThis is starting to look like effort well spent! IP checksum is the main hotspot for Virtio-net with client/server workloads e.g. running iperf in VM. Cycles saved here should translate directly into extra capacity for the NFV application.\n. This branch is taking a little bit of a different turn:\nI found a fairly straightforward formulation of checksum in C that GCC is able to automatically vectorize when compiled with -O3. I hacked the Makefile to compile this function twice: once as cksum_avx2() using -mavx2 and once as cksum() using default settings (i.e. SSE). This is simpler and faster than the hand-coded SSE and AVX routines based on C intrinsics so I have removed those.\nI have retained the AVX2 assembler variant as this is still the fastest by a significant margin.\nCurrent scoreboard:\nVARIANT          BYTES/PACKET    BYTES/CYCLE  CYCLES/PACKET\nbase                  631.331          2.921        216.117\nasm                   631.331          4.172        151.310\navx2                  631.331          3.435        183.775\nThe next step is to eliminate either the C/AVX2 implementation of the assembler one. The open problem for the assembler one right now is the wart that it temporarily overwrites the memory trailing the input which is a different and more complex interface that may not suit all usages. The open problem for the C/AVX2 implementation is that it is slower than the assembler.\n. @jsnell Yes, I know what you mean. In Snabb we pin exact versions of LuaJIT and DynASM so that we can \"geek out\" on them but have tried to be conservative with gcc, glibc, etc. I am not especially comfortable with either the auto-vectorized nor the vectorized-with-C-intrinsics versions.\nI imagine it is interesting in other projects where they are dedicated C hackers and pick one compiler (ICC, CLANG, or GCC) and geek out on its features for vectorization etc. at least until the distros come along and decide to use a different compiler, compile for a generic target architecture, skip the performance tests, etc :).\nQuestions I am struggling with now are:\n1. Is the trick of zero-padding the input good (simpler code thanks to known properties of inputs) or bad (complex interface due to lazy coding)?\n2. Is it worth writing an SSE assembler version by hand to reduce dependence on GCC?\n3. How does performance compare with theoretical limits? i.e. based on throughput and latency of the instructions involved (how busy is it keeping the ALUs and what is the limiting factor on performance?)\n. Good idea!\nTelling contributors to pick a target branch explicitly is new. I have been telling people to always target master with new features. Do we want to switch? (Seems reasonable to me -- and puts pressure on us to organize the branches such that people know which target to choose.) This would also allow us to close PRs more quickly, once merged to subsystem instead of once released to master, which may also be a feature these days (?).\nI feel that we need to establish clear common expectations about how long things take (except when other specific guidance is given). I have tried to do this to some extent in #835. This is to calibrate the \"stuck-o-meter\" and know when to communicate with people.\nExample:\n- Get review from upstream, up to 3 working days?\n- Get merge of a branch without blocking issues, up to one week? \n- Time that a change lives on a subsystem branch before going upstream, up to one week?\n- How long before the latest release is merged into each subsystem branch, up to one week?\nThese would not be hard deadlines but rather a framework for maintainers to know when they should provide guidance to contributors (\"I will get to this next week\", \"I want to merge #xxx first\", etc) and vice-versa (\"I am still waiting for feedback on my latest changes\", \"my change seems to be stuck on your branch\", etc).\nThis framework might allow maintainers to come up with comfortable and efficient routines. For example, maybe I sync my branch with upstream every Monday and I provide follow-up on each PR that is assigned to me on Tuesdays and Wednesdays. On weeks where the routine is disrupted I post a little note to manage contributors' expectations so that they don't worry about bugging me. (Or  for however I want to organize my work that is consistent with the common expectations.)\n. @eugeneia are you upstream for this?\n. Merged, thanks!\nI was a little concerned about the hard-coded PCI addresses in release.nix for running the test suite. The risk is that running the tests on somebody else's machine could hijack an important network interface that happens to have the same address as a testing interface in the lab. However, I interpret requiredSystemFeatures = [ \"performance\" ]; to mean that this script can only run on lugano servers in practice so it seems safe.\n@domenkozar what do you think about renaming performance to lugano (i.e. be more specific about what hardware we are referring to) and finding a way to remove this from release.nix that should be able to run anywhere (not just in our lab)?\n. Merged, thanks!\nLog shows that SnabbBot reported failure only on the existing performance regression that has sneaked in separately.\n. I am in favor. This does bring a related issue to mind that I want to mention.\nThere seem to be three main plausible definitions of the struct:\n```\nstruct packet_A {\n  uint8_t[PACKET_SIZE];\n  uint16_t length;\n}\nstruct packet_B {\n  uint16_t length;\n  uint8_t[PACKET_SIZE];\n}\nstruct packet_C {\n  uint16_t length;\n  char pad[62];\n  uint8_t[PACKET_SIZE];\n}\n```\nwhich one do we prefer when making this public?\nToday we use variant A. Nikolay chose this during \"straightline\" integration on suggestion from a C struct linting tool. People have suggested it may not be optimal but never really made the case e.g. with both a theory and some relevant data. Maybe now is the time to do so?\n. I will make an attempt to provide a theory and practice for changing the packet struct actually.\nTheory\nHere is how they seem to compare to me on paper:\n- A: Payload is aligned to start of struct. (This will be 128 bytes in practice due to allocation routine.) Having SIMD-word-size alignment may speed up some operations or be exploitable to simplify optimizing assembler code.\n- B: First cache line contains all the most important data: length and first 62 bytes of payload (i.e. most or all headers). May be a peculiar effect with DMA writing to the same cache line as the length.\n- C: Payload is aligned to 64-byte boundary (see above). Length and payload are on adjacent cachelines which may help with certain operations e.g. L3 cache fetching cache lines in pairs.\nSo which is best? I am not sure but it seems plausible to me that B or C is better then A. Even if they were all equivalent there may be one formulation that seems simpler and less surprising to users.\nPractice\nI will ask Hydra to create a performance report for each variant and see if there is a detectable difference. Results to follow. Place your bets! :-)\n. Hydra jobs are submitted! Here are the (currently pending build) report links:\n- A: https://hydra.snabb.co/build/3780\n- B: https://hydra.snabb.co/build/3658\n- C: https://hydra.snabb.co/build/3583\nIf these don't work out you can look for the latest successful tests at https://hydra.snabb.co/project/lukego-sandbox\n. (Oops! Got some errors on the builds. Our Hydra is still in beta :). Have contacted expert support @domenkozar :-))\n. So! Hydra has completed those benchmark runs and generated the reports linked above. Welcome to take a peek :-)\nCouple of things jump out right away:\n- basic1 is faster with B and C.\n- dpdk64 fails with C.\nOther things are harder to interpret:\n- iperf scores may have a significant difference.\n- dpdk scores may have a significant difference.\nSo next steps, which I don't have time for right at this moment, would be to get the dpdk64 benchmark working for C and write some R code to automate the comparison and draw attention to which differences are significant (and which are not) to avoid drawing the wrong conclusions by eyeball.\n. @eugeneia How about if I put an I350 in davos?\n. Just for now I think that only selftest is realistic/supported.\n. I installed a dual-port I350 NIC into davos and cabled the ports together.\n. # Backend operational\nThe backend is up and running now! Hydra runs test campaigns automatically across a cluster of machines, archives the logs (including timeline files), and publishes an Rmarkdown report to highlight anomalies.\nThis hopefully provides the data feed that we need for developing and using the Snabb Studio front-end i.e. to identify the interesting results, download their timeline files, and use the GUI to understand what is going on.\nHere is an overview of how everything works and how you can access it. I will provide both links and, mostly for convenience, screenshots. I will link to recent runs and code on my own development branches.\nCode\nbench.nix is a Nix script that runs the benchmarks and converts their output into CSV. This is built on shell fragments to call our existing benchmarks.\nreport.Rmd is the Rmarkdown document that produces the report. This is an early draft and can be extended, tweaked, and pruned over time. Just now it seems reasonable to keep adding more graphs and then later remove the ones that are not useful in practice. I am trying to start the report with the most intuitive graphs and then move on to more exotic ones. (We also need an Rmarkdown document that presents a comparison between two or more Snabb branches.)\nI have mostly learned R from two O'Reilly books: R Cookbook and R Graphics Cookbook. This is where I picked up using the \"ggplot2\" library for most graphs.\nHydra\nI have defined a Hydra Jobset to run a benchmark campaign using software versions from specific git repos/branches:\n\nThis could be enabled to run automatically when any of the branches is updated. For the moment that is disabled and I run it manually.\nThe Jobset Evaluations lists the results of each execution of this jobset.\n\nEach execution only rebuilds what is needed: for example if only the Rmarkdown document has been changed then the previous benchmark results will be reused, but if the Snabb code was changed then all the benchmarks would be rerun. This is the normal behavior for Nix i.e. it looks at the SHA256 hash of what it needs to run and checks if it has already done that in the past.\nClicking on one of these Evaluations takes me to a page that lists all of the Jobs that Hydra has built for this Jobset.\n\nThis example has 122 jobs: the CSV file, the R report, and each of the 120 individual benchmarks (20 runs each of 6 different benchmarks.)\nClicking through to the benchmark-report job page leads me to benchmark.html that is the actual report. This includes a series of graphs, the first of which is a line graph.\n\nHere we can see that three results are interestingly slower than the others: basic1 test number 6 and iperf1500 tests number 8 and 15. I can use these numbers to identify which Hydra jobs contain the relevant logs, for example benchmark basic1 run 6 is job snabb-basic1-100e6-num-6.\n\nHere I can click the links to download either log.txt, the text output from running the test, or state.tar.xz, a tarball that includes the timeline file for the Snabb process(es) that were executed.\nThese timeline files can then be loaded into the GUI to see what is going on. Getting this working smoothly is the next step.\nAnticipating the questions we will want to answer with the GUI...\n- Is there one specific event that takes much longer in the slow runs than in other runs?\n- Or is slowness distributed more evently between all events?\n- And is the slowness spread across the whole trace or localized to just the start/middle/end?\n... but perhaps the visualizations will speak for themselves and we will not have to come into the analysis with such specific questions :).\n. HUGE THANKS @domenkozar for setting up this backend for me to tweak!\n. Sorry about the slow feedback both on this PR and changes further downstream.\nI have two requests that are both negotiable:\nChange function definitions to use consistent syntax i.e. space between function name and parameter (like function foo () and not function foo()). This is for consistency with existing surrounding code and to follow the style from Programming in Lua.\nChange ingress_packet_drops to use a simpler mechanism. How about if we just had a counter called engine.ingress_packet_drops that could be incremented by any app? Then the engine simply polls that counter for changes and reacts. Would also be visible to external tools like snabb top. Then we are reusing one existing abstraction (a counter) rather than inventing two new ones (ingress_packet_drops method and ingress_drop_monitor object).\n. @wingo Good point. Relatedly, #886 is introducing a counter called in-discards (defined in RFC 7223) that every I/O app should provide. So now we are talking about three different ways to monitor packets dropped at ingress: app callback method, global engine counter, per-app counter. Which way(s) is best and how often should the counter be sampled?\nOne possibility would be to adopt the #886 style where every app provides a counter saying how many ingress packets it has discarded (updated every 1ms) and the engine would monitor those counters at some reasonable interval (perhaps also 1ms).\nIf we found that we were doing a lot of things at 1ms intervals then we might want to add a tick() method to the engine and to each app that is automatically called at this interval and is the default place to do house-keeping that is not quite cheap enough to do every breath (e.g. read a dozen device registers over PCIe).\nHowever, now I am really floating away with the fairies, so maybe better to merge this implementation and consider iterating on it e.g. when landing #886?\n. Merge, thanks! Thanks @wingo and @dpino for the quick input too!\n. Great!\nCould you please write a paragraph summary of the current state of the code i.e. what you expect to happen when executing the selftest method? This can be the text of the pull request.\nCurrently the information is much too minimal to understand what this code represents i.e. no pull request text and only a few words in the commit messages. Spending some time and effort on writing these things is a big help to the rest of us!\n. Merged, thanks!\n. Just an idea: How about if we moved this feature out of the engine and applications (e.g. lwaftr) could run it via some suitable hook such as a lib.timer timer?\nMy reflection is that the main reason I see for embedding this directly in the engine would be to make it universally available but this may be overkill if this is an optional/experimental feature that we disable by default. WDYT?\n. One thing to note is that I think the upstream tests for Snabb NFV and LWAFTR are different. On LWAFTR I believe you are gradually increasing the traffic rate until you start to see service impacted. On Snabb NFV we are blasting maximum packet rate and measuring how much of that is successfully forwarded. So in the sense of the automated testing that SnabbBot does the Snabb NFV would always be overloaded because it is blasted at 100% line rate all the time.\nThis may not be relevant to what you are thinking of here but seems worth keeping in mind for interpreting test results.\n. Closing this specific PR as superseded by #929 but noting @mwiget's comments we need to add test cases for more dynamic workloads in the NFV application & likely adopt this mechanism there too. See also #937.\n. LGTM! Thanks and sorry about the wait. Merged into next.\nLikely we can promote this a global always-on feature once we have some more experience. Meanwhile it is interesting to develop a library of timers/hooks that can be installed when desired. Emacs extends quite neatly in that way.\n. I am excited about managing processes directly with Snabb. There are a lot of nice convenience / safety / error-recovery / administrative features we could implement. I also really want to be able to store more information in /var/run/snabb directories (e.g. 64MB timeline log files) and start archiving these in a disciplined way (e.g. via a hook or config option to store them in a well-defined location with compression).\nThoughts:\n- Major change: the /var/run/snabb directories will not exist after the process terminates, right? This is a user-visible change and it seems worth explaining why this makes sense in the PR text.\n- Major change: The snabb gc command is gone. This also seems worth drawing attention to in the PR text and being clear about why it is obsolete.\n- Concern: How can we be confident that /var/run/snabb directories won't be left around without cleanup? For example, is this design robust for unexpected-but-inevitable actions like pressing ^C in the terminal or running kill -9 on the pid that is using the most CPU?\nGenerally I would find this change easier to digest with more explanation. Maybe some commentary in the source code explaining the overall intention and how that is achieved with these particular system calls?\n. Is it safe to say that we would benefit from having well-defined configuration options and convenient methods of supplying them e.g. config files and command line?\nand further that such config items would best be described with a YANG schema for the Snabb engine and parameters like busy-wait, developer-debug, etc?\n. @eugeneia Pulled!\nThis is the first instance where we have pulled in the complete history of an application-specific fork (program/lisper) rather than squashing it. This is something we have talked about being open to in the past but not actually done.\nCan be that we will start bumping the limits of default settings for Git log browsing and need to start thinking about what information we are looking for and then how to query it (e.g. using options like --merges and --first-parent etc).\n. I fast-tracked this change onto next to support @domenkozar's CI efforts i.e. to try and run the NFV test suite using VM images that were bootstrapped via Nix expressions. (The change is already landed on max-next.)\n. How about if release.nix would take the software version, etc, as parameters? i.e. instead of being a completely stand-alone release-and-test procedure it would simply be nix library code that can be referenced from other repositories e.g. nixpkgs and snabblab-nixos.\nI see a few issues with trying to bake everything into the snabb/release.nix file:\n1. Determining software version, as we already discuss.\n2. Snabb has a distributed development workflow, where independent groups can make their own releases from their own application-specific branches, and it will be conflict-prone if everybody is changing the .version on their branch and then merging with each other.\n3. The release.nix is also referencing infrastructure like lugano server hardware which seems to me logically a part of the Snabb Lab (i.e. snabblab-nixos repo) and not of the software itself (which may be built/tested/released in a completely different environment e.g. upstream nixpkgs).\n. Thanks for the name change!\nI would still like to address these issues before merge:\n- Exactly when is configure called and what are the intended uses? Seems to me like the implementation is a hook for every time engine.configure() is called independent of the content of the configuration changes and that seems potentially excessive to me (more often than I would expect from reading the documentation).\n- Do we really want to have both configure and reconfig callbacks? This is the situation on the submitted branch. I think we should either merge these together or otherwise clearly explain when to use one vs the other. (If we keep both then we should also be consistent about abbreviating their names or not i.e. config vs configure.)\n. I would like to please withdraw my objection to the implementation of configure.\nI checked how the relink() callback had been implemented (added in commit 4f166b3a62daa0809061edd5fbe7b2953743d5ad and removed in commit c4a78856731cdb4bbc062f410cd46042e7ee32fa) and it is exactly the same. Could be seen as premature optimization to make a more complex implementation that runs the callback less frequently e.g. skipping for apps whose own links and configurations have not been touched.\nSo my only request now is to please reconcile configure with reconfig either by merging them together somehow or making their names/purposes more clearly distinct :).\n. @diego Thanks for providing that context. That helps a lot.\nHow about implementing this all locally in the push method like this?\n``` lua\nfunction NDP:push ()\n   self:maybe_send_ns()\n   ... process packets ...\nend\n-- Send rate-limited neighbor solicitation message.\nfunction NDP:maybe_send_ns ()\n   self.next_ns_time = self.next_ns_time or engine.now() -- first message is immediate\n   if self.next_ns_time <= engine.now() then\n      self:send_ns()\n      self.next_ns_time = engine.now() + self.ns_interval\n   end\nend\n```\nThis seems neat, predictable, and non-bloated to me. The send_ns() function will be called at regular intervals and take the appropriate (non-)action based on the current links and configuration. How about it?\nNote that lib.timer should not be used from apps at all and this seems to be the root of our difficulty here. Those timers are persistent, like crontab entries, and can easily drift out of sync with the app's own lifetime and configuration. You don't want zombie timers accumulating and running in the background and referencing obsolete objects via closures. I suspect that the root of our whole difficulty here is trying to match the lifecycles of apps and lib.timer timers which don't align very well. (Maybe we should rename lib.timer to lib.cron to make its appropriate usage more obvious?)\nWDYT?\nIncidentally I would quite like to have a tick() timer in apps that runs at a fixed frequency e.g. 1 millisecond. This could be used for taking care of administrivia such as checking whether it is time to send a neighbor solicitation message. This is something we could add if it would simplify or speed up existing apps in practice.\n. Merged. Thanks and sorry about the wait!\n. So! This is an awesome opportunity to apply our new Hydra-based benchmarking tools to a real performance problem. First time! Exciting :-)\nHere is a setup that could work for us:\n- Define our benchmark. For example:\n  - Run l2fwd branchmark as the nearest available proxy for the lwaftr performance tests.\n  - Run on lugano servers with Intel 82599 NICs. (We have been recently experimenting with software emulated NICs in order to run tests on more generic servers but better to exclude that code for now and focus on the most realistic setup.)\n  - Run the benchmark many times (e.g. 50) and present the distribution of results. Then we can see whether the benchmark is consistently fast, consistently slow, randomly varied, bimodally varied, etc.\n- Define several branches to compare. For example:\n  - a baseline branch (e.g. master) for comparisons to see improvements and degradations.\n  - a debug branch (e.g. with more logging and profiling enabled) to provide more information to compare between good and bad runs. The overall performance of the debug branch can be compared with the baseline branch to estimate how much the debugging code is affecting the results.\n  - some number of speculative optimization branches (e.g. one or more for each person working on the problem) where experimental changes can be pushed and automatically benchmarked against the other branches. This could make optimization into a \"multiplayer game\" where you can keep pushing changes to your branch and the report comparing all branches is updated automatically.\n- Define one or more Rmarkdown reports to present comparative results for all branches. The reports could range from quite generic - like visualizing the distributions of results - to quite specific - like counting certain JIT events or performance counters.\n- Hydra jobset that automatically runs all of the above, publishes the results, and updates every time a branch is modified (reusing any results for unchanged branches).\n@domenkozar Could you hook up a setup like this, pretty please? :smile: If master is not suitable as a baseline at the moment because the matrix tests depend on new code then we could pick whatever branch works.\n. @domenkozar Awesome!\nI downloaded the CSV from Hydra and made a report in Rstudio: http://rpubs.com/lukego/192632.\nI also sent the report source to you via PR snabblab/snabblab-nixos#50. Could you please hook that into the build? I suppose that we will want to incrementally add more reports and refinements to the matrix tests. Hydra would seem to support that quite well.\nHere are my notes:\n- I arbitrarily picked a subset of software versions to focus on: guest kernel = 3.18.29, qemu = 2.4.1, dpdk = 16.04.\n- This slice performs consistently over 30 iterations i.e. does not successfully reproduce the problem that @wingo is having.\n- Looking at the broader dataset there is a big difference with QEMU 2.5.1 or QEMU 2.6. They show fewer results (more failures) and unpredictable performance (bimodal at around 2.6 and 1.3 Mpps).\n- The scores generally look low at <= 2.6 Mpps. Having the pktsize column populated would make this easier to interpret.\nSo @wingo can you give more tips on how to reproduce? The ideal would be to add the lwaftr benchmark to the test matrix with help from @domenkozar. Otherwise we need to identify a better proxy to measure the problem you are seeing e.g. perhaps switching to a newer QEMU version that is showing problematic results in this test run.\nThis test matrix seems like it will be a fantastic tool for identifying bugs and incompatibilities. Going to be an interesting problem to prioritize which problems to dig into first...\n. @domenkozar Could we please add a couple more branches to this jobset? I'm thinking it would be neat to compare:\n- snabbco/master as the baseline.\n- snabbco/next as the likely optimizations for the next release.\n- experimental optimization branches, e.g. myfork/optimize, where people can force-push their own optimizations and have them automatically evaluated.\nSo first step could be to add snabbco/next and lukego/optimize branches (and perhaps other people's e.g. wingo/optimize if they create those branches first).\nI realize that I could create my own Hydra jobset that runs the tests from the branches that I care about, and this will probably be common practice for maintainers to test their own branches in the ways they most care about, but for now I figured it is nice to be social and work this out together in one place :).\n. @wingo True I am using this issue to respond to a bimodal perf issue that was really mentioned on Slack rather than here.\nOn the topic of this specific bug: In your perf record are you capturing only the behavior while processing traffic or also e.g. the time interval while spinning the idle loop and waiting for the VM to start and connect? I see a risk of \"optimizing the idle loop\" if perf record is taking samples beyond the real test interval.\n. @eugeneia Hah! I had just done a git bisect and tracked the problem to the same commit and also written a message saying I don't understand then problem. Now I see your note above and suddenly it makes sense :).\nThe issue is the semantics of nil values in Lua tables. Lua does not permit nil as a table value and the effect of foo.bar=nil is to remove the key bar from table foo.\nSo if we set debug = nil in the timer module and then reference the key debug it will see that the key is missing and delegate the lookup to the global environment (via metatable) where it will resolve as _G.debug which is the Lua debug module.\nI will push a fix along the lines of your patch above.\n. Hydra has found a performance regression from master to next that needs to be resolved. Go Hydra! I updated the status comment on the PR text.\nLooks like it should be bisectable because one branch is consistently outperforming the other on lugano servers. @domenkozar How might I execute this test as part of a manual git bisect? I mean, is there an easy way to interactively (on lugano-1) run the test case based on a locally checked out Snabb and eyeball the result to see if it is good or bad? I am thinking a nix-build ... with the right args is all I need?\n. Are you sure we need snabblab/snabblab-nixos#35? Seems like I could run the test locally on lugano-1 with a plain nix-build if I would choose the right nix expression and arguments from in the snabblab-nixos repo?\n. Trying that. Relatedly: is there also a convenient way to run one benchmark by name e.g. l2fwd_snabb=master_dpdk=1-8-0_qemu=2-4-1-num-1? Seems a bit round-about to use the Hydra jobset as an entrypoint from the command-line (?)\n. (Removed question that I quickly realized the answer to i.e. I had an existing build in the source directory I was testing...)\n. Awesome! I was able to use nix-build to run the benchmark locally and then isolate the problem using git bisect and related commands. Great to be able to address all versions of all software dependencies with one command-line!\nHere is the command that worked for running one test against local checkouts:\n$ sudo lock nix-build jobsets/snabb-matrix-packet-ABC.nix \\\n                      -A benchmarks.l2fwd_snabb=bisect_dpdk=16-04_qemu=2-4-1-num-1 \\\n                      --arg numTimesRunBenchmark 1 \\\n                      --arg snabbAname '\"bisect\"' \\\n                      --arg snabbAsrc ~/git/snabb-bisect \\\n                      --arg nixpkgs ~/git/nixpkgs\n                      --arg benchmarkNames '[ \"dpdk\" ]' \\\n                      --arg reportName '\"report-by-snabb\"' \\\nwhich runs the nfv/dpdk benchmark and prints output like this:\nProcessed 100.0 million packets in 36.74 seconds (6400001562 bytes; 1.39 Gbps)\nMade 1,338,361 breaths: 74.72 packets per breath; 27.45us per breath\nRate(Mpps):     2.722\nThe Hydra report shows that \"good\" is around 2.7 Mpps and \"bad\" is around 2.5 Mpps so for git bisect I used 2.6 Mpps as the cutoff point.\nDiagnosis\nThe ~10% performance regression comes with the new statistics counters (#931) that landed on kbara-next with commit 9906acaaf69147e3d1869d4075d8609bdc8c7536 and on next with commit 00706efafc2bde086ae013d3db4b347cb5914b7f. So @eugeneia we need to find the slow part, optimize that, and land a fix on next.\nGreat to have performance regression tests, let's do more of this!!\n. I cherry-picked the virtio-net fix df8e83c6771a7b892324922f25d5123a69f651d9 from lwaftr repo for renegotiating virtio-net options with guests that switch drivers while running.\n. @eugeneia Looking at a new Hydra run it looks like the same performance regression is there on both next and your statistics-superset branch. If you push new code to your branch it should automatically rerun. Hopefully you can find results by clicking around the snabb-new-tests project on Hydra. Currently we have two relevant CI jobs, regression-min-2016.07 running a quick 5-iteration benchmark and regression-2016.07 running a thorough 30-iteration benchmark.\n@domenkozar I would like to generalize these CI jobs. Instead of specifically testing for a performance regression on the 2016.07 release they could be continuously showing the comparative performance of a collection of branches: master, next, lukego/optimize, eugeneia/optimize, wingo/optimize, kbara/optimize, etc. That way everybody can always interact with the CI simply by pushing to their optimization branch and waiting to see the result. Just a matter of renaming the jobsets and locking in the branch names?\nMeta: I reckon that all \"out of band\" information we are discussing should be brought \"in band\" somehow. For example, if profiler information is required for interpreting results then the CI tests should include runs that are profiled and publish the reports. This way we are all looking at the same information and can see exactly how it was produced.\n. @domenkozar This is really cool testing to have up and running, btw! I am especially impressed that Hydra is smart enough to reuse all previous results that are still valid. So if one branch is updated then its benchmarks will be rerun but the results for the unmodified branches will be reused. This seems extremely efficient and means we can include a lot of branches in the benchmark campaigns.\n. Merged, thanks!\n. Merged, thanks!\n. My impression is that this issue has appeared with QEMU 2.5.1 and was not present with QEMU 2.4. I would quite like to git bisect the QEMU repo between those releases to identify which change causes the issue.\nIs there a straightforward way to do this i.e. run the test using a QEMU from a local directory rather than a release tarball?\n. Looking over the logs this seems to be related to the guest handing over the Virtio-net device from its kernel driver (loaded at boot time) to the DPDK driver. Both on QEMU 2.4 and QEMU 2.5 there is a suspicious burst of ~500K packets before the DPDK driver renegotiates the device features. Have to determine the source of this burst and why with QEMU 2.5 we receive an illegal pointer value for a packet.\nHistorically our CI benchmarks for DPDK have not included a hand-over of the Virtio-net device from the Linux kernel to DPDK. Instead we kept it simple by disabling Virtio-net on the kernel so that the device was dedicated to DPDK. This probably explains why we are suddenly seeing this problem since we switched to the new guest image that is bootstrapped with nix.\nHere is a quick summary of the relevant bits of the logs:\nQEMU 2.4\nFirst the Snabb process starts and negotiates features with the VM. The feature list is quite extensive, including checksum offload, and this is typical of a Linux kernel driver in the VM (not DPDK).\nsnabbnfv traffic starting (benchmark mode)\nLoading program/snabbnfv/test_fixtures/nfvconfig/test_functions/snabbnfv-bench1.port\nengine: start app B_NIC\nengine: start app B_Virtio\nGet features 0x18428001\n VIRTIO_F_ANY_LAYOUT VIRTIO_NET_F_MQ VIRTIO_NET_F_CTRL_VQ VIRTIO_NET_F_MRG_RXBUF VIRTIO_RING_F_INDIRECT_DESC VIRTIO_NET_F_CSUM\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nvhost_user: Caching features (0x18028001) in /tmp/vhost_features_vhost_B.sock\nSet features 0x18028001\n VIRTIO_F_ANY_LAYOUT VIRTIO_NET_F_CTRL_VQ VIRTIO_NET_F_MRG_RXBUF VIRTIO_RING_F_INDIRECT_DESC VIRTIO_NET_F_CSUM\nThen we see a period of idleness punctuated by an unexplained burst of ~500K packets in one second:\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nNo LuaJIT profiling enabled ($NFV_PROF unset).\nNo LuaJIT dump enabled ($NFV_DUMP unset).\nload: time: 1.00s  fps: 533,934   fpGbps: 0.312 fpb: 50  bpp: 64   sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nThen we see the Virtio-net features renegotiated in a simpler way, presumably by DPDK, and traffic is processed:\nvhost_user: Caching features (0x00028000) in /tmp/vhost_features_vhost_B.sock\nSet features 0x28000\n VIRTIO_NET_F_CTRL_VQ VIRTIO_NET_F_MRG_RXBUF\nrxavail = 0 rxused = 0\nvhost_user: Connected and initialized: vhost_B.sock\nrxavail = 0 rxused = 0\nload: time: 1.00s  fps: 6,889,568 fpGbps: 4.024 fpb: 208 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 6,965,276 fpGbps: 4.068 fpb: 208 bpp: 64   sleep: 0   us\nload: time: 1.00s  fps: 6,899,028 fpGbps: 4.029 fpb: 208 bpp: 64   sleep: 0   us\nQEMU 2.5\nQEMU 2.5 starts in the same way with the Linux kernel apparently initializing the device first:\nsnabbnfv traffic starting (benchmark mode)\nLoading program/snabbnfv/test_fixtures/nfvconfig/test_functions/snabbnfv-bench1.port\nengine: start app B_NIC\nengine: start app B_Virtio\nGet features 0x18428001\n VIRTIO_F_ANY_LAYOUT VIRTIO_NET_F_MQ VIRTIO_NET_F_CTRL_VQ VIRTIO_NET_F_MRG_RXBUF VIRTIO_RING_F_INDIRECT_DESC VIRTIO_NET_F_CSUM\nGet features 0x18428001\n VIRTIO_F_ANY_LAYOUT VIRTIO_NET_F_MQ VIRTIO_NET_F_CTRL_VQ VIRTIO_NET_F_MRG_RXBUF VIRTIO_RING_F_INDIRECT_DESC VIRTIO_NET_F_CSUM\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nvhost_user: Caching features (0x18008001) in /tmp/vhost_features_vhost_B.sock\nSet features 0x18008001\n VIRTIO_F_ANY_LAYOUT VIRTIO_NET_F_MRG_RXBUF VIRTIO_RING_F_INDIRECT_DESC VIRTIO_NET_F_CSUM\nThen we have an idle period followed by the same burst of ~500K packets but followed immediately by an error:\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nNo LuaJIT profiling enabled ($NFV_PROF unset).\nNo LuaJIT dump enabled ($NFV_DUMP unset).\nload: time: 1.00s  fps: 497,213   fpGbps: 0.290 fpb: 47  bpp: 64   sleep: 100 us\nlib/virtio/net_device.lua:363: mapping to host address failedcdata<void *>: 0xffff88001fa19600\nSo the questions are: what are those packets being sent before DPDK has taken over the NIC? and why the illegal address?\nCould be e.g. that somehow the Virtio packet descriptor memory becomes garbage for a period of time.\n. Great overview! How do you do the counting?\n. Merged, thanks!\n. Merged, thanks!\n. Hydra report here: https://hydra.snabb.co/build/136747/download/2/report.html\nGreat that we are putting our new benchmarking infrastructure into practice! The report clearly shows a subtle performance regression that is very easy to miss in interactive benchmarking due to subtle variation between runs.\nI am having a read of some JIT dumps to understand why counter bumping is influencing the benchmark scores so much. Seems like it should be a very cheap operation in principle.\n. Really nice :).\n. Cool!\nI like the idea of associating types and modules with shm objects. The objects are becoming richer over time, not just counters but also e.g. histograms and timeline logs, and I would really like to have this framework for associating useful functionality with those objects e.g. pretty-printing functions.\nThis is something I have been lacking in terms of a UI for timeline objects. I already have a timeline.dump() function for pretty-printing the log but haven't had an obvious way to access that. Calling it via snabb snsh -e timeline.dump(...) seems too ad-hoc, a dedicated program like snabb timeline ... seems like overkill, so having an easy way for programs like snabb top (etc) to find the dumper seems a nice compromise.\n. One related thought on the topic of counters...\nI would really like to have counters stored directly where they are accessed e.g. directly in struct link. Having pointers to counters in struct link is uncomfortable to me because it means more instructions (chasing pointers) and potentially cache misses too. Since counters have recently been affecting overall performance I think we need to start taking care of these details.\nOne idea would be for counter.commit() to be more flexible about how it sources counters. For example, perhaps it could determine the current value of each counter by calling a function. This way we could gather counter values from where-ever the master value lives e.g. as a uint64_t in struct link. This way we would move the logic of chasing pointers to find counters from the fast path (link.transmit etc) to the slow path (counter.commit()).\nWhat do you think? (Likely this is something we should consider separately to this PR - unless there is a performance impact on this branch that we need to mitigate.)\n. @eugeneia need a [wip] tag next time :-)\n@kbara if you want to drop these changes with a rebase that is fine with me. I think we would need to toggle the \"protected\" flag off from kbara-next for a moment to make that possible.\nOtherwise git revert works too and is just a bit noisier (and would require a revert of the revert before merging this branch again). That would be the appropriate solution if other people had already merged these changes from kbara-next but I doubt that is the case right this moment.\n. Relatedly: I have also been starting to think about maintaining next as a pair of branches:\n- lukego/next would be an experimental working branch. Here I merge changes that look good to me. I test this branch as much as I like using a private Hydra jobset. If I find a problem then I will rewrite this branch with a rebase (which does not cause problems because it is a private development branch that I don't send PRs from and other people are not supposed to merge from).\n- snabbco/next is where I push changes once I am satisfied with them. If I make a mistake here then people can pull that and I have to follow it up with a fix e.g. revert commit. This is okay because the rate of mistakes should be lower due to earlier testing on the dev branch.\nThe basic intention would be to avoid merging changes onto a well-known branch before I know whether it causes a performance regression or other test failure that can only be detected by CI.\n. @eugeneia That's okay for me.\nHey there are a few outstanding issues here that I would like to resolve for this release but maybe we can take on next. The main thing is that I want Hydra to snapshot the shm folder for relevant process(es) and archive this for reference. This way we would have the timelines recorded for later inspection. I wonder what is the least-hacky way to do this? Basically we need to either run a hook before we delete the shm dir (which could be used to copy a snapshot somewhere) or we need to create the shm directory in a special location and then not delete it on termination.\nThis branch actually contains a couple of hacks in this direction that are not very effective and likely should be rolled back. One is adding the env var SNABB_SHM_ROOT that can force a new location for the shm directory. The other is in snabbnfv.traffic to explicitly save the timeline in $pwd at the end of test. I actually pushed reverts for these but cheekly reset that away quickly because I am not sure if they are part of the solution or not.\nLike I say these issues could be resolved on next so needn't block merge of this branch. I do really want to pick a solution for preserving the shm directory for Hydra though.\n. I notice that we already have _G.developer_debug inhibiting removal of the shm directory. That is probably sufficient for getting this hooked into Hydra. Good enough for now.\nGenerally I am uncomfortable with debug modes like _G.developer_debug and SNABB_DEBUG because I don't understand their consequences. Do they reduce performance? Leave files on the file system? Create verbose output? Likely I willl want some of these but not others, e.g. if I am benchmarking then I want the state kept on the filesystem for examination but I don't want to enable debug features that will impact performance. So a global debug=yes/no flag seems unlikely to do what I mean.\nHave to make a snabb.conf with a well-defined format e.g. YANG schema. One step at a time...\n. Can we move virtual_ether_mux into e.g. program/snabbnfv/traffic? If it wants to live in lib/ then I think it needs documentation.\n. @eugeneia Looks like the renaming of packetblaster_bench.sh to dpdk_bench.sh is breaking the Hydra test script. The lukego/next-tributary jobs shows the DPDK benchmarks failing on max-next with an error finding the script. Tricky... the new name is better, but renaming successfully requires cross-repo changes i.e. snabblab-nixos test expression needs to be updated to work with both the new and the old name.\nOn balance I think it makes sense to revert the change of the script name. This keeps the interface compatible for external test scripts (esp. Hydra's nix exp) and should mean that the next-tributary jobset will successfully run the benchmarks to help me avoid accidentally merging a performance regression onto next.\nCould you please push a revert of that name change and check that the lukego/next-tributary successfully benchmarks max-next & then resubmit the PR? (Sorry for the inconvenience. Intention is to establish a workflow where Hydra successfully benchmarks the *-next branches before I merge them to next.)\n. @eugeneia That did the trick, thanks!\nGoing forward could I please pull directly from max-next rather than creating the dedicated branches like max-next-v2016.08-1? This is just because I am now doing extra testing of max-next via my Hydra jobset and that helps me to merge with confidence. The consequence would be that when you have a PR open from max-next you would need to only push commits that you consider ready for merge (since they will be automatically added to the PR).\nI actually pulled max-next to get these changes now. I realize now that this brought along #975 before you PR'd that. I hope that is okay with you.\nClosing now because the relevant commits have already landed.\n. @kbara Yes I see what you mean about the difficulty of knowing whether a variation is significant or random noise. It will be interesting to see how the graphs look when we compare code that actually does have different performance. I have added next to the large matrix test now so we should soon be able to see the effect of fixing the counters performance regression, fixing the filter-blocks-iperf test setup bug, and fixing the errors in the dpdk benchmark (#965). I am guessing that these changes will really jump out of the graphs but we will see.\nCould also be that we can improve the plots to convey more information about significance. There are some examples in Plotting distributions (ggplot2).\nIf anybody should feel inspired to muck around with the graphs here is a quick start:\n1. Install RStudio. Lovely program!\n2. Download the latest large-matrix CSV file from Hydra.\n3. Play with the Rmarkdown reports either by loading them directly or just copy-pasting a few lines.\n. Couple more observations about the report...\nl2fwd by QEMU and DPDK\nWow! Looking more closely at the sliced l2fwd results there are a couple of really significant things: QEMU 2.4.1 works better than the others and DPDK 1.8 doesn't work at all.\nHere is how the data looks when we plot the success rate of the l2fwd benchmark separately for each QEMU and DPDK version combination:\nR\nsuccess = summarize(group_by(l2fwd, qemu, dpdk), success = mean(!is.na(score)))\nggplot(success, aes(qemu, dpdk, fill=success)) +\n  geom_tile(aes(fill=success)) + scale_fill_gradient(low=\"red\", high=\"white\") +\n  geom_text(aes(label = scales::percent(success)))\n\nand this suggests a lot of interesting ideas:\n- DPDK 1.8 is not working in the tests at all.\n- QEMU 2.4.1 works ~99% of the time while other QEMUs work only ~15% of the time.\n- The reason l2fwd tests seem reliable in other test setups (e.g. SnabbBot and Hydra tests on lugano servers) is likely because they happen to test with \"good\" versions of QEMU and DPDK.\nThis is awesome! Go Hydra!\niperf duplicate rows\nThe iperf success and failure counts are all multiples of 5. The CSV seems to contain duplicate rows like this:\niperf,NA,base,matrix,3.18.29,2.1.3,NA,1,17.3,Gbps\niperf,NA,base,matrix,3.18.29,2.1.3,NA,1,17.3,Gbps\niperf,NA,base,matrix,3.18.29,2.1.3,NA,1,17.3,Gbps\niperf,NA,base,matrix,3.18.29,2.1.3,NA,1,17.3,Gbps\niperf,NA,base,matrix,3.18.29,2.1.3,NA,1,17.3,Gbps\n@domenkozar Can be something with the nix expression? For example could it be that the matrix is including a row for each different DPDK version (there are 5) and then reusing the result since the test does not use that software?\nStats and modelling\nJust a thought: R has some really fancy features for fitting data to models and e.g. telling you which factor explains the most variation in results. So if we would write some nice expressions then R should be automatically tell us \"QEMU version and DPDK version each have a huge impact on l2fwd results.\"\n. Hypothesis for what is going on with QEMU here:\nThe original QEMU vhost-user feature that we contributed to QEMU 2.1 did not support resetting the Virtio-net device. This is simply something that we missed and lacked a test for. The consequence is that if the guest shuts down its Virtio-net device and reuses the descriptor memory for other things then it will lead to making garbage DMA requests e.g. causing an error in the vswitch or overwriting memory in the guest.\nLater we found this problem when we tested rebooting VMs. Reboots failed, presumably due to memory corruption during boot. We fixed this by extending QEMU to send a notification when the guest device shuts down so that the vswitch knows to stop processing DMA requests. This fix went upstream but was later reverted for reasons that I do not fully understand.\nSo my hypothesis is that QEMU 2.4.1 includes the fix, but none of the others do, and the error is hand-over inside the VM from the kernel driver to the DPDK driver.\nThe reason we did not see this with SnabbBot, even with other QEMU versions, is that we setup those VMs to dedicate the Virtio-net device to DPDK i.e. prevented the kernel from initializing it at boot. The new VMs bootstrapped with nix are first letting the kernel virtio-net driver initialize the device and then handing it over to DPDK. This means that the nix images would depend on the QEMU patch to succeed while the SnabbBot ones would not.\nI'll get in touch with QEMU upstream and see what they reckon. I would certainly like to include device reset in our test suite and have that working reliably so I am glad that the nix images are exercising this scenario and showing the problem.\n. Hard to put this stuff down :-).\nNext question: How come we still have 1% failures with QEMU == 2.4.1 and DPDK != 1.8.0?\nFirst we could take a peek at some basic statistics about these rows:\n```\nSummarize rows for failing tests with \"good\" qemu and dpdk\nsummary(subset(l2fwd,\n              subset = qemu == '2.4.1' & is.na(score) & dpdk != '1.8.0',\n              select = c(pktsize, config, snabb, dpdk)))\npktsize   config           snabb       dpdk\n 64 :9   base : 0   matrix     : 3   1.8.0:0\n 256:8   noind: 3   matrix-next: 4   16.04:8\n         nomrg:14   next       :10   2.0.0:2\n                                     2.1.0:3\n                                     2.2.0:4 \n```\nNow we have really sliced-and-diced the data: only 17 rows left out of the 37,800 results in the CSV file.\nQuick observations:\n- The failures seem to be evenly split between packet sizes: 9 failures w/ 64-bytes and 8 failures w/ 256-bytes.\n- Some software versions seem to possibly be slightly more vulnerable: Snabb next and DPDK 16.04.\n- There are no failures with the base configuration of virtio-net. That is, we only saw these failures when we suppressed either \"mergeable RX buffers\" or \"indirect descriptors\" in the feature negotiation.\nQuestion is, how confident can we be about this? I think that the R aov (Analysis of Variance) feature can answer this for us:\n```\npassfail <- subset(l2fwd, subset = qemu == '2.4.1' & dpdk != '1.8.0')\npassfail$score[!is.na(passfail$score)] <- TRUE\npassfail$score[is.na(passfail$score)]  <- FALSE\nsummary(aov(score ~ pktsize + config + snabb + dpdk, data=passfail))\n          Df Sum Sq Mean Sq F value Pr(>F)\n\npktsize        1  0.023 0.02315   2.992 0.0838 .\nconfig         2  0.126 0.06300   8.144 0.0003 ***\nsnabb          2  0.040 0.01991   2.574 0.0765 .\ndpdk           3  0.038 0.01281   1.656 0.1745  \nResiduals   2151 16.639 0.00774                   \n\nSignif. codes:  0 '' 0.001 '' 0.01 '' 0.05 '.' 0.1 ' ' 1\n```\nI believe this means that we can be 99.9% confident (***) that the configuration is affecting the failures, but only ~10% confident (.) that Snabb version or DPDK version is.\nSo now we know of one bug somewhere in the test setup - failures when virtio-net options are suppressed - and we know what further testing to do for context - something like 10,000 more tests with QEMU 2.4.1 and DPDK ~= 1.8 so that we can be sure whether software versions are significant (would be handy to know for debugging purposes).\nWe could also browse Hydra to find the logs for these 17 failing tests and review them. First step could be dumping the 17 CSV rows in order to identify the relevant test cases.\n. Looks to me like the reason for the 1% failures is mostly cases that run slowly and bump a 160-second timeout that we have in the Nix expression for the benchmark. Could be an idea to increase that timeout somewhat so that we can better differentiate between slow and failed cases. However, I hold off on that for the moment because redefining the benchmark definition would force Hydra to do a lot of work (rerunning tests for every branch based on the new definitions).\nI dumped the list of failed cases like this:\n```\nd <- read.csv(\"/home/lukego/Downloads/bench (1).csv\")\nsubset(d, subset = benchmark=='l2fwd' & qemu=='2.4.1' & dpdk!='1.8.0' & is.na(score))\n  benchmark pktsize config       snabb  kernel  qemu  dpdk id score unit\n\n4634      l2fwd      64  nomrg      matrix 3.18.29 2.4.1 16.04 14    NA Mpps\n4661      l2fwd      64  noind      matrix 3.18.29 2.4.1 16.04 11    NA Mpps\n4904      l2fwd     256  nomrg matrix-next 3.18.29 2.4.1 16.04 14    NA Mpps\n4997      l2fwd      64  nomrg matrix-next 3.18.29 2.4.1 16.04 17    NA Mpps\n5017      l2fwd      64  noind matrix-next 3.18.29 2.4.1 16.04  7    NA Mpps\n5035      l2fwd      64  noind matrix-next 3.18.29 2.4.1 16.04 25    NA Mpps\n5269      l2fwd     256  nomrg        next 3.18.29 2.4.1 16.04 19    NA Mpps\n5274      l2fwd     256  nomrg        next 3.18.29 2.4.1 16.04 24    NA Mpps\n12811     l2fwd     256  nomrg        next 3.18.29 2.4.1 2.2.0  1    NA Mpps\n12812     l2fwd     256  nomrg        next 3.18.29 2.4.1 2.2.0  2    NA Mpps\n12837     l2fwd     256  nomrg        next 3.18.29 2.4.1 2.2.0 27    NA Mpps\n12913     l2fwd      64  nomrg        next 3.18.29 2.4.1 2.2.0 13    NA Mpps\n20374     l2fwd     256  nomrg        next 3.18.29 2.4.1 2.1.0  4    NA Mpps\n20479     l2fwd      64  nomrg        next 3.18.29 2.4.1 2.1.0 19    NA Mpps\n20485     l2fwd      64  nomrg        next 3.18.29 2.4.1 2.1.0 25    NA Mpps\n27229     l2fwd     256  nomrg      matrix 3.18.29 2.4.1 2.0.0 19    NA Mpps\n28037     l2fwd      64  nomrg        next 3.18.29 2.4.1 2.0.0 17    NA Mpps\n```\nThen I manually transcribed these into job names and searched Hydra for their build links:\n```\n4634      l2fwd      64  nomrg      matrix 3.18.29 2.4.1 16.04 14    NA Mpps\nbenchmarks.l2fwd_pktsize=64_conf=nomrg_snabb=matrix_dpdk=16-04_qemu=2-4-1_num=14\nhttps://hydra.snabb.co/build/151505/\n4661      l2fwd      64  noind      matrix 3.18.29 2.4.1 16.04 11    NA Mpps\nbenchmarks.l2fwd_pktsize=64_conf=noind_snabb=matrix_dpdk=16-04_qemu=2-4-1_num=11\nhttps://hydra.snabb.co/build/155072\n4904      l2fwd     256  nomrg matrix-next 3.18.29 2.4.1 16.04 14    NA Mpps\nbenchmarks.l2fwd_pktsize=256_conf=nomrg_snabb=matrix-next_dpdk=16-04_qemu=2-4-1_num=14\nhttps://hydra.snabb.co/build/150561\n5269      l2fwd     256  nomrg        next 3.18.29 2.4.1 16.04 19    NA Mpps\nbenchmarks.l2fwd_pktsize=256_conf=nomrg_snabb=next_dpdk=16-04_qemu=2-4-1_num=19\nhttps://hydra.snabb.co/build/181719\n```\nand what I see in most (but not all) of the logs I checked is low speeds (~1Mpps or less).\n. @kbara I pushed a new version of the report that includes overlay histograms now. The Y-axis also now gives the exact number of results per bin. Better? https://hydra.snabb.co/build/203152/download/2/report.html\n(I also added the red-and-white \"success rate\" tables. I have a fix in the pipeline that will make the l2fwd picture look much better soon.)\n. @kbara matrix and matrix-next are the names of Snabb branches being tested. (Could alternatively be e.g. master and kbara-next.) So what we are doing here is comparing how  different Snabb branches/versions perform.\nScore is Gbps (benchmark=iperf) or Mpps (benchmark=l2fwd). i.e. it is a scalar value saying how fast a test went (higher is better). we kind-of get away with mixing the units because they vaguely similar in magnitude.\nCount is the number of test results that fell within a range of scores. For example in the first graph, on the left, we can see that every branch had around 6,000 failed tests (histogram bucket at 0).\nThat help?\n. Cooooool!\nHere is a fresh new NFV test matrix report that is much more exciting. This compares master with branch nfv-test that includes two small fixes for low-hanging fruit (#984 and #985) that have been causing most of the failures in this test env.\n@kbara I would be really interested to hear your take on what the report says! (Have only glanced at it so far myself but itching to take a closer look :-))\n. @kbara Thanks for taking the time to read all of this and give feedback. I am working this out as I go along so it is very helpful to be able to discuss it.\nZooming out for one sec, I see two distinct problems to solve with these benchmarks:\n1. Deployment planning: measuring the absolute performance of software in different scenarios (hardware, configuration, workload, etc) so that you can deploy it successfully.\n2. Software optimization: measuring the relative performance of software versions to evaluate whether a change would have a positive/negative/neutral impact on performance.\nOn the one hand for deployment planning we need to really care about what we are measuring and how to interpret the results in real-world terms. On the other hand for software optimization it may be reasonable to treat the benchmark results more abstractly, e.g. simply as \"scores\" for which higher values are linearly better. (Like e.g. SPECint: I can't remember what workload that represents but I know the GCC people would be thrilled to get a patch that increases the score by 2%.)\nJust now I am mostly wearing the \"software optimization\" hat and that is why I am being loose with units. The benchmark scores are numbers and I want to make them higher (move ink to the right) and more consistent (move ink upwards).\nComing back to your comments: I think we are seeing the limitations of the cookie-cutter approach of generating canned graphs for what is, for now at least, more of an exploration than a quantification. Indeed we can see that nfv-test improves on master by having fewer failures, and more results distributed across the other buckets in the histogram, but we can't really see if there are other effects hidden behind this big and dramatic one e.g. whether successful tests tend to be faster or slower.\nHave to see how the situation evolves with experience i.e. how much can we effectively summarize with predefined graphs and statistics (as few as possible) and when do we need to roll up our sleeves for some data analysis.\nRelatedly, here is my R bookshelf at the moment:\n- R for Data Science. Gentle introduction to R focused on the latest and most popular libraries (ggplot2, dplyr, etc).\n- R Cookbook and R Graphics Cookbook are interesting to leaf through.\n- Statistical Modeling: A Fresh Approach. Waiting for delivery but high hopes based on the online chapters.\nNew case study\nJust at braindump quality but maybe interesting anyway: rough draft analysis of some tests that I ran over the weekend to evaluate the impact of a QEMU patch that never went upstream.\n. @tobyriddell Thanks for the link! Please feel welcome to braindump about performance testing work here :).\nI have skimmed the paper. Looks fancy :) I don't immediately grasp the details but I think that I understand the problem they are working on i.e. optimizing use of test resources (e.g. hardware) by running exactly the number of tests needed to reach an appropriate confidence interval.\nI am cautious about this fancy approach for two reasons:\nFirst, are they optimizing for answering predefined questions at the expense of post-hoc data exploration? I want to use the datasets both for hypothesis testing and also hypothesis generation i.e. poking around interactively in the dataset to look for interesting patterns (\"I wonder if CPU microarchitecture explains some of the variation in IPsec scores?\" etc). So there is at least some value for me in generating a large and regular dataset.\nSecond, I really like the idea of having a clean division oflabor between different skill sets:\n- Data production (nix/devops).\n- Data analysis (R/statistics).\n- Optimization (LuaJIT/profiling).\nIf these activities are well separated then you only need one skill set to contribute to the project. For example, if R and statistics are your thing then it is nice to be able to simply work on the CSV files without having to operate the CI infrastructure.\nEnd braindump :).\n. @kbara Braindump... putting on the \"characterizing system performance in real world terms\" hat for a moment.\nQuoth R for data science:\n\nThe goal of a model is to provide a simple low-dimensional summary of a dataset\n\nI submit that describing the performance of an application is a statistical modeling problem. The goal is to define a model that accurately predicts real-world performance. The model is simply a function whose inputs are relevant information about the environment (hardware, configuration, workload, etc) and whose output is a performance estimate. The goodness of the model depends on how easy it is to understand and how accurately it estimates real-world performance.\nSo how would this look in practice? Let's take the simplest possible model and then make some refinements.\nModel A: Magic number\nThis simple model predicts that the application always performs at \"line rate\" 10G.\nlua\nfunction A ()\n   return 10.0 -- Gbps\nend\nThis model is very simple but there is no evidence in the formulation to suggest that it is accurate.\nModel B: Symbolic constant\nThis model predicts that the application performance is always the\nsame, regardless of environment, but uses a symbolic constant rather\nthan a literal magic number.\nlua\nfunction B (k)\n   return k.Gbps -- 'Gbps' value in table of constants k\nend\nThe constant k.Gbps could be empirically calculated e.g. as the average result of all benchmarks. This could be done as part of the release process with the result included in the release notes.\nModel C: Linear with processor speed\nThis model takes hardware differences into account by estimating that\nperformance will be linearly proportional to the CPU clock speed.\nlua\nfunction C (k, e)\n   return e.GHz * k.bitsPerCycle\nend\nHere the end-user supplies the value e.GHz based on their proposed deployment hardware and the software release notes contain the constant k.bitsPerCycle. Together they tell you the expected performance e.g. if e.GHz = 2.4 and k.bitsPerCycle = 2 then the performance estimate is 4.8 Gbps.\nHere the performance curve is modeled as a straight line that increases with GHz. The constant k.bitsPerCycle is the slope of the line. The test suite could automatically calculate this constant for a given release by running benchmarks at many clock speeds and performing a linear regression to find the best-fitting gradient. (This would be a simple one-liner in R.)\nModel D: Many factors\nThis model is more elaborate and taking a step towards practicality.\n``` lua\nfunction D (k, e)\n   local per_core = e.GHz * bitsPerCycle(k, e)\n   return per_core * math.pow(e.cores, k.multicoreScaling)\nend\n-- Return the number of bits processed per cycle.\n-- Performance depends on whether IPsec is enabled and if so then also\n-- on the CPU microarchitecture (because Haswell AES-NI is twice as\n-- fast as Sandy Bridge).\nfunction bitsPerCycle (k, e)\n   if     e.ipsec and e.haswell then return k.fastIpsecBitsPerCycle\n   elseif e.ipsec               then return k.slowIpsecBitsPerCycle\n   else                              return k.baseBitsPerCycle end\nend\n```\nHere the user supplies this information:\n- e.cores number of cores devoted to the application.\n- e.GHz clock speed of each core.\n- e.ipsec is true if IPsec will be enabled.\n- e.haswell if Haswell (or later) CPUs will be used.\nAnd the test suite calculates these constants:\n- k.multicoreScaling exponent of the exponential curve (speedup=cores^k) saying how much performance increases as more cores are added. If the scaling is perfectly linear then the value will be 1 and if it is sub-linear then it will be less than 1.\n- k.fastIpsecBitsPerCycle throughput with IPsec on a Haswell+ CPU.\n- k.slowIpsecBitsPerCycle throughput with IPsec on an older CPU.\n- k.baseBitsPerCycle throughput when IPsec is disabled.\nThis would be cool, huh? You could really understand a lot about the performance of a given release just by looking at those constants e.g. how close is the scalability to linear, how expensive are the features, how important is the choice of CPU, etc.\nI am not really sure if the R side of this would be easy, hard, or impossible. I suspect it would be straightforward by porting that function to R and feeding the benchmark CSV file into some suitable off-the-shelf \"nonlinear regression\" algorithm, but maybe I am being hopelessly naive.\nIf we would really be working with R at this level of sophistication then we could start to refine the model iteratively. We would compare actual results with predicted results and check for patterns in the \"residuals\" that indicate important details that our model has missed. This may lead us to refine the model to make better predictions e.g. allowing for e.GHz to have a nonlinear effect (e.g. on a memory-bound workload like lwAFTR with a huge binding table) or including e.bitsPerPacket to account for small packet workloads being more expensive than large packet ones.\nWrapup\nJust now I like the dream of using statistical modeling to understand application performance. The model would then represent our actual understanding of how an application behaves in different situations. This could then be communicated to end-users in whatever is the most appropriate way e.g. a table summarizing the most important workloads, an Excel macro that calculates the performance estimate for a given software release, or ... etc.\nEnd brain dump!\n. Thanks! Merged.\n. Cool! Now we have merged the performance regression fix (#983) and we can see what Hydra reckons about this as a release candidate. Could be that the full test matrix (#976) is not available until tomorrow.\n. @eugeneia This release is good to go from my perspective. I updated the releases benchmark to include next and it shows that the regression we saw in v2016.07 is gone.\nThe full test matrix is harder to interpret... work in progress... I have some fixes to push that should have a big impact there (either for 2016.09 or 2016.08.1) by resolving most of the error cases.\n. @eugeneia @kbara Interesting workflow issue :-)\nThis PR depends on #982 but that has not landed on next yet and I am not the upstream (@kbara is).\nSo there are a few different ways that this change could land on next:\n1. I could merge this onto next. This way #982 is coming along for the ride and landing before @kbara includes it in a PR.\n2. @eugeneia could rebase this PR to remove the dependency. However, this is likely wasted work if #982 is ready to land anyway.\n3. We could make @kbara the upstream for this PR and then she can combine it with #982 on kbara-next and PR the combination to next.\nHow about we go with option 3 i.e. that @kbara becomes upstream for this PR and we humbly request a fast-track on it? (I already changed Assignee.)\n. Oh, sorry, my bad!\nI am the upstream for #982 and @kbara has already asked me to merge the dependent changes.\nSorry about the noise :).\n. @wingo Awesome!\nI am really looking forward to having all of the Snabb configuration and operational data modeled in YANG.\nThe first topic I would like to thrash out here is how to ensure reliability i.e. what classes of error are we potentially vulnerable to and which ones can we eliminate by design.\nThe risk I see is that snabb config is implementing a database, the database state is being managed imperatively by a series of set operations, and the precious master copy of the configuration only exists in an internal format that depends on the exact software version(s) that processed the commands. In this case we might have to deal with all the classic database issues... backup/restore procedures, schema evolution when upgrading and downgrading the software, recovery from bugs in application-specific encoding/decoding routines, etc.\nDo those concerns make sense? If so then is there some way that we could take some of them off the table?\nI see the nix model as a potential inspiration here. Nix has a stable master format (source code), a means of converting into an operational format (a compiler), and a cache of precompiled objects (the store). You can always recover your operational objects if they are corrupted somehow. You also know when it is safe to reuse operational objects (created from the same source and same compiler) vs when they need to be rebuilt. You don't have to worry about backing up operational objects (they are reproducible), about changes in the operational format (each software version can have a completely different one), or about trashing operational objects due to software bugs (switch to a new software version and the bad state is automatically discarded).\nShould we consider doing something similar? Might snabb config maintain both a master configuration in a stable format and an (optional) store of \"compiled\" configurations that are treated as disposable and version-specific?\nOther thought to ponder: I wonder whether snabb config could also backend other operation-and-maintenance tools e.g. snabb top and so on. This might clash with the exclusivity requirement on snabb config listen?\n. Restated more concisely: Can we and should we create these two invariants: that the master configuration always exists in a well-defined format, and that when a process starts it always uses an operational configuration that is identical what the running software would bootstrap from the master configuration file?\nThis way your configuration is never corrupted and you never have to rm -rf /var/run/snabb/foo as part of your routine troubleshooting activities.\n(Related anecdote: I have recently struggled with an error from a network card for over a week. Have tried all kinds of things to make it work including PCI resets, machine reboots, etc. Turns out that what I needed to do is remove the power from the server. Presumably there was some bad operational state in the card firmware and none of the simple reset methods genuinely return to a pristine state. This is annoying to me as a user: now every time I have a hard-to-resolve error I will need to cold-start the machine because I don't have a simpler method that I can trust. Contrast this with the beloved 82599 where I can fully reset the card to a pristine state with a single MMIO register poke.)\n. Process structure thought:\nIn this architecture we have a \"configuration server\" process that is interfacing between the dataplane (private interface) and configuration agents (public interface). The invariants we want to maintain are that there is at most one such process (exclusive access to configuration state) and the process must run exactly the same software version as the dataplane (interface between them is private).\nPerhaps this would be easier if the configuration server process were a sibling of the dataplane rather than something that is invoked separately?\n1. Dataplane fork()s the configuration server during startup.\n2. snabb config is a thin client that issues commands to the server via a unix socket.\nThis way we ensure that there is always exactly one configuration server running and that its software version is identical to the dataplane that it is controlling.\nHaving a persistent singleton configuration server may also make it easier to support multiple simultaneous clients e.g. sysrepo and snabb top both accessing YANG-modelled data via the same interface at the same time (but in non-conflicting ways).\n(Since #930 we are already forking a process to cleanup the shm objects upon shutdown. Perhaps this process could evolve to handle most of the snabb config functionality?)\n. @wingo I have implemented the \"manager process spawns CPU-locked workers\" model over on #1021 as a work in progress. Plan is to use this for supporting snabbnfv with many processes sharing a 100G Mellanox NIC.\n. @wingo (Quirk of the Mellanox driver is that it wants to run one app in the manager process to configure the NIC and then I/O apps in the worker processes to attach to tx/rx queues.)\n. One apparent difference between the model here vs on #1021 is that I imagine snabb config being a \"thin client\" i.e. a very simply and generic program that sends a request to a long-running Snabb \"manager\" process to do the real work like operating on configurations and communicate updates to workers. Dunno what ally'all think about that.\n. @eugeneia Could you please push the revert of the public packet_t as discussed in https://github.com/snabbco/snabb/pull/997#issuecomment-242765041?\nI had imagined that it was just to revert commit 0214574 but as you say there it actually requires revising the documentation introduced in #913.\n. Thanks @eugeneia. I merged this straight onto next.\n. @kbara Very interesting benchmarks results for this branch on next-tributary. Mixed but broadly positive. I have defined a new next-tributary-murren jobset to benchmark more configurations on generic hardware too.\nCan be that the effect of increasing the struct link size is related to the effect of increasing the virtio vring size with a patched QEMU in https://github.com/snabbco/snabb/issues/976#issuecomment-241393994.\nLet me just dig a bit to make sure the performance impact is an overall net positive. I'd love to understand what causes the differences. Plug: Snabb Studio prototype demo that may become a game changer here :).\n. Gee it is fun browsing the benchmark datasets that Hydra produces automagically :).\nHere is a full matrix benchmark report that includes kbara-next. This is testing many different configurations (NFV features for iperf, Virtio-net features for l2fwd) on generic hardware (Hetzner \"murren\" servers w/o hardware NIC).\nObservations:\n1. Overall summary looks good. Seems like performance is becoming better and more consistent: the kbara-next (red) line has a new hump that seems to have been formed by some scores increasing (moving right).\n2. Split by benchmark shows different effects for iperf (more zero scores i.e. test failures) and for l2fwd (more high scores at ~6.5 Mpps).\n3. iperf by configuration seems to show a speedup on both the base and l2tpv3 cases (good) but a 100% failure rate on the filter case (don't worry, innocent explanation below).\n4. l2fwd by configuration shows definite speedups on packet forwarding with the base and noind configurations (less clear on nomrg). Good stuff and closely related to the QEMU vring patch that I have been exploring (in fact I will include kbara-next in those tests to see this side-by-side and in combination with the patched QEMU too).\nSo overall this looks great. The failures with iperf when packet filtering is enabled would in principle be a blocker but I know this is because a specific fix (commit 2a98aa5) has not landed in kbara-next yet:\n$ git merge-base --is-ancestor 2a98aa5 snabbco/kbara-next ; echo $?\n1\nSo I am confident that merging this code into next will improve performance overall. Great stuff :+1: \nCouple more things I want to look at before pressing merge:\n- [ ] How come the basic1 benchmark score went down? any idea @eugeneia? This is not a deal-breaker since it is a very synthetic benchmark but could be a sign of a real problem so I don't want to take a regression there too lightly.\n- [ ] Gather my thoughts on the packet_t API change.\nNote: In a perfect world my next-tributary jobset would not be testing the tributary branches directly but rather test next with each tributary branch merged. Then the results would tell me what the consequences are of merging the branch with next and e.g. we would not see the iperf \"regression\" because the fix is already present on next and would be included in the test. However, Hydra doesn't work that way at the moment, and it's not the end of the world because we will at least find problems once they land on next i.e. before release.\n. @eugeneia Thanks for that explanation. I think it is okay for basic1 benchmark scores to decrease if the reason is that it is processing fewer packets per breath. That benchmark mostly exists to catch regressions on basic operations like packet allocation, link transmit/receive, and so on. Could even be positive that it is processing smaller batches because then it will also be more sensitive to the performance of the engine itself e.g. switching between apps, polling timers, etc.\n. Merged, thanks! And sorry about the wait.\nI am actually not sold on putting packet_t into the API and wonder if we should revert that:\n- Redundancy: We can already refer to the packet struct by the name struct packet.\n- Consistency: The name struct packet works in Lua, C, and assembly.\n- Necessity: Does this really solve an \"excessive Ctypes\" problem? I don't think that referencing an existing struct by name creates a new ctype.\nHere is a quick experiment suggesting to me that ffi.typeof() will return the same Ctype for multiple calls if you are referencing an existing definition (rather than e.g. defining an anonymous struct):\n$ sudo ./snabb snsh -i\nSnabb> ffi = require(\"ffi\")\nSnabb> a = ffi.typeof(\"struct packet\")\nSnabb> b = ffi.typeof(\"struct packet\")\nSnabb> =a\nctype<struct packet>\nSnabb> =b\nctype<struct packet>\nSnabb> =(a==b)\ntrue\nSo I wonder if there is really a problematic case that will be solved with packet_t?\n. I can do that over on next.\n. btw it is very good that you have your eyes peeled for \"ctype diversity\" problems :). This is relatively easy to fix once you know what to look for but boy did @alexandergall do a lot of work to discover this from first principles over in #612. Finding such issues in submitted PRs seems like a really valuable bit of review.\n. > Given that the changes get merged onto next, it sounds like the easy solution to that problem is to simply make sure that kbara-next (and presuambly max-next) frequently merge next into themselves?\n@kbara Interesting thought. This seems like it would work well from my perspective. I suppose we would have to try it to understand all the implications. I would be happy to pull from next-tributary branches that have merged from next.\n(Separately I still think it makes sense for new features and fixes to be based on master rather than next when possible, but that is the matter of creating new changes rather then merging and integrating them.)\n. @kbara @eugeneia There are some more implications to consider with packet.clone_to_memory() (#999). Specifically I do not think that this implementation satisfies the stated goal:\n\nThe point is to have a fully valid struct packet at the end\n\nThe thing is that packet.allocate() is able to allocate memory \"under the hood\" in such a way that it satisfies specific invariants that other code can then depend on. For example, packet.allocate() ensures that packet memory is suitable for DMA by always allocating physically contiguous memory (HugeTLB) that is pinned with mlock() (can't be swapped out / relocated / lazy-allocated). We also apply a tag scheme where the virtual address of the packet data is a tagged pointer to the physical address.\nIf we want to support a packet.clone_to_memory() that creates a genuine struct packet in user-supplied memory then we need to account for these invariants in some way.\nSome options:\n1. Document all of the invariants and require the memory supplied to clone_to_memory() to satisfy them.\n2. Define multiple classes of packets e.g. those that are known to satisfy invariants (DMA-safe, etc) vs those that are not.\n3. Eliminate the invariants e.g. use the IOMMU to make all memory suitable for packets.\nOr the default option would be stick with the status quo and say that the only way to get a struct packet is to call packet.allocate() and that when you copy the payload somewhere else that is okay but it is not a struct packet.\nSomehow resolving this issue is a blocker to merging packet.clone_to_memory().\n. I think the simplest solution would be to have two separate data structures:\n- Short lived struct packet objects used for data during the breath that it is transmitted or received.\n- Long lived memory in a custom format for data that is awaiting reassembly.\nThe custom format need not be the same as struct packet e.g. you could use scatter-gather lists if that were more suitable, etc.\n(Generally speaking it is okay for struct packet objects to be long-lived too but since each one consumes 10KB of HugeTLB memory I can understand why that would not fit this particular use case.)\n. Groovy! Merged now with that API reverted in commit 54ac86d. Thanks everybody!\n. Good eye @kbara!\nI wonder how to confirm whether this is a real effect vs a random artifact due to excessive slicing-and-dicing of the dataset i.e. Type I error (xkcd 882).\nThe first idea that comes to mind is to do Tukey's Test on the whole iperf dataset and look at the confidence interval on the difference in mean value between filter benchmark on next vs virtio-opt branches.\nHere it is with the default setting of 95% confidence interval:\nr\nlibrary(readr)\nlibrary(dplyr)\nd <- read_csv(\"https://hydra.snabb.co/build/438947/download/1/bench.csv\")\niperf <- filter(d, benchmark=='iperf')\nTukeyHSD(aov(score ~ snabb*config, iperf))\nThe relevant line from the full output:\nvring-opt:filter-next:filter             -0.65285714  -1.1857199 -0.11999441 0.0042371\nMy shaky interpretation is that on the one hand this suggests there is a difference in the mean performance of the two branches (p adj = 0.004) but on the other hand the lower bound on the difference is quite close to zero (lwr = -0.199). So the best we can say with 95% confidence is that vring-opt:filter is 0.199 Gbps slower than next:filter.\nWe could also repeat this test with 99% confidence interval to be more conservative:\nr\nTukeyHSD(aov(score ~ snabb*config, iperf), conf.level=0.99)\nvring-opt:filter-next:filter             -0.65285714  -1.2672972 -0.038417072 0.0042371\nwhich puts the lower bound on the effect at -0.038 Gbps.\nHow to interpret this? I am not really sure. My instinct is to collect more data as the first step. I already have a larger (more iterations) benchmark running (evaluation 3325) and that dataset should be ready tomorrow.\nInteresting stuff! Still a risk that I am completely misapplying and misunderstanding all of these statistical tools but I feel like this is a promising approach.\n. Thinking about this some more... there is a very important statement at the beginning of R for Data Science\n\nYou can only use an observation once to confirm a hypothesis. As soon as you use it more than once you\u2019re back to doing exploratory analysis. This means to do hypothesis confirmation you need to \"preregister\" (write out in advance) your analysis plan, and not deviate from it even when you have seen the data.\n\nWe have used our initial dataset to generate a hypothesis, that the filter configuration of the virtio-opt branch has a \"hump\" at around 12 Gbps, and now to test this we need to use new data.\nSo my idea of applying Tukey's Test to the original dataset was bad for at least two reasons. First we have already \"used up\" this dataset with our hypothesis generation. Second that Tukey's Test is comparing the mean scores of the benchmarks which does not really match our hypothesis anyway.\nSo let us now take the new data from evaluation 3325 and see if the same pattern is there. We have to be careful here because this CSV file is actually a superset of the previous one i.e. it includes the original data points. The first dataset contains 5 results for each scenario and the second dataset contains 30. (Hydra is clever enough to see that the tests with id 1-5 are identical in both runs and so it shares the results.) So in order to meet our goal of only looking at new data we will need to only consider CSV rows with id>5.\nSo without further ado let us take a look at the new data (2100 results) for the iperf filter benchmark:\nr\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nd <- read_csv(\"https://hydra.snabb.co/build/433897/download/1/bench.csv\")\nnew.iperf.filter <- filter(d, benchmark=='iperf' & config=='filter' & id>5)\nggplot(aes(x=score, y=..count.., color=snabb), data=new.iperf.filter) + geom_density()\n\nThis picture seems like grounds to reject the hypothesis that the virtio-opt branch creates a spike in values around 12.5 Gbps. It looks more like scores move up from this range towards 17.5 Gbps.\nJust for a sanity-check we can also use Tukey's Test to confirm that the average score has increased with virtio-opt:\n```\n\nTukeyHSD(aov(score ~ snabb, new.iperf.filter), conf.level=0.99)\n  Tukey multiple comparisons of means\n    99% family-wise confidence level\n\nFit: aov(formula = score ~ snabb, data = new.iperf.filter)\n$snabb\n                    diff       lwr       upr p adj\nvring-opt-next 0.6805714 0.4134409 0.9477019     0\n```\nThis tells us that we can be 99% confident that the virtio-opt branch is improving average performance by at least 0.413 Gbps.\nSo: I reckon that the spike we saw in the first dataset was due to xkcd 882 i.e. we sliced our data into many different pieces and one of them showed a pattern by random chance.\n. > The reddish color of the first table with 100% successes is really distracting\nThis should be fixed now with https://github.com/snabblab/snabblab-nixos/commit/25c18022fbf099df339d5cc0491424b8e526e8ef.\n. The next tributary report presents another interesting dataset for this code. This runs on lugano hardware (real 82599 NIC) instead of murren hardware (generic hetzner with faked NIC). Because we have fewer lugano servers it also tested fewer configurations.\nThe main things that jump out at me are:\n- l2fwd benchmark scores lower on lugano hardware - but much more consistently.\n- iperf scores are spread out and capped at ~14 Gbps (likely PCIe bottleneck with hardware NIC).\nand that we need to work out which tests we want to run on which hardware platform(s).\n. @kbara Just following up on the \"xkcd 882 problem.\" On the one hand it seems like there are straightforward ways to account for this in statistics, on the other hand I don't immediately know how to apply those to visualizations.\nThe Bonferroni correction seems cool. The idea is that if you make N comparisons then you are N times more likely to see an effect due to random chance. So to maintain your confidence you need to look for N times more significance in the tests. You can do this by multiplying your P-value by N.\nSo if we would make a single comparison of two Snabb versions then we may use P=0.05 to check for a significant difference with 95% confidence. If we split the data into 10 groups and compared them separately then the Bonferroni correction says we would need to use P=0.005 as the threshold for 95% confidence instead.\nI am sure that R routines for ANOVA, etc, are applying such corrections automatically. However, the visualizations have no such corrections and are potentially misleading. Have to think about what is a good way to communicate the statistical significant of the test results e.g. visual confidence intervals or numeric statistics.\n. @eugeneia I'm happy with this when you are :)\n. > Should the graph only show 8192 queue depth?\nThe graphs for all queue sizes are being plotted. The thing is that the queue size has very little impact and so sometimes the lines are drawn directly on top of each other. You can only see the difference in the first graph which has a \"zoomed in\" Y-axis. (I should have made a note of this in the writeup.)\n\nI would have hoped that it would do considerably better, assuming that your app does \"nothing\" else besides sending packets.\n\nBraindump:\nMy working hypothesis is that the CPU is mostly idle and waiting for the NIC to process the transmit requests. I think this Snabb code can send packets at 100+ Mpps per core (hand-tuned load generator) but the NIC can only process around 9M requests on the Send Queue per second. I suspect that the ConnectX-4 is a \"scale out\" rather than \"scale up\" NIC i.e. that it processes queues slowly but can process many queues in parallel. (That would be a bit like Xeon CPUs where the high-end ones have lower per-core clock speeds but make up for that by having more cores.)\nI want to test this theory by transmitting on many Send Queues at once. The prediction is that this will boost performance and reveal the maximum practical packet rate. I need to finish a rework of the transmit code before I can run this test.\nIf this does turn out to be the case then the situation will be a little different than we are used to. The Intel 82599 can handle line-rate on a single queue and only uses multi-queue to split up the traffic across CPU resources. This would be the opposite situation where we need to split up the traffic across NIC hardware resources. This would probably be fine for most applications but a PITA for some.\nCould alternatively be that the benchmark is CPU-bound due to a bug in my load generator (it does do a little more work than the 82599 version that is tested at 200+ Mpps per core) or that there is a setting missing somewhere (e.g. on 82599 there was an obscure default register setting that needed to be changed to achieve > 12 Mpps per queue, see #628).\nEarly days... I will keep you posted.\n. @plajjan Sorry I realize this is turning into an annoying cliff-hanger :-).\nI am running a new set of tests now that is like before but separately for 1..24 parallel send queues. I will post the full results when the tests finish. Quick happy-testing shows we hit at least 60 Mpps w/ one CPU core doing transmit. I will be curious to see where the sweet-spot combinations of SendQueues+QueueSize+PacketSize.\nThis is based on the early rough version of the TX code. I am debugging the new neat version of that in parallel.\n. Superceded by #1007.\n. I am reading an absolutely fascinating book called Statistical Modeling: A Fresh Approach. It's about how to make models (functions) that succinctly account for the variations in data sets. The notion is that formal models can be used to capture details that would be too complex to present visually.\nI am using this ConnectX-4 data set as a first modeling exercise. The idea is to make a model that accounts for how performance (Mpps) depends on other variables (focusing on SendQueues which has the biggest effect).\nHere is what the raw data looks like. (The points appear in groups because several different queue lengths are tested.)\n\nThe exercise here is to make a series of models that try to account for the Mpps values.\nConstant\nThe first naive model supposes that Mpps is always the same. This can be expressed by taking a linear model (lm) for the formula Mpps ~ 1 and \"fitting\" that with the value that best suits this data:\n``` r\n\nm1 <- lm(Mpps ~ 1, data = d)\ncoefficients(m1)\n(Intercept) \n   60.11094 \n```\n\nThis model predicts that performance will always be 60.1 Mpps. We can visualize the model as a line overlayed on the data:\n\nOn the one hand this is useful information but on the other hand it does not account for any of the variations in the measured values.\nLinear\nThe second model supposes that Mpps increases linearly i.e. following a straight line. The line is defined by the \"intercept\" (the notional Mpps with 0 send queues) and a slope (how much Mpps changes for each added send queue).\n``` r\n\nm2 <- lm(Mpps ~ SendQueues, data = d)\ncoefficients(m2)\n(Intercept)  SendQueues \n 49.6810481   0.8343916 \n```\n\n\nThis model predicts that baseline performance is 49.681 Mpps and then increases by 0.834 Mpps for each send queue.\nThis may or may not be a step in the right direction but it definitely does not fit the data very closely. The goodness of the fit can be quantified with a statistic called \"R squared\" that tells us what fraction of the variation in Mpps values is accounted for by the model. The answer here is 25%.\n``` r\n\nsummary(m2)$r.squared\n[1] 0.2506815\n```\n\nSegmented\nThe last model supposes that Mpps follows a segmented line. The effect of adding send queues changes at a certain \"break\" point.\n``` r\n\nm3 <- segmented(lm(Mpps~SendQueues-1, data=d), seg.Z=~SendQueues)\n```\n\n\nThe \"fit\" tells us that we have two lines with different slopes. Initially each send queue accounts for a 15.8 Mpps increase in performance. Later each send queue accounts for a slight decrease of 0.14 Mpps. The \"break point\" is after ~4 send queues.\n```\n\nslope(m3)\n$SendQueues\n          Est.\nslope1 15.8100\nslope2 -0.1417\nsummary(m3)\n...\nEstimated Break-Point(s):\n   Est. St.Err \n 4.153  0.021 \n...\nMultiple R-Squared: 0.9998,  Adjusted R-squared: 0.9998 \n```\n\nThis model looks much more satisfying to me. This is quantified in the R-squared statistic that says we are now accounting for 99.9% of the variation in Mpps. We could add more details to the model to make it fit the data even better, but for the purpose of this exercise I feel that we have reached the point of diminishing returns.\nSummary\nSo there we have it: our best model of ConnectX-4 performance says that the first four send queues give you 15.8 Mpps each and adding more beyond that has a slightly negative effect.\nI believe that Mellanox claim the maximum packet rate for this card is around 90 Mpps. This leads me to think that there are some other factors that we could include in our tests - descriptor formats, etc - that would produce values that don't fit this model. Future steps are to identify these factors, measure them, and update the model to account for them.\nReflections\nJust now it seems to me like statistical modeling is a promising approach to characterizing system performance. I would be very happy if network equipment like NICs would be supplied with models like this to tell me what performance to expect in different configurations. I would be even happier if the constants in these models were derived directly from reproducible benchmarks on a CI system.\nMaybe in the future Snabb applications could come with such models that tell you what performance to expect based on factors like software version, configuration, clock speed, number of cores, choice of NIC, etc.\n. @plajjan Thanks!\nThis benchmark is an attempt to establish the performance characteristics of the NIC itself. The tests are always with a single CPU core driving all of the send queues. I have used an especially efficient special-case transmit routine (\"packetblaster\") to prevent a CPU bottleneck. I have also sanity-checked this by running the test at both 3.5 GHz and 2.0 GHz and not seeing a significant difference in the results. (It would be interesting to add multiple clock speeds into the experiment and then use the model to quantify any effect this may have.)\n. I think that I should repeat this testing and modeling exercise adding a couple more factors into the tests:\n- Clock speed.\n- Packet size.\nMore?\nInformally it seems like clock speed does not have much effect (would confirm that the test is not CPU-bound) and that packet size has a surprisingly large effect (e.g. 200B packet gets only ~37 Mpps). Could be that the \"packets/sec vs bytes/packet\" curve is actually problematic in the sense of #1013. Have to model that to find out :-).\n. So latest crazy idea: Suppose that we would analyze Snabb performance by looking at individual breaths rather than whole end-to-end benchmarks.\nThen each benchmark run would produce not one metric (e.g. overall average throughput) but more like 100,000 metrics (performance of a sample of breaths). This way when Hydra runs an intense benchmark (a machine-week or so) we would have around a billion data points to analyze instead of the 10,000 or so that we have now. The analysis could be done using models like in https://github.com/snabbco/snabb/issues/1007#issuecomment-244896318.\nPotential advantages:\n- Test cases could be much more diverse e.g. using randomized configurations and workloads. Anything that produces a diverse and interesting set of breaths to analyze. This would create a satisfying mess to tease apart by modeling.\n- Data sets from production deployments could be analyzed in the same way. For example we could take a timeline file from a production deployment and see how well it fits the model we came up with from the CI tests.\n- Models may point directly to optimizations. For example we may find that performance per breath is best predicted by number of packets processed, or number of bytes processed, or number of L3 cache accesses, or other specific factors and then we may be able to tune the engine by influencing these.\nSo still a pipe dream for now but it could be very interesting to turn a timeline log into a million-row CSV file and see what R can make of it.\nIncidentally the data above ^^^ is a little interesting. This breath is from snabbnfv between two VMs doing iperf with jumbo frames. (MTU 9000). This is fun because when we are copying packets to the VMs we are probably using at least 2MB of cache per 100 packets. So we see quite a bit of activity in terms of L3 hits and even L3 misses (RAM access). On the other hand the overall performance is excellent at 20 bits of throughput per CPU cycle. So even if the engine is perhaps not optimally tuned for jumbo frames they are still a very easy workload and with ~ 100 packets per breath it seems like we could do 20 Gbps of traffic for each 1 GHz of CPU. This armchair analysis might be much more satisfying as a formal model fitted to the data though...\n. Related idea:\nWe could extend LuaJIT with a global counter exits for the total number of trace-exits taken & include this alongside the CPU performance counters. Then we could measure side-trace jumps in much the same way as cache misses or branch mispredictions. This could make it possible to account for the performance of a breath in terms of how well it stayed \"on trace.\"\n. Relatedly: Nathan Owens pointed out to me via Twitter that the sexy Broadcom Tomahawk 32x100G switches only do line-rate with >= 250B packets. Seems to be confirmed on ipspace.net.\n. Here are some public performance numbers from Mellanox: https://www.mellanox.com/blog/2016/06/performance-beyond-numbers-stephen-curry-style-server-io/\nThe headlines there are line-rate 64B with 25G NIC and 74.4 Mpps max on 100G. (I am told they have squeezed a bit more than this on the 100G but I haven't found a published account of that.)\nNote that there are two different ASICs: \"ConnectX-4\" (100G) and \"ConnectX-4 Lx\" (10G/25G/40G/50G). If you needed more silicon horsepower per 100G, for example to do line-rate with 64B packets, maybe combining 4x25G NICs would be a viable option? (Is that likely to cause interop issues with 100G ports on switches/routers in practice?)\n. I tested the ConnectX-4 with every packet size 60..1500 and at both 3.5 GHz and 2.0 GHz.\n\nWhaddayareckon?\n. Good question. It looks like the size of each packet is effectively being rounded up to a multiple of 64. I wonder what would cause this?\nSuspects to eliminate:\n- Software.\n- DMA.\n- Ethernet MAC/PHY.\n. ### DMA/PCIe\nI would really like to extend our PMU support to also track \"uncore\" counters like PCIe/RAM/NUMA activity. This way we could include all of those values in the data sets.\nMeanwhile I created a little table by hand. This shows the PCIe activity on both sides of the first four distinct plateaus.\n```\nMpps  PacketSize   PCIeRdCur (M)  DRd (M)  PCIeRead (GB)  PCIeWrite (GB)\n37    190          1323           355      107            0.082\n37    250          1656           356      128            0.082\n30    260          1592           277      120            0.066\n30    316          1591           284      120            0.066\n25    320          1311           232       99            0.054\n25    380          1529           232      112            0.054\n21    384          1313           199       97            0.046\n21    440          1512           204      110            0.046\n```\nThis is based on me snipping bits from the output of the Intel Performance Counter Monitor tool. I found some discussion of its output here.\nHere is a very preliminary idea of how I am interpreting these columns:\n- Mpps: Approximate packet rate of the plateau.\n- PacketSize: Bytes per packet (+CRC).\n- PCIeRdCur (M): Millions of 64B cache lines fetched from memory via PCIe.\n- DRd (M): Millions of 64B cache lines fetched from L3 cache via DDIO.\n- PCIeRead (GB) and PCIeWrite (GB): Total data read by NIC / written by NIC over PCIe. (Docs seem to say that this is Gigabytes but the numbers only makes sense to me as Gigabits.)\nHow to interpret this? In principle it seems tempting to blame the \"64B-wide plateau\" issue on DMA if it is fetching data in 64B cache lines. Trouble is that then I would expect to see the same level of PCIe traffic for both sides of the plateau -- and probably with PCIe bandwidth maxxed out at 128Gbps (PCIe 3.0 x16 slot). However, in most cases it seems like PCIe bandwidth is not maxxed out and the right-hand side of the plateau is transferring more data.\nSo: no smoking gun from looking at PCIe performance counters.\nEthernet MAC/PHY\nI have never really looked closely at the internals of Layer-2 and Layer-1 on Ethernet. Just a few observations from reading wikipedia though:\n- 100GbE uses 64b/66b encoding.\n- 100GbE uses four physical channels.\nSo as a wild guess it seems possible that 100GbE would show some effects at 32-byte granularity (64 bit * 4 channel) based on the physical transport. However, this would only be 1/2 of the plateau size, and I don't know whether this 64-bit/4-channel grouping is visible in the MAC layer or just an internal detail of the physical layer.\nI am running a test on a 10G ConnectX-4 NIC now just out of curiosity. If this showed plateaus with 1/4 the width then it may be reasonable to wonder if the issue is PHY related (10GbE also uses 64b/66b but via only one channel).\n\n. @fmadio Yes, this sounds like the most promising line of inquiry now: Can we explain the performance here, including the plateau every 64B, in terms of the way the memory subsystem is serving DMA requests. And if the memory subsystem is the bottleneck then can we improve its performance e.g. by serving more requests from L3 cache rather than DRAM.\nTime for me to read the Intel Uncore Performance Monitoring Reference Manual...\n. @fmadio Great thoughts, keep 'em coming :).\nI added a couple of modeled lines based on your suggestions:\n- Max 100GbE showing the theoretical maximum packet rate, based on the notion that the NIC will always transmit at 100Gbps & the MAC will add 24 bytes of per-packet overhead (CRC + Preamble + Gap).\n- Max PCIe/MLX4 showing the expected PCIe bandwidth limit, based on the notion that PCIe is transferring cache lines at 112Gbps and ConnectX-4 has one cache line per packet of overhead (64B transmit descriptor).\nHere is how it looks (click to zoom):\n\nThis looks in line with the theory of a memory/uncore bottleneck:\n- Cache line granularity explains the width and alignments of the plateaus.\n- Performance curve would be smoothed where it reaches line rate, but it never does.\n- Cache lines are not being delivered to the NIC fast enough to keep the transmitter busy.\nOne more interesting perspective is to change the Y-axis from Mpps to % of line rate:\n\nLooks to me like:\n- We are delivering ~80 Gbps of packet-data cache lines to the NIC.\n- If the packet size is a multiple of 64B then all the transferred data can be sent onto the wire, but otherwise part of the last cache line is not used and throughput drops.\n- There are a couple of sweet-spots around 256B where throughput reaches 85% of line rate. Perhaps more of the cache lines were served from L3 cache vs RAM here.\nSo the next step is to work out how to keep the PCIe pipe full with cache lines and break the 80G bottleneck.\n. This is an absolutely fascinating problem. Can't put it down :).\n@fmadio Great info! So on the receive path the NIC uses \"posted\" (fire and forget) PCIe operations to write packet data to memory but on the transmit path it uses \"non-posted\" (request/reply) operations to read packet data from memory. So the receive path is like UDP but the transmit path is more like TCP where performance can be constrained by protocol issues (analogous to window size, etc).\nI am particularly intrigued by the idea of \"running out of PCIe tags.\" If I understand correctly the number of PCIe tags determines the maximum number of parallel requests. I found one PCIe primer saying that the typical number of PCIe tags is 32 (but can be extended up to 2048). \nNow I am thinking about bandwidth delay products. If we know how much PCIe bandwidth we have (~220M 64B cache-lines per second for 112Gbps) and we know how many requests we can make in parallel (32 cache lines) then we can calculate the point at which latency will impact PCIe throughput:\ndelay  =  parallel / bandwidth  =  32 / 220M per sec  =  146 nanoseconds\nSo the maximum (average) latency we could tolerate for PCIe-rate would be 146 nanoseconds per cache line under these assumptions.\nCould this be the truth? (Perhaps with slightly tweaked constants?) Is there a way to check without a hardware PCIe sniffer?\nI made a related visualization. This shows nanoseconds per packet (Y-axis) based on payload-only packet size in cache lines (X-axis). The black line is the actual measurements (same data set as before). The blue line is a linear model that seems to fit the data very well.\n\nThe slope of the line says that each extra cache line of data costs an extra 6.6 nanoseconds. If we assumed that 32 reads are being made in parallel then the actual latency would be 211 nanoseconds. Comparing this with the calculated limit of 146 nanoseconds for PCIe line rate we would expect to achieve around 70% of PCIe line rate.\nThis is a fairly elaborate model but it seems worth investigating because the numbers all seem to align fairly well to me. If this were the case then it would have major implications i.e. that the reason for all this fussing about L3 cache and DDIO is due to under-dimensioned PCIe protocol resources on the NIC creating artificially tight latency bounds on the CPU.\n(Relatedly: The Intel FM10K uses two PCIe x8 slots (\"bifurcation\") instead of one PCIe x16 slot. This seemed awkward to me initially but now I wonder if it was sound engineering to provision additional PCIe protocol/silicon resources that are needed to achieve 100G line rate in practice? This would put things into a different light.)\n. No and maybe, in that order ;-). This is transmit-only (packetblaster) and this root cause is not confirmed yet, just idea de jour.\nSome more details of the setup over in #1007.\n. The NIC probably has to use posted requests here - read request needs a reply to get the data - but maybe it needs to make twice as many requests in parallel to achieve robust performance.\n. @wingo A direct analogy here is if your computer couldn't take advantage of your fast internet connection because it had an old operating system that advertises a small TCP window. Then you would only reach the advertised speed if latency is low e.g. downloading from a mirror at your ISP. Over longer distances there would not be enough packets in flight to keep the pipe full.\nAnyway, just a theory, fun if it were true...\n. @fmadio Thanks for the info! I am struck that \"networks are networks\" and all these PCIe knobs seem to have direct analogies in TCP. \"Extended tags\" is window scaling, \"credits\" is advertised window, bandwidth*delay=parallel constraint is the same. Sensible defaults change over time too e.g. you probably don't want to use Windows XP default TCP settings for a 1 Gbps home internet connection. (Sorry, I am always overdoing analogies.)\nSo based on the info from @fmadio it sounds like my theory from the weekend may not actually fit the constants but let's go down the PCIe rabbit hole and find out anyway.\nI have poked around the PCIe specification and found a bunch of tunables but no performance breakthrough yet.\nTurns out that lspci can tell us a lot about how the device is setup:\n```\nlspci -s 03:00.0 -vvvv\n...\nDevCap: MaxPayload 512 bytes, PhantFunc 0, Latency L0s unlimited, L1 unlimited\n        ExtTag+ AttnBtn- AttnInd- PwrInd- RBE+ FLReset+ SlotPowerLimit 25.000W\nDevCtl: Report errors: Correctable- Non-Fatal- Fatal- Unsupported-\n        RlxdOrd- ExtTag- PhantFunc- AuxPwr- NoSnoop+ FLReset-\n        MaxPayload 256 bytes, MaxReadReq 512 bytes\n...\n```\nObservations:\n- ExtTag (PCIe extended tags) is supported by the device in DevCap.\n- ExtTag is however disabled in DevCtl.\n- More interesting-looking tunables exist in DevCap: MaxPayload and MaxReadReq.\nI have tried a few different settings (e.g. ExtTag+ and MaxPayload=512 and MaxReadReq=4096) but I have not observed any impact on throughput.\nI would like to check if we are running out of \"credits\" and that is limiting parallelism. I suppose this depends on how much buffer space the processor uncore is making available to the device. Guess the first place to look is the CPU datasheet.\nI suppose that it would be handy to have a PCIe sniffer at this point. Then we could simply observe the number of parallel requests that are in flight. I wonder if there is open source Verilog/VHDL code for a PCIe sniffer? I could easily be tempted to buy a generic FPGA for this kind of activity but a single-purpose PCIe sniffer seems like overkill. Anyway - I reckon we will be able to extract enough information from the uncore performance counters in practice.\nBTW lspci continues with more parameters that may also include something relevant:\nDevSta: CorrErr+ UncorrErr- FatalErr- UnsuppReq+ AuxPwr- TransPend-\nLnkCap: Port #0, Speed 8GT/s, Width x16, ASPM not supported, Exit Latency L0s unlimited, L1 unlimited\n        ClockPM- Surprise- LLActRep- BwNot- ASPMOptComp+\nLnkCtl: ASPM Disabled; RCB 64 bytes Disabled- CommClk+\n        ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-\nLnkSta: Speed 8GT/s, Width x16, TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt-\nDevCap2: Completion Timeout: Range ABCD, TimeoutDis+, LTR-, OBFF Not Supported\nDevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis-, LTR-, OBFF Disabled\nLnkCtl2: Target Link Speed: 8GT/s, EnterCompliance- SpeedDis-\n         Transmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS-\n         Compliance De-emphasis: -6dB\nLnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete+, EqualizationPhase1+\n         EqualizationPhase2+, EqualizationPhase3+, LinkEqualizationRequest-\n.  Just a note about the server that I am using for testing here (lugano-3.snabb.co):\n- CPU: E3-1650v3 (6 cores @ 3.5 GHz, 15MB L3 cache)\n- NIC: 2 x ConnectX4 100GbE (each in a separate PCIe 3.0 x16 slot)\n- RAM: 4 x 8GB DDR4 (2133 MHz)\nCould be that we could learn interesting things by testing both 100G ports in parallel. Just briefly I tested with 60B and 1500B packets. In both cases the traffic is split around 50/50 between ports. On 1500B I see aggregate 10.75 Mpps (well above single-port rate of ~6.3 Mpps) and on 60B I see aggregate 76.2 Mpps (only modestly above the single-port rate of 68 Mpps).\n. @fmadio ucevent has a mouth-watering list of events and metrics. I am working out which ones are actually supported on my processor and to untangle CPU/QPI/PCIe ambiguities. Do any happen to catch your eye? This area is obscure enough that googling doesn't yield much :-).\n. Coming full circle here for a moment, the actionable thing is that I want to decide how to write the general purpose transmit/receive routines for the driver:\n1. Optimize for CPU-efficiency.\n2. Optimize for PCIe-efficiency (w/ extra work for the CPU).\n3. Something else e.g. complicated heuristics, knobs, etc.\nThe default choice seems to be (1). However this may not really provide more than ~70G of dependable bandwidth. It would be nice to have more than this with a 100G NIC.\nIf the source of this limit could be clearly identified then it may be reasonable to work around it in software e.g. with extra memory access to ensure that the TX path is always served from L3 cache. However, without a clear picture this could easily do more harm that good, e.g. by taking cache resources away from the receive path that I have not benchmarked yet.\nMellanox's own code seems to be more along the lines of (3) but the motivations are not completely transparent to me. I am told that they can achieve up to 84 Mpps with 64B packets but I am not sure what this means e.g. if performance drops steeply when switching from 64B to 65B packets. (The utility of 64B packet benchmarks is when they show \"worst case scenario\" performance but in my tests so far this seems more like an unrepresentatively easy workload for the NIC which may be limited by I/O transfers rather than per-packet processing.)\n. Just another low-level detail that I noticed in passing but don't want to dig into right now:\nThe Xeon E5 data sheet (volume 1, volume 2) includes register definitions for extremely detailed settings like allocation of PCIe credits. Example:\n\nI believe that these settings are traditionally only tweaked by the BIOS but they seem to be accessible to Snabb. The Xeon Uncore presents its configuration options as virtual PCIe devices. The example above says \"bus 0, device 5, function 0\" which is PCI address 00:05.0:\n$ lspci -s 00:05.0\n00:05.0 System peripheral: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 Address Map, VTd_Misc, System Management (rev 02)\nand those registers can be accessed via sysfs the same way that we access the NIC. For example to read that PCIe egress credits register using a generic command-line tool:\n$ sudo setpci -s 00:05.0 840.l\n8c415b41\nCould come in handy one day.\n. I have done some experimenting with uncore counters and L3 cache behaviors. The counters seem to be working, I can measure the average access times on the memory subsystem (CBo Table of Requests) and see impact on these with driver changes, but I have not found any direct link to performance. However...\nHere is a \"that's funny...\" issue that I neglected to pause and consider earlier:\nThese \"plateaus\" of ours have been explained as cache-lines, which has seemed natural because the width matches and this is the unit of granularity of the memory subsystem that is feeding PCIe. However, if we look more closely we can see that the alignment of the plateaus is off by exactly 4 bytes. The drops are not happening at 64, 128, 192 but rather at 60, 120, 188.\nHere is a zoomed in view of a small segment to illustrate:\n\nWe might have expected the plateaus to end at 512 and 576 but in fact they are at 508 and 572. This pattern is consistent throughout the test results.\nHow to explain this?\nIs there something on the PCIe messaging layer that would account for the 60-byte alignment?\nIf not then could it point to an issue on the NIC? One of the next processing steps after DMA will be to calculate and append a 4-byte CRC to the packet so a packet that is 60B for the CPU and PCIe will at some point be 64B in the NIC.\nWhaddayasreckon?\n. One more alignment issue that I neglected to mention earlier because it is complex :).\nI have been counting the ConnectX-4 transmit descriptor as 1 cache line of overhead. However the situation is a little more complex than this:\n- Transmit descriptor is 64B.\n- Transmit descriptor includes 16B of \"inline\" payload (copied by the driver).\n- DMA pointer starts at packet.data+16 to skip the bytes already in the descriptor.\nSo what overhead does this translate to on the PCIe layer?\n- Answer 1: 48 bytes.The descriptor is 64B but we skip DMA'ing the first 16B of the payload.\n- Answer 2: 64 bytes. The packet.data address is aligned to a cache line, and PCIe is transferring whole aligned cache lines, so the first 16 bytes are transferred both in the descriptor and the payload (as padding).\nwhich makes most sense? or something else?\n. I am running some new tests to see how the edge of a plateau is affected by each DMA alignment 0..63.\nInteresting idea that the NIC could be fetching 4B extra. That would explain the alignment of the plateaus. This would be very rude though :-) because the NIC would have to fetch data beyond the address boundaries that I am supplying.\nPacketSize in the graph is always excluding FCS. The documented behavior is that the FCS is always added by the device after DMA, but there may be other undocumented options too. (On RX there is the option to strip or keep the FCS.)\nThe way I am using descriptors is like you say except that the first 16B is only inline in the descriptor and not included in the separate payload data. (In memory it is all in one place and so I bump the payload pointer in the descriptor to skip the bytes that are already inline.)\nThe ConnectX-4 descriptor format is actually made up of different kinds of segments:\nc\nstruct control     { ... }; // 16B\nstruct ethernet    { ... }; // 32B incl. some inline payload\nstruct data_gather { ... }; // 16B length+pointer to a payload fragment\nstruct data_inline { ... }; // variable size payload in the descriptor ring\nThen for each packet you have one control segment, then one ethernet segment, then one or more gather and/or inline segments.\nSnabb style is keep-it-simple so the payload is always delivered with a single data gather segment. This makes each descriptor exactly 64B and puts the first 16B of payload in the descriptor. (Inlining the ethernet header seems to be mandatory.)\nRelatedly...\nI am looking at Mellanox's latest descriptor-managing code for reference. They now have a new implementation in DPDK in addition to their OFED and Linux kernel ones. Here are a few notes:\nQuite a bit of code: more than 1000 LOC. This bugs me since the descriptor rings are basically just lists of pointer+length buffer locations. But that is me.\nThey have four separate implementations of the transmit path: mlx5_tx_burst, mlx5_tx_burst_inline, mlx5_tx_burst_mpw, mlx5_tx_burst_mpw_inline. So basically they have two optional features, inline and mpw, and they have written separate transmit routines for each option combination.\nThe default for 100G is mlx5_tx_burst and for lower speeds (\"Lx\" ASIC) is mlx5_tx_burst_mpw. Seems the mpw feature is not available on 100G.\nThe inline mode can be selected by user configuration. If enabled this means the driver will copy the entire packet into the descriptor area. Their recommendation is to do this if you are more worried about PCIe back-pressure than CPU capacity. (Guessing this only makes sense for really simple programs that don't need their CPU and L3 resources for other things e.g. packet forwarding benchmarks.)\nThe mpw mode is interesting. This hardware feature is completely undocumented as far as I can tell. From reading the code it looks like a feature for compressing transmit descriptors by packing them together in groups of 5. This could be nice since the descriptors are quite bloated. On the other hand I am concerned about line 1014. If I am interpreting this correctly then this optimization only works for consecutive packets that are exactly the same size. Is that the correct interpretation? If so then this would run the risk of performing differently in benchmarks than in real life. May be worth reporting as a bug in that case.\n. @fmadio btw thanks for the link to the Intel forum about people seeing strange PCIe performance with a CPU much like mine (low-core-count Haswell). It would be nice to rule out an issue like this but I don't immediately have another server with 100G to compare. For now I have taken the basic precaution of installing the latest CPU microcode release (no observable change).\nI do have different servers with 10G/40G cards that I will test, but in due course...\n. Wow. Mellanox really do have special-case optimizations for when consecutive packets are the same size. This kind of optimization would seem to reduce the utility of simple benchmarks based on constant-size packets for predicting real-world performance.\nThis specific case only applies to the Lx ASIC (non-100G) and when using the DPDK driver. However, if vendors are putting this kind of benchmark-targeted optimization into their silicon then perhaps it is not useful to test with fixed-size packets and we need to use e.g. IMIX workloads as @mwiget always does.\n. I am running a new benchmark that slightly varies the packet sizes. Instead of 100,100,100,100,... it sends 100,99,100,101,... so the average is the same but the value is not perfectly predictable. Expectation is that this will not affect performance but worth checking.\nI have the results for different alignments now. This is a 92K row CSV to separately test every packet size and every alignment :). The results are actually quite satisfying.\nHere is a broad overview of all results with each alignment being plotted in a different color. The alignment 0..63 is the offset of the first byte of packet data from the beginning of a cache line.\n\nHere we can see that alignment does have some effect at smaller packet sizes (below ~320) but no effect at higher sizes.\nHere is the happy news: it turns out that the confusing \"up and down\" results we have seen in the past can be explained by data alignment. Here we compare performance with 0B alignment (as in the original tests) compared with 48B alignment:\n\nThe red line (0B alignment) is bouncing up and down confusingly while the blue line (48B alignment) is following an orderly stepwise pattern.\nThere is also a reasonable explanation for why 48B alignment works better. The first 16B of the packet is being inlined into the descriptor and the DMA for the rest is starting at address+16. So if we allocate our packets directly on a cache line then the DMA will be 16B aligned. But if we allocate our packets at a 48B offset then the DMA will be perfectly aligned with the start of the second cache line.\nFood for thought. It seems like we may want to consider allocating our packets on a different alignment when we are working with ConnectX-4 NICs. This would also have implications for work like igalia/snabb#407 where the alignment of a packet can be moved during processing (e.g. when repositioning the start of a packet to efficiently add/remove a header.) If this seems like too much trouble then we could reconsider the possibility of using \"inline descriptor\" mode i.e. copying all packet data into the descriptor ring which should maximize I/O performance at the expense of CPU.\n. I am thinking seriously about indirect descriptors. The cost would be a full packet copy on both the transmit and receive paths. The benefit would be increased PCIe efficiency.\nThis would seem to be in line with the end to end principle in that it would isolate all the complex optimizations, e.g. weird DMA alignment requirements, locally inside the driver. This driver code could be optimized aggressively e.g. written in assembler if necessary.\nThe reason I suggest this possibility is that the performance we are seeing is really meh. Just now it looks like we can only depend on around 70% utilization of the link (see graph above). This is already low and likely to drop further when we test with full-duplex. So maybe it is time for desperate measures unless we can find an alternative boost.\n. Just another thought...\nPCIe efficiency squeeze could be the new reality in the 25G/50G/100G era. 10G NICs usually have 16G of PCIe bandwidth (x2) while 100G NICs have 128G bandwidth (x16). So the ratio of ethernet bandwidth to PCIe bandwidth has dropped from 1.6x to 1.28x. This is unlikely to improve e.g. PCIe 4.0 will give us 256G bandwidth but we will want to use that for 200G (2x100G) NICs.\nOr alternatively the world may decide that this PCIe bandwidth margin is too tight and that future NICs should have twice as many lanes for the same number of ethernet ports.\nI wonder which way that wind will blow?\n. ... Just arguing the other way again, thinking out aloud ...\nReasons to resist the inline descriptor mode:\n- The efficiency is not necessarily much better in practice. In principle you only need about 2 bytes of overhead (16-bit length followed by payload, repeat) but in practice on ConnectX-4 there is about 40B of metadata and up to 63 bytes of padding (64B alignment). So the amount of data being transferred over PCIe would be much the same. The data would be laid out linearly which may or may not help.\n- Could be that ~70% of line rate is acceptable when we are looking at the first NIC on the market.\n- Could be that optimization efforts are better spent on other cards e.g. the ConnectX-5 that seems to have been released now (?) that promises better performance and may support new DMA styles.\n@plajjan How about the alternative, as @sleinen alluded to earlier, of having 2x100G per processor and expecting to do ~ 70% utilization of each? Then you would have 280G of bandwidth per dual-socket server. However you would need to use a switch to spread load across your ports rather than plumbing them directly into a $megabucks 100G router port. (Do high-end routers offer compatible 100G ports yet, anyway?)\n. Just to put the DMA alignment issue into context, here is the difference when the Y-axis is the % of line rate achieved:\n\nSo it is a definite improvement but the impact is only really seen with small packet sizes where performance is not practical in either case. Looks like you have to hit around 320B per packet before you can depend on ~70% of line rate.\n. @plajjan Continuing to think out aloud...\nReality check: Each CPU has 320G of PCIe bandwidth (40 lanes @ 8Gbps). PCIe bandwidth is the least scarce resource at the hardware level. On a fundamental level it is crazy that we are talking about burning CPU resources (always precious) to conserve PCIe bandwidth when most of our PCIe lanes are not even connected.\nSo this is actually a sign of a problem somewhere else e.g. limitations in the NIC silicon or that PCIe 3.0 x16 is not a suitable choice for 100GbE. In the latter case the solution could be PCIe 4.0 or e.g. a special riser card that allows the card to consume two x16 slots.\n. So! I want to dig deeper into the potential hardware bottlenecks and learn more about what we can see with the CPU uncore performance counters. During this investigation I want to answer a few specific questions:\n1. Why exactly did my first benchmark (#1006) score so much lower than my second one (#1007)? The first benchmark allocates only one packet buffer to reference from all descriptors, while the second has a separate packet for each descriptor. There is probably a crisp and satisfying explanation for why this performs poorly. How can we use the available tools to identify this?\n2. Is the 1500B packet performance we are seeing now right? (How come we are not seeing > 80% of line rate on this easy workload?)\n3. How come when I test 2x100G on the same socket I don't see double performance? (Is there some shared resource, e.g. in the CPU uncore, that is overloaded?) Informal tests suggest that 64B packets don't achieve very much higher aggregate throughput on 2x100G while 1500B packets do score higher but not double.\nLet's take a look at where the potential bottlenecks could be. Here is the architecture of a Haswell CPU like @fmadio showed above. (This is the simple case for < 8 CPU cores. See also more complete picture.)\n\nThere seem to be a lot of potential bottlenecks here:\n- A CPU core could be overloaded. (I don't think so in this case.)\n- A CBox could be overloaded. This is the silicon responsible for serving requests to one slice of L3 cache. (The L3 cache is divided into slices, one slice per core, and memory is allocated to a slice not because of which core is accessing it but rather with a hash on the physical address i.e. sharding.)\n- The IIO responsible for the PCIe protocol could be overloaded e.g. too many requests, not enough buffers, etc. (Like a TCP/IP stack.)\n- The PCIe lanes could be overloaded i.e. all bandwidth consumed at the electrical level.\n- The iMC could be overloaded i.e. saturated capacity for memory access.\n- The ring that connects the L3 cache with the cores and the PCIe controller could be saturated.\n- The Home Agent responsible for cache coherence (MESIF protocol) could be overloaded.\n- (QPI is not a suspect in this setup because I am testing on a uniprocessor.)\nThe problem could also be an interaction between multiple components. For example the buffers provisioned in the IIO may only be sufficient for PCIe line rate below certain latency bounds on memory requests and those bounds may be exceeded under load.\nSo, it is one great big playground :).\nUncore\nHere is a little glimpse of some things that we can see in the uncore. Have to beware here: exotic performance counters often have caveats and I have not read the errata for this processor carefully. So this is only for ideas and not firm conclusions at the moment.\nPCIe read requests being made to each CBox (L3 cache slice controller):\n$ sudo ./ucevent.py --no-sum CBO.PCI_READS \nS0-CBO0.PCI_READS\n|             S0-CBO1.PCI_READS\n|             |             S0-CBO2.PCI_READS\n|             |             |             S0-CBO3.PCI_READS\n|             |             |             |             S0-CBO4.PCI_READS\n|             |             |             |             |             S0-CBO5.PCI_READS\n19,970,156.00 19,146,832.00 19,029,058.00 19,853,492.00 21,848,239.00 20,439,529.00 \n19,989,468.00 19,164,322.00 19,046,941.00 19,872,830.00 21,866,520.00 20,459,297.00 \n19,998,693.00 19,173,641.00 19,056,931.00 19,881,079.00 21,877,747.00 20,469,292.00 \n19,989,922.00 19,166,744.00 19,047,937.00 19,872,449.00 21,868,851.00 20,461,004.00 \n19,988,035.00 19,164,469.00 19,045,938.00 19,871,136.00 21,865,517.00 20,459,044.00\nLooks fairly evenly spread.\nAverage latency of requests processed on each CBox (measured in uncore cycles and I am not sure at exactly what frequency it is running):\n$ sudo python ./ucevent.py --no-sum CBO.AVG_TOR_DRD_LATENCY\nS0-CBO0.AVG_TOR_DRD_LATENCY\n|     S0-CBO1.AVG_TOR_DRD_LATENCY\n|     |     S0-CBO2.AVG_TOR_DRD_LATENCY\n|     |     |     S0-CBO3.AVG_TOR_DRD_LATENCY\n|     |     |     |     S0-CBO4.AVG_TOR_DRD_LATENCY\n|     |     |     |     |     S0-CBO5.AVG_TOR_DRD_LATENCY\n31.44 30.75 20.24 31.49 31.45 30.63 \n31.10 30.57 19.47 31.10 30.96 30.22 \n30.51 29.95 18.59 30.47 30.37 29.63 \n30.52 29.94 18.59 30.47 30.38 29.63 \n30.51 29.92 18.57 30.47 30.36 29.61\nThis looks fishy. CBox 2 is showing consistently lower latency than the others. Could be that some part of the workload is not spread across CBoxes evenly?\nHow about RAM reads and writes:\n$ sudo python ./ucevent.py --scale GB --no-sum iMC.MEM_BW_READS iMC.MEM_BW_WRITES \nS0-iMC0.MEM_BW_READS\n|     S0-iMC1.MEM_BW_READS\n|     |     S0-iMC2.MEM_BW_READS\n|     |     |     S0-iMC3.MEM_BW_READS\n|     |     |     |     S0-iMC4.MEM_BW_READS\n|     |     |     |     |     S0-iMC0.MEM_BW_WRITES\n|     |     |     |     |     |     S0-iMC1.MEM_BW_WRITES\n|     |     |     |     |     |     |     S0-iMC2.MEM_BW_WRITES\n|     |     |     |     |     |     |     |     S0-iMC3.MEM_BW_WRITES\n|     |     |     |     |     |     |     |     |     S0-iMC4.MEM_BW_WRITES\n0.87  1.00  0.79  0.96  0.00  0.01  0.01  0.01  0.01  0.00  \n0.87  0.99  0.79  0.96  0.00  0.01  0.01  0.01  0.01  0.00  \n0.87  0.99  0.79  0.96  0.00  0.01  0.01  0.01  0.01  0.00  \n0.87  0.99  0.79  0.96  0.00  0.01  0.01  0.01  0.01  0.00  \n0.87  0.99  0.79  0.96  0.00  0.01  0.01  0.01  0.01  0.00\nLooks like each DRAM channel is serving up nearly 1GB per second. Low rate of writes. (I am not sure why it lists five DRAM channels, 0..4, with the last being idle. There are only four DIMMs and I didn't think the uncore supported more than this.)\nSo... lots of metrics available. Just have to formulate some hypotheses, make predictions, and generate data to test them.\n. @plajjan Yes. This is a complicated issue with many different trade-offs. Here is how it looks to me this morning:\nIf we truly are bottlenecked on PCIe then this is a hardware problem. There must be a hardware engineer somewhere who has this as their top priority. Likely the solution is already implemented and on its way to market. Soon we will rerun the tests on a new server and the bottleneck will be gone. Solution could come in Broadwell/Skylake CPU, or ConnectX-5 NIC, or PCIe Gen4 upgrade, etc.\nOn the software side we should aim to use the best hardware interfaces available. This is an art. On ConnectX-4 none of the interfaces are really ideal, always using huge 64B descriptors for example, and we may be better off keeping it simple and expecting something more tuned in future NICs. Or depending on time-to-market vs time-to-deployment it may make sense to make more short-term optimizations e.g. burning CPU to conserve PCIe depending on how effective that will really be.\n. @plajjan Pardon me repeating your ideas in my own words. Just my way of internalizing them :).\n\nwhich NIC is next in line?\n\nGood question. I have no fixed agenda. Braindump:\nMellanox ConnectX-5. This seems to be available on the street as engineering samples for the same price as ConnectX-4. This should be straightforward to support because our existing driver should be compatible AFAIK. Seems to boast 30% higher packet rate which would be lovely. (I was not thinking about this NIC until @virtuallynathan mentioned it.)\nIntel FM10K. Datasheet has been released but is not complete enough to write a stand-alone driver. DPDK code quietly uses an Intel-proprietary interface to a software component that is not available as open source. Intel are not shipping FM10K NICs. I am not sure how OEMs like Supermicro and Silicom are handling the software side when they sell FM10K-based NICs (tell me if you know). Can be that I am out of date, last looked closely around December 2015.\nQlogic (Broadcom). Could potentially be an excellent NIC. Have to get the driver interface documentation published.\nMore?\n. @fmadio Okay. On the one hand the CPU is unlikely to be limiting PCIe performance. On the other hand I don't think the problem is completely local to the NIC because then if we ran two NICs in parallel they should each perform the same as a single NIC. In practice when increasing from 1x100G to 2x100G I see ~20% aggregate speedup on small packets and ~75% speedup on large packets. So the problem does seem to have a systemic aspect or else I would see a 100% speedup with two NICs.\nThe best theory I see now is:\n- Symptom is low throughput on 100GbE.\n- Cause is that the amount of DMA data \"in flight\" is less than bandwidth*delay of the link between NIC and memory.\n- Reason is that the NIC does not pipeline enough parallel DMA requests (analogous to small TCP window size).\nThen when testing 2 x 100G:\n- Throughput increases because two NICs means double bandwidth and double pipelined DMA request rate.\n- Speedup is less than double because heavier cache/memory workload increases delay (and therefore bandwidth*delay which requires more per-NIC window size for the same throughput).\nOn the one hand this would seem to fit. On the other hand I would still like to rule out any bottleneck on the host e.g. DDR4 memory. I have noticed heavy memory traffic even when the whole workload should reasonable fit into L3 cache. Could be that this is where the problem is instead?\nThis also makes me wonder about the role of L3 cache here. Traditionally I think of the L3 cache as existing to prevent the CPU from stalling. The value of DDIO and direct DMA to the L3 cache is so that the CPU can quickly load packet data from L3 cache instead of waiting on RAM. However, should we also be depending on L3 cache for other purposes e.g. to minimize DMA latency on the transmit path?\nRelatedly:\nI read a fantastic blog about the Intel Ivy Bridge Cache Replacement Policy. Turns out that recent Intel CPUs have moved away from simple Least-Recently-Used eviction of data from L3 cache and now try to detect multiple-use data (e.g. application data structures) vs single-use data (like packets). The cache will try to keep the multiple-use cache lines and flush out the single-use ones. Seems tricky to make very specific assumptions about how this L3 cache will behave.\nI also read a nice whitepaper from Xilinx Understanding Performance of PCI Express Systems. Check out Figure 9 that shows low performance on non-posted reads. They emphasise the importance of pipelining in \"Summary of Example 3\" though in a hypothetical context (and talking about how the memory subsystem pipelines rather than how the PCIe device does).\n. > Youre also welcome to try our fpga 100G NIC`s for snabb\nCould be interesting in the future. I would be curious to know about them if you card to say a few words.\nJust right now we need a basic high-quality COTS 100G NIC. Cheap, easy to buy, openly documented, high performance, with just basic features e.g. RSS and steering on L2 header. We are avoiding proprietary features, protocol-specific offloads, vendor software dependencies, NDAs, etc. ConnectX family seems excellent if we can make them perform.\nIn the future we may have special requirements that we can't satisfy with commodity NICs. Then more exotic options could be interesting. The FPGA-based cards that I have looked at in the past did not seem to really have compelling advantages over CPU+ASIC but of course this depends on the application and how cleverly the card can be programmed.\n. @fmadio Hey I just found your website at fmad.io for the first time. Cool! Your blog is awesome :).\nJust spit-balling...\nI would personally be excited about an FPGA based NIC where the HDL source is on Github, anybody can fork and extend it, and the tooling dependencies are reasonable (e.g. not depending on expensive synthesis tools that have to run on Windows, etc). Could be that NetFPGA is already a solution, I am not sure.\nI wonder how this could make sense for a company like yours that is investing heavily in up-front development. Have you seen the Business Source License? On the one hand this requires a commercial license for certain uses e.g. more than 1 port in production use on 3rd party hardware, or wanting to include the HDL code in an ASIC, etc. On the other hand you release all the sources, accept contributions, and the code reverts to open source after (n) years. This seems like a potentially interesting way to balance the interests of core developers (return on up-front investment), end users (can license a good product), and independent service providers (can provide services based either based on the latest costs-money code or older open source code). Reminds me a bit of Ghostscript and DOOM that would relicense their older releases as open source after a period of time.\nJust curious for your take, aware that this is a tangent :).\nBack to the immediate requirements for Snabb on 100G, we really need an ASIC solution. The use case is large ISPs deploying (say) 100Tbps of equipment like lwAFTR. In this scenario it is important to optimize the per-port costs. If you have open source code running on Xeon and connected with a commodity ASIC NIC then your total cost per 100G for hardware and software is probably less than $5K.\n. @fmadio I agree it is an exciting topic. I would love to have a commercial grade NIC that is hackable as open source and deployable on easily obtained hardware. This would open up new possibilities. This is the geek in me speaking though.\nI have looked at FPGA-based cards about 5+ years ago in the context of mobile data core networks. In that context it would have been easy to justify a higher port price (for FPGA + board + HDL) if the card would enable new useful functionality. (The price would have probably be less of an obstacle than e.g. the logistics of not being able to order the hardware from HP.)\nThe trouble in practice was that the FPGA offerings were too boring. The assumption seemed to be that you would use an off-the-shelf firmware/bitstream with a meh feature set. Then you have all of the costs of the FPGA but without any of the benefits. I suppose you could hire the vendor to create a custom firmware for your application, and maybe this is what keeps those companies in business, but this is not exciting to me as an open source networking guy.\nIf the card had been based on open source HDL, and I could make extensions and PR them to Github, and download the latest firmware from a CI system, etc, then it could have been another story. Then it might have made a lot of sense to use an FPGA card and actively contribute to the firmware.\nSome cautionary notes though...\nI have seen the \"quasi open source\" model be a problem. For example Silicom sponsored ntop.org to develop some 82599 driver code. Sounds simple: free to use with Silicom NICs but modest license fee on other 82599-based cards. Problematic in practice though: not really open source and not so tempting to contribute, hassle with license files that could become production deployment problems, and only a matter of time before pure open source alternatives came along anyway. Could be the same problem if you tried to generate licensing revenue from the HDL code.\nBack on the fun track...\nThere are actually some fun ideas I would like to play around with in HDL. For example, Snabb has its own very simple native ABI for packet ring buffers. The NIC should be able to operate on this directly instead of using its own separate descriptor ring format. If it could do this then the CPU would not need to be involved in the transmit/receive path i.e. less overhead and less driver code.\nIf a NIC HDL project were to grow and support multiple real applications then maybe stable versions could even be made into ASICs...\nFun to daydream about. Interesting also to see what comes of the Intel/Altera acquisition e.g. if we will be able to write HDL code that talks with the CPU over QPI.\n. @da4089 Good to know, thanks for the info! Just curious: do you know if any of these vendors publish their driver interface specifications? The main issue limiting our device support, aside from developer time to write drivers, has been lack of vendors willing to provide documentation outside NDA. I am not so enthusiastic about drivers that are based on secret specifications that are not available to the community. Mellanox releasing their PRM for us this year was a huge step forward on that front.\n. > would you personally contribute code to a project, where that code is directly licensed to makes someone else money?\nMaybe. Just right now if the ConnectX firmware source were available I would consider implementing my own descriptor format that would be 1/8th size of the standard one. I would also invest some effort in looking for optimizations on the NIC side. I would be eager to submit such changes to an upstream community who could provide review and CI test coverage. I would need to be able to distribute my modified firmware to other Snabb users but it would not necessarily need to be true open source. For example if Mellanox required a license fee for porting the firmware to other vendors' ASICs this would not pose an immediate problem.\nOn the other hand I would be quick to jump ship if a genuinely open source firmware came along as a practical alternative. Same way I would be more likely to contribute to OpenWRT than a Linksys vendor firmware even if I could get the source to both.\n\nIn the 148Mpps era the usual descriptor/payload design no longer works\n\nYeah. Snabb has already been treating extreme workloads, like 64B without loss, as special cases requiring special descriptor layouts. For example on 82599 we have the firehose demo application that statically allocates all memory and descriptors to suit the NIC and then operates on data synchronously in-place without any freelists, etc. Generally though for most applications that I deal with it is reasonable to assume a mean packet size of >= 256B (\"consumer internet traffic\") and cut the packet rate requirement down to ~45Mpps on 100G. Then my priority is to find the simplest software implementation that is able to meet this target.\n\nThe whole idea of software guys learning and writing HDL is completely rejected by 99% of the ASIC/FPGA community as laughable\n\nI am probably showing my naivete here. HDL looks to me like just another programming model, like CUDA, functional programming, data parallel programming, constraint programming, logic programming, etc. I will probably find it very humbling the first time that I write some HDL code in anger and be forced to eat these words :) but the only way to find out is to have a relevant project to use and try to contribute to.\n(One slightly encouraging anecdote... I spent a summer helping Mitch Bradley write firmware for the One Laptop Per Child XO 1.5. The firmware usually lives on a 1MB SPI flash chip on the motherboard. However, we had a faster development cycle than rewriting the flash by using a \"ROM emulator\" i.e. an FPGA that downloads firmware from a laptop via USB and then electrically interfaces with the board as if it were the SPI flash chip. I downloaded the Verilog code from opencores and it seemed pretty readable to me. The main obstacle to contributing to that would have been finding a Windows machine to install the Altera toolchain on and finding a suitable license for all that stinky software that open source hackers should certainly not have to pay list price for.)\n. @fmadio Yes, I bet that I am underestimating the difficulty of the transition to HDL, I am sure it will be an interesting and humbling challenge one day. Hard part is finding an excuse to start. I have had FPGAs in my peripheral vision for ~ 15 years but never found an excuse to actually choose them.\nYou seem to have found a nice niche for FPGA. You want to support features that are either slow to market, like truly line-rate 100G, or perhaps not coming in commodity NICs at all, like 100G based on 10x10G instead of 4x25G. I am a little envious.\nI have been looking more at features like offloading checksum, crypto, pattern matching, etc. The \"problem\" here is that CPUs are becoming more competitive all the time. This functionality all makes sense to execute on the CPU using x86 extensions for SIMD and crypto, etc. Often this is actually supported by the ASICs but we do it on the CPU anyway because it is simpler and provides more robust performance (e.g. works for any encapsulation, does not require tracking complicated metadata for each packet, etc).\nSo for me it is an elusive quest to find a problem that really makes sense to attack with an FPGA.\nJust another thought: If one did develop a collection of high-quality HDL code for useful networking functions then this could also potentially become attractive to established ASIC vendors. Sure, we will build this product around your next-gen NIC, but you need to include this HDL code on the ASIC...\n. Just another observation: The ConnectX-4 throughput above seems to be limited to 152M cache lines per second (counting only payload and not descriptors). This is suspiciously close to the device clock speed of 156 MHz that is reported by the firmware. If the ASIC is clocked at 156MHz and can transfer 64B per cycle then that would come close to satisfyingly explaining the performance that we see.\n. @fmadio @kbara Cool recent development for FPGA newbies.: Project IceStorm is a completely open source toolchain for Lattice ICE FPGAs. You can buy an iCE40HX8K Breakout Board for $42.88 to develop on. This is very low-end FPGA, more Aduino than Xeon, but it does seem like a big step forward to be able to bypass boring closed-source vendor tools for getting started.\nThere is no ethernet or PCIe header on that board. Should be possible to do something interesting though. Can you implement 10Mbit ethernet with GPIO? :-)\nHaving fond memories now of spending time on tropical islands programming little microcontroller boards in a friend's homebrew forth dialect... would be nice to repeat with an FPGA. ;-)\n. @zeronewb Interesting, thanks for the links! I didn't know that there was any public information about IES.\nStill: My impression is that to support FM10K we have to ask an OEM like Silicom for a binary blob that will boot the FM10K and then we need to interface with this via some quasi-open software interface? \nThis would be a big step backwards from past Intel cards where we simply read the data sheet and write the driver. If a vendor wants to run a binary blob then they should host that on the card as firmware. Software running on the host should be completely under the control of application developers, including being able to write our own independent implementations.\n. @fozog Great thoughts!\nI would love to see a packet processing framework designed around back-to-back contiguous packets. This would be a very interesting design and potentially in harmony with CPUs in many ways. This would also be quite aligned with the physical link i.e. ethernet is essentially a serial port of sorts.\nHowever, my suspicion is that this may be a short-term band-aid rather than the way forward. Seems like it is straightforward to do 40G/100G 64B line-rate using multiple 10G adapters and only a challenge with 40G/100G adapters. So what is the difference? Seems to me like the CPU, L3 cache, RAM, PCIe lanes, etc, all have enough capacity and the limitation is somewhere in the NIC.\nI would bet a cold beverage that the problem for high-packet-rate 40G/100G today is the PCIe endpoints in the NICs being under-dimensioned. Likely the PCIe endpoint is a generic IP core licensed from a third party who is now racing to fix the performance in time for the PCIe 4.0 refresh. Would you take that bet? :-). @fozog I think you make an excellent case. I will buy you that cold beverage when our paths cross one day :).\nI would quite like to play with this design in Snabb. I have been thinking related thoughts on other topics like \"Packet Copies: Cheap or Expensive?\" (https://github.com/snabbco/snabb/issues/648).\nI have a couple of objections in practical terms to adoption though:\nFirst is that we cannot implement this design using existing NICs (e.g. Intel) on the receive path, right? So the implementation benefits need to outweigh the compatibility issues. (Thinking VHS / BETA.)\nSecond is that line-rate 64B is not actually an important workload. 64B packets are tiny and nobody really wants to send millions of them across their networks back-to-back. The real reason that people focus on 64B line-rate is to avoid creating a bottleneck by introducing a new device that can't keep up with the existing ones. If you bought an expensive router that does 64B line-rate then you will not want to connect that to other devices that can't keep up e.g. a switch or a monitoring probe because then you are \"wasting\" your capacity. On the other hand if you already bought a switch that only does 256B line-rate (e.g. 100G Tomahawk) then the other devices only need to keep up with that - there is no value in being faster because the bottleneck does not change.\nSo, tongue only half in cheek, I would propose to solve this problem by specifying a minimum packet size of 256B for the network and enforcing this with policing in the core. This will be more than adequate for real-world internet traffic and it will save a lot of unnecessary engineering!. @plajjan Good point!\n@fozog I really like this idea of storing packets contiguous in memory. So instead of having a ring containing pointers to ~100 packets we could have a ring containing ~10kb of serial packet data. Then if we want to transform the packets (e.g. add/remove 802.1Q header) we make an efficient streaming copy with transformation on the way through. I think this would be most interesting if the application code used a streaming API for accessing the packets rather than pretending they are scattered in memory (even if that may be necessary for compatibility in certain cases.) This way the streaming would be an optimization for both the CPU and the PCIe (rather than improving PCIe efficiency at the expense of the CPU.)\nThis would be a very interesting experiment to play with :) even purely on the CPU side to see if the streaming programming model is convenient and efficient in practice.\nI would love to have a hackable NIC in the spirit of lowRISC to be able to experiment and productionize new features on the hardware side too... cc @fmadio :-). Just thinking that a simple way to experiment on the contiguous packet idea would be to process pcap files. These already have packets stored back-to-back with a modest amount of per-packet metadata (16B for timestamp + length.)\nChallenge could be along the lines of:\n- Transform a source pcap file (\"ingress\") to a destination pcap file (\"egress\".)\n- Perform the transformation by composing multiple independent operations (apps) e.g. add 802.1Q header, add tunnel encapsulation, update L2/L3 header with routing information.\n- Make sure the code is producing ready-to-transmit packets regularly (e.g. one at a time or in small chunks.) It would be cheating to make multiple passes over the whole input, for example, because in a real device we are operating on infinite streams of packets from the NICs.\nCould be that this would work fantastically well and we would want to update the whole of Snabb to operate on contiguous packet data instead of isolated buffers. Or could be disappointing. dunno :).. @fmadio Have you seen lowRISC? I think this is an interesting model. They have sat down and built a base hardware implementation (CPU) completely from scratch. Huge investment of effort presumably. Now they want to start producing ASICs at regular intervals so that any contributions that are upstreamed will become silicon automatically. This could then create a feedback loop where lots of users want to improve the upstream repository so that they will have better hardware available for their future projects.\nQuite a bootstrapping challenge. If it works out for CPUs then maybe somebody can try it for NICs.. @fmadio I would love to play with you guys' stuff. Just now though I am not involved in any projects that need 64B line-rate or that can use non-COTS hardware. So I am limited to experimenting with software architectures and blue-sky dreams for future projects with more exotic hardware :-).. @fozog I am really interested in the \"store packets in contiguous serial memory\" idea. I have been pondering it a little more. Current feeling is that it makes sense in niche applications - e.g. 100G 64B-line-rate packet capture using NICs that exist in 2017 - but that discrete packets are likely more suitable for general application development (e.g. main Snabb/DPDK use cases.) Few reasons:\n\nIMHO PCIe limitations should be resolved on the PCIe layer e.g. with extra bandwidth in Gen4 and PCIe endpoint silicon provisioned for more parallelism. I'm not wild about redesigning applications to work around PCIe implementation problems - e.g. max number of PCIe requests in flight - using the CPU.\nIn principle it would be nice for the L2/L3 cache prefetchers to stream all the packet data into the core before you need to touch it but I see a couple of caveats. One is that AFAIK the prefetchers need substantial warm-up and are mostly useful when you are processing megabytes at a time i.e. not well suited to small bursts of packets. Other is that L3 and RAM access are already very efficient if you make the requests in parallel: 7-cpu.com benchmarks say that these days L3 delivers a random-access cache line every 5 cycles and RAM every 5.9 cycles. This may be optimistic in practice but that order of magnitude is very low.\nContiguous packet buffers seem to impose a FIFO packet lifecycle. This will be fine for some applications: if all packets are sunk to disk or a single egress port for example. However if I am switching the packets across many different egress ports then I seem to be exposed to head-of-line blocking where one slow DMA request prevents me from reusing the block of memory.\n\nSo for the moment I am going to switch my daydreaming over to the topic of array programming in #1099 :-). @fozog You are right about 2KB aligned packet buffers. We have found that all data structures used in the traffic plane need to have a lot of entropy in the lower bits (mod 4096) of their addresses. Otherwise we see cache conflict misses that impact performance in a big way. This has bitten us on packet buffers (once upon a time 4096-aligned) and also on everything allocated as file-backed shared memory (kernel allocates on a 4096 byte page boundary.)\nI think the \"RAM delivers one random-access read every 5.9ns\" claim is following the reasoning that \"with nine women we can produce nine babies in nine months\" i.e. latency is 9 months but average throughput is one baby per month due to parallelism. In our code we are often performing the latency-sensitive operations on many packets in a row (like loading the first cache line of payload or making a lookup in a table) and so we should be well positioned to exploit instruction-level parallelism. (Compare with e.g. Linux kernel that tends to process one packet at a time and will need to find a way to overlap computation with individual high-latency operations like loading from memory and taking locks.). @fmadio You are right. If I am using contiguous buffers from both input and output then I will need to make a data copy to transfer from ingress queue memory to egress queue memory. I could make these copies in FIFO order. Once that copy is done then the ingress memory can be freed because the DMA is referencing the egress memory. If no egress memory is available then the packet is dropped (never held pinned in the ingress memory.)\nI have been imagining a zero-copy design where packet data stays in the same place i.e. the same memory is referenced by DMA descriptors for both ingress and egress. However that would seem to be the wrong design for contiguous packet buffers because then you have to somehow \"garbage collect\" memory that can be referenced by multiple DMA requests with indefinite lifetimes. Seems like the right alternative would be to accept the copies and aggressively optimize them.\nIs that how you are thinking too?. (Could also perhaps implement the zero-copy mode with reference counters on the memory blocks. Then you increment the reference count when you create a DMA descriptor and you decrement it when the DMA completes. Memory is freed when count reaches zero. This is actually what we did in Snabb back in the distant past but our experience lead us to prefer a simpler design with fewer code paths to optimize.). This issue is spinning off into an effort to design a \"simple and pleases both hardware and software people\" host-device interface called EasyNIC. Hope to chat with you guys on issues over there too :).. Merged, thanks!\n. Merged, thanks!\nNote: This PR came from kbara/kbara-next which means it misses the next-tributary tests on snabbco/kbara-next. I'd prefer to get PRs from snabbco/kbara-next if that is okay.\n. Just thinking aloud...\nChild processes will need an efficient way to check for an app network update and then a simple way to load it. The check could be done by polling a serial number in a shm object like //config/serial that the parent bumps after each call to engine.configure(). The configuration could be loaded by reading the overall configuration from a text file //config/configuration.$serial and picking the relevant bit.\nThis may involve (re)introducing some shm path syntax for accessing objects from the parent process. The children may also need to be able to access each others' DMA memory e.g. to make it easy for one app to allocate a DMA ring for a NIC and then another app to attach to the queue by using that memory.\n. I have pushed the next step now: a snabb worker ... program.\nThis is intended as an internal interface that a parent Snabb process can use to spawn a child process that will simply execute an app network on a dedicated core & react to configuration updates. (On balance I think that fork() without exec() is too messy and risks inheriting a lot of state that we don't want the child processes to have.)\nThe processes cooperate using a shared shm folder called group/. This is created by the parent process and then aliased into each child process (as a symlink).\nThe shm objects that I anticipate having in the group folder are:\ngroup/myworker/config.lua       # current configuration for myworker\ngroup/myworker/configs.counter  # incremented by parent on new config\ngroup/dma/7e42ca00              # DMA HugeTLBs shared by all processes\ngroup/pci/01:00.0/...           # Objects made available by driver\nHere is how this would work in NFV with 100G (ConnectX):\n- Master process initializes the NIC, sets up switching, allocates all descriptor rings, polls counters (non-realtime activities).\n- Worker processes are each run a separate app network (serving different VMs) provided by the master process (based on the configuration file).\n- Worker process drivers attach to the NIC via descriptor rings (allocated in shared DMA memory & with addresses provided as shm objects).\n- DMA memory would stay allocated until the whole process tree shuts down (cleaned up by the supervisor of the master process).\n- Supervisor would also automatically disable PCIe bus mastering (DMA) for all PCI devices. This is a safeguard against the descriptor ring memory being reused while still referenced by a NIC.\nWhaddayareckon?\nNext steps...\n- Add a SIGSEGV handler to automatically map DMA memory from other processes (have prototyped this before);\n- Add the engine API for running multiple app networks in subprocesses.\n. (Could be overkill to make the worker into snabb worker program. I'll try factoring it simply into a core.worker module.)\n. I pushed a refactoring where a simple core.worker module implements both the parent process and the child process code. This is currently sketch quality (compiles but won't run). Currently creating the worker with fork() but will switch this to fork()+exec() with some simple entry point (maybe snsh).\nI have a small obstacle to overcome and I'm curious for @eugeneia and @wingo point of view because this is a precursor of sorts to the YANG work in #987.\nThe problem is: How do I serialize a config object to pass it from the parent process to the child process?\nToday we already have functions like lib.store_conf() and lib.load_conf() that can serialize Lua objects to text files (pretty-print them and then parse them). The catch is that a config object includes Lua objects that cannot be directly serialized, particularly app classes which are typically tables of functions and closures.\nSo how should we serialize a config object? One idea would be to define a canonical string name for each app and then to use this to indirectly refer to the class object.\nCould also be that there are related challenges e.g. places where we use app configuration tables that include non-serializable objects though I cannot immediately think of any.\nJust now I have punted on the problem by calling non-existent config.save() and config.load() functions.\n. I have pushed some more code. The overall intention is for a group of Snabb processes (the main one + the workers) to be able to automatically share DMA memory between themselves, and to implement clean shutdown semantics where DMA is disabled for all PCI devices before any DMA memory is freed.\nHere are the specific invariants that this code aims to create:\n- Each HugeTLB allocated by a running group of Snabb processes is accessible to each process via the shm path group/dma/xxx.dma. (xxx is the physical address of the memory.)\n- DMA memory is never freed while any PCI device has bus mastering (DMA) enabled by a process in the group.\n- DMA memory is always freed during shutdown of a Snabb process group i.e. backing files on hugetlbfs do not outlive the processes.\nThe next step is to make DMA memory pointers globally valid for all Snabb processes in a group. This will be achieved with a SIGSEGV handler that detects access to unmapped DMA memory and automatically maps it in (if the address belongs to DMA memory allocated by a process in the group). I will use this immediately in the Mellanox driver so that the main process can create descriptor rings and then worker processes can easily access them.\n. I pushed the SIGSEGV handler now. This adds another invariant:\n- DMA memory allocated by any process is automatically usable by every other process in the same group.\nThis means that any address returned by memory.dma_alloc() can be shared between processes. You have to be a bit careful about this, e.g. if you pass a packet from one process to another then it may end up on the wrong freelist when you are done with it, and you would need to take care of this e.g. as in #601.\nThe immediate benefit is that the Mellanox driver will be able to allocate all of the descriptor rings in the parent process and then make the addresses available to worker processes via shm objects.\n. Overall I feel like this branch represents a pretty reasonable approach to multiprocessing. The tricky bits we need to review are potential race conditions in the interactions between processes e.g. are configurations created atomically, are notifications (e.g. counter increments) made after all data is available, are all combinations of signals and termination orders handled equivalently, etc.\nI suppose we also need a clear description of this new \"process group with workers\" concept in the manual. This concept as evolved a little during implementation.\n. I pushed an update so that the worker processes use execve(2) to create an entirely new process image. This way the worker processes are all normal Snabb instances that have been initialized from scratch. This is intended to avoid a whole class of errors from inheriting state with fork().\nAdded an initial selftest function too.\nThe main thing lacking now is a working way to serialize a configuration object so that the parent can actually provide configurations to the children.\nOverall I feel like the design here is quite reasonable and the code is short, but the exact formulation has quite some rough edges. For example I have somewhat awkwardly broken the shm abstraction in order to operate directly on the underlying directories (e.g. to create symlinks to HugeTLB files that live in a directory that is not addressable by shm paths).\n. @wingo I would be happy to replace this with a YANG-oriented mechanism. Would you like to take the lead on the multiprocess support and just cherry-pick anything you want from this branch?\nHere are the ideas on this branch that I think are significant:\n1. The notion of a group of related processes. This provides a scope for sharing certain resources e.g. PCI devices and DMA memory. Here it is simply implemented as a symlink between shm folders.\n2. Orderly shutdown of the group. Here we have a shutdown step that disables DMA on all PCI devices before allowing any DMA memory to be freed to the kernel.\n3. Having DMA memory mappings shared between all processes in the group (via the SIGSEGV handler.)\nI do like the idea of having automatic assignment of suitable CPU cores and e.g. reserving the first core of each socket for the kernel. I would ideally prefer to know that worker processes always have affinity to one specific core rather than being exposed to kernel scheduling, but I don't have a scheme worked out for that and maybe it is not practical.\n. (Oh you are talking about the CPU affinity of the management process. Yes, I agree it would be fine to let the kernel schedule that somewhere out-of-the-way, and as you say possibly worth explicitly setting affinity to a core that we are not planning to use for worker processes.)\n. The process model here is:\n- Manager process is a normal Snabb process, running the engine, but is understood to be reserved for non-realtime administrative tasks such as handling configuration updates.\n- Worker process is also a normal Snabb process but has CPU affinity and will expect the manager process to take care of anything that would require an unacceptable interruption of traffic.\n- Each Snabb process - manager and workers - is monitored by a tiny supervisor process that performs minimal cleanup actions on termination e.g. safely turning off DMA and cleaning up shm folders.\nHow does that match your model?\n. The new constraint here is that for the Mellanox driver I would really like to be able to run the NIC initialization & update logic in the manager process. The Mellanox firmware interface is really designed around the expectation that a centralized stateful thing - traditionally the kernel - will be responsible for provisioning queues etc. I could make this state available to all of the worker processes so that they can each perform provisioning actions, but at the moment this is not my preferred implementation strategy.\n. @wingo The supervisor process is actually already introduced in #930 and I am pigging backing on that here. Originally the only thing that it did was to remove the shm directory on shutdown but I found it an appealing place to implement clean DMA shutdown semantics too.\nJust now on this branch every process has its own /var/run/snabb/$pid folder like always, but underneath this we have a group/ folder that is shared between all processes (in the workers it is a symlink to the manager.) So from the outside looking in /var/run/snabb we see a flat list of all Snabb processes on the machine but internally they can use group/ to share objects with only their siblings.\n. @wingo I would like to extend the NFV application to support multiprocess support (shared 100G NIC) in the short term (this month). Do you think it is better to base that on the multiprocess support on this branch (which we will then need to reconcile with the YANG extension) or would you like to supply an alternative (aligned with YANG from the beginning)?\n. @wingo This discussion is helpful. I think I see how to make this multiprocess branch simpler and more orthogonal to your YANG work. I reckon that we can avoid the conflict and both continue hacking on our merry ways and then sync up without a collision :).\nThe problem I see is this maldesigned API function:\nlua\n-- Configure a worker process with a new app network.\nworker.configure(workername, config)\nI proposed that the parent process will pass app network configurations to the worker processes. This created an immediate problem: How do we transport the config object between processes when it has no serialized representation? Then it also creates the critical path dependency: Should we model the app network with YANG and make it serializable?\nI think a simpler solution would be for the worker process API to be agnostic to configuration. You would instead start a worker process and provide it with a startup entry point (Lua code) that can do anything. The worker would then be responsible for obtaining its configuration in some program-specific way. For example on Snabb NFV each worker could independently parse the configuration file and cherry-pick the parts that are relevant (parent is responsible for starting children but then they configure themselves.) For lwAFTR the configuration could be provided via the high-performance \"back-channel.\" Over time we will probably discover common requirements but we can factor that as library code.\nSo - thanks for the input. If that suits everybody else I will move forward with this branch for managing child processes without committing to any one parent-child configuration interface (and upstreaming via @eugeneia)?\n. Groovalicious. I will hack in that direction & remove the RFC/WIP marker when ready.\n. @eugeneia @wingo So! I would love to reach a good solution for how to specify which core a worker process should run on. (We should nail this down at least on the Lua worker API level even if different programs choose to use the API in different ways.)\nJust now on this branch the API requires the parent process to nominate the exact core number for the worker when it starts:\nlua\n-- Start a worker process with affinity to a specific CPU core.\n-- The child will execute an app network when provided with configure().\nfunction start (name, core)\nThe simplest way to incorporate this into the NFV application could be to specify which core each virtual port should be handled by. Example config file with added core parameter:\nlua\n-- NFV ports configuration: two ports on core #1 and one port on core #2.\n{ { id=\"vm1\", mac=\"00:00:00:00:00:01\", core=1 },\n  { id=\"vm2\", mac=\"00:00:00:00:00:02\", core=1 },\n  { id=\"vm3\", mac=\"00:00:00:00:00:03\", core=2 } }\nThis would seem to work okay but it is quite low-level. Is it worth inventing a new notation, e.g. to give cores symbolic names (core=\"traffic0\"), to assign actual core numbers heuristically, and to allow the user to supply some optional hints/constraints on core selection (cores = { traffic0 = \"numa=0\", traffic1 = \"core=1-6\", ... }).\n. I chatted with @wingo about core assignment on Slack and this is what sounded best at the moment:\n- The user specifies exact core numbers (as in the example above.)\n- Snabb assigns cores using lib.numa() (#1038) to make sure it is done right.\nThis is a simple and direct approach where the person creating the config file is responsible for deciding which exact cores to assign. The alternative would be to use a \"do what I mean\" heuristic to automatically select a core but this seems a bit error prone (might need to consider isolcpus, hyperthreads, other applications on the same machine, etc to avoid making mistakes.)\n@eugeneia what do you reckon about  that scheme?\n. @wingo I would quite like to land this code on your branch before next/master if that suits you. I like the idea of feature branches propagating quickly to users and the next/master branches acting as the \"trailing edge\" that aggregates everything in the background without time pressure. I realize that this is a bit complicated e.g. with the same branch potentially having multiple upstreams/PRs. Have to see how it works in practice.\nSo it sounds like what you need is:\n1. Reliable mechanism for making sure that all processes run the same software.\n2. More flexible entry-point to the worker process e.g. Lua expression instead of app network.\n3. Well-defined process shutdown semantics w/ test.\nIs that about right?\nshutdown\nThinking about shutdown... I am a fan of the Erlang style where you make cleanup code as minimal as possible and also isolate it from the rest of the code. So I would like shutdown of the parent to reliably stop the children but I would prefer not to do this with explicit stop() calls in the parent because that could fail in scenarios to heap corruption, SIGKILL, OOM, etc. The best mechanism that I see now would be for the child to run something like prctl(PR_SET_DEATHSIG, SIGTERM) to automatically receive a signal from the kernel when the parent terminates, and for this signal to be unhandled so that the kernel shuts down the worker automatically. what do you think?\nin place updates\nThinking about making the process forking robust to in-place updates. The optimal would be if the kernel would provide a way to access the exact binary that a process is running (instead of a symlink). Is there a way? I haven't found one.\nFailing that I have two backup ideas.\nOne would be that during startup we fork() a persistent \"template\" process that serves requests to fork new children with the same software. This way we preserve the executable in the running heap of that process.\nAnother variation would be for the main process to copy the snabb binary that it starts from into the shm group folder /var/run/snabb/$pid/group/snabb where it is accessible to every process in the group.\nThese all sound messy. Great if there is a better solution? Should grep some codebases for execve() to see what other people are doing.\n. @wingo This is potentially ready for you to use now.\nCommit 1ce4ca8 introduces a simpler interface worker.start(name, luacode) where luacode is evaluated as a string and expected to take care of core affinity, configuration acquisition, etc.\nCommit 5393afa makes the child processes terminate when the parent does.\nHave to still formulate a nice solution for ensuring that every process runs the exact same software version. This will ideally also work for applications like snabb config, snabb top, etc, when they want to communicate with a process using its own native software version.\nWDYT?\n. @kbara Could you please gist the output of this command?\nsudo strace -f ./snabb snsh -t core.memory\nCan you also confirm that other branches are working on your dev laptop? (My own Linux laptop lacks HugeTLB support and doesn't run Snabb.)\n. Thanks for the info. Could you please gist the output of that command when running/failing on this branch? (The gist above seems to be from a successful from of a different branch.)\n. (I have tried to reproduce this problem on a couple of lab servers but for me make test is running fine. I have also tried purging state like /var/run/snabb. I am hoping strace will provide a hint about what is different on your machine.)\n. @kbara The line failing above is memory.lua:151. So we have created a file on hugetlbfs, we have made it hugepage-sized, but the call to map it failed. alas, we do not have the error message.\nSeems likely to be some obscure kernel-related issue. Could be an edge case e.g. how it behaves when it is unable to provision a hugetlb page? I will dig a little. Could be useful if you check dmesg for a relevant message. also, what kernel are you running?\n. I have reproduced it now. Fix + explanation coming.\n. How about now with the fix on 987dd77?\nI believe the problematic case was when Snabb needs to ask the kernel to provision a new huge page after an allocation failure. This would tend to trigger if you have never run a Snabb process since boot (or after you run sysctl -w vm.nr_hugepages=0).\n. Good question. This is a DWIM feature to make Snabb easier to get started with. (Could be that it should be disabled for programs that are more concerned about correctness than convenient in installation.)\nJust now you can install and run Snabb on a new machine very easily:\ngit clone https://github.com/snabbco/snabb\nmake -C snabb\nsnabb/src/snabb packetblaster -S 60 01:00.0\nand if your machine was not already perfectly prepared then you may see some messages like this:\n[mounting /var/run/snabb/hugetlbfs]\n[memory: Provisioned a huge page: sysctl vm.nr_hugepages 0 -> 1]\n[memory: Provisioned a huge page: sysctl vm.nr_hugepages 1 -> 2]\n[memory: Provisioned a huge page: sysctl vm.nr_hugepages 2 -> 3]\n[memory: Provisioned a huge page: sysctl vm.nr_hugepages 3 -> 4]\n[memory: Provisioned a huge page: sysctl vm.nr_hugepages 4 -> 5]\nThis is Snabb attempting to DWIM instead of exiting with error messages like:\n```\nError: Failed to provision a huge page.\nPlease take the following actions:\n\nReserve huge pages in your grub.conf:\n      hugepages=nnn\n   (where nnn is the max number of pages you will need for Snabb.)\nCreate directory /mnt/hugetlbfs\nAdd this entry to /etc/fstab:\n      hugetlbfs       /mnt/hugetlbfs  hugetlbfs       defaults        0 0\nReboot.\n```\n\nwhich feels like creating a lot of schlep work for the user.\n(You could also say that it is a workaround for the Linux kernel lacking a suitable mechanism for dynamically allocating a hugetlb page, at least that I am aware of.)\n. (Could add a DWIM configuration option to the engine to control this behavior. Once the YANG support lands perhaps we can model the engine config that way?)\n. @kbara Please pull this branch again to get commit 7adc708 which fixes a file descriptor double-close bug in memory.lua. This can have unpredictable effects - was seen to asynchronously close vhost-user sockets in the NFV application. See also justincormack/ljsyscall#205.. This branch depends on #1077. Please merge @kbara. I did not pull here because it is based on a different ancestor (newer version of master) and would make this PR much more noisy to pull that too.. I pushed API documentation for the core.worker multiprocess API: https://github.com/snabbco/snabb/blob/82f335573a743fd608baa5ee966d3c5ef35183ab/src/README.md#multiprocess-operation-coreworker.. @eugeneia Sorry for the slow feedback. I think the overhead should be less than 1%.\nThe issue for me is that I don't understand why a simple change like bumping a few counters would cost more than this. I suspect there is an interesting answer but I don't know what it is. This is really a case where we need better tooling for \"zooming in\" on performance issues.\nIn a perfect world the information that I would like to have available from CI would include:\n- How do the relevant traces look on both branches? (Compare IR and machine code for the vhost_user loops to see what is different.)\n- How are the metrics instructions-per-packet and instructions-per-cycle impacted? If we are executing more instructions then there is likely a JIT issue, if our instructions are executing more slowly there is likely a CPU issue (like https://github.com/snabbco/snabb/pull/558#issuecomment-122662711).\nIn principle we could produce the trace dumps with -jdump, identify the relevant ones with -tprof, and see the detailed metrics with enhanced timeline logs (#1011). In practice we have not yet worked out how to tie these tools together with the CI jobs and it is inconvenient to have to run them manually.\n. Is there a loop in that trace? Is this IR snippet from before the loop (first iteration) or after the loop (subsequent iterations reusing work from the first)? (Can you gist the full trace?)\nIf you have run traceprof then could you show the before+after on that too? Just to see if e.g. the hotspot has moved from the loop of a trace to a side-trace which would be slower.\nThe PMU data does not quite seem to add up. If this code is only using 1% more cycles in total then it should not account for more than a 1% slowdown. However maybe I am not interpreting the info right or the measurements are not precise enough.\n. Matching up IR with mcode is laborious and heuristic in my experience. One tip is to dump with +r to see register assignments that will match in both IR and mcode. I would love to automate this matching.\nHowever the IR probably contains all of the answers. Just that it is a bit less familiar and I haven't memorized all the opcodes and so I tend to use the mcode as a crutch. Could instead refer to the IR docs.\n. Just observations from glancing at the IR:\n- There are a bunch of hashtable lookups (HREFK) and type checks and so on. This would be fine if it is happening once, on the first packet, before the trace spins in the loop. However if we are doing this on every packet then it will add up.\n- There are PROF IR nodes that I suspect are due to the LuaJIT profiler (-jp). I would suggest disabling the profiler, at least when taking PMU measurements, because it can impact the compiled code. (One reason I use traceprof instead is that it does not interact with code generation.)\n- Could try representing counters in a more efficient way e.g. as FFI objects (method call to add) or as closures (call to add). This could reduce the number of instructions required to bump a counter and make it less crucial for the JIT to \"hoist\" them outside of the inner loop. (On timeline I have a \"factory\" function that returns a closure that you call to log an event.)\n. Oh hey look at the last three lines of the IR:\n```\n\n0360    u64 XLOAD  0359\n0362    u64 ADD    0360  +1\n0364    u64 XSTORE 0359  0362\n```\n\nThis is actually exactly the code that we want. Load a u64 value from memory (XLOAD), add one (ADD), store the result (XSTORE). (Okay, even better would be to have only one ADD that keeps the accumulated value in a register until the loop terminates, but that might be asking too much.) The rest of the IR instructions look mostly like overhead that we want to eliminate (e.g. more efficient function call) or move outside of the hot code path (help the JIT to \"hoist\" the work out of the loop).\n. Traceprof should be very helpful here. It will tell us how much time was spent in each trace and whether that is in the sequential part of the trace or the looping part (which is usually better optimized).\n. The reason traceprof is important is that the same function can be compiled many times as different traces each with different IR and mcode. It can even be compiled twice in the same trace, separately for the first iteration and the subsequent ones. Traceprof tells us how much CPU-time each of these implementations is actually consuming. This tells us which traces we should be looking at, and also how well we are keeping the CPU occupied with loops (which tend to be compiled the best.)\n. Groovy. So on both branches there is a trace starting at virtq_device.lua:57 that is hot inside its loop. So it makes sense to compare the IR code for this trace on each branch, starting from the --LOOP-- line as you say. Hopefully this code will be more streamlined than what we saw above because it can reuse the work that was done on the first iteration (function lookups, type checks, etc). LuaJIT does this kind of optimization very well.\nbtw: Interesting to see the other hot trace too. On master branch this is trace 35 and it is a side-trace branching from root trace 18 (18/4). We can see below that trace 18 is flooding.lua:29. So it sounds like the flooding bridge module has an issue like lukego/blog#8 i.e. instead of staying inside a tightly compiled loop it is branching to a side-trace and repeating setup work.\n. This IR code does look fairly innocent. Looks like it bumps two counters, the first by exactly 1 (sounds like the packets counter) and the second by a variable amount (sounds like the bytes counter).\nCould still be that this innocent-looking addition does impact the generated code. Maybe the JIT is running out of registers and needs to spill temporary values onto the stack or recalculate them. I think it is worth comparing the machine code too.\nbtw: I usually run the JIT dump with -jdump=+r to show where the values are allocated (which register or which stack position). This also helps to line up IR with mcode because the register names match.\nHowever, could also be time to take a step back and look more carefully at the metrics. How much % slowdown are you seeing on average on the l2fwd benchmarks? Can you account for this with the PMU counters? (How are you taking the PMU measurements?)\n. So a few quick-fire thoughts for now:\nIf adding the counters produces 30 additional instructions per packet then this seems plausible to account for the performance hit.\nSecond pass\nJust an out of the box idea...\nI am starting to think that the best way to make LuaJIT performance simple and understandable is to factor code as multiple small loops (\"o-o-o\") rather than one big one (\"O\"). This way we see each loop separately in traceprof, we can read their code in isolation, JIT issues like side-traces are localized, and maybe we get better code (less register pressure, fewer \"barriers\" forcing accumulators to be committed to memory, etc).\nOn the one hand it would be quite a task to rewrite the whole virtio module this way e.g. one loop to collect the descriptors, one loop to copy the payloads, one loop to calculate the offloaded checksums, etc. This could turn out to be a wasted effort if the coding style did not actually help. On the other hand it may be straightforward to factor this new counter functionality as an isolated loop.\nCould consider something like this?\nlua\nfunction pull_and_count ()\n   -- get the link that we will output packets to & remember the index before we added any\n   local l = self.output.output\n   local index = l.write\n   -- run the normal pull logic to add packets to the link\n   pull()\n   -- bump counters as a post-processing step across the new packets added to the link\n   while index ~= l.write do\n      count(l.packets[index])\n      index = (index + 1) % link.max\n   end\nend\n(Operating on packets after they have been transmitted on the link is probably not sound practice in a multiprocess context, since we don't own them anymore after we transmit them, but that is a detail we could deal with.)\nSide-trace risks\nRelated issue - there are two potentially unbiased branches in this Lua code:\nlua\nlocal counters = self.owner.shm\ncounter.add(counters.txbytes, tx_p.length)\ncounter.add(counters.txpackets)\nif ethernet:is_mcast(tx_p.data) then\n   counter.add(counters.txmcast)\nend\nif ethernet:is_bcast(tx_p.data) then\n   counter.add(counters.txbcast)\nend\nThis will force the JIT to generate separate code (\"side traces\") for multicast and broadcast packets and this will make performance sensitive to the mix of unicast/multicast/broadcast traffic. This is likely being missed by our standard performance tests since those use uniform packet contents. I would recommend rewriting these to be branch-free (or alternatively making the test suite more sophisticated so that it can observe and report the effect of traffic mix on performance).\n. Relatedly: I am not really confident to dive into the details here because the code+tprof+trace all need to come from the same instance in order to fit together. If I would take these puzzle pieces from multiple different executions then I will never manage to piece them together.\nWe really need to come up with a standard way to capture all the data needed for performance analysis in one place... I also think the PMU information would be much easier to interpret if we captured it separately for each app. So much to do... one step at at time :).\n. @eugeneia There should not be any overhead from PMU nor from CPU pinning. Unless perhaps you have pinned several processes to the same core?\n\nthe reason why I threw out the \u201cbranch-free patch\u201d is that I couldn\u2019t measure its positive impact.\n\nGenerally I am on board with this philosophy i.e. be conservative about optimizations and don't add complicated code unless you are sure it is worth the trouble. However, this case seems different for two reasons. First is that this is the single longest and hottest trace in the whole application and so any surprises are likely to have severe impact. Second is that adding workload-sensitive branches is definitely reducing our performance test coverage i.e. this will cause new side-traces and machine code to be generated and we have no visibility of the impact because our tests are not exercising those paths.\n(Over time I expect that we will develop programming styles that make all of these issues more manageable. I am still hopeful that writing multiple small loops will be more robust than single large ones as we have here in virtio. This is already the effect of breaking functionality into multiple apps, but it may make sense within apps too. Have to get some more experience to really make sense of it all.)\nIn the future we could develop more sophisticated testing tools to deal with such issues. Simplest might be to test many randomized traffic workloads. Fancier (daydreaming...) might be a load tester that uses reinforcement learning to find an application's pain-points by searching for workloads that cause excessive load (e.g. cause packet drops or making a bits-per-cycle metric drop).\n. Great initiative! Have only given it a quick run-through but no problems jump out at me.\n. @eugeneia Release at will!\n. Merged, thanks!\n. Hm. The code looks very innocent but in the Hydra job this really looks like it introduces a performance regression for the l2fwd benchmark. @eugeneia any ideas?\n. Good testing. Yes, it does look like there is a voodoo problem here. I have no idea what it could be. Least crazy guess is that the new names might be creating a hash collision in a Lua table somewhere. (That could indirectly cause JIT behavior changes because hashtable lookups are specialized on the size of the hashtable and any change will require a side-trace. If this is the case then the solution may actually be to get #1037 working so that we don't have to worry about such crazy stuff. But maybe there is a more innocent explanation.)\nThe best idea that I have for debugging, which is not a great one, would be to split the change up into multiple smaller commits (e.g. four parts) and have Hydra test them all in parallel. Then when you see which commit first introduces the problem you can split that one up and test the parts. (In the spirit of git bisect.)\n. @domenkozar Just checking: Could there also be any change on the lab servers in between the test runs that Max has made? Just thinking that e.g. if we have upgraded NixOS then we might see a difference if one branch was tested before and one after.\n. (These results are sufficiently suspicious that it could be worth forcing them to rerun from scratch somehow i.e. without Hydra reusing results from previous runs on unchanged branches like master.)\n. I think we should start with the more general question: Has anything changed in the lab that influences test results? Then if that is affirmative we could look for the more specific cause e.g. munin monitoring.\nSimple way do that could be for @eugeneia to add two more branches, each with a trivial change added to the branches tested before, and then we see whether the test results are the same as we see now (i.e. determined by snabb code) or whether they are different (e.g. determined by date on which teh tests were executed).\n. NB: If the test results come back clean I will merge this branch. The idea about renaming link counters could be taken in a separate PR targeting next if we decide we care about that. (I need to check out the new Github \"reviews\" feature, it could be neat if that provided a way to distinguish between comments in passing vs specific requests. Anybody done much with it?)\n. @eugeneia Could you drop the priority (\"scheduling shares\") from 10,000 down to e.g. 100? This jobset is really big (~15K tests) and seems to be blocking various much smaller jobs with lower priorities.\n@domenkozar One day I need to ask you more about the Hydra \"declarative jobsets.\" Seems like our jobset definitions would be much easier to maintain as Nix code in a Git repo where we could use common parameters and library functions to keep them in harmony. Just now I am sure we have a haphazard mix of priorities, nixpkgs verisons, snabb branch selections, etc in our jobsets. (Hopefully we can still use the webui for ad-hoc jobs too... I would be curious to know more about the feature.)\n. @domenkozar Oh that looks really wonderful. Great that it plays nicely with the imperatively managed jobsets and that each project can have a separate jobset repo.\n. (Oh and I am able to use the Hydra command \"Bump to front of queue\" to fast-track the small jobs now as a once-off. Cool.)\n. @eugeneia Interesting! I would be curious to know if you can see the issue on the basic1 benchmark or only the more complex ones?\n. Interesting :). Challenging not only to account for the difference but even to describe it.\nHow about splitting up this change (e.g. rebase on a temp branch) into many smaller commits, for example to update app separately, and then have Hydra compare them all in parallel? This could be a divide-and-conquer strategy to isolate the issue(s).\nWhat do you think about scaling back the Hydra job a bit too? Just now it's running 7000 tests per branch with high scheduler priority (1000). I think you'd probably be fine with 1/10th the tests at 1/10th the priority.\n. First thought is that there seems to be an easy to reproduce test case here. The basic1 benchmark usually scores 30 for one branch and 32 for another. This test probably only takes some tens of seconds to run on a local machine.\n. So the commit that changes behavior is eugeneia/snabb@4af450e? I have taken a quick look at profiling this but I don't immediately see the difference (not so surprising because it is relatively small). The best path forward that I see is recursive divide-and-conquer i.e. keep on finding the smallest change that causes a behavior change and splitting that up until we only have a few lines left.\n. Digression: Just imagine how cool it would be if our built-in profiling support was so good that we could solve mysterious performance problems like this one offline and after the fact :). Then instead of running new tests all the time we could zoom in closer on the results that we already have.\nHere is a babystep in that direction: I ran snabbmark basic1 with timeline logging support and then converted the timeline to CSV. This way we can use R to analyze the performance of many different engine breaths all sampled from the same process.\nFirst simple example is a density plot showing how long (cycles) the push and pull calls to each app instance take. This is for basic1 that generates packets with a Source, duplicates them with a Tee, then discards them with a Sink.\n\nJust a proof of concept, please take with a grain of salt. Could potentially use this to make comparisons between branches and find very specific things e.g. if one particular app slows down or becomes less consistent.\n. (Just observing that Github seems to have successfully kept the above discussion intact even though we reset the branch to remove these commits. Good to know.)\n. Just fantastic :).\n. Exciting!\n. @wingo How about if I setup a Hydra job that benchmarks branch wingo/snabb#optimize compared with master and next? Then you could force-push experimental code like this that you want Hydra to benchmark before we upstream. If you create that branch and let me know then I will setup the jobset (same offer goes for everybody else).\n. (btw @domenkozar and others are working on extending Hydra to be able to automatically test Github PRs. This will be nice.)\n. Groovy. I created jobset snabb-new-tests/optimize-murren that tests many configurations (all dpdk/qemu/etc) but only a small number of iterations per scenario (3). This should provide a solid picture of the overall impact of the change but we have to be cautious about slicing-and-dicing too much (the jobset testing next does 10x more iterations).\nHave to revise the R reports soon. I would particularly like to slice-and-decide datasets into groups using tools that avoid false-positives like xkcd 882 by correcting the confidence level based on how much the data is being \"tortured.\" Visualizations are weak in this respect - easy to see patterns in randomness - and we probably need to include e.g. properly adjusted confidence intervals.\n. @domenkozar any idea on this evaluation error? https://hydra.snabb.co/jobset/snabb-new-tests/optimize-murren#tabs-errors\n. The first report is up. I know that reading these things is a bit of an art, but it looks like the change is basically neutral and does not cause any performance regression there.\nThis was tested on the murren servers without a hardware NIC. I suppose it makes sense to also test on the lugano servers that do have hardware NICs in case, for example, the alignment of memory has implications for DMA.\n(On reflection I believe that the 82599 requires DMA to be aligned to even-numbered addresses. If that is right then we might need to hold that invariant for struct packet and fall back to the copying mode in case the packet is offset by an odd amount.)\n. Any chance of a selftest() function in core.packet to exercise the basic code paths?\nLike:\n- allocate, free.\n- allocate, shift left within headroom, free.\n- allocate, shift left beyond headroom, free.\n- allocate, shift right within headroom, free.\n- allocate, shirt right beyond headroom, free.\n... and checking that free is putting the packets back into a canonical form i.e. resetting the headroom?\n. For my part I am fine with the datagram library changes.\nCausing breakage downstream is indeed regrettable. I would like to solve this by bringing more code (and tests and benchmarks) upstream so that we can take care of it while making core changes like this one. (I think this is the normal \"selfish\" reason that people upstream their code i.e. to share the responsibility for keeping it working.)\n. LGTM!\n. relatedly: I recently came across the term researcher degrees of freedom for situations where the person interpreting the data can inadvertently make uncontrolled decisions that lead to seeing things that seem more significant than they are. I believe that I have created exactly such a situation with this version of the R report that serves up a menu of sliced-and-diced graphs to choose from. Just by picking the most interesting graph from the report we are already on the way to seeing patterns in randomness. Have to think about a better presentation...\n. Just curious: How come the PR for this issue is being reviewed on the Igalia repo (Igalia/snabb#518) instead of here on the upstream snabbco repo? This is fine of course but I see a risk that feedback from people here will come late and potentially create more work for you.\n. (Sorry for not providing some feedback on the idea already btw. Just been busy. I will try to provide some useful input when the code is PR'd to this repo.)\n. (Generally could make sense for somebody to volunteer as the \"subsystem maintainer\" for Snabb process management. We have a bunch of PRs in this area coming from different applications. Could be really practical to have a branch like snabbco/process that is owned by one person who will review and merge such changes before proposing them to an upstream -next branch. Having topic-specific subsystem branches is the next logical step in our Git workflow evolution but it seems like these things have to happen at their own pace.)\n. Feedback is that it seems like you could do the work on a topic branch based on master with a PR on snabbco/snabb repo for review, and you could still merge that into the downstream lwAFTR branch any time you want. This way you have the benefit of early review of the changes that affect everybody upstream and you also have the flexibility of deciding exactly what to merge and when in your downstream branch.\nThis is what I am doing with LuaJIT for example where I have a few PRs open on upstream (e.g. LuaJIT/LuaJIT#224) but that is mostly for early visibility and my intention is to merge these changes directly onto my own branches without waiting for any approvals.\n. I suggest workflow of @eugeneia or @kbara being upstream for this and then sending it onwards to me for rubber-stamping. I have no strong feelings on this module - wasn't involved in the development - and have no access to test cases for it either.\n. Oh, or. perhaps I have misunderstood this change. Is this changing the device side of virtio-net? I will look more closely now...\n. OK. In that case I prefer not to be upstream as per ^^^ and I don't think this code has any coverage in existing Hydra tests (which currently always runs DPDK inside the guest and never Snabb).\n@kbara Sure but virtq_device is the code being exercised by CI i.e. the device (hypervisor/snabbnfv) side.\n. (The ideal would be to add a Hydra case that runs Snabb inside the VM in addition to DPDK. This should be fairly straightforward FWIW.)\n. (i.e. it would be an extension of the existing l2fwd benchmark that we run on Hydra but it would run with a different VM image i.e. one that runs the Snabb-based l2fwd instead of the DPDK-based one.)\n. This fine-grained trace-at-a-time approach currently seems less promising than the more coarse-grained approach in #1039 based on my experiments to date. More thoughts at https://github.com/LuaJIT/LuaJIT/issues/218#issuecomment-254448416.\n. Great! I am becoming very interested in these JIT flushing heuristics lately :).\n. I would like to be upstream on this please :) I am very interested in this topic at the moment.\n. Have you considered making this also react to drops on Virtio interfaces? (Hard? Bad idea?)\n. Please allow me to zoom out for a moment, not to suggest changing this patch, but rather to map out the context for this whole ingress-drops business mostly for the sake of future directions.\nHere is how I have it all in my head:\nThe problem we want to solve is to detect when a Snabb process is overloaded and then to run a callback. The callback could take corrective action (JIT flush) or provide diagnostics (log message).\nDefinition: Snabb is overloaded when the average packet arrival rate exceeds the average packet processing rate.\nThis means that during overload a backlog will grow on one or more ingress interfaces, and once the backlog exceeds the queue size we will start to drop packets. Conversely, during \"underload\" we are processing packets faster than they are arriving, on average, and this means that any backlog on ingress interfaces will be shrinking and each interface will regularly be drained empty during a pull().\nSo how do we detect overload? There are a bunch of conditions and we could use one or more of them as proxies:\n1. Packets are being dropped on ingress. Pro: Closely related to overload that exceeds tolerable bounds. Con: Has to be implemented specially for each I/O driver.\n2. I/O interface runs out of packet buffers for receive (requires a complete refill on pull()). Pro/Con: Similar to (1) but perhaps easier/harder to implement for certain I/O drivers.\n3. I/O interface consistently outputs the maximum burst size on pull() i.e. ~100 packets per breath (engine.pull_npackets) meaning that the packet arrival rate is >= the packet processing rate (because otherwise the queue would be shrinking towards zero). Pro: Should work for any I/O driver with an ingress queue size >= ~100. Con: ?\nSo putting this into the context of detecting ingress overload on a Virtio-net interface if we don't immediately have a \"dropped packets counter\" for (1) then we could use (2) by detecting if every descriptor on the receive vring is occupied with a packet (no room for more) or (3) by checking if the number of packets pulled by the virtio app is consistently engine.pull_npackets.\nHow much of this makes sense and what am I missing?\n. Yes, I like the new formulation, it's stylish. Merge it onto wingo-next?\n. @wingo Groovy. I am waiting for the next-tributary and next-tributary-murren tests to complete and then I will check it out.\nSuits me fine if you want to push more updates. I'll assume that I am welcome to merge the branch while the PR is open (unless marked [wip] etc) unless told otherwise.\nI think we are all already on the same page about this, but just in the interest of over-communicating, I generally rely on the code coming from -next branches to already be reviewed and for me to mostly be playing integrator e.g. checking the CI results for potential release-blockers and so on.\n. Merged, thanks! Code and tests look good.\n. There is no native TCP stack for Snabb. However, Antti Kantee has run the NetBSD network stack on Snabb as a proof of concept.\nWhat do you want to do?\n. That would be neat. No, today we don't have a TCP stack. I would love to find a reasonable excuse to build one :) particularly a special-purpose one that would enable applications you can't implement with off-the-shelf stacks.\nAnecdotally, the project I worked on immediately before Snabb was a specialized \"man-in-the-middle\" TCP stack for optimizing traffic over cellular networks. This acted basically like a proxy but kept TCP state (options, sequence numbers, etc) synchronized between the endpoints. Cool property was that you could pull the power cable and ~99.9% of the connections would continue without breakage. This is currently being marketed as Sandvine TCP Accelerator. There's a great writeup by Juho Snellman about it. That was a really cool problem to work on :).\n. Exciting stuff!\nThis is of course something that we sorely need, i.e. a uniform way to incorporate I/O sources into an app network when your requirements are farily generic (e.g. send and receive packets, tag/untag, switch on L2 header, hash on L3/L4 header). I suspect this would suit most Snabb applications and so this IO app has the potential to make them all very flexible: connect to hardware NICs, kernel interfaces, PCAP files, etc.\nI have two topics that I want to discuss in this context. I will start with the simple one :-).\nURI\nShould we create a uniform syntax for selecting IO interfaces?\nAs a user I would really like a concise way to tell a Snabb application what IO mechanism to use. Examples:\nsnabb nfv traffic pci:01:00.0\nsnabb nfv traffic tap:tap0\nsnabb nfv traffic raw:eth0\nsnabb nfv traffic replay:imix.pcap\nand so on. I know that people have already done work in this direction e.g. @dpino for lwAFTR. \nCould also be that the syntax would need to support more elaborate information, like the IO app parameters of MAC address + VLAN tag + RSS hash bucket, in which case we would need to choose a flexible syntax. (URI?)\nControl vs Traffic\nI have a feeling that we should make a separation between control (defining queues, setting up switching rules, choosing hash functions, etc) verses traffic (sending and receiving packets on a specific queue pair.) I say this because I think certain NICs are designed with the expectation that a single entity (traditionally the kernel) will handle all control and that it could be awkward to support these NICs in a strictly peer-to-peer fashion.\nThis would mandate having two kinds of app:\n- IO: Many instances that each perform transmit/receive on a queue pair.\n- IOControl: Single instance that handles all queue setup.\nFor example the usage could be like this:\nlua\n-- Define the queues.\napp(\"ctrl\", IOControl, {queues = {{id=\"a\", mac=\"10:10:10:10:10:10\", vlan=42},\n                                  {id=\"b\", mac=\"20:20:20:20:20:20\", vlan=43}},\n                        rssbuckets = 2})\n-- Attach to named queues.\napp(\"a1\", IO, {queue = \"a\", rssbucket = 1}\napp(\"a2\", IO, {queue = \"a\", rssbucket = 2}\napp(\"b1\", IO, {queue = \"b\", rssbucket = 1}\napp(\"b2\", IO, {queue = \"b\", rssbucket = 2}\nIt could be that I am not being sufficiently imaginative and that we can support all NICs without needing this IOControl app. I suspect this will be an implementation challenge though. Let's dig in and find out.\nThe intel_mp driver on this branch does not need an IO controller. However, this seems to be an easy case: Intel NICs have a fixed set of hardware queues (0..n) and RSS hashing does not require much state.\nThe intel10g app currently does keep state inside the app for controlling VMDq i.e. for maintaining the mapping between hardware queues (fixed) and MAC addresses (dynamic). Could be a challenge to support in intel_mp?\nThe new Mellanox ConnectX driver currently does use a separate app for control as shown in the example above. The reason is that controlling the NIC is fairly stateful: the Mellanox firmware API is based on CRUD operations on objects with opaque IDs assigned by the NIC. There are a bunch of different objects to manage: Send Queue, Work Queue, Completion Queue, Receive Queue, Receive Queue Table, Flow Table, Flow Group, etc, etc, etc, and each instance is assigned an ID by the card that needs to be remembered by the controller. So, on the one hand it is possible for each IO app to lock the NIC and perform updates (e.g. recreate Flow Tables to update MAC dispatching), but on the other hand this means defining a shared representation of all the relevant state.\nI also think that in the future some Snabb hacker somewhere will decide they want to interace with a NIC using a vendor-supplied software stack (as we have with Solarflare in the past) and that this software may not lend itself well to distributing control of the NIC between processes in a peer-to-peer style e.g. if the state we are manging are opaque data structures allocated by the vendor library that we have no way to transport between processes.\nSo, what do we think? How awkward will it be to separately define the IOControl from the IO? Does this awkwardness pay for itself in terms of simplifying implementation?\ncc @petebristow\n. Yes, I am on board with taking in the extensions to ingress drop monitor and running it on NFV.\nJust wanted to take the opportunity to muse on the future because the current implementation feels like a short-term expedient thing to me (e.g. will only detect network-sourced overloads and not VM-sourced overloads).\nI am also not sure yet on whether it's better to make global JIT flushes or selective ones or a combination of both. Interested in getting to the bottom of this and having a really robust solution. Some musing at https://github.com/LuaJIT/LuaJIT/issues/218#issuecomment-254448416.\n. Merged, thanks!\nI do think some of the commit messages are a bit terse :-) \"Tweaks\" could be \"Tweaks to performance-tuning.md.\"\nI note that numa.lua has no description of its API but I tend to see this as a symptom of a systemic documentation problem (is anybody really using the markdown docs instead of reading the source files? Github analytics suggests not more than one person per day.)\n. @kbara Do you think I should have required a README.md markdown document for numa.lua to merge?\n. @kbara (well, moot question perhaps, since the numa module issue is about markdown API documentation and you are saying that you don't tend to refer to that.)\n. @kbara (anyway, we can pick up the API discussion again in the future, and maybe the conclusion will be to keep doing what we are doing, or maybe we will come up with a new idea. Just now my problem is that I feel too bad about making markdown API documentation a hard requirement on merging changes because I'm not confident that people are really using it.)\n. I would have been comfortable asking for API documentation as comments in the source file, which are actually very valuable to me in practice, except that I don't feel that I have a mandate for this based on past discussions.\n. Groovy. Code looks good. (Github UI is not much help with a 10KLOC diff but Emacs magit-mode is awesome: makes it really easy to jump back and forth between the \"diffstat\" list of filenames with +/- lines and the actual changes.)\nJust waiting for my usual Hydra jobs (lukego/next-tributary and lukego/next-tributary-murren, evaluations 4108 and 4017) to confirm. Shouldn't be long.\n. Merged, thanks!\n. Thanks Andy :). 'Tis a pleasure to be hacking with you all!\n. Merged, thanks!\nI would love to get the usage rate on documentation up. My idea to create a PDF manual was a total failure.\nJust now I still don't find the HTML docs very appealing. Comes up really compressed in the left-hand side of the screen. However maybe that is just my bad taste. Screenshot in case it looks different on my computer:\n\n. @eugeneia Thanks for the screenshot! This helps me to understand the motivation for the layout. Having everything squeezed on the left-hand side lets you put it behind an editor on the right. I can see why you like that.\nThis does not work for me because I have different window-management habits. I probably need to build a separate interface to the documentation that works for me. Previously I tried that with PDF but I was not happy with the results. Ongoing process...\n. I am concerned that renaming the traffic policing parameters in the NFV config file (commit 16fba64a08bb0992959d4006fa46dad30106cefb) will break any existing configuration files outside the git  tree (actually to make them silently ignore the policing parameters.)\nIs it important to make incompatible changes to the NFV config file here?\n. @eugeneia I reckon old names as a deprecated fallback is good for now :+1:.\n. Great topic and very thought provoking! Ideas -\nProcesses\nI would like to clarify what the Snabb process tree will look like. Just to make sure that we all mean the same thing when we use terms like worker, supervisor, config process, etc.\nLet me make a proposal that does not necessarily represent reality on any specific branch:\n- The Main process is the initial one. This process calls the \"program\" entry point to bring up the application (including starting all related processes.) The Main process is non-realtime: it can run the engine but it is subject to the whims of the kernel scheduler. (Could run an app that answers ARP requests for example.)\n- The Worker processes each run the engine on a dedicated CPU core. These processes are started by the \"program\" logic in the Main process. They receive their configuration via some IPC mechanism (ideally based on a YANG model.)\n- The Supervisor is a tiny special-purpose process that runs some cleanup hooks on shutdown: removing stale shm/IPC files, disabling DMA on PCI devices, freeing HugeTLB memory, etc. The number of Supervisor processes is an implementation detail e.g. just one for the Main process or one each for the Workers etc.\nIn this model I imagine that the Main process could provide all general administrative interfaces e.g. serving snabb config, providing debugging interfaces like a REPL, and so on. So we would not need a separate Config process? (Could be that a long-running configuration operation would require spinning off some processing into an asynchronous child process though.)\nWorker config update protocol\nI have an opening proposal to shoot down here too :-) How about this:\nOver on #1021 I wrote a mechanism for a worker process to poll for configuration updates. The idea is that polling is cheap (check if a \"number of configurations\" shm counter has been bumped) and then the actual update is simple (can use the filesystem - syscalls to read from ramdisk should be affordable to process updates.)\nBack-channel\nCan you say a bit more about what the requirements are on the back-channel? (what is it for? is this the same thing that I mean by\"worker config update protocol\" or something else?)\n. > perhaps there is a ncie simple design lurking somewhere\nJust make facilities like config server, repl, etc be apps that run in the engine of the Main process?\n\nWe are shooting for thousands of these incremental updates/s and we need a channel for messages\n\nI see. Likely you need to use a special-purpose mechanism that makes sense to you, and the next person who has a similar requirement can propose a way to generalize it?\n. Just a note that I have checked for a potential conflict with \"process groups\" over on #1021 and I do not think there will be a problem.\nThe idea of \"process groups\" is that Snabb processes will form a tree with parent/child relationships. The implementation does not change directory layout of /var/run/snabb and should be compatible. Every process (parents, children, grandchildren, etc) has its pid listed in /var/run/snabb and so any process should be able to call claim_name().\nI suppose that as application developers we would need to decide which processes should be named e.g. only the parent or also the children. Seems like we can decide this later.\nI would welcome feedback on the process groups idea btw. Could be that it is ill-conceived or ill-described?\n. Does this PR make snabb top work with process names? That would seem like a really nice feature. I am not using snabb top myself and the reason is perceived difficulty of identifying the right process (solved very neatly with names).\nEDIT: when I say \"work with process names\" I mean for snabb top to be able to find a process based on its name being supplied on the command-line.\n. I am also concerned about the potential for mismatched Snabb versions to communicate via private interfaces that are incompatible. There could be many Snabb executables installed on a machine, $PATH could be different when a process is spawned via systemd vs managed via shell, software can be upgraded on disk while the previous version is still executing, etc. So I worry about this happening accidentally even if the policy would be that you should always use the same version.\nI am not sure of an ideal solution. One possibility could actually be for processes like snabb top and snabb config to try and respawn themselves using the same executable as the target process. If there is a Snabb process running with pid 123 then the kernel will provide /proc/123/exe as a symlink to the executable that this process started from. I am using this over on #1021 to try and spawn worker processes using the same software as the parent. The weakness I see is that a symlink is based on name rather than inode and so it seems like you will still get a mismatch of the executable has been replaced on disk.\nCould be possible to make a more fool-proof mechanism if the main Snabb process would immediately fork() a process that serves as a template for request processing i.e. could serve requests to fork new children...\n. @eugeneia Hard link to the executable would be nice but the implementation challenge is that unix hardlinks cannot span filesystems. So we cannot have a hardlink on the ramdisk (/var/run) pointing to the disk (e.g. /usr).\nCould consider copying the whole executable instead? In the context of #1021 this could go into the /var/run/snabb/$pid/group/ folder so we would only have one copy per process group. This would also make the core.worker process-spawning API robust to the snabb binary on disk being replaced while running.\n. Ready when you are @eugeneia!\n. Let's wait for v2016.12. I have already advertised next as ready for v2016.11 release in https://github.com/snabbco/snabb/pull/1060#issuecomment-258388939 so I prefer to only merge fixes to release-blockers at the moment.\n. Cool idea! The engine code today is super naive and I am sure that this sort of optimization will be very beneficial for larger app networks.\nI suspect that this optimization depends on some specific invariants about the app network. Is that right? If so then what are they?\nI suppose that if each push() method is called only once per breath then this could affect the behavior of an app network that contains a cycle. Contrived example would be an app network that loops a packet through a decapsulation app as many times as it takes to remove every encapsulation:\nsource -> decap -> sink\n          ^   |\n          +---+\nOn the master implementation this would fully process each packet every breath while on the optimized branch it would only process one decap step per breath, right? (This is not necessarily a problem but I would like to understand.)\nbtw this would probably make the \"cycles per app callback\" density graph over at https://github.com/snabbco/snabb/pull/1030#issuecomment-256485005 look nicer. Just now you can see a bimodal distribution on the calls to the Sink app: sometimes it has work to do (~4000 cycles/call) and other times it has no packets (~650 cycles/call).\n(NB: Those cycle counts include relatively high timeline logging overhead because this is an experimental branch that reads a bunch of PMU counters on each log message. RDPMC has reciprocal throughput 37 cycles according to Agner i.e. reading 7 PMU counters has latency 259 cycles.)\n. Great to be making this really happen! I am really liking the uniform interface towards so many different I/O drivers (each PCI driver, tap, raw socket, emulated). This is definitely what I want as a user.\nThe implementation is a bit sophisticated for my caveman tastes :-). It feels like a lot of cognitive load to take on new concepts like formula and virtual table, to have a Lua driver interface between modules in addition to the app interface, and to use fancy mechanisms like weak tables.\nCould we solve this problem in a stupider and more low-brow way?\nJust dreaming for a moment... one thought in the back of my head is that the IO app is basically a macro. It's a placeholder where a more specific app (or apps) should be substituted at configuration time. This seems like a substitution that could be made in some simple local way along the lines of sed s/IOControl/Intel82599/.\nFor context here is the approach I am taking on the Mellanox driver. The controller app initializes the NIC with the necessary queues and then creates shm frames containing all of the necessary information for a queue handler to attach and perform IO. Snippet:\nlua\n      -- Create shared memory objects containing all of the\n      -- information needed to access the send and receive queues.\n      --\n      -- Snabb processes will use this information to take ownership\n      -- of the queue to send and receive packets.\n      local basepath = \"/pci/\"..pciaddress..\"/\"..queuename\n      local sendpath = basepath..\"/send\"\n      local recvpath = basepath..\"/recv\"\n      local u64 = function (x) return ffi.cast(\"uint64_t\", x) end\n      shm.create_frame(sendpath,\n                       {lock     = {counter},\n                        sqn      = {counter, sqn},\n                        wq       = {counter, u64(swq)},\n                        wqsize   = {counter, sendq_size},\n                        cqn      = {counter, send_cq.cqn},\n                        cqe      = {counter, u64(send_cq.cqe)},\n                        doorbell = {counter, u64(wq_doorbell)},\n                        uar_page = {counter, uar},\n                        rlkey    = {counter, rlkey}})\n      shm.create_frame(recvpath,\n                       {lock     = {counter},\n                        rqn      = {counter, rqn},\n                        wq       = {counter, u64(rwq)},\n                        wqsize   = {counter, recvq_size},\n                        cqn      = {counter, recv_cq.cqn},\n                        cqe      = {counter, u64(recv_cq.cqe)},\n                        doorbell = {counter, u64(wq_doorbell)},\n                        uar_page = {counter, uar},\n                        rlkey    = {counter, rlkey}})\nSo in this case there is no need for the IO abstraction to share any state between apps via the Lua heap (queues table) because the Mellanox driver is responsible for doing that behind-the-scenes.\nOn reflection I suppose that this approach would have more impact on the existing drivers. For example if RawSocket needed to be split into a separate controller and IO handler then it would need to grow some code for sharing any necessary information between the two (e.g. for the IO app to discover the interface name that the IOControl was setup with so that it attaches to the right place).\nHard problem!\n. One use case that I find interesting: Can the IO framework provide a simple and uniform way for users to specify how to do IO on the command line?\nFor example supposed we had a tcpdump program in Snabb and we wanted to be able to capture on multiple processes and RSS buckets:\nshell\nsnabb tcpdump tap=tap0\nsnabb tcpdump raw=eth0\nsnabb tcpdump pci=01:00.0\nsnabb tcpdump pci=01:00.0,rss=4      # four worker processes\nsnabb tcpdump synth=64,100,250,1500  # synthetic traffic mix\nCan we cook up a good syntax for this? And a straightforward mapping onto a collection of IO and IOControl apps running in different processes?\nCould be that solving this problem will lead us to the optimal interface for the IO apps. I am not sure.\n. ... Just continuing the same train of thought. On the one hand we could have a syntax for specifying a whole tree of processes:\nsnabb tcpdump pci=01:00.0,rss=4\nand on the other hand we could specify each process separately:\nsnabb tcpdump pci=01:00.0,rss=4\nsnabb tcpdump pci=01:00.0,rssbucket=0\nsnabb tcpdump pci=01:00.0,rssbucket=1\nsnabb tcpdump pci=01:00.0,rsbuckets=2\nsnabb tcpdump pci=01:00.0,rssbucket=3\nand it may even make sense to support both styles. Snabb could internally operate as a collection of discrete processes, like in the second example, but Snabb programs may want to provide a simple command-line syntax that automatically spawns the processes just to save the user extra typing, like in the first example.\nEDIT: Could also make sense to add CPU core assignments to this example since that is also an aspect that we have suggested making the user assign explicitly.\n. Just continuing to continue...\nCould also be that command-line syntax is the past and we should start looking to YANG based configuration files now. I wonder if that would be sensible or overkill for applications like packetblaster and snabbmark? WDYT @wingo?\n. (The reason for this digression into command-line and application configuration is that I see this as the primary user of the IO and IOControl abstractions. The main purpose of this IO abstraction is to make it easy for users to say what I/O interfaces they want to use and how they want that divided up between cores. Just now my concern is that we are taking a step backwards from @petebristow's RSS interface on the intel_mp driver in terms of user friendliness and it would be nice to minimize that.)\n. Wow! I really like this new configure() mechanism.\nLooks to me like the user-facing API for creating apps is exactly the same but behind the scenes each app class now has the possibility to customize how it is instantiated. So when you write:\nconfig.app(c, name, myclass, args)\nthis can call myclass.configure(c, name, args) which can make an arbitrary update of the app network e.g. add zero, one, or many apps. This way an app like io can be a \"macro\" that expands by considering context (e.g. PCI device type) and then substituting one or more appropriate apps (e.g. Intel82599, ConnectX4, Emu, etc.)\nThis seems like a tasteful choice of mechanism. The alternatives would be to make the user explicitly call the configure function, which would not be consistent with current usage of the config api, or to call configure() from inside the engine where the mechanics might be more complicated.\nQuestion: Could we put all of the smarts directly into IO:configure() and skip the whole business of having a framework for registering PCI drivers and making each PCI driver implement a separate configure() method of its own? This could make the code simpler by reducing indirection and also potentially make it easier to spot differences/incompatibilities in how the IO is mapped onto the various drivers.\n(Could also be worth spinning out the configure() method on its own branch and PR'ing that for wider feedback. Could label as [sketch] to avoid that branch accidentally being merged instead of this one.)\n. Sounds sane!\nOne possibility that I think is worth considering is for snabb config to use a private protocol too. This would require snabb config to run exactly the same software version as the main process but that is a problem that we already need to solve internally for starting workers anyway.\nThe upside would be that we can change the interface in the future with impunity. The downside would be that all tooling would need to make requests via snabb config rather than implementing the protocol independently.\nThe mechanism could involve snabb config identifying the executable that the main process is running and using exec() to force itself to adopt that same software. Here is pseudo-code for what snabb config might do at startup:\nshell\nif [ $0 != /var/run/snabb/$mainpid/snabb ]; then\n   # Restart this process with the matching software version.\n   exec /var/run/snabb/$mainpid/snabb \"$@\"\nfi\nThis would require the main process to have copied its own executable into its shm folder. Seems like we will need to do something equivalent to that in any case for worker spawning though. I am asking stack overflow for a better solution. (I have looked into reconstructing the executable by introspecting our own memory map from /proc/$pid/maps but I suspect this is a bit complicated and ELFey with multiple segments and stuff.)\n. @eugeneia Initially I had imagined a one-size-fits-all snabb worker <name> <parent-pid> program that simply executes an app network provided by its parent. Nice and simple and would work fine for snabbnfv. However, @wingo indicated that this is too simplistic for the lwAFTR use case where they need to rapidly process updates to large configurations.\nSo I modified the core.worker module such that the worker executes arbitrary Lua code and so the program spawning the worker can decide exactly what it should do. I have not looked closely at how the config leader/follower apps work yet and I would not be surprised if they are overkill for other programs like snabbnfv with small configurations that seldom change.\n. I don't know what the best solution is but braindump upon request:\nHaving a completely general follower program snabb follower <name> <leader> that derives all of its behavior from leader-supplied configuration (am I an lwaftr? am I an l2vpn? etc) seems optimal to me. Then application developers could reuse that and not bother about multiprocess plumbing. However, I tried this with the initial snabb worker API and missed the mark so I can believe that this level of generalization could be premature (or indeed the wrong idea after all.) I would be thrilled if this design turned out to be practical using the leader/follower routines though!\nSo for now the multiprocess model is that the main and worker processes will run application-specific code and be responsible for their own configuration updates.\nThis brings us to the leader and follower routines for exporting a config from one process to others. These routines have to be called somehow and it probably does not matter exactly how. I suggested an app only because apps seem like the most well-defined units of composition that we have in Snabb. Could also run it from the code that invokes the engine (via done callback) or via timers and to me each approach is more-or-less equivalent but with minor pluses and minuses.\nI have not actually looked at the implementation closely yet and I do appreciate the potential for problems for when updating the engine config while the engine is running. This is a minus to using apps that I had not anticipated.\n(I do somewhat regret introducing timers in the first place. I see them as basically awkward and redundant with respect to apps. I initially wrote that module due to lack of imagination because every system I had worked on before always used a timer wheel somewhere and it seemed like we should have one too.)\n. Thanks for the explanation @wingo. I will merge onto next. If somebody contributes a test case showing problems with the recursive implementation before the next release then we may have to consider a revert but this seems unlikely based on quick happy testing.\n. @wingo Thanks :). I don't see a reason to make this change an exception to the usual workflow of having a maintainer to approve the change by merging it onto a tributary branch. (I think it's important that I don't e.g. merge this directly onto master because that would effectively be making a new Snabb release.)\n. Thanks Kat for humoring me and my efforts to keep the workflow decentralized even when it is admittedly a bit comical :).\n. @wingo The use case driving this is to export information from the main process that initializes a 100G NIC to the worker processes that have to attach to queues. Code: https://github.com/snabbco/snabb/blob/mellanox/src/apps/mellanox/connectx4.lua#L148-L176\nCurrently this is implemented via shm counter objects. However, it is a bit funny to use counters because these values are not logically counters (e.g. address of a DMA ring), the values are not always uint64_t, and counters are double-buffered (have to remember to counter.commit() to make them visible.)\nIs there an lwAFTR mechanism that we could reuse instead already now?. @wingo The goal is to make instances of the Mellanox IO app automatically synchronize with each other even when they are running in separate processes. The information being shared is emphemeral operational data e.g. the physical address of each DMA ring and whether it is currently online. This is currently invisible to the application & app network - it is internal state of the Mellanox apps that happens to span multiple processes.\nThe most closely related public discussion is #1068 the \"IO\" mechanism i.e. the desire for drivers to be able to run distributed across multiple processes that all want to send/receive traffic on the same network interfaces. Currently we have put a simple interface on the IO apps and expected their instances to coordinate behind the scenes somehow.\nCould alternatively model each piece of operational state with YANG and export that with Leader/Follower. So the YANG schema would have objects for e.g. the current physical address of the DMA ring for queue 7 and the index of the last successfully transmitted packet. This could then be provided to the app instances as configuration state. Is that what you have in mind? Just now this seems like taking YANG modeling a step too far - modeling state that apps actually want to hide internally - but this may be lack of imagination on my part.. @wingo I simply want to define some variables whose scope spans process boundaries. The existing shm mechanism seems like the natural choice except that its current set of user-defined types is sparse. Just now this seems orthogonal to YANG and channels: that is for synchronizing application-level configuration but what I am synchronizing is the internal state of some apps.\n@eugeneia I suggest that we take @wingo's feedback as a NACK and I will cherry-pick any changes that I need from this branch into the mellanox driver directly. Then we can consider the issue again if/when we need to propagate this functionality into the other drivers.. @wingo Just reflection: we need to establish the scope of use cases for YANG/Leader+Follower/Channels/etc. Where does the YANG universe start and end? Lots of objects in Snabb - program configuration, app networks, app state, histograms and timeline logs, LuaJIT state, etc. It would be nice to have a common view on when to use these mechanisms and when to do something else.. I understand that point of view. Shared memory variables are too low-level to be a satisfactory general solution for synchronization.\nThis is a relatively simple use case. Most of the variables are written only once, at creation time, and then used read-only. In this case shm seems simpler to me than channels because it is passive state (named values) rather than moving parts (sending messages on ring buffers). I also need to be able to handle simple exceptional cases e.g. for the worker to be able to save a tiny amount of precious state when it shuts down so that the next instance can reattach to the rings correctly.\nNo problem to keep this local in the driver for now. I do like this strategy of keeping code outside of core until it ceases to be controversial.. Merged, thanks!. > Can anyone take a guess at what is happening here?\nFirst let's try a bit of \"napkin math\" to see what the expected latency would be.\nSnabb programs typically have ~10 Mpps of processing capacity i.e. spend around 0.1us per packet. One event loop iteration (\"breath\") typically processes up to 100 packets from each I/O source. So under heavy load a snabbnfv could process 100 packets from the NIC and 100 packets from the VM and then the expected processing time would be ~20us.\nIf the packets were processed three times (snabbnfv -> vm running snabb -> snabbnfv) then that could triple the latency to ~60us. If the NIC were congested on ingress and/or egress that could also increase the \"end to end\" latency if there is a delay between packets being processed and hitting the wire. On the other hand if the traffic load is low then each processing step should take less time.\nSo - if we would estimate that latency should not exceed 60us even under moderate load then your observations seem suspiciously high and worth investigating.\n\nMy best guess would that this is a result of trying to avoid a busy wait loop.\n\nThis is an excellent hypothesis. Let's test it with more certainty to begin with.\nSnabb does have a heuristic for detecting idleness and asking the kernel to sleep. This can reduce CPU utilization but at the expense of less steady latency. Since you are interested in latency you should disable this and run Snabb in a busy-loop instead.\nThis can be done on snabbnfv with the -b command line option. This can be done in your test script inside the VM with the setting engine.busywait = true.\nCould you try that and see what the impact is?\nThinking ahead, one thing that we could try would be to merge in the \"timeline\" log support to your test branches. This would produce detailed samples of the event loop including cycle-precise timestamps. Just now some DIY is required to interpret these e.g. make sense of them as CSV files so it is only worth doing if you are up for that. (User-friendly tools are coming down the pipeline, see #1011).. Side-issue: For me it is invaluable to be able to restart the vswitch without having to restart the VM while experimenting. This \"just works\" with Snabb if you apply a small patch to QEMU. Just mentioning in case you are restarting your VMs every time and finding that frustrating. (Sadly upstream QEMU did not seem interested in this feature the last times I checked but maybe somebody has solve that more recently, if so maybe you can tell me :)). This sounds more reasonable.\nSeems like our \"idleness detector\" is lousy. First guess is that the VM is seeing false-positives on the idle check because (a) VM event loop is running at a higher frequency than snabbnfv (doing less work) and (b) snabbnfv is making packets available in batches of ~100 and so sometimes the VM will see the ingress queue as empty because the batch is not complete. (In this case it could be that our vhost-user app should poke the virtio ring buffer \"packets available index\" more frequently to reduce latency and burstiness from the VM perspective.)\nIf you want another snabbnfv tunable to experiment with you could play with engine.pull_npackets which determines the RX burst size from the NIC. The default is ~100 but this may be a little large. If you play with (say) 80 +/- 20 you may see an impact on latency and throughput. I would like to make this auto-tuning but don't know how to formulate the right heuristic yet.\nThere is no formal command line argument for tuning this but you could use snsh to override it during startup. Instead of snabb snabbnfv traffic ... you could write e.g.\nsnabb snsh -e 'engine.pull_npackets = 80' \\\n      -p snabbnfv traffic ...\n. Merged, thanks!. Sorry all for being so slow on this and holding up the release train. @eugeneia has pointed out a performance regression in the Hydra tests and I am taking the time to develop new \"timeline\" tooling to make these easier to diagnose. On the one hand it is great to be able to bisect performance problems using Hydra benchmark campaigns, on the other hand I would like to extract much more information from each test run.\nPlease bear with me this month while I push forward on the new tooling. I think it will be much more effective when we can see exactly where a performance difference comes from: which part of the engine, or which app, or which difference in workload, etc. Here is an early screenshot to show the kind of information I have in mind for being able to compare between multiple Snabb versions:\n\n. @eugeneia This should be good to go now! @wingo fixed the performance regression with #1102.. So! The timeline code didn't land in this release. Sorry about that @kbara. It's actually working well now but it's also evolving quite quickly - particularly the content of the messages that the engine logs - now that I am really using it. I'd like to stabilise my initial usage a bit before pushing it upstream. Have to make it easily accessible too: make the R tools easy to install and/or make Hydra run them automatically and publish on the web. (I'm also going to build a GUI for this but that's a medium-term thing.)\nI also wrote a new sampling profiler for LuaJIT (\"vmprofile\") on the weekend and I'm finding it indispensable already. The idea is that it very efficiently profiles the Snabb process at all times and stores the samples in the shm folder where they can be accessed both while running and after termination (if preserved.) So profiling would be \"always on.\" I have to make a PR out of this and send it up... meanwhile the basic mechanism is described at https://github.com/LuaJIT/LuaJIT/pull/290.. @eugeneia Interesting. The selftest for core.app is failing for the new breath-sorting code. It works for me when testing manually with snabb snsh -t core.app. The backtrace says that it is this assertion at core/app.lua:497 failing for SnabbBot:\nlua\n   assert(#breathe_push_order == 2)\nI don't immediately understand why this assertion would work for me on the next branch but not for SnabbBot. I see that SnabbBot also saw the same problem with #1102. @wingo any idea?. @eugeneia I was wrong about the problem only occurring on davos. Good. I made a fix in #1108 and I will merge that here if it passes the sniff test from @wingo.. @eugeneia I merged the selftest fix. Hopefully good to go!. @kbara I should have merged this already. Sorry about that. Is it okay to take this immediately after the imminent release?. I reckon that a nice first step would be to isolate whether the duplication happens internally in Snabb or due to some interaction with the operating system. Raw sockets are reasonably funky.\nCould you try reproducing the problem while running Snabb like this:\nstrace -f -e open,close,read,write,ioctl,bind snabb ...\nand making a gist of the output?. I am really busy at the moment, sorry! The answers should be in ljsyscall and Linux man pages.. Cool! I created a new label called \"portability\" to tag changes like this that will benefit future porting efforts.. Great perspective @plajjan!\nAgree that it's important to focus on what is unique about Snabb e.g. being able to create practical network equipment without already being a grizzled system programmer.\nHere is a new take on the idea. Just speculating that array-oriented operations are something that we may want to do more of in the future.\nSnabb apps typically look something like:\nlua\nwhile not link.empty(input) do\n   local p = link.receive(input)\n   local nexthop = route(p)\n   ...\nend\nHere we have many packets available at the same time but for the sake of simplicity we process them one at a time. Now suppose we had a new library for routing packets and that it operates on many packets at once. One simple way to hook that in would be to put some array/vector operations at the start of the loop and then stay packet-at-a-time on the inside. For example:\nlua\nfunction push ()\n   local packets = link.receive_all(input)   -- pre-receive all packets\n   local nexthops = routerlib.route(packets) -- pre-lookup all routes\n   for i = 1, #packets do\n      local p = packets[i]\n      local route = nexthops[i]\n      ...\n   end\nend\nSo likely incorporating batch-mode libraries is not really a hard problem when we are working in a general purpose language and the engine is feeding us batches of packets already.\nJust continuing the pseudo-code braindump :-) here is a whimsical idea. Suppose you were building a simple router that wants to sort packets into categories (\"arp for me\", \"ICMP for me\", \"IP to forward\", etc.) Could be fun to build a little pfmatch style dispatcher that sorts the packets into separate arrays for processing:\n```lua\nlocal myip = '10.0.0.1'\nlocal mymac = '10:20:30:40:50:60'\nlocal d = pfdispatch(arp  = \"ether dst ${mymac} and arp  and arp dst host ${myip}\",\n                     icmp = \"ether dst ${mymac} and icmp and ip dst host ${myip}\",\n                     fwd  = \"ether dst ${mymac} and ip   and not ip dst host ${myip}\")\nfunction push ()\n   dispatcher:load(input)\n   for p in d.arp do\n      -- reply to arp\n   end\n   for p in d.icmp do\n      -- reply to ICMP etc\n   end\n   for p in d.fwd do\n      -- forward IP packets to next hop\n   end\nend\n```\nJust thinking that this programming style of first sorting packets into categories and then processing them has some of the nice characteristics: sorting can be a highly optimized operation (in the spirit of a TCAM); LuaJIT will generate separate code for each for loop (easier to think about); performance of each loop could be measured via the timeline log (e.g. get cycles/packet for ARP vs ICMP vs IP separately.)\nProgramming is fun... :-)\nThanks for the reminder to look at P4 too. I really must look closely. I also have the impression that the Vector in VPP is not really in the sense of an array/vector programming paradigm but closer in nature to Snabb struct link i.e. a set of packets that are all at the same stage in a high-level packet-processing network. I have been saying \"array\" rather than \"vector\" to avoid confusion there.. Thanks for the kind feedback @FongHou!\nTerra looks interesting. Lately I am happy simply programming though and I don't feel the need for a metaprogramming language. Snabb-wise I think a great strength of LuaJIT is that you can learn the language so quickly by reading Programming in Lua. If you already know Python/Ruby/Scheme/Smalltalk/etc then you can get up and running very quickly. I am not sure the same is true of \"high brow\" languages like Terra, Rust, C++, Haskell, etc but maybe that's my own background speaking.\nJust need to write an excellent HOWTO for making the tracing JIT do what you want it to & keep fleshing out the associated profiling and benchmarking infrastructure :).. Howdy!\nFor the second question - make Snabb run as a bridge between VMs sharing an ixgbe port - check out the snabbnfv getting started.\nFor the first question - make a Snabb script that writes two VM virtio-net ports together directly - you can write a script that creates to VhostUser app instances and run this with snabb snsh myscript.lua. Give a yell if you have more questions about this :). Have you seen the HOWTO linked above? That goes into a lot of detail about running all the bits including QEMU. For me it is always crafting the QEMU command line that is the hardest part.... Upstream should be fine, except that if you restart Snabb you may also need to restart the VM.. @vmaffione which QEMU version are you using? Currently our CI is testing interoperability with several upstream QEMU versions (2.1.3, 2.2.1, 2.3.1, 2.4.1, 2.5.1, 2.6.0, and our patched version) and seeing exactly the same behavior/performance with all of them. However, this list needs to be updated to include the latest QEMU releases and it is possible that they have made some changes that are not backwards compatible (Virtio-net is a moving target and version 1.0 was finalized after we wrote our code.). Thanks for the report. I will add the latest QEMU versions to our CI coverage as the first step towards fixing support and ensuring that it works going forwards.. @vmaffione I updated the list of QEMU versions to test and the CI is running that now. This triggered around 2500 new test runs that take around one minute each (booting VMs and sending traffic through snabbnfv with iperf or DPDK) and are spread cross 10 servers. We can check the result once it completes and maybe identify a compatibility problem between Snabb and newer QEMU.. (The reason it takes so many tests is that it is checking many permutations of configurations and also software setup inside the guests e.g. many versions of DPDK for the tests that need it.). One CI extension that we have talked about but never done is to also test the master branches of projects like QEMU and DPDK so that we are aware of any interop issues before they are released and have the chance to make a fix ahead of time and/or engage with them to understand what is going on. But - CI ambitions are infinite... :). Odd. The tests all passed including with QEMU 2.7.1 and 2.8.0. This is testing with the unmodified sources from the upstream QEMU release tarballs.\nHere is one randomly chosen set of logs (stdout) from two QEMU VMs connected by a Snabb process and running iperf in between: logs. Can you spot what might be a difference compared with your setup? (Are you using the upstream QEMU or a vendor branch?) If the difference is not clear could you please gist more complete information including the full QEMU command-line and output?. Interesting. I don't see anything relevant in the QEMU 2.9.0-rc release notes. I also don't immediately see a reason when browsing the commits to master:\nqemu$  git log --oneline v2.8.0..master | grep vhost-user\n0c0eb30 tests: fix vhost-user-test leaks\ne7c83a8 vhost-user: delay vhost_user_stop\n79cad2f qemu-options.hx: add missing id=chr0 chardev argument in vhost-user example\ne0b283e vhost-user: delete chardev on cleanup\nc5f048d vhost-user: Add MTU protocol feature and op\n2858bc6 virtio: avoid using guest_notifier_mask in vhost-user mode\ne10e798 tests/vhost-user-bridge: use contrib/libvhost-user\n7b2e5c6 contrib: add libvhost-user\n98206d4 tests/vhost-user-bridge: do not accept more than one connection\n9652f57 tests/vhost-user-bridge: indicate peer disconnected\n4e4212d tests/vhost-user-bridge: remove unnecessary dispatcher_remove\n3d1ad18 tests/vhost-user-bridge: remove false comment\nThe error message strikes me as a little suspicious. Are you sure this is coming from the vhost-user subsystem? It kind of sounds like it is from trying to talk with the kernel where it would like to use /dev/vhost-net acceleration but that fails so it falls back on read()/write() from userspace. Could possibly be a bug introduced on the master branch that breaks argument parsing so that you are not really testing vhost-user even?\nIf you have the patience it would be interesting to git bisect to identify the relevant commit on QEMU.. The curious thing is that our CI is testing 9 different versions of QEMU i.e. our old fork + the last 8 major upstream releases without any patches. None of these is failing in the CI tests. So I need to double-check that the CI is really testing what it is supposed to.. @vmaffione It would help if you can verify that the problem happens with a released version of QEMU like 2.8.0. Just to rule out the possibility of interference from a recent change in master that is not included in the tests that I am doing.. Here is 2.7.1: http://download.qemu-project.org/qemu-2.7.1.tar.xz. Hm... I see something suspicious in the CI that it may be building the right QEMU but actually running the test with the standard packaged version. In that case the CI is not really testing what it thinks it is. @domenkozar do you agree with that assessment based on the runtime dependencies graph showing qemu-2.5.1 on a test that is supposed to be using 2.7.1?. Thanks @vmaffione for the extra details. Could be that we are lagging badly on QEMU support due to a CI error. Looking into it.. @vmaffione I have manually tested 2.7.1 and 2.8.0 now. 2.7.1 is working for me but 2.8.0 is showing the faling back on userspace virtio error message. So - thanks for persisting with the report. This seems to be both an interop problem between Snabb and QEMU 2.8.0 and also likely a CI bug since the problem is not visible there (but cannot yet rule out something else e.g. related to peculiar command line arguments.). @mwiget Nice procedure! On reflection my suggestion is only that we include the shm directory in support dumps and not that it be the only thing we capture.\nI really like the idea of picking up the Snabb binary. I am actually meaning to write some code to copy that into the shm directory too, mostly to make the core.worker process robust to software updates while running, but also with the advantage you mention if this means it is included in support info.\n(I have half wondered if it would even make sense to compile the source code into the Snabb binary instead of the bytecode as we do now. This could also be very useful for \"deep support.\" Could have it as a build option for people who prefer to obfuscate that e.g. in a proprietary product.). Good writeup! To me it makes sense to keep snabb minimal but to be more liberal with related tooling e.g. tests, benchmarks, log analysers, etc. It's definitely important to nail down the dependencies but this can naturally by done by making them run under Hydra.\nSo \ud83d\udc4d from me on writing tests suites and related tools using the technologies that you consider most appropriate and providing a nix expression to make them run with the right dependencies. This way we can always run the tests under the Hydra CI with consistent dependency versions, lazy people like me can use nix to install the dependencies automatically in development environments, and people can also use any other methods they like personally e.g. apt-get, yum, docker, make install, etc.\nSound reasonable?\nFurther braindump...\nI have been pondering this quite a bit lately because I am working on Snabb-related development tools that I want to write in R and Pharo - domain-specific tools for statistical analysis and interactive visualization - and I will need a solution for making these easy to run including dependencies. I see nix as the solution here but I am working out the exact details.\nFor one simple example, I have an R program called timeliner that reads a Snabb \"shm\" folder and produces statistics and visualizations. The basic source code for this program looks like:\n```\n!/usr/bin/env Rscript\n... R code ...\n```\nand the issue is that it only actually runs if you have a suitable version of R installed and also suitable versions of all the R libraries that my code depends on. This is potentially a problem because I really have no idea which Linux distros (let alone OSX) satisfy that criteria. So a simple solution is to make a timeliner-nix wrapper script that uses the nix-shell shebang trick so that nix provides the necessary dependencies. This is a simple two-liner:\n```\n!/usr/bin/env nix-shell\n!nix-shell -i Rscript -p 'with pkgs; with rPackages; [ R dplyr ggplot2 bit64 ]'\n... R code ...\n```\n... and so if you are in doubt about the dependencies you can just install nix and run the timeliner-nix wrapper to automatically get the same versions that I am using. If you are sure you have the dependencies right then you can run timeliner directly instead and you don't need nix.\nThat's not as convenient as deploying Snabb - one binary with no dependencies - but seems like a reasonable compromise for optional development tools.. Great stuff :). Thoughts - mostly using this PR as a vehicle for revisit the way we deal with dependencies in Snabb in case we can improve and/or clarify our strategy.\nCurious: Did you consider the trade-offs between making nDPI a runtime dependency (shared library installed by the user) vs a build-time dependency (static library linked into snabb binary)? If so then how did you see the pros/cons and what lead you to choose the former? (Just curious because this will be a recurring theme for other people building Snabb applications that have external dependencies who may be able to benefit from your experience.)\nI would prefer to upstream the real development branch rather than a squashed version. If every branch has the same view of the commit history then Git will be a powerful tool for keeping them synchronized even in exotic scenarios e.g. if we want to make a sweeping refactoring based on master that touches snabbwall and other applications. Having a squashed history would make the master branch a second-class citizen: if we touched any of the code that we don't have the full history for then we would end up with hard-to-resolve conflicts.\nRelatedly: we have been squashing history of projects that we embed with git subtree e.g. LuaJIT and ljsyscall. This may be limiting in practice. I would like to be able to pull from multiple branches of the subtrees and end up with the union of all the commits. For example on LuaJIT we may pull v2.1 for general bug fixes but also pull several other branches for specific features. Ordinarily git merge is smart about combining changes from multiple branches but with git subtree pull --squash it seems to me like the branches end up overwriting each other i.e. pulling from a new branch will revert code that was previously pulled from other branches. Could be that preserving the history is the solution. (This is bothering me at the moment when I want to pull an experimental change to LuaJIT for testing on Hydra but git subtree expects me to rebase it onto exactly the same LuaJIT parent commit that Snabb most recently merged.)\nSo: Snabbwall looks good to me and I'd just like to take this opportunity to revisit how we handle dependencies and thrash out the best workflow. Sorry about that :).. P.S. Cool that you already pushed nDPI support into nixpkgs :-). This will take care of the dependency from a CI perspective.. One more topic: How should the snabbwall branch flow upstream? Should you PR it directly to next or via wingo-next, kbara-next, or max-next? These paths all lead fairly swiftly to master so it's mostly a social matter of who will do integration work like resolving conflicts from other branches and checking CI performance results.. Groovy.\n@aperezdc Happily I think we will be able to use git subtree pull without --squash to start including the history of our subtrees going forward, even though the older commits have been squashed. I am meaning to pull the latest LuaJIT and ljsyscall so I will try doing it this way and see how it looks.. So! Here are my proposed next steps @takikawa:\n1. Pick a long-lived branch where you will maintain snabbwall. Here people can pull the latest and greatest changes and they can also send pull requests for you to consider. You are welcome to host this branch on the snabbco repository or you may prefer to put it on Igalia or your personal fork or create a snabbwall Github organization or ... etc. Can also change your mind later. Just please make this branch \"protected\" so that people can safely pull knowing it won't be rebased later.\n2. Send a PR updating src/doc/branches.md with a description of your branch so that everybody knows it exists and where to find it.\n3. Eventually, find your next-hop upstream. Just have to find somebody who is willing to review & merge your changes onto a branch that is feeding into master. Sounds like kbara-next or wingo-next is the most likely candidate since you already have a working relationship with them. You guys can agree this privately between yourselves -- just let me know if there is a problem you need help with (and please record what you decide in branches.md.)\nThat should be all. In principle this process is documented in src/doc/git-workflow.md though this may require some revision and shortening in practice.\nSound okay?. Cool!\nFrom past discussions the path upstream for new ports is first to maintain them on a branch and then to bring them upstream when they clearly represent a \"net win\" for the community. So if you are interested in driving an ARM port forward the first step is to create a branch that is \"open for business\" i.e. where you are accepting pull requests and merging new releases from upstream etc. If you (or anybody else) would like to do that the basic method is to update doc/branches.md to list your branch there & send a PR of that update to advertise it.\nI think that ports have wonderful potential to bring in more developers, let us deploy applications on more platforms, encourage us to take a more disciplined approach to hardware dependencies, and so on.\nI am curious about a bunch of aspects of an ARM port... do we need new NIC drivers to make this practical? (Is ARM64 usually an SoC? what ethernet controllers do they use?) How do we test the port on CI? (Do we run the tests in QEMU? Do we acquire ARM servers? Do we use an ARM public cloud?) and so on.\nI also see costs for ports and the benefits need to outweigh these before we can merge a port onto the master branch. New architectures can introduce complexity like having to write code compatible with relaxed memory models, optimizing for divergent microarchitectures (e.g. both in-order and out-of-order execution), can be a disincentive to using special features of x86 like the TSC clock and performance counters, can be a disincentive to writing assembler code which some of us quite enjoy :-), and can be a problem for users if in practice it is hard to predict which features work on which platforms, and so on. Quite a basket! My feeling is that a well-maintained port for a practical platform can provide more than enough benefit to outweigh these costs, but it's definitely a high bar to clear before IMHO it makes sense to merge onto the master branch for all Snabb users.\nEnd braindump! Great hacking :-) I know that a bunch of other people are interested in ARM64 too.. (Would also be interesting to know why you are interested in having an ARM64 port of Snabb :-)). @jialiu02 Sounds great!\nPlease pick where you want to maintain this branch - you can host it on this repository, or on your own fork, or on a new Github organization, etc, as you prefer - and send an update to branches.md describing the branch and letting everybody know that it is open for business.\n(I'd suggest hosting the branch on this repo because then the community will have visibility of the PRs. However you may prefer another location if you want to build a sub-community, want to have direct control of the Github hooks, etc. It's up to you and you can change your mind later. See also related branch creation advice at https://github.com/snabbco/snabb/pull/1109#issuecomment-286693846.)\nAside: I am personally curious about what is driving the interest in ARM64. Is it users who want to deploy on ARM64? Or hardware vendors who want to bootstrap demand for ARM64 products? Or large customers who want to leverage ARM64 as a BATNA in price negotiations with Intel? You don't have to answer this, but further down the line I think the master branch will put the most weight on the needs of the first group i.e. people who are looking for the most practical deployment platform for Snabb applications. There are plenty of other projects like DPDK for vendors to use as battle grounds :).\nOnward!. > I'd love to see an aarch64 port of Snabb\n@kbara Just curious: why? I can see the costs of supporting multiple architectures but what are the benefits?. > And it's better to create a new branch after you finish merging luajit v2.1 because we need some new fixes in it for aarch64.\nNote: On the one hand you can wait for the luajit 2.1 update to land on master (couple of weeks or so) but on the other hand you could git merge lukego/luajit-upgrade-history to pull those changes in right now without having to wait.\nOverall the intention is for master to be the \"trailing edge\" that keeps all other branches eventually consistent. It's not supposed to be on the critical path for new feature development - at least for features that are urgent.. @kbara Cool. Yes, I would love to have a solid arm64 port living on a branch, and to merge that when there are some ARM64 platforms that kick Intel's butt for some interesting applications. Meanwhile x86-64 is improving all the time and so the bar is rising too :-)\n(I have a weakness for RISC-V and lowRISC but that's too far down my priority stack to play with...). Here is a Hydra run showing that the performance impact of the new LuaJIT changes from upstream is neutral: https://hydra.snabb.co/build/1077739/download/2/report.html.\nHave to get Hydra automatically testing PRs!. @kbara Thanks but I have merged it directly onto the luajit branch and I will take it to next from there. If we decide we want more hands and eyes on maintaining out LuaJIT fork then we can manage that by sharing the luajit branch somehow (e.g. multiple committers and LGTM convention or spinning it off to a separate Github organization or ...).\nJust thinking it will be easier if over time the branches become more topic-specific (e.g. LuaJIT) and reviewers work in smaller groups with common subject matter interest (e.g. luajit, aarch64, etc.) This way we won't spread the *-next branches too thin (still acting as a valuable catch-all of course.)\nFeeling the way along as always... shout if I mess up!. (This is actually a really awful graphic. However it should serve very well to contrast with a good one. I'm working on that now. The important missing ingredient above is showing the variance.). So! Please indulge me in some experimenting aloud.\nThe weakness of the graph above is that it is only showing averages over one second. This leaves interesting questions unanswered. For example, what were the typical numbers of packets processed in each breath? The timeline contains detailed information that can be used to answer these questions. The challenge is to find a suitable visualization for showing more about the distribution of values than simply their average.\nThis is actually quite important in the timeline I am using as an example. The graphic shows that the packets/breath burst size average around 10-30 packets. However, check out this excerpt from the log entries:\npackets bytes cycles cycles_per_packet\n1        6  3048   1869          311.5000\n2      166 84328 121259          730.4759\n3        0     0   1506               Inf\n4       10  5080   2222          222.2000\n5        0     0   1317               Inf\n6        0     0  48941               Inf\n7        8  4064   1814          226.7500\n8        0     0   1407               Inf\n9        0     0  33948               Inf\n10      25 12700   2527          101.0800\n11       0     0   1356               Inf\n12      14  7112  21985         1570.3571\n13       2  1016   1754          877.0000\n14       0     0   1415               Inf\n15     168 85344  93893          558.8869\n16       0     0   1425               Inf\n17       1   508  14519        14519.0000\n18       2  1016   1751          875.5000\n19       0     0   1285               Inf\n20       0     0   1534               Inf\n21       0     0   1299               Inf\nHere we see that each breath is different. Most of the time there are no packets available but occasionally we get over 150 at the same time. So we can speculate that there is some burstiness in our I/O sources (in this case a VM running DPDK l2fwd and a ConnectX-4 100G NIC.)\nHere is a first attempt to illustrate the distribution of values for each metric:\n\nThere is a lot of information here:\n- The average (mean) is plotted as before: solid lines connected by dots.\n- The median value is plotted as a dotted line.\n- The 25th to 75th percentile ranges are plotted in dark background ink.\n- The 5th to 95th percentile ranges are plotted in light background ink.\nThis puts some things into a different light.\nLooking at packets/breath:\n- The median value (dotted line) is close to zero. This means that on most breaths there were no packets available to process i.e. we polled our I/O sources and came up empty.\n- The 95th percentile (top edge of the light blue area) is waaay above the mean. This suggests that when we do find packets available to process there are usually quite a lot of them (~100-200.)\nThis suggests a very specific way to approach performance optimization. There is no point breaking out the code profiler and trying to shave off CPU cycles: we are already processing the packets faster than they are arriving from the I/O sources. However we should look closely into why we are receiving packets in batches like this and how we can influence this (could it be a feedback loop involving our own internal batching too?)\nCool to build tools! I don't think this visualization is quite the right one but it seems to be on the right track. (I did find an absolutely gorgeous example of showing every quantile value in a band but it's a bit hard to reproduce.). Perfect, merged! This PR will automatically close on the next release when the change lands on master.. Good catch! I merged this onto next and added a note about the Slack chat.. Awesome work!!!!. Linking in your own C code at runtime (.so) or compile time (.o) is generally no problem.\nThis would usually be done using the LuaJIT FFI which does not require your shared object to be linked with Lua. Can that work for you, or do you really need the Lua C API for some reason?\nIn principle you should be able to make both work, but we have always used the FFI in Snabb because it makes life so easy. I don't have an example based on the Lua C API to point to.. @suraj0208 There is quite a bit of FFI code landing in the next release via #1116 (Snabbwall) i.e. a binding to the nDPI library (interfaced as an external shared object.)\nCurrently we are relying on the LuaJIT documentation for making people aware of the FFI and how to use it. That may not be optimal but we have limited documentation writing resources of our own at the moment.. Just a quick first reaction:\nThis specific error is from an address translation routine attempting to translate a \"guess physical\" address (i.e. an address that the VM thinks is a physical address) into a Snabb host address (i.e. the address inside the Snabb process memory map.) This involves taking an address provided by the guest (here it looks like a receive buffer address on a vring) and looking it up using a translation table that was provided by QEMU with a vhost-user protocol message.\nThe error is saying that the address translation failed. This could be due to a problem with the table or a problem with the address. Previously these issues have often been due to an obscure edge case or negotiated-feature-combination on the vring where the host is expecting one descriptor format and the guest is using another.\nMore thoughts to follow....... Does 0x2036a8b0 look to you like a valid address for the VM side? Just wondering whether the problem is with the translation or that the address is untranslatable (mangled somehow on the vring.)\nGenerally the biggest source of bugs in the Virtio-net code for us is the complexity around the many permutations of descriptor format options. The main test coverage we have for this is our DPDK+Configuration test where we run the DPDK l2fwd application inside a VM and benchmark it in a test matrix that covers many DPDK versions and also several Virtio-net options (default vs without mrg_rxbuf vs without indirect descriptors.) However this is not exhaustive since there are more combinations of options and different guests (Linux, DPDK, netmap, etc) can all format descriptors in different ways.\nOne possible approach to troubleshooting would be to see how dependent the error is on the specific virtio-net options that are negotiated (if at all.) If you suspect the problem is with the mem table translation it could be worth adding some printouts to see what mapping Snabb has received in vhost_user.lua.. @vmaffione It would be very convenient to have a reproducible test case based on a NixOS guest. Then I could run it interactively and we could also have the CI include multiple netmap versions in its test matrix. Do you know if anybody has ever packaged netmap for NixOS? (Guessing it would be similar to the dpdk package.)\nThe simplest way to incorporate such a test case would be if the VM would have one virtio interface and simply retransmit every packet that it receives. Then we could reuse the existing test scaffolding that we use for running dpdk l2fwd in the guest.. Is it similarly easy to run a program that loops back all received packets on a virtio-net port?. I can take a look at cooking up a reproducible test case if you can post for me source location and build instructions for all the relevant components. Just needs to be a stable location e.g. release tarball or a Git branch that won't be deleted or rebased.. Thanks, that sounds doable. I will try to make time for this during the week.. My view is that the host side of a Virtio-net interface is extremely complex. We have to deal with every combination of quirks and features desired by the guests. (The guests, in contrast, can just pick the features they like best and use those.) This level of complexity requires fully automated testing to avoid \"whack-a-mole\" situations where fixing one guest breaks another.\nThe goal here as I understand it is to support netmap/virtio-net guests on a snabbnfv host. The only valid way to do that from my perspective is add coverage to the CI. Otherwise we only know that Snabb master branch supported netmap master branch on date X.\n(This is my view after ~ 5 years of dealing with the complicated moving target that is Virtio-net specification and guests from the device-side perspective.). relatedly: the CI also provides hundreds of machine-hours of testing for each kind of guest and experience suggests that this is necessary for detecting certain classes of bugs. For example poll-mode drivers run at high frequencies and tend to make obscure race conditions more plausible, for example see this memory barrier error that affected both Snabb and DPDK on the host side but only showed up approximately once per hour of sustained traffic.. @vmaffione Could you also tell me which guest kernel version(s) you have seen this problem with? One thing our CI currently lacks is to test many different Linux guests so the issue could be kernel version too.. We support both and choose layout based on the features negotiated by the guest.. Good detective work. Could be an issue with multiple memory regions in the map. I have noticed in the past that guests have tended to serve all DMA from the same region and this does not exercise the code very well.. @eugeneia I reckon this release is good to go although there are some interesting tidbits in the test results to acknowledge:\n\nHydra shows performance is the same as before (good.)\nHydra shows snabbnfv breakage with QEMU 2.7.1 and 2.8.0. This is a genuine problem that appears now because we did not have test coverage before (not due to changes in this Snabb release as we see from same errors with both master and next). I will create a separate issue to track this.\nSnabbBot is failing the build due to the Snabbwall selftest.sh not passing. This seems to be a software dependency issue and the solution I see is that we need to come up with a convention for adding nix expressions that specify the required dependencies for test cases. (Could also consider running those tests under Hydra instead if that makes life easier. Currently our Hydra jobsets are a bit spread out but we could unify them.)\n\nSo overall it seems like this branch is :+1: with new features and no genuine regressions. However, it seems like we should take some kind of short-term action to make peace between SnabbBot and the Snabbwall tests, otherwise SnabbBot will be failing every PR that comes in the next release cycle. Guessing it is okay to ship this branch and then correct that in SnabbBot either by providing the necessary dependencies or disabling that specific test case. What do you think @eugeneia?. @eugeneia That sounds good to me. If we can make that fix without needing to hack the master branch, i.e. by editing out-of-tree code in snabb_box.nix, then it seems safe to release now to me.. @eugeneia The machines have finally risen up and asserted their dominance :-).\nI temporarily disabled that requirement on the master branch. You should be able to push now.. Here is an early screenshot from a heap-allocation profiler that I cooked up. (Click to zoom.)\n\nThis is with the snabbmark basic1 benchmark. Data is from a timeline log that is augmented to stamp the size of the Lua heap on each event so that the precise amount of allocation at each step is visible. The graph suggests that each processing step usually allocates 0 bytes but that occasionally they do allocate, most strikingly the engine.polled_timers event that often shows ~50 bytes of allocation. Overall quite nice to see so little allocation.. Early days but I am quite impressed with the Lua/LuaJIT GC. I have made an experimental patch to defer all GC until the end of a breath. This is mostly intended to make it more measurable: see all GC time in one place and don't let it get mixed in with the processing time of apps. (If we care about apps allocating memory then we can measure that separately, like on the diagram immediately above.)\nI ran snabbmark basic1 to process 10 billion packets and took 31K samples of GC latency. I am being conservative and assuming a slow 2GHz clock when converting from cycles to microseconds. I think it is easiest to summarize the quantiles numerically:\n10%      25%      50%      90%      99%    99.9%   99.99%     100% \n 0.11300  0.12200  0.25200  1.55600  6.08188 10.87198 79.61105 81.73600\nThis says that the median (50%) GC latency was 0.25us and that in 99.9% of breaths the GC latency was <11us. The worst case in the sample is ~80us.\nThis looks fairly respectable to me. I would like to do a little more due diligence to make sure that these results are valid, and to compare them with a more substantial application like snabbnfv, and I would also like to understand the reason for the bad cases taking ~80us in case this can be improved (e.g. maybe it's due to an unnecessary memory-remapping system call, that would be nice...)\nCan also be interesting to visually compare the total duration of app callbacks when GC is deferred until later (red) vs when it is allowed to happen anytime (blue). Click to zoom:\n\nHere we see a bimodal pattern for the duration of each callback, but for Sink the deferred garbage collection is making the callback time more predictable and more consistently <10us. This seems like a nice property for \"divide and conquer\" because it will be easier to investigate the remaining outliers if we know that they are not related to GC.\n(The durations are actually much more consistent than they look in this visualization... nearly all of the data points are down the bottom in that thick ink... if you really care then here is the ECDF for the same data.). One change in our ljsyscall tree which is not upstream is f890ca0d1e183da51637066d910fb998829a042d which fixes a file descriptor double-close bug. However, I believe that an equivalent fix went upstream in justincormack/ljsyscall#188 by @aperezdc  and that this whole class of double-close bug was resolved with justincormack/ljsyscall#205. Just to be safe I cc @justincormack.. (Aside: If there is a way for git subtree to squash the early history of a project and then continue with full history from a certain point onwards then that would be very interesting. I have experimented and only managed to import the complete history or a series of squashes.). @eugeneia Could the issue here be that SnabbBot is trying to fetch git submodules, and we have referenced some via packages that we have imported? (Can you remind me where the SnabbBot code lives to check such details?). I pulled master partly to make SnabbBot repeat the test and see if the obscure Git error is the same.. @wingo are you up for being upstream on this one?. I pushed the vmprofiler tool to studio/studio#5. Just now it has to be invoked via nix-build which is a bit verbose but I hope it's usable as a start! Going forward the idea is both to put a slicker front-end on (not manually typing nix expressions) and also take more advantage of nix (nice visualizations etc.). Merged, thanks!. Howdy!\nWe are lacking an example. The root issue is that our multiqueue code was written in parallel with the QEMU side and then this evolved before it landed upstream there (took a year or two.) So it is possible that we have some compatibility issues with QEMU mainline and somebody needs to check into that :-). @teknico Good topic! Seems like we should have enough experience to make informed choices nowadays. What is your proposal exactly, and what pain points will it relieve?. I feel pain point 2. It can be very surprising how names propagate e.g. when a module uses packet.seeall to import all the global names, and then the module is used as a class-like metatable, and so each \"instance\" object ends up inheriting and exporting all global names as if they were its own fields. So I am on board with tightening things up.\nI am not so sure about point 1. Lua puts a few pervasive modules into the global namespace, like table and string, because otherwise they would need to be imported all the time. Snabb does the same with a few modules like packet and link. Is this unreasonable?\nSnabb has a tradition of treating lines of code as a scarce resource. We strive to implement as much functionality as we can using as few lines as possible (\"less is more.\") I feel that adding a hundred lines that all say local packet = require(\"core.packet\") to the code base would be an about-turn where we decide that lines of code are cheap. This makes me uncomfortable, and I would prefer to find a way to solve our problems while reducing lines of code.\nOne sub-issue about local for subroutines. If we make all of our subroutines local then we need to declare them before they are called. This could require reorganizing many source files. There are a few alternatives: we could reorder code such that the lower-level functions always appear before their higher-level callers (like in C and Forth); we could adopt an object-oriented style to indirect our way out of this restriction (mirroring fenv in a \"class\" table, essentially); we could \"forward declare\" the subroutines with added local declarations at the top of the file; etc. Have you thought about this issue already and do you have a proposal for what we should do?\nOne more issue that is perhaps more on the topic of the API. I would really like a coding style that makes new users' first steps pleasant. I am not comfortable with every new Snabb hacker always having to type local packet = require(\"core.packet\") as their first step in the project. I would be much happier if their first line was function do_my_awesome_thing_with_packet (p) for example. Do you know what I mean?\nI am also a bit confused by this:\n\nremove addition of names to the global namespace by defining a table for each module where to put the public names it defines\n\nI feel like there is a mixup here. Writing function foo () ... end will not add a name to the global namespace. Rather, it will add a name to your file-local namespace that was created by module() and is accessible via getfenv(). So I don't feel that we have a problem with polluting the global namespace today, with the exception of the names explicitly installed in the core.main module at startup.. Just thinking about this a bit more...\nGoal is to find a balance between qualities like aesthetics, robustness, clarity, brevity, and so on. On the one hand we can take a holistic approach, considering a whole programming style at once, or on the other hand we can take a piecemeal approach considering one aspect at a time: Should we use module? Should we use package.seeall? Should we use object:method syntax? etc.\nMy concern about the piecemeal style is hill climbing to a local maximum. Can be that we use global names to improve brevity, great, but that this makes the code more ambiguous, bummer. Or can be that we use very explicit naming conventions to reduce ambiguity, great, but that this increases the ratio of boilerplate to actual code, bummer. So the risk is that we end up with a solution that nobody really likes through a series of steps that each seem reasonable in isolation.\nSo I am drawn to the holistic approach. For example, we could propose rewrites of the sprayer.lua example program and consider the virtue of adopting one of the candidate programming styles as the canonical default for Snabb code. This way we would be looking for a global maximum that balances the parameters in a suitable way.\nHere is a quick fantasy idea along those lines. My idea de jour is to introduce a separate namespace for modules and variables such that you can see the difference lexically. Module names would be Capitalized and automatically loaded with require behind the scenes (looking first in the current directory and then at the top-level.)  Here's the revised sprayer.lua written in fantasy style with reckless disregard for existing practice :)\n```lua\n-- sprayer.lua\nmodule(..., package.seeall)\nfunction new ()\n   local count = 0\n   local input, output\n-- Expect one input and one output link.\n   local function relink (inputs, outputs)\n      input  = assert((#inputs == 1)  and inputs[1], \"one input link required\")\n      output = assert((#outputs == 1) and outputs[1], \"one output link required\")\n   end\n-- Drop every second packet.\n   local function push ()\n     for packet in Link.receive_all(input) do\n        if count % 2 == 0 then\n           Link.transmit(output, packet)\n        else\n           Packet.free(p)\n        end\n        count = count + 1\n     done\n   end\n-- Make an app instance object\n   return { relink = relink, push = push }\nend\n```\nSome suggested virtues:\n\nClosure per instance makes it convenient to put state into local variables e.g. count.\nNo call to setmetatable() i.e. eliminate a high-brow concept that is not really needed.\nNo shadowing of modules with variables e.g. local packet does not shadow module Packet.\nModule references can be detected lexically e.g. for linters, code-completion, and other tooling.\nNo boilerplate: Capitalized module names are automatically loaded with require (first looking in current directory and then in the top-level.)\nDon't rely on any magical state e.g. for the engine to add fields like input and output to the app object.\n\n(Overall this style is quite similar to Erlang with a lexical distinction between modules and variables, minimizing indirection in module references, and providing at least some properties that make life easier for creating development tools.)\nWhaddayareckon?\n. One more wrinkle.\nI have made a long investigation (see whole thread around https://github.com/LuaJIT/LuaJIT/issues/248#issuecomment-273841796) into the overhead of different function calling styles. Refer to a module as a global? Cache the module in a local? Cache the individual functions in locals?\nI believe there is an interesting optimization that could be conveniently applied to the Capitalized module loading machinery that I sketched above.\nThe short version is that we can optimize module lookups by wrapping the module with an FFI object. The FFI object will use the module as its metatype and so all of the functions and variables from the module will be visible. The benefit is that LuaJIT semantics allow the compiler to treat metatypes as immutable. In practice this means that the JIT will perform the table lookups at recording time. The machine code will then contain direct pointers to the table values (e.g. Lua function objects) rather than performing runtime lookups.\nSo when we first see a reference to a Capitalized module, such as Packet, we could do something like this to put an optimized version of the module into the global namespace:\nlua\n-- m is an ordinary Lua module\nlocal m = require('Packet')\n-- t is a unique FFI type that forwards to module m\nlocal t = ffi.metatype(ffi.typeof(\"struct {}\"),\n                       {__index = m})\n-- o is an instance that forwards to module m but with optimized lookups\nlocal o = ffi.new(t)\n-- Global name will point to the FFI object where lookups are optimized\n_G.Packet = o\nThis would potentially allow us to write things like Bit.xor(...) with the benefit of compiler optimizations that usually require caching the function in a local.\nThe caveat is that it would complicate life if we want to update modules. If we replace a function in a module then the old one can still be referenced from machine code in traces. So we would need to either not do that, or to flush the JIT in some appropriate way to regenerate the mcode.. Is it reasonable to classify this discussion as subjective?\nYou feel that extending the namespace for free variables in a module harms readability, while I feel that voluminous require() lines harms readability. So the answer to this question:\n\nIs there any way to get lookup optimizations and code encapsulation without harming readability?\n\nIs \"that depends on who you ask.\" \ud83d\ude04 \nThere are also secondary issues that may be significant. I have found it a problem in practice to use the same lexical conventions for module names and variables, e.g. I have often written local packet = ... which shadows the packet module and directly leads to bugs. So in that sense I feel that making separate lexical conventions for modules and variables, Packet and packet, is effectively splitting up the namespace where I suffer the most conflicts in practice. (Using a linter may also address this problem in a different way.)\nI should also stress that I am not married to the Capitalized modules idea. However, in my mind this discussion is framed in holistic terms, \"How should Snabb code look?\", and that these details are not amenable to piecemeal changes (especially when they require big global changes to the code base.). I also reflect that Lua ordinarily has a certain amount of \"magic\" when it comes to loading modules. If you write require(\"foo\") the normal expectation is that Lua will search multiple directories to find .../foo.lua and also check for other patterns like .../foo/init.lua. (See package.path.)\nThe historical reason that we don't have this in Snabb, and always use fully qualified names within a single directory structure, is that LuaJIT does not support package.path for modules that are compiled into the executable via the C linker.\nI have never been satisfied with this situation. I don't think the line local basic_apps = require(\"apps.basic.basic_apps\") deserves to exist in any program, let alone our beautiful little Snabb :). It's one step away from SimpleBeanFactoryAwareAspectInstanceFactory. So I do think that our code could benefit from the introduction of some new naming shortcuts, even if the ones we already have today may have been poorly chosen.. > I guess it is, and enough has been said.\nSo we can close the present discussion. However, I consider this topic to be both subjective and important. Somebody needs to take this bull by the horns and create the idiomatic Snabb programming style. But that is a big messy task and I understand that it is not what you want to pursue here :).. The CI report looks ambiguous to me. Have to take a closer look. Can be that the test run is still too small.\nGreat stuff on this branch :). I'm having a look at the changes to the core now. Somehow I am not finding the magit workflow that worked well for browsing large merges in the past. However now it seems like git merge --no-commit --no-ff <branch> does the job nicely i.e. it will stage all of the changes and let me explore the diff as if I were reviewing my own working copy.. I'm running a really big benchmark of master vs lwaftr here: https://hydra.snabb.co/eval/5833.\nThe code looks good to me. I have only looked in detail at the changes to core code and my understanding of the YANG+CONFIG parts is superficial for now (looking forward to taking it for a spin in the future.)\nIt kind of looks to me like the changes to app.lua have been sorted out between @wingo and @eugeneia in previous discussions and that these changes will not cause any compatibility problems with other branches that haven't landed upstream yet. Is that the case?. Is there a benchmark for the multi copy? I'm wondering if plain AVX would be the same speed if it's memory bound. There seems to be surprisingly little difference between avx/avx2 on memcpy routines in practice (glibc.)\nI think it's a problem actually to error on pre-AVX2. Guessing most people want that (Ivy Bridge / Xeon v2) as a deployment option for another couple of years.. Hydra seems happy! Looks like this branch has improved some configurations/setups of the iperf benchmark for snabbnfv. Could be relatively obscure cases (e.g. running a patched QEMU with a larger vring) but always nice to move forward anyway.. @wingo Do you want to do more work on this branch before merge?. Great hacking @wingo! This is merged onto #1148 with next hop next.. This looks quite exciting :). > They are run as part of the CI, it doesn't pin the test to a single core though so lib.pmu complains. @eugeneia @lukego Is that a problem with my tests or a CI problem ?\nGood question. Someone, somehow needs to arrange for this test to run with CPU affinity. Here is one idea: How about if the test cause would automatically take affinity to the core it is already running on?\nlua\nlocal S = require(\"syscall\")\nlocal numa = require(\"lib.numa\")\nlocal current_cpu = S.getcpu().cpu\nnuma.bind_to_cpu(current_cpu)\nThis should be safe and \"polite\" since you are only locking affinity to a core that has already been assigned to you.\nCould also implement this logic as a library routine in lib.numa and/or snsh -t if that made more sense to people.. @petebristow I'm looking into integrating the tests with CI. Good exercise to support running the perl code with suitable dependencies. I'm packaging Net::Patricia with nix over at https://github.com/NixOS/nixpkgs/pull/25908.. @petebristow Suit you if I merge this onto next now? I plan to separately take another look at the general problem of providing test suites with their required software dependencies.. Merged via #1203. Sorry about the wait!. This looks like a memory leak. The process is successfully allocating huge pages, but eventually it asks for too many, and the kernel is not able to provide more than 1251 in total. It's okay for packetblaster because it stops allocating new pages after a while.\nJust need to add a packet.free(p) into the method Sprayer:process_packet() :). I propose that we adopt the core.worker multiprocess model for spawning cooperating Snabb processes, provided that this will suit @petebristow. This framework did not exist when @petebristow was developing the intel_mp driver and his application, and so it is natural that he didn't use it, but now that it works (and is ready to land on master via #1133) I am hoping that it will suit him too.\nI also think the \"master process spawns tree of workers\" model will be able to accommodate more NICs in a consistent way going forwards. Some cards allow the simple peer-to-peer process structure, like 82598 and I350, while others really want to be at least partly configured from a kernel-like central point, like ConnectX and FM10K. If we have a non-realtime parent process that is able to implement the shared setup functionality then we should have all of the bases covered.\nPete, whaddayareckon?. I have belatedly pushed the API documentation for the multiprocess-engine branch ( #1021) here: https://github.com/snabbco/snabb/blob/82f335573a743fd608baa5ee966d3c5ef35183ab/src/README.md#multiprocess-operation-coreworker.\n@petebristow How does that API look to you?. Have been having a look at how to integrate intel_mp with the core.worker module. I hope it would be straightforward to replace the shell code for running Snabb processes with Lua code in a \"main\" process to start them with core.worker.\nHere's a quick example of how one of the test cases might look:\nlua\nfunction test_10g_1q_blast ()\n   local header = [[ m = require(\"apps.intel_mp.selftest\") ]]\n   worker.start(\"testsend\", header..[[ m.testsend(\"SNABB_PCI_INTEL1\") ]])\n   worker.start(\"testrecv\", header..[[ m.testrecv(\"SNABB_PCI_INTEL0\") ]])\n   sleep(1)\n   local RXDGPC = counter.open(\"group/intel_mp_test/testrecv.RXDGPC\")\n   assert(counter.read(RXDGPC) > 10000)\nend\nwhich is translated from the shell code:\n```shell\n!/usr/bin/env bash\nSNABB_SEND_BLAST=true ./testsend.snabb $SNABB_PCI_INTEL1 0 source.pcap &\nBLAST=$!\nSNABB_RECV_SPINUP=2 SNABB_RECV_DURATION=5 ./testrecv.snabb $SNABB_PCI_INTEL0 0 > results.0\nkill -9 $BLAST\ntest cat results.0 | grep \"^RXDGPC\" | awk '{print $2}' -gt 10000\nexit $?\n```\nCould be a reasonable approach? and if this would work for translating the intel_mp test cases then maybe it would also work for @petebristow's real applications?\nThere is one trick here for having one Snabb process read a counter value from another. I supposed that a simple solution would be for the worker to create an shm alias (symlink) from its RXDGPC counter into the shared group/ shm directory. That way any process in the group can access the counter using a well-defined name (I prefixed it with the name of the worker process.) Just an idea.. @wingo I wonder actually whether the YANG support would be suitable even for such simple cases as test suite programs? The intel_mp tests run a few related programs and need to provide some basic configuration data (PCI address, delay before sending traffic, whether to loop traffic) and need to extract some operational data (NIC counters.)\nIt would be quite neat if we are ready to depart from the world of passing configuration data via environment variables and extracting operational data using awk!\nJudging by the lib.yang doc on the lwaftr branch (#1133) the mechanism for exporting operational data from Snabb counters to YANG models is not there yet? (So maybe no good way yet for one process to read the RXDGPC counter from another?). @wingo Neat :)\nSo would it be reasonable for one Snabb process to provide configuration to another by constructing the value as a Lua object and serializing that to a file with yang.print_data_for_schema()?\nSo if one Snabb process wanted to provide configuration to another, might it construct the configuration as a Lua object and then call yang.print_data_for_schema_by_name() to write that to a configuration file? That would be instead of putting the values into environment variables or directly into the Lua code passed as a string to worker.start().. I would also really like a precise snabb --version command!\nI see a few issues related to running git describe in the Makefile:\n- Is not available when building from a tarball (no .git database to query.)\n- Is not available when building from nix (excludes .git because contents not deterministic.)\n- Is potentially exposed to weird Git-isms (e.g. which tags you have pulled into your repo.)\nIs there a way to make this more deterministic so that it works with tarballs and nix too?. This looks like an excellent solution for the scenario where a developer is building and distributing a binary directly from their Git working tree, and may have commited or uncommitted changes since the most recent tag. This is an important case for sure, but it seems like we should cover other ones too:\n\nBuild from a tarball like https://github.com/snabbco/snabb/archive/v2017.04.tar.gz.\nBuild with tools like nix that don't provide .git but may provide another useful ID.\nBuild with an explicit version to exclude all the magic e.g. for a release with own version scheme.\n\nDo you agree? If so then what would be the simplest way?\nOne idea - best I have at the moment :) - would be to add a couple of optional version numbers to the Makefile arguments: make VERSION=lukego-5.0 EXTRAVERSION=patch2 to build a binary that identifies as lukego-5.0 (patch2). The default value of VERSION could be included in the source tree (e.g. master branch version.)  The default value of EXTRAVERSION could be picked up with magic like git describe, or the Nix derivation ID from the environment, etc.. Let me try to be a proper upstream and make less rambling requests via the Review interface.... Great! The diff looks good to me, thanks for those changes.\nTarball build fails for me though:\n$ wget https://github.com/Igalia/snabb/archive/snabb-version.tar.gz\n$ tar zxf snabb-version.tar.gz\n$ cd snabb*/\n$ make\n...\nGEN       obj/core/version.lua.gen\n/bin/sh: obj/core/version.lua.gen: No such file or directory\n/bin/sh: line 1: obj/core/version.lua.gen: No such file or directory\n/bin/sh: line 11: obj/core/version.lua.gen: No such file or directory\n/bin/sh: line 12: obj/core/version.lua.gen: No such file or directory\nMakefile:78: recipe for target 'obj/core/version.lua.gen' failed\nmake[1]: *** [obj/core/version.lua.gen] Error 1\nmake[1]: Leaving directory '/tmp/snabb-snabb-version/src'\nMakefile:11: recipe for target 'all' failed\nmake: *** [all] Error 2\n. Thanks, this works for me now both with git and tarball builds.\n\nThe only problem with this patch is that it always causes the dependencies of core/version.lua to be rebuilt -- the core_version_lua.o and the snabb binary. I tried for a while to fix that but couldn't find the right way. Oh well. WDYT\n\nI have also spent some time on this now and not found a solution. I am a bit sad-panda about this. On the one hand it is nice to have version information included in the build by default, on the other hand I will miss the reassuring make: 'snabb' is up to date. message and the property of the snabb binary only being rebuilt when the code and/or version has actually changed. I will just suck that up and merge this week unless somebody else knows a way to get the best of both worlds?. Sorry about the slow turnaround. This branch is not building for me, neither from tarball (command above) or from my working checkout.\n/tmp/bcd/snabb-snabb-version]$ make -j\nGEN       obj/version.lua.gen\n/bin/sh: ../generate-version-lua.sh: No such file or directory\nMakefile:56: recipe for target 'snabb' failed\nmake[1]: *** [snabb] Error 127\nmake[1]: Leaving directory '/tmp/bcd/snabb-snabb-version/src'\nMakefile:11: recipe for target 'all' failed\nmake: *** [all] Error 2\n. Clever! I did not think of including the version-generation as part of the step to build snabb so that it only happens when that would be done anyway.\nSimple and works like a charm:\n$ make\nmake: 'snabb' is up to date.. Great reproducible benchmark!\nLet me use this as an opportunity to test the latest profiling tool, VMProfile (#1125). This is a sampling profiler that collects ~1000 samples per second to show how much time is being spent in each app and in the engine itself, and also to break that time down into categories like JIT code vs FFI calls vs LuaJIT C code vs Lua interpreter vs Garbage collection.\nHere is the high-level overview of the profiler report:\n$ cat overview.txt \n     profile percent\n apps.socket      71\n      engine      23\n      Source       2\n        Sink       1\n         Tee       1\nThis is quite striking: hardly any time being spent in the apps that actually process packets (Source, Sink, Tee) and most time being spent in the engine (23%) and the apps.socket (71%). This is the opposite of what we want and expect i.e. for most time to be spent processing packets.\nHere is another high-level global view of what kind of code is executing:\n$ cat what.txt \n    what percent\n  interp    39.9\n       c    25.7\n      gc    17.9\n foreign     7.6\n    head     4.8\n    loop     4.1\n    exit     0.0\n  record     0.0\nHere we see that most of the time is spent in the code we don't want to run, like the Lua bytecode interpreter (interp 39.9%) and the LuaJIT C runtime system (c 25.7%) and the garbage collector (gc 17.9%). Little time is spent in more legitimate activities like JIT code (head 4.8% and loop 4.1%) and FFI calls (foreign 7.6%).\nSo both of these first reports confirm that something fishy is going on and we are not spending our time running nice fast machine code, which is what we want.\nWe can also zoom in on the specific activities of each app:\n$ cat where.txt\n     profile      where percent\n        Sink    loop.35     1.2\n      Source    loop.29     1.8\n      Source foreign.29     0.5\n         Tee    loop.33     1.0\n apps.socket     interp    31.8\n apps.socket          c    20.0\n apps.socket         gc    10.4\n apps.socket      gc.64     6.1\n apps.socket      gc.42     1.0\n apps.socket foreign.42     0.7\n      engine     interp     8.0\n      engine          c     5.7\n      engine foreign.60     2.4\n      engine foreign.66     1.1\n      engine foreign.77     1.1\n      engine    head.38     0.8\n      engine foreign.38     0.7\nHere we see specifically that the Sink, Source, and Tee apps are all running sensible code (machine code and FFI) while apps.socket is running mostly the interpreter and (probably as a side-effect) runtime system C and garbage collection routines. Suspiciously the engine is also running a significant amount of interpreted code which we also need to account for.\nSo: this tells us that we should be looking for the reason(s) that apps.socket and engine are running interpreted, which will most likely show up as trace abort/blacklist messages in the JIT trace/dump file. So - the next step is to read that.\nPeeking ahead: Likely we will solve a couple of specific problems to make the code JIT instead of interpreting, and this will probably automatically resolve most/all of the runtime system calls (C/GC) by optimizing them away, and then if we are still not satisfied with the performance we could look at individual machine code traces to see the code that runs (for example loop.35 in the profiler report is referring to the code we can find by opening the JIT dump and searching for TRACE 35 and then scrolling down to the --LOOP-- section of the machine code.)\nPhew!\nOngoing related work here is to make the JIT tracing functionality be always enabled (see https://github.com/raptorjit/raptorjit/pull/63) and to make all the profiling/analysis tools really easy to run (see https://github.com/studio/studio/pull/6).. So! The next step is to understand why some code is running in interpreted mode. We can look in our dump1.txt file to work this out.\nThere are two main things we can expect to see in the dump: abort message where the JIT made an unsuccessful attempt to compile some code, and blacklist messages where the JIT gave up on compiling a piece of code because of too many aborts.\nThis is a bit of a needle-in-a-haystack situation because the dump is global and also contains e.g. a bunch of messages for things happening during startup and so on. So for the moment some manual legwork is needed, and in the future we need better tools for interpreting these dumps (some related work here: https://github.com/raptorjit/raptorjit/pull/63).\nJust scanning down the dump file for abort and blacklist messages that seem relevant. Here is one:\n---- TRACE 36 start syscalls.lua:362\n0001  UGET     2   0      ; t\n0002  TGETS    2   2   0  ; \"fdset\"\n0003  CALL     2   2   1\n0000  . FUNCC               ; ffi.meta.__call\n---- TRACE 36 abort syscalls.lua:363 -- NYI: unsupported C type conversion\nHere is what we can see:\n\nThe JIT started tracing at syscalls:362 which is the function mkfdset.\nThe trace aborted when it reached the bytecode FUNCC which is a call to a C function.\nThe reason for the abort is a C type conversion that is \"NYI\" (i.e. non-JIT'able.)\n\nSpeculating a little bit I suspect this is due to this line at socket/raw.lua:62:\nlocal t, err = S.select({readfds = {self.sock}}, 0)\nHere we are using ljsyscall (module S) to call the C function select() and we are passing the parameter, a file descriptor set, as a Lua object. Probably the JIT is not able (\"NYI\") to generate machine code that converts a Lua object into a C fdset and so it forces the code to be interpreted.\nSo based on this theory the simplest fix that I can recommend would be to modify socket/raw.lua to avoid calling select() and instead to just call read() and accept that it will fail if no data is available. The select() is not really needed and it seems like the performance problem is due to conversion from Lua to C of its file descriptor set argument.. Just a few further interesting bits from the dump...\nHere is the select() function being blacklisted by the JIT:\n---- TRACE 79 start c.lua:664\n0001  UGET     5   0      ; syscall\n0002  UGET     6   1      ; sysselect\n0003  UGET     7   2      ; int\n0004  MOV      8   0\n0005  CALL     7   2   2\n0000  . FUNCC               ; ffi.meta.__call\n0006  UGET     8   3      ; void\n0007  MOV      9   1\n0008  CALL     8   2   2\n0000  . IFUNCF   4          ; c.lua:27\n---- TRACE 79 abort c.lua:27 -- blacklisted\nThe last bytecode where it bails out is IFUNCF which is a call to an interpreted (blacklisted) function. So the blacklisting propagates because if one function is blacklisted then its callers will also be blacklisted, and their callers, and so on. This likely explains why the vmprofile is also showing the engine running interpreted code.\nOverall it is a process for us to learn which specific Lua/LuaJIT language constructs cannot be JIT'ed and avoid using them in our apps. Looks like if we want to call select() we would need to explicitly create the fdset object instead of relying on automatic conversion from a Lua table. There are other optimization tips around too.\nI have a medium-term goal to make much better tools for understanding what the JIT is doing and why.... I pushed a fix for this problem on #1153. Just now I am seeing this performance:\n$ sudo ./snabb snsh -jdump=+rT,dump1.txt -p snabbmark socket_test 100e6 8\nProcessed 100.1 million packets in 10.65 seconds (rate: 9.4 Mpps).\nThat branch is based on master rather than this branch. You will get a conflict if you merge it. Here is one idea to resolve that:\ngit remote add lukego https://github.com/lukego/snabb\ngit fetch lukego socket-unix-push-pull\ngit merge lukego socket-unix-push-pull\ngit checkout --theirs src/apps/socket.lua\n. Sorry about the wait. Hopefully fixed in 56a5acf.. Yep that did it. Merged onto next now.. @wingo Yes, I suppose that you probably need write access to do some obvious nefarious thing. Could be that it's not so unreasonable a default to allow read access to all objects, perhaps with the exception of DMA memory that could still be root-only (to avoid accidentally capturing user traffic.)\nI suppose the main thing is to add a description to the manual. Then users can decide what steps they need to take for their deployment model (e.g. do they allow unprivileged users to execute arbitrary commands) and their threat model (e.g. do they care about local privilege escalation.)\nWDYT?. @wingo Thanks for the review. Addressed in c6d3a07. I suggest there that if you want to control permissions you can set SNABB_SHM_ROOT=/var/run/some-locked-down-directory/snabb.. I made /var/run/snabb default to rwxr-xr-x permissions in 961fc43. This is stricter than the rwxrwxrwxt that we had before but seems likely to work for all use cases. @eugeneia is that okay for you? Looks like the original permissions came in with your commit 0df7f4af78a2dc071a7d326a5fe52ae597fa2e56.. Merged. The SnabbBot failure appears to be a false-negative performance regression test result.. @eugeneia Sorry about the wait! I reckon this is good to go :+1:. I really love the work here and on @petebristow's LPM branch at #1136 .\nHaving tight implementations of important data structures with cycle-counting benchmark coverage just adds a lot of joy to life :).\nI briefly tested this branch on a non-AVX2 CPU (grindelwald - Ivy Bridge) and the fallback seems to be working fine. Performance here on Ivy Bridge also looks in line with your numbers from Haswell+ which is nice.\n$ sudo taskset -c 1 ./snabb snabbmark hash 16\nbaseline: 15.02 cycles, 5.56 ns per iteration (result: 0)\nmurmur hash (32 bit): 402.31 cycles, 149.00 ns per iteration (result: 2167721464)\nsip hash c=1,d=2 (x1): 41.17 cycles, 15.25 ns per iteration (result: 3935169236)\nsip hash c=1,d=2 (x2): 31.03 cycles, 11.49 ns per iteration (result: 3935169236)\nsip hash c=1,d=2 (x4): 28.73 cycles, 10.64 ns per iteration (result: 3935169236)\nsip hash c=1,d=2 (x8): 28.74 cycles, 10.65 ns per iteration (result: 3935169236)\nsip hash c=2,d=4 (x1): 66.87 cycles, 24.77 ns per iteration (result: 4024933664)\nsip hash c=2,d=4 (x2): 55.18 cycles, 20.44 ns per iteration (result: 4024933664)\nsip hash c=2,d=4 (x4): 52.80 cycles, 19.56 ns per iteration (result: 4024933664)\nsip hash c=2,d=4 (x8): 51.87 cycles, 19.21 ns per iteration (result: 4024933664)\nOne fine day I would love to make a little R hack to summarize all of these various microbenchmarks and how they compare on each relevant CPU microarchitecture.... Merged! Thanks for the patience.. Great test case!\nThere is probably a simple explanation. This kind of test has been done successfully with a few existing Snabb applications in the past. Question is how to find that simple explanation :).\nI have been thinking about how to apply the new logging on the diagnostics branch and the Studio analysis tools. I have an idea!\nThe Snabb process exposes a lot of diagnostic information in /var/run/snabb/$pid during execution. This is updated in realtime and includes the timeline log (for the past ~10 minutes), the VMProfile data, and the engine latency histogram data.\nToday we already have some tools and methods for analyzing this data and comparing it between multiple process. This case is different though: we want to compare one process with itself at different points in time. So how to do that?\nOne approach would be to periodically capture the /var/run/snabb/$pid data and \"pretend\" that each snapshot is from a different process. So for example we could run tools like studio snabb vmprofile to see how the process is operating at 0h, 6h, and 12h, etc. Then we look for what is different between the processes.\nHow does that sound? If good then I reckon you could setup the logging something like the script below. This assumes the Snabb process has the lukego/diagnostics branch merged in. (That's not landed on master yet because it has some performance overhead that needs to be dealt with.)\nshell\nwhile true; do\n    stamp=$(date +%Y-%m-%d_%H.%M)\n    sudo tar cf snabb-$stamp.tar /var/run/snabb\n    sudo xz -f -0 snabb-$stamp.tar\n    echo -n \"Snapshot: \"; ls -lh snabb-$stamp.tar.xz\n    sleep 1h\ndone\nThis should be a good opportunity to improve the existing Studio tools and possible even make some special extensions for comparing the same process at different points in time (e.g. making deltas of the vmprofile and latency data to isolate specific time periods.)\n. Edit: Script has sleep 1h to snapshot every hour now. Initially it had the short sleep 10 interval that I used for testing.. I would also like to have some rrd-like functionality built into Snabb.\nThis should work very well for counters (core.counter module) since those mostly represent simple events that can be counted. Could also work for the latency histogram and VMProfile data too. Won't work for timelines though because those are really log files rather than counters.\nQuestion is how much of this we should build into the Snabb dataplane (e.g. make the counter shm objects contain rrd arrays instead of scalar values), how much we should put into a management function (e.g. parent process takes rrd samples of the workers), and how much we should put outside Snabb entirely (e.g. external tool querying via YANG model.) And, naturally, who has time to work on something like that and when :).. @raj2569 You should talk with @alexandergall who has previously made a daemon to expose Snabb data via SNMP with NetSNMP, and with @wingo &co who are now working on making Snabb data available via YANG models (which often have mappings to SNMP MIBs.)\nI also like the idea of keeping the Snabb process \"dumb\" and putting the smarts into external tools. The dataplane produces the data and somebody else processes it.. @raj2569 Great! Could you post the data somewhere so that I can have a little noodle around before suggesting the right commands to use?. @raj2569 Great detective work! Thank you for your patience. I am now building new tools and writing new documentation about how to diagnose and fix performance problems.\nThe work in progress optimization guide is here: https://github.com/lukego/raptorjit/blob/optimization-guide/doc/optimization.md\nLooking at your data the part that jumps out at me is the high and growing interpreter use. Generally speaking I would expect the interpreter use to be less than 1% but here it starts at 10.7% and we see it consistently growing up to 36.7%. So I would say that you need to identify the reason that some code is running interpreted and make a correction so that it will JIT compile instead. This is basically the same situation as we had in #1147 where the socket app contained code that was not JIT-able.\nPlease take a look at the \"Eliminate interpreter mode\" section and see if it is helpful!. @raj2569 I am curious about that too :-)\nHopefully we will find out along the way. Could be that there is an interesting explanation, could be that it magically disappears after switching to JIT mode, could be that performance still degrades in JIT mode and finding out why is the next step.... This is amazing work!\nI need to wrestle a little more with my trace visualization before I can take a proper look but I am excited about consolidating everything on one really nice Intel driver.. Should we close this PR as superseded by #1199?. Nice!. @wingo Land this one via wingo-next?. Good idea!\nWe actually have a very fancy backtrace formatter imported into the repo but not currently used: lib/lua/StackTracePlus.lua. It does handy things like show local variable values. Do you want to consider using that before we take this change upstream or keep it as-is?. Merged, thanks!. Nice one.\nAside: I have a half-baked idea to reduce the need for these kind of optimizations in the future.\nGenerally it bothers me that we need to worry about this kind of optimization i.e. narrowing the scope of local variables that reference pointer/int64 values to prevent them from escaping from traces (ensuring they can be \"sunk.\") The trouble is that the trace structure is not obvious from looking at source code and so it is a black art to know when this is needed (basically you need to run -jdump and look for unsunk allocations and ponder their root cause.) I don't think we want Snabb hackers to be forced to dabble in black arts.\nMy idea is that we extend LuaJIT/RaptorJIT so that if a boxed constant (ptr/int64) cannot be sunk then it is allocated on the stack rather than as a garbage-collected object on the heap. This way the basic penalty for an unsunk allocation would be much less: you spill the value from a register onto the stack with a few mov instructions and no heap allocation or garbage collection.\nYou would still need to pay the full price if the value then escapes from the stack e.g. if you store it in a table or return it from a function outside of a trace. But these situations should hopefully be easier to identify when looking at the source code.. (The challenge with that idea is that you can't currently store a boxed value on the stack. So we would need to introduce wider stack slots where the first word can be a tagged pointer to the second word. Then we would need to make sure these values don't outlive the lifetime of that stack slot: if that would happen then we need to transport the value onto the heap and rewrite the pointer. This may be closely related to the Lua concept of open/closed upvalues where local variables are only committed to closures when they escape the stack.). Merged, thanks!. @eugeneia This should be good to go :+1:.\nThere actually seems to be a significant performance increase on the iperf benchmarks, with results clustering more tightly around 15G and less often falling down to 10G. I am very curious to account for this but the release needn't wait on that.. Updated blurb above!. @wingo Yeah. This one will need some time to bake and for people to consider the pros and cons. This will reasonably take some months and an important early milestone is to make it testable e.g. so that you can just merge this branch to test migration to RaptorJIT.\nThe main early value of RaptorJIT will be compatibility with the Studio tools which are just starting to approach a usable state now. The idea is to make that so useful that people will demand RaptorJIT :). Just not quite there yet.. Superseded by #1264.. Hi Matthias,\n\nIs this the right place for such questions? I apologize if not.\n\nYes, this is the right way to ask your question. Sorry about the wait.\n\nWhat is the best way to access payload data? ffi.string and struct.unpack (from lua-struct) could be expensive operations. Are they? Is vstruct better? Is there such functionailty available in snabb already?\n\nIf you are satisfied with the performance of the struct package, etc, then that is fine. Could be reasonable for this application.\nDigression for how it is usually done in realtime applications on live traffic...\nJust want to remove headers from the front of a packet? packet.shiftleft(p, numBytes) will only cost a few cycles.\nWant to construct a new packet by reference the payload of an existing one? packet.from_pointer(other_packet.data + offset, length).\nWant to access protocol fields in a structured way? See lib.protocol library.\nMore details in the docs.\n\nCan I develop my tool without the compile loop somehow?\n\nYes. You can write a Lua script and run it like this:\n$ sudo snabb snsh myscript.lua\nSee snabb snsh --help for more information.\n\nIf I store the data, say s2 from above in a global list, how would I know that the PCAP file has ended and I need to write out the results. \n\nI'm not sure that I understood this one, but I do see a challenge that an app network doesn't necessarily know when it is \"finished\" because they usually run forever. So could need some hack... :).. Merged via #1203.. @eugeneia This should be good to go :+1:. The Hydra report is happy.. @eugeneia Should be possible to push now. I disabled the requirement of that test for the moment. I'll check on davos... I think it's rather a DNS/router issue. I wonder if there is a TCP port monitoring service that could tell us when it goes down so that we can catch that right away?. Nice improvement! Merged via #1203.. Merged via #1203. The SnabbBot test failure is almost certainly a false-negative, showing a packetblaster regression when this branch is changing unrelated code.. Merged into #1203 and en route to master.. Merged via #1203.. Thanks for the link. I have seen some suspicious spikes in timeline logs in the past and it's possible that they are related to these voltage switches. I will have to check on this when I swing back around there.\nI know that some people in the world are strictly sticking with 128-bit SIMD registers for performance reasons and it will be interesting to dig deeper into what suits the Snabb use cases best.. It would be interesting to know if there is any CPU performance counter register counting these voltage switches. There certainly should be.. I setup a Hydra job running on lugano servers (i.e. with 82599 NICs) comparing the performance of master, next, wingo-next, and this branch (labelled intel-mp). This should help to catch any regressions related to the driver e.g. some obscure DMA prefetch setting.\nThe first report will be on this build and new builds will start when a branch is updated on the evaluations page.\nThis is a first step at least, we could run a different benchmarking campaign if we think of one that makes more sense.. This branch looks really wonderful to me :). The intel_mp driver has really come up well! The initial Hydra test results with hardware NICs look fine too, I'll run a few more to be sure.\nI am thinking through the \"user experience\" when other Snabb hackers pull this branch into their own tree. They probably have their own \"downstream\" applications that are using intel10g. Is pulling this branch going to break anything? what action is required to update their programs? will this catch them by surprise somehow? I think we should have a solid understanding of this and I am thinking about it.\nOne specific Q: what is the reason for 7d1397714b6f70009b897d2afcce8f9c775e147b?. So! I am thinking we want to reach the point where we have one unified Intel driver that supports their many and varied cards. This branch is a big step in that direction because the intel_mp driver is The Right Thing and because it fully replaces the existing uses of the \"legacy\" intel10g driver.\nSo what remains to reach the final goal? when is it safe to delete intel10g.lua? and what are the risks of causing frustration to users and slipping backwards on the agenda?\nThe challenge that I see is to smoothly support the \"downstream\" code in people's applications. If somebody has forked Snabb and developed an application then they are probably using intel10g. Pulling this branch won't change anything - their code will still use the legacy driver - and if we pull their code upstream in the future then maybe we end up maintaining the legacy driver forever.\nSo it seems like some migration strategy is needed. I am tempted to suggest that we write down exactly how to migrate from the old driver to the new one and then print a loud \"deprecation\" warning when an application uses the legacy driver. Then we could hopefully remove intel10g in some reasonable timeframe e.g. the end of the year.\nMore things to consider:\n- Should we rename intel_mp to just intel to position it as the default driver?\n- Should we make migration automatic by having the intel_app API automatically forward to the new driver?\n- Is there some immediate risk of breakage for downstream projects from merging this PR? For example, if people are using the PCI API to automatically detect which driver to use based on hardware, and this branch will switch from intel10g to intel_mp, then will this be transparent to the user? (I'm thinking the interfaces to the apps may be too different e.g. links named rx/tx on one driver and input/output on another?)\nSorry to slow down this fantastic work with such long and rambling discussions. I am just trying to put myself into the shoes of application developers who expect to be able to pull the master branch and get maximum new good stuff with minimum breakage that they have to stop and fix.. @wingo Thanks for the discussion :). I do think it makes sense to pull this branch \"as is\" and let the two Intel drivers co-exist for a while, as you say. Then we can reasonably hope that applications will gradually migrate to intel_mp over the coming months and then if that goes smoothly then we can remove intel10g. If there are hiccups then the lifetimes of the drivers can overlap a bit more as needed.\nI think that downstream application developers will really appreciate all the work that you guys have done to make this code compatible. It's fantastic that you all made the effort to update the in-tree code. I surely appreciated that :) so thank you.\nLike you say, the master branch needs to be focused on moving ahead and continuously improving. It also needs to be predictable enough that people will actually pull from it regularly :-) but this change seems nice in that respect & probably any extra compatibility hacks would end up backfiring (people not really knowing which driver they were using, etc.)\nI am negotiating with Hydra to run a somewhat larger benchmark here. I had manually aborted a few evaluations earlier to manage the test backlog and I'm working out how to tidy up that mess now :).. There is a performance regression on the snabbnfv iperf benchmark when running on lugano servers i.e. with a hardware 82599 NIC. This is a bit tricky to see because the R code is erroring on the Hydra build, I think that I need to update it somehow (could be that it is not prepared for how few config permutations this test covers.)\nMeanwhile here is a quick CDF based on the CSV data and an ad-hoc R script:\nR\nlibrary(ggplot2)\ndata <- read.csv(\"https://hydra.snabb.co/build/1934426/download/1/bench.csv\")\niperf <- filter(data, benchmark == \"iperf\")\nggplot(iperf, aes(color=snabb)) + \n  stat_ecdf(aes(x=score)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"iperf benchmark\", y = \"percent of scores below threshold\")\n\nHave to think a little about a reasonable strategy for reproducing this for development purposes and for determining whether the difference is due to CPU/NIC hardware issues or JIT issues.... Here is the standard report that I generated locally in RStudio on my laptop. Have not looked closely yet but I do notice there are quite a few failures on intel-mp branch and we should be able to browse the logs for those on Hydra. (Could also be that they are failures due to timeout e.g. JIT issue caused the test to run too slow and it was killed.)\nI gotsta get CI building these updated reports.... @takikawa Today I want to try to make this test setup a bit simpler: remove \"known failing\" cases from the CI jobs and make the tests easier to reproduce locally on arbitrary Linux boxes. Can chat on Slack if you like.\nJust now for example if you like at the \"Success and failure\" graph you can see that all branches are failing on tests with QEMU versions 2.7.1 and 2.8.0 (because Snabb is not compatible today.) You can also see that with other QEMU versions the master/next branches have 100% success while intel-mp is more spotty. But, it's no fun to try and differentiate between \"expected\" and \"unexpected\" failures in the Hydra UI and so life would be simpler if we remove the unsupported QEMUs from these Hydra tests.. @takikawa Just as a baby-step I updated the CI jobs to skip every test that is expected to fail (i.e. incompatible versions of QEMU/DPDK.) So now any failures on Hydra should represent real problems (whether crash or timeout.) The first updated result should be Evaluation 6918.\nI have been trying to hack up a little Nix script that would quickly and easily run a customized benchmark on an arbitrary Linux box, e.g. Ubuntu with or without a hardware NIC, but I didn't manage to get that working yet. Have to think about how to tease out the basic test cases from the overall \"run a matrix of tests on a Snabb Lab NixOS server\" framework.. @takikawa @wingo Do you guys want me to pull these PRs straight onto next? I didn't immediately find what we decided when we discussed the upstreaming path for snabbwall (do I pull directly or does it come via wingo-next.). @takikawa Could you please add a commit on this branch to update the src/doc/branches.md entry that says what the \"next hop\" branch is? Just now I am not sure if that's next (my job to merge this) or wingo-next (@wingo's job.) Either way is fine for me. (The existing entry says that kbara-next is the next hop but methinks that is stale.). Hm. I pushed a recent commit from this branch (c3ca3ae) to Hydra as lukego-optimize and it is showing 100% failure rate introduced on the iperf-with-pcap-filter benchmark. Have to isolate which change is responsible for this. It's tricky because this should be covered by SnabbBot but does not seem to be running (Github is showing a green tick but that seems to be only for generating the manual and not running the tests.)\nI'll look for the root cause.. @mwiget Thanks for the tip. Maybe we should loosen the assertion in apps.vlan to allow vlan=0 and revert this change.\nWhat do you think @alexandergall?. I reverted this change and updated the vlan app to accept id=0 in ebd0aa9174d67a25d1996bb7c715d95772c715ea on my integration branch #1203. cc @alexandergall no action required.. I created raptorjit/raptorjit#85 to track the JIT issue. I think that I found the line of code responsible at least so far.. This has been talked about much but I don't think that anybody has done it.\nAmazon really has three network interfaces the last time I checked:\n- Plain ol' presumably with a tap device connecting the hypervisor underneath.\n- \"Enhanced networking\" with an Intel 82599 Virtual Function (up to 10G.)\n- \"Elastic Network Adapter (ENA)\" with a Virtual Function from Amazon's own custom hardware NIC (up to 20G.)\nThe ENA seems to only be available on really big machines for now and so the Intel-based \"enhanced networking\" seems highly relevant and valuable. This should likely be a straightforward extension of the intel_mp driver #1199. The transmit/receive queues on the VF are similar to the driver support we already have, but the control part is different (the NIC provides a little mailbox mechanism for making requests to a driver running on the hypervisor.) More info in the datasheet.\nOne thing that has always struck me about EC2 is that amazon provision very little bandwidth per core. If you want a VF with a full 10G of bandwidth then you need an instance with 64 vCPUs (32 cores). So Amazon are only provisioning around 300Mbps of bandwidth per CPU core. So the question is what to do with all those CPU cycles? :-) Even at 14.4 Mpps line rate you would have around 5000 cycle per packet budget if my envelope math serves.. Sounds basically reasonable to me too. The change is small and the benefit is tangible.\nThe only concern I have is that if a kernel would ever provide us with non-HugeTLB memory for some reason then this would be really bad. DMA of ~4KB+ jumbo frames would cause a buffer overrun in physical memory. So we need to be confident that this won't happen.\nLikely this does not happen in practice, since the memory is backed by hugetlbfs, but please consider whether or not you think it is worth checking the HUGE flag of /proc/kpageflags to protect against such an error.\nI am taking a moment to be thankful that our interface towards the kernel and OS is so narrow and that we have extremely little exposure to this kind of quirk compared with most projects :).. Great detective work everybody! This all sounds reasonable to me. Since we are mapping the memory in shared mode on hugetlbfs and with mlock I don't think the kernel really has any option except to map the huge page into our address space.\nFirst one to make a PR out of this rocks :) we can slip the fix quickly into the next release.. Merging as soon as Hydra has a munch. Good to have proper coverage on wingo-next now :).. Groovalicious! Merged. Sorry about the wait.. @wingo want me to pull this now?. Thanks Ben! Merged. cc @alexandergall who I know has been working on related apps recently and may need to cover this case too.. @eugeneia Could you get SnabbBot to test this please?. Merged, thanks!. So! I have been away on vacation and I am back now. Should we cut these changes immediately as the 2017.11 release and move on directly towards landing the new code on wingo-next and other PRs? what say you @eugeneia @wingo and all?. @eugeneia I suggest let's ship this branch now. That will reduce the backlog of changes and also get SnabbBot happy with master thanks to @wingo's fix to the lpm4 selftest case. Then we can focus on getting wingo-next merged and releasing that.. I'll merge this on the next sweep through PRs unless @petebristow has objections.. Merged on #1251. Thanks!. Thanks @wingo.\n@eugeneia Perhaps we should disable the performance tests on SnabbBot since they are mostly firing on false negatives? Hydra will always catch regressions a step or two before they merge to master so we have a safety net.. Okidok. I am fine with any alternative including leaving things as they are.. Thank you for using a Pull Request to raise this issue. It is very helpful to have the exact code you are working with connected with the bug report. That way the source code line numbers in the trace can be looked up in the right code version. :+1:\nThe JIT log does not explicitly say what is being blacklisted, but it does include a lot of aborts for the same region of code, and that seems likely to be the problem (or in any event something worth understanding.)\n130930:---- TRACE 112 start 95/18 intel_mp.lua:595\n 134405:---- TRACE 112 abort intel_mp.lua:596 -- loop unroll limit reached\n 134408:---- TRACE 112 start 95/18 intel_mp.lua:595\n 137883:---- TRACE 112 abort intel_mp.lua:596 -- loop unroll limit reached\n 137886:---- TRACE 112 start 95/18 intel_mp.lua:595\n 141361:---- TRACE 112 abort intel_mp.lua:596 -- loop unroll limit reached\n 141364:---- TRACE 112 start 95/18 intel_mp.lua:595\n 144839:---- TRACE 112 abort intel_mp.lua:596 -- loop unroll limit reached\nMy working hypothesis is that this code is being blacklisted and this is causing some important code path to run interpreted. Could be that raptorjit/raptorjit#102 is the solution i.e. make the JIT give up on the unrolling optimization after a while instead of giving up on the whole trace.\nWe have seen other instances of blacklistings for code starting in intel_mp lately right? Does anybody already have some insight into these? (I have \"swapped out\" the traces that I was looking at before vacation but maybe somebody can refresh my memory? cc @alexandergall @wingo @eugeneia). @raj2569 Could you please try merging #1242 and post a dump? I hope this will make the JIT trace easier to read by breaking the traces up into smaller parts (always start a new trace when entering/exiting an app callback to avoid mixing too much code together.). I am using this problem as a test case for improved profiler functions. Have updated the profiler backend in raptorjit/raptorjit#124 with information that should point to a solution, and now I need to update the Studio frontend so that we can see what it means :).. Merged via #1251. Thanks @krawthekrow and @alexandergall for the review! Sorry about the latency.. Early feedback seems to be that this change does not introduce problems, but it remains to be seen how effective it really is for simplifying performance diagnostics. I will see if it helps to resolve #1239 before really proposing it for merge.. I am running a performance regression test on this now. I would be very interested to hear if somebody can confirm that this change renders \"the parenthesis trick\" obsolete i.e. the function with the tailcall will still be blacklisted but we won't care because the calls will all be successfully inlined anyway (cc @alexandergall @eugeneia @wingo.). @eugeneia Let me know if you prefer to merge this onto max-next or for me to merge directly onto next once the next release window opens.. @eugeneia Let me know if you prefer to merge this onto max-next or for me to merge directly onto next once the next release window opens.. Huzzah! The new intel_mp driver is now adopted as the default and the intel10g driver (which represents some of the first lines of code ever written in Snabb) becomes a legacy backup option.\nGreat hacking to all the many intel_mp hackers !!! :+1:. Sorry about the wait. I wanted to recommend this for release now but I see a performance regression that needs to be resolved. I will first check if this is related to overuse of trace barriers in #1242.. Confirmed that the performance regression is due to the JIT barrier. The lukego-optimize performance with that change reverted matched master again. I will test using fewer barriers (on entry or exit to an app, but not both) and see if that is better. Otherwise the JIT barrier might not be suitable (too expensive) for app/engine transitions.. I have reverted the calls to jit.tracebarrier() around app callbacks. These seem to be a bit of a performance drag overall and their benefit is not really established.\nSo the jit.tracebarrier() primitive still exists but the engine doesn't currently use it.\nLet's wait for the standard Hydra tests to complete now and then we should be :+1: for the release.. @eugeneia There is a performance regression on the iperf benchmark but I propose that we ship this now anyway.\nI have been combing through recent CI results with @wingo on Slack and it seems like the problem is caused by voodoo. I have another branch with almost identical contents that does not show the issue. The only difference is whether the jit.tracebarrier() primitive exists in the C code. Having that code present seems to provoke the problem even though it is never called.\nI am reluctant to make a \"nonsense\" change to \"solve\" this problem. I would prefer to accept it for now and focus on finding the root cause of why we see variance in the iperf benchmark. I see the new RaptorJIT tooling as the way to do this and so I want to spend my time now on integrating that with Snabb. Hence my willingness to accept this symptom of the root problem (wider variance on the iperf benchmark) for the moment.\nHypothetically if the problem is something obscure, like whether two Lua loop bytecode addresses hash into the same JIT hot counter, then there are probably very many different ways that it could be provoked (e.g. choice of C compiler version) and so I am not really confident that nailing down one such issue in the test environment would translate into a real world benefit. This would need to be solved more thoroughly in the JIT after seeing exactly what is really going on.\nWDYT?. @eugeneia Glad to hear that this change has helped you. I pushed it here and let's see what Hydra says tomorrow.. @eugeneia This was worth a try but the new benchmark results show the same issue: https://hydra.snabb.co/build/2822290/download/2/report.html#iperf.. @dpino Cool see you there :).\n@wingo floated the idea that on Friday we could even book a conference room somewhere for an informal miniconf where we can each present what we are working on at the moment? Is anybody up for presenting something? (anyone up for organizing/moderating?)\nThis could be a nice venue for talks/demos that require familiarity with Snabb. I would be glad to show the RaptorJIT/Studio trace profiling tools that I am hacking.. Groovy :) I am updating the info above as people leave comments.. Seems like everybody is happy with JAM as the suggested hotel so I've booked in there too. I plan to arrive Friday afternoon and hang out with whoever is around over the evening before the conference. Sounds like we are all up for some informal chats/demos. Have to think about how to bump into each other at the main conference, sounds like that is kind of massive.... Merged, thanks!. Woohoo! Merged.. > Is it possible to leave out the trace and profile generation?\n@wingo Let's dig in to this a bit. On the one hand it is easy to make this configurable and that may well make sense. On the other hand it is a design goal to make the runtime behavior so smooth that you want to keep this enabled during benchmarking and in production.\nThe auditlog will make some write() system calls but only when the JIT is active. This should not happen in the steady-state. I believe that we also already have system calls when the JIT is working e.g. memory protection flags being updated where machine code is generated.\nThe profiler will not make any system calls in the steady-state. The logging is passive via file-backed shared memory. There will however be a SIGPROF signal every millisecond. This event will execute one small C function that bumps one counter in the current shared memory region.\nHow does that sound to you on paper? Still like something you would want to turn off?. Awesome! Merged.. @eugeneia @wingo Is the lib.numa change in https://github.com/snabbco/snabb/pull/1269 important for this release?. Here are the Hydra results: https://hydra.snabb.co/build/3062995/download/2/report.html. They are a bit curious but I reckon we can ship this fella.. JFYI a similar tool is text2pcap, but who likes external dependencies anyway :).. One related mechanism that we have is main.shutdown() that executes both on normal termination and also when the process crashes. This is where we are doing sensitive things like ensuring that DMA is disabled before we return HugeTLB memory to the kernel for reuse.\nYou can see how this works at the bottom of main.lua i.e. on startup we spinoff a separate process that will perform cleanup after the main process dies.\nI suppose the question is how much do you care about the stop() method running? The engine can run it for normal termination but if that's not enough then we have the shutdown mechanism.. One compromise might be to add an engine.stop() that runs engine.configure(config.new())? \nPersonally I quite like the view that \"orderly termination is only a special case of process crashing, and not a particularly interesting one.\" That is, if you want your application to be robust then you need to handle abnormal termination (crash) correctly, and once you have written that logic the simplest approach is often to treat normal shutdown the same way.\nJust from a user perspective I really like being confident that hitting a process with ^C or kill -9 won't screw things up e.g. create an invalid state that prevents the process from starting the next time.. The issue with polling for DONE0 seems to be that the I350 is a quad-port controller but on this machine only two of the ports are actually used and whether the driver sees them as port 0/1/2/3 depends on complicated EEPROM settings. So if we poll for PHY to initialize for port 0 there is a risk that this never happens because port 0 is mapped to pins on the ethernet controller that are not actually connected to anything on the PCB.\nSeems to me from the I350 data sheet that we should be allowed to skip polling for the PHY to come up. This seems to be necessary before changing PHY settings but our driver is instead accepting the defaults from EEPROM. Right?. The XL710 is a sore point at the moment. Everybody wants it, quite a few people are willing to put some work into the development, we have plenty of hardware in the lab, but nobody has taken the first step of adding the support yet.\nThe feeling seems to be that the hardware interface has been made gratuitously more complex since the 82599/X520 but that is life.... Thanks for the report!\nThis problem seems to go away when merging the latest lukego/raptorjit-upstream branch. This could be (?) due to a new fix for a problem where RaptorJIT would start thrashing once the maxtraces threshold is reached. So performance would plummet once you recorded the 1000th trace (default setting.)\nI want to take a closer look with the tooling. Just have to tweak the test to produce the audit.log. Currently that is bypassed by calling engine.breathe() directly instead of engine.main().. > I guess the resolution will be to remove traceprof entirely, and rely on Studio or related tools to examine the auditlog instead.\nYes. Traceprof is superseded by the newer auditlog/vmprofile features in RaptorJIT.. Merged onto snabbco/raptorjit via lukego/raptorjit-upstream.. Awesome with these bug reports! I'm 1-2 days away from being back in the saddle to help chase them down.. The latest blockers identified by @wingo at this point are:\n\n\nNot so good perf on basic1-100e6; that could be statistical though. Better to use hydra here\nA failure in the tap app's selftest. Seems to be legit. I didn't see it locally because I don't run with SNABB_TAPTEST.. Shipable I reckon @eugeneia!. Merged!. If you do decide you want to share huge pages between processes then check out the trick in memory.lua that does this for DMA memory. The basic technique is to use ljsyscall to automatically mount /var/run/snabb/hugetlbfs where we can allocate file-backed huge pages and map them from multiple processes.. The PC-losering problem just won't die eh :).\n\n\nI have a vague feeling that Linux signals can be setup to not interrupt system calls i.e. to defer the signal until after the system call completes. This could be a nice solution since we are not that interested in profiling system calls? I can't find how to do that right now so it is possible that I dreamed it.. Would it solve the problem to switch our setitimer(2) call from ITIMER_PROF to ITIMER_VIRTUAL?\nThis should only profile the time spent running in userspace and a reasonable person might expect (...) that this would guarantee that system calls are never interrupted.\nThis would make vmprofile less useful for profiling code that spends time in kernel space but that is not especially relevant to Snabb (and reasonably well covered by perf externally too.). (JFYI I didn't merge this on my sweep through open PRs just now because based on the discussion it seems like changes are pending.). Great stuff, merged!\n(This looks like a lot of commits only because raptorjit branch was lagging master.). I'm pulling this change in since @wingo's request seems to be satisfied and I know that he has his hands full with a newborn baby right now :). @eugeneia whose review(s) do we need on this one?. I've been a very lazy lab tech. I need to reinstall the OS on davos. I will put that onto my TODO list for today.. I didn't quite get through the OS reinstallation process on davos in the time available today. I'll come back to that tomorrow. I have the machine sitting at the installer prompt at the moment. cc @eugeneia . Any feedback on this @dpino @xray7224?. @eugeneia Revert pushed! Sorry about the wait.. Thanks!\n@dpino does this look good to you?. Awesome!. I'm running a CI job now to regression test this patch with 13 different QEMU versions from 2.1 to 3.0rc. I'll pull it onto next once that's looking good.. The CI report looks good. It did compatibility tests on a matrix of QEMU and DPDK (in guest) versions and the success coverage is much better with this branch than with master.\nHere is the graph of test success rate by QEMU and DPDK version combination followed by some explanatory notes.\n\nThe top panel (lukego-optimize) is this code and the bottom panel (master) is the baseline. There seems to be a specific issue with QEMU 2.7.1 and a few DPDK versions but that's not a major concern for us. There are also a couple of cases where <100% success was observed but I suspect these were spurious timeouts rather than anything systematic.\nGreat stuff! I merged this onto my integration branch at #1372 now.. @eugeneia I pulled in #1375 and this should be ready to rock'n'roll.. I'm taking the liberty of upstreaming this one myself since many maintainers have their hands full this summer.. Thanks! I've merged this onto my integration branch #1372 and I'll land that on next once I've tested that a bit more.. This ready to merge?. Hm. This appeared in commit 420fa7571432ba787aefa6a6e746adf1069762af. That changed bc.lua, bcsave.lua, and luajit.h. Looks to me like the real mistake was in luajit.h i.e. one trailing zero too many. Would it work to remove to change that instead so that all files say 10000? This would symbolically mean 01.00.00 whereas 100000 would be 10.00.00 i.e. RaptorJIT 10.0.. Is this resolved by the merge of #1389 that should include an equivalent fix? I also added test coverage for bytecode version in upstream RaptorJIT.. Have to think about this one a bit. What kind of states can packets be in, how do we identify the packets in each state when a process terminates/crashes, what is required for each state to be safe to reuse?\nStates would include:\n\nPackets on the process freelist.\nPackets on inter-process links.\nPackets on intra-process links in the local app network.\nPackets in private use by apps e.g. queued by device drivers for DMA.\n\nThis branch does seem like a step in the right direction anyway i.e. when a worker process terminates the packets on its freelist should be fair game for reuse.. @eugeneia do you want me to merge this branch now?. Great writeup!\nI have to read the JIT code and think about this but please indulge me in a hot-take :-)\nIs this just a bug in lj_record.c? If the JIT is waiting for the program counter to return to the start of the loop in order to complete the trace, and this is not happening because the loop is terminating, then perhaps the JIT should instead complete the trace when it reaches the FORL i.e. \"pretend\" for the purpose of code generation that the branch back into the loop is taken? This seems straightforward if the taken branch was about to immediately complete the loop without recording any further instructions.\nGenerally I am bugged whenever heuristics like \"leaving loop in root trace\" lead to blacklistings. Just shouldn't happen IMHO. . I have been able to reproduce this with a minimal example over at raptorjit/raptorjit#203.. Holy smokes!\nThere is so much awesome code on this branch!\nSorry to keep it waiting for so long. I've made a single pass through the diff and merged it all with a big grin :).. I pulled RaptorJIT into next to start integration testing it with Studio. It's about to arrive via max-next so no value in waiting longer :-). Is the problem with next the conflict in .version? I pushed a fix for that now, sorry!. The FFI is limited to defining max 64K types. Each time you call ffi.typeof(\"struct ...\") you are creating at least one new type (and possibly more for pointers and references.) So it makes sense that after some tens of thousands of iterations this loop will exhaust all of the available 16-bit \"ctype-id\" values and you will get an error when trying to define a new type.\nYou probably need to reformulate your program to define fewer types and reuse them. This means at least caching the results of your calls to ffi.typeof(...). You might also find that you only need one type e.g. that you can move the object size from the type into the value with struct { int size; uint8_t values[0]; }.\nI don't think this issue touches on operating system shared memory issues -- it's the Lua runtime.\nDoes that help?. Just remember that every time you call ffi.typeof(\"struct ...\") you will define a new C type, even if the structure definition is the same. So if you only need one type then be careful to only call ffi.typeof() once and to reuse the type object that it returns.. Great case study and write-up!. Just fwiw, the simplified way that I read examples like this is to say:\n\nThe JIT will compile each inner loop separately.\nCalled functions are effectively copy-pasted (inlined) into the loop and then freshly compiled there.\n\nSo in this case\n\nThere are three for ... end loops.\nThese all qualify as inner loops because they don't call any function that contains another loop.\nEach loop will be specialized separately: the first two for d1 and the third for d2.\n\nSo I'd expect to have three looping traces each specialized for one FFI ctype and then perhaps some \"background noise\" traces that compile individual functions or connections between loops.\nI like this simplified view as a way to quickly sketch how the most important parts of a program - its inner loops - are likely to be compiled.. So this is ~700 LOC to support all Intel NICs present and future? That would be nice!. (I am really looking forward to reading this code properly once I have the next Studio release out the door...). @mwiget That's one more good reason to book a Snabb Lunch in Switzerland soon! cc @alexandergall @sleinen. Thanks @emmericp @dpino I updated above.\nThe Ixy talk will be a real highlight! Great lead-in to my talk too :-). (What hardware interface will Ixy target next? i40e? fm10k? ConnectX? AVF? other? That's what I'd love to hear your thoughts about over a beer @emmericp.). Updated the text above based on the notion that at least @lukego @petebristow and @eugeneia will meet up at JAM Hotel in the evening e.g. for pizza and a few beers like last year. Everybody welcome!. This PR is a bit malformed. I'll try again.... I wonder if a better PCIe core is available that can do 16G? I'm asking the Twitterverse.\nPCIe bifurcation should be something that we can live with if we have to but I think (?) that significantly restricts the set of compatible motherboards/slots.\nP.S. Does using the Lattice PCIe core also lock us into their proprietary toolchain instead of Yosys/nextpnr? How are the licensing costs for people deploying EasyNIC?. (also: sounds plausible to ship an EasyNIC with 8Gbps PCIe bandwidth if this can later be upgrade to 16Gbps with a firmware update (new PCIe endpoint core). This is a bit like the way Mellanox bring hardware to the market quickly, it's hair-raising to see all the stuff that is fixed in their early firmware updates!). @mithro LiteEth/LitePCIe looks really interesting, great if there is something there that we can reuse!\nEasyNIC is all about making software developers happy by creating a simple and beautiful interface for device drivers. So we need to have control over the configurations registers and DMA model exposed to the host -- don't want to reuse that part off the shelf. But that's only a small part of the puzzle.\n@insekt It's not a hard requirement but I would love for the finished FPGA NIC to be supported by Yosys/nextpnr. This way software people would be tempted to build their own firmwares and potentially make adaptations for their own applications by forking the firmware (\"OpenWRT\" style.) The reason I am attracted to ECP5 over Xilinx/Intel is the good open source toolchain support.\nJust speaking for myself, I have once in my life setup all the Xilinx proprietary developer tools, and I don't think I will ever bother to do that again. It's a major culture clash for software people who are used to gcc/clang/etc and expect to just do git clone && make.\n(also: the genesis of the EasyNIC project is activism. NIC vendors are making everything complex and proprietary and pushing people into their massive software ecosystems like DPDK. This sucks and we need a \"NIC for hackers\" to push back on that. So the project is philosophically aligned with Yosys/nextpnr/etc who are pushing in the hardware/FPGA space.). @insekt Great that you are getting information about the PHY. If the lifetime is very short that would be a problem.\nOn the other hand 10G ethernet and PCIe 2.0 are also basically \"outdated technology.\" Sooner or later we will need to update to 25G ethernet and PCIe 3.0. Maybe over the next few years? So it could make sense to build the 10G NIC using simple \"trailing edge\" technology and count on rethinking choice of FPGA, PHY, etc in the future.\nOf course if the VSC8486 is already unobtainium next year then that would not be much fun.... @mithro Goal here is to make a NIC that Snabb users (for example) can deploy applications with. These are typically people working in ISPs or other network operators. In this context 10G and PCIe 2.0 is acceptable at the moment but 1G would be too awkward for most people.\nThe attraction of starting directly with 10G, to me, is that we de-risk important aspects of the project early on. For example, if there is not really a working 10G FPGA+PHY combo that will be available for the next few years then we might be heading for a dead-end and need to rethink the whole project (e.g. switch from FPGA to SoC, etc.). > 10G PHY (Marvell 88X3310) - XAUI - ECP5 - XGMII - ECP5-5G - PCIe x4\nAwesome!\nSo the idea is that one ECP5 talks high-speed SERDES to the 10G PHY, the other ECP5 talks high-speed SERDES to PCIe, and then between the ECP5s we have lower-speed (more parallel) connection?\n\nIP soft core XAUI<->XGMII is required\n\nCan go fishing on Twitter perhaps...\nGuessing that really any 10G ECP5-to-ECP5 I/O interface would be okay provided it doesn't need SERDES? (Since we control the code on both sides we are not locked into a standard, even though the standard might represent the best solution.)\n\nIP soft core PCIe x4 for ECP5-5G that is capable to handle 10Gpbs\n\nSuggest we start with PCIe 1.0 x4 (8Gbps) and plan to upgrade to PCIe 2.0 x4 (16Gbps) in a future firmware update.\nHave you looked at LiteEth and LitePCIe btw? I'm thinking it would be wonderful if we could use open-source IP cores instead of vendor ones (especially since people seem to be releasing such cores lately and presumably looking for users of them?)\n\nAdditional ECP5 will also add some cost.\n\nECP5s cost around $5 for low-end up to $50 for high-end right? This seems quite reasonable to me.\n\nYeap, there is not open source or free IDE for Xilinx Kintex 7. ICE WebPACK doesn't support it. But I think for the current stage of the EasyNIC project it's not so critical.\n\nThis seems like the toughest decision for the project to me. The ECP5 version would have less I/O bandwidth but end-users might realistically hack the firmware to add features and fix bugs (\"OpenWRT style\"). The Xilinx version would provide the simple EasyNIC driver interface but otherwise be more like a traditional/opaque NIC (I don't think software people will think Xilinx tools are fun.)\nI have a strong preference for the hackable ECP5 variant myself but I might be unusual here since I'm mostly thinking about lab use rather than deployment. @alexandergall as somebody who deploys his own applications in a production environment do you have thoughts on which EasyNIC would be more interesting?\n\nWorking with Xilinx also opens up the doors to 25G and 100G cards.\n\nThis is definitely important in the future. Question is timing though. Just now the Xilinx FPGAs don't have mature open source toolchain support and so to me it seems tempting to wait until they do before adopting.\n\nHow do you imagine that? What kind of SoC? NPU or what?\n\nNo, I have no specific ideas in this area, I only mean that in past discussions about EasyNIC it did not sound like FPGA was a viable solution and so \"something else\" would be needed instead. That was until @daveshah1 pointed out that ECP5-5G has enough SERDES to do 10G.. > Yes, but it's better not to reinvent the wheel =)\nRight. I only mean that if we are looking for an existing open-source core then we might find something that works even if it was not designed for ethernet.. > But this PHY provides min. 2 ports 10G.\nIf we were \"stuck\" with a second SFP+ then we could connect that up to the FPGA for future use.\nMyricom used to offer NICs with 1xMAC and 2xPHY that would automatically use the second port as a backup when link was lost on the first port. This is quite nice if you want to connect a server to redundant switches and handle failover transparently in hardware. This is not something we'd need to implement but it would be fun as a potential feature to be added by a user.\n(Aside: I miss Myricom NICs. They did such a great job of their technology. I hope to see more cool stuff from those people one day.). Ouch!\n@daveshah1 is it really true that PCIe ECP5 dev boards are not available anymore??. @insekt And from your past research one potential alternative is a Kintex7 board with PCIe x8 and 4xSFP+ from Aliexpress for $166? https://www.aliexpress.com/item/xilinx-board-xilinx-fpga-board-xilixn-fpga-development-board-pcie-board-Kintex-7-XC7K420T-XC7K325T-xilinx/32907109444.html. @insekt What do you think about using an ECP5 board with PCIe x1 for prototyping and derisking? Could this be sufficient for everything except the full-speed PCIe core? (Guessing we could \"amplify\" internally to drive the MAC/PHY at full speed e.g. by quadruplicating each packet on the transmit path?). Why remove LuaJIT as a subproject?\n. selftest() will fail on systems that don't have the right symlink in /etc? maybe better to comment out this line or test with a symlink inside the snabbswitch repo instead.\n. Hard for me to see why ram_to_io_addr is not set to nil here?\n. Looks like use_hugetlb is not configurable anymore after this change: the value is checked only when this module is loaded (when it will have the default value in the source file) instead of when memory is allocated (when it might have been changed on the command line).\nCould be that we need a 'config' module where parameters like these could already be set via e.g. command line before the modules are loaded?\n. The changed lines are mostly switching this function from 3-space to 4-space indentation. I don't like mixing whitespace changes with functional changes (and would prefer 3 spaces for the sake of consistency)\n. I suppose that we can delete the module_init function entirely.\n. Everything that I've written is 3 space indentation (that's the default setup in Emacs lua-mode for me). I would be happy for all of the code to be this way :-) though I am not religious about this stuff.\nI would like to:\n- be consistent about indentation within a given file\n- not pull whitespace changes that are not intentional\nCould you update the pull request please? (If it's painful with rebasing so many commits then for me it's OK to squash the commits more e.g. into just 1 or 2 commits total. I am reading the overall diff rather than the individual commits at the moment.)\n. I think it's a sufficiently minor test that you can comment it out for now, somebody can uncomment it if they rewrite readlink() for some reason.\n. The only trick is that we don't have a \"configuration\" step in Snabb Switch at the moment :-) but maybe we can create one after merging these changes.\n. I love this design idea: to handle neighbor solicitation response as a small discrete app that \"does one thing well\" instead of adding it as a feature to the IPv6 next-hop-routing app.\n. I suspect there is an easy-to-optimize performance issue here. The syntax {...} is probably allocating a garbage-collectable table and the syntax function()...end is probably allocating a garbage-collectable closure and that's possibly expensive to do inside the main traffic loop.\nIf the profiler reveals that vows:push() is expensive (or that garbage collection is) then you might want to try creating the {...} table and closure in the initialization function and reuse it in push().\nI could also be totally wrong on this guess, not having done any profiling :-). Do you have a reproducible benchmarking setup?\n. I wonder about that too. I suppose it will be worth doing that kind of stuff if the performance difference is significant in practice, once we are done with optimization. I hope we will have the basic app transmit/receive functionality so fast that we don't need to care.\n. True. And I suppose the decap could be tested with Source->vpws{encap}->vpws{decap}->Sink. Or perhaps even reducing the encap work with something like Source->vpws{encap}->Loop->vpws{decap}->Sink to encapsulate one batch of packets and then replay them without recreating them.\n. I like bitfields in C structs. I think they are currently an \"NYI\" in LuaJIT though: that the JIT doesn't understand them and as a consequence code that uses them will be interpreted instead of JIT'd. (If that's true you would see an NYI warning in the JIT trace from -jdump.)\nWe should see about getting support for bitfields in LuaJIT, either by asking Mike Pall or hacking it ourselves (it will be awesome if we can develop LuaJIT-hacking expertise of our own).\n. Indeed.\nI asked Mike about this when he gave his talk in Zurich: How can Snabb Switch contribute to LuaJIT?\nHe said he'd appreciate more people understanding the LuaJIT internals and being able to contribute code. He's already pretty satisfied in terms of creating consulting work and getting LuaJIT into widespread use.\n. Yep! Mike stepped briefly out of the shadows last October and gave a surprise three-hour talk about LuaJIT internals at the Zurich IT Geeks meetup group: http://www.meetup.com/zhgeeks/events/141272252/\nHe mostly walked through code examples and showed how they are compiled and why. There is alas no video available and only scant notes published on the meetup page.\nIt was a great evening, but don't worry, nobody became an instant expert on LuaJIT internals from being there :-)\n. The datagram library looks like a pretty reasonable point in the design space for packet parsing to me.\nThere seem to be two extremes:\n1. Low-level with one massive C struct/union describing every possible kind of packet.\n2. High-level with all packet headers becoming Lua dictionaries and all values Lua values.\nand, if I understand correctly, this design is in the middle basically putting a Lua API over raw memory accessed via C structs.\nAm I on the right track so far?\n. Here is one example of running the profiler:\nluke@chur$ ./snabbswitch -jp=l2 -t apps.rate_limiter.rate_limiter\n...\nselftest passed\n13%  lib.lua:26 < pci.lua:40\n 6%  packet.lua:73 < packet.lua:58\n 5%  freelist.lua:14 < buffer.lua:41\n 4%  rate_limiter.lua:90 < app.lua:62\n 4%  link_ring.lua:40 < link_ring.lua:20\n 4%  rate_limiter.lua:103 < app.lua:62\n 3%  freelist.lua:23 < packet.lua:27\n 3%  link_ring.lua:20 < rate_limiter.lua:90\n 3%  packet.lua:56 < rate_limiter.lua:103\n 3%  freelist.lua:24 < packet.lua:27\n 3%  link_ring.lua:45 < link_ring.lua:29\n 3%  packet.lua:79 < packet.lua:58\n 3%  freelist.lua:16 < buffer.lua:41\nsee more details on the profiler here: http://permalink.gmane.org/gmane.comp.lang.lua.luajit/3413\ndoes that help?\n. I have some ideas.\nFirst, I have been thinking more about how packets should \"ideally\" be divided up into buffers, and learning more about the constraints imposed on us by the virtio_net driver in Linux guests running under KVM.\nCurrent theory:\n1. The first iovec in a packet should be just big enough to contain all of the packet headers (~128 bytes).\n2. The remaining iovecs should be \"pretty big\" e.g. 4KB.\nThen we should feel free to rewrite the first small iovec to our heart's content for hacking headers etc but try to reuse the remaining iovecs verbatim to avoid hacking/copying excessive amounts of data.\nIn practice the most common packets are probably:\n1. Control packets (ICMP, TCP SYN, etc) that fit in one iovec.\n2. ~1500-byte data packets that are split into two iovecs (one small iovec and one big).\n3. ~9000-byte jumbo packets that are split into two or three iovecs (one small iovec and some big).\n4. ~64KB \"TSO\" packets that will be segmented in hardware (one small iovec and many big).\nThis scheme would let us do header-hacking on all such packets with a bounded cost in terms of data copies. (If we want to do encap/decap of 64KB TSO packets we only pay for the header rewrite, not the whole 64KB.)\nThe scheme should also work reasonably well for Virtio to KVM. I believe that for sending to Virtio it's okay for us to control the sizes of the iovecs but they have to start on the 4KB-boundry that we are given by the VM (so to decapsulate we need to rewrite the whole header rather than simply shorten the prefix in memory).\nThe scheme should also work well for Intel hardware which has a \"header split\" feature that can split the packet headers into a separate buffer from the payload i.e. create this iovec layout on packets arriving from the network.\nDoes that make any sense? This is stuff I've been working out this past couple of weeks while hacking vhost_user support for KVM.\n. That scan_devices() call is also really time consuming and noisy in strace so we should take it out of the default startup sequence.\n. No. Try running with strace and see if it's hanging in a system call?\n. (Okay, wild guess, it could be related to signal handlers. The profiler is driven by SIGITIMER and the io.popen() is going to generate a SIGCHILD. Do you have the same issue on chur?)\n. The \"split\" feature is reasonably smart: it will look for the header boundary and can fallback to a fixed size when the header is not recognized or too long. I think it also works with Direct Cache Access to pre-load the header part into the CPU's cache.\nDetails are in section 7.1.10 of the Intel 82599 data sheet.\n. Here's my understanding of Virtio:\nThe VM chooses what size of buffers to give us. The default behavior on Linux is unhelpfully pairs of 12-byte (for metadata) and 1514-byte (for payload) buffers, independent of the configure MTU of the device. However, if we advertise the \"we can merge buffers\" (i.e. iovec-style) feature then we will always be given 4KB buffers and we can create packets using N buffers and we can choose the size of each buffer (but not the starting address, that's fixed).\nI suspect we could also have the VM give us ~64KB buffers by advertising different flag. This could lead to a simple design with one buffer per packet, but currently my feeling is that header-splitting is worth the trouble if it saves us a lot of packet copies (and even from loading payload into the CPU's cache at all if nobody ever looks at it).\n. Virtio 1.0 is currently undergoing standardization by OASIS btw. I only discovered that recently. Here's where the action is: https://www.oasis-open.org/committees/documents.php?wg_abbrev=virtio\n. Are you calling os.exit() or otherwise terminating the process before it manages to terminate?\n(How about if you use -t before test.vpws-perf? the profiler stuff is in main.lua and related to the command line.)\n. Could be that you're using an older version of master as the basis and it has bugs?\nSoon time to start thinking about creating integration branches as stable points to develop against..\n. Great to list relevant links :-) that's really helpful to do, and I so often forget myself..\n. snabbswitch package can probably have a short (even empty?) dependency list.\nPerhaps we will want to make a snabbswitch-nfv meta-package that installs the base snabbswitch + all of the virtualisation stuff, but I think we can take that later.\n. I don't think there should be a default value for the PCI address. This is too machine-dependent. Instead this test should 'skip' if the address is not set manually via the environment variable.\nalso: please match the existing style of the code for capitalisation. maybe not obvious, but in this code it's lower_case_with_underscores for all function/variable/constant names (but CamelCase for classes). I don't say it's the best :-))\n. vhost_user_test.sock would be better as a variable e.g. SNABB_TEST_VHOST_USER_SOCKET.\n. Looks like README.md.src was missing on the pull request that I merged. Could you please send a new one for it?\n. I suggest we can have just two variables (names slightly changed from the above):\n- SNABB_TEST_VHOST_USER_SOCKET filename of the vhost_user socket file.\n- SNABB_TEST_INTEL10G_PCI_ID PCI ID of an Intel 82599 network device.\n... which contain information that needs to be supplied by the developer to make the tests meaningful (otherwise they are better skipped).\nI don't think that we need to have configuration options for the /proc and /sys paths (?) that should be consistent for Linux hosts.\n. I'd prefer to call os.getenv() here directly instead of splitting out to a separate function. (also: camelCase.)\n. I don't want to make this kind of change: it adds code and indirection, and I'm trying to ruthlessly minimize both of those things.\n. It would be nice if make test output was self-explanatory so we didn't need to add this additional read me.\n. .travis.yml should be in a separate commit?\n. This assertion is failing in selftest for me. I think the reason is that open() doesn't get called. Adding the open() call to new() looks like it would break RawVhost elsewhere.\n. Please add a space between fill_data and the opening ( for consistency with existing code in packet.lua and core/ in general.\n. Please a space before the ( for consistency with existing code in register.lua.\n. So what's with all the new naming conventions in the code base foo, __bar, baz_mt, etc? can't we just call things foo, bar, and baz? :-)\n. OK. I simply observe that our code base has a rich mix of different programming styles :-) and it will be an interesting exercise to make it more consistent over time. However I reckon that now the important thing is to add the functionality/tests that we need and not worry _too much about minor consistency. It will be fun to fight/edit it later :).\n. Can add some more comments to describe what this app is about?\n. Can pull these PCI addresses from environment variables and skip the tests if this is missing? (as in commit 87be43a and with same env variable names?)\n. Should make test and make test_ci run these new tests? (delete old test code?)\n. The user needs to understand what tests passed, what didn't run, and what failed (and why).\nThe make test output makes this pretty clear:\nTEST      lib.hardware.pci\nTEST      lib.hardware.bus\nTEST      lib.hardware.vfio\nTEST      apps.intel.intel_app\nSKIPPED   testlog/apps.intel.intel_app\nTEST      apps.vhost.vhost_user\nSKIPPED   testlog/apps.vhost.vhost_user\nTEST      apps.vhost.vhost_apps\nSKIPPED   testlog/apps.vhost.vhost_apps\nTEST      apps.example.asm\nTEST      apps.rate_limiter.rate_limiter\nTEST      apps.packet_filter.packet_filter\nTEST      apps.packet_filter.packet_filter_benchmark\nTEST      apps.ipv6.ipv6\nTEST      core.lib\nTEST      core.timer\nTEST      core.app\nTEST      core.memory\nTEST      core.link\nThe new test framework needs to be able to communicate the same basic information to the user i.e. so that they don't think they have tested more than they really did.\n. The best idea I have is for the CI to execute as many tests as possible, print details about failures (for the Travis-CI log), and finish up with an error exit code if > 0 tests failed. (The \"skip\" code is only meaningful within the Makefile -- you can drop that concept if you want.)\nHow do you envision running the tests? From the Makefile, from the shell, or from Lua?\n. Bear with me for a moment: I'm a sleep-deprived new parent who is still getting the hang of Github workflows.\nHere is what I think we are aiming to accomplish:\n1. Have more tests, make them easy to add, make them easy to selectively run.\n2. Keep everything working with Travis-CI.\n3. Avoid duplication e.g. multiple testing frameworks.\nRight?\nIf so it seems to me like we need to:\n1. Replace all old selftest() functions with new _t files. (Issue: what happens to print() statements?)\n2. Have make test and make test_ci run all the tests they should. (Issue: make test runs each module's tests in a fresh snabbswitch process but the recursive mode of test.lua reuses the same process.)\nI also have proposed a new way to run code that's not linked into snabbswitch in Issue #144 i.e. as scripts in #!/usr/bin/env snabb style. I wonder: could the _t.lua files be converted into such scripts? (Calling into test.lua as a library and being executed recursively with the make test machinery?)\nThat's a lot of brain dump. Like I say, bear with me :-). The main issue with merging code now is it seems like the new test framework is an addition to the old one instead of an update or replacement.\nalso: I reflect that I don't always pick up the \"big picture\" when reading diffs. It would really help for pull requests to include an email-length description of what the change does, why, and how it relates to existing code.\n. The way make test works is that all output is saved to a file for (optional) later inspection -- even for tests that pass. This makes it possible for tests to print a bunch of random information e.g. how many packest/second they achieved. This means it's okay/encouraged to make verbose printouts in test code because the user only sees if it they go looking.\nThis is redirected to a file on the Unix shell level. I suppose in Lua there is also a stdout variable that can be modified to redirect print() etc?\n. We should do additional filtering somewhere here:\nThe database configuration includes records for every compute host (potentially hundreds) but we only want to generate config files for the local traffic processes on our node.\nSo we should have a check along the lines of:\nif vif_details.zone_host == myhostname then\n   ...\nend\nwhere myhostname is the unix host name of the machine.\n. If you would like to convert an IP address to a pretty-printed canonical string you can FFI call inet_ntop(3).\n. ffi.string() will intern the spec as a Lua string. Does Lua have any relevant limitations with respect to such interned strings? (How about if we have a DoS with a million different address combos per second? will this behave OK or will the GC blow up? If such problems exist how can we mitigate them?)\n. Could you access the current time via engine.now()? That is intended as an efficient way to access the current time from an app. (It is updated once per breathe.)\n. The data structure returned by new() seems central to the whole design. How about a comment explaining what it is and what is good/bad about it?\n. What is this function? (Comment to explain?)\n. Calling this FFI constructor will allocate a Lua object that needs to be garbage collected, right? I will be curious to see how this affects performance if this is done on every packet, but if it would become a problem then it seems like we could reuse an object instead pretty easily. So probably best to leave it as it is, test performance, and learn something about the LuaJIT GC :-).\n. This behaviour seems reasonable to me for dealing with too-small buffers.\n. I don't think this is safe. There can be some existing buffers in the receive queue that are smaller.\nCould we simplify a bit:\n- Buffer size is always initialised to 16KB. (Remove rx_buffersize parameter - not needed.)\n- Buffer size automatically shrinks when a smaller buffer is added to the RX queue (e.g. the first buffer, which is probably < 16 KB).\nThat seems simple and sufficient?\nIf needed in the future we can come up with a safe algorithm for dynamically growing the buffer size again once the smaller buffers have been consumed.\n. Sounds good.\nDoes this mean we can safely remove the whole API for rx_buffersize and make the diff smaller? I mean, because internally the intel10g module can auto-tune to find the right buffer size (by starting with the maximum and then clamping each time a buffer is added).\n. Array should be 16 bytes for ipv6\n. Call be fickle but.... could we make this a hex number as a string like \"deadbeef12345678\" and have the app parse it using lib.hexundump()? I think it is a nice theme for app configurations to be in a format that's easy for humans to enter.\n. (The value would have to be lengthened/shortened to the right size if not the right number of digits given, with a warning printed for shortening.)\n. This should be local conf\n. We can drop this printout while we're at it..\n. Could be nice to have some really basic pretty-printing, e.g. for the top-level key/value pairs to be printed on separate lines? Otherwise it's a bit hard to read the output (both directly and with diff e.g. when neutron config unit test fails.)\n. Could you add the callback argument to the (new, shiny) documentation for the Sink app too?\n. You could remove the iovec access because it is not used.\n. Is this function vulnerable to creating dangling pointers due to remapping memory? If so, could we fix that?\nExample:\n1. Lua code calls counter module and gets the address of a counter for repeated use.\n2. munmap() invalidates the old mapping and then mmap() returns a different address (for whatever reason).\n3. Lua code tries to use the remembered counter value and causes a SIGSEGV because it is no longer mapped.\nIs this a valid error scenario? (I do like the idea of counters having stable virtual addresses: then I expect they can be incremented very cheaply indeed and so we need not be shy of doing so in the fast path).\nIf indeed this would be a problem, could we solve it simply by skipping the munmap()? Then even if mmap() happened to choose a new address then all previous mappings would remain valid as separate mappings?\n. I really like the generality of this design. Great that it is so easy to store structured data instead of single scalar values, and that the way this data is interpreted is flexible and open. (This data structure is starting to remind me of Forth's Dictionary.)\nHow do ensure that readers see a consistent value? The counter module depends on x86_64 loading and storing 64-bit values atomically so that reads and writes won't conflict. Do we need some kind of protocol to extend this to structured data, in case a write happens part-way through a read? (I wonder what would work if so.. a double buffer?)\n. I have been clarifying volatile and related issues with Mike Pall on the LuaJIT list.\nConclusions:\nThere is a risk that the JIT optimizations will eliminate or reorder loads and stores.\nvolatile is not the solution: LuaJIT ignores that. Probably we should not use it: I see a risk that we will expect it to behave in a way that it does not.\nCompiler barriers are a solution. For example, immediately before reading or writing a memory mapped register we could always call a lib.c function:\nvoid compiler_barrier() {}\nand LuaJIT will flush loads/stores from registers to memory before making an FFI call.\n. You know, this could really be relevant to the bugs that you are seeing. The first times the NIC is initialised the code will be running interpreted. Once the init code gets hot and compiled then the behaviour could change.\n. Large block of commented out code. Should be deleted? (I don't think you really want to push this change?)\n. Inconsistent with the state_check description above?\nI expect the logic to be that a packet must match the rules OR match a known connection, but not both. The connection table is essentially a set of exceptions to the rules that allow all packets belonging to connections that we allowed to be established. (In other words, if we allow the first packet for a connection then we will allow all the future packets too, even if they don't match the rules e.g. because they are flowing in the opposite direction.)\n. Can we do this on an EXIT trap to make sure it happens even with ^C? (Could be that this is how the whole stop_bench_env function should be invoked?)\n. Thanks for explaining.\nI don't immediately see the use case for state_check. Could it be that state_track is all we need in the API? cc @javierguerragiraldez \n. Documentation says that rules is required, but it seems to be optional in the code?\n. That sounds reasonable. Thanks for explaining. If the PacketFilter has an API that makes sense and is accurately documented then that is excellent.\nI am also thinking about the effect of changes to one part of the code base on other parts. The \"packet filtering ecosystem\" that we have today looks like this:\n- PacketFilter app.\n- snabbnfv-traffic process (uses and configures PacketFilter app).\n- neutron2snabb program (translates Neutron configuration to snabbnfv-traffic).\n- Neutron API Extensions where we need a section explaining our interpretation of Neutron Security Groups.\nSo Max and I need to describe our Neutron configuration interface and make sure that it is correctly mapped onto the PacketFilter app.\n. This change may have unintended consequences: The traffic process will be started as a daemon in the OpenStack context and initially its configuration file may not exist. This change would seem to cause the traffic process to terminate instead of polling for the configuration to be created.\nCan we skip this change or make it in another way that does not change the high-level behavior of the traffic process?\n. What does the actions.restart[1] = nil do?\nIf this is a trick to empty an array then is there a risk that filling in the first element will revive all of the original following elements?\n. The packetfilter setting should be in profile rather than vif_details.\nprofile contains configuration supplied by the user and vif_details contains information determined by the OpenStack code. (This is OpenStack Neutron semantics and not some funny invention of mine :-))\n. Does this change mean that we now include a PacketFilter app in the app network, even if no rules are defined?\nThat would be nice in principle - consistency - but in practice I wonder if this is a potential performance regression when no filtering is defined, which is the most important case in practice (e.g. for hosting router-like devices).\n(Is there also a potential functional regression e.g. in the difference between no filtering (nil) vs no rules ({})? I am not sure.)\n. OK.\nWe need a way to disable security group filtering entirely for a port. Do you know a suitable one?\n. @n-nikolaev Does --no-security-groups work as expected today or do we need to implement that explicitly?\n. Do we really want to reintroduce open_device()? (I don't see it being used in this change set?)\n. (Just reminds me: Generally in Snabb Switch we need a better way to refer to apps and network interfaces. Global names for the driver like apps.intel.intel10g.driver are excessively verbose, and PCI addresses for network interfaces are excessively specific - won't work for e.g. raw socket on eth0 even though some applications would work with one.)\n. I think the driver = ... lines in solarflare.lua and intel10g.lua could use a brief comment to say that these are to make it easier for driver modules to be used interchangably.\n. Curious: Why do we return 0 here? (What does that mean to Linux and Virtio-net, and is this better than returning C.VIO_NET_HDR_F_NEEDS_CSUM so that the guest will test the checksum and see the error?)\n. Comment here seems like the right place to say that we expect network byte order?\n. This TODO can disappear now? (Maybe worth sweeping for more comments that we have obsoleted?)\n. Multiple instances of the Solarflare app can be instantiated on the same PCI device. Packets received from the network will be dispatched between apps based on destination MAC address and VLAN.\n. I think the caller should assume there is always space available in the packet buffer to store new headers. The datagram module can internally check this with an assertion.\nThe intention of the current design (straightline) is that packet buffers are fixed-size and larger than any actual network packet. (Currently defined as 10KB in packet.h).\n. I think that we can remove this sentence. The app developer should not have to think about how much space is available in the packet buffer, it is the job of the core developers to ensure that this is always sufficient for any realistic scenario.\n(Like we don't ask programmers to check that they have enough stack space available before calling a subroutine and instead expect the runtime system to ensure that this is the case for any sane usage.)\n. This code for dispatching on a subcommand (sprayer <subcommand> ...) is a bit tricky as an introduction to Snabb Switch. I think that we should simplify this somehow.\nSome ideas:\n- Stop using subcommands and simply flatten out the command-name space. (s/snabbnfv traffic/snabbnfv-traffic/)\n- Standardize (move) subcommand logic somewhere else (e.g. main.lua or lib.lua) to make the example program simpler.\n- Change the example to use a different command-line syntax (e.g. -r <file> for replay and -s <file> for spray).\n. Meta-answer: I would say that each program can choose individually how it wants to present itself to users. For example, snsh will be very Lua-scripting-oriented but packetblaster might prefer to blend in with the usual Unix/C tools.\nThe decision is a little tricky in programs like snabbnfv where Lua is convenient but possibly distracting. I suppose the documentation guide does not have to tell people what to do in these situations beyond suggesting being thoughtful. If some kind of common practice arises over time then we could highlight that.\n. Little background: Yes, you can restart the Snabb Switch process and it will reconnect to the guests and continue to provide their networking. This is currently the one-and-only change on the v2.1.0-vhostuser branch that is not upstream in QEMU. (The other changes on that branch have landed upstream in QEMU after the 2.1 release.)\nThis guide should also work with the latest upstream QEMU with the exception that restarting the Snabb Switch and continuing to do networking will only work if you apply commit f393aea. That change requires some thinking about how to do \"right\" for QEMU, the trouble is that there is no analogous situation where you reboot the kernel underneath a running QEMU process. The other two changes on our branch are already upstream in QEMU.\n. This change highlights a subtle issue with shm shared memory objects: you have to be very careful about storing pointers in them.\nGenerally speaking pointers are only valid in one process because each process has a private virtual address space. This looks like a bug on the struct counter * pointers: they will only be valid for the process that initializes the link and if another process uses those pointers it will lead to memory corruption or a SEGFAULT.\nThe exception is pointers that are valid in all processes. The struct packet * pointers are uniquely okay because we do actually ensure that the same packet has the same address in every process.\nSo a new solution has to be found for the counters: we can have them inline in the struct, or we can remove them entirely, but we can't have them as pointers (even to other shm objects because shm.map() does not return consistent addresses for a given object).\nAside: I am also having a really hard time tracking down a bug that is caused by setting these struct counter * pointers to counters allocated with ffi.new() instead of shm.map() but I think that this is unrelated.\n. Is one of these u8 fields supposed to be called i8 instead?\n. How are you going to exclude it? I worry that if we rebase PRs then we risk creating more problems than we solve...\n. (I wonder if this is a matter of policy that we need to agree on or whether every subsystem branch can have its own policy about rebases... have to ponder that.)\n. Oh I suppose you don't need a rebase: in this case you can simply merge up to the commits that you want, since the ones that you don't want come later.\n(Sorry for rapid-fire comments... :-))\n. There is a bug here inherited from my hacky demand mapping:\nHuge page size is hard-coded to 2MB but this is not safe and we should use the value detected in memory.lua.\n. Include explanation of the name \"spoon\"? (I suspect there is a pun that I am missing completely?)\n. Accidental change?\n. Is this safe?\nI expect the gc program to only remove \"stale\" resources that are not in use by any running process. On casual reading though it seems like this could be disruptive? (spoon test script would break if there was a GC during startup?)\n. This line should probably be uncommented right? I mean: if a genuine segfault occurs then we want the process to exit with status 139.\nCurrently it looks like the segfault behavior has turned into an infinite loop:\n$ sudo ./snabb snsh -e 'require(\"ffi\").cast(\"int*\", 0)[0] = 0'\n^C\nSorry about the slow feedback. I think this change is in good shape. Just want to let my subconscious ponder a little bit if there are any subtle issues that we may have missed particularly regarding the memory sharing. (Can also debug these after a merge to next of course.)\n. (I can also just make any fixes like this during merge. This is in code that I wrote anyway.)\n. Is this a rewrite of the Tee app to preserve the functionality but make the code shorter? That would be very welcome: nothing is better than reducing code size while preserving functionality :-).\nReview is a bit hard though when there is no commit message explaining why the change is being made. Since this is included with the commit that adds the Virtio-net driver I am left wondering whether this is actually modifying the Tee behavior in some way that the Virito-net driver depends on.\n(Just as feedback for the future: I'm not suggesting to go back and rebase it now.)\n. Principle of least surprise: How come this app is inspecting the packet payload and requesting that the checksum be recalculated?\nI have a few objections to this:\n1. How about if the sender has already calculated the checksum? Then it is wasteful to ask for the device to repeat this work. (This will have overhead on the hypervisor in situations where the checksum is not offloaded onto the NIC.)\n2. How about if the sender is supposed to pass packet payload verbatim? e.g. a Layer-2 device that should pass packets unmodified regardless of the validity of their Layer-3/Layer-4 checksums.\n3. How does this really behave and are there some invariants that need to be taken care of? (For example does the packet need to already have a valid \"header pseudo-checksum\" stored in its checksum field?)\nOne way to take care of all these objections would be to skip the checksum offload feature and simply transmit packets verbatim. Then the other apps that are feeding the packets would need to take care of the checksums e.g. preserving the one that is already there, or calculating with our SIMD routines, or calculating a delta when updating headers. That may be unacceptable for other reasons but that would be an interesting discussion in its own right.\n. Is it important to add an FFI metatable to the packet struct in this PR? This looks like an unrelated change that is coming along for the ride.\nI do actually think that it is a good time for us to define a unified \"Snabb API\" and that FFI metatables are probably the right way to dispatch calls to packets, links, etc (instead of full module calls that can be slow and local caching of functions that is verbose). However I am not so wild about doing this piecemeal unless this is solving an important problem somehow.\n. Agree this makes sense as part of an improved API. Responded on #605.\n. This should be explained in the README for the app. I suppose that any behaviour is fine provided that the user understands it.\n. Note: On Snabb NFV at least there is overhead for requesting checksum offload from the \"device\". This will become a SIMD computation on the snabbnfv process and it will impact packet rate. Likewise on packets being received so that checksum validity can be reported.\nI would suggest a best practice of not negotiating offloads unless they are actually needed. There will be analogous situations for other hypervisor networking implementations.\n. Have to be careful about benchmarking. I think you will see completely different performance and profiler results depending on what hypervisor networking you are using on the QEMU level.\nIf the Linux kernel is doing the hypervisor networking then the packet rate will probably be pretty lousy and I can well believe that interrupt signalling can be the bottleneck.\nIf the hypervisor is doing kernel-bypass networking (e.g. snabbnfv) then it should be possible to achieve good results and it should be possible to suppress all interrupts so the bottleneck will not be there.\nIt would be interesting to know which scenario(s) this driver is optimized for. Personally I am much more interested in the scenario with kernel-bypass networking on the hypervisor (e.g. snabbnfv) because that is the only setup where I can envision practical end-to-end performance. In that sense I would not be tempted to make optimizations based on profiling with the kernel underneath: but maybe you guys care about that scenario aswell or instead.\n. I had missed the packet.prepend() to add the virtio-net header on the front. I do expect that this will be a performance issue if you were pushing a few million packets per second. If this turns out to be the case then it would be interesting to find a way to eliminate that operation. Could potentially be done by using two vring descriptors per packet (this is a common/legacy mode of operation, I think, Nikolay?) or more drastically we could reserve a little scratch area at the start of the struct packet where such metadata can be written.\n. (Could also be that the packet.prepend() doesn't turn out to be so expensive. It will be interesting to check that. There is AVX2 support landing in DynASM so we will have the option to try optimizing these SIMD copy routines ourselves before we go changing data structures.)\n. > What happens if write fails to transmit p.length data?\nGood question!\nBreaking it down:\n1. How much data should be buffered?\n2. Should this be buffered by the kernel, by Snabb, or both?\n3. What should happen when the buffer is full?\nI would like to make a case in favor of the behavior on the PR now:\n1. Buffer should be sufficient for one \"burst\" of packets. Ballpark 256KB?\n2. The kernel already has a buffer, and we should be able to adjust its size by setsockopt(), so don't need to buffer in Snabb too.\n3. If the buffer is full (EAGAIN or EWOULDBLOCK) then we should drop the packet immediately, provided that we trust the buffer to be big enough to handle bursts and only fill up during actual congestion (insufficient throughput from the kernel).\n. If you want to experiment/benchmark with removing the packet.prepend() then I have a proposal for a simple way to do that:\nWe could add a \"scratch\" area of free space before the packet payload and this app could write the virtio-net header at the end of this space (without moving the payload). The scratch area would be a bit like a stack red zone.\nThe new definition of the packet struct would be:\nc\nstruct packet {\n  unsigned char scratch[64]; // new\n  unsigned char data[...];\n  int length;\n}\nThis seems like a simpler solution for this use case compared with extending the packet struct with an extra indirection to allow repositioning the start of the payload (as discussed on #637). That would be a bigger change and I am not quite sure how to weight the pros/cons of that yet. I really like having a simple canonical representation of packets like we do today and wouldn't like to rush into giving that up.\n. The engine does not support this return nil, err return value. Rather it is always expecting an app instance to be returned. I think the best action here would be to raise an exception with error(...reason...) which the engine then should catch and handle (doesn't right now by default - that is something else we need to fix separately).\n. I think that a packet.free(p) is needed here to avoid a resource leak.\n. Good question! Handling unexpected I/O errors is not something we handle very sophisticatedly today.\nRaising an error (like this assert) is probably a reasonable strategy. Really we need to restore the engine code that catches these and restarts individual apps. Restarting the app does seem like the most reasonable response to an unrecognized error (e.g. maybe somebody manually deleted the tap device with tunctl and it needs to be recreated). This would be the normal crash-only software or let it crash design applied at the app level.\nFor non-fatal errors like the EAGAIN we should also bump core.counter counters. Today we are pretty slack with counters and that is a pity because they are crucial for understanding what is going on with a running system. I think that each app should have a well-defined set of counters (tx_dropped, etc) as part of its interface and documentation on equal footing to its configuration items. Maybe the Tap app could be an early adopter :-) up to you.\n. This is off at a tanget, but just as an example of the sort of counters we should be keeping in our apps in the future: YANG model for IP router statistics.\nIf somebody builds a generic IP router on Snabb Switch then they will want to be able to derive precisely those counters from the apps they are using. So it is good for us to start having the standard IETF/YANG counter definitions in the back of our minds at least.\n(The other important counters are from the standard SNMP MIBs but the YANG people are already onto that an have well-defined mappings e.g. in Section 3 \"Relationship to IP MIB\" of the internet draft linked above.)\n. It is quite delicate :-). I started out with an unrolled version of the inner loop and then found that the looping version delivered the same performance. There have been other very innocent code variations that were much slower though. I want to use the PMU to explore these differences.\nI would like to try unrolling the outer loop though to see if coping several packets in parallel could help.\n. @eugeneia I have a few concerns about this code:\n- Feature creep on Source. Could be a special-purpose inline app instead? (And refactor into Source if and when other apps need the same functionality? and even then perhaps make it optional rather than changing standard behavior?)\n- The variable txSeqNo is global to the module so the sequence numbers from a given app will be non-deterministic and have discontinuities depending on how many instances are running. Better to make it a local instance variable of the app?\n- Generally this change reduces consistency with the surrounding code (introducing camelCase and different use of whitespace around operators).\n. Just a small thing for us to think about in the future:\nI would like to avoid these license and copyright lines in the source files. To me it is boilerplate that doesn't add value. We already spell out the license in the COPYING file and Git already tracks the authorship very directly. If those mechanisms are not sufficient for some reason then it would be best to open an Issue to discuss that rather than starting to add random notes in the source code.\nCould be that we should make some improvements e.g. write a CONTRIBUTING.md that spells things out in more detail, perhaps add a file where people can note that they have assigned copyright for certain of their contributions to somebody else (e.g. employer), perhaps help contributors get more visibility for their contributions in some way, etc.\nHowever for now there are few enough instances that it seems safe to ignore :). There are also important exceptions e.g. where we have imported some code that is MIT licensed that may have implications one day.\n. Please somebody file an issue proposing this and explaining why it is valuable for Snabb Switch.\nIf we are going to do it then we should at least be systematic.\nI am skeptical. I see this as a practice that projects cargo-cult from each other that is not actually valuable in practice. I am prepared to be convinced though.\n@nnikolaev-virtualopensystems I am also a little on guard since you guys at VOSYS do so much branding on the Snabb repository e.g. putting the company name into your handles and putting the company logo as your avatars. I asked you guys to explain why this is important, to see if we could find an alternative way to reach the same goal, and you didn't engage in that discussion. I don't think you guys are being very thoughtful or communicative about this stuff and if other people do it too then it will become a problem.\n. note: The actual copyright holder is determined by a contract between the author and that other party. Writing this in the source file does not make it so. In practice these lines are often wrong e.g. in OpenStack many people write \"Copyright OpenStack Foundation\" because they think the contributor agreement includes copyright assignment when it actually does not (and so copyright probably belongs to their employer as a matter of fact).\nUgh.\n. Question is what is the high-level intention of this program? If the idea is to have absolute minimum overhead for benchmarking purposes then learning bridge could be counter productive.\n. @eugeneia I think there are two different hats that one can wear when reviewing code:\nCommunity member: just giving feedback, nothing is binding, no responses are required.\nUpstream maintainer: sole person responsible for accepting/merging the branch or spelling out explicitly what needs to be done before that can happen.\nIf you are the person who is resposible for upstreaming the PR and you don't press \"Merge\" then this is implicitly strong feedback \"this code is not currently acceptable to me and so I have declined to merge it.\" In that case the contributor needs to clearly understand what are the deal-breakers so that they can fix them and the branch can be merged. The maintainer might say \"You have to change A to B and remove C\" or \"we need to get an ack from @somebody or @someonelse\" and then once that is done the contributor will expect the branch to merge.\nThis is why I have been stressing the importance of unambiguously dispatching incoming PRs between upstream maintainers. Then everybody knows who is wearing which hat and what is the path forward.\nOverall there will be agreement between O(log(numberofmaintainers)) people who each merge the change onto their upstream branch as it makes its way to master.\n. How about a link to some widely accepted technical writing style guide or a list of pithy quotes? I really do think that \"preachy\" is a tone that we absolutely want to avoid.\nI know that I am a broken record but I still feel like the core problem we need to fix re: documentation is to make it more usable/accessible. Otherwise it just feels like a tax.\n. Performance impact should be measured automatically by the CI. If this testing is not sufficient then we should improve it.\n. FYI: You can see the performance regression tests at the top of the results page that is linked from the little green tick / red cross next to the commit.\nHere is the snippet from this PR:\nChecking for performance regressions:\nBENCH basic1-100e6 -> 0.967078 of 14.58 (SD: 0.0979796 )\nBENCH packetblaster-64 -> 0.997168 of 10.592 (SD: 0.0292575 )\nBENCH packetblaster-synth-64 -> 0.991587 of 10.698 (SD: 0.00979796 )\nBENCH snabbnfv-iperf-1500 -> 0.899233 of 5.478 (SD: 0.326399 )\nBENCH snabbnfv-iperf-jumbo -> 0.997787 of 6.326 (SD: 0.191896 )\nBENCH snabbnfv-loadgen-dpdk -> 0.991281 of 2.6838 (SD: 0.0322639 )\n@eugeneia this actually looks suspicious to me. Is SnabbBot being quite conservative about flagging problems with there is high variance in results? If I am reading this right then the iperf-1500 result had around -10% for the mean value but this is accepted due to high variance / error bars i.e. likely false negative?\n. @eugeneia Thanks for explaining. One day we can revisit this once we also have a better handle on the whole \"scientific testing\" concept. For now it is probably sufficient that we check more closely before making a release. I would not like to ship even a 1% performance regression if we can avoid it but I understand that it is non-trivial to verify this (particularly on a reasonable processing budget).\n. I'm thinking that 56 bytes of padding should be enough here when int is 8 bytes. However we should probably be more explicit and declare read and write as either int64_t or int32_t. (I have a vague recollection that Mike Pall discourages the use of uint64_t with FFI for some reason -- must understand that properly one day.)\nGenerally LuaJIT does understand declarations like __aligned__((64)) but I think C only supports that on structs/arrays.\n. The nix branch should have a well-defined upstream next-hop where @domenkozar sends his Pull Requests. We maintainers should pick one as part of accepting #836.\nI propose that nix feeds upstream to kbara-next. what do you think @kbara and others?\nThis process is supposed to be explained in point 3 of the text quoted above about updating branches.md. How should we make that clearer?\n. Note: There are two separate processes both in this document and in actual practice. One is for making an individual contribution, such as a new feature or a bug fix, and one is for sending a subsystem branch upstream, e.g. merging the nix branch with its upstream next-hop.\nThe document is supposed to clearly explain that there are two processes, one for contributors and one for maintainers, and to spell out the details of each. Obviously it is not doing so perfectly :). How could we improve it?\n. @domenkozar Can you clarify how systemd timers work e.g. with a link to documentation or an example nixos module? I have never heard of them before and perhaps @eugeneia has not either.\n. This looks like a neat solution to me!\n. How frequently do these counters need to be synchronized with hardware?\nThis code is synchronizing them every breath but that adds up to a significant number of PCIe accesses even e.g. for NICs that are completely idle. This may cause performance degradation in scenarios that we don't have CI performance coverage on at the moment e.g. app network with very many NICs where most are idle but some are active.\nOne alternative would be to use a timer to update every  e.g. one millisecond.\n. Is it safe to introduce this potentially unbiased branch onto the \"fast path\" for Virtio-net?\nThe risk I see is that on a workload with 50/50 mix of unicast/multicast traffic you will take the penalty of both a LuaJIT side-trace and also a CPU branch-misprediction half of the time. This could be significant and we don't currently have performance test coverage for such a workload.\nOne alternative would be to write this branch-free (using arithmetic, bitwise operators, and min/max). Sketch:\nlua\n-- Set unicast and multicast to 0 and 1 as appropriate.\nlocal multicast = bit.band(packet.data[0], 1)\nlocal unicast = 1 - multicast\ncounter.add(self.counters['in-multicast'], multicast)\ncounter.add(self.counters['in-unicast'], unicast)\nThis way the same instructions would execute every time and only the values would change.\n. Is DWIM a good approach to deciding which Snabb process to monitor?\nAlternative would be to invent a simple and unambiguous way to refer to a particular process e.g. by name.\nRelated issue: lugano-1 currently has 1,657 stale process directories in /var/run/snabb. Do people really run snabb gc at appropriate intervals in practice? This is relevant to me now because I would like to create relatively large timeline logs in shared memory but I don't want this to linger around chewing up scarce space on a ramdisk folder i.e. continuing to consume memory after termination.\n. This resize function seems potentially error-prone to me: if you resize the packet but don't fill in all of the data then you will be leaking unknown information from the previous user of the packet.\nOne alternative would be for encapsulation/encryption to use a new packet.pad(p, n) that always fills with zeros and for decapsulation/decryption to use a new function akin to packet.from_pointer() to set payload and size at the same time.\n. Sounds good to me. Can be further optimized in a more complicated way if/when necessary.\nSide note: \"Gbps\" metric is only meaningful when also saying which processor you are using. I don't know if you are testing a 2GHz CPU or a 4GHz CPU or something between. This is something we need to be careful of so that users don't deploy on a 2GHz machine and expect the same per-core performance that we have measured on a 4GHz machine. Just something to keep in mind.\n. Side note on side note: I wonder if bits per cycle (aka Gbps per GHz) could be a handy metric for this kind of thing. For example if you are seeing 13 Gigabits per second on a 3.5 GHz lugano server then you could say you are seeing 3.7 bits-per-cycle with 1KB packet size on Haswell.\nI will also be interested in seeing comparative performance between Haswell (v3) and Ivy Bridge (v2) and Broadwell (v4). Haswell's AES-NI hardware is supposed to be twice as fast as Ivy Bridge for AES-GCM and I am not sure whether this improves again in Broadwell. Also Intel provide two reference implementations, one optimized for Ivy Bridge and one for Haswell, and we have only implemented the Haswell-oriented one and not checked how that performs on Ivy Bridge. (Intel say the Haswell code works on older CPUs but with lower performance -- I am assuming this is very slight but have not quantified.)\n. (These performance measurement activities are perhaps best addressed by just making sure that make benchmarks covers IPsec and waiting until we have Hydra running that on all available CPU generations. Morally we should not have to be doing this kind of testing by hand these days.)\n.   @eugeneia fwiw I see this as a critical bug: our default behavior is to create a new ramdisk folder on each invocation and continue until all system resources are consumed and the server presumably fails with out-of-memory errors. If that wipes out somebody's network they are not going to be satisfied with \"oh, well, maybe add a cron job.\"\nGenerally I see it as extremely dangerous to choose default behavior that is optimal for developers in the lab but pessimal for production deployments. I also don't think it is reasonable to expect people doing the packaging to take care of these issues with e.g. out-of-tree crontab stuff.\n... but this is off-topic for this issue now, sorry.\n. Good catch! I was imagining that system call was the preferred kernel interface, shows what I know.\nDid you mean to also remove the syscall.sysctl() that sets vm.nr_hugepages? Seems redundant with the line above now. \n. I pushed a quick fix in 205375943f0ccc8a5796d50f0fcde488aabbc02e.\n. (Rationale: already merged to next so best resolved there either directly or as PR target. New edge in git workflow graph again...)\n. I am in favor of documenting our ABI i.e. the memory layout of these structures.\nThese basic data structures like packet and link need to be usable by e.g. assembler code and this does not seem compatible with defining them via an abstract Lua API.\n. I am seeing my Snabb processes print jit.flush when running on next and as a user this is a little confusing. I think we should either make this message more verbose to explain to the user what is going on, and/or only print the message when debugging is enabled.\nExample seeing the message during snabbnfv startup:\nsnabbnfv traffic starting (benchmark mode)\nLoading program/snabbnfv/test_fixtures/nfvconfig/test_functions/snabbnfv-bench1.port\nengine: start app B_NIC\nengine: start app B_Virtio\njit.flush\nGet features 0x28000\n VIRTIO_NET_F_CTRL_VQ VIRTIO_NET_F_MRG_RXBUF\nGet features 0x28000\n VIRTIO_NET_F_CTRL_VQ VIRTIO_NET_F_MRG_RXBUF\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nand then later while processing traffic (that is always arriving faster than it can be processed, in this benchmark):\nload: time: 1.00s  fps: 4,606,149 fpGbps: 2.690 fpb: 170 bpp: 64   sleep: 4   us\nload: time: 1.00s  fps: 4,626,202 fpGbps: 2.702 fpb: 170 bpp: 64   sleep: 4   us\njit.flush\nload: time: 1.00s  fps: 4,508,809 fpGbps: 2.633 fpb: 169 bpp: 64   sleep: 4   us\nload: time: 1.00s  fps: 4,519,463 fpGbps: 2.639 fpb: 168 bpp: 64   sleep: 5   us\n. The details of this callback are surprising to me.\nBased on the name start I would expect the callback to be run once when an app instance is first created and before any traffic is processed. Looking at the code it seems like start() is actually always called, on every app instances, each time you run engine.configure() (even if your new config is identical to the old one).\nCould you please update either the name, the description, or the implementation to make the behavior & appropriate uses more obvious?\n. Is the naming of input vs output clear to everybody else?\nPreviously we had link.transmit() incrementing the txpackets which seems like a direct match. Now we have link..transmit() incrementing the input_packets which seems more ambiguous to me. (Does the sender input packets onto a link, or does the receiver input packets from a link?)\nIf this is just me then it doesn't matter but otherwise we could consider e.g. enqueue and dequeue as perhaps being harder to misinterpret.\n. (Or even added_bytes and removed_bytes or something.)\n. Thanks! I had missed that & could well be the reason that my early tests are showing slow traces being detected but not corrected :-).\n. Could we do this in a way that is more general? This code is being merged onto other branches and called from other programs now.\n. Yes that sounds fine to me.\n. This comment is a little cryptic for me:\n\nThis implementation is recursive; we should change it to be iterative instead.\n\nI don't immediately see a problem with recursion. Could we please extend this comment to explain what the problem with recursion is, or remove the comment if recursion is actually okay, or actually rewrite this iteratively in case it is really important?\n. Question: is compute_breath_order() robust to cycles in an app network? (Could a cyclic link between apps cause an infinite recursion in the topological sort routine?)\n. I am slightly concerned that a 100-app limit could make the NFV application explode with larger configurations. Our OpenStack test suite does have a \"many VMs\" test cases but that exercises less than the maximum number of apps. I suppose that ideally I would either beef that test up (we have throttled it due to resource constraints in the nested-virtualized test env) or write a new test case.\nGenerally question on my mind is: How much stack space is okay with LuaJIT? and how much is this algorithm going to need? You seem to be plugging in values from gut feeling here? I feel like some languages blow up with around 100 stack frames while others handle millions gracefully.\nI did a quick test with a recursive factorial function and it seems okay up to around 30,000 stack frames:\n$ ./snabb snsh -e 'function factorial (n) return (n==0) and 1 or n*factorial(n-1) end  print(factorial(30000))'\ninf\nand then exhausts the stack with 35,000:\n$ ./snabb snsh -e 'function factorial (n) return (n==0) and 1 or n*factorial(n-1) end  print(factorial(35000))'\n[string \"function factorial (n) return (n==0) and 1 or...\"]:1: stack overflow\nIf this topological sort is using O(n) stack space then we should be fine to well beyond 100 apps, right?\n. ahem! That whole init() function is actually dead code. Removed in 876addc.\nThat's from an older version where core.worker imposed specific behavior on the worker process i.e. running an app network provided by the parent. These days the worker simply links itself into the process tree and runs the provided Lua code.. Request: Add an else to provide some meaningful version ID when ../.git is not available.\n(Ideally leave open the possibility for adding more ways to automatically detect the version ID in the future e.g. detecting a nix derivation name.). Request: Eliminate the special logic for building even when the core.version module is missing. Simpler to make sure this module always exists. Otherwise code that uses core.version will always need to check if it is there or not.. ",
    "rahul-mr": "Hi Luke,\nOn 01/07/2013 07:56 PM, Luke Gorrie wrote:\n\nI see that changes I made to the selftest procedure (now calling selftest2() instead of selftest() in intel.lua) means this Issue doesn't reproduce out of the box. Sorry about that. I will try to make a fix now so that running the switch reproduces the problem again. Update to follow.\nDoes the code compile and run for you btw?\n\nI can confirm that the selftest2() procedure seems to be working, but \nthe selftest() procedure is not printing any statistic.\nRegards,\nRahul\n. On 01/07/2013 08:20 PM, Luke Gorrie wrote:\n\nWow cool that it runs! :-) You are the second person after me to run the switch!\nHe He 8-)\nLooks like I have broken selftest() quite a bit with recent hacking. I will now extend selftest2() to also support receive and then we can try to reproduce the problem with that.\nOK.\nDoes the code make any sense btw? I am still learning Lua and I think especially the way I'm doing object-oriented programming - lots of \"M.\" prefixes - is a bit clunky and can be better.\n\nI haven't played with lua much (I mostly program in python, D). Since \nLua wasn't really designed for heavy-duty OOP, I guess it'll always look \na bit awkward (but hey, it works!).  The code does make sense btw :-)\nLooking forward for the updated selftest.\nRegards,\nRahul\n. I'm having a look at the updated selftest. Will report any interesting findings.\nRegards,\nRahul\n. OK, Found something interesting:\nFile: intel.lua ;  function init_receive(): Line 252:\nregs[RXDCTL] = bits({GRAN=24, WTHRESH0=16})\nthis line which sets Receiver Descriptor Control (RXDCTL) register was commented out. Un-commenting the line, has drastically cut down the Missed Packets Count, while increasing the  Receive No Buffers Count.\nBEFORE:\nStatistics for PCI device 0000:00:04.0:\n       1,109,458 MPC        Missed Packets Count\n          80,667 PRC64      Packets Received [64 Bytes] Count\n          80,667 GPRC       Good Packets Received Count\n       1,190,213 GPTC       Good Packets Transmitted Count\n       5,162,688 GORCL      Good Octets Received Count\n      76,174,336 GOTCL      Good Octets Transmitted Count\n               2 RNBC       Receive No Buffers Count\n      76,176,896 TORL       Total Octets Received (Low)\n      76,177,408 TOTL       Total Octets Transmitted (Low)\n       1,190,279 TPR        Total Packets Received\n       1,190,283 TPT        Total Packets Transmitted\n       1,190,286 PTC64      Packets Transmitted [64 Bytes] Count\nAFTER:\nStatistics for PCI device 0000:00:04.0:\n         232,479 MPC        Missed Packets Count\n         818,720 PRC64      Packets Received [64 Bytes] Count\n         818,734 GPRC       Good Packets Received Count\n       1,051,232 GPTC       Good Packets Transmitted Count\n      52,399,680 GORCL      Good Octets Received Count\n      67,279,488 GOTCL      Good Octets Transmitted Count\n              24 RNBC       Receive No Buffers Count\n      67,281,472 TORL       Total Octets Received (Low)\n      67,281,856 TOTL       Total Octets Transmitted (Low)\n       1,051,283 TPR        Total Packets Received\n       1,051,286 TPT        Total Packets Transmitted\n       1,051,289 PTC64      Packets Transmitted [64 Bytes] Count\nMaybe the thresholds in RXDCTL register needs adjustment?\n. OK, I've sent a pull request: https://github.com/SnabbCo/snabbswitch/pull/31 \n. On 01/09/2013 07:01 PM, Luke Gorrie wrote:\n\nCongratulations you are the first contributor of a patch:-)\n\nYay! :-)\n. Luke, do you think this issue should be closed as it is no longer reproducible as originally described?\n. I made a simple FDB prototype  : https://github.com/rahul-mr/snabbswitch/blob/fdb/src/fdb.lua\n(Disclaimer: it is not very efficient and may not be what you are looking for. ;-) )\nIt also tries to address issue #13 - whenever an address (key) is looked up in the FDB dictionary, it will return the corresponding port number if the entry's expiry time has not passed. Otherwise, it will set the key's value to nil (and hopefully the old value (a dictionary) gets GC'd )\nHere is the sample output when running the (modified) selftest : \n```\nFDB test\nDBG: selftest(): starting\nDBG: i=1\n F[00:00:00:00:00:00] = 0\n F[01:01:01:01:01:01] = 1\n F[02:02:02:02:02:02] = 2\nkey = 01:01:01:01:01:01 ; value = { port = 1, expiry = 277447711470485 }\n key = 00:00:00:00:00:00 ; value = { port = 0, expiry = 277445711465500 }\n key = 02:02:02:02:02:02 ; value = { port = 2, expiry = 277449711471322 }\nDBG: sleeping 2 seconds\nDBG: i=2\n F[00:00:00:00:00:00] = nil\n F[01:01:01:01:01:01] = 1\n F[02:02:02:02:02:02] = 2\nkey = 01:01:01:01:01:01 ; value = { port = 1, expiry = 277447711470485 }\n key = 02:02:02:02:02:02 ; value = { port = 2, expiry = 277449711471322 }\nDBG: sleeping 2 seconds\nDBG: i=3\n F[00:00:00:00:00:00] = nil\n F[01:01:01:01:01:01] = nil\n F[02:02:02:02:02:02] = 2\nkey = 02:02:02:02:02:02 ; value = { port = 2, expiry = 277449711471322 }\nDBG: sleeping 2 seconds\nDBG: i=4\n F[00:00:00:00:00:00] = nil\n F[01:01:01:01:01:01] = nil\n F[02:02:02:02:02:02] = nil\nDBG: sleeping 2 seconds\n```\n. Thanks for the pointers, Luke. I'll look into it.\nGood luck with the virtio stuff! :-)\n. Another throwaway prototype (based on previous discussion): https://gist.github.com/rahul-mr/4734924\n. Pete, if you were referring to the 2 second aging time in the \"throwaway\" prototype, it was meant for testing purposes only  - I wanted a quick, simple test (hence maximum entries = 5, timeout = 2 sec) for the data structure proposed by Luke.\n. On 01/12/2013 03:30 PM, Luke Gorrie wrote:\n\nThe commit above adds a failing test case. I will hack some code to construct a TCP/IP header next and then hand back to you Rahul.\n\nThanks for the testcase. the TCP/IP header is not urgent - I can try \nwriting one myself for the test case. I just wanted to avoid code \nduplication (in case I somehow missed it in the source files).\nRegards,\nRahul\nPS: I accidentally closed the original github comment (and I think I \ncannot undo it - which is a shame). Probably I should stick to email \nreplies :-)\n. On 01/12/2013 04:19 PM, Luke Gorrie wrote:\n\nCool I'll leave it for now then.\nGithub does take some getting used to. I'm impressed with the email integration.\nGithub does a lot of impressive stuff. Apart from this minor quirk, it \nhas worked quite well for me (I guess I'm used to being asked for \nconfirmation before I delete stuff).\n\nSince you are working on memory mapping stuff, have you looked at IOMMU \n? It seems many processors support it nowadays and it looks like a good \nfit for snabbswitch.\nRegards,\nRahul\n. On 01/12/2013 07:57 PM, pkazmier wrote:\n\nRe: IRC, I'm sitting in #snabsswitch on freenode rahul.\n\nCool, I'll check out #snabsswitch :-)\n. Update: It seems Luajit doesn't support using packed bit fields like so:\nC\n   struct tx_context_desc {\n            unsigned int tucse:16,\n                         tucso:8,\n                         tucss:8,\n                         ipcse:16,\n                         ipcso:8,\n                         ipcss:8,\n                         mss:16,\n                         hdrlen:8,\n                         rsv:2,\n                         sta:4,\n                         tucmd:8,\n                         dtype:4,\n                         paylen:20;\n         } __attribute__((packed));\nlua\n    local function add_txbuf_tso (address, size, mss, context)\n      print \"DBG: starting add_txbuf_tso\"\n      ui_tdt = ffi.cast(\"uint32_t\", 0)\n      ui_tdt = tdt\n      ctx = ffi.cast(\"struct tx_context_desc *\", txdesc._ptr + ui_tdt)\n      ctx.tucse  = 0    --TCP/UDP CheckSum End\n      ctx.tucso  = 0    --TCP/UDP CheckSum Offset\nRunning selftest_tso results in the following traceback:\nintel.lua:422: NYI: packed bit fields\nstack traceback:\n    main.lua:17: in function '__newindex'\n    intel.lua:422: in function 'add_txbuf_tso'\n    intel.lua:766: in function 'add_tso_test_buffer'\n    intel.lua:736: in function 'selftest_tso'\n    selftest.lua:31: in main chunk\n    [C]: in function 'require'\n    main.lua:11: in function <main.lua:3>\n    [C]: in function 'xpcall'\n    main.lua:22: in main chunk\n    [C]: in function 'require'\n    [string \"require \"main\"\"]:1: in main chunk\nLuajit source https://github.com/LuaDist/luajit/blob/master/src/lj_errmsg.h#L184 seems to confirm it.\n. Note: work related to this issue is committed to the issue33 branch here: https://github.com/rahul-mr/snabbswitch/commits/issue33\nThe up-to-date version of intel.lua: https://github.com/rahul-mr/snabbswitch/blob/issue33/src/intel.lua\nBTW it needs a lot of testing/fixing :-)\n. > Good direct workaround to the problems with constructing a TCP/IP header in code :)\nbeing lazy helps some times :-D\n. On 01/16/2013 06:07 PM, Luke Gorrie wrote:\n\nTook this for a spin now! Here is the output from my run:\n\nThere are a few things that need fixing (for e.g.: two register values \naren't set correctly - i've left a couple of notes in the code). I'm \nworking on it.\n. Update: add_txbuf_tso() and selftest_tso() started showing signs of life :-) https://github.com/rahul-mr/snabbswitch/commits/mem_high_issue33\nHere is the output from the latest test run (with debugging info removed):\nNIC transmit tso test - defaults\nselftest: TCP Segmentation Offload (TSO)\nwaiting for old traffic to die out ...\nWaiting for linkup............. ok\nStatistics for PCI device 0000:05:00.0:\nadding tso test buffer...\nwaiting for packet transmission...\nStatistics for PCI device 0000:05:00.0:\n                   1 GPTC       Good Packets Transmitted Count\n                  64 GOTCL      Good Octets Transmitted Count\n                  64 TOTL       Total Octets Transmitted (Low)\n                   1 TPT        Total Packets Transmitted\n                   1 PTC64      Packets Transmitted [64 Bytes] Count\nsize    mss     txtcp   txeth   txhw\n4       1500    1       1       1\nNIC transmit tso test - size=4096, mss=1500\nselftest: TCP Segmentation Offload (TSO)\nwaiting for old traffic to die out ...\nWaiting for linkup............ ok\nStatistics for PCI device 0000:05:00.0:\nadding tso test buffer...\nwaiting for packet transmission...\nStatistics for PCI device 0000:05:00.0:\n                   3 GPTC       Good Packets Transmitted Count\n               4,270 GOTCL      Good Octets Transmitted Count\n               4,270 TOTL       Total Octets Transmitted (Low)\n                   3 TPT        Total Packets Transmitted\n                   3 PTC1522    Packets Transmitted [Greater than 1024 Bytes] Count\n                   1 TSCTC      TCP Segmentation Context Transmitted Count\nsize    mss     txtcp   txeth   txhw\n4096    1500    1       3       3\nNIC transmit+receive loopback tso test - size=4096, mss=1500\nselftest: TCP Segmentation Offload (TSO)\nwaiting for old traffic to die out ...\nWaiting for linkup............ ok\nStatistics for PCI device 0000:05:00.0:\nadding receive buffers...\nadding tso test buffer...\nwaiting for packet transmission...\nStatistics for PCI device 0000:05:00.0:\n                   3 PRC1522    Packets Received [1024 to Max Bytes] Count\n                   3 GPRC       Good Packets Received Count\n                   3 GPTC       Good Packets Transmitted Count\n               4,270 GORCL      Good Octets Received Count\n               4,270 GOTCL      Good Octets Transmitted Count\n               4,270 TORL       Total Octets Received (Low)\n               4,270 TOTL       Total Octets Transmitted (Low)\n                   3 TPR        Total Packets Received\n                   3 TPT        Total Packets Transmitted\n                   3 PTC1522    Packets Transmitted [Greater than 1024 Bytes] Count\n                   1 TSCTC      TCP Segmentation Context Transmitted Count\nsize    mss     txtcp   txeth   txhw\n4096    1500    1       3       3\n. > Looks like TSO in action i.e. hardware sending multiple physical frames for one large software frame?\nYes :-)\nI've tested it only for the following scenario:\nInput: 1 Ethernet frame (without CRC) containing an { IPv4 + TCP } payload [TCP payload size (without headers) = 4096 bytes]\nInput frame size (without CRC) = ({Ethernet + IPv4 + TCP} headers) + (TCP payload) \n                               = 54 + 4096 \n                               = 4150 bytes\nOutput: 3 Ethernet frames of maximum size 1504 bytes (Headers + TCP payload  = 1500 bytes ; CRC = 4 bytes)\n```\nOutput frame size = {Ethernet + IPv4 + TCP} headers  + Max TCP payload + Ethernet CRC\n                  = 54 + 1446 (max) + 4\n                  = 1504 bytes (max)                                       \nTotal bytes transmitted = 2 * (54 + 1446 + 4) + (54 + (4096 - (2*1446)) + 4) \n                        = 4270 bytes\n```\nThe TCP payload segmentation and the output frames' IPv4 packet length/checksum,TCP checksum header fields etc. is handled by the NIC hardware.\nBTW It needs more test cases (IPv4+UDP, IPv6+{TCP,UDP}).\nEdit: change line Output:  3 Ethernet ...\n. Update: I've upated the TSO selftests: https://github.com/rahul-mr/snabbswitch/commits/mem_high_issue33\nLooks like {TCP,UDP)/{IPv4,IPv6} segmentation is working :-)\nHere's the output of the current testcases:\nNIC tx tso test - defaults (TCP, IPv4, size=4, mss=1442)\nselftest: TCP Segmentation Offload (TSO)\nWaiting for linkup............... ok\n[Before]\nStatistics for PCI device 0000:05:00.0:\n[After]\nStatistics for PCI device 0000:05:00.0:\n                   1 GPTC       Good Packets Transmitted Count\n                  64 GOTCL      Good Octets Transmitted Count\n                  64 TOTL       Total Octets Transmitted (Low)\n                   1 TPT        Total Packets Transmitted\n                   1 PTC64      Packets Transmitted [64 Bytes] Count\nsize    mss     txtcp   txeth   txhw\n4       1442    1       1       1\nNIC tx tso test - TCP, IPv4, size=4096, mss=1442\nselftest: TCP Segmentation Offload (TSO)\nWaiting for linkup.............. ok\n[Before]\nStatistics for PCI device 0000:05:00.0:\n[After]\nStatistics for PCI device 0000:05:00.0:\n                   3 GPTC       Good Packets Transmitted Count\n               4,216 GOTCL      Good Octets Transmitted Count\n               4,216 TOTL       Total Octets Transmitted (Low)\n                   3 TPT        Total Packets Transmitted\n                   3 PTC1522    Packets Transmitted [Greater than 1024 Bytes] Count\n                   1 TSCTC      TCP Segmentation Context Transmitted Count\nsize    mss     txtcp   txeth   txhw\n4096    1442    1       3       3\nNIC tx+rx loopback tso test - TCP, IPv4, size=4096, mss=1442\nselftest: TCP Segmentation Offload (TSO)\nWaiting for linkup.............. ok\n[Before]\nStatistics for PCI device 0000:05:00.0:\n[After]\nStatistics for PCI device 0000:05:00.0:\n                   3 PRC1522    Packets Received [1024 to Max Bytes] Count\n                   3 GPRC       Good Packets Received Count\n                   3 GPTC       Good Packets Transmitted Count\n               4,216 GORCL      Good Octets Received Count\n               4,216 GOTCL      Good Octets Transmitted Count\n               4,216 TORL       Total Octets Received (Low)\n               4,216 TOTL       Total Octets Transmitted (Low)\n                   3 TPR        Total Packets Received\n                   3 TPT        Total Packets Transmitted\n                   3 PTC1522    Packets Transmitted [Greater than 1024 Bytes] Count\n                   1 TSCTC      TCP Segmentation Context Transmitted Count\nsize    mss     txtcp   txeth   txhw\n4096    1442    1       3       3\nNIC tx tso test - UDP, IPv4, size=4096, mss=1454\nselftest: TCP Segmentation Offload (TSO)\nWaiting for linkup.............. ok\n[Before]\nStatistics for PCI device 0000:05:00.0:\n[After]\nStatistics for PCI device 0000:05:00.0:\n                   3 GPTC       Good Packets Transmitted Count\n               4,192 GOTCL      Good Octets Transmitted Count\n               4,192 TOTL       Total Octets Transmitted (Low)\n                   3 TPT        Total Packets Transmitted\n                   3 PTC1522    Packets Transmitted [Greater than 1024 Bytes] Count\n                   1 TSCTC      TCP Segmentation Context Transmitted Count\nsize    mss     txtcp   txeth   txhw\n4096    1454    1       3       3\nNIC tx+rx loopback tso test - UDP, IPv4, size=4096, mss=1454\nselftest: TCP Segmentation Offload (TSO)\nWaiting for linkup............ ok\n[Before]\nStatistics for PCI device 0000:05:00.0:\n[After]\nStatistics for PCI device 0000:05:00.0:\n                   3 PRC1522    Packets Received [1024 to Max Bytes] Count\n                   3 GPRC       Good Packets Received Count\n                   3 GPTC       Good Packets Transmitted Count\n               4,192 GORCL      Good Octets Received Count\n               4,192 GOTCL      Good Octets Transmitted Count\n               4,192 TORL       Total Octets Received (Low)\n               4,192 TOTL       Total Octets Transmitted (Low)\n                   3 TPR        Total Packets Received\n                   3 TPT        Total Packets Transmitted\n                   3 PTC1522    Packets Transmitted [Greater than 1024 Bytes] Count\n                   1 TSCTC      TCP Segmentation Context Transmitted Count\nsize    mss     txtcp   txeth   txhw\n4096    1454    1       3       3\nNIC tx tso test - TCP, IPv6, size=4096, mss=1422\nselftest: TCP Segmentation Offload (TSO)\nWaiting for linkup.............. ok\n[Before]\nStatistics for PCI device 0000:05:00.0:\n[After]\nStatistics for PCI device 0000:05:00.0:\n                   3 GPTC       Good Packets Transmitted Count\n               4,256 GOTCL      Good Octets Transmitted Count\n               4,256 TOTL       Total Octets Transmitted (Low)\n                   3 TPT        Total Packets Transmitted\n                   3 PTC1522    Packets Transmitted [Greater than 1024 Bytes] Count\n                   1 TSCTC      TCP Segmentation Context Transmitted Count\nsize    mss     txtcp   txeth   txhw\n4096    1422    1       3       3\nNIC tx+rx loopback tso test - TCP, IPv6, size=4096, mss=1422\nselftest: TCP Segmentation Offload (TSO)\nWaiting for linkup............ ok\n[Before]\nStatistics for PCI device 0000:05:00.0:\n[After]\nStatistics for PCI device 0000:05:00.0:\n                   3 PRC1522    Packets Received [1024 to Max Bytes] Count\n                   3 GPRC       Good Packets Received Count\n                   3 GPTC       Good Packets Transmitted Count\n               4,256 GORCL      Good Octets Received Count\n               4,256 GOTCL      Good Octets Transmitted Count\n               4,256 TORL       Total Octets Received (Low)\n               4,256 TOTL       Total Octets Transmitted (Low)\n                   3 TPR        Total Packets Received\n                   3 TPT        Total Packets Transmitted\n                   3 PTC1522    Packets Transmitted [Greater than 1024 Bytes] Count\n                   1 TSCTC      TCP Segmentation Context Transmitted Count\nsize    mss     txtcp   txeth   txhw\n4096    1422    1       3       3\nNIC tx tso test - UDP, IPv6, size=4096, mss=1434\nselftest: TCP Segmentation Offload (TSO)\nWaiting for linkup............ ok\n[Before]\nStatistics for PCI device 0000:05:00.0:\n[After]\nStatistics for PCI device 0000:05:00.0:\n                   3 GPTC       Good Packets Transmitted Count\n               4,232 GOTCL      Good Octets Transmitted Count\n               4,232 TOTL       Total Octets Transmitted (Low)\n                   3 TPT        Total Packets Transmitted\n                   3 PTC1522    Packets Transmitted [Greater than 1024 Bytes] Count\n                   1 TSCTC      TCP Segmentation Context Transmitted Count\nsize    mss     txtcp   txeth   txhw\n4096    1434    1       3       3\n```\nNIC tx+rx loopback tso test - UDP, IPv6, size=4096, mss=1434\nselftest: TCP Segmentation Offload (TSO)\nWaiting for linkup.............. ok\n[Before]\nStatistics for PCI device 0000:05:00.0:\n[After]\nStatistics for PCI device 0000:05:00.0:\n                   3 PRC1522    Packets Received [1024 to Max Bytes] Count\n                   3 GPRC       Good Packets Received Count\n                   3 GPTC       Good Packets Transmitted Count\n               4,232 GORCL      Good Octets Received Count\n               4,232 GOTCL      Good Octets Transmitted Count\n               4,232 TORL       Total Octets Received (Low)\n               4,232 TOTL       Total Octets Transmitted (Low)\n                   3 TPR        Total Packets Received\n                   3 TPT        Total Packets Transmitted\n                   3 PTC1522    Packets Transmitted [Greater than 1024 Bytes] Count\n                   1 TSCTC      TCP Segmentation Context Transmitted Count\nsize    mss     txtcp   txeth   txhw\n4096    1434    1       3       3\n```\n. It seems the 82574 doesn't have Large Receive Offload support (inverse of TSO). The 82599 NIC has got it (Intel calls it Receive Side Coalescing (RSC) )\n. Update: added support for multiple descriptors and vlan tagging : https://github.com/rahul-mr/snabbswitch/commits/stt\n(new branch)\nAlso, a boatload of selftests to go with the added features ;-)\n. On 01/12/2013 02:55 PM, Luke Gorrie wrote:\n\n@rahul-mr could be interesting for you since you have already been hacking in this area?\n\n@lukego I'll have a look at it.\n. Great find, Pete! \nTesting in bern (using current master) the RNBC is eliminated but I'm still getting MPC:\nNIC transmit test\nintel selftest: pciaddr=0000:05:00.0 secs=1\nWaiting for linkup............ ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:05:00.0:\n           1,207,661 GPTC       Good Packets Transmitted Count\n          77,291,072 GOTCL      Good Octets Transmitted Count\n          77,294,592 TOTL       Total Octets Transmitted (Low)\n           1,207,741 TPT        Total Packets Transmitted\n           1,207,744 PTC64      Packets Transmitted [64 Bytes] Count\nNIC transmit+receive loopback test\nintel selftest: pciaddr=0000:05:00.0 secs=1 receive=true loopback=true\nWaiting for linkup............. ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:05:00.0:\n             115,436 MPC        Missed Packets Count\n             930,656 PRC64      Packets Received [64 Bytes] Count\n             930,668 GPRC       Good Packets Received Count\n           1,046,125 GPTC       Good Packets Transmitted Count\n          59,563,584 GORCL      Good Octets Received Count\n          66,952,576 GOTCL      Good Octets Transmitted Count\n          66,954,432 TORL       Total Octets Received (Low)\n          66,954,816 TOTL       Total Octets Transmitted (Low)\n           1,046,175 TPR        Total Packets Received\n           1,046,176 TPT        Total Packets Transmitted\n           1,046,179 PTC64      Packets Transmitted [64 Bytes] Count\n\nRahul if you would find it useful to reboot bern/arbon/VM please feel free, they should reboot cleanly (tell me if you need the KVM command line for arbon - should be in root's shell history and directory ~/vm).\n\nThanks, Luke. I've restarted bern.\n. Hi all,\nToday I looked into this a bit. It seems the problem was with adjusting the values in RX regs  (like I've mentioned in Issue #1):\nupdated settings (in function init_receive):\nregs[RXDCTL] = bits({ GRAN=24, PTHRESH1=1, HTHRESH1=9, WTHRESH1=17 })\n      regs[RXCSUM] = 0                 -- Disable checksum offload - not needed\n      regs[RADV] = 1     --  1 * 1us rx intrrupt absolute delay\n      regs[RDTR] = 10    -- 10 * 1us rx interrupt delay timer\nupdated output:\n```\nNIC transmit test\nintel selftest: pciaddr=0000:00:04.0 secs=1\nWaiting for linkup............ ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:00:04.0:\n           1,203,908 GPTC       Good Packets Transmitted Count\n          77,050,944 GOTCL      Good Octets Transmitted Count\n          77,061,760 TOTL       Total Octets Transmitted (Low)\n           1,204,100 TPT        Total Packets Transmitted\n           1,204,103 PTC64      Packets Transmitted [64 Bytes] Count\nNIC transmit+receive loopback test\nintel selftest: pciaddr=0000:00:04.0 secs=1 receive=true loopback=true\nWaiting for linkup.............. ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:00:04.0:\n           1,012,516 PRC64      Packets Received [64 Bytes] Count\n           1,012,534 GPRC       Good Packets Received Count\n           1,012,544 GPTC       Good Packets Transmitted Count\n          64,803,008 GORCL      Good Octets Received Count\n          64,803,584 GOTCL      Good Octets Transmitted Count\n          64,805,824 TORL       Total Octets Received (Low)\n          64,805,952 TOTL       Total Octets Transmitted (Low)\n           1,012,599 TPR        Total Packets Received\n           1,012,604 TPT        Total Packets Transmitted\n           1,012,607 PTC64      Packets Transmitted [64 Bytes] Count\n```\nEDIT: Edited for clarity \nNote: I guess the threshold values can be tweaked further  ;-)\n. > Cool! I'll read up on those register settings in the morning. Nice find.\nPete, Your detective work inspired me to take a closer look at this issue. So, thank you for the inspiration and please continue your detective work! :-)\n. On 02/10/2013 12:52 AM, pkazmier wrote:\n\nRahul, based on your patch, it looks like you solved the MPCs on bern/arbon by choosing a value of 2 for PTHRESH, HTHRESH, and WTHRESH. In addition, you set the timers RDTR to 10 and RADV to 1. How did you come to choose these values?\n\nTrial and Error FTW ;-) Based on the info from the 82574L DS, I tried a \nset of values that seemed sensible - the DS gives some general \nguidelines for choosing the threshold values (and ofcourse, the range of \nvalues). The values were then tweaked based on feedback from the \ntransmit+receive loopback selftest.\nBTW I guess setting the threshold values to the e1000e driver defaults \nshouldn't be a problem. Given that ATM we only have a tx+rx loopback \nselftest running for a second, I wouldn't expect any drastic change in \nthe results. But in the future, I expect these values to be tweaked \nbased on actual load experienced by the NIC.\nRegards,\nRahul\n. I hope the NIC firmware is up-to-date.\nHave you tried testing it with ethtool (after the e1000e driver is loaded)?\n. On 01/15/2013 10:28 PM, Luke Gorrie wrote:\n\nBooting with intel_iommu=off does not help.\n\ndunno if this helps, but can u try  booting kernel with \niommu=force,memaper=3 (taken from here: \nhttp://whiteboard.ping.se/Linux/IOMMU)\ncheck this also:\nhttp://www.mjmwired.net/kernel/Documentation/x86/x86_64/boot-options.txt#230\n. Hmm, it seems the latest commit https://github.com/SnabbCo/snabbswitch/commit/f23d5cb464ec46cb7de5c423b89c4ec491ec633e doesn't fix this issue. It is interesting to note that during testing with the current master, the 'high' 32 bits of the rx/tx descriptor base addresses always remained 0x00000000 \n. Sample run in arbon:\nselftest: memory\nKernel HugeTLB pages (/proc/sys/vm/nr_hugepages): 10\n  Allocating a 2MB HugeTLB: Got 2MB at 0x3aa00000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x32000000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x20800000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x20200000\nKernel HugeTLB pages (/proc/sys/vm/nr_hugepages): 10\nHugeTLB page allocation OK.\nselftest: pci\nScanning PCI devices:\npciaddr         vendor  device  iface   status\n0000:00:00.0    0x8086  0x1237  -       -\n0000:00:01.0    0x8086  0x7000  -       -\n0000:00:01.1    0x8086  0x7010  -       -\n0000:00:01.3    0x8086  0x7113  -       -\n0000:00:02.0    0x1013  0x00b8  -       -\n0000:00:03.0    0x10ec  0x8139  eth0    up\n0000:00:04.0    0x8086  0x10d3  -       -\n0000:00:05.0    0x1af4  0x1001  -       -\nSuitable devices: \n  0000:00:04.0\nselftest: intel device 0000:00:04.0\nNIC transmit test\nintel selftest: pciaddr=0000:00:04.0 secs=1\nWaiting for linkup............. ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:00:04.0:\nNIC transmit+receive loopback test\nintel selftest: pciaddr=0000:00:04.0 secs=1 receive=true loopback=true\nWaiting for linkup............ ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:00:04.0:\nWhile it is working in Bern:\nselftest: memory\nKernel HugeTLB pages (/proc/sys/vm/nr_hugepages): 8\n  Allocating a 2MB HugeTLB: Got 2MB at 0xfb200000\n  Allocating a 2MB HugeTLB: Got 2MB at 0xfb400000\n  Allocating a 2MB HugeTLB: Got 2MB at 0xfba00000\n  Allocating a 2MB HugeTLB: Got 2MB at 0xdb400000\nKernel HugeTLB pages (/proc/sys/vm/nr_hugepages): 8\nHugeTLB page allocation OK.\nselftest: pci\nScanning PCI devices:\npciaddr         vendor  device  iface   status\n0000:00:00.0    0x8086  0x0158  -       -\n0000:00:01.0    0x8086  0x0151  -       -\n0000:00:02.0    0x8086  0x016a  -       -\n0000:00:06.0    0x8086  0x015d  -       -\n0000:00:16.0    0x8086  0x1c3a  -       -\n0000:00:1a.0    0x8086  0x1c2d  -       -\n0000:00:1c.0    0x8086  0x1c10  -       -\n0000:00:1c.5    0x8086  0x1c1a  -       -\n0000:00:1c.6    0x8086  0x1c1c  -       -\n0000:00:1c.7    0x8086  0x1c1e  -       -\n0000:00:1d.0    0x8086  0x1c26  -       -\n0000:00:1e.0    0x8086  0x244e  -       -\n0000:00:1f.0    0x8086  0x1c56  -       -\n0000:00:1f.2    0x8086  0x1c02  -       -\n0000:00:1f.3    0x8086  0x1c22  -       -\n0000:04:00.0    0x8086  0x10d3  eth0    up\n0000:05:00.0    0x8086  0x10d3  -       -\n0000:06:00.0    0x1b21  0x1042  -       -\nSuitable devices: \n  0000:05:00.0\nselftest: intel device 0000:05:00.0\nNIC transmit test\nintel selftest: pciaddr=0000:05:00.0 secs=1\nWaiting for linkup.............. ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:05:00.0:\n           1,207,447 GPTC       Good Packets Transmitted Count\n          77,277,376 GOTCL      Good Octets Transmitted Count\n          77,280,320 TOTL       Total Octets Transmitted (Low)\n           1,207,517 TPT        Total Packets Transmitted\n           1,207,520 PTC64      Packets Transmitted [64 Bytes] Count\nNIC transmit+receive loopback test\nintel selftest: pciaddr=0000:05:00.0 secs=1 receive=true loopback=true\nWaiting for linkup............. ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:05:00.0:\n             179,933 MPC        Missed Packets Count\n             868,259 PRC64      Packets Received [64 Bytes] Count\n             868,276 GPRC       Good Packets Received Count\n           1,048,224 GPTC       Good Packets Transmitted Count\n          55,570,176 GORCL      Good Octets Received Count\n          67,087,040 GOTCL      Good Octets Transmitted Count\n                  39 RNBC       Receive No Buffers Count\n          67,088,960 TORL       Total Octets Received (Low)\n          67,089,408 TOTL       Total Octets Transmitted (Low)\n           1,048,275 TPR        Total Packets Received\n           1,048,280 TPT        Total Packets Transmitted\n           1,048,282 PTC64      Packets Transmitted [64 Bytes] Count\nNote that I've been running the tests only in arbon till now. So, maybe this issue happens only when running under KVM ?\n. Update: the issue is now showing up in bern:\nselftest: memory\nKernel HugeTLB pages (/proc/sys/vm/nr_hugepages): 10\n  Allocating a 2MB HugeTLB: Got 2MB at 0xfb600000\n  Allocating a 2MB HugeTLB: Got 2MB at 0xfb800000\n  Allocating a 2MB HugeTLB: Got 2MB at 0xdb400000\n  Allocating a 2MB HugeTLB: Got 2MB at 0xfba00000\nKernel HugeTLB pages (/proc/sys/vm/nr_hugepages): 10\nHugeTLB page allocation OK.\nselftest: pci\nScanning PCI devices:\npciaddr         vendor  device  iface   status\n0000:00:00.0    0x8086  0x0158  -       -\n0000:00:01.0    0x8086  0x0151  -       -\n0000:00:02.0    0x8086  0x016a  -       -\n0000:00:06.0    0x8086  0x015d  -       -\n0000:00:16.0    0x8086  0x1c3a  -       -\n0000:00:1a.0    0x8086  0x1c2d  -       -\n0000:00:1c.0    0x8086  0x1c10  -       -\n0000:00:1c.5    0x8086  0x1c1a  -       -\n0000:00:1c.6    0x8086  0x1c1c  -       -\n0000:00:1c.7    0x8086  0x1c1e  -       -\n0000:00:1d.0    0x8086  0x1c26  -       -\n0000:00:1e.0    0x8086  0x244e  -       -\n0000:00:1f.0    0x8086  0x1c56  -       -\n0000:00:1f.2    0x8086  0x1c02  -       -\n0000:00:1f.3    0x8086  0x1c22  -       -\n0000:04:00.0    0x8086  0x10d3  eth0    up\n0000:05:00.0    0x8086  0x10d3  -       -\n0000:06:00.0    0x1b21  0x1042  -       -\nSuitable devices: \n  0000:05:00.0\nselftest: intel device 0000:05:00.0\nNIC transmit test\nintel selftest: pciaddr=0000:05:00.0 secs=1\nWaiting for linkup.............. ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:05:00.0:\nNIC transmit+receive loopback test\nintel selftest: pciaddr=0000:05:00.0 secs=1 receive=true loopback=true\nWaiting for linkup.............. ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:05:00.0:\n. Update (on bern):\n1) I have tried using memory.install() to use the memmap-ed address space for the {rx,tx}desc/buffers but still couldn't get the NIC to print statistics.\n2) Tried to bind the pci device (0000:05:00.0) to the e1000e device driver:\n```\necho -n \"0000:05:00.0\" > /sys/bus/pci/drivers/e1000e/bind\ndmesg | tail -n 10\n[503057.724204] e1000e 0000:05:00.0: Disabling ASPM L0s L1\n[503057.724224] e1000e 0000:05:00.0: PCI INT A -> GSI 18 (level, low) -> IRQ 18\n[503057.724249] e1000e 0000:05:00.0: setting latency timer to 64\n[503057.724533] e1000e 0000:05:00.0: irq 56 for MSI/MSI-X\n[503057.724540] e1000e 0000:05:00.0: irq 57 for MSI/MSI-X\n[503057.724546] e1000e 0000:05:00.0: irq 58 for MSI/MSI-X\n[503057.999025] e1000e 0000:05:00.0: eth1: (PCI Express:2.5GT/s:Width x1) 30:85:a9:a3:c5:38\n[503057.999030] e1000e 0000:05:00.0: eth1: Intel(R) PRO/1000 Network Connection\n[503057.999109] e1000e 0000:05:00.0: eth1: MAC: 3, PHY: 8, PBA No: FFFFFF-0FF\n```\n3) Ran the ethtool on eth1:\n```\nroot@bern:~# ethtool -t eth1\nThe test result is FAIL\nThe test extra info:\nRegister test  (offline)         0\nEeprom test    (offline)         0\nInterrupt test (offline)         0\nLoopback test  (offline)         13\nLink test   (on/offline)         0\nroot@bern:~# ethtool -t eth1 online\nThe test result is PASS\nThe test extra info:\nRegister test  (offline)         0\nEeprom test    (offline)         0\nInterrupt test (offline)         0\nLoopback test  (offline)         0\nLink test   (on/offline)         0\n```\n4) Unbind the device:\n```\necho -n \"0000:05:00.0\" > /sys/bus/pci/drivers/e1000e/unbind\n```\n5) Ran selftest again and still couldn't get the NIC to print any statistic.\n. @pkazmier:\n\nWeird, I still don't see it on my machine. I ran the selftest 2,997 times over night and it's still working. Rahul, are you executing a different part of the code than I am? I'm only running the selftest as defined in selftest.lua. I'll leave my loop going all day long while I'm off at work to see if I can get it to happen.\n\nI'm working on issue #33 ATM (branch I'm working on: https://github.com/rahul-mr/snabbswitch/commits/mem_high_issue33 ). The selftest.lua I'm using includes selftest_tso() which ends up calling add_txbuf_tso().  I'm reviewing the code I've written to see if I ended up doing something stupid. Will you be able to test it in your hardware (disclaimer: alpha quality code ;-) ) to see if that triggers this issue? But IIRC the issue got triggered y'day in arbon while running the master branch. \nMy concern is that whatever triggered the DMA access problem, we should still be able to get the NIC to work without a reboot.\n@lukego:\n\nI wonder if there are some new tools we could add to our toolbox for this situation?\n\nThough it isn't much, I've added the definitions of the rx/tx diagnostics registers to help with the debugging.\n. Update:\nFWIW segfaults shown in kernel log messages:\nArbon:\n```\n$ dmesg | grep -i snabb\n[231898.115174] snabbswitch[18548]: segfault at 20200004 ip 000000000043f279 sp 00007fffff752e90 error 4 in snabbswitch[400000+77000]\n[276311.218457] snabbswitch[9178]: segfault at 20a65a89 ip 000000000043f279 sp 00007fff568091c0 error 4 in snabbswitch[400000+76000]\n[276375.002616] snabbswitch[9245]: segfault at 20865a89 ip 000000000043f279 sp 00007fff367a2a20 error 4 in snabbswitch[400000+76000]\n$ addr2line -e src/snabbswitch 000000000043f279\nlj_crecord.c:0\n```\nBern:\n```\n$ dmesg | grep -i snabb\n[ 1220.493591] snabbswitch[2266]: segfault at 0 ip 000000000043f2f1 sp 00007fff359b8100 error 6 in snabbswitch[400000+76000]\n[ 1252.896728] snabbswitch[2280]: segfault at 0 ip 000000000043f2f1 sp 00007fffee7b97c0 error 6 in snabbswitch[400000+76000]\n[ 1765.469630] snabbswitch[2300]: segfault at 0 ip 000000000043f2f1 sp 00007ffffdc87a40 error 6 in snabbswitch[400000+76000]\n[ 1783.146961] snabbswitch[2312]: segfault at 0 ip 000000000043f2f1 sp 00007fff16c8eaa0 error 6 in snabbswitch[400000+76000]\n[ 1803.573392] snabbswitch[2324]: segfault at 0 ip 000000000043f2f1 sp 00007fff9c046890 error 6 in snabbswitch[400000+76000]\n$ addr2line -e ~/prog/snabbswitch/src/snabbswitch 000000000043f2f1\nlj_crecord.c:0\n```\nNow I'm using gdb to run snabbswitch (using a debug build of luajit and snabbswitch) - no segfaults yet.\n. On 01/23/2013 01:10 AM, Luke Gorrie wrote:\n\nI have also written some code (uncommitted) to decode the kpageflags file\nto see information about physical pages (e.g. does the kernel agree that\nthey are part of Huge pages, etc). I put this code down for the moment when\nit seemed like everything was working:-)\nInteresting!\nOne idea with Arbon: Can we have broken the VM by running something (e.g.\nsnabbswitch or e1000e bind) on the host? I don't_think_  I have done that\nsince the last reboot but I can imagine it may mess up the IOMMU trickery\nthat makes PCIe bypass work.\nHmm, I've used the e1000e bind only on Bern. But I agree that I/we might \nhave inadvertently broken the PCI passthrough functionality ;-)\n. > Just got home from work, it's still running fine on my machine (5,729 runs now) and no segfaults in my dmesg. Nice find Rahul.\n\nThanks, Pete :-)\nBTW did you find any significant change in the MPC (Missed Packets Count) and RNBC (Receive No Buffers Count) in the NIC transmit+receive loopback test with the current master?\n. Update: I would like to start with an apology - it seems the issue is triggered only when I run add_txbuf_tso. So the earlier report that the issue was triggered with the @snabbco master branch seems to be incorrect (It seems I may have mixed up branches while testing. To avoid the possibility of mixing up branches and getting incorrect findings, from now onwards I'll make a fresh git clone of @snabbco snabbswitch master and test with that). sorry for the incorrect findings :-(\nNow on to the good news: The issue is not in accessing DMA memory, but the reset function doesn't set the GIO Master Disable bit (of CTRL register) and wait for GIO Master Enable Status bit to be cleared (in STATUS register).\nThis is the updated reset():\nlua\n  function reset ()\n      regs[IMC] = 0xffffffff                 -- Disable interrupts\n      regs[CTRL] = bit.bor(regs[CTRL], bits({GMD=2})) -- GIO Master Disable\n      --local deadline = C.get_time_ns() + \n      C.usleep(10)\n      assert( not bitset(regs[STATUS], 19) ) -- GIO Master Enable Status \n      regs[CTRL] = bits({FD=0,SLU=6,RST=26,PHY_RST=31}) -- Global reset\n      C.usleep(10); assert( not bitset(regs[CTRL],26) )\n      regs[IMC] = 0xffffffff                 -- Disable interrupts\n   end\nNow the add_txbuf_tso works as expected (multiple runs without any errors) :-)\nwaiting for packet transmission...\nStatistics for PCI device 0000:05:00.0:\n                   1 GPTC       Good Packets Transmitted Count\n                  64 GOTCL      Good Octets Transmitted Count\n                  64 TOTL       Total Octets Transmitted (Low)\n                   1 TPT        Total Packets Transmitted\n                   1 PTC64      Packets Transmitted [64 Bytes] Count\nsize    mss     txtcp   txeth   txhw\n4       1500    1       1       1\nSo one possible explanation for the previously observed behaviour is that since a Device Reset (RST) will not clear the NIC's internal  Packet Buffer Allocation register (and possibly other undocumented internal registers?) and since we haven't issued a GIO Master Disable, the NIC will not clear all the pending requests (Page 35, Page 295 of 82574L DS)\nEdit: 82574L not 83574L ; grammar fix\n. Update: A new issue has cropped up now: assert( not bitset(regs[STATUS], 19) ) fails sometimes. Investigating...\nUpdate2: It seems calling M.init() at the end of selftest_tso() fixes the assertion issue.\nUpdate3: Page63 of DS - types of reset\n. Update: the following reset() seems to work:\n``` lua\n   function reset ()\n      regs[IMC] = 0xffffffff                 -- Disable interrupts\n      regs[CTRL] = bit.bor(regs[CTRL], bits({GMD=2})) -- Set GIO Master Disable\n      C.usleep(1000) -- wait 1ms to clear all pending requests\n      print(\"DBG: reset: GIO Master Enable Status: \"..tostring(bitset(regs[STATUS], 19)) ) --GIO Master Enable Status \n      regs[CTRL] = bits({FD=0,SLU=6,RST=26,PHY_RST=31}) -- Global reset [ will (hopefully!) clear GIO Master Disable ]\n      C.usleep(10); assert( not bitset(regs[CTRL],26) )\n      regs[IMC] = 0xffffffff                 -- Disable interrupts\n   end\n```\n. > Hi Rahul, where did you find the recommendation to disable the GIO master? I'm trying to learn what this is as its not documented well in my DS (or more likely I don't understand what's going on). How was this causing the issue? I assume I'll need to add to my code as well, but I'm not having any issues so I'm unsure what this is doing. Is this related to enabling bus mastering in init_pci?\nHi Pete, as mentioned in an earlier post, please check: Page 295 and Page 35 of 82574L DS\n. On 01/25/2013 07:40 AM, pkazmier wrote:\n\nThanks! I completely missed that last paragraph in that post! I think I was too focused on the code. I'll read up and if I have any more questions I'll ask later.\nNo worries ! :-)\n. On 01/25/2013 08:10 AM, pkazmier wrote:\nTo cross check, I went through the code for the igb and e1000e linux drivers. Guess what is the very first thing they do in their reset functions - disable master polling (see the comment about sticking). The igb_disable_pcie_master function below sets the CTRL.GMB bit, so it looks like you nailed this! I notice they set this prior to masking the interrupts, should we do the same?\nPage 78 of 82574L DS says:\n\n\n4.6.1  Interrupts During Initialization\nMost drivers disable interrupts during initialization to prevent \nre-entrancy. Interrupts\nare disabled by writing to the IMC register. Note that the interrupts \nneed to be disabled\nalso after issuing a global reset, so a typical driver initialization \nflow is:\n1. Disable interrupts\n2. Issue a global reset\n3. Disable interrupts (again)\n4. \u2026\nAfter the initialization completes, a typical driver enables the desired \ninterrupts by\nwriting to the IMS register.\n\nI think the way we are doing it is safe :-)\n. Update: In addition to the updated reset(), it seems it is important to do the following (in selftest_tso()) to avoid NIC lockups:\n``` lua\nprint \"waiting for old traffic to die out ...\"\n--C.usleep(100000) -- Wait for old traffic from previous tests to die out\nregs[CTRL] = bit.bor(regs[CTRL], bits({GMD=2})) -- Set GIO Master Disable\nC.usleep(10000) -- wait 10ms\nprint(\"DBG: selftest_tso: GIO Master Enable Status: \"..tostring(bitset(regs[STATUS], 19)) ) --GIO Master Enable Status\nregs[CTRL] = bit.band(regs[CTRL], bit.bnot(bits({GMD=2})) ) -- Clear GIO Master Disable\n```\n. Update: This is the current setup that seems to work (for multiple runs):\n``` lua\n   local function clear_rx()\n      rdt = 0\n      regs[RDT] = 0\n      regs[RDH] = 0\n   end M.clear_rx = clear_rx\nlocal function clear_tx()\n      tdt = 0\n      regs[TDT] = 0\n      regs[TDH] = 0\n   end M.clear_tx = clear_tx\nfunction M.selftest_tso (options):\n  -- blah blah blah\n\n  print \"waiting for old traffic to die out ...\"\n\n  regs[CTRL] = bit.bor(regs[CTRL], bits({GMD=2})) -- Set GIO Master Disable\n  C.usleep(10000) -- wait 10ms\n  print(\"DBG: selftest_tso: GIO Master Enable Status: \"..tostring(bitset(regs[STATUS], 19)) ) --GIO Master Enable Status\n  regs[CTRL] = bit.band(regs[CTRL], bit.bnot(bits({GMD=2})) ) -- Clear GIO Master Disable\n\n  --  blah blah blah\n\n  print \"waiting for packet transmission...\"\n  C.usleep(1000000) -- wait for 1s transmit\n  M.clear_tx()\n  C.usleep(1000) -- wait for 1 ms\n  M.clear_rx()\n  C.usleep(1000) -- wait for 1 ms\n\n  -- blah blah blah\n\n  regs[CTRL] = bit.bor(regs[CTRL], bits({GMD=2})) -- Set GIO Master Disable\n  C.usleep(1000) -- wait 1ms\n  print(\"DBG: selftest_tso: GIO Master Enable Status: \"..tostring(bitset(regs[STATUS], 19)) ) --GIO Master Enable Status\n  regs[CTRL] = bit.band(regs[CTRL], bit.bnot(bits({GMD=2})) ) -- Clear GIO Master Disable\n\nend\n```\n. On 01/25/2013 01:45 PM, Luke Gorrie wrote:\n\narbon-VM seems in the DMA-stuck state. Do you know a \"cure\" for this that doesn't require a reboot? I tried a replacement reset() above and your @rahul-mr/issue33 branch but can't get the counters moving.\n\nAs far as I can see, the reset() doesn't bring the NIC out of its \nlocked-up state - only a hardware reboot works once the NIC enters the \nlocked-up state (the PCIe master disable/enable can prevent but not \ncure).\nI'm thinking of trying to directly write to the PCIe Power Management \nControl/Status Register (PMCSR) [page 266, 82574L DS] to do a D0 -> D3 \n-> D0 power state transition and see if that helps.\n. > I'm thinking of trying to directly write to the PCIe Power Management Control/Status Register (PMCSR) [page 266, 82574L DS] to do a D0 -> D3 -> D0 power state transition and see if that helps.\nTried it - doesn't seem to fix the problem. arbon needs a reboot.\n. > I did a reboot on arbon. \n\nHere's the procedure I use after reboot to get the devel VM back up, in case you want to do it yourself any time:\n\nThanks, Luke :-)\n. Sent a pull request: https://github.com/SnabbCo/snabbswitch/pull/41\n. > What is that third argument to protected supposed to be? When I read the code, I took it to imply the total number of bytes for the memory you wanted to protect. In your example, I guess I would have thought that the third argument should have been a 2 like this:\n\nlua\nlocal total_len = protected(\"uint16_t\", context, 14+2, 2) \ntotal_len[0]\n\nTrue. And that's exactly what I ended up using and like Luke mentioned, it is confusing (and already caused a bug!) because all I care about is the number of elements of given type and not the exact num of bytes required.\nTo illustrate it further, say I want to access an array of 5 elements of type structA. In this case, I'll have to call it as:\nlua\nlocal total_len = protected(\"structA\", context, 14+2, 5*ffi.sizeof(\"structA\")) \ntotal_len[0]\nWith the fix, it becomes:\nlua\nlocal total_len = protected(\"structA\", context, 14+2, 5) \ntotal_len[0]\nIf the proposed change is not useful, I can close this issue/fix. :-)\n. On 01/17/2013 12:05 AM, Luke Gorrie wrote:\n\nSo on consideration I think your fix is good. Makes sense for the bound to be in the same units as the index.\nCan you make a pull request that also updates intel.init_dma_memory() to use the new interface? That function should really be allocating descriptors in terms of num_descriptors * sizeof(a descriptor) rather than raw bytes. (Perhaps also avoid merge commits in the history if they are not needed?)\nOk, I'll send a new pull request.\nThe reason for the index being in bytes is really historical. To start with I had assigned fixed memory addresses like 0x1010000 to each DMA structure (read descriptors, write descriptors, etc) and so it seemed natural to think in bytes.\nGood to know how the function evolved :-)\n. Well, that was the fix for issue #1 which was merged in the master branch.\n\nIf you look at the other commits, those were merged from the master branch.\nThe only change from the current master is the line 42 in intel.lua (as shown in 'Files Changed' tab)\nLet me know if it doesn't work - If you want, I can send a new pull request (without all the commit history)\n. On 01/16/2013 11:58 PM, Luke Gorrie wrote:\n\nIs this as it should be or does it make sense to eliminate the merge commit somehow? (would you do that or would I?)\n\nI think polluting the master history with unrelated merge commits is not \nbeneficial. Will send a new pull request :-)\n. variable 'buffer_size' was unused\n. I think regs[IMC] = 0xffffffff is correct.\nIf I understand this correctly, regs[IMC] = 0xffffffff will clear all bits in the Interrupt Mask Set/Read (IMS) register while \nregs[IMC] = 0 leaves the bits in IMS register unchanged (btw IMS register is not defined in the file probably because interrupts are unused). \nNow, the doc says that initially all bits (except bit 18 - MNG, bit 19 - Reserved) of IMS register are 0 which means all interrupts except for the MNG/Reserved ones (whose inital values are unknown) are disabled by default (Probably that explains why it was working even without this change ;-) )\nEDIT: update for clarity\n. ",
    "pkazmier": "Re: point 2 is the 1 minute aging time that you suggest only for the case when the FDB is getting towards peak? I just checked the 802.1d standard (http://standards.ieee.org/getieee802/download/802.1D-2004.pdf page 45) the recommended aging time for the FDB is 300s, which is fairly commonplace in my experience. \nOn a side note, looking that up reminded me of fond days reading \"Interconnections\" by Radia Perlman.\nI haven't touched snabb for a week, day job has been a grueling. However, I'm still working my way through the documentation of the ethernet controller as I really want to understand the code we have in place completely before trying to find something else to tackle.\n. Luke, no comments on my earlier comment? Why 1-2s for an aging time? \n. Oh, by the way, here is the output for now ...\nkaz@monad:~/src/snabbswitch$ sudo src/snabbswitch \nselftest: memory\nOK\nselftest: pci\nScanning PCI devices:\npciaddr     vendor  device  iface   status\n0000:00:00.0    0x8086  0x1237  -   -\n0000:00:01.0    0x8086  0x7000  -   -\n0000:00:01.1    0x8086  0x7010  -   -\n0000:00:01.2    0x8086  0x7020  -   -\n0000:00:01.3    0x8086  0x7113  -   -\n0000:00:02.0    0x1013  0x00b8  -   -\n0000:00:03.0    0x1af4  0x1000  -   -\n0000:00:04.0    0x1af4  0x1001  -   -\n0000:00:05.0    0x1af4  0x1002  -   -\nNo suitable PCI Ethernet devices found.```\n. I was holding that one back ... didn't want the first contributor commit to be for something so trivial. You wouldn't have had anything good to tweet about! \n. Re: IRC, I'm sitting in #snabsswitch on freenode rahul.\nOn Jan 12, 2013, at 3:12 AM, Luke Gorrie notifications@github.com wrote:\n\nTrue! I will prepare a bit of test case to show what I have in mind and let \nyou know when it's checked in. \nCurrently no IRC channel. I find asynchronous communication most convenient \nat the moment - operating in batch mode a bit - but don't let that stop you \ncreating one :). \nOn 11 January 2013 18:55, rahul-mr notifications@github.com wrote: \n\n@lukego https://github.com/lukego Regarding TSO: I've had a look at the \nrelevant documentation in the datasheet and looked at what you've done so \nfar in intel.lua . I was wondering if you can elaborate on that test case \nyou've described (64Kpacket and read it in smaller packets). I haven't seen \nany ffi cast of IP packet/TCP segment structs in the device driver code, \nhave you implemented it somewhere else or are you planning to use some \nother lib? \nBTW do we have an IRC channel for snabbswitch? \n\u2014 \nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/issues/33#issuecomment-12155655. \n\u2014\nReply to this email directly or view it on GitHub.\n. Cool! Looking forward to testing on my box. Wish I had more time like you guys!\n. I added more detail to my development environment and home network setup in case anyone cares :-)\n. I thought it was odd as well. It seems the virtio network interface in my guest does not include a net directory in /sys/bus/pci/devices/.../, which means the interface is nil in the table returned by device_info(). Here is some more output from my system:\n\n\n```\nkaz@monad:/sys/bus/pci/devices/0000:00:03.0$ lspci\n00:00.0 Host bridge: Intel Corporation 440FX - 82441FX PMC [Natoma] (rev 02)\n00:01.0 ISA bridge: Intel Corporation 82371SB PIIX3 ISA [Natoma/Triton II]\n00:01.1 IDE interface: Intel Corporation 82371SB PIIX3 IDE [Natoma/Triton II]\n00:01.2 USB controller: Intel Corporation 82371SB PIIX3 USB [Natoma/Triton II] (rev 01)\n00:01.3 Bridge: Intel Corporation 82371AB/EB/MB PIIX4 ACPI (rev 03)\n00:02.0 VGA compatible controller: Cirrus Logic GD 5446\n00:03.0 Ethernet controller: Red Hat, Inc Virtio network device\n00:04.0 SCSI storage controller: Red Hat, Inc Virtio block device\n00:05.0 RAM memory: Red Hat, Inc Virtio memory balloon\nkaz@monad:/sys/bus/pci/devices/0000:00:03.0$ lspci -v -s 00:03.0\n00:03.0 Ethernet controller: Red Hat, Inc Virtio network device\n        Subsystem: Red Hat, Inc Device 0001\n        Physical Slot: 3\n        Flags: bus master, fast devsel, latency 0, IRQ 10\n        I/O ports at c060 [size=32]\n        Memory at febf1000 (32-bit, non-prefetchable) [size=4K]\n        Expansion ROM at febe0000 [disabled] [size=64K]\n        Capabilities: \n        Kernel driver in use: virtio-pci\nkaz@monad:/sys/bus/pci/devices/0000:00:03.0$ ls /sys/bus/pci/devices/0000\\:00\\:03.0\nbroken_parity_status      device         firmware_node  modalias   remove     resource1         subsystem_vendor\nclass                     dma_mask_bits  irq            msi_bus    rescan     rom               uevent\nconfig                    driver         local_cpulist  numa_node  resource   subsystem         vendor\nconsistent_dma_mask_bits  enable         local_cpus     power      resource0  subsystem_device  virtio0\nkaz@monad:/sys/bus/pci/devices/0000:00:03.0$ \n```\n. Moving onwards ... my new NIC arrived and has been installed!\nUnfortunately, I'm unable to pass the NIC directly through to my guests because I have a Gigibyte motherboard that does not support VT-d (at least according to what I've gathered from the KVM and Xen pages). Instead, I'll simply run snabbswitch on my KVM host directly. On a side note, I was under the impression that the snabbswitch binary was to be all inclusive based on your firmware blog post, but it would not run by itself as it was missing dependencies, so I just ended up rsync'ing my whole snabbswitch directory to the host (not a big deal).\nThe Intel ports are eth2 and eth3 on my system. Before running the test, I prepared and validated my capture environment for troubleshooting. With eth2 bound, configured, and up on my KVM host, I connected the port to my Apple Airport (wifi hub), fired up Wireshark on my iMac's wireless NIC, and configured Wireshark with a capture filter for frames from eth2. I confirmed that I was able to see traffic from the NIC.\nWith the test environment ready, I modified the device ID in pci.lua to 0x105e and let her rip. Here is what I saw (spoiler, not much, but that just means I'll have something to hack around with so that's actually good news!)\nroot@world:/home/kaz/src/snabbswitch# src/snabbswitch \nselftest: memory\nKernel HugeTLB pages (/proc/sys/vm/nr_hugepages): 0\n  Allocating a 2MB HugeTLB: Got 2MB at 0x02000000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x02200000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x01c00000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x36800000\nKernel HugeTLB pages (/proc/sys/vm/nr_hugepages): 4\nHugeTLB page allocation OK.\nselftest: pci\nScanning PCI devices:\npciaddr         vendor  device  iface   status\n0000:00:00.0    0x8086  0x2e30  -       -\n0000:00:01.0    0x8086  0x2e31  -       -\n0000:00:02.0    0x8086  0x2e32  -       -\n0000:00:1b.0    0x8086  0x27d8  -       -\n0000:00:1c.0    0x8086  0x27d0  -       -\n0000:00:1c.1    0x8086  0x27d2  -       -\n0000:00:1d.0    0x8086  0x27c8  -       -\n0000:00:1d.1    0x8086  0x27c9  -       -\n0000:00:1d.2    0x8086  0x27ca  -       -\n0000:00:1d.3    0x8086  0x27cb  -       -\n0000:00:1d.7    0x8086  0x27cc  -       -\n0000:00:1e.0    0x8086  0x244e  -       -\n0000:00:1f.0    0x8086  0x27b8  -       -\n0000:00:1f.2    0x8086  0x27c0  -       -\n0000:00:1f.3    0x8086  0x27da  -       -\n0000:01:00.0    0x8086  0x105e  -       -\n0000:01:00.1    0x8086  0x105e  eth3    down\n0000:02:00.0    0x10ec  0x8168  eth0    up\n0000:03:00.0    0x10ec  0x8168  eth1    up\nSuitable devices: \n  0000:01:00.0\n  0000:01:00.1\nselftest: intel device 0000:01:00.0\nNIC transmit test\nintel selftest: pciaddr=0000:01:00.0 secs=1\nWaiting for linkup.............. ok\nGenerating traffic for 1 second(s)...\n^C\nroot@world:/home/kaz/src/snabbswitch#\nNothing was emitted from the card according to my Wireshark captures, which means my next steps are to review intel.lua and the Intel documentation for the 82571 card. Here is a link if anyone is looking for it: http://developer.intel.com/content/dam/www/public/us/en/documents/manuals/pcie-gbe-controllers-open-source-manual.pdf. That documentation is all foreign to me, so this should be interesting!\nThe wife is calling ... time to go for now ...\n. For future reference, here is the output of ethtool for the NIC in question:\n```\nethtool -d eth2\nMAC Registers\n0x00000: CTRL (Device control register)  0x000C0241\n      Endian mode (buffers):             little\n      Link reset:                        normal\n      Set link up:                       1\n      Invert Loss-Of-Signal:             no\n      Receive flow control:              disabled\n      Transmit flow control:             disabled\n      VLAN mode:                         disabled\n      Auto speed detect:                 disabled\n      Speed select:                      1000Mb/s\n      Force speed:                       no\n      Force duplex:                      no\n0x00008: STATUS (Device status register) 0x00080380\n      Duplex:                            half\n      Link up:                           no link config\n      TBI mode:                          disabled\n      Link speed:                        1000Mb/s\n      Bus type:                          PCI Express\n      Port number:                       0\n0x00100: RCTL (Receive control register) 0x00000000\n      Receiver:                          disabled\n      Store bad packets:                 disabled\n      Unicast promiscuous:               disabled\n      Multicast promiscuous:             disabled\n      Long packet:                       disabled\n      Descriptor minimum threshold size: 1/2\n      Broadcast accept mode:             ignore\n      VLAN filter:                       disabled\n      Canonical form indicator:          disabled\n      Discard pause frames:              filtered\n      Pass MAC control frames:           don't pass\n      Receive buffer size:               2048\n0x02808: RDLEN (Receive desc length)     0x00000000\n0x02810: RDH   (Receive desc head)       0x00000000\n0x02818: RDT   (Receive desc tail)       0x00000000\n0x02820: RDTR  (Receive delay timer)     0x00000000\n0x00400: TCTL (Transmit ctrl register)   0x30000008\n      Transmitter:                       disabled\n      Pad short packets:                 enabled\n      Software XOFF Transmission:        disabled\n      Re-transmit on late collision:     disabled\n0x03808: TDLEN (Transmit desc length)    0x00000000\n0x03810: TDH   (Transmit desc head)      0x00000000\n0x03818: TDT   (Transmit desc tail)      0x00000000\n0x03820: TIDV  (Transmit delay timer)    0x00000000\nPHY type:                                unknown\nroot@world:/home/kaz/src/snabbswitch# \n``\n. Quick question: I noticedphy_lock()andphy_unlock()` functions, but they are never used. Are they supposed to be? \n. Thanks and will do. Password changed. Time for me to go to bed (2:30am here in Dallas, TX).\nOn Jan 13, 2013, at 2:13 AM, Luke Gorrie notifications@github.com wrote:\n\nPete you have these user accounts now to access the existing test lab for reference:\nssh -p 54322 pkazmier@arbon.snabb.co # development KVM instance\nssh pkazmier@bern.snabb.co # has eth1 cabled to the development instance, can be useful for tcpdump etc\npassword \"snabbswitch\" but please change that (not that these machines are sensitive in any way).\nalso when you run snabbswitch please do it like this:\nflock -x /tmp/snabb.lock ./snabbswitch ...\nto avoid colliding with me and Rahul on the same machine :)\n\u2014\nReply to this email directly or view it on GitHub.\n. Sorry ... pressed the wrong button! I'm with rahul ... where was my \"are you sure you want to close this issue?\" prompt.\n. Re: PCIe setup for DMA, I'm all set now. I was confused before. I do see the flag being set correctly for mastering and it is confirmed with lspci -v.\n\nI'm slowly making my way through code and documentation. I discovered that I'll need to use SWSM.SWESMBI to lock PHY instead of using EXTCNF_CTRL.MDIO like you do for the 82574. I also believe the PHY RESET process is also different for my board than yours according to the documentation I have, so I was going to fix that next.\nFun stuff when your driving blind!\n. Hi Luke,\nSorry, haven't had much time to do any coding (full-time job, wife, exercise), but tomorrow morning before work I hope to see if I can force link to go down on my card to validate that snabb is talking to the hardware (plus I can visually see the port). As you can see below, after the \"Waiting for linkup.....\" message, I added a print_status and it shows the link is down. Clearly, one of them is incorrect, so that's where I was going to start. During my lunch time tomorrow, I'll also hack around as well (I'm working from home tomorrow so I have Internet access - at work it's blocked). Here is the dump you wanted to see. I added the print_status before the test loop and after it. Plus, I still get that weird stack trace on every run at the end. If you want an account on the box, I can set one up for you, but we just have to keep in mind it's the KVM host unfortunately that feeds my apartment network connectivity.\n```\nroot@world:/home/kaz# ./snabbswitch \nselftest: memory\nKernel HugeTLB pages (/proc/sys/vm/nr_hugepages): 8\n  Allocating a 2MB HugeTLB: Got 2MB at 0x10000000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x10200000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x10400000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x10600000\nKernel HugeTLB pages (/proc/sys/vm/nr_hugepages): 8\nHugeTLB page allocation OK.\nselftest: pci\nScanning PCI devices:\npciaddr         vendor  device  iface   status\n0000:00:00.0    0x8086  0x2e30  -       -\n0000:00:01.0    0x8086  0x2e31  -       -\n0000:00:02.0    0x8086  0x2e32  -       -\n0000:00:1b.0    0x8086  0x27d8  -       -\n0000:00:1c.0    0x8086  0x27d0  -       -\n0000:00:1c.1    0x8086  0x27d2  -       -\n0000:00:1d.0    0x8086  0x27c8  -       -\n0000:00:1d.1    0x8086  0x27c9  -       -\n0000:00:1d.2    0x8086  0x27ca  -       -\n0000:00:1d.3    0x8086  0x27cb  -       -\n0000:00:1d.7    0x8086  0x27cc  -       -\n0000:00:1e.0    0x8086  0x244e  -       -\n0000:00:1f.0    0x8086  0x27b8  -       -\n0000:00:1f.2    0x8086  0x27c0  -       -\n0000:00:1f.3    0x8086  0x27da  -       -\n0000:01:00.0    0x8086  0x105e  -       -\n0000:01:00.1    0x8086  0x105e  -       -\n0000:02:00.0    0x10ec  0x8168  eth0    up\n0000:03:00.0    0x10ec  0x8168  eth1    up\nSuitable devices: \n  0000:01:00.0\n  0000:01:00.1\nselftest: intel device 0000:01:00.0\nNIC transmit test\nintel selftest: pciaddr=0000:01:00.0 secs=1\nWaiting for linkup.............. ok\n\n\n\nDEBUG: After linkup wait, before execution of traffic\nMAC status\n  STATUS      = 00080381\n  Full Duplex = yes\n  Link Up     = no\n  PHYRA       = no\n  Speed       = 1000 Mb/s\nTransmit status\n  TCTL        = 3103f0fa\n  TXDCTL      = 01410000\n  TX Enable   = yes\n  TDH         = 0\n  TDT         = 0\n  TDBAH       = 00000000\n  TDBAL       = 10880000\n  TDLEN       = 524288\n  TARC        = 00000403\n  TIPG        = 00602006\nReceive status\n  RCTL        = 0603803a\n  RXDCTL      = 01010000\n  RX Enable   = yes\n  RX Loopback = no\n  RDH         = 0\n  RDT         = 0\n  RDBAH       = 00000000\n  RDBAL       = 10800000\n  RDLEN       = 524288                                                                                                                  [92/1864]\n  RADV        = 10\nPHY status\n  Autonegotiate state    = complete\n  Remote fault detection = no remote fault detected\n  Copper Link Status     = copper link is down\n  Speed and duplex resolved = no\n  Speed                  = 1000Mb/s\n  Duplex                 = half-duplex\n  Advertise 1000 Mb/s FD = yes\n  Advertise 1000 Mb/s HD = no\n  Advertise  100 Mb/s FD = yes\n  Advertise  100 Mb/s HD = yes\n  Advertise   10 Mb/s FD = yes\n  Advertise   10 Mb/s HD = yes\n  Partner   1000 Mb/s FD = yes\n  Partner   1000 Mb/s HD = no\n  Partner    100 Mb/s FD = yes\n  Partner    100 Mb/s HD = yes\n  Partner     10 Mb/s FD = yes\n  Partner     10 Mb/s HD = yes\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:01:00.0:\nDEBUG: After test, leaving selftest\nMAC status\n  STATUS      = 00080381\n  Full Duplex = yes\n  Link Up     = no\n  PHYRA       = no\n  Speed       = 1000 Mb/s\nTransmit status\n  TCTL        = 3103f0fa\n  TXDCTL      = 01410000\n  TX Enable   = yes\n  TDH         = 214\n  TDT         = 213\n  TDBAH       = 00000000\n  TDBAL       = 10880000\n  TDLEN       = 524288\n  TARC        = 00000403\n  TIPG        = 00602006\nReceive status\n  RCTL        = 0603803a\n  RXDCTL      = 01010000\n  RX Enable   = yes\n  RX Loopback = no\n  RDH         = 0\n  RDT         = 0\n  RDBAH       = 00000000\n  RDBAL       = 10800000\n  RDLEN       = 524288\n  RADV        = 10\nPHY status\n  Autonegotiate state    = complete\n  Remote fault detection = no remote fault detected\n  Copper Link Status     = copper link is down\n  Speed and duplex resolved = no\n  Speed                  = 1000Mb/s\n  Duplex                 = half-duplex\n  Advertise 1000 Mb/s FD = yes\n  Advertise 1000 Mb/s HD = no\n  Advertise  100 Mb/s FD = yes\n  Advertise  100 Mb/s HD = yes\n  Advertise   10 Mb/s FD = yes\n  Advertise   10 Mb/s HD = yes\n  Partner   1000 Mb/s FD = yes\n  Partner   1000 Mb/s HD = no\n  Partner    100 Mb/s FD = yes\n  Partner    100 Mb/s HD = yes\n  Partner     10 Mb/s FD = yes\n  Partner     10 Mb/s HD = yes\nNIC transmit+receive loopback test\nintel selftest: pciaddr=0000:01:00.0 secs=1 receive=true loopback=true\nWaiting for linkup............. ok\nDEBUG: After linkup wait, before execution of traffic\nMAC status\n  STATUS      = 00080381\n  Full Duplex = yes\n  Link Up     = no\n  PHYRA       = no\n  Speed       = 1000 Mb/s\nTransmit status\n  TCTL        = 3103f0fa\n  TXDCTL      = 01410000\n  TX Enable   = yes\n  TDH         = 0\n  TDT         = 0\n  TDBAH       = 00000000\n  TDBAL       = 10b80000\n  TDLEN       = 524288\n  TARC        = 00000403\n  TIPG        = 00602006\nReceive status\n  RCTL        = 0603807a\n  RXDCTL      = 01010000\n  RX Enable   = yes\n  RX Loopback = yes\n  RDH         = 0\n  RDT         = 0\n  RDBAH       = 00000000\n  RDBAL       = 10b00000\n  RDLEN       = 524288\n  RADV        = 10\nPHY status\n  Autonegotiate state    = complete\n  Remote fault detection = no remote fault detected\n  Copper Link Status     = copper link is down\n  Speed and duplex resolved = yes\n  Speed                  = 1000Mb/s\n  Duplex                 = half-duplex\n  Advertise 1000 Mb/s FD = yes\n  Advertise 1000 Mb/s HD = no\n  Advertise  100 Mb/s FD = yes\n  Advertise  100 Mb/s HD = yes\n  Advertise   10 Mb/s FD = yes\n  Advertise   10 Mb/s HD = yes\n  Partner   1000 Mb/s FD = yes\n  Partner   1000 Mb/s HD = no\n  Partner    100 Mb/s FD = yes\n  Partner    100 Mb/s HD = yes\n  Partner     10 Mb/s FD = yes\n  Partner     10 Mb/s HD = yes\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:01:00.0:\nDEBUG: After test, leaving selftest\nMAC status\n  STATUS      = 00080381\n  Full Duplex = yes\n  Link Up     = no\n  PHYRA       = no\n  Speed       = 1000 Mb/s\nTransmit status\n  TCTL        = 3103f0fa\n  TXDCTL      = 01410000\n  TX Enable   = yes\n  TDH         = 214\n  TDT         = 213\n  TDBAH       = 00000000\n  TDBAL       = 10b80000\n  TDLEN       = 524288\n  TARC        = 00000403\n  TIPG        = 00602006\nReceive status\n  RCTL        = 0603807a\n  RXDCTL      = 01010000\n  RX Enable   = yes\n  RX Loopback = yes\n  RDH         = 0\n  RDT         = 32761\n  RDBAH       = 00000000\n  RDBAL       = 10b00000\n  RDLEN       = 524288\n  RADV        = 10\nPHY status\n  Autonegotiate state    = complete\n  Remote fault detection = no remote fault detected\n  Copper Link Status     = copper link is down\n  Speed and duplex resolved = yes\n  Speed                  = 1000Mb/s\n  Duplex                 = half-duplex\n  Advertise 1000 Mb/s FD = yes\n  Advertise 1000 Mb/s HD = no\n  Advertise  100 Mb/s FD = yes\n  Advertise  100 Mb/s HD = yes\n  Advertise   10 Mb/s FD = yes\n  Advertise   10 Mb/s HD = yes\n  Partner   1000 Mb/s FD = yes\n  Partner   1000 Mb/s HD = no\n  Partner    100 Mb/s FD = yes\n  Partner    100 Mb/s HD = yes\n  Partner     10 Mb/s FD = yes\n  Partner     10 Mb/s HD = yes\nselftest: intel device 0000:01:00.1\nintel.lua:261: attempt to redefine 'rx_desc' at line 2\nstack traceback:\n        main.lua:17: in function \n        [C]: in function 'cdef'\n        intel.lua:261: in function 'new'\n        selftest.lua:20: in main chunk\n        [C]: in function 'require'\n        main.lua:11: in function \n        [C]: in function 'xpcall'\n        main.lua:22: in main chunk\n        [C]: in function 'require'\n        [string \"require \"main\"\"]:1: in main chunk\nroot@world:/home/kaz# \n```\n. Progress!! Now link up is consistent and I'm now seeing stats for the first time! Running late for work so didn't get a chance to actually look at any of this output, but thought I'd share progress.\n\n\n\n```\nroot@world:/home/kaz# ./snabbswitch \nselftest: memory\nKernel HugeTLB pages (/proc/sys/vm/nr_hugepages): 8\n  Allocating a 2MB HugeTLB: Got 2MB at 0x10000000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x10200000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x10400000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x10600000\nKernel HugeTLB pages (/proc/sys/vm/nr_hugepages): 8\nHugeTLB page allocation OK.\nselftest: pci\nScanning PCI devices:\npciaddr     vendor  device  iface   status\n0000:00:00.0    0x8086  0x2e30  -   -\n0000:00:01.0    0x8086  0x2e31  -   -\n0000:00:02.0    0x8086  0x2e32  -   -\n0000:00:1b.0    0x8086  0x27d8  -   -\n0000:00:1c.0    0x8086  0x27d0  -   -\n0000:00:1c.1    0x8086  0x27d2  -   -\n0000:00:1d.0    0x8086  0x27c8  -   -\n0000:00:1d.1    0x8086  0x27c9  -   -\n0000:00:1d.2    0x8086  0x27ca  -   -\n0000:00:1d.3    0x8086  0x27cb  -   -\n0000:00:1d.7    0x8086  0x27cc  -   -\n0000:00:1e.0    0x8086  0x244e  -   -\n0000:00:1f.0    0x8086  0x27b8  -   -\n0000:00:1f.2    0x8086  0x27c0  -   -\n0000:00:1f.3    0x8086  0x27da  -   -\n0000:01:00.0    0x8086  0x105e  -   -\n0000:01:00.1    0x8086  0x105e  -   -\n0000:02:00.0    0x10ec  0x8168  eth0    up\n0000:03:00.0    0x10ec  0x8168  eth1    up\nSuitable devices: \n  0000:01:00.0\n  0000:01:00.1\nselftest: intel device 0000:01:00.0\nNIC transmit test\nintel selftest: pciaddr=0000:01:00.0 secs=1\nWaiting for linkup.............. ok\n\n\n\nDEBUG: After linkup wait, before execution of traffic\nMAC status\n  STATUS      = 00080383\n  Full Duplex = yes\n  Link Up     = yes\n  PHYRA       = no\n  Speed       = 1000 Mb/s\nTransmit status\n  TCTL        = 3103f0fa\n  TXDCTL      = 01410000\n  TX Enable   = yes\n  TDH         = 0\n  TDT         = 0\n  TDBAH       = 00000000\n  TDBAL       = 10880000\n  TDLEN       = 524288\n  TARC        = 00000403\n  TIPG        = 00602006\nReceive status\n  RCTL        = 0603803a\n  RXDCTL      = 01010000\n  RX Enable   = yes\n  RX Loopback = no\n  RDH         = 0\n  RDT         = 0\n  RDBAH       = 00000000\n  RDBAL       = 10800000\n  RDLEN       = 524288\n  RADV        = 10\nPHY status\n  Autonegotiate state    = complete\n  Remote fault detection = no remote fault detected\n  Copper Link Status     = copper link is down\n  Speed and duplex resolved = no\n  Speed                  = 1000Mb/s\n  Duplex                 = half-duplex\n  Advertise 1000 Mb/s FD = yes\n  Advertise 1000 Mb/s HD = no\n  Advertise  100 Mb/s FD = yes\n  Advertise  100 Mb/s HD = yes\n  Advertise   10 Mb/s FD = yes\n  Advertise   10 Mb/s HD = yes\n  Partner   1000 Mb/s FD = yes\n  Partner   1000 Mb/s HD = no\n  Partner    100 Mb/s FD = yes\n  Partner    100 Mb/s HD = yes\n  Partner     10 Mb/s FD = yes\n  Partner     10 Mb/s HD = yes\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:01:00.0:\n                  61 PRC64      Packets Received [64 Bytes] Count\n                   1 PRC511     Packets Received [256-511 Bytes] Count\n                   1 PRC1023    Packets Received [512-1023 Bytes] Count\n                  63 GPRC       Good Packets Received Count\n                   2 BPRC       Broadcast Packets Received Count\n                  61 MPRC       Multicast Packets Received Count\n           1,536,393 GPTC       Good Packets Transmitted Count\n               4,881 GORCL      Good Octets Received Count\n          98,329,536 GOTCL      Good Octets Transmitted Count\n                  63 RNBC       Receive No Buffers Count\n               4,881 TORL       Total Octets Received (Low)\n          98,330,816 TOTL       Total Octets Transmitted (Low)\n                  63 TPR        Total Packets Received\n           1,536,423 TPT        Total Packets Transmitted\n           1,536,425 PTC64      Packets Transmitted [64 Bytes] Count\nDEBUG: After test, leaving selftest\nMAC status\n  STATUS      = 00080383\n  Full Duplex = yes\n  Link Up     = yes\n  PHYRA       = no\n  Speed       = 1000 Mb/s\nTransmit status\n  TCTL        = 3103f0fa\n  TXDCTL      = 01410000\n  TX Enable   = yes\n  TDH         = 29760\n  TDT         = 28797\n  TDBAH       = 00000000\n  TDBAL       = 10880000\n  TDLEN       = 524288\n  TARC        = 00000403\n  TIPG        = 00602006\nReceive status\n  RCTL        = 0603803a\n  RXDCTL      = 01010000\n  RX Enable   = yes\n  RX Loopback = no\n  RDH         = 0\n  RDT         = 0\n  RDBAH       = 00000000\n  RDBAL       = 10800000\n  RDLEN       = 524288\n  RADV        = 10\nPHY status\n  Autonegotiate state    = complete\n  Remote fault detection = no remote fault detected\n  Copper Link Status     = copper link is down\n  Speed and duplex resolved = no\n  Speed                  = 1000Mb/s\n  Duplex                 = full-duplex\n  Advertise 1000 Mb/s FD = yes\n  Advertise 1000 Mb/s HD = no\n  Advertise  100 Mb/s FD = yes\n  Advertise  100 Mb/s HD = yes\n  Advertise   10 Mb/s FD = yes\n  Advertise   10 Mb/s HD = yes\n  Partner   1000 Mb/s FD = yes\n  Partner   1000 Mb/s HD = no\n  Partner    100 Mb/s FD = yes\n  Partner    100 Mb/s HD = yes\n  Partner     10 Mb/s FD = yes\n  Partner     10 Mb/s HD = yes\nNIC transmit+receive loopback test\nintel selftest: pciaddr=0000:01:00.0 secs=1 receive=true loopback=true\nWaiting for linkup.............. ok\nDEBUG: After linkup wait, before execution of traffic\nMAC status\n  STATUS      = 00080383\n  Full Duplex = yes\n  Link Up     = yes\n  PHYRA       = no\n  Speed       = 1000 Mb/s\nTransmit status\n  TCTL        = 3103f0fa\n  TXDCTL      = 01410000\n  TX Enable   = yes\n  TDH         = 0\n  TDT         = 0\n  TDBAH       = 00000000\n  TDBAL       = 10b80000\n  TDLEN       = 524288\n  TARC        = 00000403\n  TIPG        = 00602006\nReceive status\n  RCTL        = 0603807a\n  RXDCTL      = 01010000\n  RX Enable   = yes\n  RX Loopback = yes\n  RDH         = 0\n  RDT         = 0\n  RDBAH       = 00000000\n  RDBAL       = 10b00000\n  RDLEN       = 524288\n  RADV        = 10\nPHY status\n  Autonegotiate state    = complete\n  Remote fault detection = no remote fault detected\n  Copper Link Status     = copper link is down\n  Speed and duplex resolved = no\n  Speed                  = 1000Mb/s\n  Duplex                 = half-duplex\n  Advertise 1000 Mb/s FD = yes\n  Advertise 1000 Mb/s HD = no\n  Advertise  100 Mb/s FD = yes\n  Advertise  100 Mb/s HD = yes\n  Advertise   10 Mb/s FD = yes\n  Advertise   10 Mb/s HD = yes\n  Partner   1000 Mb/s FD = yes\n  Partner   1000 Mb/s HD = no\n  Partner    100 Mb/s FD = yes\n  Partner    100 Mb/s HD = yes\n  Partner     10 Mb/s FD = yes\n  Partner     10 Mb/s HD = yes\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:01:00.0:\n             727,122 MPC        Missed Packets Count\n             799,605 PRC64      Packets Received [64 Bytes] Count\n             799,613 GPRC       Good Packets Received Count\n           1,526,740 GPTC       Good Packets Transmitted Count\n          51,175,616 GORCL      Good Octets Received Count\n          97,711,616 GOTCL      Good Octets Transmitted Count\n              22,967 RNBC       Receive No Buffers Count\n          97,712,640 TORL       Total Octets Received (Low)\n          97,712,832 TOTL       Total Octets Transmitted (Low)\n           1,526,765 TPR        Total Packets Received\n           1,526,766 TPT        Total Packets Transmitted\n           1,526,767 PTC64      Packets Transmitted [64 Bytes] Count\nDEBUG: After test, leaving selftest\nMAC status\n  STATUS      = 00080383\n  Full Duplex = yes\n  Link Up     = yes\n  PHYRA       = no\n  Speed       = 1000 Mb/s\nTransmit status\n  TCTL        = 3103f0fa\n  TXDCTL      = 01410000\n  TX Enable   = yes\n  TDH         = 20430\n  TDT         = 19283\n  TDBAH       = 00000000\n  TDBAL       = 10b80000\n  TDLEN       = 524288\n  TARC        = 00000403\n  TIPG        = 00602006\nReceive status\n  RCTL        = 0603807a\n  RXDCTL      = 01010000\n  RX Enable   = yes\n  RX Loopback = yes\n  RDH         = 13060\n  RDT         = 13060\n  RDBAH       = 00000000\n  RDBAL       = 10b00000\n  RDLEN       = 524288\n  RADV        = 10\nPHY status\n  Autonegotiate state    = complete\n  Remote fault detection = no remote fault detected\n  Copper Link Status     = copper link is down\n  Speed and duplex resolved = no\n  Speed                  = 1000Mb/s\n  Duplex                 = half-duplex\n  Advertise 1000 Mb/s FD = yes\n  Advertise 1000 Mb/s HD = no\n  Advertise  100 Mb/s FD = yes\n  Advertise  100 Mb/s HD = yes\n  Advertise   10 Mb/s FD = yes\n  Advertise   10 Mb/s HD = yes\n  Partner   1000 Mb/s FD = yes\n  Partner   1000 Mb/s HD = no\n  Partner    100 Mb/s FD = yes\n  Partner    100 Mb/s HD = yes\n  Partner     10 Mb/s FD = yes\n  Partner     10 Mb/s HD = yes\nselftest: intel device 0000:01:00.1\nintel.lua:265: attempt to redefine 'rx_desc' at line 2\nstack traceback:\n    main.lua:17: in function \n    [C]: in function 'cdef'\n    intel.lua:265: in function 'new'\n    selftest.lua:20: in main chunk\n    [C]: in function 'require'\n    main.lua:11: in function \n    [C]: in function 'xpcall'\n    main.lua:22: in main chunk\n    [C]: in function 'require'\n    [string \"require \"main\"\"]:1: in main chunk\n```\n. Here are the changes I made thus far in case you are interested: https://github.com/pkazmier/snabbswitch/commit/19091e679a34799f1184e7b5023d0f8a7c721239\n. Fixed the printing of duplex settings for the 82571 card. The PHY port status (17) is different on the 82571, which would result in apparently random duplex settings in the PHY section of print_status: https://github.com/pkazmier/snabbswitch/commit/c1863ffbef4d28f8184857a153ff3f92744d48e8\n\n\n\nHere is the output now (and I confirmed that I see the frames being sent on the wire via tshark)\n```\nkaz@world:~$ sudo ./snabbswitch \nselftest: memory\nKernel HugeTLB pages (/proc/sys/vm/nr_hugepages): 8\n  Allocating a 2MB HugeTLB: Got 2MB at 0x10000000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x10200000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x10400000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x10600000\nKernel HugeTLB pages (/proc/sys/vm/nr_hugepages): 8\nHugeTLB page allocation OK.\nselftest: pci\nScanning PCI devices:\npciaddr     vendor  device  iface   status\n0000:00:00.0    0x8086  0x2e30  -   -\n0000:00:01.0    0x8086  0x2e31  -   -\n0000:00:02.0    0x8086  0x2e32  -   -\n0000:00:1b.0    0x8086  0x27d8  -   -\n0000:00:1c.0    0x8086  0x27d0  -   -\n0000:00:1c.1    0x8086  0x27d2  -   -\n0000:00:1d.0    0x8086  0x27c8  -   -\n0000:00:1d.1    0x8086  0x27c9  -   -\n0000:00:1d.2    0x8086  0x27ca  -   -\n0000:00:1d.3    0x8086  0x27cb  -   -\n0000:00:1d.7    0x8086  0x27cc  -   -\n0000:00:1e.0    0x8086  0x244e  -   -\n0000:00:1f.0    0x8086  0x27b8  -   -\n0000:00:1f.2    0x8086  0x27c0  -   -\n0000:00:1f.3    0x8086  0x27da  -   -\n0000:01:00.0    0x8086  0x105e  -   -\n0000:01:00.1    0x8086  0x105e  eth3    up\n0000:02:00.0    0x10ec  0x8168  eth0    up\n0000:03:00.0    0x10ec  0x8168  eth1    up\nSuitable devices: \n  0000:01:00.0\nselftest: intel device 0000:01:00.0\nNIC transmit test\nintel selftest: pciaddr=0000:01:00.0 secs=1\nWaiting for linkup............ ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:01:00.0:\n           1,531,838 GPTC       Good Packets Transmitted Count\n          98,037,952 GOTCL      Good Octets Transmitted Count\n          98,039,872 TOTL       Total Octets Transmitted (Low)\n           1,531,877 TPT        Total Packets Transmitted\n           1,531,879 PTC64      Packets Transmitted [64 Bytes] Count\n\n\n\nDEBUG: After test, leaving selftest\nMAC status\n  STATUS      = 00080383\n  Full Duplex = yes\n  Link Up     = yes\n  PHYRA       = no\n  Speed       = 1000 Mb/s\nTransmit status\n  TCTL        = 3103f0fa\n  TXDCTL      = 01410000\n  TX Enable   = yes\n  TDH         = 25577\n  TDT         = 24374\n  TDBAH       = 00000000\n  TDBAL       = 10880000\n  TDLEN       = 524288\n  TARC        = 00000403\n  TIPG        = 00602006\nReceive status\n  RCTL        = 0603803a\n  RXDCTL      = 01010000\n  RX Enable   = yes\n  RX Loopback = no\n  RDH         = 0\n  RDT         = 0\n  RDBAH       = 00000000\n  RDBAL       = 10800000\n  RDLEN       = 524288\n  RADV        = 10\nPHY status\n  Autonegotiate state    = complete\n  Remote fault detection = no remote fault detected\n  Copper Link Status     = copper link is up\n  Speed                  = 1000Mb/s\n  Duplex                 = full-duplex\n  Advertise 1000 Mb/s FD = yes\n  Advertise 1000 Mb/s HD = no\n  Advertise  100 Mb/s FD = yes\n  Advertise  100 Mb/s HD = yes\n  Advertise   10 Mb/s FD = yes\n  Advertise   10 Mb/s HD = yes\n  Partner   1000 Mb/s FD = yes\n  Partner   1000 Mb/s HD = no\n  Partner    100 Mb/s FD = yes\n  Partner    100 Mb/s HD = yes\n  Partner     10 Mb/s FD = yes\n  Partner     10 Mb/s HD = yes\nNIC transmit+receive loopback test\nintel selftest: pciaddr=0000:01:00.0 secs=1 receive=true loopback=true\nWaiting for linkup............ ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:01:00.0:\n             727,161 MPC        Missed Packets Count\n             799,960 PRC64      Packets Received [64 Bytes] Count\n             799,969 GPRC       Good Packets Received Count\n           1,527,136 GPTC       Good Packets Transmitted Count\n          51,198,400 GORCL      Good Octets Received Count\n          97,736,896 GOTCL      Good Octets Transmitted Count\n              22,838 RNBC       Receive No Buffers Count\n          97,737,920 TORL       Total Octets Received (Low)\n          97,738,176 TOTL       Total Octets Transmitted (Low)\n           1,527,160 TPR        Total Packets Received\n           1,527,162 TPT        Total Packets Transmitted\n           1,527,164 PTC64      Packets Transmitted [64 Bytes] Count\nDEBUG: After test, leaving selftest\nMAC status\n  STATUS      = 00080383\n  Full Duplex = yes\n  Link Up     = yes\n  PHYRA       = no\n  Speed       = 1000 Mb/s\nTransmit status\n  TCTL        = 3103f0fa\n  TXDCTL      = 01410000\n  TX Enable   = yes\n  TDH         = 20741\n  TDT         = 19671\n  TDBAH       = 00000000\n  TDBAL       = 10b80000\n  TDLEN       = 524288\n  TARC        = 00000403\n  TIPG        = 00602006\nReceive status\n  RCTL        = 0603807a\n  RXDCTL      = 01010000\n  RX Enable   = yes\n  RX Loopback = yes\n  RDH         = 14471\n  RDT         = 12709\n  RDBAH       = 00000000\n  RDBAL       = 10b00000\n  RDLEN       = 524288\n  RADV        = 10\nPHY status\n  Autonegotiate state    = complete\n  Remote fault detection = no remote fault detected\n  Copper Link Status     = copper link is up\n  Speed                  = 1000Mb/s\n  Duplex                 = full-duplex\n  Advertise 1000 Mb/s FD = yes\n  Advertise 1000 Mb/s HD = no\n  Advertise  100 Mb/s FD = yes\n  Advertise  100 Mb/s HD = yes\n  Advertise   10 Mb/s FD = yes\n  Advertise   10 Mb/s HD = yes\n  Partner   1000 Mb/s FD = yes\n  Partner   1000 Mb/s HD = no\n  Partner    100 Mb/s FD = yes\n  Partner    100 Mb/s HD = yes\n  Partner     10 Mb/s FD = yes\n  Partner     10 Mb/s HD = yes\n```\n. Figured out why I was getting that stack trace before ... it occurred when I had the other side of my NIC connected to my wifi hub. Now that I've connected the other side of the NIC to the other port on the 82571, I no longer get the crash.\n\n\n\nUpdate: the stack trace was resolved with https://github.com/SnabbCo/snabbswitch/commit/de334f35d36d17365460b8d8cd9c667f04e7638d\n. Hi Luke,\nI'm looking for your guidance in terms of how you'd like me to integrate the 82571 code. If you look at the previous 6 commits on the http://github.com/pkazmier/snabbswitch/commits/iss34-intel-82571 branch, you'll find the changes I made to get the 82571 working. In summary, the changes consisted of the following:\n- PCI device ID: I had to change so a suitable device would be found.\n- PHY reads/writes: PHY reads and writes are synchronized with the PHY locking code.\n- PHY reset procedure: The procedure to reset the 82571 card is vastly different than the 82574 due to the synchronization issues associated with the multi-port card.\n- PHY status differences: The 82571 PHY status register varies slightly from the 82574. For example, the copper link status is not available in bit 3 like it is on the 82574. This resulted in a couple of changes in the way stats are printed.\n- PHY locking code: The 82571 uses different registers to obtain a PHY lock from firmware. In addition, I had to implement software locking (SWSM.SMBI) to synchronize access between drivers. Recall that this is a multi-port card, so it is possible that one port could be bound to snabbswitch and the other bound to the OS. The locking methods take an options parameter that allows one to request the type of lock desired (software, firmware, or both). By default, both locks are acquired and released. However, during the PHY reset process, there were steps that had to be executed in between the acquisition of software and firmware locks, which is why I refactored the code in this manner.\nBased on the above, do you have any thoughts on how you'd like me to move forward? Did you want to create a subclass that overrides the specific methods based on PCI device ID? Did you want low tech and a bunch of if statements? Did you want a different module for the specific cards (probably not based on your prior comments)?\nAny suggestions would be appreciated (no rush as work will be grueling this week).\nThanks,\nPete\nps. Here is the output from the a latest run:\nroot@world:/home/kaz# ./snabbswitch \nselftest: memory\nKernel HugeTLB pages (/proc/sys/vm/nr_hugepages): 12\n  Allocating a 2MB HugeTLB: Got 2MB at 0x66800000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x63200000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x69e00000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x6d800000\nKernel HugeTLB pages (/proc/sys/vm/nr_hugepages): 12\nHugeTLB page allocation OK.\nselftest: pci\nScanning PCI devices:\npciaddr         vendor  device  iface   status\n0000:00:00.0    0x8086  0x2e30  -       -\n0000:00:01.0    0x8086  0x2e31  -       -\n0000:00:02.0    0x8086  0x2e32  -       -\n0000:00:1b.0    0x8086  0x27d8  -       -\n0000:00:1c.0    0x8086  0x27d0  -       -\n0000:00:1c.1    0x8086  0x27d2  -       -\n0000:00:1d.0    0x8086  0x27c8  -       -\n0000:00:1d.1    0x8086  0x27c9  -       -\n0000:00:1d.2    0x8086  0x27ca  -       -\n0000:00:1d.3    0x8086  0x27cb  -       -\n0000:00:1d.7    0x8086  0x27cc  -       -\n0000:00:1e.0    0x8086  0x244e  -       -\n0000:00:1f.0    0x8086  0x27b8  -       -\n0000:00:1f.2    0x8086  0x27c0  -       -\n0000:00:1f.3    0x8086  0x27da  -       -\n0000:01:00.0    0x8086  0x105e  -       -\n0000:01:00.1    0x8086  0x105e  -       -\n0000:02:00.0    0x10ec  0x8168  eth0    up\n0000:03:00.0    0x10ec  0x8168  eth1    up\nSuitable devices: \n  0000:01:00.0\n  0000:01:00.1\nselftest: intel device 0000:01:00.0\nNIC transmit test\nintel selftest: pciaddr=0000:01:00.0 secs=1\nWaiting for linkup............ ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:01:00.0:\n           1,530,090 GPTC       Good Packets Transmitted Count\n          97,926,080 GOTCL      Good Octets Transmitted Count\n          97,927,168 TOTL       Total Octets Transmitted (Low)\n           1,530,115 TPT        Total Packets Transmitted\n           1,530,117 PTC64      Packets Transmitted [64 Bytes] Count\nNIC transmit+receive loopback test\nintel selftest: pciaddr=0000:01:00.0 secs=1 receive=true loopback=true\nWaiting for linkup............ ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:01:00.0:\n             726,052 MPC        Missed Packets Count\n             800,553 PRC64      Packets Received [64 Bytes] Count\n             800,562 GPRC       Good Packets Received Count\n           1,526,619 GPTC       Good Packets Transmitted Count\n          51,236,416 GORCL      Good Octets Received Count\n          97,703,936 GOTCL      Good Octets Transmitted Count\n              22,840 RNBC       Receive No Buffers Count\n          97,704,960 TORL       Total Octets Received (Low)\n          97,705,216 TOTL       Total Octets Transmitted (Low)\n           1,526,646 TPR        Total Packets Received\n           1,526,648 TPT        Total Packets Transmitted\n           1,526,649 PTC64      Packets Transmitted [64 Bytes] Count\nselftest: intel device 0000:01:00.1\nNIC transmit test\nintel selftest: pciaddr=0000:01:00.1 secs=1\nWaiting for linkup............ ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:01:00.1:\n           1,530,169 GPTC       Good Packets Transmitted Count\n          97,931,136 GOTCL      Good Octets Transmitted Count\n          97,932,352 TOTL       Total Octets Transmitted (Low)\n           1,530,196 TPT        Total Packets Transmitted\n           1,530,198 PTC64      Packets Transmitted [64 Bytes] Count\nNIC transmit+receive loopback test\nintel selftest: pciaddr=0000:01:00.1 secs=1 receive=true loopback=true\nWaiting for linkup.......... ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:01:00.1:\n             725,515 MPC        Missed Packets Count\n             800,522 PRC64      Packets Received [64 Bytes] Count\n             800,531 GPRC       Good Packets Received Count\n           1,526,051 GPTC       Good Packets Transmitted Count\n          51,234,368 GORCL      Good Octets Received Count\n          97,667,584 GOTCL      Good Octets Transmitted Count\n              22,840 RNBC       Receive No Buffers Count\n          97,668,608 TORL       Total Octets Received (Low)\n          97,668,800 TOTL       Total Octets Transmitted (Low)\n           1,526,076 TPR        Total Packets Received\n           1,526,078 TPT        Total Packets Transmitted\n           1,526,080 PTC64      Packets Transmitted [64 Bytes] Count\n. See pull request #48.\n. I've started looking into this again, but nothing seems to stand out as to why your machines (or NIC) have Missed Packet Counts, when mine does not. I tried a few things to see if I could help isolate further:\n- Lowering the number of transmit descriptors by a factor of 16 to 2,048 eliminated the MPCs, but they return when the number of transmit descriptors go back to 4,096. I figured by lowering the number of transmit descriptors, I could try and rule out a flow control issue by limiting how many packets could be sent relative to the number of receive descriptors that are allocated.\n- Lowering the speed of the NIC to 10 or 100Mbs also eliminated the problem. I suppose this was going to be obvious, but I wanted to validate as I've hate to assume.\nThe MPCs only manifest themselves on arbon/bern when at 1Gbs. I'll keep reading the documentation to see if I can find anything. The docs indicate the MPC could be due to PCI bus congestion, but I couldn't find any tools out there to measure bandwidth of the bus. Is it too much to ask for a 'pcitop' command?\n. Cool! I'll read up on those register settings in the morning. Nice find. \nOn Feb 9, 2013, at 1:14 AM, rahul-mr notifications@github.com wrote:\n\nHi all,\nI looked into this a bit. It seems the problem was with adjusting the values in RX regs (like I've mentioned in Issue #1):\nregs[RXDCTL] = bits({ GRAN=24, PTHRESH1=1, HTHRESH1=9, WTHRESH1=17 })\n  regs[RXCSUM] = 0                 -- Disable checksum offload - not needed\n  regs[RADV] = 1     --  1 * 1us rx intrrupt absolute delay\n  regs[RDTR] = 10    -- 10 * 1us rx interrupt delay timer\nThe current output:\nNIC transmit test\nintel selftest: pciaddr=0000:00:04.0 secs=1\nWaiting for linkup............ ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:00:04.0:\n           1,203,908 GPTC       Good Packets Transmitted Count\n          77,050,944 GOTCL      Good Octets Transmitted Count\n          77,061,760 TOTL       Total Octets Transmitted (Low)\n           1,204,100 TPT        Total Packets Transmitted\n           1,204,103 PTC64      Packets Transmitted [64 Bytes] Count\nNIC transmit+receive loopback test\nintel selftest: pciaddr=0000:00:04.0 secs=1 receive=true loopback=true\nWaiting for linkup.............. ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:00:04.0:\n           1,012,516 PRC64      Packets Received [64 Bytes] Count\n           1,012,534 GPRC       Good Packets Received Count\n           1,012,544 GPTC       Good Packets Transmitted Count\n          64,803,008 GORCL      Good Octets Received Count\n          64,803,584 GOTCL      Good Octets Transmitted Count\n          64,805,824 TORL       Total Octets Received (Low)\n          64,805,952 TOTL       Total Octets Transmitted (Low)\n           1,012,599 TPR        Total Packets Received\n           1,012,604 TPT        Total Packets Transmitted\n           1,012,607 PTC64      Packets Transmitted [64 Bytes] Count\n\u2014\nReply to this email directly or view it on GitHub..\n. Rahul, based on your patch, it looks like you solved the MPCs on bern/arbon by choosing a value of 2 for PTHRESH, HTHRESH, and WTHRESH. In addition, you set the timers RDTR to 10 and RADV to 1. How did you come to choose these values?\n\nBased on your findings, this morning I spent some time going through the various linux drivers (e1000e [driver for our cards 82571 and 82574], igb [other 1Gig cards], and ixgbe [10Gig cards]) to see what values they use for PTHRESH, HTHRESH, WTHRESH, RDTR, and RADV. Here are the values that they use:\n```\n      -- e1000e Settings (82563/6/7, 82571/2/3/4/7/8/9, 82583)                                                                                 \n      --   pthresh 32 descriptors (half of the internal cache)                                                                                 \n      --   hthresh 4 descriptors                                                                                                               \n      --   wthresh 4 descriptors                                                                                                               \n      --   RDTR 0x20 (docs seem to indicate we should use ITR instead of DTR, will investigate later)                                                                                                                          \n      --   RADV 0x20              \n      --                                                                                                                                       \n      -- igb Settings (82575/6-, 82580-, and I350-based):                                                                                      \n      --   pthresh 8 descriptors                                                                                                               \n      --   hthresh 8 descriptors                                                                                                               \n      --   wthresh 4 descriptors                                                                                                               \n      --   I think they use ITR instead of DTR\n      --                                                                                                                                       \n      -- ixgbe (82598-, 82599-, and X540 10Gig)                                                                                                \n      --   pthresh 32 descriptors (half of the internal cache)                                                                                 \n      --   hthresh 4 descriptors                                                                                                               \n      --   wthresh 8 descriptors                                                                                                                                                                                                                                                                \n      --   I think they use ITR instead of DTR\n```\nSo I decided to try the values specified in the e1000e driver for snabbswitch:\n-- Values for 82571 and 82574 controllers (same as e1000e driver)                                                                          \n      regs[RXDCTL] = bits({GRAN=24, PTHRESH5=5, HTHRESH2=10, WTHRESH2=18})       \n      regs[RDTR] = 0x20                                                                                                                          \n      regs[RADV] = 0x20\nMPCs are gone on bern/arbon (see bern output below), and the change didn't alter the good behavior I was already seeing on my machine.\nIC transmit test\nintel selftest: pciaddr=0000:05:00.0 secs=1\nWaiting for linkup............. ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:05:00.0:\n           1,206,966 GPTC       Good Packets Transmitted Count\n          77,246,528 GOTCL      Good Octets Transmitted Count\n          77,249,600 TOTL       Total Octets Transmitted (Low)\n           1,207,037 TPT        Total Packets Transmitted\n           1,207,040 PTC64      Packets Transmitted [64 Bytes] Count\nNIC transmit+receive loopback test\nintel selftest: pciaddr=0000:05:00.0 secs=1 receive=true loopback=true\nWaiting for linkup............... ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:05:00.0:\n           1,032,471 PRC64      Packets Received [64 Bytes] Count\n           1,032,487 GPRC       Good Packets Received Count\n           1,032,496 GPTC       Good Packets Transmitted Count\n          66,079,872 GORCL      Good Octets Received Count\n          66,080,320 GOTCL      Good Octets Transmitted Count\n          66,082,176 TORL       Total Octets Received (Low)\n          66,082,688 TOTL       Total Octets Transmitted (Low)\n           1,032,544 TPR        Total Packets Received\n           1,032,547 TPT        Total Packets Transmitted\n           1,032,551 PTC64      Packets Transmitted [64 Bytes] Count\nRahul, should we stick with your values or use the values from e1000 driver? I wasn't sure if you had any basis for your values (not that I'm implying the authors of e1000 did either) or if it was more trial and error. \nBTW, while in the Linux driver code this morning, I also took a look at the values used for TXDCTL. Here are the findings:\n-- e1000e Settings (82563/6/7, 82571/2/3/4/7/8/9, 82583)                                                                                   \n      --   pthresh 31 descriptors                                                                                                                \n      --   hthresh 1 descriptors                                                                                                                 \n      --   wthresh 1 descriptors (comment from e1000.h header file):                                                                             \n      --     in the case of WTHRESH, it appears at least the 82571/2                                                                             \n      --     hardware writes back 4 descriptors when WTHRESH=5, and 3                                                                            \n      --     descriptors when WTHRESH=4, so a setting of 5 gives the                                                                             \n      --     most efficient bus utilization but to avoid possible Tx                                                                             \n      --     stalls, set it to 1.                                                                                                                \n      --   https://patchwork.kernel.org/patch/1614951/                                                                                           \n      --                                                                                                                                         \n      -- igb Settings (82575/6-, 82580-, and I350-based):                                                                                        \n      --   pthresh 8 descriptors                                                                                                                 \n      --   hthresh 1 descriptors                                                                                                                 \n      --   wthresh 16 descriptors                                                                                                                \n      --                                                                                                                                         \n      -- ixgbe (82598-, 82599-, and X540 10Gig)                                                                                                  \n      --   pthresh 32 descriptors                                                                                                                \n      --   hthresh 1 descriptors                                                                                                                 \n      --   wthresh 8 descriptors (or 1 if ITR is disabled)\nThe current value of TXDCTL is 0x01410000 (PTHRESH 0, HTHRESH 0, WTHRESH 1). Luke, where did you come up with that value? Did you set it yourself or pull from an existing source? I decided to test with the values that e1000e uses above 0x0101011F (PTHRESH 31, HTHRESH 1, WTHRESH 1) and everything still looks good. So the question I have for you guys is whether we should tweak this value as well to the same value used by e1000e driver?\n. Agreed. \nOn Feb 10, 2013, at 4:22 AM, rahul-mr notifications@github.com wrote:\n\nOn 02/10/2013 12:52 AM, pkazmier wrote:\n\nRahul, based on your patch, it looks like you solved the MPCs on bern/arbon by choosing a value of 2 for PTHRESH, HTHRESH, and WTHRESH. In addition, you set the timers RDTR to 10 and RADV to 1. How did you come to choose these values?\n\nTrial and Error FTW ;-) Based on the info from the 82574L DS, I tried a \nset of values that seemed sensible - the DS gives some general \nguidelines for choosing the threshold values (and ofcourse, the range of \nvalues). The values were then tweaked based on feedback from the \ntransmit+receive loopback selftest.\nBTW I guess setting the threshold values to the e1000e driver defaults \nshouldn't be a problem. Given that ATM we only have a tx+rx loopback \nselftest running for a second, I wouldn't expect any drastic change in \nthe results. But in the future, I expect these values to be tweaked \nbased on actual load experienced by the NIC.\nRegards,\nRahul\n\u2014\nReply to this email directly or view it on GitHub..\n. Sounds good to me.\n\nRe: Lua, it's a strange language, seems somewhat \"hacky\" to me and lacks some basic library functionality that most high-level scripting languages offer (obviously a tradeoff on size), but LuaJIT's ability to call C is drop dead simple. I've never used any FFI interfaces before, as I don't play in C land often, but I can't imagine it could be any easier.\nWhat are your thoughts? How are you handling writing in something that is not a lisp or erlang :-)\n. I made a comment on one of the commits about how the rx and tx descriptor base addresses are set. Might this be something in your chase for the odd hugetbl behavior?\nhttps://github.com/SnabbCo/snabbswitch/commit/f5a3431ab266786f663c6e1d006c12765c25970b#commitcomment-2465756\n. It's stopped working on arbon at the moment:\npkazmier@snabbdev:~/snabbswitch/src$ sudo flock -x /tmp/snabb.lock ./snabbswitch\n[sudo] password for pkazmier: \nselftest: memory\nKernel HugeTLB pages (/proc/sys/vm/nr_hugepages): 10\n  Allocating a 2MB HugeTLB: Got 2MB at 0x32200000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x31c00000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x24c00000\n  Allocating a 2MB HugeTLB: Got 2MB at 0x20a00000\nKernel HugeTLB pages (/proc/sys/vm/nr_hugepages): 10\nHugeTLB page allocation OK.\nselftest: pci\nScanning PCI devices:\npciaddr         vendor  device  iface   status\n0000:00:00.0    0x8086  0x1237  -       -\n0000:00:01.0    0x8086  0x7000  -       -\n0000:00:01.1    0x8086  0x7010  -       -\n0000:00:01.3    0x8086  0x7113  -       -\n0000:00:02.0    0x1013  0x00b8  -       -\n0000:00:03.0    0x10ec  0x8139  eth0    up\n0000:00:04.0    0x8086  0x10d3  -       -\n0000:00:05.0    0x1af4  0x1001  -       -\nSuitable devices: \n  0000:00:04.0\nselftest: intel device 0000:00:04.0\nNIC transmit test\nintel selftest: pciaddr=0000:00:04.0 secs=1\nWaiting for linkup.............. ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:00:04.0:\nNIC transmit+receive loopback test\nintel selftest: pciaddr=0000:00:04.0 secs=1 receive=true loopback=true\nWaiting for linkup.............. ok\nGenerating traffic for 1 second(s)...\nStatistics for PCI device 0000:00:04.0:\n. FWIW: Ever since I got my Intel controller working, I've yet to stumble across a case where it stops working (i.e., no packet statistics are displayed). I've got the day off from work for today, so I hope to do some more investigation. In the meantime, I fired up this loop on my box to see if it's stops working:\n```\nroot@world:/home/kaz# while /bin/true\n\ndo\necho -n \"TIMESTAMP: \"\ndate\n./snabbswitch \ndone >> /tmp/debug.txt\n```\n\nIn another window, I'm just tailing the log file tail -f /tmp/debug.txt | egrep 'TIMESTAMP|TPT'. I'll keep an eye on this to see if it ever stops working like it has on arbon. Like bern, my host is not running in a VM, but rather the host directly.\n(and yes, I've disabled my call to memory.install so that I'm using HugeTLB pages instead)\n. Re: my last comment, the loop has been running for quite some time now (1,621 runs to be exact), and it still runs successful. I cannot seem to reproduce the issue where the stats stop printing out when I run snabb in my environment. I'll let in run overnight though.\n. Weird, I still don't see it on my machine. I ran the selftest 2,997 times over night and it's still working. Rahul, are you executing a different part of the code than I am? I'm only running the selftest as defined in selftest.lua. I'll leave my loop going all day long while I'm off at work to see if I can get it to happen.\n. Just got home from work, it's still running fine on my machine (5,729 runs now) and no segfaults in my dmesg. Nice find Rahul.\n. The MPC and RNBC counts are roughly 700,000 and 22,000 respectively. I don't run the current master as it doesn't support my NIC, but other than a few changes for PHY access, it's more or less the same. Because I recently got this working, I have no other baseline to make comparisons as to whether or not there were significant differences in results. \nNeed to head out for a bit. Good luck!\n. Hi Rahul, where did you find the recommendation to disable the GIO master? I'm trying to learn what this is as its not documented well in my DS (or more likely I don't understand what's going on). How was this causing the issue? I assume I'll need to add to my code as well, but I'm not having any issues so I'm unsure what this is doing.  Is this related to enabling bus mastering in init_pci?\n. Thanks! I completely missed that last paragraph in that post! I think I was too focused on the code. I'll read up and if I have any more questions I'll ask later.\n. To cross check, I went through the code for the igb and e1000e linux drivers. Guess what is the very first thing they do in their reset functions - disable master polling (see the comment about sticking). The igb_disable_pcie_master function below sets the CTRL.GMB bit, so it looks like you nailed this! I notice they set this prior to masking the interrupts, should we do the same?\n```\nstatic s32 igb_reset_hw_82575(struct e1000_hw *hw)\n{\n    u32 ctrl, icr;\n    s32 ret_val;\n/*\n * Prevent the PCI-E bus from sticking if there is no TLP connection\n * on the last TLP read/write transaction when MAC is reset.\n */\nret_val = igb_disable_pcie_master(hw);\nif (ret_val)\n    hw_dbg(\"PCI-E Master disable polling has failed.\\n\");\n\n/* set the completion timeout for interface */\nret_val = igb_set_pcie_completion_timeout(hw);\nif (ret_val) {\n    hw_dbg(\"PCI-E Set completion timeout has failed.\\n\");\n}\n\nhw_dbg(\"Masking off all interrupts\\n\");\nwr32(E1000_IMC, 0xffffffff);\n\n...\n``\n. What is that third argument toprotectedsupposed to be? When I read the code, I took it to imply the total number of bytes for the memory you wanted to protect. In your example, I guess I would have thought that the third argument should have been a2` like this:\nlocal total_len = protected(\"uint16_t\", context, 14+2, 2) \ntotal_len[0]\n. After searching uncle google, I think I found a way to send only a single commit. I'm closing this and going to try that.\n. Re: footnotes, they will render in other formats as well, but they render as traditional footnotes. In the standard pandoc HTML output, this means at the bottom of the entire book. It's ugly, but I'm sure some clever javascript could reposition them on the page though. I will try to beautify the HTML template next and see if I can get the marginal notes working in there as well. As for the epub format, I haven't looked at the output, so I'm not sure how footnotes are rendered yet.\nAdding a footnote uses pandoc's standard markdown. You can do inline footnotes:\n--- Comments are written in standard markdown. Pandoc provides\n--- a few extensions to markdown^[Markdown was designed with\n--- only the HTML medium in mind]. One of these extensions is the\n--- support for footnotes. I simply modified the Latex template to\n--- convert all of these footnotes to marginal notes. There is not\n--- special syntax other than markdown.\nYou can also write larger footnotes if you desire:\n--- Comments are written in standard markdown. Pandoc provides\n--- a few extensions to markdown[^label]. One of these extensions\n--- is the support for footnotes. I simply modified the Latex template\n--- to convert all of these footnotes to marginal notes. There is not\n--- special syntax other than markdown.\n---\n--- [^label]:\n---     Markdown was designed with only the HTML medium in mind.\n---     This note will be rendered the same, but this style allows one\n---     to write longer notes without disrupting the flow of the main text.\n---    \n---     It can by placed anywhere in the document below the point it\n---     was used. I find the paragraph beneath its use is a convenient\n---     location. The note body may contain multiple paragraphs. Each\n---     paragraph must be indented four spaces.\nIn terms of proper use, I don't think we'll be able to use the footnote concept as a shortcut to provide a brief comment on a code block. Although marginal notes are nice, my intent was not to change the semantics of a footnote. We may not be able to control the placement in all output mediums. I'd stick with additional exposition to the text one is already writing in a comment (references, brief comments, annotations, etc ...).\nRe: the new layout, sounds logical to me.\n. No access here at work, but will look when I get home. I'm no latex expert,\nbut  the easy way of ruling out any of our custom tweaks to the template is\nto modify the makefile and remove the --template option that invokes our\ncustom template. This will tell you if it is us or just something wrong\nwith the pandoc latex writer. I'll take a peek tonight when I have access.\nBased on the error message, I'd look around any of your markdown lists (the\nenumerate latex package helps format those).\nOn Monday, April 29, 2013, Luke Gorrie wrote:\n\nI created a branch called bookediting and tried to improve the overall\nreadability of a coupe of chapters.\nHowever I get a pandoc/LaTeX error when trying to make the book:\n$ make book\npandoc --template=doc/template.latex --latex-engine=lualatex -V fontsize=10pt -V monofont=droidsansmono -V monoscale=.70 -V verbatimspacing=.85 -V mainfont=droidserif -V sansfont=droidsans -V documentclass:book -V geometry:top=1.0in -V geometry:bottom=0.75in -S --toc --chapters  -o doc/snabbswitch.pdf doc/snabbswitch.md\npandoc: Error producing PDF from TeX source.\n! Undefined control sequence.\n\\enit@endenumerate ->\\enit@after\n                                 \\endlist \\ifx \\enit@series \\relax \\else \\if...\nl.481 \\end{enumerate}\nmake: *** [doc/snabbswitch.pdf] Error 43\nAny idea how to debug this? :-)\n?\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/pull/59#issuecomment-17175293\n.\n. \n",
    "justincormack": "The FUSE kernel API looks ok, probably easier and more lightweight to bind to that rather than linking to libfuse, which has its own event loop, but either would work.\nI think it might be easier to start by modifying KVM and getting that working, then adding FUSE after. Or lguest rather than KVM as it is even smaller and simpler as a proof of concept.\n. vmprof.c is the culprit, does use it. I didn't notice this until I upgraded Ubuntu to 13.04 so maybe got linked without being specified before.\n(I do wonder if statically linking the whole thing with Musl rather that relying on whatever glibc is on the system might make more sense as a firmware).\n. Combined into a different branch - see the other pull request. I seem to get away with just knowing enough git to do whatever I want to do, so not sure what the best way to do that is...\nI know what you mean about doing profile-driven optimisation; however if used right the ffi structs can be easier to read than straight Lua code sometimes as they automatically attach the metamethods to any struct of the same type, including structs within other ones, and the fact that you get a garbage collection method. For ljsyscall I ended up using ffi structs for pretty much everything that is defined on the C side, though not sure that would make so much sense here. But as you say its easy to change.\n. I guess the model would be to convert the register descriptions to a (packed) struct definition and then feed that to LuaJIT (or C). It would still need an overlay (Lua or ffi) for the stats registers that reset.\nIt probably makes sense to convert a few more drivers to the same model to see what works though before making any changes.\n. Bizarre, it usually just closes when you merge. What options is it giving you? I guess you can just close them both...\n. Updated.\n. This mail suggests that the performance issue is only with Sandy Bridge machines, more recent ones should be fine, not sure what you are testing on\nhttp://permalink.gmane.org/gmane.comp.networking.dpdk.devel/7409\n. No, I can't find any specific thing that would explain it. There seem to be a lot of workarounds and fixes for specific hardware, but nothing general, so maybe it is machine/pci device specific?\nIt might perhaps be best to recommend a 3.16 or later kernel for iommu as a placeholder.\n. Confirmed that is working correctly on Atom:\nselftest: checksum\nno avx2\nsse2: 1000/1000\nselftest: ok\n. It looks unrelated to this patch, but hard to tell what the issue is...\n. I doubt if it is dlsym. I will have a look on one of my Musl systems and\nsee what's up.\n. Yes it is failing to load \nlocal STP = require(\"lib.lua.StackTracePlus\")\nWhat is odd is there seems to be no change from master here, and the symbols in the binary look exactly the same, both under Musl and glibc... Other than the Musl embedded symbol names have three underscores not one eg luaJIT_BC___syscall_util\n. A bunch of these are simple Posix compliance/portability/correctness issues, not in any way Musl specific, I might just submit these as individual issues so we can narrow down the Musl specific issues.\nSee #527 #528 not 100% sure about some of the others I think they need a bit of cleanup.\n. Arguably this change is not a portability change, it is a correctness change. The documentation for memcpy tells you to use #include <string.h>  but you decided to wing it without. It is unclear if it happens to work in glibc or you have a missing compiler warning, are you sure? Just fix it.\nIn terms of easy wins on portability, the easiest of all is to stop hard coding gcc in the Makefile and build with clang too.This gives you a different sett of warnings, always useful.\nNext up is to build and test on more Linuxes. I have started using Docker for this, eg https://github.com/justincormack/docker-frankenlibc which builds at https://registry.hub.docker.com/u/justincormack/docker-frankenlibc/builds_history/234523/ - it is not quite a perfect test, as it does not run with the correct kernel for each system, but I have a reasonably comprehensive collection of virtual machines too.\nThen porting to Musl will fix general Posix compatibility and glibc-specific oddness, plus giving you a smaller static binary.\nThen there are two more complicated things, other operating systems eg FreeBSD, which is going to have different interfaces for some things, and other architectures, which may have different synchronisation issues.\nBut generally, you want to merge correctness fixes irrespective of a particular target, and general changes that make it easier to progress towards portability and testability, eg I am inclined to send a patch that replaces gcc in the Makefile with $(CC) in order to be able to test with gcc5 or clang, because now the Makefile is hostile to testing different compilers, I dont want to have to do that in the context of eg making Snabb work with gcc5, as it probably already does but its annoying to even find out now.\n. Yes I have to agree that the C code is a problem... and Luarizing it gets rid of most of the issues. The main necessary C code is gcc intrinsics for now (and they are simple with no deps).\n(and ljsyscall just got ppc64 and arm64 support in the last week, which are the two interesting not amd64 architectures right now).\n. @nnikolaev-virtualopensystems I find it quite annoying having to use a different repo for docker builds. It is only adding a couple of files, and now travis-ci supports docker it is useful for eg tests too, and you can do automated builds to docker hub on every change.\n@eugeneia tooling for creating populated VM images without loopback mounts largely seems to require booting qemu to boot an installer. There are some other workarounds, eg mkisofs can build a bootable cd image without root.\n. You cant very usefully use Dockerfiles not at the root - they can only see files for ADD commands below them in the tree. It is kind of annoying, but if you have a main dockerfile that builds everything you can still run tests via parameters to the docker RUN command.\n. Subtrees are much more useful in this sort of project and have all sorts of advantages (like all the code being there on checkout).\n. \"I could not resist having another look at methods for storing and sharing big files like VM images in a controlled way.\"\nThe advantage of dockerhub is you can do automated builds on it, and then retrieve them by tag. You can use travis to do this too, although the authentication is more complicated.\n. You can apply for an official MAC prefix, probably better than using random\naddresses.\n. A few thoughts\na. A standalone statically linked executable would be really nice, I really appreciate the ease of install of for example Go projects that do this now.\nb. I would still recommend doing 4 first, ie merging this patch, as it is generally helpful, and makes it easier to experiment with how to get to 1.\nc. What size and alignment for memcpy is most important? Guessing around 1k, maximally aligned would be typical. This is really a separate issue, maybe we should have our own memcpy code to be in control anyway, will look at performance.\nd. I did do some work ages ago on dealing with the dlsym issue, by generating a dlsym function which looks like the normal one but just returns pointers to the static functions. See https://github.com/justincormack/ljsyscall/blob/master/examples/dl.c but it was only semi scripted as a proof of concept. I think this needs more thought in terms of a generic simple solution. Also in terms of generated code size, you lose some of the advantage of static linking if you simply link all symbols in, rather than just the ones you use, so it needs a bit of fine tuning. I think I might try a generic script to work with ljsyscall statically linked as a first pass on this.\nSo I think it might still be best to split this up into a few different activities even if aim is 1.\n. One thing that is different is that we write the tests for special cases in Lua so they are inlineable, so we don't need to run the tests at all in a tight loop if they are static. We can also use the built in luajit memcpy (which we could change if it is non optimal). So we need a slightly different test setup.\n. As I now work for a company, in which I have had to raise the issue of\nproviding attributions for code we use, having a central source file for\ncopyrights is hugely more convenient than writing code to scrape every file.\nOn the other side, if I sign off my commits with my work email that is all\nthe attribution they want.\nOn 26 Jan 2016 1:19 pm, \"Luke Gorrie\" notifications@github.com wrote:\n\ncompanies want to put their name out there\nHere are some suggestions for perfectly fine ways for people and companies\nto be visible in the community:\n- Maintain valuable subsystem branches on a MyCompany/snabbswitch\n  Github fork.\n- Give public presentations about Snabb Switch.\n- Blog about Snabb Switch.\n- Represent Snabb Switch in other communities (OpenStack, OPNFV, ETSI,\n  FOSDEM, etc).\n- Sponsor work on a project that is related to Snabb Switch (LuaJIT,\n  NixOS, etc).\n- Donate important equipment to the lab.\n- Buy Google Adwords for \"snabb switch\" searches.\nThese all seem like valuable activities that can be very naturally\nassociated with the person doing them and their employer.\nHowever, I agree that it is negative for the project to inject direct\nadvertising into the development process e.g. by trying to maximize the\nnumber of times that other developers type your company name or look at\nyour company logo and so on. This comes across as antisocial to me, like if\nmy neighbour puts up a Coca Cola billboard that I have to look at from my\nliving room window. There are better ways to promote a brand.\nJust a personal opinion:\nI think it is also a reality of open source projects that individuals get\na large share of the credit for their work, more so than in less\ntransparent communities where nobody knows who is \"making the sausage.\"\nThis is empowering to individual developers and companies need to accept\nthat.\nOver time as Snabb Switch grows the experienced developers will have\nbetter and better opportunities available to them and their employers will\nneed to work harder and harder to keep them. For example, a senior Linux\nkernel hacker can pick and choose between any number of employers large or\nsmall. I see this as a good thing: a virtuous circle. I don't think it\nwould reflect well on a company to be seen as fighting this by trying to\ntransfer credit from its employees to its corporate entity.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/SnabbCo/snabbswitch/issues/729#issuecomment-174978281\n.\n. I will be around at Fosdem (but not until late on Friday).. I am arriving fairly late on Friday, and already commited for dinner on Sat but let me know where you will be and will see how to meet up.... \n",
    "distributed": "Thanks for the quick answer.\nI don't understand what __sync_synchronize() is supposed to do, exactly, either. Not having access to my favorite Linux box at the moment, I compiled it on my OS X computer with what identifies itself as \"i686-apple-darwin11-llvm-gcc-4.2 (GCC) 4.2.1\". The __sync_chronize() compiles down to... nothing. I'm surprised, as I would have expected it to be an MFENCE or something.\nI just had a quick glance over the i82599 data sheet. Seems like these days even network controllers are allowed to perform all kinds of dirty tricks that processors already do ;)\nI find good information on memory access ordering, cache coherency and, generally, architecture quite hard to come by. What sources do you consult for that kind of stuff?\n. The Software Developer's Manual has little information that I can correlate to these issues. Maybe I'm not looking in the right places, though.\nOne thing I did find is this:\n\nAn I/O agent can perform direct memory access (DMA) to write-back memory and the cache protocol maintains cache coherency.\n\nThe manual does not define the term \"I/O agent\", though.\nI checked the Linux driver code for the Intel Pro 100 family of adapters. It claims to work across multiple architectures, big- and little-endian as well as over a range of memory architectures. It makes relatively little use of the Linux DMA system and, IIRC, only has a write barrier in one place.\n. ",
    "javierguerragiraldez": "AFAICT, __sync_synchronize() uses the x86 asm 'lock' prefix, which isn't an instruction by itself: it modifies the next instruction to have full memory barrier behaviour.\nBut, I don't know if this memory barrier is intended only for CPU cores, or if it's also handled by the motherboard and/or PCI interface.  If not, then i guess the kernel treats the mmap()ed PCI space as non-cacheable in the TLB... \nlooking around on the Linux sources (drivers/pci/pci-sysfs.c) it uses some interesting flags for the mmap, and the whole TLB-building code is very IO-specific, including the warning (\"accessing these pages have side effects\"), so i guess it's marked as not-cacheable from the start.\n. hum.... the branch is right; maybe i should do the merge to trunk and send that as the pull request?\n. yes, it's the same on github as on local.  i condensed a little over 20 commits to those 11 ones that i feel are more readable.\nmaybe another round with rebase -i to distill a little more...\ndo you want it just tighter, or only the merge?\n. now it's all squashed to a single commit, including the small fixups we discussed\n. (ups, wrong button)\n. On Tue, Dec 3, 2013 at 7:37 AM, Luke Gorrie notifications@github.com wrote:\n\nJavier, could you take a look at make test? I suspect that some failing cases only lack some simple vfio initialization somewhere:\nERROR     testlog/core.memory\nERROR     testlog/apps.intel.intel_app\nERROR     testlog/apps.ipv6.ipv6\nERROR     testlog/apps.vhost.vhost\nERROR     testlog/apps.vhost.vhost_apps\n(I have not dug in but those seemed to work before merging the vfio patch)\n\nok, will check!   it does look like some initialization missing\n\nJavier\n. including a copy of vfio.h (or a subset) would solve the compile-time dependency, but the VFIO-related functions wouldn't work on old kernels.  there are several assert()s, so they should just fail fast without ill effects.\nthe runtime code in bus.lua does a couple of tests (checking for some files/dirs in /sys) to choose between the vfio-based and direct-pci versions of the code, so that should be almost-working too, unless the original pci-based code has already bitrot...  in any case, it shouldn't call any vfio.c function if the kernel doesn't show the required /sys entries.\n. To handle a device using the VFIO API, a process must take hold of all devices from each iommu group.\nIn this case, intel_app.selftest() tries to open the \"0000:01:00.0\", device, vfio.get_vfio_fd() calls open_device_group() for the group 15, which also contains the \"0000:01:00.1\" device.  But when it tries to open it too, fails, because the /sys/bus/pci/devices/0000:01:00.1/driver link was missing.\nThe original PCI-based method depended on unbinding the NIC devices from the kernel network drivers.  When unbound, a device doesn't have the /sys/..../driver link.  But the VFIO API is implemented as a kernel driver, so a device must be bount to it before any process can use it.\nThe binding and unbinding typically done with shell scripts, but it's also implemented in Lua.  To unbind a device from the current driver:\nsudo ./snabbswitch -l lib.hardware.pci -e \"lib.hardware.pci.unbind_device_from_linux('0000:01:00.1')\"\nTo bind a device to the vfio driver (unbinding from the current driver if needed):\nsudo ./snabbswitch -l lib.hardware.vfio -e \"lib.hardware.vfio.setup_vfio('0000:01:00.1')\"\nSince you need whole groups, there's a second argument to make it bind all devices of the same iommu group as the one mentioned:\nsudo ./snabbswitch -l lib.hardware.vfio -e \"lib.hardware.vfio.setup_vfio('0000:01:00.1', true)\"\nThe idea of keeping these setup procedures separated from the main task is to make it possible to call these during startup with full root privileges, and later start the switch process(es) with less privileges.\nAfter binding 0000:01:00.1 to the vfio driver, the initialization succeeds... but fails in apps/intel/intel10g.lua:\n$ sudo ./snabbswitch -t apps.intel.intel_app\nWaiting for linkup. ok\napps/intel/intel10g.lua:165: assertion failed!\nstack traceback:\n        core/main.lua:104: in function <core/main.lua:102>\n        [C]: in function 'assert'\n        apps/intel/intel10g.lua:165: in function 'receive'\n        apps/intel/intel_app.lua:29: in function 'pull'\n        core/app.lua:53: in function 'breathe'\n        apps/intel/intel_app.lua:68: in function 'selftest'\n        core/main.lua:45: in function <core/main.lua:31>\n        [C]: in function 'xpcall'\n        core/main.lua:109: in main chunk\n        [C]: in function 'require'\n        [string \"require \"core.main\"\"]:1: in main chunk\n. about the crash on the selftest, I think it's related to this commit: 87b82468b1561451ea56afebd7e8f072aa440436 (\"vfio: Map Snabb virtual addresses 1:1 onto IOMMU addresses.\") i've reverted it in my experimental branch...\n. ok, merging both drivers makes sense.  at the least, it will eliminate the duplicated register description :-)\nthe multiqueue.lua generalizes the \"many-apps per HW device\" idea.  it turned out much simpler than i thought, so it's not really hard to incorporate it into the app constructor.\nalso note that it's just a 14-line function.  most of the rest is a mock object to properly test it.\nto the chopping table!\n. Hi.\nhope this is what you meant.  I pulled both OO-like thingies (macaddress and index-set) from the lib.lua to their own /src/lib/ files.\nI also found how to squash commits on one branch and keep them on another... (did i mention how i miss Fossil?)\nI hope Alexander's PR gets merged soon too; I'd like to use his packet-managing code for the tests of next features (multibuffer, filters, etc)\n. ok, i think the three commits is an artifact of doing a rebase with the PR open.  i'll close this before rebasing again.\n. new rebase!  amazingly, the open PR just caught all changes without surprises\n. On Mon, Apr 21, 2014 at 3:59 AM, Luke Gorrie notifications@github.com wrote:\n\nWould it be reasonable for you to send these changes as separate pull requests (topic branches)? It's good to have the split into separate commits but I think it's justified to take a step further because I see these as unrelated changes.\n\nyes, they're mostly unrelated, only done together because of\nhistorical accident.   i'll tidy it up tonight.\n\nJavier\n. Sure, what would be the requirements? just a return code when invoked on the command line?\n. Ok.  I've found that rebasing with the PR open does work, so it seems this is some way to show code for discussions, and with everything on the table, it's possible to choose which goes first.  other fixes can go ahead in parallel so when your PRs get merged, mine should have better names and rebasing after yours should be easier.\n. > It should, what is the diagnostics?\nWell, one way we could work is that we both open PRs, with them open, we all three discuss on both of them, fixing those things that don't depend on each other, and choose which one should go first.  then the other one can rebase, a little more discussion, and get pulled.\nI guess it's workable with two PRs open at the same time, more than that sound like too hard to keep in order.\nalthough, I'd prefer if there could be more discussion before raising a PR, after all, everybody can look at everybody else's forks and branches....\n. > Let's see what Javier cooks up and then revisit the patches if that's okay for you.\nto make it go faster, the issues i should hit tonight:\n1. naming conventions\n2. use variables for PCI addresses, skip test if not there.\n3. return codes for passed/fail/skipped.  i guess if one fails return fail, but what if none fails but some are skipped? return 'pass'? (because all that were tested passed) or skipped? (because not everything was tested)\nanything else?\n. closing this, i've split in three different PRs\n. ok, i'll rebase my PRs on this.   should I keep them as three different (doc, test fw, fixes)? or should I kill the documentation one?\n. hum... make book does work here, but only after cd src.\n. Ok... this is now compatible with the Travis CI, and the output is readable.  I've converted several .selftest() functions to testfiles.  I haven't removed all print()'s, but they're now captured and shown at the end.\nThere's also support for conditionally skipped tests.  Just put a boolean in the table instead of a function (or subtable) and it will be shown as a skipped test.  That makes it easy to use a condition like:\n{\n    test_nic = not os.getenv(\"SNABB_TEST_INTEL10G_PCIDEVA\") or function ()\n        ......\n    end,\n}\n. ok.... still familiarizing myself with git history warping.  how's now?\n. so should i split it in two set of fixes: one for the ethernet driver and one for the packet manipulation? (which was broken for multi-iovecs too)\n. I have split this branch in two, one with only the fixed code in Intel10g (intel_multiiovec), and another with all the manipulation utility functions and a testfile that uses them to test many cases.  what i don't like is that the \"fixes only\" branch can't test the multi-iovec case, because there's no way to create such packets for the testcase...\n. I still think the main app loop (or rather the breath() function) gets it wrong. When a link has data, the app is called but the flag is unconditionally cleared. If not all packets are processed it will skip the next loop, at least until new packets arrive\n. or get rid of the has_new_data flag and use not link.empty() instead.  (slightly complicated because of the local link variable that shadows the link module)\n. I vote for 2.\nIf the app isn't rushed to consume all packets, then other apps should have a chance first, so 1 sounds like too aggressive when the app doesn't care.  3 might be appropriate if the app really needs more input, but then it's easy for it to quickly return if there's not new packets.\nI'm not sure this is related, but i'm sure the current behaviour isn't correct in some cases.  I guess it would be triggered intermitently when the app doesn't process all the input.  for example, when the output is temporarily full.  your initial description \"The issue seems to be too few packets being allowed through packet_filter1.output\" sounds like the perfect recipe for it.\n. is it better now?\n. just extended the commit message.  also fixed the failing Travis CI (it was failing at sending the 'skip' result, duh!), but now it failed on the packet filter app. :-(\n. seems very nice (and extensive!).  definitely will try to pass that through the drivers.\nnitpicking: there are a couple of places where it applies the \"Shlemiel the painter's algorithm\", building a string by consecutive concatenation.  AFACIT it's only in the generation phase, so it's not important, but it stands out while reading the code.\n. rebased after merge of #176 \n. rebased for current master ([7209414])\nAlso added verification to intel_app.selftest() as mentioned in #258\n. very interesting, can you share the configs? or better yet a whole failing test setup?\n. ok.... i have two related ideas:\n- this patch removes some \"force to work\" bits, relies more on having real cables plugged in.  since your test doesn't try to send packets down the wire, it doesn't need a good cable.  maybe should we expose some flag to set the FLU (force link up) bit?   test: can you try to revert only the set_SFI() function?\n- the NIC at 0000:84:00.1 fails to negotiate link up more than half the times.  it seems that it has a bad cable.  test: can you try with the card at 0000:87:00.1 ?\n. ok, so I would say that the new initializations expect the link to be reliably up before considering it to be \"active\".  That is certainly helpful when actually using the Ethernet wire, but not if you only want the chip itself.\nI think one fix would be to add a configuration flag to tell the driver that we want to use the chip without caring about if the wire is plugged in or not.  Setting this \"force up\" flag would allow previous behavior while still doing the whole initialization.\nwithout the flag, it would still activate the app, but without forcing the link to appear up.   This would be better in context of #264 \n. I think in the real world, a \"force up\" flag would be useful if you want to use the chip as a switch and don't care about communication out of the physical box.  Otherwise, you'd need to plug a cable (and connect the other end somewhere) just to activate the card.\nAbout testing close to the real world... in my tests on Davos, two cards are very solid (0000:82:00.0/1 and 0000:87:00.0/1) and the other two fail too much (0000:02:00.0/1 and 0000:84:00.0/1).  It would be interesting to see if the standard driver agrees with that; if so, then these are useful to test any 'fail-safe' or diagnostic features, but for everything else, we should use the 'good' ones.\nWe haven't defined any status reporting mechanism for apps.  It's easy to print a cautionary message, but what about an optional .status() method?  No idea about what to return from that...\n. Just added a local switching setup to intel_app.selftest().  It indeed fails on 0000:84:00.1, (curiously enough, it does work on 0000:84:00.0).  I think we can safely attribute it to physical failure.\nThe link-up wait does timeout (after 0.5 sec, way more than the 0.3 mentioned in the docs), but doesn't disable the chip because it does work for other things.  Issue #264 proposes changing the whole state flow, so I'll continue there.\n. it seems it's not this specific merge what broke the 'no-link' functionality.  I've reverted to just before it and couldn't make it work on 0000:84:00.1.  It even fails on 0000:82:00.1, which confirms my original intention about this patch being a more reliable initialization code.  I think this means this commit should stay.  For issue #264, I'll try to find an initialization recipe that works with and without the cable plugged in.\n. yes, this was no longer true.\nalso corrected to respect the merged PR #237, which moved most of the code from designs/neutron/snabbnfv-traffic to lib/nfv/config.lua\n. The last commit fixes the merge history mismatch, hopefully the snabbot would approve it!\n. nice blog post.  i realized that my idea is similar to yours but extending to more than two steps, making it look like a ladder instead of a two-step swap.\nThe main advantage of more steps is that you can tune the coarseness level.  with two steps the minimum expiry time would be a single swap and the maximum is twice that.  with ten steps the difference is just 10%.\nWith typical conntrack timeouts between two hours and 5 days, I assumed that 100% jitter time would be just too much... but honestly, who cares if a connection lasts 4 hours instead of 2?\nThis version should be roughly workable, except that it should be rebased after Max's other changes.  The next obvious changes are: a) move the \"if time has passed, swap steps\" check out of the inner loop and make it happen only once per breath cycle, and b) immediately kill a connection when seeing a FIN/FIN ACK or RST packet.\n. I found that I had totally forgot about an essential feature of stateful filters: they're bidirectional.  Even more, they're more important for the 'other' direction.\nSince the packet_filter app is unidirectional, I'll create a 'named pool' of conntrack tables.  A rule on a given filter app 'feeds' a conntrack table on any packet that passes the rule, and another rule (possibly on a diferent app) 'checks' the contrack table and passes packets if on an existent connection.\nnow back to the drawing table...\n. It's getting better all the time...\nIt already does track connections on named spaces, I've added a dump feature to see what's getting there, but I think i should reuse some IP->string code.\nThe biggest change is that a packet_filter config string is no longer just list of rules, now it has to declare if it does tracking (recording any accepted connection) and or passing (accepting a packet that belongs to an existing connection) and under which name.  This change means the neutron2snabb scripts would have to change too.\nA feature still missing is to detect TCP stream closing and actually close the relevant conntrack entry.\n. what about this state flow:\n- Initialization code does the whole bit-fiddling to set the chip, and goes to 'starting' state.\n- in 'statrting' state any available packet is moved between links and chip queues.  I guess the chip won't send them, so the queue could fill and stop transmission.\n- on each breathing cycle, the app checks if the chip has detected a link up.  if so, the app goes to 'running' state.  packets should flow to/from the wire.\n- if the 'starting' state has lasted too long, it goes to the 'no_wire' state, and either sets FLU or LOOPBACK, to make it useful for local switching.\n- while on the 'running' state, it's conceivable the cable can be (un)plugged at any time.  It seems the state bits reflect that, but I think there's no need to switch modes on the chip, since there's no carrier, it simply won't send packets and eventually the buffers would start to fill until the cable is plugged again.\nI think this would also solve Max's observations on #240.\n. Good spot: I don't know how big are the virtio buffers usually.  The 82599 receive descriptors don't have a 'size' field; all buffers in each queue are assumed to be the same size, set by a 4-bit field in the SRRCTL register (one per receive queue), it allows from 1KB to 16KB in 1KB increments.\nCurrently the driver assumes it's 4KB (set in M_sf:set_receive_descriptors ()) because that's the hardcoded size of the link rings.\nHow is it with virtio drivers?  I guess the VM allocates them, but is there a default, or some hint that we can set?  Is the Linux kernel driver the one responsible for this? or can an application set the buffer size with some socket option?\nWhat I can do is to make the buffersize reported to the chip a configuration option.  That way, if the linux default is 2K, we could configure intel10g.lua to assume 2KB per buffer.  But what if it gets a smaller buffer? it would be easy to just ignore it, but how would other apps react to that?\n. ok, i'm going to change the test from init to add_receive_buffers().  rx_buffersize isn't mandatory, at the moment it defaults to 4KB (the previous hardcoded value).  The failure on lib.nfv.config was because i forgot the -1 on the testing loop.\n(i'm updating with those changes right now)\n. ok, now add_receive_buffer dynamically adapts to given buffer size.  intel_app replaces any buffer smaller than 1024 with a generic one.\nI'm not sure what to do with those \"too small\" buffers.  now i'm just disposing them because buffer.free() seems intelligent enough.  but what would happen to the just allocated buffer when the packet is deref()ed?  will it return to the generic freelist?  if so, the next intel_app:add_receive_buffers() would add less buffers, potentially zero...\n. hum... that part of virtio/net_device.lua copies packets without first checking if it could allocate the destination.  should it be fixed on this PR?\n(related question: if reverted, how can it be reopened?  last time i had to do a bit of careful cherry-picking)\n. added the transmit part of the offload; it simply builds a context descriptor for each packet, compares with the last one sent by the app and only sends it if it's different.  must be doing something right because the chip is consuming descriptors form the queue, but discarding all packets.\n. packets flowing!\n. Thanks!  I was wondering how to do a full-cycle test... i'll check this test setup.\nabout C.VIRTIO_NET_F_CSUM, this is what the virtio app reports to the VM, right?  i guess in theory it should check which of these features are supported  by the next app 'downstream'... but probably only the hardware driver apps really need to care about checksum...\nin any case, i think it's appropriate to simply force this flag for testing purposes.\n. the context descriptor needs the following info about the packet(s):\n1. is it IPv4?  if so, it needs an IP checksum\n2. is it UDP? TCP? neither?  each has a different checksum style\n3. MAC header length (typically 14 bytes, could be 18 if VLAN tagged by software)\n4. IP header length\nthe chip already knows where to put each checksum style.\nAFAICT, the packet.info contains MACLEN+IPLEN, but not each one independently.  Also there's no hint about the specific protocol... unless we infer it from the checksum offset?\n. I've been checking the lib/nfv/selftest.sh scripts, finally got them to run semi-reliably on chur.\nAdding C.VIRTIO_NET_F_CSUM to the set of supported features makes ethtool -k eth0 within the VM to report tx-checksumming and tx-checksum-ip-generic as on, but rx-checksumming, tx-checksum-ipv4, tx-checksum-ipv6 and tx-checksum-sctp remain off [fixed].\nChecking both lib/virtio/virtio.h and the Linux kernels for other VIRTIO_NET_F_xxxx flags, I see some related to TSO, but nothing about RX or protocol-specific checksumming.  Is that a limitation of the current virtio flags?  I guess @n-nikolaev is the one to ask about it.\nTSO seems easier now that most of the context descriptor code is in place.  It's mostly about defining a huge packet (probably in more than one buffer) with a few fields off and enabling some extra bits.  Each segment will have a copy of the header, but with a recalculated checksum and (if TCP) byte sequence.  Of course, if the emulated eth0 doesn't advertise it, the VM won't emit those huge packets in the first place.\n. Ok, just rebased\n. Debugged, rebased, cleaned and rewritten.  This should be ready for merge.\nThe bugs with the virtio device are fixed, but there still  seems to be some issues with tunneling.\n. added CPU-based checksum on want_modify()\n. On Mon, Jan 12, 2015 at 2:22 PM, Max Rottenkolber\nnotifications@github.com wrote:\n\n@javierguerragiraldez @lukego I now believe that we have a regression in the intel driver. To see for yourself run the intel_app selftest repeatedly on any server, on any card. The error varies (sometimes unbalanced packets, sometimes not enough packets, sometimes sent == prevsent, sometimes everything passes).\n\nI'll take a long look tonight.\npreviously, i've seen that on Davos 0000:82:00.0/1 and 0000:87:00.0/1\ndo work consistently, while :02: and :84: fail most of the time.\non chur I use :82: by default, but there it usually worked quite good\non any card.\n\nJavier\n. I get the selftest() to consistently run with the :82: card in chur, but only after unbinding it (both /0 and /1) from the ixgbe driver.  The :84: card passes the 'in chip' tests, but fails tests that actually put bytes in the wire.\nThe NFV tests are better in that the script does connect with the VMs, but fails on the first ping, apparently the packets reach the intel_app but don't appear in the other end.\n. > I suppose that we should be automatically unbinding the NICs from the OS\n\ndriver before we use them.\n\nI think some very early versions did that. I don't remember why it was\nremoved, maybe around the issues with non-root users.\nA similar preparation will be needed to migrate to SR/IOV, although that\nwould have to be on a selection process, not initialization.\n\nJavier, do the NFV tests work for you on the \"good\" NIC? That is the most\ninteresting question from my perspective.\n\nUnfortunately no. I think there is a regression, but can be something\nsimple like a mismatch between the app and the scripts.\nFrom the log it seems the packets reach the Intel app but are not\ntransmitted.\n\nJavier\n\n\u2014\nReply to this email directly or view it on GitHub.\n. > > Javier, do the NFV tests work for you on the \"good\" NIC? That is the most interesting question from my perspective.\nUnfortunately no. I think there is a regression, but can be something simple like a mismatch between the app and the scripts.\n\nfinally got the NFV test running, at least with \"good\" NICs (:82: in chur).  it was a silly thing, a MAC address mismatch between the config script and the VM images.  It was on my local config, so I guess that's why it didn't affect everybody.\nAll tests pass on the master branch, but on this (offload) the tunnel tests fail.  Just removing the C.VIRTIO_NET_F_CSUM flag from the advertised virtio features allows it to pass, without changing the code at all, so I guess it's because the Linux driver sends fragmented packets when this flag is active, and the tunnel hasn't been tested on this case.\nI see that the Virtio_A app is dying at line https://github.com/SnabbCo/snabbswitch/blob/master/src/lib/virtio/net_device.lua#L247 because b was nil.  Adding lots of print() to code that I don't really understand, it seems that the Virtio_B app has 8192 packets on the self.vring_transmit_buffers freelist, but Virtio_A is empty.  Should it have been initialized by then?  Or is it because the app hasn't passed through a :push() cycle yet?\n. > Maybe the intel selftest is really sensitive to minor hardware failures, e.g. packet loss on wire?\nNot really.  For example, the switch-like 'filter by MAC' test builds two packets, one with the right destination address and the other to a bogus destiny.  Then sends both as many times as possible in a second.  Finally it tests that the receiving app gets roughly half as many packets as they were sent.  And when I mean \"roughly half\" I mean anywhere between 25% and 75%.  That way, if the filters are badly defined we get either 0 or 100% and the test fails.  But even lets say, 30% packet loss (which would be disastrous in a real network) would result in 35% received packets, well within the 25%-75% passing range.\n. > this bug will probably disappear during the Great Simplification\nYes, it looks very probable.  Still, I tried with changing the want_modify() function to always return a coalesced clone, and didn't seem to make a difference (besides a huge slowdown!).\n. The selftest() is working again (after straightline merge), but I can't get the NFV selftest.sh to correctly boot the VM images on chur.  Is that a known issue?  I see some other PRs have similar snabbot reports...\n. > you can see how qemu fails by removing the /dev/null pipes off the qemu calls in bench_env/common.sh\nGreat, that's what I was looking for.\n. I managed to get the NFV tests working on the master branch, it was mostly about configuration and not code.  This branch does pass the selftest but not NFV; I think there's something fishy between this and virtio code, but can't pinpoint it yet.\n. ... and iperf passes!\none fixed one small -- TODO in lib/vritio/net_device.lua: to copy packet flags from the link to virtio.\nstill lots to clean up, and not stellar performance (max so far is 8.80 Gbits/sec with jumbo frames), but enough for tonight!\n. After several nights of very small performance improvements, average bandwidth when using jumboframes is around 9Gb/sec (as reported by iperf between 2 VMs over the NIC 'switch') on grindelwald.\nRebasing to upstream/master was almost uneventful, except that i'm no longer able to run the NFV tests (selftest() still works), so coudln't test bandwidth on interlaken.\nThis should be now ready to merge, but it would be nice to see it under NFV tests again...\n. Full rebase and cleanup.\nPerformance is steady at little under 10Gb/sec on grindelwald.  Adding @lukego 's \"don't allocate tx\" patch ([5b5df1928], not included here) gets over 11.5Gb/sec.\n. Finally passes all NFV tests!\nIt was mostly about setting checksum valid/needed flags when the NIC doesn't bother to check.\n. The transmission and reception checksum issues are quite different.\nAbout transmission:\n\nLet's take an example packet that the VM has requested checksum offload for and then Snabb Switch has added encapsulation to. The headers marked with (*) are the ones that don't have valid checksums yet and need to have this fixed before transmission.\nEthernet (outer)\nIPv6 (outer)\nEthernet (inner)\nIPv4 (inner)\nTCP (inner)\nI don't see from the code how this would happen today. The hardware needs to know to checksum the inner headers instead of the outer ones. Or, if hardware can't do that, then it has to be done in software by Snabb Switch.\n\nI'm trying to make the TX checksum handle the inner headers by telling the NIC that there's a 80-byte ethernet header instead of the usual 14-byte one.  The outer IPv6 header doesn't have checksum field, so it can be ignored.  It seems to work already, but i want to test it more before committing.\nAbout reception checksum verifying:\nIt seems that the Linux kernel admits that not every packet can be verified by every hardware checksum engine, so a packet might not have the PACKET_CSUM_VALID flag, but if it has PACKET_NEEDS_CSUM, then the kernel will check it in software.  If neither flag is set, the packet is blindly discarded.\nThere's no way to tell the RX checksum engine how it should check the packet, so I can't play with the offsets to make it verify the inner headers.  The docs say that it won't check any kind of encapsulation, and it seems to happen exactly so: the packets are signaled as \"no checks performed\"\nSo, we could either do our own check, or admit the hardware doesn't help and pass it to the kernel.  The pros of letting the kernel do it:\n- it's already done and debugged!\n- it can manage lots of different cases\n- it's probably been optimized the hell out of it\nthe pros of doing it ourselves:\n- it happens \"outside\" of the VM, on another CPU core.  might be better use of resources.\n- we can focus on a few cases that matter most to us, maybe achieving better optimization than the all-powerful code in the Linux kernel.\nFinally, i think there might be some opportunities to avoid some checks if we could pass some flags from the transmitting app(s) to the receiving end.  It seems there's no space for this in the L2TPv3 header... unless we use one or two of the 'reserved' bits mentioned in the RFC.\n. Finally includes the 'inner packet' checksum offload.\nI've verified that it does update the correct field in the inner packet, and the Linux driver accepts it.  Also, modifying another byte of the packet makes it reject the packet, so I guess the checksum is correct.\nThis offload code doesn't use the .csum_start and .csum_offset fields of the packet struct, because the NIC uses maclen and iplen plus types (IPv4 vs. nothing (IPv6) and TCP/UDP/SCTP/nothing).  Still, I've added the update to these fields in the tunnel app, because the Linux kernel driver might want to use them at some point.\n. > Victory at last :-). This is a very impressive piece of programming! Celebration is in order\nGlad you like it.  I'm always worried when it takes me so many days and the result is so few lines of code!\n\nI have an second idea. (Uh-oh.) How about if we measure the benefit of hardware checksum offload before we commit to it?\n\nWhile testing it I haven't seen any obvious improvement in bandwidth.  I guess we could see less guest CPU usage.  If we can prove that, even a \"same performance as the kernel\" implementation could be beneficial.  Now, if we can use SSE and the kernel can't.... sounds tempting to test at the very least.\nNow that you've found a fast-and-tested checksum function, it should be easy to write a simple app that turns PACKET_NEEDS_CSUM into PACKET_CSUM_VALID by adding checksum at one direction and by checking it at the other.  Other packets should just pass through.\nBut I still don't get how the .csum_start/.csum_offset fields work in the general case.  In our selftest, during iperf I saw the IPv6/TCP packets had 0x36/0x10 on those fields, pointing to the TCP checksum field.  But what about IPv4/TCP?  both headers need a checksum, so which one would those fields point to?\n. The ratelimiter.selftest() passes, still missing some nfv/selftest.sh test.\n. > Could you follow up with an NFV test case too please?\nsure, i was on that last night, but i think chur died on me... any\nnews about that watchdog thing?\n. > If you mean lib.watchdog.watchdog, it should be ready to use.\nNo, I mean the hardware watchdog on supermicro mainboards, to do a hard reset if/when some test crashes the kernel.\n. rebased and working as before.\n. reconfigurations are tricky.  i guess some global parts of the VLAN definition aren't flushed correctly when reconfigured.\nDo you know how to do a git fetch from the specific branch of your fork?  Github only gives me the master URL...\n. Each app instance holds a 'VF' object; and several of them share the same 'PF' instance.  Currently, a PF is released only when the last VF is closed.  I think the current 'reload' app action does a :stop()/:new() for each app (when there's no :reload() method), so there might not be any opportunity to reload the PF.\nMaybe doing a full :reload() of any app should force reloading all those sharing the same PF?  The only two alternatives i see are rewriting the generic app.lua code, or else making the :reload() causes the app to enter a 'waiting for the rest to reload too' state.\n. while the closing app should free all those resources, the immediate issue doesn't seem to be memory-related since just setting the initial VLAN to the same used for the test makes it pass.  IOW, it's the VLAN changing what makes it fail.  Fortunately, most of the VLAN settings are per queue pool, so I'm feedling with the bits hoping to get the reloaded status as similar as possible to a newly setup subdevice.\nNote that it's not just that once the VLAN is set it doesn't change anymore, since making them the same first, and then changing to almost anything makes the test fail too:\n- set 43/43 => pass (original test)\n- set 43/24 => fail (ok too)\n- first set 24/42, then 43/43 => fail (@eugeneia test)\n- first set 43/43, then 43/43 => pass (almost the original)\n- first set 43/43, then 43/48 => fail (would be ok, but...)\n- first set 43/43, then 48/48 => fail (darn!)\n(note: I did manage to fail from empty freelists, by reloading too many times, but that needs around 30 app reloads, so it does mean :close() isn't releasing resources yet)\n. I think there's a misunderstanding here:\n\nHowever, the driver will set the buffer size to 1KB as soon as it receives a small packet (an ARP request for example), because it compares the packet size with the current rx_buffersize and sets it to the packet size if this is smaller (rounded to 1KB resolution, being 1K the smallest possible value allowed).\n\nIt doesn't compare with the size of the packet, it compares with the size of the input buffer offered by upstream code (which might be a VM's Linux kernel).\nThe point is that we are telling the NIC the size of the buffer where it can write, so it can't be bigger than the buffer we are offered.  It's ok to tell the chip a smaller size (it will just split a big packet into more buffers), but if we tell a bigger size, then it will happily overwrite whatever is beyond that in RAM.\n. > @javierguerragiraldez As per luke's request I removed the newly commented-out code (in 86e85b4). Is that sensible?\nNo objection from me. I didn't delete it originally because this whole thing passed through a very long phase of swinging code around and not knowing what made the failure rate rise or fall  :-)\nStill, I think this would change a lot when the straightline branch gets merged...\n. Added real tests and squashed commits.  Should be ready for merge if the snabbot agrees.\n. On Tue, Feb 10, 2015 at 10:55 AM, Luke Gorrie notifications@github.com wrote:\n\nA problematic answer would be: The table size has no fixed limit. Connections are added without bound. Performance of the Snabb Switch process will degrade as the Lua garbage collector is invoked and swap memory is needed. A denial of service attack will cause the Linux \"Out Of Memory Killer\" to be invoked.\n\nsince the connection tracking tables are just Lua tables, the answer\nis roughly on these terms.  since the track specs are very tiny\nstrings it would take a really heavy and specific DoD attack to hit\nswap memory, but it's a real possibility.\nsince Lua tables don't report the number of items stored, the easiest\nsolution would be to keep a counter each time a new connection is\nadded.  it might also complicate a little the expiration process,\nsince it might have to check how many connections are actually being\ndisposed.\nthe \"right\" solution would be to replace the tables with something\nelse.  like our own hash table or bloom filter (probably based on\nSIPhash, which is crypto-safe and on the same order of throughput as\nmurmur). with size limits and expiration built-in.\nactually, a fixed-size hash table isn't so hard to do...\n\nJavier\n. On Tue, Feb 10, 2015 at 11:37 AM, Kristian Larsson\nnotifications@github.com wrote:\n\nWhat ballpark are we talking about for the size of these tables? Thousands? Millions?\n\njust throwing numbers around, i guess tens of thousands shouldn't be a\nproblem.  Millions of tracked connections might or might not;\ndepending on hard-to-guess hash collisions.  LuaJIT's string hash is\nquick and predictable performance-wise, but it's not the best in\navoiding collisions on similar strings.\n\nI suppose that one usually doesn't track the state on incoming connections but rather on outgoing ones and therefore the incoming DDoS will hit a (hopefully) rather small table, correct?\n\nright.  unlike iptables, it's explicit which connections are tracked,\nso you could just track outgoing connections, or even start tracking\nonly established connections.\nfor example: let pass all incoming SYN packets, track outgoing SYN+ACK\npackets, and block incoming packets without SYN that don't belong to a\ntracked connection.\nthat way, a SYNflood wouldn't fill the connection tables, and if any\nsyn-cookie code in the server chooses to drop a connection it won't be\nneedlesly tracked.\n\nIf on the other hand one tracks incoming connections I think this could easily blow up. DDoS attacks are easily in the millions of new states per second...\n\nright.  with high performance networks it's easy to overwhelm any\nlimited resource.  the delicate part here is to provide a\nnon-catastrofic degraded case.\n\nJavier\n. > Correct me if I'm wrong, but the double-buffer method you describe can't guarantee atomicity without using a true atomic test-and-set operation, can it? Still not sure how far we should take this.\nAFAIK, test-and-set are critical to arbitrate between two or more writers.  If the writer/reader roles are well-defined, consistency checks (like Luke's proposal) can be enough.\nIn double-buffer techniques (like the one described in LMDB docs) you only need to guarantee that the 'guard' value (current index or version counter) is modified last, as seen by every party.  I've read somewhere that even if x86 CPUs have heavy instruction reordering, memory writes are guaranteed to be in-order.  But I don't really trust that kind of affirmations without some spec definitions, so it might be better to add some kind of memory barrier.  OTOH, most of the non-flush memory barrier primitives in GCC are NOPs in x86.\n. > Do you see why SnabbBot failed this on the Intel selftest?\nthe manyreconf() test aborts when an iteration fails to advance the counters.  In this case each iteration passed almost 1M each, but # 48 saw only 4873 packets and # 49 not even one, so the test failed. \nThis is typically what happens when some allocated resource taps out and the packet flow grinds to a halt, meaning that i'm still not releasing or reusing everything on each reconfig.\nI think this test run churns packets faster than what i've seen manually, thus maybe in my test i didn't manage to totally leak out and with 'just' 100 iterations of 0.25sec each.\nI'll try to make it happen earlier changing the preallocated amounts, time of iterations, size of packets, etc.  to find exactly what resource is leaking.\n. > if we have no rules ({}) then it seems all packets should be dropped?\nRight, a no-rules filter would drop everything.\nCurrently, the behaviour of a global state_check is to check before any rule, and if it finds a match, the packet is passed.  Therefore, a filter with only a global state_check and no rules means \"pass only known connections\", which I think is a pretty common case.\n. > So does our PF app actually support connection tracking across PF apps?\nYes, that's why I put the tracking in named tables.  Just use the same name on both ingress and egress PF apps.\n. > If vif_details.stateless_pf is a false value, then the returned PacketFilter config will contain state_track=<port.id> and state_check=<port.id> (in both ingress and egress filters). If I understand this correctly that means:\n\n\nIf the port establishes an egress connection to a client, the client's packets on that connection will pass regardless of the port's ingress rules.\nIf a client establishes an ingress connection to the port, the port's packets on that connection will pass regardless of the port's egress rules.\n\nRight? Wrong?\n\nRight.\nIn the 'global' part (not as part of any rule), the state_track=... is applied after the rules and tracks packets that pass any rule, while the state_check=... is applied before the rules, and passes (without further checking) packets that match a tracked connection.\n. I've just drafted a simple checksum app using this.  Some observations:\n- It only processes packets with flags C.PACKET_NEEDS_CSUM set and C.PACKET_CSUM_VALID clear.  Anything else is just passed.\n- Processed packets get the flags reversed to signal that the checksum is valid and no need for further processing.\n- Incoming packets are just verified, and the flags are set accordingly if they pass.  If not, they're dropped.\n- At the point I'm almost sure this is not how these functions should be used.  It seems the packets we'll see have part of the checksum already done (the pseudo header?); but update_checksum() starts by clearing the checksum field.\n- The current test show 2.5Mpps, but it seems to be a limitation of basic_apps.Source and basic_apps.Sink.  A big hint for this is that I get the same number if all packets are passed or all are calculated, and the same results either on grindelwald or interlaken.\n. > I am really warming up to the idea of deleting the checksum offload flags/fields struct packet and doing the whole thing locally in the Virtio-net module.\nit's working like that in my ipchecksum-simd branch!\nperformance on grindelwald seems to be roughly on par with the hardware checksum, around 9.8Gb/sec on jumboframes.\n. > Let's focus on 1500-byte frames for now. That is the more important case.\nstill way slower there: 3.6Gb/sec on grindelwald... it's profiling time!\n. The packet API has changed significantly not long ago (the straightline merge), this line should be changed to packet.free(p).\n. Hi.  Yes, I had seen your point 3, it was really bugging me out, i had do the sum manually lots and lots of times, yet the small packets kept giving non-zero sums.  Even so, the kernel did accept those.\nThe thing resulted in a different interpretation of the initial value between the generic and the SIMD versions.  It works now but it would be much better to agree on either network order or host order.\n. Done!\nThe if len >... test is gone, now all checksum functions have to interpret the initial value as network order.  The lib.checksum.selftest() ensures that the functions are consistent with many random initial values.\n. done!  it rebased cleanly, hope the 'bot likes it :-)\n. This might be a relevant introduction to some issues around NUMA, as seen by Linux: http://events.linuxfoundation.org/sites/events/files/eeus13_shelton.pdf  Written by FusionIO guys, so it focuses a bit on block-devices.\n. > I like the idea of making the freelist into a shared FFI object in some simple way. (I'm not sure the simplest way to create shared FFI state: a C global variable perhaps?) This should work fine.\nYes, a global static C variable could work.  Or it could be injected by the loader, like the links are.\n. Shared freelist, got 1.4MPkt/sec!  (roughly same as baseline)\n. > Looks like the file core/stats.lua is missing at the moment.\nD'oh!\ni'll push it at lunchtime...\n. Great!  if you want to do it yourself, the first things that could be simplified are the core.freelist, which wouldn't need the receive() function, and core.packet would just call freelist.new('core/packet/freelist', 'struct packet *', max_packets) instead of checking if it has the global lightuserdata pointer already injected.\nSimilarly with the core.link objects, the app:inject_links() method could simply pass the registered names of the links and then do for i,name in ipairs(inputlinknames) do input[i]=shm.map(name,'struct link*') end from the 'inside'.\n. This is very nice!   In the sandbox branch I certainly wanted a namespace for shared objects, but didn't made the leap from that concept to the SysV IPC shmem.  I was only thinking on a FFI name->blob+type dictionary.  (And just now noted it's called POSIX shm... am I that old?)\nBut I think this API is a little too \"filesystem-like\".  I'd rather have something like just a \"named ffi.new()\".  Maybe something like this (very untested!!):\n```\nfunction shm_get(name, type)\n   local ctype = ffi.typeof(type)\n   local size = ffi.sizeof(ctype)\nlocal fd, err = S.shm_open(name, 'rdwr')\n   if not fd then error(\"shm_open \" .. name .. \": \" .. tostring(err)) end\nlocal mem, err = S.mmap(nil, size, \"read, write\", \"shared\", fd, 0)\n   if mem == nil then error(\"mmap failed: \" .. tostring(err)) end\nS.shm_unlink(name)\nlocal obj = ffi.cast(ffi.typeof(\"$&\", ctype), mem)\n   return ffi.gc(obj, function(o)\n      if o.__gc then o.__gc(o) end\n      S.munmap(o, size)\n   end)\nend\n```\nIn short, this does an shm_open() (maybe creating), mmap() and shm_unlink().  The unlink won't destroy the object as long as there's an active mmap.  The ffi.gc() call sets the munmap() to happen on GC.  I don't know if there's a way to check if the ctype already has a __gc metamethod, which the ffi.gc() would override.  The o.__gc check would only work in the common case where the metatable is it's own __index (so, not in the ffi.metatype(ctype, {__index=methods}) style, which I personally don't like).\nWith this, when two different parts of the code do an shm_get() with the same name, they would get pointers to the same blob, without having to care which one creates and which 'receives' the object.  When the last instance is GC'ed, the shm object would be destroyed (since all open() are immediately close()d)\nThe most obvious con of this vs. the current proposal is that two different pieces of code could use the same object with different types and hilarity ensues.  Although, I don't see this as a too serious danger; at least not worse than currently with C or FFI structs.\nAbout the current shm lib... could it be reimplemented on top of either of these ideas?  it could be as simple as documenting: \"the frobnulator system keeps status data on a struct { int fribles, unsigned long tribbles} shared object named \"frobble\".  get it with local frobble = shm_get(\"frobble\", \"struct{....}\").\"  Mmmmh... maybe it's worthwhile to add a readonly option?\n. > Particularly, the engine could set shm.path = app.name when switching between apps\non sandboxed apps, each one would have its own copy of the shm namespace, so shm.path could be set at instantiation time and left there.\n. this looks much better than the original lib.ipc.shmem.counter, which I had to fix a bit to make it work on the sandbox branch.  In that case there were 9 closely related counters, so if done now, I'd use a struct to bind them all and share with core.shm, maybe even a two level struct like this:\nstruct {\n    struct {\n        uint64_t packets, bytes, bits;\n    } current, previous, reported;\n}\n. I've been checking what would be needed to split the Intel10g driver like that.  Currently, each 'vf' device object holds a reference to the 'pf' device object, but there's almost no function call through this reference, it's mostly to access some 'common area' registers to do some setup, like MAC address, enabling/disabling queues, etc.\nI think the simplest would be that any setup that needs access to those common registers would be done directly to the 'pf' app, and the 'vf' apps would just accept what is done.  Something like this:\n```\nnic0 args={vmdq='32*4', pools={\n    [1]={mac='52:54:00:01:01:01', vlan=18, rate_limit=1e9},\n    [2]={mac='52:54:00:02:02:02', vlan=18, rss='ipv6'},\n    [3]={mac='52:54:00:10:10:10', vlan=1, mirror_pools={1,2}},\n}\nnic0rx1 args={rxpool=1},\nnic0tx1 args={txpool=1},\nnic0rx2a args={rxpool=2, queue=0},\nnic0rx2b args={rxpool=2, queue=1},\nnic0rx2c args={rxpool=2, queue=2},\nnic0rx2d args={rxpool=2, queue=3},\n......\n```\nThere, the 'vf' apps would only deal with moving packets to/from a single queue ring and don't care about initialization.  seems really doable, and quite simple to parallelize.  it even seems like we could finally get some use for RSS: just set a separate but equal 'app network' for each queue in a single queue pool.  in the example i'm using 4 queues per pool, instead of the more common 64*2 scheme.  the four \"pool 2\" apps would be connected to different instances of, say, a tunnelling/compression app, so the CPU/heavy work would be distributed among 4 cores.\n. well, in the bit definition it only says \"Reserved. Must be set to 0.\" and the initial value is 1b; so it seems this should be done anyway.\nSection 4.6.7 \"Receive initialization\" ends with \"Set bit 16 of the CTRL_EXT register and clear bit 12 of the DCA_RXCTRL[n] register[n].\" as the last steps to initialize the receive path.\nI guess we haven't seen this issue before because it's somewhat hardware dependent, and it's only missing in the non-VMDq setup.\n. Sounds like a very concrete case of eating our own dogfood.  In theory such a setup would be able to support any network function, so it becomes an incentive to write then and make them actually work.\nThe inter-process mesh seems heavy at first, but a full mesh of 32 nodes needs 992 unidirectional links.  assuming we use the current two-256-packet rings, it's just 2_256_sizeof(ptr)*992, less than 4MB in rings.  Each process just manages 31 inputs, 31 outputs and a whatever local configuration (i guess typically just one input and one output more).\n. Cycles are avoided by the split horizon tagging of internal links.  It's a much cheaper (and deterministic) algorithm.  Suitable here because topology is fixed.\n. Regarding performance, I think it will be a challenge but doable.  My first (and so far, only) inter-process tests gave me 3Mpps, but I suspect that it was pushing too few packets per cycle.  I guess that the first hardware bottleneck is the constant cache-line locks and flushes that occur when modifying the ring indexes.  Other microbenchmarks have shown that there's an upper limit of 10~20 Mcycles/sec per shared resource.  If each cycle moves more than 10 packets, even a figure as low as 3 Mcycles/sec would give us the desired throughput.\n. I'm not sure your first diagram is correct...\nthis is how i'm reading the output:\nstream A->B:\n114,293,295 sent on source_1_a.tx -> bridge_stream_1_a.l2 (loss rate: 0%)\n85,720,035 sent on bridge_stream_1_a.stream_1_b -> bridge_stream_1_b.stream_1_a (loss rate: 24%)\n85,720,035 sent on bridge_stream_1_b.l2 -> sink_1_b.rx (loss rate: 0%)\nstream B->A:\n114,293,295 sent on source_1_b.tx -> bridge_stream_1_b.l2 (loss rate: 0%)\n114,293,295 sent on bridge_stream_1_b.stream_1_a -> bridge_stream_1_a.stream_1_b (loss rate: 0%)\n114,293,295 sent on bridge_stream_1_a.l2 -> sink_1_a.rx (loss rate: 0%)\n. ah, ok.\ni don't get the second one... i'm assuming the mac addresses are like:\n1A: 01:00:00:00:00:01\n1B: 01:00:00:00:00:02\n2A: 02:00:00:00:00:01\n2B: 02:00:00:00:00:02\nif so, then this is how i read the output:\nstream 1A->1B>\n30,771,360 sent on source_1_a.tx -> bridge_stream_1_a.l2 (loss rate: 0%)\n      30,771,360 sent on bridge_stream_1_a.stream_1_b -> bridge_stream_1_b.stream_1_a (loss rate: 0%)\n      30,771,360 sent on bridge_stream_1_b.l2 -> sink_1_b.rx (loss rate: 50%)\nthe 50% loss means that the 1B bridge was getting as many \"stray\" packets as expected ones.  most of them seem to come from the 2B bridge.\nstream 1B->1A:\n30,771,360 sent on source_1_b.tx -> bridge_stream_1_b.l2 (loss rate: 0%)\n      30,771,360 sent on bridge_stream_1_b.stream_1_a -> bridge_stream_1_a.stream_1_b (loss rate: 0%)\n      46,157,040 sent on bridge_stream_1_a.l2 -> sink_1_a.rx (loss rate: 25%)\n25% loss while delivering more packets than it supposedly receives... again, bridge 2B seem to be the source.\nstream 2A->2B:\n30,771,360 sent on source_2_a.tx -> bridge_stream_2_a.l2 (loss rate: 0%)\n      30,771,360 sent on bridge_stream_2_a.stream_2_b -> bridge_stream_2_b.stream_2_a (loss rate: 0%)\n      61,541,955 sent on bridge_stream_2_b.l2 -> sink_2_b.rx (loss rate: 20%)\nthis one delivers a bigger fraction, the extra packets seem to be 30M from bridge 1B and 15M from 1A.  a funny point, those 15M from 1A to 2B register a 49% loss, so it was supposed to be 30M\nstream 2B->2A:\n30,771,360 sent on source_2_b.tx -> bridge_stream_2_b.l2 (loss rate: 0%)\n      30,771,360 sent on bridge_stream_2_b.stream_2_a -> bridge_stream_2_a.stream_2_b (loss rate: 0%)\n      61,542,465 sent on bridge_stream_2_a.l2 -> sink_2_a.rx (loss rate: 33%)\nnow the extra packets are from bridge 1A and 1B, 30M each.\n\"stray packets\"\n30,771,360 sent on bridge_stream_1_a.stream_2_a -> bridge_stream_2_a.stream_1_a (loss rate: 0%)\n      15,386,190 sent on bridge_stream_1_a.stream_2_b -> bridge_stream_2_b.stream_1_a (loss rate: 49%)\n      30,771,360 sent on bridge_stream_1_b.stream_2_a -> bridge_stream_2_a.stream_1_b (loss rate: 0%)\n      30,771,360 sent on bridge_stream_1_b.stream_2_b -> bridge_stream_2_b.stream_1_b (loss rate: 0%)\n             255 sent on bridge_stream_2_a.stream_1_a -> bridge_stream_1_a.stream_2_a (loss rate: 0%)\n             255 sent on bridge_stream_2_a.stream_1_b -> bridge_stream_1_b.stream_2_a (loss rate: 0%)\n      30,771,360 sent on bridge_stream_2_b.stream_1_a -> bridge_stream_1_a.stream_2_b (loss rate: 0%)\n      30,771,360 sent on bridge_stream_2_b.stream_1_b -> bridge_stream_1_b.stream_2_b (loss rate: 0%)\nit seems that the bridge 2A is the only one that send most of the packets it get from source 2A to the intended destination (bridge 2B), and only 255 packets to each of the other two (1A and 1B).  the other three bridges receive 30M packets locally and send (or try to send) all 30M to the other three bridges.  it does sound like either a misconfiguration, or maybe a small bug that results in losing most of the split horizon tags.\n. sorry, small brain glitch: it seems the issue is in the MAC address learning or table applying, not split horizon\n. on a third sight, if you sum all the packets that each bridge gets from the other bridges, it corresponds to the number of packets it tries to send to its local sink.\nfor example, bridge 1A receives 30M from bridge 1B, 255 from 2A and 30M from 2B, that's 60M.  it delivers 46M after 25% loss, that is it tries to send 60M to sink 1A.\nso, it seems that there was 30M packets from source 2B intended for sink 1A.\n. well, calling counter.add() is known to be significantly slower than either doing the 'local function cache' dance: local c_add=counter.add at the top, or adding FFI metatype methods and just call engine.frees:add(), engine.freebytes:add(p.length).\n. What about using a single shm object with all the counters in it?\n. > Returning packet buffers to the freelists of the processes that need to allocate them: the link struct is extended so that free packets can be chained backwards to where they came from. This part I don't understand yet and have to read more closely.\nI think this is a crucial point and there's quite some room for improvement, in all three different forms:\n1. cleaning/optimizing the current implementation (removing some \"just in case\" code)\n2. tightening the theory (i guess the head/mid/tail rings can be replaced with 'normal' head/tail kind; maybe some invariant analysis might even allow folding the two rings into one, with full packets from head to tail and empty \"payback\" packets from tail to head)\n3. using normal rings and replenish freelists some other way.\nI would really like to explore option 2, in the best case it might even converge with current links, just don't pay back packets when used intra-process.\n. > One part that seems to be missing is incremental updates of the app network.\nYes it's totally missing.  In the current form, child forks simply call engine.main() until the master finishes.  But since each child has the complete code available, it shouldn't be a problem to update their sub-networks too... if we get the config to them.  Three obvious options:\n1. kill everything and re-fork.  drastic and maybe slow.  (hundreds of miliseconds? maybe a couple secs? i guess it depends on app initialization code)\n2. serialize the config and send (or share) to the childs, with a signal so everybody calls engine.configure().  should each child resume working as soon as reconfigured? or should everybody pause and restart only after everything is consistent again?\n3. formalize the \"config reading\" method and let every child discover the change in the config source and reupdate iself.  Any real world setup must get the config from somewhere and then feed to the core.config object (config source -> Lua code -> core.config -> core.engine).  we could say: \"write your configs to a JSON file (or yaml, or Lua, or csv) and switch this flag, the engine will reread and reconfigure\", each child would do the same and change its own small part.  (and the master would spawn/kill processes if needed)\nA related point is that the current engine_state \"metaglobal\" flag is checked on a big, wide loop, where engine.main() runs for a second between checks.  If we keep this flag, it could easily be checked on every breath cycle.\nOf course I'd like to kill this flag and use some kind of signals, but couldn't really understand how does ljsyscall manages them.  sigsuspend() and SIGSTOP/SIGCONT are really tempting, but I didn't manage to setup signal handlers, so only got killed forks.  :-(\n. > How about if we switch the model to allowing processes to be started separately? We could still fork() the processes when creating an app network but this would be a convenience and not a requirement: you could also run genuinely separate app networks and still connect them with cross-process links.\nhum.... i think we still need some way to associate all the intervening process into a group.  Maybe the easiest would be to specify a 'home' directory in the command line; then all the shm objects are relative to that dir.  Another nice thing is getting rid of the S.getpgid() i'm using for several global objects; simply use constant, \"home-relative\" filenames.\n\n\nA way to privately allocated DMA memory and then map it into new processes on-demand as they receive packets. (Hint: catch SIGSEGV and mmap() the mapping.)\n\n\nI like this.  how about a subdir with all the DMA mappings, and from time to time (on SISEGV?, SIGHUP?, inotify on the dir?) mmap() everything there.  would this need a \"map directory\"? or just good, descriptive names?\n\n\nA way to pass packets between different processes over ordinary links and still recover them to the right freelist -- even when processes crash and restart. (Hint: Garbage collect packets.)\n\n\nhum... if each process \"owns\" a subset of those DMA chunks, it could be easy to determine where each packet should return.  what i don't like is to allow any process to update the free list, it should be private... maybe just marking the packet as available, so the originating process can GC it.  i think it would need some careful tuning to avoid it becoming a big drain, but reverting to the basic link structs is tempting.\n. Maybe superfluous if the packet GC idea pays out, but I really wanted to simplify the packet-swapping inter-process link.  Now it's 130 lines less!  :-)\n. > Suppose that transmit() on an interlink always frees exactly one packet.\nI had a related idea: keep the whole ring full, from head to tail with data packets, from tail to head with payback packets.  To transmit(), first free a packet to make space; on receive() fill back the spot with an empty packet.\nWhat I don't like about it is that it keeps a number of packets tied up.\nAlso, this last change doesn't improve (or degrade) performance a bit.  I guess that means the loop is being throttled by something else...\n. .... and done.  no free index, no harvesting on :full().\n. now the interlink struct is usable to link apps in the same process, replacing the old link (it's almost identical now).  A simple source->sink in a child process runs at 35-38Mpps, so it seems the overhead is low enough.\nInter process links still run at 3-7Mpps, and when there's both kinds of link, the local link gets only 200-400Kpps, around than 1% of the local only case.\nin this sample one second run, source and 'localsink reside in process 1, and sink in process 2.  The inter process link gets 6.7Mpps, while the intraprocess is only 390Kpps\n6,774,414 sent on source.output -> sink.input (loss rate: 0%)\n         390,915 sent on source2.output -> localsink.input (loss rate: 0%)\nThe profiler always reports 100% compiled, and the numbers are only slightly affected by commenting out the pace_breathing() call to avoid any possibility of sleeping in the loop.  Changing initialization order, so the local link appears first in the list of output links of source, or using two separate Source apps (in the same process) doesn't change situation either.\nIncreasing the ringsize made it even worse.  This is with 1024 packet rings:\n6,615,785 sent on source.output -> sink.input (loss rate: 0%)\n          72,633 sent on source.output2 -> localsink.input (loss rate: 0%)\nThe low inter-process performance, and the wide variation range (3.5-7 Mpps) are starting to worry me, as I don't see much opportunity to optimize.  And the way intra-process links get penalized really confuses me.  It looks like an algorithm failure, not just code overhead.\n. Another confusing test: adding a third link.\ntwo inter-process and one intra-process link: the sum of the two inter-process remains slightly more than 3Mpps.\nSometimes it's roughly balanced:\n1,765,518 sent on source.output -> sink.input (loss rate: 0%)\n         129,285 sent on source.output2 -> localsink.input (loss rate: 0%)\n       2,199,103 sent on source2.output -> sink.input (loss rate: 0%)\nSometimes it's not:\n2,434,132 sent on source.output -> sink.input (loss rate: 0%)\n         148,410 sent on source.output2 -> localsink.input (loss rate: 0%)\n         954,612 sent on source2.output -> sink.input (loss rate: 0%)\nOne inter-process and two intra-process links: the sum of the two intra-process is limited to a around 150Kpps: (and both local links get exactly the same number of transfers)\n3,473,584 sent on source.output -> sink.input (loss rate: 0%)\n          87,465 sent on source.output2 -> localsink.input (loss rate: 0%)\n          87,465 sent on source2.output -> localsink.input (loss rate: 0%)\nIt really looks like there's some funny bug blocking local links, but only if there's at least one inter-process; with two intra-process, the total transfer is over 50Mpps, there's very little variation, and both links seem to be in lockstep:\n27,872,775 sent on source.output2 -> localsink.input (loss rate: 0%)\n      27,872,775 sent on source2.output -> localsink.input (loss rate: 0%)\n. It's just a simple configuration in src/apps/basic/test_fork.lua selftest(), so I just do snabbswitch snsh -t apps.basic.test_fork.\n. I've got a vague feeling that a test so tight (just Source and Sink, no processing at either side), the whole system becomes just a test of trashing one or two cache lines between two cores.  The profiler reports a significant part of the time in just setting the .has_new_data flag; that sounds like most writes become inter-process sensitive.\nI'll try to double-buffer the inter-link struct, to allow the transmitting app to quickly push into a non-shared buffer, and then commit into the main ring as blindly as possible.\nThe first step, of course is to use :nwritable() instead of constantly testing not l:full().  It's encouraging that this simple change is the first one to really affect performance.  Although in some counter-intuitive way... it doesn't make the inter-process link any faster, just reduces the impact on the local link.\n. hum... separating the read and write indexes by putting them at either side of the stats struct, or even of the packet ring didn't make any noticeable difference.  Neither did a simple private front buffer.\nWhat did make a small difference was only storing the link in the core.app table/array at the process that holds the receiving app.  It made the breathe loop slightly simpler, avoiding a few checks in the link struct.\n. This last patch includes some different small fixes.  Those that seem to help but might not be worth it:\n- Each link has two pointer arrays, one for full packets and one for the empty \"payback\" packets.  It seems to slightly reduce cache thrashing.\n- Allow the config.cpu() to specify the engine.busywait flag for each process.  Useful because in tests it's common that a fork doesn't include any \"receiving\" app, so the main loop doesn't see any work and starts to sleep.\n- Don't check for full link on each :transmit(), assume that the sending app has already called :full() or :nwritable().  Sending a packet on a full link becomes an error.  Just like calling :receive() on an empty link corrupts it.\nBut what seems to be today's key improvement is the front buffer: a private packet ring that holds packets until it gets full, then pushes to the real link as fast as possible.  In fact it doesn't seem to be much faster than calling :transmit(); the advantage is that it avoids small cycles, getting more packets across for each inter-process cache flushing.\nOther ideas to get the same effect include:\n- do it implicitly in the sending app. something like:\nLua\nlocal nw = l:nwritable()\nif nw >= xx then for i=1, nw do\n    l:transmit(....)\nend end\ncon: this would have to be done for any app that might be used as \"the last app before an inter-process link\".  Simply preferring this style for all apps doesn't sound right, much less coder-friendly.\n- write a \"buffer\" app that does the same thing and have the \"network implementer\" add to the specs.\nIf this approach proves to be the way to go, then I think it could be transparently added to inter-process links, either by the engine configuration code or by the link itself.\nAlternatively, thinking in the \"don't fork, let separate executions connect themselves\" scheme, I guess that we could replace the interprocess links by transmit/receive apps, paired by a common name.  something like this:\nin config 1:\nLua\n    config.app(c, 'trans1', transmit, {linkname='link_1'})\n    config.link(c, 'someapp.output -> trans1.input')\nin config 2:\nLua\n    config.app(c, 'recv1', receive, {linkname='link_1'})\n    config.link(c, 'recv1.output -> getthis.input')\nWhere transmit and receive both shm.map() the packet ring in the 'link_1' file and implement the write and read of the ring, buffering packets if necessary.\n. some simple tests with the simple inter-proc app and returning to 'normal' links seem to be stable around 8Mpps on grindelwald.  Once I clean it up, it would make the minimum PR much smaller, especially if I remove the whole fork idea in favor of the multiple executions.  I still haven't been able to make that work cleanly because the biggest modifications are in memory.lua, not my favorite part of SnS...\n. This last change (the ability to pin a process to a set of CPU cores) finally makes performance stable!\ntesting on grindelwald, which is 2x12cores @2.7GHz\n| Source -> Transmit / Receive -> Sink | packet count on 1 sec |\n| --- | --- |\n| No CPU pinning | 4-8Mpps, sometimes 10... |\n| both to the same core | 10-11Kpps (ouch!) |\n| different cores, same chip | 11-12Mpps (best!) |\n| different chips | 5.7Mpps |\nwould be interesting to check on two sides of a hiperthreading core, I guess it would be slower than two separate cores, but better than same core.\n. > would the memory situation be simplified if I implement #571?\nYes, i've tried to do it myself, but no luck so far.  I think it must be straightforward, but the exact enchantments on hugetables are evading me (and that's before trying to catch SIGSEGV).\n. Lot's of cleanup to do (mostly removing fork.lua, all the pre/postfork() hooks and the \"per-fork\" configurations), but I got 15Mpps between two different SnS instances :smile:.  Happy to confirm that the packet size doesn't affect this figure, it doesn't matter if it's 60 or 1500 bytes per packet.\nTo reproduce, start a packet-producer (just a basic_apps.Source -> inter_proc.Transmit('spoon_link') setup).  It will sit there until killed:\nsnabbswitch snsh program/spoon/produce.lua\nThen on a different terminal, start the packet consumer (a simple inter_proc.Receive('spoon_link') -> basic_apps.Sink).  It will pull packets for a second and then report:\nsnabbswitch snsh program/spoon/consume.lua\n. I think this looks a little less hacky: use a simple shared struct (via core.shm) to store the DMA hugepage ids.  In the C code it looks like a simple int array; no need to arbitrate, because the addresses (and therefore the array indexes) are unique.\nThe included program/spoon/director.sh is an idea of how to run several Snabb processes: start everything in background and wait for any of them to die.  I guess it could check if it should be restarted; but if not, then kill everything else.\nThe cleanup function removes 'global' objects: any inter-process link and the DMA ids, previously unmapping them.\n. With 6 inter-process links crossing 12 processes, each on a different core, all on the same chip... the per-link throughput falls slightly, from 12-1.3Mpps to 11.5-11.7Mpps.  In all, this test clocks around 70Mpps of cumulative throughput.  (and over 100Mpps when doing 9 links on 18 cores, enough to saturate a 100GbE link if the packet averages 125 bytes or more)\n. superceded by #601\n. I've done some tests on the current next branch (which at the moment is the same as the just released 'mango') on Davos 89:00.0/.1 card:\n- snabb snsh -t apps.intel.intel_app passes without modification, but that's because the first tests are loopback, and take some time (100 restarts, 100 reconfigs...)  when it tries to send packets, the card seems to be already warmed up.\n- commenting out the three first tests makes the fourth one (sq_sq()) fail on 89:00 but it still passes on the SFP+ cards (i.e. 86:00)\n- simply adding a 2 sec delay between engine.configure(c) and engine.main({...}) makes it pass again.  1.5 sec seems not to be enough, 2.5 doesn't make it better.\n. > sq_sq runs successfully but mq_sq fails. It seems mq_sq needs a 2 second pause too:\nright.  it's because some of these tests not only reconfigure the app network but also reset the NIC.\n. I think full() is wrong when read >= size-1.  this should work:\nfunction full (r)\n    return r.write == r.read + size - 1\nend\nBut in all, is there a measurable difference?  I remember doing some benchmarks and finding no difference between\nwhile not empty(in) and not full(out) do ... end\nand\nfor _ = 1, min(nreadable(in),nwritable(out)) do ... end\neven if the first form does a check on every iteration, while the second one does only at the start and it's an integer count from there on.  I guess the index pointers become essentially free once they get cached in.\n. I think it is a byte-order issue; commit 2deca0b made all checksum functions to explicitly work only in host order, but C.pseudo_header_initial() returns the 'initial checksum' in network order.\nIt seems the missing ntohs() is in checksum.lua line 70, like this:\nreturn ipsum(buf+headersize, len-headersize, ntohs(initial)) == 0\n. great!  it's much easier to hack around if there's a working PoC.\nI'll go hacking with this tonight, next stop: remove fork()s!\n. i've been cleaning those in my tests \"manually\" with a grep line to only ipcsrm -m those with zero active references (the last column in ipcs).  I've also done a slightly longer bash script to use the ids written to /tmp/0x*0000 files.\nin the end, i think any multi-process gathering of SnabbSwitch instances would be managed by  a 'director' that includes some cleanup options.\n. Nice way to get big, impressive numbers :smile:.\nOne thing that does stand out is that there's not a single if..then in the whole Lua file!  Of course, as the comment says, this code assumes that links never overflow nor underflow.\nAs comparison, the current SnS can get over 50Mpps when doing a simple Source/Sink network.  Of course, that's much less steps than this demo, but it includes a fully working link, a sizeable freelist and copying of a 120-byte packet (yes, the same one over and over, so it's probably in cache all the time).\nI don't see the 'original' code that's being compared to the asm and lua-asm, but I guess it does two packet clones (one at the source and one at the tee), three link transmissions (source->tee and 2*tee->sink) and two allocations/frees (the original and the copy) for each source->tee packet.  That is, slightly more than twice the steps, which correlates neatly with the 27Mpps you mention.\n. As a barely-related side-thought: one thing I'd like to do in the near future is the split-app driver we've talked some time ago: one app to configure the NIC, and one or more extra apps pushing packets in and out of the chip's rings.\nThis would allow different CPU cores to handle different packet streams, not only for VMDq, but also for RSS.  The nice thing is that it wouldn't require any communication between those processes, removing many obvious scalability limits.  100G could be doable with current JIT schemes.\n. Fascinating read!  I'd like to be even half as fluent in the JIT tracings...\nAbout the sandboxing style; yes, in one of the first proposals, you specified a file path to the config.app() call, and it instantiated the code with load() instead of require().  I think i still memoized the resulting chunk, but if it's better to compile it for each instance, then it's the easiest way to do.\nIn the end it was left behind because the multi-[state/thread/process] schemes didn't really needed any different style, but could be easily resurrected if it's desired.  I recall that the code style change for the apps was well received, but it was non-trivial changed for some of them.\n. what are the cons of not applying the patch?  i guess depending on the VM guest, it could transmit garbage packets, or resend old packets.\nwith the patch, the con seems to be just losing packets in the event of Snabb restarting.\ngarbage packets are ugly, and things could crash if an uninitialized ring element has bad pointers.  resending old data is also bad news on the networks, maybe it could cause connections to be dropped.\nlosing packets when some critical piece of the network restarts seem a lot more manageable.\n. > To me the main challenge is when a different process requests a queue from a VF. It seems that some sort of interprocess communication will be needed (a master process initializes all the VFs and a slave process requests a TX/RX queueu from a VF, or much simpler, a VF).\nI'm not sure that IPC is absolutely needed.... the 'second' process could just open the device and grab a couple of pools, blindly hoping that the main NIC initialization is already done.\n. this is where my memory gets fuzzy; but:\n- yes, I think just sharing could work.  It sounds like making that fail would need some specific blocking in the kernel.\n- another option would be to set the rings to point not to the 'usual' dma-capable allocation, but to some other mmap()ed and named file, so the second process would mmap() this file and find it already associated with the ring.  I vaguely remember that there was an option to hook the head/tail registers to a specific memory address, so the second process wouldn't have to access the device registers.\n. i thought this was re-adding it!  my first checkout didn't pull LuaJIT; when i got it working, the diff showed this.  probably it would be better to add deps/ to .gitignore to prevent my fix from mangling the commits.\n. right.  this was the easiest check i found before writing the code.  should change to something internal.\n. it's supposed to be set to something during configuration.  either by set_use_physical_memory(), or anything else from another layer.\n. right.  i tested it only by manually changing the line above.  could be part of a config function for the module (like set_use_physical_memory(), mentioned above), or an extra module.\n. is there any criteria for telling which files use 3 and which 4 spaces?  i realized a little late that my editor had reindented it, for changes on other 3-space files i did manage to revert reindentations and change to 3 spaces on file-by-file basis.\n. ok\n. ok, i'll put a little more love here.  this is the kind of enhancements that should be backed by some failing tests...\n. heh, right... just to clarify a little: \n1. names like _foo() are private.  typically locals... Lua doesn't care about that convention, but i've used it at some places to make it evident that these aren't functions that can be called somewhere else.  In this specific snippet there are several local functions that are created just for the tests.  if they're too ugly i'm not against removing the leading _.\n2.  __setup() is a function that runs before each other entry on the same group.\n3. CamelCase vs. snake_case? again, no real preferences.  sf_to_vf seems shorter than SingleFunctionToVirtualFunction.  Not sure which looks better in the report...\n4. tables in the form xxx_mt are usually metatables, but i'm not sure i've used that too frequently here...\n. according to http://www.lua.org/manual/5.1/manual.html#2.1  only those with uppercase after the _, like _VERSION\n. sure, i do have that, but wasn't sure which PR would go first.  now that other is merged, i'll do that as part of the rebasing.\nrelated question: since the test framework runs several tests by definition, should it return a \"skipped\" exit code when some tests passed and some were skipped? \n. of course, the display for the user is explicit for each test; but the exit code for the CI server is only one integer.  i guess that if a single test fails, i should return the 'fail' exit code.  but if there's 0 failures, X skips and Y passes?  should the exit code be \"pass\", or \"skipped\"?\n. i run them mostly from the cli:\n.snabbswitch test <testfile|dir>...\nthat works simply because main() executes test as a module.  specifying . as the dir runs all testfiles it founds.\nrecently I also made it easier to run from Lua, just passing the name of a testfile or a directory. as an example the intel10g_app.selftest() function finishes with require (\"test\")('src/apps/intel/intel10g_t')  (https://github.com/SnabbCo/snabbswitch/pull/150/files#diff-5498327d017e5e286ca09140b566c1d3R118)\n. you're right, this is mostly an extra and not a replacement (yet).  in part because it started as a proposal, but got very usable.  now that most of the other parts are merged, i'll do a rebase and rework most of this to better integrate with the rest.\n1. I can replace all selftest() functions, no problem.   currently both print() and io.write() pass through, and should be avoided in test functions; if some feedback is desired, the framework stores any return values and optionally displays at the end, even if the test passed.  I could add some patching to print(), or better expose a test.log().  probably should also change the report functions (of links, apps, etc.) to return strings instead of directly printing... or conversely, that's a good argument in favor of patching print()\n2. i think it's easy to add shebangs to the testfiles, and modify make test to just execute them.  Still, while developing it's much nice to run big subsets of tests, so i'd like to keep that option.  it's the same option that allows the code to declaratively specify subtests either as a table or as a directory.\n. > This is redirected to a file on the Unix shell level. I suppose in Lua there is also a stdout variable that can be modified to redirect print() etc?\nyou can change io.output() and that makes io.write(...) to go another file, but that doesn't affect print().  of course, that's easy to patch: print=function(...) io.write(table.concat({...},'\\t'),'\\n') end.  but if i'm doing that, i'd prefer to make print() go to the test log and not to another file.\n. i guess there should be a ... % num_descriptors here\n. do you mean the spec object?  it's supposed to be very short lived, just to create the table key. Ideally LuaJIT optimizations should eliminate it as an 'object', but i don't know how the escape analysis copes with the ffi.string() call.\n. It's the whole idea of the spec table.  I guess interning a size-limited (14 bytes for IPv4, 38 for IPv6) string is the fastest way in Lua to hash a bunch of bytes.  Handling a million strings shouldn't be a problem, but a million per second... we'll have to test and see.\n. yes, i was sure there was a standard function, but this was faster to type than looking for it... of course this isn't used in normal operation, just for report/debugging\n. didn't know about that!  yes, once per breathe is more than enough resolution for this.\n. yes, this needs documenting.  changed quite a few times while setting in a readable syntax for consumers of this module.  In short, the module doesn't return a table, but a function. The parameter is the name of the conntrack table you're interested in, and it returns the table of closures to handle this specific table.\n. just removing this line would get the behaviour you describe; at app init the buffersize is set at 16KB.\n. I think the easiest would be some kind of double-buffering. Either with an atomic value to indicate which copy is the current one, or by having a monotonically increasing counter on the struct itself and just using the largest one.\nIt might be necessary some reordering barrier to make sure the critical atomic flag (the current index or the generation counter) is seen as updated last by every party.  Does anybody know what kind of non-reordering guarantees the x86 model provides?  I don't think CAS instructions apply too well on mmap()'ed blocks.\n. About the volatile keword, it's only there to be consistent with the return value of map_pci_resource().  I agree that it's superfluous and would be better removed.  Even in C it's dubious that would be any benefit as an argument or return value.\nAbout barriers in initialization code, most of the places where order is important, the Intel docs advise to insert a wait, sometimes a few microseconds, sometimes up to a millisecond.  In those cases we use C.usleep(), which should have the barrier effect as well.  (and it's hard to imagine some reordering/pending write to last as long as a microsecond).\nWhere we could be missing some defensive coding is more in the data handling itself.  Maybe the M_sf:sync_transmit () race is related to this?\nFinally, I don't think the initialization code is ever compiled.  Not even in the manyreconf() calls it tightly enough.  What would be a simple way to check that?\n. right, i'm still cleaning it up.  during the after-straightline rebase there were some conflicts that confused me, so i commented out most to later compare with the previous history.\n. More than once I've found the iptables connection tracking behaviour to be limiting.  In short, it tracks all packets on a single table and then you can write rules that check a packet.  In this implementation, both the tracking and checking can be used on a rule or globally, and also to use separate, named tables.  This allows the user to only track packets that pass a certain rule, or to discriminate different connections in different tracking tables.\nIf we wanted to only emulate iptables, we could use only the 'inside rule' state_check, and an implicit global state_track, both storing in a single table.\n. > Can we skip this change or make it in another way that does not change the high-level behavior of the traffic process?\nwhat I wanted to avoid was the forcefully killing of the process, so that the profiling output could be seen.  maybe it would be better to catch SIGTERM and do an ordered exit instead of just dying\n. > I guess the proper way to clear the array is to traverse it and set all elements to nil.\nIn plain Lua it's the only way, but LuaJIT (2.1) includes table.clear(t) that does exactly that.  Of course, it has a lot of warnings that it's only for very specific cases, usually it's better to let the GC do its magic, etc.\n. Is this for the benefit of C files?  In Lua code, if you require('core.counter') you have this, and can use struct counter in any ffi.typeof() or ffi.cdef[[]], no matter if this struct is defined on its own file or inline inside core/counter.lua.\n. this is redundant, since core.counter already defines the struct counter.  In fact, the require() function won't do anything because package.loaded.counter_h is already non-nil.\n. any reason for this two-line for style?  i haven't seen it anywhere before.\n. don't think so... in link.lua the require('core.link_h') is after the require('core.counter'), so it should work without pulling this specifically.\nIn fact, unless a struct or const is used in a C function, I see no value in using .h files.  Maybe somebody dislike the ffi.cdef[[...]] embedded definitions?\nThe only C code I've written since we pulled ljsyscall was for the pthreads experiment, but that was very short-lived.  The current fork()based doesn't need a single .c file.\n. Agreed, this is fit for a style branch.\n. I'd like to know what does it depend on... for a while I dabbled on giving it an extra offset, since it seems to pick a semi-random point and then grow downwards.  But then I realized that any unused page-size segment of this array wouldn't get paged in, so by giving it an outrageous size we only waste hard disk space, not RAM.  So, it wouldn't be crazy to just set it at 8*1024*1024 to cover the whole segment.\n. The memory.cleanup() marks for deletion all the hugepages registered in the map_ids structure, but that only means they will be deleted when the last process to use a given hugepage unmaps it (or finishes).  While a hugepage is used by a process, it can be used by another process.  I tried to make it fail by marking at several times, but it always worked.\nI'm even tempted to restore the shmctl() call just after allocation, as it was before, making this cleanup() function unnecessary.  I guess it would only fail if:\n- process A allocates some packets from its pool\n- those packets are transferred to process B\n- process A finishes\n- since no other process has mapped A pool, it's destroyed\n- process B receives the packet pointers and tries to map it.\nIt not only seems very unlikely, but also useless.... can anybody come up with a different scenario that could fail because of marking pages as soon as allocated?\n. yes, it's a bad joke (not a fork and piles nicely, matching convex with concave).  i plead the second hard thing.\n. Yes, this is unrelated.  It's the kind of things that have been in my fork for a long time and I found indispensable.  sometimes I forget to backport to master style before PR'ing... or maybe I'm hopeful that it would finally be merged before this one.\nThe :dump() method was there mostly for debugging.  should be removed from the PR.\n. I forgot that was there... no, there's no new behavior; just a kata of porting generic apps to OOP style.  (and making it more readable, I guess)\n. AFAICT, it's the protocol for asking checksum offloading, it does the minimum (and mostly constant time) and leaves the bulk of the task to the virtio 'device'.  I guess you're right that a 'switch-like' level 2 device shouldn't care about payload; but equivalent code is in the virtio-net kernel driver, so I ported it here.  Of course, Linux networking isn't the paragon of well-layered structure...\n. This is an adapter to be able to read/write to a non-mmap()'able PCI resource, especially since it requires that the operations are in the exact size of the field.  I don't know if there are other similar cases.\n. the virtio specs allow for multipart packets, but the added complexity is significant (much worse than what hardware NICs do, IMHO).  Currently this line doesn't stand out in profiler runs, the queue notification alone (a single 16-bit IO) takes 90%\n. on a pure polling driver, there seems to be no need for :sync_receive(); it's still there mostly as a placeholder for some notification suppression schemes that might help significantly with performance.  so far, none have yielded any consistent advantage, unfortunately.\n. the header is used mostly to reassemble multipart packets and a few other flags for other kinds of devices (like virtio-block)\n. mostly, yes; but 1500 bytes don't make much difference either.  of course, all benchmarks must be repeated for any significant code change.\n. ",
    "blitz": "x86 is fully coherent with respect to DMA, i.e. PCI devices DMA'ing into memory behave just like the processor writing to memory. So there is no problem there. The memory barriers on x86 are mainly for memory consistency reasons. The compiler as well as the processor may reorder loads and writes for efficiency. The effect for the programmer is almost identically. Memory barriers that compile to \"nothing\" are not pointless, as they force the compiler to adhere to program order with respects to memory accesses. Instructions like \"mfence\" on x86 force the processor to adhere to program order for memory accesses.\nI highly recommend watching this talk by one of the guys in the C++ 11 committee:\nhttp://herbsutter.com/2013/02/11/atomic-weapons-the-c-memory-model-and-modern-hardware/\nHe explains these topics exceptionally well.\n. :+1: \n. ",
    "jhonan": "Ohoh. I didn't expect my small patch to be set into markdown.\nOnly trying to be helpful...\nJamie\n. ",
    "PrimeEuler": "Any more work done on a 32bit compile? I'd love to get this switch operational on a beaglebone or rasberry pi running ubuntu.\n. ",
    "shortweekend": "Debian 7.4 amd64:\nAdded \"deb http://http.debian.net/debian wheezy-backports main contrib\" to /etc/apt/sources.lst\napt-get update\napt-get -t wheezy-backports install linux-libc-dev\nThis package contains the Linux support headers for userspace development, i.e., /usr/include/linux/vfio.h\nWith the above package installed, snabbswitch compiles under Debian 7.\n. Debian 7.4 amd64:\nAdded \"deb http://http.debian.net/debian wheezy-backports main contrib\" to /etc/apt/sources.lst\napt-get update\napt-get -t wheezy-backports install linux-libc-dev\nThis package contains the Linux support headers for userspace development, i.e., /usr/include/linux/vfio.h\nWith the above package installed, snabbswitch compiles under Debian 7.\n. Thank you for the quick reply.\nIndeed VMware Fusion has VMnet devices such as the Bridged network adapter known as vmnet0, the Host-only network adapter known as vmnet1, and the NAT network adapter known as vmnet8. One can also manually add virtual adapters with the 'vmnet-cfgcli' command. However I was not able to find any data sheets about  these devices. How would one access or list them from within snabbswitch?\nIf I were to follow the easier vhost path, how can I run a basic test to check its availability from within my compiled snabbswitch firmware?\nThanks\n. You've described some interesting tracks to explore. I'll give them a try and report about my experimentations and wandering.\nI'm also interested in understanding where one can start with a vmxnet driver implementation but I cannot promise much dedication ;)\n. Sorry for the mess. I clicked close by accident :)\nI'm getting an error with the vhost_apps selftest at first but it works on the second trial:\nsudo src/snabbswitch -t apps.vhost.vhost_apps\ncore/memory.lua:42: Couldn't allocate a chunk of ram\nstack traceback:\n    core/main.lua:104: in function \n    [C]: in function 'assert'\n    core/memory.lua:42: in function 'allocate_next_chunk'\n    core/memory.lua:32: in function 'dma_alloc'\n    core/buffer.lua:32: in function 'new_buffer'\n    core/buffer.lua:50: in function 'preallocate'\n    apps/vhost/vhost_apps.lua:47: in function 'selftest'\n    core/main.lua:45: in function \n    [C]: in function 'xpcall'\n    core/main.lua:109: in main chunk\n    [C]: in function 'require'\n    [string \"require \"core.main\"\"]:1: in main chunk\nSecond trial:\nsudo src/snabbswitch -t apps.vhost.vhost_apps\nlink report\nsource.out->tapvhost.rx 40,944 packet(s) transmitted\ntapvhost.tx->sink.in    0 packet(s) transmitted\nMy environment:\nVMware Fusion 6.02 (bridged networking)\nUbuntu Server 13.10 (amd64)\n. Trying to wrap my head around this. What's going on in this test:\nlua\nfunction selftest ()\n   app.apps.source   = app.new(basic_apps.Source)\n   app.apps.tapvhost = app.new(TapVhost:new(\"snabb%d\"))\n   app.apps.sink     = app.new(basic_apps.Sink)\n   app.connect(\"source\", \"out\", \"tapvhost\", \"rx\")\n   app.connect(\"tapvhost\", \"tx\", \"sink\", \"in\")\n   app.relink()\n   buffer.preallocate(100000)\n   local deadline = lib.timer(1e9)\n   repeat app.breathe() until deadline()\n   app.report()\nend\nIt seems like it's the Source app that is generating the packets while I would like to capture real packets reaching the IP address of the Ubuntu guest (like during an ssh session) and dump them somewhere. Can the TAP vhost be the source? Could you show me an example of how capturing real packets with vhost can be done?\nI would also like to test the Raw socket alternative but without a selftest or example, it's not easy to understand how it's supposed to work.\nThank you for your time and help\n. Awesome! I'll keep digging around and experimenting until I feel I can tackle some developement. I'll probably come back with more questions :)\nThanks a lot\n. What am I missing here:\nsudo src/snabbswitch -t apps.vhost.vhost_apps\nlink report\nsource.out->tapvhost.rx 40,944 packet(s) transmitted\ntapvhost.tx->sink.in 0 packet(s) transmitted\nShouldn't 40,944 packets go down the Sink? Just as many as Source transmitted.\nThe Rate Limiter example seems to work that way:\nsource.output->rate_limiter.input   45,520,000 packet(s) transmitted\nrate_limiter.output->sink.input 20,017,166 packet(s) transmitted\nAlso...\nifconfig snabb0 192.168.100.1/24 up\nping -b 192.168.100.255\n...is not changing the number of the packets transmitted. The link report is still showing 40,944 packets.\nThank you for your patience :)\n. I reverted the code in buffer.lua to its state prior to that commit. The packets transmitted by Source are now changing, but I still get 0 packets transmitted by tapvhost.tx to Sink.\nsource.out->tapvhost.rx 1,072,735 packet(s) transmitted\ntapvhost.tx->sink.in    0 packet(s) transmitted\nUnless I bring up the interface:\nifconfig snabb0 192.168.100.1/24 up\nping -b 192.168.100.255\nIn which case I do get packets transmitted from tapvhost:\nsource.out->tapvhost.rx 10,664,266 packet(s) transmitted\ntapvhost.tx->sink.in    9 packet(s) transmitted\nI'm probably missing something in how tapvhost or Sink are supposed to work.\n. Your walkthrough is very helpful! Thanks\n. ",
    "agladysh": "Doh. Yet another OOP implementation.\nAre you really sure this is needed here?\nNo offense, but to me it is a design smell...\n. Luke, usually, I'm all in favor of experimentation and creative freedom.\nHowever, adding an OO-framework on a global level may have longstanding consequences.\nSince the framework is already there, it is too easy to just keep using it, without regard to applicability of OOP for a given task. We all have seen where this road leads to.\nFurthermore, since the framework is already there at a global level (not at an application level), this framework will be implicitly made the \"official\" one, not an experimental one as it probably should be.\nIt is your call, of course, but \u2014 are you sure that you want to go there?\n. @alexandergall There is a ton of available OOP-frameworks in Lua \u2014 everyone who comes to Lua from OOP background makes it his duty to invent one.\nMy position \u2014 in general, not regarding your code\u00a0\u2014 is that if one wants to do so much OOP in Lua that it needs a framework (even if it is a single class function), then that person must be doing something wrong in a way that is not natural to the language.\nFor example, I'm troubled by the fact that your framework has isa method. Why would this be ever needed in a language with duck-typing?\n(Sorry if I sound acerbic, nothing personal.)\n. @lukego Sounds reasonable. This was a knee-jerk reaction on my part to the big red flag of seeing class.lua in a pull request :-) Probably not a best thing to cultivate contributor base :-)\n. @alexandergall Just in case, I have not made a single commit in this project myself, so listen to Luke, he is the maintainer. I'm just ranting in background :-)\n. @lukego Keep in mind, however, that a mess that is created by badly chosen OOP framework often can be cleaned up only by a rewrite from scratch. This is why I'm reacting this way :)\n. @alexandergall Regarding that \"flaw of Lua\" remark \u2014 if you're interested in language design, I recommend you to read up a bit on Lua design and philosophy, then we will be able to discuss it further :-)\nAs for the way to do a \"simple inheritance\" in this project \u2014\u00a0this is a question for Luke as a maintainer, since it is largely a matter of taste in Lua.\n. @alexandergall Simplest possible inheritance in Lua is implemented by setting __index metamethod to the parent (or rather prototype) object. You can see that, among other things, being done in your class.lua. No frameworks needed to do that, just a setmetatable call.\nEverything above that is optional and a matter of taste. \nI suspect that Luke at this point is fine with your implementation\u00a0\u2014 he is right about this being early days. But, just for fun, I will look closely at your code tonight and maybe come up with something more to my own liking (FWIW).\n. See also: #104.\n. Should be closed in favor of #129.\n@altexy Maybe we should update pull requests instead next time?\n. @vladfedin, take a look pls.\n. OK, thanks. @altexy, can you please verify if the package installs everything what we need?\n. See also: #93\n. @lukego Answer to the both questions is: perhaps. That is, some research and experimentation are needed, but these two options are the first ones to try.\n. Please do not view this comment as a negative \"you can't do that\" post. If I am not being constructive somewhere, please point me there, and I will fix that.\nHere is my analysis (note that I did not seriously try to tackle reloading problem with LuaJIT instead of Lua, so my view may be a bit outdated):\n1. The very simplest and bullet-proof way of code reloading is a restart of a system process (maybe of a child worker process). As discussed on the list, it is concluded that Snabb can not afford that.\n2. The next simplest way to do reloading that I see \u2014\u00a0is to keep one Lua state per application. This solves several problems described below (except maybe cleaning FFI \u2014 this needs to be looked at), but has its own price (I can elaborate on that if needed). On reload you simply discard the state and create a new one. \nSomething like LPSM maybe can be used to avoid paying costs of re-loading whole codebase. (But this probably will require some kind of intervention from Mike.) See also: http://www.tecgraf.puc-rio.br/~lhf/ftp/lua/#lper\n3. If we can not afford state-per-application for some reason, then the next thing I would go for is to wrap each application into a in-state sandbox and to reload that sandbox as a whole. \nIf system-level Snabb fails, then it should be restarted as a process.\nYes, it is tempting to be able to upgrade a single module, but, IMHO, it is a road to creating a separate ecosystem for Snabb modules, incompatible with the rest of the Lua world. Simply because such modules will require much harder coding discipline (see below).\n4. What should be cleaned up on application reload:\n4.1.\u00a0Application code and state. Here lays the main part of coding discipline problem \u2014\u00a0you can not have any app objects that are referenced in the code that is not being reloaded.\n4.2. Any pure Lua module dependencies. A solvable problem, but\u00a0we will have to invent a good way to keep several versions of the same module. There is no good implementation for that, as far as I'm aware. (LuaRocks tries that and, last time I looked, fails miserably.)\n4.3. Any C Lua module dependencies. In addition to problems described in 4.2, there is a problem of reloading shared library code. Also should be solvable.\n4.4. Any FFI constructs. This should be researched. I've got a vague impression that FFI definitions for a shared library can not be unloaded. Maybe I am wrong or Mike fixed that.\n5. If we're reloading a single module, then it is more or less the same as for the application. Except that we will have to invent a way to know what exactly to reload (it should be relatively simple for a sandboxed application, not sure if it is as simple for a Lua module in general).\nA smart enough wrapper for require() and/or its innards probably should solve this for the majority of the Lua code out there (except legacy stuff). A great care should be taken with the design of the wrapper though \u2014\u00a0to keep the thing fully compatible with the rest of Lua world.\n6. In conclusion:\n6.1. Making code that is written specifically for the Snabb to be reloadable would be probably a relatively easy task \u2014 provided we institute and follow a set of special rules to enforce that, and we are ready to pay the price for these rules. \n6.2. A generic solution for reloadable Lua code is also possible, but will probably require much more effort. Ideally it should be complemented by the static code analysis tools (not to mention a dedicated test suite with good coverage) that will prevent human errors. Luckily, static code analysis for Lua is on the rise, and it is also doable.\n6.3. A partial solution, like one described in item 3 is probably preferable, and I would recommend exploring it first (provided that item 2 is not possible or desirable for some reason). I can elaborate on the design of the sandbox as I see it.\n. @vnaum The problem is that now tests are in the same files.\n. Erm. Something wrong in LJ innards? Weird!\n. @dvv, @altexy what does git --version say on these machines?\n. @dvv So, does the master build on your system?\n. > [dvv@pagocmb snabbswitch]$ uname -a\n\nLinux pagocmb 3.13.6-1-ARCH #1 SMP PREEMPT Fri Mar 7 22:47:48 CET 2014 x86_64 GNU/Linux\n\nThe problem was in wrong Git configuration on a local machine, it seems.\n. Javier, we're doing CI integration, and this seems related. :-) Should we coordinate our efforts?\n. Well, the most basic requirement is not to break our efforts on fixing TravisCI builds. See PRs #147 and #146 (more are coming).\n. So, maybe it is a good idea to rebase your work on #147 (or simply wait on merging your work until these two PRs get to master).\n. In fact, #147 contains Travis CI integration. It currently fails, since there is new broken code in master. We'll send a fix today.\nBTW, it looks like your code, see this and below:\nhttps://github.com/SnabbCo/snabbswitch/commit/2a7d78d987e382bf62d826136686e01e513959a9#commitcomment-6072062\n. Well, Luke just closed our PR, so you have the priority. Sorry for the noise then.\n. This doesn't really matter anymore, but:\n\nI've found that rebasing with the PR open does work\n\nIt should, what is the diagnostics?\n. > Please help me out and keep it simple.\nLuke, can you please suggest a way that suits your preferred style of development better?\n. Just in case:\n\nAlready the reason I'm taking time to merge changes is that PRs are too complex and including both changes that I want to merge and others that I don't. \n\nWe usually have no problem with rearranging commits and splitting pull requests \u2014 if/when you give us the feedback. Also, we can refrain from sending more than one pull request at a time if that helps :-)\n. Luke, it is your call, but we've fixed everything :-)\nWith #146, #147 and #148 Travis gives all-green (except for one intermittently failing test \u2014\u00a0apps.packet_filter.packet_filter, which @altexy will look at).\nhttps://travis-ci.org/logiceditor-com/snabbswitch/builds/23457364\nMy opinion is that working CI will help with the development. I can help Javier with the rebase if there'll be conflicts.\n. We implement 2 and 3 in #146 and #147, so, maybe you could cherry-pick something from there (unless these are the parts that Luke doesn't want to merge :) )...\n. NB, for history: Since this PR is based on a branch from #146, a better way to view the diff would be\nhttps://github.com/logiceditor-com/snabbswitch/compare/nm%2F4105%2Fselftests_v2...nm%2F4088%2Ftravis_tests_v5\n. NB: This is a rare bug. If you see one on your machine or in CI, please chime in.\n. Is this related to #153?\n. @essej-sehtam You probably can get more information on this from the LuaJIT mailing list:\nhttp://luajit.org/list.html\nPlease make sure to research this issue first \u2014\u00a0it is likely to be already explained somewhere.\n. @lukego Indeed, this would be an interesting information. Didn't mean to distract the OP from that :-)\n. @essej-sehtam Snabb Switch uses a development version of LuaJIT 2.1, not 2.0\n. I'd say that this should be benchmarked before refactoring. If this is compiled away, it should not generate much overhead. (But LuaJIT can be finicky in that matter...)\n. It would be awesome indeed. But we need to coordinate it with Mike, so it would not become a forking experience instead of hacking one \u2014 which would be a bit less awesome, I believe :-)\n. What, Mike gave a talk somewhere? Why did I miss that? :-( Is there a link or something?\n. I suggest explicitly documenting lower_case for constants guideline in the README. UPPER_CASE for constants vs. lower_case for variables and function names is at least just as frequent (thanks to constants-as-C-macros).\n. Aren't ^__[a-z] and ^_[a-z] reserved in Lua? I vaguely remember Roberto saying something on that matter...\n. Right! Thank you for clarification (and sorry for not looking it up).\n. ",
    "alexandergall": "Can you please point me to the existing OO framework? I didn't mean to add \"yet another one\" for the heck of it.  Also, please note that this is my first work in Lua, so I appreciate all feedback on style and technique, but please be gentle :)\n. The fact that there is a ton of OOP-frameworks  appears to me to be a flaw of Lua, really. Frankly, I don't care what we use, I just need simple inheritance. It's already there? Great, tell me how it works and I'll use it.\n. @agladysh: I have no intention to enter that particular rat-hole :) But I'm all ears to learn how to do this with the existing framework.\n. OK :) the latest commit is https://github.com/alexandergall/snabbswitch/commit/0e3116f1511111097fa197446f5fd418870f847c\nIt contains some performance improvements and bug fixes. class.lua should probably be moved to some other place, I guess.\n. Yes, but this will take some time. I just merged with your master and realized that there are substantial changes in the app API and my code no longer runs. This is not the place to discuss this, but I'm hitting a wall right from the start, because config.app() requires the arg to the class' new() method to be a string, but some of my classes expect multiple arguments :(\n. OK, got it. \n. I'm just about ready to send a new pull request. Regarding class.lua, I propose to put it in lib/lua/class.lua and put the corresponding require in core/main.lua. I thought that maybe strict.lua could be put in lib/lua as well and maybe other similart stuff in the future. What do you think?\n. On Thu, Apr 10, 2014 at 9:19 PM, Javier notifications@github.com wrote:\n\nfor lib/lua you mean the system Lua libraries? or a lua/ subdir inside\nsnabbswitch's src/lib/ dir? If the former, I'd advise against it.\nSnabbswitch should be self-contained... we're including LuaJIT itself for a\nreadon. If the latter, then I don't get why a lua/ subdir. wouldn't it be\njust src/lib/class.lua ? after all, almost everything is Lua here, isn't\nit?\nI mean a directory in Snabb's src/lib/. The idea was to put stuff there\nthat has something to do with the way we use Lua for this project rather\nthan something related to a specific feature like what's in src/lib now. I\nproposed to move strict.lua to that place as well.\n\n\nAlex\n. Ok. Give me a minute to get acquainted with  git rebase :)\n. Rebasing into separate commits would be way too complicated due to my rather badly organized commits and the fact that I merged twice with master.  Sqashing everything into a single commit is ugly, I agree.  I have decided to re-play my stuff onto a fresh branch of the current master in a better organize manner and post a new PR for that and close this PR. \n. Works. Thanks!\n. Please ignore this pull request. It's so simple to use libpcap for the matching that it doesn't make sense to use my half-assed version :) \n. The difference in size between the executable in my vpn-performance branch and the current master is about 12.5 kiB or 1.5% of the total size. I don't know how much of this is due to the code pulled in from libpcap. I assume that this is not a problem.\nBuilding libpcap from scratch will almost certainly blow the compile time budget, though. Apparently, there are no alternative BPF implementations that are ready for use as dependency out there. One way forward could be to extract one of the engines from http://src.carnivore.it/users/common/bpftk and make it part of Snabb Switch. How about that?\n. Nope, doesn't work. It still depends on libpcap for the bpf compiler.\n. So, we could take one of the engines from http://src.carnivore.it/users/common/bpftk and the compiler from https://github.com/torvalds/linux/tree/master/tools/net and package it with Snabb Switch (I guess that just pulling in the git repositories wouldn't work). Alternatively, we could only include the engine and let apps that want to use it supply the pre-compiled bytecode directly (e.g. taken from the output of tcpdump -ddd).\n. Ah, the subtleties of free software :) So we're lucky that libpcap is BSD.\nI'm fine with that. Can I rebase my code to the current master and submit a new pull request?\n. Cool. Can't wait to test it :)\n. Looks good. I hope that my framework turns out to be useful. It would be easy to adapt this to the changes of my own pull request. \n. Yes, that's ok. I guess we'll have to sort out the issue with the inclusion of libpcap into the build environment before my request can be merged.\n. I just discovered a bug in the ICMPv6 neighbor discovery code. I'm closing this request rather than posting a fix afterwards. \n. Sorry. My own sleftest failed. Sigh. Please excuse the noise.\n. I don't know how to quantify this effect apart from the feedback from the JIT compiler, ranging from interpreter fallback and trace aborts to function blacklisting.\nTo me, it's basically the same category as NYIs, where the reasoning simply is: avoid interpreted code in the packet processing path.\nTo assess whether it's still needed at a later point in time, I suppose you'd have to revert to the original loops and check what the compiler says.\nAs usual, this is all guesswork and trial-and-error. At the heart of all this hackery is my worry about occasional \"bad runs\" that my app experiences.\n. The nfv  selftest is only enabled on davos. I can't run it there myself because I don't have root credentials. Can you please run it and check the output of the failed test?\n. Got it. When the test creates the nd_light app, it passes it the MAC address as string instead of the on-the-wire format. One way to get this right is to call lib.protocol.ethernet:pton(). I guess the API needs to be documented better.\nThe bug gets exposed now because the new version of nd_light passes this value on to some other code that expects it to be in a particular format. \n. Well, of course it breaks keyed_ipv6_tunnel. It's an incompatible change. My understanding was that someone will adapt the tunnel app. Maybe I misunderstood and Max and Luke expected me to do this? Anyway, the selftest should probably catch this. \n. Not sure what the failed designs.neutron selftest is complaining about. I don't see a connection with my code. Comments welcome.\n. No, only the app that clones a packet needs to be aware of it (and call cow_iovec() to create a private copy of a shared buffer). Handling a packet that contains shared buffers does not requrie any special treatment.\n. We discussed this before: https://groups.google.com/forum/#!searchin/snabb-devel/alexander.gall/snabb-devel/NYOggzdc3oY/RjGVNK1ag1wJ\nThe symptoms are still the same as back then. I've had to side-step the dead-app code ever since, because the performance impact is massive for me. Of course, if someone truly understands what's going on and knows whether it's fixable rather than a fundamental limitation with pcall(), that would be great. Until then, I'd like to have a knob to turn it off.\n. I'd like to reiterate: the fact appears to be that returns of functions called by pcall can currently not be compiled (all of the NYIs occur exactly at the end of the push() methods executed by pcall()). The result is fallback to the interpreter after every invocation. I don't see how this can be reconciled with our performance goals.\nIn my benchmarks using iperf over a tunnel, the consequence is 5-10% interpreted code, 1-5% GC and about 10% increase in CPU cycles. I'm a bit surprised that nobody else appears to have noticed.\n. To be more precise: the pcall itself is compiled but this appears to be a situation where returns to a lower frame than the starting frame of the trace cannot be compiled. Since we can't control where traces start, this can probably not be avoided.\nAnyway, I close the PR because on top of it all, the \"dead_code_detection\" hack is buggy, too.\n. Agreed. A better choice to make this configurable would probably be to use a variable in the app module so one can do e.g.\nlocal app = require(\"core.app\")\napp.dead_app_detection = false\n. It's certainly good to make the usage consistent across all apps. My only objection would be that a lot of the require code looks a bit awkward because most modules provide a single app that tends to have the same name as the module, e.g. \nlocal baz = require(\"foo.bar.baz\").baz\nA nice one is\nlocal RateLimiter = require(\"apps.rate_limiter.rate_limiter\").RateLimiter\nBut I can live with that :)\n. Darn, the procedural interface of counter.lua is broken.  Need to fix that before merging.\n. futex(7) could just be what we need in this case. With a futex, there is no system call overhead in the uncontended case, which I assume will be, by far, the common case. I would suggest to use one futex per shared segment, but it would be easy to do that on a per-element basis as well.\n. One could safeguard against a lockup caused by a faulty monitor by using a timeout for the FUTEX_WAIT operation and deactivate locking in the traffic process when this happens. But, yes, this could get a bit complicated.\nWhat frequency for snapshots do you have in mind?  On my routers, I like to be able to see counter updates in near real-time when I'm logged in and looking at things. For the purpose of graphing with the standard 5 minute intervals, you could obviously tolerate a much bigger lag. For my SNMP data, I'd like to have near real-time data again, e.g. for the operational status of my VPN tunnels, even though most of the data is static. In any case, I see demand for fairly short snapshot intervals and this is probably a problem.\n. Ok, that would certainly be sufficient :) I would have guessed that the performance impact would be more severe. Still, it looks a tad excessive to me...\nHere's a crazy idea: let's declare the interface to not guarantee consistency of data for the consumer. The hypothesis here is that such events are very rare (are they?). The consumer can chose to accept this or implement checks that reduce the likelihood of accepting wrong data, e.g. reading the same object twice or comparing it with previous data to spot \"outliers\".  Too crazy?\n. Correct me if I'm wrong, but the double-buffer method you describe can't guarantee atomicity without using a true atomic test-and-set operation, can it? Still not sure how far we should take this.\n. Ack.  I see different use cases with respect to requirements (perfect vs \"probabilistic\" integrity) as well as access models (pure reader/writer vs mixed access).\nI would classify my SNMP case as a pure reader/writer (producer only writes, consumer only reads) where probabilistic integrity is good enough (most data is static, writes are rare and reads infrequent, e.g. seconds or minutes apart). Couners for traffic statistics appear similar to me.\nAn example for a pure reader/write with a requirement for perfect integrity would be, for example, if we used a separate Snabb process to manage the ARP/neighbor cache and have traffic-forwarding processes performing lookups in these tables.\nWe might end up with different arbitration schemes and leave it up to the apps to chose on, including the option to use none.\n. On Wed, Feb 4, 2015 at 7:13 AM, Luke Gorrie notifications@github.com wrote:\n\nMerged. Even if there is an issue with the counter interface we can fix that when we add code that uses it.\n\nOk. I'll probably suggest a change for the result of the register()\nmethod, which is useless (and misleading) as it is now.\n. I guess I need to recover more gracefully if the directory for the shmem files doesn't exist (this is what breaks some of the test cases).\n. Well, this PR is just a pragmatic hack to get the most basic monitoring working. Trying to come up with a scheme that will also work for NETCONF/YANG is futile at this point, because we haven't even started to tackle the whole configuration/monitoring issue. It would certainly make sense to collect the data in a manner that makes it easy to implement various front-ends for accessing it, but we (well, at least I :) don't know yet what that should look like.\nOf course, support for SNMP is a must if any of this stuff should work in a real-world network. I don't know if we need a list of MIBs that we need to support up front (but the IF-MIB is an obvious must-have). The designer of a Snabb module should decide which MIBs are relevant. In my case, for example, I need the MIBs from the \"PWE3\" (pseudowire edge-to-edge emulation) framework to make the whole thing fit in our own monitoring system. One of the strenghts of our project will be that we will be able to implement quickly whatever the operators need, in contrast to the fossilizied systems of (some) vendors.\n. On Thu, Feb 19, 2015 at 10:28 AM, Luke Gorrie notifications@github.com wrote:\n\nAlex, what do you think about moving this SNMP instrumentation from the\nintel10g object into the link object?\nThe link object would need to track more state (admin/operational status)\nand the apps/drivers would have to supply this. The benefit would be that we\ncould monitor any link with SNMP.\n\nI'm not sure how exactly a generic link should be modeled as an\ninterface, but, yes, I guess that could make sense. It doesn't seem\nlike a straight-forward hack to tie it into the app system, though. In\nfact, the object that this code is monitoring is precisely not the\nlink that connects the intel driver with another app but the\n\"external\" interface through which the driver pulls in packets from\nthe outside. I'll have to ponder this a little :)\n\n(I'd like to add this SNMP monitoring support to the Virtio-net interfaces\nfor NFV too.)\n\u2014\nReply to this email directly or view it on GitHub.\n. Corner case: loadgen doesn't trigger dynamic idling because it doesn't perform any frees.\n. Sigh. I don't know how to run program/snabbnfv/selftest.sh myself on interlaken and I can't make sense of SnabbBot's testlog.\n. Can someone please help me find the reason why the snabbnfv selftest fails?  I guess this has something to do with it:\n\nqemu (/home/max/bench_env/vma.img) log:\nQEMU waiting for connection on: unix:vhost_A.sock,server\nqemu-system-x86_64: -netdev type=vhost-user,id=net0,chardev=char0: chardev \"char0\" went up\nQEMU 2.1.0 monitor - type 'help' for more information\n(qemu) \n(process:29278): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\nThis is not enough to diagnose the actual problem.\nThere is also a failure of a ping with a large packet\nping6 -s 9000 -c 1 fe80::5054:ff:fe00:1%eth0\nPING fe80::5054:ff:fe00:1%eth0(fe80::5054:ff:fe00:1) 9000 data bytes\nConnection closed by foreign host.\nJUMBOPING failed.\nBut that must have failed before, too.  The -s option only specifies the size of the ICMP payload, i.e. the entire packet will be 9000+8+40+18 = 9066 bytes, including Ethernet, CRC, IPv6 and ICMP header. The MAXFRS register used to be set statically to 9018 (and this default is unchanged by my code).\n. Thanks. I'll try that. I can't access any of the lab servers since yesterday evening, though. Not sure whether Luke's IP address has changed and the DNS for lab1.snabb.co needs to be updated or it's just me.\n. I suspect that the MTU problem is not due to MAXFRS but the code I've introduced that prevents the sending of frames which are larger than the MTU in M_sf:transmit():\nif p.length > self.mtu then\n  if self.snmp then\n    local errors = self.snmp.ifTable:ptr('ifOutDiscards')\n    errors[0] = errors[0] + 1\n  end\n  packet.free(p)\nelse\n  ...\nI do think that this is correct and the app should not generate such frames. In fact, what is the MTU configuration of the VMs? They should use an MTU that is compatible with that of the snabbnfv instance. It probably doesn't, because otherwise it would not generate these large frames (i.e. TCP MSS and fragmentation of Pings). The NIC drops frames larger than MAXFRS, but maybe not if packets are only switched through it (not sent on the wire).\n. Bingo. We should sort out the MTU business ASAP. This is fundamental stuff that we need to get right. \n. The hardware vs software question is a good one, indeed. Software has at least two advantages:\nThe code will work for any NIC, which implies that the same information will be available throughout, even if some hardware doesn't provide counters for everything.\nIt will cover cases which bypass the hardware stats registers, as appears to be the case for traffic switched by the NIC.\nThe only downside is possibly, as you say, performance. But the code will be extremely centralized (as part of the generic driver) and we can optmise the hell out of it. This should actually be fairly ease to check.\nPersonally, I have a strong tendency to get rid of as much hardware assistance as possible.\nWhile my code currently uses the SNMP objects directly, I think it would be better to devise our own system of counters and statistics and generate all external representations (like the MIBs) out of that. The shmem mechanism seems like a good starting point because it solves two problems at the same time: communicating the data to other processes and provide stable storage for data across invocations of Snabb processes (e.g. interface counters).\n. Now something specific to this PR. I still stand by my statement, that dropping giant frames in transmit() is correct and I suggest to add this check on receive() as well to make all use-cases behave the same way (i.e. when traffic bypasses the MAXFR logic on ingress). If I'm correct, this will demonstrate that the current setup for the snabbnfv selftest is flawed, probably because the VMs use an MTU that is too large.\nThis is a bit orthogonal to the question whether counters should be maintained in software. It's primarily code that makes snabbnfv behave correctly and consistently wrt to the MTU.\n. @lukego not crazy at all. I've been thinking about that for a long time. I believe that this is the way to go in the long run. Just like I would like to see control-plane stuff (including neighbor discovery/ARP) moved out of the packet-processing apps (could be done by the Interface app as well). The question is how to share an interface with other apps, but you also had some ideas about that, too.\n. > > If I'm correct, this will demonstrate that the current setup for the snabbnfv selftest is flawed,   probably because the VMs use an MTU that is too large.\n\nDo I understand correctly that you mean the NIC is configured with too small of an MTU compared with the VMs, but that the selftest does not detect this because the NIC is not enforcing MTU for locally switched packets?\n\nExactly. My check in transmit() already broke this and I suspect that a similar check in receive() would, too.\n. Agreed. Thanks.\n. It appears that the logic that installs pflua in the snabbswitch/src directory is missing?\n. No hurry. I should be working on other stuff anyway :)\n. Encouraging news. I've tried pflua with my app and I see a significant performance boost. I run my test on interlaken with loadgen. There are 3 calls to the pcap.filter match() method in the decapsulation path (one in nd_light, one in a \"dispatcher\" that demuxes tunneled packets into the proper pseudowire based on src/dst and a match for the tunnel protocol). The profiler for a typical run shows this:\n100%  Compiled\n  -- 15%  filter.lua:32 < dispatch.lua:76 < app.lua:71 \n  --  6%  filter.lua:32 < nd_light.lua:251 < nd_light.lua:285\n  --  6%  packet.lua:71 < datagram.lua:205 < pseudowire.lua:597\nThe packet-rate is around 2.6Mpps. The performance is dominated by the BPF matches, followed by the actual decapsulation (using AVX memmove).\nWith pflua, I get\n100%  Compiled\n  --  7%  packet.lua:71 < datagram.lua:205 < pseudowire.lua:597 \n  --  6%  datagram.lua:188 < l2tpv3.lua:49 < pseudowire.lua:598 \n  --  4%  datagram.lua:189 < l2tpv3.lua:49 < pseudowire.lua:598 \n...\n  --  3%  [string]:5 < nd_light.lua:251 < nd_light.lua:285 \n...\n --  1%  [string]:6 < dispatch.lua:76\nThe BPF overhead is a lot smaller and the packet-rate is up to ~3.3Mpps!\nI'll do more thorough tests, but this is looking very good. Cool!\nOh, the packet rates without the profiler running are 3.0Mpps vs 4.2Mpps :)\n. > The code looks nice too. I like the way you simplified the generic checksum routine. Is there any meaningful performance impact on that?\nI didn't check, but I don't expect a difference, because of the inlining in the old code.\n\nBut we should remove the comment at the top of the file saying that the generic checksum is based on the DPDK one (http://dpdk.org/browse/dpdk/tree/lib/librte_net/rte_ip.h) because it will not be anymore.\n\nOk.\n. > On first reading it sounded to me like we are saying that the caller needs to pad the input with a zero-byte if it is odd-length. Could be worth emphasising that this is not required?\nMaybe it shouldn't be mentioned at all, since it's part of the algorithm.\n\nTony Rogvall who wrote the original SIMD checksum had an interesting idea for an API tweak for optimization btw. The idea is to actually insert zeros into the area following the payload so that no loop is needed to take care of the unaligned part at the end.\n\nBut how would one make sure that there is room for the padding bytes after the data?\n. @lukego a generic token bucket is certainly a good idea. I would still prefer to integrate it in the logging function (and provide a take() method as a pass-through to the token bucket) to avoid a meltdown due to a badly written app.\nAlso, I think it's important to somehow log the number of discarded messages.\n. With this new version, you can do\nif logger:can_log() then logger:log(...) end\nI tried this with one of my test cases and saw a drop of the packet rate from 7Mpps to 5.3Mpps (on interlaken) when logging a message for every single packet.  I believe that the performance drop is exclusively due to the call of clock_gettime().  Maybe this is the time to revisit the RTDSC issue :) With that, the logger could be essentially hitless, I think.\n. Oh, and I've added an auto-throttling mechanism that will reduce the logging rate when too many messages are being discarded.  In the extreme case, like my test case with a message for every packet, only one message per 10 seconds will actually be logged to prevent clobbering of the log file.\n. This differs from lib.ipc.shmem in two ways: the ctype is part of the metadata and each object exists as a single entity in the file system.  The ctype can only be used by receivers that use LuaJIT, so the type still needs to be inferred from the name in the general case (e.g. my SNMP agent written in Perl).\nApart from the ctype, which could easily be added to the index file of lib.ipc.shmem, this is a degenerate case of that framework where every index contains just one object, right? The goal is to make the code simpler and this version certainly is. But will it remain so?\nI think it's the common case that an application is handling a whole bunch of objects, which would be collected in a directory, I suppose.  So, the application will have to perform all the stuff that comes with that (creating it, walking it to find all objects etc.). I would immediately add code to the module that would do that for me, because I don't want to repeat it in every app. My guess is that we would end up with something not unlike lib.ipc.shmem...\n. Actually, I have never used this mechanism. How exactly would one structure the objects? The name space is flat, isn't it? Maybe I should read up on that before posting lengthy comments :)\n. I think that it would be easy to re-write lib.ipc.shmem to use this as basis and just add the index and handling of custom types (e.g. OctetStr in lib.ipc.shmem.mib').  The API could probably remain unchanged. The index file would simply include the full path of the object (the size would no longer be needed).  I think I would keep the name space versioning, because I feel that this will be something that we need later on.\n. @lukego I use Emacs. Will change to spaces. BTW, I'm slightly annoyed by the way the Lua mode handles indention when splitting long commands on multiple lines.  This sometimes looks awful and makes it hard to obey the 80-columns limit. Is anybody else bothered by that? \n. @lukego  No, I didn't use bench_env. I don't need VMs for for my performance tests, just a loadgen on one of the looped NICs and an instance of my app. I simply do that manually. \n. Bingo. This fixes my problem :) It is puzzling why a reserved bit would have such an effect and why the default value is not 0.  OTOH, the data sheet is explicit what to do :/  @lukego it seems to me that labeling this with \"disable relaxed ordering\" is a misnomer, since that's not what it does, AFAICT. There are other flags that deal with relaxed ordering for descriptor write back and data.\nMaybe the initialization sequence needs to be ironed out a bit? And I would really like to enable DCA properly, because it looks like it could be very relevant for us. In fact, I wonder why nobody has done this yet?\n. Interesting. Up to now, I had also assumed that DDIO was there and enabled, allthough it confused me that the data sheet has absolutely nothing to say about it (at least I can't find anything). The section on DCA made me believe that DDIO might not actually be supported, but if it's the way you say, all is fine :)\n. The learning part of the current bridge implementation does incur a substantial overhead (I included some estimates in the comments of the PR). It's a function of the parameters of the Bloom filter as well as the number of output ports assigned to each input port. Storing of the source and destination MAC addresses of a packet in a Bloom filter storage cell can take up to a few hundred cycles (but it's O(1) with respect to the number of objects in the filter and the lookup operation should be pretty fast), i.e. that alone will limit the packet rate substantially.\nI did not yet perform any serious performance tests with that thing, apart from my VPLS app, which has a fairly modest performance target, so I'm very curious how it will do under heavy scrutiny :)\n. STP would put certain links in \"blocking\" mode (not forward onto nor accept packets from the link) to create a tree topology with a dynamically seleted bridge at the root (there are some knobs to influcence the decision who becoms the root bridge). With virtual links, it's much easier to set up a full mesh, which will also allow the packets to travel more directly than with STP, where there can be multiple hops in the path from one bride to another.\n. Didn't have time to look at it in detail yet, but there is at least one gotcha: the MAC addresses starting with 01 happen to have the multicast bit set, so they are always flooded to all ports.  Try with a proper unicast MAC address.\n. Darn, but there is also a bug in the learning bridge! The section\nif not is_mcast then\n     bf:store_value(mem, 6, eth_dst)\n  end\nshould be\nif not is_mcast then\n     bf:cell_clear(eth_dst)\n     bf:store_value(mem, 6, eth_dst)\n  end\nWill submit a fix.\n. Forget my \"fix\". I wasn't thinking straight.  Sorry for the confusion.\nThe expected result is no traffic on the cross-links that connects bridges 1 and 2, except for the first burst of packets when the destination addresses haven't been learned yet.\n. When I did some test runs, I sometimes saw flooding behaviour, i.e. the lookup of destiantion MAC addresses failed when it should have succeeded. The effect goes away when I make subtle changes somewhere else in the code or when I turn off JIT.  It looks similar to issues with optimizations I've had a while back when I wrote that code. I thought it was fixed with a bug-fix by Mike, but apparently not all of it :(\nDo you see this at all?\n. The last issue aside (I'll have to ponder this a bit), I've made some crude performance checks with the 2-bridge topology. In that case, the learning is just overhead. When I comment out that code, I get a baseline of about 30Mpps. I use perf stat -e cycles to get the total number of CPU cycles for the entire application and divide that by the number of processed packets, which gives a sort of per-packet cost. In my case, I get a little over 80 cycles per packet.\nWhen I add just the code for the Bloom-filter encoding of the destination MAC address for each packet, I'm up to 590 cycles per packet, which is way more than it should be and doesn't match my previous tests at all :(\nThe encoding involves two passes of the Murmur hash function over the MAC address.  My previous tests estimated each pass at about 25 cycles. In this test, the first pass alone apparently amounts to about 240 cycles, while the second pass adds just 60 cycles (which is still too much). I thought maybe a caching effect, but perf's cache-miss counters don't suggest that.\nThis is hard to analyze and it probably depends a lot on the dynamics of the compiler as well, particularly due to the inner loops of the Bloom filter.\n. @eugeneia the loss on some of the inter-bridge links comes from the fact that the learning bridge app processes only one source port per call of push(), see the comment there. I wanted to avoid yet another loop, but this may have been misguided, don't really know. If you iterate over all source ports, the packet rate becomes completely symmetric and there shouldn't be any loss.\n. The performance of the Bloom filter in this context is clearly disappointing.  It behaves completely different when compared to being run in isolation. This is the tracing JIT working against us this time, I guess :( Run in an isolated test, execution of both hash functions takes about 40 cycles, which is pretty good. I have no idea why it takes hundreds of cycles in this case.\nEach round in the Bloom filter loop takes 5-10 cycles, so with 12 rounds in the default configuration (1000 MAC addresses with fals-positive rate <0.01%), storing of an address still takes around 150 cycles.\nI don't have a hunch about what performance to expect from a different approach using an optimized standard MAC address table.  One thing to keep in mind is that any such algorithm will have some non-trivial scaling property as the number of stored addresses increases, where as the Bloom filter overhead is constant.\nA disadvantage of the Bloom Filter is that a destination has to be looked up in every possible outgoing port, where as a standard address table lookup would yield the output port immediately.\nThe whole thing is pretty much an experiment, nobody uses this in a switch :)  It would still be neat if I could get it to perform well. I'm sure that it would give us an edge at least in the case where a large number of MAC addresses need to be learned.\nAnyway, to get 30Mpps on a 2.4GHz CPU, you need to get it all done within 80 cycles. I would have thought that's impossible, but maybe not?\n. I've got one step closer to understanding the bad performance of the Bloom filter code in the context of the labswitch. It appears that performance goes down the drain as soon as the JIT-ed code of the learning bridge contains a side trace. This is the distinguishing feature between fast and slow runs in my tests.\nWhen perftool support is enabled in LuaJIT, perf top will usually show three traces using the bulk of the cycles (roughly corresponding to the main loop, the snabbmark loop and the main loop in the learning bridge). When one of these traces is a side trace of one of the others (indicated by, e.g. TRACE 10/4 in the JIT dump file, which says that the trace is called from side exit #4 in trace 10 in this case), a penalty of several hundred cycles per packet appears, even if the side trace is basically doing nothing.\nMy current guess is that somehow the overhead of switching between traces in this scenario has a pretty large overhead (maybe due to stack slot loads or something like that). I wonder whether this has something to do with the \"trace stitching\" code that has been disabled in recently by Mike due to subtle bugs, if I remember correctly.\nI don't really understand when exactly a side trace is created. For example, when I call the murmur hash function directly from within the learning bridge rather than through the bloom filter module, there is no side trace and only the expected 20 additional cycles due to the hash function.\nFascinating.... and frightening :)\n. Ha, I actually understand why side traces are created and how to avoid it in this case. Once again, it's a subtle detail but I've learned something new and it seems very clear now :) I'll write it up and post it to the ML when I find the time.\nI've speeded up the hash function a bit. It now takes only ~15 cycles to pass over a MAC address in the ideal case, i.e. when all constants can be pre-loaded to registers outside the loop. When executed as part of a larger trace, like in this case, some operands are fetched from memory/spill slots due to register pressure. This increases the cycle budget by a little over a factor of 2. I don't know if that could be avoided.\n. @eugeneia No, the source address is always learnt. The check for the multicast address avoids storing the destination in a bloom filter object.\n. Ah, ok :) I should have seen that but I'm in a hacking frenzy and don't pay enough attention right now :)\n. Nice, though I do need the N:N case with split-horizon for my use case. How do you avoid loops without split-horizon? Do you just assume that the topology is loop-free?\nI'm a bit uneasy about how colliding (truncated) MAC addresses are handled. In the worst case, the wrong host gets all the packets and the legitimate host none at all. That's a subtle failure that will drive you nuts when you hit it. And you can't detect it either, because the upper three bytes have been thrown away.\n. I don't think it's relevant in your context, but I just want to make sure there are no misconceptions: you seem to assume that a switch manipulates the destination address of  a packet during forwarding (that's what you mean by \"swap a MAC\" if I understand correctly). That's not how a switch works, i.e. the packet is forwarded unmodified.\nUnfortunately, I'm unfamiliar with the conecpt of FBP, so I can't comment on its application to packet processing, at least not with a substantial amount of reading :)\n. Well, tests are good :)  Note that I didn't write any of those modules and I never really looked at that code (though the ipv4 and tcp checksum and pseudo-header code looks like it has been taken over from the ipv6 and icmp modules, which I did write). Maybe the checksum methods should be documented better?\nThe datagram module can be used to create synthetic packets without having to use hexdumps.  The API is a bit awkward if one wants to relocate a pre-allocated header into a packet buffer (e.g. to update the checksum in a packet), but that can be done as well.\n. Very useful, thanks! Now I don't have to collect this information by hand via perf top and the dump file any more. \nI've just rewritten my learning bridge app using a more standard MAC table instead of a Bloom filter. The first version suffered from the \"hot side trace\" syndrome as well. I'll try to write it all up because I learned a lot during this exercise.  The upshot is that it is non-trivial to influence the compiler's decisions which traces to select, i.e. it's easy to say that we would like to have separate traces for something, but it can be very hard to achieve and the effect can be easily lost when the compiler's strategy changes in a future release.  For the bridge app, I ended up having to write some code in C to hide it from the compiler. A bit ugly but it made all the difference.\n. @lukego Interesting, thanks for checking that.  This could eliminate the problem for apps but you can still run into it with other modules that use some kind of instantiation mechanism (like I did with the Murmur hash function). So I think we need some programming guidelines anyway (it's probably not as complicated as one might think). Also, I wonder whether there could be situations where this approach backfires, i.e. cases in which forcing different traces actually degrades performance. None come to mind right now, but it's difficult to anticipate.\n. I deleted the branch by accident.\n. I didn't have time to look at it yet, so I'm very happy about your progress :) It looks like you nailed it down pretty well.\n. Thinking about it a bit more, I'm pretty sure that we're dealing with two distinct problems. One is on the sending side, which you solved. The other is on the receiving side, because I already verified that the receiving NIC is logging input drops. There doesn't seem to be a similar pre-fetching mechanism on the RX side, but I haven't looked very hard yet.\n. I think the SnabbBot failure in c67235b was due to the log rate-limiting in nd_light, which suppressed the \"Resolved next-hop\" message that the selftest was looking for. This is actually fixed by 6a2d0f1.\n. Um, no, c67235b is the newest commit :( I guess we need a more robust way to check whether ND has succeeded than parsing the messages.\n. Messages were discarded because the token bucket was empty at startup. This should be fixed now.\n. The main reason for using the logger is not the rate-limiting but the unification of the logging. My suggestion is to use that for all messages that are operationally relevant to have a common format and a way to write them to well-defined places.  Later on we could use it to interface with a standard syslog mechanism as well. The rate-limiter is just a safeguard.\nI've been using it for my own stuff extensively and it has worked well so far (try it by logging a message for every packet at 20Mpps :)\nI did think about the documentation but was looking in the wrong place (src/core/README). Do'h!\n. I use it in my l2vpn code (which I haven't pushed yet). That code is modular in the sense that there can be different combinations of transport and tunnel headers and it makes sense to push the headers individually rather than as a single block. The delayed commit method allows me to keep the code modular and still avoid a performance impact by moving the packet around three times in a row.\n. length would need to be updated as well (and the assert in shiftleft could simply check n <= p.length, I think).\nApart from those details and the non-trivial alignment problem, this would certainly simplify lib.protocol significantely. It would also make it straight-forward to \"relocate\" a protocol header to a packet buffer, like with the old multi-buffer approach. This feature was lost with the current design because headers don't stay in a fixed location (you can still do it but only in a convoluted way by calling new_from_mem after all headers have been pushed).\n. A benchmark would certainly make sense. I'm not sure what would be most useful to capture the performance of the actual bridge if we run the packet generator in the same process, for example. Do we want separate benchmarks for flooding and unicast performance? It would also be useful to benchmark the bridge when the MAC table is full.\n. Currently, my primitive benchmark is to run a two-port bridge with a slightly modified Source app that lets me create synthetic packets with pre-defined headers. On my system (Xeon E5-2620v2, 2.4GHz), I get around 11.5-12Mpps with 64-byte packets. I guess this should at least be translated into some metric like cycles per packet before it could be used as a regression test in the CI.\nOne important variant of this check is to fill the MAC table with random data to simulate a busy system and exercise the code paths associated with that. With the C-wrapper for the hash table, performance remains basically unchanged (that's where all of my attempts to write it in pure Lua failed miserably). But thats's just some ad-hock code right now.\n. Creating new objects in the processing loop is definitely a no-go. One way to avoid actual allocations is to call the object's free() method before exiting push(), which will put the instance on a free list. The next call of the consructor returns an object from that list, which will keep the code free of garbage. The list manipulations still introduces some processing overhead, of course.\nPR #637 extends the framework to allow immediate \"recycling\" of an object instance by promoting the standard constructors (new, new_from_mem) to instance methods. Calling new on an existing object is equivalent to calling free followed by a call to the new class method but avoids going through the free list.\nThe upshot is that you can pre-allocate a protocol object and use it in the push loop with fairly low overhead.\nHere's an example from my current tunneling code:\nhttps://github.com/alexandergall/snabbswitch/blob/vpn/src/program/l2vpn/tunnels/l2tpv3.lua#L40\nAlso note that I don't use parse() and pop() for decapsulation but pop_raw for efficiency, because in this case I already know the type of header.\n. The new method (from PR #637) should perform better than free(). Maybe you want to give that a try. I think my app went from ~1Mpps to ~6Mpps with this method and some other optimisations (like pop_raw vs parse``pop). If you want ultimate speed, nothing beats twiddling the bits directly, of course :)\n. The destination address in vpws.lua is just a dummy value that should never appear on the wire. It will be overweritten either by a statically configured address or the address discovered by nd_light.\nAnyway, vpws.lua will be replaced by the l2vpn program that I'll push to the vpn branch soon.\n. @andywingo @lukego I would certainly appreciate if I can replace my MAC table hack with this. Unfortunately, I don't have the time right now (need to finish the NixOS packaging first). I haven't looked at the code either, but curious how this affects the issue I was having with \"branchy loops\" in my bridge app. If someone volunteered to write a performant bridge app for me, I wouldn't mind :) \n. @andywingo Thanks for the explanation. That sounds terrific and should eliminate a big part of the problem. There was a branchy loop in the bridge code as well (determining whether to unicast or flood a packet). I might come back to you for advice how to eliminate that one, too. And, yes, 50ns/lookup is nothing I would complain about :) My performance goal for the entire VPN app is fairly moderate, something like 6-8Mpps. But getting faster is always nice. \n. @lukego The clock on my system is 3.5GHz.\n. I agree (and pledge guilty for having abused \"class methods\" for non-object related functions). I see your point concerning the constructor, too. That could be easily implemented with lib.lua.class as well (just a pain to change all the modules that currently use the method style, but that's on me).\n. Maybe it's helpful to the discussion if I explain my motivation to go that particular path in the lib.protocol code. It was obvious to me that there will be lots of opportunities for code-reuse in the handling of protocol headers and the class model with actual inheritance seemed like a good fit. I'd be curious how people who don't like this approach would have written that code. I don't have any strong feelings towards any style, but I do care a lot about re-using code.\nAFAICS, there are currently not many other parts of the Snabb codebase that would benefit from a non-trivial class hierarchy, but I do observe that at least the intel10g app uses inheritance internally and I actually find that code hard to read because the hierarchy is not obvious. I think that a more modular approach for writing drivers would be beneficial in general, e.g. separating generic from hardware-specific code. That might give us a better idea about what coding style to prefer.\n. Yes, that was a dead-end (and, yes, replacing my mac_table with ctable is on the to-do list).\n. A bit late to the party :) @mwiget I've added that code a long time ago: https://github.com/snabbco/snabb/blob/master/src/apps/intel/intel10g.lua#L202\n. @eugeneia wrt if-index: I use a static mapping of PCI addresses to if-index (or ifIndex in SNMP-speak) for my SNMP agent, see https://github.com/alexandergall/snabb-snmp-subagent/blob/master/README.md#interface-subagent\nI suggest you do the same.\n. Ok, to be more precise then: in SNMP, the object ifDescr contains a textual identifier of an interface (e.g. \"GigabitEthernet2/1\" on a Cisco device, i use the PCI address for a physical interface). The mapping is from that name to the index. There must be an analogue to ifDescr in the YANG model.\n. @eugeneia I can't comment much on the specific issue wrt VMDq stats. From the perspective of monitoring via NC/YANG or SNMP, it seems clear that every interface (irrespective of whether it's hardware or virtual) should be exposed by name/index (the primary handle for monitoring being the name).\nI'm not sure how useful it is to be able to link a virtual interface to a physical one.  If you do have dedicated counters, you expose them through the stats of the virtual interface and if you don't have them, the counters of the physical interface are more or less useless. At least with SNMP, there is a mechanism for this kind of mapping with the ifStackTable, though this is rarely used in practice. I don't know if there is a corresponding YANG model.\nI must confess that I didn't follow this PR or #696 closely, so I applogize for not knowing what exactly has been discussed.\nI would prefer if we had a more generic framework for exposing data for the purpose of monitoring which is not biased towards any particular northbound interface. I'm sure that you can't assume any kind of mapping between YANG models and SNMP MIBs in general. That would make it necessary to have another layer that would transform the raw data to something specific for each protocol.\nFor example, the lib.ipc.mib module is aware of data types specific to SNMP and this is part of the API for external programs that will consume this data (for example my SNMP subagents or snabb-top). Which brings me to your point 2. How much of lib.ipc.mib do you plan to support with this replacement? It's not just 64-bit counters and there is some tricky stuff related to time stamps in SNMP, see https://github.com/alexandergall/snabb-snmp-subagent#handling-of-the-snmp-timeticks-type.\nAlso, just getting the fields from RFC7223 wouldn't make me happy either. Not all of ifTable, ifXTable is mapped to objects in the YANG model.  In particular, ifDesc is specifically excluded, but it is the de-facto standard field for interface names in many devices (Cisco for example), even if that is not RFC-compliant.\nAlso, I use lib.ipc.mib to manage other MIBs as well. May I ask you to keep lib.ipc until we have sorted this out? You can mark it as deprecated, so nobody uses it for new code.\n. @eugeneia I basically agree but I'm not sure about the restriction to uint64_t and that everything is called a \"counter\". For example, timestamps, the physical MAC address or the operational status do fit into a uin64_t but they are not really counters. I find it confusing if they are labelled as such. Also, let's assume that I want to populate the ifName, ifDescr and ifAlias SNMP objects, which are strings and can all be different. How would I expose those? Through another branch in /var/run/snabb/<pid>? Which one?\nI guess the question is related to how we want to organize the /var/run/snabb/<pid> directory (appologies if this has already been discussed and I missed it).  I think I understand that you would like to treat interfaces controlled by a process equal to links between apps within that process at least wrt packet counters, but an interface is a more complex thing and it's also \"system-wide unique\". For my particular application, I don't even care which process controls the interface and it is easier to refer to a constant location, e.g., /var/run/snabb/interfaces/<name> (actually /var/lib/snabb/shmem in my current implementation) rather than having to track changes of the PID.\n. > I don't want to restrict the available shmem data types to uint64_t, there will definitely be more data types in the future such as @lukego\u2019s timeline. What I don't want to do is to implement data types like int32_t, gauge32, datetime, macaddr_t on the storage level when they all easily fit a data type that is already implemented.\nI agree with that. What I'm saying is that we need something other than uint64_t even now (at least a sequence of bytes of variable size to store, for example, a string) and that labelling everything stored in a uint64_t as \"counter\" is a mistake.\n\n\nAlso, let's assume that I want to populate the ifName, ifDescr and ifAlias SNMP objects\n\nRight now, just like higherLayerIf I suppose.\n\nWhich is done... how?\n\n\nI guess the question is related to how we want to organize the /var/run/snabb/ directory\n\nI am absolutely not set on the layout of /var/run/snabb. @lukego doesn't like that we reference instances by pid and I am starting to agree. The layout used currently will break down once apps can have shmem objects other than counters. So it will definitely change but I am not thinking about that too much right now because that's future problems.\n\nAgain: we already live in that future if we stop calling everything that fits into a uint64_t a counter :) I think that we should try to find a more flexible naming convention for /var/run/snabb now rather than using one which we already know will have to be changed. It shouldn't be too hard (famous last words :)\n. > This takes the SNMP code from intel10g.lua and moves it into its own module (using a common interface for app counters). Haven't tested it but theoretically init_snmp should work with other apps such as vhost_user as well. The caveat is that it is not automatically started by the intel app but must be manually started for a given app. This is a sketch.\nSomething like this would certainly make sense. I don't worry about starting a separate process for this outside the Snabb framework (for my appliance, I would simply add another systemd serivce).\n\nAre there any data types we need to support besides uint64_t and byte strings? (digression: I am OK with the core.counter API for dealing with uint64_t, e.g. as an interface that has set, add and read. For some reason the name \u201ccounter\u201d doesn't bother me much.)\n\nAn array of uint64s might make sense. For example, I'm thinking about exposing the MAC table of the bridge app via the BRIDGE-MIB. Using separate \"counters\" for this could be excessive for large tables. The number of entries in the table would be determined by the size of the file.\nConcerning the name \"counter\": why don't we simply call it uint64 (or just int or something), because that's what it actually is?\n\nThe upside of the current layout is that readers of the shm directory can determine the type of a memory mapped object based on the layout. (That was my original motivation for this layout: to avoid having to encode types in the objects or map object names to types.)\nThe downside is that the directory structure is scattered. E.g. not a single directory containing all resources managed by an app.\nAlternative ideas welcome!\n\nA simple alternative would be to add the type as an extension to the object's name. e.g. Foo_NIC/txpackets.counter (or Foo_NIC/txpackets.uint64 :)\nIf we want different views of the same data (per app, per object etc.) we could consider creating a hierarchy in /var/run/snabb consisting of symlinks to the actual data.\n. > The current code (merged in #931) is actually written to run in the same process. I aimed for a drop-in replacement.\nNice. I suppose I would then do something like\ninit_snmp(..., engine.app_table[Intel82599].counters, ...)\nbefore running the engine?\n\n\nConcerning the name \"counter\": why don't we simply call it uint64 (or just int or something), because that's what it actually is?\n\nFrom my point of view uint64_t is the data type while core.counter is a specific API including fancy things like double buffering. I would argue core.counter is 90% a problem specific API and only 10% a uint64_t.\n\nAnother approach would be to implement a API for storing a simple number (e.g. the interface administrative status) which doesn't need any of the fancy stuff and specialize for the semantics of a counter where you need it. Probably violates the KISS principle :) Anyway, it's not terribly important and I'll stop arguing about it.\n. @eugeneia yes. I guess we need a self-test for the flooding bridge, too.\n. Can you guys please also consider https://github.com/eugeneia/snabb/pull/14? Sorry, I'm losing track of who merges what from whom :)\n. The new VPN code will remain on the l2vpn topic branch (in that sense, the replacement is already there and has been so for a long time). You can phase out the old vpws code whenever you like.\n. I believe, in your first version, you did not store the packet in the object's pkt slot when the loop was left via the return. Adding self.pkt = p before the return would have fix it as well, I suppose (or use goto to break out of the loop instead of return).\n. Can you elaborate why you believe your fix is simpler than changing just the check to\nif cast(bit.tobit(uint32_ptr_t, payload)[0]) ~= bit.tobit(tag) then\nThe tobit on the rhs is redundant, but it seems safer to make this symmetric in case a future change would make tag to no longer be the result of a BitOp.\n. Are you sure those are the only things that break? it could be that other cases simply don't excercise the case where the difference matters (i.e. use numbers < 0x80000000), couldn't it?\n. Unfortunately, I don't really remember any of this, but doesn't tonumber involve a conversion to a double that you'd like to avoid in many (most?) cases? That would be something to consider, I guess. This point would be moot if LuaJIT optimized the conversion away when it would matter, which may well be the case :)\nOne could argue that the same issue exists for anything that involves BitOps and to make this clear in the description of the endian conversion API, which would have to document how to use it safely.\n. Just out of curiosity: would\nfunction htonl(b) return band(0xFFFFFFFF, bswap(b)) end\nhave made any difference (unless I'm overlooking a semantic difference, in which case please ignore)?\n. As you've already diagnosed yourself, you're trying to send a packet on the raw socket that exceeds the MTU of the underlying device. I don't know where that packet is coming from but this looks like an invalid setup, i.e. you're effectively connecting two layer-2 domains with different MTUs. The only proper solution is to make sure that the MTU matches on all interfaces that are part of your app network.. OK, I actually remember this gotcha now :) The problem is the \"TCP segmentation offloading\" feature on the ingress interface, which is usually enabled by default and causes the NIC to coalesce multiple TCP segments into a single large segment, which then exceeds the MTU. Try this to fix it:\nethtool -K wlp1s0 tso off\nethtool -K enp0s31f6 tso off\nThat should be applied to any interface which is used by Snabb via raw sockets (and should probably be documented).. I'm sure that offloading is the issue here (you can confirm that easily by capturing packets with tcpdump). You may have to disable the generic offloading features as well (gro off, gso off).. The problem is that the data packet is smaller than the size of the protocol header, so it's impossible to parse. Your app is expecting to receive an ethernet frame (inferring from the name eth_pkt) but is getting something that can't possibly be one.. @raj2569 as @lukego says, I have working SNMP support to expose the ifTable and some other MIBs in my l2vpn program (currently maintained on the l2vpn branch, soon to be merged https://github.com/snabbco/snabb/tree/l2vpn/src/program/l2vpn). The code predates the current shm implementation and uses a slightly different approach based on the stuff in lib/ipc/shmem, which is a bit awkward. I'm planning to replace it with a thinner layer based on the standard shm, so it's probably not a good idea to look at that code right now :(\nIt basically creates raw data items for all MIB objects in a special-purpose shared memory segment (e.g. from the standard counters provided by the NIC drivers). Those segments are read by https://github.com/alexandergall/snabb-snmp-subagent, which uses the AgentX protocol to hook into a standard SNMP daemon (e.g. snmpd provided by NetSNMP).  Documentation is poor, please drop me a line if you're interested.. The base new_from_mem method from the header class has always had this assert and would never return nil, but extensions of new_from_mem from derived classes do return nil in some cases, e.g. the gre class when the header contains unsupported features. Historically, this is why the check for nil is present in parse_match, but it is also clearly wrong to distinguish the cases when calling the base method or an extended method.\nSo, I think this change should go forward, but please also fix the code where the check for nil is missing (in particular, in extended methods like that from the gre class).. This looks good (wrt lib.protocol). 22fa8c8: In some instances, the code might have been save already due to previous consistency checks but asserts are cheap and probably more robust in case such pre-conditions are no longer met in future changes.\nI believe this PR is ready to be merged.. Actually, the second use-case could not be rewritten like the first one, because it would require output links to be present when input links are processed. I don't see how this could be done without a post_link-style method.. Suggested post_link method in PR #1188. Ok. Unfortunately, apps/config is terra incognita for me (still living in the stone age of app configuration).. When the frame is created, the counters are set to their default values, hence the original\ncounter.set(self.shm.mtu, self.mtu)\nin new() has no effect. The patch sets the default value instead.\n. Can we please proceed with this PR?. Thanks. So can someone please merge?. Any plans to fix this in LuaJIT or are we waiting for the transition to RaptorJIT?. @petebristow can you have a look at this, please?. 0x6558 is a perfectly fine Ethertype (coincidentally, it's listed explicitly in the original GRE specification RFC1701), just like 0x8100 and 0x86dd and everything else on https://www.iana.org/assignments/ieee-802-numbers/ieee-802-numbers.xhtml i.e. the list of the latter two is not the \"correct list\". The class_map only encodes what the current implementation of the protocol framework can decode automatically. It doesn't imply that any other values are filtered in any way. So, adding 0x8100 and 0x86dd is, of course, fine, but don't remove the existing mapping. The title of this PR should be changed as well :). Here is a case where calling stop() from main.shutdown would be useful. I have a bunch of basically identical processes running on separate RSS queues. Each process is run as a separate systemd service. When a service is started, RSS is set up properly and the process is getting its share of traffic. But when it is terminated with systemctl stop, the RSS queue remains active and I'm losing traffic. Calling stop() in that case would clean up RSS and re-direct all traffic to the remaining instances. . I just gave it a quick try. It seems like packet frees are double-counted on the sending side:\n```\n Kfrees/s        freeGbytes/s    breaths/s    \n 7618.53         0.98            86200          \nLinks (rx/tx/txdrop in Mpps)    rx      tx      rxGb    txGb    txdrop \n queue                           3.80    3.80    0.98    0.98    0.00 \n input1.output -> rss.vlan1      3.80    3.80    0.98    0.98    0.00 \n rss.default_1 -> default_1.inpu 3.80    3.80    0.98    0.98    0.00 \n default                         3.80    3.80    0.98    0.98    0.00 \n. A preliminary benchmark with my test case yields significantly lower performance compared to #1186, maybe around 20-30% in terms of CPU cycles. At a first glance, a lot of time is spent in the loop of `packet.allocate()`\n      while freelist_nfree(group_fl) > 0\n      and freelist_nfree(packets_fl) < packets_allocated do\n         free(freelist_remove(group_fl))\n      end\n``\nIt's a pretty compact root trace but top of the list ofLLC-load-missesand second incyclesin terms ofperfmetrics.. This fix brings performance to about the same level as #1186 for my test case. The suspicious trace from my previous comment has become insignificant wrt performance.. I think @petebristow was concerned that if the unbinding is done without obtaining the lock first, it could create some kind of collision between concurrent Snabb processes. I don't know if that's an issue.. I agree. The statement I had in mind was this  https://snabb.slack.com/archives/C0H3TPBV2/p1518174897000123 I guess he didn't suggest that there is an actual problem.. This appears to lead to a race condition. If I start multiple Snabb instances concurrently, only one runs successfully, the others die with the symptom of #1286. Adding aC.sleep(1)after the call tounbind_device_from_linux()`, this problem goes away.\nI don't know how exactly the unbinding is handled by the kernel. I suppose it's synchronous for the first process (otherwise that one would see the same problem). The other processes check for the writeability of /sys/bus/pci/devices/<address>/driver/unbind. The driver subdirectory is removed when the device is unbound. But if that happens before the actual unbinding, the process will attempt to map the memory immediately and fail. \nI guess the question is if there is a reliable check whether a device is bound or not.\n. I guess my suspicion was correct. I still don't know what the best method is, but for now, I'm using this hack:\nfunction map_pci_memory (device, n, lock, wait)\n   assert(lock == true or lock == false, \"Explicit lock status required\")\n   root_check()\n   local filepath = path(device)..\"/resource\"..n\n   local f,err  = S.open(filepath, \"rdwr, sync\")\n   assert(f, \"failed to open resource \" .. filepath .. \": \" .. tostring(err))\n   if lock then\n     assert(f:flock(\"ex, nb\"), \"failed to lock \" .. filepath)\n   end\n   local st = assert(f:stat())\n   local mem, err = f:mmap(nil, st.size, \"read, write\", \"shared\", 0)\n   if wait then\n      while err and err.errno == const.E.INVAL do\n         mem, err = f:mmap(nil, st.size, \"read, write\", \"shared\", 0)\n      end\n   end\n   assert(mem)\n   return ffi.cast(\"uint32_t *\", mem), f\nend\nThe precondition for using the new wait flag is that the caller has done a unbind_device_from_linux(). One should probably use a timeout as well.. Argh, and there is yet another race condition in pci.device_info. The net subdirectory vanishes when the device is unbound. So, in the code\ninfo.interface = lib.firstfile(p..\"/net\")\n      if info.interface then\n         info.status = lib.firstline(p..\"/net/\"..info.interface..\"/operstate\")\n      end\nif the unbinding happens after the lib.firstfile call but before the lib.firstline call, the latter will fail. This just happened to me.. But if you release the lock you can't use it to determine who's the master, can you?. Still unsolved I believe. Haven't looked at it since then.. Yes to both, though I don't know what exactly is required for the rate limiter of lwaftr.. I'd prefer to do the logger stuff in separate PRs, if that was your implication. That would mean that we have a temporary dependency of core.lib to lib.token_bucket until that's sorted out. Would that be a problem?. Making this optional is probably not a good idea.. The statement about pause frames sent the unicast address is probably wrong. The issue is whether the NIC is supposed to pass MAC control frames to the host or not, which is controlled by the DPF bit of the MFLCN register. This bit should be set by the driver by default when FC is enabled.. I think this causes a regression with at least the load method of snabb config. I'm using a schema which contains a list that is parsed to a Lua table instead of a cltable with this PR. When I try to load an instance of it with snabb config load I get\nWARN: lib/cltable.lua:38: attempt to index field 'keys' (a nil value)\nI traced this to lib.ptree.support.add_child_objects(). The grammar that's passed to this function marks the list in question to be a cltable while the config is a Lua table.\n. Steps to reproduce. Create the files\nsnabb-pf-v1.yang\n```\nmodule snabb-pf-v1 {\n  namespace snabb:pf-v1;\n  prefix pf-v1;\nlist foo {\n    key \"id\";\nleaf id {\n  type uint8;\n}\nleaf data {\n  type string;\n}\n\n}\n}\n```\npf-v1.cfg\nfoo {\n  id 1;\n  data \"bar\"; \n}\nfoo {\n  id 2;\n  data \"baz\"; \n}\nsetup.lua\n```\nlocal basic_apps = require(\"apps.basic.basic_apps\")\nlocal app_graph = require('core.config')\nreturn function (conf)\n   local graph = app_graph.new()\n   app_graph.app(graph, \"source\", basic_apps.Source)\n   app_graph.app(graph, \"sink\", basic_apps.Sink)\n   app_graph.link(graph, \"source.output -> sink.input\")\n   return {foo=graph}\nend\nStart with\nsnabb ptree --name foo snabb-pf-v1.yang setup.lua pf-v1.cfg\nThen\n./snabb config load  pf-v1.cfg\npf-v1.cfg: loading compiled configuration from pf-v1.cfg.o\npf-v1.cfg: compiled configuration is up to date.\nlib/cltable.lua:38: attempt to index field 'keys' (a nil value)\n```\n. Pushed the changes suggested by @dpino. Ready to merge for me.. I used the same style as for other such definitions, e.g. https://github.com/snabbco/snabb/blob/8f7678224586746885e798c778da9a5b8f505bdc/src/apps/intel_mp/intel_mp.lua#L469\nI'm not sure if that was deliberate either (do we need to expose these to users of the module?). I have tested this modification with the l2vpn program in a production-like environment. The engine contained about 30 apps (20 different app classes) and about 60 links. In any realistic scenario, changes are very high that the system is idle after a regular start. I confirmed that it exhibits exactly the behavior described in this issue.\nThe \"flattening\" of the loops has reduced the noise of the compiler significantly, in particular when confronted with changing workloads (e.g. direction of bulk traffic from encap-heavy to decap-heavy, from ipv4 only to ipv6 only, from unfragmented to fragmented traffic). The observed effect is that there are much fewer aborted traces and much fewer implicit blacklistings of side traces. Performance has become much more stable across all workloads and \"catastrophic JIT failures\" have practically vanished. Those failures would result in highly suboptimal traces or even substantial interpreter fallback (>50%) when the system was exposed to a wide range of different workloads.\nI am confident enough to include this change in the next production release of l2vpn. I would be very much interested to see the effect this will have on other Snabb programs.\n. Looking a bit at the code it seems to me that the semantics of enumeration have just not been implemented yet. When enum_validator() is called, the substatements have been parsed into the array\n{ {\n    if_features = {},\n    kind = \"enum\",\n    name = \"foo\",\n    value = 1\n  }, {\n    if_features = {},\n    kind = \"enum\",\n    name = \"bar\",\n    value = 2\n  } }\nThe value from the configuration is then used as a key into this array, which is completely bogus. What should happen is that the missing values are assigned according to 9.6.4.2 of RFC6020 (in this example they are set explicitely but that's optional) and the value from the configuration needs to be compared with the name attribute of the enums. The first part probably needs to be done somewhere else.\n. I have implemented this here https://github.com/alexandergall/snabbswitch/tree/yang-enumeration. I tried to guess where the code should go :) Can you please have a look (I'm also unsure about the style of error messages this should generate)?\nI'm not sure whether it's actually useful to set the numeric value in the final Lua table or the name of the selected enum. My code currently does the latter, i.e. in the example above, the result is\n{ test = \"foo\" }\nThis seems more useful since otherwise the Lua code processing this table needs to define the same name-to-value mapping as the schema.\n. Proposed fix: https://github.com/snabbco/snabb/pull/1416. In this case, the traffic going north->south is passed through untouched. Would it make sense to bypass the app in that direction to avoid the overhead?\n. For this case, I think a simple benchmark could be created with Source->vpws->Sink because the actual contents of the packet is irrelevant for encapsulation.\n. Ah, good you mention this.  I spent some time pondering how to do this right. Bit fields are compiler-dependent by nature, which makes it hard to write portable code. OTOH, we're bound to a specific compiler, so this is probably not much of an issue. Endianness with bit fields is a PITA, though and I believe that there are some limitations on the way those fields can (or rather can't) cross certain byte-boundaries.\nSo, I decided to manage bit fields with the bit library (which is supported by JIT, I believe), see core.lib.bitfield(). I obviously forgot the stuff in na.lua :)  I'd be interested in what you think about this approach.\n. Can somebody please enlighten me how to make the profiler produce any output at all? Using -jp stalls the process and nothing works. strace shows \nclone(child_stack=0, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0x7f2b01ef29f0) = ? ERESTARTNOINTR (To be restarted)\n--- SIGPROF (Profiling timer expired) @ 0 (0) \nWhen I use a bigger sampling interval, e.g. i100, the app runs but I get no output from the profiler.\n. Yes.  I'm a bit worried about the way I'm mapping Lua objects to packet memory, though.  An expert review of my ffi.cast-ing would certainly be appreciated :) \nSome parts are clearly suboptimal, like the header:clone() thing combined with datagram:push(), which actually copies the header twice.\nAll in all, I'm not perfectly happy with this solution.\n. Weird. This doesn't work at all for me in my VPN lab. I'll try to apply it to the simple Source->vpws->Sink case.\n. Ok, so the profiler has a problem with the call to scan_devices() in core.app.module_init() on my system. I'll skip this for now and worry about it later.\n. My analysis was wrong. It's the io.popen() call in core.lib.files_in_directory() when called from lib.hardware.bus.host_has_vfio() that causes the profiler to hang. Any idea why?\n. strace shows a tight loop of the clone() call I posted before followed by rt_sigreturn() with \"--- SIGPROF (Profiling timer expired) @ 0 (0) ---\" in between.\nI can get around this by increasing the sampling interval, but then I simply don't get any output from the profiler. If I disable the call to scan_devices() as well as the io.popen(), there is no clone() stuff but the profiler still doesn't say peep. I'll try this on another host to see if it makes any difference. Confused :/\n. I haven't requestd access to your lab yet. I'll do that tomorrow and see if this is reproducible there.\n. I was thinking along similar lines. The 4K boundary requirement for Virtio is interesting. It seems desireable to have a generic framework that does not need to know about requirements of any particular network device, e.g.  with a kind of \"shim\" layer that transforms the raw packet buffers into a standard format (and vice versa). This was sort of the idea of my datagram class.\nDoes this Intel \"split\" feature have any kind of intelligence or does it simply map a fixed number of bytes into a separate buffer?\nThe system should also be able to deal with non-standard packets (like very large headers) with an \"acceptable\" performance penalty. \n. The issue with the hung process is not there on chur (which I'd also like to understand), but the profiler still doesn't produce any output:\ngall@chur:~/projects/snabbswitch/src$ sudo ./snabbswitch -jp test.vpws-perf\nlink report\nsource.tx->vpn.customer 100,000 packet(s) transmitted\nvpn.uplink->sink.rx     100,000 packet(s) transmitted\nMust be something silly...\n. Ok, using require(\"jit.p\").start() in my code works. Just not on the command line. Oh, well.\n. No, I have merged with master not long ago (unless my git foo failed me ;) But I'm happy for now with this workaround.\n. You should also check for \"End Of Packet\", i.e.\n  assert(bit.band(wb.xstatus_xerror, 3) == 3) \nOtherwise, multi-buffer packets will be interpreted as separate packets.  It would also be fairly trivial to support multi-buffer packets here, just like in transmit()\n. That was an ill-fated attempt to subclass TapVhost in a simple-minded manner. I don't want to rewrite it and it's probably best to remove the entire RawVhost thing. I never used it anyway (RawSocket, OTOH, is useful for debugging, because you can attach wireshark to such an interface).\n. Darn, yes. Thanks.\n. On Thu, Jan 22, 2015 at 5:51 PM, Luke Gorrie notifications@github.com wrote:\n\nIn src/lib/ipc/shmem/shmem.c:\n\n+\n+bool shmem_unmap(void mem, size_t size) {\n-  if (munmap(mem, size) == -1) {\n-    perror(\"munmap\");\n-    return(false);\n-  }\n-  return(true);\n  +}\n  +\n  +// Note: the io.* file handle passed to us is converted to a Unix\n  +// filehandle (FILE ) by LuaJIT\n  +char shmem_grow(void fh, void old_mem, size_t old_size, size_t new_size) {\n-  int fd;\n-  void mem;\n  +\n-  if (old_mem != NULL) {\n\nIs this function vulnerable to creating dangling pointers due to remapping memory?\n\nYes. That's why the shmem class keeps the locations of objects as an\noffset relative\nto the base address. Also see the comment of the base() method.\nNow, this makes the return value of the register() method rather\nuseless :(  I've added\nthat as an afterthought to implement the semantics of your original\ncounter.lua code.\nClearly, I did not think this through.\n\nIf so, could we fix that?\nExample:\nLua code calls counter module and gets the address of a counter for repeated use.\nmunmap() invalidates the old mapping and then mmap() returns a different address (for whatever reason).\nLua code tries to use the remembered counter value and causes a SIGSEGV because it is no longer mapped.\nIs this a valid error scenario? (I do like the idea of counters having stable virtual addresses: then I expect they can be incremented very cheaply indeed and so we need not be shy of doing so in the fast path).\n\nFor counters, we could simply register all counters first, then use\nthe base() method to\ncreate a pointer to the array of counters and use indices to access\nthem. The indices could\neven be used as names through an \"enum\"-style table. This would all be\nresolved at compile\ntime and should be as efficient as using pointers with the current code.\nThat wouldn't work for segments with mixed types and the return value\nof register() would still\nbe unsafe to use.\n\nIf indeed this would be a problem, could we solve it simply by skipping the munmap()? Then even if mmap() happened to choose a new address then all previous mappings would remain valid as separate mappings?\n\nDoesn't mmap() necessarily create a new mapping when the old one isn't\nmunmap()-ed? Not sure.\nI wonder how likely it is for mmap() to chose a new address when\nexplicitely asking for\na mapping to the same address. We'd still have to deal with it, even\nit is unikely.\n. On Thu, Jan 22, 2015 at 6:12 PM, Luke Gorrie notifications@github.com wrote:\n\nIn src/lib/ipc/shmem/shmem.lua:\n\n+--\n+-- Describes a memory region of length 11, which contains an object\n+-- named 'foo' that consists of 4 bytes starting at offset 0 and an\n+-- object named 'bar' consisting of 7 bytes starting at offset 4.  The\n+-- type of the object is implied by its name and is not part of the\n+-- description in the index. Each name must be unique.\n+--\n+-- An object is added to the region by calling the method register(),\n+-- which takes a string, a ctype object and an optional value as\n+-- arguments.  The ctype must refer to a complete type such that the\n+-- size of the object is fixed when it is added to the index.  The\n+-- following example adds an unsigned 32-bit number called \"counter\"\n+-- and a struct named \"bar\":\n+--\n+--  local counter_t = ffi.typeof(\"uint32_t\")\n+--  local bar_t = ffi.typeof(\"struct { uint8_t x; char string[10]; }\")\n\nI really like the generality of this design. Great that it is so easy to store structured data instead of single scalar values, and that the way this data is interpreted is flexible and open. (This data structure is starting to remind me of Forth's Dictionary.)\n\nReminds me of a Forth extension module I once had for my C64 :)  I did\nsome programming in Forth, but forgot most of it in the 30+ years\nsince :(\n\nHow do ensure that readers see a consistent value? The counter module depends on x86_64 loading and storing 64-bit values atomically so that reads and writes won't conflict. Do we need some kind of protocol to extend this to structured data, in case a write happens part-way through a read? (I wonder what would work if so.. a double buffer?)\n\nI thought about that. My initial idea is to use semaphores. This could\nbe a named semaphore or an unnamed one that is placed directly in the\nshared memory. I'm not sure about the performance implications, but\nimplementation would be simple, I guess.\n. Oops, that was a little simple-minded by me. It will also not be reflected in #actions.restart. I guess the proper way to clear the array is to traverse it and set all elements to nil.\nI'm sure this is a Lua FAQ and I fell right into the trap :) \n. I would favour the traversal method, but maybe we shouldn't bother at all as long as that logic is bypassed when use_restart is false. The dead-app detection code has other performance issues that need to be addressed at some point.\n. This change has broken the packetpblaster program, which checks for the driver apps.intel.intel10g explicitely.\n. Yes, indeed. Thanks!\n. Yes, it was just moved after the definition of the endian-conversion stuff to avoid forward declarations.\n. I guess you're right.\n. Well, the endian-conversion did change, of course.\n. Ok, thanks. Maybe one day I'll even understand why BitOps using signed integers is considered a good thing in the first place :)\nI'll remove this function and call bit.tobit explicitely when it's needed.\n. I don't think that a single bit.tobit() justifies a wrapper function any more. My original method was cumbersome enough to make it seem useful.\nI've actually known the interoperation argument before, but I haven't really understood why the cross-platform problem can't be solved without exposing a counter-intuitive API. Oh, well :)\n. I didn't bother because ICMP is not particularly performance critical.\n. > Why is this a multiple of kilobits?\nI guess this has sneaked in from my faulty SNMP code. For SNMP, there are two objects: ifSpeed and ifHighSpeed. The correct values for 10Gbps are ifSpeed = 4294967295 and ifHighSpeed = 10000 (RFC3635). My code used ifSpeed = 10000000, which is wrong and made it to here. The speed counter has nothing to do with SNMP and should say 10000000000 (hope I counted that right :). \n. I'm not sure this is a problem in practice (it will occur after something like 10 years of processing 14Mpps or something, I guess) but, sure, using a \"box\" for that would be safe. BTW, why exactly are we keeping lastfreebytes and lastfreebits? They're not used anywhere. . Ok, I assume that % optimizes for this case?. Note that this is from the original code (I only added a do...end to limit the scope of locals). This would be a cosmetic change, I suppose?. I'd have expected that this would be taken care of by the optimizer, but if it's not, that change would certainly be a good idea.. I just checked and the compiler does indeed specialize to bitops if the module is a power of 2.. It's a uint8_t.. ",
    "krisk84": "Will do, thanks!\nOn Thu, Feb 27, 2014 at 3:36 AM, Luke Gorrie notifications@github.comwrote:\n\nYou should now be able to ssh krisk84@chur.snabb.co. Let me know if you\nhave trouble!\n\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/pull/91#issuecomment-36220815\n.\n\n\nKristian Kielhofner\n. ",
    "altexy": "Implemented as rate_limiter app, should be closed.\n. Implemented as packet_filter app.\nPacket Filter still has some issue, but this ticket should be closed.\n. From lukego:\n\nThis is a minimalistic implementation of the same function that\nAlex Gall has been working on. (I met with Alex yesterday, I don't\nthink he plans to optimize his app to the level that I need, so I'd like to make a separate one.)\nThe App would have an input/output called \"encapsulated\" and an\ninput/output called \"decapsulated\".\nThe \"decapsulated\" link receives raw Ethernet packets. Each of\nthese packets is prepended with a simple IPv6/L2TPv3 header and\ntransmitted on the \"encapsulated\" link.\nThe \"encapsulated\" link does the reverse: it receives IPv6/L2TPv3\npackets, strips off that header, and transmits the remaining raw\nEthernet onto the \"decapsulated\" link.\n\nfrom http://tools.ietf.org/html/draft-mkonstan-keyed-ipv6-tunnel-01:\n\nRFC4719 [RFC4719] describes encapsulation of Ethernet over L2TPv3.\nParaphrasing from this document, the Ethernet frame, without the\npreamble or frame check sequence (FCS), is encapsulated in L2TPv3 and\nis sent as a single packet by the ingress router.\n\nShould I remove any \"preamble\" during encapsulation?\nThe same for other direction, should I prepand such preamble?\nHow can I test this SnS app to be sure i'm doing right things?\nPlease provide me with any hint.\nI see next configuration parameters:\nlocal IPv6 address\nremote IPv6 address\nCookie\nSession ID, optional\nIt should be possible to use zero-copy approach in both direction.\nShould I check before decapsulation that packet contains valid fields:\nVersion, Next header, Source and Destination addresses?\nRegards\nAlexander Altshuler\n. Thanks Luke,\nI will proceed tonight.\n. One small detail:\nSessions ID/Cookie should be specified separately for both directions.\n. Hi Luke\nI have first working version of tunnel passed encap-decap test.\nEncapsulated traces looks well, although Wireshark cannot parse it.\nI hope Monday I will send you pull-request.\nI will still need some work with IPv6 part.\nOne thing is not clear for me in tunnel itself - should we specify Session ID or not.\n\nSession ID.  In the \"Static 1:1 mapping\" case described in\n Section 2, the IPv6 address resolves to an L2TPv3 session\n immediately, thus the Session ID may be ignored upon receipt.  For\n compatibility with other tunnel termination platforms supporting\n only 2-stage resolution (IPv6 Address + Session ID), this\n specification recommends supporting explicit configuration of\n Session ID to any value other than zero.  For cases where both\n tunnel endpoints support one-stage resolution (IPv6 Address only),\n this specification recommends setting the Session ID to all ones\n for easy identification in case of troubleshooting.  The Session\n ID of zero MUST NOT be used, as it is reserved for use by L2TP\n control messages RFC3931 [RFC3931].\n\nCurrently I set it to \"all ones\" before tx and ignore upon rx.\nYou wrote that you have access to Cisco router with custom firmware which support this tunnel draft.\nCan you ask about tunnel configuration example?\nIt may give me a clue before we will do real interoperability test.\nAlexander\n. Yes, I will close it immediately.\n. At the moment selftests code are mixed with library code.\nSo it is installed with snabbswitch executable.\nBut some test require also test data.\nAt the moment it is not simple to distinguish them.\nI have create a new issue about - https://github.com/SnabbCo/snabbswitch/issues/124\n. The simplest way is  to run any test which does not require test data, rate_limiter for example:\nsudo snabbswitch -t apps.rate_limiter.rate_limiter\nEnd of rate_limiter selftest FYI:\nLua\n if not ok then\n      print(\"selftest failed\")\n      os.exit(1)\n   end\n   print(\"selftest passed\")\n. @vnaum, is it enough info to proceed?\n. alex@ubuntu:~$ uname -a\nLinux ubuntu 3.11.0-18-generic #32-Ubuntu SMP Tue Feb 18 21:11:14 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\nalex@ubuntu:~$ gcc -v\nUsing built-in specs.\nCOLLECT_GCC=gcc\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/4.8/lto-wrapper\nTarget: x86_64-linux-gnu\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu/Linaro 4.8.1-10ubuntu9' --with-bugurl=file:///usr/share/doc/gcc-4.8/README.Bugs --enable-languages=c,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-4.8 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --with-gxx-include-dir=/usr/include/c++/4.8 --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-gnu-unique-object --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\nThread model: posix\ngcc version 4.8.1 (Ubuntu/Linaro 4.8.1-10ubuntu9) \nalex@ubuntu:~$ make -v\nGNU Make 3.81\nCopyright (C) 2006  Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.\nThere is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A\nPARTICULAR PURPOSE.\nThis program built for x86_64-pc-linux-gnu\nalex@ubuntu:~$ luajit -v\nLuaJIT 2.0.2 -- Copyright (C) 2005-2013 Mike Pall. http://luajit.org/\n. Sorry for that, the same with rate limiter, I will send you pull requests with missed files soon.\n. https://github.com/SnabbCo/snabbswitch/pull/132\n. ",
    "theo19": "Even if this is closed, it might be worth to add that in early 2017, the draft has eveolved to the standard \"Keyed IPv6 Tunnel\", see https://tools.ietf.org/html/rfc8159 \nIt is afaik also generally supported in linux as per the ip-l2tp command, http://man7.org/linux/man-pages/man8/ip-l2tp.8.html . I was looking into the DPDK site list of NICs (imagine how cool it would be if such a long list would exist for snabb ...) for those that support 100G, and think one more recent interesting addition - based on cost consideration - is obviously the entry \"15. QEDE Poll Mode Driver\" that refers to Qlogic FastLinQ QL45000 series (resold by some major server vendors) http://dpdk.org/doc/guides/nics/qede.html\nThe Qlogic site at http://driverdownloads.qlogic.com/QLogicDriverDownloads_UI/SearchByProduct.aspx?ProductCategory=325&Product=1263&Os=26 shows not just the drivers, but also allows to download detailed documentation dated late 2016: for the chipset register specification (approx. 350 pages) and a developers guide (approx. 130 pages). \nThe overall offering of real 100G NIC chips will probably keep on broadening - hopefully the industry reshuffling will also drive more players to open up their documentation.  Qlogic's steps after takeover by Cavium does certainly help. \nI am also curious on the outcome of another group, as Emulex was swallowed by Avago, who later took over Broadcom, but retained the brand name for the whole group as Broadcom (stock ticker is still AVGO). We should expect a pretty competitive 100G NIC from price performance point of view from them. A quite open question here is if Broadcom will remain rather locked up wrt. documentation as this is how they managed it in the past. Certainly Qlogic/Cavium, Mellanox and others have embraced the idea of maximising their usefulness in context of FOSS projects more than some others... the official website still shows no 100G NIC product, but IMHO it really can't take very much longer until they come out of the stealth mode with somthing like that.. ",
    "virtuallynathan": "Does this being close imply that Juno onwards does support Snabb NFV out of the box? \n. Definitely not a SnabbSwitch expert, but I've been trying to follow along. I would guess that making use of the PMU module: https://github.com/SnabbCo/snabbswitch/blob/master/src/lib/pmu.lua would allow you to see what the L1/L2/L3 cache hit ratio looks like as the execution ramps up, and things potentially start to be kept in one of the cache levels. \nCool idea for an app, by the way! \n. Interesting! I found these docs/files on the D-1500's PMU's from Intel: \n332427-001.pdf\nxeon-d-1500-uncore-performance-monitoring.zip\nBit over my head at the moment, but they may be of help to you or Luke. \n. Looks like the D-1500 (Broadwell-DE) lives at the bottom of the file: https://github.com/SnabbCo/snabbswitch/blob/master/src/lib/pmu_cpu.lua#L10510\nIt seems that L3 cache stats should exist:\n``` lua\n    [\"mem_load_uops_retired.l1_hit\"] = 0x01d1,\n    [\"mem_load_uops_retired.l2_hit\"] = 0x02d1,\n    [\"mem_load_uops_retired.l3_hit\"] = 0x04d1,\n    [\"mem_load_uops_retired.l1_miss\"] = 0x08d1,\n    [\"mem_load_uops_retired.l2_miss\"] = 0x10d1,\n    [\"mem_load_uops_retired.l3_miss\"] = 0x20d1,\n    [\"mem_load_uops_retired.hit_lfb\"] = 0x40d1,\n    [\"mem_load_uops_l3_hit_retired.xsnp_miss\"] = 0x01d2,\n    [\"mem_load_uops_l3_hit_retired.xsnp_hit\"] = 0x02d2,\n    [\"mem_load_uops_l3_hit_retired.xsnp_hitm\"] = 0x04d2,\n    [\"mem_load_uops_l3_hit_retired.xsnp_none\"] = 0x08d2,\n    [\"mem_load_uops_l3_miss_retired.local_dram\"] = 0x01d3\n```\n(TIL xsnp = cross-core snoop)\n. As far as other switches go, Mellanox Spectrum can do line-rate at all\npacket sizes. Based on their \"independent\" testing, it seems Broadcom's\nspec is not 100% accurate, see page 11:\nhttp://www.mellanox.com/related-docs/products/tolly-report-performance-evaluation-2016-march.pdf\nI haven't seen a number for Cavium Xpliant.\nOn Thu, Sep 8, 2016 at 2:51 AM Luke Gorrie notifications@github.com wrote:\n\nRelatedly: Nathan Owens pointed out to me via Twitter that the sexy\nBroadcom Tomahawk 32x100G switches only do line-rate with >= 250B packets.\nSeems to be confirmed on ipspace.net\nhttp://blog.ipspace.net/2015/12/broadcom-tomahawk-101.html.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/snabbco/snabb/issues/1013#issuecomment-245549003, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ADT7LazQEjat9-FbjcATRK1Oi_fHKjOZks5qn9qVgaJpZM4J2yWB\n.\n. Wonder if Mellanox has any ConnectX-5 cards available you could compare with for shits and giggles. \n. Other NIC Options:\nNetronome: https://www.netronome.com/media/redactor_files/PB_Agilio_Lx_1x100GbE.pdf\nCOMBO Cards: https://www.liberouter.org/technologies/cards/\nBittWare makes a bunch that can be flashed with NIC IP: http://www.alteraboards.com/products/?f=fpga-board-platform%2Cpcie&c=network-packet-processing%2Ccompute-data-center%2Chdl%2Cshipping&prod\n. The ENA driver is open source, and is in DPDK:\nhttps://github.com/amzn/amzn-drivers\nhttp://dpdk.org/doc/guides/nics/ena.html\n\nThe ENA now supports up to 25Gbps within a Placement Group, and is available with that performance on: m4.16xlarge, i3.16xlarge, and r4.16xlarge (and the more specialized G3.16xl, P2.16xl, X1.32xl/X1e.32xl, and F1.16xl instance types):\nhttps://aws.amazon.com/about-aws/whats-new/2017/09/announcing-improved-networking-performance-for-amazon-ec2-instances/\nENA is available on all instance sizes of: I3, R4, G3, P2, F1, X1, and m4.16xlarge. ENA will be available on the C5 instance family as well: https://aws.amazon.com/about-aws/whats-new/2016/11/coming-soon-amazon-ec2-c5-instances-the-next-generation-of-compute-optimized-instances/\nC3, C4, D2, I2, R3, and M4 (excluding m4.16xlarge) instances use the Intel 82599 VF interface for enhanced networking. Instances older than these do not have Enhanced Networking. \nMore detail on AWS networking: https://youtu.be/AyOAjFNPAbA?t=1825 (timestamp is about NICs, but the whole video is interesting)\nDisclaimer: I work at AWS (but I do not work on this). . There's some info in the README here:\nhttps://github.com/amzn/amzn-drivers/tree/master/kernel/linux/ena\nI'll see if there's anything else. . ",
    "thisisrmr": "The Neutron security group rule seems to be very basic:\nmysql> describe securitygrouprules ;\n+-------------------+--------------------------+------+-----+---------+-------+\n| Field             | Type                     | Null | Key | Default | Extra |\n+-------------------+--------------------------+------+-----+---------+-------+\n| tenant_id         | varchar(255)             | YES  |     | NULL    |       |\n| id                | varchar(36)              | NO   | PRI | NULL    |       |\n| security_group_id | varchar(36)              | NO   | MUL | NULL    |       |\n| remote_group_id   | varchar(36)              | YES  | MUL | NULL    |       |\n| direction         | enum('ingress','egress') | YES  |     | NULL    |       |\n| ethertype         | varchar(40)              | YES  |     | NULL    |       |\n| protocol          | varchar(40)              | YES  |     | NULL    |       |\n| port_range_min    | int(11)                  | YES  |     | NULL    |       |\n| port_range_max    | int(11)                  | YES  |     | NULL    |       |\n| remote_ip_prefix  | varchar(255)             | YES  |     | NULL    |       |\n+-------------------+--------------------------+------+-----+---------+-------+\nIt seems there is no way to specify SMAC/DMAC etc. Is that an issue?\n. On 13-Mar-2014, at 3:35 PM, Luke Gorrie notifications@github.com wrote:\n\nTo be consistent with normal OpenStack operation I believe we need to lock the VMs to only use the SMAC that has been assigned to them. I don't think we need SMAC/DMAC filtering beyond that.\n\nOK\n. Do you plan to have Tempest installed on chur?\n. Yeah I think running tempest inside a vm seems to be the best approach atm.\n. I'm curious to know why this particular approach is taken. Are there any issues with the approach I took with the mech driver[1] and agent[2] ?\n[1] https://bitbucket.org/thisisrmr/sns-neutron/src/a2550944e05b9e40dcfb6634e5281686ee9c7243/neutron/plugins/ml2/drivers/mech_snabbswitch.py?at=sns-qos-tunnel2 \n[2] https://bitbucket.org/thisisrmr/sns-neutron/src/a2550944e05b9e40dcfb6634e5281686ee9c7243/neutron/plugins/snabbswitch/agent/snabbswitch_neutron_agent.py?at=sns-qos-tunnel2\n. Hmm, the last time I tried to run without an agent, booting the vm failed at the scheduling stage (since the port was not reported \"up\" by a agent). I don't know if that check got removed from nova (btw are you sure you were not running an agent on the host?). \n. IIUC we need a neutron agent running on each compute node if things are to work properly. The snabbswitch_neutron_agent.py was modeled after the NEC neutron agent (the simplest among the lot) and all it does is report the list of available ports at a regular interval. The alternative is to depend on the linuxbridge agent (but in that case, you have to test against an agent that is even more complicated and is being modified a lot). IIRC there was some talk about neutron getting a more flexible agent (don't know if/when that's going to happen).\nIf you're concerned about RabbitMQ, all I can say is that there are much more ugly things happening under the hood - try grep-ing a port-id in the openstack logs :).\nAlso, in issue #191 you've mentioned having an agent for error and statistics reporting. what about that?\n. The 'snabbswitch_neutron_agent.py' script reports the current state of neutron ports (by checking the availability of corresponding vhostuser sockets).\nIs there anything else that needs to be reported?\n. Is this issue still valid?\n. Cool :-)\nIs the bandwidth-aware host selection (Nova scheduling) also working as expected?\n. Thanks for the link. Explains the current situation well.\n. ",
    "gongysh2004": "env:\nubuntu 13.04\nlua 5.1\ngongysh@gongysh-ThinkPad-T530:~/git/snabbswitch/src$ uname -a\nLinux gongysh-ThinkPad-T530 3.8.13.13 #1 SMP Tue Mar 4 12:18:12 CST 2014 x86_64 x86_64 x86_64 GNU/Linux\ngongysh@gongysh-ThinkPad-T530:~/git/snabbswitch/src$ dpkg -l | grep lua\nii  cpu-checker                               0.7-0ubuntu1                               amd64        tools to help evaluate certain CPU (or BIOS) features\nri  liblua5.1-0:amd64                         5.1.5-4                                    amd64        Shared library for the Lua interpreter version 5.1\nii  libluajit-5.1-common                      2.0.0+dfsg-1                               all          Just in time compiler for Lua - common files\nii  lua5.1                                    5.1.5-4                                    amd64        Simple, extensible, embeddable programming language\nrc  lua5.2                                    5.2.1-3                                    amd64        Simple, extensible, embeddable programming language\n. It seems we must use root privilege to run the make test.\nIf I run 'sudo make test' the problem goes. so can someone change the doc to indicate it?\n. ",
    "vladfedin": "May be it's also possible / desirable to somehow install auto-testing script?\n@altexy can you help us here on any other files needed?\n. This one is now obsolete, see #131\n. Fixed BIG_LETTERS naming, removed situational defaults, added few additional checks, including   filename for the vhost_user socket file and updated doc accordingly. Thanks for feedback!\n. Obsolete, see #134.\n. Added test_ci mode to makefile, in order to pass test exitcode to travis. Also added travis.yml that runs tests and prints output. Now a bit more is going on while travis ci is called, though tests still pretty much failing due to different reasons (like Unable to open file: /proc/sys/vm/nr_hugepages).\n. Next iteration - #139\n. I see no problem to squash it somewhere if needed. Do you have any preferences on this?\n. Can you please elaborate on best way to do it? Embed help info to makefile-SKIPPED messages, or make new help target to output help info, may be in case of SKIPPED or ERROR results redirect (or duplicate) output from testlog to console or may be something else?\n. ",
    "vnaum": "Link to binary packages (snabb/nova/qemu/libvirt/neutron) I've built:\nhttp://vnaum.com/misc/201403/\n(had to skip autotests with DEB_BUILD_OPTIONS=nocheck)\nI've built from my source repos:\nhttps://bitbucket.org/vnaum/nova/commits/branch/xmas-demo\nhttps://bitbucket.org/vnaum/sns-neutron/commits/branch/sns-agent1\nhttps://bitbucket.org/vnaum/libvirt-1.2.1/commits/branch/master\nhttps://github.com/vnaum/qemu/commits/vhost-user-v8\n. > May be it's also possible / desirable to somehow install auto-testing script?\nDo end users have any need for this test data?\nIf it's just for development purposes - let it stay in source repo, no need to clutter binary package.\nIf it can be handy for users as an example of how snabbswitch can work or as a template for custom scripts/scenarios,\n/usr/share/doc/snabbswich/examples is a good place for it (with comments).\nIf test data is essential for snabbswitch (snabb is useless without it, and everybody will need it) - /usr/lib/snabbswich is a correct place.\n\nSo it is installed with snabbswitch executable.\n\nWait, there is some installation script / routine?\nLast time I checked 'install' was not even a target in Makefile, so I had to add it.\nAnyway, for package building purposes it's quite easy to tell what goes into a package: only stuff that \"make install\" installs does.\n. > can we get away with so little?\nFrom my point of view we totally can.\nIs manpage sitting somewhere in source tree, or it has to be written yet?\nThere are 3 manpage templates in /debian dh_make provides, .1, docbook .sgml and docbook .xml - feel free to use one and remove .ex suffix.\nThere also is an init.d skeleton file there. Same stuff, add your options and remove .ex suffix.\n. If we're talking about debian/ubuntu packages, it's not a problem for package build scripts.\nOnly stuff that \"make install\" installs goes into binary packages.\n. Well, It's never empty \u2014 you depend on libc at least, and probably need some minimum kernel version, but the idea behind this was \"user installs snabbswitch and gets every requirement pulled\". As you suggest, meta-package can be done that depends in both snabb and \"everything\".\n. ",
    "dvv": "Today's attempt gives:\n...\nDIR       obj/doc/obj\nmkdir -p obj/doc/obj\nLUA       obj/lib/hardware/vfio_lua.o\nluajit -bg -n lib.hardware.vfio lib/hardware/vfio.lua obj/lib/hardware/vfio_lua.o\nluajit: error loading module 'jit.bcsave' from file './jit/bcsave.lua':\n        ./jit/bcsave.lua:1: unexpected symbol near '..'\nMakefile:57: recipe for target 'obj/lib/hardware/vfio_lua.o' failed\nmake[1]: *** [obj/lib/hardware/vfio_lua.o] Error 1\nmake[1]: Leaving directory '/home/dvv/projects/SNABB/snabbswitch/src'\nMakefile:9: recipe for target 'all' failed\nmake: *** [all] Error 2\n. Some points: https://groups.google.com/forum/#!topic/snabb-devel/Zezo2AIo4Dk\n. @altexy or someone who succeeds to build: please post versions of software on your setup:\nshell\nuname -a\ngcc -v\nmake -v\nluajit -v\n. I believe make 4.0 could introduce some brainer. Also, worths to temporarily remove system luajit to be sure it does not interfere with local snabb's one.\nStill, digging\n. @altexy what is the type of src/jit/bcsave.lua?\nOn my setup, after a fresh clone, it is vanilla text file containing\n../../deps/luajit/src/jit/bcsave.lua\nUPDATED\n. $ git --version\ngit version 1.9.2\n. OOPS, somehow had false core.symlinks in .gitconfig :]\nSo the subj transforms to\nlib/hardware/vfio.c:91:21: error: cast from pointer to integer of different size [-Werror=pointer-to-int-cast]\n     dma_map.vaddr = (uint64_t)buffer;\n                     ^\ncc1: all warnings being treated as errors\nwhich is another story.\n. Right. Hence, issue closed. Added final note to snabbswitch google group https://groups.google.com/forum/#!topic/snabb-devel/Zezo2AIo4Dk\n. ",
    "jfenton": "Hey Luke, still asks for password. Can you double-check it's updated your side?\n. Bingo! Thanks.\n. ",
    "Greezlee": "this needs a rebase to the current master\n. ",
    "sleinen": "Travis fails because -lpcap isn't available in the build environment.  I think we need to add \"libpcap-dev\" to the apt-get install command in the before_install line in .travis.yml.\n. equal() looks good to me now!\n. On Sun, Apr 12, 2015 at 10:23 AM, Kristian Larsson <notifications@github.com\n\nwrote:\nhttps://tools.ietf.org/html/draft-huang-netmod-acl-01\n\nNote that this revision has expired in 2013 (though there are newer ones\nthat have expired only last year :-)\nBut check out:\nhttps://tools.ietf.org/html/draft-ietf-netmod-acl-model\nSimon.\n. I don't feel qualified to comment on the concrete API, but the approach with the snabb module sounds very attractive to me.  It could make Snabb more \"approachable\" and give it(s core) a clearer identity.\nNote that I'm speaking as someone who has been interested in Snabb switch for a long time, but never programmed to it so far.  So I completely ignore the cost of moving to such a model!\n. On Mon, Jun 29, 2015 at 8:31 AM, Luke Gorrie notifications@github.com\nwrote:\n\nThe default behavior of Linux when the IOMMU is disabled has been\ninconsistent at best.\n\nDo you mean, \"...when the IOMMU is enabled has been inconsistent at best\"?\nSimon.\n. You mean 82599 rather than 82559, right?\n. @lukego You realize that Torch7 was originally (co-)developed just a few hills away from you at IDIAP in Martigny? I would have suggested that you go visit those folks to chat about their API design, except they all seem to work for the FaceTwitGoog now...\n. I quickly converted this to CSV, uploaded it to Google Drive, and created a simple line chart.\nhttps://docs.google.com/spreadsheets/d/1Guo_iBC5P0i5uAkvJHWirZy_Z9nFxxyzOee9LIrraG4/edit#gid=1289069081\nDon't see any significant variation between the branches. A few outliers are due to runs with zero results.\nIn principle this is something that could be automated using the Drive API, although there may be better ways to to this. There are certainly smarter ways to visualize the results\u2014this is just a quick hack.\n. > Second that the packetblaster benchmark seems to have around a 3% failure rate. That is high! \nNote that your gist also contained two runs that my stupid Emacs query-replace-regexp didn't convert because they contained the unexpected string \"Terminated\" between the snabbnfv-iperf-jumbo and snabbnfv-loadgen-dpdk results:\nrun: 12\nSat Feb  6 13:08:39 UTC 2016\nSwitched to and reset branch 'busywait+ndesc/32768'\nbasic1-100e6 32.7 -\npacketblaster-64 12.65 -\npacketblaster-synth-64 12.69 -\nsnabbnfv-iperf-1500 0 -\nsnabbnfv-iperf-jumbo 0 -\nTerminated\nsnabbnfv-loadgen-dpdk 0 -\nrun: 8\nSat Feb  6 13:49:22 UTC 2016\nSwitched to and reset branch 'busywait+ndesc/1024'\nbasic1-100e6 31.9 -\npacketblaster-64 12.69 -\npacketblaster-synth-64 0 -\nsnabbnfv-iperf-1500 0 -\nsnabbnfv-iperf-jumbo 0 -\nTerminated\nsnabbnfv-loadgen-dpdk 0 -\n...as well as a third ungrokkable entry at the end. I think that one was truncated, as it misses the final snabbnfv-loadgen-dpdk line that all the other entries had.\n. > Anybody knows how to get Google to tell us what is going on there?\nSelect appropriate region\nMenu: Insert -> Chart ->\n  Type \"line chart\", play around with options until it looks nice\n  Maybe use \"Recommendations\"; this automagically ignores boring columns such as the ones with all-0 results.\nThen \"Move to separate sheet\" the resulting chart using its menu.\nI did this to a copy of your spreadsheet:\nhttps://docs.google.com/spreadsheets/d/16pkBveTUa6vFRLSnJle2nqEdmIwVsFZYHjoYdHdoVtw/edit?usp=sharing\nNote I also added a row 1 with the benchmark titles, and used them as legend for the graphs.\nI could only use column B as the index, so this completely mixes \"master\" and \"busywait\".  It would probably be better to combine each pair of a master+busywait row into a single row with separate result columns for master and for busywait, so that the chart then contains separate graphs for these cases.\nProbably the even smarter thing to do would be to learn R and statistics and generate publication-quality graphs with the proper statistic metrics on it.  But I don't have time for that now :-)\n. Looks good to me.  I was left with a few open questions:\n- What is the API for enabling/disabling error logging\u2014I guess that this involves modifying the minimum priority that should be logged.\n- What are the defaults for the optional entries and stringtablesize arguments (and when would I want to care about stringtablesize)? Though these are the kind of details that I wouldn't mind UTSLing for.\n. Nice description of the problem, and I agree with your conclusions.  For most real applications you should be able to reduce b) to c) at the cost of additional ports.  Exceptions? Artificial constraints such as \"dragster-race\" competitions (Internet2 land-speed records) or unrealistic customer expectations (\"we only ever buy kit that does line rate even with 64-byte christmas-tree-packet workloads\").\nCost of additional ports may be a problem, but that needs to be weighted against development costs as well.  (You can formulate that as a time-to-market argument where you have the choice of either getting a working system now and upgrade it to the desired throughput once additional ports have gotten cheaper, or waiting until a \"more efficient\" system is developed that can do the same work with just one port :-)\n. \"options.detectd_dead_apps\" looks like a typo. Shouldn't this be \"options.dead_app_detection\"?\n. ",
    "essej-sehtam": "I don't see the restriction in LuaJIT 2.0 release.  And no discussion that I can see.  The #ERROR shows up in Snabb Switch version of LuaJIT.  \nThe intended application: Compare Snabb Switch in a proof-of-concept for PowerPC based embedded products that today use conventional hardware Ethernet Switch Devices from Broadcom & Marvell.  Proof-of-concept target hardware is an Embedded Freescale P1021 multi-core PowerPC e500v2 processor running Linux 2.6.32, with PCIe to Intel 82574L initially.  Really just looking to use Snabb Switch as is, but cross compiled for PowerPC e500v2.\n. Thank you.  I will keep that in mind while I look into some other alternatives.\n. ",
    "llelf": "I'm trying band and NUMMODE=2 but it's not going anywhere yet (+5% maybe)\n. Yeah, I will look.\nActually it's already ~4.5 with some packet messing\n. Right now it can do 3.7 / 5.0 or 4.0 / 4.5  (100M/10M accordingly). With different allocators, so pick only one!\nChanging Hz, gc() manually, changing gc parameters doesn't help.\nMaybe it's even \"normal\" for luajit. Or there are some unnecessary allocations but I don't get it yet.\n. ~lelf/s4/ on grindelwald\nbut it's all big mess there :P\n. Luajit used to insert gc() to VirtioNetDevice:transmit_packets_to_vm (this is the one profiling show) and packet:free. Although right now I see even more gc() calls. It's all unpredictable somewhat.\nAnd I don't know what objects exactly are gc'ed. Memory profiler would be great\n. All significant changes (if I didn't forget something) are in pull request. Should be 4.1...4.3Mp/s.\nIt appears it's easy to find where. Just look to \u201ccall lj_mem_newgco\u201d in asm output.\nAlso it appears that luajit inserts one even into link:receive. Copies packet on return probably. I don't know yet what (apart from ugly things like inlining call or returning index) to do with it.\n. > Do you think we can get the same performance on 100e6 as on 10e6? That would be ideal.\nEverything is possible. But let's rewrite it in C first! (Nah, I'm joking about the last part)\nHow are you getting that number for 6 ports? By changing NFV_CPI[01] and then dividing overall rate by 6?\nI'll try that argument thing tonight, not sure if it will help though. But at least I understand why luajit want to create an object there (as opposed to the fact that I've got no clue about why it wants yet some more down the trace ;).\n. ",
    "SnabbBot": "[make test] at 5ab8d34e1175bc8877380324b6c5b0a7e94fa6b7\nDIR       testlog\nTEST      lib.tlb\nTEST      lib.hardware.pci\nTEST      lib.hardware.vfio\nTEST      lib.hardware.bus\nTEST      core.link\nTEST      core.lib\nTEST      core.memory\nTEST      core.app\nTEST      core.timer\nTEST      apps.intel.intel_app\nTEST      apps.ipv6.ipv6\nTEST      apps.vhost.vhost_apps\nTEST      apps.vhost.vhost_user\nSKIPPED   testlog/apps.vhost.vhost_user\nTEST      apps.keyed_ipv6_tunnel.tunnel\nTEST      apps.vpn.vpws\nTEST      apps.rate_limiter.rate_limiter\nTEST      apps.fuzz.fuzz\nTEST      apps.example.asm\nTEST      apps.packet_filter.packet_filter_benchmark\nTEST      apps.packet_filter.packet_filter\n. [make test] at 24c3b759df6755d777c647d253022afb9d69665e\nDIR       testlog\nTEST      lib.tlb\nTEST      lib.hardware.pci\nTEST      lib.hardware.vfio\nTEST      lib.hardware.bus\nTEST      core.link\nTEST      core.lib\nTEST      core.memory\nTEST      core.app\nTEST      core.timer\nTEST      apps.intel.intel_app\nTEST      apps.ipv6.ipv6\nTEST      apps.vhost.vhost_apps\nTEST      apps.vhost.vhost_user\nSKIPPED   testlog/apps.vhost.vhost_user\nTEST      apps.keyed_ipv6_tunnel.tunnel\nTEST      apps.rate_limiter.rate_limiter\nTEST      apps.fuzz.fuzz\nTEST      apps.example.asm\nTEST      apps.packet_filter.packet_filter_benchmark\nTEST      apps.packet_filter.packet_filter\n. [make test] at 15f8c33d7e0a1b4d9565778a24244ba8b8445b83\nDIR       testlog\nTEST      lib.tlb\nTEST      lib.hardware.pci\nTEST      lib.hardware.vfio\nTEST      lib.hardware.bus\nTEST      core.link\nTEST      core.lib\nTEST      core.memory\nTEST      core.app\nTEST      core.timer\nTEST      apps.intel.intel_app\nTEST      apps.ipv6.ipv6\nTEST      apps.vhost.vhost_apps\nTEST      apps.vhost.vhost_user\nSKIPPED   testlog/apps.vhost.vhost_user\nTEST      apps.keyed_ipv6_tunnel.tunnel\nTEST      apps.rate_limiter.rate_limiter\nTEST      apps.fuzz.fuzz\nTEST      apps.example.asm\nTEST      apps.packet_filter.packet_filter_benchmark\nTEST      apps.packet_filter.packet_filter\n. # Running integration tasks for 17bf5f on davos:\n/home/max/snabb_bot/tasks/benchmarks.sh\nComparing with SAMPLESIZE=5)\n(benchmark, mean score, standard deviation, abbrev. sha1 sum)\nbasic1-10e6 dd8265 10.82 0.0748331\nbasic1-10e6 17bf5f 10.86 0.0489898\n/home/max/snabb_bot/tasks/test.sh\nDIR       testlog\nTEST      lib.hardware.pci\nTEST      lib.hardware.bus\nTEST      lib.hardware.vfio\nTEST      lib.tlb\nTEST      core.app\nTEST      core.lib\nTEST      core.timer\nTEST      core.link\nTEST      core.memory\nTEST      apps.fuzz.fuzz\nTEST      apps.ipv6.ipv6\nTEST      apps.example.asm\nTEST      apps.rate_limiter.rate_limiter\nTEST      apps.keyed_ipv6_tunnel.tunnel\nTEST      apps.vhost.vhost_user\nSKIPPED   testlog/apps.vhost.vhost_user\nTEST      apps.vhost.vhost_apps\nTEST      apps.intel.intel_app\nSKIPPED   testlog/apps.intel.intel_app\nTEST      apps.packet_filter.packet_filter_benchmark\nTEST      apps.packet_filter.packet_filter\nTEST      apps.vpn.vpws\n. Running integration tasks for 6f64c3e on davos:\n/home/max/snabb_bot/tasks/benchmarks.sh\nComparing with SAMPLESIZE=5)\n(benchmark, abbrev. sha1 sum, mean score, standard deviation)\nbasic1-100e6 e43188d 11.46 0.135647\nbasic1-100e6 6f64c3e 0 0\nbasic1-10e6 e43188d 10.98 0.0979796\nbasic1-10e6 6f64c3e 0 0\n/home/max/snabb_bot/tasks/test.sh\nDIR       testlog\nTEST      lib.hardware.pci\nERROR     testlog/lib.hardware.pci\nTEST      lib.hardware.bus\nERROR     testlog/lib.hardware.bus\nTEST      lib.hardware.vfio\nERROR     testlog/lib.hardware.vfio\nTEST      lib.tlb\nERROR     testlog/lib.tlb\nTEST      core.app\nERROR     testlog/core.app\nTEST      core.lib\nERROR     testlog/core.lib\nTEST      core.timer\nERROR     testlog/core.timer\nTEST      core.link\nERROR     testlog/core.link\nTEST      core.memory\nERROR     testlog/core.memory\nTEST      apps.fuzz.fuzz\nERROR     testlog/apps.fuzz.fuzz\nTEST      apps.ipv6.ipv6\nERROR     testlog/apps.ipv6.ipv6\nTEST      apps.example.asm\nERROR     testlog/apps.example.asm\nTEST      apps.rate_limiter.rate_limiter\nERROR     testlog/apps.rate_limiter.rate_limiter\nTEST      apps.keyed_ipv6_tunnel.tunnel\nERROR     testlog/apps.keyed_ipv6_tunnel.tunnel\nTEST      apps.vhost.vhost_user\nERROR     testlog/apps.vhost.vhost_user\nTEST      apps.vhost.vhost_apps\nERROR     testlog/apps.vhost.vhost_apps\nTEST      apps.intel.intel_app\nERROR     testlog/apps.intel.intel_app\nTEST      apps.packet_filter.packet_filter_benchmark\nERROR     testlog/apps.packet_filter.packet_filter_benchmark\nTEST      apps.packet_filter.packet_filter\nERROR     testlog/apps.packet_filter.packet_filter\nTEST      apps.vpn.vpws\nERROR     testlog/apps.vpn.vpws\n. Running integration tasks for 598f683 on davos:\n/home/max/snabb_bot/tasks/benchmarks.sh\nComparing with SAMPLESIZE=5)\n(benchmark, abbrev. sha1 sum, mean score, standard deviation)\nbasic1-100e6 e43188d 11.42 0.0748331\nbasic1-100e6 598f683 11.52 0.116619\nbasic1-10e6 e43188d 11 0.0894427\nbasic1-10e6 598f683 11 0.0632456\n/home/max/snabb_bot/tasks/test.sh\nDIR       testlog\nTEST      lib.hardware.pci\nTEST      lib.hardware.bus\nTEST      lib.hardware.vfio\nTEST      lib.tlb\nTEST      core.app\nTEST      core.lib\nTEST      core.timer\nTEST      core.link\nTEST      core.memory\nTEST      apps.fuzz.fuzz\nTEST      apps.ipv6.ipv6\nTEST      apps.example.asm\nTEST      apps.rate_limiter.rate_limiter\nTEST      apps.keyed_ipv6_tunnel.tunnel\nTEST      apps.vhost.vhost_user\nSKIPPED   testlog/apps.vhost.vhost_user\nTEST      apps.vhost.vhost_apps\nTEST      apps.intel.intel_app\nSKIPPED   testlog/apps.intel.intel_app\nTEST      apps.packet_filter.packet_filter_benchmark\nTEST      apps.packet_filter.packet_filter\nTEST      apps.vpn.vpws\n. Running integration tasks for b463f28 on davos:\n/home/max/snabb_bot/tasks/benchmarks.sh\nComparing with SAMPLESIZE=5)\n(benchmark, abbrev. sha1 sum, mean score, standard deviation)\nbasic1-100e6 323d787 11.54 0.08\nbasic1-100e6 b463f28 0 0\nbasic1-10e6 323d787 11 0.0632456\nbasic1-10e6 b463f28 0 0\n/home/max/snabb_bot/tasks/test.sh\nDIR       testlog\nTEST      lib.hardware.pci\nERROR     testlog/lib.hardware.pci\nTEST      lib.hardware.bus\nERROR     testlog/lib.hardware.bus\nTEST      lib.hardware.vfio\nERROR     testlog/lib.hardware.vfio\nTEST      lib.tlb\nERROR     testlog/lib.tlb\nTEST      core.app\nERROR     testlog/core.app\nTEST      core.lib\nERROR     testlog/core.lib\nTEST      core.timer\nERROR     testlog/core.timer\nTEST      core.link\nERROR     testlog/core.link\nTEST      core.memory\nERROR     testlog/core.memory\nTEST      apps.fuzz.fuzz\nERROR     testlog/apps.fuzz.fuzz\nTEST      apps.ipv6.ipv6\nERROR     testlog/apps.ipv6.ipv6\nTEST      apps.example.asm\nERROR     testlog/apps.example.asm\nTEST      apps.rate_limiter.rate_limiter\nERROR     testlog/apps.rate_limiter.rate_limiter\nTEST      apps.keyed_ipv6_tunnel.tunnel\nERROR     testlog/apps.keyed_ipv6_tunnel.tunnel\nTEST      apps.vhost.vhost_user\nERROR     testlog/apps.vhost.vhost_user\nTEST      apps.vhost.vhost_apps\nERROR     testlog/apps.vhost.vhost_apps\nTEST      apps.intel.intel_app\nERROR     testlog/apps.intel.intel_app\nTEST      apps.packet_filter.packet_filter_benchmark\nERROR     testlog/apps.packet_filter.packet_filter_benchmark\nTEST      apps.packet_filter.packet_filter\nERROR     testlog/apps.packet_filter.packet_filter\nTEST      apps.vpn.vpws\nERROR     testlog/apps.vpn.vpws\n. Running integration tasks for 2f19036 on davos:\n/home/max/snabb_bot/tasks/benchmarks.sh\nOK: basic1-100e6 on 2f19036 : 0.994746 of 11.42\nOK: basic1-10e6 on 2f19036 : 1 of 10.92\n/home/max/snabb_bot/tasks/test.sh\nTests successful.\n. When I try it, DPDK won't start because of the following error:\nERROR: This system does not support \"RDRAND\".\nPlease check that RTE_MACHINE is set correctly.\nI remember this from the last time we tried to run DPDK on chur. We didn't solve it back then.\n. ",
    "eugeneia": "That is wrong indeed, it works if you swap x and y in the second loop but maybe it should just compare #x == #y?\n. Fixed here (also added a regression test): https://github.com/SnabbCo/snabbswitch/pull/247\n. While its not currently included in SnabbBot reports, our benchmark utility (src/scripts/cperf/cperf.sh) does indeed compute the standard deviation of the results of n runs where n is configurable.\nI was going to ramble about standard deviation being more likely to yield false positives but then again there is only one way to find out! :)\n. This breaks NIC switching in the test cases I am working on. E.g. pings that were switched between two Intel apps previously are dropped after this merge.\n. Check out https://github.com/eugeneia/snabbswitch/tree/snabbnfv_testing (which includes this merge), you will need a ~/bench_conf.sh, Try mine: https://gist.github.com/eugeneia/3b3d739f701c6c17ee55\nRun the test as follows:\ncd snabbswitch/src\nsudo lib/nfv/selftest.sh\nThis will create snabb.log under snabbswitch/src which contains snabb's output.\nThe first test case in lib/nfv/selftest.sh will run `snabbnfv-traffic' with https://github.com/eugeneia/snabbswitch/blob/snabbnfv_testing/src/test_fixtures/nfvconfig/test_functions/same_vlan.ports\nThen it will boot two VMs and try to ping #2 from #1.\nEdit: BTW this is on davos.\n. Using 0000:87:00.1 instead of 0000:84:00.1 does indeed hide the problem.\nIf I revert set_SFI(), I can get it to work about 50% of the time on 0000:84:00.1, so while it has some effect its not exactly a fix.\n. Maybe I am wrong but I don't feel like a \"force up\" flag would help the situation (unless it has some use in real world applications). For testing purposes, it doesn't really make sense to mask failures that will happen in production. E.g. I would like the testing environment to be as close to the real thing as possible, and not have an altered environment (which possibly invalidates the tests).\nCan't we just print a warning like \" did not receive 'link up', do not expect it to work\"? That way you know something is wrong with the NIC and not with the app itself.\n. Yeah will do, also this might need a little more work/thinking now or later on. Two thoughts:\n1\nShould apps that fail to configure/start/stop be restarted? I tend to think so, consider this case: Due to a NFV configuration change a 65th Intel app is created and its configuration will fail, but another one will be stopped.(E.g. because we don't have control over the order in which compile_config_actions will spit out actions, and there is some hardware limit in intel apps). As of now snabb crashes with a message \"pool overflow\", but the app could also be configured again 2s later and run just fine.\nOf course we'd need to keep track which \"dead apps\" failed during configure and not try to stop() them. Also this is a little bit of a recursive situation as apps can \"die\" during restarting.\nMy point is, as of now we can run into exceptional situations that are caused by a combination of individual app semantics and configure semantics and I suspect there will be more of these cases to come. Should it be part of configure semantics to always perform stop before start and the apps responsibilty to never fail as long as the configure behaves correctly (we have yet to define a semantic interface fo start, stop, ...)? And/Or should core.app handle these cases?\nOne the one hand, handling arbitrary error cases in core.app is sort of a \"wildcard\", we don't really know what its gonna do to future apps. On the other hand, if well designed, that can also be a good approach as it \"fixes/handles\" errors whose source is not even yet created. In  a way core.app would accept a declaration of what the user wants, and log (non-fatal) errors if it fails to achieve those goals, but always keep trying. The user would debug/develop his apps and app network by figuring out how core.app can achieve his declaration of goals. Snabb would only crash due to bugs in Snabb itself, not bugs in individual apps itself (e.g. never crash. Its job would be to try implement the declared network, not to succeed at it). Maybe there will be an app that fails to start/stop/pull/push every 10th time for perfectly good real-world reasons. A future core.app might handle that app very well, and even report something smart as \"app X causes overall decrease of network uptime to 99.9% and packet loss of 0.023%.\". Could be a network engineers wet-dream feature (I imagine), Maybe you would implement faulty apps on purpose to simulate a \"chaos-monkey\" and test your network design.\nThis is getting pretty meta, not sure if my thoughts came across. Bottom line I feel there is big potential in availability safeguards in core.app and as of this PR it might not do enough.\n2\nI really don't like how I tag app.name... ;)\n. Rebased on master. I also fixed up my gripe with tagging app.name, its done only in one place ops:start. The previous code was simply unnecessary.\n. Minor nitpick: There is a comment\n-- XXX This is assumed to be ingress-only. Egress is simply NYI.\nat https://github.com/SnabbCo/snabbswitch/pull/245/files#diff-e6dc95ae179bd7a0f203adc2547680c0L84 which I guess can be removed with this commit?\n. The neutron2snabb part is fine. We should test this in the nfv/selftest.sh though, but I am very happy with this diff. Don't see any problem.\n. E.g. just merge this already! ;)\n. This will probably conflict with neutron2snabb egress filters, please merge that PR first then I will resolve the eventual conflicts.\n. Please rebase onto current master. I somehow managed to get a broken patch through SnabbBot which breaks the tests for this PR. Sorry for the inconvenience.\n. I would like to try to complete this. Is there a reason why it uses the pcap-ng format instead of the old pcap format we already have implemented in lib.pcap.pcap?\n. nd_light still succeeds to perfom ND. But it doesn't play well with keyed_ipv6_tunnel yet. Lets re-iterate:\n- nd_light now overwrites etherner src and dst.\n- keyed_ipv6_tunnel is unchanged (prepends L2PTv3 header)\nSo I'd expect nd_light to smash the L2PTv3 header and tunnel to cast the wrong section of the packet to a header struct. From the tests output it looks as if the ping packet is delivered though, but VM_B doesn't answer. No packets are dropped by the tunnel. Am I missing something?\n. I wasn't sure who should do it thats why I brought it up again. I was hesistant to do it myself because I am a little bit intimidated by the code. I am on it now.\n. Now I know what confused me yesterday: Turns out that keyed_ipv6_tunnel already does the right thing. It adds a dst:<default_gateway> src:0 Ethernet header on the encapsulation path (part of header_template) and it failed before because nd_light added a second Ethernet header.. This also explains my test results (packets passing the tunnel).\n. This PR breaks lib/nfv/selftest.sh:\nRestarting Virtio_B (died at 1413207491.867421: lib/virtio/net_device.lua:220: attempt to index local 'b' (a nil value))\nSnabbBot didn't catch it because lib/nfv/selftest.sh didn't exist back then. Can we revert and reopen this one?\n. I didn't know how long it would take to find the bug that's why I suggested reverting this PR for the time being but I don't mind how the fix gets into master. Just make sure master is \"broken\" as short as possible.\n. Yes #286 did fix it!\n. After this merge I get the following error during nfv/selftest.\nlib/virtio/net_device.lua:276: assertion failed!\n. Looking good!\n. Please rebase onto current master. I somehow managed to get a broken patch through SnabbBot which breaks the tests for this PR. Sorry for the inconvenience.\n. @javierguerragiraldez, @lukego \n\nERROR     testlog/lib.nfv.config\n\nThis failure is caused by a known regression, rebasing onto master fixes it.\n\nERROR     testlog/apps.intel.intel_app\n\nI can get the intel test to pass on 0000:82:00.0/1@chur but it fails on 0000:84:00.0/1@chur with \"wrong proportion of packets passed/discarded\".\nThen again for some cards on davos I get the same error and for others I get \"not enought packets somewhere\".\n\nERROR     testlog/lib.nfv.selftest.sh\n\nOn chur: The vhost_user app has trouble with memory, see this snabb log:\nhttps://gist.github.com/eugeneia/fc06af4cc7b27fa84f4c\nOn davos: PING fails, I assume because the intel app is failing.\n. I double checked the results on grindelwald and the vhost_user crash happens there too. So as far as I can tell we have at least one  regression that slipped into master and affects the vhost_user app.\nOr was grindelwald upgraded too? Grindelwald has Linux 3.13.0-37 while chur and davos run 3.13.0-43.\n. On Chur the master branch passes all tests now. On Grindelwald I get:\n\nRestarting Virtio_B (died at 1421076458.855534: lib/virtio/net_device.lua:382: mapping to host address failedcdata: 0xffff88003bd2cd00)\n\nSee the full log: https://gist.github.com/eugeneia/6489768c57e434036269\n. @lukego https://gist.github.com/eugeneia/f0434505c44c03313a1d\nI used grep -v nanosleep to shrink the file, its >20MB otherwise.\n. @javierguerragiraldez @lukego I now believe that we have a regression in the intel driver. To see for yourself run the intel_app selftest repeatedly on any server, on any card. The error varies (sometimes unbalanced packets, sometimes not enough packets, sometimes sent == prevsent, sometimes everything passes).\nI have ran the intel selftest about a hundred times now, on branch master and keep-nfv, on davos and chur, on different cards. I can't see a pattern, the failures/successes seem random. \n. @javierguerragiraldez Doh! I didn't think of maybe unbinding the cards at all. Sorry for causing such a ruckus.;) I was honestly confused.\n\nThe :84: card passes the 'in chip' tests, but fails tests that actually\nput bytes in the wire.\n\nThis is semi-known, \"some\" cards have bad cables and we always avoid them instead of keeping track of them.\n. After unbinding all cards on davos from the ixgbe driver I sill get this failure in about one of 20 runs:\n\nSend a bunch of packets from Am0\n...\nwrong proportion of packets passed/discarded\nEXITCODE: 1\n\nIt really helps to do something like for i in {1..20..1}; do RUNTEST; done.\nEdit: On Chur I had 18 of 20 runs fail on 86 and couldn't get any failures in 60 runs on 82 (as Javier also experienced). Maybe the intel selftest is really sensitive to minor hardware failures, e.g. packet loss on wire? I am really not a hardware dude and have no idea how to detect/handle packet loss at the hardware layer.\n. @javierguerragiraldez The NFV selftest still runs fine for me (when I run it manually, the SnabbBot failures where PING fails are still a mystery to me). If the VMs don't boot its most likely an issue with the qemu settings in bench_conf.sh, you can see how qemu fails by removing the /dev/null pipes off the qemu calls in bench_env/common.sh.\n. > that watchdog thing\nIf you mean lib.watchdog.watchdog, it should be ready to use. Nobody uses it at the moment but as far as I know its supposed to be a supervision tool available to design authors.\n. Sure why not! :)\n. It hangs somewhere, we need to make sure that on davos local connections aren't blocked by iptables after a reboot. That's usually the issue, I then run iptables -P INPUT ACCEPT and it works but I didn't dare to make a persistent change yet (not an iptables expert, especially on Ubuntu).\n. @lukego First time the regression test was triggered, so I was both surprised and happy to see it in action. :) @clopez seems to have it covered.\n\nCould be worth making that error message more prominent?\n\nMaybe. I don't consider it high priority but we could maybe make SnabbBot's Gists be Markdown documents with fancy formatting. Right now I really have more crucial things on the TO-DO so I suggest waiting for more complaints before acting.\n. I am all for simple. I tried to solve this incrementally but this indeed became hairy rather fast.\nSo lib.timer as an abstraction over core.app functions that deal with time? Do we even need a ticktock method?  What if core.app exposed now() and maybe nano_now() and lib.timer provided an easy to use timer abstraction over those?\n. Superseded by #316 .\n. Closed this again, because I opened it too early (I thought #293 was merged already).\n. Please rebase onto current master. I somehow managed to get a broken patch through SnabbBot which breaks the tests for this PR. Sorry for the inconvenience.\n. @dpino On chur you can copy /home/max/bench_conf.sh and edit it to fit your needs:\n- Copy /home/max/test/* (the VM images) and adjust the paths\n- Change the telnet ports as these can clash\nThe only \"new\" thing about this replacement is the NFV_CONFIG variable which should be set to test_fixtures/nfvconfig/test_functions/snabbnfv-bench.port. (Already set in my bench_conf)\nI always forget to set RUN_LOADGEN=true, so this is something to keep in mind. (Already set in my bench_conf)\nNFV_PACKETS controls the number of packets to be sent.\nIt may make sense to use a different PCAP and/or KERNEL. Luke said something about a \"no-virtio kernel\"?\n. Oops you copied the file right after a tried to use the\nbzImage-no-virtio-net' kernel from grindelwald. It works again for me\nonce you setKERNEL' back to:\nKERNEL=/opt/bench/bzImage\n. @lukego Shall I still set up the no-virtio kernel/VMs on chur?\n. @lukego  Edit: Ah I get it you depend on my PR. The way I deal with this is usually is to wait until the branch I depend on is merged and then rebase onto master. I usually use -i because sometimes commits will be duplicated which can then be dropped.\n. > Do you know how to do a git fetch from the specific branch of your\n\nfork?  Github only gives me the master URL...\n\nYep, one convenient way is to:\n1. Add my repo as a \"remote\", e.g.\n git remote add mrottenkolber \\\n   https://github.com/eugeneia/snabbswitch'\n\n\n\nCheckout a new branch and pull from the new remote, e.g.\ngit checkout master;\n git checkout -b my-branch;\n git pull --rebase mrottenkolber lib/nfv/selftest-keep-nfv\n. I totally agree, it's exactly my conclusion: There are two different timer use cases at the moment, and this patch only addresses one of those.\n\n\nIn case this got lost though: \"App-timers\" should not bring down snabb but lead to app death and subsequent restarts. So whatever implements the \"app-timers\" imho needs to address this and apps must not be allowed to create global timers.\nRegarding core.lib.timer(), yes with two reservations:\n- It doesn't support \"repeating\" timers\n- It could be made to optionally use engine.now()\nCould be added though (if we wanted to mostly keep the current timer capabilities):\n-- Return a function that will return false until NS nanoseconds have elapsed.\nfunction timer (ns, mode, timefun)\n   timefun = timefun or C.get_time_ns\n   local deadline = timefun() + ns\n   if mode == \"repeating\" then return function ()\n       if timefun() >= deadine then\n          deadline = deadline + ns\n          return true\n       else return false end\n   end else return function () return timefun() >= deadline end end\nend\nPS: Shouldn't core.lib.timer() ensure that it doesn't return true twice? I don't know how realistic it is to be called twice in a nanosecond.\n. > should we have separate timer and repeating_timer functions instead?\nProbably!\n\nHow should we represent time in Snabb Switch? integer nanoseconds or floating-point seconds?\n\nI dislike floating-point seconds because its hard to tell the precision of the timer. I find an API that says \"returns time in nanoseconds\" better than one that says \"returns time in seconds (as a floating point number with nanosecond precision, except sometimes, because IEEE)\".\n. @lukego So I have been pondering about yet another timers rework, but satisfied I am not: https://github.com/SnabbCo/snabbswitch/compare/master...eugeneia:timers-v3\n(This passes everything but the RateLimiter test :-1: )\nThe essence of this branch is to remove core.timer and replace it with a thin layer over a generalized lib.timer. Arguably the code does get a bit simpler at some points but I found the following caveats:\n- There is a timer use case not covered by this: Defining and running timers from \"outside\" of the engine. (This is kind of known, but still begs the question if there is a feature mismatch.)\n- The new core.timer has no notion of \"stacking timers\". E.g. if a repeating timer has an interval of 100ns but is only called every 200ns then its effective interval will be 200ns. This is the reason why the new timer can not really satisfy the old core.timer selftest.\n- The RateLimiter selftest fails with the new timer (I assume because of the previous issue)\nThe more I mess around with new timer implementations the more I feel that the current one is simply better. Both in functionality and performance. Right now I feel it would make more sense to catch and log errors in timer callbacks separately from the app restart framework and just be happy with the existing implementation. Maybe someone else wants to take a shot at this with a fresh mind, but I personally don't feel like I am improving things. ;)\n. Seems I broke basic command line argument parsing, gimme a sec. ;)\nEdit: Nope its just that the firewall was up again.\n. Yeah about the error handling: It boils down to the statement vs expression debate. If you try your example in LuaJIT instead of doing nothing it will wait for more input. E.g. the non-expression mode behaves completely differently because it doesn't expect a return value. And there is no way really to tell because the thunk I get in \"statement mode\" will simply be nil. E.g. the artificial return in \"expression mode\" generates the error.\nOh, and do we want the REPL to be always there? I thought maybe something lispy like an option to drop in the REPL on error would be cool. Also note I am not really sure about the scope of code loaded by loadstring, from my experients it seems to be the empty scope.\n. Thanks for the proof-read, I didn't expect so much valuable feedback so fast! =) As for your suggestions: Yes to all cases! :+1: \n\nBTW, have you considered something like readthedocs.org or gitbook.io for keeping a     fairly good-looking document up to date with latest git repo?\n\nWe could let SnabbBot render the current documentation automatically. I don't consider that very important though, because if the docs change too often (besides stylistic fixes and new additions) that would be a bad sign since they are written as a contract for the user. So including rendered versions with the releases seems good enough to me for now.\nRegarding other doc tools: I am actually quite fond of our tool chain. The docs automatically embed into GitHub (albeit, ToC rendering would be nice). They are managed just like any other source file so it should be easy to keep them up to date with the code. And then we heavily use a Ditaa pre-processing step to be able to easily embed graphics using a funny ASCII syntax (its a bit buggy but generally fun to work with).\n. @lukego This should be merged alongside the documentation PR (already merged).\n. Just so it shows up on the record: The precedent is set by basic_apps, intel10g and pcap which are modules each providing multiple apps. So this is not only consistency but also the most common denominator.\n. @lukego Javier thought of what slipped by me yesterday: The cards were all bound to the ixgbe driver after reboots so we had intel test failures everywhere.... which put me off quite a bit. WIll rerun the CI today.\n. @lukego There is still some room for false positives by SnabbBot, I ran the intel selftest 60 times before rerunning SnabbBot again and it had one failure on the card SnabbBot uses... See my snabb-devel post on this dilemma.\n. @lukego Regarding lib.hardware.pci.unbind_device_from_linux(): Should intel_app do the unbinding? @javierguerragiraldez stated that we did that before but stopped doing that. Why? \nIf not the documentation of intel_app should probably state that the NIC needs to be unbound. snabb-nfv-traffic/lib.nfv.config could be the second best place to unbind the NICs before they are used.\n. @javierguerragiraldez As per luke's request I removed the newly commented-out code (in 86e85b4). Is that sensible?\n. He could just add my Snabb fork (on GitHub) as a remote and then pull this branch into a branch on his repository? That's how I access third party branches and it works quite well as far as I can tell.\n. @lukego Is adding  column to the \"security rules\" table an option?\n. @lukego One way I see is to add a securty_rules slot to vif_details that maps security rule ids to a map of key value pairs (to be inserted into the rule by neutron2snabb). It would basically be a way to inject into neutron security rules. This would be very very wrong from an API design standpoint... heck its wrong from any standpoint. Let's do it?\n(I assume OpenStack's secrules table spec implicitly describes a stateless PF mechanism?)\n. @lukego I think semantics are best tested in the PacketFilter selftest, which has good coverage imho. Regarding performance, I feel like I don't really understand how the stateful PacketFilter works. When is a table \"full\"?\n. The reason this happens (i think)is because SnabbBot will:\n- clone the repo\n- checkout the base (old submodule)\n- fetch submodules\n- build / test\n- checkout HEAD\n- build / test again\nStrictly speaking its an error SnabbBot I guess, it should update the submodules after every checkout.\n. Just ran the selftest 100 times on davos and had 2/100 failures. Big improvements here! \n. I want to document one very weird (possibly Intel_app related) failure case here even though I have no idea why it happens yet:\nIt happened twice so far: SnabbBot fails the NFV selftest universally because \"PING failed\". PING usually never fails, it really means packets don't go through the hardware I guess. The weird thing is it all works when I run the selftest manually, only SnabbBot triggers the fail. Which sucks because SnabbBot and me we do the same thing. So there is a chance that I am just really unlucky and never trigger the \"timing bug?\" manually but SnabbBot hits it consistently??? My solution: Reboot davos, unbind all 10-G nics, try again. So yeah... what?\nMaybe someone sees something I don't see here. The fact that rebooting \"helps\" tells me its hardware related, but that I can't reliably reproduce it (but SnabbBot seems to do) puts me off really.\nUpdate: #316 never leads to PING failures. I should compare that branch with master and figure out what's causing this. tl;dr: regression alarm.\n. From Neutron/SecurityGroups (for my own reference and this discussion thread):\n\nBehavior\nThe basic characteristics of Neutron Security Groups are:\nFor ingress traffic (to an instance)\n  Only traffic matched with security group rules are allowed.\n  When there is no rule defined, all traffic are dropped.\nFor egress traffic (from an instance)\n  Only traffic matched with security group rules are allowed.\n  When there is no rule defined, all egress traffic are dropped.\n  When a new security group is created, rules to allow all egress traffic are automatically added.\n\"default security group\" is defined for each tenant.\n  For the default security group a rule which allows intercommunication among hosts associated with the default security group is defined by default.\n  As a result, all egress traffic and intercommunication in the default group are allowed and all ingress from outside of the default group is dropped by default (in the default security group).\n\nIt seems like we are not following the spec correctly as of now: AFAIK when no rules are given, we pass all packets instead of dropping them.\nRegarding stateful PF: I could only find mention of stateful PF in the referenced AWS docs, which also disagrees with Neutron:\n\nYou can specify separate rules for inbound and outbound traffic.\nBy default, no inbound traffic is allowed until you add inbound rules to the security group.\nBy default, all outbound traffic is allowed until you add outbound rules to the group (and then, you specify the outbound traffic that's allowed).\nResponses to allowed inbound traffic are allowed to flow outbound regardless of outbound rules, and vice versa (security groups are therefore stateful).\nInstances associated with a security group can't talk to each other unless you add rules allowing it (exception: the default security group has these rules by default).\n\nE.g.: Stateful by default (outbound responses are allowed by default).\nSo we could with little changes default to the Neutron spec. Then we could maybe allow a flag in our vif_details extension to implement the AWS stateful behavior? E.g. not expose the more fine grained API of our NFV implementation to Neutron and instead implement the expected behavior(s)?\n. > The Neutron rules are actually stateful even though they don't spell this out. When they talk about \"inbound traffic\" they are talking about two-way connections that originate with an inbound packet.\n@javierguerragiraldez So does our PF app actually support connection tracking across PF apps? E.g. start tracking a connection in the ingress PF and pass packets based on that tracking in the egress PF?\n. Ah-ha! :)\n. > 1. No packet filtering.\nE.g. without Neutron since Neutron specifies that no rules == no packets, right?\n\n\nNeutron security groups without stateful filtering.\nNeutron security groups with stateful filtering.\n\n\nI now check for the vif_details.stateless_pf field, to determine if PF should be stateless.\nEdit: Maybe the field should be called \"filter_mode\" and contain \"stateful\"/\"stateless\"/\"to be invented\" while defaulting to \"stateful\".\nIf vif_details.stateless_pf is a false value, then the returned PacketFilter config will contain state_track=<port.id> and state_check=<port.id> (in both ingress and egress filters). If I understand this correctly that means:\n- If the port establishes an egress connection to a client, the client's packets on that connection will pass regardless of the port's ingress rules.\n- If a client establishes an ingress connection to the port, the port's packets on that connection will pass regardless of the port's egress rules.\nRight? Wrong?\nI removed the (previously mentioned) check for #rules > 0 and always return a PacketFIlter configuration because Neutron specifies:\n\nWhen there is no rule defined, all traffic are dropped.\n\nMy WIP branch: https://github.com/eugeneia/snabbswitch/compare/stateful-nfv-pf\n. @lukego Can this be merged or do you want to amend further? Going to tag the 2015.03 release any time now.\n. @lukego Seems to have been a one-off. Hmmm. We should probably investigate how the taskset -c 1 was able to always trigger this bug and understand what its really about, seeing that the issue didn't just disappear.\n. @lukego A quick further investigation led to something: It seems to depend on the CPU core in use. So when I use taskset to force a CPU on a different NUMA node than the NIC it will fail, but like wise I can force the task to run on a affine node and it will work. So  removing the taskset all together led to random failures. I now explicitly pin the snabb_bot process to CPU 6 which is on the same node as the NIC used by snabb_bot. Let's see if my theory holds and we won't see any more ping failures.\n. @lukego Snabb does not crash, is there a scenario in which Snabb endures a \"failure to allocate a HugeTLB page\" without at least throwing an error?\nRegarding 3.: core.memory passes.\nRegarding 4.:\nshmget(IPC_PRIVATE, 2097152, IPC_CREAT|SHM_HUGETLB|0600) = 843677701\nshmat(843677701, 0, 0)                  = 0x2aaaaac00000\nshmat(843677701, 0x50042fe00000, 0)     = 0x50042fe00000\nshmdt(0x2aaaaac00000)                   = 0\nshmctl(843677701, IPC_RMID, 0)          = 0\nshmget(IPC_PRIVATE, 2097152, IPC_CREAT|SHM_HUGETLB|0600) = 843710470\nshmat(843710470, 0, 0)                  = 0x2aaaaac00000\nshmat(843710470, 0x50042fc00000, 0)     = 0x50042fc00000\nshmdt(0x2aaaaac00000)                   = 0\nshmctl(843710470, IPC_RMID, 0)          = 0\nshmget(IPC_PRIVATE, 2097152, IPC_CREAT|SHM_HUGETLB|0600) = 843743239\nshmat(843743239, 0, 0)                  = 0x2aaaaac00000\nshmat(843743239, 0x50042fa00000, 0)     = 0x50042fa00000\nshmdt(0x2aaaaac00000)                   = 0\nshmctl(843743239, IPC_RMID, 0)          = 0\nshmget(IPC_PRIVATE, 2097152, IPC_CREAT|SHM_HUGETLB|0600) = 843776008\nshmat(843776008, 0, 0)                  = 0x2aaaaac00000\nshmat(843776008, 0x50042f800000, 0)     = 0x50042f800000\nshmdt(0x2aaaaac00000)                   = 0\nshmctl(843776008, IPC_RMID, 0)          = 0\nshmget(IPC_PRIVATE, 2097152, IPC_CREAT|SHM_HUGETLB|0600) = 843808777\nshmat(843808777, 0, 0)                  = 0x2aaaaac00000\nshmat(843808777, 0x50042f600000, 0)     = 0x50042f600000\nshmdt(0x2aaaaac00000)                   = 0\nshmctl(843808777, IPC_RMID, 0)          = 0\nshmget(IPC_PRIVATE, 2097152, IPC_CREAT|SHM_HUGETLB|0600) = 843841546\nshmat(843841546, 0, 0)                  = 0x2aaaaac00000\nshmat(843841546, 0x50042f400000, 0)     = 0x50042f400000\nshmdt(0x2aaaaac00000)                   = 0\nshmctl(843841546, IPC_RMID, 0)          = 0\nshmget(IPC_PRIVATE, 2097152, IPC_CREAT|SHM_HUGETLB|0600) = 843874315\nshmat(843874315, 0, 0)                  = 0x2aaaaac00000\nshmat(843874315, 0x50042f200000, 0)     = 0x50042f200000\nshmdt(0x2aaaaac00000)                   = 0\nshmctl(843874315, IPC_RMID, 0)          = 0\nshmget(IPC_PRIVATE, 2097152, IPC_CREAT|SHM_HUGETLB|0600) = 843907084\nshmat(843907084, 0, 0)                  = 0x2aaaaac00000\nshmat(843907084, 0x50042f000000, 0)     = 0x50042f000000\nshmdt(0x2aaaaac00000)                   = 0\nshmctl(843907084, IPC_RMID, 0)          = 0\nshmget(IPC_PRIVATE, 2097152, IPC_CREAT|SHM_HUGETLB|0600) = 843939853\nshmat(843939853, 0, 0)                  = 0x2aaaaac00000\nshmat(843939853, 0x50042ee00000, 0)     = 0x50042ee00000\nshmdt(0x2aaaaac00000)                   = 0\nshmctl(843939853, IPC_RMID, 0)          = 0\nshmget(IPC_PRIVATE, 2097152, IPC_CREAT|SHM_HUGETLB|0600) = 843972622\nshmat(843972622, 0, 0)                  = 0x2aaaaac00000\nshmat(843972622, 0x50042ec00000, 0)     = 0x50042ec00000\nshmdt(0x2aaaaac00000)                   = 0\nshmctl(843972622, IPC_RMID, 0)          = 0\nshmget(IPC_PRIVATE, 2097152, IPC_CREAT|SHM_HUGETLB|0600) = 844005391\nshmat(844005391, 0, 0)                  = 0x2aaaaac00000\nshmat(844005391, 0x50042ea00000, 0)     = 0x50042ea00000\nshmdt(0x2aaaaac00000)                   = 0\nshmctl(844005391, IPC_RMID, 0)          = 0\nshmget(IPC_PRIVATE, 2097152, IPC_CREAT|SHM_HUGETLB|0600) = 844038160\nshmat(844038160, 0, 0)                  = 0x2aaaaac00000\nshmat(844038160, 0x50042e800000, 0)     = 0x50042e800000\nshmdt(0x2aaaaac00000)                   = 0\nshmctl(844038160, IPC_RMID, 0)          = 0\nshmget(IPC_PRIVATE, 2097152, IPC_CREAT|SHM_HUGETLB|0600) = 844070929\nshmat(844070929, 0, 0)                  = 0x2aaaaac00000\nshmat(844070929, 0x50042e600000, 0)     = 0x50042e600000\nshmdt(0x2aaaaac00000)                   = 0\nshmctl(844070929, IPC_RMID, 0)          = 0\nshmget(IPC_PRIVATE, 2097152, IPC_CREAT|SHM_HUGETLB|0600) = 844103698\nshmat(844103698, 0, 0)                  = 0x2aaaaac00000\nshmat(844103698, 0x50042e400000, 0)     = 0x50042e400000\nshmdt(0x2aaaaac00000)                   = 0\nshmctl(844103698, IPC_RMID, 0)          = 0\nshmget(IPC_PRIVATE, 2097152, IPC_CREAT|SHM_HUGETLB|0600) = 844136467\nshmat(844136467, 0, 0)                  = 0x2aaaaac00000\nshmat(844136467, 0x50042e200000, 0)     = 0x50042e200000\nshmdt(0x2aaaaac00000)                   = 0\nshmctl(844136467, IPC_RMID, 0)          = 0\nI do suspect it has something to do with the NIC simply because the symptom is packets not arriving. There is no crash or anything, just absence of I/O.\n. >  \"PING failed\" is also what you see if the VM fails to boot, right?\nNo since for a VM to be considered \"up\" the caller needs to successfully telnet into the VM and have it ping itself.\n\nThinking of the future: nice if SnabbBot would attach more log files to the gist. I would quite like to see the output of the snabb process, the two QEMU processes on the host, and the ping/iperf/etc processes inside the VMs.\n\nOK. Have increased verbosity in my ci-updates branch.\n\nWhich processes does the taskset apply to?\n\nGood question. The man page only talks about the target process, not the children spawned by it. The taskset seems superfluous since bench_env prepends everything with a numactl call anyways. Maybe they are stepping on each others toes? To be honest, I've never questioned the variables controlling numactl (NODE_BIND[0-9]+).\nRegarding isolcpus, as far as I can tell we don't do that on davos (or chur) because it broke numactl which is used quite heavily in bench_env:\n$ cat /proc/cmdline\nBOOT_IMAGE=/vmlinuz-3.13.0-43-generic root=/dev/mapper/davos--vg-root ro\n. @lukego numactl -N 0|1 triggers the bug as well. E.g. when binding the testscript to node 0 PING fails (NIC is on node 1).\n. > Looks like each NUMA node has 1024 hugepages available. QEMU 1 wants 512, QEMU 2 wants 512, Snabb Switch wants ~16... there won't be enough if all three processs are running on the same node.\nBut isn't our failure case happening in the opposite scenario, when all three processes are not on the same node?\nI tried anyways and increasing the number of hugepages does not fix the issue\n. I guess the most curious thing is that you actually have to tasket/numactl the parent command (e.g. SnabbBot's test task) instead of the make invocation to trigger the bug.\n. See #413 \n. @alexandergall Can you rebase your failing PR's onto current master? I think they fail because of a previously fixed regression that's not in your current base.\n. @lukego This changes $QEMU <ARGS> >/dev/null &2>1 to $QEMU <ARGS> >$IMAGE.log &2>1 (e.g. write qemu output to a file) and the NFV selftest will cat $IMAGE.log on assertion failures.\nE.g. it should look like this:\n```\n[...]\nPING failed.\nVM (path/to/image0.img) log:\nblablaba\nVM (path/to/image1.img) log:\nblablaba\nsnabb log:\nblablabla\n``\n. I see you figured it out, awesome. Everyone should always do agit pull --rebase  master` before pushing a PR. E.g.rebase your fork's master onto the main repo's master and then rebase your branch onto the up-to-date master. I do see a bit of a manual merge here (e.g. f24fc79) that's superfluous if you had rebased.\nFor instance:\n```\nAt some point I forked snabbco/snabbswitch on GitHub and then I\nclone it locally:\ngit clone https://github.com/eugeneia/snabbswitch.git\nNow I want to stay in sync with the mainline so I add a remote:\ngit remote add snabbco https://github.com/SnabbCo/snabbswitch.git\nWhen ever I want to sync (always before opening a PR) I can just:\n(I do this whenever a PR gets merged really, why fall behind?)\ngit checkout master\ngit pull --rebase snabbco master\ngit checkout my-working-branch\ngit rebase master\ngit push -f\nAlong the way I will solve all possible merge conflicts.\n```\nI suggest you go back to where you first opened the PR and do the rebase-fu. I suspect most test failures will disappear and generally you will get a cleaner history. You can do this all while the PR is open. A push -f will rewrite history, and SnabbBot will reevaluate.\nOne conflict I suspect you will see is regarding the lib.pci.open_usable_devices function. I removed that function not too long ago, and this PR reintroduces it without any explanation as far as I can tell. On rebase conflict-resolution you will probably see its gone and can than decide if you really want it (I suspect not).\n. > It seems to me like the reason that rebasing is solving test problems is that we have had regressions on our default branch that people base changes on. I see a couple of potential solutions to this. First to be more careful to avoid letting in regressions, which we are already doing.\nIn this specific case I think its the worst of all worlds to have multiple fixes of the same regressions, each months apart, without any context. I think rebasing open PR's is a very sensible choice because the history will be clean and accurately historic once it's merged. Compare the history of e.g. Hans' branch vs the history of Snabb mainline.\nThis is not rebasing the Snabb master, but instead editing the point of a branch being merged. Merging with the current master is in any way superior than merging with an ancient version of master. Not rebasing is just repeating already resolved merges and giving a lot of opportunity to mistakes. (E.g. reintroducing previously purged functions).\nBtw rebasing leaves the timestamps intact so again, this is when was it written vs when was it merged.\n. We went way off topic here which sucks because I am really talking about two specific issues (see inline comments above) I see in this PR that were caused by the branch not being rebased onto SnabbCo/snabbswitch:master and instead (I assume) mistakenly merged. The rebase vs merge is not really important here, what's important is that the mainline master must always precede your branch. For instance if you get a conflict where mainline deletes code and your branch changes a line of that code, you must throw away your changes and accept mainline's deletion. Otherwise you undid an intentional change in mainline unrelated to your branch. \nIt took me ~30 minutes just to track down how these changes slipped in, which proves my point being that the history will be harder to understand without the rebase described by me which should be done at 4f0d42a.\nCan we please rewind in time, forget the whole git philosophy discussion, and focus on the fact that the way this branch is currently merged is problematic. In the end I don't care how we get a clean merge of this, I can guarantee you that rebasing is the smartest and easiest way to do it, and I am happy to explain why in another thread, but please... stop derailing this.\n(I don't think I should try to answer any of @lukego or @plajjan comments here because this would just worsen the situation for @hanshuebner really.)\n. @lukego Ok. I ran the rebase I proposed just out of curiosity this afternoon and it indeed needed a ton of manual conflict resolution. So its no surprise a few small things slipped through. \n@hanshuebner I threw out ~20 commits because they didn't apply to the current master anymore. I also adjusted a few documentation changes that were still targeting the pre-christmas tree.\nI wonder if we can remove doc/device-interface.md? As far as I can tell its obsoleted by the current src/README.md (App chapter).\nAlso for simplicities sake I might want to leave out a few bench_env related changes that might benefit from their own PR.\nI will post you guys with a rebased version of the branch for review once I polished it a bit further.\n. @hanshuebner How do I test the solarflare driver? I hear there is a solarflare NIC in chur, which device is it?\n. @lukego Can't get rid of this using make clean && make, any hints?\nselftest: ./snabb binary portability\nScanning for symbols requiring GLIBC > 2.7\nGLIBC_2.14 memcpy\n^^^ Error ^^^\n(You might just need to 'make clean; make' at the top level.)\nEXITCODE: 1\n. >  That directory should probably not stay in the mainline once the driver is integrated, I just find it useful as a quick confidence test.\nA standalone driver test would be important to keep, can we turn it into a selftest similar to make apps.intel.intel_app? The simplest way to add it as a unit test is to add an executable src/program/solarflare/selftest.sh shell script that invokes the test and exits with status 1 on failure, status 0 on success and status 43 if skipped (for instance if the hypothetical environment variables SNABB_TEST_SOLARFLARE_PCIDEV[A|B] are not set).\n\napps/solarflare/solarflare.lua:23: ef_vi library does not have the correct version identified, need 201502, got djr_b1c7f30a84a0\n\nWhat is this about? Is this something external or did I miss a submodule update along the rebase?\n. > Are you using grindelwald? \nNot yet. ;) I just asked because I get that GLIBC > 2.7 error too and thought I maybe they are related and I broke something along my rebase.\n. > Could be that you need to make clean at the top-level e.g.\nI did that and it didn't fix things.\n. @lukego Got it was a mistake when merging on my side.\n. @lukego @hanshuebner Requesting review on #409 (Solarflare branch Rebased)!\nI have tested this on grindelwald to full success. I have to say I am very impressed that the NFV selftest works flawlessly with the solarflare NIC, I know its supposed to be that way but anyways: Great work Hans! :beers: \n. Should zone_gbps maybe be an upper limit to rx_police_gbps and rx_police_gbps?\n. @lukego Assinged you just for pinging purposes.\n. Uhm yeah I can explain that failure, I forgot about that case it should return a skipped code (43) I guess.\n. Wait, failure is this:\n\napps/solarflare/solarflare.lua:23: ef_vi library does not have the correct version identified, need 201502, got djr_b1c7f30a84a0\n\n@lukego That library is as far as I know unrelated to hardware. Should it be a submodule?\nE.g. I don't feel comfortable adding a special case to skip when we can't find this library because its not really a case of not having the right hardware. The driver module can't be loaded without that library at all.\n. @lukego I pushed a snabbmark test specific check that will skip the test if it can't load the solarflare module to solve this for now.\n. Oh and I just realized I kind of broke snabbmark solarflare.... I added a 10 second timeout as a general error case, so that the benchmark makes sense as a selftest, but obviously that prevents you from testing with more packets.... just a second! :)\n. Ok I added an optional timeout parameter to snabbmark solarflare to provide a safeguard for the selftest. Let's see what SnabbBot says but should be good to go.\n. @lukego Yeah sorry was playing around with plotting our perf data and disabled it for a little while (and then forgot to reenable).\n. @lukego Not sure wasn't able to reproduce. Although the \"benchmarks\" failure is a bug in the CI AWK script.\n. @lukego Performance regression in the iperf benchmark. Seems reproducible but I can not explain why. At first I suspected my changes to vhost_user but they were not the issue. When run manually cperf.sh check reports:\n```\niperf-1500 409a93f 3.082 0.0597997\niperf-1500 2babe20 3.136 0.134104\niperf-jumbo 409a93f 6.468 0.194874\niperf-jumbo 2babe20 6.692 0.271028\n```\nThese results don't reproduce SnabbBot's results so I forced him/her/it to rerun a third time and hope this was just a fluke.\n. eh... doh! Since cperf now persists results SnabbBot obviously repeatedly reported the fluke...\n. @lukego If its necessary and it works I have no objections! :)\n. @lukego  Hrmm I just noticed this would mean to basically copy the whole PacketFilter configuration docs... and how formal should the grammar description be? E.g. if we don't mention that its a Lua expression we kind of need to define the syntax for strings and numbers....\n. eh this is merged already :(\n. Yup, will rerun!\n. @lukego I simplified this a bit and added detailed explanations of the code while adding the getting started guide to the source tree. We should be able to see if the explanation gets more or less complicated when revised for possible API changes. I have one problem though: I can not require require(\"program.example-spray.sprayer\"). The file is there but I get this error:\nprogram/example-spray/example-spray.lua:4: module 'program.example-spray.sprayer' not found:\n    no field package.preload['program.example-spray.sprayer']\n    no file './program/example-spray/sprayer.so'\n    no file '/usr/local/lib/lua/5.1/program/example-spray/sprayer.so'\n    no file '/home/max/snabbswitch/deps/luajit/usr/local/lib/lua/5.1/program/example-spray/sprayer.so'\n    no file '/usr/local/lib/lua/5.1/loadall.so'\n    no file './program.so'\n    no file '/usr/local/lib/lua/5.1/program.so'\n    no file '/home/max/snabbswitch/deps/luajit/usr/local/lib/lua/5.1/program.so'\n    no file '/usr/local/lib/lua/5.1/loadall.so'\nstack traceback:\n    core/main.lua:96: in function <core/main.lua:94>\n    [C]: in function 'require'\n    program/example-spray/example-spray.lua:4: in main chunk\n    [C]: in function 'require'\n    core/main.lua:46: in function <core/main.lua:32>\n    [C]: in function 'xpcall'\n    core/main.lua:101: in main chunk\n    [C]: in function 'require'\n    [string \"require \"core.main\"\"]:1: in main chunk\nAny hints in debugging this would be welcome. The file src/obj/program/example-spray/sprayer_lua.o does exist.\n. Oh, could this be related to the dash (-) character in the directory name? Should I replace it with underscores?\n. Yup it was the dashes. Fixed.\n. Oh I just added the prose like yesterday so you couldn't have known. :)\n. You know you kind of introduced me to git rebase -i so you reap what you sow I guess. ;P\nI don't feel like it will be awkward to have the examples in the main tree. The \"example_\" prefix makes it quite clear that these are not \"normal\" programs. I personally am always glad if I clone a repository and have some simple examples included that I can digest.\nIf we had something like a \"production LTS only operator use\" release, then we could simply tag a HEAD with the examples removed.\nOne thing that I see is that the example_replay is actually a useful program. example_spray is not though (as far as I can tell). It would be really elegant to have the \"examples\" just be simple but useful production programs/apps that just happen to be written and documented with great care and love.\n. Sure, go ahead! :)\n. >  (process:29278): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\nThis is a phantom. I reproduced the jumboping failure without this message appearing.\nWhen I increase default.mtu to 9062 jumboping succeeds but the subsequent iperf fails. When I set default.mtu to 16380 iperf succeeds too. So it appears to be indeed related to the MAXFRS register.\n. Regarding running the snabbnfv selftest I can only refer to https://github.com/SnabbCo/snabbswitch/blob/master/src/program/snabbnfv/README-selftest.md\nDo you have access to chur? If so you can copy over the references assets.\n. I can only access davos too.\n. @lukego Yeah sure, on it!\n. @lukego The failure was due to a missing , and (the important part) using ip6 and icmp instead of icmp6. Apparently ip6 and icmp means \"IPv6 and ICMPv4\". This is probably also a bug in neutron2snabb. I just checked it, neutron2snabb doesn't implement icmp as a possible protocol value.\n. https://github.com/Igalia/pflua/issues/159\n. @lukego Not sure if you noticed, I created a PR updating the documentation, the fuzzer and fixing a few bugs on your fork: https://github.com/lukego/snabbswitch/pull/1\n. @lukego Yup, reran it and passed.\n. @lukego It would be nice to have some way to differentiate between intentional errors (e.g. thrown using error) and cases we did not see coming. But since we can not \"enumerate\" all possible error conditions I guess we'll have to fix 'em when they pop up.\n- [x] Review all calls to error and their messages.\n. The error messages we throw aren't that unclear (maybe driver errors should always print the PCI address of the device used?) but they don't advertise where they actually come from so a stack trace is the only way to figure that out. On the other hand end users won't care.\n. I am not sure I like the local snabb = ... pattern. Currently we do have the core API globally accessible to every file without doing any requires and without the snabb. prefix. Do we want people to explicitly specify an API version in each of their files? I feel this should be handled in a declarative way independent from the code (e.g. a build option) and I feel the option of using multiple API versions in one file is not important.\n. What if the snabb variable was implicitly defined (globally), then you could redefine it manually (local snabb = require(\"semver\")) or through some Makefile trickery.\nI think we should eliminate patterns that will effectively will be the same in almost every file, e.g. a common header. I feel the same way about our module(...,package.seeall) dance.\n. I just meant that an app would use the latest and greatest by default or lock to a specific version using the local snabb assignment.\n\nand passing incompatible objects.\n\nThis is a good point. To be honest to me the multi-api-version scenario sounds a bit wonky to me. I feel like keeping radical API revisions in a long lived feature branch until we are happy with it might be the simpler and more failure proof approach.\nHow about divide and conquer? Split this train of thought into (a) a branch with the API we want, (b) considerations on how/when to integrate it. But let us not get one in the way of the other.\n. I personally feel like right now is still a sensible moment to make radical changes, but we should ask Alex and the people who actually feel the pain.\n. > Make filename used by lib.ipc.shmem.top configurable to allow for multiple Snabb Switch instances (figure out a scheme)\n@lukego Do we currently have a policy for where Snabb instance files should go? How about multiple instances? Something like /var/snabb/<pid>/... or /tmp/snabb/<pid>/... (last one has the advantage of being user writable by default)?\n. @lukego Multiple instances are supported in eugeneia/snabbtop-fs, still a WIP though.\n. > I hope that we will be able to use #513 to more easily expose state from the core and the apps to SnabbTop but we will see.\n@lukego Me too, the friction between lib.ipc.shmem.top and core.app is what I feel most unsatisfied about.\n. Its not ready yet, I am still working on using the new core.shm for snabb top. Next release! :)\n. @lukego I just pushed a version of snabb top that uses core.shm/core.counter. It shares each links rxpackets/... as counters under /links/<linkspec>/rxpackets. It also adds a new configs counter that gets incremented with each configuration so snabb top can tell when to reopen the link counters.\nThis branch also moves all the engine stats variables into counters under /engine. You should read over the code, as it touches quite a bit of the hot path. I am sure we will find things that we want to do differently.\nCurrently there are a ton of test failures on tests that try to read link.stats I assume. But the NFV selftest and snabb top run.\n. Closing because this branch's history is fubar. I will compile this branch's work in a fresh branch.\n. @lukego Merged into eugeneia:documentation-fixups (am I doing this right?)\n. The Installation and Configuration with Neutron (previously \"Software Installation\") have intersecting content. We should either:\n- Merge them into one document\n- Split the scopes clearly (which?)\n. Actually Configuration with Neutron (previously \"Software Installation\") seems outdated. I will rm it.\n. @lukego Could you go through the Wikis and delete any pages that you can part with? Right now this PR imported:\n- Documentation Guide\n- (Getting Started Guide is already moved)\n- NFV Architecture\n- NFV Compute Node Requirements\n- NFV Installation\n- Neutron API Extensions\nOr alternatively tell me what I missed. I can't find any more non-outdated or obsolete pages. One edge-case: \"Snabb Lab\" should probably just stay in the Wiki, right?\n. Since you kept Device Driver Programming I reckon I should import that too? Regarding Writing an application configuration, should we really keep it around? Seems to duplicate info from the \"Getting started guide\".\nRegarding \"Scenario tests\", I honestly don't understand much of its content.\n. Closing because this was merged into next.\n. @lukego I hacked something up that generally works: https://github.com/eugeneia/snabbswitch/compare/snabbtop...eugeneia:snabbtop-fs\nIt implements the /var/run/snabb/<pid>/<resource> scheme. snabb top will either select the only running instance or expect the user to specify one (by pid). Both core.app and snabb top agree on using a shared \"resource\" (\"core-stats\").\nThere is one (recurring) problem though: We need a reliable way to run cleanup code on abnormal exits. In this case its important that the  directory is deleted when snabb is killed so that we don't leave behind stale resources. That probably mean reading a signalfd during pace_breathing?\n. I'll play the UNIX's advocate here. I think using the PID (publicly) is smart because the process starting Snabb can get it without much knowledge about Snabb internals. Furthermore the calling process is generally in a better spot to manage the instances resources:\n- It can easily tell when Snabb exits\n- It can create symlinks for arbitrary purposes (snabb top doesn't care whether the identifying string is a PID or \"foo\")\n- It can archive / snapshot the resources\n- Enabling arbitrary management of the instance resources would require as little as an option to snabb: \"Don't cleanup the instance directory.\"\nWhile I see the merits of the behaviors you propose, I think they don't need to be implemented by Snabb Switch. I'd say: have a simple and functional default behavior and enable independent extension. The idea here is: Operators love simple interfaces and flexibility.\n. Crystal clear! :)\nI like the proposed workflow because I imagine it will lift some pressure of @lukego to vouch for every single PR and decentralize some of the meta arguments (like how to merge X). I know luke loves to get his hands dirty so I reckon he will have more time for that and spend less time on micro managing? :)\n. @plajjan The momentum of Snabbo:master will decelerate. Instead of many small PRs it will advance in bigger chunks of better tested and integrated merges. This means less work for people hacking on master and hopefully less work related to merge conflicts in general.\nEach \"topic branch\" will also be more independent and can choose its own pace / relation to master.\n. > Is SnabbBot process nfv btw? or do I need to ge my bench_env\n\nworking again? :)\n\nI assume you mean if SnabbBot will run for PRs on the snabb-nfv repo? No\nit won't. I can make it so tomorrow. For now SnabbBot will run every\nminute (e.g. as fast as it can) on SnabbCo/snabbswitch though. That is\nwith a lock so that it won't interfere with itself.\n. Whoops I did see that PR at some point, well there goes short term memory. Your PR (#489) solves this in a generally neater way (no useless loop) but it doesn't check for is_usable. I am not sure if it should since the driver(s) unbind(?) the device anyways?\n. @lukego f5f7175 was not ready please merge the rest of the PR. (It's quite usable now).\n. Yeah I hadn't intended for snabb top to land in next either. Let's give it more testing next month.\n. The title confuses me. Shouldn't it be updated to say 2015.06(.1)? If so should I do some sort of bug fix release tomorrow? If I remember correctly you requested weekly bug fix releases (which is quite frequent, no?).\n. Seems sane to me. So as of now fixes does not contain new commits. I'd suggest we update the title as soon as it does.\n. Oh right I mistakenly did mention that commit in the Kiwi release even though it wasn't in. My bad, fixed.\n. @lukego Retest still fails:\nSUBMODULES\nError: Submodule version mismatch\nluajit:\nrequire: v2.0.4-296-g5de95ed\nfound: v2.0.3-365-g5de95ed\nljsyscall:\nrequire: v0.10-65-g7081d97\nfound: v0.10-65-g7081d97\npflua:\nrequire: 13b194e7a214faae5c3d2916a998418fc841dbc2\nfound: 13b194e7a214faae5c3d2916a998418fc841dbc2\nPlease update and rebuild submodules.\n. @lukego There was some PCI related cleanup that fell into the same category.\n. That would work for the runtime files case but not so much for the \"disable DMA\" case (unless we persist which devices are open). I generally like this idea because we can add it later on and just stop thinking about it right now. (I can deal with the stale runtime files issue right now by running snabb gc in bench_env and/or manually).\n. Note to self: never delete branches......\n. Closing because #930 landed.\n. @lukego I started working on porting Snabb Top to core.counter, which will only work together well if I update the existing core statistics counters to use the new core.shm too (frees, ..., breaths). Just wanted to let you know to avoid us both potentially doing the same thing.\n. @lukego I had SnabbBot rerun it and it seems fine now.\n. If you wonder while this failed: don't. Rerunning it now.\n. @lukego How are we doing on the release? I could do it today. Couldn't hurt to include #530 and #531 as well.\nEdit: Also #534 (just minor documentation fixes)\n. https://github.com/SnabbCo/snabbswitch/releases/tag/v2015.07\n. @javierguerragiraldez Thanks for doing this. I was slightly annoyed by tabs every now and then too.\n@lukego Merging this right before release sounds good to me.\n. I consider test_env to be ready: I cold-tested it on grindelwald and it worked without requiring user intervention (except having to upgrade g++ to 4.9, since qemu wouldn't build otherwise). The work on test_env will not be done until it supports all use cases currently supported by bench_env (what are those?).\nIn this branch the snabbnfv selftest automatically uses test_env (when run via SnabbBot) davos. bench_env is not removed and will work unchanged.\n. > I would like to host the assets somewhere other than the lab. The lab is not always online and its uplink is much slower than its downlink.\nI consider the assets being hosted on davos OK for now because the uplink (2 MB/s) is enough to fetch the (compressed) assets in two minutes and there will not be much demand for the assets (you get them once and they pretty much never change).\n\nI wonder what is the easiest way to host them on a CDN? (Dropbox? S3? Google Drive?)\n\nIf we move them we should make sure they are delivered over HTTPS.\n. > I would like to work on diversifying. I think our core infrastructure should all be on professional hosted platforms (Github, etc) and that our supplementary infrastructure is more spread out (e.g. servers with interesting hardware to run tests on).\nI can look into setting up a static hosting solution based on Amazon S3 for snabb.co (which could then also host other files) but the easiest way to get a HTTPS capable download for the assets would be to use an existing DropBox account I guess.\n\nOne very important test case that is often hard to setup is the benchmark pushing packets through a DPDK virtual machine. The benchmark should run N parallel instances and measure how many packets/second can be forwarded through them. This has been the key benchmark in our NFV optimization efforts and it is really a pity that we don't have it hooked into the CI yet.\n\nThis should be the next test case supported by test_env then. On it!\n\nRhetorical question: Who should use test_env and why?\n\nIt should be used by the people writing integration tests. It's purpose is to launch reproducible test environments without requiring setup or configuration by the person running the test.\n\nThe README defines it as a replacement for bench_env but most people will not know what that is. Is test_env a component of the snabbnfv test suite?\n\ntest_env is indeed very snabbnfv centric as of now. I am not sure if it will ever be useful for testing anything else. I would categorize it as \"Snabb Development Glue\".\n\nWhen would you use it directly instead of via program/snabbnfv/selftest.sh?\n\nWhen you want to run VM(s) connected via snabbnfv to telnet into. E.g. poke around, manual testing.\n\nCould tmux simplify test_env?\n\nI am not sure, not a big user of tmux or screen. Can you propose a pseudo session using a tmux based test_env? \n\nto terminate the processes am I supposed to kill the shell that sourced test_env so that the shutdown hook runs?\n\nYes, test_env works just like bench_env in that regard. E.g. when your test script using test_env exists all VMs and snabb instances will be killed and temporary files cleaned up.\n\nCan we delete bench_env once we merge test_env? If not then why not?\n\nWe can delete bench_env once all its use cases are supported by test_env (e.g. DPDK).\n\nand it looks like the test suite ran OK. Slightly ambiguous because I do see errors scrolling by and have been assuming those are expected e.g. this timeout:\n\nThat's all fine. The timeout is PcapFilter successfully dropping unwanted packets. :)\n\nNo, I didn't use bench_env. I don't need VMs for for my performance tests, just a loadgen on one of the looped NICs and an instance of my app. I simply do that manually.\n\ntest_env could help with that by automatically taking care of locking and NUMA node binding based on the PCI address.\n. @lukego Do we have a working setup of the DPDK based test somewhere?\n. ### Hosting assets\n\nCan we host the test assets on Github? They could have their own repo and we could even link that as a submodule e.g. program/snabbnfv/test-assets. I have the impression that Git has some extensions for dealing with large files and that Github supports this use case up to a point, but I am not sure what point that is :-).\n\nhttps://help.github.com/articles/conditions-for-large-files/ says:\n\n\"If you push a file larger than 100 MB to GitHub, Git will reject the push and tell you which file is too large\"\n\ntmux\n\nCould even be that tmux would simplify sending commands to VMs via their stdio but I don't know if that would really be an improvement.\n\nAs far as I can tell tmux only supports send-keys as far as scripted input goes, which is no improvement over telnet (on the contrary it seems complicated to get the command output).\n\nCheck it [tmux] out and see what you think?\n\nI have a working branch that uses tmux instead of Bash's & (https://github.com/eugeneia/snabbswitch/tree/test_env-tmux-refactor), it doesn't change the original behavior except that you can do a tmux attach-session and find windows for each process started by test_env. I think this might come in handy for debugging and should not be intrusive otherwise.\nShell functions vs Shell scripts\n\nalso a thought: is it important for test_env to be based on shell functions that are sourced, or could it also be based on stand-alone scripts e.g. to start QEMU with the right arguments? Separate scripts could be handy for ad-hoc reuse in other contexts. Generally I am a lot more comfortable with adding a directory to my $PATH to access some handy scripts like snabbnfv-qemu than with sourcing a library of shell functions.\n\nThe main difference between sourcing a script and running a script is that sourcing a script can modify the current environment. So while shell functions can set up exit hooks and keep track of PIDs, if test_env was based on shell scripts then the user would have to manage the processes themselves. (E.g.: snabb_pid=$(test_env/start_sabb.sh ...)) Edit: Or use other means of persistence, e.g.files.\n. > If we are using tmux then we don't need to track $snabb_pid anymore, right? We can just kill the tmux session and it should take down all the child processes with it? (Looks like that is how you are doing it already?) So maybe shell scripts instead of shell functions is easier to adopt in this model too?\nGenerally yes, its doable. Practically its still super annoying though. test_env has some implicit counters $snabb_n and $qemu_n that are incremented and spliced into log and asset paths (so the user doesn't have to say, log this first snabb switch instance to snabb0.log. etc). Also it still cleans up Vritio socket files on exit. Plus handy functions such as ip and mac used by the NFV selftest to get network addresses of VMs by index.\nAll these things can be solved using temporary files (as of now only one test_env instance per user is supported anyways) but after starting to refactor to shell scripts I want to stop and reflect a bit. A shell script based test_env would end up being more awkward for the NFV selftest to use. Maybe the concerns using source can be alleviated by running bash -rcfile <test_env>? Are we trying to do too many things at once here? If we want a \"swiss army knife\" for automatically and manually testing multiple NFV scenarios is Bash the right language? (remember there is a DPDK case yet to come, and on the ML we have been talkling about testing with multiple QEMU versions, this is getting more complex fast). I am comfortable with a 200 line shell script but once test_env gets as large as bench_env it will be a pain to develop.\n\nIt is easy to imagine generalizing this to other use cases like @kbara's too, e.g. if you want to automatically start two QEMU guests with well-known vhost-user socket names, but then be able to start/restart the snabbswitch instance manually. But if the qemu startup is a stand-alone script then perhaps you just call that manually instead of using the framework to start the tmux.\n\nThe only thing missing to support @kbara  is that test_env's snabb and qemu wrappers also accept NUMA nodes directly instead of only PCI addresses. I have a added a commit that does this to the tmux branch, now you can:\n```\nbash -rcfile program/snabbnfv/test_env/test_env.sh\nqemu   \nqemu   \nsnabb  \n```\n. I fast-forwarded the tmux branch into this one. I think is strikes a good balance between simplicity and interactivity. My next goal would be to add a DPDK based environment.\n. @lukego I need to know though: What QEMU parameters should a DPDK powered VM be started with? What kind of snabb instance fits in between packetblaster and a DPDK VM specifically?\nEdit: It's the same as without DPDK except a different NFV config right?\n. Heh, I just ran into that myself.\n. @lukego Try this:\ncd snabbswitch\nflock -x -n /home/max/snabb_bot/lock taskset -c 6 /home/max/snabb_bot/tasks/test.sh\nflock -x -n /home/max/snabb_bot/lock /home/max/snabb_bot/tasks/test.sh\n. That seems to have  fixed it. The ND failure was actually caused by a race between the check and Snabb applying the config and kicking off the ND resolution. See https://github.com/SnabbCo/snabbswitch/pull/531\n. It was the IOMMU change, #531 fixes an unrelated problem with the test suite that I falsely believed to be related.\n. It would be nice to gain insight in how IOMMU affects our code. From what I could gather its a feature geared towards virtual machines efficiently using hardware, and is indeed related to DMA. Could it be a safety feature of IOMMU that prevents successful DMA from the snabb process or the NIC (depending on which NUMA node its running on)?\nhttps://en.wikipedia.org/wiki/IOMMU\nhttp://www.mjmwired.net/kernel/Documentation/Intel-IOMMU.txt\nAbove is the only Linux documentation I can find about IOMMU, it has one interesting bit:\n```\nFault reporting\nDMAR:[DMA Write] Request device [00:02.0] fault addr 6df084000\nDMAR:[fault reason 05] PTE Write access is not set\nDMAR:[DMA Write] Request device [00:02.0] fault addr 6df084000\nDMAR:[fault reason 05] PTE Write access is not set\n```\nSo I assume we should look out for error messages like these in syslog (I checked davos and the only DMAR it has is this one: Jun 29 09:13:29 davos kernel: [    0.000000] ACPI: DMAR 000000007e00b300 000190 (v01 A M I   OEMDMAR 00000001 INTL 00000001)\n. @lukego Its simply a matter of enabling it in SnabbBot, which I will do upon release. Otherwise SnabbBot would fail this bench on PRs targeting 2015.06.\n. Aha!! It works! Was indeed a mac address thing. Ideally I would want a NFV config that forwards ALL packets to a port. E.g. wildcard MAC.\n. Closing because #547 landed.\n. Closing because I didn't intend this PR to target master. Oops.\n. @lukego Yup, on it!\n. This should go to next, ignore!\n. @lukego There actually was an error in this fix. Added a commit to get it right.\n. @lukego On it!\n. Sure! :)\n. Have a script that interacts with the GitHub API to publish releases? I imagine it would:\n1. Get the status of the next branch\n2. If OK, extract merges to build change log and push release using GitHub API\n3. Maybe check a custom \"release\" status that indicates manual blocking of a release and/or points to a hand written change log.\nE.g. I imagine the release bot could check for a snabbswitch/VERSION file that contains the release name. If it doesn't exist it sets a \"no release\" status.\n. @lukego I got one more though: #586\n. @jeffbrl No it can't as of now. It would be certainly possible to make it do so though. Keep in mind that Snabb NFV uses the built-in switching of its underlying NIC, so you would need to plug a bridge app between RawSocket and the ports. You might want to checkout our ongoing brainstorm on labswitch, here is a very generic program that lets  you define switched ports in an arbitrary ways.\n. Included in #577 \n. Closing, I want to merge this into next actually.\n. Oh we are using scanners now? I love scanners!\nThis maps fairly well to my existing labswitch branch. Currently the bottleneck in my design is that all L2 ports share the same bridge app, which obviously wont scale. Now I have this possibly stupid idea for a design, where every L2 port has its own bridge app, which is connected to every other port's bridge app in an N:N mesh.\n\nThis is analogous to a physical \"everyone-to-everyone\" wiring, except we don't get to maintain a cable matrix by hand. The mesh ports obviously need to be in a split horizon group in order to avoid cycling packets around. See apps/bridge/README.md.\nThe idea is that there is no central bottleneck and switching is work is given to the ports (and their CPU cores) that actually care.\n. I implemented the sketched design in my lab-switch branch. It works as imagined but pays a ~30% performance toll (measured by iperf), which is to be expected since each packet travels through 30% more links I guess?\n. @da4089 By STP you mean Spanning Tree Protocol? I do not think STP applies here. This is all virtual, we do not pay a cost for having more links, just for hops. Maybe I am not getting it, what do you think would applying STP give us?\n. I added a snabbmark for snabblab in my lab-switch branch:\nsnabbmark labswitch <npackets> <packet-size> [<nstreams>]\n    Test labswitch performance by sending <npackets> of <packet-size>\n    over <nstreams> through the switch. The default for <nstreams> is 1.\nThe results are... improvable. Will talk about the results tomorrow.\n. \n./snabb snabbmark labswitch 2e8 64 1\nStream 1:\nSending from 01:00:00:00:00:01 to 01:00:00:00:00:02\nSending from 01:00:00:00:00:02 to 01:00:00:00:00:01\nSwitched 200.0 million packets in 57.30 seconds (rate: 3.5 Mpps).\nlink report:\n         114,293,295 sent on bridge_stream_1_a.l2 -> sink_1_a.rx (loss rate: 0%)\n          85,720,035 sent on bridge_stream_1_a.stream_1_b -> bridge_stream_1_b.stream_1_a (loss rate: 24%)\n          85,720,035 sent on bridge_stream_1_b.l2 -> sink_1_b.rx (loss rate: 0%)\n         114,293,295 sent on bridge_stream_1_b.stream_1_a -> bridge_stream_1_a.stream_1_b (loss rate: 0%)\n         114,293,295 sent on source_1_a.tx -> bridge_stream_1_a.l2 (loss rate: 0%)\n         114,293,295 sent on source_1_b.tx -> bridge_stream_1_b.l2 (loss rate: 0%)\n\n./snabb snabbmark labswitch 2e8 64 2\nStream 1:\nSending from 01:00:00:00:00:01 to 01:00:00:00:00:02\nSending from 01:00:00:00:00:02 to 01:00:00:00:00:01\nStream 2:\nSending from 02:00:00:00:00:01 to 02:00:00:00:00:02\nSending from 02:00:00:00:00:02 to 02:00:00:00:00:01\nSwitched 200.0 million packets in 95.82 seconds (rate: 2.1 Mpps).\nStream 1: 76.93  million packets\nStream 2: 123.08  million packets\nlink report:\n          46,157,040 sent on bridge_stream_1_a.l2 -> sink_1_a.rx (loss rate: 25%)\n          30,771,360 sent on bridge_stream_1_a.stream_1_b -> bridge_stream_1_b.stream_1_a (loss rate: 0%)\n          30,771,360 sent on bridge_stream_1_a.stream_2_a -> bridge_stream_2_a.stream_1_a (loss rate: 0%)\n          15,386,190 sent on bridge_stream_1_a.stream_2_b -> bridge_stream_2_b.stream_1_a (loss rate: 49%)\n          30,771,360 sent on bridge_stream_1_b.l2 -> sink_1_b.rx (loss rate: 50%)\n          30,771,360 sent on bridge_stream_1_b.stream_1_a -> bridge_stream_1_a.stream_1_b (loss rate: 0%)\n          30,771,360 sent on bridge_stream_1_b.stream_2_a -> bridge_stream_2_a.stream_1_b (loss rate: 0%)\n          30,771,360 sent on bridge_stream_1_b.stream_2_b -> bridge_stream_2_b.stream_1_b (loss rate: 0%)\n          61,542,465 sent on bridge_stream_2_a.l2 -> sink_2_a.rx (loss rate: 33%)\n                 255 sent on bridge_stream_2_a.stream_1_a -> bridge_stream_1_a.stream_2_a (loss rate: 0%)\n                 255 sent on bridge_stream_2_a.stream_1_b -> bridge_stream_1_b.stream_2_a (loss rate: 0%)\n          30,771,360 sent on bridge_stream_2_a.stream_2_b -> bridge_stream_2_b.stream_2_a (loss rate: 0%)\n          61,541,955 sent on bridge_stream_2_b.l2 -> sink_2_b.rx (loss rate: 20%)\n          30,771,360 sent on bridge_stream_2_b.stream_1_a -> bridge_stream_1_a.stream_2_b (loss rate: 0%)\n          30,771,360 sent on bridge_stream_2_b.stream_1_b -> bridge_stream_1_b.stream_2_b (loss rate: 0%)\n          30,771,360 sent on bridge_stream_2_b.stream_2_a -> bridge_stream_2_a.stream_2_b (loss rate: 0%)\n          30,771,360 sent on source_1_a.tx -> bridge_stream_1_a.l2 (loss rate: 0%)\n          30,771,360 sent on source_1_b.tx -> bridge_stream_1_b.l2 (loss rate: 0%)\n          30,771,360 sent on source_2_a.tx -> bridge_stream_2_a.l2 (loss rate: 0%)\n          30,771,360 sent on source_2_b.tx -> bridge_stream_2_b.l2 (loss rate: 0%)\n. @javierguerragiraldez Yeah there was a swapped label in the first diagram, sorry. Should be fixed now.\n. Some thoughts about the results: The first case with only one stream is pretty straight forward, except for 25% packet loss on a link. The results are deterministic, so there seems to be some kind of math behind when links will drop packets. We should figure that out.\nThe second case is disappointing for multiple reasons, first of all there is heavy packet loss on multiple links again. Then there are only 120M packets sent from the sources and 200M received on the sinks. This is because many packets cross the \"stream border\", e.g. go astray. This means that either I am using the learning bridge app the wrong way, or the learning bridge app does not work as I expected. This also leads to reduced overall performance. Ideally we would see very few packets crossing the dashed line. I am doing something very wrong here, I hope its not too obvious... ;)\n@alexandergall thoughts? \n. > i'm assuming the mac addresses are like:\n\n1A: 01:00:00:00:00:01\n1B: 01:00:00:00:00:02\n2A: 02:00:00:00:00:01\n2B: 02:00:00:00:00:02\n\nThat is correct.\n. All packets that reach the bridge will be forwarded to its sink: Either a packet matches the source MAC of the port (from the packets sent by the source) and passes it to the sink, or it does not match the source MAC of the port in which case it gets flooded to all ports, e.g. the sink.\n. Doh! :)\n. With non multicast MAC addresses and a little fix on the benchmark side1, but without Alex's bug fix (what does it do?), the result now looks like this, which is nicer indeed:\nsnabb snabbmark labswitch 2e8 64 2\nStream 1:\nSending from 00:00:00:00:01:01 to 00:00:00:00:01:02\nSending from 00:00:00:00:01:02 to 00:00:00:00:01:01\nStream 2:\nSending from 00:00:00:00:02:01 to 00:00:00:00:02:02\nSending from 00:00:00:00:02:02 to 00:00:00:00:02:01\nSwitched 200.0 million packets in 109.33 seconds (rate: 1.8 Mpps).\nStream 1: 100.00  million packets\nStream 2: 100.00  million packets\nlink report:\n          50,002,185 sent on bridge_stream_1_a.l2 -> sink_1_a.rx (loss rate: 0%)\n          50,001,930 sent on bridge_stream_1_a.stream_1_b -> bridge_stream_1_b.stream_1_a (loss rate: 0%)\n                 255 sent on bridge_stream_1_a.stream_2_a -> bridge_stream_2_a.stream_1_a (loss rate: 0%)\n                 255 sent on bridge_stream_1_a.stream_2_b -> bridge_stream_2_b.stream_1_a (loss rate: 0%)\n          50,001,930 sent on bridge_stream_1_b.l2 -> sink_1_b.rx (loss rate: 0%)\n          50,001,930 sent on bridge_stream_1_b.stream_1_a -> bridge_stream_1_a.stream_1_b (loss rate: 0%)\n                 255 sent on bridge_stream_1_b.stream_2_a -> bridge_stream_2_a.stream_1_b (loss rate: 0%)\n                 255 sent on bridge_stream_1_b.stream_2_b -> bridge_stream_2_b.stream_1_b (loss rate: 0%)\n          50,001,930 sent on bridge_stream_2_a.l2 -> sink_2_a.rx (loss rate: 0%)\n                 255 sent on bridge_stream_2_a.stream_1_a -> bridge_stream_1_a.stream_2_a (loss rate: 0%)\n                 255 sent on bridge_stream_2_a.stream_1_b -> bridge_stream_1_b.stream_2_a (loss rate: 0%)\n          50,001,930 sent on bridge_stream_2_a.stream_2_b -> bridge_stream_2_b.stream_2_a (loss rate: 0%)\n          50,001,930 sent on bridge_stream_2_b.l2 -> sink_2_b.rx (loss rate: 0%)\n                 255 sent on bridge_stream_2_b.stream_1_a -> bridge_stream_1_a.stream_2_b (loss rate: 0%)\n                 255 sent on bridge_stream_2_b.stream_1_b -> bridge_stream_1_b.stream_2_b (loss rate: 0%)\n          50,001,930 sent on bridge_stream_2_b.stream_2_a -> bridge_stream_2_a.stream_2_b (loss rate: 0%)\n          50,001,930 sent on source_1_a.tx -> bridge_stream_1_a.l2 (loss rate: 0%)\n          50,001,930 sent on source_1_b.tx -> bridge_stream_1_b.l2 (loss rate: 0%)\n          50,001,930 sent on source_2_a.tx -> bridge_stream_2_a.l2 (loss rate: 0%)\n          50,001,930 sent on source_2_b.tx -> bridge_stream_2_b.l2 (loss rate: 0%)\n1:  I was counting packets received by the sinks, now I count packets transmitted to or dropped by the sink links. I believe that is a better metric, since it doesn't involve the sink app.\n. Yup, and that's whats happening. :+1: \n. @alexandergall I noticed that too, but its not a huge issue. I don't have any loss either way on my current lab-switch head. I can't reproduce the non-deterministic flooding behavior you mentioned. I am currently working on replacing the bloom filter with an under-engineered table based implementation (inspired by Luke's suggestion) to see if that has any merit.\n. So after a reckless attempt to reduce the cost of the learning bridge app I have learned that the order of expensiveness is as follows:\n1. The hash function\n2. Lookup in MAC table / filter\n3. Lookup of destination port filter\nI came to this conclusion by ripping out the bloom filter, replacing it with a Lua table and a dummy hash function for MAC addresses with neglectable cost. What is then left as the main cost is mapping the destination port to its MAC table and the source port to its destination port, e.g. plain Lua table lookups.\n. My current feeling is that the learning bridge app is too generic for this particular use case. I don't see us getting 30Mpps optimizing it (but I might be wrong!). My next step would be to design a switching app catering just for lab switch and see how fast I can get it.\n. I replaced the learning bridge with a minimal switching app with the following results:\n```\nSwitched 200.1 million packets in 12.10 seconds (rate: 16.5 Mpps).\n[Stream 1: 200.1 million packets]\nSwitched 200.0 million packets in 12.03 seconds (rate: 16.6 Mpps).\nStream 1: 100.02  million packets\nStream 2: 100.02  million packets\nSwitched 200.0 million packets in 14.15 seconds (rate: 14.1 Mpps).\nStream 1: 66.67  million packets\nStream 2: 66.67  million packets\nStream 3: 66.67  million packets\nSwitched 200.0 million packets in 16.11 seconds (rate: 12.4 Mpps).\nStream 1: 50.01  million packets\nStream 2: 50.01  million packets\nStream 3: 50.01  million packets\nStream 4: 50.01  million packets\n```\nThe most expensive operation here seems to be the Lua table lookup. So I doubt I can hit 30 Mpps this way. Ideas for a faster lookup and a better hash would be appreciated though! The profiler report looks like this:\n100%  Compiled\n  -- 21%  switch.lua:lookup < switch.lua:method < app.lua:with_restart < app.lua:breathe < app.lua:main\n  -- 15%  link.lua:receive < switch.lua:method < app.lua:with_restart < app.lua:breathe < app.lua:main\n  -- 11%  switch.lua:insert < switch.lua:method < app.lua:with_restart < app.lua:breathe < app.lua:main\n  --  6%  switch.lua:hash < switch.lua:method < app.lua:with_restart < app.lua:breathe < app.lua:main\n  --  6%  link.lua:transmit < switch.lua:method < app.lua:with_restart < app.lua:breathe < app.lua:main\n  --  6%  ethernet.lua:is_mcast < switch.lua:method < app.lua:with_restart < app.lua:breathe < app.lua:main\n  --  6%  snabbmark.lua:method < app.lua:with_restart < app.lua:breathe < app.lua:main < snabbmark.lua:labswitch\n  --  5%  link.lua:transmit < snabbmark.lua:method < app.lua:with_restart < app.lua:breathe < app.lua:main\n  --  4%  packet.lua:free < basic_apps.lua:method < app.lua:with_restart < app.lua:breathe < app.lua:main\n  --  3%  switch.lua:method < app.lua:with_restart < app.lua:breathe < app.lua:main < snabbmark.lua:labswitch\n  --  3%  packet.lua:data < switch.lua:method < app.lua:with_restart < app.lua:breathe < app.lua:main\n  --  3%  link.lua:receive < basic_apps.lua:method < app.lua:with_restart < app.lua:breathe < app.lua:main\n. I accidentally clicked \"close\", sorry!\n. @alexandergall Yeah I realized that while writing the comment and then accidentally hit close&comment, then I deleted my comment... sorry for the lost context!\n. I want to wrap up my experiments here a bit. My current lab-switch branch uses a simple switch app that manages to duplex switch 18 Mpps on grindelwald.\nIt can reach this throughput  because instead of hashing it simply uses the last three bytes of MAC addresses as learning keys (which as far as I can tell is good enough and has the nice side effect of yielding a reasonable upper limit of entries in the mac table: 2^24). It also does less, e.g. its an 1:N switch instead of an N:N bridge. One can get another performance boost out of its samplegap option which allow to specify that only 1 in N incoming packets is used to learn addresses. With samplegap=200 I get 20 Mpps switched.\nA test on real hardware (two packetblasters, two NICs, one labswitch instance with one intel app per NIC) gives me 7 Mpps. I don't think that's too bad is it? Unsure.\n. > Nice, though I do need the N:N case with split-horizon for my use case. How do you avoid loops without split-horizon? Do you just assume that the topology is loop-free?\nIt's not a drop-in replacement for bridge, but rather a replacement for bridge in the labswitch use-case. I needed the split-horizon semantics in bridge because labswitch used it as a 1:N switch (every port has its own switch). I don't need the split-horizon semantics now because there are no loops as far as I can tell: every packet that reaches a switch is delivered to the port, the packets sent by the port are delivered to one or all of the N links selectively. Does that make sense?\n\nI'm a bit uneasy about how colliding (truncated) MAC addresses are handled. In the worst case, the wrong host gets all the packets and the legitimate host none at all. That's a subtle failure that will drive you nuts when you hit it. And you can't detect it either, because the upper three bytes have been thrown away.\nI agree with @alexandergall that the truncated MAC method sounds very problematic. If you have two hosts on different ports with colliding MAC addresses then service will be badly disrupted for one or both.\n\nI am uneasy about this too as I am unable to prove/test real-world effect, here is the reasoning though:\n- For the collision case to happen there has to be two devices of the same vendor with the same last three bytes on the network, which is not impossible but unlucky depending on the number of devices.\n- Because every port has its own switch, the collision only causes a problem if the colliding devices talk to the same third port.\n- Because of learning (last learned address overwrites table entry) the collision only causes a problem if above happens at the same time.\nFor a lab setup with 10-100 MAC addresses I don't see a problem, 1000 MAC addresses could cause slight packet loss depending on traffic, with 10000 MAC addresses you will almost definitely have a collision that will prevent the ports with the colliding MAC addresses to talk to the same third port at the same time. It really depends on what you expect from the switch.\n. Would it help if the failure case was better? E.g. I've tested modifying the table to store key -> {mac, port} and lookup to assert key == key(mac') and mac == mac' which is not much slower. This ensures that collisions in the key space never lead to packets being send to the wrong port. If keys collide they simply overwrite the previous record, e.g. unlearning it.\n. @javierguerragiraldez You can use snabb gc to cleanup stale runtime files: #558 \nPersonally I feel adding new special syntax is overkill for this situation. We now have:\nfoo/bar => /var/run/snabb/<pid>/<path>/foo/bar  (for ourself)\n/foo/bar => /var/run/snabb/<pid>/foo/bar\n//foo/bar => /var/run/snabb/foo/bar             (for outside ourself)\nWe can use \"//\"..pid..\"/path\" to reference another processes resource, and depending on layout, define a function that builds a Group PID path for us, without having to support that specific layout in core.shm.\n. I am closing this because there is currently no activity. Please reopen it when necessary.\n. So we have a heavy performance regression on the basic1 benchmark here that warrants some investigation. I am not surprised that there some penalty to using counter.add instead of x = x +1.\nSee this graph (last commit is this PR's HEAD) for the complete results:\n\n. I am trying to get certain about what causes the slowdown here, I used require(\"jit.p\").start('5vlm1') to get some measurements:\nOn the snabbtop2 branch the profiler reports 100% or 99% compiled and in the latter case 1% GC or 1% JIT compiler (differs from run to run). Notable is that the top quarter is held by calls to core.counter.\nOn the master branch the profiler always reports 100% compiled. The top quarter varies from run to run but consists mostly of transmitting/freeing packets.\nIts a bit confusing that the reports differ quite a bit between runs, and I can not really tell were the GC is coming from, but I think its safe to say that core.counter is the bottleneck in the snabbtop2 branch.\nsnabbtop2\nProcessed 100.0 million packets in 10.20 seconds (rate: 9.8 Mpps).\n99%  Compiled\n  --  8%  counter.lua:34 < link.lua:64 < basic_apps.lua:110 < app.lua:72 < app.lua:294\n  --  7%  packet.lua:47 < basic_apps.lua:27 < app.lua:72 < app.lua:278 < app.lua:233\n  --  6%  counter.lua:34 < link.lua:63 < basic_apps.lua:110 < app.lua:72 < app.lua:294\n  --  5%  packet.lua:47 < basic_apps.lua:110 < app.lua:72 < app.lua:294 < app.lua:233\n  --  5%  counter.lua:34 < link.lua:47 < basic_apps.lua:83 < app.lua:72 < app.lua:294\n  --  5%  counter.lua:34 < packet.lua:87 < basic_apps.lua:84 < app.lua:72 < app.lua:294\n  --  5%  counter.lua:34 < link.lua:64 < basic_apps.lua:27 < app.lua:72 < app.lua:278\n  --  4%  counter.lua:34 < packet.lua:90 < basic_apps.lua:84 < app.lua:72 < app.lua:294\n  --  4%  counter.lua:34 < link.lua:46 < basic_apps.lua:83 < app.lua:72 < app.lua:294\n  --  4%  packet.lua:90 < basic_apps.lua:84 < app.lua:72 < app.lua:294 < app.lua:233\n  --  3%  counter.lua:34 < link.lua:46 < basic_apps.lua:106 < app.lua:72 < app.lua:294\n  --  3%  link.lua:46 < basic_apps.lua:83 < app.lua:72 < app.lua:294 < app.lua:233\n  --  3%  counter.lua:34 < link.lua:47 < basic_apps.lua:106 < app.lua:72 < app.lua:294\n  --  2%  link.lua:76 < link.lua:57 < basic_apps.lua:110 < app.lua:72 < app.lua:294\n  --  2%  link.lua:63 < basic_apps.lua:110 < app.lua:72 < app.lua:294 < app.lua:233\n  --  2%  counter.lua:34 < packet.lua:86 < basic_apps.lua:84 < app.lua:72 < app.lua:294\n  [...]\n 1%  Garbage Collector\n  -- 100%  app.lua:289 < app.lua:233 < snabbmark.lua:48 < snabbmark.lua:17 < main.lua:56\nmaster\nProcessed 100.1 million packets in 8.58 seconds (rate: 11.7 Mpps).\n100%  Compiled\n  -- 14%  packet.lua:90 < basic_apps.lua:84 < app.lua:76 < app.lua:297 < app.lua:234\n  -- 11%  link.lua:28 < basic_apps.lua:106 < app.lua:76 < app.lua:297 < app.lua:234\n  --  9%  link.lua:28 < basic_apps.lua:83 < app.lua:76 < app.lua:297 < app.lua:234\n  --  8%  packet.lua:46 < basic_apps.lua:27 < app.lua:76 < app.lua:281 < app.lua:234\n  --  5%  packet.lua:46 < basic_apps.lua:110 < app.lua:76 < app.lua:297 < app.lua:234\n  --  4%  link.lua:56 < link.lua:37 < basic_apps.lua:110 < app.lua:76 < app.lua:297\n  --  3%  link.lua:45 < basic_apps.lua:110 < app.lua:76 < app.lua:297 < app.lua:234\n  --  3%  freelist.lua:16 < packet.lua:81 < packet.lua:90 < basic_apps.lua:84 < app.lua:76\n  --  2%  link.lua:44 < basic_apps.lua:27 < app.lua:76 < app.lua:281 < app.lua:234\n  --  2%  packet.lua:86 < basic_apps.lua:84 < app.lua:76 < app.lua:297 < app.lua:234\n  --  2%  packet.lua:47 < basic_apps.lua:27 < app.lua:76 < app.lua:281 < app.lua:234\n  --  2%  freelist.lua:24 < packet.lua:44 < basic_apps.lua:27 < app.lua:76 < app.lua:281\n  --  2%  link.lua:44 < basic_apps.lua:110 < app.lua:76 < app.lua:297 < app.lua:234\n  [...]\n. > @eugeneia Looks like we also need to create /var/run/snabb with more liberal permissions:\nYeah. Make it sticky? Like /tmp so everyone can create PID sub directories but only the owner and superuser can delete?\n. Superseded by #568 \n. Closing because this PR is superseded by #683.\n. I can't build this branch as of now?\nmake[1]: *** No rule to make target `jit/vmdef.lua', needed by `obj/jit/vmdef_lua.o'.  Stop.\nmake[1]: Leaving directory `/home/max/snabbswitch/src'\nmake: *** [all] Error 2\n. The segfault is weird. Did we miss something? I can't think of a counter (e.q. configs) that is e.g. open for writing twice. The NFV selftest failing is probably related.\nCan't explain the (reproducable) intel failure either...\n. @lukego I found the bug and fixed it in a PR on your fork: https://github.com/lukego/snabbswitch/pull/4\nIf SnabbBot will be happy I think this is fit for merge and we can tackle the remaining issues when required (e.g. when multi-process snabb lands)?\nEdit: Except the permissions issue when creating /var/run/snabb, let me fix that. Edit2: fixed.\n. @lukego Ugh... I just manually tested next for release and realized snabb top doesn't actually work. First there is a bug in snabb top that makes it crash when deallocating counters. I actually have a fix for that and its not a big issue but now the link counters always read 0, still troubleshooting that one.\n. Ok got it, see #582.\n. You should try to run the NFV selftest:\ncd src; sudo TESTPCI=<pci_address> TELNET_PORT0=5000 TELNET_PORT1=5001 ./program/snabbnfv/selftest.sh\nThis requires tmux, wget, make, telnet and numactl in the PATH.\n. @dpino Success on the NFV selftest side! I am glad to see its easy to run the NFV selftest now (assuming it just ran out of the box for you).\n. > One idea would be that the selftest could have a \"warm up\" period. That is, if the link is not actually usable when it reports \"link up\" then the first step of selftest could be to loop until a packet is actually delivered (or timeout) before proceeding to the real tests.\nI want to chime in and just suggest we document the eventual \"workaround\" really well. Like a big flashy comment plus explaining it in the commit message. I remember debugging NIC drivers as being not fun, and think we had issues before when our 10G driver code-rotted slightly. I am very fond of the strict intel selftest we currently have a exactly because of that. Maybe even even do a ifdef 82599 T3 dependent relaxation?\n. I am always for simplicity but I wonder what is the benefit here? Did rx/txbytes actually cost us much? It seems like an interesting metric to me so I would consider keeping it.\n. The current application of this idea is in #601 (Inter proc: multiprocessing capabilities, take 2).\n. I built virtio_net as a module and a kernel without it being built in (CONFIG_VIRTIO_NET=m), then I installed that module into a copy of the test_env qemu image by hand (I know that's bad workflow, how do I do it right? E.g. as a modification to the bench_env/README). I ran luke's test case which consists of:\n```\nifconfig eth0 down\nrmmod virtio_net\nNever got to the steps below\nmodprobe virtio_net\nifconfig eth0 up\n```\nand got the following:\nlib/virtio/net_device.lua:357: mapping to host address failedcdata<void *>: 0xffff88001e80a430\nstack traceback:\n    core/main.lua:116: in function <core/main.lua:114>\n    [C]: in function 'error'\n    lib/virtio/net_device.lua:357: in function 'map_from_guest'\n    lib/virtio/net_device.lua:129: in function 'packet_start'\n    lib/virtio/virtq.lua:67: in function 'get_buffers'\n    lib/virtio/net_device.lua:122: in function 'receive_packets_from_vm'\n    lib/virtio/net_device.lua:108: in function 'poll_vring_receive'\n    apps/vhost/vhost_user.lua:76: in function 'method'\n    core/app.lua:76: in function 'with_restart'\n    core/app.lua:283: in function 'breathe'\n    core/app.lua:237: in function 'main'\n    program/snabbnfv/traffic/traffic.lua:127: in function 'bench'\n    program/snabbnfv/traffic/traffic.lua:59: in function 'run'\n    program/snabbnfv/snabbnfv.lua:15: in function 'run'\n    core/main.lua:56: in function <core/main.lua:32>\n    [C]: in function 'xpcall'\n    core/main.lua:121: in main chunk\n    [C]: at 0x0044e670\n    [C]: in function 'pcall'\n    core/startup.lua:1: in main chunk\n    [C]: in function 'require'\n    [string \"require \"core.startup\"\"]:1: in main chunk\n. @lukego Can we move the docker discussion into a separate issue?\n. @nnikolaev-virtualopensystems Does this mean this issue is \u201cfixed\u201d? Can we do anything further than ensuring the kernel was compiled with the right parameters?\n. Closing because I think this is superseded by #680 (except point 3 I guess, open a separate issue when required?)\n. I can look into this if you like. Just let me know so we don't end up duplicating effort again. ;)\n. Fixed in #679.\n. > @eugeneia SnabbBot not testing this change?\n@lukego I can't ssh into davos (or grindelwald for that matter), so maybe a network issue?\n. Closing because its a duplicate of solved issue #657.\n. @lukego Yup already started working on this, see: https://github.com/eugeneia/snabbswitch-docker\n. After tinkering on this for a while here is a progress update. My working branch can be inspected  here: https://github.com/SnabbCo/snabbswitch/compare/master...eugeneia:simplify-test\nTo just see the docker related changes see: https://github.com/SnabbCo/snabbswitch/commit/03ba0a710808fc4dbd8bd5eb84901e8f4e57e4d9\nFirst of all a summary of whats done (docker-wise):\n- There is a new top-level target make docker that will build a docker image containing a full Snabb Switch test environment.\n- In src/ you can call scripts/dock.sh <command> to run command on the current Snabb Switch tree within a fresh docker container based on mentioned image.\nIssues I have encountered include:\n- The Dockerfile is small but the script to build the test assets is \"big\". The reason is that the asset VM images are not docker images and docker is not designed to build \"child images\" (it would be useful if there was an easy way to turn docker images into disk images to be used by qemu). To build the VM images we use mount -o loop ..., chroot and possibly other (e.g. mkfs?) privileged commands. Since docker build can not be run in privileged mode, we need to invoke the asset building in a docker run followed by a docker commit. See https://github.com/docker/docker/issues/1916\n- The original idea of using volumes to share test assets does not work out, because volumes are bound to containers (instances of images) and can not be shared on DockerHub which is for sharing images.\n- I see a 30% performance degradation in the packetblaster/dpdk benchmark when run in docker. I have not yet been able to figure out why.\n- The resulting docker image is 5GB...\n. @nnikolaev-virtualopensystems @justincormack Maybe having the docker related files in a git submodule would be a good compromise.\n. I tried a different approach today which I think would be a better fit: Have a program/snabbnfv/test/vm/ubuntu/14.04/Dockerfile, and use docker export to \u201cburn\u201d the resulting tar archive onto a sparse RAW image suitable for qemu.\n```\n$ mkdir mnt\n$ dd if=/dev/zero of=qemu.img bs=1 count=0 seek=2G\n0+0 records in\n0+0 records out\n0 bytes (0 B) copied, 0.000332418 s,\n$ mkfs.ext2 -F qemu.img\nmke2fs 1.42.9 (4-Feb-2014)\nDiscarding device blocks: done                          \nFilesystem label=\nOS type: Linux\nBlock size=4096 (log=2)\nFragment size=4096 (log=2)\nStride=0 blocks, Stripe width=0 blocks\n131072 inodes, 524288 blocks\n26214 blocks (5.00%) reserved for the super user\nFirst data block=0\nMaximum filesystem blocks=536870912\n16 block groups\n32768 blocks per group, 32768 fragments per group\n8192 inodes per group\nSuperblock backups stored on blocks: \n    32768, 98304, 163840, 229376, 294912\nAllocating group tables: done                          \nWriting inode tables: done                          \nWriting superblocks and filesystem accounting information: done\n$ sudo mount -o loop qemu.img mnt\n$ cd mnt\n$ docker export guest | sudo tar x\n```\nSadly I couldn't get the docker-native user space to boot with our kernel yet:\nMount failed for selinuxfs on /sys/fs/selinux:  No such file or directory\n[   14.183740] random: init urandom read with 11 bits of entropy available\n[   15.109652] init: plymouth-upstart-bridge main process (73) terminated with status 1\n[   15.111734] init: plymouth-upstart-bridge main process ended, respawning\n[   15.752925] init: plymouth-upstart-bridge main process (83) terminated with status 1\n[   15.753559] init: plymouth-upstart-bridge main process ended, respawning\n[   16.179781] init: plymouth-upstart-bridge main process (89) terminated with status 1\n[   16.188983] init: plymouth-upstart-bridge main process ended, respawning\nThe Dockerfile I used:\n```\nFROM ubuntu:14.04\nMAINTAINER Max Rottenkolber (@eugeneia)\nRUN mkdir /hugetlbfs\nRUN cp /etc/init/tty1.conf /etc/init/ttyS0.conf\nRUN sed -i '$s/.*/exec \\/sbin\\/getty -8 115200 ttyS0 linux -a root/' /etc/init/ttyS0.conf\nRUN printf \"auto eth0\\niface eth0 inet manual\\ndns-nameserver 8.8.8.8\\n\" > /etc/network/interfaces\nRUN apt-get update && apt-get install -y ethtool tcpdump netcat iperf\n```\nAny hints whats the error here would be appreciated. If I could get this to work it would be really something!\n. Turns out I needed to undo a dockerism introduced by FROM ubuntu:14.04, namely re-enable the init system.\n```\nReactivate init, see\nhttps://github.com/tianon/docker-brew-ubuntu-core/blob/dist/trusty/Dockerfile\nRUN rm /usr/sbin/policy-rc.d \\\n    && rm /sbin/initctl \\\n    && dpkg-divert --local --rename --remove /sbin/initctl\n```\nWhile it is slightly ugly to have this magic in the Dockerfile, it works! I can docker build/create/export a VM image to use in test_env.\n. Today I added a Makefile target make test_env that will build the guest image, kernel and qemu. The exact versions of each can be controlled by make variables. E.g.\nmake test_env\nwill build the default NFV test environment (program/snabbnfv/test/{vm/ubuntu/14.04,kernel/ubuntu-trusty,qemu/SnabbCo}).\nIf I would want to test with a different VM image I would have to create e.g. vm/ubuntu/12.04/Dockerfile and then run\nmake NFV_GUEST_OS=ubuntu NFV_GUEST_VERSION=12.04 test_env\nLikewise there are variables for building other kernels and qemu versions (NFV_GUEST_KERNEL, NFV_QEMU). While new guest images can be defined as Dockerfiles, kernels and qemu versions can be added in form of Makefiles that produce a bzImage or qemu builds respectively.\nGoing further, I would now add a Makefile target to build Docker images containing different  test_env builds which could then be distributed using DockerHub and used by others to run test.\nRight now its still in proof of concept state: No DPDK related stuff is built, I ignored kernel modules for now, the Makefile needs some more work as some of the additions I made don't play well with existing rules (don't try to run my snabb-docker branch yet)... details. :)\n@lukego What do you think? I obviously like it and I think it serves all our use cases:\n- Express all asset creation in a formal way and automate building\n- Allow for zero-effort testing\n- Allow for testing with different environments (e.g. power user, Snabb Bot instances)\n. > 1. Will it be easy to take ready-made test environment from Dockerhub and run it with a new version of Snabb Switch? (I mean e.g. if we have 20 test environments defined and the CI wants to run each of them to test a PR.)\nI would like to handle this by \"mounting the Snabb tree\" in containers as volumes. E.g. images never contain Snabb Switch but instead we have an entrypoint/script that launches a container with the current PWD \"mounted\" under /snabbswitch. This script could have an option to run a shell instead of snabb for interactive testing. A volume here is not a container-owned docker volume but a \"host volume\" (e.g. docker -v /host/path:/container/path).\n\n\nCan we use this for ad-hoc testing? For example, would I be able to quickly drop into an interactive tmux session in a container with various interesting assets available and ready to run (e.g. a specific QEMU and a Juniper vMX guest image that I would start manually)?\n\n\nSee above. In this case we could just run a shell in the container and do anything we would do on the host.\n\n\nDoes building QEMU and virtual machines really belong in src/Makefile? Or is it specific to the NFV application and better contained in src/program/nfv where it will not confuse/distract/annoy people who don't care about NFV?\n\n\nNo. Maybe this should even go in a git subtree. I ran into problems with having a kernel tree checked out below src/ and src/Makefile slowing to a crawl due to millions of files. So if we want this below src/ we need to deal with that too.\n\n\nHow should the \"user interface\" to individual test cases really look? For example selftest of a NIC. Is it better to provide arguments as environment variables, command-line arguments, or Lua scripts? I mean that even if we like SNABB_PCI0 and so on for running the entire CI test suite we might prefer a different mechanism for interactively running individual test cases. (This comment comes specifically from seeing snabbmark move one of its parameters from the command line into an environment variable and wondering whether this is the right trend.)\n\n\nIn our tests/bechmarks, environment variable act like keyword parameters and provide a super-set of the functionality provided by regular parameters. We could support both but that would add lots of boilerplate for little or no additional functionality.\nRegarding file size: Currently I use the spare file approach mentioned by @kbara and so far it works. I would prefer not compressing anything and leaving that up to the docker hub transport layer because otherwise we spend time waiting for things to decompress on each test run. I can imagine even specifying exact final sizes to dd, since the image sizes are predictable (ubuntu:14.04 is <250MB).\n. I am having trouble getting our QEMU build to run in Docker:\n/root/.test_env/qemu/obj/x86_64-softmmu/qemu-system-x86_64: error while loading shared libraries: libgnutls-deb0.so.28: cannot open shared object file: No such file or directory\nBut the missing library does not exist in trusty: http://packages.ubuntu.com/search?suite=trusty&arch=any&mode=filename&searchon=contents&keywords=libgnutls-deb0.so.28\nCan anyone enlighten me on how we solved this in the lab?\n. @lukego The difference is that in your PoC QEMU was compiled from source inside the container, and I am currently cross-compiling it \u201cahead of time\u201d. QEMU was linked to the outdated libgnutls-deb0.so.28 because was installed on grindelwald (some sort of auto-detection in the configure script I guess). Once I removed it I was able to build a QEMU that successfully starts inside the container (I believe the package was in Canonical's Snabb Switch ppa). It still won't run in the container because it can't find a bios file, e.g. apparently you can't even move a QEMU build without breaking it.\nSo basically I can't simply compile QEMU and copy it into the Docker image unless I am building it on an equivalent host. I guess the solution is to compile QEMU inside a suitable container...\n. An update: I have pushed my first image to DockerHub: https://hub.docker.com/r/eugeneia/snabb-nfv-test/ (@lukego I was unable to create a SnabbCo repository, DockerHub permission issue?) The image is built using https://github.com/eugeneia/snabbswitch-docker (make image).\nYou can now (using my modified Snabb Switch branch: https://github.com/eugeneia/snabbswitch/tree/snabb-docker ) do the following on machines where docker is installed:\ndocker pull eugeneia/snabb-nfv-test\nSNABB_PCI0=0000:88:00.0 SNABB_PCI1=0000:88:00.1 scripts/dock.sh make test\nAnd it will run the test suite in a fresh docker container. You can substitute make test with whatever command you like, e.g. ./snabb snsh -i, ...\n. The kernel/DPDK image I am building as of now yield bad performance, caused by the DPDK VM dropping lots of packets (unrelated to docker):\nPort statistics ====================================\nStatistics for port 0 ------------------------------\nPackets sent:                 89430904\nPackets received:            141823839\nPackets dropped:              52392935\nAggregate statistics ===============================\nTotal packets sent:           89430904\nTotal packets received:      141823839\nTotal packets dropped:        52392935\n====================================================\nAny hints on what could be the cause of this are highly appreciated.\n. I just compared with the DPDK VM we use on master and found out that it does not suffer from the packet drops. (Same machine, same snabb/packetblaster build.) So I have to assume that the problem lies in the VM I build.\n. The difference seems to be the DPDK version, currently we use https://github.com/virtualopensystems/dpdk/commit/36c248ebc629889fff4e7d9d17e109412ddf9ecf plus this ad-hoc diff:\n```\ndiff --git a/examples/l2fwd/main.c b/examples/l2fwd/main.c\nindex 4069d7c..99a1088 100644\n--- a/examples/l2fwd/main.c\n+++ b/examples/l2fwd/main.c\n@@ -102,8 +102,8 @@\n /\n  * Configurable number of RX/TX ring descriptors\n  /\n-#define RTE_TEST_RX_DESC_DEFAULT 128\n-#define RTE_TEST_TX_DESC_DEFAULT 512\n+#define RTE_TEST_RX_DESC_DEFAULT 0\n+#define RTE_TEST_TX_DESC_DEFAULT 0\n static uint16_t nb_rxd = RTE_TEST_RX_DESC_DEFAULT;\n static uint16_t nb_txd = RTE_TEST_TX_DESC_DEFAULT;\ndiff --git a/lib/librte_eal/linuxapp/kni/ethtool/igb/kcompat.h b/lib/librte_eal/linuxapp/kni/ethtool/igb/kcompat.h\nindex 4c27d5d..a590797 100644\n--- a/lib/librte_eal/linuxapp/kni/ethtool/igb/kcompat.h\n+++ b/lib/librte_eal/linuxapp/kni/ethtool/igb/kcompat.h\n@@ -3842,7 +3842,7 @@ static inline struct sk_buff __kc__vlan_hwaccel_put_tag(struct sk_buff skb,\n #define HAVE_ENCAP_TSO_OFFLOAD\n #endif / >= 3.10.0 /\n-#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0) )\n+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,13,0) )\n #ifdef NETIF_F_RXHASH\n #define PKT_HASH_TYPE_L3 0\n static inline void\ndiff --git a/lib/librte_pmd_virtio/virtio_ethdev.c b/lib/librte_pmd_virtio/virtio_ethdev.c\nindex d0b419d..6bb776d 100644\n--- a/lib/librte_pmd_virtio/virtio_ethdev.c\n+++ b/lib/librte_pmd_virtio/virtio_ethdev.c\n@@ -799,6 +799,7 @@ eth_virtio_dev_init(__rte_unused struct eth_driver *eth_drv,\n    } else {\n        hw->max_rx_queues = 1;\n        hw->max_tx_queues = 1;\n+virtio_dev_cq_queue_setup(eth_dev, 2, SOCKET_ID_ANY);\n    }\neth_dev->data->nb_rx_queues = hw->max_rx_queues;\n\n```\nThe DPDK VM I build on the other hand uses https://github.com/virtualopensystems/dpdk/commit/7807fbbcd2ae9c1da8c9e0d20a3d5a4f783e9d6f\nI bet the culprit is somewhere between these commits.\n. I built a new image with DPDK (https://github.com/eugeneia/dpdk/commits/v2.1.0-snabb) as @nnikolaev-virtualopensystems suggested but got similar results:\nAggregate statistics ===============================\nTotal packets sent:           84511007\nTotal packets received:      123580066\nTotal packets dropped:        39069059\n====================================================\n. Testing with the \u201clegacy\u201d DPDK (I reproduced virtualopensystems/dpdk@36c248e plus diff) yields the same packet loss. That makes me think I am doing something different with the VM configuration. What could affect DPDK performance? Hugepages?\n. I take that back: Actually the \u201clegacy build\u201d has ~20% less packet loss and yields ~30% better performance, I think that's actually the performance hit I am hunting down here.\n. Closing because #626 landed which covers \u201cDockerization of CI\u201d.\n. @lukego That is another reason why SnabbBot broke down: It failed to allocate huge pages.... Maybe that's really the only reason actually.\n. @lukego Nope its the leaking (try sudo ipcs | wc -l on davos). Can I safely reboot davos nowadays? ipcrm doesn't seem fit for the task.\n. @lukego Do you have any pointers of what Docker sandboxing actually provides? I have a hard time finding out what it sandboxes and what not.\n. But we also use shared memory objects for the counters? Won't that lead to segfaults?\n. @lukego Seems like davos was rebooted and didn't sysctl -w vm.nr_hugepages so qemu wouldn't run. I've added sysctl -w vm.nr_hugepages=10240 to rc.local.\n. @lukego I assume you want to test src/program/snabbnfv/packetblaster_bench.sh (which is self documenting). Since that does not exist in v2015.07 my approach would be to backport #547 into v2015.07.\n. I am closing this because it seems to be a stale issue. Please reopen/object in case this is still relevant.\n. @lukego I take it we're ready for release here? If so, I will go ahead and do the routine.\n. Closing because this issue is old and should be resolved by now.\n. This is super cool! I love how simple the interface is and can't wait to find a good reason to experiment with this. Its also a very small patch considering how much it provides in terms of functionality. Awesome!\nRegarding the documentation:\n- I would prefer if the implementation details are not mentioned in the user manual. Either move it into source code comments or a separate file? The hints regarding busyloop can be a hint, e.g. \"Note: you should enable busyloop to gain optimal performance and latency. See. imp notes\".\n- It might be worth to explicitly state that inter-proc link names are global (within the system). Might seem obvious but these are the kind of statements which sync with the reader. E.g. Reading the docs a user might think \"These names are probably system wide?\" and then get explicit confirmation and be like \"OK. That's what I thought\".\n- Run a spell checker. Always helps me catch typos that I'd miss otherwise.\n. @lukego thinks this patch should be merged into a project branch that uses and battle tests this functionality before we merge it into mainline. So to anybody reading this: Go ahead and use this code and let us know how it went. We think its good, but we can't guarantee it. :+1:\nNote: Merging will require some conflict resolution.\n. Closing because this was merged together with #618.\n. @lukego What is the status here. Did any code related to this issue land?\n. @lukego Let me know when this is ready for release! :)\n. See https://github.com/SnabbCo/snabbswitch/releases/tag/v2015.10\n. > So some clarification of the workflow for that would be very helpful. Maybe another README that explains the \"big picture\" like how to bootstrap everything and how to know what versions of stuff you are really running?\nYou need to do a git submodule update --init, the kernel/qemu/dpdk sources are included as submodules. A README is definitely needed here.\n. @lukego On the other hand I think its crucial to be able to reproduce CI results manually. One of the core goals I have in mind for the updated CI is to reduce its divergence from manual testing. E.g. I think the value of a CI degrades when its substantially different from running make test. I was often asked how to reproduce CI\u00a0results locally and I think I want to avoid that in the future.\nAlso note that the intel selftest is skipped by defaul too, as a counter point, here I run make test in 20 seconds:\n```\n$ time sudo make test\nDIR       obj/testlog\nTEST      core.lib\nTEST      core.shm\nTEST      core.link\nTEST      core.app\nSKIPPED   testlog/core.app\nTEST      core.timer\nTEST      core.memory\nTEST      core.counter\nTEST      apps.intel.intel_app\nSKIPPED   testlog/apps.intel.intel_app\nTEST      apps.keyed_ipv6_tunnel.tunnel\nTEST      apps.rate_limiter.rate_limiter\nTEST      apps.packet_filter.pcap_filter\nTEST      apps.socket.raw\nTEST      apps.vpn.vpws\nTEST      apps.vhost.vhost_user\nSKIPPED   testlog/apps.vhost.vhost_user\nTEST      lib.protocol.tcp\nTEST      lib.protocol.ipv4\nTEST      lib.protocol.ipv6\nTEST      lib.hardware.pci\nTEST      lib.hash.murmur\nTEST      lib.checksum\nTEST      lib.pmu\nSKIPPED   testlog/lib.pmu\nTEST      lib.bloom_filter\nTEST      lib.ipc.shmem.shmem\nTEST      program.snabbnfv.nfvconfig\nSKIPPED   testlog/program.snabbnfv.nfvconfig\nTEST      program.snabbnfv.neutron2snabb.neutron2snabb_schema\nTEST      program.snabbnfv.neutron2snabb.neutron2snabb\nTEST      apps/solarflare/selftest.sh\nSKIPPED   testlog/apps.solarflare.selftest.sh\nTEST      selftest.sh\nTEST      lib/watchdog/selftest.sh\nTEST      program/snabbnfv/neutron2snabb/selftest.sh\nTEST      program/snabbnfv/selftest.sh\nSKIPPED   testlog/program.snabbnfv.selftest.sh\nTEST      program/packetblaster/selftest.sh\nSKIPPED   testlog/program.packetblaster.selftest.sh\nreal    0m19.544s\nuser    0m7.180s\nsys 0m0.798s\n``\n. @lukego  regarding the failed benchmarks, your best bet would be to run them individually. To see the actual command run e.g. bysnabbnfv-iperf-1500`:\n```\n$ cat src/bench/snabbnfv-iperf-1500 \n!/bin/bash\nset -e\nout=$(program/snabbnfv/selftest.sh bench)\nExtract floating point Gbits number from output.\necho \"$out\" | grep IPERF-1500 | cut -d \" \" -f 2\n% \n```\nE.g. for this case, see what running program/snabbnfv/selftest.sh bench gives.\n. I wonder if we should change all instances of #!/bin/bash to #!/usr/bin/env bash to improve portability? I noticed that the linux kernel sources as well as the qemu sources contain numerous \u201c#!/bin/bash scripts\u201d as well so this seems to be a widespread problem.\n. https://github.com/eugeneia/snabbswitch-docker now has a README. Its not very friendly yet but should give a good overview of the current state.\n. I would check out what DPDK VM is up to:\nSNABB_PCI0=0000:01:00.0 SNABB_PCI1=0000:01:00.1 scripts/dock.sh /bin/bash\nprogram/snabbnfv/packetblaster_bench.sh &\ntelnet localhost 5000\nscreen -r\nI assume you gave it a bit of time to start up the DPDK VM.\n. Sure, I can't get in to to chur right now though (lab1.snabb.co:2020, right?). I think you need to add my public key. Are you sure that you are in the docker container when trying telnet/tmux?\n. SnabbBot did not gain consciousness, I was accidentally logged in as him.\nRegarding the missing -t option, that's fairly annoying... I had to remove it because it would break stuff by converting \\n to \\r\\n.\n. @lukego Try git submodule update --depth 1, I think that is what I did.\nRegading finding that error message (I had to add the -t flag as you did, I also had to do scripts/dock.sh \"cd ..; make\" to build everything):\nSNABB_PCI0=0000:88:00.0 SNABB_PCI1=0000:88:00.1 scripts/dock.sh /bin/bash\nscripts/dock.sh program/snabbnfv/packetblaster_bench.sh > bench.log &\ntelnet localhost 5000\nscreen -r # (there was no screen to resume)\ncat /etc/rc.local # Will tell you how DPDK is started\n[...]\n/root/dpdk/examples/l2fwd/x86_64-native-linuxapp-gcc/l2fwd -c 0x1 -n1 -- -p 0x1\n. Oh that's a mistake, no docker inside docker. Sorry! Should be\nprogram/snabbnfv/packetblaster_bench.sh > bench.log &\n. > make image doesn't run on NixOS. should I be running that inside docker? (any particular requirements?) The problem I hit was bc not being available in PATH.\nRegarding make image, it should not be run in docker, and it should not call bc either, the rule looks like this:\nimage: assets/qemu assets/bzImage assets/qemu.img assets/qemu-dpdk.img\n    $(E) \"DOCKER BUILD  $(IMAGE)\"\n    $(Q) (cp image/$(IMAGE)/Dockerfile assets/ && docker build -t $(IMAGE) assets/)\nNow the eugeneia/snabb-nfv-test image does install bc inside the container but I don't see right now how that could be related.\n. Oh that sounds like its failing on the kernel's make deb-pkg? Could that be it? I see your point, let me think about building the kernel inside docker.\n. The solution to this would be to use an intermediary container for building the kernel (can do, not that much of an issue).\n\nit seems like I am bumping into the same problem that docker is supposed to solve.\n\nThat statement is a bit unfair: We solved the problem of running snabb on machines that only have docker. Building the image that is used to run snabb on these machines, on machines that only have docker is another thing. Double trouble.\n\nI think we need to automate the asset building with a Dockerfile instead of a Makefile.\n\nInside the Docker builder we can not, \u201cburn\u201d VM images so we would end up with a Makefile that does things like: docker build, docker run, docker commit, ... This was indeed my first approach and it was much messier than what we have now.\n. > I am struggling with running things a bit though. I am still not sure how you were able to see the error message from DPDK but I didn't find it. It might make life easier if there is an easy way to attach to the tmux and see the processes?\nI launched a bash shell inside a eugeneia/snabb-nfv-test container, and ran the benchmark script in the background, which will start a VM that listens for telnet on localhost:5000. Then I connect to that VM, realize that DPDK didn't start / crashed. Attemping to launch it manually again yields the error message. The VM could maybe mount a host OS directory and write a log to it? The output of the QEMU processes are generally not useful for debugging  stuff that happens inside the VM (but feel free to tmux attach-session inside the container, its all there). There is no easy way for peeking into the VM in place because its rarely what you want to see, and if you do you can telnet in and poke around in any capacity. The point is a lot of hair is already on fire here and every new requirement adds icing to the cake. This whole docker/qemu business feels a ot like this movie, Inception I think was the title. ;)\n\nI tried building the assets inside an Ubuntu container with a Dockerfile like this:\n\nThat can't work since the VM images are built using docker. What you could do is make bzImage from within a container that has your working directory mounted as a voulme (-v /host/dir:/container/dir).\n\nbut for some reason I can't check stuff out:\n\nThe soultion is to manually fetch what you need, in case of qemu:\n(cd qemu/SnabbCo/qemu; git fetch v2.4.0-snabb:v2.4.0-snabb)\ngit submodule update qemu/SnabbCo/qemu\nFor the kernel just checkout master and use that it doesn't really matter:\n(cd kernel/ubuntu-trusty/ubuntu-trusty; git checkout master)\n\nGeneral observation: running commands partly on the host and partly in a container seems to be asking for trouble. I see this on NixOS but I guess this would also bite e.g. with different versions of Ubuntu inside and outside.\n\nI don't really like the idea of having yet another Dockerfile for building the kernel. I think it might be worth getting rid of the make deb-pkg routine and install headers manually in the guests instead (its debian specific anyways). I think its sensible to have some dependencies for building images, and keep those small. Having extra images to poduce assets in feels overly complex to me. Generally, I would strongly prefer removing levels of indiection instead of adding more.\n\nQuickly experiment with new CI-like tests on any machine. Like: fire up a tmux session containing two VMs with vhost-user interfaces, or with packetblaster attached to a couple of interfaces, and then be able to manually run a Snabb instance to process traffic.\n\ntest_env can totaly do this right now:\nbash -rcfile program/snabbnfv/tet_env/test_env.sh\nI use this regularily to test stuff in an ad-hoc way. I remeber you not liking the fact that it uses bash functions, and it could surely be improved upon, but functional it is.\n\nRun CI test suite on any docker host (including chur).\n\nI take it, if this could be solved with DPDK configure options that would not be good enugh. We want to be able to build the image on NixOS?\n. Building a kernel on NixOS is non-trivial as it turns out (e.g. using make targz-pkg instead of make deb-pkg will fail because of missing /bin/pwd, really!). My Makefiles break in other surprising ways as well... Not liking NixOS right now.\n\nI actually think that git-annex looks very interesting:\n\nI like the idea of not supplying the NFV assets in docker images. A monolithic docker image is somewhat convenient but the amount of duplication is insane.\n\nNew images can be sent in PRs (together with corresponding bootstrapping script)\n\nBootstrapping images is very hard actually. To elaborate what I have tried:\n- Bootstrap \u201cnatively\u201d: Super complicated and arbitrary shell script that's definitely not portable and can't be run in Docker\n- Bootstrap from Docker containers: Fairly straight forward and portable but can also not be run in Docker.\nWe don't want either of those in the snabbswitch repository.\n. I just added a dockerfile for a build environment to compile the kernel in to snabbswitch-docker. I was able to build an image (on NixOS) that runs the DPDK benchmark on chur.\n. @lukego I have been testing the new SnabbBot in action (on both grindelwald and chur), here is how it looks: https://github.com/eugeneia/snabbswitch/pulls\n. > (5) making it easy to deploy CI to a new Snabb Lab server. Have we solved this? This would be cool to actually do. I suppose that the CI could run either via Docker, NixOS, or simply a shell script. I suppose that step one would be to run a CI that only tests on the host environment that it finds (e.g. QEMU found in $PATH) and second step would be to parameterize it to test multiple environments (once we solve that problem to our satisfaction). Or?\n\n(6) making the CI deployable to third party environments e.g. somebody's private GitLab/Jenkins setup or OpenStack 3rd-party CI, etc. Could be that we can keep it simple here and just use solution (1) i.e. the CI would execute a one-liner that runs the tests in a Docker container. If that were too hard we could alternatively use a dedicated machine with the same software as the Snabb Lab and have CI run tests in that via SSH.\n\nI think we are pretty close to ticking these two off.  This PR's version of SnabbBot has very limited requirements: Docker, some standard CLI tools (the most exotic being jq). It is run as a cron job and can be pointed to any repository. E.g. anybody could run their own SnabbBot on their FooCompany/snabbswitch fork, or contribute test hardware to SnabbCo/snabbswitch's CI. Creating new test images is not easy but doable when you know what you are doing and once done, any SnabbBot instance could run the resulting test image by fetching it from docker hub. Using images to reproduce tests locally is also straight forward and reproducing SnabbBot results given an image should be comfortable.\n(Downside: SnabbBot needs to run with root privileges in order to delete files created by Docker run)\nWhat's missing is a way to run the tests without docker. E.g. a clean way to use just the assets outside of the image. I think the simplest way to consolidate this is to add a mechanism to snabbswitch-docker that will push just the assets to an alternate repository, e.g. git-lfs or something similar.\nI suppose we focus on transitioning to the new SnabbBot in the coming release (hopefully we can make it to that date) and then incrementally tackle the other issues.\n. I have evaluated git-annex for our VM storage needs today but I think its overkill: It does many things that we don't actually need (our VM images are not precious, we don't care if they are lost). I think a simple directory containing VM images, hosted on Dropbox or something else, will be enough. Maybe store files containing URL and hash in the repository and fetch them manually?\nMaybe I am overreacting to the volume of git-annex's features...\n. I added a tarball task to the snabbswitch-docker repository that will create a plain old tar archive of the bzImage and Guest/DPDK images. For now, I propose we distribute those tarballs using Dropbox for instance.\nI would like to get this merged and switch the CI to the new SnabbBot over the next release. While not solving all our problems, I think there are some major advancements in here that should be battle-tested. Let's roll this out and further improve on this in incremental steps. I have been sitting on this egg for way too long already. ;)\n. > Obstacle to merge: CI is failing on this PR. Can resolve?\nNot really. There are incompatibilities with \u201cold SnabbBot\u201d and the new SnabbBot. I could run the new SnabbBot on all open PRs and it would fail on everyone but this one. Some assurance that this works is https://github.com/eugeneia/snabbswitch/pulls where I tested the new SnabbBot.\nThis will be a \u201cone off\u201d manual intervention and \u201cold PRs\u201d (based on commits before snabb-docker) will fail on the new SnabbBot.\nShould I try to come up with a seamless transition strategy? I am not sure its a viable effort.\n. How about you do the regular release routine and I take care of the SnabbBot merge and transition along with the release? Alternatively you could just merge it into next at the end.\n. Yes please merge it.\n. How should we handle \u201cold\u201d PRs that fail the new SnabbBot, ask their authors to rebase before pushing new changes?\n. > Could even be that the CI should test not the exact submitted change but the master branch with the PR merged? Could be interesting to see what other CIs do e.g. the Rust one.\nI thought about this today as well... instead of checking out HEAD it would then check out BASE and merge with HEAD, if that fails it would fail the CI? That would mean no passing PRs with merge conflict unless I am missing something.\n. My (different) view:\n\n1 Does the PR branch work?\n\nChecked by the submitter manually before PR.\n\n2 Does the target branch (e.g. master) work with the PR merged?\n\nThat's actually what we want to know during the PR phase.\n\n3 Does an integration branch (e.g. next) work with the PR merged?\n\nThis would be checked along with the PR that wants to merge next into master.\nSo I think checking the merge instead of what we do now actually makes sense and I am even tempted to hack it into SnabbBot before deploying it since it would relieve the burden from people to update their PRs.\n. (3) is even done \u201cautomatically\u201d by the CI since the \u201cnext PR\u201d will trigger a test of master with next merged.\n. @lukego Turns out new CI is failing old PRs anyways because (?) GitHub's \u201cPR base branch\u201d points to the commit from when the PR was created (instead of the current). Guess I will need to ask authors to merge master after all.\n. I am not sure that would be a good idea, since pr/xxx+current-master is a moving target. E.g. each release would invalidate previous CI results. Without some further logic to retest open PRs after each release the behavior would be the same, except that SnabbBot assumes you always want to PR\u00a0against master (could be configurable).\nThe proposed behavior makes sense, but it would also complicate things.\n. Agreed. I have a branch where SnabbBot works as follows:\n- \u201cresult id\u201d == sha1 of master + sha1 of PR head, when master changes PRs will be retested\n- whats being tested is the PR's head with master merged in\n- merge failure leads to build failure\nI have tested it and would like to deploy. Any objections?\n. I am having trouble getting a submodule pointing to https://github.com/SnabbCo/qemu to checkout the v2.4.0-snabb branch. How would I go about \u201cupdating\u201d a submodule?\nWhat I tried went as follows:\n(cd to/submodule\n    git fetch origin v2.4.0-snabb:v2.4.0-snabb\n    git checkout v2.4.0-snabb)\ngit add to/submodule\ngit comit -m \"blah\"\ngit push\nSo far so good, but on another repository I was not able to sync:\ngit pull --rebase\ngit submodule update to/submodule\nfatal: reference is not a tree: c9cea8f431c929f70a9371f4b379ab66c15c5293\nUnable to checkout 'c9cea8f431c929f70a9371f4b379ab66c15c5293' in submodule path 'to/submodule'\nI tried all the variants of this procedure I could find on the net and each failed with the error message above.\n. I think the issue is that I added the submodules with --depth 1 because I wasn't feeling like downloading the whole kernel repository. The downside seems to be that clones of the repository have to manually fetch new revisions.\nI have pushed an updated eugeneia/snabb-nfv-test image (tag latest) containing the v2.4.0-snabb version of QEMU.\n. The snabbco/nfv-dpdk2.1 image contains QEMU 2.4 and the newest DPDK. All without patches. I reckon we need to work on #665 to start recommending QEMU 2.4.\n. From the SnabbBot report:\n\nOK: packetblaster-64 on c29f053 : 1.12294 of 9.468\n\nTranslation: 12% faster than master.\n. Superseded by #697.\n. I am not 100% convinced by using the rate limited logger everywhere. I see cases where it doesn't add much value and could complicate things (e.g. the \u201cND kickoff\u201d message, really no need to rate limit it). If you think its implementation is mature enough so that we will not be bitten by it I will take your word on it though. Generally, you know I am a big fan of code unification and cleanup and this looks like good work to me.\nIn any case, the changes to public functions of core.lib should be documented in src/README.md.src!\n. Not sure why SnabbBot failed here, trying to rerun it now.\n. > The datagram abstraction now offers a more efficient method to add multiple headers to a packet by using an intermediate buffer, which reduces the number of copy operations to one (each push operation implied a memmove of the entire packet since the adoption of the straighline design).\nThe way I see this portion of the PR adds plenty of complexity to the datagram interface but I can not find instances where the new functionality is used. Where is the options map of datagram:new actually used?\n. Very cool to follow up on this, especially since I had my own attempt on making a l2 switch fast. :)\n@alexandergall Do you think it would make sense to add a benchmark based on the learning bridge app? I feel like it would be a great addition to our basic1 benchmark. (Maybe revive the benchmark we used in https://github.com/SnabbCo/snabbswitch/issues/555 ?)\n. > I guess this should at least be translated into some metric like cycles per packet before it could be used as a regression test in the CI.\nI don't think this translation is necessary for performance regression tests, as they are always measured against a previous result on the same hardware. E.g. in the future, once we have multiple SnabbBot instances running, we will effectively be able to tell how a change affects performance on a given CPU architecture / NIC. The effective \u201cbenchmark score\u201d is computed by the simple formula HEAD_RESULT / BASE_RESULT.\n. #626 introduces make benchmarks which is somewhat related.\n. Possibly related: https://github.com/SnabbCo/snabbswitch/pull/439#issuecomment-100935622\n. Should we also allow hyphens in directory names? Was there a specific reason not to allow them? I feel like it would be useful to formulate the current naming restrictions and document them somewhere.\n. Public functions of core.lib should be documented in src/README.md.src. Refer to the documentation-guide for pointers.\nEdit: Also, shouldn't this be in lib.hardware.pci?\n. Is this obsoleted by #669 as well?\n. @dpino Interesting! If I run the selftest using make like so\nsudo SNABB_TEST_INTEL10G_PCIDEVA=0000:86:00.0 make program.snabbnfv.nfvconfig\nit will produce the segfault as described. That's definitely a hint to follow.\n. Fixed by #664 \n. Assigned @dpino because #651 \n. Closing this because #655 was merged.\n. May I suggest interactively rebasing this PR and splitting it into logical commits? Too many unrelated changes in two bulk commits.\n. > I would reuse the performance test we already have and replace DPDK in the guest with Snabb Switch. Check out Max's new CI infra!\nI would be more than happy to see DPDK replaced with Snabb Switch. :)\n. Obsoleted by #681.\n. Short answer: When we have an alternative that's simpler, sure.\nLong answer: core.shm is indeed a kind of DSL. I think its API is nice (map, unmap, unlink), the \u201cname syntax\u201d is slightly foreign but useful. I found core.shm to be a breeze to work with and using it very enjoyable. top and gc are short, simple programs, even though the need for gc is a wart.\n\nFor example to me it is not immediately obvious how much risk there is of multiple Snabb Switch processes on the same machine accidentally over-sharing link objects when using the multiprocessing support from #601 [...]\n\nIf I understand correctly, its safe as long as only one process transmits on a given \u201clink name\u201d. Note: This should be well documented.\n\nI wonder if we should consider moving to a new counter format\n\nDoes core.shm restrict our choices in counter formats? I would defer deciding on core.shm's fate until compared it with its potential replacements.\n. I was able reproduce the bug, but not deterministically. Try:\nfor i in $(seq 100); do sudo SNABB_TEST_INTEL10G_PCIDEVA=0000:84:00.0 program/packetblaster/selftest.sh || break; done\nIts an old regression, it goes back beyond v2015.10. I wasn't able to test v2015.09 yet ( can not get 5feb63a465c39b65cd9c554e63684eeaae9848f5 from http://luajit.org/git/luajit-2.0.git).\n. Should I rerun SnabbBot on next? Since this regression exceeds this release cycle I don't think it should block the release.\n. @lukego Done.\n. Obvious bug, obvious fix. Or not? I would really like to see the following:\n1. A reproducible test case that makes snabb crash due to leaked fd's\n2. A fix that proves itself by passing the above test\n3. The fix should include the regression test mentioned above in RawSocket's selftest\nI imagine this PR would have the following commit messages:\n1. Add \u201cfile descriptor leak test\u201d to RawSocket:seltest (fails to pass CI)\n2. Fix file descriptor leak in RawSocket (passes CI)\n. Please fix 2ebd725, and revert 52fa53e and 19b8f95, then I think this PR is good to go. Sorry for the confusion.\n. Am I right in assuming this PR is superseded by #655?\n. Closing it then. :key: \n. :+1: Makes the regular expressions less nasty. Could you please merge master to make SnabbBot pass the branch?\n. I was unable to reproduce the CI failure and don't think its related to this PR. See #663.\n. This PR also fixes #644.\n. I also agree with this rationale. I have come into contact with link drops during #555 and having a clear definition of what it means for a link to drop packets is important to have, imho. I added the enhancement label so this issue can be found when \u201clooking for work\u201d.\n. I previously had no concrete preference regarding this topic, but after being exposed to this in a couple more specific areas I think I can formulate and argue for a clear design. In my favored design every dropped packet is dropped explicitly. Whether an app is lossy (\u201casynchronous\u201d) or lossless (\u201csynchronous\u201d) is part of the apps internal logic, and documented API. In brief:\n- Calling link.transmit on a full link should be an error. We remove the drop counters from links. Right now links are too fancy. Links should be dumb pipes, micro-buffers so to speak. Their implicit packet dropping behavior is a misfeature, and in my opinion always worse than the explicit alternative. (\u201cHmm, why do these links drop packets?\u201d is always a bad question to have to ask, links should not have logic, its apps that drop packets, not links. Links must be trivial data structures.)\n- Most simple apps will be synchronous (basic apps, test apps). There is no benefit in having to wonder if packets were dropped by Tee, Join or Sink, they should have clearly specified, simple behavior, and reasoning about them must be trivial.\n- Most serious apps will have to drop packets in one case or another. It is imperative that each drop is done explicitly by the app. @plajjan's writeup gives me the impression that this is a wide field, that we should not attempt to \u201csolve\u201d with an implicit behavior of the framework. Power to the application developers. Let them decide when to buffer, how to buffer, etc.\nIn a way this design is already reality:\n- Every case where link.transmit drops a packet today is a bug. We pretty much don't understand when and why link drops happen, we only make sure that they do not happen in practice. Bold claim: every app that calls link.transmit on a full link does so by accident.\n- No serious app relies on links to drop packets. When an app drops a packet it wants to do so in a prominent fashion: packet.free(p); counter.add(dropped_because_xyz_counter).\nI say obey the pattern that emerged naturally, do away with the wildcard feature that is link drops. Where to drop packets should be a problem solved by application developers and not by the framework. I suggest to provide the tools to solve the problem on an application by application basis (the link API), and avoid forcing a shoe for all sizes (links dropping packets).\n. > My suggestion is that the engine regulates the rate of incoming packets to around 100 per breath, links can hold up to 1,000 packets (10:1 safety margin), and links are expected to overflow under a diverse range of exceptional circumstances but not in normal operation.\n@lukego You have convinced me of this strategy. I had thought of an app network as a black box, but given the generous safety margin I think the behavior will be the same for the simple cases I had in mind, and in the exceptional cases the app network state will reflect the real world and give insight to the problem. E.g. following my suggestion in an extreme case the packets would be dropped at the front or the end of the app network without really knowing which app/link is the culprit, while with your suggestion it would be visible which parts of the app network can not account for the load.\nSo in addition to increasing link sizes, I think we should remove link.full. E.g. get rid the widely used pattern of transmitting only when there is room in the output link, or refrain from allowing apps to be synchronous/blocking.\n. What if the push method of peripherals where blocking? This kind of goes against the asynchronous principle, but is there a better way to deal with this scenario? Is there ever a case where you want to drop packets at the app network ends? Why generate a packet if it will be dropped?\n. See #950 for an implementation of the asynchronous behavior described in https://github.com/snabbco/snabb/issues/656#issuecomment-209754177\n. Closing this because #669 landed.\n. Since 2015.11 the error message is as follows (not much better):\nselftest: intel_app\ncore/lib.lua:54: Unable to open file: /sys/bus/pci/devices/0000:04:00.0/vendor\nstack traceback:\n    core/main.lua:126: in function <core/main.lua:124>\n    [C]: in function 'error'\n    core/lib.lua:54: in function 'firstline'\n    lib/hardware/pci.lua:38: in function 'device_info'\n    apps/intel/intel_app.lua:150: in function 'selftest'\n    program/snsh/snsh.lua:29: in function <program/snsh/snsh.lua:29>\n    core/lib.lua:405: in function 'dogetopt'\n    program/snsh/snsh.lua:63: in function 'run'\n    core/main.lua:58: in function <core/main.lua:32>\n    [C]: in function 'xpcall'\n    core/main.lua:155: in main chunk\n    [C]: at 0x0044ee90\n    [C]: in function 'pcall'\n    core/startup.lua:1: in main chunk\n    [C]: in function 'require'\n    [string \"require \"core.startup\"\"]:1: in main chunk\n. Fixed in #679.\n. I think this is ready except for the to-be-resolved merge conflict. @dpino can you merge with master?\n. All the PCI addresses I ever needed to type when working with Snabb Switch have the form 0000:XX:00.Y. So a far as I can tell I only need a pattern XX:.Y := 0000:XX:00.Y. Is this coincidental? When are the domain/device fields relevant, does it depend on the specific hardware?\n. Is this PR obsoleted by #669  and can be closed?\n. No and it shouldn't be necessary. In odd cases I can do it manually by removing the cached result from the CI instance. In general, when this happens there is a regression in Snabb Switch or a bug in the CI. If you experience a failure that you can not reproduce you should mention @eugeneia (me) so I can investigate. If its really unrelated to your PR, its my responsibility and I need to know about it.\n. Gremlin strikes again: https://gist.github.com/SnabbBot/c00ac257cbb83af3954e\n. And again in #700 : https://gist.github.com/SnabbBot/6f03246d0344f55ff1a2\n. #736: https://gist.github.com/SnabbBot/279f6999c99689e2b24c\n. #892: https://gist.github.com/SnabbBot/654445d188bf5446d2825142085dda9b\n. @lukego I started testing from a clean slate, to be honest I didn't know anything and that's why I did not want to pollute the issue with previous speculations.\nI am quite sure that its unrelated to NUMA affinity, as nothing changed there and the other benchmarks are not affected.\nSo here we go, fresh start. There are two different versions of DPDK being tested:\n- \u201clegacy\u201d - this is the version we used previously in the opaque image, and we are using now for the CI\n- v2.1.0-snabb - the current version of DPDK with our patches applied.\nI also tested on two different machines:\n- grindelwald - Intel(R) Xeon(R) CPU E5-2697 v2 @ 2.70GHz\n- davos - Intel(R) Xeon(R) CPU E5-2603 v2 @ 1.80GHz\nFinally I tested with two different versions of QEMU (2.4.0 and 2.1.0). Now to the results:\n1. On grindelwald, it performs worse (~25%) when using v2.1.0-snabb instead of legacy DPDK (4 Mpps vs 3 Mpps), but QEMU versions do not make a difference\n2. On davos using legacy DPDK, it performs worse (~30%) when using QEMU 2.4 instead of 2.1 (2.4 Mpps vs 3.5 Mpps)\nSo while there is a performance hit when upgrading to DPDK 2.1, the reason for the decrease in performance on davos seems to be indeed the new QEMU (2.4.0) version.\n@lukego On a side node, legacy DPDK does indeed seem to work with 1000 byte packets.\n. Good news, I have found the culprit: QEMU is the bottleneck, not DPDK as I thought.I have built a docker image identical to what the CI uses except that it contains QEMU https://github.com/SnabbCo/qemu/tree/v2.1.0-vhostuser plus the patch which increases hard-coded Virtio vring size (https://github.com/SnabbCo/qemu/commit/7a94322b279c5dd8fc5f2cb429814e0411ca0b0e).\n(I actually reproduced the wrong QEMU version before, that's why I couldn't reproduce on grindelwald.)\nThis image should be an almost exact replica of the opaque blobs and QEMU we used in bench_env times (except a slight difference in the QEMU source for which I couldn't find the patch / context):\nhttps://hub.docker.com/r/eugeneia/snabb-nfv-test-legacy/\nYou can reproduce like so:\n$ docker pull eugeneia/snabb-nfv-test-legacy\n$ SNABB_TEST_IMAGE=eugeneia/snabb-nfv-test-legacy \\\n  SNABB_PCI_INTEL0=0000:01:00.0 \\\n  SNABB_PCI_INTEL1=0000:01:00.1 \\\n  scripts/dock.sh program/snabbnfv/packetblaster_bench.sh\nThis yielded expected performance (5.3Mpps on grindelwald) and no DPDK packet loss when I ran it.\n. > Next I would really like to get in control of the patches. Specifically I would like to migrate over to testing with the latest releases of QEMU and DPDK without any patches applied.\nI am in the process of building a patch-less \u201cvanilla\u201d image.\n. Here is the \u201cvanilla\u201d image containing latest stable QEMU and DPDK (and kernel 3.19 instead of 3.13 because DPDK required 3.14+): https://hub.docker.com/r/eugeneia/snabb-nfv-test-vanilla/\n. No I tested it and it works for me (albeit with decreased performance, since both the DPDK and QEMU \u201cregressions\u201d kick in.\nchur$ SNABB_PCI_INTEL0=0000:03:00.0 \\\n      SNABB_PCI_INTEL1=0000:03:00.1 \\\n      SNABB_TEST_IMAGE=eugeneia/snabb-nfv-test-vanilla \\\n      scripts/dock.sh program/snabbnfv/packetblaster_bench.sh\n[...]\nRate(Mpps): 2.076\nEdit: I have only tested on chur.\n. My guess is that its these two patches that are impacting the throughput:\n- https://github.com/virtualopensystems/dpdk/commit/7807fbbcd2ae9c1da8c9e0d20a3d5a4f783e9d6f ([l2fwd] let the driver select Rx/Tx desc number)\n- https://github.com/SnabbCo/qemu/commit/7a94322b279c5dd8fc5f2cb429814e0411ca0b0e (Increase hard-coded Virtio vring size from 256 to 8192)\nAnd maybe this one (I am not sure what it does): https://github.com/virtualopensystems/dpdk/commit/dae0a7f57e5656ad6c8422a5ea6a368cf306ae24 ( [virtio] Initialize the queues even if VIRTIO_NET_F_CTRL_VQ is not negotiated)\nWill try to verify this guess.\n. I did some tests, results in relative numbers:\n- legacy: 1\n- vanilla: 0.6\n- vanila + https://github.com/virtualopensystems/dpdk/commit/7807fbbcd2ae9c1da8c9e0d20a3d5a4f783e9d6f + https://github.com/SnabbCo/qemu/commit/7a94322b279c5dd8fc5f2cb429814e0411ca0b0e: 0.8\nI could confirm that https://github.com/virtualopensystems/dpdk/commit/dae0a7f57e5656ad6c8422a5ea6a368cf306ae24 is unrelated to performance. We already knew https://github.com/virtualopensystems/dpdk/commit/7807fbbcd2ae9c1da8c9e0d20a3d5a4f783e9d6f makes up for 20%, so I am thinking I misapplied https://github.com/SnabbCo/qemu/commit/7a94322b279c5dd8fc5f2cb429814e0411ca0b0e (the code changed a bit since then, this is my adaption: https://github.com/eugeneia/qemu/commit/101ec94aa1a2573dae14c2e0f80b4006f5148549). Makes sense?\n. Regarding an easy way: I scratched my head a bit, but I ended up just branching snabbswitch-docker and building new images. The image building process takes a bit, but at least if you decide to share you can just docker push it. Also: its convenient to compare results between runs where the only difference in SNABB_TEST_IMAGE.\n. >  We already knew virtualopensystems/dpdk@7807fbb makes up for 20%\nI take that back: Removing the last DPDK patch from the equation does not affect performance. That leaves us with:\n- vanilla + https://github.com/eugeneia/qemu/commit/101ec94aa1a2573dae14c2e0f80b4006f5148549 : 0.8 of legacy\nI might be approaching this from the wrong direction (e.g. eliminating patches that don't help instead of \u201cbisecting\u201d to the first bad commits) but since we have only ~3 relevant patches and probably thousands of commits from QEMU and DPDK upstream (which I probably don't understand)...\n. OK, I am now reasonably certain that DPDK is the component we need to patch / focus on.\nI have ran another test using a vanilla/legacy hybrid image, using \u201clegacy DPDK\u201d and vanilla QEMU + https://github.com/eugeneia/qemu/commit/101ec94aa1a2573dae14c2e0f80b4006f5148549:\n- legacy DPDK,  QEMU 2.4.1+101ec94: 1.0 of eugeneia/snabb-nfv-test-legacy\nThere could maybe be one detail invalidating the result, which is kernel versions (legacy doesn't compile with 3.19 so it uses 3.13, vanilla doesn't compile with 3.13 so it uses 3.19). I guess I could apply https://github.com/eugeneia/dpdk/commit/75f58c618230cd75e3551c8e2d4d0bb4d93ef586 to vanilla to be really sure.\nAnyways, my takeaway from this is that we need to focus on DPDK and find out where the performance decrease comes from. If I understand correctly, l2fwd is just an example program, and is mostly untouched since 2013. So maybe it needs to be updated to adapt to DPDK development.\n. Latest insights on this issue:\n\nDPDK 2.1 l2fwd application negotiates two additional Virtio-net options, \u201cIndirect Descriptors\u201d and \u201cMergeable RX buffers\u201d, and this triggers lower performance in this benchmark environment.\n. SNABB_PCI0=[...] DOCKERFLAGS=-t scripts/dock.sh program/snabbnfv/packetblaster_bench.sh\nor\nSNABB_PCI0=[...] DOCKERFLAGS=-t scripts/dock.sh ./snabb snabbnfv -B ...\n\nDOCKERFLAGS=-t is useful because it enables sigterm etc...\nSee src/doc/testing.md for other variables like SNABB_TEST_IMAGE etc.\n. No, not really. :disappointed:  You could edit this line in between runs. Suboptimal, I know...\n. Closing because #1001 landed.\n. Fixed in #679.\n. @lukego I am thinking about making my winter intern extend packetblaster to generate its own packages. It seems like a well-sized task for an intern. :)\n. @lukego Are we ready for release here?\n. @kbara The question is, is it worth adding the complexity in order to avoid a selftest.sh script like included in this PR?\n. @petebristow regarding docker: you can simply scripts/dock.sh \"(cd ..; make) && make test\".\n. How are we on the first 2016 release? I think there are some PRs that should be merged as well such as the little bug fixes ala #679 \n. A rich looking release candidate! @lukego unless you oppose I would do the release now.\n. In that case I would like to retain the ability to always print stack traces (a debug option or similar). I understand that end users might be put off by stack traces but I think developers profit from seeing the guts no matter how trivial the error.\n. Merged into max-next.\n. @lukego I think we merged this at an unfortunate point, the next release will have to follow up with the unmerged commits in this PR following 09ac90a (including the documentation besides other things).\n. Merged into max-next (up to f19c5c1).\n. Looks polished and I understand the motivation for TestSink. On the other hand I have never really longed for TestSink (as writing these kind of tests is straight forward) and I am averse to fixing a problem that does not exist.\nOne way of disproving me would be to convert the test cases to which TestSink applies to use TestSink (you could amend the conversions to this PR). The way I see it, if the result is less code in total and prettier to look at, then my skepticism is refuted, and if we want to adopt TestSink we'd want to convert the applicable selftests anyways. I tried it for keyed_ipv6_tunnel to get a feeling:\n``` diff\ndiff --git a/src/apps/keyed_ipv6_tunnel/tunnel.lua b/src/apps/keyed_ipv6_tunnel/tunnel.lua\nindex 9bf92ea..e638deb 100644\n--- a/src/apps/keyed_ipv6_tunnel/tunnel.lua\n+++ b/src/apps/keyed_ipv6_tunnel/tunnel.lua\n@@ -239,11 +239,11 @@ end\n prepare_header_template()\nfunction selftest ()\n+   local testsink = require(\"apps.testsink.testsink\")\n    print(\"Keyed IPv6 tunnel selftest\")\n    local ok = true\nlocal input_file = \"apps/keyed_ipv6_tunnel/selftest.cap.input\"\n\n\n\nlocal output_file = \"apps/keyed_ipv6_tunnel/selftest.cap.output\"\n    local tunnel_config = {\n       local_address = \"00::2:1\",\n       remote_address = \"00::2:1\",\n@@ -253,19 +253,20 @@ function selftest ()\n    } -- should be symmetric for local \"loop-back\" test\nlocal c = config.new()\n+   config.app(c, \"test\", testsink.TestSink, { fuzzy = false })\nconfig.app(c, \"source\", pcap.PcapReader, input_file)\n+   config.app(c, \"tee\", basic_apps.Tee)\nconfig.app(c, \"tunnel\", SimpleKeyedTunnel, tunnel_config)\n-   config.app(c, \"sink\", pcap.PcapWriter, output_file)\n-   config.link(c, \"source.output -> tunnel.decapsulated\")\n+   config.link(c, \"source.output -> tee.input\")\n+   config.link(c, \"tee.input -> tunnel.decapsulated\")\nconfig.link(c, \"tunnel.encapsulated -> tunnel.encapsulated\")\n-   config.link(c, \"tunnel.decapsulated -> sink.input\")\n+   config.link(c, \"tunnel.decapsulated -> test.rx\")\n+   config.link(c, \"tee.output -> test.comparator\")\napp.configure(c)\napp.main({duration = 0.25}) -- should be long enough...\n-- Check results\n-   if io.open(input_file):read('a') ~=\n-      io.open(output_file):read('a')\n-   then\n+   if #engine.app_table.test.errs > 0 then\n   ok = false\nend\n```\n\n\nIn the above case (which I chose randomly) I feel that TestSink is detrimental: more abstraction to know of, requirement for Tee to reproduce the particular case, one more line in total. The question is if it is worth grokking the TestSink and harness APIs for the provided functionality. Maybe I am not using it properly though...\nCould it be that the main functionality here is the fuzzy option? Generally, I like the idea of an app to match/compare packet streams, and writing a file to disk to compare seems backwards. Could this PR be stripped down to a simple, orthogonal app that frees us of the \u201ccompare pcap dumps\u201d pattern?\n. Great list of pain-points!\n\nselftest() in socket/raw.lua looks painful for a replay type test\n\nI actually think its OK looking. It uses a handful of well-known Snabb-isms (lib.protocol.datagram, core.link, push/pull API), mind that maybe I am too used to reading this already but consider this: it's a white-box test with a transparent implementation, it doesn't hide what's going on behind abstractions, yet it reuses core Snabb concepts effectively.\n\nselftest() in socket/raw.lua ignores the race condition of interacting with linux, fuzzy helps here\n\nStrongly agree. I think a packet matching app like TestSink could be a great tool for the chest.\n\nselftest() in tap/tap.lua doesn't work as it miscalls C.memcmp\n\nI don't think it does? Anyways this danger would indeed be eliminated by a packet matching app.\nEdit: Right, it doesn't check the return value of memcmp...\n\nselftest() in tap/tap.lua tries to reimplement engine.main() badly\n\nI don't think that's the case, its a white-box test that transparently exercises the app API.\n\nnot calling engine.configure(config.new()) between tests can leave debris from previous test runs, counters for example\n\nUsually not an issue because make test runs each test in its own instance. It shouldn't be necessary to call engine.configure(config.new()) either as any call to configure will delete apps that are not in the new configuration.\n\nA pcapfile1 == pcapfile2 doesn't given any indication which packets are broken or how.\n\nAgree, good argument for a specialized packet matching app.\n\nThe required boilerplate leads to shortcuts, along the lines of foo == encode(decode(foo)), which was used in selftest in tunnel.lua. Which has caught me out in the past where both encode and decode have the same error. harness doesn't support directly messing with config\n\nfoo == encode(decode(foo)) tests if encode/decode preserve the input and nothing else that's true. I disagree with the notion that its the boilerplate's fault for not having written a more elaborate test, it is certainly trivial to extend it. While harness might simplify this particular case, I am worried about future tests being forced to fit into the harness API. In free form tests like we have now, nothing keeps you from writing types of tests that haven't been thought of before. Once harness is widespread on the other hand, people will think in its terms. Thinking about how to extend the harness API\u00a0for a new use-case is imho more mental overhead than just writing the (well-understood) boilerplate.\n\nThere are at least 2 approaches to test failure, os.exit(), error(). It might be nice to have something a bit more structured.\n\nI would advise to always use error and assert when possible as the stack trace is handy, but sometimes os.exit is more fitting because you want to run tests even if previous tests failed and exit with a final status or skip tests.\nI am pro TestSink (maybe rename it to just Match) and contra harness. I am personally a fan of \u201cno curtains\u201d, bottom-up software. Abstractions should be small and orthogonal as I want to build more complex programs from small, easy to understand, pieces. I already know that I will need to read the source code of harness 100 times because every time I debug a test I have to recap how harness works because it hides how the test works, and then forget about it again. Furthermore I fear that it will need to be extended with new requirements for yet unknown tests and I bet that will lead to either a monster or throwing it away again and replacing it with something else.\nMatch could be a very generic and useful app that fits in with the spirit of basic_apps. Its documentation has to be proofread and improved though. Maybe its interface should be more extendable (for other possible matching modes).\nlua\nconfig.app(c, \"m\", test.Match , { mode = \"strict\"|\"fuzzy\"|... }) -- important to define a default for `mode'.\nIn any case, \u201cfuzzy\u201d and \u201cstrict\u201d have to be better defined in the documentation so that its crystal clear to use.\nAs to harness: Maybe we can find smaller, composable abstractions for the problems that it solves which then can be composed with existing apps. I am not against what harness does but how it does it: it does not compose.\n. Merged into documentation-fixups.\n. Seems we duplicated effort here, I had already fixed this in #679 \nI think I like my version better because it yields a more specific error messages (for potentially more error cases).\n. Merged into max-next.\n. Assuming make install works otherwise, I think its safe to accept this patch for now and possibly replace make install in another PR. Merged into max-next.\n. I am merging this into max-next as its a change regarding general correctness of C code.\nCc @lukego \n. @justincormack Does this ring any bells to you? Seems to be ljsyscall related on first look.\n. Does anyone mind if I change the title to something reflecting \u201cwe should really print a message at startup to complain about running on an unsupported platform instead of hitting a random error\u201d?\n. I think you ran into the same issue as I did: You merged the LuaJIT patches but left the Snabb-DynASM-bridge (snabbswitch/src/dasm_*) untouched. @capr is in the process of updating the latter. See https://github.com/eugeneia/snabbswitch/pull/8#issuecomment-169983464\n. Re: failing build. I agree but I can't really tell why it happens (repeatedly) for this PR.\n. Great progress! I will try to meet you half way and get the IO(Control) abstractions in order and think about nefarious selftests.\n. Since this was rejected from LuaJIT with some explanation by Mike Pall I would had assumed this is not the right way to go and there would be an iteration of the solution. We can merge this and it will be reasonably simple to deal the resulting LuaJIT divergence because its tracked in Git but let me provide an anecdote so we all know what this means:\nThe performance regression described in https://github.com/SnabbCo/snabbswitch/pull/707#issuecomment-174639201 took quite some effort to track down, one of the reasons for this was that it is only visible without our patches to QEMU and DPDK. When upgrading QEMU and DPDK our patches did no longer apply uncovering the mentioned regression. One might argue that this could have been avoided if we had not ingested the patches, and avoided diverging from upstream.\nSo imho there should be a cost/benefit analysis to this change that includes the cost of diverging from LuaJIT. It would be nice to avoid it altogether, but at the very least we have to document this explicitly. E.g. \u201cah, yes, there was this patch we applied and we forgot all about it\u201d is a realistic scenario that has happened before.\n. I suggest the following: I merge this onto max-next and create a bug issue \u201cLuaJIT upstream divergence\u201d in which we can track the process of working with LuaJIT upstream to obsole this patch. Does that sound reasonable?\n. @lukego Maybe not single out, but also track the existing divergences. I believe there is only the DynASM Lua-mode changes besides this one? Would be nice to have a definitive list. (Is there a git command to effectively diff lib/luajit with e.g. LuaJIT:v2.1?)\n. Merged into max-next.\n. @andywingo This Match has support for fuzzy matching and generally gives you more information than comparing Pcap files (see Match:log). I guess we could add a method to write errors to a Pcap file on demand.\n. >  I'm happy to move it to apps/match but what's changed since the PR690 where you suggested TestSink should be moved to basic_apps as Match?\nMy bad, I said \u201cin the spirit of basic_apps\u201d but did not mean to imply it should be in the module basic_apps. Misleading communication on my side, sorry!\n\nAlso I pulled in the changes from max-next so that I got the fix for config.parse_app_config so the my tests passed. That's added lots of files to the PR that probably don't make sense and probably not what I wanted to do? Can you give me a pointer on what I should of done?\n\nI think you don't really need that bug fix, you can just pass in {} as the empty configuration to avoid triggering the bug. As to your branch, if you are not happy with it, you can always close this PR and supersede it with a new branch, just reference #708 to indicate it to be a continuation of this PR. We try to avoid rebasing \u201cpublic\u201d branches to preserve history, one way to do this is to just leave the branch you are unhappy with as is and start  a new branch (PR branches should be based on master).\n. @petebristow What is the status here? I am somewhat betting on this landing sooner or later. Can I help you with anything regarding this PR?\n. Closing in favor of #861.\n. As of now we do not have the resources to support more platforms. By resources I mean neither the infrastructure (hardware, CI system) nor the people (that know the details of a platform). So currently I can only imagine a third party maintained, long lived fork that supports a new architecture.\nThe good news is that we have monthly releases which can be merged downstream, and anybody can run their own SnabbBot on their own hardware. So at least there are some tools that support the process.\nI think a hierarchy where upstream merges from platform specific forks, but the forks stay autonomous is most practical. In the case when a platform fork \u201cdies\u201d we can simply stop merging with it and put \u201cno longer supports $platform, sorry\u201d into the release notes. Of course, the code will already be in upstream, and no longer actively maintained. I am wary of a situation where we gather substantial unmaintained code in upstream. On the other hand we have had that situation previously as well, and it wasn't a big problem.\nMy point is, in order to support a new platform, we have to have \u201ca guy for that\u201d.\n. @lukego Are we ready for release?\n. Ok done: https://github.com/SnabbCo/snabbswitch/releases/tag/v2016.02\nMistake on my side: the commit message mistakenly says v2016.03... :no_good: not sure what to do now, any amendment would clobber the tag (and history).\n. The CI only has two states: success / failure. I think we only ever check the logs of failed CI results, so your suggestion would effectively hide the error. Imho this kind of problem is rare enough to warrant going the extra mile and actually fixing the problem, in this case merging the fix in LuaJIT (if I read https://github.com/LuaJIT/LuaJIT/issues/126 correctly then there is actually a fix in LuaJIT's v2.1 branch, right?).\n. Merged into max-next.\n. Closing because #722 was merged, please reopen on demand.\n. It doesn't eliminate the problem that snabbco/luajit#snabb needs dedicated maintenance. E.g. I'd still be in a test/ask-cosmin/repeat loop. The advantage would be that we don't have to sync LuaJIT and DynASM-LuaMode in the Snabb repository. :+1: \n. > Probably needs some documentation. What kind?\n@andywingo What do you mean by \u201cWhat kind?\u201d Generally any public API has to be accompanied by a complete API documentation. Also useful would be to update this PR's description to tell:\n- what this code does (motivation)\n- how it does it (unless trivially obvious)\n- if there are any related discussions/references, e.g. #718\n. > Thanks @eugeneia, that's what I was asking for. I am surprised a bit by the conventions tho -- to me a PR shouldn't say what the code does, the code should say it, augmented by comments if needed. This module is lacking the documentation block of course. I also don't really get the purpose of out-of-line per-function documentation; it is bound to drift out of sync. We're always spelunking in the code anyway, to me the right place to document is there. Obviously we disagree about how to do things :)\nBasically, what @kbara said, but to elaborate a bit:\n- I need to know the intention to be able to do efficient code review. I can only tell that the code is incorrect if I know what its intentions are. \n- We need complete and correct API documentation to efficiently use the code. The whole point of an API is that I do not need to read the code.\n\nI also don't really get the purpose of out-of-line per-function documentation; it is bound to drift out of sync.\n\nDocumentation and actual API will not drift out of sync because I do code review and will not accept PRs that break the API (or lack documentation).\n. @andywingo No lecturing intended at all. I am sorry if my response came across as harsh, it was not intended to, my sole intent was to answer the questions asked.\n@lukego raises some points that apply here regarding PR intents (probably a good idea to \u201cstandardize\u201d the [draft] and [wip] tags, actually we could use labels for that if we work out the permissions).\nE.g. my perspective was \u201cwhat is required for this to land in the next release\u201d but maybe that isn't the intent at all here. I have indeed not followed the ffi hash table discussion, so some context in the PR description would help me to get into the code. Where is that discussion by the way, is #718 a follow up?\n. @wingo Eh, please no README.foo.md, any reason why ctable couldn't be in its own subdirectory? Alternatively I suggest just using src/lib/README.md with a subsection for ctable. I would prefer the subdirectory.\n. @wingo I agree that the repeating module names are not pretty but plenty of other Snabb Switch modules already look like that (for the same reason). Its just a cosmetic issue that does not impede on functionality so I tend to not consider this an argument against the sub directory.\n. The documentation build system does not dictate the module hierarchy, but rather the general build system. E.g. in order to have a module that consists of N files and having a directory that contains all N files the module hierarchy has to reflect this. Imho this issue is not severe enough to justify changing the build system.\nI personally think there is value to having documentation (in form of README.md) and code in a dedicated directory (GitHub plays nicely with this, as it will display the README.md when browsing a directory). See /SnabbCo/snabbswitch/tree/master/src/lib/hardware for an example where this works nicely (in both ways, e.g. hardware/ contains two modules that share a README.md).\nThat being said, I am not against putting ctable's docs into src/lib/README.md.\n. > I don't think this would require much changes to the maintenance workflow that you are already using today. One thing is that you would need to keep application-specific code in program/lwaftr/ (including stuff you just can't be bothered arguing in review about at the moment)\nI have to voice strong discomfort here since I need to review every changed file to weed out forgotten debug statements etc. Going through a PRs \u201cchanged files\u201d and categorizing them between \u201cdo care about\u201d and \u201cdo not care about\u201d based on directory prefix is not something I would like to do. Generally I also like the parallel branches model way better than the parallel module prefixes that I think are being proposed here: would it not be an annoyance for the \u201clwaftr subtree\u201d to be forcefully updated every month? Either we ignore compatibility and lwaftr has to accept \u201cmaster's\u201d pace or we ensure no breakage and \u201clwaftr\u201d glues master to the floor.\nI feel very good about the workflow described in https://github.com/SnabbCo/snabbswitch/pull/722#issuecomment-179082817 because it gives a very clear model of what I have to care about (a single edge) and what I don't have to care about (a ton of edges). Imho its a good sign when lots of stuff can happen concurrently without synchronization.\nRegarding rebasing vs merging. I propose a radical no-rebase policy for public branches, and I consider any branch public that has a PR attached to it. My motivation is as follows: Whenever a PR's branch is rebased I have to read all of it again because I have no way of telling what changed. With merges and non-destructive pushes I have a very clear (even supported by git) way of telling whats new since I last looked at it (it will be the difference between my local prXXX branch and the PR's branch). In the case where a rebase can not be avoided at all I recommend closing the original PR, and open a new one with a new branch referencing the old one (this is important, always reference any information/discussion/PRs related in the description as it will save a lot of detective work). \nAnother thing I feel very strongly about is to divide and conquer. E.g. this thread is now about roughly four different issues of which three have very little to do with ctable. GitHub does not allow for nested comments (which I think is a great thing, nested comments are the enemy of focused discourse) and as such we now have very long thread that doesn't add much in terms of PR\u00a0review but that I still have to fully read when I want to review this PR. It would be better if every implicit sub thread would be a new issue or PR referencing this one.\nThe above also applies to code, let me give an example: from my perspective, changes to the Makefile hurt this PR and should be in its own PR. Imagine the Makefile changes break something, and while debugging a month later you find out PR #XXX \u201cadd something completely unrelated\u201d was the culprit. That's non intuitive and can be easily avoided by staying on topic. Of course what \u201con topic\u201d means is relative, but how the documentation is built and the Makefile is imho off topic for a PR about a small well-encapsulated hashable module.\nOnce PRs become these intertwined balls of loosely related things that yet manage to choke each other become a lot less \u201cattractive\u201d. They become unwieldy and it's more fun to kill them off and start fresh than to merge them, simply because merging a complicated diff is much less satisfying than merging a simple isolated diff. The oversight is lost, I can't reliably account for it any more. Now these emotions might be related to my very limited multitasking capabilities, but I know that by dividing concerns it is possible to reduce a lot of what I like to dub \u201dbrain freeze\u201d, and I think everybody profits from this. This thread is becoming a black hole sucking in precious attention and this elaborate comment is not helping at all, which imho proves my point. This thread has a lot of good discussion and code going but I think every sub discussion and unrelated change would profit from being in its own thread and PR.\n. As per my elaborate campaign: My recommendation for the documentation for ctable would be to just put it into lib/README.md and forget about it for now. I don't like to see Makefile changes in this PR, I would prefer if these issues were addressed separately.\nRegarding the documentation itself, some things I noticed:\n- The headline should not use the monospace tag, e.g. read \u201cCtable\u201d instead of \u201cctable\u201d\n- Introduce concepts such a \u201ca ctable\u201d as described in https://github.com/SnabbCo/snabbswitch/blob/master/src/doc/documentation-guide.md#markup-conventions\n- Separate implementation details (these should be comments in the source) from user documentation\n- Prepend method definitions with the class name. E.g. ctable:new instead of :new\n- Why is ctable:new a function? Shouldn't it be a method?\n- An \u201cFFI type\u201d is a \u201cctype\u201d in LuaJIT terminology, be explicit\n- \u201cIn the future, value_type will be optional; a nil value_type will create a set\u201d, if it is not so it should not be in the documentation\n- \u201c[0, 0xFFFFFFFF)\u201d appears to be a typo\n- \u201cparams\u201d its preferable to spell out words completely\n- I would make the configuration keys a single listing and prefix optional keys with Optional.\n- I think /not/ is supposed to be **not**\n- \u201cMethod :lookup_ptr\u201d's description has some formatting issues (generally, proofreading the rendered document will avoid these issues)\n- Comma separate function/method arguments\n- \u201cHash functions\u201d: what range? Formatting broken\n. > I don't see how your comment helps, @eugeneia. I know you're frustrated but I think we're getting somewhere; all discussion has been relevant to the PR at hand. If you want to review the code, I would be delighted; it's over on the code tab :)\nI am sorry you feel that way. Why would you think I am frustrated? I was simply replying to the different issues that where touched on. I think you missed my point: yes, the different discussions all do relate to this PR but they do not have to be in a single thread and we can use GitHub's cross referencing to make it more easily digestible.\n\nIn the future, would it be possible to make specific comments as line notes? \n\nGenerally this is how I prefer it too, but the case of proofreading documentation it tends to involve creating a LOT\u00a0of commits. Maybe comments on source are cheap and I shouldn't be afraid of the overhead.\n. Merged into max-next.\n. I will close this as it doesn't seem to be going anywhere. Please reference this PR in case an ljsyscall based alternative is found.\n. Merged into max-next.\n. Regarding PR target branch: Currently I would recommend always targeting master because it reflects what the CI is doing (it will try to merge with master and test the result). Resolving merge conflicts when merging into a release candidate branch like  max-next is the responsibility of that respective branch's maintainer.\n. I wouldn't talk about code ownership when referring to our current workflow. E.g. Luke and I currently both feel responsible for reviewing all PRs (and a lot of people help us independently), and who takes the \u201clead\u201d on the feedback of a PR is generally decided on a \u201cfirst come first serve\u201d basis. E.g. if I see a PR where Luke is already involved deeply I trust his judgement and only check for maintenance related issues (documentation, ...).\nGenerally speaking, I like that everyone involved in Snabb Switch seems to actively do code review. There are few if any PRs where I am the sole reviewer. I would probably be helplessly overloaded if this were not the case.\nFrom my observation we have a rather democratic structure with a flat hierarchy. I don't \u201cown\u201d any code, instead I usually just moderate. When I am having doubts about a patch I voice it and usually ask for other alternative opinions, so in a way we have lots of tiny polls here and there in which the people who care about a given patch find consensus. Edit: And being a designated maintainer really just means that you have to care about all changes, no? :-)\n. Merged into max-next.\n. @lukego Yeah we are aware of this, but didn't come up with an elegant solution yet, I think the Match app from #708 could solve this.\n. @lukego What do you think, is the flaky Synth:selftest a deal breaker? If not I would merge this into max-next and fix the selftest when Match is ready.\n. > Is synth envisaged to be a general purpose packet crafter? \n\nWould you see synth eventually replacing this [scapy] use case?\n\nSynth was born mainly with this PR\u00a0in mind, e.g. to support a synthesizing packetblaster. It could certainly be extended, or replaced by a more sophisticated app for that matter.\n\nIf it doesn't what's the line of reasoning to not include this in Source?\n\nSynth is Ethernet aware so I felt it was too specific for basic_apps.\n\nIs the use case for synth particularly wide spread as it currently stands?\n\nI don't think so / I don't know.\n\nDoes having Match join Synth under test make sense?\n\nI thought the same thing. I think so, yes.\n\nMatch doesn't help where tests haven't run for long enough but would help where more than 1 breath is taken.\n\nWhy is that? That seems like an arbitrary limitation. I imagined this would be a perfect case for \u201cfuzzy\u201d matching, e.g. ensure at least one packet of each specified size is sent.\n. @lukego I will merge it then and take personal responsibility for remembering to fix the test. :-)\n. Silly question: what difference does it make who has copyright on what line of code? I only know copyright in conjunction with commercial licensing and royalties. Since Snabb Switch is Apache licensed I don't see a point. Are people inclined  commercially re-license parts of the Snabb Switch code base, but without the bug fixes and/or improvements of third-party authors?\nFrom Andy's link:\n\nBut be careful when removing the notices of other developers. Since free software licenses require licensees to preserve notices, wrongfully removing one is a violation of the license from that contributor and may be copyright infringement. If it\u2019s absolutely clear that every remnant of a developer\u2019s contribution has been removed, then it is probably OK to remove the associated copyright notice; otherwise, it\u2019s best to keep it around. However, a requirement to \u201cpreserve\u201d or \u201creproduce\u201d a developer\u2019s copyright notice does not necessarily require that the notice be kept in exactly the same place it started; it\u2019s usually acceptable to move notices from individual source files to a central attribution file, for example.\n\nI read this as \u201cits a liability to have copyright information, and you can incriminate yourself if you mess up\u201d.\n. Thanks for pointing this out to me, I remember now that I read about an open source project having trouble to re-license to a different open source license because they could not reach consensus between the many different copyright holders once, if I remember correctly.\nSnabb Switch code should in my opinion be able to land in MIT (or GPL etc.) licensed projects.\n. Cc @lukego \n. @lukego Yup. :-)\n. @mwiget I assume we indeed support this model, e.g. the test suite passes for these NICs?\n. @lukego I would merge this since the card does work for lwaftr but I feel like we need to track that we need to document that the card is not supported by the CI, maybe a source comment would be enough. What do you think?\n. I'll merge this then, a bug should be filed when problems with this card become apparent.\n. > > Then we would have the simple flow:\n\n\nmaster <- next <- max <- wingo\n\nIn the beginning, sure. I expect after the first handfull of merges I would want to go directly to next.\n\nI am find with this too.\nAs for \u201crouting\u201d, I think its too early to have a good answer. There may be some obvious ownership (Snabb NFV, lwaftr) but I hope that we will be able to self-assign on a \u201cwhatever seems reasonable\u201d basis.\nI am not convinced by the \u201crandom assignment\u201d bot. That is not to say I stand against it, but just to say I would not want to be randomly assigned. Indulging in a PR is something I want to keep self-determined for myself at least (with the exception that anyone is of course welcome to mention me any time, but when I am mentioned I assume there is some motive). I expect the random assignment to produce more noise than do good.\n. > A couple final questions. One, I see there is a SnabbCo/max-next. I could manage a SnabbCo/wingo-next but that sounds unnecessary. If it's just in wingo/wingo-next then maybe the second \"next\" is not needed. Is wingo/next a fine name? If so I can change the branch name then.\nThe branches are mirrored on SnabbCo/snabbswitch and this is why they need unique names in that namespace, next is taken.\n\nTwo, what do I do with my commits? I propose that I add them to wingo-next only via PRs. I can get anyone to review them. In particular I want to lean on @kbara and @dpino here. If I feel that I need feedback from @eugeneia or @lukego I will tag you and/or make a PR against max-next.\n\nI recommend to open PR's against master as that is what the CI checks and as such the GitHub \u201cmerge conflict\u201d message will potentially be misleading, but its not a big issue. Maybe the CI should merge with the target branch instead of always using master?\n. Regarding reading documentation: I have briefly experimented with GitBook and readthedocs.org the other day and failed to make them read Snabb Switch documentation, e.g. I lost patience while trying to debug why they couldn't import our Markdown files. I had this experiment with hosting pre-compiled html versions of our genbook.sh at a canonical place a year (or so) ago but forgot about that as I felt there was no demand. I would suggest this approach again though, make doc/snabbswitch.html should still work (and the dependencies are documented in doc/README.md). Its easy to optimize the CSS\u00a0for API referencing and its single page e.g. C-f works (is more sophisticated searching of interest?).\n. > Could SnabbBot perhaps make doc/snabbswitch.html and post the result on the Gist?\nYes, but for that I request a solid address I can serve HTTP from, e.g. docs.snabb.co. I don't want to do the \u201cdocumentation is at lab1.snabb.co:2008/~max/...\u201d dance again. :)\n. I am closing this because it is (being) solved in #824 #805 #790 \n. Imho we have found a reasonably good middle ground between independent libraries and framework-like structure. I feel like core.* should be the subject of the API covered in this issue, while lib.* and apps.* should remain libraries. My reasoning is that it would be worthwhile to have multiple libraries or apps (e.g. lib.foo and lib.bar) that solve the same problem in different ways. @capr recently explained to me the virtues of libraries vs frameworks, and my takeaway was that with a set of libraries the \u201cuser\u201d can choose what subset he uses freely because libraries are designed to be composable while an opinionated framework may paint the user into a corner because its designed to be used as a whole, e.g. \u201call or nothing\u201d.\nThe above may be a bit tangential, but I see an approach that minimizes any risks that come with designing snabb.*: if we start with core.* and add layers to the onion step-by-step we can review at each step and think about if we liked the onion without the next layer better than the onion with the next layer. Makes sense?\n. @dpino Without having looked too closely, I am absolutely in love with the the idea that we could use this as an alternative to DPDK's l2fwd. It wouldn't replace DPDK as we need that for interop testing but it could make benchmarking much simpler. :+1: \n. @dpino Hey, no rush at all. I am super supportive of taking it easy.\n. Non-blocking questions:\n1. I think I previously missed the fact that l2fwd operates on two duplex ports. Unless I am missing something l2fwd is still fundamentally incompatible with the  packetblaster->snabbnfv<->vm(snabb+l2fwd) scenario since l2fwd needs two duplex ports but the VM only has one (vhost_user)? This is not a problem with this PR, just want to clear up my understanding.\n2. To be honest I do not think I really understand the motivation behind l2fwd/bounce. I see no other use case for bounce than in conjunction with l2fwd, and to me it seems the scope of l2fwd is very limited: to test/benchmark two duplex ports. Why two? Why not test each NIC individually? Why are bounce and l2fwd separate programs when they only work together (my assumption)?\n. I would like other opinions on this as well. I have a very different picture of what l2fwd should do (e.g. run on a single NIC, receive on rx, swap ethernet src and dst, and send them back on tx), and this would imho simplify it while obsoleting bounce altogether. Note that this view is very centered on my own contact with DPDK's l2fwd and might be completely uneducated/ignorant.\n. @dpino Would the model I described cover your use case as well?\n. @dpino We use l2fwd to test a duplex port, with the slight ugliness that our \u201csoftwire\u201c port (Snabb NFV) does the benchmarking. What if packetblaster was extended to measure incoming bandwidth, and l2fwd behaved like in DPDK? Then we could set up the following topology:\npacketblaster<->(virtio|NIC|snabbnfv)<->l2wfd)\nThen we could benchmark/test any duplex \u201csoftwire\u201d with packetblaster, given l2fwd sits on the other end. We would have two orthogonal parts: a benchmarking tool packetblaster and a simple forwarder l2fwd that would hopefully be generic enough to serve a bulk of benchmarking scenarios.\n. qemu-dpdk.img contains the following /etc/rc.local:\n```\n!/bin/sh\nmount -t hugetlbfs nodev /hugetlbfs\necho 64 > /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages\nmodprobe uio\ninsmod /root/dpdk/x86_64-native-linuxapp-gcc/kmod/igb_uio.ko\n/root/dpdk/tools/dpdk_nic_bind.py --bind=igb_uio 00:03.0\nscreen -d -m /root/dpdk/examples/l2fwd/x86_64-native-linuxapp-gcc/l2fwd -c 0x1 -n1 -- -p 0x1\nexit 0\n```\nI can't really tell you what the cryptic l2fwd invocation means though.^^\n. >  I was hesitant to use the name \"Snabb\" because of potential conflicts with SnabbCo\nMe too. Wouldn't be opposed to dropping the \u201cSwitch\u201d.\n. Done in v2016.04.\n. I am all for using a non-Java drop in replacement of ditaa; its not like I am a fan of Java its just that ditaa is super convenient. Regarding its non-determinism: yep I noticed that too, while its not critical it sure is confusing. Regarding make markdown not working in the lab: I haven't noticed because I do that locally, and suspect others do the same. I think you are theexception when it comes to doing anything beyond git pull and running snabb in the lab. Anyhow we should make sure that the snabb development dependencies are installed in the lab.\n. #824 might be related.\n. #827 \n. Closing because #827 was merged, please reopen on demand.\n. @lukego I think we are thinking of #640 .\n. Merged into max-next.\n. Seems non-intrusive to me. Merged onto max-next.\n. My observation: in #747 mention-bot mentioned, dpino, alex and me. Imho that's a 2/3 failure rate. I think dpino wrote the code in question, so one might argue he would like to know about this, and I would probably have mentioned him anyways. Alex and me on the other hand... not so much. E.g. I think mention-bot is overzealous, but maybe I am just mad because the machines are taking away my job. ;-)\n. I am on board and agree with the above \u201creview\u201d bullet points.\n. @mention-bot though is already annoying me. I tried to turn it off by blocking it, in case that doesn't prevent it from notifying me I will complain more.\n. > Contributor opens a PR and one person is unambiguously assigned as the \"upstream\". The upstream either merges the change directly or engages with the contributor to tell them exactly what they can do to get the change merged. The interaction should be something crisp like:\n\n\nMerged, thanks!\nThis needs a README and a selftest and then I will merge it.\nI need an ack from [persons] before I am confident enough to merge this.\nHave you considered doing FOO? Would you like to revise or do you prefer that I merge as-is?\n\nThat's it! This process is then applied recursively from the first upstream to the next upstreams until the change lands on master.\n\nI struggled a bit wrapping my head around what's required of me in the new model but this explains it very well. This is inspiration material for CONTRIBUTING.md!\n. Would it be sensible to close PRs that we merged and re-open them when further amendments are needed? I see this as a way to keep the PR tab lean that's not invasive.\n. I feel the desire to close PRs that I merged because that way I don't have to remember that I merged them when looking for PRs I need to act on.\n. A label would serve my purpose just as well. Is it possible to create new labels? I know I can't just add a new label on my own. My first instinct was to give the PR I merged the merged label, but GitHub doesn't allow me to.\n. Yep, works for me this way. Didn't look hard enough.\nI have added a light purple merged label that I used to label the PRs merged: https://github.com/SnabbCo/snabbswitch/labels/merged\n@kbara @wingo Would this be useful to you as well to label PRs you merged?\n. @aperezdc Do we need the https://github.com/justincormack/ljsyscall/pull/188 patch for this patch to work?\n. Okay, then I am merging this into max-next right away. Great detective work in this PR! I like when there are a lot of references to give context. :-)\n. Ack! Regarding #700, the way I parsed the conversation/code is that the change was not architecture specific but really about correctness. There was C code and it used void * for a value that was not a void *, even though its a long shot (and I am no expert in the field of C typing) I see the possibility that this added strictness will help with a future debugging session.\n. One thing I think we might want to add is a notation dictionary. E.g. pick a set of useful notations such as [range_start,range_end) and describe them in a \u201cNotations\u201d chapter. On the one hand these notations can be very useful on the other hand they can be cryptic if you are unfamiliar with them, and how do you google for [a,b)? See https://github.com/SnabbCo/snabbswitch/pull/722#discussion_r52469592 for example.\n. I use the documentation frequently on GitHub as well as by reading the Markdown source to \u201cload in\u201d context when working with modules and also to track and identify API changes in code.\n. We have documentation-fixes which is maintained by me as well and I think it could very well accept PRs that are about high level documentation considerations. \nRegarding this PR: which phrases are preachy? I didn't intend to sound preachy obviously. ;-)\nAs to the suggestion to point to an external guide: we do this in some way already by saying to use the Lua reference manual as orientation, but for this particular case I find it unpractical to say \u201cderive what we expect from you from this 'book'\u201d. I don't know of any applicable style guide or other existing documentation guide either, and think this PR adds some valuable pointers that imho are not obvious and very useful. E.g. I had to figure this out myself and past-me would have benefited from a brief \u201cgeneral writing guide\u201d.\n. @kbara I say merge it as is for now. I am interested in editing the writing to be less preachy but feel like I would not be able to accomplish this right now without more concrete feedback. I do think that without the explanations the section would be of little use. Any concrete suggestions/feedback can be given in form of PRs, issues or further comments which I would then react to.\n. I think that the current documentation handling is extremely simple, sophisticated and scalable. I disagree with the implication that having the artifacts in the repository or installing ditaa is a problem. I am intuitively against all suggestions in this thread so far because I expect none are thought through to the end and I expect them to complicate things and/or deprive us of valuable working features.\n- SaaS please no. I tried to make mkdocs and readthedocs consume Snabb Switch's markdown files and ended up trying to decipher opaque error messages from their Python/Ruby/Whatever web apps. I would have considered them an option if they managed to build something useful form the Markdown in the Snabb repository without intervention, but even that would probably have been a mistake. I will not email a SaaS support hotline, so I am against SaaS.\n- Split repositories: I expect this would be equivalent to just throwing all the docs away and letting them rot. I don't see the upsides to this administrative overhead, I don't see how we \u201csimplify the build\u201d by doing that. You only need ditaa (which is trivially easy to install on any OS, including Slackware) if you want to change a diagram\nI propose to cover  https://github.com/SnabbCo/snabbswitch/pull/754#issuecomment-183353494 in this discussion as well:\n\nI also adopted an experimental documentation style here: to write real documentation in markdown syntax but include it inline at the top of the source file. The Makefile could extract this for inclusion in a manual. I still think the key to high-quality documentation is to put it somewhere that people will see it every day. Flame on! :-)\n\nI don't think source file headers are more prominent than README.md files. I don't think people should be tricked  into seeing the docs. If someone has the desire to read the docs I am sure they will find them. I don't think the key to high-quality documentation is to move it into source files. I think extracting documentation from source code is a waste of time and introducing pointless complexity.\n. @lukego The reason I did not propose alternatives is because I don't really get what this issue is about. It doesn't spell out a set of requirements I want to try to satisfy, and I am not convinced there is actually a bug here. I think this issue is a junction of the numerous documentation related complaints over the past weeks. To effectively act on the complaints we need to work out requirements and separate issue topics. I see two mostly unrelated topics in this thread:\n- How we write documentation: I am fine with the status-quo, but given a new set of requirements I am willing to find a satisfying fix. However I have to say, I don't find the implicit requirements @lukego stated in this issue particularly compelling (\u201cDo not require Java to render diagrams\u201d, \u201cDo not store artifacts in repository\u201d). I see many benefits that depend on our use of ditaa and having the docs where they are now, and only cosmetic benefit to the stated requirements. To me that sounds like much work for little effect.\n- How we read documentation: I am more inclined to focus on this topic and set requirements ignoring \u201cHow we write...\u201d because this topic should dictate most of its requirements. I still think we should provide canonical URLs to pre-rendered HTML/PDF/EPUB versions of the docs.\nI can do the above on davos using a simple cronjob or hacking it into SnabbBot (I am somewhat inclined to separate CI and doc rendering but @lukego pointed out that having a link to the updated docs for PRs would be a nice thing). I am thinking of a new bot \u201cSnabbDoc\u201d that runs in parallel to SnabbBot and pushes links to HTML docs to GitHub PRs.\n. @lukego Sounds perfect! I think the primary should be a standalone HTML file at doc.snabb.co/<ref|branch|tag> or similar. Packagers can include the documentation by using make docs.\nI would make this happen with:\n- doc.snabb.co pointing to davos:80\n- Hack up a frugal little brother of SnabbBot that does all the documentation rendering, I think this will yield much effect with little code\n. Fixed in #805.\n. As far as I know our NixOS fleet can be trivially redeployed when a patch is available (might even happen automatically scheduled, bit sure).\n. Good points! I have created #904 to cover the first four points. (I am not too certain about the others).\n. Scanned through the lot, looks good to me. Moving existing copyright notices to a central file has to be done in a separate PR.\n. > Could also move each library into a dedicated lib/foo/ subdirectory with its own README. Then on Github the directory listing would act as a table of contents. Could also hack require() to make short names like require(\"lib.ctable\") work.\nI am inclined to go with this and would be willing to refine this PR for that purpose.\n\nHowever: I really think we need to nail down which user experience we are trying to optimize foremost. \n\nI don't think we can get around providing standalone HTML/PDF/EPUB manuals as  the primary target as this is the de-facto standard as far as I can tell. The upside is that this doesn't conflict with organizing the documentation in a GitHub friendly way at all. So I think we can optimize organization for GitHub and render a standalone manual independently without restrictions that arise from this.\n. > Why not just files in markdown that are meant to be read in github?\nWhile having documentation formatted on GitHub is nice, I feel we can't be limited by GitHub in documentation UI (things like having a ToC etc). Also, Package maintainers for distributions usually want to ship documentation so being able to build a standalone HTML (or maybe even man page one day, I think pandoc can do this as well) is useful to them.\n. @lukego I am generally not against lib/<module>.md files but I think this leads to a bigger mess and inconsistency than sub-directories and patching require. E.g. without moving files out of sub-directories into lib/ it would still be inconsistent and I do not think that is what we want. Alternatively we could just move the README.md files (e.g. protocol/README.md -> protocol.md). The latter I actually see as a viable option, the only downside is that we loose GitHub rendering when browsing the directories.\nIs patching require off the table? My personal preference would still be to move each library into a sub-directory and patch require to load existing files in this order:\nrequire(\"foo\") =>\n  foo.lua\n  foo/foo.lua\nI do not see right now how this could break things, but making require more fancy is obviously a culprit. I generally think we should not rush this decision into 2016.03, is the current situation really that bad?\n. Closing in favor of #931.\n. This is super cool obviously. I propose a counterpart to this in snabbtop, it should be able to list the counters of a selected app. E.g. so we can do this from a shell:\n$ snabbtop tunnelXY\ndrop_bad_cookie         10\ndrop_bad_length          0\ndrop_bad_local_address   7\ndrop_bad_protocol       29\ndrop_bad_remote_address  3\nShould be straight forward to implement and I would be willing to tackle this.\n. I would probably use a slot similar to zone that can optionally be populated in new instead of a dedicated method. E.g.\nfunction MyApp:new (arg)\n  ...\n  local o = {}\n  o.counters = {'drop_bad_checksum', 'drop_bad_cookie', ...}\n  ...\nend\nAny reason why you chose a method?\n. @capr In order to merge this I see the following steps:\n1. Merge with master to resolve conflicts\n2. Merge with eugeneia:esp2-dev again (sorry!) to remove obsolete changes to core/lib*\nI would suggest not squashing any history. I think this might bite us and has not much gain.\nIdeally I would like all changes outside of src/program/lisper to be in individual PRs as @wingo has done it for the lwaftr merge, but this might not be practical anymore in this case.\n. @capr On second thought, @lukego and me are investigating a memory corruption (preumably) somewhere in the ASM code related to the IPsec code. Might be better to strip out the IPsec code from this PR for now so you don't have to wait for us fixing this bug.\nSorry for the back and forth! :(\n. @lukego On the IPsec side I would like some review on #845, but this could as well land seperately. So independent from that, what should be the flow, ipsec->lisper->max-next or ipsec->max-next+lisper->max-next?\n. Merged as follows:\n((ipsec->capr:lisper4)+cleanup)->max-next\n. Partially fixed in #861, now left are the pcap files used by Snabb NFV / test_env.\n. > I would like to tweak the COPYRIGHT section so that we are not encouraging people to register their copyrights but merely telling them how they can do so\nGood point. Done.\nRegarding the question who is upstream for my PRs: I thought it would be either @wingo or @kbara depending on whoever self-assigns. Before we implement a complicated \u201cwho-is-upstream\u201d I will propose to rephrase the question to \u201cwhy should I not be upstream?\u201d (e.g. \u201cbecause I am the PR author\u201d, \u201dbecause I don't have time\u201d, \u201cbecause I know somebody else who should be upstream\u201d, ...). When there is no clear answer to that question we self-assign. If the decision was \u201cwrong\u201d it can be contested at any time.\n. @kbara Yes, I am sure we will edit it later on based on feedback from when its prominent but for now I think its a good initial version.\n. I actually wouldn't mind to have this in scripts/. Either way, knowing how long @lukego has been thinking about this I am pleased to think we have found a manageable solution. :-)\nIn danger of loosing focus and simplicity I have to voice a feature request: Could we add an optional PCI argument? Usually I just want to know if a given PCI address is free, because I want to test functionality and not necessarily performance. So when you want you want to do a CPU intensive  test you could lock a whole server (which would fail if I had locked a single PCI adress), but two people could still lock XYZ at the same time (which would fail if someone else did lock first).\n. I think its reasonable to incrementally extend this script when the need arises. So should I merge this or is this PR obsoleted by https://github.com/snabblab/snabblab-nixos/issues/20 and can be closed?\n. @domenkozar Sorry, overlooked this for quite a while. Can you update the selftests to pass?\n. Related: #661 \n. Can do. I suppose this \u201cservice\u201d will in practice be limited to one fix release per month, and will occur between the first and third week?\n. @lukego Oh, I actually did not mean to imply to delay bug fix releases but rather that they will occur in the beginning of a release cycle in practice.\n. Super cool feature, love to see snabb top extended. Until now I wasn't sure if anybody except me used it. ;-)\n@lukego I think the histogram module in this PR would be useful in other applications as well and should be a core.shm extension (core.histogram) similar to core.counter. What do you think?\n. @lukego Outside of docker you will be missing ~/.test_env, the hacky solution is to copy /root/.test_env from the docker image to your $HOME. I should talk to @domenkozar  about installing these files for each user by default.\n. @domenkozar You would need to add a NixOS package for https://github.com/eugeneia/snabbswitch-docker (see $(VM_TARBALL) task in the Makefile) that adds the contents of the created tarball into /etc/skel/.test_env/. \n. Cool! Two things come to my mind:\n- The documentation of lib.equal should probably say what types it supports to make more prominent that this feature exist. E.g. I would not expect it.\n- Extending the unit tests of lib.equal to test this case would not hurt, even though as is the code looks trivial.\nI merged this as is, but if you decide to append anything let me know and I will merge the new commits.\n. I think this is too fancy for apps.basic_apps and belongs under apps.test.*. What do you think @lukego?\n. @wingo I like the idea of extending RateLimiter and combining it with Repeater. It seems like a nice composing solution, but the choice is yours.\n. @lukego Nope, as far as I can tell @wingo has kindly opened all changes outside of apps/lwaftr and program/lwaftr in separate PRs. LGTM.\n. LGTM. @nnikolaev-virtualopensystems any objections?\n. I would personally like the core API and libraries to be memory safe. I see this as one of the major benefits of using Lua instead of C. If that is not possible due to performance requirements then the unsafe API should be explicitly documented as such. So at the very least this is a documentation bug since the documentation of shiftleft does not document the dangers:\n\n\u2014 Function packet.shiftleft packet, length\nTruncates packet by length bytes from the front.\n\nI think this would be a good general rule for the documentation: if its not safe the extent of unsafeness should be documented. A function whose documentation that does not explicitly state unsafeness should be safe.\n. @lukego I am aware that LuaJIT's FFI API is unsafe but that does not mean our core API has to be. E.g. if I am writing an IPsec/ESP app I want to be able to use (a subset of) an API that is memory safe. There should be a simple way to write safe-by-default Snabb code.\n. > I do think that clearly documenting the preconditions that these functions depend on is an unambiguously excellent idea.\nI agree that this is the easiest solution right now, and in some way also the most correct. I will treat this as a documentation bug, and take lead on this.\n. @wingo @lukego While thinking about how to document shiftleft and shiftright I noticed its not as easy as we might have thought:\n- shiftleft - this one is indeed \u201ceasy\u201d, 0 <= bytes <= packet.length(p).\n- shiftright - this one on the other hand: 0 <= bytes and bytes+packet.length(p) <= max_payload. I do not want to refer to max_payload in the documentation as it is not really a public concept...\nI would definitely welcome some confirmation on these assertions hurting performance, as the involved API cruft is not as miniscule as I would like it to be. @wingo Could you help me out in running the relevant benchmark(s)?\n. @domenkozar its a separate issue (the error comes from packet.append\u2019s bounds check), using shiftleft wouldn't have left us with this detailed error message. ;-)\n. Closing because #913 landed.\n. @lukego Any reason for the README. prefix? I think that is odd, otherwise LGTM.\n. I want to reject this on the basis that this is not universally true: some tests and snabb programs can be run by an unprivileged user given /var/run/snabb was already created by another snabb instance. It is created to be world-writeable and sticky:\n-- Create root with mode \"rwxrwxrwt\" (R/W for all and sticky) if it\n-- does not exist yet.\n...\nlocal status, err = S.mkdir(root, \"01777\")\n. I can run make test unprivileged just fine:\nmax@davos:~/snabbswitch/src$ make test\nDIR       testlog\nTEST      program.snabbnfv.nfvconfig\nSKIPPED   testlog/program.snabbnfv.nfvconfig\nTEST      program.snabbnfv.neutron2snabb.neutron2snabb\nTEST      program.snabbnfv.neutron2snabb.neutron2snabb_schema\nTEST      lib.hash.murmur\nTEST      lib.hardware.pci\nTEST      lib.protocol.datagram\nTEST      lib.protocol.ipv6\nTEST      lib.protocol.tcp\nTEST      lib.protocol.ipv4\nTEST      lib.ipc.shmem.shmem\nTEST      lib.traceprof.traceprof\nTEST      lib.checksum\nTEST      lib.bloom_filter\nTEST      lib.pmu\nSKIPPED   testlog/lib.pmu\nTEST      core.app\nSKIPPED   testlog/core.app\nTEST      core.counter\nTEST      core.lib\nTEST      core.timer\nTEST      core.link\nTEST      core.shm\nTEST      core.main\nTEST      core.memory\nTEST      apps.ipv6.nd_light\nTEST      apps.rate_limiter.rate_limiter\nTEST      apps.keyed_ipv6_tunnel.tunnel\nTEST      apps.vhost.vhost_user\nSKIPPED   testlog/apps.vhost.vhost_user\nTEST      apps.socket.raw\nERROR     testlog/apps.socket.raw\nTEST      apps.tap.tap\nSKIPPED   testlog/apps.tap.tap\nTEST      apps.intel.intel1g\nSKIPPED   testlog/apps.intel.intel1g\nTEST      apps.intel.intel_app\nSKIPPED   testlog/apps.intel.intel_app\nTEST      apps.packet_filter.pcap_filter\nTEST      apps.bridge.mac_table\nTEST      apps.vpn.vpws\nTEST      program/packetblaster/selftest.sh\nSKIPPED   testlog/program.packetblaster.selftest.sh\nTEST      program/snabbnfv/selftest.sh\nSKIPPED   testlog/program.snabbnfv.selftest.sh\nTEST      program/snabbnfv/neutron2snabb/selftest.sh\nTEST      selftest.sh\nTEST      lib/watchdog/selftest.sh\nTEST      apps/solarflare/selftest.sh\nSKIPPED   testlog/apps.solarflare.selftest.sh\nTEST      apps/tap/selftest.sh\n(Bug report: apps/tap/selftest.sh should really be skipped or fail but it succeeds because it uses sudo).\n. @lukego I know, and I am not dismissing your concerns. Let's split out the general usability and error message discussion to individual issues and stick to privileges here.\n@domenkozar I don't think adding a sudo is enough, but rather we should add a paragraph that explains when and why Snabb needs to be run as root. Makes sense?\n. Something along the lines of \u201cSnabb Switch needs to be run with root privileges when run for the fist time in order to create /var/run/snabb, and some tests need to be run with root privileges in order to succeed.\u201d\n. Makes sense... Nevermind me @domenkozar!\n. No, leave it open. It will be closed automatically once its on master.\n. > Simply call check_root() on startup and never run as a non-superuser? \nDid we not even do that at some point? I agree with @kbara that this would be a good thing for now, but the downside is that we hide the real bugs with this. We probably should work on defining granular privilege requirements and treat inconsistencies as bugs. If we just refuse to start up these \u201cbugs\u201d are invisible.\nI think in reality the cases where we need privileges are actually limited (and snabb can do many useful things that do not require them), I see:\n1. Access to /var/run/snabb: we could test if we can write to /var/run/snabb or /var/run and if not, fall back to a different root (~/.snabb/run for instance)\n2. Access to DMA: we could have the check_root() in initialization functions of modules that actually use DMA and report better messages, e.g. \u201cXYZ needs privileges\u201d.\n3. Allocate huge pages: I believe this can be done unprivileged, depending on the OS configuration. We could detect if we are unable to and print a helpful message.\nAnything I missed?\n. I think there was a misunderstanding and I do not think this PR should be closed.\nThe issue about buffering/dropping packets is solved to my satisfaction in this PR, and can be reviewed later by a more general initiative about buffering in app networks (I do not like that this PR changes a default for the sake of avoiding a boolean true default value, but this is not a blocker for me).\nOn the issue of the default rate I stand firmly as explained in the respective sub-thread, but I do not think that this small change conflicts with the PR as a whole.\n. > bucket size (who cares)\nThis is a misrepresentation according to the API.\n\n\u2014 Key bucket_capacity\nRequired. Bucket capacity in bytes. Should be equal or greater than rate. Otherwise the effective rate may be limted.\n. > Musing and bikeshedding are a major potential problem. My mindset here is that the foreground discussion is between the submitter (@wingo) and the upstream (@eugeneia) and everything else is parenthetical background discussion that you can cherry-pick ideas from but otherwise ignore. If this is hard in practice, because the chatter is distracting, then we could declare that general musings are off-topic for incoming PRs and should be raised separately e.g. as their own PRs.\n\nThis! In my opinion this PR should have landed (minus arbitrary defaults, and ideally retaining default behavior). It adds a little feature (back pressure mode in RateLimiter) that we don't understand very well but that is proven to be useful and a natural way forward in our orthogonal app ecosystem, and most of all its not intrusive (given it retains the default).\nIt's perfectly fine to replace this feature with something else once we figured out a general strategy, but right now I think it can't hurt to spell out a requirement (composable RateLimiter) for that strategy in used code. Now that @wingo moved this feature back into a private API it is less visible that it could and should be IMHO.\nSo I think a good a good etiquette would be to note musings as comments pointing to an issue that will be further discussion of these musings. Also, don't let anyone other than the assignee talk you into closing a PR, what did he review the changes for if they just get closed under his nose? ;-)\n. >  This is not so great\nI fail to see why. Can you elaborate? As far as I am concerned this changes the API around file system semantics that are not relevant to the API. E.g. before you could first map an object read only, then another process could map that object, write to it, and it would work. Now you need to make sure that you open an object only after it was created by another process. Correct me if I am wrong.\n. @wingo Sorry, somehow I missed the previous comments. I gave this a lot of thought and while I generally feel bad about extending an API based on gut feeling, I do share the same gut feeling and feel that this new API is more hygienic.\nWhat is missing though is the adaption of snabb top. As it is now this PR would break snabb top as far as I can tell?\n. It works because #795 updates snabb top to use open instead of map I presume. Please keep PRs atomic in the future (if there was a CI test for snabb top this would not have passed).\n. @wingo Oops, my bad. Somehow I was absolutely sure that snabb top uses shm.map (while it doesn't).\n. LGTM.\n. I would prefer this to be in core.lib (and built into core.config.parse_app_args). Anything that speaks against that?\nCc @lukego \n. @lukego Completely agree that core.lib needs a cleanup. As parsing table configurations is a core competency of Snabb (every serious app needs to do it), I would persist on having this in core.lib and it being built into config.parse_app_arg. The current pattern to parse app configurations is absolutely horrible and this is a chance to end this swiftly. Cleaning up core.lib is a separate task.\n. I would like to have this, but this PR seems forgotten. I would offer to take over integrating this, given there is demand.\n. Maybe it would be better to give these kind of PRs unique names. E.g. since he next release will be \u201cUvilla\u201d, something like \u201cMerge max-next/uvilla/1 into next\u201d.\n. Just to chime in here: from the SnabbNFV perspective there is currently one NIC App per MAC-address (VMDq), RSS is not applicable if I understand correctly. If the 82599 were single-app/many-links in VMDq mode instead of \u201csub-apps\u201d that would be much better for my current concerns in #886. I don't like the multi-app approach of the 82599 because it has all the disadvantages but doesn't actually yield any benefits. Assuming that the same thing that https://github.com/snabbco/snabb/pull/897 does for RSS can be done for VMDq on the other hand makes this approach interesting.\nSo I see one I/O interface that is 1-N (which I would love to have right now) and one that is 1-1 (which I think might be smarter):\n\nFrom the SnabbNFV perspective the disadvantage for 1-1 (the first on the diagram) would be possible locking overhead even though everything runs in a single process/thread. On the same hand there is a big advantage in that the SnabbNFV application could be horizontally scaled across cores?\nWe we would end up with interfaces still, but simpler ones:\n- simpleIf(<pci>) => rx/tx \u2014 Can be instantiated once per <pci>, has all traffic\n- rssIf(<pci>) => rx/tx \u2014 Can be instantiated many times per <pci>, has some traffic\n- vlanIf(<pci>, <vlan>) => rx/tx \u2014 Can be instantiated many times per <pci>, has traffic by <vlan>\n- macIf(<pci>, <mac>) => rx/tx\u2014 Can be instantiated many times per <pci>, has traffic by <mac>\n- ...\n. @lukego I assume this (\u201clukego merged ...\u201d) was an automated event? Did the branch syncing change? If so, how?\n. Or so I thought, bear with me to figure out why SnabbDoctor failed on this PR.\n. Also see the SnabbDoc status below for a demo. :-)\n. > I do think that programmer friends don't let programmer friends write their own CSS though. Consider finding a popular pandoc template in this style?\nI had written that for my own website, its more or less copy/paste. If anybody finds a good template I have no issue with scrapping the CSS. I personally prefer my own CSS though, so if I am to be bothered with this at all it will be my ~100 lines that I understand well. I don't touch other peoples CSS for... reasons. Let's just say I am the son of a graphic designer, its an emotional topic. ;-)\n. > Sorry to poke on what is acknowledged to be an emotional topic.\nOh, my CSS is not emotional for me, other CSS is. E.g. the readthedocs CSS is a typographic catastrophe. It gets basic things like text rhythm so wrong, its like a searing iron stuck right into my eye. That's what I mean with graphic designer heritage. So my choice would be the first option because I am extremely biased.\nRealizing my own bias, I will remove myself from this discussion starting now. :-)\n. @lukego Can you publish a commit to the changes somehow so I can append it to this PR? Its hard to evaluate without being able to resize the viewport.\nI think I like the header tweaks. Regarding the spacing, I choose the minimal spacing in order to support viewports as small as possible (e.g. I never have my browser fullscreen). I think the optimal solution would be to use the coming \u201cflexbox\u201d feature, but I did not go down that route because its not yet standardized across browsers.\n. I am not exactly a friend of the PDF file format to be honest, but let's not decide that here and now.\nI will merge this as-is onto max-next for now, as I see it as an immediate albeit imperfect improvement. We can iterate in separate PRs.\n. I would like to replace the [sketch] and [wip] tags with labels so that they can be queried. Can we pull this off? I read that collaborators can apply labels, do people have to be manually elected to be collaborators for public repositories like this one?\n. I could label PRs opened by people who do not have the required permissions yet, given that this group of people will be small.\n. Actually I need the reverse: show me everything that's not [wip]. Can't find docs about title queries... :/\n. I don't see how this change could affect anything in a bad way, while it makes things work for you. I will happily merge it.\n. I agree with the reasoning. Should we add a named esp branch on upstream then, that\u2014presumably\u2014I will maintain?\n. @lukego Who is next hop for this? Should I merge it into documentation?\n. I think the app/ and program/ distinction is great because it gives an incentive to write better code. E.g. stuff in program/ is really just glue code almost by definition, and not the actual core functionality. So when you are writing code outside program/ you can be a clean-freak, and inside program/ you can \u201cjust make it work\u201d.\nI can see why a flat namespace would make sense, but I also agree with @wingo that having \u201ctwo stories\u201d seems off. On the other hand I really like our directory hierarchy because it structures my work as a reviewer.\nThesis: a framework\u2014even an arbitrary one\u2014will stimulate collaboration, because it provides a shared identity.\n. What is the \u201cZen of Snabb\u201d? That's a section I wouldn't expect in a manual, or at least not be able to guess what it is about. Otherwise, I like the structure.\nI disagree with use formatting (fixed width fonts) in headers, but that is a detail. More importantly we have to define the module naming scheme that this ToC uses before we use it. E.g. currently there is no source anywhere in the code.\n. Its only a convention I chose because I felt it was readable (probably because I am used to this notation), and useful to note the type of the described object. If we wanted a notation for return values I would probably suggest appending this: \u201c\u21d2 ret1, ret2, ...\u201d\n. @mwiget @kbara Huge amount of information about some \u201cdark arts\u201d, great! While skimming over it I got the impression that it is in a WIP state. I would still merge it given its usefulness but I request that the file (not sure if we really need the example PDF in the repository?) be moved to src/doc/performance-tuning.md.\nBesides fixing the TODOs, in the future I think some sections need to be proofread and clarified, e.g.:\n\nThe default of 512 seems too small, based on load test at IMIX line rate tests against lwaftr, 1024 or 2048 gave equally good results.\n\nThis does not really clearly say whats what, IMHO.\nAlso please use text formatting pervasively, e.g.\n\nDefined in src/core/app.lua and enabled before calling engine.main() via\n\nvs\n\nDefined in src/core/app.lua and enabled before calling engine.main() via\n. @kbara I say swap out the PDF with the PNG version and then I will merge it. Deal?\n. I will merge this as soon as @domenkozar acks this.\n\nBy the way, I have a nix expression for SnabbBot lying around, should I PR that as well?\n. Hello and welcome @yu-kasuya, and thanks for the contribution! :+1: \n. :+1: Teeny tiny concern: people might expect to find a drivers branch and wonder what <person-next> branches are about, or even next. Imho this should document the whole picture of the current situation, and be updated accordingly when things establish.\nI see minor overlap with #761, or at least this should be linked to from CONTRIBUTING.md.\n. I believe this is in need of manual merge conflict resolution.\n. For some reason this PR doesn't show up in https://api.github.com/repos/snabbco/snabb/pulls and therefore is invisible for SnabbBot. I will try to close and reopen it just to see what happens.\n. Figured it out, see #910 \n. @domenkozar Ideally, I would pass this off to you as-is. I am fine with any changes you think appropriate as this (UNIX sysop / Nix) is far from my core expertise.\n. documentation-fixes is somewhat underused these days. I see value in a documentation branch based on the fact that it will only ever contain documentation changes and never changes to code. So you (next) could trust me to ensure that documentation never changes code which would potentially ease reviewing the changes.\nI think replacing documentation-fixes with documentation ( a branch for all documentation changes without code changes) is a good move. I think the next hop should be next, and I would gladly maintain this branch.\n. I would be interested in what others think about the \u201cContributor Covenant\u201d. I don't think the text is trivial. Are we binding ourselves to follow it literally or is it more of a non-authoritative guideline?\n. I had to point SnabbBot and SnabbDoc to the new repository URLs, apparently the redirects do not work for GitHub's API.\n. > Thought: it would be neat to have a testing branch as the upstream for changes like this. Great opportunity for somebody new to become a Snabb upstream maintainer. Any takers?\nI would propose @petebristow, but I don't think he has the time right now. :(\n. @lukego In the current state this program seems like feature bloat to me:\n- It solves tasks that should probably be solved using env(1)\n- It introduces a new mapping from short options to environment variables, which are painful enough as they are\nI think this should at least replace the existing ./snabb snsh -t. Right now I can imagine very well in what scenario this is useful, and I think the program has good intentions. On the other hand the \u201cfor now\u201d functionality seems unattractive to me.\n. @plajjan You could add a benchmark to program/snabbmark/, this is the place where we are currently collecting various micro benchmarks.\n@kbara I would assume that this app is generally useful, so from my point of view it makes sense to have it as an independent app.\n@plajjan Since this app is already in next in form of src/apps/lwaftr/vlan.lua I would like to wait for it to land on master (ETA: early next week), and then have the changes needed to move src/apps/lwaftr/vlan.lua to src/apps/vlan/ appended to this PR. Does that sound sensible?\n. @plajjan Could you add a command to reproduce the crash and attach the error message / stack trace?\n. @plajjan I believe I have managed to reproduce the issue, it is a bug in core.shm that is fixed in max-next (I assume in the course of the changes made by @wingo in #794).\n. I will do the release right now, so it should be in master in half an hour or so. :) I tend to keeping issues open until they are fixed on master. This way you can query for a list of open bugs for the current release easily.\n. Fixed in v2016.04.\n. I am afraid I do not understand your questions in context of Snabb. What exactly are you trying to do?\n. @heijiu I am closing this because I do not think this issue is particularly related to Snabb.\n. @kbara I do the following (my upstream remote is called \u201csnabbco\u201d:\n1=<PRNUMBER>\ngit fetch snabbco pull/$1/head:pr$1\ngit merge --no-ff --edit -m \"Merge PR #$1 () into $(git rev-parse --abbrev-ref HEAD)\" pr$1\ngit branch -D pr$1\nI have the following git aliases in my ~/.gitconfig that make life easy:\n[alias]\n        pr = \"!pr() { git fetch snabbco pull/$1/head:pr$1; }; pr\"\n        merge-pr = \"!merge_pr() { git merge --no-ff --edit -m \\\"Merge PR #$1 () into $(git rev-parse --abbrev-ref HEAD)\\\" pr$1 && git branch -D pr$1; }; merge_pr\"\n. @dpino @plajjan From my stand point, to merge this I would like to see\n1. apps.lwaftr.vlan removed\n2. Replace all uses of apps.lwaftr.vlan with apps.vlan.vlan\n. Its was rightfully pointed out in #865 that I should probably not be the next hop for this, but @wingo or @kbara instead. Unless there is some argument against merging this I would just go ahead and merge it, and look more closely if I should be the next hop next time, OK? Alternatively you could assign yourselves and take over the wheel.\n. @kbara Since @lukego is the next hop for you and @wingo I think its sensible to have one of you guys as the next hop for all lwaftr related changes in the future.\nCan I go ahead and merge this anyways?\n. @lukego @domenkozar Might be a good time to update/remove obsolete lab information that is scattered around.\n. @lperkov There is src/scripts/dock.sh which is the entry point the CI uses to run tests within docker.\nI usually do this to test within docker:\n```\ncd snabb/src\nTo run e.g. the Snabb NFV selftest:\nSNABB_PCI0=0000:01:00.0 scripts/dock.sh make program/snabbnfv/selftest.sh\nOr to get an interactive shell, to use e.g. snabb top to interact\nwith a proces running in the container.\nDOCKERFLAGS=-t SNABB_PCI0=0000:01:00.0 scripts/dock.sh bash\n```\n. > For performance we obviously need to measure pps/bps and not just a binary return value so either the test function (selftest() ?) need to be modified to return something else or we always run these from snabbmark? In addition the result need to be compared with something - possibly a previous commit, i.e. everything is baselined from an earlier version. We could define threshold values such that a perf test fails if the performance drops with more than X% (long discussion to follow this I'm guessing ;)\nSnabbBot is doing most of what you are describing already, see the green ticks next to commits on PRs. The interface it uses for that is make benchmarks, and the individual benchmark entry points are in src/bench/. Right now there are too little benchmarks enabled, everybody is welcome to populate src/bench/!\nSnabbmark and make benchmarks are not properly documented because we are pretty sure that they are not the end of the story. We needed some place to put micro benchmarks and some interface for CI so that's where we are at now. I expect at least snabbmark to be replaced by a better solution akin to #688 in the near future.\n\nI find the structure of snabbmark and packetblaster to be a bit weird. [...] how will it look with 100 different applications?\n\nOne reason for why it is convenient to have a bunch of real world applications and various benchmarks in snabbmark is so that we have a good use-case coverage while developing the Snabb core framework. If I decide its a good idea to replace core API X with core API Y I can see what impact that change has on the bulk of the code base. If I decide to \u201coptimize\u201d core API Z I can use snabbmark to confirm the performance impact with more use cases than I had in mind. In my opinion having 100 \u201csnabbmarks\u201d would be a good thing.\nThe reason we have multiple Snabb applications in the upstream repository is that we find it is the most effective way of collaborating and benefiting from changes. We do believe that this workflow is \u201cLinux scale\u201d. See #746. Nothing should prevent you from changing the README\u00a0in your fork though.\n. > @eugeneia are you happy with functional testing as is? i.e. a selftest() in each module that does varying things? Thoughts on using proper unit test framework with concepts of test units / cases?\n@plajjan I personally like our simple selftest convention, and have had mostly negative experiences with unit test frameworks (imho little value, and usually you spend lots of time working around their restrictions, ultimately the tests become very artificial). We have functional unit tests as well as minor and major integration tests accessible in a uniform way (make), and I think pure unit tests are rather the exception than the norm (currently), most selftests perform some kind of integration testing (using NIC, OS capabilities, ...). So from my point of view a unit test framework would be overkill for the few tests it would apply well to.\n. @petebristow @dpino Using %x will also match upper case letters (whereas [0-9a-f] does not), am I right?\n. @petebristow Just tested it on Lua and LuaJIT and %x managed to match upper case hexadecimal letters for me.\n. Calling usleep will block the whole snabb process as well (timers will not be run for instance). I suggest using engine.now instead:\n```\nfunction Delayed_start:new (delay)\n   return setmetatable({ start = now()+delay },\n                       { __index = Delayed_start})\nend\nfunction Delayed_start:push()\n   if engine.now() < self.start then return end\n   for _ = 1, link.nreadable(self.input.input) do\n      link.transmit(self.output.output, link.receive(self.input.input))\n   end\nend\n``\n. I don't think blocking is an option as it breaksengine.main({duration=...})`, but you are right: we need to resolve the back pressure debate! See #656.\nAs of now anyways, the solution I suggested would work with Repeater, as it will only transmit packets when there is room in its output link, e.g. some packets would be buffered in Delayed_start.\nCc @lukego \n. @petebristow I merged it and replaced some tabs with spaces along the way, could you make sure your editor does not emit tabs?\n. I was about to merge this when I noticed that this kind of \u201csoft-breaks\u201d the API for RO/RC\u00a0type registers. E.g. bits and bytes will not be available on read-only and read-counter registers, even though their reading functionality would be useful. Could you factor out the readers and provide them for RO/RC registers as well?\n. I want to make this change because it enables the \u201ccommon I/O\u201d API we need. So, are we set on input/output over rx/tx? The statistics framework currently uses {rx,tx}{bytes,packets,errors,...} as standard names, should these be changed to {input,ouput}{bytes,packets,errors,...} accordingly?\n. I opened https://github.com/snabbco/snabb/pull/1025 to resolve this issue. Help is welcome!\n. @kbara Please make sure SnabbBot passes this before merging.\n. > Does anyone know why assert(S.open()) works in ljsyscall/test/test.lua but requires a tostring() in snabb ?\n@petebristow I believe ljsyscall redefines assert in that file to use tostring.\n. I really like the visualizations, Pharo seems like a kick-ass tool for this!\n. @domenkozar We messed up the documentation build chain in this release, see https://github.com/snabbco/snabb/pull/885\n. @domenkozar You should be able to fix the SnabbDoc failure by merging with master.\n. @domenkozar I had thought of SNABB_TEST_FIXTURES as a functionality for edge cases, e.g. automated Nix builds. On regular lab servers you have to make sure that each user has their own ~/.test_env as it contains persistent VM  images. E.g. when running things in docker, copy on write will cause a fresh VM image to be duplicated on every run. When run bare though, the tests will destructively modify the VM images. So you can't have two test_env instances using the same SNABB_TEST_FIXTURES as of now.\n. iperf performance regression detected but I can not see any reason why so I am having SnabbBot go at it again.\n. Had to manually kill off SnabbBot for d616abf because the nfvconfig selftest got stuck. Can't really tell why this happened.\n. @wingo Sorry for the confusion/misleading title. This PR is 90% messing with counters and 10% thinking about how they might map to YAN, the JSON part is superficial.\n. @wingo So again to be clear, you are the lead on all things YANG. Consider snabb top --yang to be merely a stub that will be superseded by a more well-planned implementation.\nEdit: Btw, most of the confusion probably came from me confusing RFC7223 with YANG. I hope I have fixed the terminology in the PR title/text.\n. @lukego @wingo \n- [ ] there are some mandatory fields which I have omitted since I didn't know how to map them: if-index (unique ID for interface), admin-status(?), oper-status(?), and related to if-index: higher/lower-layer-if, which are about interface hierarchies such as NIC > VMDq* (this is where it intersects with the stats counters, e.g. the Intel10G app has limited per-VMDq stats and more detailed NIC-global stats). We have to discuss / decide how to map these.\n- [ ] YANG provides a way to add extension modules. I think we should define custom interface types that include the statistics data that we have. E.g. total packets transferred where we collect that and RFC7223 only mentions split Unicast and Multicast stats which we probably do not want to count at every interface (or do we?).\n. @wingo I am just brainstorming from the statistics perspective,  and write things down that I think might be relevant. I am mostly trying to find out which tasks are based on the Snabb-YANG model and which are independent. E.g. even without having the model defined, we can tell that\n- we already have an interface hierarchy in Intel10G that is currently ignored\n- there will be different statistics collected by different interface types\nI absolutely want to wait for a your YANG spec for any mapping to that. At the same time I would like to get other issues out of the way in parallel. My next steps would be to:\n- dig into the Intel10G driver and think about how to expose the \u201csuper stat counters\u201d\n- find out what is good practice regarding where to collect which statistics\n- (and then I see things like #871 which I am thinking about extending to counter names, related janitorial changes, etc...)\nDoes that make sense to figure out these kinds of things in parallel?\nRegarding JSON: disregard the mockup in this PR, I have no preference.\n. @alexandergall But not every interface has a PCI address? E.g. virtual interfaces, tunnel interfaces, ... I think this kind of indexing will have to be addressed in core.app.\n. I realize now that I had overestimated the statistics provided by the 82599 NIC. There are actually just 16 register \u201cgroups\u201d (see rx/txcounter options) for collecting stats. So it is not the case that every VMDq instance has its own statistics, but instead you have to allocate one for each and there are not enough groups for every instance. I suspect this feature is so obscure that it will be of limited use, and these restrictions make it unsuitable for collecting per-app statistics.\nWhere does that leave us? I am leaning towards exposing the per-PCI-address global statistics in counters/<pciaddr> and counting per-app statistics in software (possibly limited statistics visible). Alternatively per-app statistics could be only exposed by Intel10G apps that have rx/txcounter IDs (possibly complicated code).\n. @alexandergall I have added per-PCI address statistics SHM counters for Intel10G in https://github.com/snabbco/snabb/pull/886/commits/f0ed10b70781505e38809acd189744067993f516, two observations:\n1. In this model, I would really like to expose relations between \u201ccounter groups\u201d in SHM, e.g. it should be possible to inspect the counters of app \u201cVQ_Intel_xyz\u201d, and see that its parent is \u201c0000:01:...\u201d. This way it will be possible to relate the statistics of an app\u2019s link to the respective hardware statistics. Since I need IDs for interfaces anyways, I am thinking it might be a good idea to implement these in a way compatible with YANG/Netconf.\n2. I have noticed this duplicates a lot of your ifTable MIB code from #427. I would like to replace that code during this effort. This would mean scrapping lib.ipc and updating your client to use the shmem objects in /var/run/snabb that I promise will be a super-set of what you depend on. Thoughts?\n. @alexandergall My gripes is not with lib.ipc.* in general, but with its direct use in intel10g.lua. E.g. right now there are two statistics interfaces exposed by intel_app in this branch. I want to replace if the MIB interface with a more generic one.\n\nI would prefer if we had a more generic framework for exposing data for the purpose of monitoring which is not biased towards any particular northbound interface. I'm sure that you can't assume any kind of mapping between YANG models and SNMP MIBs in general. That would make it necessary to have another layer that would transform the raw data to something specific for each protocol.\n\nThis is my point of view as well. I propose that we expose a super-set of RFC7223 counters with some simplifications. Different \u201creaders\u201d of our statistics interface will have to process the data to a specific format such as RFC7223 or ifMIB.\n- the only data type is uint64_t\n- the name of an interface is stored only in the directory structure (e.g. /var/run/snabb/<pid>/counters/<name>/<stat-counter>)\n- time/date is represented as seconds elapsed since epoch\n- I probably don't want to expose unicast packet counts but instead total packets / multicast / broadcast\n- I only want to expose fields that are actually implemented in a meaningful way. Readers must provide sensible defaults for fields that are not present.\nIf I implemented the schema described above, and refactored the code in M_sf:init_snmp into its own module, keeping the exact functionality intact, would that be OK for you?\n. I don't want to restrict the available shmem data types to uint64_t, there will definitely be more data types in the future such as @lukego\u2019s timeline. What I don't want to do is to implement data types like int32_t, gauge32, datetime, macaddr_t on the storage level when they all easily fit a data type that is already implemented.\n\nAlso, let's assume that I want to populate the ifName, ifDescr and ifAlias SNMP objects\n\nRight now, just like higherLayerIf I suppose.\n\nI guess the question is related to how we want to organize the /var/run/snabb/ directory\n\nI am absolutely not set on the layout of /var/run/snabb. @lukego doesn't like that we reference instances by pid and I am starting to agree. The layout used currently will break down once apps can have shmem objects other than counters. So it will definitely change but I am not thinking about that too much right now because that's future problems.\n. Here I attempted to find a common ground between RFC7223 and ifTable MIB: https://github.com/eugeneia/snabb/compare/f4834a5...eugeneia:statistics-superset\nThis takes the SNMP code from intel10g.lua and moves it into its own module (using a common interface for app counters). Haven't tested it but theoretically init_snmp should work with other apps such as vhost_user as well. The caveat is that it is not automatically started by the intel app but must be manually started for a given app. This is a sketch.\n\nAgain: we already live in that future if we stop calling everything that fits into a uint64_t a counter :) I think that we should try to find a more flexible naming convention for /var/run/snabb now rather than using one which we already know will have to be changed. It shouldn't be too hard (famous last words :)\n\nThe reason I am reluctant to do this now is because I think requirements will be more clear when @wingo comes up with a model. Can't hurt to get started with what we know though!\nAre there any data types we need to support besides uint64_t and byte strings? (digression: I am OK with the core.counter API for dealing with uint64_t, e.g. as an interface that has set, add and read. For some reason the name \u201ccounter\u201d doesn't bother me much.)\nRight now the directory structure is <type>/<object>/<instance-of-type>, e.g. adding a bytestring  type could mean:\n- adding core.bytestring similar to core.counter\n- having bytestrings/<object>/<instance> in addition to counters/<object>/<instance>\n- make sure that when apps create counters/bytestrings they are created in the right directory\nE.g. an app Foo_NIC could have a bytestring description at bytestrings/Foo_NIC/description and a counter txpackets at counters/Foo_NIC/txpackets.\nThe upside of the current layout is that readers of the shm directory can determine the type of a memory mapped object based on the layout. (That was my original motivation for this layout: to avoid having to encode types in the objects or map object names to types.)\nThe downside is that the directory structure is scattered. E.g. not a single directory containing all resources managed by an app.\nAlternative ideas welcome!\n. > > This takes the SNMP code from intel10g.lua and moves it into its own module (using a common interface for app counters). Haven't tested it but theoretically init_snmp should work with other apps such as vhost_user as well. The caveat is that it is not automatically started by the intel app but must be manually started for a given app. This is a sketch.\n\nSomething like this would certainly make sense. I don't worry about starting a separate process for this outside the Snabb framework (for my appliance, I would simply add another systemd serivce).\n\nThe current code (merged in #931) is actually written to run in the same process. I aimed for a drop-in replacement.\n\nAn array of uint64s might make sense. For example, I'm thinking about exposing the MAC table of the bridge app via the BRIDGE-MIB. Using separate \"counters\" for this could be excessive for large tables. The number of entries in the table would be determined by the size of the file.\n\nHmm that seems awful specific. I will look into making core.shm more generic to allow for variable length types so that these types of uses are possible.\n\nConcerning the name \"counter\": why don't we simply call it uint64 (or just int or something), because that's what it actually is?\n\nFrom my point of view uint64_t is the data type while core.counter is a specific API including fancy things like double buffering. I would argue core.counter is 90% a problem specific API and only 10% a uint64_t.\n\nA simple alternative would be to add the type as an extension to the object's name. e.g. Foo_NIC/txpackets.counter (or Foo_NIC/txpackets.uint64 :)\n\nI like that idea!\n. > Nice. I suppose I would then do something like\n\ninit_snmp(..., engine.app_table[Intel82599].counters, ...)\nbefore running the engine?\n\nExactly. :-) I haven't tested the code but I have been careful about maintaining semantics.\n. https://github.com/snabbco/snabb/releases/tag/v2016.04.1\n. Seems like the performance regression is real. Source: https://github.com/snabbco/snabb/pull/896\n. @lukego You are right, sorry.\n. SnabbBot flagged 4a1ab24 for a SnabbNFV performance regression. I reset the result to get another measurement.\n. @petebristow The top-level comment in intel1g.lua says this supports both Intel I350 and 82599 (awesome?), but it does not implement VMDq as does does the existing Intel10G driver, am I right? If my understanding is correct, maybe rename the module to intel_rss to avoid confusion?\nA section about this driver in README.md would be nice. E.g., describe configuration, modes of operation etc.\n. @petebristow I am fine with intel_herd or intel_mp. :-) I just skimmed the code and it looks fantastic (very straight forward). I think with the coming documentation and tests for 82599 this is ready to be merged!\n. @petebristow If you merge #942 SnabbBot will run selftest.sh.\n. I think the best way to find out is to create that PR. I suspect that the PR would be quite involved though as it opens up a can of safety considerations. I also see overlap with: #774 #740 #734 #789\n. To avoid stalling this PR I will merge it as is and open a separate PR\u00a0to discuss the issue at hand.\n. @AkihiroSuda @dpino is hacking on a fast IP checksum implementation in the same spirit here: https://github.com/snabbco/snabb/pull/1275. Any specific reason why this was closed? @alexandergall\u2019s bridge apps need a \u201cpost setup\u201d method as well (it needs the links to be available):\nhttps://github.com/snabbco/snabb/blob/master/src/apps/bridge/base.lua#L152\n. apps.bridge.* would only benefit from a start method in which links are available\n. I will merge this because we have existing apps (apps.bridge) that profit from this, and the need has been made clear in this thread.\n@dpino Do you mind if I edit the documentation a bit?\n. @domenkozar I guess since as-is the change \u201cbreaks\u201d the existing Docker images. Since the initrd is read-only (right?) we could simply use .test_env/initrd if it exists?\n. I have merged this because to me this feature seems critically useful. That being said this is also \u201cnew stuff\u201d for me and I would be interested to hear what @lukego thinks about this. From my point of view this PR could be an indicator for a great ability we get thanks to LuaJIT but also could point to some difficulties the JIT nature poses to us.\n. This spec reflects the reality of SnabbNFV (literally a one-to-one translation from the API) and I would like to keep it that way. E.g. the (completely legitimate) issues pointed out here (except for my bits/bytes blunder) are really bugs in SnabbNFV imho.\nThis is a first win for YANG as it showcased the bad configuration of SnabbNFV. I will continue by fixing these issues in SnabbNFV and update the YANG spec accordingly, probably all in this PR since it tracks the bugs nicely.\n. I guess we should mark this PR as WIP and only merge a YANG spec once we are satisfied with it / SnabbNFV\u00a0has a reasonable configuration API.\n. @alexandergall Does this look good to you?\n. @leinilein Would you be willing to add a selftest that tests for this regression as @alexandergall has pointed out? If so I would imagine you could benefit from merging #861 into this branch. If not I would merge this as is.\n. @leinilein First of all, no need to hurry, take your time! :+1: Regarding selftests: we organize tests as selftest functions in modules. E.g. if the module foo.bar has a function selftest it can be run using any of the following commands:\nsudo ./snabb snsn -t foo.bar\nmake foo.bar\nmake test\nThe selftest function should exit with a non-zero status to indicate failure, or engine.test_skipped_code to indicate that the test was skipped (for whatever reason). Does that help?\n. I tried this fix and it did not work for me. I used https://github.com/eugeneia/snabb/commit/753fd9f7ea540f45fcbeb8428e990e2d9458f4d2 instead.\n. Closing because this was superseded by #968.\n. Cc @petebristow @wingo\n. Need upstream for this, @kbara would you be interested?\n. I bet a drink on C, even though I think its an unfair bet since I don't know which benchmark is used. :P\n. @kbara @lukego Can we merge this and decide on the packet structure layout another time?\n. @domenkozar I agree that raw is a waste of space, on the other hand its simple. I think we should support both in case valuable test images in RAW might be provided to us by third parties?\n. Merging this as currently we only really support RAW images.\n. @dpino I would prefer to keep merged PRs open and their branches alive until the changes actually hit master (at which point they are closed automatically). My reasoning is that if we find new problems on the way from e.g. max-next to master it is convenient to document/solve these issues in the original PR thread.\n. @lukego In that case it would be trivial to run the I350 selftest, do we want to run the SnabbNFV integration tests using the I350 as well though?\n. Which one is it though?\nmax@davos:~/snabbswitch/src$ lspci | grep I350\n01:00.0 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)\n01:00.1 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)\n05:00.0 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)\n05:00.1 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)\n0a:00.0 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)\n0a:00.1 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)\nI have unbound 01:00.0 and SnabbBot will use it from now on.\n. > Change ingress_packet_drops to use a simpler mechanism. How about if we just had a counter called engine.ingress_packet_drops that could be incremented by any app? Then the engine simply polls that counter for changes and reacts. Would also be visible to external tools like snabb top. Then we are reusing one existing abstraction (a counter) rather than inventing two new ones (ingress_packet_drops method and ingress_drop_monitor object).\n:+1: \n@dpino Would you be OK with it if I made the changes as described above?\n. Agree as well. :+1:  Good point about waiting for the incoming statistics changes.\n. Great to see us evolve past the ad-hoc benchmark framework run by SnabbBot, it feels increasingly underpowered nowadays.\n. Looks good to me. Welcome to Snabb @xray7224, and thank you for the fix!\n. I think your proposal is sensible. :+1: The limited per-queue statistics register arrays of the 82599 are a constant culprit for me as well... \n. That is interesting since only the selftest changed between 04 and 05, e.g. must be a change in a dependency.\n``` diff\n% git diff v2016.04...v2016.05 apps/socket/\ndiff --git a/src/apps/socket/raw.lua b/src/apps/socket/raw.lua\nindex f7e6dc7..a6066b3 100644\n--- a/src/apps/socket/raw.lua\n+++ b/src/apps/socket/raw.lua\n@@ -80,16 +80,20 @@ end\n function selftest ()\n    -- Send a packet over the loopback device and check\n    -- that it is received correctly.\n-   -- XXX Beware of a race condition with unrelated traffic over the\n-   -- loopback device.\n    local datagram = require(\"lib.protocol.datagram\")\n    local ethernet = require(\"lib.protocol.ethernet\")\n    local ipv6 = require(\"lib.protocol.ipv6\")\n-\n-   -- Initialize RawSocket.\n-   local lo = RawSocket:new(\"lo\")\n-   lo.input, lo.output = {}, {}\n-   lo.input.rx, lo.output.tx = link.new(\"test1\"), link.new(\"test2\")\n+   local Match = require(\"apps.test.match\").Match\n+\n+   -- Initialize RawSocket and Match.\n+   local c = config.new()\n+   config.app(c, \"lo\", RawSocket, \"lo\")\n+   config.app(c, \"match\", Match, {fuzzy=true})\n+   config.link(c, \"lo.tx->match.rx\")\n+   engine.configure(c)\n+   local link_in, link_cmp = link.new(\"test_in\"), link.new(\"test_cmp\")\n+   engine.app_table.lo.input.rx = link_in\n+   engine.app_table.match.input.comparator = link_cmp\n    -- Construct packet.\n    local dg_tx = datagram:new()\n    local src = ethernet:pton(\"02:00:00:00:00:01\")\n@@ -99,22 +103,16 @@ function selftest ()\n                         dst = localhost,\n                         next_header = 59, -- No next header.\n                         hop_limit = 1}))\n-   dg_tx:push(ethernet:new({src = src, \n-                            dst = dst, \n+   dg_tx:push(ethernet:new({src = src,\n+                            dst = dst,\n                             type = 0x86dd}))\n-   -- Transmit packet.\n-   link.transmit(lo.input.rx, dg_tx:packet())\n-   lo:push()\n-   -- Receive packet.\n-   lo:pull()\n-   local dg_rx = datagram:new(link.receive(lo.output.tx), ethernet)\n-   -- Assert packet was received OK.\n-   assert(dg_rx:parse({{ethernet, function(eth)\n-      return(eth:src_eq(src) and eth:dst_eq(dst) and eth:type() == 0x86dd)\n-   end }, { ipv6, function(ipv6)\n-      return(ipv6:src_eq(localhost) and ipv6:dst_eq(localhost))\n-   end } }), \"loopback test failed\")\n-   lo:stop()\n+   -- Transmit packets.\n+   link.transmit(link_in, dg_tx:packet())\n+   link.transmit(link_cmp, packet.clone(dg_tx:packet()))\n+   engine.app_table.lo:push()\n+   -- Run engine.\n+   engine.main({duration = 0.01, report = {showapps=true,showlinks=true}})\n+   assert(#engine.app_table.match:errors() == 0)\n    print(\"selftest passed\")\n-- XXX Another useful test would be to feed a pcap file with\n\n```\n. Good points!\n\nMajor change: the /var/run/snabb directories will not exist after the process terminates, right? This is a user-visible change and it seems worth explaining why this makes sense in the PR text.\n\nYes, this PR makes snabb remove its own runtime directory. This makes sense because as of now the lab servers are littered with stale runtime directories. Ideally there would be a way to pass a --debug or --backup option to keep the files if that is desired. This would require additional command line option parsing before forking (NYI).\n\nMajor change: The snabb gc command is gone. This also seems worth drawing attention to in the PR text and being clear about why it is obsolete.\n\nsnabb gc is obsoleted by processes cleaning up after themselves. Also: snabb gc never really worked, it failed to remove runtime directories when other processes where reusing PIDs.\n\nConcern: How can we be confident that /var/run/snabb directories won't be left around without cleanup? For example, is this design robust for unexpected-but-inevitable actions like pressing ^C in the terminal or running kill -9 on the pid that is using the most CPU?\n\nIf you kill -9 (SIGKILL) the supervisor then you are out of luck (no way to react to that signal): the child will be orphaned. In all other cases this is robust, e.g. sending SIGKILL to the child (the process that will actually use CPU cycles) will lead to clean shutdown of the supervisor, as well as sending SIGTERM or SIGINT to the supervisor or child.\nWhat is currently not covered is handling of SIGCONT to the child, e.g. it is not supported to stop (SIGSTOP) and continue the child. I felt this was not something we want to do, but it might be sensible to have a good default behavior for this case.\n. > If you kill -9 (SIGKILL) the supervisor then you are out of luck (no way to react to that signal): the child will be orphaned.\nActually not true, the child will be killed by SIGHUP (S.prctl(\"set_pdeathsig\", \"hup\")), but the runtime directory will not be deleted.\n. > What is currently not covered is handling of SIGCONT to the child, e.g. it is not supported to stop (SIGSTOP) and continue the child. I felt this was not something we want to do, but it might be sensible to have a good default behavior for this case.\nAlso wrong, SIGSTOP/SIGCONT were handled just fine... Don't really know how though?\n. Added support for the worker to be stopped/continued, so that caveat is gone.\n\nAn option to keep the data directory around upon termination would be valuable.\n\nI have made _G.developer_debug configurable via environment (SNABB_DEBUG) and main no longer deletes the run-time directory when _G.developer_debug is true.\n. @teknico Good point, currently no. I think this should be documented along with the sub-program mechanism in place of this outdated section.\n. @kbara snabb gc doesn't work (never did) because it fails when a (new, possibly non-snabb) process with a matching pid exists. It could maybe find out the name of that process and disregard it if it is not \u201csnabb\u201d but for now rm -r /var/run/snabb is more reliable.\n. @kbara No need to merge new commits from this branch, they are related to the performance regression in next.\n@alexandergall Generally everything happens on the snabbco repository, e.g. opening PRs on my fork will be invisible to everyone but me. Best to close/reopen your PR on the source repository, then it will be picked up by the regular maintenance flow.\n. @lukego Can you merge #926 as well, please?\n. @lukego Agree, I only \u201ctrust\u201d this history because I reviewed every changed file myself (and reverted a handful of minor unrelated changes I deemed out of scope), the history as shown per GitHub is opaque to me.\n. Is there any way we could avoid the .version file? The way I see it it will be wrong/misleading in most commits and this information is already encoded in git tags.\n. Since nobody has come forward for a while, how about we crowdfund a capable web designer to do it? For instance, I\u2019d consider one of my office mates capable, this would probably be easy work for him.\n. Resolved in #1050 \n. Doesn't seem to work with qemu in eugeneia/snabb-nfv-test?\n. I can't reproduce it either. Reopening does not trigger a SnabbBot build (only new commits), did that now manually.\nEdit: for the record, the failure we are talking about is here https://gist.github.com/SnabbBot/3447fc270c52b233ce438a39b9a11b5f\n. I can reproduce it on davos, seems to be a QEMU bug that is triggered here? Here is the qemu logs:\n```\nmax@davos:~/snabbswitch/src$ cat qemu0.log\nQEMU waiting for connection on: disconnected:unix:vhost_A.sock,server\nqemu-system-x86_64: -netdev type=vhost-user,id=net0,chardev=char0: chardev \"char0\" went up\n[    0.000000] Initializing cgroup subsys cpuset\n[    0.000000] Initializing cgroup subsys cpu\n[    0.000000] Initializing cgroup subsys cpuacct\n[    0.000000] Linux version 3.13.11-ckt26 (eugeneia@chur) (gcc version 4.9.3 (GCC) ) #3 SMP Tue Oct 6 14:01:23 UTC 2015 ()\n[    0.000000] Command line: earlyprintk root=/dev/vda  rw console=ttyS1 ip=fe80::5054:ff:fe00:0000\n[    0.000000] KERNEL supported cpus:\n[    0.000000]   Intel GenuineIntel\n[    0.000000]   AMD AuthenticAMD\n[    0.000000]   Centaur CentaurHauls\n[    0.000000] e820: BIOS-provided physical RAM map:\n[    0.000000] BIOS-e820: [mem 0x0000000000000000-0x000000000009fbff] usable\n[    0.000000] BIOS-e820: [mem 0x000000000009fc00-0x000000000009ffff] reserved\n[    0.000000] BIOS-e820: [mem 0x00000000000f0000-0x00000000000fffff] reserved\n[    0.000000] BIOS-e820: [mem 0x0000000000100000-0x000000001ffdffff] usable\n[    0.000000] BIOS-e820: [mem 0x000000001ffe0000-0x000000001fffffff] reserved\n[    0.000000] BIOS-e820: [mem 0x00000000feffc000-0x00000000feffffff] reserved\n[    0.000000] BIOS-e820: [mem 0x00000000fffc0000-0x00000000ffffffff] reserved\n[    0.000000] NX (Execute Disable) protection: active\n[    0.000000] SMBIOS 2.8 present.\n[    0.000000] Hypervisor detected: KVM\n[    0.000000] No AGP bridge found\n[    0.000000] e820: last_pfn = 0x1ffe0 max_arch_pfn = 0x400000000\n[    0.000000] x86 PAT enabled: cpu 0, old 0x7040600070406, new 0x7010600070106\n[    0.000000] found SMP MP-table at [mem 0x000f64b0-0x000f64bf] mapped at [ffff8800000f64b0]\n[    0.000000] Scanning 1 areas for low memory corruption\n[    0.000000] Using GB pages for direct mapping\n[    0.000000] init_memory_mapping: [mem 0x00000000-0x000fffff]\n[    0.000000] init_memory_mapping: [mem 0x1fc00000-0x1fdfffff]\n[    0.000000] init_memory_mapping: [mem 0x1c000000-0x1fbfffff]\n[    0.000000] init_memory_mapping: [mem 0x00100000-0x1bffffff]\n[    0.000000] init_memory_mapping: [mem 0x1fe00000-0x1ffdffff]\n[    0.000000] ACPI: RSDP 00000000000f62c0 000014 (v00 BOCHS )\n[    0.000000] ACPI: RSDT 000000001ffe176c 000038 (v01 BOCHS  BXPCRSDT 00000001 BXPC 00000001)\n[    0.000000] ACPI: FACP 000000001ffe0bda 000074 (v01 BOCHS  BXPCFACP 00000001 BXPC 00000001)\n[    0.000000] ACPI: DSDT 000000001ffe0040 000B9A (v01 BOCHS  BXPCDSDT 00000001 BXPC 00000001)\n[    0.000000] ACPI: FACS 000000001ffe0000 000040\n[    0.000000] ACPI: SSDT 000000001ffe0c4e 0009B6 (v01 BOCHS  BXPCSSDT 00000001 BXPC 00000001)\n[    0.000000] ACPI: APIC 000000001ffe1604 000078 (v01 BOCHS  BXPCAPIC 00000001 BXPC 00000001)\n[    0.000000] ACPI: HPET 000000001ffe167c 000038 (v01 BOCHS  BXPCHPET 00000001 BXPC 00000001)\n[    0.000000] ACPI: SRAT 000000001ffe16b4 0000B8 (v01 BOCHS  BXPCSRAT 00000001 BXPC 00000001)\n[    0.000000] SRAT: PXM 0 -> APIC 0x00 -> Node 0\n[    0.000000] SRAT: Node 0 PXM 0 [mem 0x00000000-0x0009ffff]\n[    0.000000] SRAT: Node 0 PXM 0 [mem 0x00100000-0x1fffffff]\n[    0.000000] NUMA: Node 0 [mem 0x00000000-0x0009ffff] + [mem 0x00100000-0x1ffdffff] -> [mem 0x00000000-0x1ffdffff]\n[    0.000000] Initmem setup node 0 [mem 0x00000000-0x1ffdffff]\n[    0.000000]   NODE_DATA [mem 0x1ffdb000-0x1ffdffff]\n[    0.000000] kvm-clock: Using msrs 4b564d01 and 4b564d00\n[    0.000000] kvm-clock: cpu 0, msr 0:1ffd7001, boot clock\n[    0.000000] Zone ranges:\n[    0.000000]   DMA      [mem 0x00001000-0x00ffffff]\n[    0.000000]   DMA32    [mem 0x01000000-0xffffffff]\n[    0.000000]   Normal   empty\n[    0.000000] Movable zone start for each node\n[    0.000000] Early memory node ranges\n[    0.000000]   node   0: [mem 0x00001000-0x0009efff]\n[    0.000000]   node   0: [mem 0x00100000-0x1ffdffff]\n[    0.000000] ACPI: PM-Timer IO Port: 0x608\n[    0.000000] ACPI: LAPIC (acpi_id[0x00] lapic_id[0x00] enabled)\n[    0.000000] ACPI: LAPIC_NMI (acpi_id[0xff] dfl dfl lint[0x1])\n[    0.000000] ACPI: IOAPIC (id[0x00] address[0xfec00000] gsi_base[0])\n[    0.000000] IOAPIC[0]: apic_id 0, version 17, address 0xfec00000, GSI 0-23\n[    0.000000] ACPI: INT_SRC_OVR (bus 0 bus_irq 0 global_irq 2 dfl dfl)\n[    0.000000] ACPI: INT_SRC_OVR (bus 0 bus_irq 5 global_irq 5 high level)\n[    0.000000] ACPI: INT_SRC_OVR (bus 0 bus_irq 9 global_irq 9 high level)\n[    0.000000] ACPI: INT_SRC_OVR (bus 0 bus_irq 10 global_irq 10 high level)\n[    0.000000] ACPI: INT_SRC_OVR (bus 0 bus_irq 11 global_irq 11 high level)\n[    0.000000] Using ACPI (MADT) for SMP configuration information\n[    0.000000] ACPI: HPET id: 0x8086a201 base: 0xfed00000\n[    0.000000] smpboot: Allowing 1 CPUs, 0 hotplug CPUs\n[    0.000000] PM: Registered nosave memory: [mem 0x0009f000-0x0009ffff]\n[    0.000000] PM: Registered nosave memory: [mem 0x000a0000-0x000effff]\n[    0.000000] PM: Registered nosave memory: [mem 0x000f0000-0x000fffff]\n[    0.000000] e820: [mem 0x20000000-0xfeffbfff] available for PCI devices\n[    0.000000] Booting paravirtualized kernel on KVM\n[    0.000000] setup_percpu: NR_CPUS:256 nr_cpumask_bits:256 nr_cpu_ids:1 nr_node_ids:1\n[    0.000000] PERCPU: Embedded 27 pages/cpu @ffff88001fc00000 s81536 r8192 d20864 u2097152\n[    0.000000] kvm-clock: cpu 0, msr 0:1ffd7001, primary cpu clock\n[    0.000000] KVM setup async PF for cpu 0\n[    0.000000] kvm-stealtime: cpu 0, msr 1fc0d000\n[    0.000000] Built 1 zonelists in Node order, mobility grouping on.  Total pages: 128873\n[    0.000000] Policy zone: DMA32\n[    0.000000] Kernel command line: earlyprintk root=/dev/vda  rw console=ttyS1 ip=fe80::5054:ff:fe00:0000\n[    0.000000] PID hash table entries: 2048 (order: 2, 16384 bytes)\n[    0.000000] xsave: enabled xstate_bv 0x7, cntxt size 0x340\n[    0.000000] Checking aperture...\n[    0.000000] No AGP bridge found\n[    0.000000] Memory: 498824K/523768K available (7379K kernel code, 1146K rwdata, 3380K rodata, 1336K init, 1448K bss, 24944K reserved)\n[    0.000000] SLUB: HWalign=64, Order=0-3, MinObjects=0, CPUs=1, Nodes=1\n[    0.000000] Hierarchical RCU implementation.\n[    0.000000]  RCU dyntick-idle grace-period acceleration is enabled.\n[    0.000000]  RCU restricting CPUs from NR_CPUS=256 to nr_cpu_ids=1.\n[    0.000000]  Offload RCU callbacks from all CPUs\n[    0.000000]  Offload RCU callbacks from CPUs: 0.\n[    0.000000] NR_IRQS:16640 nr_irqs:256 16\n[    0.000000] Console: colour VGA+ 80x25\n[    0.000000] console [ttyS1] enabled\n[    0.000000] allocated 2097152 bytes of page_cgroup\n[    0.000000] please try 'cgroup_disable=memory' option if you don't want memory cgroups\n[    0.000000] tsc: Detected 1799.993 MHz processor\n[    0.008000] Calibrating delay loop (skipped) preset value.. 3599.98 BogoMIPS (lpj=7199972)\n[    0.008000] pid_max: default: 32768 minimum: 301\n[    0.008000] Security Framework initialized\n[    0.008000] AppArmor: AppArmor initialized\n[    0.008000] Yama: becoming mindful.\n[    0.008103] Dentry cache hash table entries: 65536 (order: 7, 524288 bytes)\n[    0.009328] Inode-cache hash table entries: 32768 (order: 6, 262144 bytes)\n[    0.010482] Mount-cache hash table entries: 1024 (order: 1, 8192 bytes)\n[    0.012010] Mountpoint-cache hash table entries: 1024 (order: 1, 8192 bytes)\n[    0.013274] Initializing cgroup subsys memory\n[    0.013946] Initializing cgroup subsys devices\n[    0.014619] Initializing cgroup subsys freezer\n[    0.015334] Initializing cgroup subsys blkio\n[    0.016007] Initializing cgroup subsys perf_event\n[    0.016726] Initializing cgroup subsys hugetlb\n[    0.018542] mce: CPU supports 10 MCE banks\n[    0.019261] Last level iTLB entries: 4KB 512, 2MB 0, 4MB 0\n[    0.019261] Last level dTLB entries: 4KB 512, 2MB 0, 4MB 0\n[    0.019261] tlb_flushall_shift: 6\n[    0.042169] Freeing SMP alternatives memory: 32K (ffffffff81e6e000 - ffffffff81e76000)\n[    0.048600] ACPI: Core revision 20131115\n[    0.050099] ACPI: All ACPI Tables successfully acquired\n[    0.050931] ftrace: allocating 28502 entries in 112 pages\n[    0.064346] Enabling x2apic\n[    0.064806] Enabled x2apic\n[    0.065474] Switched APIC routing to physical x2apic.\n[    0.069251] ..TIMER: vector=0x30 apic1=0 pin1=2 apic2=-1 pin2=-1\n[    0.070146] smpboot: CPU0: Intel(R) Xeon(R) CPU E5-2603 v2 @ 1.80GHz (fam: 06, model: 3e, stepping: 04)\n[    0.071662] Performance Events: 16-deep LBR, IvyBridge events, Intel PMU driver.\n[    0.072724] ... version:                2\n[    0.073299] ... bit width:              48\n[    0.073894] ... generic registers:      8\n[    0.074465] ... value mask:             0000ffffffffffff\n[    0.075287] ... max period:             000000007fffffff\n[    0.076005] ... fixed-purpose events:   3\n[    0.076581] ... event mask:             00000007000000ff\n[    0.087794] KVM setup paravirtual spinlock\n[    0.089872] x86: Booted up 1 node, 1 CPUs\n[    0.090470] smpboot: Total of 1 processors activated (3599.98 BogoMIPS)\n[    0.091817] NMI watchdog: enabled on all CPUs, permanently consumes one hw-PMU counter.\n[    0.092243] devtmpfs: initialized\n[    0.095604] EVM: security.selinux\n[    0.096006] EVM: security.SMACK64\n[    0.096492] EVM: security.ima\n[    0.096940] EVM: security.capability\n[    0.099024] pinctrl core: initialized pinctrl subsystem\n[    0.099869] regulator-dummy: no parameters\n[    0.100108] RTC time: 14:11:02, date: 06/20/16\n[    0.100811] NET: Registered protocol family 16\n[    0.101585] cpuidle: using governor ladder\n[    0.102186] cpuidle: using governor menu\n[    0.102817] ACPI: bus type PCI registered\n[    0.103409] acpiphp: ACPI Hot Plug PCI Controller Driver version: 0.5\n[    0.104145] PCI: Using configuration type 1 for base access\n[    0.105820] bio: create slab  at 0\n[    0.106567] ACPI: Added OSI(Module Device)\n[    0.107196] ACPI: Added _OSI(Processor Device)\n[    0.107849] ACPI: Added _OSI(3.0 _SCP Extensions)\n[    0.108007] ACPI: Added _OSI(Processor Aggregator Device)\n[    0.109982] ACPI: Interpreter enabled\n[    0.110532] ACPI Exception: AE_NOT_FOUND, While evaluating Sleep State [_S1] (20131115/hwxface-580)\n[    0.111948] ACPI Exception: AE_NOT_FOUND, While evaluating Sleep State [_S2_] (20131115/hwxface-580)\n[    0.112826] ACPI: (supports S0 S3 S4 S5)\n[    0.113418] ACPI: Using IOAPIC for interrupt routing\n[    0.114148] PCI: Using host bridge windows from ACPI; if necessary, use \"pci=nocrs\" and report a bug\n[    0.115848] ACPI: No dock devices found.\n[    0.119112] ACPI: PCI Root Bridge [PCI0] (domain 0000 [bus 00-ff])\n[    0.120012] acpi PNP0A03:00: _OSC: OS supports [ASPM ClockPM Segments MSI]\n[    0.121021] acpi PNP0A03:00: _OSC failed (AE_NOT_FOUND); disabling ASPM\n[    0.122028] acpi PNP0A03:00: fail to add MMCONFIG information, can't access extended PCI configuration space under this bridge.\n[    0.124038] acpiphp: Slot [3] registered\n[    0.124661] acpiphp: Slot [4] registered\n[    0.125278] acpiphp: Slot [5] registered\n[    0.125897] acpiphp: Slot [6] registered\n[    0.126551] acpiphp: Slot [7] registered\n[    0.127174] acpiphp: Slot [8] registered\n[    0.127798] acpiphp: Slot [9] registered\n[    0.128031] acpiphp: Slot [10] registered\n[    0.128669] acpiphp: Slot [11] registered\n[    0.129299] acpiphp: Slot [12] registered\n[    0.129945] acpiphp: Slot [13] registered\n[    0.130583] acpiphp: Slot [14] registered\n[    0.131214] acpiphp: Slot [15] registered\n[    0.131846] acpiphp: Slot [16] registered\n[    0.132030] acpiphp: Slot [17] registered\n[    0.132669] acpiphp: Slot [18] registered\n[    0.133309] acpiphp: Slot [19] registered\n[    0.133959] acpiphp: Slot [20] registered\n[    0.134599] acpiphp: Slot [21] registered\n[    0.135229] acpiphp: Slot [22] registered\n[    0.135866] acpiphp: Slot [23] registered\n[    0.136030] acpiphp: Slot [24] registered\n[    0.136669] acpiphp: Slot [25] registered\n[    0.137329] acpiphp: Slot [26] registered\n[    0.137966] acpiphp: Slot [27] registered\n[    0.138607] acpiphp: Slot [28] registered\n[    0.139242] acpiphp: Slot [29] registered\n[    0.139879] acpiphp: Slot [30] registered\n[    0.140030] acpiphp: Slot [31] registered\n[    0.140670] PCI host bridge to bus 0000:00\n[    0.141298] pci_bus 0000:00: root bus resource [bus 00-ff]\n[    0.142128] pci_bus 0000:00: root bus resource [io  0x0000-0x0cf7]\n[    0.143057] pci_bus 0000:00: root bus resource [io  0x0d00-0xffff]\n[    0.143984] pci_bus 0000:00: root bus resource [mem 0x000a0000-0x000bffff]\n[    0.144007] pci_bus 0000:00: root bus resource [mem 0x20000000-0xfebfffff]\n[    0.152735] pci 0000:00:01.3: quirk: [io  0x0600-0x063f] claimed by PIIX4 ACPI\n[    0.153848] pci 0000:00:01.3: quirk: [io  0x0700-0x070f] claimed by PIIX4 SMB\n[    0.190271] ACPI: PCI Interrupt Link [LNKA] (IRQs 5 10 11)\n[    0.191445] ACPI: PCI Interrupt Link [LNKB] (IRQs 5 10 11)\n[    0.192402] ACPI: PCI Interrupt Link [LNKC] (IRQs 5 10 11)\n[    0.193517] ACPI: PCI Interrupt Link [LNKD] (IRQs 5 10 11)\n[    0.194598] ACPI: PCI Interrupt Link [LNKS] (IRQs *9)\n[    0.195874] ACPI: Enabled 16 GPEs in block 00 to 0F\n[    0.196202] vgaarb: setting as boot device: PCI:0000:00:02.0\n[    0.198043] vgaarb: device added: PCI:0000:00:02.0,decodes=io+mem,owns=io+mem,locks=none\n[    0.199332] vgaarb: loaded\n[    0.199775] vgaarb: bridge control possible 0000:00:02.0\n[    0.200229] SCSI subsystem initialized\n[    0.200975] ACPI: bus type USB registered\n[    0.201662] usbcore: registered new interface driver usbfs\n[    0.202554] usbcore: registered new interface driver hub\n[    0.203429] usbcore: registered new device driver usb\n[    0.204135] PCI: Using ACPI for IRQ routing\n[    0.205108] NetLabel: Initializing\n[    0.205665] NetLabel:  domain hash size = 128\n[    0.206362] NetLabel:  protocols = UNLABELED CIPSOv4\n[    0.207172] NetLabel:  unlabeled traffic allowed by default\n[    0.208113] hpet0: at MMIO 0xfed00000, IRQs 2, 8, 0\n[    0.209002] hpet0: 3 comparators, 64-bit 100.000000 MHz counter\n[    0.215197] Switched to clocksource kvm-clock\n[    0.218504] AppArmor: AppArmor Filesystem Enabled\n[    0.219280] pnp: PnP ACPI init\n[    0.219763] ACPI: bus type PNP registered\n[    0.221268] pnp: PnP ACPI: found 11 devices\n[    0.221912] ACPI: bus type PNP unregistered\n[    0.228571] NET: Registered protocol family 2\n[    0.229487] TCP established hash table entries: 4096 (order: 3, 32768 bytes)\n[    0.230572] TCP bind hash table entries: 4096 (order: 4, 65536 bytes)\n[    0.231558] TCP: Hash tables configured (established 4096 bind 4096)\n[    0.232566] TCP: reno registered\n[    0.233106] UDP hash table entries: 256 (order: 1, 8192 bytes)\n[    0.233990] UDP-Lite hash table entries: 256 (order: 1, 8192 bytes)\n[    0.234984] NET: Registered protocol family 1\n[    0.235656] pci 0000:00:00.0: Limiting direct PCI/PCI transfers\n[    0.236575] pci 0000:00:01.0: PIIX3: Enabling Passive Release\n[    0.237478] pci 0000:00:01.0: Activating ISA DMA hang workarounds\n[    0.238668] microcode: CPU0 sig=0x306e4, pf=0x1, revision=0x1\n[    0.239571] microcode: Microcode Update Driver: v2.00 tigran@aivazian.fsnet.co.uk, Peter Oruba\n[    0.240897] Scanning for low memory corruption every 60 seconds\n[    0.242070] Initialise system trusted keyring\n[    0.242787] audit: initializing netlink socket (disabled)\n[    0.243603] type=2000 audit(1466431862.966:1): initialized\n[    0.279344] HugeTLB registered 2 MB page size, pre-allocated 0 pages\n[    0.281982] zbud: loaded\n[    0.282561] VFS: Disk quotas dquot_6.5.2\n[    0.283209] Dquot-cache hash table entries: 512 (order 0, 4096 bytes)\n[    0.284608] fuse init (API version 7.22)\n[    0.285306] msgmni has been set to 974\n[    0.285938] Key type big_key registered\n[    0.286751] Key type asymmetric registered\n[    0.287390] Asymmetric key parser 'x509' registered\n[    0.288176] Block layer SCSI generic (bsg) driver version 0.4 loaded (major 252)\n[    0.289338] io scheduler noop registered\n[    0.289946] io scheduler deadline registered (default)\n[    0.290755] io scheduler cfq registered\n[    0.291430] pci_hotplug: PCI Hot Plug PCI Core version: 0.5\n[    0.292318] pciehp: PCI Express Hot Plug Controller Driver version: 0.4\n[    0.293410] ipmi message handler version 39.2\n[    0.294132] input: Power Button as /devices/LNXSYSTM:00/LNXPWRBN:00/input/input0\n[    0.295247] ACPI: Power Button [PWRF]\n[    0.295936] GHES: HEST is not enabled!\n[    0.324185] ACPI: PCI Interrupt Link [LNKC] enabled at IRQ 11\n[    0.353272] ACPI: PCI Interrupt Link [LNKD] enabled at IRQ 10\n[    0.354407] Serial: 8250/16550 driver, 32 ports, IRQ sharing enabled\n[    0.381940] 00:05: ttyS0 at I/O 0x3f8 (irq = 4, base_baud = 115200) is a 16550A\n[    0.409209] 00:06: ttyS1 at I/O 0x2f8 (irq = 3, base_baud = 115200) is a 16550A\n[    0.411445] Linux agpgart interface v0.103\n[    0.413343] brd: module loaded\n[    0.414491] loop: module loaded\n[    0.415413] blk-mq: CPU -> queue map\n[    0.415965]   CPU 0 -> Queue 0\n[    0.417114]  vda: unknown partition table\n[    0.418925] scsi0 : ata_piix\n[    0.419450] scsi1 : ata_piix\n[    0.419939] ata1: PATA max MWDMA2 cmd 0x1f0 ctl 0x3f6 bmdma 0xc060 irq 14\n[    0.421008] ata2: PATA max MWDMA2 cmd 0x170 ctl 0x376 bmdma 0xc068 irq 15\n[    0.422660] libphy: Fixed MDIO Bus: probed\n[    0.423401] tun: Universal TUN/TAP device driver, 1.6\n[    0.424186] tun: (C) 1999-2004 Max Krasnyansky maxk@qualcomm.com\n[    0.425237] PPP generic driver version 2.4.2\n[    0.426221] ehci_hcd: USB 2.0 'Enhanced' Host Controller (EHCI) Driver\n[    0.427211] ehci-pci: EHCI PCI platform driver\n[    0.427892] ehci-platform: EHCI generic platform driver\n[    0.428711] ohci_hcd: USB 1.1 'Open' Host Controller (OHCI) Driver\n[    0.429656] ohci-pci: OHCI PCI platform driver\n[    0.430341] ohci-platform: OHCI generic platform driver\n[    0.431131] uhci_hcd: USB Universal Host Controller Interface driver\n[    0.432171] i8042: PNP: PS/2 Controller [PNP0303:KBD,PNP0f13:MOU] at 0x60,0x64 irq 1,12\n[    0.434182] serio: i8042 KBD port at 0x60,0x64 irq 1\n[    0.434943] serio: i8042 AUX port at 0x60,0x64 irq 12\n[    0.435838] mousedev: PS/2 mouse device common for all mice\n[    0.437077] input: AT Translated Set 2 keyboard as /devices/platform/i8042/serio0/input/input1\n[    0.438683] rtc_cmos 00:00: RTC can wake from S4\n[    0.439724] rtc_cmos 00:00: rtc core: registered rtc_cmos as rtc0\n[    0.440857] rtc_cmos 00:00: alarms up to one day, 114 bytes nvram, hpet irqs\n[    0.442024] device-mapper: uevent: version 1.0.3\n[    0.442825] device-mapper: ioctl: 4.27.0-ioctl (2013-10-30) initialised: dm-devel@redhat.com\n[    0.444133] ledtrig-cpu: registered to indicate activity on CPUs\n[    0.445186] TCP: cubic registered\n[    0.445816] NET: Registered protocol family 10\n[    0.446681] NET: Registered protocol family 17\n[    0.447383] Key type dns_resolver registered\n[    0.448242] Loading compiled-in X.509 certificates\n[    0.450137] Loaded X.509 cert 'Magrathea: Glacier signing key: 8010a9ac88dc2483c24f76e5c89af2b22d50934a'\n[    0.451582] registered taskstats version 1\n[    0.453030] Key type trusted registered\n[    0.453850] Key type encrypted registered\n[    0.454471] AppArmor: AppArmor sha1 policy hashing enabled\n[    0.455361] IMA: No TPM chip found, activating TPM-bypass!\n[    0.456460] regulator-dummy: disabling\n[    0.457211]   Magic number: 4:603:182\n[    0.457933] rtc_cmos 00:00: setting system clock to 2016-06-20 14:11:02 UTC (1466431862)\n[    0.459236] BIOS EDD facility v0.16 2004-Jun-25, 0 devices found\n[    0.460183] EDD information not available.\n[    0.580867] ata2.00: ATAPI: QEMU DVD-ROM, 2.4.0, max UDMA/100\n[    0.582372] ata2.00: configured for MWDMA2\n[    0.583623] scsi 1:0:0:0: CD-ROM            QEMU     QEMU DVD-ROM     2.4. PQ: 0 ANSI: 5\n[    0.585784] sr0: scsi3-mmc drive: 4x/4x cd/rw xa/form2 tray\n[    0.586650] cdrom: Uniform CD-ROM driver Revision: 3.20\n[    0.588514] sr 1:0:0:0: Attached scsi generic sg0 type 5\n[    1.236125] tsc: Refined TSC clocksource calibration: 1799.988 MHz\n[   12.508790] md: Waiting for all devices to be available before autodetect\n[   12.510472] md: If you don't use raid, use raid=noautodetect\n[   12.512046] md: Autodetecting RAID arrays.\n[   12.513070] md: Scanned 0 and added 0 devices.\n[   12.514115] md: autorun ...\n[   12.514771] md: ... autorun DONE.\n[   12.516031] EXT4-fs (vda): couldn't mount as ext3 due to feature incompatibilities\n[   12.518207] EXT4-fs (vda): mounting ext2 file system using the ext4 subsystem\n[   12.521344] EXT4-fs (vda): mounted filesystem without journal. Opts: (null)\n[   12.522939] VFS: Mounted root (ext2 filesystem) on device 253:0.\n[   12.524844] devtmpfs: mounted\n[   12.527117] Freeing unused kernel memory: 1336K (ffffffff81d20000 - ffffffff81e6e000)\n[   12.528831] Write protecting the kernel read-only data: 12288k\n[   12.533372] Freeing unused kernel memory: 800K (ffff880001738000 - ffff880001800000)\n[   12.537405] Freeing unused kernel memory: 716K (ffff880001b4d000 - ffff880001c00000)\nMount failed for selinuxfs on /sys/fs/selinux:  No such file or directory\n[   12.570271] random: init urandom read with 5 bits of entropy available\n[   12.605138] init: plymouth-upstart-bridge main process (74) terminated with status 1\n[   12.606373] init: plymouth-upstart-bridge main process ended, respawning\n[   12.623898] init: plymouth-upstart-bridge main process (85) terminated with status 1\n[   12.625277] init: plymouth-upstart-bridge main process ended, respawning\n[   12.636596] init: plymouth-upstart-bridge main process (90) terminated with status 1\n[   12.637890] init: plymouth-upstart-bridge main process ended, respawning\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n[  116.368982] BUG: soft lockup - CPU#0 stuck for 34s! [bash:553]\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n(process:60): GLib-CRITICAL **: g_io_channel_unref: assertion 'channel != NULL' failed\n```\nmax@davos:~/snabbswitch/src$ cat qemu1.log\nQEMU waiting for connection on: disconnected:unix:vhost_B.sock,server\nqemu-system-x86_64: -netdev type=vhost-user,id=net0,chardev=char0: chardev \"char0\" went up\n[    0.000000] Initializing cgroup subsys cpuset\n[    0.000000] Initializing cgroup subsys cpu\n[    0.000000] Initializing cgroup subsys cpuacct\n[    0.000000] Linux version 3.13.11-ckt26 (eugeneia@chur) (gcc version 4.9.3 (GCC) ) #3 SMP Tue Oct 6 14:01:23 UTC 2015 ()\n[    0.000000] Command line: earlyprintk root=/dev/vda  rw console=ttyS1 ip=fe80::5054:ff:fe00:0001\n[    0.000000] KERNEL supported cpus:\n[    0.000000]   Intel GenuineIntel\n[    0.000000]   AMD AuthenticAMD\n[    0.000000]   Centaur CentaurHauls\n[    0.000000] e820: BIOS-provided physical RAM map:\n[    0.000000] BIOS-e820: [mem 0x0000000000000000-0x000000000009fbff] usable\n[    0.000000] BIOS-e820: [mem 0x000000000009fc00-0x000000000009ffff] reserved\n[    0.000000] BIOS-e820: [mem 0x00000000000f0000-0x00000000000fffff] reserved\n[    0.000000] BIOS-e820: [mem 0x0000000000100000-0x000000001ffdffff] usable\n[    0.000000] BIOS-e820: [mem 0x000000001ffe0000-0x000000001fffffff] reserved\n[    0.000000] BIOS-e820: [mem 0x00000000feffc000-0x00000000feffffff] reserved\n[    0.000000] BIOS-e820: [mem 0x00000000fffc0000-0x00000000ffffffff] reserved\n[    0.000000] NX (Execute Disable) protection: active\n[    0.000000] SMBIOS 2.8 present.\n[    0.000000] Hypervisor detected: KVM\n[    0.000000] No AGP bridge found\n[    0.000000] e820: last_pfn = 0x1ffe0 max_arch_pfn = 0x400000000\n[    0.000000] x86 PAT enabled: cpu 0, old 0x7040600070406, new 0x7010600070106\n[    0.000000] found SMP MP-table at [mem 0x000f64b0-0x000f64bf] mapped at [ffff8800000f64b0]\n[    0.000000] Scanning 1 areas for low memory corruption\n[    0.000000] Using GB pages for direct mapping\n[    0.000000] init_memory_mapping: [mem 0x00000000-0x000fffff]\n[    0.000000] init_memory_mapping: [mem 0x1fc00000-0x1fdfffff]\n[    0.000000] init_memory_mapping: [mem 0x1c000000-0x1fbfffff]\n[    0.000000] init_memory_mapping: [mem 0x00100000-0x1bffffff]\n[    0.000000] init_memory_mapping: [mem 0x1fe00000-0x1ffdffff]\n[    0.000000] ACPI: RSDP 00000000000f62c0 000014 (v00 BOCHS )\n[    0.000000] ACPI: RSDT 000000001ffe176c 000038 (v01 BOCHS  BXPCRSDT 00000001 BXPC 00000001)\n[    0.000000] ACPI: FACP 000000001ffe0bda 000074 (v01 BOCHS  BXPCFACP 00000001 BXPC 00000001)\n[    0.000000] ACPI: DSDT 000000001ffe0040 000B9A (v01 BOCHS  BXPCDSDT 00000001 BXPC 00000001)\n[    0.000000] ACPI: FACS 000000001ffe0000 000040\n[    0.000000] ACPI: SSDT 000000001ffe0c4e 0009B6 (v01 BOCHS  BXPCSSDT 00000001 BXPC 00000001)\n[    0.000000] ACPI: APIC 000000001ffe1604 000078 (v01 BOCHS  BXPCAPIC 00000001 BXPC 00000001)\n[    0.000000] ACPI: HPET 000000001ffe167c 000038 (v01 BOCHS  BXPCHPET 00000001 BXPC 00000001)\n[    0.000000] ACPI: SRAT 000000001ffe16b4 0000B8 (v01 BOCHS  BXPCSRAT 00000001 BXPC 00000001)\n[    0.000000] SRAT: PXM 0 -> APIC 0x00 -> Node 0\n[    0.000000] SRAT: Node 0 PXM 0 [mem 0x00000000-0x0009ffff]\n[    0.000000] SRAT: Node 0 PXM 0 [mem 0x00100000-0x1fffffff]\n[    0.000000] NUMA: Node 0 [mem 0x00000000-0x0009ffff] + [mem 0x00100000-0x1ffdffff] -> [mem 0x00000000-0x1ffdffff]\n[    0.000000] Initmem setup node 0 [mem 0x00000000-0x1ffdffff]\n[    0.000000]   NODE_DATA [mem 0x1ffdb000-0x1ffdffff]\n[    0.000000] kvm-clock: Using msrs 4b564d01 and 4b564d00\n[    0.000000] kvm-clock: cpu 0, msr 0:1ffd7001, boot clock\n[    0.000000] Zone ranges:\n[    0.000000]   DMA      [mem 0x00001000-0x00ffffff]\n[    0.000000]   DMA32    [mem 0x01000000-0xffffffff]\n[    0.000000]   Normal   empty\n[    0.000000] Movable zone start for each node\n[    0.000000] Early memory node ranges\n[    0.000000]   node   0: [mem 0x00001000-0x0009efff]\n[    0.000000]   node   0: [mem 0x00100000-0x1ffdffff]\n[    0.000000] ACPI: PM-Timer IO Port: 0x608\n[    0.000000] ACPI: LAPIC (acpi_id[0x00] lapic_id[0x00] enabled)\n[    0.000000] ACPI: LAPIC_NMI (acpi_id[0xff] dfl dfl lint[0x1])\n[    0.000000] ACPI: IOAPIC (id[0x00] address[0xfec00000] gsi_base[0])\n[    0.000000] IOAPIC[0]: apic_id 0, version 17, address 0xfec00000, GSI 0-23\n[    0.000000] ACPI: INT_SRC_OVR (bus 0 bus_irq 0 global_irq 2 dfl dfl)\n[    0.000000] ACPI: INT_SRC_OVR (bus 0 bus_irq 5 global_irq 5 high level)\n[    0.000000] ACPI: INT_SRC_OVR (bus 0 bus_irq 9 global_irq 9 high level)\n[    0.000000] ACPI: INT_SRC_OVR (bus 0 bus_irq 10 global_irq 10 high level)\n[    0.000000] ACPI: INT_SRC_OVR (bus 0 bus_irq 11 global_irq 11 high level)\n[    0.000000] Using ACPI (MADT) for SMP configuration information\n[    0.000000] ACPI: HPET id: 0x8086a201 base: 0xfed00000\n[    0.000000] smpboot: Allowing 1 CPUs, 0 hotplug CPUs\n[    0.000000] PM: Registered nosave memory: [mem 0x0009f000-0x0009ffff]\n[    0.000000] PM: Registered nosave memory: [mem 0x000a0000-0x000effff]\n[    0.000000] PM: Registered nosave memory: [mem 0x000f0000-0x000fffff]\n[    0.000000] e820: [mem 0x20000000-0xfeffbfff] available for PCI devices\n[    0.000000] Booting paravirtualized kernel on KVM\n[    0.000000] setup_percpu: NR_CPUS:256 nr_cpumask_bits:256 nr_cpu_ids:1 nr_node_ids:1\n[    0.000000] PERCPU: Embedded 27 pages/cpu @ffff88001fc00000 s81536 r8192 d20864 u2097152\n[    0.000000] kvm-clock: cpu 0, msr 0:1ffd7001, primary cpu clock\n[    0.000000] KVM setup async PF for cpu 0\n[    0.000000] kvm-stealtime: cpu 0, msr 1fc0d000\n[    0.000000] Built 1 zonelists in Node order, mobility grouping on.  Total pages: 128873\n[    0.000000] Policy zone: DMA32\n[    0.000000] Kernel command line: earlyprintk root=/dev/vda  rw console=ttyS1 ip=fe80::5054:ff:fe00:0001\n[    0.000000] PID hash table entries: 2048 (order: 2, 16384 bytes)\n[    0.000000] xsave: enabled xstate_bv 0x7, cntxt size 0x340\n[    0.000000] Checking aperture...\n[    0.000000] No AGP bridge found\n[    0.000000] Memory: 498824K/523768K available (7379K kernel code, 1146K rwdata, 3380K rodata, 1336K init, 1448K bss, 24944K reserved)\n[    0.000000] SLUB: HWalign=64, Order=0-3, MinObjects=0, CPUs=1, Nodes=1\n[    0.000000] Hierarchical RCU implementation.\n[    0.000000]  RCU dyntick-idle grace-period acceleration is enabled.\n[    0.000000]  RCU restricting CPUs from NR_CPUS=256 to nr_cpu_ids=1.\n[    0.000000]  Offload RCU callbacks from all CPUs\n[    0.000000]  Offload RCU callbacks from CPUs: 0.\n[    0.000000] NR_IRQS:16640 nr_irqs:256 16\n[    0.000000] Console: colour VGA+ 80x25\n[    0.000000] console [ttyS1] enabled\n[    0.000000] allocated 2097152 bytes of page_cgroup\n[    0.000000] please try 'cgroup_disable=memory' option if you don't want memory cgroups\n[    0.000000] tsc: Detected 1799.993 MHz processor\n[    0.008000] Calibrating delay loop (skipped) preset value.. 3599.98 BogoMIPS (lpj=7199972)\n[    0.008000] pid_max: default: 32768 minimum: 301\n[    0.008000] Security Framework initialized\n[    0.008000] AppArmor: AppArmor initialized\n[    0.008006] Yama: becoming mindful.\n[    0.008706] Dentry cache hash table entries: 65536 (order: 7, 524288 bytes)\n[    0.010054] Inode-cache hash table entries: 32768 (order: 6, 262144 bytes)\n[    0.011289] Mount-cache hash table entries: 1024 (order: 1, 8192 bytes)\n[    0.012009] Mountpoint-cache hash table entries: 1024 (order: 1, 8192 bytes)\n[    0.013355] Initializing cgroup subsys memory\n[    0.014093] Initializing cgroup subsys devices\n[    0.014847] Initializing cgroup subsys freezer\n[    0.016007] Initializing cgroup subsys blkio\n[    0.016734] Initializing cgroup subsys perf_event\n[    0.017507] Initializing cgroup subsys hugetlb\n[    0.019418] mce: CPU supports 10 MCE banks\n[    0.020064] Last level iTLB entries: 4KB 512, 2MB 0, 4MB 0\n[    0.020064] Last level dTLB entries: 4KB 512, 2MB 0, 4MB 0\n[    0.020064] tlb_flushall_shift: 6\n[    0.043304] Freeing SMP alternatives memory: 32K (ffffffff81e6e000 - ffffffff81e76000)\n[    0.049902] ACPI: Core revision 20131115\n[    0.051448] ACPI: All ACPI Tables successfully acquired\n[    0.052088] ftrace: allocating 28502 entries in 112 pages\n[    0.064325] Enabling x2apic\n[    0.064853] Enabled x2apic\n[    0.068006] Switched APIC routing to physical x2apic.\n[    0.070081] ..TIMER: vector=0x30 apic1=0 pin1=2 apic2=-1 pin2=-1\n[    0.072003] smpboot: CPU0: Intel(R) Xeon(R) CPU E5-2603 v2 @ 1.80GHz (fam: 06, model: 3e, stepping: 04)\n[    0.073663] Performance Events: 16-deep LBR, IvyBridge events, Intel PMU driver.\n[    0.076801] ... version:                2\n[    0.077467] ... bit width:              48\n[    0.078130] ... generic registers:      8\n[    0.078777] ... value mask:             0000ffffffffffff\n[    0.079664] ... max period:             000000007fffffff\n[    0.080005] ... fixed-purpose events:   3\n[    0.080687] ... event mask:             00000007000000ff\n[    0.089431] KVM setup paravirtual spinlock\n[    0.091936] x86: Booted up 1 node, 1 CPUs\n[    0.092008] smpboot: Total of 1 processors activated (3599.98 BogoMIPS)\n[    0.093469] NMI watchdog: enabled on all CPUs, permanently consumes one hw-PMU counter.\n[    0.094936] devtmpfs: initialized\n[    0.099234] EVM: security.selinux\n[    0.099841] EVM: security.SMACK64\n[    0.100007] EVM: security.ima\n[    0.100507] EVM: security.capability\n[    0.102660] pinctrl core: initialized pinctrl subsystem\n[    0.103634] regulator-dummy: no parameters\n[    0.104110] RTC time: 14:11:04, date: 06/20/16\n[    0.104893] NET: Registered protocol family 16\n[    0.105670] cpuidle: using governor ladder\n[    0.106256] cpuidle: using governor menu\n[    0.106899] ACPI: bus type PCI registered\n[    0.107583] acpiphp: ACPI Hot Plug PCI Controller Driver version: 0.5\n[    0.108143] PCI: Using configuration type 1 for base access\n[    0.109905] bio: create slab <bio-0> at 0\n[    0.110759] ACPI: Added _OSI(Module Device)\n[    0.111470] ACPI: Added _OSI(Processor Device)\n[    0.112007] ACPI: Added _OSI(3.0 _SCP Extensions)\n[    0.112811] ACPI: Added _OSI(Processor Aggregator Device)\n[    0.114928] ACPI: Interpreter enabled\n[    0.115575] ACPI Exception: AE_NOT_FOUND, While evaluating Sleep State [\\_S1_] (20131115/hwxface-580)\n[    0.116934] ACPI Exception: AE_NOT_FOUND, While evaluating Sleep State [\\_S2_] (20131115/hwxface-580)\n[    0.118510] ACPI: (supports S0 S3 S4 S5)\n[    0.119165] ACPI: Using IOAPIC for interrupt routing\n[    0.120015] PCI: Using host bridge windows from ACPI; if necessary, use \"pci=nocrs\" and report a bug\n[    0.121885] ACPI: No dock devices found.\n[    0.125623] ACPI: PCI Root Bridge [PCI0] (domain 0000 [bus 00-ff])\n[    0.126688] acpi PNP0A03:00: _OSC: OS supports [ASPM ClockPM Segments MSI]\n[    0.127842] acpi PNP0A03:00: _OSC failed (AE_NOT_FOUND); disabling ASPM\n[    0.128025] acpi PNP0A03:00: fail to add MMCONFIG information, can't access extended PCI configuration space under this bridge.\n[    0.130248] acpiphp: Slot [3] registered\n[    0.130939] acpiphp: Slot [4] registered\n[    0.131647] acpiphp: Slot [5] registered\n[    0.132033] acpiphp: Slot [6] registered\n[    0.132764] acpiphp: Slot [7] registered\n[    0.133443] acpiphp: Slot [8] registered\n[    0.134116] acpiphp: Slot [9] registered\n[    0.134803] acpiphp: Slot [10] registered\n[    0.135526] acpiphp: Slot [11] registered\n[    0.136032] acpiphp: Slot [12] registered\n[    0.136732] acpiphp: Slot [13] registered\n[    0.137416] acpiphp: Slot [14] registered\n[    0.138129] acpiphp: Slot [15] registered\n[    0.138823] acpiphp: Slot [16] registered\n[    0.139522] acpiphp: Slot [17] registered\n[    0.140033] acpiphp: Slot [18] registered\n[    0.140744] acpiphp: Slot [19] registered\n[    0.141448] acpiphp: Slot [20] registered\n[    0.142146] acpiphp: Slot [21] registered\n[    0.142859] acpiphp: Slot [22] registered\n[    0.143467] acpiphp: Slot [23] registered\n[    0.144030] acpiphp: Slot [24] registered\n[    0.144626] acpiphp: Slot [25] registered\n[    0.145217] acpiphp: Slot [26] registered\n[    0.145811] acpiphp: Slot [27] registered\n[    0.146407] acpiphp: Slot [28] registered\n[    0.147017] acpiphp: Slot [29] registered\n[    0.147654] acpiphp: Slot [30] registered\n[    0.148030] acpiphp: Slot [31] registered\n[    0.148646] PCI host bridge to bus 0000:00\n[    0.149253] pci_bus 0000:00: root bus resource [bus 00-ff]\n[    0.150052] pci_bus 0000:00: root bus resource [io  0x0000-0x0cf7]\n[    0.150968] pci_bus 0000:00: root bus resource [io  0x0d00-0xffff]\n[    0.151888] pci_bus 0000:00: root bus resource [mem 0x000a0000-0x000bffff]\n[    0.152007] pci_bus 0000:00: root bus resource [mem 0x20000000-0xfebfffff]\n[    0.160734] pci 0000:00:01.3: quirk: [io  0x0600-0x063f] claimed by PIIX4 ACPI\n[    0.161823] pci 0000:00:01.3: quirk: [io  0x0700-0x070f] claimed by PIIX4 SMB\n[    0.195261] ACPI: PCI Interrupt Link [LNKA] (IRQs 5 *10 11)\n[    0.196405] ACPI: PCI Interrupt Link [LNKB] (IRQs 5 *10 11)\n[    0.197558] ACPI: PCI Interrupt Link [LNKC] (IRQs 5 10 *11)\n[    0.198739] ACPI: PCI Interrupt Link [LNKD] (IRQs 5 10 *11)\n[    0.199855] ACPI: PCI Interrupt Link [LNKS] (IRQs *9)\n[    0.200536] ACPI: Enabled 16 GPEs in block 00 to 0F\n[    0.201592] vgaarb: setting as boot device: PCI:0000:00:02.0\n[    0.202482] vgaarb: device added: PCI:0000:00:02.0,decodes=io+mem,owns=io+mem,locks=none\n[    0.203600] vgaarb: loaded\n[    0.203992] vgaarb: bridge control possible 0000:00:02.0\n[    0.204205] SCSI subsystem initialized\n[    0.204836] ACPI: bus type USB registered\n[    0.205461] usbcore: registered new interface driver usbfs\n[    0.206287] usbcore: registered new interface driver hub\n[    0.207058] usbcore: registered new device driver usb\n[    0.207894] PCI: Using ACPI for IRQ routing\n[    0.208261] NetLabel: Initializing\n[    0.208856] NetLabel:  domain hash size = 128\n[    0.209611] NetLabel:  protocols = UNLABELED CIPSOv4\n[    0.210457] NetLabel:  unlabeled traffic allowed by default\n[    0.211474] hpet0: at MMIO 0xfed00000, IRQs 2, 8, 0\n[    0.212237] hpet0: 3 comparators, 64-bit 100.000000 MHz counter\n[    0.216041] Switched to clocksource kvm-clock\n[    0.222582] AppArmor: AppArmor Filesystem Enabled\n[    0.223412] pnp: PnP ACPI init\n[    0.223949] ACPI: bus type PNP registered\n[    0.225458] pnp: PnP ACPI: found 11 devices\n[    0.226165] ACPI: bus type PNP unregistered\n[    0.232930] NET: Registered protocol family 2\n[    0.233805] TCP established hash table entries: 4096 (order: 3, 32768 bytes)\n[    0.234812] TCP bind hash table entries: 4096 (order: 4, 65536 bytes)\n[    0.235724] TCP: Hash tables configured (established 4096 bind 4096)\n[    0.236728] TCP: reno registered\n[    0.237292] UDP hash table entries: 256 (order: 1, 8192 bytes)\n[    0.238268] UDP-Lite hash table entries: 256 (order: 1, 8192 bytes)\n[    0.239342] NET: Registered protocol family 1\n[    0.240113] pci 0000:00:00.0: Limiting direct PCI/PCI transfers\n[    0.241124] pci 0000:00:01.0: PIIX3: Enabling Passive Release\n[    0.242095] pci 0000:00:01.0: Activating ISA DMA hang workarounds\n[    0.243390] microcode: CPU0 sig=0x306e4, pf=0x1, revision=0x1\n[    0.244455] microcode: Microcode Update Driver: v2.00 <tigran@aivazian.fsnet.co.uk>, Peter Oruba\n[    0.245919] Scanning for low memory corruption every 60 seconds\n[    0.247149] Initialise system trusted keyring\n[    0.247925] audit: initializing netlink socket (disabled)\n[    0.248853] type=2000 audit(1466431865.383:1): initialized\n[    0.284651] HugeTLB registered 2 MB page size, pre-allocated 0 pages\n[    0.287340] zbud: loaded\n[    0.287951] VFS: Disk quotas dquot_6.5.2\n[    0.288693] Dquot-cache hash table entries: 512 (order 0, 4096 bytes)\n[    0.290190] fuse init (API version 7.22)\n[    0.290938] msgmni has been set to 974\n[    0.291622] Key type big_key registered\n[    0.292544] Key type asymmetric registered\n[    0.293250] Asymmetric key parser 'x509' registered\n[    0.294084] Block layer SCSI generic (bsg) driver version 0.4 loaded (major 252)\n[    0.295368] io scheduler noop registered\n[    0.296063] io scheduler deadline registered (default)\n[    0.296982] io scheduler cfq registered\n[    0.297724] pci_hotplug: PCI Hot Plug PCI Core version: 0.5\n[    0.298682] pciehp: PCI Express Hot Plug Controller Driver version: 0.4\n[    0.299834] ipmi message handler version 39.2\n[    0.300677] input: Power Button as /devices/LNXSYSTM:00/LNXPWRBN:00/input/input0\n[    0.301913] ACPI: Power Button [PWRF]\n[    0.302648] GHES: HEST is not enabled!\n[    0.330972] ACPI: PCI Interrupt Link [LNKC] enabled at IRQ 11\n[    0.360160] ACPI: PCI Interrupt Link [LNKD] enabled at IRQ 10\n[    0.361252] Serial: 8250/16550 driver, 32 ports, IRQ sharing enabled\n[    0.388262] 00:05: ttyS0 at I/O 0x3f8 (irq = 4, base_baud = 115200) is a 16550A\n[    0.415487] 00:06: ttyS1 at I/O 0x2f8 (irq = 3, base_baud = 115200) is a 16550A\n[    0.417714] Linux agpgart interface v0.103\n[    0.419490] brd: module loaded\n[    0.420620] loop: module loaded\n[    0.421602] blk-mq: CPU -> queue map\n[    0.422210]   CPU 0 -> Queue 0\n[    0.423346]  vda: unknown partition table\n[    0.425301] scsi0 : ata_piix\n[    0.425853] scsi1 : ata_piix\n[    0.426373] ata1: PATA max MWDMA2 cmd 0x1f0 ctl 0x3f6 bmdma 0xc060 irq 14\n[    0.427466] ata2: PATA max MWDMA2 cmd 0x170 ctl 0x376 bmdma 0xc068 irq 15\n[    0.429172] libphy: Fixed MDIO Bus: probed\n[    0.429963] tun: Universal TUN/TAP device driver, 1.6\n[    0.430796] tun: (C) 1999-2004 Max Krasnyansky <maxk@qualcomm.com>\n[    0.432152] PPP generic driver version 2.4.2\n[    0.432877] ehci_hcd: USB 2.0 'Enhanced' Host Controller (EHCI) Driver\n[    0.433978] ehci-pci: EHCI PCI platform driver\n[    0.434728] ehci-platform: EHCI generic platform driver\n[    0.435615] ohci_hcd: USB 1.1 'Open' Host Controller (OHCI) Driver\n[    0.436702] ohci-pci: OHCI PCI platform driver\n[    0.437478] ohci-platform: OHCI generic platform driver\n[    0.438368] uhci_hcd: USB Universal Host Controller Interface driver\n[    0.439471] i8042: PNP: PS/2 Controller [PNP0303:KBD,PNP0f13:MOU] at 0x60,0x64 irq 1,12\n[    0.441593] serio: i8042 KBD port at 0x60,0x64 irq 1\n[    0.442421] serio: i8042 AUX port at 0x60,0x64 irq 12\n[    0.443382] mousedev: PS/2 mouse device common for all mice\n[    0.444673] input: AT Translated Set 2 keyboard as /devices/platform/i8042/serio0/input/input1\n[    0.446159] rtc_cmos 00:00: RTC can wake from S4\n[    0.447135] rtc_cmos 00:00: rtc core: registered rtc_cmos as rtc0\n[    0.448259] rtc_cmos 00:00: alarms up to one day, 114 bytes nvram, hpet irqs\n[    0.449426] device-mapper: uevent: version 1.0.3\n[    0.450172] device-mapper: ioctl: 4.27.0-ioctl (2013-10-30) initialised: dm-devel@redhat.com\n[    0.451344] ledtrig-cpu: registered to indicate activity on CPUs\n[    0.452324] TCP: cubic registered\n[    0.452987] NET: Registered protocol family 10\n[    0.453883] NET: Registered protocol family 17\n[    0.454655] Key type dns_resolver registered\n[    0.455553] Loading compiled-in X.509 certificates\n[    0.457508] Loaded X.509 cert 'Magrathea: Glacier signing key: 8010a9ac88dc2483c24f76e5c89af2b22d50934a'\n[    0.459100] registered taskstats version 1\n[    0.460624] Key type trusted registered\n[    0.461501] Key type encrypted registered\n[    0.462183] AppArmor: AppArmor sha1 policy hashing enabled\n[    0.463100] IMA: No TPM chip found, activating TPM-bypass!\n[    0.464331] regulator-dummy: disabling\n[    0.464995]   Magic number: 4:603:182\n[    0.465753] rtc_cmos 00:00: setting system clock to 2016-06-20 14:11:05 UTC (1466431865)\n[    0.467137] BIOS EDD facility v0.16 2004-Jun-25, 0 devices found\n[    0.468168] EDD information not available.\n[    0.588877] ata2.00: ATAPI: QEMU DVD-ROM, 2.4.0, max UDMA/100\n[    0.590407] ata2.00: configured for MWDMA2\n[    0.591674] scsi 1:0:0:0: CD-ROM            QEMU     QEMU DVD-ROM     2.4. PQ: 0 ANSI: 5\n[    0.593961] sr0: scsi3-mmc drive: 4x/4x cd/rw xa/form2 tray\n[    0.594884] cdrom: Uniform CD-ROM driver Revision: 3.20\n[    0.596792] sr 1:0:0:0: Attached scsi generic sg0 type 5\n[    1.240143] tsc: Refined TSC clocksource calibration: 1799.989 MHz\n[   12.516613] md: Waiting for all devices to be available before autodetect\n[   12.517688] md: If you don't use raid, use raid=noautodetect\n[   12.518679] md: Autodetecting RAID arrays.\n[   12.519278] md: Scanned 0 and added 0 devices.\n[   12.519915] md: autorun ...\n[   12.520453] md: ... autorun DONE.\n[   12.521279] EXT4-fs (vda): couldn't mount as ext3 due to feature incompatibilities\n[   12.522571] EXT4-fs (vda): mounting ext2 file system using the ext4 subsystem\n[   12.525226] EXT4-fs (vda): mounted filesystem without journal. Opts: (null)\n[   12.526287] VFS: Mounted root (ext2 filesystem) on device 253:0.\n[   12.527709] devtmpfs: mounted\n[   12.529498] Freeing unused kernel memory: 1336K (ffffffff81d20000 - ffffffff81e6e000)\n[   12.530870] Write protecting the kernel read-only data: 12288k\n[   12.534403] Freeing unused kernel memory: 800K (ffff880001738000 - ffff880001800000)\n[   12.538098] Freeing unused kernel memory: 716K (ffff880001b4d000 - ffff880001c00000)\nMount failed for selinuxfs on /sys/fs/selinux:  No such file or directory\n[   12.563353] random: init urandom read with 5 bits of entropy available\n[   12.591004] init: plymouth-upstart-bridge main process (74) terminated with status 1\n[   12.592528] init: plymouth-upstart-bridge main process ended, respawning\n[   12.610188] init: plymouth-upstart-bridge main process (84) terminated with status 1\n[   12.611746] init: plymouth-upstart-bridge main process ended, respawning\n[   12.623241] init: plymouth-upstart-bridge main process (90) terminated with status 1\n[   12.624742] init: plymouth-upstart-bridge main process ended, respawning\n. @domenkozar The bug isn't triggered in QEMU v2.4.1, I opened #948 to avoid this issue.\n. Ugh I fat-fingered the close button, sorry!\n@dpino I would say new is the counterpart to stop. I think configure makes sense, as this callback lets apps react to the way they were configured.\n. @lukego Last commit renames and documents the callback as per #946.\n. :+1: So I guess I will rename configure to link in #945.\n\ncreate counters by appname in this case\n\nNote that #931 implements private namespaces for per-app counters by default.\n. @kbara merge conflicts with which branch, kbara-next?  Current policy would be for upstream (you) to resolve that conflict. If I merged kbara-next into this branch this PR would include all commits in master..kbara-next\n. I reverted the removal of link.full and link.nwritable, given their potential use in QoS apps. Also fixed some bugs. I am removing the [sketch] and pushing for this to land! :-)\n. @kbara Do you think this could land in the September release?\n. @kbara You could merge next into kbara-next, then merge this PR.\n. @petebristow Since this PR overlaps a lot with #897 could you pull any changes that are not covered by #897 (e.g. applicable to your HEAD), except 1ccd4d7.\n@alexandergall I don't want to accept 1ccd4d7 because I just extracted that code from intel10g in #931 and there is no need to duplicate it, besides I don't want SNMP / ifMIB / RFC7223 specific code in drivers. Once #897 lands I will merge it with my statistics branch and lib.ipc.shmem.init_snmp from #931 will be able to attach to intel1g through the standard statistics interface.\n. @lukego I have tracked down the regression to the statistics counters in vhost_user. I pushed e3fcbbf to #931 which reverts a premature optimization I did, and this removes half of the slowdown. That leaves us with a 2.5% regression. Now looking into what can be done about the remaining slowdown.\nEdit: well \u201cpremature optimization\u201d is the wrong term here really as I tried to outsmart the compiler and failed.\n. Profiling tells me that 10% of time is spent in counter.add, and does not highlight vhost_user in particular. Theory: more counters \u2192 more work. Possible solutions I can think of:\n- optimize counter.add\n- update vhost_user to invoke counter.add less, e.g. aggregate updates like intel10g does\n. Alternatively we could omit counting successfully processed rx/tx packets in vhost_user, and rely on the existing link statistics to reflect these. (rx/tx packes/bytes app-counters are mostly redundant with respect to link stats.)\n. @kbara Sure! This is super handy, I will merge this as-is.\n. @lukego @kbara actually I think this might not be ready yet, there is another performance regression in here that I haven't figured out yet.\n. uhm... false alarm? ^^' I got things mixed up in my testing, shm_frame is actually OK. Can we still paddle back? I'm super sorry for the misinformation.\n. :fireworks: Looking great! Can't wait to look at some cool plots. :-)\nDepending on whether #972 gets accepted, I would adapt the API of core.timeline to provide create and open (instead of new) and register it as the type timeline. Makes sense? It would probably also make sense to add a snabb top -t option to pretty-print regions of timeline objects.\nSuper minor nitpick, question: When formatting optional parameters in Markdown we used *[param]*, but I prefer to use [*param*] (and that is what I adopted for #972). E.g. italic marks the symbol, braces are semantic hints. Do you mind if I change that to the latter style when merging this?\nEdit: I am now realizing why we used *[param]*, the alternative renders to a link... hmm.\nEdit2: Apparently only in GitHub comments.\n. I think adding SNABB_SHM_ROOT is an appropriate solution, combined with SNABB_DEBUG=yes (which causes the runtime directory to persist after exit) it should do what we need. \n. @lukego Good point, done.\n. @lukego I have added a symlink instead, lets see if that works.\n. There is lib.getenv for that purpose, so you don't have to do that check ad-hoc.\n. I have merged master into ipsecand pushed it, would GitHub to update the PR soon(?).\n. Hmm GitHUb doesn't seem to pick up the updated branches, maybe close/reopen? Not sure if that will do anything.\n. As I just commented in #972 I don't think this is ready yet. \n. Thanks, again sorry for the confusion!\n. I have previously done this by using shm.children and probing the resulting table. Alternatively we could use pcall to catch the error, but that would be less granular. Obviously convenient, merged (I will edit the documentation a bit to describe the return type). :+1: \n. @kbara currently no, but there are at least open pull requests that would benefit from it, see https://github.com/snabbco/snabb/pull/897/files#r75841977\nBasically, code that invokes struct packet benefits from this:\n% grep -R \"struct packet\" snabb/src\nsnabb/src/apps/intel/intel10g.lua:   self.txpackets = ffi.new(\"struct packet *[?]\", num_descriptors)\nsnabb/src/apps/intel/intel10g.lua:   self.rxpackets = ffi.new(\"struct packet *[?]\", num_descriptors)\nsnabb/src/apps/intel/intel10g.lua:   self.txpackets = ffi.new(\"struct packet *[?]\", num_descriptors)\nsnabb/src/apps/intel/intel10g.lua:   self.rxpackets = ffi.new(\"struct packet *[?]\", num_descriptors)\nsnabb/src/apps/bridge/learning.h:  struct packet *p;\nsnabb/src/apps/bridge/learning.h:                          handle_t port, handle_t group, struct packet *p,\nsnabb/src/apps/bridge/mac_table.c:                          handle_t port, handle_t group, struct packet *p,\nsnabb/src/apps/lwaftr/binding_table.lua:   ret.packet_queue = ffi.new(\"struct packet * [32]\")\nsnabb/src/apps/solarflare/solarflare.lua:   self.rxpackets = ffi.new(\"struct packet *[?]\", RECEIVE_BUFFER_COUNT + 1)\nsnabb/src/apps/solarflare/solarflare.lua:   self.tx_packets = ffi.new(\"struct packet *[?]\", TX_BUFFER_COUNT + 1)\nsnabb/src/apps/socket/raw.lua:   -- Construct packet.\nsnabb/src/apps/ipv6/nd_light.lua:      p = ffi.new(\"struct packet *[1]\"),\nsnabb/src/doc/core-data-structures.md:struct packet {\nsnabb/src/doc/core-data-structures.md:  struct packet *packets[256];\n. Actually, looking at the grep matches, it might even make sense to expose a packet_ptr_t since its use from Lua code seems to be popular.\n. @lukego ping/done.\n. @lukego I think basic1 score went down because it is processing less packets per breath due to being limited to engine.pull_npackets (see https://github.com/snabbco/snabb/pull/997/files#diff-9f48c75e1c64e2dff09340bf4a6d9815L22). Before it would have filled up the whole link on each breath.\n. Might actually make sense to revert the change I referred to above and see how that affects basic1. A case could be made for the original behavior, since Source, being a load generator, should keep its output links saturated? On the other hand, limiting to engine.pull_npackets emulates the behavior of a NIC app and might yield more realistic results. We can now also tweak engine.pull_npackets and see how that affects the various benchmarks.\n. Good analysis. My mistake thinking ffi.typeof would create a new ctype every time. I guess reverting cbed511 and updating src/README to refer to struct packet instead of packet.packet_t is the way forward then?\n. How would you feel if I renamed the function to  to_pointer (as a counterpart of packet.from_pointer)?\n. On second look clone_to_memory does something different than I had thought: it expects the destination to point to a struct packet (it sets dstp.length), so its not really the inverse of from_pointer. Wouldn't it make more sense to add an optional destination arguments to clone and new_packet?\n```\n-- Create a new empty packet.\nfunction new_packet (ptr)\n   local p = ffi.cast(packet_ptr_t, ptr or memory.dma_alloc(packet_size))\n   p.length = 0\n   return p\nend\n-- Create an exact copy of a packet.\nfunction clone (p, p2)\n   if not p2 then p2 = allocate() end\n   ffi.copy(p2, p, p.length)\n   p2.length = p.length\n   return p2\nend\n``\n. Closing as per #998.\n. We decided to merge this with some refactoring, and additional bug fixes (RFC4303 never gets old):\n- [Hydra report](https://hydra.snabb.co/build/599710/download/2/report.html#density-plot) (resync-reorgis what was merged)\n- [Diff of resync-reorg](https://github.com/snabbco/snabb/compare/f140d1c...41180a8)\n. SnabbBot reported a spurious [failure](https://gist.github.com/SnabbBot/33a9ca4b19071f7b0da0278825f5a1e6) in the Snabb NFV\u00a0tests, rerunning now.\n. I would prefer to merge this once the replacement lands upstream. Might not make a difference, but I don't know who is usingapps.vpn`?\n. @dpino seems like this breaks the packetblaster replay selftest somehow.\n. I think this change might need to be replicated in the snabb_doc module in snabblab-nixos, am I right?\nSee: https://github.com/snabblab/snabblab-nixos/blob/master/modules/snabb_doc.nix#L80\n. Do you think the failure in #1016 is related?\nWould be interesting to know why 82eecef fixes things, if it does.\n. I have added a change that sets the zone during config.app and ops.reconfig, and makes the handler in core.main print the current zone. This makes it more obvious where errors come from.\n. Cc @kbara @wingo \n. :+1:\nThe configuration serial is already stored in engine/configs. The parent PID could be stored in parent-pid so that the parent can be accessed using /{parent-pid}/foo.\nThe SnabbBot failure seems spurious (we really have to do something about those).\n. @lukego Hah, sorry for removing alias like a couple weeks ago. ;-)\nI think the YANG effort will require a way to serialize configs as well, so it seems sensible to expect to have config.save and config.load.\nI like the code in this PR very much! :+1:\n. So this is the simplest way I could think of to \u201cserialize\u201d app classes: https://github.com/lukego/snabb/compare/multiprocess-engine...eugeneia:multiprocess-engine (I feel that I proposed this exact syntax before, but to which occasion?)\nIt extends core.config to accept app classes as string identifiers in the form \"<module>[/<var>]\" and updates core.app accordingly. The main caveat is that the programmer has to ensure he only uses string identifiers for classes when using core.worker. We could just stop supporting app classes passed as Lua objects, but that seems harsh.\nAnother issue is that this change conflicts with #1019, which uses class.config to reject invalid app arguments during config.app(!). I chose to parse app arguments in config because I felt it was a good idea to fail fast, but this could be done in engine.configure as well.\nIf I come up with something better I will let it be know. :-)\n. > Originally the only thing that it did was to remove the shm directory on shutdown but I found it an appealing place to implement clean DMA shutdown semantics too.\nCan\u2019t resist telling history: it was about DMA shutdown semantics (#509) 400 PRs earlier, for which I failed to find a solution back then. Then came #930 and now we are full-circle :+1: \n. Wait, now who is the lead on this? @wingo? If so I will un-assign myself. \n. :+1: for the generalized solution!\n@lukego I will happily be upstream.\n. I agree with the decision, its unrealistic to reliably tell how the host is configured, and what\u2019s already running on it. AFAIK there is no way to tell if a core is free? Anyhow, this doesn\u2019t shut any doors, we can add smart middle ware later on.\n. > Thinking about shutdown... I am a fan of the Erlang style where you make cleanup code as minimal as possible and also isolate it from the rest of the code. So I would like shutdown of the parent to reliably stop the children but I would prefer not to do this with explicit stop() calls in the parent because that could fail in scenarios to heap corruption, SIGKILL, OOM, etc. The best mechanism that I see now would be for the child to run something like prctl(PR_SET_DEATHSIG, SIGTERM) to automatically receive a signal from the kernel when the parent terminates, and for this signal to be unhandled so that the kernel shuts down the worker automatically. what do you think?\nThe worker already does this in the relation to the supervisor:\nhttps://github.com/snabbco/snabb/blob/master/src/core/main.lua#L176\n\nAnother variation would be for the main process to copy the snabb binary that it starts from into the shm group folder /var/run/snabb/$pid/group/snabb where it is accessible to every process in the group.\n\nI actually like that approach, sure its a bit of complicated code, but it solves all the shmem API problem, no?\n. @lukego What would be the acceptable cost (in %) of adding vhost_user statistics?\n```\nmodel.tables(aov(score ~ snabb, read.csv(\"https://hydra.snabb.co/job/max/counters-experiments/benchmark-csv/latest/download-by-type/file/CSV\")), type=\"means\")\nTables of means\nGrand mean\n7.864391 \nsnabb \nsnabb\n          master vhost_user-stats \n           8.038            7.691\n```\nThis seems to indicate that the current vhost_user-stats branch costs us ~5% total performance, which seems like too much.\n. I have done some ad-hoc testing using lib.pmu and so far it looks like vhost_user-stats is using ~3% more instructions and ~1% more cycles in total (~2% more instructions per cycle). This correlates well with the performance impact we are seeing, I guess? (edit: scratch that nonsense)\nHere is what IR is added for each counter.add:\n```\n\n0329 >  p32 HREFK  0132  \"shm\" @3\n0330 >  tab HLOAD  0329\n0331 >  p32 UREFC  net_device.lua:  #0\n0332 >  tab ULOAD  0331\n0333    int FLOAD  0332  tab.hmask\n0334 >  int EQ     0333  +15 \n0335    p32 FLOAD  0332  tab.node\n0336 >  p32 HREFK  0335  \"add\" @13\n0337 >  fun HLOAD  0336\n0338    int FLOAD  0330  tab.hmask\n0339 >  int EQ     0338  +7\n0340    p32 FLOAD  0330  tab.node\n0341 >  p32 HREFK  0340  \"txbytes\" @6\n0342 >  cdt HLOAD  0341\n0343 >  fun EQ     0337  counter.lua:\n0344 >  nil PROF \n0345    u16 FLOAD  0342  cdata.ctypeid\n0346 >  int EQ     0345  +1281\n0347    p64 ADD    0342  +8\n0348    u64 XLOAD  0347\n0350    u64 ADD    0348  0311\n0352    u64 XSTORE 0347  0350\n0353 >  nil PROF \n0354 >  p32 HREFK  0340  \"txpackets\" @4\n0355 >  cdt HLOAD  0354\n0356 >  nil PROF \n0357    u16 FLOAD  0355  cdata.ctypeid\n0358 >  int EQ     0357  +1281\n0359    p64 ADD    0355  +8\n0360    u64 XLOAD  0359\n0362    u64 ADD    0360  +1\n0364    u64 XSTORE 0359  0362\n```\n\nAny hints on how I can find out which region of mcode belongs to a IR region?\n. This gist has both dumps (master vs vhost_stats). Wasn't aware of traceprof, will give it a try. While the dumps where generated with the profiler enabled, the PMU measurements were collected without the profiler on.\n. Traceprof confirmed that we are looking at the right trace:\n```\nwith stats\ntraceprof report (recorded 36737/36737 samples):\n 26% TRACE  81:LOOP          ->loop    virtq_device.lua:57\n 19% TRACE  40      (21/4)   ->21      link.lua:80\n  7% TRACE  21               ->loop    flooding.lua:29\n  4% TRACE  23:LOOP          ->loop    synth.lua:36\n  3% TRACE  89:LOOP          ->loop    virtq_device.lua:57\n  2% TRACE  21:LOOP          ->loop    flooding.lua:29\n  1% TRACE  42      (29/0)   ->35      app.lua:329\n  1% TRACE  88:LOOP          ->loop    basic_apps.lua:80\n  1% TRACE  38      (23/15)  ->28      synth.lua:35\nmaster\ntraceprof report (recorded 36275/36275 samples):\n 20% TRACE  35      (18/4)   ->18      link.lua:80\n 18% TRACE  75:LOOP          ->loop    virtq_device.lua:57\n  7% TRACE  18               ->loop    flooding.lua:29\n  4% TRACE  17:LOOP          ->loop    synth.lua:36\n  3% TRACE  85:LOOP          ->loop    virtq_device.lua:57\n  2% TRACE  18:LOOP          ->loop    flooding.lua:29\n  1% TRACE  83:LOOP          ->loop    basic_apps.lua:80\n  1% TRACE  32      (17/15)  ->23      synth.lua:35\n```\nBut the IR I referenced is indeed outside of the loop. (Lines below -- LOOP -- are inside the loop, right?)\n. Ok, so the difference in the LOOP IR is really just:\n0631    u64 XLOAD  0383                                       \n0632    u64 ADD    0631  +1\n0633    u64 XSTORE 0383  0632\n0634    u64 XLOAD  0393\n0635    u64 ADD    0634  0616\n0636    u64 XSTORE 0393  0635\nWhich seems very efficient for two counters (number of packets and size, I removed the mcast/bcast for more isolation). So does this point to a cache issue of sorts?\nLooking at the experiment I kicked off yesterday, using meta-methods for counters, the results seem to confirm that the problem is not with the code generated.\n. Good tip using +r, I think I found the machine code, which is much longer than the IR (lines not matching are also only in the dump with statistics). I can\u2019t really tell what the remaining instructions do, but this might support the \u201crunning out of registers\u201d theory?\nIR                                     MCODE\n                                       0bc9da82  mov r8d, [rsp+0x10]\n0631 r14      u64 XLOAD  0383          0bc9da87  mov r14, [rdx+0x8]\n0632 r14      u64 ADD    0631  +1      0bc9da8b  add r14, +0x01    \n0633          u64 XSTORE 0383  0632    0bc9da8f  mov [rdx+0x8], r14\n0634 r14      u64 XLOAD  0393          0bc9da93  mov r14, [rcx+0x8]\n0635 r14      u64 ADD    0634  0616    0bc9da97  add r14, r10      \n0636          u64 XSTORE 0393  0635    0bc9da9a  mov [rcx+0x8], r14\n                                       0bc9da9e  mov r14d, 0x2e          \n                                       0bc9daa4  cmp r14d, r10d          \n                                       0bc9daa7  cmovl r14d, r10d        \n                                       0bc9daab  xor r10d, r10d          \n                                       0bc9daae  add r14d, +0x04         \n                                       0bc9dab2  jo 0x0bc90058  ->50      \n                                       0bc9dab8  add r14d, +0x05         \n                                       0bc9dabc  jo 0x0bc90058  ->50      \n                                       0bc9dac2  xorps xmm6, xmm6        \n                                       0bc9dac5  cvtsi2sd xmm6, r14d     \n                                       0bc9daca  mulsd xmm6, [0x40047880]\n                                       0bc9dad3  cvttsd2si r14, xmm6     \n                                       0bc9dad8  test r14, r14           \n                                       0bc9dadb  jns 0x0bc9daeb          \n                                       0bc9dadd  addsd xmm6, [0x4178d7b8]\n                                       0bc9dae6  cvttsd2si r14, xmm6     \n                                       0bc9daeb  add r14, [r11+0x8]      \n                                       0bc9daef  mov [r11+0x8], r14      \n                                       0bc9daf3  mov [r13+0x2800], r10w  \n                                       0bc9dafb  mov r14, [r9+0x8]       \n                                       0bc9daff  mov [r9+r14*8+0x18], r13\n                                       0bc9db04  add r14, +0x01          \n                                       0bc9db08  mov [r9+0x8], r14\nI got the PMU measurements using this ad-hoc patch and running\nDOCKERFLAGS=-t SNABB_PCI_INTEL0=soft SNABB_PCI_INTEL1=soft sudo -E lock scripts/dock.sh \"NFV_PMU=yes program/snabbnfv/dpdk_bench.sh\"\n. At least the IR/mcode/traceprof dumps are always from a single run. By the way, I was seeing a huge slowdown when using PMU, is that expected? Could also be related to numactl pinning to a specific core.\nI agree that we need a test case for a mix of multicast/broadcast/unicast packets, the reason why I threw out the \u201cbranch-free patch\u201d is that I couldn\u2019t measure its positive impact.\nWill ponder more on the suggestions in this thread and report back. :-)\n. > Unless perhaps you have pinned several processes to the same core?\nYep that was it... :-)\nI agree with the rationale for avoiding the branches until we are at least able to test the branches... good thinking!\n. > On the one hand it would be quite a task to rewrite the whole virtio module this way e.g. one loop to collect the descriptors, one loop to copy the payloads, one loop to calculate the offloaded checksums, etc. This could turn out to be a wasted effort if the coding style did not actually help. On the other hand it may be straightforward to factor this new counter functionality as an isolated loop.\nFrustratingly, my first couple attempts at this seem to make matters worse: https://hydra.snabb.co/build/818389/download/2/report.html#split-by-benchmark\n. @dpino @kbara What\u2019s the status on this PR in light of #1024 ? I am with @kbara that using a signed type is unnecessary, but the test case introduced here looks to be still valid?\n. I can cherry pick the test from this PR; no problem. Merging the test then.\n. Geat catch! :+1: \n. I agree that ntohl/htonl should be plain bit reverses or no-ops. \n. See further discussion in #1030 . :+1: Cc @lukego \n. I am not sure how to read the results. I started a new run for 51c135f51c19dad9295b3e100048ab53e8be8144 and get this:\n\n\n```\n\nsummary(aov(score ~ snabb, read.csv(\"~/tmp/bench.csv\")))\n               Df Sum Sq Mean Sq F value   Pr(>F)  \nsnabb           1     33   33.07   44.96 2.08e-11 ***\nResiduals   13998  10296    0.74                     \n\n\nSignif. codes:  0 \u2018\u2019 0.001 \u2018\u2019 0.01 \u2018\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n2800 observations deleted due to missingness\n```\n```\n\nmodel.tables(aov(score ~ snabb, read.csv(\"~/tmp/bench.csv\")), type=\"means\")\nTables of means\nGrand mean\n\n6.501847 \nsnabb \nsnabb\nlink-name-regression               master \n               6.550                6.453 \n```\n. Looks like the issue is not with absolute performance but the distribution of results. So where does the variance come from? :O\n. Pollution of test results was also my first suspect. Here is the evaluation the with the dummies added: https://hydra.snabb.co/eval/4011\nLet\u2019s see what that gives.\n. The regression is in fact real: https://hydra.snabb.co/build/689209/download/2/report.html\n. id is the identifier for a given test run. This jobset tested two benchmarks (basic l2fwd with 64 and 265 byte packets), on four different versions, with 100 iterations. Each benchmark/version combination is run 100 times, hence the id. I don\u2019t think the id axis in meaningful for the box plot. As to the choice of axis, I don\u2019t know.\n. Here is a report with the basic benchmark included:https://hydra.snabb.co/build/737970/download/2/report.html\n. I split up the changes into smaller chunks like so, note that every commit follows the previous one, e.g. they are linear (bottom to top).\n07d8d2b lwaftr\n3cde879 lisper\na41c927 standardize stats counters.\n4af450e standardize device ports.\n73f4eff standard link counter names\neb9d141 Merge PR #1028 (v2016.10 release) into master\nHere are the results (slightly hard to read because they are so crowded): https://hydra.snabb.co/build/767819/download/2/report.html\n. It indeed looks to me as if there is some hash collision thing going on, or at least, I find it peculiar that link-counter-names seems to be the only branch not affecting performance (its a metatype). Maybe this tells us something about regular LuaJIT tables that we have so far ignored, and points us towards metatypes?\nThen there is the stats-counter-names sweet-spot which seems to improve things over master. This commit mostly entails longer/different keys for statistics counters.\nport-names drops the ball, OK, maybe the port keys are now too regular and collide somewhere. We had a mix of rx/tx input/output, now its lots of input/output.\nlisper and lwaftr only change the respective programs, I am inclined to rule them out as factors. Here is a report without them:\nhttps://hydra.snabb.co/build/764551/download/2/report.html\n. The issue with ruling out lisper and lwaftr is that linearly speaking, without them this branch is indicated as a significant net plus in l2fwd. The big dip in port-names is gone after amending stats-counter-names, and reintroduced by the seemingly unrelated changes.\n. @wingo thanks for that insight/theory! I will retest this together with #1066 to check.\n. Great news! @wingo\u2019s theory seems to manifest: https://hydra.snabb.co/build/858603/download/2/report.html#density-plot\n. I have pushed a commit that fixes some formatting bugs introduced by 9dc52b3.\n. Slightly off-topic...\n\nwe are the only users of this code\n\nThere have been two instances of students asking for help with virtio_net on snabb-devel (latest instance: https://groups.google.com/forum/#!topic/snabb-devel/8eLfuiKksjY ). Sadly, I couldn\u2019t help them because I have never used virtio_net and neither have a way to test it. Since there seems to heightened interest in this, I think it would be very valuable to make virtio_net a first class Snabb citizen like vhost_user, and integration with the l2fwd benchmark would go a long way!\n. LGTM! lib.numa should probably have a README though.\n. I will be my own upstream on this minor change, if that\u2019s OK.. I believe having IOControl is a good conservative choice. From my experience tinkering with the software emulation of the IO layer, \u201cincrementally declarative\u201d IO instance management gets complicated even with no hardware involved. I.e. this PR would be much simpler if there was IOControl. \n. Superseded by #1068.\n. LGTM!\n@lukego Seeing you are already onto #1039 I assigned this to you, OK?\n. There is a new \u201cnh_fwd\u201d  app that seems to be very lwaftr centric, or at least the README is vague to me. Why does it not belong into apps/lwaftr?\n. This landed upstream and can be closed right? I am gonna bet on that. ;-). Closing because the issue is resolved.\n. Good catch. I am pretty sure that the SnabbBot performance regression indicated here is spurious, so I will merge it.\n. Related: https://github.com/snabbco/snabb/issues/1050\n. @lukego I edited that screenshot to show my rationale for this layout:\n\nThe white space is going to be somewhere, dividing it is wasting it. Every other way to deal with this is worse. Mic drop ;-)\n. This seems to trigger a bug visible in apps.vlan. Some edge case related to the shallow copying of functions?\n. @wingo @plajjan pointed out that it depends on the kind of change, quote:\n\nFYI, YANG is very strict when it comes to making modifications between revisions so for example you can't rename a leaf if you are to follow the rules. We might want to discuss how to deal with versioning (simple way is to put version in the name, like my-model-v1.yang).\n\nBasically the versioned name makes sure we can \u201creuse\u201d the \u201csnabb-nfvconfig\u201d name even if there are fundamental changes at the price of minimal ugliness.\n. I appended #1051 because I thinks its obviously not positive.\n. Good catch! I don\u2019t think its strictly necessary, I considered this more of a style issue (avoid units in leaf names).\nI see multiple options to avoid the silent breakage:\n- make nfvconfig accept the old keys as a deprecated fall back\n- back out of the change\n- \u2026?\n. @wingo @dpino Can I get you guys to take a peek at this?\n. @xray7224 I am curious, is there a specific reason why you did not reorganize the <pid> directories into by-pid/ after all? \n. @wingo I don\u2019t think we can or should support a use case where an out-of-tree snabb top is used.\n@xray7224 I don\u2019t know what could possibly break so its easy for me to advocate a clean directory structure, but from my experience these hacks will just bite us later. I also think option 1 is reasonable.\nAlso :+1: to what @lukego said. This is a long standing issue, and I am not innocent in this regard. :-)\n. @wingo That\u2019s what I was referring to, too. Since top is built into the snabb executable, a matching version should always be available and used I think.\n. @lukego In that case, how crazy would using a hard link be? I.e. /var/run/snabb/123/snabb.exe\n. @xray7224 Sorry for the long silence. I think this change should also update snabb top so that it can attach to named instances (see https://github.com/snabbco/snabb/pull/1058#issuecomment-258377887). I would like to merge it once that issue is addressed.\nThe remaining issues can be separately:\n\nimplement snabb top so that it always uses the snabb executable of the instance it attaches to\npossibly clean up directory structure. Ehh... I had forgotten that this is upstreamed via igalia/snabb, nevermind my last comment.. This did land upstream and can be closed, no? \u2753 \ud83d\udd75\ufe0f\u200d. Probably a spurious failure, davos is suffering from those... I have reset the cache for this PR so it should be reevaluated soon.\n. @wingo See also ./snabb snabbmark nfvconfig for a benchmarking configuration transitions:\n\n```\n  snabbmark nfvconfig    \n    Benchmark loading  and transitioning from \n    to   times.\nUses SNABB_PCI0.\n\n```\n. @wingo the reference I posted is not related to the regression, just an existing micro-benchmark for configuration transitions that I thought you might find useful for this PR.\nYou can run the snabbnfv-iperf-1500-tunnel+crypto benchmark on lugano-1/3 using:\nSNABB_IPERF_BENCH_CONF=program/snabbnfv/test_fixtures/nfvconfig/test_functions/crypto-tunnel.ports \\\nsudo -E lock program/snabbnfv/selftest.sh bench\n. @lukego You\u2019re right, what a complicated mess. I simplified and came up with this imho sane approach:\n- there is a single IO macro app that configures an arbitrary number of queues\n- its configuration looks like this:\n{type=<type>, device=<device>, queues={<name>=<qconf>, ...}\n- <name> will be the name of the queue app\n- each driver has to implement this API in any way it wants (lib.hardware.pci forwards to the correct device driver for <device>, for Tap <device> would be the interface name\n- we can implement any command line syntax we want, it just has to compile to the above API. E.g. compile_io_arg(<string>, <name_pattern>) \u21d2 <arg>\n. The GitHub diff view is broken right now...\n. Actually its not, my mistake. I reverted the leftovers and now its looking good.\n. @wingo In concept yes, the idea is to allow the configure method to call config.app and config.link on its argument. Of course, bad configure callbacks could do bad things.\n. @lukego I refactored all the framework bits to be centralized in apps.io.common, I agree that its less intrusive that way.\n. @lukego I ignored this previously, but I have to ask: why should Leader and Follower be apps instead of programs? That seems to break every control/data plane boundary we have, and seems very complicated for now apparent (to me) reason. Is an instance supposed to be/run multiple Followers?\n. I should stop using data/control plane as terms, I think these are overloaded.\nNever before did an app mutate the app graph. There is a \u201cmanagement\u201d API (core.engine) that changes the app graph and invokes the \u201cdata engine\u201d. So I am thinking the worker that consumes configuration changes (follower) from the main process (leader). Any follower will have exactly one leader? Then the follower process should run the follower program that manages an app graph based on its input from the leader program.\n. @wingo Yes core.app (it is exposed as the global engine :-)). By program I mean the top level code executed by the worker, e.g. whatever the Lua expression passed to it runs.\nI imagined the leader to spawn and set up communication with N follower workers (running the follower \u201cprogram\u201d at their top level), which do something conceptually similar to snabbnfv, along the lines of:\nwhile true do\n   local changes = poll_config_changes(leader_channel)\n   if incremental_actions_possible(changes) then\n      engine.configure(next_configuration(changes))\n   end\n   -- The engine doesn\u2019t have to run for a duration, could also be a done callback\n   engine.main({duration=1})\nend\n. I would recommend to avoid implementing leader/follower as apps, if possible at all. While @lukego\u2019s suggestion is cute, this basically means that the follower app is issuing calls to engine.configure, possibly re-configuring (or, you know, stopping) itself, seems overly exiting to me. (On the other hand this gives rise to hot loading new versions as Lua files... :-O)\nI don\u2019t see an immediate difference between running the code inside or outside of the breathe loop, unless the reconfiguration needs to happen in between a single breath. The polling could happen in the done callback of engine.main, and schedule incremental updates on demand.\n\nI also want to avoid O(N) operations in the workers. I especially don't want to have to compare binding tables. For that reason I think there is an advantage for pushing \"what changes to apply\" logic out of the worker process.\n\nNo argument here, but I don\u2019t think leader/follower need to be implemented as apps to accomplish that.\n. My concern is with apps handling reconfiguration of apps specifically, this recursive behavior seems overly complicated to me. Having the top level code implement follower behavior seems much more intuitive, and simple to me.\nE.g., when follower:pull calls apply_config_actions, how will that affect the rest of the breath? IIRC the breathe order will be changed in the middle of a breath? This wouldn\u2019t be a concern if follower was not implemented as an app and treated engine as a black box.\n. LGTM!. I hope its OK I label this \u201cmerged\u201d since its already merged in the respective upstream path.. Oops, I think I had this mixed up in my mind with the fd double close fix. I see no reason not to merge this into max-next, so that\u2019s what I will do.. Good catch!. The snabbnfv selftest should probably bail out if it can\u2019t find the QEMU images it depends on, which I assue is the case here?. @kbara I can\u2019t really reproduce this. It seems that test_env really finds an image in your case, but the image won\u2019t come up. This is how it looks for me:\n-bash-4.3$ env | grep SNABB\nSNABB_PCI0=0000:01:00.0\nSNABB_TEST_FIXTURES=/var/lib/snabb-test-fixtures/\nSNABB_PCI_INTEL0=0000:01:00.0\nSNABB_PCI_INTEL1=0000:01:00.1\nSNABB_KERNEL_PARAMS=init=/nix/var/nix/profiles/system/init\n-bash-4.3$ sudo SNABB_PCI0=0000:01:00.0 program/snabbnfv/selftest.sh \nDefaulting to SNABB_TELNET0=5000\nDefaulting to SNABB_TELNET1=5001\nDefaulting to SNABB_IPERF_BENCH_CONF=program/snabbnfv/test_fixtures/nfvconfig/test_functions/same_vlan.ports\nUSING program/snabbnfv/test_fixtures/nfvconfig/test_functions/other_vlan.ports\nDefaulting to MAC=52:54:00:00:00:\nDefaulting to IP=fe80::5054:ff:fe00:\nDefaulting to GUEST_MEM=512\nDefaulting to HUGETLBFS=/hugetlbfs\nDefaulting to QUEUES=1\nDefaulting to QEMU=/run/current-system/sw/bin/qemu-system-x86_64\nCouldn't find QEMU image: /root/.test_env/qemu0.img\n-bash-4.3$ echo $?\n43\nMaybe you have stale test assets in ~/.test_env, or SNABB_TEST_FIXTURES?. Cc @lukego . Cc @alexandergall, who I believe might have some use for this. Initially, he complained about my \u201cwe only need 64bit counters stance.\u201d :-). @lukego SnabbBot reports a test failure: https://gist.github.com/SnabbBot/243a52b310a2545d2f65d1616df36c49#file-log-L33. Hi @vmaffione, \n\n\nYes, currently Snabb is a single threaded execution environment (one Snab instance per core). We have plans for making use of multiple cores. One approach we evaluated is to use existing NIC RSS features to \u201cshard\u201d incoming packets across multiple Snabb engines. This design is simple and efficient, but not all types of applications map well to this model. An alternate strategy we have experimented with is to provide \u201ccross-engine links\u201d, i.e. links between apps running in separate engines (Snabb instances). This design is very flexible, but is tricky to implement, as the overhead of cross-engine links must be very small for this to be feasible.\n\n\nThe Snabb engine schedules the apps to run. It is configurable how many packets apps will process in one batch, and if the engine should employ a pacing strategy to save cycles at the cost of some initial latency when idle. The apps themselves are \u201cpassive\u201d, implemented as a set of callbacks for the engine to execute when sensible (i.e. push is called when there are new packets on any input link of the app). An app can\u2019t (read \u201cshould not\u201d) block, be it for semaphores or I/O. Generally speaking, the OS level synchronization constructs as mentioned by you are way too expensive to qualify for use in Snabb apps.. @takikawa the selftest fails on SnabbBot, presumably due to missing dependencies. Is there a list of those somewhere so I can add them to snabb_bot.nix?. Exciting release! :-). Ideally I would add the missing dependency to snabb_bot.nix before doing the release, but I don\u2019t immediately see whats missing, so I might postpone the fix until after the release to forgo further stalling from my side?. Heh, actually I can\u2019t push the release onto master because SnabbBot did not sign off:\n\n\n% git push snabbco master\nCounting objects: 1, done.\nWriting objects: 100% (1/1), 883 bytes | 0 bytes/s, done.\nTotal 1 (delta 0), reused 0 (delta 0)\nremote: error: GH006: Protected branch update failed for refs/heads/master.        \nremote: error: Required status check \"davos-eugeneia/snabb-nfv-test-vanilla\" is failing.        \nTo git@github.com:snabbco/snabb.git\n ! [remote rejected] master -> master (protected branch hook declined)\nerror: failed to push some refs to 'git@github.com:snabbco/snabb.git'. Regarding https://github.com/snabbco/snabb/pull/1122#issuecomment-293210223 (sorry, saw this just now), the SnabbBot module on davos is this one.\nLooks like it could be indeed SnabbBot trying to run git submodule update (from back when we were using submodules instead of subtrees?). I think I remember @dpino reporting a similar problem, but can not find the issue right now.. I updated snabb bot on davos, it no longer attempts to fetch submodules and that appears to fix things.\nThe new test failure seems legit though,\nsrc/testlog/program.snabbnfv.nfvconfig:\n../lib/pflua/src/pf/types.lua:44: attempt to redefine 'bpf_insn'\nWonder if that is a clash with the new ljsyscall?. @wingo No it died :-/ Still working on sorting out davos.. My guess is that the intel_mp selftest is somehow affected by the changes to core, given that it seems reproducible. Sorry for the SnabbBot delays, seems like we hit some new docker related bugs since I redeployed davos. Quite the pandora\u2019s box I opened there\u2026. A temporary file is created here via mktemp but never deleted:\nhttps://github.com/snabbco/snabb/blob/42e29409cd3cdd6159d68865a70b87f158b5d3f6/src/program/wall/tests/filter-pcaps.test#L16. @petebristow I tested this a bit (came in handy just at the right time, thanks!) and I am experiencing segmentation faults due to some memory corruption inside lpm4_dxr.lua.\nWhen I use lpm4_248 on the same workload I do not experience these crashes. Before your go on a bug hunt, this may be due to me cherry picking this PR onto my branch, and might not appear on next.. The permissions for /var/run/snabb (sticky, world writable\u2014like /tmp) were chosen with a multi-user scenario in mind. I.e. given /var/run/snabb exists a non-superuser could start Snabb (e.g. create a pid sub-directory in /var/run/snabb). Since we agreed on a root-only user story, I think the world-writable sticky magic is obsolete.\n:+1: . @lukego ready to release, but since davos seems to be down and thus failed to produce SnabbBot results, I am unable to push to master:\nremote: error: GH006: Protected branch update failed for refs/heads/master.        \nremote: error: Required status check \"davos-eugeneia/snabb-nfv-test-vanilla\" is expected.. The previous results were actually bogus, the engine pacing just got confused by the changes and the processes would mostly idle, hence busywait affects the results a lot:\n$ sudo ./snabb snsh linkshare_test.lua\nlink report:\n         127,394,265 sent on group/test.link (loss rate: 0%)\nworker source: pid=10443 alive=true status=nil\n12.7394313 Mpps. Today I experimented with using the ring buffer design from #1186 for core.link. The good is that this change (21410c9) doesn\u2019t affect snabbmark basic1 results much, on the other hand it doesn\u2019t improve the inter-process performance as much as I hoped, for reasons yet unknown to me.\n$ sudo ./snabb snsh linkshare_test.lua\nlink report:\n         154,885,679 sent on group/test.link (loss rate: 0%)\nworker source: pid=11401 alive=true status=nil\n15.4885729 Mpps. By the way, I like this design more and more (as opposed to the dedicated inter-process links from #1186). In theory it should be even faster because there are less links in total, and the packet balancing is done in bulk instead of for every transmitted packet individually.\nSo actually I expect(ed) performance >= #1186. Which means either I made some errors while copying the ring buffer design or there is something else going on, both can probably be spotted using PMU analsysis?. Closing as superseded by #1288.. Yep, I guess this and #1185 can be closed now i favor of #1228.. @wingo Could I possibly test/merge this as-is? Support in apps/config (which I believe is in flux?) could be added later on. I would add a note to https://github.com/snabbco/snabb/blob/master/src/apps/config/README.md explaining that post_link is not yet supported?\nRationale: this seems already useful for downstream projects that don\u2019t use apps/config. > Initialisation of the shm frame needs to be done in the constructor\nmethod to avoid being overwritten by the frame created in core.app.\nCould you elaborate on that? I feel like I should understand this, but I don\u2019t. core.app calls shm.create_frame only once after calling the new method of an app, so how does it get overwritten?\n-bash-4.3$ ./snabb snsh -i\nSnabb> Foo = { shm = \"foo\" }\nSnabb> function Foo:new () return setmetatable({}, {__index=Foo}) end\nSnabb> f = Foo:new()\nSnabb> =Foo.shm\nfoo\nSnabb> =f.shm\nfoo\nSnabb> f.shm = \"bar\"\nSnabb> =Foo.shm\nfoo\nSnabb> =f.shm\nbar. Is this PR still relevant given #1237 was merged? I guess not so I am cautiously closing this PR.. @lukego Right, the problem here is that SnabbBot doesn\u2019t have the environment required for lib/ipsec/selftest.sh, and that\u2019s why it fails. I can think of some solutions:\n\nskip the test when SNABB_IPSEC_DONT_TEST_E2E is set\nadd the required dependencies to the VM images in the Docker containers used by SnabbBot\nmake SnabbBot use Nix instead of Docker to test snabb\n\nI am somewhat leaning towards the first option... burning containers for SnabbBot is somewhat annoying and I don\u2019t see much gain here (the test is trivial to run in the lab or in hydra).. @lukego alright, I went with the easy path and disabled this test for snabb bot (trusty doesn\u2019t support extended sequence numbers, it appears). Ready to be merged!. I am fine with either way! \ud83d\udc4d . I have started a Hydra jobset that compares my base branch (vita), a version that includes attempts to manually inhibit tail calls to C functions (vita-stop-tco), a version with this PR merged (vita-record-blacklisted), and finally a version with both of the latter (vita-stop-tco+record-blacklisted, or \"st+rb\" in the report). Results will be here:\nhttps://hydra.snabb.co/eval/7590. https://hydra.snabb.co/eval/7621#tabs-still-succeed\nI increased the number of iterations a bit, Tukey\u2019s test still doesn\u2019t see any differences. This may be very well because the outliers don\u2019t impact the average result much.\nTo me it looks like record-blacklisted does have fewer \"teeth\" (i.e. dramatically bad traces) in the linegraph: https://hydra.snabb.co/build/2473885/download/2/report.html#line-graph\nIt also \"looks\" like the optimizations cancel each other out... no definitive answer from me here!. I think this can go straight to next. \ud83d\udc4d. Totally agree. I think #1244 is not included in this release, or is it? Maybe, as a last resort I would test to see if it resolves this regression like it did mine. This doesn\u2019t block the release from my point of view though. Ready when you are!. @lukego That sounds really nice! I could talk a bit about random stuff around Vita. :-). I booked in a flat really close to JAM and will be arriving at lunch time. See you guys there. :-). I am actually surpised myself that I merged this... my bad!. Since max-next is free I will be the next hop instead as discussed on Slack with @wingo.. Right, this can\u2019t work right now since IP addresses are converted to numbers before the validators see them\u2026 I didn\u2019t notice this before because I have disabled that behavior on my downstream branch, my bad!\nI guess I can guard the pattern validator on the invariant that the value to be validated is a string.. That being fixed, the next error appears to be a lwaftr test assuming ranges are single intervals:\nhttps://gist.github.com/SnabbBot/47076ad6d12879c3ce4d9ba4bf8ccbd7#file-log-L816\nEdit: Actually they just use a different format.. All good to go!. I am in no particular rush to get this merged, so I guess feel free to pick it up whenever convenient.. Merged the current master. Note that besides the pattern validator this PR also...\n\nadds contextual errors to validators by passing in a parser\n\nfixes a bug where explicit values were not recognized in ranges. Actually, I found a glaring omission in the Regex implementation (duh) over the weekend, so please put merging this on hold until I come up with a fix. :-)\n\n\n[x] Implement Regex quantifiers properly (their backtracking is broken). Ok should be better now! :-). > I need to provide instructions for examining trace and profiler data with Studio.\n\n\nVita comes with a tool snabb trace that analyzes vmprofile data and parses JIT dumps (think of it as a super-light battery-included alternative to Studio on terminals), I will update this tool to work with this branch and maybe upstream it? It would give users roughly the same abilities as the old traceprofile plus some more (e.g. get the byte/machine code of an interesting trace) in cases where Studio isn\u2019t available (e.g. live in production).. > So what about profiling? That isn't continuous and always-on and always writing to disk somewhere?\n@wingo vmprofile just increments counters in a shared memory object. I have already been using it in Vita and can attest to the non-existant impact\u2014on performance that is, it has amazing analytical impact! :-). Closing this one in favour of #1316.. I did this the wrong way, I will merge ipsec onto max-next instead and do a second max-next -> next PR.. @wingo Right, I was under the impression that the changes in https://github.com/snabbco/snabb/pull/1267/commits/a0c9d21c8f5b2bc7acae0fe1fc74f375ad20da24 (Merge branch 'economic-selftests' into max-next) were too trivial to warrant their own PR. Probably the wrong decision in retrospect.\nOn v2017.12 and next (without 2c71c23) I get an error1 when running the lib.numa seltest. Maybe 2c71c23 is the wrong solution, if you agree I can revert that commit.\n```\n\u2192 numactl -H\nNo NUMA available on this system\n\u2192 sudo ./snabb snsh -t lib.numa\nselftest: numa\ncore/main.lua:26: Function not implemented\nStack Traceback\n(1) Lua function 'handler' at file 'core/main.lua:168' (best guess)\n    Local variables:\n     reason = string: \"core/main.lua:26: Function not implemented\"\n     (*temporary) = C function: print\n(2) global C function 'error'\n(3) Lua global 'assert' at file 'core/main.lua:26'\n    Local variables:\n     v = nil\n(4) Lua global 'bind_to_numa_node' at file 'lib/numa.lua:116'\n    Local variables:\n     node = number: 0\n(5) Lua global 'bind_to_cpu' at file 'lib/numa.lua:103'\n    Local variables:\n     cpu = number: 0\n     cpu_and_node = table: 0x419eb8c8  {cpu:0, node:0}\n(6) Lua global 'test_cpu' at file 'lib/numa.lua:134'\n    Local variables:\n     cpu = number: 0\n     node = nil\n(7) Lua field 'selftest' at file 'lib/numa.lua:152'\n    Local variables:\n     (for index) = number: 0\n     (for limit) = number: 1023\n     (for step) = number: 1\n     cpuid = number: 0\n(8) Lua function 'opt' at file 'program/snsh/snsh.lua:31' (best guess)\n    Local variables:\n     arg = string: \"lib.numa\"\n(9) Lua field 'dogetopt' at file 'core/lib.lua:425'\n    Local variables:\n     args = table: 0x4062b8d8  {1:-t, 2:lib.numa}\n     actions = table: 0x40754520  {t:function: 0x40e89798, q:function: 0x40754568, P:function: 0x40626f78 (more...)}\n     opts = string: \"hl:p:t:die:j:P:q:\"\n     long_opts = table: 0x40628810  {jit:j, help:h, program:p, test:t, load:l, package-path:P, debug:d, sigquit:q (more...)}\n     opts = table: 0x419e9460  {1:t}\n     optind = number: 3\n     optarg = table: 0x406288d8  {1:lib.numa}\n     (for generator) = C function: builtin#6\n     (for state) = table: 0x419e9460  {1:t}\n     (for control) = number: 1\n     i = number: 1\n     v = string: \"t\"\n(10) Lua field 'run' at file 'program/snsh/snsh.lua:65'\n    Local variables:\n     parameters = table: 0x4062b8d8  {1:-t, 2:lib.numa}\n     profiling = boolean: false\n     traceprofiling = boolean: false\n     start_repl = boolean: false\n     noop = boolean: true\n     program = nil\n     opt = table: 0x40754520  {t:function: 0x40e89798, q:function: 0x40754568, P:function: 0x40626f78 (more...)}\n(11) Lua function 'main' at file 'core/main.lua:67' (best guess)\n    Local variables:\n     program = string: \"snsh\"\n     args = table: 0x4062b8d8  {1:-t, 2:lib.numa}\n(12) global C function 'xpcall'\n(13) main chunk of file 'core/main.lua' at line 230\n(14)  C function 'require'\n(15) global C function 'pcall'\n(16) main chunk of file 'core/startup.lua' at line 3\n(17) global C function 'require'\n(18) main chunk of [string \"require \"core.startup\"\"] at line 1\n    nil\n```. @wingo No, this appears to be unrelated to #1269 . Is this still an issue? Because SnabbBot seems to rightfully complain, but this seems like an important fix.. I cherry picked https://github.com/Igalia/snabb/pull/1198 onto max-next, closing this PR.. Oh right, I should add some info about the non-NUMA system:\n\u2192 uname -a\nLinux perth 4.4.111workbench-hugetlbfs #1 SMP Thu Jan 18 13:18:28 CET 2018 x86_64 Intel(R) Core(TM) i3-4010U CPU @ 1.70GHz GenuineIntel GNU/Linux\nThe kernel isn\u2019t very special except that I enabled support for hugetlbfs. Admittedly,\n```\nCONFIG_NUMA is not set\n```\nso that might in fact make the difference.. https://github.com/snabbco/snabb/pull/1269#issuecomment-361420456\n^ this does indeed return -1 on my system.\n\u2192 sudo ./snabb snsh -i\nSnabb> ffi = require(\"ffi\")\nSnabb> ffi.cdef(\"int numa_available(void);\")\nSnabb> numa = ffi.load(\"numa\")\nSnabb> =numa.numa_available()\n-1\nWhat about the progressive approach in this PR though? Wouldn\u2019t it be better to just not try to bind to numa nodes on systems where has_numa returns false?. @wingo thoughts?\nI realized there are two possible interpretations of has_numa:\n\n\u201chas NUMA capabilities\u201d (i.e., supports S.set_mempolicy('bind', node))\n\u201chas Non-Uniform-Memory-Architecture\u201d (this is what has_numa currently implements, I think)\n\nI had mistakenly assumed 1 in 090d617. The current state of this PR isn\u2019t really affected by this, but we could add a function supports_numa (with the code from @dpino) that checks for 1  and use that where this PR uses has_numa. This way the code paths would stay uniform for all systems like before, except for systems like mine which presumably don\u2019t have NUMA support compiled into the kernel. It\u2019s more code though\u2026. Any idea what this test failure could be about (scroll way down for the error message)?. @lukego No, not really. Kernels without NUMA support compiled in seem to be rare enough that this has negligible impact.. Really cool! I think the intel_mp test failure is transient as I can\u2019t reproduce it locally (cc @petebristow @asumu).\n@dpino Works for me, except not on the loopback interface. Might be worth fixing, your call! If you give me the go I will merge this to max-next.\n```\n\u2192 sudo ./snabb dnssd lo\nCapturing packets from interface 'lo'\nlib/protocol/ethernet.lua:52: invalid mac address \nStack Traceback\n(1) Lua function 'handler' at file 'core/main.lua:168' (best guess)\n    Local variables:\n     reason = string: \"lib/protocol/ethernet.lua:52: invalid mac address \"\n     (temporary) = C function: print\n(2) global C function 'error'\n(3) Lua method 'pton' at file 'lib/protocol/ethernet.lua:52'\n    Local variables:\n     self = table: 0x40d146b0  {pton:function: 0x40d14978, new:function: 0x40d14be8, src_eq:function: 0x40d15000 (more...)}\n     p = string: \"\"\n     i = number: 0\n     (for generator) = Lua function 'splitter' (defined at line 193 of chunk core/lib.lua)\n     (for state) = string: \"\"\n     (for control) = string: \"\"\n     v = string: \"\"\n(4) Lua method 'build' at file 'lib/protocol/dns/mdns_query.lua:37'\n    Local variables:\n     self = table: 0x40d22748  {src_ipv4:127.0.0.1, src_eth:}\n     queries = table: 0x41bd88e0  {1:_services._dns-sd._udp.local}\n     dgram = table: 0x41bd8920  {_offset:0, _push:table: 0x41bd8948, _recycled:false, _instance:true, _packet:cdata1>: 0x41bd8d28 (more...)}\n     (temporary) = Lua function 'ethernet' (defined at line 36 of chunk lib/protocol/ethernet.lua)\n     (temporary) = table: 0x40d146b0  {pton:function: 0x40d14978, new:function: 0x40d14be8, src_eq:function: 0x40d15000 (more...)}\n     (temporary) = table: 0x41bd8f10  {dst:cdata: 0x41bd8f40}\n(5) Lua local 'method' at file 'program/dnssd/dnssd.lua:82'\n    Local variables:\n     self = table: 0x40d224d8  {input:table: 0x40d22830, zone:program.dnssd.dnssd, output:table: 0x40d22808 (more...)}\n     now = number: 1.51878e+09\n(6) Lua global 'with_restart' at file 'core/app.lua:96'\n    Local variables:\n     app = table: 0x40d224d8  {input:table: 0x40d22830, zone:program.dnssd.dnssd, output:table: 0x40d22808 (more...)}\n     method = Lua function 'DNSSD' (defined at line 75 of chunk program/dnssd/dnssd.lua)\n     status = nil\n     result = nil\n     (*temporary) = boolean: true\n(7) Lua upvalue 'thunk' at file 'core/app.lua:524'\n    Local variables:\n     (for index) = number: 1\n     (for limit) = number: 2\n     (for step) = number: 1\n     i = number: 1\n     app = table: 0x40d224d8  {input:table: 0x40d22830, zone:program.dnssd.dnssd, output:table: 0x40d22808 (more...)}\n(8) Lua local 'breathe' at file 'core/histogram.lua:98'\n    Local variables:\n     start = number: 133731\n(9) Lua field 'main' at file 'core/app.lua:479'\n    Local variables:\n     options = table: 0x41bd7b88  {report:table: 0x41bd7be8}\n     done = nil\n     no_timers = nil\n     breathe = Lua function '(anonymous)' (defined at line 96 of chunk core/histogram.lua)\n(10) Lua field 'run' at file 'program/dnssd/dnssd.lua:167'\n    Local variables:\n     args = table: 0x4125e450  {1:lo}\n     opts = table: 0x40d214c0  {interface:lo}\n     args = table: 0x40d216c0  {}\n     duration = nil\n     c = table: 0x40d21738  {links:table: 0x40d218f8, apps:table: 0x40d21898}\n(11) Lua function 'main' at file 'core/main.lua:67' (best guess)\n    Local variables:\n     program = string: \"dnssd\"\n     args = table: 0x4125e450  {1:lo}\n(12) global C function 'xpcall'\n(13) main chunk of file 'core/main.lua' at line 230\n(14)  C function 'require'\n(15) global C function 'pcall'\n(16) main chunk of file 'core/startup.lua' at line 3\n(17) global C function 'require'\n(18) main chunk of [string \"require \"core.startup\"\"] at line 1\n    nil\n\u2192 sudo ./snabb dnssd wlan0\nCapturing packets from interface 'wlan0'\n{name: _services._dns-sd._udp.local; domain_name: _amzn-wplay._tcp}\n{name: _services._dns-sd._udp.local; domain_name: _amzn-wplay._tcp}\n{name: _services._dns-sd._udp.local; domain_name: _amzn-wplay._tcp}\n{name: _services._dns-sd._udp.local; domain_name: _amzn-wplay._tcp}\n{name: _services._dns-sd._udp.local; domain_name: _amzn-wplay._tcp}\n{name: _services._dns-sd._udp.local; domain_name: _amzn-wplay._tcp}\n{name: _services._dns-sd._udp.local; domain_name: _amzn-wplay._tcp}\n  C-c C-c\n``. Closing this because it was merged via #1285.. Why not? :-). @dpino In case you haven\u2019t already, I recommend playing withlib.pmu` in that snabbmark. Should be an interesting study case for peeking at the inner working of the CPU!. On a AMD Ryzen 5 1600 (turbo off):\ndyser$ sudo taskset -c 3 ./snabb snabbmark checksum\n[pmu: /sbin/modprobe msr]\nsh: /sbin/modprobe: No such file or directory\nNo PMU available: requires /dev/cpu/*/msr (Linux 'msr' module)\nC: Size=44 bytes; MPPS=14 M: 6.60 ns per iteration (result: 24346); 0.15 ns per byte\nASM: Size=44 bytes; MPPS=14 M: 4.39 ns per iteration (result: 24346); 0.10 ns per byte\nC: Size=550 bytes; MPPS=2 M: 56.49 ns per iteration (result: 19817); 0.10 ns per byte\nASM: Size=550 bytes; MPPS=2 M: 23.79 ns per iteration (result: 19817); 0.04 ns per byte\nC: Size=1516 bytes; MPPS=1 M: 140.99 ns per iteration (result: 52607); 0.09 ns per byte\nASM: Size=1516 bytes; MPPS=1 M: 73.39 ns per iteration (result: 52607); 0.05 ns per byte\nLGTM as well!. Merged into max-next.. Note that, currently, shutdown is run by the process that supervises snabb, which doesn\u2019t have access to the app graph.\nThe new test failure is weird btw: https://gist.github.com/SnabbBot/ec0315294ddd326fd1931e00ce17d5f3#file-log-L21\n. Given that it is possible to have the behavior implemented in this PR by explicitly doing\n-- ordered shutdown\nengine.configure(config.new())\nat the end of a Snabb program, I am inclined to pass on this PR. My rationale is that a program might genuinely don\u2019t care about stopping its apps (test scripts, etc), so why prescribe that behavior?\n@alexandergall What you are describing would probably be best served by a cleanup hook in the intel_mp app similar to this one:\nhttps://github.com/snabbco/snabb/blob/master/src/lib/hardware/pci.lua#L195\nhttps://github.com/snabbco/snabb/blob/master/src/core/main.lua#L186. I don\u2019t think it does at this point. There is a list of supported cards here: https://github.com/snabbco/snabb/blob/master/src/lib/hardware/pci.lua#L70. @wingo should be good to go!. There are some lwaftr test failures that appear to be related to the new lib.ptree. Is that something you recognize @wingo ?. @dpino that does not seem to fix it.. \ud83d\udc4d \ud83c\udf86  Looks good to me! Cc @lukego \nUnless there are further changes required to #1277 this could be ready to go I suppose?. Superseded by  #1285 (go via the fixes branch). Closing this because it was merged via #1285.. @alexandergall Just pushed a fix to the bogus free accounting bug you spotted. This little change also seems to positively impact performance on rudimentary testing, so it might be worth another test run.. I have pushed some further commits that\n\ninstall a shutdown handler that detaches crashed Receivers/Transmitters. It should now be possible to recover from a crashed process by restaring it.\n\nadd the glue required to have interlinks show useful info in snabb top -l. @alexandergall 154d63c should fix the issue where supervisors created SHM directories under SNABB_SHM_ROOT.. @wingo Should be good to go. I don\u2019t think there is a perf regression: https://hydra.snabb.co/build/3241437/download/2/report.html. @wingo the name argument to :new() is now reverted (I was able to achieve the same by using :link().) Take me to wingo-next! :-). Test failure seems to be lwaftr related... one more nondeterministic test to squash? :-). FWIW I didn\u2019t mean to sound ungrateful, rather I think these issues are not directly related to interlink, and I have mixed feelings about sneaking in fixes to these in this imho unrelated PR. E.g.:\n\n\nShould shutdown handlers maybe be installed with <coremod>.add_shutdown_handler(myhandler)? That would resolve the perceived layering violation as well but is out of scope of this PR\u00a0imho. Or maybe only core modules should be able to install shutdown handlers, and lib/hardware/pci.lua should really be in core/, then that would require formulating a policy and adapting that would possibly also affect lib/interlink.lua. Not something I want to decide right here, so I went with the \u201cchange as little as possible\u201d route.\n\n\nSimilarily for the struct freelist definition. Is it perfect? Probably not. Do the proposed changes affect this PR in particular? Not really. If I do them in this PR I would claim responsibility, even though I don\u2019t have a particular opinion or insight on the topics at hand.\n\n\nThanks for the review! :tada: :balloon: \nEdit: it\u2019s not lib/memory.lua that has a shutdown handler but lib/hardware/pci.lua. I don\u2019t think the test failure in testlog/program.lwaftr.tests.propbased.selftest.sh on this PR is related to this change. Cc @dpino @wingo \n. What about the intel_mp failure on this PR? Might be related to the recent VMDq changes? Cc @asumu \n. What is the status on this?. Interviewed @wingo about this on Slack. LGTM, merging onto max-next. :-). @lukego might be a good idea to merge this into snabbco/raptorjit?. @wingo seems to be another flaky lwaftr test failure? https://gist.github.com/SnabbBot/ec04788225ade5801630df6389d1506e#file-log-L819. > The \"meta-data\" mechanism stores data in the packet buffer, which is a somewhat questionable practice. Comments on this are welcome.\nVPP seems to have a generic metadata field for such purposes (source: https://www.asumu.xyz/blog/2018/03/22/how-to-develop-vpp-plugins/). Adding a similar multi-purpose field to struct packet would potentially enable optimizations in lib.ipsec as well. Food for thought.. LGTM. Lovely documentation! I took the liberty and included it in genbook.sh. Ugh, wrong target branch.. Oh, GitHub actually lets me change the base branch without opening a new PR, cool!. Merged into max-next \ud83c\udf89. Is this supposed to remove the docs for apps.pcap.pcap?. LGTM\nnote to self: sort out these weird false positive merge conflicts in SnabbBot,. Just for the record: the test failures were due to intel_app sometimes putting multiple test traffic streams into the same RSS bucket where the tests expected packets in multiple packets, am I right?. Hi @xnhp0320 :-)\n88da49f and ae53f4a look good to me, but the other commits seem out of scope of this PR (especially 3c77fb2 is suspicious, is this for a kernel without hugetlbfs support?) I will cherry pick those two commits if that\u2019s ok?. No problem at all. Merged, thank you!. Closing because the requested changes landed in v2018.06. Includes #1324 #1325 and #1330. These all LGTM. As to the changes in lib.ctable and lib.scheduling, these seem innocent enough. :-). FWIW there is an ad-hoc implementation of this functionality in vita so this will be very useful to me! :heart: . Interesting failure reported by SnabbBot in siphash, can\u2019t reproduce locally though: https://gist.github.com/4766cb25b23757d8a0d70473e8b988c6. @takikawa and/or @petebristow maybe? I suppose 2814b7b and 33c0912 should be straight forward and can be cherry picked. OTOH, 9c4e3ee is a bit of a bummer, its seemingly 99% there an I have no idea which needle in which haystack the remaining 1% is. Reproducing my observation on a different i350 might rule out a hardware issue on my side?. > Changing testsend.snabb by adding in a Sample app after the Repeater fixes the imbalance\u2026\n:man_facepalming: I actually remember being suspect of Repeater but brushed off the theory because it worked on the 82599. Thanks!\nTurns out the lack of backpressure in Repeater caused an imbalance in the packets emitted by testsend.snabb. I have workerd around this by using lwaftr\u2019s RateLimitedRepeater and setting the rate to 1\u00a0Gbps for the 1G tests.\nOn my local machine all the tests pass now, but I have noticed test_1g_vmdq_mirror.snabb to be flaky on my hardware at least. Sometimes, nic1 receives no packets. \\o/. I have pushed 703eb9a to mask the flakyness I observe with test_1g_vmdq_mirror.snabb, not really satisfying but all I can think of right now. From my side this PR is ready to be merged.. Eh, and I found (and fixed) another bug (d29763d), and added test cases to cover the faulty behaviour. :-). - Maybe we should move lib.logger out from core/lib to lib/ to avoid core/lib repending on lib/?\n- Could we re-use this token bucket implementation for RateLimiter and lwaftr\u2019s RateLimitedRepeater? . No, I don\u2019t think so. As long as I can merge both onto max-next to get the desired result. :-). Snabb only supports x86_64 at the moment. I believe the issue you ran into is resolved for GC64 which we will start using with the move to RaptorJIT, but RaptorJIT for now only support x86_64 as well. Note that we also use x86_64 assembly directly in Snabb. So as far as I know, making this fly would require:\n\nporting RaptorJIT to ARM64 (this might be relatively easy since most (all?) of the required code is already in LuaJIT\nporting any x86_64 specific parts of Snabb to ARM64\n\nsomebody maintaining the result. @pavel-odintsov the RaptorJIT landing page says it better than I could, but for me personally the key benefits are:\n\n\nfast iteration/experimentation cycle for features that matter to server applications like Snabb\n\nintegration with powerful analysis tooling (Studio)\n\nThe general motto, I believe, is narrowing the scope of RaptorJIT relative to LuaJIT on a specific usecase so we can add features we need without getting into the way of LuaJIT\u2019s other downstreams.. LGTM, merged onto max-next.. @lukego This is good to go except my lib.numa fixes (#1269) are broken, so we should revert \n3120d7c.. ~~@lukego I am afraid I found another regression: I have bisected a SnabbNFV\u00a0selftest failure down to https://github.com/snabbco/snabb/pull/1361/commits/d4c2eca9fbee860a438217832e6661f789aad524 (the merge commit, not the commit to be merged.) So https://github.com/snabbco/snabb/pull/1361/commits/d4c2eca9fbee860a438217832e6661f789aad524 should be reverted from the release until I can figure out what is wrong with it.~~. Actually, never mind my last comment. The test was non-deterministic during bisect so the result is likely bogus. So this boiled down to the NFV\u00a0selftest failing every other time on lugano-1. That is something to look into, but seems to not be affected by changes between v2018.04 and #1361. I am going forward with the release.. I believe there are registers in the NICs we support that govern this behavior. If we agree on an excpected behavior (i.e., not forwarding these, even if in promiscous mode / OR: add VMDq-less non-promiscous mode support) I could take a shot at this in https://github.com/snabbco/snabb/pull/1353 or a follow-up.. (There is no need to close merged PRs, they will auto-close once they end up on master via a release. The merged label is to indicate that its on one of the upstream branches.). @takikawa @dpino Unless you have any objections I would merge this onto max-next.. LGTM!. While trying to integrate this I noticed this would require additional changes to lib.yang.binary, hmm.. Should be good to go? See d354bebf68c for a merge onto #1379.. Looking great!\n\nSnabb's YANG libraries now support uniqueness constraints for YANG data,\n\nI tested this by adding a unit test\n```diff\nmodified   src/lib/yang/data.lua\n@@ -1914,6 +1914,45 @@ function selftest()\n       length_test \"++++++++++++++++++++++\";\n    ]])\n\n-- Test uniqueness restrictions.\nlocal unique_schema = schema.load_schema([[module unique-schema {\nnamespace \"urn:ietf:params:xml:ns:yang:unique-schema\";\nprefix \"test\";\n+\nlist unique_test {\nkey \"testkey\"; unique \"testleaf\";\nleaf testkey { type string; mandatory true; }\nleaf testleaf { type string; mandatory true; }\n}\n}]])\n+\n-- Test unique validation (should fail)\nlocal success, err = pcall(load_config_for_schema, unique_schema,\nmem.open_input_string [[\nunique_test {\ntestkey \"foo\";\ntestleaf \"bar\";\n}\nunique_test {\ntestkey \"foo2\";\ntestleaf \"bar\";\n}\n]])\nassert(success == false)\n+\n-- Test unique validation (should succeed)\nload_config_for_schema(unique_schema,\nmem.open_input_string [[\nunique_test {\ntestkey \"foo\";\ntestleaf \"bar\";\n}\nunique_test {\ntestkey \"foo2\";\ntestleaf \"bar2\";\n}\n\n]])\n+\n    influxdb_printer_tests()\nprint('selfcheck: ok')\n```\n\n\nwhere \u201cTest unique validation (should fail)\u201c does not fail. I had expected it to catch that testleaf is not unique.\nAnother minor nitpick I have is that snabb bot no longer supports dumb terminals, although I suspect I am the only person to care about that. It would be neat if snabb top had a mode (--dumb?) where it dumps the available information without requiring \u201cadvanced\u201d terminal capabilities (also so you can do stuff like sudo ./snabb top > foo.txt which currently crashes the new snabb top).. > FWIW the consistency checker is actually run from...\nI suppose I don\u2019t mind as long as it is invoked from the public API in lib.yang.yang, moving the test case there doesn\u2019t make it pass though. I suspect the uniqueness check is only done for key leaves?. I have some reservations about the new snabb top:\n\nit lacks the often used  (by me) -l feature to list arbitrary SHM objects (I think this could be added back in easily though)\nwhen I tested it with vita it turned out to be somewhat awkward to use because I was constantly struggling to see statistics that fell beyond the truncation threshold\n\nAs it is, this program is not a replacement for snabb top to me. I propose renaming it to snabb monitor (for example) and treating it as an addition instead of a replacement for snabb top. WDYT?\nI have some ideas for the old snabb top to make it more useful alongside this monitoring program.. Superseded by https://github.com/snabbco/snabb/pull/1401. LGTM!. Yes this is good to go.. Merged into max-next.. @alexandergall If you think this is ready I can I merge this onto max-next.. LGTM! (See #1355). Works for me!. Yup! All good.. @wingo I think this impacts lib.ptree\u2019s configuration model: if a worker is stopped due to a configuration update, I believe its packets would leak without this change?. Yep!. Minor nitpick: maybe it should still be a local variable? . @corsix Thanks a TONNE for the detailed review, I will devour all that info and become more x86 wise. <3. I have adopted some of the changes suggested by @corsix, which show quite a substantial improvement in performance. I have omitted the loop rotations, since they didn\u2019t seem to make a measurable difference when testing and make the code harder to read imho.\nI also didn\u2019t do the BMI2 dependent changes since the oldest lab server doesn\u2019t support them (grindelwald has an ivy bridge CPU), also I would have to add the BMI2 instructions to DynASM first. Might be worth to do a CPU feature detection here?\nBefore:\nPMU analysis (numentries=400000, keysize=64)\nbuild: 6.7974 seconds\nlookup64: 576.12 cycles/lookup 215.73 instructions/lookup\nbuild(direct_pointing): 7.3970 seconds\nlookup64(direct_pointing): 441.36 cycles/lookup 150.59 instructions/lookup\nAfter:\nPMU analysis (numentries=400000, keysize=64)\nbuild: 6.9591 seconds\nlookup64: 507.08 cycles/lookup 186.47 instructions/lookup\nbuild(direct_pointing): 7.5430 seconds\nlookup64(direct_pointing): 404.69 cycles/lookup 132.36 instructions/lookup. @corsix  I am having trouble patching in the BMI2 instructions from https://github.com/LuaJIT/LuaJIT/commit/fe651bf6e2b4d02b624be3c289378c08bab2fa9b\nI get errors like\nlib/poptrie_lookup.dasl:98: error: bad char `w' in pattern `F20F38wF7rM' for `shrx':\n      | shrx v, key, offset\nlib/poptrie_lookup.dasl:109: error: bad char `w' in pattern `0F38wF5rM' for `bzhi':\n   | bzhi rax, vec, rcx\nlib/poptrie_lookup.dasl:130: error: bad char `w' in pattern `0F38wF5rM' for `bzhi':\n   | bzhi rax, vec, rcx\n and can\u2019t seem to find where the w modifier is actually implemented?. @corsix On another note, I am currently updating this code to work with keys up to 128 bits (IPv6) and am wondering if you know any neat tricks for extracting 6 bits from a uint8_t[16] at an arbitrary offset in x86 asm? (I suppose what is currently an uint64_t argument will have to become a pointer.). Without BMI2 (this branch):\nPMU analysis (numentries=400000, keysize=64)\nbuild: 6.7322 seconds\nlookup: 55733.43 cycles/lookup 87694.39 instructions/lookup\nlookup64: 504.83 cycles/lookup 186.46 instructions/lookup\nbuild(direct_pointing): 7.5217 seconds\nlookup(direct_pointing): 11152.73 cycles/lookup 25908.96 instructions/lookup\nlookup64(direct_pointing): 416.15 cycles/lookup 132.36 instructions/lookup\nWith BMI2 (https://github.com/eugeneia/snabb/commits/poptrie-bmi2):\nPMU analysis (numentries=400000, keysize=64)\nbuild: 7.3056 seconds\nlookup: 39330.65 cycles/lookup 72852.83 instructions/lookup\nlookup64: 431.63 cycles/lookup 151.47 instructions/lookup\nbuild(direct_pointing): 7.8106 seconds\nlookup(direct_pointing): 12443.39 cycles/lookup 26317.73 instructions/lookup\nlookup64(direct_pointing): 376.19 cycles/lookup 112.10 instructions/lookup. @corsix Thanks for the suggestion! I ended up with something like this for extracting from a 16 byte array, probably not optimal... wdyt?\nmov rxc, offset\nmov rax, [key]\nmov rbx, [key+8]\ncmp rcx, 63\njng >1\nsub rcx, 64\nmov rax, rbx\n1:\nshrd rax, rbx, cl\nand rax, 0x3F. @corsix Neat! Thanks!\n\ncmovnz rax, rbx\n\nTIL there are conditional movs on x86 :-)\nI have added support for 128 (and 32) bit keys (the interface now expects pointers to byte arrays for keys), a well as conditional use of BMI2 instructions (since they have a substantial impact.)\nI have adjusted the micro benchmark so that it does not measure a cold cache. It is not exactly representative of a real workload either, but yields more accurate feedback for testing changes. I have also noticed that for 400,000 entries the benchmark will now OOM, but could confirm that this will be fixed by switching to RaptorJIT.\nI think the results tell us that its pretty fast with warm caches, and cost increases roughly linearly with key size (prefix length).\nPMU analysis (numentries=400000, keysize=32)\nbuild: 6.0666 seconds\nlookup: 5604.90 cycles/lookup 14653.72 instructions/lookup\nlookup32: 81.79 cycles/lookup 118.72 instructions/lookup\nlookup64: 81.77 cycles/lookup 118.72 instructions/lookup\nlookup128: 99.13 cycles/lookup 138.95 instructions/lookup\nbuild(direct_pointing): 6.0043 seconds\nlookup(direct_pointing): 2605.19 cycles/lookup 4805.54 instructions/lookup\nlookup32(direct_pointing): 46.62 cycles/lookup 79.66 instructions/lookup\nlookup64(direct_pointing): 47.05 cycles/lookup 79.65 instructions/lookup\nlookup128(direct_pointing): 51.88 cycles/lookup 84.86 instructions/lookup\nPMU analysis (numentries=400000, keysize=64)\nbuild: 17.5607 seconds\nlookup: 48630.60 cycles/lookup 58459.67 instructions/lookup\nlookup64: 131.99 cycles/lookup 153.46 instructions/lookup\nlookup128: 144.89 cycles/lookup 185.23 instructions/lookup\nbuild(direct_pointing): 17.5885 seconds\nlookup(direct_pointing): 4672.99 cycles/lookup 12343.01 instructions/lookup\nlookup64(direct_pointing): 92.95 cycles/lookup 114.37 instructions/lookup\nlookup128(direct_pointing): 96.92 cycles/lookup 131.13 instructions/lookup. Mhm, seems something slipped in that breaks the build: https://gist.github.com/SnabbBot/0bc074efa5966f84146172c84f71d5fb#file-log-L159. - [x] Need to resolve a conflict with next. I plan to do this by merging next into this branch once next builds.. @lukego Yep!\nFurther, there were some trivial conflicts with next I resolved in 91ffca3. If SnabbBot agrees, this should be good to go.. Oooh neat!!. Neat! I feel like this looks reasonably straight forward, seems within grasp for us to copy the a C function signatures and structures and do this 100% using LuaJIT ffi and ljsyscall, no? :-). Hi @MyraBaba,\n\nWe want to develop a new Layer 4 firewall . Is snabb a good starting point for this ?\n\nI think so, yes. Have you looked at snabb wall?\n\nWe are new to to Lua by the way.\n\nIn my experience Lua is easy to pick up. It is a small language and has much fewer corner cases than say C. :-). This is probably a no brainer. Lua only gives us pcall so we use that. On failure we tag the app with its error and time of death. Once tagged, the app is considered dead.\n. We walk through all apps, restart the ones that have been dead for restart_delay and keep the rest.\n. In order to identify apps from app_array in app_table, we need to know its name.\nI don't like this, suggestions? It works, but maybe this should be done differently.\n. See above.\n. We poll for dead apps once every breath. I thought this would kill performance, but I couldn't measure a drop. So I kept it this way because its siimple and clean.\n. Do not call dead apps.\n. But when we call apps, we call them protected using with_restart.\n. I changed this a little, because I think dead apps should be reported. We do not call report on dead apps though, because they could die again (if they throw during report).\n. Apps can die (throw) during their report routine, so we protect that too. \n. Test it!\n. E.g. move this from neutron2snabb to generalized lib.hexundump and use it in the app directly?\n. .Good cach, fixed.\n. I'd rather throw an error when the requested length is less than yielded by the input string? Really no point in wanting shortening and easy to blow feet.\n. Proposal.\n. Like this?\n. Wait, stop. This is stupid I didn't know there already was a lib.hexundump. Excuse my ignorance. Be right back....\n. Using hexundump. Throw error if part of input would be ignored (e.g. >16 hex digits.).\nNote: hexundump pads from the right, as opposed to what we talked about. Doesn't matter though right?\n. Oh, yes. Good catch! Will have to see if I repeated that mistake.\n. Prepend local.\n. When I went over this I realized how bad this is (destructively modifying the conf, which might be shared if it was passed as a table). So I sneaked in a fixup here, see lines 127 and 148.\n. apply_config_actions now messes with core.timer's timer table. Spaghetti? :-1: \n. Changes in timer logic.\n. timer.new() takes an app.\n. Example of a loose timer.\n. Example of an owned timer.\n. I can't do half a job :( I think I can come up with something simple, but arrays will probably be printed like:\n{\n  1,\n  2,\n  3,\n}\n. #313 This adds a simple pretty printing algorithm.\n. I changed this because TimerTable:activate needs some time even before the engine runs.\n. This describes the following portion:\n\nIf used inside a rule, a packet must belong to an\nexisting connection in addition to any other condition in the rule to pass.\n\nThe \"out of rule\" behavior is described in the global state_check option (even further above).\n. I guess the code could complain at that point instead of defaulting, a PF without rules is probably never what the user wants and mostly a mistake.\n. @lukego I could make this more strict, but since we can not reliably check for the correct type of the rules table anyways... We say it must be a list, which we might want to call \"array\" because that's the closest thing PIL describes. We can't AFAIK easily check that rules is indeed an array though.\nI think the current code is OK: The manual says do it that way and the code handles user errors on a range between forgiving (default to empty rule array) to undefined (I don't know what happens when the table has e.g. string keys, or non-table values, it might ignore those keys or throw up. I don't think that in this specific case there is imminent danger for an API user to rely on undefined behavior).\n. Yes probably, I only learned about the existence of traps from bench_env, excuse my ignorance. I will open a PR to fix this.\n. This function was intentionally removed in a previous PR #382 . It shouldn't be reintroduced here. AFAIK its not used anywhere.\n. > Does this change mean that we now include a PacketFilter app in the app network, even if no rules are defined?\nYes. As per Neutron/SecurityGroups specification:\n\nWhen there is no rule defined, all traffic are dropped.\n. > The packetfilter setting should be in profile rather than vif_details.\n\n@lukego This line looks a bit fishy too now that I think about it. We never use the gbps field in nfvconfig... (and some sort of tx_police_gbps is missing too).\n. @n-nikolaev No rules or no filter configs? E.g. the first case (an empty filter config) would drop all packets.\n. Ok I am pretty sure this branch will break this as of now. E.g. if we look at the code referenced above, even in case of secbindings[port.id] being false an empty filter config will be returned. I assume that --no-security-groups leads exactly to this case, so the config should only be returned when secbindings[port.id] is a true value, e.g. the return conditional should be moved up into if secbindings[port.id] then?\nE.g.:\ndiff --git a/src/program/snabbnfv/neutron2snabb/neutron2snabb.lua b/src/program/snabbnfv/neutron2snabb/neutron2snabb.lua\nindex 393045f..7d8f1c5 100644\n--- a/src/program/snabbnfv/neutron2snabb/neutron2snabb.lua\n+++ b/src/program/snabbnfv/neutron2snabb/neutron2snabb.lua\n@@ -80,28 +80,28 @@ end\n function filter (port, secbindings, secrules, direction, type)\n    local rules = {}\n    direction = direction:lower()\n    if secbindings[port.id] then\n       for _,r in ipairs(secrules[secbindings[port.id].security_group_id]) do\n          if r.remote_group_id == \"\\\\N\" then\n             if r.direction:lower() == direction then\n                local NULL = \"\\\\N\" -- SQL null\n                local rule = {}\n                if r.ethertype        ~= NULL then rule.ethertype        = r.ethertype:lower() end\n                if r.protocol         ~= NULL then rule.protocol         = r.protocol:lower()  end\n                if r.port_range_min   ~= NULL then rule.dest_port_min    = r.port_range_min    end\n                if r.port_range_max   ~= NULL then rule.dest_port_max    = r.port_range_max    end\n                if r.remote_ip_prefix ~= NULL then rule.remote_ip_prefix = r.remote_ip_prefix  end\n                table.insert(rules, rule)\n             end\n          end\n       end\n-   end\n-   if type == \"stateless\" then\n-      return { rules = rules }\n-   else\n-      return { rules = rules,\n-               state_track = port.id,\n-               state_check = port.id }\n+      if type == \"stateless\" then\n+         return { rules = rules }\n+      else\n+         return { rules = rules,\n+                  state_track = port.id,\n+                  state_check = port.id }\n+      end\n    end\n end\n. Good catch! This should be indeed removed.\n. The overflow is checked for in packet.append (used by datagram:payload).\n\nThe caller has to ensure that the underlying packet has enough free space to accommodate for length additional bytes.\n\nI meant this to be read as \"will raise an error on overflow\". Should I reword to make that meaning more implicit?\n. @alexandergall Good catch! willfix asap :)\n. Agree, I just blindly copied the snabbnfv hierarchy without thinking.\n\nStop using subcommands and simply flatten out the command-name space. (s/snabbnfv traffic/snabbnfv-traffic/)\n\nI think this is the smartest move, we're not exactly running out of namespace are we?\n. s/langauge/language\n. Might be a good time to figure this one out.\n. I think last time we talked about our Lua centric configuration format etc. we (I at least) conceded that we should probably take the boring route of using JSON as a config format. Sure we can't have comments then and the format is sub-par for the writer but at least its very well defined and we can just point to the JSON spec.\nOther than that the snabbnfv --help got much simpler with the PcapFilter app since we now just point to the filter-expression man page.\n. Could value default to 1 if not supplied to add? E.g. like Common Lisps INCF.\n. Are counters initialized to 0 or do I have to initialize them using set?\n. Might be worthwhile to point out that every Snabb NFV configuration can define multiple ports so that n VMs can connect to one 10G port.\n. link.h uses struct counter. Is that what you mean by \"for the benefit of C files\"?\n. I copied the pattern from the core.packet_h require just above. Indeed the requires of both packet_h and counter_h are redundant. I suggest we fix this in a separate \"style branch\", since we have a lot of unnecessary require calls (especially to core packages) in most modules.\n. That's just so the lines aren't too long. Too distracting?\n. I have no personal preference, I wasn't sure about the scope of cdef's so this was my cautious approach to just mimic what I knew worked. I could imagine a C program wanting to read snabb counters so that would be an argument for having the header file. Since this also applies to the other header files in core as well I say lets move this into a seperate general discussion on whether we want C header files and in wjich cases.\n. I am not going to include this in my merge for reasons of hygiene, please add your public key in a different PR.\n. Yeah basically I checked out the PR as a branch, chopped off the last commit and am now appending my edits.\n. Was bitfield just moved within the file or is there some significant change I am not seeing here?\n. There's a typo here:  pakcet\nPlease use M-x ispell (or equivalent if you are not using Emacs), I know its a chore but its the only way I know of to avoid this systematically.\n. Also note that this introduces a new convenient but strictly speaking redundant public function that we will have to support. Its also questionable whether datagram:data will make easier or harder to read, one more thing to have to look up. Is it worth it?\n```\ndata, len = d:data()\nvs\np = d:packet()\ndata, len = p:data(), p:length()\n``\n. This should beRawSocket:stop. See the App API reference which lists the \u201cspecial\u201d methods: https://github.com/SnabbCo/snabbswitch/tree/v2015.10/src#app\n. What about the Solarflare driver?\n. Strictly speaking the attributes are not part of the [public API](https://github.com/SnabbCo/snabbswitch/tree/master/src#packet-corepacket). So following the API you would need toreadthe data into a buffer and then callpacket.from_pointer(which is a wrapper aroundpacket.allocateandpacket.append). \n. I think its safe to \u201cdie\u201d upon a write error. If Linux breaks, we can't do anything about it. :)assertanderrorare fine!\n. You can definitely use other apps in the selftest! E.g. other tests use the [pcap](https://github.com/SnabbCo/snabbswitch/tree/master/src/apps/pcap#pcapreader-and-pcapwriter-apps-appspcappcap) apps to test predefined packet streams and compare the output to a known result.\n. Shouldn't it say \u201cVirtioNet\u201d here?\n. At least add comments that describe usage and purpose of these functions, as of now I have no idea what they even do. And functions that are notlocaltocore.libindeed require documentation insrc/README.md.src`. Without the documentation I can hardly tell where the functions belong.\n@andywingo Undocumented public functions in core.lib are a bug, I'd appreciate any tips on which functions are inadequately documented.\n. I don't think this function justifies extending the packet API. virtual_to_physical is public, what's wrong with:\n```\nlocal memory = require(\"core.memory\")\nphys = memory.virtual_to_physical(packet.data(p))\n``\n.cfg` is being converted to a string here because Apps are supposed to be able to accept their configuration as strings. The common pattern to read an app config is this:\nlua\nfunction App:new (arg)\n      local conf = config.parse_app_arg(arg) or {} -- The default\n      ....\nend\n. I think Match is a bit too fancy for apps/basic/basic_apps.lua and would move it two its own module apps/match/match.lua.\n. Actually, the above breaks when arg is an empty table, this would be more correct:\nlua\nfunction App:new (arg)\n      local conf = config.parse_app_arg(arg) or {}\n      conf.foo = conf.foo or \"default\"\nend\n. I am already talking with @hb9cwp about reverting this change (its also responsible for the performance regression), I believe it slipped in only accidentally.\n. @hb9cwp You missed this line. :-)\n. > I would like to avoid these license and copyright lines in the source files. To me it is boilerplate that doesn't add value. We already spell out the license in the COPYING file and Git already tracks the authorship very directly. If those mechanisms are not sufficient for some reason then it would be best to open an Issue to discuss that rather than starting to add random notes in the source code.\nAbsolutely agree, git blame should work for the people who really need to know who wrote a specific line of code. I get that people want to express their authorship but I don't think it makes sense to spell it out on a per-file basis. Once somebody unrelated changes a line in a \u201ccopyrighted file\u201d the copyright notice becomes bogus, no? Ah, the beauty of open source... :-)\n. Alex has made the precedent for OO style in Snabb and I have accepted/adopted his conventions because I felt they were good. In these terms the module ctable is a class, which would have a class method new to instantiate ctable objects. You can find examples for this in lib.protocol and its documentation. (Generally this is how I do it and what I recommend: look for prior examples in existing code and try to avoid breaking conventions where possible.)\n. To add to that I just noticed the hash functions of the ctable module, from an OO perspective these would probably be class methods as well? There is a precedent in lib.protocol where header classes have class and instance methods as well but the distinction is not made in the documentation. I guess this is a FIXME, I am somewhat out of answers.\n. Yes they are definitely distinct from core.* (but I don't think that is necessarily an issue), imho the question is if the style works well for the API. But, we are digressing again. I say do as you see fit for now.\nEdit: but please add the prefixes, e.g. \u201cFunction ctable.new ... returns a ctable.\u201d and \u201cMethod ctable:foo ...\u201d\n. Since there are some FIXMEs in the code I assume this will be iterated upon. I will leave some notes of things I thought about while going through this PR:\nMost of the introduction (from \u201cIn Lua parlance...\u201d to line 35) are implementation details. I would avoid talking about them in the documentation. The reader most likely only cares about how to use it, not how it works (the how can be source code comments). This case is especially questionable because implementation details are mixed with usage information, so you have to read the whole lot even if you are only looking for either one.\n. Do we need a \u201cNotation\u201d chapter in the documentation that lists and explains the notations used throughout the docs? While the [...,...) notation might be obvious to 99% of readers, we still need to either avoid it or document it. There has to be a cost/value calculation for each special notation, e.g. how often is it useful vs how many notations does the reader have to know about to read the docs.\n. I still think it should say ctable:resize. It even says \u201cresize the ctable\u201d in the description. If its an object of type ctable then the method should be ctable:resize.\n. I think parse_params and set are over engineered. Current Snabb idiom would suggest this way more obvious (and less) code:\nassert(params.required1, \"error blah\")\n...\nparams.optional1 = params.optional1 or \"default-value\"\n...\n. @dpino I want to merge this, do you want to amend anything to this before I do? E.g. swapping Ethernet source and destination would be a low hanging fruit. It would make sense to test if this l2fwd works in the context of packetblaster->snabbnfv<->vm(snabb+l2fwd)\n. Why is there a while loop here? I see three cases and no loop:\n- -h/--help\u2014print usage\n- -x/--foo\u2014error\n- foo\u2014program\n. @nnikolaev-virtualopensystems Am I right in assuming that would make l2fwd a two-way street?\n. I can relate to the last point, might be worth to spin this out into core.lib in the future then. We already have some common code for app configurations which then should be unified with this.\n. Its a confusing piece of code that I had to read a couple times to know what its doing. I expected a case where it actually loops but since usage always exits it never does.\n\nIt's for if Snabb gets any more options.\n\nThat explains the motivation behind this and I get it. I recommend to avoid coding for the future as its confusing for readers.\n. Yeah, the two of us tend to talk past each other. FIXME ;-)\nI generally do not set the requirements for accepting a PR, so I will usually not tell you or anybody else to \u201cdo X\u201d. Instead, the project is developed in consensus of the contributors, and I can not truthfully represent that consensus on my own. So what I do is point out issues I see and give arguments as to why I see them as issues. E.g. I supply a single \u201cvote\u201d to the consensus, which is not itself a veto. Deciding what to do with my view is up to you. If you are convinced by my argument you might change the code or try to avoid similar issues in the future. If you are not convinced I would ideally get back an argument that explains why. Unless anybody else takes issue with the code I will merge it, and I will usually wait for a while to give others the chance to look at the PR, discuss it and maybe agree on changes.\nSo in this particular case I am simply making you aware of the fact, that as a reader of the code I found a particular part of it confusing (its just feedback). I couldn't tell from your reply if you understood my confusion, so I explained it in depth in the hope that you become aware of the general issue. (I also pointed out which part of your comment I found helpful, to give you a feel of what I would like to hear.) My goal (and I think this benefits everybody) is to reduce workload by increasing empathy with the consumers of code: when code is harder that necessary to read for reviewers, it will be more work for all. So in order to reduce that work I am letting you know how it would be easier for me (the reviewer in this case). Same goes for comments I guess, we need to learn what we actually want to hear from each other.\nSo to sum it up: You don't need to ask me what to do because I won't tell you what to do. It's your call, not mine. What I will tell you is my perspective on your code/comments/... and maybe you can use this second pair of eyes  to make it better.\n. My concern is that non-normative sections in the documentation can lead to other relying on a specific behavior that is not actually part of the API. But I might be wrong, feedback in #753 would be appreciated,\n. @wingo While the notation you used serves as an example this is in no way personally about you or the documentation you wrote (other people wrote documentation too, even before last week). Could we agree on not taking these kind of things to a personal level? I find this extremely annoying. If I have a personal issue I will contact you personally in private.\n\nIt feels like more of a blog post than documentation ;) Or rather: too much opining for the guidelining.\n\nAbsolutely agree.\n. I don't think stating three words would be enough, and I think that explaining the concepts with reasoning and specific examples is what communicates the intent clearly.\n. I guess we should rename with_restart to something like invoke_app_method. Nothing critical but this change makes the functions name a misnomer.\n. @lukego Standard deviation is only printed and not used in any equation. Any result below 85% of master results is considered a failure. Might be a ridiculous margin, I had no idea of what would be a good margin at the time of picking this number. If I remember correctly 0.85 was chosen because it yielded an acceptable false positive rate. If I were to revisit that code, I would not calculate average at all and use minimum instead.\n. @lukego Do you want to rename with_restart before I merge this or leave it as is?\n. I think there has to be an assert here to check for overflow. See packet.prepend.\n. I assume this is a debug leftover?\n. Is this not a NOOP?\n. From the conservative perspective:\n- The require should be at the top of the file (I like to use local requires in selftests but I am not sure if we should expand the use of this pattern)\n- core.* should not require lib.*, @wingo pointed out a precedent in https://github.com/SnabbCo/snabbswitch/pull/764#issuecomment-188227602 but I consider it to be a bug\n- thus: lib.histogram should depend on core.shm and be core.histogram (and reuse core.shm.root, see core.counter)\n. Oh, the header levels are off in lib/README*. Not sure if pandoc is smart about this or not.\n. What exactly do the changes to LoadGen do? Does the LoadGen documentation need an update to reflect conf.report_rx?\n. Yeah I think it should be in core. :+1: I think to even remember @lukego longing for a histogram shm module once or twice.\n. \u201cJust to have a default\u201d is not an argument for having a default. I don't see a case where a default is useful, and not having an error when accidentally failing to supply rate and bucket_capacity is dangerous. Please revert this.\n. I don't like the name \u201cleaky\u201d as it is usually used to describe leaking resources, which is not the case here. I think \u201cdrop\u201d is the right name here.\nI don't like that the default behavior changes, but on the other hand the fact hat no tests fail make me wonder if drop=true is always the wrong choice and if we need it at all. I see two routes here:\n1. Show that drop=false is consistently superior to drop=true for our use cases (hint: the parts of NFV\u00a0selftest that test RateLimiter are currently ridiculously relaxed because the results were... weird). Do not implement drop in the first place, instead just change the behavior to drop=false.\n2. Be conservative and retain the previous default of drop=true.\n. Actually, looking at push:  leaky does not change the behavior that much.The only difference is where the packets are dropped (before being pushed on the link or after being popped from the link). I am inclined to think the new behavior is the correct one and that we should drop leaky altogether. See also: #656 \n. No offense taken. If in the future, I add a typo to nfvconfig it could lead to a hard to detect  bug where Snabb NFV is broken because it fails to configure RateLimiter. In my mind a default rate is always wrong because there is no sensible default rate, and the whole \u201craison d'etre\u201d of RateLimiter is to select a rate. I see using set_rate in any case other than to dynamically change the rate as a hack to avoid the declarative nature of app networks. Why do you need a procedural interface to something that appears to be solvable in a declarative fashion?\n. The I don't see why you can't supply the default values in your application. I will not accept this particular change.\n. I think the use case is perfectly valid and I am completely on board with this PR, just not with the particular change of adding default values to rate and bucket_size as there are no sensible defaults. You can set default values in your program when instantiating the app without pushing the defaults onto other users of the app. Rejecting the default values does not conflict with your use case as far as I can tell.\n. @capr Not exactly a fan of this code, can you change this to simply copy stuff into o._config (preferably without the loop)?\n. @capr I don't think the changes to core/main should be in this PR. Are they required for lisper to work?\n. @capr I think you can revert them by reverting these commits:\n```\ncommit e5b1a66ed8b73d6256cd5f06fa27ace5f2793931\nMerge: 9f74d16 e285507\nAuthor: Cosmin Apreutesei cosmin.apreutesei@gmail.com\nDate:   Sat Nov 14 13:16:33 2015 +0200\nmerge with master\n\ncommit 654cc55f03f29fdfa0c20074fb118134c97ae75b\nAuthor: Cosmin Apreutesei cosmin.apreutesei@gmail.com\nDate:   Sun Oct 25 13:17:41 2015 +0200\nmake snabb exe prefer to load modules from filesystem to built-in ones\n\ndiff --git a/src/core/main.lua b/src/core/main.lua\nindex fcde79b..8fdbe3f 100755\n--- a/src/core/main.lua\n+++ b/src/core/main.lua\n@@ -3,6 +3,7 @@ module(...,package.seeall)\n -- Default to not using any Lua code on the filesystem.\n -- (Can be overridden with -P argument: see below.)\n --package.path = ''\n+package.loaders1, package.loaders[2] = package.loaders[2], package.loaders1\nlocal STP = require(\"lib.lua.StackTracePlus\")\n local ffi = require(\"ffi\")\n@@ -72,13 +73,13 @@ end\n-- programname(\"snabbnfv-1.0\") => \"snabbnfv\"\n-function programname (program) \n+function programname (program)\n    program = program:gsub(\"^./\", \"\") -- /bin/snabb-1.0 => snabb-1.0\n    program = program:gsub(\"[-.].$\", \"\") -- snabb-1.0   => snabb\n    return program\n end\n -- modulename(\"nfv-sync-master.2.0\") => \"program.nfv.nfv_sync_master\")\n-function modulename (program) \n+function modulename (program)\n    program = programname(program)\n    return (\"program.%s.%s\"):format(program, program)\n end\n@@ -86,7 +87,7 @@ end\n -- Return all command-line paramters (argv) in an array.\n function parse_command_line ()\n    local array = {}\n-   for i = 0, C.argc - 1 do \n+   for i = 0, C.argc - 1 do\n       table.insert(array, ffi.string(C.argv[i]))\n    end\n    return array\ncommit 4c1e8a53c44e8fb3e2c89899e58a0818af13c6bc\nAuthor: Cosmin Apreutesei cosmin.apreutesei@gmail.com\nDate:   Thu Oct 22 18:14:38 2015 +0300\nlisper (init)\n\ndiff --git a/src/core/main.lua b/src/core/main.lua\nold mode 100644\nnew mode 100755\nindex 9fb08ab..fcde79b\n--- a/src/core/main.lua\n+++ b/src/core/main.lua\n@@ -2,7 +2,7 @@ module(...,package.seeall)\n-- Default to not using any Lua code on the filesystem.\n -- (Can be overridden with -P argument: see below.)\n-package.path = ''\n+--package.path = ''\nlocal STP = require(\"lib.lua.StackTracePlus\")\n local ffi = require(\"ffi\")\n```\n. Maybe just do this (ensure master is up to date):\ngit checkout master src/core/main.lua\ngit add src/core/main.lua\ngit commit -m \"Revert changes to core.main.\"\n. @lukego Why does snabbswitch.markdown require snabb though?\n. We haven't figured that out completely yet. I think in most cases the PRs should be against master, then again some PRs only make sense targeting a specific branch (e.g. PRs related to the IPsec topic branch for instance). I have to update SnabbBot to merge PRs with their target branch instead of a fixed branch (master in our case) for the latter case to work well.\nI think to remember that @wingo thinks we should use the target branch to designate the next upstream hop. I don't think this is a good idea because contributors shouldn't be required to know who the next hop is, and if its not the right hop then I think its hard to rectify without closing/opening a new PR.\n. @lukego This might have broken things? It worked before but now I get:\nMARKDOWN  obj/program/lwaftr/doc/README.ndp.md\nmake: *** [obj/program/lwaftr/doc/README.ndp.md] Error 1\n. This function really takes gcm_data.expanded_keys as its second argument (which itself has no type attached to it besides uint8_t). My reasoning here is that since the expanded_keys field is the first field of the gcm_data struct the pointer to a gcm_data object will be the same as the pointer to gcm_data.expanded_keys. This way we get a pretty definitive type check.\nIs this a reasonable assumption or is this insane?\n. I originally boxed gcm_data because I thought this was the only way to translate &gcm_data from the C test code that ships with the Intel code (maybe it even was before we swapped out the ASM with DynASM code?). Anyways this works and is much simpler. Again, I am no longer referring to the expanded_keys field explicitly.\n. This doesn't work\n1. for this PR because the old version of SnabbDoc is run\n2. for other PRs because they might be behind next\nSee #856 for a fix targeting next. I think merging that PR should make the CI pass.\n. @mwiget I am not a big fan of the code duplication here. Could the shared code below be put into a shared function?\n. This kind of magic is awkward to have in two places. Does packetblaster lwaftr do this too? If not, how does it affect performance?\n. @mwiget With the risk of being super annoying: the code before and after uses of config_loadgen is identical in both cases as far as I can tell. How do you feel about moving that code into config_loadgen and rename it to run_loadgen?\nE.g. instead of\nlocal patterns = args\n   local nics = packetblaster.config_loadgen(c, patterns)\n   assert(nics > 0, \"<PCI> matches no suitable devices.\")\n   engine.busywait = true\n   intel10g.num_descriptors = 32*1024\n   engine.configure(c)\n   local t = timer.new(\"report\", packetblaster.report, 1e9, 'repeating')\n   timer.activate(t)\n   if duration then engine.main({duration=duration})\n   else             engine.main() end\nwe would have\npacketblaster.run_loadgen(c, args, duration)\n. There were attempts at running cleanup code (#509) but we noticed that we do not want to handle signals inside the main engine loop. Any unwind protection will probably require an architectural change (parent/supervisor process). We could run snabb gc as a daily cron job on lab servers, I usually benefit from stale snabb resources when testing related features.\nRegarding snabb top's DWIM selection (which is not part of this PR\u00a0btw, the code was just moved): I think it is optimal, if there is only one instance then there is no need to supply the pid, if there is more than one instance then you need to be explicit. I could imagine an alias feature that creates a symbolic link to the resource, but by-pid referencing should stay to allow for standard UNIX daemon idioms.\n. I see two options:\n1. Divide resize up into grow and shrink (makes the code slightly awkward and API kind of encourages branching, although not in this specific case) :\n--- Set packet data length.\n-function resize (p, len)\n-   assert(len <= max_payload, \"packet payload overflow\")\n-   p.length = len\n+-- Grow packet data length.\n+function grow (p, o)\n+   local len = p.length\n+   local new_len = len + o\n+   assert(new_len <= max_payload, \"packet payload overflow\")\n+   ffi.fill(p.data+len, o)\n+   p.length = new_len\n+end\n+\n+-- Shrink packet data length.\n+function shrink (p, o)\n+   local len = p.length\n+   local new_len = len-o\n+   assert(new_len >= 0, \"packet payload underflow\")\n+   p.length = new_len\n1. Always zero packet memory in free (gives a pretty solid protection against information leaks, performance impact?):\nfunction free (p)\n    counter.add(engine.frees)\n    counter.add(engine.freebytes, p.length)\n+   -- Zero packet data\n+   ffi.fill(p.data, p.length)\n [...]\n. @lukego:\n\nThis resize function seems potentially error-prone to me: if you resize the packet but don't fill in all of the data then you will be leaking unknown information from the previous user of the packet.\n\nHow about this: I take a slight performance hit (pretty synchronous >13Gbit/s with the changes) which is somewhat amortized by the now obsolete call to ffi.fill for zeroing padding.\n. Makes sense, honestly I had/have trouble getting a good understanding of Intel10G stat registers code. Could definitely use a thorough review with SHM stats in mind (e.g. get rid of Intel10G.dev.get_rx/tx_stats() altogether). In this PR I attempted to be make mostly non-disruptive (light bolt-on) changes.\n. This was indeed tested on lugano-1 (Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz). I think the most interesting part is actually the (annotated) profiler output:\n$ sudo lock ./snabb snabbmark esp 1e7 1024 encapsulate Fpv\nlocking /var/lock/lab.. flock: getting lock took 0.000002 seconds\n81%  lib/ipsec/aes_128_gcm.lua:encrypt\n  <- 100%  TRACE  24          ->loop     \n 4%  lib/ipsec/esp.lua:encapsulate\n  <- 92%  TRACE  24          ->loop     \n  <-  8%  JIT Compiler\n 3%  program/snabbmark/snabbmark.lua:esp\n  <- 100%  Garbage Collector\nEncapsulation (packet size = 1024): 13.15 Gbit/s\n$ sudo lock ./snabb snabbmark esp 1e7 1024 decapsulate Fpv\nlocking /var/lock/lab.. flock: getting lock took 0.000002 seconds\n83%  lib/ipsec/aes_128_gcm.lua:decrypt\n  <- 100%  TRACE   7          ->loop     \n 5%  lib/ipsec/esp.lua:decapsulate\n  <- 100%  TRACE   7          ->loop     \n 3%  program/snabbmark/snabbmark.lua:esp\n  <- 94%  Garbage Collector\n  <-  6%  TRACE   7          ->loop     \n 3%  core/packet.lua:clone\n  <- 100%  TRACE   7          ->loop     \nDecapsulation (packet size = 1024): 13.86 Gbit/s\nWe see 80% of the time used by our AES DynASM routines (\u201ca\u201d for annotate shows this more definitively). I read this as 20% overhead due to ESP on 1KB packets (vs plain AES-GCM).\n. @petebristow ping. Pretty sure it was mistakenly leftover though.\n. Yes please, if the fix is in the original PR we can merge it however we like and at least be sure that you ACKed it.\n. @lukego By the way, if you mention the #XXX PR in commit messages these commits will show up in the PR thread, which is nice imho.\n. Please don't use the packet fields directly and use the public functions packet.data and packet.length instead. It's a bit more verbose, but as of now the fields are not really part of the public API.\n. http://snabbco.github.io/#packet-core.packet describes the core.packet API. It doesn't mention the actual packet structure which is why I am uneasy about using the fields directly instead of going through the API functions.\n. I think documenting the ABI is a separate issue, since that would not define the FFI field names (data/length) if I understand correctly(?).\nOne advantage (imho) of the API functions is that they are read-only. I am fine with documenting the field names as well and encouraging their use, or even getting rid of the API functions. Either way, I would like that API to be defined before I recommend its use.\nI have encountered direct uses of the fields before as well, but considered them as semi-bugs until now.\n. App methods (except new) need to be called in protected mode using with_restart, otherwise a single faulty app can bring down the whole process. See how pull/push are called.\n. Good catch, it should indeed say Gigabits.\n. I don't understand why it being local broke this PR? Is it because it was lexically defined after it was used? If so I think the proper solution would be to move the function definition up instead of making it globally accessible. Nitpick, opinions?\n. Ugly but sensible imho.\n. It is a mystery to me why this works. E.g. I had assumed I needed to pass worker_pid to shutdown since the worker goes on to create the /var/run directory based on S.getpid(), but apparently when the worker calls S.getpid() it returns the parent pid? confused\n. Should be assert(ffi.sizeof(rxdesc_t) == 16, ...)? Not sure how much sense the assertion makes though, in which cases would it not hold true?\n. @lukego @dpino What do you think about the name configured, e.g. the callback to be called after an app was configured?\n. I think we can just change it when the time comes, no?\n. I get why you are reluctant to documenting an API that may change, but documenting the exact ctype of packets is the whole point of this PR. Besides, we haven't made any promises with regards to API stability. If we document the ctype now, and change it next week, then we will at least have documented the API change.\nI don't see a downside to documenting the current state, even if it might change soon. Instead, I see immediate gain by merging this PR now, and not let it be stuck waiting on future changes.\n. @lukego Ok where do I start: the breaking change is that this sets _G.developer_debug to nil instead of false. Then in core.timer, debug is set to _G.developer_debug. The next thing I can't explain:\n- When _G.developer_debug is true or false the value of timer.debug will be true or false respectively\n- However when _G.developer_debug is nil the value of timer.debug is the following table:\nSnabb> for k,v in pairs(timer.debug) do print(k,v) end\ntraceback   function: builtin#141\nsetlocal    function: builtin#133\ngetupvalue  function: builtin#134\nsetupvalue  function: builtin#135\nupvalueid   function: builtin#136\ngetlocal    function: builtin#132\ngetregistry function: builtin#126\ngetinfo function: builtin#131\nsethook function: builtin#138\nsetmetatable    function: builtin#128\nupvaluejoin function: builtin#137\ngethook function: builtin#139\ndebug   function: builtin#140\ngetmetatable    function: builtin#127\nsetfenv function: builtin#130\ngetfenv function: builtin#129\n. This patch would fix it, but I am honestly interested in why this happens.\n``` diff\ndiff --git a/src/core/main.lua b/src/core/main.lua\nindex f736e25..343e2f9 100644\n--- a/src/core/main.lua\n+++ b/src/core/main.lua\n@@ -29,7 +29,7 @@ ffi.cdef[[\n       extern char** argv;\n ]]\n-_G.developer_debug = lib.getenv(\"SNABB_DEBUG\") and true\n+_G.developer_debug = (lib.getenv(\"SNABB_DEBUG\") and true) or false\n debug_on_error = _G.developer_debug\nfunction main ()\n``\n.$logdiris actually supposed to persist (it caches results, otherwise everything is recomputed all the time).\n. Given that the packet ctype is now public (actually depending on #996), the patternffi.new(\"struct packet *[?]\",...` could be written as\ndesq_t = ffi.typeof(\"$ *[?]\", packet.packet_t)\nself.rxpackets = ffi.new(desq_t, self.ndesq)\nCc @kbara \n. Seems cleaner to me to not invoke the C name directly, in case the ever definition changes. Also, if desq_t was defined module-wide, then the JIT would see one ctype instead of different ctypes for each app instance.\n. Do we know how other implementations interpret the RFC? If not, could you compare with a mature implementation? OpenBSD?\n. Looks to me as if we could return false in the failure branch if resync failed and avoid the proceed variable altogether?\n. These should be configurable in new like spi etc. What could be sensible defaults?\n. Never mind my advice above, as we have decided against exposing packet_t, see https://github.com/snabbco/snabb/pull/996#issuecomment-243049072.\n. This looks like its a global setting, shouldn't this be per PCI device / app instance? Maybe even a configuration option of the driver app?\n. Why not use Loadgen for --no-loop? I don\u2019t think its a good idea for a switch to change the underlying driver effectively, since they might have differing performance characteristics.\n. I think this is the core change required here, no?\n. I don't think the switch does anything for synth, does it?\n. None of these (sizes or source or destination) are mandatory.\n. I think we can omit these two lines if we use link.transmit(l, packet.clone(p))?\n. On the other hand that would double the copying done, never mind.\n. Agree! Will merge this as is.\n. @dpino Great that you spotted this, I had already forgotten that I removed the parameter. As per this PR the app arg can be a single value of any type, unless myapp.config is set (then it must be a table, potentially string encoded). One solution would be to extend PcapWriter to branch based on its argument:\nfunction PcapWriter:new (file)\n   local mode = \"w\"\n   if typeof(file) == 'table' then\n      mode = file[2]\n      file = file[1]\n   end\n   ...\nIts not beautiful, but its backwards compatible. You could invoke PcapWriter like this then:\nconfig.app(c, \"writer\", PcapWriter, \"foo.pcap\")\nconfig.app(c, \"writer+\", PcapWriter, {\"foo.pcap\", \"w+\"})\nIf this covers your use cases I can append that change to this PR, let me know!\n. I just wondered: what happens to the state stored in the anti-replay window on resync? Doesn't it need to be updated/cleared to reflect the advance in seq_high?\n. Pedantic: should be  assert(S.fork())\n. So here is a potential mistake in translation that could affect benchmark results (although not of the l2fwd benchmark). I changed \u201cpackets transmitted onto the link\u201d to \u201dpackets read off the link\u201d because I felt it was correct in this instance, and further instances below. E.g. I think its \u201ccheating\u201d to stop when the Source has emitted N packets, as opposed to when these N packets have actually been processed.\nOpinions?\n. I think there is a jit.flush(tr) missing here.\n. I think this should be shm.exists(args.counter). SHM paths are no longer resolved based on a changing shm.path.\n. This!\n. Wait, shouldn\u2019t this be value == nil, as in \u201cif value is nil, delete the entry and set the corresponding key in the backing table to nil, too\u201d.\n. As per the above I would suggest to test deletion here.\n. In theory you could use the new core.lib.parse function for this.\n. On second look the latter is performed unconditionally.\n. Naive question: why is the pull order relevant?\n. This line (configuration = new_config) is important for a PR I am working on, which uses weak keys in lua tables to store per-configuration metadata.\n. You\u2019re right, I was overly complicating things\n. @wingo Still, why is it preferable to mutate configuration (in many surgical steps) instead of going with the pattern proposed by the core.config API to just swap it out?\n. this looks fishy, new[linkspec] makes no sense, even if its a typo for new.links[linkspec].\n. Should probably be\nif new.apps[fa] then add_action('unlink_output', fa, fl) end\nif new.apps[ta] then add_action('unlink_input', ta, tl) end\nadd_action('free_link', linkspec)\n. :+1: \n. Technically speaking, we don\u2019t have to call link that often, from the docs:\n\nOptional. Called any time the app\u2019s links may have been changed (including on start-up). Guaranteed to be called before pull and push are called with new links.\n\nE.g., its OK to just call each apps link methods once after reconfiguration, and its not necessary that the links have actually changed (apps may not rely on that).\n. Not sure what these were used for, I think this was for assembler apps / ABI?\n. I see mostly race conditions here. In my mind configuration is the last argument to engine.configure (verbatim), which mutates the engine to match that argument. Did we not come to the conclusion that followers/workers have their own private configuration?\n. No, checking for link changes in a pull function is a big caveat imho, it would make a the respective pull callbacks ugly.\nIts kind of mind bending to me that Follower and Leader are supposed to be apps, instead of programs. To me it seems simpler to stream configuration changes in between calls to enginine.main, and only apply them once they are complete.\n. Then good riddance! :-)\n. Actually, main.exit() does exist but currently simply calls out to os.exit(). I think the idea was to be prepared for potential cleanup code in the engine?. I guess it depends on how you intend run this. In this case make test will execute apps/interlink/selftest.snabb from src/, and that\u2019s also where I tend to run the script from.\n@petebristow champions this convention if I understood him correctly and I lean towards following suit. Since snabb is rarely in PATH, the next best guess seems to be that its in PWD. Of course you can always do snabb snsh <script>.snabb.. The code actually works as intended because the group parent will have created group/packets.freelist before it starts any children. I.e. shm.exists(\"group/packets.freelist\") tests if we are the group parent, which is not something that changes over time.\nI agree with the other point about not always having a group freelist, I\u2019ll look into that.. Yep that\u2019s right the comments were off.  :-/  An interlink can go from DXUP to {T,R}XUP back to DXUP. It must deallocated once both ends have detached. I.e., more than two processes can attach to a single interlink during its life time. As long as either end is attached, both receiver and transmitter can be superseded by new processes. Should be fixed now.. IIRC prior to this change the line was #!/usr/bin/env snabb snsh which consults PATH but that doesn\u2019t help much usually. On the other hand #!snabb snsh won\u2019t consult PATH but execute ./snabb.. I think @lukego introduced them in 45b23d4029e717fdb693e1e18ec2826507fd16a7 and 8b1bc0e13b275d34f020bdb6ccec24ffd916024b but even then they were not used. I\u2019d say its safe to drop them.. This is the bit that enables configuring inter process links without the somewhat redundant configuration by using its own name. I liked that you can simply config.app(c, \"SharedLink\", Receiver), but I agree at a second glance that this might be unnecessairly magic. Alteratively, the synatx would be: config.app(c, \"AppName\", Receiver, \"SharedLink\").. Honestly, I don\u2019t see the difference between core calling into lib/ vs apps/, its both outside core. As it stands, lib/interlink and apps/interlink form a single whole, the reason the shutdown hooks are in apps/interlink is because that is where SHM paths are chosen.. [?] doesn\u2019t work because this branch maps it as a SHM object and core.shm would then fail to determine the sizeof this ctype. So I guess the remaing purpose of max_packets is to give this value a name in the source.. Agree, have a patch just for that already in the barrel.\nAbout the other bits (to which I can for some reason not reply individually)...\n\n\nIt\u2019s not like max_packets was configurable at run time before, so I see it as a net-zero to change it to your formulation, which I might also prefer. Replacing these uint64_ts with 32 bit ints seems like a good idea, regardless. Both of these don\u2019t appear to me to be related to this PR in particularily, so I tend to pass.\n\n\nYeah I could shuffle around the code some... but I honestly don\u2019t see the benefit right now.\n\n\nLet\u2019s get this merged and improve on it incrementally?. What does diff.rxpackets > diff.txpackets tell us? Should it signify failure? In my experience this does happen (I have so far attributed it to measurement inaccuracy, due to queueing and whatnot) and feel like it does not imply failure.\nIn my mind, the only definitive predicate we have is that\n\ndiff.rxpackets < diff.txpackets \u2192 failure\n\nso the predicate for success should be the inverse of that, i.e.\n\ndiff.rxpackets >= diff.txpackets \u2192 success\n\nI might be missing something but there goes my train of thought.. Right, I the goal here is to print a definitive lost %. With this change it prints\nLoss: #ingress_drop ingress drop + #link_drop packets lost (<total%>). Before, the percentage only included the #link_drop packets.. I experience different behavior of snabb loadtest on master, e.g. here on a Intel i350:\ndyser$ sudo ./snabb loadtest find-limit -b 1e9 ./program/snabbnfv/test_fixtures/pcap/http_google.pcap A B 22:00.1 ./program/snabbnfv/test_fixtures/pcap/http_google.pcap B A 23:00.1\nWarming up at 1.000000 Gb/s for 5 seconds.\nApplying 0.500000 Gbps of load.\n  A:\n    TX 116280 packets (0.116280 MPPS), 59709780 bytes (0.500004 Gbps)\n    RX 118864 packets (0.118864 MPPS), 61217340 bytes (0.512561 Gbps)\n    Loss: 0 ingress drop + -2584 packets lost (-2.222222%)\n  B:\n    TX 116280 packets (0.116280 MPPS), 59709780 bytes (0.500004 Gbps)\n    RX 118862 packets (0.118862 MPPS), 61215820 bytes (0.512548 Gbps)\n    Loss: 0 ingress drop + -2582 packets lost (-2.220502%)\nFailed; 2 retries remaining.\nApplying 0.500000 Gbps of load.\n  A:\n    TX 116280 packets (0.116280 MPPS), 59709780 bytes (0.500004 Gbps)\n    RX 116486 packets (0.116486 MPPS), 59990001 bytes (0.502285 Gbps)\n    Loss: 0 ingress drop + -206 packets lost (-0.177159%)\n  B:\n    TX 116280 packets (0.116280 MPPS), 59709780 bytes (0.500004 Gbps)\n    RX 116484 packets (0.116484 MPPS), 59989260 bytes (0.502279 Gbps)\n    Loss: 0 ingress drop + -204 packets lost (-0.175439%)\nFailed; 1 retries remaining.\n...\nI attributed this to measurement inaccuracy, but it could also be a bug somewhere I suppose? My thinking is that even if we read all stats off the NIC counters (which I propose) there can be discrepancies due to say aliasing of the timers that sync NIC stats. \\o/. While the change is backwards compatible (accepts both spellings), a quick internet search indicates that \u201cqueuing\u201d is actually the more common spelling nowadays. Seems I have trusted my spell-checker too much. I suppose I will revert this for simplicity.. Yeah, its a tough part. The crux is that the loop is really there to check that the list doesn\u2019t have multiple keys, which would make it unsuitable for being represented as a Lua table. I have rewritten this function to make that clearer. Previously, only the header comment hinted at this (and not too prominently at that.). I usually write it in the former way (I think because lua-mode auto-indents it nicely that way?) but I find both versions readable. I don\u2019t mind either way. :+1: . ",
    "ba1020": "Hi I'm a freelancer and contractor in Free and OpenSource software. I rock networks and esp those from nics intel.\n. ",
    "xgp": "Thanks Luke!\n. ",
    "wingo": "Thank you! :)\n. I should note that we found that the precise way in which you iterate through a pcap file can affect results.  In pflua we now use this helper: https://github.com/Igalia/pflua/blob/master/src/pf/savefile.lua#L64\nIt builds and returns a regular Lua table of packet pointers.\n. Nice!  You might like http://luatime.org/archives/2015/01/27/generating-arbitrary-test-data for comparison.\n. You probably know, but FYI support for vlan is one of the few remaining to-be-implemented operators in pflua.\n. The speedups are a pleasant surprise :)\nNote that we found a couple bugs recently affecting e.g. icmp6.   The expander for \"icmp6\" had a bug whereby it expanded to the pflang equivalent of ip6[proto_offset] == ICMP, which had the side effect of asserting that the packet was ipv6 -- meaning \"icmp6 or icmp\" would never see the \"icmp\" side.  It now expands to the equivalent of (ip6 and ip6[proto_offset] == ICMP).  Anyway, details, but you might run into this bug and it's probably worth updating pflua.\n. I think BPF describes the bytecode / VM, more than the language itself.  Dunno.  Of course if there's a name that people use most, sure, makes sense.  But if BPF only describes the VM (and I don't know if that's the case) then it would be confusing to introduce the term, given that pflua in its default configuration doesn't produce or consume BPF bytecode.  Your call!  (Also FYI: am going on holiday for a few days.  See you on Tuesday!)\n. I think I would add a \"-c\" option to Snabb itself (handled in src/core/main.lua), which does a S.sched_setaffinity to the core in question.  Initially we can allow just one core in the list.  WDYT?\nI also want to investigate the impact of S.sched_setscheduler(0, \"fifo\", 1) at some point, but I need to build some metrics to let me know the impact of this.\n. What would happen if the owner goes to alloc a packet and there are none available?  Just return?\nI would also wonder about PID collisions.  A GC would have to be super-reliable, and so you'd have to consider this possibility.  Perhaps instead of PID you could use cookies read from /dev/urandom when a Snabb process starts up, and reify the cookie -> PID association into ramfs at startup so that tools could know what apps are running.\n. Dammit, made this PR against the wrong branch.  Sorry for the noise!\n. The assert on \"shiftleft\" should be\nlua\nassert(p.data + p.length <= p.buffer + ffi.sizeof(p.buffer))\nbut yeah.  What would happen if the result weren't even-numbered?  Error?  Seems a strange restriction; I guess we could check if N is odd and memmove in that case, without cost in the normal case that you're shifting by a known amount.\n. apparently this code is over at https://github.com/dpino/snabbswitch/compare/find_devices currently\n. Why not make \"snabb top\" be able to populate an RRD file?  It seems to me that if we did RRD/RRA writing directly from the data plane, so to speak, that is overhead and complexity that would be nice to avoid.  Dunno.  Exposing raw named counters seems to me like a nice facility that can be built on fairly flexibly :)\n. OK I am mentally working through your list of objections and I see what you mean.  However I don't see how using a different counter format would fix these.  Would you avoid exposing counters to external processes?\n. Ignorant questions: when you copy a packet off of a device using DMA, what is the cache effect?  Is it as expensive as a full cache miss?  Does the memory at the source DMA address get copied into the cache, and if so which caches?  Similarly for writing, does writing a packet to a device via DMA cause anything to be written to cache or does it skip that step somehow and write directly to memory?\n. Thanks for the DDIO link!\nInteresting that keeping the packets in L3 is a function of how much packet data there is.  With average-size 500-byte packets you then have 2000 packets or so per megabyte, and probably you have 2 MB L3 per core... though increasing the number of packets per breath might improve throughput up to a point it's interesting to see that at some point it would diminish throughput by causing more memory traffic for in-flight buffers.  Dunno if this makes sense though :)\n. I should note this is on an E5-2620v3 system, the same as interlaken.\n. perhaps relevant: https://www.mail-archive.com/linux-kernel@vger.kernel.org/msg1031365.html\n. Sadly, if I try with inst_retired.any I don't get anything either on cycles or on inst_retired.any.\n. Aaaaaaaaah, finally got it I think.  We weren't enabling IA32_PERF_GLOBAL_CTRL.  Adding this fixes it for me afaiu:\ndiff\ndiff --git a/src/lib/pmu.lua b/src/lib/pmu.lua\nindex 68fd5f4..581282d 100644\n--- a/src/lib/pmu.lua\n+++ b/src/lib/pmu.lua\n@@ -201,6 +201,11 @@ function setup (patterns)\n    local ndropped = math.max(0, #set - pmu_x86.ngeneral)\n    while #set > pmu_x86.ngeneral do table.remove(set) end\n    local cpu = cpu_set()[1]\n+   -- All available counters are globally enabled\n+   -- (IA32_PERF_GLOBAL_CTRL).\n+   writemsr(cpu, 0x38f,\n+            bit.bor(bit.lshift(0x3ULL, 32),\n+                    bit.lshift(1ULL, pmu_x86.ngeneral) - 1))\n    -- Enable all fixed-function counters (IA32_FIXED_CTR_CTRL)\n    writemsr(cpu, 0x38d, 0x333)\n    for n = 0, #set-1 do\n. Somehow I had gotten this to work on another system without the writemsr patch.  I think it's because I ran perf stat true beforehand, so it already set the PMU MSRs.  That would also explain why the code worked for Luke -- he probably tried using perf on that system already.  Just a theory though, I haven't verified it.\n. Fixed in #672\n. Fixes #671 \n. I think on master this must be working fine.  I was working on an older branch and had back-ported the dynasm update but not the luajit update.  Closing, sorry for the noise!\n. In general, looking pretty good to me!  Some comments above.  Especially looking forward to a version that avoids checksumming :)\nI realize this might not be a thing you can do quickly, but what are your thoughts about testing and benchmarking?  We should put some measure in place to make sure the functionality doesn't regress, and if we can, it would be good to ensure that performance doesn't regress either.  /Cc @lukego for his thoughts there.\n. Yes!  When we were testing, we had a test setup in which one host process was providing the vhost-user interface for two NICs, not via Snabb-NFV but written in Snabb and using the Snabb vhost-user bits.  Turns out the performance was limited by the host and not the guest.  When running under snabb-nfv, we were able to hit our 4MPPS target for the lwaftr.  Excellent work @nnikolaev-virtualopensystems and team!\nThe test application in the guest, which was similar to the DPDK's l2fwd, was also seeing some trace aborts.  It turned out this was something to do with the guest's configuration.  I don't remember the details; @dpino knows more.  Anyway with a different guest program or even with the lwaftr we don't spend any time in the interpreter.\n. Clarification: switching to snabb-nfv in the host provided the speedup because you have to run it in a core-per-NIC configuration, so the host side had two cores to service traffic instead of one.\n. Regarding the return value from the benchmark routine -- I think we are getting into dangerous microbenchmark weirdness territory here.  If the compiler notices that a loop does nothing visible, it's free to remove it entirely, if it can prove that the loop terminates.  Alternately if a statement in a loop doesn't contribute to any output value or side effect, it can be removed or hoisted or any such thing.  These are real transformations that happen in real optimizing compilers.\nThe loops that you test Luke are approximately useless -- their performance will change as LuaJIT changes, and they have no value on their own.  They are useful if you know what you're testing, i.e. you've looked at the traces and the assembly and you know what the code maps to and you're interested in testing some low-level code generation thing -- and I'm sure you know this! -- but that mapping can change over time.  Semantically they mean nothing and so the optimizer is free to remove them entirely.\nThere is also the concern that a benchmarking loop can have a bug.  If you don't check that the function being benchmarked computes the right value, optimizations can break the property being tested.\nIn summary I think it's important for a benchmark to define its expected return value in some way :)  In particular if the return value gets written into the JSON data point, there's no way the compiler can remove the test under consideration.  It has to compute the value, so it has to run the test.\n. I think definitely you will want APIs both that can build a model in Lua and which can take Yang language snippets.  The snippet compiler would use the Lua API.\nI would think that using an external library would be more trouble than it's worth, but that's just an off the cuff reaction :)\n. It seems that having external Yang modules written in Yang syntax with inline documentation would make operators the happiest in terms of saying \"this is what this network appliance does\" and \"this is how I configure this device\" and \"this is what information I can get out of it\".  Of course it helps Snabb people answer these questions too :)\nYang is fine as far as describing the data model goes, but from what I can tell the standard way to do things is to use XML for describing instances of Yang data, which is not very Snabby.  I would be happy if we could avoid XML in Snabb.  On the other hand there is a JSON representation these days, https://tools.ietf.org/html/draft-ietf-netmod-yang-json-06 / https://github.com/mbj4668/pyang/wiki/XmlJson; perhaps that would be not so terrible.  I don't know.  Most JSON tools can't do 64-bit numbers though, so perhaps that's not so useful, though given that a JSON document would have a Yang schema, representing numbers as strings is a valid solution.\nI must admit I much prefer a language that looks like Yang over some generic JSON thing, much less XML.  The Yang language itself is semantically rich but easy to parse, which is the exact opposite of XML.  (I say this as someone who maintains a full XML parser.)\nSomeone will eventually want to hook up Snabb+Yang with Netconf, and there of course we'd need some pretty good XML capabilities.  I think that's not a near-term thing though, so I leave it a bit unspecified.  A first crack at Snabb and Yang would not do XML.\nI think you'd want a Lua interface to look something like this:\nlua\nlocal yang = require('yang')\nlocal schema = yang.load_module('acme-system')\n-- Throw an error if the config is invalid.\nlocal program_config = schema:load_config[[\n  foo bar;\n  baz {\n    qux 42;\n  }\n]]\n-- Use the value of the foo leaf\nprint(program_config.nodes.foo.value)\nprint(program_config.nodes.baz.qux.value)\nprogram_config.nodes.baz.qux.value = 10\nThis is assuming some kind of Yang-like syntax for configuration instances.  Whatever.  I don't know.  I'm pretty sure you will want the config to be available as a normal Lua data structure, nicely nested, but I don't know how the API could be -- you want to be able to nicely reference child nodes and value but also query things like the node's schema.  Not sure.  I think I would use some kind of \"active record\" pattern where writing to a value will validate that value as well.  Of course there are validation steps to take that need the whole tree, so we'd need an additional step there.\nThe yang modules would be somehow compiled into the Snabb binary.  They could be compiled in as source files, or some preprocessor could run on them so they validate at build-time.  The output of the preprocessor would be Lua code that builds a schema using the Lua API of the Yang module.\nThat's fine enough as far as configuration goes, but you also want data as well.  Here it gets a bit more irritating.  The solution we have right now with counters is nice.  I think ideally we'd find a way to write both configuration and state into the trees in the SHM file system and then use a Yang model to interpret those trees.  That way you can still monitor a Snabb process from the outside.  It's attractive too from an eventual Netconf perspective too -- a candidate configuration can be an additional tree.  We can actually store all configurations in the same way NixOS does to enable rollback to any configuration state in the past.\nThere's more work to spec out here: we'd need a standard yang data model for many parts of Snabb (links, counters, etc, even app networks).  I don't fully understand reconfiguration.  And specific programs like the NFV or the lwAFTR will have their own Yang modules too.  So, lots to do here.  Yang is pretty big!\n. Thank you all for this discussion!  I've been learning a lot :)\nI definitely agree with @mwiget and @plajjan that baking NETCONF into snabb is probably the wrong way to go.  There are just too many ways these things could be put together.  At the same time, having a data model for Snabb configuration and state data is really attractive to me as a Snabb programmer.  It's of course necessary from the operator's point of view, but it's worth pointing out that having a proper data model would be useful in Snabb itself.  I know I would much rather publish and rely on a YANG model for the lwaftr that we've been working on than have a bunch of out-of-date, incomplete README files.\nIt's worth mentioning of course that some of NETCONF's operational model might be worth incorporating into Snabb though -- candidate configs, for example.  It might be.  It's a step significantly farther down the line, but it's something worth thinking about.\nI think the type and representation issues that @plajjan mentions are pretty important though.  When you get data in from JSON, you have to know how to interpret that data -- you need a schema of some kind.  For example if you get a string, you should know if it's meant to be a string or a 64-bit integer.  To me this reasoning starts to indicate that we should have schemas and validation inside Snabb.  That way we get reliable operation and nice error messages.\nOne point that I don't see very clearly is overhead when your configuration+state data is large.  For example, in the lwaftr we will have millions of entries in our \"router\" table.  Internally the source of truth is a big flat hash table implemented as a contiguous range of ffi memory.  Probably we don't want to expose a simple \"get\" operation that turns this all into JSON and spits it over a socket.  Probably we would want to copy this data structure into the SHM memory somewhere and then the external entity that translates between Snabb and YANG instance data would have to know a bit more about this table.\nTo an extent you can do this just with exported SHM data.  Of course if you want notifications, you need a socket of some kind like the NetSNMP case.  You probably need the socket for reconfiguration too, if you support candidate configurations and rollback and all those things.\nRegarding other projects, rewriting, and re-implementation: I have an opinion and I would like peoples' thoughts on it :)\nWhere functionality is logically related to the domain of the program (e.g., configuration and state data for the lwatr), I think we should (re)implement the functionality in Snabb.  The point at which re-implementation can stop is the point at which our domain ends.  One way to end the domain is via exposing a standard, \"commodity\" interface to the world -- in our case, data as instances of the YANG data model.  Our domain might possibly go as far as a NETCONF operational configuration model, if at some eventual point we agree that model (candidate, commit, etc) to be the right way to configure Snabb applications, but not the NETCONF protocol or server.  We should aim to share data and protocols with the non-Snabb world but not code.  I don't think, for example, that we should be linking to libyang in any part of Snabb that does domain-specific work, whether the process that's doing the data plane or an external process.  On the other hand if we expose, for example, YANG instance data in a Snabb-standard but in non-XML format, it's perfectly fine at that point to implement a NETCONF agent via sysrepo.\n. Thinking a bit more concretely.  Let's say we incorporate Yang schemas into Snabb in some meaningful way -- we can configure programs in terms of yang data nodes.  We define a standard way to export data types to /run/snabb somehow.  Some data types or nodes can define custom ways they are exported via Lua code and some kind of registry, for example a routing table doesn't need to make a gajillion directories.  All the data is in /run/snabb, you just need to interpret it.\nTo interpret the data you need.... a schema!  So we make a Snabb program that can read the tree in /run/snabb, from a separate process, as a snapshot, using the Snabb data model modules, the same ones that wrote it out.  I have no idea what the concurrency implications are, relative to concurrent modification by the Snabb program.  In some ideal world I would like configuration data to be immutable and persistent like functional data structures -- changing the value of a leaf would result in a new tree that shares state with the previous configuration.  Actually this doesn't need to be in shmfs, as it's just configuration data.  But who knows.  That would mean \"the configuration as a value\" would be a handle on a configuration directory tree.  The tree would indicate its schema at the top, so the external process could turn that into some kind of textual representation.  Of course we could skip the tree and just make a textual representation from the beginning, but perhaps that's not as good as being able to selectively read parts of the tree.  I don't know.  I think in Snabb we will want to be able to reason about configurations from within our domain, so my initial thoughts would be to take this tack, but perhaps there are simpler solutions.\nOf course for state data you definitely want to use shmfs and to mutate values in place, but you don't have the same concurrency concerns -- one \"get\" operation doesn't have to show values all from the same \"time\".  Or perhaps you could lock the tree when making updates.\nAnyway, summary: no socket needed in the base case.  All you need is the filesystem tree(s) and the schema.  An external Snabb process can interpret the tree.  It needs to be a Snabb process because maybe some particular nodes need domain-specific interpretation.  That Snabb process could do a few operations: export the whole configuration tree; export the whole state tree; get a particular value; get some set of matching values.\nA question arises of syntax and language.  What language should this Snabb config/state data reader produce?  What language should the Snabb config parser consume?  Obviously the same language, but not obviously any language in particular.  I think I would define our own language.  Of course as a compiler person that's what you would expect :)  But to me it sounds reasonable.  Something with yang-like syntax, with the yang data model for the instance data, and for the love of god not xml.  The external NETCONF agent if any could handle translation to/from XML, but it would be a straightforward translation -- no need to renest or translate between data models.\n. More concretely more concretely more concretely!  Let's walk through a few operations.\nSay we have a NETCONF server that manages a Snabb lwAFTR instance.  The NETCONF server is written with sysrepo and gets a request for /foo/bar on the lwAFTR using YANG module qux.  Let's say that we add the concept of a \"name\" to Snabb programs -- you can start a program with a name.  If no name is supplied, you use the PID.  OK so the lwaftr was started with --name lwaftr1, so its state data will be in /run/snabb/lwaftr1, and its configuration data will be in, oh I don't know, /var/run/snabb/lwaftr1.  The NETCONF daemon invokes this program:\nsnabb query lwaftr1:qux:/foo/bar\nSnabb query looks for /run/snabb/lwaftr1/schema and /var/run/snabb/lwaftr1/schema for the config and state schemas.  It checks that qux is somehow supported by the schema, and loads up the yang data model.  It then uses the Snabb yang data model library to read the /foo/bar value out of the tree, serializes it to the snabb config language, writes that out to stdout, and is done.\nThe NETCONF daemon then translates that data to XML and responds appropriately.\nIf the NETCONF daemon needs to do xpath matching, it might be able to translate the xpath filter to a filter that snabb query can understand, or perhaps not.  In the latter case, after getting a more general reply from snabb query and converting it to XML, the NETCONF daemon would then filter the XML using XPath on its side.\nSnabb should probably also have the abilty to create configurations via a tool -- you load in a configuration from the snabb config language, it validates it against the model and makes a tree in /var/run/snabb, and optionally updates /var/run/snabb/NAME/CONF_NAME to point to that new tree.  We can version configurations and roll backward and forward, like Guix and Nix do.  Snabb programs can be started up with configuration as a text file, or if they are started by name they can just use the last configuration (or the startup configuration) in /var/run/snabb/NAME.\n. Another consideration.  What if your Snabb program natively implements Yang data model A internally, but externally it needs to provide Yang data model B also?  You might have the same configuration or state data mapping to multiple instances.  You probably don't want the Snabb program exporting multiple trees of identical data in different formats.\nI don't really know the answer to this question.  I would assume the Snabb application would treat data model A as canonical, and some external shim could provide data model B, given a way to translate between them.  Kinda crappy though.\n. Unfortunately I am still not able to use ymm(n) for n >= 8 on this branch.  Perhaps @corsix knows what commits I'm missing.  I did the merge with --strategy=subtree.\nThe error:\ndasm error: variable register out of range. :40\n. OK :) I'll follow @capr's work and close this branch if it's eventually unneeded.\n. FWIW manually examined the DynASM changes and copied them into the Lua DynASM.  This branch seems to work for me.\n. Cc @capr -- should I submit a patch to your DynASM repo?\n. Pretty sure the failing build is a failure of the bot (stale checkout).  I think this one is good to go.  Been using it in https://github.com/Igalia/snabbswitch/pull/204.\n. https://github.com/LuaJIT/LuaJIT/pull/123 for the upstream PR.\n. @nnikolaev-virtualopensystems I don't know; I think the virtio-net issues are in a different place, but I am not sure.  @dpino has been the one looking into that.  I'll let you know if I find something though!\n. Hi :) Discussion veered off into the weeds a bit here on this issue, but I would like to merge this PR to Snabb as a first step towards merging the lwaftr (https://github.com/SnabbCo/snabbswitch/pull/722#issuecomment-179098565).  What's the right way to do that?  Perhaps a special lwaftr integration branch?  I want to make sure this patch is acceptable though before relying on it.  If it is acceptable I see no reason to delay it landing in -next; but I understand if you want to use this as a testing ground for some theory about git branches and network topologies :)\n. I hear what you're saying @eugeneia, perhaps it isn't clear that I definitely made this analysis for our purposes in the lwaftr.  I quote from the comment that I linked in my last note:\n\n[The patch from #707] is not merged yet to our [lwaftr] branch but I was going to merge today. If I don't merge it I have to avoid merging some other things I had planned for today. Upstream discussions are complicated: https://github.com/LuaJIT/LuaJIT/pull/123#issuecomment-172511232. To me in the Snabb context it's an obviously good change; in a more general LuaJIT context I can see that Mike can consider it a somewhat lateral move (better for some cases, worse for others), but unfortunately he didn't comment on an idea for improving it; a bit irritating but oh well.\n\nDo you want to add to this analysis?  Do you want it recorded somewhere?  What's the next step?\nFor me a suitable next step would be just to merge this patch, given that as Luke says, to reproduce upstream issues we need to build from upstream anyway.  But perhaps you have a concrete suggestion.\n. Neat.  I guess this is for test suites and such.  Initial thought: I'm not sure how useful it is, given that when there's a problem you usually want to be able to see the output of a link as a pcap file, and then compare it to an expected output as a pcap file.  In your patch I don't think you get that kind of context.  For that reason I don't think I'd be tempted to use it.\nCosmetics:\nPerhaps it should be named Check or something; \"match\" implies that we match packets and then do something with them.  Keep lines to within the 80-char line limits.  Compare to nil instead of \"nil\", or just test for truthiness.\n. if merged, we should close https://github.com/SnabbCo/snabbswitch/pull/705.\n. s/guy/person/, but yes :)\n. For me, LGTM, I'm happy to merge into wingo-next.  Procedural question: you OK with me resolving the merge conflicts?\n. I merged this onto wingo-next and was waiting for github to mention this PR but I guess I have to mention the PR by name, somehow :)  Anyway, merged, will get in next time wingo-next goes to next.\nI agree with your general anti-portability vibe these days btw! :fistbump:\n. Note that the PodHashMap supports keys of any length.   It has optimized equality routines for keys that are numbers and keys that are 4 and 8 bytes long.  Adding more optimized comparators is easy.\nFor longest-prefix matching, you could look up a number of masked key lengths in parallel using a lookup streamer.  Or you could use binary search, depending on how many routes there are.  Probably parallel lookups of N key lengths are better.\nI suspect the PodHashMap fulfills most requirements.  It's on my list to submit to Snabb core soonish.  Incidentally if you have any opinions about names, they are welcome :)  cdatamap, cdatahash, ffimap, ffihash, podmap, etc...\n. plajjan -- it can make sense though.  If you are working on 32-bit keys and you associate N bytes of data with each key, then you will have (N*8)GB of data.  You can fetch all 32 values in parallel (well, I think L2 can only have 16 reads outstanding on haswell, but that will change at some point, and many of the fewer-bit reads will actually be in l2).  So 6 bytes of data per IP, if mapping L2 to L3 address, then you just have 48 GB of data.  It's a bit much but it's doable.\n. Alternately you could use a hash table for the longer prefix matches, and use an exhaustive packed table for the shorter matches.  That would probably be the sweet spot if memory is at a premium or for routing IPv6.\n. Another possibility would be to store a side table mapping IP to prefix length.  This table would probably be much smaller than the entire routing table, so you could store it as a sorted vector and use binary search.  If there were 256 distinct ranges of IP addresses corresponding to different prefix lengths in the table, then you get just 8 fetch+compare+cmov instructions, and no branches and the whole thing is likely in l2.  Then you use the prefix to look into the hash table.\n. @plajjan so i am very ignorant and I want to make that clear :)  Thanks for your patience :)\nLet's say you have 700K routes.  You put them all in a PodHashMap keyed by lowest IP and prefix length.  On the side, you sort them in ascending order of lowest IP address, treating them as a map from [ip_low, ip_high) -> prefix_len.  You coalesce contiguous ranges that map to the same prefix_len, and fill in any gaps with the outer prefix_len.  How many entries would you end up with?  If it's a few hundred or a few thousand, then you can consider using binary search over that side table, as the lwaftr does to look up PSID parameters (we use the rangemap for this; https://github.com/Igalia/snabbswitch/blob/lwaftr_rambutan/src/apps/lwaftr/rangemap.lua).  If it approaches the cardinality of the routing table itself, it's a lose then.\n. Looks nice.  See also https://github.com/Igalia/snabbswitch/pull/204, specifically https://github.com/Igalia/snabbswitch/blob/wip-multi-copy/src/apps/lwaftr/multi_copy.dasl.\n. Probably needs some documentation.  What kind?\nThis is ported from the PodHashMap that we use in the lwaftr.  We have performance tests but I don't know how to support them in Snabb yet.\nI also removed the streaming lookup bits from this table.  We can add those in a separate PR, as they rely on a couple of dynasm modules.  As you like.\n. Another thing I left out was support for saving the table to disk and loading it back again.  Will do that in a future commit.\n. @alexandergall Here's a test MAC table:\n``` lua\nfunction mac_table_check()\n   local uint32_ptr_t = ffi.typeof('uint32_t')\n   local uint16_ptr_t = ffi.typeof('uint16_t')\n   local key_t = ffi.typeof('struct { uint8_t mac[6]; }')\n   local value_t = ffi.typeof('struct { uint16_t lo; uint16_t hi; }')\n   local cast = ffi.cast\n   local bor = bit.bor\nlocal function hash_mac(key)\n      local hi = cast(uint32_ptr_t, key.mac)[0]\n      local lo = cast(uint16_ptr_t, key.mac + 4)[0]\n      -- Extend lo to the upper half too so that the hash function isn't\n      -- spreading around needless zeroes.\n      lo = bor(lo, lshift(lo, 16))\n      return hash_i32(bxor(hi, hash_i32(lo)))\n   end\n-- 14-byte entries\n   local occupancy = 2e5\n   local params = {\n      key_type = key_t,\n      value_type = value_t,\n      hash_fn = hash_mac,\n      max_occupancy_rate = 0.4,\n      initial_size = ceil(occupancy / 0.4)\n   }\n   local ctab = new(params)\n-- Fill with { i, 0 } => { bnot(i) }.\n   do\n      local k = key_t()\n      local v = value_t()\n      for i = 1,occupancy do\n         cast(uint32_ptr_t, k.mac)[0] = i\n         cast(uint16_ptr_t, k.mac+4)[0] = 0\n         cast(uint32_ptr_t, v)[0] = bnot(i)\n         ctab:add(k, v)\n      end\n   end   \nlocal pmu = require('lib.pmu')\n   local has_pmu_counters, err = pmu.is_available()\n   if not has_pmu_counters then\n      print('No PMU available: '..err)\n   end\nif has_pmu_counters then pmu.setup() end\nlocal function measure(f, iterations)\n      local set\n      if has_pmu_counters then set = pmu.new_counter_set() end\n      local start = C.get_time_ns()\n      if has_pmu_counters then pmu.switch_to(set) end\n      local res = f(iterations)\n      if has_pmu_counters then pmu.switch_to(nil) end\n      local stop = C.get_time_ns()\n      local ns = tonumber(stop-start)\n      local cycles = nil\n      if has_pmu_counters then cycles = pmu.to_table(set).cycles end\n      return cycles, ns, res\n   end\nlocal function check_perf(f, iterations, max_cycles, max_ns, what)\n      require('jit').flush()\n      io.write(tostring(what or f)..': ')\n      io.flush()\n      local cycles, ns, res = measure(f, iterations)\n      if cycles then\n         cycles = cycles/iterations\n         io.write(('%.2f cycles, '):format(cycles))\n      end\n      ns = ns/iterations\n      io.write(('%.2f ns per iteration (result: %s)\\n'):format(\n            ns, tostring(res)))\n      if cycles and cycles > max_cycles then\n         print('WARNING: perfmark failed: exceeded maximum cycles '..max_cycles)\n      end\n      if ns > max_ns then\n         print('WARNING: perfmark failed: exceeded maximum ns '..max_ns)\n      end\n      return res\n   end\nlocal function test_lookup(count)\n      local result\n      local k = key_t()\n      for i = 1, count do\n         cast(uint32_ptr_t, k)[0] = i\n         result = ctab:lookup_ptr(k).value.lo\n      end\n      return result\n   end\ncheck_perf(test_lookup, 2e5, 300, 100, 'lookup (40% occupancy)')\nend\n```\nAdd that to ctable.lua and test on your machines.  Here on this old i7-3770 sandy bridge desktop I get about 50ns/lookup.  This CL doesn't have streaming lookups yet though, so I can't give those numbers.\n. Thanks @eugeneia, that's what I was asking for.  I am surprised a bit by the conventions tho -- to me a PR shouldn't say what the code does, the code should say it, augmented by comments if needed.  This module is lacking the documentation block of course.  I also don't really get the purpose of out-of-line per-function documentation; it is bound to drift out of sync.  We're always spelunking in the code anyway, to me the right place to document is there.  Obviously we disagree about how to do things :)\n. I feel like I'm being lectured-to here a bit and I don't enjoy it.  Respectfully @eugeneia I would have thought that as a person who does code review that you would have followed some of the long, long discussions about ffi hash tables that have taken place over the last 3 months and for that reason we would start from implementation concerns rather than process.  Anyway, thank you for the documentation link, and I look forward to your comments on the code.\n. @alexandergall if you use the streaming lookup, there are no branches at all if you have no hash collisions.  That's because it fetches the entire bucket, so to speak, into cache, then uses a branchless binary search for the hash in that bucket.  (I use the terminology you use in your mac table.)  If you use the API in this PR you can get side traces though because of the linear probe.  On the other hand this API fetches less memory than a streaming lookup, which can make it competitive.  In practice we haven't experienced the branchy loop problem (yet?), and this table is integrated in the lwaftr.\nQuestion for you though: what is your performance target?  Is 50 ns/lookup OK?\n. On an interlaken-alike haswell E5-2620v3, 20 runs of that benchmark I pasted above:\nlookup (40% occupancy): 109.41 cycles, 34.24 ns per iteration (result: 62143)\nlookup (40% occupancy): 108.10 cycles, 34.07 ns per iteration (result: 62143)\nlookup (40% occupancy): 109.62 cycles, 34.31 ns per iteration (result: 62143)\nlookup (40% occupancy): 109.20 cycles, 34.17 ns per iteration (result: 62143)\nlookup (40% occupancy): 109.52 cycles, 34.28 ns per iteration (result: 62143)\nlookup (40% occupancy): 109.35 cycles, 34.22 ns per iteration (result: 62143)\nlookup (40% occupancy): 109.16 cycles, 34.16 ns per iteration (result: 62143)\nlookup (40% occupancy): 109.44 cycles, 34.53 ns per iteration (result: 62143)\nlookup (40% occupancy): 109.65 cycles, 34.57 ns per iteration (result: 62143)\nlookup (40% occupancy): 107.28 cycles, 34.04 ns per iteration (result: 62143)\nlookup (40% occupancy): 109.63 cycles, 34.31 ns per iteration (result: 62143)\nlookup (40% occupancy): 109.41 cycles, 34.24 ns per iteration (result: 62143)\nlookup (40% occupancy): 109.85 cycles, 34.63 ns per iteration (result: 62143)\nlookup (40% occupancy): 101.95 cycles, 31.91 ns per iteration (result: 62143)\nlookup (40% occupancy): 109.46 cycles, 34.26 ns per iteration (result: 62143)\nlookup (40% occupancy): 109.28 cycles, 34.20 ns per iteration (result: 62143)\nlookup (40% occupancy): 102.32 cycles, 32.27 ns per iteration (result: 62143)\nlookup (40% occupancy): 109.40 cycles, 34.24 ns per iteration (result: 62143)\nlookup (40% occupancy): 109.42 cycles, 34.24 ns per iteration (result: 62143)\nlookup (40% occupancy): 109.40 cycles, 34.24 ns per iteration (result: 62143)\n. Hi :)  Picking up this thread.  The reason I initially asked about the documentation format was because there was no obvious place to which to add documentation, and the lib.foo modules in general are undocumented, with some exceptions, and the modules that are documented have their own subdirectories.  There was no example.  I will instead add a lib/README.ctable.md and include it in genbook.sh.  Let me know if there is a better way, @eugeneia :)\n. @eugeneia Anything is possible :)  However it seems to me that it would be worse to require the user to do\nlua\nlocal ctable = require('lib.ctable.ctable')\nthan\nlua\nlocal ctable = require('lib.ctable')\nFor that reason I think a subdirectory is not the right thing.  Thoughts on naming are still welcome though :)\n. I feel strongly enough about the unnecessary module hierarchy that I will resubmit a patch without the additional subdir.  Though it's a factor to consider, to me the documentation build system should not dictate the module hierarchy.\nFeel free to NAK if you also feel strongly about this.\n. So, probably the documentation needs to move; but its final location depends on how it's integrated into the document, so please see the genbook change, @eugeneia.\nThese commits went ahead and added README.ctable.md even after you expressed some disagreement there.  I was going to change, but then looking on how to integrate the docs into the main book I hesitated and left them where they are for now.  I didn't add lib/README.md because I didn't know how to section the documentation for this module, relative to the other modules that are in lib/ but already in the genbook.sh.  To have a comprehensible documentation of lib/ means that to me perhaps we need to not have a src/lib/README.md.\nI anticipate adding a few more of these specialized data structures in other commits, so let's set the pattern correctly here.  @eugeneia for me this PR is ready to go, please tell me the changes I must make.  Thanks! :)\n. For what it's worth I didn't rebase in this PR -- I added commits.  Since they changed the things that @plajjan commented on, github folded thae comments.  I'm OK rebasing or not, doesn't matter much to me, but even in a situation where we do decide rebasing is good, it shouldn't happen while review is underway :)\n. For the lwaftr, things are a bit more complicated.  There is some functionality that we need that is not implemented in this PR:\n- Loading and saving to disk\n- Streaming lookups, which eneeds\n  - binary search dynasm stuff\n  - streaming copy dynasm stuff\nAll of these things exist in the PodHashMap already and have been pretty well tested for our use case and for performance, FWIW.  I have this feeling again that master is receding before my eyes :)\nI have a non-interest in maintaining a general-purpose long-lived data-structures branch -- it's too broad a remit :)  I could do a short-lived branch to change other ctable-like things to use it, though.  Like your data-structures branch but more lowly falutin'.  The risk I see is that I don't want to have to adapt the lwaftr twice: once to the ctable API, and then once again when/if we change it.  That makes me nervous :)\n. @lukego Sure :)  At one early point I had a vision of landing the thing nicely, in a dozen commits or so, all of which making sense on their own: but maybe that is an unattainable thing :)\nAnother question, and it's a current one: we have one change to LuaJIT, https://github.com/SnabbCo/snabbswitch/pull/707.  Not merged yet to our branch but I was going to merge today.  If I don't merge it I have to avoid merging some other things I had planned for today.  Upstream discussions are complicated: https://github.com/LuaJIT/LuaJIT/pull/123#issuecomment-172511232.  To me in the Snabb context it's an obviously good change; in a more general LuaJIT context I can see that Mike can consider it a somewhat lateral move (better for some cases, worse for others), but unfortunately he didn't comment on an idea for improving it; a bit irritating but oh well.\nSo, complicated situation, but we felt comfortable making that change while we had a separate branch.  WDYT about that patch in Snabb itself?  There is a PR here: https://github.com/SnabbCo/snabbswitch/pull/707\n. @plajjan Either way is fine.  There is no pre-supplied hash function for 16-byte values, so you will have to build one, and you can use ffi.cast to access the key in any way you like.  Likewise for equality it will fall back to use C.memcmp instead of inlining the equality check.  Both of these are probably areas for potential improvement, and the dependency is just on the size in bytes and not how it's divided up.\nI would use uint8_t[16] because it works better with other parts of snabb, like lib.protocol.\n. @lukego LuaJIT hashes object by identity, not by content.  The exception are strings, which have an embedded hash:\nhttps://github.com/SnabbCo/snabbswitch/blob/master/lib/luajit/src/lj_str.c#L132\nIt's a Jenkins hash like the one in ctable.lua, but it's not available as a C function.  However writing these functions in LuaJIT is faster than calling out to C AFAIU.  The assembly ends up quite nice, and you avoid the optimization barrier.  Maybe 16-byte values are on the edge of what can be done nicely in an inline way.  We could certainly generate better hash functions though for arbitrary key sizes I think.\n. I don't see how your comment helps, @eugeneia.  I know you're frustrated but I think we're getting somewhere; all discussion has been relevant to the PR at hand.  If you want to review the code, I would be delighted; it's over on the code tab :)  In particular you mention the makefile change.  It's a stand-alone commit: https://github.com/Igalia/snabbswitch/commit/9666348f1a09ae8d8edf58c65382ac6b21b4a14a\n. I also see how it's easy to get the impression that there was rebasing in this PR, and that would be bad.  But there wasn't!  That was just a confusion.\n. Thanks for the review.  In the future, would it be possible to make specific comments as line notes?  That way I can be sure I am addressing your comments.  I will try to respond to some comments in the source and fix the rest.\n. Okeysmokes, I have addressed the review comments, I think.  Please take a look @eugeneia when you can.  Thanks!\nThe open question that remains is which feed to next to choose or create.  Should I create a branch to do this and convert the conntrack module, @lukego?  I'd also be happy to do that as a followup, if you like.\n. Heh, the code isn't all that much, and having a use in-tree would help.  I'll handle the conntrack thing in a followup.  Probably some other things will come first though.\n. What is the status here?\n. Thanks for this!  More consideration of the intent of a PR sounds good.  Since a PR is the only good way in Github to talk about a potential change to code, I don't think using them for sketches or WIP work is mis-use in any way.  It does sound like explicitly tagging a PR with an intent can help the reviewer know what is being asked of them.  I suppose it's possible for a PR to change intents as it goes, so what is being asked of the reviewer can change too.\nTo change the target branch of a PR, you have to close it and create a new one.  To me I don't think it is unreasonable to ask the submitter to choose the target branch appropriately, nor is it unreasonable for a reviewer to see a change and say \"this looks more appropriate for XXX branch, would you mind closing this one and creating a new PR for that branch, see foo.md\".  That would avoid to have to have the concept of topics reified anywhere :)\nOne more concrete question: Is the intention for all commits to go through max-next?  I have been making my PRs against next.  I'm happy to change, though I wonder if we're not putting too much of a burden on @eugeneia to have to be the sole reviewer for all things Snabb.\n. I don't understand the testing story with master and next; thank you for bearing with my ignorance :)  For example, in V8 there is no \"next\" branch.  Everything is done on master, and sometimes things branch off for release.  Before the release, master could be frozen, to allow for more extensive testing.  If I make a change, I make it against the latest code, and I test it before and after.  I validate it and the bots validate it and if it's reviewed it can go in.  If later it turns out to not be a good change it's reverted.  All browser engines that I've worked on work this way.\nOK, so that's not our model: fine.  However I don't really get the testing story for us.  If changes accumulate in these big buffers like max-next, how can I ever hope either (a) to test the code on my own against the proper corresponding tip-of-tree if I base the PR on master, or (b) know that when merged that my code would have a particular effect or not?  I especially wouldn't want the person doing the merge to have to fix things up, because they might make a mistake.  I would prefer them to reject the merge in that case!\n. Basically I get that this branching structure reduces the burden on you, @lukego, and I really respect that :)  I also like that releases from master roll out every month in a really regular way, and I think @eugeneia is doing some great reviewing and release management there.  So as a user of the main snabb trees things are great.  When I try to contribute though I see more and more barriers.  I don't know how to test my changes to make sure they are correct, and now it seems we have serialized the review process through first {Max then you, topic branch then next}, instead of parallelizing.  Dunno.  I am happy to maintain a topic branch but I don't see how my work would avoid conflicting with Max's -- if Max has ownership on all core/ and lib/ changes and also core apps, well, there's nothing else besides specialized apps, right?\n. Yeah I've been trying to type something into this box for a while :)  Instead, some thoughts.  (1) Good that we recognize the current high patch buffering as a problem.  What, for you @lukego, is the right rate at which things land in next?  If I maintained a topic branch I would want to merge to next as often as possible -- if I see something as good for my branch, for me it would be fine for next too.  If I didn't think it was good enough for next I wouldn't land it in mine, and particularly I would want to ask @eugeneia or you for feedback on things I'm not clear on.  If I took on this role for some part of Snabb there would be a lot of this in the beginning, as I cultivate my inner snabbiness :)\nPut a different way: I would always want to be opening merge requests for my topic branch to -next.  I guess I'd make new branches every time I want to do this?  I suppose that would be OK.  If I added some additional commits to wingo-next I'd just make a new branch and PR and close the old one?\n. One thing I like about V8 and related projects is that you explicitly list reviewers.  There they have the concept of owners and you need a LGTM from an owner to land a change, but there are a number of owners for a given piece of code, and you can also ask non-owners for additional review.  I would like it if in Snabb we cultivated a culture of reaching out to particular people for review; it helps grow the set of people who are able to do review.  I'm not sure how to fit that in with a culture of single owners, though.\nI'm open to going ahead with this branch plan but I would like to note some skepticism for there being just one path of people that any particular patch can take to get into mainline.\n. I think a \"safe bug fixes\" branch is probably orthogonal to topic branches -- though how would a safe bug fix flow?  First to master, never to next?  First to master, next merges in master sometimes?  First to next, then to safe bug fixes via a separate merge?  Unclear to me.  But OK, we can figure out something :)\nI am also not sure that such a conceptual distinction as \"new features\" vs \"extensions\" is a good way to partition PRs.  If anything, I would want the domain expert of an area to review both a feature and any change to it.\nRight now @eugeneia owns the whole tree, and that's great :)  He's a great reviewer and experienced snabb hacker.  I don't want to suggest taking away any of that from him!  But yet, that seems to be what you are proposing @lukego, and so I don't really know how to proceed.  Please excuse again my ignorance; I am much more used to models of shared ownership :)\n. Hmm!  I guess I don't know what to say.  To me if I don't have either they ability to tell someone \"LGTM, go ahead and merge it\" or \"LGTM, I will merge it\", I don't have ownership -- I can have input or whatever but it's not the same and I don't get involved in the same way, and actually I step away because I don't want to give conflicting input, unless I feel strongly about the thing.  The PR about the \"match\" app was such a case -- I realized I didn't own the thing and that by making any feedback I was just muddying the issue, even before the PR had any other comments :)\nMore concretely... if we were to be dividing up PRs in some way that reflects the content of the PR and not who decides to take it first / feels they would be fine reviewing it, to me that's dividing up ownership, isn't it?  Dunno.  I certainly wouldn't want to get into a conflict about whether my branch is or isn't an appropriate way for a patch to enter the mainline :-P\n. I like this last idea.  Say, a reviewer will be randomly assigned for new tickets if no one is @-tagged in the PR description.  They can be assigned by @-mention or by the assignee field; either way.  Of course an assignee can pass the buck to someone they feel is more appropriate.  I would be happy to maintain such a branch.\nI really do lament taking up your time on these procedural things.  Hopefully it will pay off :)\n. A clarification!  You note in https://github.com/lukego/blog/issues/14 that V8 does not do merge commits: it always rebases.  Same for Firefox and Safari, FWIW.  When you go to land a patch, the bot that handles it will rebase onto the tip-of-tree.  In https://github.com/SnabbCo/snabbswitch/issues/725#issuecomment-174184755 I expressed some concern about testing and you rightly said that CI is important here.  However there are some problems which you can only detect after the fact: increased flakiness after commit X, a performance decrease after commit Y that needed manual inspection or which only occurs on platform Z, etc.\nAdopting a merge-based development pattern doesn't necessarily tank this metrics-based post-commit QA approach, but it can be really negative if the branches buffer lots of commits.  If I see that before merge X, things were 10% better in some way, but merge X pulled in some important patches, what do we do?  Following metrics across multiple branch heads is hard on the brain, so I assume we'd only have good metrics on master.  We wouldn't know before the merge that the merge had bad perf news....  Dunno.  We can make it work :)  But it sure would be nice if in Snabb we would be able to say \"Commit XYZ slowed us down / introduced flakiness / etc.  Please back it out.\"\n. Yes, I didn't mean to be too negative :)  However I think we probably need to look beyond the kernel when it comes to post-commit monitoring of the performance or stability impacts of changes.  My impression is that the kernel has pretty good patch review but terrible QA.\n. I respectfully disagree that it is necessary to attach any specific kind of header to a file in order to indicate its license.  The example header that you quote is just an example from a non-normative appendix.  We can make other decisions.\n. As an external resource, I think https://softwarefreedom.org/resources/2012/ManagingCopyrightInformation.html is fairly sensible.  Snabb currently uses the centralized copyright notice; scan down that page for \"centralized\" for more details.  We could add file-level notices referring to a top-level notice, but this is not required.  All options that I mention in the issue description are available to us.\n. > Silly question: what difference does it make who has copyright on what line of code?\nWell :)  Let's say I do some work on Snabb for Microsoft.  (I haven't; it's an example.)  They agree to submit the result upstream under the apache license (yay) but they keep the copyright (well OK, doesn't matter to me, the copy in Snabb is Apache-licensed).  Now, as copyright owner, they can relicense their code.  If I were owner, they couldn't.  Being able to relicense your code can be important.  Maybe they wanted to re-contribute that code to an MIT-licensed repo of Lua code.  Dunno.  Lots of possibilities.  Some parts of Snabb aren't very Snabb-specific and can stand on their own.\nAssigning copyright to a customer is a common contractual obligation, even when releasing the result as free software.  As a Snabb contractor I am under some of these obligations right now, even.\n. Thanks for this discussion.  Having a spelled-out norm will help future contributions go more smoothly.\nI think that for the same reason that we should avoid per-file copyright notices for individuals, we should also avoid them for corporations..  My reasoning is that per-file notices establish a kind of ownership that is negative to the project.  Someone could get resentful if BigCorp dumped some code on a project but then Jane Hacker was left to maintain it.  Also, having notices just for corporate contributions values corporations over people.  Finally, if we have a policy of avoiding copyright notices in files, we can avoid this discussion in the future always.\nI do think a central COPYRIGHT file is a good idea though.  Ideally it would start by listing all contributors, as determined by git, and then add on some optional notes about how work by Jane Hacker in 2016-2018 was copyright Big Co Ltd.\n. @nnikolaev-virtualopensystems the question of copyright notice policy is precisely what this issue/question is about :)  I have argued that any per-file notice has a negative effect on the project; if you propose to allow notices, you would need to argue for them instead of re-stating the question :)\nLet's cut to the chase though: companies want to put their name out there.  I'm pretty sure that's why you want per-file notices.  But it's bad for the project, I think (reasons above) and not necessary for companies either: your company was hired to do virtio-net in Snabb not because of copyright notices, but because your company does respected work in this field, and everyone who is capable of doing a git log knows this.  You don't need to stamp an immutable line in a source file to get the word out!\n. Hi :)\n\nFirst request: how do we define the \"downstream\" branches that feed into max-next and wingo-next?\n\nLet's list this in branches.md.\n\nIf both branches will be processing PRs sent to the main Github repo then what is the \"sharding function\" (\"routing table\") for unambiguously deciding whether a given change is for max-next or wingo-next?\n\nI think if the committer wants a particular path they can choose to @-tag one of us.  Otherwise ideally we get SnabbBot to @-tag one of us at random.  Until we have that kind of bot integration, I think the first person that comments on the ticket and says \"I'll take this one\" gets it.  WDYT @eugeneia ?  I'm happy to share any work that you would like to share.\n\nThe Linux kernel has the MAINTAINERS file that defines the \"first hop\" for new incoming changes. We would seem to need some equivalent. I would like contributors to always know what is their next-hop\n\nI guess we add this to our new contributing.md that we were discussing in #729 will have this.  I can add that in a separate PR, if that works for you.\n\nI would prefer not to pull the same changes from both max-next and wingo-next.\n\nI was going to initially suggest that the first few wingo-next merges go through max-next, to improve review quality; wdyt?  I'm happy either way.\n. > Then we would have the simple flow:\n\nmaster <- next <- max <- wingo\n\n\nIn the beginning, sure.  I expect after the first handfull of merges I would want to go directly to next.\n\nI am more cautious about inventing new protocols for coordinating branches. Then the analogy is to more complex mechanisms like OSPF, BGP, VRRP, etc that multiple routers need to support and interoperate with. I feel like it is too early to be dealing with this kind of complexity in the small network that we have for the immediate future (less than 10 nodes compared with thousands for Linux or millions for the internet).\nSo I would request that we focus on innovative workflows within branches but make the interactions between branches as boring as possible. Reasonable?\n\nI lost you here :)  I think based on your following comments you are suggesting that @-tagging a reviewer is too complicated.  However this goes against what you were saying in https://github.com/SnabbCo/snabbswitch/issues/725#issuecomment-173973826 and which I agreed with.  For all of the reasons that I mentioned in that PR, I do not think partitioning the snabb tree into owners is a good idea at this time.\n. A couple final questions.  One, I see there is a SnabbCo/max-next.  I could manage a SnabbCo/wingo-next but that sounds unnecessary.  If it's just in wingo/wingo-next then maybe the second \"next\" is not needed.  Is wingo/next a fine name?  If so I can change the branch name then.\nTwo, what do I do with my commits?  I propose that I add them to wingo-next only via PRs.  I can get anyone to review them.  In particular I want to lean on @kbara  and @dpino here.  If I feel that I need feedback from @eugeneia or @lukego I will tag you and/or make a PR against max-next.\n. Thanks for the merge, let's give things a go!  Wheee!\nHowever!  On one point I think I didn't express myself right, when I said:\n\nTwo, what do I do with my commits? I propose that I add them to wingo-next only via PRs. I can get anyone to review them. In particular I want to lean on @kbara and @dpino here. If I feel that I need feedback from @eugeneia or @lukego I will tag you and/or make a PR against max-next.\n\nMy question was about code that I write, not code that I merge in.  I assume that for code that I write I need some kind of review process.  What is that process?  Above is my proposal.\n. Since both I and @eugeneia can use the assignee field, we can use that.  There are only 19 PRs so we can probably just go through and assign ourselves.  It makes some weird incentives though -- some part of me wants no open PRs assigned to me, but obviously it's the submitter's job to make a nice PR.  I guess we should feel free to close PRs that are going nowhere, knowing they can be re-opened.\n. Are we OK in the long term with the existence of both core.lib module and the lib module tree?  If no one else finds this confusing, I will ignore it :)\n. I do not like being negative but I am not so sure that this is a good idea :)  If we move the implementation of our code to this module layout, great.  However anything that adds an abstraction between what code I'm using and where it's implemented is less great.  When I see that a module does require('lib.foo') I know where to go and look for that code; if it's some synthetic API, not so much.  How we build that API could mitigate that, but I wonder if the extra abstraction would not be a mistake.\n. > Did anyone ever manage to speed up a loop by caching loop-constant things in locals?\nYep, quite often.  The reason: loop-constant things are indeed hoisted to the loop pre-header, so in the main loop trace you don't need to reload them.  But if you have a side trace, it rejoins the loop at the pre-header, and so has to evaluate the invariants again.  Manual hoisting avoids a penalty for side traces.\n. IIRC L2 can only have 16 outstanding misses on Haswell:  http://www.realworldtech.com/haswell-cpu/5/\nI think could be the throughput bottleneck when working on data outside of L2.\n. It would be really great to have this app :)  Right now we are landing changes to the virtio-net interface without the benefit of CI or benchmarking, at least in core; sub optimal!\n. Annoyingly, this fix doesn't work on GuixSD, where they don't have /usr at all, not even for /usr/bin/env.  They do have /bin/sh as the only thing in /bin though; I don't know why they make that exception.  Anyway this fix LGTM.\n. Hah, I wasn't thinking, sorry :)  Will take some time to get my merging hat on.  Luke does the PR policy apply to you too, or do you also have a next branch?  I am happy to take this one as a first merge.\n. OK, to be clear in the future for this commit I should have said \"LGTM, I will merge onto wingo-next.\"\nTo reply to @domenkozar, having to think about whether a script is bash or portable bourne shell is a bigger change I wouldn't feel comfortable doing.  (fwiw /bin/sh is bash on guix but that is not relevant.)  I think I was just griping, not adding value to the discussion :)\n. Thanks for the note, I would have done that except that I was in the middle of another change.\n. Nice!  Before, I was hesitant to use the name \"Snabb\" because of potential conflicts with SnabbCo; having some clarity will help the project I think.\nRegarding Snabb Foo and branding, the points you mention make sense to me.  I might relax them to be a bit more aspirational -- e.g. someone could start their project called Snabb Foo with an aspiration to be small/compact, sync with master, etc, but if it's early days you never know how the thing is going to go.  Apache licensing should be a must, though.\nRegarding other uses of the word Snabb -- I don't have experience in this domain, but https://wiki.gnome.org/FoundationBoard/Resources/LicensingGuidelines is a not-long document that can hint to what problems might arise.  Have you registered the trademark?  I guess it's a good idea, though who knows :)\nSpecifically: we should encourage the use of Snabb in open source product names which meet the Snabb Foo criteria above.  We should prohibit Snabb in company names  (besides yours, if you decide to keep it) or in proprietary product or service names.  We should prohibit \"snabb\" in domain names, besides snabb.co, perhaps unless they are subsidiary to something else (e.g. snabb.microsoft.com).   We should adapt this rule: \"Do not use GNOME Trademarks more prominently than your company, product or service name.\".  Probably we should prohibit creation of a Snabb logo, without authorization from the copyright holder.  Maybe this is too much, feel free to take/leave/modify/etc :)\n. Yeah probably we should include some implementation notes as well, for example how to provide this api using lib.lua.class and so on.  No need for urgent changes, but agreement is a good thing to look for :)\n. > I will advocate for Occam's Razor i.e. the technically simplest and most minimal solution that actually covers our requirements. If that requires procedures, classes, subclasses, class methods, multimethods, predicate dispatch, etc, then so be it. But I would presume everything to be unnecessary complexity until proven otherwise.\nI think this is a straw man argument :)  This PR advocates attaching FFI metatables to data types, that's all.  It's done in in such a way that you can use functions or methods, and the resulting procedural interface is harmonious with those in core.packet etc AFAICT.\nHowever I would like to go further, and to advocate globally for preferring method syntax where it's obvious what the data types are.  For example if you know P is a packet, then use p:free() rather than packet.free(p).  You might choose to use packet.free(p) if you feel that it's not obvious that P is a packet and you want to help the reader know where the free function is coming from.\nThe reason for this is to reduce interface ceremony.  As a use example:\nlua\nlocal histogram = require('core.histogram')\n...\nlocal h = histogram.new(1e-6, 1)\nh:add(42)\nfor count, lo, hi in h:iterate() do\n  ...\nend\nIt's clear that h is a histogram, so the method syntax reduces the degree to which the user is battered with the word \"histogram\".  Compare to:\nlua\nlocal histogram = require('core.histogram')\n...\nlocal h = histogram.new(1e-6, 1)\nhistogram.add(h, 42)\nfor count, lo, hi in histogram.iterate(h) do\n  ...\nend\nor worse:\nlua\nlocal histogram = require('core.histogram')\n-- someone will do this eventually\nlocal histogram_add = histogram.add\n...\nhistogram_add(h, 42)\nor worse:\nlua\nlocal histogram = require('core.histogram')\n-- someone will do this too\nlocal add = histogram.add\n...\nadd(h, 42)\nAnd of course if there's no local require, like #734 proposes, then there's no ceremony at all:\nlua\nlocal h = snabb.histogram.new(1e-6, 1)\nh:add(42)\nfor count, lo, hi in h:iterate() do\n  ...\nend\nAnd it's all super-low-tech.  I find this very snabby but others may disagree :)\n. cc @kbara for her recent document where she pulled in some machine-level performance tuning suggestions from @mwiget \n. Haha, I neglected to see that the ticket was made by @kbara :)  Ignore me please :)\n. Thanks!\n. To me it's not a failure to mention someone who's not a domain expert in the code at hand, dunno.  I think mentioning you @eugeneia is a success in some way, as you actually maintain an integration branch, and it's conceivable that alex might also have a good comment as an experience snabber.  Your mileage evidently varies :)\n. OK I now side with @eugeneia :)  At the very least if the original issue or PR @-mentions a person, having mention-bot chime in is not necessary and a bit annoying :)\n. @eugeneia sure, i'll use it :)  thanks!\n. Gaaaah, VLAN filtering only works if VMDq is enabled also.  The documentation does not mention this!\n. AFAIU, without VMDq, an 82599 app will accept any incoming MAC address, and will not stamp outgoing packets with the MAC, and won't insert a VLAN tag on outgoing packets either.\n. Apparently app:report() on intel apps also needs vmdq enabled to work: https://github.com/Igalia/snabbswitch/pull/260\n. @kbara @lukego I was going to say exactly the opposite!  That documentation was incorrect, incomplete, and misleading!!! https://github.com/SnabbCo/snabbswitch/issues/749\n. I would have been much better off reading the source code than the documentation.\n. I don't think reviewing our own patches is a pattern we should be encouraging :)  @kbara that would mean you or me would be the next hop for this one.  I do not like the change as it stands, which made me hesitant to take responsibility for getting it in (or keeping it out of course).  OTOH if you would like to take it @kbara I will not NAK it.\n. @lukego: For breathe() latency I have https://github.com/Igalia/snabbswitch/pull/256.  Will refactor into something mergeable soon, as we need to be able to use it in delivered applications.\nRegarding documentation and source files: I really like this style of things.  It is close to the code so it is unlikely to drift out of sync (and it's easier to review for correctness), it is readable as-is as text and it can also be extracted.  I wonder though that the quality of the markdown would slip -- i.e. if reading it on github doesn't highlight rendering faults, then we will tend to make more faults.  Not sure what the solution is there; it's not something that's amenable to automated checking, though automated generation of course is a good idea.\n. Relatedly, see https://github.com/Igalia/snabbswitch/pull/276.  The kind of telemetry that I am interested in is often real-time in nature, but I guess that's related to the Yang discussion.\n. Nice!  Have you noticed any particular performance impact?  What about declaring memory dependencies/effects of DynASM blocks?\n. \nAbove is a diagram of what happens to incoming packets, from the 82599 datasheet.  I think the feature we need is called RSS, or receive-side scaling.  The traffic is dispatched to different pools according to a hash function of its contents.  The RSS hash function is specified by Microsoft, AIUI: https://msdn.microsoft.com/en-us/library/windows/hardware/ff570725(v=vs.85).aspx.  The benefits of hashing compared to simple round-robin distribution is that multi-packet operations like reassembly will be possible in the sub-processes, as the packets that are related to each other are all directed to the same sub-processes.\n. Probably the right thing to do is to initialize the NIC only if needed.  If it's already in VMDq mode, we don't initialize it, and instead add a pool to the VT pool list, whatever that means :)\n. Thanks for the thoughts @lukego .  The immediate motivation here is for 10Gb NICs actually -- looking for a short-term win for the lwAFTR workload to give it some headroom on small packets.  In that regard, from the outside the number of Snabb processes servicing a NIC (or in our case, pair of NICs) should be moot -- whether we handle traffic with one or three processes, no problem.  That generalizes to the 100Gb workload of course too.  However, what you say about wanting ARP/ND \"in the firehose\" so to speak is true and it complicates the plan to just run any number of worker processes to service a NIC(-pair).\n. Cc @dpino\n. Thanks for the tip.  For some reason I thought we would need to enable virtualization, but perhaps that is just an artifact of how the 82599 driver is written and not an essential thing.  With world enough and time I would spend a couple days refactoring that driver to be more like the intel1g code :)\n. We have about 1.8M, of which a little less than 1M are pcap files, in program/lwaftr.  Some 600K are images for benchmarks.  That might impede a merge, FWIW.  They are not amenable to the \"packetblaster synth\" treatment, as many of them are error cases and so on.\n. Regarding lib/README.md, it was requested explicitly by @eugeneia in #722.  I still think lib/README.module.md is nicer for browsing on Github, because as you say it is a bit overwhelming to have everything in one vertical scroll, and it seems that GitHub doesn't do tables of contents in markdown files.  Dunno.\n. On Mon 22 Feb 2016 15:10, Max Rottenkolber notifications@github.com writes:\n\nHowever: I really think we need to nail down which user experience\nwe are trying to optimize foremost.\nI don't think we can get around providing standalone HTML/PDF/EPUB\nmanuals as the primary target as this is the de-facto standard as far as\nI can tell. The upside is that this doesn't conflict with organizing the\ndocumentation in a GitHub friendly way at all. So I think we can\noptimize organization for GitHub and render a standalone manual\nindependently without restrictions that arise from this.\n\nWhy not just files in markdown that are meant to be read in github?\nPersonally I don't see the need for PDF or EPUB at all.  Markdown\ndocuments are easy to improve (once you have a branch with the basic\ncontents, you can edit online in github to make sure the formatting\nlooks right; a fairly low-latency cycle) and on github they link really\nwell to other files, can embed images natively, and ascii art is just as\ngood as the generated graphs.\nI tried hard to avoid giving my opinion but I seem to have failed.  Feel\nfree to ignore me :)\n. FWIW, core.lib does import lib.lua.alt_getopt.  Dunno if bug or feature!\n. FWIW @alexandergall  you will probably want to wait until the streaming interface to lib.ctable lands before porting; otherwise you may hit the branchy loop problem.  Haven't prepared that yet, probably within a couple weeks though.  FYI :)\n. Merged, thanks.\n. I was once of your opinion, @nnikolaev-virtualopensystems :)  And still my heart goes that way.  But for big apps with a lot of history, I think it can be sufficient to review the changes the branch brings relative to master, understanding that the more \"private\" files in app/foo and program/foo are more under the control of the \"foo\" project than Snabb itself.  It would seem to be true that this strategy makes git history less useful, but as long as the merge doesn't bring unwanted changes to non-foo parts of Snabb things should be OK, I think.  Just a thought :)\n. Regarding squashing: in the lwaftr we squashed once, before the initial merge, but will not do so again.  It's merges from here on out.  It is, as you say @lukego, very much the one time in which you throw away history.\n. Also in the lwaftr, any pull requests or other work that were outstanding now need to be rebased onto the new lwaftr branch.  The old branches are dead, long live the new HEAD!\n. LGTM as well.  (I still am not clear about who is the \"upstream\" for patches written by @eugeneia, @kbara, or me; I guess in this case with a positive review we should feel free to land them on our own branches?)\n. avec plaisir :)\n. There are a number of modules that require the freelist module:\n./core/packet.lua:8:local freelist = require(\"core.freelist\")\n./apps/intel/intel_app.lua:10:local freelist = require(\"core.freelist\")\n./apps/basic/basic_apps.lua:4:local freelist = require(\"core.freelist\")\n./apps/solarflare/solarflare.lua:4:local freelist = require(\"core.freelist\")\n./lib/virtio/virtq_device.lua:6:local freelist  = require(\"core.freelist\")\n./lib/virtio/net_device.lua:6:local freelist  = require(\"core.freelist\")\n./program/snabbmark/snabbmark.lua:8:local freelist      = require(\"core.freelist\")\nBut AFAICT only the packet module uses it.  @xrme, if you have no plans to use the freelist module more widely, would you mind inlining the freelist module into packet.lua and removing freelist.lua?  Then we can get rid of the ffi.typeof of the tagless struct to avoid this hazard in the future.  Probably you will have to update the documentation as well, but it will just be deletion.  Thanks!\n. Regarding the perf regression, I only see that on the snabbnfv-iperf-1500 case, and then only sometimes.  That case seems to have a large standard deviation so perhaps that's normal.  I don't suspect this change as having significant perf impact.\n. @nnikolaev-virtualopensystems regarding alignment, since on this freelist we're always reading and writing on the back, and we expect the freelist to never be empty, I don't see an immediate way that differing alignment can help; do you?  Happy to learn :)\nRegarding perf, I'm always happy for more perf :) but @xrme is in the middle of a refactor to pull multiple packets off the freelist at a time, from assembly, promising some significant gains; with that in mind I think it's OK to keep the focus on simplicity for now :)\n. Neat feature, thanks for explaining it!  I had no idea :)\n. Merged.  If you have anything you want added, let's do it in another PR.  Cheers :)\n. Originally from https://github.com/Igalia/snabbswitch/pull/276.\n. Snabb top has been really useful to us -- for example if we are experiencing packet loss, we need to find where it happens: between which apps, or if not between any apps then on ingress.  It helped us have insight on whether to apply backpressure or not (usually we don't, now, following #656).  It's good stuff :)\nI can look at switching to use core.shm.map; good suggestion.  I guess by default core.shm.map should clear the file if a previous PID had it open, though.\n. I can believe that.  I feel like sometimes if we start taking too many side traces, LuaJIT doesn't react appropriately, and if you do a jit.flush() things get back to good performance.  Something to look at in the future.\n. Sure, or apps.rate_limiter; dunno.  Or, perhaps we could add a mode to the RateLimiter that doesn't discard packets.\n. Somewhat related to the #656.\n. Closing this one; #793 is the successor.\n. I have rebased this work onto max-next and am going to open a fresh PR so that we only see the changes relative to max-next.\n. Related to #749.\nFor me I am OK merging this as it fixes a bug, but ideally I'd prefer a patch that adds support for mac address filtering to the SF interface.  LMK, otherwise I merge this later.\n. Looking good.  Would you mind changing \"promisc\" to \"promiscuous\" throughout, and changing set_promiscuous_mode to take a boolean argument instead of having the separate unset function, please.  Thanks :)\n. I don't know that this is a bug, fwiw.  Lots of things in Snabb are unsafe in general although they may be safe in particular.  As a user of shiftleft I certainly wouldn't want to propose adding an assertion to it without analyzing perf impact.\n. @lukego No argument there; this bug is to add assertions, which is fine but requires an analysis of the perf impact IMO.  Just trying to avoid the situation where we could merge in the next release and have to spelunk a perf regression.\n. On my side I apologize for FUDding the issue -- I know it can be irritating when you just try to make things better and people come with all kinds of questions and no data.\nI would like to avoid a pattern of performance degradation over time though and I can see that happening if we start trying to \"add\" some degree of assertions/safety to unsafe interfaces with which people have already tested their performance targets and for which there are no automated perf tests.  I also don't think our perf infrastructure is precise enough to track 0.5% performance losses -- correct me if I'm wrong.  In the web browsers world we do sweat 0.5% perf losses because it's easy for them to add up to a big loss over time.\nHow about, if someone wants to add safety to any packet interfaces, I agree to test them for performance on my workload and give you good feedback :)  See you on the PRs, then?\n. For what it's worth, LGTM\n. Morning :)\nI agree that in general we do not want to apply back-pressure.  However for synthetic loads, often you do.  I would agree with the sentiment @lukego that this functionality could belong in a specialized app, but there are a few of these apps already -- the repeater (when joined with the pcap reader), the packet generator in snabb lwaftr, and I think Marcel has one in Snabb VMx.  This functionality should exist in one place.  Whether it's an app or a library is not so important to me, but adding the functionality to RateLimiter does seem to work.\n. OK.  I will change this code to require rate and bucket size even when it's not necessary; a true grumble here but whatever.  (I feel like my use case is not being considered as being as good or as important, and that the existing use case is forcing my code to go through ceremony for no benefit.  The bucket size in particular is a useless knob for synthetic workloads.)\n. Welp, I guess we'll just keep this module local to the lwaftr then; I don't need this on the blocking path to a merge and we can move things around later I guess.\n. I guess, if you say so ;)  To me the discussion has shown me a few things.  I should never treat a merge as being a no-brainer; that there are many people who can say no, and no one to say yes; that there are many use cases and the ones that are already in git come first; that if one reviewer suggests A and you do it, that another will come later and say that the original thing was better; and so on :)\n. I think @lukego that your input was timely, much better now than later.  To me I see the fundamental conflict as being that Snabb was made with some apps shaping its development, and that extensions proposed during the course of new development might not be as well-suited to the needs of those apps.  In this concrete case, apps which generate synthetic test workloads do not have the same requirements as apps which limit real traffic.  They differ in approaches to backpressure but also as we see in default values (acceptable in one use case, apparently not in the other).\n. Of course it's harder to understand how a new use case could affect existing core code when you can't see how it affects the new code as it's not in git yet.  I think it's a definite best practice to only touch your app/ and program/ directories, and I think we would have avoided a lot of problems in the lwaftr upstreaming if we did that.\n. @lukego is backpressure never appropriate for test generators?  Seems silly to include rate-limiting functionality in all test generating apps.  Dunno.\n. @lukego ok :)  bear in mind that the future of Snabb includes a RateLimitingPcapReader and a RateLimitingAndRepeatingPcapReader :)\n. @eugeneia are you sure?  255*1500 suffices to rate-limit to anything between 0 and 10Gb in our test harness.\n. I think rather that a bigger bucket just allows for spikier traffic.  E.g. if you limit to 1Gb and your bucket is 1Gb, you could push 10Gb for 0.1s and 0 for the rest, and you still fit under the limit.\n. Hi :)  Good morning, I like the musings.  I admit that it can be frustrating (to me) when a PR causes a bunch of musings ;) but I prefer that we share a common understanding of the goals and patterns of the project and this seems an OK way to come to it.\n@lukego i think you will find that the concrete approach you are suggesting for rate limiting won't work, because the rate limit is on bytes and not packets.\nI think the logical end of your hack-direction is to remove the repeater also, and any other synthetic-test-related middlebox.  It could be this is the right thing for production data-planes, but it will make writing test applications much harder -- backpressure is perfectly fine in those situations and building app networks is an expressive way to compose a compound network function.  Removing backpressure entirely will leave a hole that should be filled by some pattern :)\n. > E.g. before you could first map an object read only, then another process could map that object, write to it, and it would work. Now you need to make sure that you open an object only after it was created by another process. Correct me if I am wrong.\nThis is correct.  It could be that we always want to be using create().  It seems wrong to me though in some deep way :)  Like, a process that is just reading data doesn't want to create it if it doesn't exist.  The fact that a counter doesn't exist could be interesting information that would be perturbed by the reader.  For example if there is counter data that has some kind of complicated interior structure I would want to be sure to initialize it correctly; dunno.\nConcretely snabb top will show latency information if the latency histogram is there, and not otherwise.\nPerhaps this is worrying too much though; other thoughts are welcome.\n. Added on a couple commits I found while porting the histogram to use this.\n. I would like to have the histogram stuff off my plate by the end of the week, so review here would be appreciated @eugeneia :)\n. Thanks for the review.  I am not sure how snabb top would break; it does seem to work for me in #795, but I will check tomorrow to make sure.\n. Hmm, OK.  I don't see the change you refer to in #795; would you mind pointing it out?  I do try to keep these things atomic but I occasionally make mistakes :)\n. Thanks very much for the review and merge; cheers :)\n. Immediately updated via force-push as I had some development detritus around, sorry for the confusion.\n. Ready for review.  Thanks in advance :)\n. Good q.  The reason is that because the var_1 variable is threaded through the iteration, it can act as the functional state for the iterator.  If the iterator function doesn't have free variables (which in this case it does, due to \"prev\"), that means I think that the loop can be better optimized.  But that requires the counter or the iteration state to come first.\nI agree that count is more interesting than the index.  Probably just as interesting as lo and hi though.  Given that none of the use cases currently use \"index\", and \"index\" can be calculated just by incrementing a variable in the loop, perhaps it's better to do lo, hi, count then and keep more state in the closure; wdyt?  Or count, lo, hi?\n. FWIW the SHM stuff is being reviewed as part of #794.\n. Updated to address feedback.  Thanks for the review!\n. Hehe :)  I admit, I really like common lisp's keyword arguments :)\n. @lukego Seems to me the way in which histogram differs from counter is that histogram offers the OO metatable.  However like counter it also provides the functions.  You do not need to use method syntax.  WDYT about that style?  Perhaps these musings should go to #740.\n. Would you mind providing a configuration parameter, \"overwrite\", which controls whether you get w+ or w?  I am thinking that probably not overwriting should be the default, for safety's sake :)\n. AFAIU with this design, you could connect directly to the NIC if you wanted to handle all of its traffic, so no difference from the current status.\nJust brainstorming the sorts of things we'd need to write if we do this :)\n- A software version of VMDq, to start with, to replace the current functionality of the Intel82599 drivers.  It should dispatch to a number of queues via VLAN or MAC address.\n- VLAN stripping / tagging, and stamping of outgoing MAC addresses.  Should this be done by the workload program / app network I guess?  An open question.\n- There is some strange mirror facility in the Intel cards that might also need to be replicated; not sure though.\nWould we use virtio-style queues between processes or something bespoke?  I don't know the tradeoffs.  I would lean towards bespoke, of course :)\nSome things that this would enable us to have:\n- Same queueing / dispatch functionality with all NICs.\n- Easier custom queueing / dispatch (e.g. you could transparently do dispatch on L3 header fields in flexible ways).\n- The machinery involved would be re-usable to break program networks into more processes, which might help scaling.\n- On the other hand, this facility might allow for more optimal resource usage.  Let's say you have an app which can process 25 Gbps.  If you run it on a 10g port you're wasting cores; if you run it on a 100Gbps port you can't keep up.  By allowing us to divide incoming traffic into a queue count flexibly, in a way that suits the performance of the workload, we might be able to use cores in a better way.  Of course this goes the other way too, for when an app can only handle 5 Gbps and you need more horizontal scalability.\nNeedless to say I like it and would love to have this as part of the Snabb story, contingent on it actually working :-))\n. Are you thinking of copying packet data from the dispatcher to sub-process ring buffers, or sharing packets in some global pool and passing around descriptors instead?\nc\nstruct ipc_link {\n  struct packet packets[LINK_RING_SIZE];\n  uint64_t read, write;\n};\n^ that would be a delightful link, if it performed ok :)\n. Though, I guess since you can't write a packet atomically, you run the risk of corruption.  I suppose you could detect that via how far the write pointer advanced while you were reading...  a big challenge in any case :)\n. Regarding process, sounds about right to me.  It's a little inconvenient in some ways, because usually you need the lwaftr to test the change, so until now it's been rare that you can make the change directly on master and also test it.  Probably that will change with the merge, as last month's lwaftr will usually be in master and probably we can test it directly.\nI see two social points that we will need to agree on, more or less.  One is that the changes that we make to core, for whatever reason, will always be made with an upstream thought in mind.  Cc @kbara, @dpino.  Otherwise we make them on a side branch that is not meant for merging to lwaftr.\nThe second point is that upstream be willing or interested to take such changes :)  Having an upstream core merge blocking a future lwaftr merge is a negative situation, though one we could get into.  I think though that we should generally be OK in this regard, having seen the core changes that the lwaftr merge is bringing in (none, outside the PRs), so probably this is not something to worry about.\nI agree with the direct merge to next of the branches.md change; thanks!  Onwards and upwards :)\n. @lukego Sure.  One of the examples I was thinking of was the set_rate / leaky=false thing from #793; not sure what our goal should be regarding testing all features of apps.  To me it is reasonable to include those tests as part of the programs using the apps (nfv, lwaftr, etc), though I agree a self-test is better, if more difficult to write :)\n. Very nice work!  LGTM; I guess you are the upstream for this though?\n. I like some things out of this but many others are not clear :)  To me you would only want to do this change if we also flattened the directory structure.  But to the extent that directory structure provides value, to me we should reflect that in module names, so that we have just one story there and not two.  WDYT?\nI tend to think that core vs lib is a distinction that provides value in the way by selecting some code to set the direction for the project, style-wise, and allow \"contrib\" code to come in even if it's different in style.  At the same time \"core vs lib\" has costs too that this would do away with.  Anyway: how would you provide the benefits of a separate \"core\" without actually having hierarchy?\n. I think there's still an open question of how to document methods versus functions, at least for me.  If a data type has a method that is not available as a free procedure -- i.e., if it were only pkt:free() and never packet.free(pkt) -- then I don't know how to refer to the data type in the manual.\nTo state it another way: before the parenthesis, to me it seems that all of the names should denote bindings accesible in the normal Lua namespace.  For example packet.free is such a binding, packet being the conventional way we denote modules and free being a property of that module; but in pkt:free there is no such thing as pkt.  packet:free is in some way the worst of all worlds: the packet denotes a binding (the module itself), but not one which has a free method (unless the packet module has a free procedure, of course).\n. Somewhat related to #740.\n. @plajjan this PR depends on #877 and #878 \n. As far as \"depending on\", I mean that it depends on the review of those patches.  I think those other PRs can be reviewed on their own and once they are merged I can merge this one.\nAs far as interfaces go, I would like to see if I can incorporate the packet queue, the enqueue routine, and the flush routine into the lookup streamer itself.  Perhaps then we rename it to LookupQueue or something.  The user would instantiate the queue with a queue size and some kind of handler procedure:\n``` lua\nlocal function have_result(app, pkt, entry)\n   if entry then\n      --- process packet etc\n      app:process(pkt, entry.value)\n   else\n      --- drop packet etc\n      app:drop(pkt)\n   end\nend\nlocal queue = ctab:make_lookup_queue(32, have_result, app)\n...\nlocal function push(app, pkt)\n   while not link.isempty(app.input)\n      local pkt = link.receive(app.input)\n      app.queue:enqueue(pkt, get_key(pkt))\n   end\n   -- flush any remaining enqueued packets\n   app.queue:flush()\nend\n```\nDunno!\n. Yeah!  I'm down with that in general.  The funny thing is, we already have an array of packets to work on in a batch: the link.  Perhaps we should work directly there?  Perhaps in slices?\nIt would certainly simplify the interface to streaming lookup if we could assume that the user is maintaining a parallel array of packets or other data corresponding to the lookup entries.  Right now the lwaftr code is structured around the illusion of processing packets one-by-one, when in reality there is a little queue in the middle.  Perhaps it would be best to explicitly partition the stages as you have done above.\n. I want to merge this PR.  It applies directly (at least it merged without conflicts locally into a branch) and passes the real test (snabb-nfv-test-vanilla) above.\nRegarding potential refactors into a different way of looking things up in https://github.com/snabbco/snabb/pull/879#issuecomment-212313565 -- I think we should punt until a bit later.  This change will allow us to ditch podhashmap so that we're completely on ctable, something I need already to be able to migrate the lwaftr to work with the yang configuration stuff.  I still think it's a good idea that we should explore when not so many things are up in the air.\n. This branch is merged into Igalia:lwaftr.  We need to spend some time upstreaming that to SnabbCo:master. If it's just a soft limit to give feedback to the user when they are forgetting to free a packet, to me it sounds like the intel app should increase the limit appropriately as part of its initialization.  If you instantiate 100 VF's with the default ring buffer size, then you are implicitly raising the limit.  The problem that the limit detects AFAIU is that someone forgot to call packet.free, not so much that someone instantiated too many apps.\n. What's going on here?  I thought Yang was being planned as part of #696.  This seems to be much more ad-hoc with no design document.\n. I agree that JSON is probably not the best representation for this data, because YANG's set of data types does not map particularly well to JSON.  For me you always need a schema for a computer to interpret YANG instance data, if you are not serializing in a representation that is compatible with the XML representation.\n. I definitely get the need for good counters :)  Maybe I was just spooked by the name of the PR, especially given the long discussion on #696.  I am using #696 as a base to start work on YANG in Snabb.  Is that the wrong thing to do now?  I am confused :)\n. @lukego: Makes sense to me, sure.  Integrating all of this does have some unknowns, but sure.\n@plajjan I can see the value of a JSON serialization when talking to a tool that expects the standard JSON YANG mapping and which also has the schema on-hand.  Just dumping JSON without a schema is likely to run into problems as you note: https://github.com/snabbco/snabb/issues/696#issuecomment-171906554.\nBut given that JSON is a fair bit of ceremony to parse, that it's hard to give good errors for a YANG  JSON production (by the time you are validating the instance data you don't have source locations on-hand), that you have to shove numbers into strings, well at that point given that you have to build some representation of the schema in many YANG-processing places, then you might as well serialize using a nicer language (e.g. the text argument to the load_config example in https://github.com/snabbco/snabb/issues/696#issuecomment-171634370).  You could then translate that minimal language to the XML representation using an external tool, so as not to deal with XML from Snabb.\nBacking up.... it's easy to write out data in JSON.  It's not so easy to consume it, when you factor in the types and the need to validate that instance data conforms to a schema.  Snabb will need to validate configurations with good messages for the user to indicate when something is wrong with the configuration.  I see JSON as being in the way of that goal -- not nice to write by hand, not nice to give errors for, does not support the data types we need.  But I think this is more of a discussion for #696.  This PR is limited to export of state data.  A YANG integration in Snabb will have to import and export configuration data too.\n. @eugeneia all good by me, and the functionality of this patch is really excellent and quite welcome :)  great stuff!\n. Related to your second point @eugeneia, how close to implementing this are you?  If you can afford to wait a month, then I think we will have made some YANG progress in some way and it will be more clear how to represent YANG instance data coming from Snabb.  I think that in our output language, we will need to be able to import modules and declare certain properties as being extensions, i.e.\nnamespace mib-ii-smi { default; }\nnamespace http://example.com/ethernet { prefix eth; }\n// ...\ninterface { type hardware; eth:foo-property bar; }\nBig-picture-wise we are defining a language whether we want to or not, whether it is built on JSON or otherwise, and it will have a grammar, and we should specify that grammar formally, and it should be extensible, and it should be pleasant to deal with.  Could be based on json; I'd go for something more ad-hoc but ymmv.  WDYT?\n. Yeah totes, working in parallel is great.  I suspect that given that we need to end up with a coherent whole but that we have different focuses in the near term (you, AFAIU: exposing the RFC 7223 set of counters, me: exposing the YANG counters for the lwAFTR based on an internet draft, in addition to implementing some kind of minimal schema for consuming configuration and state data) that we will develop and figure out in parallel, and then reconcile in a few weeks.  As long as we keep in close enough communication we could end up with a nice result.  Let me know if I have misunderstood :)  Perhaps we should have a tracker bug for conversations, given that you are not on Slack or IRC AFAIU.\n. Would you mind rebasing this one on top of wingo-next?  I had to do some housekeeping which invalidated this PR.  Tx for your patience :)\n. LGTM, though feedback from @eugeneia / @lukego is welcome of course\n. lgtm!\n. @lukego the use case is in snabb lwaftr check -- Nicola needs to check that on runs of the lwaftr that use pcap files as inputs, that the counters change in the ways that we expect.  Since the counters are the public interface, I didn't want to grovel in the app_table.lwaftr state to get them; better to open by name, but they were already opened by the lwaftr...\nIn practice I don't think anyone is closing counters except via link.free on reconfigure, so it is a moot point practically speaking i think :)\n. It would seem that #887 hasn't been merged to next yet, so this PR would also bring that one in.\n. Fixed the merge conflict.  I'm going to close this PR though and open a new one branched off a specific revision, so that I can merge in more things.\n. Replaced by #895 \n. I don't see any significant safety consideration, for what it's worth, by using well-known names for fields of a struct with a stable, documented ABI, especially considering that we need to access the packet data directly without any out-of-bounds-checking machinery.  YMMV of course :)\n. lgtm, thanks :)\n. lgtm!\n. Yeah I feel like we already have these expectations in place, you just have to hang around enough to understand them -- which is something of an antipattern in a way; if we recognize that we'd like review within a few days then we might as well write it down and make it easier for new contributors to help make Snabb better. Of course there's a balance; I don't mean to obligate reviewers to do things, because as we all know we get pulled away from things from time to time and don't always have the time.  But usually it's just that something comes in, it's too gnarly for an immediate yes/no, and so it sits in the queue :)  So +1 to a general etiquette/expectations writeup of when to expect things to happen, more or less, and when a ping is welcome.\n. definitely needs some @lukego thoughts here :)\n. For me if it's only the name of the app, then maybe a start() function is overkill. Too many things it could do, and not having the links set up is a bit irritating.  What if instead we pass some additional table of parameters as a second argument to the constructor?  That table would have {name=..., shmpath=..., ...}, i.e., whatever information the engine decides to make for the app.  Dunno, just an idea.  I don't want to trample on the \"arg\" to the constructor, as probably someone already has an app with a \"name\" configuration parameter :P\n. You say \"If every app would be responsible of passing those attributes as a second argument in the constructor I'm afraid that would end with inconsistent naming of those parameters among different apps.\".  Not sure what you mean by that; if it's core/app.lua which is responsible for creating and passing the second argument to the constructor, it's a standard interface, and all apps access it in a standard way.\nI would go farther and create the links before creating the apps, so that you could already have everything an app needs before constructing it.  No more code, just moving some things around.  YMMV tho :)\n. Will need a documentation update to mention the new parameter; and probably an update to mention \"ingress_drop_count\" to app writers.  Probably needs a comment as well in the source explaining what it's about.  Draft text, quite verbose:: \"When the snabb breathe cycle runs, it usually doesn't drop any packets: packets pulled into the network are fully pushed through, with no residual data left in link buffers.  However if the breathe() function takes too long, it's possible for it to miss incoming packets deposited in ingress ring buffers.  That is usually the source of packet loss in a Snabb program.\nThere are several things that can cause packet loss: the workload taking too long on average, and needing general optimization; the workload taking too long, but only during some fraction of breaths, for example due to GC or other sources of jitter; or, the workload was JIT-compiled with one incoming traffic pattern, but conditions have changed meaning that the JIT should re-learn the new patterns.  The ingress drop monitor exists to counter this last reason.  If the ingress drop monitor detects that the program is experiencing ingress drop, it will call jit.flush(), to force LuaJIT to re-learn the paths that are taken at run-time.  It will avoid calling jit.flush() too often, in the face of sustained packet loss, by default flushing the JIT only once every 20 seconds.\"\n. @kbara regarding whether to be on or off by default -- I suspect that if you are already dropping packets at a constant rate, then a once-in-a-while jit.flush() won't matter for your goals -- it won't hurt them anyway, and if the problem is ultimately side traces that should be main traces instead, then it may help.  Dunno.  The idea is that snabb processes should heal themselves if they can.  Of course the overhead of self-doctoring might be too much, as you say.  Curious to see what snabb-bot will say!\n. a failure: https://gist.github.com/SnabbBot/cc90f0b8beb8747648e52d9df85326da#file-log-L294\n. For me this patch LGTM, and it appears to have no effect on the benchmarks numerically.  I don't know though if the benchmarks drive Snabb to the point that it drops packets though.  Anyway, PTAL @lukego, since this is core stuff an opinion would be welcome.\n. @lukego that sounds fine to me fwiw, although it means more machinery in intel10g and other apps to actually accumulate that counter value, and I don't know how often to do it.  Should the app add to that counter at every pull?\n. @lukego All of your thoughts sound right to me.  I think that in the future we'll still have the ingress_drop_monitor object because we need a place to hold the state like \"did I flush the JIT recently\" and \"what's my current threshold for when I should flush JIT\".  MHO is that we should merge this code and iterate on it in #886 like you suggest to make the ingress_drop_monitor check counters instead of calling the method, if that makes sense.\nApologies for the parens thing, somehow I got started with that coding convention in Lua and I seem to have infected the team.  We need to do a global search and replace on the lwaftr code, and until then be more careful when porting code to core.\n. I think we should disable the jit.flush code by default.  I apologize for arguing for it -- it works well for the lwaftr but I don't feel comfortable rolling that code out to general production right now.  There are two major problems: one is the bad error message.  The second is that it counts packets immediately after a jit.flush as well.  So let's say you have a legitimate case where jit.flush could help, you detect the dropped packets and you flush: cool.  However the counter effectively re-sets right at the flush -- right when we expect to see more dropped packets because of the flush.  Instead we should stop monitoring for a period of time after a flush, instead of simply delaying the next flush, if any.  We will submit a fix for this, but this sort of heuristic immaturity is what makes me uncomfortable rolling this code out to production.\nWe should change the default for the ingress drop monitor to false.\n. LGTM, as I mentioned in the other PR I think we should work at reducing the number of things we have to document, or at least reduce the number of ways for things to go silently wrong.  For example instead of requiring taskset, we can use sched_setaffinity via a standard --cpu argument.  We can warn if hyperthreading is enabled.  We can warn if we do --cpu to a core that doesn't have NUMA affinity to the PCI devices we use.  We can warn if we are using PCI devices but haven't set up core affinity.  We can warn if IRQ's are being delivered to our core.  The ring buffer settings should be generally fine with the new 1024 packet setting and though we should document the setting, we shouldn't encourage users to change it unless they just want to play around with things.\n. Choo choo, on board for v2016.06 train departing shortly\n. This is an instance of the class of bugs also including #749.  Of course the fix here will be more limited.\n. Thanks for the review, removed the unnecessary line.\n. FWIW I never understood why the ingress drop counter negatively impacted Snabb NFV, especially on the 550-byte workload we were using to test.  Why would Snabb NFV drop packets on ingress?  Its whole job is to forward them.  I can only think it was misconfigured in some way.\n. I think the warning message in our case should indicate not only that we are flushing the JIT, if that's what we're doing, but also that the user probably wants to take a look at their configuration to verify the CPU affinity, the memory affinity, the PCI affinity, the scheduling, the IRQ delivery, and all that.  In practice for us that's what the \"jit.flush\" warnings have been meaning for us lately, that the user's setup is borked in some way.\n. Indeed: https://github.com/snabbco/snabb/blob/master/src/apps/vhost/vhost_user.lua#L173-L207\nIf the file is up to date with itself (????) we don't set the new features at all!\n. https://github.com/snabbco/snabb/pull/440/commits/1f27b5b42f8958f30f55c3be0332c4c7e14f304e was the commit that added this strange cache\n. Neat!  Will try.  One question: the C.VIRTIO_NET_F_MQ feature is offered by net_device.lua, but it doesn't make it through to the guest.  Could that be QEMU getting in the way?\n. I have tested this one locally and it works for me.\n. Interesting!  Note that https://github.com/Igalia/snabb/pull/354/commits/df8e83c6771a7b892324922f25d5123a69f651d9 implies that this code is running in all cases, whether or not MRG_RXBUF is negotiated, if QEMU first negotiated to have MRG_RXBUF on.\n. I note that when I run perf record on a lwaftr process, for comparison, I see something more like:\n7.32%  snabb    perf-1768.map      [.] 0x000000000bc998cd\n   6.32%  snabb    perf-1768.map      [.] 0x000000000bc98fcd\n   6.11%  snabb    perf-1768.map      [.] 0x000000000bc9cce6\n   5.34%  snabb    perf-1768.map      [.] 0x000000000bc97782\n   4.63%  snabb    snabb              [.] lj_BC_TGETS\n   2.25%  snabb    snabb              [.] lj_tab_len\n   1.97%  snabb    perf-1768.map      [.] 0x000000000bc896c6\n   1.66%  snabb    snabb              [.] lj_meta_tget\n   1.33%  snabb    snabb              [.] lj_tab_get\n   1.16%  snabb    snabb              [.] lj_ctype_getfieldq\n   0.99%  snabb    snabb              [.] lj_BC_JLOOP\n   0.97%  snabb    snabb              [.] lj_cdata_index\n   0.81%  snabb    snabb              [.] full_memory_barrier\n   0.79%  snabb    snabb              [.] lj_tab_getinth\n   0.74%  snabb    snabb              [.] lj_BC_CALL\n   0.74%  snabb    snabb              [.] lj_tab_getstr\n   0.59%  snabb    libm-2.21.so       [.] __ieee754_log_avx\n   0.57%  snabb    perf-1768.map      [.] 0x000000000bca13ef\n   0.56%  snabb    perf-1768.map      [.] 0x000000000bca13bf\n   0.49%  snabb    snabb              [.] lj_cf_ffi_meta___index\n   0.43%  snabb    [vdso]             [.] __vdso_clock_gettime\n   0.43%  snabb    perf-1768.map      [.] 0x000000000bc9493c\n   0.42%  snabb    perf-1768.map      [.] 0x000000000bca140b\n   0.40%  snabb    snabb              [.] lj_BC_JFUNCF\n   0.39%  snabb    perf-1768.map      [.] 0x000000000bca149e\n   0.39%  snabb    snabb              [.] lj_vm_exit_interp\n   0.38%  snabb    libc-2.21.so       [.] __memmove_avx_unaligned\n   0.37%  snabb    snabb              [.] lj_cconv_ct_ct\n   0.37%  snabb    perf-1768.map      [.] 0x000000000bca142e\n   0.36%  snabb    perf-1768.map      [.] 0x000000000bc94931\n   0.31%  snabb    perf-1768.map      [.] 0x000000000bca13a8\n   0.31%  snabb    perf-1768.map      [.] 0x000000000bc9243c\n   0.30%  snabb    snabb              [.] lj_vmeta_call\n   0.30%  snabb    snabb              [.] lj_cont_ra\n. I don't know whether to trust this data.  The BC_ things would seem to indicate bytecode handlers.  In that case that would mean that both processes are running interpreted, a significant portion of the time!\n. Test plan sounds great :)  It will be nice to be able to evaluate changes in a more scientific way.  Looking forward to the benchmarking game :)\n. This particular bug was not filed because of the bimodal perf I was seeing: it was filed because perf indicates that there are opportunities for optimization around allocation.\nThe bimodal perf bug is somewhat hard to reproduce.  FWIW I run qemu like this:\n$ qemu-system-x86_64 --version\nQEMU emulator version 2.4.0, Copyright (c) 2003-2008 Fabrice Bellard\nI invoke qemu like this:\nsudo numactl -m ${NUMA_NODE} taskset -c ${QEMU_CORE} qemu-system-x86_64 \\\n        --enable-kvm -name ${NAME} -drive file=${DISK},if=virtio \\\n        -m ${MEM} -cpu host \\\n        -fsdev local,security_model=passthrough,id=fsdev0,path=${SHARED_LOCATION} \\\n            -device virtio-9p-pci,id=fs0,fsdev=fsdev0,mount_tag=share,rombar=0 \\\n        -object memory-backend-file,prealloc=yes,id=mem,size=${MEM},mem-path=/dev/hugepages,share=on \\\n        -numa node,memdev=mem \\\n        -chardev socket,id=char1,path=${VHU_SOCK1},server \\\n            -netdev type=vhost-user,id=net0,chardev=char1 \\\n            -device virtio-net-pci,netdev=net0,addr=0x8,mac=52:54:00:00:00:01 \\\n        -chardev socket,id=char2,path=${VHU_SOCK2},server \\\n            -netdev type=vhost-user,id=net1,chardev=char2 \\\n            -device virtio-net-pci,netdev=net1,addr=0x9,mac=52:54:00:00:00:02 \\\n        -netdev type=tap,id=net2,ifname=${IFACE},vhost=on,script=${NETSCRIPT} \\\n            -device virtio-net-pci,netdev=net2,addr=0xa,mac=52:54:00:00:00:03 \\\n        -vnc :${VNC_DISPLAY} &\nAnd then inside I run the lwAFTR.  Note that we are running from an old branch for these tests because we needed some NFV fixes regarding negotiation and support for not enabling MRG_RXBUF.  Still working on porting forward to master.  I would recommend not working on the bimodal perf bug until we have been able to merge forward; should happen in the next week or so.\n. lukego: I attach to a running snabb nfv process when it is processing traffic, so I think those perf runs are measuring the right thing.\n. I like the idea of the config process being a sibling to the dataplane.  Incidentally the supervisor/worker/config process separation probably works even better with a --cpu argument instead of numactl et al, because you only want to have the data plane bound to the CPU and not the config/supervisor processes, I think.\n. @lukego very much agreed, the version skew problems between a long-running data plane and a sometimes-updated-in-strange-ways \"snabb\" binary means that a long-running mostly-idle config process is guaranteed (modulo fork/exec race conditions at start!) to speak the same low-level language as the data plane, and that a simple protocol between the config client and the manager will last.\n. @kll not ignoring you, just that the answer is a bit long.  to specify an entire configuration it will look similar to the vmx's configuration (https://raw.githubusercontent.com/mwiget/vmxlwaftr/igalia/tests/lwaftr3.txt).  specifically for the lwaftr we have to have a custom yang module because our tunables aren't precisely the same as the ones in the IETF draft spec so it will look like http://sprunge.us/QUQJ we think.  we have a yang schema, to publish shortly.\n. We will support both the standard and an extended yang model.  I am not sure why you are asking these questions -- evidently you have generated lwaftr configurations and you are aware that there are settings that do not have corresponding knobs in the standard yang model.  I have discussed this with Ian, yes, at length, over many months.  After six months of discussion I am sure you can understand that I am happy to move beyond design choices and get into implementation :)\n. I think in the first question you are conflating syntax and data model.  The lwaftr currently uses an ad-hoc configuration syntax, and an ad-hoc data model.  It is ad-hoc because adhering to standards is costly and not the first thing you build.  We are going to migrate the lwaftr to a YANG data model, and try to do so in a way that other Snabb programs will want to migrate too.  Given the model, the syntax is the least important thing.  I have an agreement with my users to deliver the syntax described above; but expressing it as XML or json is possible to build.\nAbout data models.  The set of configurables in the standard model is not the same as the set of configurables for the lwaftr; check the lwaftr documentation and compare to ietf-softwire.  The lwaftr configurables are largely but not entirely a superset; there are schema-valid configurations in the ietf-softwire model that are not valid snabb lwaftr configurations.\nWe will deliver both a custom and the standard yang model.  The custom one will be delivered first because it reflects how the lwaftr operates.  The IETF chose to standardize a suboptimal data model and that will have to be shimmed on afterwards.  I have raised these concerns with the softwire working group.\n. New design document here, updated for developments over the last few months and put in the form of documentation: https://github.com/Igalia/snabb/tree/yang/src/program/config/README.md\nThat document links to the new internals document: https://github.com/Igalia/snabb/blob/yang/src/apps/config/README.md\nThis will make it to the lwaftr branch shortly and upstream for the next release.  Some parts are still falling into place, but the document describes where we are going.\n. Do I understand correctly that these are full duplex receive and transmit tests, and that they are being limited by the transmit side because of the non-posted semantics of the way the NIC is using the PCIe bus?\n. Neat PR :)\nRegarding configuration.  I think different network functions are going to have different needs.  In particular in the lwAFTR, I don't think a generic serialization facility is going to work for the sorts of configuration updates that we expect.  For the lwAFTR we need to compile the configuration to an efficient binary format that can be just slurped into memory at startup.  #987 is where we would like to go, I think.\nI would like to see other Snabb programs migrate towards declarative YANG configurations.  In that way we have a model for what \"configuration change\" means that besides reloading an entire configuration also encompasses add/remove/set operations on container elements, we have a protocol for communicating changes between processes (#987) that won't cause the data plane to lose packets, and we can verify configurations both against a YANG schema and with program-specific checks.\nI will be working on this full-time for the next few weeks fwiw.  I look forward to seeing what patterns end up working for you for the receive-side steering and virtual 100G cases you are looking at here.  I do want to push back a little though on any generic attempt to codify what configuration means in a Snabb context without considering the lwAFTR's use cases.  Thanks :)\n. What operational CPU/NUMA affinity concerns does a multi-process Snabb have?  My working idea is that only the data planes should be bound to the data-plane CPUs, and that the kernel should even be configured to have nohz on those cores.  Does that match your impression?\n. I would imagine for a PCI device with affinity for NUMA node 1, that the first CPU on that node should handle any auxiliary processes, as that CPU is the one that will be handling interrupts for that socket.  So the process should have NUMA affinity but unspecified or specified-to-first-CPU-on-node CPU affinity.\n. I would be happy to take more responsibility around here though I worry that I might not fully page in your needs.  In fact, maybe the manager process can handle configuration-related concerns?  Probably best to do configuration in a sub-process for isolation reasons, but I don't know.\nAs far as the NFV (e.g.) and YANG go -- we have a branch in which we use YANG data models to configure the lwAFTR, I think the first step would be to get that into shape, publish, and see what you think.  It will take a week before we can start doing \"snabb config\"-like things I think.  I wil retake the configuration part of the conversation then.\n. Interesting, for some reason I thought the manager would also be the supervisor.  Cool.  In that case that works for me.  What is the mapping of the various processes to /run/snabb/PID ? Do they share one directory?  cc @xray7224 \n. As this is an RFC/WIP does it need to be assigned to anyone, @eugeneia ?  I didn't think of myslf as lead fwiw :)\n. Good question... I am also on the hook for deliverables at the end of this month and so we will be racing together :)\nI think the text approach you mention here will not work for the lwaftr, even in the case where you can't make an incremental update and instead you have to just reload the config -- it will be too slow.  For that reason we are compiling configurations to a binary format (Igalia#517) so that the data plane can load the configuration faster.  I hope to be done with that bit by tomorrow.\nHowever although that configuration-compilation approach is generic and can work for any YANG model -- see WIP design here: https://github.com/Igalia/snabb/blob/8c8dc727b0ded04be1899412c766bebd9530345d/src/lib/yang/README.md -- it does mean a different configuration syntax for your users and probably a different form of the configuration internally, relative to how any specific app (in your case the NFV but this also affects the lwaftr) is currently configured.\nFor that reason there's a bit of change cost and I think you are the right person to decide when/if you will be able to pay it.  Again, I expect to have this config facility shippable by the end of the week.  What does your timeline look like in that regard?\n. @lukego yes your analysis is exactly how I see it.  We are doing things that sound similar on the surface but that doesn't mean that they are actually similar in essence or that they should share implementation.  There could be points that we need to coordinate on but I don't think configuration right now is one of them; configuration-as-app-network-specification works for you right now and not for me and that's OK :)\n. I like this code. In the short term I'd like to use this code, and I would like be able to run my own code to configure the app network in the worker (and the main process of course).  For now I only need a single worker.  What's your timeline here?  I'll use an app pair to communicate between the main process and the worker, so until it lands I can still make progress by running the manager-side app from an unrelated process.\nOpen question: does C-c on the manager reliably stop workers?  I don't see any call to stop() in your patch.\n. Nice, thanks!  I think @kbara is looking into this in the short term; I will help if needed.  FYI also I now have the leader/follower thing implemented from #1069 and am just fixing a couple of lwaftr bugs.  Things are looking good!\n. You will need Igalia#809 at least if this is to work with intel_mp.  Do you want to make a separate merge branch that is based here, then merges in master, then pr's back to master?. Probably you want the worker.lua part of 3e3281998c7c1f3636b1b379be890bc1d3aeb011 as well.. You might also need 876addc17287ed7c8fc133e3e24452aa3373f429. I wonder if this naming change regressed because previously the ordering of the app_array and link_array could depend on how things hashed, and thus the expected number of visits through that array could vary depending on how names changed.  Maybe after #1066 the name change is not a regression!\n. @lukego done :) i assume you want it named \"optimize\" just so that I can test other things by force-pushing to that branch?  Sounds good to me :)  We definitely need to throw some science at this one!\n. Added minimum packet alignment and updated the wingo/optimize head too.\n. Report with minimum alignment will be here: https://hydra.snabb.co/build/658140\n. My conclusion is that perf-wise this doesn't regress any benchmark.  The open question is its effect on lib.protocol.datagram et al (cc @alexandergall).  Are we OK with these changes?  If so let's merge.  If not let's talk :)\nThe context is that I'd like to get the lwaftr merged back to master early this week, and this is one of the few changes we made to core code.\n. Good idea @lukego and nice catch @kbara, can do.  What do you think of the datagram changes tho Luke?  Any out-of-tree uses of the datagram library will be silently broken (like @alexandergall's vpn).\nMHO: breaking datagram users is too bad but it is the right thing to do as this ultimately makes encap and decap operations like vlan tagging much cheaper, we should just own that decision.\n. Hydra run here: https://hydra.snabb.co/build/679729\n. Well goll darnit it looks like that last change tanked performance.  Will give it another go.  Sadness.\n. Actually the commit seemed to tank correctness, not performance :)  I was thinking that since new_packet set the p.length, that free_internal didn't have to.  But the dual of free_internal is actually allocate, not new_packet :P\n. perf to be here: https://hydra.snabb.co/build/682109\n. OK all good now, ptal @lukego \n. thanks :) I will merge to wingo-next.\n. @kbara yeah hard to read that one indeed.  the lower mode does seems to have lower probability tho\n. Looks good in general, note though that the PID directory gets created already.  I.e. anonymous snabb processes have those dirs already.  Maybe it should be \"claim_name()\" or something, as it is an exclusive resource, and claiming a name could fail.  Dunno :)\n. I think the review is happening in Igalia/snabb because that's where we expect to merge it -- we'll need it soon.  But yeah, this kind of change is tricky as we don't want to create a situation where a downstream change in Igalia/lwaftr prevents an upstream merge.  There's a tension between getting the right upstream review and having a common work-in-progress space for downstream teams.\nWe're definitely interested in upstream feedback (hence this issue:) so we'll open a PR against upstream then.\n. stats will be here: https://hydra.snabb.co/build/659075\n. @lukego wdyt about this one?  Only affects Snabb guests, not DPDK or other guests.  There is no corresponding change for the host because the choice of descriptor layout is up to the guest, not the host.  The PR is hard to read though because it includes earlier patches from #1032; perhaps just look at the last two commits.\nNicola Larosa and @domenkozar are working on getting lwaftr tests into Hydra but I think we don't yet have them wired up.  Still, obvious improvement I think, simplifying the code and giving us twice the descriptor count.\n. ping :)\n. That linked perf analysis includes the result of #1032.  Let me see if I find a new one.\n. This is changing the driver side of virtio-net.\n. @kbara there is no up-to-date perf thing.  however it will not show on the graphs because #736 hasn't landed; we don't have any automated tests with snabb guests AFAIU, neither integration nor benchmarking :(\n. I verrrrry much understand not wanting to land something without CI, but I think the right thing to do here is just merge this so we can merge the lwaftr.  Merging it does not make the situation worse and this is the code that we are shipping and we are the only users of this code, and in parallel @dpino  has #736 open and Domen and Nicola are working on getting hydra+lwaftr+qemu+... working too, so things are on the right track IMO.  @kbara wdyt?\n. Sorry, meant to do this against lwaftr\n. Did you mean to tag #1309, @lukego ?  AFAIU the change there is orthogonal; you pretty much always want to know if the system is dropping on ingress.\n. Thanks @kbara, will merge to wingo-next.  Just the NFV thing left to do before merging.\n. Tx for review, already in next.  Readme on the way in #1048.\n. hydra: https://hydra.snabb.co/build/685340\n. No idea why there are new hydra failures here; will look into it I guess\n. (For that matter, what is going on with the standard tests?)\n. Bah, I think there are ingress drop monitor patches missing from this PR.  Will fix.\n. Apologies for the confusion; I had missed a commit that gave the ingress drop monitor the ability to have different actions (https://github.com/Igalia/snabb/pull/361).  Rebased to include that patch before anything else.\n. perfies will be https://hydra.snabb.co/build/704184\n. Regarding virtio, it's hard.  Right now it's impossible to drop packets on a host -> guest interface, since the packet allocation is controlled by the guest.  Even now we don't fully drain the last link in the app graph that feeds VhostUser:pull().\n[update: edited this para]\nIn practice to know whether it's the NFV or the guest which is being slow, you look at that last link.  If the NFV is dropping packets on ingress that means the host is too slow.  Otherwise if the NFV is dropping packets on the last link then the guest is slow.  If the host isn't dropping packets on ingress then all is fine.\nWhat about this do you think should change?  I would think we could drain the ->VhostUser link in VhostUser:push or so, perhaps emitting a warning.\n. This PR originally had two unrelated things: a change to the way the NFV loop runs, and a way to use the ingress drop monitor to warn when the NFV is slow.  Those changes were unrelated.  I have now force-pushed the branch to only make the former change and will do something else for the latter one.  I will push to wingo/optimize so we can get numbers/CI on this one and will ping when results are in.\n. Hydra results will be https://hydra.snabb.co/build/719109\n. Hydra results up.  The relevant comparison is between wingo-optimize and master, as wingo-optimize is just one commit beyond master.\n. Well, I dunno.  Hydra results do not show a regression and also do not show a clear progression.  WDYT @lukego?  I could go either way on this, though I lean towards landing it.\n. Will do, thanks :)\n. FYI I will push additional merges on this PR until someone assigns themselves to give it the :+1: or :no_good_man: :)\n. @kbara this branch is already past a, b, and, c, and is now for luke or whoever merges to master.  i just wanted to put the heads up that this branch could get more patches until someone (luke i guess) assigns themself to indicate start-of-review.\n. Updated, ptal @kbara.  Thanks :)\n. Tx for review, added mention of packet loss for hyperthreading.\n. I will merge to wingo-next and send to luke for merging today :)\n. Note that the first patch is already part of wingo-next as it came from #1039.\n. I believe that the changes to IngressDropMonitor in https://github.com/snabbco/snabb/pull/1045 are effectively included in this one just because it's coming already from the lwaftr, but this branch does not add the drop monitor to the NFV.  I will keep https://github.com/snabbco/snabb/pull/1045 open though because it includes the change to fix the tips URL.\n. I suggest putting off merging this until #1047 lands, FWIW.  I think it has a minor conflict.\n. Merged to lwaftr which is under review in #1047.\n. Thanks for the merge, and I will be less terse in the future :)\n. I am happy to write docs fwiw, can do that in a followup\n. Hydra run here: https://hydra.snabb.co/build/737038\n. The nh_fwd (next-hop forward) app is part of snabb vmx.  It sits outside (around, really) a snabb data plane and forwards control traffic to a virtualized control plane, such as an Ubuntu image or a Juniper vMX switch. That way it learns the next hop and can stamp on the L2 address of where the packets should go next.  It is not lwaftr-specific.\n. Added a couple patches to lwaftr that address Max's feedback.  @kbara how does it look to you?\n. So I made the rookie mistake of just making this a PR from lwaftr instead of branching off lwaftr and PR'ing from that.  I will merge https://github.com/snabbco/snabb/pull/1047/commits/3d33fb6d834222306b32fef8eee39a59de69516e to wingo-next and pr that to next.\n. @dpino thanks for the clarification on that, let's fix the readme issue in a followup\n. Closing this PR as it's already merged and I made the rookie mistake of PRing this from a live branch which continues to get updates and thus false-alarm notifications.  Thank you all!\n. Done, thanks.\n. Hydra report here: https://hydra.snabb.co/build/739387\nRelevant diff will be between next and wingo-optimize, though the comparison to master is interesting as well.  I expect no performance change; in this case I'm just using Hydra mostly for its CI purposes.\n. Hydra's done, I see no significant difference between next and wingo-optimize.  Let's ride this train to next!\n. AFAIU the hydra jobs don't show a significant change relative to next.  WDYT @lukego ? :)\n. Neat.  Thanks for being an amazing project technical lead and steward; you have made a really lovely virtual place to work :-)\n. For the YANG schema, isn't the internal versioning sufficient?  SGTM though, we can work in parallel :)\nAlso as regards the relationship between YANG schemas and data configuration, you might be interested by https://github.com/Igalia/snabb/issues/519, https://github.com/Igalia/snabb/pull/517, and https://github.com/Igalia/snabb/pull/525.\n. @dpino is on holiday for the next couple weeks I think.  @emmericp do I understand that you are using something other than Snabb NFV on the host to provide networking to the QEMU instance?\n. @emmericp strange stuff.  I'm sorry you're hitting these bugs.  We tested this in a limited environment with Snabb NFV on the host and running one of our data planes on the guest (the lwaftr).  Here is the script that we use to start things going: https://github.com/snabbco/snabb/blob/master/src/program/lwaftr/virt/lwaftrctl\nDoes that help at all? The networking bit looks like\n-netdev type=tap,id=net2,ifname=${IFACE},vhost=on,script=${NETSCRIPT} \\\n            -device virtio-net-pci,netdev=net2,addr=0xa,mac=52:54:00:00:00:03\nBut that would seem to be very similar to what you have.\n. $ qemu-system-x86_64 --version\nQEMU emulator version 2.4.0, Copyright (c) 2003-2008 Fabrice Bellard\n. I think we did test with other qemu versions though.\n. Good morning :)\nYour process design looks good to me.  I guess you would multiplex REPL sessions with config clients somehow?  I would be tempted to have the main process communicate with the world via message passing and not spawn a standard REPL that \"blocks\" fromthe perspective of the main app; probably you have a message-passing REPL in mind?  I'm a little concerned about complexity, as we have a deliverable soon and having to think about the interaction between config and other facilities complicates things.  But, perhaps there is a ncie simple design lurking somewhere.\nAbout the update protocol and back channel.  I think a simple counter poll is sufficient for a \"reload your configuration\" signal but not for an \"add this entry to the binding table\" message.  We are shooting for thousands of these incremental updates/s and we need a channel for messages.  WDYT?  Does that explain the need for a back channel a bit more?  The initial idea was to re-use the apps.lwaftr.channel abstraction but you mentioned in the past that a struct link might work just as well, so that's where that idea came from.  To check for messages is just to check the link for pending messages (or the ring buffer in equivalent ways) so it's very cheap shared memory operations, not syscalls.\n. /cc @kbara @xray7224 \n. Interesting idea for config server etc to be apps, will look into that.  As far as the backchannel goes, we could have the config app in the main process be responsible for sending messages to the workers, who run their own app I guess, or perhaps the backchannel is a part of the lwaftr app (as it is now, actually).  OK!\n. Probably we should put off making a change to the PID structure of /run/snabb until some time later -- people have different versions of tools and I think this PR lets old \"snabb top\" keep on working for a while.\n. @eugeneia the use case wasn't external snabb top, it was rather a snabb top from some previous version or some other mainline-headed but not-precisely mainline branch\n. Sorry I have to insist about the version thing.  There may be many versions of snabb on a system and you don't want \"snabb top\" to start doing weird things depending on which version/branch/etc of snabb top you run.  I do not want these bugs from my users!\n. seems i forgot to add https://github.com/Igalia/snabb/commit/580d8c7ee7b118c1bda7da9b3d941f27fbf9ee2b to this branch\n. @eugeneia would you perhaps mind having a look at this one?  No rush though, I just wanted to get it on board one of the upstream trains :)\n. Fixed nit, good catch :)\nAlso it turns out the slot is not leaked.  The next slot will be allocated at #values, which may be any index which precedes a nil and in practice seems to be the first one.  So the normal case of adding and removing shouldn't grow the values array.  Additionally if it did grow and get holey, that would be LuaJIT's problem to switch to a sparse array representation and not a packed array.\nRe ctable and parameters it is a good idea for later :)\n. I think this PR is pretty low-risk; the only tricky change is the \"named programs\" one.  For me it could hit master for v2016.11, or we could wait until v2016.12, either way.  WDYT @eugeneia and @lukego ?\n. Makes sense to me.  I didn't realize that non-core code could be risky but it turns out it can be if the test suites are flaky or don't run on the main CI servers.\n. Closing this PR, as it keeps getting new commits as we merge to lwaftr.  I'll instead merge via wingo-next.\n. This point was about internal representation of IPv4 values.  In yang modules they are of course inet:ipv4-address; it's just a question of how that data type is represented internally.\n. Yeah we are trying to represent internal data in types derived from yang schemas.  It mostly works but you have to add some special cases for certain types; e.g. in yang there is no primitive type for IPv4 addresses as you know, and so the ipv4 typedef is actually as a string (!) whose syntax is restricted by a regular expression (!!!).  So we have overrides internally to efficiently represent IPv4 and IPv6 addresses and CIDR prefixes and ethernet addresses.  Before, we mapped IPv4 to uint8_t[4].  Now we map to uint32_t.\n. Bah, meant to PR this against Igalia/lwaftr.  closing, sorry for noise\n. Closing PR as it continues to get commits from Igalia/lwaftr and generating notifications -- @chrgraf I think what @kbara suggests sounds like the right thing.\n. Indeed, multi-app cycles will cause packets to stay in links after a breathe() finishes.  However as long as the network is capable of processing the input traffic rate, this will not lead to packet drops (at least as long as we don't artificially drain/drop non-empty links after each breath) as the next breath will process those packets.  I don't think there is a significant difference in this case relative to what happens now with the fixpoint loop though.\n. this error appears to be a timeout.\n. What is going on with the test suite?  I do not understand the NFV failure.\n. Hydra for this branch: https://hydra.snabb.co/build/842422/download/2/report.html\nhttps://hydra.snabb.co/build/843455/download/2/report.html\n. For motivation for this PR see https://github.com/Igalia/snabb/issues/562.  Specifically I want to be able to compute a set of changes in the main process and then ship that set of changes to the worker to be applied.\n. /cc @lukego regarding #1021 \n. Seems I am neglecting to call the link methods, and that's making the bridge unhappy.\n. Appears to be a significant regression: https://hydra.snabb.co/build/860978/download/2/report.html  I wonder how???\n. @eugeneia is that the regression?  Certainly snabbbot appears to think there is a 25% regression on basic1, which I guess is due to reconfiguration (and reconfiguration only afaiu).  My aim here is to allow us to separate compute_configure_actions and apply_configure_actions between processes; certainly we should not regress on that basic benchmark tho.\nI don't understand how this could affect data plane performance tho.  What is the benchmark in the hydra results whose perf has gone down so much?  Is it reconfiguration as well?\n. (incidentally snabb-bot did a great job in catching this perf problem and NACKing the PR :)\n. optimize-murren on b551d27: https://hydra.snabb.co/build/866422/download/2/report.html\noptimize-murren on c708b62: https://hydra.snabb.co/build/867116/download/2/report.html\nbenchmarks-murren-small on b551d27: https://hydra.snabb.co/build/864965/download/2/report.html\nbenchmarks-murren-small on c708b62: https://hydra.snabb.co/build/868183/download/2/report.html\n. I don't understand the snabbnfv-iperf-1500-tunnel+crypto regression from snabbbot.  How would I test that?\n. tx for the tip!  I need to get better at being about being able to run NFV tests.\n. Regarding YANG and snabbmark -- I don't know :)  I don't see it for snabbmark, but I could be missing something.  How would you see that working?  Command-line stuff is nice for developers I think; YANG is nice internally for parsing, serialization, validation (somewhat), compilation, and binary loading of configuration data from users; but if the interface is from a developer perhaps it is overkill?  I don't know.\nI must also say that I am at my limit for hackpower over the next 3/4 weeks and so I am going to be ignoring more things than usual, even meritorious hacks like this one :)\n. I also like the app:configure mechanism.  Very tasteful!\n. I assume that app:configure is side-effect-free besides its effect on the app graph?  If so that works well with my work on multi-process configuration.\n. FWIW I am currently sketching ideas out here: https://github.com/Igalia/snabb/tree/config-leader.  Very early state right now, it only does ping/pong :P\n. The question about whether we can eliminate the public protocol is an interesting one.  If we have the binary corresponding to a running snabb instance, that could indeed solve the problem for us.  However on a practical level it's just labelling -- the protocols are different (one is high-level RPC and the other is just an apply-these-actions protocol) so whether the \"public\" one is really public or not can be decided later.\n. How could a follower be a program?  It has to be in the data plane in order to mutate the app graph.  Apps are how we compose different bits of code into a snabb process.  It could just as well be a timer of course, if that's the thing.\n. You mean core.app, not core.engine I think?\nWhat do you mean by \"program\"?  Do you mean process, Snabb instance consisting of multiple processes, or general subroutine-like thing?  Best to adopt Luke's process terminology in #1021 I think if we are talking about processes.\nFor me a follower is a component which should run in a worker process, and a leader is a component that should run in a main process.  I have implemented both as apps, but that's a detail.\n. The loop you give is pretty much what the follower does.  It polls for actions and applies those actions.  It could run via a timer instead.  It happens to run as an app because that's what Luke suggested.\n. The follower does have to listen for changes at a high rate though.  We are aiming for a minimum 1000 changes per second without dropping packets, for binding table updates at least.  Only processing updates every second would probably stall the data plane because the batch of updates would be too large.\n. I also want to avoid O(N) operations in the workers.  I especially don't want to have to compare binding tables.  For that reason I think there is an advantage for pushing \"what changes to apply\" logic out of the worker process.\n. It's not calling engine.configure, but rather engine.apply_config_actions; see https://github.com/Igalia/snabb/tree/yang/src/apps/config for details.\nBut yes.  Thing is, for me running something from a timer vs running it from an app is not so dissimilar.  Both are effectively concurrent processes that can be composed into an engine.  How are they different for you?  Do your arguments against apps apply to timers too?\n. Follower:pull() can cause breathe_pull_order and breathe_push_order's bindings to change but it doesn't mutate those tables.  It could cause the breath to start with one pull order but then run a new push order (because it's a pull function and pull functions currently run first).  I see what you're saying though but I'm not going to hit it in my case.\nBasically I am looking for another word from Luke here since he suggested implementing as an app, and that's what I'm going to ship tomorrow ;)  Can refactor into something else over time if that's the right thing.\n. lgtm & merge it yourself directly? :)\n. Excellent idea!  The docs should probably note that this is a class method, not a method on an instance, and that it's meant to add to configuration and not have other side effects.\n. Closing to actually just merge the v3.0.0 release, no additional patches\n. TBH I am not sure what the upstream lwaftr head is for but as long as it's there, it seems like using it for released versions of the lwAFTR seems like a fine thing.  Feedback is welcome of course @kbara @lukego @eugeneia :)\n. Good evening!  I don't think this will make the next release via the lwaftr, as that merge will probably take a little while (January release, but most Igalia folks are on holiday, so I could see that slipping too perhaps).  Anyway if you want it in master faster, we can work in parallel.. You seem to be going in a parallel direction to where we're going in the lwaftr :)  I would be interested in a bigger picture design document.. If the need is just for information, the leader/follower apps include a generic one-way main-to-worker channel: https://github.com/Igalia/snabb/blob/yang/src/apps/config/README.md.. If the goal of this information is to simply configure the worker, then given that you want to move to YANG anyway, I would define a YANG module for the parts of the mellanox drivers that need this configuration (sendpath/receivepath), then use that YANG module to serialize that data.  If the data use is more complicated (e.g. worker->main process communication) then that might not be a great option.\nI am a little concerned that there's been a public ongoing design for multiprocess configuration, backs and forths from everyone, then something like this going in through the back door without a big picture design that I have seen, well it makes me nervous :)  Perhaps that is just me being high-strung right before a deliverable :). If this measure is seen to be a stopgap, then that's cool too.  In that case I would suggest to put these data structures somewhere other than core.. Could be.  If an app has a private channel, by all means make a private channel.  If you want to model that data with YANG, then it's possible to compile a public configuration to a private configuration and ship that data over a channel ring buffer.  Or if you want an app-specific mechanism, that's cool too (and that code should surely live in the app's directory and not core).  But I still didn't get one part of what you are doing -- are you updating these values in place?  Updating strings in place seems not like a good idea :). I would not use YANG (or our yang code anyway) for things like \"index of the last successfully transmitted packet\".. Apologies for being grumpy and thank you for humoring me :)  FWIW the core of my objection is really against SHM-based configuration (app-internal or otherwise) in core, at least at this time.  I don't think there is a robust general way that can work; I could be wrong but we should start out app-specific instead of starting in core IMO.  Counters and synchronization variables for me are fine tho, there's fewer ways to misuse those :). Closing this as the new lwaftr configuration format has landed in master with the last release.  Still, an embarassing bug, our apologies @amanand !. benchmarks-murren-small: https://hydra.snabb.co/eval/4974 / https://hydra.snabb.co/build/987961\noptimize-murren: https://hydra.snabb.co/eval/4973 / https://hydra.snabb.co/build/986242/download/2/report.html\n. @kbara did some initial work in this area on https://github.com/Igalia/snabb/commit/cef09bd37ba813c77417b9f8a14bb12f88aa2433; I'm removing that branch as part of a cleanup operation (we're not working on it at Igalia) but wanted to leave a link around somewhere :)\nRelatedly, I'm closing this PR as well.  From Luke's comment I think Snabb's approach is to register the existence of other development branches but only to merge in ones that we've agreed on maintaining in the future.. If I read https://hydra.snabb.co/build/1263403/download/2/report.html right, this branch has no significant perf impact.  Is that how you read it, @lukego?. I guess the multi_copy test failed because it uses dynasm+AVX2 and the host that it runs on doesn't do AVX2.  I suppose I can skip the test in that case, or fall back to a different implementation.. I believe that the core/app.lua changes have been hashed out, yes -- I understand that @eugeneia would still prefer that the app graph not cause manipulations to itself and I understand that :)  I think at some point I would like to refactor to make it so that worker processes that are participating in a leader/follower config relationship will poll for config changes outside of the app graph (e.g. as part of the main loop, possibly via a different main loop) instead of from inside it.  That should be a small change.\nAdditionally there was a performance question in https://github.com/snabbco/snabb/pull/1067#issuecomment-261184224.  I didn't understand it and didn't look into it farther, so we should make sure to check that particular case in our benchmarking runs.. Morning :) I did do benchmarking of a number of different options and this solution was fastest, though I suspect normal AVX would not be too terribly slower.  I didn't keep the other versions around though.\nThe multi_copy code is used in the streaming lookup interface to the ctable.  It's not used in the \"normal\" ctable lookup path and indeed couldn't really be used there -- it's really for streaming in multiple keys at a time, unlike the binary-search dasl that could be used with single lookups (but isn't currently).  So current ctable users aren't affected by this addition as their interfaces still work.  The only use of this code currently is the lwaftr, and we only deploy the lwaftr on machines with AVX2 so no problem there either afaiu.. /cc @eugeneia for the snabb-bot changes. Regarding lib.pmu_x86, the bot does seem to run the test successfully: https://gist.github.com/SnabbBot/9b1632f41a542da562cefdb16927e523#file-log-L887\nI am inclined to think that what must be a problem in snabbvmx or in its test suite should not cause the bot as a whole to have to use a particular kernel version -- given that the lib.pmu_x86 test passes, we should be able to figure out what's going wrong with this test on 4.x and fix it, or disable the test on master for the time being (instead of rolling back the kernel).  WDYT @dpino?  In the meantime we can file a bug to re-enable that test and prioritize that work.. @eugeneia the snabb-bot run has been going overnight, since around 18h yesterday.  Is that normal?. @lukego Sorry for the delay; I was on a short holiday from when you wrote.  I would like to fix snabb-bot before merging but I have no other functional changes queued.  I think the snabb-bot failures are related to configuration rather than to functionality, fwiw.. Actually lookin at things again, I think the intel_mp test failure is spurious.  We certainly didn't change that code.  @eugeneia would you mind running the bot again on this PR?. Just pushed a trivial commit to force the bot to run again.. Well!  That was horrible.  I spent days ignorantly trying to understand things from a constructive perspective and then gave up and did a bisect.  It's wasn't quite a real bisect as the intel_mp driver only came in with v2017.04, but at each step between master and lwaftr I merged in #1086 and ran a test that just did two testup.snabb invocations on different queues on the same device.  If both complete with true, then great, otherwise one would hang and that would be the result.  Apparently the culprit is 9fe91ac51483232372abfc042e5120c5ad7d4be1 from the multi-process worker-and-supervisor code, and indeed it makes sense; the intel_mp driver expects bus mastering to not be tied to any particular process, I think.  Gotta knock off for today but will poke tomorrow.  /cc @lukego and @petebristow who will find this interesting I think.. The bot ran, yay!  From what I can tell, the failure noted in the bot run was a perf regression:\nERROR snabbnfv-iperf-1500-crypto -> 0.805118 of 1.016 (SD: 0.102489 )\nHowever a previous run on this branch before this unrelated bus-mastering patch succeeded with 1.0, so I am inclined to think, no problem.. This one's all ready to go, I think!  WDYT the next step is @lukego?  Do you want to do #1021 first and separately?  Should I just merge this to wingo-next ?. Thank you for the review and the merge, yay!!!!. Conversation on Slack indicates that this might be a bug in intel_mp, similar to some that were caught in the older 82599 driver in its development phase.  Just putting down this note for the record :). This was on a freshly rebooted machine fwiw, and this was the first access that these cards had from Snabb.. I followed up these two test runs (the failing and the succeeding run) with 10 additional intel_mp/selftest.sh runs as above, and they all succeeded.. I just soft-rebooted the server, under the theory that it was some kind of first-run phenomenon, but I could not reproduce it then.  I guess we archive this report.  Feel free to reopen if we can actually reproduce it in the future.. The issue here was that one of the processes was left spinning waiting for RXDCTL.Enable to read as 1, which wasn't happening because the other process exited and turned off bus mastering.  This was fixed in the merge related to #1145.  Closing this one.. Example test run on master, FWIW:\n```\n[wingo@davos:~/snabb/src/apps/intel_mp]$ sudo lock sh -c 'for dev in 01:00.0 01:00.1; do for q in 0 1 2 3; do for i in 0 1 2 3 4 5 6 7 8 9; do time ./testup.snabb $dev $q; echo \"./testup.snabb $dev $q result: $?\"; done; done; done'\nlocking /var/lock/lab.. flock: getting lock took 0.000005 seconds\ntrue\nreal    0m6.067s\nuser    0m0.030s\nsys 0m0.022s\n./testup.snabb 01:00.0 0 result: 0\ntrue\nreal    0m4.073s\nuser    0m0.036s\nsys 0m0.018s\n./testup.snabb 01:00.0 0 result: 0\ntrue\nreal    0m4.074s\nuser    0m0.038s\nsys 0m0.019s\n./testup.snabb 01:00.0 0 result: 0\ntrue\nreal    0m4.071s\nuser    0m0.032s\nsys 0m0.021s\n./testup.snabb 01:00.0 0 result: 0\ntrue\nreal    0m6.063s\nuser    0m0.027s\nsys 0m0.017s\n./testup.snabb 01:00.0 0 result: 0\ntrue\nreal    0m4.074s\nuser    0m0.036s\nsys 0m0.021s\n./testup.snabb 01:00.0 0 result: 0\ntrue\nreal    0m4.070s\nuser    0m0.032s\nsys 0m0.020s\n./testup.snabb 01:00.0 0 result: 0\ntrue\nreal    0m4.073s\nuser    0m0.035s\nsys 0m0.021s\n./testup.snabb 01:00.0 0 result: 0\ntrue\nreal    0m4.070s\nuser    0m0.033s\nsys 0m0.020s\n./testup.snabb 01:00.0 0 result: 0\ntrue\nreal    0m4.072s\nuser    0m0.034s\nsys 0m0.021s\n./testup.snabb 01:00.0 0 result: 0\ntrue\nreal    0m4.071s\nuser    0m0.037s\nsys 0m0.017s\n./testup.snabb 01:00.0 1 result: 0\ntrue\nreal    0m6.071s\nuser    0m0.036s\nsys 0m0.017s\n./testup.snabb 01:00.0 1 result: 0\ntrue\nreal    0m4.070s\nuser    0m0.033s\nsys 0m0.018s\n./testup.snabb 01:00.0 1 result: 0\ntrue\nreal    0m4.074s\nuser    0m0.034s\nsys 0m0.022s\n./testup.snabb 01:00.0 1 result: 0\ntrue\nreal    0m6.071s\nuser    0m0.035s\nsys 0m0.018s\n./testup.snabb 01:00.0 1 result: 0\ntrue\nreal    0m4.070s\nuser    0m0.032s\nsys 0m0.021s\n./testup.snabb 01:00.0 1 result: 0\ntrue\nreal    0m4.070s\nuser    0m0.037s\nsys 0m0.015s\n./testup.snabb 01:00.0 1 result: 0\ntrue\nreal    0m4.070s\nuser    0m0.034s\nsys 0m0.019s\n./testup.snabb 01:00.0 1 result: 0\ntrue\nreal    0m4.062s\nuser    0m0.024s\nsys 0m0.020s\n./testup.snabb 01:00.0 1 result: 0\ntrue\nreal    0m4.068s\nuser    0m0.030s\nsys 0m0.019s\n./testup.snabb 01:00.0 1 result: 0\ntrue\nreal    0m6.063s\nuser    0m0.026s\nsys 0m0.019s\n./testup.snabb 01:00.0 2 result: 0\ntrue\nreal    0m4.072s\nuser    0m0.034s\nsys 0m0.019s\n./testup.snabb 01:00.0 2 result: 0\ntrue\nreal    0m4.070s\nuser    0m0.033s\nsys 0m0.019s\n./testup.snabb 01:00.0 2 result: 0\ntrue\nreal    0m4.072s\nuser    0m0.035s\nsys 0m0.020s\n./testup.snabb 01:00.0 2 result: 0\ntrue\nreal    0m4.070s\nuser    0m0.027s\nsys 0m0.025s\n./testup.snabb 01:00.0 2 result: 0\ntrue\nreal    0m4.072s\nuser    0m0.036s\nsys 0m0.018s\n./testup.snabb 01:00.0 2 result: 0\ntrue\nreal    0m4.063s\nuser    0m0.027s\nsys 0m0.020s\n./testup.snabb 01:00.0 2 result: 0\ntrue\nreal    0m4.073s\nuser    0m0.035s\nsys 0m0.020s\n./testup.snabb 01:00.0 2 result: 0\ntrue\nreal    0m4.072s\nuser    0m0.038s\nsys 0m0.017s\n./testup.snabb 01:00.0 2 result: 0\ntrue\nreal    0m4.071s\nuser    0m0.035s\nsys 0m0.019s\n./testup.snabb 01:00.0 2 result: 0\ntrue\nreal    0m4.070s\nuser    0m0.035s\nsys 0m0.017s\n./testup.snabb 01:00.0 3 result: 0\ntrue\nreal    0m4.077s\nuser    0m0.040s\nsys 0m0.020s\n./testup.snabb 01:00.0 3 result: 0\ntrue\nreal    0m4.072s\nuser    0m0.033s\nsys 0m0.021s\n./testup.snabb 01:00.0 3 result: 0\ntrue\nreal    0m4.070s\nuser    0m0.030s\nsys 0m0.022s\n./testup.snabb 01:00.0 3 result: 0\ntrue\nreal    0m4.070s\nuser    0m0.034s\nsys 0m0.019s\n./testup.snabb 01:00.0 3 result: 0\ntrue\nreal    0m4.064s\nuser    0m0.026s\nsys 0m0.019s\n./testup.snabb 01:00.0 3 result: 0\ntrue\nreal    0m4.070s\nuser    0m0.030s\nsys 0m0.022s\n./testup.snabb 01:00.0 3 result: 0\ntrue\nreal    0m6.069s\nuser    0m0.028s\nsys 0m0.025s\n./testup.snabb 01:00.0 3 result: 0\ntrue\nreal    0m4.070s\nuser    0m0.035s\nsys 0m0.017s\n./testup.snabb 01:00.0 3 result: 0\ntrue\nreal    0m4.063s\nuser    0m0.023s\nsys 0m0.023s\n./testup.snabb 01:00.0 3 result: 0\ntrue\nreal    0m6.057s\nuser    0m0.032s\nsys 0m0.020s\n./testup.snabb 01:00.1 0 result: 0\ntrue\nreal    0m4.058s\nuser    0m0.036s\nsys 0m0.017s\n./testup.snabb 01:00.1 0 result: 0\ntrue\nreal    0m4.047s\nuser    0m0.028s\nsys 0m0.015s\n./testup.snabb 01:00.1 0 result: 0\ntrue\nreal    0m4.048s\nuser    0m0.024s\nsys 0m0.019s\n./testup.snabb 01:00.1 0 result: 0\ntrue\nreal    0m4.048s\nuser    0m0.025s\nsys 0m0.018s\n./testup.snabb 01:00.1 0 result: 0\ntrue\nreal    0m4.050s\nuser    0m0.025s\nsys 0m0.020s\n./testup.snabb 01:00.1 0 result: 0\ntrue\nreal    0m4.065s\nuser    0m0.040s\nsys 0m0.019s\n./testup.snabb 01:00.1 0 result: 0\ntrue\nreal    0m4.048s\nuser    0m0.023s\nsys 0m0.021s\n./testup.snabb 01:00.1 0 result: 0\ntrue\nreal    0m4.050s\nuser    0m0.026s\nsys 0m0.019s\n./testup.snabb 01:00.1 0 result: 0\ntrue\nreal    0m4.050s\nuser    0m0.023s\nsys 0m0.022s\n./testup.snabb 01:00.1 0 result: 0\ntrue\nreal    0m4.051s\nuser    0m0.026s\nsys 0m0.019s\n./testup.snabb 01:00.1 1 result: 0\ntrue\nreal    0m4.062s\nuser    0m0.037s\nsys 0m0.020s\n./testup.snabb 01:00.1 1 result: 0\ntrue\nreal    0m4.056s\nuser    0m0.032s\nsys 0m0.019s\n./testup.snabb 01:00.1 1 result: 0\ntrue\nreal    0m4.050s\nuser    0m0.025s\nsys 0m0.020s\n./testup.snabb 01:00.1 1 result: 0\ntrue\nreal    0m4.050s\nuser    0m0.027s\nsys 0m0.018s\n./testup.snabb 01:00.1 1 result: 0\ntrue\nreal    0m4.051s\nuser    0m0.028s\nsys 0m0.019s\n./testup.snabb 01:00.1 1 result: 0\ntrue\nreal    0m4.048s\nuser    0m0.027s\nsys 0m0.016s\n./testup.snabb 01:00.1 1 result: 0\ntrue\nreal    0m4.057s\nuser    0m0.036s\nsys 0m0.017s\n./testup.snabb 01:00.1 1 result: 0\ntrue\nreal    0m4.051s\nuser    0m0.026s\nsys 0m0.020s\n./testup.snabb 01:00.1 1 result: 0\ntrue\nreal    0m4.049s\nuser    0m0.022s\nsys 0m0.022s\n./testup.snabb 01:00.1 1 result: 0\ntrue\nreal    0m4.051s\nuser    0m0.027s\nsys 0m0.020s\n./testup.snabb 01:00.1 2 result: 0\ntrue\nreal    0m4.048s\nuser    0m0.024s\nsys 0m0.018s\n./testup.snabb 01:00.1 2 result: 0\ntrue\nreal    0m4.048s\nuser    0m0.023s\nsys 0m0.019s\n./testup.snabb 01:00.1 2 result: 0\ntrue\nreal    0m4.059s\nuser    0m0.037s\nsys 0m0.018s\n./testup.snabb 01:00.1 2 result: 0\ntrue\nreal    0m4.050s\nuser    0m0.027s\nsys 0m0.018s\n./testup.snabb 01:00.1 2 result: 0\ntrue\nreal    0m4.064s\nuser    0m0.041s\nsys 0m0.018s\n./testup.snabb 01:00.1 2 result: 0\ntrue\nreal    0m6.049s\nuser    0m0.026s\nsys 0m0.018s\n./testup.snabb 01:00.1 2 result: 0\ntrue\nreal    0m4.060s\nuser    0m0.040s\nsys 0m0.015s\n./testup.snabb 01:00.1 2 result: 0\ntrue\nreal    0m4.050s\nuser    0m0.027s\nsys 0m0.018s\n./testup.snabb 01:00.1 2 result: 0\ntrue\nreal    0m4.060s\nuser    0m0.033s\nsys 0m0.022s\n./testup.snabb 01:00.1 2 result: 0\ntrue\nreal    0m4.048s\nuser    0m0.023s\nsys 0m0.019s\n./testup.snabb 01:00.1 3 result: 0\ntrue\nreal    0m4.051s\nuser    0m0.028s\nsys 0m0.019s\n./testup.snabb 01:00.1 3 result: 0\ntrue\nreal    0m4.048s\nuser    0m0.025s\nsys 0m0.018s\n./testup.snabb 01:00.1 3 result: 0\ntrue\nreal    0m6.051s\nuser    0m0.027s\nsys 0m0.018s\n./testup.snabb 01:00.1 3 result: 0\ntrue\nreal    0m4.047s\nuser    0m0.025s\nsys 0m0.017s\n./testup.snabb 01:00.1 3 result: 0\ntrue\nreal    0m4.050s\nuser    0m0.024s\nsys 0m0.021s\n./testup.snabb 01:00.1 3 result: 0\ntrue\nreal    0m6.058s\nuser    0m0.033s\nsys 0m0.020s\n./testup.snabb 01:00.1 3 result: 0\ntrue\nreal    0m4.056s\nuser    0m0.032s\nsys 0m0.019s\n./testup.snabb 01:00.1 3 result: 0\ntrue\nreal    0m4.062s\nuser    0m0.038s\nsys 0m0.019s\n./testup.snabb 01:00.1 3 result: 0\ntrue\nreal    0m4.062s\nuser    0m0.043s\nsys 0m0.015s\n./testup.snabb 01:00.1 3 result: 0\n``. I reduced the linkup_wait timeout from 2s to 0.1s and ran thetestup.snabb` test again on master on both devices, 4 queues, 10 times each.  Times below fwiw:\n3.167\n3.376\n3.376\n3.376\n3.465\n3.465\n3.466\n3.467\n3.470\n3.474\n3.476\n3.576\n3.665\n3.667\n3.677\n3.679\n3.679\n3.683\n3.766\n3.766\n3.773\n3.774\n3.777\n3.868\n3.875\n3.876\n3.876\n3.953\n3.954\n3.954\n3.954\n3.954\n3.954\n3.956\n3.957\n3.961\n3.961\n3.961\n3.961\n3.961\n3.961\n3.961\n3.962\n3.962\n3.962\n3.962\n3.962\n3.962\n3.962\n3.962\n3.962\n3.962\n3.962\n3.963\n3.963\n3.963\n3.963\n3.964\n3.965\n3.965\n3.965\n3.967\n3.968\n3.970\n3.971\n3.974\n3.974\n3.976\n4.002\n4.066\n4.076\n4.166\n4.176\n4.177\n4.264\n4.276\n4.368\n4.376\n4.377\n4.468. Those results were with two i350 devices, fwiw.  Waiting for 82599 linkup seems to be a lot more variable: between 0.5s and 8s (!!!!).. with this simple change:\n``patch\n[wingo@davos:~/snabb/src/apps/intel_mp]$ git diff\ndiff --git a/src/apps/intel_mp/README.md b/src/apps/intel_mp/README.md\nindex a51a2ab..7847f7e 100644\n--- a/src/apps/intel_mp/README.md\n+++ b/src/apps/intel_mp/README.md\n@@ -49,6 +49,12 @@ light or not. The default isfalse.\n *Optional* Number of secondsnew` waits for the device to come up. The default\n is 120.\n+\u2014 Key linkup_wait_recheck\n+\n+Optional If the linkup_wait option is true, the number of seconds\n+to sleep between checking the link state again.  The default is 0.1\n+seconds.\n+\n \u2014 Key mtu\nOptional The maximum packet length sent or received, excluding the trailing\ndiff --git a/src/apps/intel_mp/intel_mp.lua b/src/apps/intel_mp/intel_mp.lua\nindex c0a6e47..d90cbc2 100644\n--- a/src/apps/intel_mp/intel_mp.lua\n+++ b/src/apps/intel_mp/intel_mp.lua\n@@ -257,6 +257,7 @@ Intel = {\n       mtu = {default=9014},\n       rssseed = {default=314159},\n       linkup_wait = {default=120},\n+      linkup_wait_recheck = {default=0.1},\n       wait_for_link = {default=false},\n       master_stats = {default=true},\n       run_stats = {default=false}\n@@ -291,6 +292,8 @@ function Intel:new (conf)\n       mtu = conf.mtu or self.config.mtu.default,\n       rssseed = conf.rssseed or self.config.mtu.default,\n       linkup_wait = conf.linkup_wait or self.config.linkup_wait.default,\n+      linkup_wait_recheck =\n+         conf.linkup_wait_recheck or self.config.linkup_wait_recheck.default,\n       wait_for_link = conf.wait_for_link\n    }\n@@ -757,10 +760,10 @@ function Intel1g:init ()\n    self.r.CTRL_EXT:set( bits { AutoSpeedDetect = 12, DriverLoaded = 28 })\n    self.r.RLPML(self.mtu + 4) -- mtu + crc\n    self:unlock_sw_sem()\n-   for i=1, math.floor(self.linkup_wait/2) do\n+   for i=1, math.floor(self.linkup_wait/self.linkup_wait_recheck) do\n       if self:link_status() then break end\n       if not self.wait_for_link then break end\n-      C.usleep(2000000)\n+      C.usleep(math.floor(self.linkup_wait_recheck * 1e6))\n    end\n end\n@@ -866,7 +869,7 @@ function Intel82599:init ()\n    pci.unbind_device_from_linux(self.pciaddress)\n    pci.set_bus_master(self.pciaddress, true)\n\nfor i=1,math.floor(self.linkup_wait/2) do\nfor i=1,math.floor(self.linkup_wait/self.linkup_wait_recheck) do\n       self:disable_interrupts()\n       local reset = bits{ LinkReset=3, DeviceReset=26 }\n       self.r.CTRL(reset)\n@@ -881,9 +884,9 @@ function Intel82599:init ()\n       self.r.AUTOC2(0)\n       self.r.AUTOC2:set(bits { tenG_PMA_PMD_Serial = 17 })\n       self.r.AUTOC:set(bits{restart_AN=12})\nC.usleep(2000000)\n       if self:link_status() then break end\n       if not self.wait_for_link then break end\n\nC.usleep(math.floor(self.linkup_wait_recheck * 1e6))\n    end\n-- 4.6.7\n\n\n```\nI have managed to actually hang one of the 82599's; testup.snabb doesn't work at all on it; the link never appears to be up!. The fix for the \"hang\" was to not include the reset logic in the loop for the 82599's, which mirrors what is done in the stable Intel82599 driver.. I wonder if the linkup poll period should be configurable though -- seems like a useless configuration option, one that we should just set appropriately.  I could make it a file-local variable instead.  WDYT @petebristow ?. This PR fixes #1141 fwiw.  I would say that 2s polling is not a good default and is specifically a regression relative to Intel82599 where it wasn't necessary.  Something needs to be done though, wdyt is the right way forward @petebristow ?. Hi, I see what you are saying, I didn't notice that bit before.  I have no objection to the loop actually, but I want a more reactive response to link-up.  You can see here that the older driver polls more frequently between resets: https://github.com/snabbco/snabb/blob/master/src/apps/intel/intel10g.lua#L394.. I'll update the patch appropriately.. Updated to re-introduce reset loop for 82599 cards and factor out the linkup wait loop.. wdyt @petebristow ?. The previous build failure was actually a bot failure I think.  Merged master to this branch and we'll see what the results are now.  @petebristow review still welcome, though @takikawa perhaps you would be available to review as well?. Good morning, early riser @lukego :) You could certainly describe configuration with a custom YANG schema.  That would automatically define a textual syntax for expressing configurations, and the yang code can easily produce \"normal\" Lua objects from that that the test could use directly.\nAs far as counters go, I think that document is a little out of date.  We have a working thing that's not very organized; really a prototype.  What it does is look for any state data in a YANG schema, collecting all non-\"config\" leaves in the schema.  Counters that are under the apps/ subtree that have the same name as these state leaves will be exported when building a state tree, e.g. via https://github.com/Igalia/snabb/blob/lwaftr-2017-04-24/src/lib/yang/state.lua#L93.  This tree of Lua values can be serialized to a string using the API or using snabb config get-state, if the program has \"snabb config\" support.\nLooking at it now I see that it can be better in many ways :). Yes certainly.  You'd need a schema of course but maybe we're heading that way :)\nFor more high-bandwidth communications you can serialize to the binary format instead, but for this use case the text format sounds fine.. Regarding tarballs: this patch also updates the \"dist\" target to copy a core/version.lua into the tarball.  Of course you might use some other way to create a tarball.\nFrom Nix -- good question.  I guess we can use an environment variable (SNABB_VERSION or so), and default its value to git describe if it's unset and there is a .git.  FWIW Guile has a similar thing, and we ship a .tarball-version file to fall back on.\nRegarding tags -- yes, can be weird.  Right now it restricts to tags that start with \"v\".  For the lwaftr this works for our purposes as we tag releases with v3.1.9 etc... dunno though.  I think it will be OK though, as this is a function of your git checkout/repo, and you can control this, and when it really matters, you are setting a tag already because you're about to release.. I should mention a couple of virtues of git-describe.  One, it includes the most recent tag; good for general orientation.  Two, for revisions that don't correspond to tags, it shows the number of commits since that tag -- so you can order two version numbers.  Third, for revisions that don't correspond to tags, it shows the abbreviated hash; useful.  Gives you a path back to the source from the version.  Finally it appends -dirty if you have local changes.  All of these are useful things to know.  When I log into a deployed machine and see a Snabb running, I would like to know what source it corresponds to, and if it's \"new enough\".  A simple checksum of sources is an oracle to verify that it's the same or different than a particular source checkout or a particular binary, but embedding a bit more git-isms lets me find the source more easily than a simple checksum, in addition to letting me compare versions.. I think I addressed the review comments.  See changelog for https://github.com/snabbco/snabb/pull/1146/commits/fdf662d615b325eb6ae264d841aeffa970d22468.  An example:\n```\nwingo@rusty:~/src/snabb/src$ make\nGEN       obj/core/version.lua.gen\nLUA       obj/core/version_lua.o\nLINK      snabb\nBINARY    2.8M snabb\nwingo@rusty:~/src/snabb/src$ ./snabb --version\nsnabb 2017.04 (2-gfdf662d6)\nCopyright (C) 2012-2017 Snabb authors; see revision control logs for details.\nLicense: https://www.apache.org/licenses/LICENSE-2.0\nSnabb is open source software.  For more information on Snabb, see\nhttps://github.com/snabbco/snabb.\nwingo@rusty:~/src/snabb/src$ make VERSION=2017.05\nGEN       obj/core/version.lua.gen\nLUA       obj/core/version_lua.o\nLINK      snabb\nBINARY    2.8M snabb\nwingo@rusty:~/src/snabb/src$ ./snabb --version\nsnabb 2017.05 (fdf662d6)\nCopyright (C) 2012-2017 Snabb authors; see revision control logs for details.\nLicense: https://www.apache.org/licenses/LICENSE-2.0\nSnabb is open source software.  For more information on Snabb, see\nhttps://github.com/snabbco/snabb.\n``. @lukego ah good catch.  fixed, i think.. There must be a way to fix the rebuilding issue, even if it makes the makefile more complicated.  I will take a poke.. PTAL; I think I fixed the rebuild issue.. I don't quite understand the snabbdoc build error; do you, @lukego?  I would think we can merge this tonext` at this point though.. Very elegant code @takikawa, thank you!\nI think we should look at getting this upstream.  For me the blockers for this PR are:\n\n\n[x] Tests passing (though I see you just pushed something that might have fixed it)\n\n\n[ ] Documentation and also updating programs/README\n\n\nAs far as nice-to-haves, we should work on squeezing out more performance I think.  Just for the record here were some of the ideas we discussed offline:\n\n\n[ ] Instead of the app handling both IPv4 and IPv6, specialize apps for v4 and v6.  This will decrease the branchiness of inner loops and allow specialization of the flow_key_t and the hash function.  We just have to add Marcel's V4V6 app and we're good.\n\n\n[x] ~Unroll the hash function for the key size, instead of having a loop in the hash function.  Incidentally we should add the hash function to snabbmark, once the lwaftr is merged (as I think it adds a hash snabbmark).~ (Hash function perf noodling here: https://github.com/takikawa/snabb/pull/1; ultimate PR is here: https://github.com/snabbco/snabb/pull/1155)\n\n\n[ ] Somehow batch lookups, using something like ctable's streaming interface.  It would involve a bit of ctable hacking though as we need to update the value in place.\n\n\nI had another thought as well:\n\n[ ] Decrease maximum occupancy factor for the flow tables.  Since we have large keys and values, displacing entries in the robin-hood \"stealing\" phase is probably something to avoid; it would seem useful to throw more memory at this problem.\n\nOnce the YANG stuff merges, we should also look at defining YANG schemas for the apps.  But that would be a next step I think.. If I rebase this branch on top of #1155, then I get the following perf results:\n$ sudo program/ipfix/tests/bench.sh 03:00.0 83:00.0 03:00.1 3\nProbe on 03:00.0. Packetblaster on 83:00.0.\nBENCH (size 64 (1000 pkts) for 5 iters, 10 secs)\nbytes: 2,429,946,180 packets: 40,499,103 bps: 1,854,491,798 Mpps: 3.8635245798\nbytes: 2,223,592,020 packets: 37,059,867 bps: 1,621,577,350 Mpps: 3.3782861463474\nbytes: 2,820,261,420 packets: 47,004,357 bps: 2,091,215,698 Mpps: 4.3566993717338\nbytes: 2,392,917,900 packets: 39,881,965 bps: 1,792,486,505 Mpps: 3.7343468871679\nBENCH (size 64 (5000 pkts) for 5 iters, 10 secs)\nbytes: 3,172,963,140 packets: 52,882,719 bps: 2,381,553,672 Mpps: 4.9615701504257\nbytes: 3,077,283,060 packets: 51,288,051 bps: 2,262,685,312 Mpps: 4.7139277335203\nbytes: 2,211,917,160 packets: 36,865,286 bps: 1,690,453,439 Mpps: 3.5217779981568\nbytes: 2,170,702,860 packets: 36,178,381 bps: 1,641,645,644 Mpps: 3.4200950935193\nbytes: 2,394,988,740 packets: 39,916,479 bps: 1,810,862,566 Mpps: 3.7726303464727\nBENCH (size 64 (10000 pkts) for 5 iters, 10 secs)\nbytes: 3,026,383,380 packets: 50,439,723 bps: 2,318,671,700 Mpps: 4.8305660432872\nbytes: 2,888,177,580 packets: 48,136,293 bps: 2,163,687,512 Mpps: 4.5076823170883\nbytes: 2,932,730,040 packets: 48,878,834 bps: 2,196,794,784 Mpps: 4.5766558005224\nbytes: 1,992,924,060 packets: 33,215,401 bps: 1,506,907,704 Mpps: 3.1393910511536\nbytes: 3,093,302,580 packets: 51,555,043 bps: 2,272,759,007 Mpps: 4.7349145986862\nBENCH (size 200 (1000 pkts) for 5 iters, 10 secs)\nbytes: 7,793,340,828 packets: 39,761,943 bps: 5,946,834,283 Mpps: 3.7926239054501\nbytes: 8,086,524,684 packets: 41,257,779 bps: 6,178,524,066 Mpps: 3.9403852463692\nbytes: 8,181,886,720 packets: 41,744,320 bps: 6,245,499,104 Mpps: 3.9830989187226\nbytes: 7,162,425,452 packets: 36,542,987 bps: 5,467,070,948 Mpps: 3.4866523908569\nbytes: 8,044,202,600 packets: 41,041,850 bps: 6,025,642,111 Mpps: 3.8428839994153\nBENCH (size 200 (5000 pkts) for 5 iters, 10 secs)\nbytes: 7,435,783,124 packets: 37,937,669 bps: 5,316,610,007 Mpps: 3.3906951578136\nbytes: 7,104,245,204 packets: 36,246,149 bps: 5,322,134,921 Mpps: 3.3942187000055\nbytes: 7,060,566,800 packets: 36,023,300 bps: 5,196,164,451 Mpps: 3.3138803897482\nbytes: 8,704,353,532 packets: 44,409,967 bps: 6,581,658,084 Mpps: 4.1974860230122\nbytes: 6,964,267,884 packets: 35,531,979 bps: 5,163,723,627 Mpps: 3.2931910891152\nBENCH (size 200 (10000 pkts) for 5 iters, 10 secs)\nbytes: 8,960,865,004 packets: 45,718,699 bps: 6,656,818,050 Mpps: 4.2454196748629\nbytes: 8,616,566,112 packets: 43,962,072 bps: 6,572,625,809 Mpps: 4.1917256440365\nbytes: 8,982,657,264 packets: 45,829,884 bps: 6,863,528,718 Mpps: 4.3772504585072\nbytes: 7,370,641,740 packets: 37,605,315 bps: 5,414,566,386 Mpps: 3.4531673384103\nbytes: 7,768,253,416 packets: 39,633,946 bps: 5,558,911,447 Mpps: 3.5452241374124\nBENCH (size 400 (1000 pkts) for 5 iters, 10 secs)\nbytes: 11,370,978,432 packets: 28,714,592 bps: 8,516,503,220 Mpps: 2.6882901580412\nbytes: 11,523,519,612 packets: 29,099,797 bps: 8,396,065,624 Mpps: 2.6502732400069\nbytes: 11,515,038,480 packets: 29,078,380 bps: 8,627,239,199 Mpps: 2.723244696698\nbytes: 11,119,005,216 packets: 28,078,296 bps: 8,245,050,840 Mpps: 2.6026044320985\nbytes: 11,296,050,084 packets: 28,525,379 bps: 8,163,239,831 Mpps: 2.5767802499122\nBENCH (size 400 (5000 pkts) for 5 iters, 10 secs)\nbytes: 11,510,349,048 packets: 29,066,538 bps: 8,756,350,212 Mpps: 2.7639994358153\nbytes: 11,490,348,276 packets: 29,016,031 bps: 8,687,880,317 Mpps: 2.7423864639385\nbytes: 11,268,639,756 packets: 28,456,161 bps: 8,129,533,996 Mpps: 2.5661407816943\nbytes: 11,494,982,664 packets: 29,027,734 bps: 8,610,374,284 Mpps: 2.7179211755278\nbytes: 10,887,651,720 packets: 27,494,070 bps: 8,081,555,265 Mpps: 2.5509959803068\nBENCH (size 400 (10000 pkts) for 5 iters, 10 secs)\nbytes: 10,747,805,508 packets: 27,140,923 bps: 7,804,359,129 Mpps: 2.4634971998134\nbytes: 10,983,153,852 packets: 27,735,237 bps: 7,936,385,042 Mpps: 2.5051720463053\nbytes: 10,966,896,864 packets: 27,694,184 bps: 8,059,028,467 Mpps: 2.5438852487005\nbytes: 10,886,399,964 packets: 27,490,909 bps: 8,311,811,844 Mpps: 2.623677981111\nbytes: 10,995,697,548 packets: 27,766,913 bps: 8,400,542,821 Mpps: 2.6516864967191\nBENCH (size 800 (1000 pkts) for 5 iters, 10 secs)\nbytes: 11,636,648,380 packets: 14,618,905 bps: 8,703,234,501 Mpps: 1.366713960617\nbytes: 11,866,649,396 packets: 14,907,851 bps: 9,065,951,620 Mpps: 1.423673307294\nbytes: 11,498,235,124 packets: 14,445,019 bps: 8,078,105,307 Mpps: 1.2685466878074\nbytes: 11,832,229,560 packets: 14,864,610 bps: 9,040,561,320 Mpps: 1.4196861370782\nbytes: 11,637,161,004 packets: 14,619,549 bps: 8,710,566,569 Mpps: 1.3678653531761\nBENCH (size 800 (5000 pkts) for 5 iters, 10 secs)\nbytes: 11,637,212,744 packets: 14,619,614 bps: 8,717,927,604 Mpps: 1.369021294672\nbytes: 11,824,699,400 packets: 14,855,150 bps: 9,027,581,171 Mpps: 1.4176477970239\nbytes: 11,821,105,460 packets: 14,850,635 bps: 8,612,492,195 Mpps: 1.352464226747\nbytes: 11,820,057,128 packets: 14,849,318 bps: 8,937,885,867 Mpps: 1.4035624792465\nbytes: 11,820,138,320 packets: 14,849,420 bps: 8,692,461,521 Mpps: 1.3650222238867\nBENCH (size 800 (10000 pkts) for 5 iters, 10 secs)\nbytes: 11,312,541,060 packets: 14,211,735 bps: 8,415,938,960 Mpps: 1.3215984548653\nbytes: 11,321,376,660 packets: 14,222,835 bps: 8,323,783,179 Mpps: 1.3071267555305\nbytes: 11,328,348,028 packets: 14,231,593 bps: 8,258,508,758 Mpps: 1.2968763753559\nbytes: 11,003,158,148 packets: 13,823,063 bps: 7,598,128,537 Mpps: 1.1931734511879\nbytes: 11,349,940,324 packets: 14,258,719 bps: 8,670,502,246 Mpps: 1.3615738453088\nBENCH (size 1516 (1000 pkts) for 5 iters, 10 secs)\nbytes: 12,054,619,584 packets: 7,972,632 bps: 8,190,412,316 Mpps: 0.67711742033928\nbytes: 12,002,240,880 packets: 7,937,990 bps: 8,990,602,378 Mpps: 0.74327069927688\nbytes: 11,926,202,400 packets: 7,887,700 bps: 8,928,613,376 Mpps: 0.73814594717243\nbytes: 12,057,038,784 packets: 7,974,232 bps: 8,263,649,834 Mpps: 0.68317210930077\nbytes: 11,880,332,856 packets: 7,857,363 bps: 8,812,794,663 Mpps: 0.72857098738441\nBENCH (size 1516 (5000 pkts) for 5 iters, 10 secs)\nbytes: 11,970,824,544 packets: 7,917,212 bps: 9,159,577,324 Mpps: 0.75724018886785\nbytes: 11,938,048,920 packets: 7,895,535 bps: 8,859,316,142 Mpps: 0.73241700912824\nbytes: 11,953,044,936 packets: 7,905,453 bps: 9,124,178,354 Mpps: 0.75431368673544\nbytes: 11,547,523,512 packets: 7,637,251 bps: 8,258,038,960 Mpps: 0.68270824738908\nbytes: 11,959,469,424 packets: 7,909,702 bps: 9,135,323,441 Mpps: 0.75523507285074\nBENCH (size 1516 (10000 pkts) for 5 iters, 10 secs)\nbytes: 11,434,436,496 packets: 7,562,458 bps: 8,290,216,918 Mpps: 0.68536846215748\nbytes: 11,731,428,072 packets: 7,758,881 bps: 8,706,718,074 Mpps: 0.7198014281074\nbytes: 11,754,670,536 packets: 7,774,253 bps: 8,977,837,655 Mpps: 0.74221541464048\nbytes: 11,740,989,960 packets: 7,765,205 bps: 8,554,694,082 Mpps: 0.70723330708757\nbytes: 11,742,972,192 packets: 7,766,516 bps: 8,879,809,235 Mpps: 0.73411121329346\nNote that the bps is payload bits per second, not wire bits per second.  Anyway.  If I then remove the FNV hash and use the SipHash1,2 that's default for ctables after #1155, as is done in https://github.com/Igalia/snabb/tree/ipfix, then I get:\n$ sudo program/ipfix/tests/bench.sh 03:00.0 83:00.0 03:00.1 3\nProbe on 03:00.0. Packetblaster on 83:00.0.\nBENCH (size 64 (1000 pkts) for 5 iters, 10 secs)\nbytes: 2,862,697,500 packets: 47,711,625 bps: 2,080,904,979 Mpps: 4.3352187069202\nbytes: 2,497,746,960 packets: 41,629,116 bps: 1,772,785,205 Mpps: 3.6933025112978\nbytes: 2,598,707,700 packets: 43,311,795 bps: 1,946,803,826 Mpps: 4.0558413042383\nbytes: 2,504,879,460 packets: 41,747,991 bps: 1,876,369,579 Mpps: 3.9091032907837\nbytes: 2,511,337,980 packets: 41,855,633 bps: 1,899,147,082 Mpps: 3.9565564212358\nBENCH (size 64 (5000 pkts) for 5 iters, 10 secs)\nbytes: 2,831,509,980 packets: 47,191,833 bps: 2,163,600,259 Mpps: 4.5075005401681\nbytes: 2,721,287,100 packets: 45,354,785 bps: 2,077,201,197 Mpps: 4.3275024945833\nbytes: 2,549,200,500 packets: 42,486,675 bps: 1,856,400,156 Mpps: 3.8675003263137\nbytes: 2,581,977,240 packets: 43,032,954 bps: 1,935,180,710 Mpps: 4.0316264810792\nbytes: 2,815,469,460 packets: 46,924,491 bps: 1,978,406,106 Mpps: 4.1216793876588\nBENCH (size 64 (10000 pkts) for 5 iters, 10 secs)\nbytes: 2,754,434,580 packets: 45,907,243 bps: 2,035,798,928 Mpps: 4.2412477678422\nbytes: 2,846,689,020 packets: 47,444,817 bps: 2,173,614,331 Mpps: 4.5283631915685\nbytes: 2,587,728,300 packets: 43,128,805 bps: 1,902,687,092 Mpps: 3.9639314430278\nbytes: 2,888,965,500 packets: 48,149,425 bps: 2,206,655,246 Mpps: 4.5971984301358\nbytes: 2,414,391,660 packets: 40,239,861 bps: 1,808,646,909 Mpps: 3.7680143957243\nBENCH (size 200 (1000 pkts) for 5 iters, 10 secs)\nbytes: 8,724,824,948 packets: 44,514,413 bps: 6,294,216,361 Mpps: 4.0141685981377\nbytes: 8,436,785,112 packets: 43,044,822 bps: 6,319,518,444 Mpps: 4.0303051304775\nbytes: 8,709,648,276 packets: 44,436,981 bps: 6,344,991,026 Mpps: 4.0465503992432\nbytes: 9,347,894,248 packets: 47,693,338 bps: 7,136,011,151 Mpps: 4.551027520119\nbytes: 7,875,309,204 packets: 40,180,149 bps: 5,586,036,030 Mpps: 3.562522978774\nBENCH (size 200 (5000 pkts) for 5 iters, 10 secs)\nbytes: 9,663,753,540 packets: 49,304,865 bps: 7,383,533,104 Mpps: 4.708885908626\nbytes: 7,912,546,264 packets: 40,370,134 bps: 5,712,803,707 Mpps: 3.6433697115603\nbytes: 8,210,951,560 packets: 41,892,610 bps: 6,211,692,889 Mpps: 3.961538832871\nbytes: 8,879,551,072 packets: 45,303,832 bps: 6,586,995,803 Mpps: 4.2008901805131\nbytes: 9,051,079,884 packets: 46,178,979 bps: 6,536,110,080 Mpps: 4.1684375512409\nBENCH (size 200 (10000 pkts) for 5 iters, 10 secs)\nbytes: 9,292,046,792 packets: 47,408,402 bps: 7,032,106,492 Mpps: 4.4847617936532\nbytes: 7,324,075,276 packets: 37,367,731 bps: 5,336,083,521 Mpps: 3.4031144909645\nbytes: 8,056,787,760 packets: 41,106,060 bps: 6,037,332,905 Mpps: 3.8503398635178\nbytes: 9,254,213,108 packets: 47,215,373 bps: 6,803,174,718 Mpps: 4.3387593870346\nbytes: 8,463,399,952 packets: 43,180,612 bps: 6,168,844,400 Mpps: 3.9342119902863\nBENCH (size 400 (1000 pkts) for 5 iters, 10 secs)\nbytes: 11,366,245,044 packets: 28,702,639 bps: 8,346,270,377 Mpps: 2.6345550435406\nbytes: 11,378,068,812 packets: 28,732,497 bps: 8,286,950,335 Mpps: 2.615830282523\nbytes: 11,204,684,964 packets: 28,294,659 bps: 8,396,449,934 Mpps: 2.6503945501208\nbytes: 11,456,457,012 packets: 28,930,447 bps: 8,663,514,604 Mpps: 2.73469526655\nbytes: 11,405,706,840 packets: 28,802,290 bps: 8,540,339,151 Mpps: 2.6958141260237\nBENCH (size 400 (5000 pkts) for 5 iters, 10 secs)\nbytes: 11,476,782,900 packets: 28,981,775 bps: 8,758,801,242 Mpps: 2.7647731194967\nbytes: 11,397,432,420 packets: 28,781,395 bps: 8,230,368,211 Mpps: 2.5979697635744\nbytes: 11,461,714,704 packets: 28,943,724 bps: 8,753,783,571 Mpps: 2.7631892585228\nbytes: 11,375,963,676 packets: 28,727,181 bps: 8,681,582,155 Mpps: 2.7403984076618\nbytes: 11,486,779,524 packets: 29,007,019 bps: 8,689,718,613 Mpps: 2.7429667342002\nBENCH (size 400 (10000 pkts) for 5 iters, 10 secs)\nbytes: 11,013,314,796 packets: 27,811,401 bps: 8,447,111,388 Mpps: 2.6663861707098\nbytes: 10,981,962,288 packets: 27,732,228 bps: 8,229,096,631 Mpps: 2.5975683812095\nbytes: 11,006,915,832 packets: 27,795,242 bps: 8,320,358,511 Mpps: 2.62637579264\nbytes: 10,951,293,276 packets: 27,654,781 bps: 8,206,161,155 Mpps: 2.5903286477165\nbytes: 10,981,670,832 packets: 27,731,492 bps: 8,222,860,370 Mpps: 2.5955998644764\nBENCH (size 800 (1000 pkts) for 5 iters, 10 secs)\nbytes: 11,846,238,364 packets: 14,882,209 bps: 8,957,150,422 Mpps: 1.4065876920007\nbytes: 11,734,260,268 packets: 14,741,533 bps: 8,625,736,918 Mpps: 1.3545441141433\nbytes: 11,759,042,932 packets: 14,772,667 bps: 8,647,596,157 Mpps: 1.3579767835149\nbytes: 10,852,329,680 packets: 13,633,580 bps: 7,434,277,725 Mpps: 1.1674431101109\nbytes: 11,734,479,964 packets: 14,741,809 bps: 8,874,772,878 Mpps: 1.3936515198735\nBENCH (size 800 (5000 pkts) for 5 iters, 10 secs)\nbytes: 11,720,461,608 packets: 14,724,198 bps: 8,949,083,912 Mpps: 1.4053209661335\nbytes: 11,742,670,804 packets: 14,752,099 bps: 8,631,850,589 Mpps: 1.355504175563\nbytes: 11,836,551,044 packets: 14,870,039 bps: 8,952,590,281 Mpps: 1.4058715894411\nbytes: 11,841,155,904 packets: 14,875,824 bps: 9,040,472,360 Mpps: 1.4196721672273\nbytes: 11,828,226,476 packets: 14,859,581 bps: 9,027,469,579 Mpps: 1.4176302731967\nBENCH (size 800 (10000 pkts) for 5 iters, 10 secs)\nbytes: 11,332,004,852 packets: 14,236,187 bps: 8,602,109,738 Mpps: 1.3508338157053\nbytes: 11,333,618,344 packets: 14,238,214 bps: 8,570,384,126 Mpps: 1.3458517786125\nbytes: 11,343,425,860 packets: 14,250,535 bps: 8,496,459,686 Mpps: 1.3342430411993\nbytes: 11,299,239,104 packets: 14,195,024 bps: 8,232,811,280 Mpps: 1.2928409674255\nbytes: 11,342,722,196 packets: 14,249,651 bps: 8,578,734,529 Mpps: 1.3471630855928\nBENCH (size 1516 (1000 pkts) for 5 iters, 10 secs)\nbytes: 11,915,157,240 packets: 7,880,395 bps: 8,683,293,376 Mpps: 0.71786486243388\nbytes: 11,915,656,200 packets: 7,880,725 bps: 9,009,712,697 Mpps: 0.74485058678764\nbytes: 12,022,376,184 packets: 7,951,307 bps: 9,006,623,044 Mpps: 0.74459515909052\nbytes: 12,049,884,000 packets: 7,969,500 bps: 8,864,014,533 Mpps: 0.73280543432414\nbytes: 11,884,867,344 packets: 7,860,362 bps: 8,900,514,824 Mpps: 0.73582298485383\nBENCH (size 1516 (5000 pkts) for 5 iters, 10 secs)\nbytes: 11,965,063,824 packets: 7,913,402 bps: 9,177,536,036 Mpps: 0.75872487076421\nbytes: 11,943,909,432 packets: 7,899,411 bps: 8,950,450,409 Mpps: 0.73995125740562\nbytes: 11,148,730,488 packets: 7,373,499 bps: 7,906,438,267 Mpps: 0.65364072976378\nbytes: 11,937,557,520 packets: 7,895,210 bps: 9,029,299,128 Mpps: 0.74646983533436\nbytes: 11,932,694,928 packets: 7,891,994 bps: 9,023,866,689 Mpps: 0.74602072495545\nBENCH (size 1516 (10000 pkts) for 5 iters, 10 secs)\nbytes: 11,755,535,400 packets: 7,774,825 bps: 8,904,408,967 Mpps: 0.73614492121847\nbytes: 11,675,533,968 packets: 7,721,914 bps: 8,585,664,120 Mpps: 0.70979366074173\nbytes: 11,682,344,016 packets: 7,726,418 bps: 8,590,722,137 Mpps: 0.71021181692513\nbytes: 11,680,782,120 packets: 7,725,385 bps: 8,358,003,613 Mpps: 0.69097252093621\nbytes: 11,758,751,424 packets: 7,776,952 bps: 8,976,427,730 Mpps: 0.74209885339564\nNow my ad-hoc graphing game isn't really on point at the moment so I don't have a nice view as to which is better.  I should also note that I saw some variance before with the 64-byte case, with one case going to 5.1MPPS; it happened to be with SipHash but the variance was high enough that perhaps that's not a crucial detail.  However :)  It does seem that switching to SipHash helps perf, so I think it's a good change.. How would making /var/run/snabb readable directly enable backdoors?  I would have thought that would require write capability, but I am missing something :). Sounds good to me.  We should note that this is a information leakage -- allowing access to mapped DMA memory is obviously allowing processes to read Snabb's memory.  We may need to add a knob in the future, but this is probably good enough for now.. Fixed the test failure by fixing up the test in question, and fixed the doc failure by merging from next.  Ready to go in, I think :). Where is the code here?\nI would guess there's some increasing amount of polymorphism.  Could be wrong of course.. I think just use maskmovps then.  It's the same thing, runs just as fast.. lgtm!. Failing test appears to be a property-based test, that two runs produce the same output; of course ctable entries can indeed be reordered if the hash seed changes.  grr. I did some performance testing; no regressions for lwaftr as far as I can tell.. Hi, is this a bug report? :)  Please provide more information including a summary of the problem, steps to reproduce, what output or behavior you expected, and what output or behavior you observed.  Thanks :). Would it be OK if this one waited until after the 2017.08 release?  I would like to try it out with other network functions over a longer timeframe before landing.. Sounds great.  I look forward to working with RaptorJIT (and studio :). One pulls packets into the network; the network then processes those packets by calling push functions of the apps in the network.  First, all apps with pull functions get their pull functions called -- that pulls packets off the NIC and into Snabb.  Then for all links like a.out -> b.in, Snabb will call b:push() so that b can take packets off of its in input link and \"process\" those packets, possibly pushing them out some other link.. This PR builds on https://github.com/snabbco/snabb/pull/1149 but also rebases it.  Probably we can close #1149, wdyt @takikawa ?. Thanks for the review @dpino!  Merged to wingo-next.. I believe the CI error here is spurious:\nERROR snabbnfv-iperf-1500-tunnel+crypto -> 0.847917 of 0.96 (SD: 0.0363318 )\nThere's nothing that this patch touches that's related to that.. Does #1288 completely supersede this PR?. Looks about right but also needs support in apps/config.. A newer update on this phenomenon: https://blog.cloudflare.com/on-the-dangers-of-intels-frequency-scaling/. regarding avx512, it could be only a subset of features; https://github.com/openssl/openssl/commit/79337628702dc5ff5570f02d6b92eeb02a310e18. Here's some more docs on how haswell and broadwell behave:\nhttps://en.wikichip.org/wiki/intel/frequency_behavior#Base.2C_Non-AVX_Turbo.2C_and_AVX_Turbo. I guess I am mixing the two things: the frequency throttling, and the transition latency.  All Xeon chips since E5 v3 I think have different base frequencies when executing AVX2 versus not.  For Skylake and later, this is a per-core thing and depends on which kinds of AVX you're talking about; before (e.g. E5 v3 Haswell) it's much more coarse and per-socket (i.e., one core runs AVX2, all cores on die capped at the \"AVX2 turbo max\", which if you have turbo boost off as we usually do, is the \"AVX2 base frequency\", commonly 25% slower than marked frequency; e.g. on E5-2620v3 it is 2.1 Ghz vs 2.6 Ghz nominal).\nAs far as transition latency goes, it is hard to find numbers.  Fabian Giesen describes his experience here: https://gist.github.com/rygorous/32bc3ea8301dba09358fd2c64e02d774.  For us probably given that we process a new batch of packets every 100 us, if we use AVX2, we never really transition out of it.  Even then, I think the original comment on the intel forum is wrong: there's not a complete delay; 256-bit instructions are still retired, only they're shunted through the 128-bit unit and there's a lot of overhead in that transition period.\nStepping back: I think most Snabb apps are not computationally limited by the kinds of instructions that AVX2 could speed up.  Consider the following test case, run on our E5-2620 v3 (Haswell-EP):\n[wingo@snabb1:~/snabb/src]$ sudo taskset -c 3 ./snabb snabbmark hash\n[pmu /sys/devices/cpu/rdpmc: 1 -> 2]\nbaseline: 9.08 cycles, 3.11 ns per iteration (result: 0)\nmurmur hash (32 bit): 27.02 cycles, 8.45 ns per iteration (result: 593689054)\nsip hash c=1,d=2 (x1): 28.04 cycles, 8.76 ns per iteration (result: 1740456520)\nsip hash c=1,d=2 (x2): 21.99 cycles, 6.87 ns per iteration (result: 764162108)\nsip hash c=1,d=2 (x4): 13.26 cycles, 4.14 ns per iteration (result: 3982310842)\nsip hash c=1,d=2 (x8): 12.56 cycles, 3.93 ns per iteration (result: 2201963874)\nsip hash c=2,d=4 (x1): 43.24 cycles, 13.52 ns per iteration (result: 3246680284)\nsip hash c=2,d=4 (x2): 39.92 cycles, 12.48 ns per iteration (result: 3394909516)\nsip hash c=2,d=4 (x4): 22.09 cycles, 6.91 ns per iteration (result: 1928778572)\nsip hash c=2,d=4 (x8): 21.33 cycles, 6.67 ns per iteration (result: 2241889326)\nHere, the x1 variants use normal scalar x86-64 assembly; the x2 variants use SSE and 128-bit registers; and the x4 and x8 variants use AVX2 and 256-bit registers.\nI see the results and I think well of course, the AVX2 variants are best, great.  But how much is a network function limited by hashing??  If this is the only AVX2 in a program, probably very little.  If processing a packet takes 200 non-AVX cycles, then that's 77ns at 2.6 Ghz (nominal clock rate), but 95ns at 2.1 Ghz (AVX base clock rate).  The difference of almost 20ns for the packet probably isn't made up by increased efficiency of AVX2.\nSo, we should consider not using 256-bit registers in Snabb. It could be a decent solution to just use SSE4 or at least the 128-bit subset of AVX2, to avoid the frequency limitations.. Althernately, as \"light\" AVX2 instructions seem to run at full rate on Skylake and later, perhaps we limit them to that processor family -- i.e. disable on Haswell.. Documentation for turbo bins on Xeon E5 servers: https://www.intel.com/content/dam/www/public/us/en/documents/specification-updates/xeon-e5-v3-spec-update.pdf\nFor the machines in Igalia's lab (2x E5-2620 v3), some excerpts from those charts here:\n\n\nInterestingly for this SKU, the AVX2 turbo bins are the same as the non-AVX turbo bins.  I don't think that's generally the case, though the charts don't make it easy to compare.. For me I have two questions:\n(1) Should we recommend that turbo boost be enabled?  In what circumstances?\n(2) Should we back off from YMM (256-bit) register use in Snabb for the moment?  Does this decision depend on whether turbo boost is enabled or not on our targets, whether all cores or just one are in use, whether we're talking about production or a benchmarking machine, and what specific part we're talking about?. Regarding turbo boost -- I think that if you want the highest possible performance, you should enable it.  If only one core is active, your perf boost could be up to 33% (3.2 Ghz vs 2.4 Ghz on the E5-2620v3).  If multiple cores are active, say half your cores, then the boost is more modest: 2.9 Ghz on the E5-2620v3, or 20%.  If all cores are active, the boost is minimal, somewhere around 5-10%.\nHowever -- if you\n  * cannot tolerate temperature or load-related downward changes in frequency\n  * need to do benchmarking to help determine whether a software change is good or not and you don't want results to depend on temperature or how many other cores are active\nThen you need to disable turbo boost.\n\nNow, how does this relate to AVX2?\nThe thing is, with turbo boost off, using AVX2 will downclock the CPU to the AVX2 \"base frequency\".  On Skylake and later, this is per-core, but on Haswell-EP and Broadwell-EP (Xeon E5 v3 and v4, respectively) it's per-socket apparently.  For the E5-2620 v3 it's 12.5% lower (2.1 Ghz vs 2.4 Ghz).  Although 256-bit code can do more per cycle, unless the performance is limited by the AVX2 code, the result will be that all the non-AVX2 code runs slower, so your code gets slower.  Using AVX2 on a machine with turbo boost off is a lose.\nHowever!  With turbo boost on, the CPU may \"turbo\" up to the corresponding AVX2 turbo bin, which in the case of the E5-2620v3 match the non-AVX2 speeds.  So in that case, as long as you run AVX2 often enough to avoid the warmup delay, then using AVX2 may indeed improve performance!!!\nI have some measurements showing me thing but I need to collect some more with turbo boost disabled.  Will report back with details.. Incidentally here's a better link on how to understand haswell turbo bins: https://www.microway.com/knowledge-center-articles/detailed-specifications-intel-xeon-e5-2600v3-haswell-ep-processors/\nFrom that page, all-core frequency ranges:\n\nFrequency ranges with a single core active:\n\nComparison for skylake: https://www.microway.com/knowledge-center-articles/detailed-specifications-of-the-skylake-sp-intel-xeon-processor-scalable-family-cpus/. An HPC-flavor review of Skylake, Broadwell, and Haswell, with attention to turbo boost: https://indico.cern.ch/event/668301/contributions/2732540/attachments/1572581/2485483/TurboBoostUpAVXClockDownPrelim.pdf. In an absolutely bonkers perplexing result -- I don't see any downclocking when running AVX2 without turbo boost on our servers.  Measured by looking at /proc/cpuinfo.  Granted, only one core was active when I was running these tests.\nThe Snabb lwAFTR uses AVX2 in three places:\n  * Validation of checksums on incoming IPv4 packets.\n  * SipHash to find binding table entry for incoming packets, and to detect hairpinning.\n  * Parallel streaming copy of multiple hash table entries as once.  (AVX2 lets us load more per cycle.)\nI tested the lwAFTR using the snabb loadtest find-limit tool, which does a bisection on applied load from 0 Gbps to 10 Gbps, to determine the highest load at which no packets are dropped.  The lwAFTR was run like this, on our E5-2620v3:\nsudo ./snabb lwaftr run --cpu 11 --name lwaftr --ingress-drop-monitor=warn --conf program/lwaftr/tests/data/icmp_on_fail.conf --on-a-stick 82:00.1\nThis test configuration has only a handful of binding table entries, so it doesn't test cache footprint of the binding table.  The load generator was run like this:\nfor i in `seq 5`; do sudo ./snabb loadtest find-limit -D 1 --cpu=3 program/lwaftr/tests/benchdata/ipv4-0094.pcap NIC NIC 02:00.1 | tail -1; done\nThis does 5 bisections.  At each point, load is applied for one second.  If no packets are dropped, then that's a pass, bisection continues at a higher load level.  If packets are dropped, the load-tester retries three times, then fails, continuing the bisection at a lower load level.  The tail -1 makes it so that we get 5 final results.\nThis test load is a single packet, repeated millions of times.  It means that all packets are going to take the same branches, on the lwaftr side.  It's not a great test in that sense, but it does do all the computation for each packet, so I think it's a fine test for AVX2 vs not AVX2.\nTo test without AVX2, I manually short-cutted the tests in the checksum and siphash modules that could take advantage of AVX2.  For multi_copy.dasl, I rewrote it in SSE2.\nResults:\n\nSummary:\nW T F\nThe different bars represent different invocations of lwaftr run.  I did 5 loadtests for each invocation, hence the little histogram on each bar.  The loadtest results within an invocation are fairly constant, but there's some inter-invocation variance.\nConclusion is that disabling AVX2 code slows down the system.  I think this is because on this system, we aren't actually penalized back to the AVX2 base clock rate.  I suspect that's because this is a memory-limited benchmark, so we're not really burning the CPU.  But it invalidates the whole theoretical analysis above, which makes me mad!!!!!!!!!!!!!. Maybe a better chart here; tweaked some of the display parameters:\n\n. Sadly, getting rid of lwcounter is going to require some changes that need to go in lwaftr first.  I'll retarget a new PR to Igalia/lwaftr.. Closing; going to land via Igalia/lwaftr.. Superseded by https://github.com/Igalia/snabb/pull/891.. Nice!  Any perf notes?. Honestly as regards external users -- I think Snabb should offer no stability guarantees of any kind for out-of-tree code.  Correct and thorough-enough change logs yes, stability no.  In that regard I think it's an anti-goal to keep API or ABI stability, as it's cost that actively prevents benefits!  API includes but is not limited to app names, module names, link names, FFI type names, app configurations, and module exports.\nEven for in-tree code I think we should offer only a best-effort approach.  If e.g. intel_mp changes name to intel, then it's easy to just rename all uses in the tree, no problem.  But if there's a bigger change and no one is maintaining an in-tree app or VNF or whatever, I think we should have an expectation that we can remove that code after some period of non-maintenance.  Snabb will change and that's a good thing; the challenge is how to enable change.\nIf a downstream out-of-tree user doesn't have time to update, that's fine, they don't have to update!  They have branched from some release, and when they are ready to update, they can as part of their internal processes of merging a new Snabb release.  System working as expected.\nFor me the concerns in this PR are not about compatibility from an API POV, they are about performance, reliability, and feature parity.  Is this driver rock-solid?  We don't know yet.  For that reason I would keep intel10g around for another release or three, in case people need to compare.  But then assuming all goes well I would delete it, sure, as it is providing no more benefit.  People will find out that the file is gone when their require('apps.intel.intel_app') starts throwing exceptions :). Changes look great, thanks!. Wow that's nuts, good catch!  I guess our main uses have been power-of-2 sized.  Another possibility would be to hack LuaJIT to turn the division into a multiplication, perhaps; http://www.hackersdelight.org/divcMore.pdf.\nRegarding reassembly, incidentally I had a properly extracted version of the IPv4 reassembler that I made on Monday, moments before my laptop was stolen :P  Oh well.. I haven't heard of anyone doing this yet.  Sounds like fun :). @lukego Sure.  We have been running this on lwaftr and it's the only patch to src/core/app.lua (i.e. no followups), so I am pretty confident.  If you don't get around to it though, it is coming via lwaftr.. Aaah, I thought I had pushed this one somewhere!  IN the meantime, dug it out and merged to #1231 via #1232.  Closing this one.. @lukego wingo-next contains pretty solid intel_mp changes from @takikawa and Nicola, only that there was one regression on i210 cards (iirc) that needs sorting out (FAILED: ./test_1g_sw_sem.snabb).  I would like to get that fixed and merged this week then I will get a lwaftr upstreaming sent upstream; we just finished a development cycle and so have a bunch of things to send.  If you need to make a Snabb release before merging wingo-next (#1231) that's fine too.. The build failure appears to be just that the LPM tests need CPU affinity, or they error out.  I made a patch some weeks ago to fix this but due to Snabb's needlessly complicated merge policies, now we have to live with every PR to master failing for the next month :(\n/cc @lukego; #1232 fixes this error. \"Failure\" appears to be a simple benchmark statistical anomaly unrelated to this patch; merging.  Thanks @dpino!. Tests passing on the Igalia snabb1 server, so I merge to wingo-next:\n[wingo@snabb1:~/snabb/src]$ sudo SNABB_PCI_INTEL0=81:00.0 SNABB_PCI_INTEL1=81:00.1 apps/intel_mp/selftest.sh \n[mounting /var/run/snabb/hugetlbfs]\n./test_10g_1q_blast.sh: line 9:  1724 Killed                  SNABB_SEND_BLAST=true ./testsend.snabb $SNABB_PCI_INTEL1 0 source.pcap\nPASSED: ./test_10g_1q_blast.sh\n./test_10g_1q_blast_vmdq.sh: line 9:  1832 Killed                  SNABB_SEND_BLAST=true ./testsend.snabb $SNABB_PCI_INTEL1 0 source.pcap\nPASSED: ./test_10g_1q_blast_vmdq.sh\n./test_10g_2q_blast.sh: line 11:  1846 Killed                  SNABB_SEND_BLAST=true ./testsend.snabb $SNABB_PCI_INTEL1 0 source.pcap\nPASSED: ./test_10g_2q_blast.sh\n./test_10g_2q_blast_vlan.sh: line 18:  1863 Killed                  SNABB_SEND_BLAST=true ./testsend.snabb $SNABB_PCI_INTEL1 0 source-vlan.pcap\nPASSED: ./test_10g_2q_blast_vlan.sh\n./test_10g_2q_blast_vmdq_auto.sh: line 19:  1887 Killed                  SNABB_SEND_BLAST=true ./testsend.snabb $SNABB_PCI_INTEL1 0 source2.pcap\nPASSED: ./test_10g_2q_blast_vmdq_auto.sh\n./test_10g_2q_blast_vmdq.sh: line 19:  1911 Killed                  SNABB_SEND_BLAST=true ./testsend.snabb $SNABB_PCI_INTEL1 0 source2.pcap\nPASSED: ./test_10g_2q_blast_vmdq.sh\n./test_10g_come_and_go.sh: line 13:  1935 Killed                  SNABB_SEND_BLAST=true ./testsend.snabb $SNABB_PCI_INTEL1 0 source.pcap\nPASSED: ./test_10g_come_and_go.sh\nPASSED: ./test_10g_linkup.sh\nPASSED: ./test_10g_rate_limit.snabb\nPASSED: ./test_10g_rss_tab.snabb\nPASSED: ./test_10g_rxq_disable.snabb\nPASSED: ./test_10g_sw_sem.snabb\nload: time: 1.00s  fps: 2,044     fpGbps: 0.001 fpb: 0   bpp: 0    sleep: 100 us\nPASSED: ./test_10g_txq_stop.snabb\nPASSED: ./test_10g_vlan.sh\nPASSED: ./test_10g_vmdq_mirror.snabb\nload: time: 0.14s  fps: 11,777,669 fpGbps: 6.501 fpb: 203 bpp: 60   sleep: 0   us\nload: time: 0.10s  fps: 12,659,977 fpGbps: 6.986 fpb: 102 bpp: 59   sleep: 0   us\nload: time: 0.10s  fps: 12,101,023 fpGbps: 6.680 fpb: 203 bpp: 60   sleep: 0   us\nPASSED: ./test_10g_vmdq_pool_sel.snabb\nPASSED: ./test_10g_vmdq_race.snabb\nload: time: 0.14s  fps: 11,778,056 fpGbps: 6.501 fpb: 203 bpp: 60   sleep: 0   us\nload: time: 0.11s  fps: 12,013,707 fpGbps: 6.629 fpb: 203 bpp: 59   sleep: 0   us\nPASSED: ./test_10g_vmdq_reconfig_mac.snabb\nPASSED: ./test_10g_vmdq_tx.sh. Yay the tests pass, after fixing the bugs that I introduced!  I think this branch is ready to go!. This patch went into the lwaftr via https://github.com/Igalia/snabb/pull/927, and allows tests to complete when there is no CPU affinity.. Applied to wingo-next in #1231.. I am having trouble with the upstream bots for some reason; will PR this against Igalia/snabb to see what those bots think.. There is a test failure due to an intel_mp change that's already upstream, for intel1g cards only.  Merging this code as it's unrelated and we'll fix the intel1g problem later.. Excellent investigation, thank you!  Note that we still do need to do some investigation to see how max_displacement varies with different random seed instances, but that can come later.. Rebased to fix a bug in packetblaster.. This build failure is only ERROR snabbnfv-iperf-1500 -> 0.828682 of 5.16 (SD: 0.709084 ).  If I interpret this correctly, that would indicate perf of 4.28, which is 0.884 off from the mean; a bit more than a standard deviation, again if I understand it correctly.  I looked to see and it seems we don't have any pending perf patches in lwaftr.  So, I will push this branch to wingo/optimize to see what Hydra thinks of its perf.. FWIW @lukego I still get some value from these tests.  In particular the snabbnfv-loadgen-dpdk test on #1231 let me know that the bench mode of snabbnfv was broken with that branch, so I landed a fix.  In this case the benchmark could be an early warning perf error, but it's more of an indicator that I need to look more into Hydra results, whereas sometimes I don't bother.  So I appreciate them being there.\nWhether the snabb-bot benchmarks should gate merges or not is another question :). Merging as it seems to fix the benchmark issue.. It would seem that perf results are OK: https://hydra.snabb.co/eval/7681. In a way that's good to know that the results are the same.  I was unsure whether to blame a voodoo change to the software itself or some problem with the statistics!. Appears there is a regression here: https://hydra.snabb.co/build/2944856/download/2/report.html  Will investigate.  AFAIU since this is murren, it's not NIC-related at all.. Merged in from next to remove jit.tracebarrier() calls.  Let's see how perf is now :). New benchmarks here, after removing jit.tracebarrier: https://hydra.snabb.co/build/2977392/download/2/report.html.  Seems to be a strong progression in iperf, I suspect due to getting the repeater to run in the beginning of the breathe cycles by changing from a push function to a push and a pull function; l2fwd results are mixed but appear neutral.. OK as this is merged to lwaftr upstream and wingo-next has the rest of the lwaftr updates, I will pull the trigger here and merge to wingo-next, so that it all goes into v2017.12.. whoops, wrong target branch. I think this is a good patch!  I think though that we should avoid merging unreviewed patches.  Probably you just asked on slack; in that case we can note the reviewer in the comments.  Something to fix for next time :). Awesome!!!!  Given that it just affects program.l2vpn, I think we should merge as-is.  I would do it already if wingo-next weren't already blocked due to a perf regression; as is if no one else gets to it, I will merge when wingo-next flushes to next.. Given that wingo-next has a lot of changes to some of the same stuff and that it's ready to go modulo some unrelated perf regression, I propose holding off here; unless you want to change the base branch to wingo-next of course.  WDYT?. Is it possible to leave out the trace and profile generation?  Or to have it controllable by a switch?  Part of me wants to preserve that \"quiet strace\" thing that current Snabb dataplanes have.  Or is this irrational?. Ah good.  Yeah syscalls, writes, all that during JIT code generation are no problem at all.  We could spawn an Xeyes window and it would be fine :)  I am only concerned about steady-state syscall load.  \nSo what about profiling?  That isn't continuous and always-on and always writing to disk somewhere?. What's the current timeline for getting trace/profile data out of a Snabb process?\nHow is this branch doing as far as performance goes?  The snabb-bot data is generally good but showing snabbnfv iperf having up to 100% slowdown.\nI guess the test suite needs fixing up as well (above error is a core.memory error).\nI am available to work on this soon, just trying to get your feeling here.. I am working on raptorjit+snabb currently.  The branch is https://github.com/Igalia/snabb/tree/raptorjit.  Current additions relative to this branch are https://github.com/Igalia/snabb/pull/1032, a merge from master (https://github.com/Igalia/snabb/pull/1033), and a fix for DynASM 64-bit immediates (#1302).. From what I can see of the log there are two blockers here:\n(1) Not so good perf on basic1-100e6; that could be statistical though.  Better to use hydra here\n (2) A failure in the tap app's selftest.  Seems to be legit.  I didn't see it locally because I don't run with SNABB_TAPTEST.. The lib.numa selftest should also work on non-NUMA systems.  It works for me.  What's the deal?  I would appreciate to be asked to review lib.numa changes.. I think this PR would be better if the LPM test disabling and the lib.numa changes first had their own PRs before reaching max-next.  I had the impression that max-next is a merge branch and not a development branch; is that correct?. @eugeneia I've never seen that error before; weird.  Yes please revert, the proper thing to do is to fix the bug.  Can you open an issue for that please?  I believe the error corresponds to an \"ENOSYS\" from ljsyscall, probably on the \"S.get_mempolicy\" syscall.  What kernel version are you running?  Did you specialize the kernel configuration perhaps?. Cc @dpino for the correctness; @eugeneia I wonder if we introduced this lib.numa problem with the patch for #1200, and I wonder if this might fix it.. I think the test failure is due to a race condition in the property-based test, whereby for some reason the tester couldn't connect to the dataplane being tested.  In general #1294 should solve the irritation, and we should fix these on the Igalia side of things.. LGTM.  I still prefer to pretend that a system without non-uniform memory is a NUMA system with just one node, but this is fine :). Neat.  Note, i think one of the virtues of the current generic implementation is the simplicity; you get to check optimized results against something that's more or less comprehensible and more or less easy to check against the reference implementation.  But perhaps that isn't so important.  Where do you see this going?. It would be nice if the benchmarks printed comparable numbers -- nanoseconds per byte and nanoseconds per checksum.. What system did you use to check the timings?  Can you try with the E5-2620v3 servers we have and also with a skylake?  I guess a skylake laptop or desktop, given that I don't think we have skylake xeon servers yet.  In the context of https://github.com/snabbco/snabb/issues/1194#issuecomment-412836640 I think we should probably remove the AVX2 and SSE variants.  Probably need to add a \"snabbmark\" case for this hash versus the C hash.  I think also you need to do a randomized test to make sure this version computes the same as the reference one written in C; i.e. for all lengths from the min to the max, generate a few random packets, compute checksum via C and dynasm, and assert dynasm result equals C.. Just an example run on my old Ivy Bridge i7-3770 desktop:\n$ sudo taskset -c 2 ./snabb snabbmark checksum\nC: Size=44 bytes; MPPS=14 M: 30.16 cycles, 7.88 ns per iteration (result: 30438); 0.18 ns per byte\nASM: Size=44 bytes; MPPS=14 M: 21.35 cycles, 5.60 ns per iteration (result: 30438); 0.13 ns per byte\nC: Size=550 bytes; MPPS=2 M: 227.78 cycles, 59.74 ns per iteration (result: 15425); 0.11 ns per byte\nASM: Size=550 bytes; MPPS=2 M: 139.13 cycles, 36.50 ns per iteration (result: 15425); 0.07 ns per byte\nC: Size=1516 bytes; MPPS=1 M: 610.06 cycles, 159.51 ns per iteration (result: 8540); 0.11 ns per byte\nASM: Size=1516 bytes; MPPS=1 M: 386.77 cycles, 101.32 ns per iteration (result: 8540); 0.07 ns per byte\nOn our old E5-2620v3 (Haswell-EP) server:\n$ sudo taskset -c 4 ./snabb snabbmark checksum\n[pmu /sys/devices/cpu/rdpmc: 1 -> 2]\nC: Size=44 bytes; MPPS=14 M: 27.07 cycles, 11.29 ns per iteration (result: 14151); 0.26 ns per byte\nASM: Size=44 bytes; MPPS=14 M: 20.04 cycles, 8.35 ns per iteration (result: 14151); 0.19 ns per byte\nC: Size=550 bytes; MPPS=2 M: 342.32 cycles, 142.64 ns per iteration (result: 36504); 0.26 ns per byte\nASM: Size=550 bytes; MPPS=2 M: 133.46 cycles, 55.61 ns per iteration (result: 36504); 0.10 ns per byte\nC: Size=1516 bytes; MPPS=1 M: 942.14 cycles, 392.58 ns per iteration (result: 41872); 0.26 ns per byte\nASM: Size=1516 bytes; MPPS=1 M: 380.32 cycles, 158.51 ns per iteration (result: 41872); 0.10 ns per byte. On a Skylake mobile CPU (i7-7500U):\n$ sudo taskset -c 1 ./snabb snabbmark checksum\nNo PMU available: CPU not recognized: GenuineIntel-6-8E\nC: Size=44 bytes; MPPS=14 M: 7.08 ns per iteration (result: 14899); 0.16 ns per byte\nASM: Size=44 bytes; MPPS=14 M: 5.81 ns per iteration (result: 14899); 0.13 ns per byte\nC: Size=550 bytes; MPPS=2 M: 93.85 ns per iteration (result: 13253); 0.17 ns per byte\nASM: Size=550 bytes; MPPS=2 M: 24.20 ns per iteration (result: 13253); 0.04 ns per byte\nC: Size=1516 bytes; MPPS=1 M: 268.12 ns per iteration (result: 6361); 0.18 ns per byte\nASM: Size=1516 bytes; MPPS=1 M: 69.42 ns per iteration (result: 6361); 0.05 ns per byte. For me this is LGTM!. Belated LGTM :). Looking really great.  I like the design and the implementation looks good too :)\nFor comparison, VPP's \"memif\" is not dissimilar, and it additionally allows for third-party apps to participate in a flow: https://docs.fd.io/vpp/18.01/libmemif_doc.html / https://docs.fd.io/vpp/18.01/db/df1/src_2plugins_2memif_2memif_8h_source.html\nWe might envision something like interlinks allowing this in some future, perhaps like snabb firehose.  Anyway, that's thinking a bit farther on :)  Great work!!!!. Just missing documentation and benchmarking to merge!. The reason I provided feedback on the final two points (max_packets and core->apps layering violation) is that they represent an accumulation of technical debt and cruft.  I think it's reasonable to ask for new features, especially those that touch core, to not add fixmes.  It's fine to fix in a followup of course.. Regarding the propbased test failure, that is #1294.. lgtm :). Merged onto wingo-next, thanks!. This PR is now based on https://github.com/Igalia/snabb/pull/1049, which adds a Lua implementation of stdio, unrelated to fibers except in that it provides the extension points needed to layer cooperative non-blocking concurrency on top.  Apologies for any confusion but this was the right way to do it!. I am especially pleased at the small diff here -- this is a full-featured concurrent ML implementation in less than 1000 physical lines of code, including tests!!!\nwingo@sparrow ~/src/snabb/src$ git diff --stat lua-io..fibers\n src/lib/fibers/channel.lua | 129 +++++++++++++++++++++++++++++++++++++++\n src/lib/fibers/epoll.lua   |  87 +++++++++++++++++++++++++++\n src/lib/fibers/fiber.lua   |  98 ++++++++++++++++++++++++++++++\n src/lib/fibers/file.lua    | 124 ++++++++++++++++++++++++++++++++++++++\n src/lib/fibers/op.lua      | 150 ++++++++++++++++++++++++++++++++++++++++++++++\n src/lib/fibers/sched.lua   |  86 ++++++++++++++++++++++++++\n src/lib/fibers/sleep.lua   |  59 ++++++++++++++++++\n src/lib/fibers/timer.lua   | 170 ++++++++++++++++++++++++++++++++++++++++++++++++++++. Closing, as fibers came into lwaftr via https://github.com/Igalia/snabb/pull/1060 and will make it upstream eventually.  Wheee ~~~~. @eugeneia a candidate for the fixes branch?  it would improve CI indicators for open pull requests. Are intel_mp tests not running on CI???  I don't see them in the log.\nI don't see the collision possibility, do you? Unbinding is idempotent and we keep our same locking discipline as before.. See comment at https://github.com/Igalia/snabb/pull/1031#issuecomment-374173979 for more details about the failure.. I get the same error if I put require('test.lua') at the top of core/startup.lua, so it's nothing that Snabb Lua code is doing.. Heh, I think the problem is https://github.com/snabbco/snabb/pull/1264/commits/c2c2ed2a9716636fa817376e876b09ab256242a8; these variables need to be static. Weird.  I changed those file-local variables to be static (clearly they are meant to be static) but I still have the problem.  So I built a custom binary thinking that maybe the way we were linking was causing some problem, but no:\n```\nwingo@sparrow ~/src/raptorjit-snabb$ cat test.c\n/ Use of this source code is governed by the Apache 2.0 license; see COPYING. /\ninclude \ninclude \"lua.h\"\ninclude \"lualib.h\"\ninclude \"lauxlib.h\"\ninclude \nint main(int argc, char *argv)\n{\n  lua_State L = luaL_newstate();\n  luaL_openlibs(L);\n  return luaL_dostring(L, \"loadfile('./test.lua')()\");\n}\nwingo@sparrow ~/src/raptorjit-snabb$ gcc -Ilib/luajit/src -Wl,--no-as-needed -Wl,-E -Werror -Wall -o test test.c lib/luajit/src/raptorjit.a -lrt -lc -ldl -lm -lpthread\nwingo@sparrow ~/src/raptorjit-snabb$ ./test\nopening\nopened  3\n```. Additionally the commit https://github.com/snabbco/snabb/commit/c2c2ed2a9716636fa817376e876b09ab256242a8 makes a second copy of lj_auditlog.c, I think by mistake (as it's in src/lj_auditlog.c, its location in raptorjit).  That causes the second copy to be linked into snabb, and I didn't notice the second copy which also has the non-static variables.  Fixing the static variable issue in raptorjit is good but not necessary; removing the double-included file does appear necessary.. Or heh, it seems it's just that you can't embed loads from immediate 64-bit addresses.  Have to load the immediate address into a register with movq (which has special support for 64-bit immediates), then dereference the register; fair enough.. Blocks https://github.com/snabbco/snabb/pull/1264, fix in the works. Blocks https://github.com/snabbco/snabb/pull/1264. This bug is a bit weird.  What the selftest does is attempt to run a test case for a while, with stateful filtering on and off, and also with native compilation on and off.  The error I was initially seeing was the fourth combination: stateful filtering on, and native compilation on.\nHowever those conditions aren't necessary to reproduce the bug.  Simply change the selftest to do this:\nfunction selftest ()\n   print(\"selftest: pcap_filter\")\n   for i=1,10 do\n      selftest_run(false, 3.726, 0.0009)\n   end\n   print(\"selftest: ok\")\nend\nThat will run with the default Lua-generating pflua backend, and with no stateful filtering.  It fails for me on the fourth iteration:\nwingo@sparrow ~/src/raptorjit-snabb/src$ make && sudo strace -ttTv -o /tmp/log ./snabb snsh -t apps.packet_filter.pcap_filter\nmake: 'snabb' is up to date.\nselftest: pcap_filter\nRun for 1 second (stateful = false)...\nlink report:\n             634,275 sent on pcap_filter.output -> sink.input (loss rate: 0%)\n          17,019,720 sent on repeater.output -> pcap_filter.input (loss rate: 0%)\n                 161 sent on source.output -> repeater.input (loss rate: 0%)\nok: accepted 3.7267% of inputs (within tolerance)\nRun for 1 second (stateful = false)...\nlink report:\n             503,751 sent on pcap_filter.output -> sink.input (loss rate: 0%)\n          13,517,346 sent on repeater.output -> pcap_filter.input (loss rate: 0%)\n                 161 sent on source.output -> repeater.input (loss rate: 0%)\nok: accepted 3.7267% of inputs (within tolerance)\nRun for 1 second (stateful = false)...\nlink report:\n             478,383 sent on pcap_filter.output -> sink.input (loss rate: 0%)\n          12,836,598 sent on repeater.output -> pcap_filter.input (loss rate: 0%)\n                 161 sent on source.output -> repeater.input (loss rate: 0%)\nok: accepted 3.7267% of inputs (within tolerance)\nRun for 1 second (stateful = false)...\nlink report:\n                 185 sent on pcap_filter.output -> sink.input (loss rate: 0%)\n               4,998 sent on repeater.output -> pcap_filter.input (loss rate: 0%)\n                 161 sent on source.output -> repeater.input (loss rate: 0%)\nerror: accepted 3.7015% (expected 3.726% +/- 0.00090)\napps/packet_filter/pcap_filter.lua:157: selftest failed. I confirm that the modified selftest works fine on master with luajit; no performance degradation and no blacklist.. I attempted to see if recent heuristic changes in RaptorJIT are relevant.\nI reverted these patches individually:\n * f7212ccb97ea39effb9c311b5a7affc8f66d3bf2\n * 01dc8448637f80151fa8de4ede765ff81efe7536\n * ce6fbb4240fa3d626b6a1eb8ddafd3d36b335751\n * f5d810e97c1d38f3ead28f965c328dc7eac487e9\nHowever the behavior is the same.. I think I don't have time to solve this one until I am back from ONS.  @lukego perhaps it would make a good test for raptorjit tooling -- without -jv I feel like I'm driving blind!. Thank you for this fix!  I can confirm this seems to fix the problem.. Fixed in https://github.com/Igalia/snabb/pull/1035 and https://github.com/Igalia/snabb/pull/1036. Fixes #1302 . @eugeneia i think it's already on raptorjit; but it is also good for mainline i think.  want to merge to max-next perhaps?. Current status: only blocked on #1303.  All other tests passing AFAIU.. Interestingly, the other times I manage to get the thing to crash, the backtrace is pretty much the same:\n```\n(gdb) bt\n0  0x0000000000452481 in lj_alloc_free (msp=0x7ffff7fcf010, ptr=) at lj_alloc.c:1266\n1  0x0000000000419bbd in gc_sweep (g=g@entry=0x7ffff7fcf3d8, p=0x7ffff6b90238, lim=25, lim@entry=40) at lj_gc.c:383\n2  0x000000000041a835 in gc_onestep (L=L@entry=0x7ffff7fcf378) at lj_gc.c:610\n3  0x000000000041af4d in lj_gc_step (L=L@entry=0x7ffff7fcf378) at lj_gc.c:659\n4  0x000000000045b4c0 in lj_cf_ffi_cast (L=0x7ffff7fcf378) at lib_ffi.c:542\n5  0x00000000004176c5 in lj_BC_FUNCC ()\n6  0x000000000045897a in lj_cf_package_require (L=0x7ffff7fcf378) at lib_package.c:322\n7  0x00000000004176c5 in lj_BC_FUNCC ()\n8  0x000000000045897a in lj_cf_package_require (L=0x7ffff7fcf378) at lib_package.c:322\n9  0x00000000004176c5 in lj_BC_FUNCC ()\n10 0x0000000000412dc8 in lua_pcall (L=, nargs=, nresults=, errfunc=) at lj_api.c:1074\n11 0x000000000040d413 in main ()\n(gdb) bt full\n0  0x0000000000452481 in lj_alloc_free (msp=0x7ffff7fcf010, ptr=) at lj_alloc.c:1266\n    F = 0x7ffff6ced1c8\n    B = <optimized out>\n    I = 0\n    nsize = <optimized out>\n    p = 0x7ffff6ced200\n    fm = 0x7ffff7fcf010\n    psize = 80\n    next = 0x7ffff6ced250\n\n1  0x0000000000419bbd in gc_sweep (g=g@entry=0x7ffff7fcf3d8, p=0x7ffff6b90238, lim=25, lim@entry=40) at lj_gc.c:383\n    ow = 33\n    o = <optimized out>\n\n2  0x000000000041a835 in gc_onestep (L=L@entry=0x7ffff7fcf378) at lj_gc.c:610\n    old = 171146182\n    g = 0x7ffff7fcf3d8\n\n3  0x000000000041af4d in lj_gc_step (L=L@entry=0x7ffff7fcf378) at lj_gc.c:659\n    g = 0x7ffff7fcf3d8\n    lim = 1600\n    ostate = -2\n\n4  0x000000000045b4c0 in lj_cf_ffi_cast (L=0x7ffff7fcf378) at lib_ffi.c:542\n    id = 9\n    o = 0x7ffff6c90a58\n\n5  0x00000000004176c5 in lj_BC_FUNCC ()\nNo symbol table info available.\n6  0x000000000045897a in lj_cf_package_require (L=0x7ffff7fcf378) at lib_package.c:322\n    name = 0x7ffff7fd9168 \"core.main\"\n    i = 1\n\n7  0x00000000004176c5 in lj_BC_FUNCC ()\nNo symbol table info available.\n8  0x000000000045897a in lj_cf_package_require (L=0x7ffff7fcf378) at lib_package.c:322\n    name = 0x7ffff7fd8e58 \"core.startup\"\n    i = 1\n\n9  0x00000000004176c5 in lj_BC_FUNCC ()\nNo symbol table info available.\n10 0x0000000000412dc8 in lua_pcall (L=, nargs=, nresults=, errfunc=) at lj_api.c:1074\n    g = 0x7ffff7fcf3d8\n    oldh = 0 '\\000'\n    ef = <optimized out>\n    status = <optimized out>\n\n11 0x000000000040d413 in main ()\n```\nThis second run was from re-running in the same GDB session; perhaps ASLR doesn't apply to multiple runs within one GDB session?  I just wonder because the addresses are pretty much the same.  Slight difference on the limit in frame 1 but the structure is the same, the outer addresses are the same, and the address on the innermost frame is the same.  I had to re-run 20 or 30 times to get this failure though.  Will try again in separate GDB sessions.. Nope, seems consistent, even when running in separate GDB instances.. Replacing lj_alloc.c's\n```c\ndefine LJ_ALLOC_MMAP_PROBE 1\nwithc\ndefine LJ_ALLOC_MMAP32 1\ndefine LJ_ALLOC_MMAP_PROBE 0\n```\ndoes not fix the problem, therefore I conclude that the problem is not related to the block allocator rewrite from 2016.. Removing the ffi.gc call in lib.ctable doesn't fix it either :/. I have a reliable reproducer!  Drop into test.lua and run as snabb snsh test.lua:\n```lua\nlocal ctable = require('lib.ctable')\nlocal ffi = require('ffi')\nlocal bit = require('bit')\nprint(\"selftest: ctable\")\nlocal occupancy = 1e5\nlocal params = {\n   key_type = ffi.typeof('uint32_t[1]'),\n   value_type = ffi.typeof('int32_t[1]'),\n}\nlocal ctab = ctable.new(params)\n-- Fill with {i} -> { bnot(i) }.\nlocal k = ffi.new('uint32_t[1]');\nlocal v = ffi.new('int32_t[1]');\nfor i = 1,occupancy do\n   k[0] = i\n   for j=0,0 do v[j] = bit.bnot(i) end\n   ctab:add(k, v)\nend\ncollectgarbage()\nprint('hey!!!')\n-- Save the table out to disk, reload it, and run the same\n-- checks.\nlocal tmp = os.tmpname()\ndo\n   local file = io.open(tmp, 'wb')\n   local function write(ptr, size)\n      local str = ffi.string(ptr, size)\n      file:write(ffi.string(ptr, size))\n   end\n   local stream = {}\n   function stream:write_ptr(ptr, type)\n      assert(ffi.sizeof(ptr) == ffi.sizeof(type))\n      write(ptr, ffi.sizeof(type))\n   end\n   function stream:write_array(ptr, type, count)\n      write(ptr, ffi.sizeof(type) * count)\n   end\n   ctab:save(stream)\n   file:close()\nend\ndo\n   local file = io.open(tmp, 'rb')\n   -- keep references to avoid GCing too early\n   local handle = {}\n   local function read(size)\n      local buf = ffi.new('uint8_t[?]', size, file:read(size))\n      table.insert(handle, buf)\n      return buf\n   end\n   local stream = {}\n   function stream:read_ptr(type)\n      return ffi.cast(ffi.typeof('$', type), read(ffi.sizeof(type)))\n   end\n   function stream:read_array(type, count)\n      return ffi.cast(ffi.typeof('$', type),\n                      read(ffi.sizeof(type) * count))\n   end\n   ctab = ctable.load(stream, params)\n   file:close()\nend       \nos.remove(tmp)\nprint('collecting...')\ncollectgarbage()\nprint('collected.')\n```\nIt will crash at the last collectgarbage() call with the backtrace as above.. Actually excuse me, it doesn't have the same backtrace because this time it's an explicit collectgarbage call that initiates the sweep instead of a ffi.cast call.  Backtrace:\n```\n(gdb) bt full\n0  0x0000000000452481 in lj_alloc_free (msp=0x7ffff7fcf010, ptr=) at lj_alloc.c:1266\n    F = 0x7ffff6ced1c8\n    B = <optimized out>\n    I = 0\n    nsize = <optimized out>\n    p = 0x7ffff6ced200\n    fm = 0x7ffff7fcf010\n    psize = 80\n    next = 0x7ffff6ced250\n\n1  0x0000000000419bbd in gc_sweep (g=g@entry=0x7ffff7fcf3d8, p=0x7ffff6dcc950, lim=13, lim@entry=40) at lj_gc.c:383\n    ow = 33\n    o = <optimized out>\n\n2  0x000000000041a835 in gc_onestep (L=L@entry=0x7ffff7fcf378) at lj_gc.c:610\n    old = 17830369\n    g = 0x7ffff7fcf3d8\n\n3  0x000000000041b128 in lj_gc_fullgc (L=0x7ffff7fcf378) at lj_gc.c:717\n    g = 0x7ffff7fcf3d8\n    ostate = -2\n\n4  0x000000000041311d in lua_gc (L=L@entry=0x7ffff7fcf378, what=what@entry=2, data=) at lj_api.c:1183\n    g = 0x7ffff7fcf3d8\n    res = 0\n\n5  0x0000000000453202 in lj_cf_collectgarbage (L=0x7ffff7fcf378) at lib_base.c:426\n    res = <optimized out>\n    opt = 2\n    data = <optimized out>\n\n6  0x00000000004176c5 in lj_BC_FUNCC ()\nNo symbol table info available.\n7  0x00000000004532e8 in lj_cf_dofile (L=0x7ffff7fcf378) at lib_base.c:406\n    fname = <optimized out>\n\n8  0x00000000004176c5 in lj_BC_FUNCC ()\nNo symbol table info available.\n9  0x000000000045897a in lj_cf_package_require (L=0x7ffff7fcf378) at lib_package.c:322\n    name = 0x7ffff7fd9168 \"core.main\"\n    i = 1\n\n10 0x00000000004176c5 in lj_BC_FUNCC ()\nNo symbol table info available.\n11 0x000000000045897a in lj_cf_package_require (L=0x7ffff7fcf378) at lib_package.c:322\n    name = 0x7ffff7fd8e58 \"core.startup\"\n    i = 1\n\n12 0x00000000004176c5 in lj_BC_FUNCC ()\nNo symbol table info available.\n13 0x0000000000412dc8 in lua_pcall (L=, nargs=, nresults=, errfunc=) at lj_api.c:1074\n    g = 0x7ffff7fcf3d8\n    oldh = 0 '\\000'\n    ef = <optimized out>\n    status = <optimized out>\n\n---Type  to continue, or q  to quit---\n14 0x000000000040d413 in main ()\nNo symbol table info available.\n```\nThere is a bit of nonderminacy in the test because the ctable's hash function is randomized, but I get the same crash every time.. Slightly shorter reproducer.\n```lua\nlocal ctable = require('lib.ctable')\nlocal ffi = require('ffi')\nlocal params = {\n   key_type = ffi.typeof('uint32_t[1]'),\n   value_type = ffi.typeof('int32_t[1]'),\n}\nprint('creating empty ctable')\nlocal ctab = ctable.new(params)\nprint('first gc')\ncollectgarbage()\nprint('writing ctable out to a file')\nlocal tmp = os.tmpname()\nlocal file = io.open(tmp, 'wb')\nlocal function write(ptr, size)\n   local str = ffi.string(ptr, size)\n   file:write(ffi.string(ptr, size))\nend\nlocal stream = {}\nfunction stream:write_ptr(ptr, type)\n   assert(ffi.sizeof(ptr) == ffi.sizeof(type))\n   write(ptr, ffi.sizeof(type))\nend\nfunction stream:write_array(ptr, type, count)\n   write(ptr, ffi.sizeof(type) * count)\nend\nctab:save(stream)\nfile:close()\nprint('reading ctable back in from file')\nlocal file = io.open(tmp, 'rb')\nlocal anchor = {}\nlocal function read(size)\n   local buf = ffi.new('uint8_t[?]', size, file:read(size))\n   table.insert(anchor, buf)\n   return buf\nend\nlocal stream = {}\nfunction stream:read_ptr(type)\n   return ffi.cast(ffi.typeof('$', type), read(ffi.sizeof(type)))\nend\nfunction stream:read_array(type, count)\n   return ffi.cast(ffi.typeof('$', type),\n                   read(ffi.sizeof(type) * count))\nend\nctab = ctable.load(stream, params)\nfile:close()\nos.remove(tmp)\nprint('clearing anchor')\n-- If you comment out this line, the test passes.\nanchor = nil\nprint('final collection...')\ncollectgarbage()\nprint('success.')\n```\nI am still a bit perplexed.. Got a better example that doesn't depend on anything outside LuaJIT!\n```lua\nlocal ffi = require('ffi')\nprint('creating data')\nlocal byte_t = ffi.typeof('uint8_t')\nlocal byte_count = 72\nlocal data = ffi.new('uint8_t[?]', byte_count)\nprint('making string out of data')\ndata_str = ffi.string(data, byte_count)\nprint('loading data from string')\nlocal anchor = {}\nlocal function read(size)\n   assert(size == #data_str)\n   local buf = ffi.new('uint8_t[?]', #data_str, data_str)\n   table.insert(anchor, buf)\n   return buf\nend\nlocal function read_array(type, count)\n   return ffi.cast(ffi.typeof('$*', type),\n                   read(ffi.sizeof(type) * count))\nend\nlocal data2 = read_array(byte_t, byte_count)\nprint('clearing anchor')\n-- If you comment out this line, the test passes.\nanchor = nil\nprint('final collection...')\ncollectgarbage()\nprint('success.')\n```. Well -- I can't get it to repro just running raptorjit.  Have to use snabb snsh currently.. This repro case is nice because there will be no JIT compilation at all.  Here's as short as I think I can get it:\n```lua\nlocal ffi = require('ffi')\n-- 72 bytes.\nlocal data_str =\n   '012345678901234567890123456789012345678901234567890123456789012345678901'\nprint('loading data from string')\nlocal anchor = {}\nlocal function read_bytes()\n   local buf = ffi.new('uint8_t[?]', #data_str, data_str)\n   table.insert(anchor, buf)\n   return ffi.cast('uint8_t*', buf)\nend\nlocal data2 = read_bytes()\nprint('clearing anchor')\n-- If you comment out this line, the test passes.\nanchor = nil\nprint('running gc...')\ncollectgarbage()\nprint('success.')\n```\nPut that in a test.lua and run as snabb snsh test.lua.. LOL, even smaller:\n```lua\nlocal ffi = require('ffi')\n-- 72 bytes.\nlocal data_str =\n   '012345678901234567890123456789012345678901234567890123456789012345678901'\nlocal buf = ffi.new('uint8_t[?]', #data_str, data_str)\n-- If you comment out this line, the test passes.\nbuf = nil\nprint('running gc...')\ncollectgarbage()\nprint('success.')\n```. FYI as with the last time I did do a grep on our source tree for '[?]', to find VLA FFI types that might be initialized with this same antipattern.  I didn't find any more other than this one.. Ah sorry, wrong target branch; closing.. Surely this should be v2018.04 or something? :). Hi!  This is a terrible error message :P  Thanks for reporting.\nUnfortunately it does correspond to an error situation -- Snabb does not implement drivers for these devices:\n```\nlspci | grep -i ether\n03:00.0 Ethernet controller: VMware VMXNET3 Ethernet Controller (rev 01)\n0b:00.0 Ethernet controller: VMware VMXNET3 Ethernet Controller (rev 01)\n```\nCurrently the lwAFTR targets only those devices that have native Snabb drivers, mainly the Intel 82599 / i210 / i350 cards.  For evaluation purposes, we should probably extend it to support tap interfaces, which would allow you to evaluate the lwAFTR, though not at high performance.\nIn the long term if you are interested in running the lwAFTR or some other Snabb network function on VMware devices, the right thing to do is to write a Snabb driver.  It's not so bad -- about 1000 lines or so of code, probably a bit less considering it's a virtual interface already.. Aah, the docs removal is an error!  Will fix.. Ping @eugeneia :-). Hydra results looking good I think: https://hydra.snabb.co/build/3366285/download/2/report.html. merging my own pr to my own branch, la la la\n\n. @lukego i think this one is good to go!. Code change looks good to me, but can we run a test that does reach the performance limits to compare before and after?  e.g. with 150-byte packets.. The error is in core.app's selftest:\nERROR during tests:\nsrc/testlog/core.app:\nRestarting app2 (died at 2079996.764626: core/app.lua:737: Push error.)\nRestarting app1 (died at 2079996.764626: core/app.lua:731: Pull error.)\nRestarting app2 (died at 2079998.764633: core/app.lua:737: Push error.)\nRestarting app1 (died at 2079998.764633: core/app.lua:731: Pull error.)\nRestarting app3 (died at 2080000.762363: core/app.lua:743: Report error.)\nRestarting app2 (died at 2080000.764703: core/app.lua:737: Push error.)\nRestarting app1 (died at 2080000.764703: core/app.lua:731: Pull error.)\nselftest: app\nempty -> c1\nc1 -> c1\nc1 -> c2\nc2 -> c1\nc1 -> empty\nc_fail\napps report:\napp3\napp2    [dead: core/app.lua:737: Push error.]\napp1    [dead: core/app.lua:731: Pull error.]\napps report:\napp3\napp2    [dead: core/app.lua:737: Push error.]\napp1 [dead: core/app.lua:731: Pull error.]\nWhich is weird.  I wonder if it's because the selftest runs core.app.main() twice and that now has raptorjit code and somehow there's a race condition there.  Unrelated to this PR, I think, but something to take a look at!. Thanks for doing that perf investigation!  All in all, LGTM.. Good idea.  I do hesitate a bit -- without a concurrency-safe hash table (something we don't want i think), we'd need to MAP_PRIVATE, but in that case a write might cause a new hugepage to be created, which might fail and fault!. Closing, will reopen as a merge from upstream.. Ah, we don't have ljsyscall's history in our repo -- better to reopen this bug, as it's the right thing.. Needs updating after the #1263 merge; would you mind updating?  Would be sufficient to merge in wingo-next and push the fix.. The perth-eugeneia builder looks like a build problem.  The davos-eugeneia problem looks is an interesting failure but spurious and not reproducible AFAIU, and not related to anything in this PR AFAIU.  For me this is good to go.  I wish our CI infrastructure were better tho :(. FYI the lwAFTR's loadgen uses its own implementation, copied from the rate limiter: https://github.com/snabbco/snabb/blob/master/src/apps/lwaftr/loadgen.lua#L39.. Bah, opened against the wrong repo; sorry about that!. Thanks for the test case!\nFWIW the consistency checker is actually run from lib.yang.yang.load_config_for_schema -- i.e. the lib.yang.yang module, not the lib.yang.data module.  This is because of some circularities between thelib.yang.data and lib.yang.path_data module.  Choosing the right place to run the constency checker wasn't obvious to us -- do you think it should be run elsewhere?\nRegarding snabb top, I entirely agree that it should support piping to a file. It should detect whether the output is a TTY or not and act accordingly.  You can see there's the beginning of that code there but it's not yet implemented; I think it should be, though!. @eugeneia It looks like I actually misunderstood one of the patches, looking back... The thing that I thought was uniqueness constraints was something else, relating to influxdb output syntax (https://github.com/Igalia/snabb/pull/1058#issue-185456848, see the definition of ULEAF).  Uniqueness constraints are not yet implemented (but the consistency checker is the right place for it i think).  I will revise the PR description.  Thank you for the test case and apologies for the noise!. Following a Slack conversation, it seems that removing the old \"snabb top\" was an error -- @eugeneia still uses it in some ways.  However we think that the new \"snabb top\" is more like \"top\" -- and the parts of the old one that were useful were more like \"ps\".  This PR also removes \"snabb ps\" incidentally -- so we've freed up that name.  Therefore the proposal is to re-add the old \"snabb top\", but as \"snabb ps\".. We don't actually want to negotiate checksum offloading, do we?\n. can we indent by 3 spaces? :)\n. My inner Max cringes at the thought of more undocumented functions here :)  To me, it would seem an easier merge to not add these to core.lib.\n. Likewise the dump function is probably not needed at all in core.packet, and we probably shouldn't be adding the metatype here -- or if we do, we should document it.  I believe these changes are from @javierguerragiraldez originally, and though they are likely very nice changes it will simplify our lives to not include them in this PR.  Seems to me, anyway; wdyt?\n. I believe Luke's convention is \"local function foo (a, b)\".  Note local and function on the same line, and space before the paren, not after.  Also note that we should be indenting by 3 spaces.  Thanks :)\n. This would appear to call cksum_generic on all transmitted packets.  AFAIU we should not be doing this -- it's the app's responsibility.  We don't do this on intel cards and it seems there is no reason to do it on virtio interfaces.  Besides that, this calls the generic checksum routine, not the avx2 optimized one; at the least we should be using lib.checksum.ipsum instead.\n. AFAIU this change is unrelated to the patch; feel free to submit separately, or not at all if you don't feel like submitting it.\n. A final note here -- I'm skeptical about the value of these functions being in lib.checksum.  I mean, it also seems that we don't need them, but if we do need them, to me they don't seem to be generic enough to warrant a place here, and instead we should define them closer to their use.\n. This function is a source of a trace abort.\ne.g. in a -jv output there are these lines\n[TRACE --- (20/0) zone.lua:38 -- NYI: unsupported variant of FastFunc ffi.typeof at lib.lua:713]\n[TRACE --- (20/0) zone.lua:38 -- NYI: unsupported variant of FastFunc ffi.typeof at lib.lua:713]\n[TRACE --- (20/0) zone.lua:38 -- NYI: unsupported variant of FastFunc ffi.typeof at lib.lua:713]\n[TRACE --- (20/0) zone.lua:38 -- NYI: unsupported variant of FastFunc ffi.typeof at lib.lua:713]\n[TRACE  22 (20/0) zone.lua:38 -- fallback to interpreter]\nAFAIU This means that a trace 22, coming from trace 20, met something it couldn't compile, so it is falling back to the interpreter.  I don't know why it printed it like 4 times, did it try 4 times?  I don't know.  Probably so, thinking maybe if it tried again the trace would go somewhere else.  It means that this allocation won't be sunk, the whole thing will go on the interpreted path, etc.  It's only used in one place; let's just inline it into its caller.  Same with fstruct and fieldrd.\n. what does this foo.__index = foo do?  I've usually seen metatables being set on objects like setmetatable(x, { __index=VirtioVirtq }) but I haven't seen something like this.  I am still learning though :)\n. Why not just error out here?  It would be the porter's job to fix this.\n. You might want to experiment unrolling this manually.  I got some significant speedups by having more loads in flight.\n. Yes, delicate indeed :)  One thing to try is instead of doing load, store, load, store, to do load, load, store, store.  That was what worked best for me.  Good luck :)\n. I was about to weigh in with support for avoiding these headers, but for folks like vosys and igalia that do work for multiple customers, potentially assigning some copyright to external entities, it's probably important to write down who owns that copyright.  I don't know of a nice way to avoid copyright headers in this regard.  [edit: we could use commit logs for this if needed]\n. One use case for this program is for when you ship a program to a customer and they report that nothing is working :)  In that case it's useful to have a simple snabb application they can try that doesn't depend on your application code, and which they can use to benchmark performance as well.\nI think we should steal ideas from the dpdk but we don't necessarily need to match the particular feature set of any given application.  In this case I think a minimal counterpart to packetblaster is a good feature set.\n. > Separate implementation details (these should be comments in the source) from user documentation\nWe can certainly remove implementation details from here, but the performance characteristics of this table are user-visible features and it's hard to explain them without describing how the table works.  WDYT?\n. > Prepend method definitions with the class name. E.g. ctable:new instead of :new\n\nWhy is ctable:new a function? Shouldn't it be a method?\n\nThis is a function exported by the module.  It is not a method on an object.  I believe this documentation is correct;  LMK if there are more concerns.\n. This isn't a typo: [] indicate inclusive bounds, () indicate exclusive bounds.  I think it's explained in the next sentence.\n. > Prepend method definitions with the class name. E.g. ctable:new instead of :new\nSo, new is a function, but there are many methods like resize.  I was a bit worried though that there would be confusion between the module name and the class name, and for that reason left off the LHS on these definitions.  In fact partly for that reason we don't mention the name \"class\" in these docs, as it doesn't really exist.  What's the right solution here?  Is this acceptable as I've described it above?\n. I do the same, but in this particular case I have been really weirded out by the OO conventions of the protocol libraries; I find them distinctly non-snabby.  Compare to core.packet for example.  Sometimes I avoid using the protocol libraries just because I find them too weird!\n. Just one more note:\n\nEdit: but please add the prefixes, e.g. \u201cFunction ctable.new ... returns a ctable.\u201d and \u201cMethod ctable:foo ...\u201d\n\nIn Function ctable.new, the left hand side ctable is the module:\nlua\nlocal ctable = require('lib.ctable')\nBut in \u201cMethod ctable:foo ...\u201d, the left-hand-side is just a ctable.  This method is not available otherwise!  For example you can't get to this function except as a property of a ctable.  Notably it is not an export of the `ctable' module.  You sure you want to document it in these two ways?\n. I would do this, but it has three drawbacks:\n- It doesn't allow for boolean values that default to true.\n- It modifies the incoming parameters.\n- It doesn't give an error if the user mis-types a parameter name.\nThe last one is the most important to me.\n. It's for if Snabb gets any more options.  LMK if you want s/while/if/ and s/do/then/; it will save a character ;)  By only checking for --help / -h the existing code won't detect invalid options, instead treating them as program names; for that reason I thought being more general would be more reliable.\n. Excuse my denseness, I have trouble knowing what your comments mean for me.  I prefer to leave it in, you prefer to leave it out, however what I don't know is whether you as reviewer require me to change the word \"while\" to \"if\" and \"do\" to \"then\".  If so please let me know.  Tx :)\n. There are a lot of things that are great about better doc standards but I don't share the opinions in this \"general considerations\" section.  Concretely, my differing opinions:\n- Documentation can be boring but it does not have to be.\n- Documentation can be both good and fun.  Good is a requirement, fun is a nice-to-have.\n- 11 lines on being concise is too much\n- It feels like more of a blog post than documentation ;)  Or rather: too much opining for the guidelining.\n- It is a backhanded commentary on my latest doc merge without @-tagging me, but expressed as a PR instead of an issue to discuss.\n- A lost opportunity to use the word \"concision\", which sounds like scissors and also rhymes with \"precision\"!  A travesty!\n. for me, lines 102 to 131 can be removed.  They add no more meaning than the words \"precise\", \"concise\", and \"uniform\".  I also find them a bit preachy; YMMV.\n. Unrelated to above comments, but related to the line in question:  If there is ever more than one freelist used by a function, then this switch could cause branchy traces as each ffi.typeof('struct {}') will create a fresh type.  Makes me nervous!\n. Thanks for doing the refactor, looks great :)  I have two more nits before landing:\n1. Would you mind changing to unsigned integers please, unless you have a reason to use signed integers\n2. I wonder about \"struct freelist\" -- I can see this name as easily colliding with a name from some other user's module, struct tags being global and all.  Unless you need this struct to have a tag, in this case local freelist_t = ffi.typeof[[struct { ... }]] is more appropriate.  In that case I would change the ffi.new call below to be local packets_fl = freelist_t(max_packets, 0, max_packets).\nThanks!\n. Interesting!  Would you mind quantifying what you did to benchmark this change?\n. Sounds good to me, only thing is I'm not sure that align((64)) works with ffi.new.  It certainly wouldn't work if it were just malloc.  Incidentally, why do you consider alignment to be important if we are always working on the back of the list, a dynamic location which will likely be unaligned?  I also note that it's not the number of links in the network but the number of places the packet ingresses and egresses from Snabb.  For me I leave it up to @xrme to determine whether he wants to incorporate a change like this, I won't block the commit because of it, though I'm happy to accept follow-ups :)\n. Tx for feedback.  I wouldn't want to do this without some deeper perf testing however, and if we know that we're shifting by less than the MTU there shouldn't be a problem; dunno.\nRelatedly, shiftleft could underflow p.length.\n. No :)  It makes a new local variable named breathe which is initially bound to the value of the global breathe variable.  If we measure latency, then we overwrite our local copy but not the global variable.\n. Fine with me, I wasn't sure whether I wanted to propose this code to be \"core\", but if it seems \"core\" enough to you it's core enough for me :)  Sure, can re-use core.shm.root too.\n. yep, good catch\n. I don't understand what sort of error could occur if the user fails to supply one of these parameters.  For what it's worth, we have been using this bucket capacity to be a fine default in the lwaftr's load tester, across a broad range of throughputs, and there has been no problem.  As for rate, if you use the set_rate() api exclusively, as we do in the load tester via the promise-based transients, it doesn't matter what the initial rate you set is because you will overwrite it from the beginning.\nI think the burden is rather on you to specify the way in which these are harmful.\n. (Apologies, my message sounded rude; I didn't intend that.  My question was, what is the nature of the bug you are worried about when adding these defaults?)\n. I think there's no better or worse, both behaviors have their use case.  For processing real traffic, you will want to drop packets.  For processing synthetic traffic from a repeater or a pcap file, you won't.\n. Incidentally, boolean configuration values don't default well to \"true\"; you end up having to interpret a nil value to mean the default, when for the user it could be simply that they meant to specify a nil value as the boolean value for the parameter (which is a perfectly valid false value of course).  So if we control this via a boolean parameter I think it should be phrased in a way that the default is false :)\n. Right.  So, we need to be able to create load transients -- loads whose applied throughput varies over time.  We use the set_rate() interface for this.  Because a timeline naturally has a starting point, the natural thing for it to do is to call set_rate() in the beginning, and so it doesn't make sense to manage the limit of the rate limiter outside the timeline.\nRegarding typos, I agree that it can be a problem, and it goes for anything that takes tables of options.  I think rather than forbidding defaults, we should assert that options that are specified are known.  See https://github.com/SnabbCo/snabbswitch/blob/next/src/lib/ctable.lua#L74 for one possible solution.\nRegarding the bucket capacity -- while I think it can make sense to force the user to know about the algorithm, it's also OK to provide a default that just works.  It would seem that we can provide one, especially for synthetic workloads where we are not dropping packets.  Maybe it's just that I'm mostly concerned with that workload, and that with real traffic you want to think more about the bucket capacity.\n. Please provide a reason. Thanks.\n. To clarify. If you think my use case is invalid, that is fine, I can go back to use the specialized app. please let me know, thanks.\n. Good feedback; certainly!\n. I am actually not sure how important it is for this data structure to take precisely a page.  I agree though, will change.\n. Still uses the name \"snabb switch\", fwiw, here and below; not a blocker but FYI\n. I think this could be just app instead of app_table[name]; am I missing something?\n. Part of the design or spec should be what additional things you can do in start() compared to the constructor.  In your code, the links aren't set up, so we can't do linky things; dunno.\nTo expand on @dpino's use case -- we need the app name and we think waiting for the first push() is kinda hacky :)\n. Can you point to documentation for this @eugeneia?  To me it is useful to know that a packet is just a data and a length, and part of the core Snabb value proposition.  I haven't seen a PR that goes by that changes this, though I could have missed something :)\n. Yeah I know what you mean.  Still, Luke has given several presentations with a lovely slide that's just\n``` c\n// The maximum amount of payload in any given packet.\nenum { PACKET_PAYLOAD_SIZE = 10*1024 };\n// Packet of network data, with associated metadata.\nstruct packet {\n    unsigned char data[PACKET_PAYLOAD_SIZE];\n    uint16_t length;           // data payload length\n};\n```\nand so I dunno, it's part of the public messaging.  There are quite a lot of uses out there already fwiw, and many of them introduced since you made the data/length functions in January last year.\n. this needs to be just now().  i wonder how this worked before :)\n. do we need to require('jit').flush() instead?  not sure.\n. Yeah, sorry that this crept in; good catch :)  Does making this point to a non-lwaftr document in snabbco/snabb work for you?  If so I will make sure that such a document exists and use its URL.\n. I don't know how to do that and I don't want to describe something that hasn't landed on master yet.  In a followup perhaps.\n. I have the feeling that when people were tweaking ring buffer sizes in the hopes of dropping fewer packets, they were grasping at straws... if you are fine with it I prefer this tone.  If we think there should be a new default we should do more science and change the default, not recommend that users choose a specific different default.  As it is users are welcome to do what experimenting they like.\n. Good catch\n. I think it's implicit here.  I am interpreting \"might be nice\" comments to mean \"think about it but nothing required\" and I have thought and here I choose not to act :)  Please do lmk if I misinterpret this.\n. What do you mean? :)  This does not ring any bells.\n. Yes.  IMO people should use the ingress drop monitor or the ingress drop counter, mentioned later.\n. I don't remember enough to say something here.  Do you? :)\n. Not sure how this squares with this point -- if the oldest supported CPU is Sandy Bridge, it's not just a perf drop, sometimes iommu on means you get no packets at all iirc.  Best to always say \"don't have iommu on\"!\n. For me, no -- we already mention /sys and lstopo, and we do mention numactl -H above.  It's enough.\n. To me that is more in basic configuration and not performance tuning.  Not sure.  You have any reason that it should?\n. As this is a PR for master which doesn't have these knobs yet afaiu, I'll leave these out for now.  But yes we should doc them!\n. This gives me the shivers -- what if someone upgraded this snabb in place?  What if it was moved or made clean?  In any case we need an assert and anerror message so we don't fall off and keep running the parent.\n. Well... in my patch it isn't.  This is more of a set; you want pulls to come first.  Probably the ideal is to run all pulls needed by each chain; if there are parallel disjoint graphs you should run pull-1 then all pushes in that chain, then pull-2 then all pushes in that chain, and so on.  Right now the patch does the simple thing though.  It could be that pull-1 and pull-2 are both consumed by push-N though, so it's a reasonable first step.\n. I am not sure that what you are working on is a good idea.  it does not seem to be a strategy that will survive a multiprocess world.\n. Yes, it is robust.  Apps are added to the visited set before they are visited, and not revisited if they are in the visited set.  On a higher level, this is an implementation of a reverse post-order numbering via depth-first-walk, an algorithm that can be applied to graphs with cycles.\n. The problem is that if the graph is large, you can run out of stack space.\n. I think a rewrite is not \"really important\"; any failure will not be silent, and it is unlikely to fail with normal (up to 100 apps) graphs.  I haven't tested it though.  I would be fine keeping both the implementation and the comment fwiw.\n. The stack space usage is O(n), yes.  We should be fine for more than 100 apps; I was just throwing out a number :)\n. indeed!\n. A very good question!\nI guess I would first note that what we are doing is a fundamentally place-oriented operation underneath -- we are mutating the graph of instantiated apps and the links between them, not creating an entirely new graph.  When we're done we can't use the old idea of what the apps and links are because it is no longer valid.\nYou are right though that don't have to mutate the primordial app.configuration table.  Even though we are mucking with links and such, we could avoid mutating the configuration.  However consider: in a multiprocess configuration as in https://github.com/Igalia/snabb/blob/yang/src/apps/config/README.md, the follower would be left with an out-of-date app.configuration.  Maybe this is OK?  The follower is designed to not need to know about the current configuration; it just applies changes.  I kept in the mutation so that the follower would have an up-to-date app.configuration though.\n. I get what you're saying.  OTOH consider a configuration change that results in a big action list, and the follower is going to process that action list from another process.  You start streaming the actions across to the follower but for whatever reason (perhaps it is reading the actions as you write them, perhaps it limits the rate at which it takes actions, perhaps you fill the statically allocated ring buffer) it doesn't apply them all in the same breath.  In that case you could run the next push() after changing the app's links but before calling link().\nI would prefer some static flag on apps indicating that their links changed, which they could check themselves in their pull functions.  I guess that could be a followup; wdyt?\n. AFAIU also we should always unlink the sides -- if we're throwing away the link, then it was indeed linked before, and surely we should also unlink both sides.\n. No, they were used for the fixedpoint iteration in app.breathe.\n. The code in this PR as-is introduces no race conditions AFAIU; do you agree?\nAs far as whether the leader/follower setup from https://github.com/Igalia/snabb/blob/yang/src/apps/config/README.md introduces race conditions: the follower does not read from app.configuration at all.  The leader has an idea of the app graph of the followers because it knows the current high-level configuration of the instance as a whole, and it can get the app graph (the thing you get when you call core.config.new(); there are two different things here with the name \"configuration\" and I am calling core.config the app graph to differentiate) via local app_graph = setup_fn(conf).  When a new configuration comes, the leader computes a new app graph, calls compute_config_actions to get the config actions, then ships those actions to all followers.\nThe followers can lag the leader of course.  If there is an operation that for some reason requires that the follower state matches the leader state, the leader can just wait until the followers have consumed all their updates.\nHope that helps.  Do you see a race condition there?  I don't but I would be delighted if you had a counterexample.\n. I think it's reasonable to check for link changes in a pull function, given that very few apps have link functions.  It makes things very obvious and matches the not-so-recent shift that we had from apps spawning timers to apps running timeouts manually in pull functions.  Your mileage varies evidently :-)  I am fine with whatever as my apps don't call link functions, and the quadratic cost only comes in if the link function exists and is linear in link count.\n. Like what if we had a relink_count property on an app.  Any time an input or output is relinked, we increment that counter.  Certainly not bad from core.app's perspective but again YMMV from the app's perspective.\n. Good question, it certainly looks like it.  Perhaps given that Luke is proposing #1021 separately, we comment there?. @petebristow notes that this looks like the wrong thing (and if it isn't it needs a comment I think): https://github.com/snabbco/snabb/pull/1133#pullrequestreview-38643931\nWDYT @lukego?. The intention was for the module to always exist; did I make an error?  The intention was for core/version.lua to be a \"built source\", like the lua modules that the dasl files compile into.\nI didn't want to add version.lua to git because if the build process always overwrites it, then sadness would ensue.  WDYT?. Snabb's core.main now calls lib.randomseed, so lib.random_bytes should always exist.. SipHash keys are 16 bytes log.  However a hash \"seed\" is much less -- in practice, just 4 bytes.  So we set 4 bytes of the SipHash key from the \"seed\", and leave the rest as zeros.. add an assert around the tonumber, perhaps?. I think this is not a good change.  If a test spawns so many sub-processes with this environment variable set that it's too spammy, that test needs to filter sub-process output.. I don't think this is necessary.  lib.randomseed should be called only once in a process's lifetime.  I think the multiple printouts that you see in the logs correspond to many subprocesses.. Did you mean to commit this?  This change applies only to lwaftr, not Snabb:wingo-next.. Yes you are right.  Will fix in a followup.  Thanks for the review!. Here we would need to return a default value, but then below we are only reading 8192 bytes, whereas for 1024 nodes that would already take 1024*9=9216 bytes.  So this won't scale arbitrarily.  I think it could be OK to instead of calling get_max_numnodes, just always use 4096 or something large.  Avoids many potential problems.  WDYT?. Can we use core.lib.readfile instead?. This needs a comment about what's going on here; why the 9 (I know, because NNNNNNNN,), why the 32.. It's because timer functions are called without any arguments, and here we need to call tick(self).. I do not know.  This comes from follower.lua.  I guess you mean os.exit(0) btw.  Can fix I guess :). I will make a note of it!. Does it make sense to run via #!../../snabb ?  That is what the intel_mp tests do, FWIW.. Do you have hydra comparisons that this additional rebalance_freelists call is perf-neutral?  If it would be possible to put this call in the pull or push function of an interlink app, that would be better, of course -- is it possible?  (Maybe not.). probably should indent this assignment flush left. There is a TOCTTOU here; probably better to always shm.create.  shm.create does not add O_EXCL so it should silently succeed if the file already exists.. More broadly, does it always make sense to have a group freelist, even if the system only has one process?  I guess it is OK but it is a point to consider.. It sure would be nice to be able to transfer batches of packets to the group freelist, instead of transferring them one by one.  In that case you could potentially even avoid the spinlock around the group freelist and use atomic ops instead.  Anyway, something to consider for the future, it's clear that this works for now :). I don't quite get this description; the state transition diagram below suggests that DXUP -> TXUP is valid.  I would have assumed that DXUP can only transition to DOWN, based on the description.  What am I missing?. Should the comment be, \"both ends have detached\" ?. I believe you might even be able to pass { max = max_packets } as the initarg!. Surely PWD is not in your $PATH though, right?. Probably should update this printout to remove mention of SNABB_PCI0/SNABB_PCI1.  Otherwise lgtm. Is this really what you want?  At some point counter.read(frees) will stop being equal to lastfrees because the tonumber() may lose precision.  Seems to me better to make lastfrees etc to be uint64_t values in some allocated FFI data.. We already have an environment variable for making deterministic random numbers for tests: SNABB_RANDOM_SEED.. If we were to keep this, we should reduce this to one line (rss_hash_key or math.random(2^32).  But I think instead we should make the RSS tests set SNABB_RANDOM_SEED.  WDYT?. I had no idea that #!foo searched for foo in the current working directory instead of the path!  Strangest thing.  SGTM.. I feel like this change needs a bit more attention, perhaps from @lukego.  I feel like maybe the app shouldn't have access to its name, dunno.  And of course you can always put the name in the app's configuration, if that's your thing...  WDYT?\nIn any case, if we make this change, we should document it.. I feel like core shouldn't be calling into apps.  Would it be a good idea to move this part of interlinks to lib?  Can be a followup if you agree.. I think if we're making max_packets static then we might as well remove it from struct freelist :P  Otherwise I would keep this as [?] and instantiate as appropriate.. Can you move this outside the loop?  I.e. right after the vmprofile.start.. I think the interlink apps would do better to use the standard way to declare app parameters.  It could default to the app name for the link name, but the interlink name should be configurable.  WDYT?. Alternately we could move lib.interlink into core, and make it choose the SHM paths.  WDYT?. What I mean to say is that you could have:\nc\nenum { MAX_PACKETS=1000000 };\nstruct freelist {\n  int32_t lock[1];\n  uint32_t nfree;\n  struct packet *list[MAX_PACKETS};\n};\nThat way you could still access max_packets by ffi.C.MAX_PACKETS, but since we are really requiring that freelists have a fixed size, we do away with the notion that the freelist size is configurable.  Also that uses uint32_t for the occupancy for better struct packing.  WDYT?\nOr, if preserving variable-sized freelists is important (probably it is), then we need to use some other variable-size-friendly core.shm mapper.. I think we need to change the default above at line 34 to verbose={default=true}, to keep the current behavior for existing callers.. I dunno, I think since there's these two ways to deal with the schema -- either as provided by the user or as received from the remote -- I think it makes sense to move all argument handling to test.lua, to avoid making this common.lua too specific on the commands being used.  WDYT?. Shoudn't it still be diff.rxpackets == diff.txpackets ?. AFAIU this is a presentation issue.  Right now the load tester will print separately the % of packets dropped by the NIC at ingress, and the % that were lost elsewhere.  Your change wants to sum the % dropped at ingress to the % lost elsewhere, right?. Hum, for me for two boxes that are directly linked together, I expect rxpackets == txpackets as the sole success condition.  That's what I see when I run tests.  I never see rxpackets > txpackets.  When would that be OK?. ah i see.  yes definitely, good change!. ",
    "dpino": "Resolved conflict.\n. I measured the time execution of the selftest with and without the patch. Here are the results:\nUsing pairs\nselftest: timer\nok (973,855 callbacks in 0.3686 seconds)\nIterating using an index\nselftest: timer\nok (973,855 callbacks in 0.2085 seconds)\nI don't believe this will have a big impact in Snabbswitch, but it's an improvement nice to have IMHO.\npairs is significantly much slower than iterating an array with an index (9 times slower). When the table is an array, it's possible to iterate it using an index or ipairs (80% slower than using an index). If the table is a hash (sparse array), the only way possible to iterate it is by using pairs. The reason why is that the size of an array in Lua is calculated as the number of elements in the array until the first nil. A sparse array may contain arbitrary nils, for instance in the first position, which makes it not possible to use ipairs or an index.\nI grepped the codebase searching for patterns such as:\nfor _, val in pairs(list) do\nThis is OK if list is a sparse array. It can be rewritten as for _, val in ipairs(list) otherwise, or using an index. I made all the replacements possible (this patch, plus another one in a different PR).\n. > Performance notice: the JIT compiler specializes to the identity of namespace objects and to the strings used to index it. This effectively turns function cdata objects into constants. It's not useful and actually counter-productive to explicitly cache these function objects, e.g. local strlen = ffi.C.strlen. OTOH it is useful to cache the namespace itself, e.g. local C = ffi.C. \nSource: http://luajit.org/ext_ffi_semantics.html\n. Well, I modified test apps/vpn/vpws.lua, which was using IPv6 pton, and added code to check ntop was working correctly, printing out its value, but I didn't make the change permanent as it was relevant to the test.\nI think is better I add selftests to ipv6.lua and ipv4.lua to check that at least this function is working correctly.\n. I've followed the steps commented by @eugeneia but it's not working for me yet. This is what I've done so far:\n- Copied /home/max/bench_env.sh to /home/dpino.\n- Change the NIC to 0000:00:09.\n- Change the telnet ports to 6060 (vma) and 6061 (vmb).\n- I also run sudo ./snabb -t apps.intel.intel_app to check the card was working OK (it was).\n- Then I run sudo scripts/bench_env/loadgen-snabb-nic-guest.sh but it seems to get stalled. I paste below the tail of the output.\nSNABB=scripts/bench_env/../..//snabb\nLocking 0000:09:00.0\nLocking 0000:09:00.1\nConnect to guests with:\ntelnet localhost 6060\nAnd it gets frozen at this point. Any suggestion or hint?\n. Thanks Luke. I could successfully log into 'grindelwald'. I setup an environment in my $HOME:\n- Copied snabbswitch and snabb-nfv.\n- Copied your bench_env.sh and set NFV_PCI0=\"0000:01:00.0 0000:03:00.0\" (same values for NFV_PCI1).\n- Adjust paths to snabbswitch and snabb-nfv.\nAfter that I tried to run again scripts/bench_env/loadgen-snabb-nic-guest.sh, but I gets stalled as in chur. Here's the output I got:\nSNABB=scripts/bench_env/../..//snabb\nLocking 0000:01:00.0\nLocking 0000:03:00.0\nLocking 0000:01:00.1\nLocking 0000:03:00.1\nConnect to guests with:\ntelnet localhost 5500\ntelnet localhost 5501\nI'm still not sure whether this is the benchmark I should run :/ I don't see any path to a VM in bench_env.sh, is it not necessary?\nLastly, could you give access to grindelwald to Carlos L\u00f3pez (clopez@igalia.com) too?\n. OK, I managed to run the benchmark successfully. I didn't realize 0000:01:00.X and 0000:03:00.X were not working (and that's why Luke had 4 NICs setup instead of 6). I got the following output:\nConnect to guests with:\nOn 0000:05:00.0 got 3.357\nOn 0000:82:00.0 got 3.226\nOn 0000:84:00.0 got 3.194\nOn 0000:88:00.0 got 3.196\nRate(Mpps): 12.973\n. I managed to run the \"NFV loadgen benchmark\" in grindewald, with and without the patch applied. Here are the results:\nPatch not applied\nConnect to guests with:\ntelnet localhost 5500\ntelnet localhost 5501\ntelnet localhost 5502\ntelnet localhost 5503\nOn 0000:05:00.0 got 3.357\nOn 0000:82:00.0 got 3.226\nOn 0000:84:00.0 got 3.194\nOn 0000:88:00.0 got 3.196\nRate(Mpps): 12.973\nPatch applied\nConnect to guests with:\ntelnet localhost 5500\ntelnet localhost 5501\ntelnet localhost 5502\ntelnet localhost 5503\nOn 0000:05:00.0 got 3.342\nOn 0000:82:00.0 got 3.250\nOn 0000:84:00.0 got 3.194\nOn 0000:88:00.0 got 3.205\nRate(Mpps): 12.991\nSo it seems it doesn't make a difference. I would let @n-nikolaev decide whether this logic is correct or not. Summarizing, in d62ffc1 (net_device: Refactor vrings) variable should_continue was redeclared within the for loop in VirtioNetDevice:transmit_packets_to_vm. Since the variable within the for loop is different than the more external should_continue, this external variable is never updated.\n. @lukego Yes, that's correct, I got similar results https://github.com/SnabbCo/snabbswitch/pull/315#issuecomment-63958365\n. The undeclared var was actually not needed.\nThe main goal of the patch is to provide a way to do a -jdump using an environment variable like NFV_PROF does. It also adds support to write down the profiling and dumping to a file, which was missing in this case.\nWhile taking a look at the undeclared var thing, I noticed the -jdump in /core/main.lua works different than -jp, although both commands have a similar syntax in LuaJIT.\nsudo ./snabb -jp=Flvm,/tmp/output apps.fuzz.fuzz\nsudo ./snabb -jdump /tmp/output apps.fuzz.fuzz\nIt's not possible to pass parameters to jdump (sometimes we run it with +rsT), so I refactored it to have the same syntax as -jp.\nsudo ./snabb -jdump=+rsT,/tmp/output apps.fuzz.fuzz\n. I'm having some trouble running loadgen.\n- I added the remote vosys in my snabbswitch cloned repo.\n- I checked the vosys/straighline branch and built snabb.\n- If I understood correctly, I should now switch to snabbco/master to run loadgen.\n- I run loadgen but it gets stalled.\nSNABB=./scripts/bench_env/../..//snabb\nLocking 0000:01:00.0\nLocking 0000:01:00.1\nConnect to guests with:\ntelnet localhost 5500\nIt also gets stalled even if I build snabb from snabbco/master and run loadgen. An older master works though, so I discard it's a problem with the NIC.\nWith regard to QEMU I'm using /home/dpino/workspace/snabbswitch/src/snabb, which is a copy of /home/luke/hacking/snabb-nfv/qemu/x86_64-softmmu/qemu-system-x86_64. \n. Thanks n-nikolaev, that worked. I'm still a bit confused, shouldn't I run the snabb that I just build from the straightline branch, instead of /home/luke/hacking/pristine/snabbswitch/src/snabb, if I want to try the changes in the straightline branch?\n. OK, here's what I'm doing.\nFirst about the master branch. Currently I'm working in grindelwald, using NIC 0000:01:00. I have a cloned repo of snabbco/snabbswitch at /home/dpino/workspace/snabbswitch/src. I have a local branch (master-old) pointing to a an old commit in master. If I build and run loadgen-snabb-nic-guest.sh on that branch it works:\n```\nsnabbswitch/src (master-old) $ make\nsnabbswitch/src (master-old) $ sudo ./scripts/bench_env/loadgen-snabb-nic-guest.sh\nSNABB=./scripts/bench_env/../..//snabb\nLocking 0000:01:00.0\nLocking 0000:01:00.1\nConnect to guests with:\ntelnet localhost 5500\nOn 0000:01:00.0 got 3.636\nRate(Mpps): 3.636\nWaiting QEMU processes to terminate...\nFinished.\n```\nThis is my ~/bench_conf.sh file.\n```\nRUN_LOADGEN=true\nSnabb NFV traffic process configuration file\nNFV_CONFIG=/home/dpino/workspace/snabbswitch/src/test_fixtures/nfvconfig/test_functions/snabbnfv-bench.port\nTrace file defining which packets to replay as a traffic source.\nPCAP=/opt/bench/256.pcap\nHow many packets to run for the benchmark.\nNFV_PACKETS=100e6\nTelnet port numbers for VM serial ports.\n(VMs will use consecutive ports starting from this base)\nTELNET_PORT0=5500\nTELNET_PORT1=5600\nNFV_PCI0=\"0000:01:00.0\"\nNFV_PCI1=\"0000:01:00.1\"\nGUEST_MEM=1024\nQEMU=/home/dpino/workspace/snabb-nfv/qemu/x86_64-softmmu/qemu-system-x86_64\nSNABB=\"/home/dpino/workspace/snabbswitch/src/snabb\"\n```\nIf I switch to snabbco/master and do the same, the benchmark gets stalled (it never ends).\n```\nsnabbswitch/src (master-old) $ make\nsnabbswitch/src (master-old) $ sudo ./scripts/bench_env/loadgen-snabb-nic-guest.sh\nSNABB=./scripts/bench_env/../..//snabb\nLocking 0000:01:00.0\nLocking 0000:01:00.1\nConnect to guests with:\ntelnet localhost 5500\n```\nNow, with regard to the straightline branch, running sudo /home/luke/hacking/pristine/snabbswitch/src/snabb /home/luke/hacking/pristine/snabbswitch/src/designs/loadgen/loadgen /opt/bench/256.pcap 0000:01:00.1 works, so that should be the expected result.\nloadgen\noink\nscan\nscanning\nscanned\nscanned\n0000:01:00.1\nconf\nwaiting...\nmain\nTransmissions (last 1 sec):\n0000:01:00.1    TXDGPC (TX packets) 3,313,786   GOTCL (TX octets)   861,482,440\nTransmissions (last 1 sec):\n0000:01:00.1    TXDGPC (TX packets) 4,464,796   GOTCL (TX octets)   1,160,833,440\nTransmissions (last 1 sec):\n0000:01:00.1    TXDGPC (TX packets) 4,463,589   GOTCL (TX octets)   1,160,530,800\nTransmissions (last 1 sec):\n0000:01:00.1    TXDGPC (TX packets) 4,464,251   GOTCL (TX octets)   1,160,719,820\nTransmissions (last 1 sec):\n0000:01:00.1    TXDGPC (TX packets) 4,463,976   GOTCL (TX octets)   1,160,622,580\nI added remote vosys (https://github.com/virtualopensystems/snabbswitch.git) and checked out branch vosys/straightline. I built snabb in that branch and run the master loadgen script:\nsnabbswitch/src (straightline) $ make\nsnabbswitch/src (straightline) $ sudo ./snabb /home/luke/hacking/pristine/snabbswitch/src/designs/loadgen/loadgen /opt/bench/256.pcap 0000:01:00.1\n...hacking/pristine/snabbswitch/src/designs/loadgen/loadgen:4: module 'core.buffer' not found:\n    no field package.preload['core.buffer']\n    no file './core/buffer.so'\n    no file '/usr/local/lib/lua/5.1/core/buffer.so'\n    no file '/home/dpino/workspace/snabbswitch/deps/luajit/usr/local/lib/lua/5.1/core/buffer.so'\n    no file '/usr/local/lib/lua/5.1/loadall.so'\n    no file './core.so'\n    no file '/usr/local/lib/lua/5.1/core.so'\n    no file '/home/dpino/workspace/snabbswitch/deps/luajit/usr/local/lib/lua/5.1/core.so'\n    no file '/usr/local/lib/lua/5.1/loadall.so'\nstack traceback:\n    core/main.lua:185: in function <core/main.lua:183>\n    [C]: in function 'require'\n    ...hacking/pristine/snabbswitch/src/designs/loadgen/loadgen:4: in main chunk\n    [C]: in function 'dofile'\n    core/main.lua:109: in function <core/main.lua:50>\n    [C]: in function 'xpcall'\n    core/main.lua:190: in main chunk\n    [C]: in function 'require'\n    [string \"require \"core.main\"\"]:1: in main chunk\n. @n-nikolaev OK, cool, thanks.\nSo doing a recap this is what I should do:\n1) In a different terminal, run sudo /home/luke/hacking/pristine/snabbswitch/src/snabb /home/luke/hacking/pristine/snabbswitch/src/digns/loadgen/loadgen /opt/bench/256.pcap 0000:82:00.1. This generates traffic.\n2) In another terminal, from the straightline branch and run sudo ./scripts/bench_env/loadgen-snabb-nic-guest.sh. The var RUN_LOADGEN in bench_env.conf should be empty because there's another snabb instance running loadgen.\n. @n-nikolaev OK, I got it running, but I'm not sure whether is working OK.\nIn one terminal I got loadgen running:\nTransmissions (last 1 sec):\n0000:84:00.1    TXDGPC (TX packets) 4,464,328   GOTCL (TX octets)   1,160,723,200\nTransmissions (last 1 sec):\n0000:84:00.1    TXDGPC (TX packets) 3,704,678   GOTCL (TX octets)   963,222,780\nTransmissions (last 1 sec):\n0000:84:00.1    TXDGPC (TX packets) 4,463,986   GOTCL (TX octets)   1,160,636,620\nTransmissions (last 1 sec):\n0000:84:00.1    TXDGPC (TX packets) 4,464,379   GOTCL (TX octets)   1,160,733,080\nTransmissions (last 1 sec):\nIn another one, I launch sudo ./scripts/bench_env/loadgen-snabb-nic-guest.sh but it never ends, I never got a summary report, as I used to.\nI'm using 0000:84:00 at this moment cause either 0000:01:00 and 0000:03:00 seemed not to work. I got the following while running loadgen in 0000:01:00 (sudo /home/luke/hacking/pristine/snabbswitch/src/snabb /home/luke/hacking/pristine/snabbswitch/src/designs/loadgen/loadgen /opt/bench/256.pcap 0000:01:00.1):\nTransmissions (last 1 sec):\n0000:01:00.1    TXDGPC (TX packets) 0   GOTCL (TX octets)   0\nTransmissions (last 1 sec):\n0000:01:00.1    TXDGPC (TX packets) 0   GOTCL (TX octets)   0\nTransmissions (last 1 sec):\n0000:01:00.1    TXDGPC (TX packets) 0   GOTCL (TX octets)   0\nTransmissions (last 1 sec):\n0000:01:00.1    TXDGPC (TX packets) 0   GOTCL (TX octets)   0\nTransmissions (last 1 sec):\n0000:01:00.1    TXDGPC (TX packets) 0   GOTCL (TX octets)   0\nHere's my ~/bench_conf.sh:\n```\nRUN_LOADGEN=\nSnabb NFV traffic process configuration file\nNFV_CONFIG=/home/dpino/workspace/snabbswitch/src/test_fixtures/nfvconfig/test_functions/snabbnfv-bench.port\nTrace file defining which packets to replay as a traffic source.\nPCAP=/opt/bench/256.pcap\nHow many packets to run for the benchmark.\nNFV_PACKETS=100e6\nLuaJIT profiler settings\nexport NFV_PROF=z2F\nexport NFV_DUMP=+rsT\nTelnet port numbers for VM serial ports.\n(VMs will use consecutive ports starting from this base)\nTELNET_PORT0=5500\nTELNET_PORT1=5600\nPort(s) to use for VMs\nNFV_PCI0=\"0000:84:00.0\"\nNFV_PCI1=\"0000:84:00.1\"\nUncomment these to test with many ports in parallel instead:\nNFV_PCI0=\"0000:01:00.0 0000:03:00.0\" # Not working\nNFV_PCI1=\"0000:01:00.1 0000:03:00.1\" # Not working\nGUEST_MEM=1024\nQEMU=/home/dpino/workspace/snabb-nfv/qemu/x86_64-softmmu/qemu-system-x86_64\nSNABB=\"/home/dpino/workspace/snabbswitch/src/snabb\"\n```\nSorry for the inconvenience.\n. @n-nikolaev Sorry, I was using it temporary for testing. I'd consider using 82 but I need to know whether 01 and 03 are working correctly. This is what I'm getting:\nsudo /home/luke/hacking/pristine/snabbswitch/src/snabb \\\n    /home/luke/hacking/pristine/snabbswitch \\\n    /src/designs/loadgen/loadgen /opbench/256.pcap 0000:0X:00.X\n0000:01:00.1\nTransmissions (last 1 sec):\n0000:01:00.1    TXDGPC (TX packets) 0   GOTCL (TX octets)   0\nTransmissions (last 1 sec):\n0000:01:00.1    TXDGPC (TX packets) 0   GOTCL (TX octets)   0\nTransmissions (last 1 sec):\n0000:01:00.1    TXDGPC (TX packets) 0   GOTCL (TX octets)   0\n0000:01:00.0\nTransmissions (last 1 sec):\n0000:01:00.0    TXDGPC (TX packets) 3,315,437   GOTCL (TX octets)   861,911,180\nTransmissions (last 1 sec):\n0000:01:00.0    TXDGPC (TX packets) 4,464,610   GOTCL (TX octets)   1,160,783,260\nTransmissions (last 1 sec):\n0000:01:00.0    TXDGPC (TX packets) 4,463,673   GOTCL (TX octets)   1,160,550,560\n0000:03:00.1\nTransmissions (last 1 sec):\n0000:03:00.1    TXDGPC (TX packets) 0   GOTCL (TX octets)   0\nTransmissions (last 1 sec):\n0000:03:00.1    TXDGPC (TX packets) 0   GOTCL (TX octets)   0\nTransmissions (last 1 sec):\n0000:03:00.1    TXDGPC (TX packets) 0   GOTCL (TX octets)   0\n0000:03:00.0\nTransmissions (last 1 sec):\n0000:03:00.0    TXDGPC (TX packets) 3,314,888   GOTCL (TX octets)   861,769,740\nTransmissions (last 1 sec):\n0000:03:00.0    TXDGPC (TX packets) 4,464,237   GOTCL (TX octets)   1,160,702,920\nTransmissions (last 1 sec):\n0000:03:00.0    TXDGPC (TX packets) 4,463,963   GOTCL (TX octets)   1,160,608,280\n. @n-nikolaev OK, done. Thanks. I added a comment in https://github.com/SnabbCo/snabbswitch/issues/347.\n. Confirmed, I get similar results:\nA (uppercase)\nRate(Mpps): 2.74\nRate(Mpps): 2.843\nRate(Mpps): 2.737\nRate(Mpps): 2.835\nRate(Mpps): 2.723\na (lowercase)\nRate(Mpps): 3.555\nRate(Mpps): 3.527\nRate(Mpps): 3.463\nRate(Mpps): 3.571\nRate(Mpps): 3.544\n. Since yesterday I cannot manage to run loadgen in grindelwald. To discard there was something wrong with my local branch I switched to vosys/straigtline.\nFirst, I selected a NIC it's guaranteed to be working fine:\n``` bash\n$ sudo SNABB_TEST_INTEL10G_PCIDEVA=0000:05:00.0 SNABB_TEST_INTEL10G_PCIDEVB=0000:05:00.1 ./snabb -t apps.intel.intel_app\nlink report\n1177760 sent on nicBm1.tx -> sink_ms.in3 (loss rate: 0%))\n2 sent on source_ms.out -> repeater_ms.input (loss rate: 0%))\n0 sent on nicAs.tx -> sink_ms.in1 (loss rate: 0%))\n2355945 sent on repeater_ms.output -> nicAs.rx (loss rate: 0%))\n1177727 sent on nicBm0.tx -> sink_ms.in2 (loss rate: 0%))\nselftest: ok\n```\nI checked ixgbe driver is not loaded.\nbash\n$ lsmod | grep ixgbe\nApparently there's no other snabbswitch instance running in the machine, so I discard the NIC is being used by another process.\nHere's my ~/bench_conf.sh\n``` bash\nPCAP=/opt/bench/256xff.pcap\nNFV_CONFIG=/home/dpino/workspace/snabbswitch/src/test_fixtures/nfvconfig/test_functions/snabbnfv-bench.port\nNODE_BIND0=0\nSNABB_LOG0=snabblog.txt\nNFV_PCI0=\"0000:05:00.0\"\nNFV_SOCKET0=/tmp/dpino_vhost_A.sck\nNODE_BIND1=1\nSNABB_LOG1=/dev/null\nSNABB_LOG1=loadgen.log\nNFV_PCI1=\"0000:05:00.1\"\nNFV_SOCKET1=/tmp/dpino_vhost_B.sock\nNFV_PACKETS=10e6\nQEMU\nGUEST_MEM=1024\nHUGETLBFS=/hugetlbfs\nKERNEL=/opt/bench/bzImage-no-virtio-net\nQEMU=/home/dpino/workspace/snabb-nfv/qemu/x86_64-softmmu/qemu-system-x86_64\nGuest instance #0\nTAP0=tap0\nTELNET_PORT0=7111\nGUEST_IP0=192.168.2.10\nGUEST_MAC0=52:54:00:00:00:00\nIMAGE0=/home/dpino/virt/ubuntu-trusty.img\nBOOTARGS0=\"earlyprintk root=/dev/vda rw console=ttyS0 ip=$GUEST_IP0 hugepages=32\"\nGuest instance #1\nTAP1=tap1\nTELNET_PORT1=7112\nGUEST_IP1=192.168.2.11\nGUEST_MAC1=52:54:00:00:00:01\nIMAGE1=/home/dpino/virt/ubuntu-trusty.img1\nBOOTARGS1=\"earlyprintk root=/dev/vda rw console=ttyS0 ip=$GUEST_IP1\"\n```\nAny hint appreciated. Thanks :)\n. Same thing. I was using 05 temporally because I got the following result running the intel_app selftest on 82:\nTransmitting bidirectionally between nicA and nicB\n2475 sent on source2.out -> nicB.rx (loss rate: 0%))\n0 sent on nicB.tx -> sink.in2 (loss rate: 0%))\n0 sent on nicA.tx -> sink.in1 (loss rate: 0%))\n2475 sent on source1.out -> nicA.rx (loss rate: 0%))\nsq_sq: missing packets\nThe selftest on 05 looked better. Yesterday, when it was working, I was using 82.\n. I got it working on 05 by using a ~/bench_conf.sh similar to Luke's. It looks like this:\n``` bash\nPCAP=/opt/bench/256xff.pcap\nNFV_CONFIG=/home/dpino/workspace/snabbswitch/src/test_fixtures/nfvconfig/test_functions/snabbnfv-bench.port\nLuaJIT profiler settings\nexport NFV_PROF=vl\nNODE_BIND0=0\nSNABB_LOG0=snabblog.txt\nNFV_PCI0=\"0000:05:00.0\"\nNFV_SOCKET0=/tmp/dpino_vhost_A.sck\nNODE_BIND1=1\nSNABB_LOG1=/dev/null\nSNABB_LOG1=loadgen.log\nNFV_PCI1=\"0000:05:00.1\"\nNFV_SOCKET1=/tmp/dpino_vhost_B.sock\nNFV_PACKETS=10e6\nQEMU\nGUEST_MEM=1024\nHUGETLBFS=/hugetlbfs\nKERNEL=/opt/bench/bzImage-no-virtio-net\nQEMU=/home/dpino/workspace/snabb-nfv/qemu/x86_64-softmmu/qemu-system-x86_64\nGuest instance #0\nTAP0=tap0\nTELNET_PORT0=5500\nGUEST_IP0=fe80::5054:ff:fe00:0\nGUEST_MAC0=52:54:00:00:00:00\nIMAGE0=/opt/bench/ubuntu-trusty.img\nBOOTARGS0=\"earlyprintk root=/dev/vda rw console=ttyS0 ip=$GUEST_IP0 hugepages=32\"\nGuest instance #1\nTAP1=tap1\nTELNET_PORT1=5600\nGUEST_IP1=fe80::5054:ff:fe00:1\nGUEST_MAC1=52:54:00:00:00:01\nIMAGE1=/media/disk/bench-env-chur/vmb.img\nBOOTARGS1=\"earlyprintk root=/dev/vda rw console=ttyS0 ip=$GUEST_IP1\"\n```\nInterestingly running loadgen using this conf I got the following results:\nport_id: A (uppercase)\nRate(Mpps): 3.631\nRate(Mpps): 3.542\nRate(Mpps): 3.607\nRate(Mpps): 3.543\nRate(Mpps): 3.577\nport_id: a (lowercase)\nRate(Mpps): 3.354\nRate(Mpps): 3.406\nRate(Mpps): 3.401\nRate(Mpps): 3.407\nRate(Mpps): 3.435\nBoth results are similar, although A is slightly better, which compare to the previous results is a little bit strange :/\n. Lowercase / uppercase of port_id is no longer an issue, performance is the same in both cases.\nport_id=\"a\"\nRate(Mpps): 4.424\nRate(Mpps): 4.419\nRate(Mpps): 4.418\nRate(Mpps): 4.38\nRate(Mpps): 4.412\nport_id=\"A\"\nRate(Mpps): 4.423\nRate(Mpps): 4.411\nRate(Mpps): 4.428\nRate(Mpps): 4.425\nRate(Mpps): 4.426\n. FWIW, Igalia/pflua#159 is fixed and extra support for dotted triples, dotted doubles, etc will land soon (Igalia/pflua#176).\n. Code for the DNAT and connection tracking apps, are actually in one single app at this moment: apps/conntrack.\nHow to run the app:\nsudo ./snabb snsh apps/conntrack/main.lua \nResults written at: /tmp/output.pcap\nWhat the app does is to read packets from input.pcap and write down the results in output.pcap. The source and destination address will be changed according with the configuration of the proxy variable:\nlua\nlocal proxy = {\n   ip   = \"192.168.1.1\",\n   mask = \"255.255.255.0\",\n   port = \"80\"\n}\nTo check that the addresses were successfully rewritten what I did was to open the output.pcap file with WireShark. BTW, you can run this app directly in your workstation, no need to log into Snabb Lab.\n. This is the result of the selftest for the driver:\nsudo SNABB_TEST_INTEL10G_PCIDEVA=\"0000:04:00.0\" \\\n    SNABB_TEST_INTEL10G_PCIDEVB=\"0000:04:00.1\" \\\n    $SNABB_SRC/snabb snsh -t apps.intel.intel_app\n```\nSend a bunch of packets from Am0\nhalf of them go to nicAm1 and half go nowhere\nlink report:\n                   0 sent on nicAm0.tx -> sink_ms.in1 (loss rate: 0%)\n           3,742,877 sent on nicAm1.tx -> sink_ms.in2 (loss rate: 0%)\n           7,486,290 sent on repeater_ms.output -> nicAm0.rx (loss rate: 0%)\n                   2 sent on source_ms.out -> repeater_ms.input (loss rate: 0%)\n\nTransmitting bidirectionally between nicA and nicB\nlink report:\n                   0 sent on nicA.tx -> sink.in1 (loss rate: 0%)\n                   0 sent on nicB.tx -> sink.in2 (loss rate: 0%)\n           3,874,215 sent on source1.out -> nicA.rx (loss rate: 0%)\n           3,874,215 sent on source2.out -> nicB.rx (loss rate: 0%)\nsq_sq: missing packets\n```\n. @lukego When I run that test the NICs were cabled together 04:00.0 to 05:00.0. We changed it and cabled the ports of 04:00.0 together, but got the same result while testing the card.\n```\nSend a bunch of packets from Am0\nhalf of them go to nicAm1 and half go nowhere\nlink report:\n                   0 sent on nicAm0.tx -> sink_ms.in1 (loss rate: 0%)\n           4,333,922 sent on nicAm1.tx -> sink_ms.in2 (loss rate: 0%)\n           8,668,514 sent on repeater_ms.output -> nicAm0.rx (loss rate: 0%)\n                   2 sent on source_ms.out -> repeater_ms.input (loss rate: 0%)\n\nTransmitting bidirectionally between nicA and nicB\nlink report:\n                   0 sent on nicA.tx -> sink.in1 (loss rate: 0%)\n                   0 sent on nicB.tx -> sink.in2 (loss rate: 0%)\n           4,813,125 sent on source1.out -> nicA.rx (loss rate: 0%)\n           4,813,125 sent on source2.out -> nicB.rx (loss rate: 0%)\n```\n@sleinen Right, fixed :)\n@eugenia\nHere is the result of running the SnabbNFV selftest.\nsudo TESTPCI=0000:04:00.0 TELNET_PORT0=6060 TELNET_PORT1=6061 \\\n    program/snabbnfv/selftest.sh\nWaiting for VM listening on telnet port 6060 to get ready... [OK]\nWaiting for VM listening on telnet port 6061 to get ready... [OK]\nUSING program/snabbnfv/test_fixtures/nfvconfig/test_functions/same_vlan.ports\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nPING succeded.\n[  3]  0.0-10.0 sec  5.16 GBytes  4.43 Gbits/sec\nIPERF succeded.\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nJUMBOPING succeded.\n[  3]  0.0-10.0 sec  12.3 GBytes  10.6 Gbits/sec\nIPERF succeded.\ntx-checksumming: on\nTX-CHECKSUMMING succeded.\ntx-checksumming: on\nTX-CHECKSUMMING succeded.\nUSING program/snabbnfv/test_fixtures/nfvconfig/test_functions/tx_rate_limit.ports\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nPING succeded.\nIPERF (RATE_LIMITED) succeded.\nIPERF rate is 814 Mbits/sec (900 Mbits/sec allowed).\nRATE_LIMITED succeded.\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nJUMBOPING succeded.\nIPERF (RATE_LIMITED) succeded.\nIPERF rate is 814 Mbits/sec (900 Mbits/sec allowed).\nRATE_LIMITED succeded.\nUSING program/snabbnfv/test_fixtures/nfvconfig/test_functions/rx_rate_limit.ports\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nPING succeded.\nIPERF (RATE_LIMITED) succeded.\nIPERF rate is 814 Mbits/sec (1200 Mbits/sec allowed).\nRATE_LIMITED succeded.\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nJUMBOPING succeded.\nIPERF (RATE_LIMITED) succeded.\nIPERF rate is 814 Mbits/sec (1200 Mbits/sec allowed).\nRATE_LIMITED succeded.\nUSING program/snabbnfv/test_fixtures/nfvconfig/test_functions/tunnel.ports\nResolved next-hop fe80::5054:ff:fe00:0 to 52:54:00:00:00:00\nResolved next-hop fe80::5054:ff:fe00:1 to 52:54:00:00:00:01\nND succeded.\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nPING succeded.\n[  3]  0.0-10.0 sec  10.6 GBytes  9.07 Gbits/sec\nIPERF succeded.\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nJUMBOPING succeded.\n[  3]  0.0-10.0 sec  8.42 GBytes  7.23 Gbits/sec\nIPERF succeded.\nUSING program/snabbnfv/test_fixtures/nfvconfig/test_functions/filter.ports\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nPING succeded.\nConnection to fe80::5054:ff:fe00:0001%eth0 12345 port [tcp/*] succeeded!\nPORTPROBE succeded.\nTrying ::1...\nConnected to localhost.\nEscape character is '^]'.\nnc -w 1 -q 1 -v  fe80::5054:ff:fe00:0001%eth0 12346\nnc: connect to fe80::5054:ff:fe00:0001%eth0 port 12346 (tcp) timed out: Operation now in progress\nroot@vma:~# \nroot@vma:~# Connection closed by foreign host.\nFILTER succeded.\nUSING program/snabbnfv/test_fixtures/nfvconfig/test_functions/stateful-filter.ports\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nPING succeded.\nConnection to fe80::5054:ff:fe00:0001%eth0 12345 port [tcp/*] succeeded!\nPORTPROBE succeded.\nTrying ::1...\nConnected to localhost.\nEscape character is '^]'.\nnc -w 1 -q 1 -v  fe80::5054:ff:fe00:0001%eth0 12348\nnc: connect to fe80::5054:ff:fe00:0001%eth0 port 12348 (tcp) timed out: Operation now in progress\nroot@vma:~# \nroot@vma:~# Connection closed by foreign host.\nFILTER succeded.\nTrying ::1...\nConnected to localhost.\nEscape character is '^]'.\nnc -w 1 -q 1 -v  fe80::5054:ff:fe00:0000%eth0 12340\nnc: connect to fe80::5054:ff:fe00:0000%eth0 port 12340 (tcp) timed out: Operation now in progress\nroot@vma:~# \nroot@vma:~# Connection closed by foreign host.\nFILTER succeded.\n. @lukego That's correct. I run the tests again to confirm it. I updated the PR printing out a warning in case device \"0x151c\" (82599 T3) is selected.\n. @lukego OK, I will give it a try.\n. thanks @javierguerragiraldez I tried the same steps on our machine with the T3 card and I got the following results (tested branch is 'next'):\nCurrent state:\n```\nSend a bunch of packets from Am0\nhalf of them go to nicAm1 and half go nowhere\nlink report:\n                   0 sent on nicAm0.tx -> sink_ms.in1 (loss rate: 0%)\n           4,284,632 sent on nicAm1.tx -> sink_ms.in2 (loss rate: 0%)\n           8,569,785 sent on repeater_ms.output -> nicAm0.rx (loss rate: 0%)\n                   2 sent on source_ms.out -> repeater_ms.input (loss rate: 0%)\n\nTransmitting bidirectionally between nicA and nicB\nlink report:\n                   0 sent on nicA.tx -> sink.in1 (loss rate: 0%)\n                   0 sent on nicB.tx -> sink.in2 (loss rate: 0%)\n           4,811,340 sent on source1.out -> nicA.rx (loss rate: 0%)\n           4,811,340 sent on source2.out -> nicB.rx (loss rate: 0%)\nsq_sq: missing packets\n```\nsq_sq fails and mq_sq is not being executed. If I add a 2 second pause between sq_sq's configure and main:\n```\nTransmitting bidirectionally between nicA and nicB\nlink report:\n           1,336,576 sent on nicA.tx -> sink.in1 (loss rate: 0%)\n           1,336,576 sent on nicB.tx -> sink.in2 (loss rate: 0%)\n           2,663,220 sent on source1.out -> nicA.rx (loss rate: 0%)\n           2,663,220 sent on source2.out -> nicB.rx (loss rate: 0%)\n\nSend traffic from a nicA (SF) to nicB (two VFs)\nThe packets should arrive evenly split between the VFs\nlink report:\n                   0 sent on nicAs.tx -> sink_ms.in1 (loss rate: 0%)\n                   0 sent on nicBm0.tx -> sink_ms.in2 (loss rate: 0%)\n                   0 sent on nicBm1.tx -> sink_ms.in3 (loss rate: 0%)\n           8,574,885 sent on repeater_ms.output -> nicAs.rx (loss rate: 0%)\n                   2 sent on source_ms.out -> repeater_ms.input (loss rate: 0%)\nmq_sq: missing packets\n```\nsq_sq runs successfully but mq_sq fails. It seems mq_sq needs a 2 second pause too:\nSend traffic from a nicA (SF) to nicB (two VFs)\nThe packets should arrive evenly split between the VFs\nlink report:\n                   0 sent on nicAs.tx -> sink_ms.in1 (loss rate: 0%)\n           1,731,322 sent on nicBm0.tx -> sink_ms.in2 (loss rate: 0%)\n           1,731,364 sent on nicBm1.tx -> sink_ms.in3 (loss rate: 0%)\n           3,463,410 sent on repeater_ms.output -> nicAs.rx (loss rate: 0%)\n                   2 sent on source_ms.out -> repeater_ms.input (loss rate: 0%)\nselftest: ok\n. I pushed a new commit that adds a time sleep before running tests 'sq_sq' and 'mq_sq'. The 2 second wait only runs if the card model is Intel 82599 T3.\n. @lukego: Sure, merged :)\n. The patch admits program names to have hyphens, but directories still need to be named with underscore.\n. I tested the code again. After the PR, directory names can either have a hyphen or an underscore. It works in both cases, but only one of them can be allowed (in other words, snabb-nfv and snabb_nfv cannot coexist, it triggers a compilation error).\nAs for me, I think both allowed. However, as a style guide rule I would prefer to favour hyphens to separate compound names over underscores.\nWith regard to hyphens in module names, I found this information but I think this is old Lua behaviour:\n\"Moreover, if the module name has a hyphen, its prefix up to (and including) the first hyphen is removed. For instance, if the module name is a.v1-b.c, the function name will be luaopen_b_c.\"\nhttp://lua-users.org/lists/lua-l/2009-07/msg00074.html\nSnS creates modules returning tables, which is the recommended way of creating modules since Lua 5.2, instead of given modules a name using the module function. \n. I like the proposed changes. I also find it redundant to have a program called snabb-XXX and have to type snabb two times in case of running it as a parameter of the snabb binary. Summarizing the proposed changes:\n- Allow alphanumerics and '-' in version names.\n- If argv[0] has a \"snabb-\" prefix, remove it a treat the rest of the string as the program name. After that rely on the mechanism for executing programs.\n. Updated PR. This is how program name is selected now:\n- snabb foo => foo\n- snabb-foo => foo\n- snabb snabb-foo => foo\nWhat is still not addressed yet is the question concerning hyphens being converted to underscores. That code is still in the programname function and it is necessary so programs such as example_replay and example_spray can work. If we agreed on using hyphens instead of underscores in program names, I can rename the dirs in a different PR. I don't see any technical limitation why it wouldn't be possible but I'm aware of Max's https://github.com/SnabbCo/snabbswitch/pull/439#issuecomment-100935622\n. Updated PR:\n- Moved function to lib/hardware/pci.lua. \n- Documented function in lib/hardware/README.md.src.\n. Sorry, I closed this PR by mistake. I restored the dpino:add_find_devices branch but it seems this PR cannot be reopened. I can create a new PR if necessary.\n. PR rebased to master. I added a patch removing the constraint that limits find_devices search to intel devices.\n. I implemented the expansion in #660. It's slightly different to what @lukego suggested. IMHO the inconvenient of expanding \"1\" as \"0000:01:00.0\" is that there's not short form for device == 1. The only way to write such address is using the full form \"0000:00:01.0\". I think the schema should be similar to IPv6 abbreviation, where \"::1\" means \"0:0:0:0:0:0:0:1\". So, \"0000:04:00.0\" would be abbreviated as \"4:\". A single \"4\" gets expanded to \"0000:00:04.0\". There are more examples in the PR.\n. @eugenia Right. I close this PR :)\n. I'm not sure about how to run it with make. I run it as:\nbash\n$ sudo SNABB_TEST_INTEL10G_PCIDEVA=0000:86:00.0 \\\n   ./snabb snsh -t program.snabbnfv.nfvconfig\nThis is what I got. Apparently it seems to work fine. Machine is grindelwald.\nselftest: lib.nfv.config\ntesting:    program/snabbnfv/test_fixtures/nfvconfig/switch_nic/x\nengine: start app id0_Virtio\nengine: start app id0_NIC\nOpened SolarFlare interface p10p1 (MAC address 00:90:f5:ef:aa:67, MTU 1514)\ntesting:    program/snabbnfv/test_fixtures/nfvconfig/switch_filter/x\nengine: reconfig app id0_NIC\nOpened SolarFlare interface p10p1 (MAC address 00:90:f5:ef:aa:67, MTU 1514)\nengine: start app id0_Filter_in\nload: time: 0.30s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\ntesting:    program/snabbnfv/test_fixtures/nfvconfig/switch_qos/x\nengine: stop app id0_Filter_in\nengine: reconfig app id0_NIC\nOpened SolarFlare interface p10p1 (MAC address 00:90:f5:ef:aa:67, MTU 1514)\nload: time: 0.27s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\ntesting:    program/snabbnfv/test_fixtures/nfvconfig/switch_tunnel/x\nengine: reconfig app id0_NIC\nOpened SolarFlare interface p10p1 (MAC address 00:90:f5:ef:aa:67, MTU 1514)\nengine: start app id0_Tunnel\nengine: start app id0_ND\nSending neighbor solicitation for next-hop 2::2\nload: time: 0.29s  fps: 3         fpGbps: 0.000 fpb: 0   bpp: 86   sleep: 100 us\ntesting:    program/snabbnfv/test_fixtures/nfvconfig/scale_up/y\nengine: stop app id0_Tunnel\nengine: stop app id0_ND\nengine: reconfig app id0_NIC\nOpened SolarFlare interface p10p1 (MAC address 00:90:f5:ef:aa:67, MTU 1514)\nengine: start app id1_Virtio\nengine: start app id1_NIC\nOpened SolarFlare interface p10p1 (MAC address 01:90:f5:ef:aa:67, MTU 1514)\nload: time: 0.29s  fps: 7         fpGbps: 0.000 fpb: 0   bpp: 86   sleep: 100 us\ntesting:    program/snabbnfv/test_fixtures/nfvconfig/scale_up/x\nengine: reconfig app id0_NIC\nOpened SolarFlare interface p10p1 (MAC address 00:90:f5:ef:aa:67, MTU 1514)\nengine: reconfig app id1_NIC\nOpened SolarFlare interface p10p1 (MAC address 01:90:f5:ef:aa:67, MTU 1514)\nengine: start app id63_Virtio\nengine: start app id48_NIC\nOpened SolarFlare interface p10p1 (MAC address 30:90:f5:ef:aa:67, MTU 1514)\nengine: start app id10_NIC\nOpened SolarFlare interface p10p1 (MAC address 0a:90:f5:ef:aa:67, MTU 1514)\nengine: start app id52_Virtio\nengine: start app id6_NIC\nOpened SolarFlare interface p10p1 (MAC address 06:90:f5:ef:aa:67, MTU 1514)\nengine: start app id45_NIC\nOpened SolarFlare interface p10p1 (MAC address 2d:90:f5:ef:aa:67, MTU 1514)\nengine: start app id34_NIC\nOpened SolarFlare interface p10p1 (MAC address 22:90:f5:ef:aa:67, MTU 1514)\nengine: start app id35_NIC\nOpened SolarFlare interface p10p1 (MAC address 23:90:f5:ef:aa:67, MTU 1514)\nengine: start app id40_NIC\nOpened SolarFlare interface p10p1 (MAC address 28:90:f5:ef:aa:67, MTU 1514)\nengine: start app id61_Virtio\nengine: start app id61_NIC\nOpened SolarFlare interface p10p1 (MAC address 3d:90:f5:ef:aa:67, MTU 1514)\nengine: start app id60_Virtio\nengine: start app id49_Virtio\nengine: start app id59_Virtio\nengine: start app id12_NIC\nOpened SolarFlare interface p10p1 (MAC address 0c:90:f5:ef:aa:67, MTU 1514)\nengine: start app id42_NIC\nOpened SolarFlare interface p10p1 (MAC address 2a:90:f5:ef:aa:67, MTU 1514)\nengine: start app id38_NIC\nOpened SolarFlare interface p10p1 (MAC address 26:90:f5:ef:aa:67, MTU 1514)\nengine: start app id24_NIC\nOpened SolarFlare interface p10p1 (MAC address 18:90:f5:ef:aa:67, MTU 1514)\nengine: start app id58_Virtio\nengine: start app id39_Virtio\nengine: start app id51_NIC\nOpened SolarFlare interface p10p1 (MAC address 33:90:f5:ef:aa:67, MTU 1514)\nengine: start app id57_Virtio\nengine: start app id46_NIC\nOpened SolarFlare interface p10p1 (MAC address 2e:90:f5:ef:aa:67, MTU 1514)\nengine: start app id3_NIC\nOpened SolarFlare interface p10p1 (MAC address 03:90:f5:ef:aa:67, MTU 1514)\nengine: start app id56_NIC\nOpened SolarFlare interface p10p1 (MAC address 38:90:f5:ef:aa:67, MTU 1514)\nengine: start app id55_Virtio\nengine: start app id45_Virtio\nengine: start app id28_NIC\nOpened SolarFlare interface p10p1 (MAC address 1c:90:f5:ef:aa:67, MTU 1514)\nengine: start app id25_NIC\nOpened SolarFlare interface p10p1 (MAC address 19:90:f5:ef:aa:67, MTU 1514)\nengine: start app id15_Virtio\nengine: start app id41_NIC\nOpened SolarFlare interface p10p1 (MAC address 29:90:f5:ef:aa:67, MTU 1514)\nengine: start app id35_Virtio\nengine: start app id25_Virtio\nengine: start app id54_Virtio\nengine: start app id36_NIC\nOpened SolarFlare interface p10p1 (MAC address 24:90:f5:ef:aa:67, MTU 1514)\nengine: start app id42_Virtio\nengine: start app id32_NIC\nOpened SolarFlare interface p10p1 (MAC address 20:90:f5:ef:aa:67, MTU 1514)\nengine: start app id18_NIC\nOpened SolarFlare interface p10p1 (MAC address 12:90:f5:ef:aa:67, MTU 1514)\nengine: start app id50_Virtio\nengine: start app id62_Virtio\nengine: start app id12_Virtio\nengine: start app id52_NIC\nOpened SolarFlare interface p10p1 (MAC address 34:90:f5:ef:aa:67, MTU 1514)\nengine: start app id32_Virtio\nengine: start app id22_Virtio\nengine: start app id51_Virtio\nengine: start app id53_NIC\nOpened SolarFlare interface p10p1 (MAC address 35:90:f5:ef:aa:67, MTU 1514)\nengine: start app id60_NIC\nOpened SolarFlare interface p10p1 (MAC address 3c:90:f5:ef:aa:67, MTU 1514)\nengine: start app id8_NIC\nOpened SolarFlare interface p10p1 (MAC address 08:90:f5:ef:aa:67, MTU 1514)\nengine: start app id31_NIC\nOpened SolarFlare interface p10p1 (MAC address 1f:90:f5:ef:aa:67, MTU 1514)\nengine: start app id33_NIC\nOpened SolarFlare interface p10p1 (MAC address 21:90:f5:ef:aa:67, MTU 1514)\nengine: start app id37_Virtio\nengine: start app id27_Virtio\nengine: start app id20_Virtio\nengine: start app id30_Virtio\nengine: start app id50_NIC\nOpened SolarFlare interface p10p1 (MAC address 32:90:f5:ef:aa:67, MTU 1514)\nengine: start app id10_Virtio\nengine: start app id53_Virtio\nengine: start app id47_Virtio\nengine: start app id23_NIC\nOpened SolarFlare interface p10p1 (MAC address 17:90:f5:ef:aa:67, MTU 1514)\nengine: start app id43_NIC\nOpened SolarFlare interface p10p1 (MAC address 2b:90:f5:ef:aa:67, MTU 1514)\nengine: start app id13_Virtio\nengine: start app id24_Virtio\nengine: start app id14_Virtio\nengine: start app id21_NIC\nOpened SolarFlare interface p10p1 (MAC address 15:90:f5:ef:aa:67, MTU 1514)\nengine: start app id7_Virtio\nengine: start app id6_Virtio\nengine: start app id5_Virtio\nengine: start app id63_NIC\nOpened SolarFlare interface p10p1 (MAC address 3f:90:f5:ef:aa:67, MTU 1514)\nengine: start app id21_Virtio\nengine: start app id31_Virtio\nengine: start app id62_NIC\nOpened SolarFlare interface p10p1 (MAC address 3e:90:f5:ef:aa:67, MTU 1514)\nengine: start app id11_Virtio\nengine: start app id59_NIC\nOpened SolarFlare interface p10p1 (MAC address 3b:90:f5:ef:aa:67, MTU 1514)\nengine: start app id13_NIC\nOpened SolarFlare interface p10p1 (MAC address 0d:90:f5:ef:aa:67, MTU 1514)\nengine: start app id41_Virtio\nengine: start app id57_NIC\nOpened SolarFlare interface p10p1 (MAC address 39:90:f5:ef:aa:67, MTU 1514)\nengine: start app id44_Virtio\nengine: start app id9_Virtio\nengine: start app id8_Virtio\nengine: start app id44_NIC\nOpened SolarFlare interface p10p1 (MAC address 2c:90:f5:ef:aa:67, MTU 1514)\nengine: start app id43_Virtio\nengine: start app id49_NIC\nOpened SolarFlare interface p10p1 (MAC address 31:90:f5:ef:aa:67, MTU 1514)\nengine: start app id4_Virtio\nengine: start app id4_NIC\nOpened SolarFlare interface p10p1 (MAC address 04:90:f5:ef:aa:67, MTU 1514)\nengine: start app id2_Virtio\nengine: start app id40_Virtio\nengine: start app id47_NIC\nOpened SolarFlare interface p10p1 (MAC address 2f:90:f5:ef:aa:67, MTU 1514)\nengine: start app id58_NIC\nOpened SolarFlare interface p10p1 (MAC address 3a:90:f5:ef:aa:67, MTU 1514)\nengine: start app id33_Virtio\nengine: start app id23_Virtio\nengine: start app id34_Virtio\nengine: start app id29_NIC\nOpened SolarFlare interface p10p1 (MAC address 1d:90:f5:ef:aa:67, MTU 1514)\nengine: start app id37_NIC\nOpened SolarFlare interface p10p1 (MAC address 25:90:f5:ef:aa:67, MTU 1514)\nengine: start app id54_NIC\nOpened SolarFlare interface p10p1 (MAC address 36:90:f5:ef:aa:67, MTU 1514)\nengine: start app id36_Virtio\nengine: start app id26_NIC\nOpened SolarFlare interface p10p1 (MAC address 1a:90:f5:ef:aa:67, MTU 1514)\nengine: start app id16_Virtio\nengine: start app id39_NIC\nOpened SolarFlare interface p10p1 (MAC address 27:90:f5:ef:aa:67, MTU 1514)\nengine: start app id38_Virtio\nengine: start app id46_Virtio\nengine: start app id56_Virtio\nengine: start app id28_Virtio\nengine: start app id27_NIC\nOpened SolarFlare interface p10p1 (MAC address 1b:90:f5:ef:aa:67, MTU 1514)\nengine: start app id48_Virtio\nengine: start app id30_NIC\nOpened SolarFlare interface p10p1 (MAC address 1e:90:f5:ef:aa:67, MTU 1514)\nengine: start app id16_NIC\nOpened SolarFlare interface p10p1 (MAC address 10:90:f5:ef:aa:67, MTU 1514)\nengine: start app id5_NIC\nOpened SolarFlare interface p10p1 (MAC address 05:90:f5:ef:aa:67, MTU 1514)\nengine: start app id19_NIC\nOpened SolarFlare interface p10p1 (MAC address 13:90:f5:ef:aa:67, MTU 1514)\nengine: start app id17_Virtio\nengine: start app id22_NIC\nOpened SolarFlare interface p10p1 (MAC address 16:90:f5:ef:aa:67, MTU 1514)\nengine: start app id15_NIC\nOpened SolarFlare interface p10p1 (MAC address 0f:90:f5:ef:aa:67, MTU 1514)\nengine: start app id18_Virtio\nengine: start app id20_NIC\nOpened SolarFlare interface p10p1 (MAC address 14:90:f5:ef:aa:67, MTU 1514)\nengine: start app id2_NIC\nOpened SolarFlare interface p10p1 (MAC address 02:90:f5:ef:aa:67, MTU 1514)\nengine: start app id9_NIC\nOpened SolarFlare interface p10p1 (MAC address 09:90:f5:ef:aa:67, MTU 1514)\nengine: start app id19_Virtio\nengine: start app id29_Virtio\nengine: start app id17_NIC\nOpened SolarFlare interface p10p1 (MAC address 11:90:f5:ef:aa:67, MTU 1514)\nengine: start app id11_NIC\nOpened SolarFlare interface p10p1 (MAC address 0b:90:f5:ef:aa:67, MTU 1514)\nengine: start app id55_NIC\nOpened SolarFlare interface p10p1 (MAC address 37:90:f5:ef:aa:67, MTU 1514)\nengine: start app id7_NIC\nOpened SolarFlare interface p10p1 (MAC address 07:90:f5:ef:aa:67, MTU 1514)\nengine: start app id14_NIC\nOpened SolarFlare interface p10p1 (MAC address 0e:90:f5:ef:aa:67, MTU 1514)\nengine: start app id3_Virtio\nengine: start app id26_Virtio\nSending neighbor solicitation for next-hop 2::2\n. Apparently the other command works, but actually status code is 139 (SIGSEGV):\nbash\nOpened SolarFlare interface p10p1 (MAC address 0e:90:f5:ef:aa:67, MTU 1514)\nengine: start app id3_Virtio\nengine: start app id26_Virtio\nSending neighbor solicitation for next-hop 2::2\ndpino@grindelwald src (master) $ echo $?\n139\n. @lukego I'm interesting in benchmarking the driver using the approach you commented:\n\"If the hypervisor is doing kernel-bypass networking (e.g. snabbnfv) then it should be possible to achieve good results and it should be possible to suppress all interrupts so the bottleneck will not be there.\"\nI'm not sure of what this means. To clarify, would be the idea to launch a virtual machine using the vhost-user driver plus snabbnfv instead of using a tap interface? I have tried that but once inside the guest I cannot see the NIC registered at /sys/class/net, which makes sense since it's bypassing the kernel. The virtio-net driver needs a NIC with a PCI address on the guest side [1].\nThis is the command I'm using to launch the VM:\nsudo $QEMU \\\n    -m 1024 -kernel $KERNEL \\\n        -append 'earlyprintk root=/dev/vda rw console=ttyS0' \\\n    -numa node,memdev=mem \\\n        -object memory-backend-file,id=mem,size=1024M,mem-path=/hugetlbfs,share=on \\\n    -netdev type=vhost-user,id=net0,chardev=char0 \\\n        -chardev socket,id=char0,path=/home/igalia/dpino/socket/vhost_A.sock,server \\\n        -device virtio-net-pci,netdev=net0,mac=52:54:00:00:00:01 \\\n    -M pc -smp 1 -cpu host --enable-kvm -serial stdio \\\n    -drive if=virtio,file=$IMG,format=raw -curses\n[1] https://github.com/dpino/snabbswitch/commit/e9ce5d90a10ca02275c78a62c6020bec9acd01f1#diff-369c9508a86f051bb8fb8bed85095bfdR63\n. @lukego Thanks, it worked. I managed to run virtio_net/selftest using snabbnfv as hypervisor. Now I need a better way of measuring performance, plus running the lwaftr on the guest side and see how it performs.\n. @lukego With regard to the PCI address needed by the driver, I refer to the PCI address that it needs when initializing the driver, same as the Intel82599 driver does. Relevant lines:\nhttps://github.com/SnabbCo/snabbswitch/pull/645/files#diff-6752da38bfe8e57abc0421a80fd9c5d3R63\nhttps://github.com/SnabbCo/snabbswitch/pull/645/files#diff-c74e02ebb8719147b6734d961b00ca20R84\nThe address is used to access the PCI Base Address Register and setup accordingly http://wiki.osdev.org/Virtio.\n. With regard to the performance problems we had in the implementation of the lwaftr due to packet copies, besides packet copy another reason for the bad performance was having to allocate/deallocate several lib.protocol objects per packet operation. What the code refactoring did by using ethernet, ipv4 and ipv6 struct headers instead directly was two things: 1) optimize packet copying by avoiding unnecessary operations, as it was mentioned and 2) avoid excessive object lib.protocol.{eth,ipv4,ipv6} allocation/deallocation per packet.\nI observed that for instance in the implementation of VPWS (apps/vpn/vpws.lua) there are several lib.protocol objects (ipv6, gre, ethernet) instantiated in the constructor. The value of these objects never change. They are reused and copied into a new datagram per push operation. However, in our case the data these headers contain vary per push operation, so we needed to create new ethernet, ipv4 and ipv6 lib.protocol objects per push.\nHere's a diff of one of the code refactorizations: https://github.com/Igalia/snabbswitch/commit/3a357b2365ce24c900d6800888ea8e9fbe427f74\nThe add_ipv6_header and add_ethernet_header functions, removed in this commit, instantiated new lib.protocol.ipv6 and lib.protocol.ethernet objects respectively.\n. FWIW, our previous code followed that principle, using free() on the lib.protocol objects after push. Here it's an example: https://github.com/Igalia/snabbswitch/blob/3a357b2365ce24c900d6800888ea8e9fbe427f74/src/apps/lwaftr/lwaftr.lua#L87). However performance improved if we skipped object creation and use the struct headers to cast and manipulate packet data directly. Maybe we were doing something wrong, dunno.\nWith regard, to pop_raw, it's what we used too for decapsulation.\n. Yes, sure. We would love to give the lib.protocol classes a try after the overhaul. I prefer to use a proper class than to manipulate the packets directly, and it's better for the whole project itself. 1Mpss to 6Mpps, really good improvement :)\n. @eugeneia Thanks for the pointer :) With regard to testing, a loop that leaves the descriptors open until Snabb Switch sounds the way to go. I like the idea of the \"stress tester\" too.\nI tested the patch manually. I run the selftest while monitoring file descriptors with strace:\n```\nsudo strace -e trace=open,close,read,write,connect,accept ./snabb snsh -t apps.socket.raw\nwrite(4, \"\\0\\0\\0\\0\\0\\2\\0\\0\\0\\0\\0\\1\\206\\335\\0\\0\\0\\0\\0;\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 54) = 54\nread(4, \"\\0\\0\\0\\0\\0\\2\\0\\0\\0\\0\\0\\1\\206\\335\\0\\0\\0\\0\\0;\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 10240) = 54\n+++ exited with 0 +++\n```\nWith the patch applied the last call is a close:\nwrite(4, \"\\0\\0\\0\\0\\0\\2\\0\\0\\0\\0\\0\\1\\206\\335`\\0\\0\\0\\0\\0;\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 54) = 54\nread(4, \"\\0\\0\\0\\0\\0\\2\\0\\0\\0\\0\\0\\1\\206\\335`\\0\\0\\0\\0\\0;\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 10240) = 54\nclose(4)                                = 0\n+++ exited with 0 +++\nI think I will go with @eugeneia suggestions first, and after try to generalize the code so it can be reusable for other apps. \n. Here's the output of the selftest:\n$ sudo ./snabb snsh -t apps.socket.raw\nopen raw socket: Too many open files\nSelftest passed\nOnce the system reports \"Too many open files\", the loop is exited and opened descriptors are closed.\n. Done. Now constructors return an error code in case objects could not be created. The test includes an assert that verifies creation of RawSocket object was fine.\nI pushed 3 additional commits. I think it makes sense at least to squash the reverted patches.\n. @eugeneia That's correct.\n. I merged RawSocketDev into RawSocket. I made some slight modifications too like adding a stop call at the end of the test, and make transmit return the length of the packet transmitted. I have a patch in the queue implementing the change @nnikolaev-virtualopensystems suggested, retry transmit in case the whole length of a packet was not transmitted. Let me know what do you think.\n. @lukego: Sorry, it took me a while to update the PR. I added some code to handle err in the constructor. error(errno) prints out a message according to errno and stops execution. I had to push force to update as the branch needed to be rebased against master.\n. @lukego Thanks for the long explanation. Now I have much clearer mental modal. Initially I clicked the \"resolve conflict\" button that merges master into the PR, but that didn't seem to me right. Now I know it's safe to do that.\n. There's a related PR https://github.com/SnabbCo/snabbswitch/pull/641\n. I like this more. Thanks for detailed explanation and links. I updated the PR. In this case I squashed the previous commit as it's just one line of code.\n. @eugeneia I rebased the PR and push forced again. I was able to merge the PR into current master:\n```\ncommit adbb255b66ba08b30e4b417813dbffff62864d50\nMerge: e285507 145bdec\nAuthor: Max Rottenkolber max@mr.gy\nMerged PR #670 (v2015.12 release) onto master\n\n```\n. It seems it can be merged now :)\n. Before I was working with the virtio_net driver I thought the only use case for PCI address simplification would be the pattern you describe \"XX:00.Y\" which could be conveniently write as \"04:00.1\", for instance. \nThe virtio_net driver takes a PCI address from devices at /sys/class/net, which in the VM I use for testing have the form \"00:0X.Y\" (many devices in my laptop at /sys/bus/pci/devices follow this pattern too). A PCI address were device is the only significant part could be simplified as \":04.0\"  or \"4.0\" or even \"4\". I think most of the time we will be dealing with this two type of addresses. The multiple examples are there to show the function works well and it's consistent. Basically what it does is to fill up with zeros what's left unwritten.\n. @eugeneia Right. Closed PR.\n. Thanks for the pointer. I tried to reproduce the bug but I got stuck in the NFV test. Is there any way of launching the CI bot when tests fail?\n. @nnikolaev-virtualopensystems the patch had the effect of reducing the amount of aborted traces due to \"loop unroll limit reached\", however the reason for LuaJIT to blacklist several traces which eventually made it to run in interpreted mode was due to a misconfiguration in the testing script in the guest (the cards were cross sending packets with each other).\nIn any case, the main reason for the poor performance we observed was that the host was handling two cards with one core, when actually how we are would like to run the lwaftr virtualized is by bringing up two cards where each of them is managed by a different snabbnfv process running in a different core. With this setting performance improves. There are additional comments in https://github.com/Igalia/snabbswitch/issues/199\n. @nnikolaev-virtualopensystems Sure, I can give more details on the configuration part. To make a recap the settings were the following:\nThere are two cards connected as this:\n81:00.0 <-> 02:00.0\n81:00.1 <-> 02:00.1\nIn the testing environment, packetblaster sends packets on 02:00.0. A script sets up 81:00.0 and 81:00.1 to be managed by vhost-user, so QEMU can use these cards and expose them to the guest as 00:08.0 and 00:09.0. These are devices that are going to be managed by the VirtioNet driver inside the guest. They are set as 'eth0' and 'eth1':\nconfig.app(c, \"eth0\", virtio_net.VirtioNet, {pciaddr=pci1})\nconfig.app(c, \"eth1\", virtio_net.VirtioNet, {pciaddr=pci2})\nIn the original script that I tested these cards were cross sending packets with each other:\nconfig.link(c, \"eth0.tx -> eth1.rx\")\nconfig.link(c, \"eth1.tx -> eth0.rx\")\nA measurement script in the host listens to traffic received on 02:00.1. Instead of sending back what is received from eth1 to eth0, what I did was to pipe it to a sink. With that change profiles on LuaJIT do not report switched to interpreted mode and all the backlisted traces are gone. Traffic can still be measured by the measurement script.\nconfig.link(c, \"eth0.tx -> eth1.rx\")\nconfig.link(c, \"eth1.tx -> sink.rx\")\nRegarding the host, I don't know if that could be concluded for every Snabb Switch application. The original script that I used for testing brought up 2 NICs managed by VhostUser on the same program. Performance of the lwAFTR was around 2.5 MPPS under that setting. I didn't tried though bringing up the NICs with SnabbNFV and run each process on the same core. That would be an interesting experiment (check whether there's any difference of running that setting as two processes running on a core, or just one process).\n. @nnikolaev-virtualopensystems I understand what you mean, sinking one of the links is like sending half of the traffic. I can comment on what I observed:\nIn the testing code, the performance sending traffic bidirectionally or only in one direction was actually the same, although when traffic was sent bidirectionally LuaJIT reported 45% time execution in interpreted mode (many traces were aborted).\nWhen I first started to run the lwAFTR using the virtio_net driver (with traffic generated by 'transient' from the host) I realized that LuaJIT didn't report any % time executed in interpreted mode and there was no trace aborts, so I concluded this was an issue in the testing apps code.\nMaybe there's a different app layout for benchmarking a bidirectional flow of traffic, I don't know, I'd need to look into that. But even running the lwAFTR using 2xVirtioHost cards with one process LuaJIT didn't report switching to interpreted mode.\n. @eugenia Sorry, I was a bit away lately. I will tackle these issues in the coming days and verify it works in the context of packetblaster->snabbnfv<->vm(snabb+l2fwd). Thanks everyone for the review and comments.\n. I think this is working now. Here is how it works:\nLet's assume the following scheme:\nHost1          Host 2\n01:00.0 <-> 01:00.0\n01:00.1 <-> 01:00.1\nL2Fwd creates a full-duplex softwire in Host2 between 01:00.0 and 01:00.1.\nPacketblaster blasts packets into Host 1 01:00.0 which eventually reach Host 2 01:00.0, because both cards are wired together. The packets are forwarded to Host 2 01:00.1 and transmitted. Finally the packets arrive to Host1 01:00.1\nTo make packets come back the other way, it's necessary to create a packet bouncer in Host 1 01:00.1. I added a new packetblaster mode called 'bounce' which takes only two PCI params, the NIC where to blast packets to and the NIC of the bouncer, which sends back any traffic it receives.\nWith regard to mimic the MAC addresses scheme DPDK's l2fwd app uses, I didn't follow it as packets get through anyway. I think what's interesting of this app is to have a tool which can be a counterpart of packetblaster and can help verify NIC links are working.\nExample of use:\nHost 1\n$ sudo ./snabb packetblaster bounce my.pcap 0000:02:00.0 0000:03:00.0\nTransmissions and receptions (last 1 sec):\n02:00.0 TXDGPC (TX packets) 433,968 GOTCL (TX octets)   240,418,272\n02:00.0 RXNFGPC (RX packets)    7,205   GORCL (RX octets)   3,991,570\nTransmissions and receptions (last 1 sec):\n02:00.0 TXDGPC (TX packets) 1,848,822   GOTCL (TX octets)   1,024,116,090\n02:00.0 RXNFGPC (RX packets)    1,573,190   GORCL (RX octets)   871,551,692\nTransmissions and receptions (last 1 sec):\n02:00.0 TXDGPC (TX packets) 2,177,641   GOTCL (TX octets)   1,206,404,804\n02:00.0 RXNFGPC (RX packets)    2,177,631   GORCL (RX octets)   1,206,408,128\nHost 2\n$ sudo ./snabb l2fwd -v 0000:02:00.0 0000:03:00.0\nReport (last 1 sec):\nlink report:\n              95,052 sent on nic1.tx -> nic2.rx (loss rate: 0%)\n               2,944 sent on nic2.tx -> nic1.rx (loss rate: 0%)\nload: time: 0.98s  fps: 100,419   fpGbps: 0.449 fpb: 6   bpp: 550  sleep: 35  us\nReport (last 1 sec):\nlink report:\n             942,990 sent on nic1.tx -> nic2.rx (loss rate: 0%)\n             225,032 sent on nic2.tx -> nic1.rx (loss rate: 0%)\nload: time: 1.00s  fps: 1,070,037 fpGbps: 4.785 fpb: 12  bpp: 550  sleep: 0   us\nReport (last 1 sec):\nlink report:\n           3,120,653 sent on nic1.tx -> nic2.rx (loss rate: 0%)\n           2,402,703 sent on nic2.tx -> nic1.rx (loss rate: 0%)\nload: time: 1.00s  fps: 4,355,314 fpGbps: 19.477 fpb: 22  bpp: 550  sleep: 0   us\nReport (last 1 sec):\nI also tested the l2fwd app running within a guest using the virtionet driver. It works fine too.\n. Apparently, I don't see any reason why CI failed cc @eugeneia \n. @eugeneia My understanding after reading http://dpdk.org/doc/guides/sample_app_ug/l2_forward_real_virtual.html is:\n- There's a traffic originator running in one side (host A).\n- Packets reach a destination host (host B) and get forwarded between two ports on the same host. That's what the l2fwd app does AFAIU.\n- Packets finally reach host A on a different port and go back the same way.\nHost A actually needs two packet generators, an app that creates synthetic packets as the packetblaster does and something that will bounce traffic.  Checking whether packets are being received in the packetblaster originator app tells traffic has gone all the way through the links.\nWith regard to the packetblaster-bounce/l2fwd tandem, I agree with your comment.  The point is that as the traffic generators run in a host independent of the host that has created the port forwarder, l2fwd/bounce cannot be on the same app.\nOTOH, packetblaster-bounce (NIC1 <-> NIC2) can be used to measure throughput of NIC2, received packets and egress link load towards NIC1.  Currently it doesn't print those reports but that would be easy to fix.\ncc @lukego @wingo @nnikolaev-virtualopensystems \n. @eugenia Totally understand, I also want to hear other opinions.  With regard to MAC addresses and swapping them, I forgot to mention that I obvious that part because it's not necessary. NICs when instantiated as SingleFunction, which is the case of packetblaster, work in promiscuous mode.\n. @eugenia I'm not a user of DPDK's l2fwd app so it's hard for me to tell how much of this app can replace it. If the use case you described is enough for ditching DPDK's l2fwd in favour of this app I can implement it that way. I see your description more like the bouncer mode I added to packetblaster, but I can put that functionality in its own app. It will be handy too for me to test a VirtioNet link. I think I will create a brand new PR.\n. @eugeneia I'm retaking this task. Your suggestion sounded good to me, however there's something I don't understand yet and it's how DPDK's l2fwd is currently being used.\nAFAIU, DPDK is only used in the packetblaster benchmark test. The test uses qemu-dpdk.img VM which has DPDK on it.\nFor testing purposes, I replaced qemu-dpdk.img in the packetblaster benchmark for qemu.img. The benchmark works but it's much slower. I was thinking about why it was working and my conclusion is that when SnabbNFV runs on its most basic form all the traffic that is received on the VM is sent back to the host because the configuration goes like this:\nlua\n      config.link(c, NIC..\".tx -> \"..VM_rx)\n      config.link(c, VM_tx..\" -> \"..NIC..\".rx\")\nThe packets are received on the NIC running packetblaster as both cards are wired together. Is this the reason why the packets in a VM are sent back to the host and traffic can be benchmarked?\nAs for the qemu-dpdk.img scenario I understand the benchmark goes faster as DPDK skips the VM kernel, what I don't see is how DPDK or DPDK's l2fwd is run or who starts it :/ I grepped for \"l2fwd\" in the code base and I didn't find any results. OTOH, I see the source code of DPDK is in the /root folder, although I'm not sure if it's built as the build folder is empty.\n. @eugeneia Thanks, I was missing that part :)\n. I have given a try to implement RSS support. The comments in this thread have been very helpful.\nThinking in the current context of the lwAFTR, we need to set vmdq = true (it seems VLAN tag stripping by hardware is only available in VMDq mode). Probably we will need to modify the if conf.vmdq branch in Intel82599:new() https://github.com/SnabbCo/snabbswitch/blob/master/src/apps/intel/intel_app.lua#L38\nlua\n  if conf.vmdq then\n      if devices[conf.pciaddr] == nil then\n         devices[conf.pciaddr] = {pf=intel10g.new_pf(conf):open(), vflist={}}\n      end\n      local dev = devices[conf.pciaddr]\n      local poolnum = firsthole(dev.vflist)-1\n      local vf = dev.pf:new_vf(poolnum)\n      dev.vflist[poolnum+1] = vf\n      return setmetatable({dev=vf:open(conf)}, Intel82599)\n   else\nIn https://github.com/SnabbCo/snabbswitch/issues/522#issuecomment-114013203 @javierguerragiraldez suggested to adapt the Intel82599 driver to support RSS in the following way: \n1) When a device is first initialized, it initializes its virtual devices too:\nlua\nnic0 args={vmdq='32*4', pools={\n    [1]={mac='52:54:00:01:01:01', vlan=18, rate_limit=1e9},\n    [2]={mac='52:54:00:02:02:02', vlan=18, rss='ipv6'},\n    [3]={mac='52:54:00:10:10:10', vlan=1, mirror_pools={1,2}},\n}\nLater when the same process, or a different one, initializes the same driver, it simply requests a TX/RX queue from the VF pool.\nlua\nnic0rx1 args={rxpool=1},\nnic0tx1 args={txpool=1},\nTo me the main challenge is when a different process requests a queue from a VF. It seems that some sort of interprocess communication will be needed (a master process initializes all the VFs and a slave process requests a TX/RX queueu from a VF, or much simpler, a VF). Is it possible to use the shared memory mechanisms to share an object as complex as the VF object? If not, what would be the right way to share the TX/RX queues among different processes?\n. @javierguerragiraldez Thanks for the input Javier. I thought about that, but the physical device is accessed via mapped pci memory and has an exclusive lock on the device. If a second process tries to map the same physical device, there's an error. I can change the lock to a shared lock, that would allow that two different processes can read the same device. Do you think that would work?\n. @wingo OK, let me try to hack a patch that enables mac address filtering in SF.\n. @wingo The additional patches add support for promisc parameter in SF which activates or deactivates promiscuous mode.  When promiscuous mode is off, the NIC filters Rx packets by MAC destination address. That's reflected by printing out the the RXNFGPC (Non-Filtered Good Rx Packets Counter) and RXDGPC (DMA Good Rx Packet Counter) counters. When promiscuous mode is off RXDGPC is zero.\nThe point is that to properly test non promiscuous mode, it's necessary to craft packets with MAC addresses of the tested NICs.\nAnother possibility could be to add a method set_mac which overrides the actual MAC of the NIC. I think there's a way to restore the MAC from the firmware. Let me know if this more what you had in mind.\n. Closing this PR as macaddress is now used in SF mode (commit: 28a3de15)\n. FWIW, I'm aligned with this CoC and I think is positive to have one.\n. Looking good. Supporting VLAN tags is the main concern to me.\n. @mwiget thanks for tackling the issues. If VLAN tagging can be managed by VMDq it works for me too.\n. As for me, no more changes required. LGTM.\n. @plajjan That's correct, the convention is to use 3-spaces indention. With regard to a style guideline I remember reading the codebase follows the Lua Style Guideline http://lua-users.org/wiki/LuaStyleGuide. I remember reading also the rule of thumb is to write code in the same style as the code that it has been written so far. I think those notes were on the \"size budget\" page in the Wiki, but I cannot find it anymore cc @lukego \n. @plajjan Cool, thanks. LGTM.\n. LGTM.\n. I kind of like the proposed design. Let's say that here is where the app can access any additional attributes that engine adds to the app. If every app would be responsible of passing those attributes as a second argument in the constructor I'm afraid that would end with inconsistent naming of those parameters among different apps.\nThe links initialization need the apps are placed in the apps array (new_table_table) and the input and output arrays for each app. If there's a need for the apps to do something with the links after they are setup (and before push and pull starts) perhaps is worth to emit a new event to the apps (app.linked?). But so far I've never came across such use case, so I dunno.\n. When I first read your comment I was thinking of every programmer being responsible of creating this second argument:\nlua\nconfig.app(\"name\", class, arg1, { appname = \"name\", shmpath = \"...\" })\nIf it's the engine the responsible of creating this second argument and pass it to the constructor, I take my comment aback because the names will be consistent. \nStill it's the programmer responsibility to be aware of this second argument and do something with it. It will go like this:\nlua\nfunction App:new(arg, extra)\n   local o = {\n      prop1 = arg.prop1,\n      appname = extra.appname,\n      shmname = extra.shname,\n   }\n   return setmetatable(o, { __index = App })\nend\nThat versus decorating an object with new properties (which the programmer still needs to learn about). I dunno, I see pros and cons in both approaches. Somehow Snabb is already doing this property decoration with the links (input and output). Before attempting any change I'd like to hear comments from other people, what do they think?\n. I just remembered the use case of the NDP and ARP apps which need to send a solicitation packet to resolve an IP address to a MAC address. Currently these apps need to wait until the push event to send that request while they could do it as soon as the links are ready, in a start event for instance https://github.com/Igalia/snabb/blob/lwaftr/src/apps/lwaftr/ipv6_apps.lua#L184 \n. Sorry, I closed it by mistake.\n. What it was pending to decide in this PR was whether to make links available when start is called or not. I'm more lean to this approach as @wingo suggested. The reason is that we have already found use cases in the implementation of the lwAFTR where we need to use the links before pullis called. \nFor instance, there are cases where we need to send Neighbor Solicitations requests. We wait until the links are available to do that, that means the pullmethod of the app. As a result, the pull method is bloated with logic of sending request packets and processing traffic coming from the links.\nhttps://github.com/Igalia/snabb/blob/lwaftr/src/apps/lwaftr/ipv6_apps.lua#L186\nI would rather prefer to put the request packets in the links as soon they are available, although the packets are not going to be processed until the main loop starts.\nIf everyone agrees in this approach then we need to move the start event after the links setup.\n. @eugeneia No problem, go ahead. Glad to see this PR merged :)\n. @eugenia Sorry, my fault. I was batch closing several remote branches on my fork (git push remote :branch) and I think I closed this one by mistake. I agree with keeping remote branches alive until they are merged.\n. FWIW, I agree with the proposed solution (merge ingress_drop_monitor now and iterate on it later)\n. Backported from https://github.com/Igalia/snabb/pull/322\n. It seems some benchmark tests failed but I think it's not due to the PR.\n. LGTM. This line can be removed too https://github.com/wingo/snabb/blob/2550c50c6c23653affc909fdbbe73a82321a44ed/src/apps/intel/intel10g.lua#L723\n. @lukego Sounds good to me. Recently we learned the feature was not working nice with SnabbNFV and that was a big motivation to disable it. Having the feature enabled by default for the lwAFTR but not for SnabbNFV, for instance, sounds like a better approach, or at least I like it more. I'm going to give this a try. \n. Proposed implementation in https://github.com/snabbco/snabb/pull/929\n. @adw555 Yes, I rewrote RawSocket to use ljsyscall in January, although I think it has been patched a few times later. When I rewrote I only validated the selftest kept consistent bu I didn't try example_spray. I will take a look. Thanks for reporting!\n. @adw555 I have checked that the amount of transmitted packet is significantly lower in the May release than in the October release. I run example_spray reading a pcap file, instead of an interface, and writing to a file.\nIn my tests I used a pcap file of 40K packets:\nbash\n$ capinfos v4v6.pcap | grep \"Number\"\nNumber of packets:   40 k\nThen I  run example_spray from October to May.\nbash\n$ sudo ./snabb example_spray v4v6.pcap /tmp/output.pcap\nHere are my results:\nv2015.10\nlink report:\n              40,000 sent on capture.output -> spray_app.input (loss rate: 0%)\n              20,000 sent on spray_app.output -> output_file.input (loss rate: 0%)\nv2015.11\nlink report:\n              40,000 sent on capture.output -> spray_app.input (loss rate: 0%)\n              20,000 sent on spray_app.output -> output_file.input (loss rate: 0%)\nv2015.12\nlink report:\n              40,000 sent on capture.output -> spray_app.input (loss rate: 0%)\n              20,000 sent on spray_app.output -> output_file.input (loss rate: 0%)\nv2016.01\nlink report:\n              36,465 sent on capture.output -> spray_app.input (loss rate: 0%)\n              18,232 sent on spray_app.output -> output_file.input (loss rate: 0%)\nv2016.02\nlink report:\n              39,780 sent on capture.output -> spray_app.input (loss rate: 0%)\n              19,890 sent on spray_app.output -> output_file.input (loss rate: 0%)\nv2016.03\nlink report:\n              40,000 sent on capture.output -> spray_app.input (loss rate: 0%)\n              20,000 sent on spray_app.output -> output_file.input (loss rate: 0%)\nv2016.04\nlink report:\n              40,000 sent on capture.output -> spray_app.input (loss rate: 0%)\n              20,000 sent on spray_app.output -> output_file.input (loss rate: 0%)\nv2016.05\nlink report:\n               9,690 sent on capture.output -> spray_app.input (loss rate: 0%)\n               4,845 sent on spray_app.output -> output_file.input (loss rate: 0%)\nCould you give it a try to the April release? If it's working OK, then there's something that has changed in v2016.05.\n. I don't manage to run example_spray on eth0. Maybe I'm doing something wrong. This is how I try to run it:\nlua\n(v2016.02) $ sudo ./snabb example_spray eth0 /tmp/output.pcap\nlib/pcap/pcap.lua:56: Unable to open file: eth0\nstack traceback:\n        core/main.lua:126: in function <core/main.lua:124>\n        [C]: in function 'error'\n        lib/pcap/pcap.lua:56: in function 'records'\n        apps/pcap/pcap.lua:13: in function 'new'\nIt works though on a tap interface.\n$ sudo ip tuntap add tap0 mode tap\n$ $ sudo ./snabb example_spray tap0 /tmp/output.pcap\nlink report:\n                  21 sent on capture.output -> spray_app.input (loss rate: 0%)\n                  10 sent on spray_app.output -> output_file.input (loss rate: 0%)\nAny hints?\n. I found out the reason for the \"regression\" between v2016.04 and v2016.05. The regression was introduced in #882 when max_packets increased from 1e5 to 1e6. 1e5 gets better results in this case.\nv2016.04\nlua\n(v2016.04) $ sudo ./snabb snsh -p example_spray v4v6.pcap /tmp/output.pcap\nlink report:\n              40,000 sent on capture.output -> spray_app.input (loss rate: 0%)\n              20,000 sent on spray_app.output -> output_file.input (loss rate: 0%)\nv2016.05\nlua\n(v2016.05) $ sudo ./snabb snsh  -p example_spray v4v6.pcap /tmp/output.pcap\nlink report:\n              15,300 sent on capture.output -> spray_app.input (loss rate: 0%)\n               7,650 sent on spray_app.output -> output_file.input (loss rate: 0%)\nv4v6.pcap is a mix of IPv4 and IPv6 packets. It contains 40K packets. The file can be downloaded here: http://http://people.igalia.com/dpino/v4v6.pcap\nI also increased the duration of example_spray up to 10 seconds, to give it time to the script to process the whole file.\n. @eugeneia @lukego I agree with Luke's point of view, the snippet of code is run every time configure is called so better call the event configure instead of start. However that's not what I originally intended. Originally the trigger of the event was from ops:start(), but when doing it from there the links are not set up at that point, so that makes start less useful. So I moved the code to after the links are setup, but that changes the semantic of the event as the code is called every time.\nI'm OK with calling it configure, but as apps can implement stop I think having a start counterpart makes sense. If going for this solution, the start loop should only be called if ops.start was executed.\n. @eugeneia Taking a look at Alexander's patch I think it makes sense to call it configure too, as he originally called that method post_config. Agree with the name change.\n. I think I can explain best what I need if I describe the use cases I'm trying to fix.\nI would like to give apps a chance to complete their initialization once they have been configured (with their links setup too). app:reconfig is only called if an app already exists in the app_array so it cannot be used.\nOne example that could take advantage of this new event is the NDP app in the lwAFTR.\nhttps://github.com/Igalia/snabb/blob/lwaftr/src/apps/lwaftr/ipv6_apps.lua#L186\nThere's a case where the NDP app needs to send Neighbour Solicitation packets until a parameter is resolved (self.dst_eth). At this moment, we are implementing this logic in the push method, because only at that point links are setup. However, we could start sending NS packets as soon as the app and the links are ready.\nIt could be argued that the links won't be processed until push is called in any case, but putting this logic in two different methods would make the code cleaner and the push method less bloated IMHO.\nAnother use case is creating counters by app name. An app name is added to an app on start, but since there's no start callback apps can react to, the app has to wait until push to create its counters and it has to do it only once. Something like:\nlua\nfunction App:push ()\n    if not self.added_counters then\n       self.counter = counter.open(self.appname..\"/counter\")\n       self.added_counters = true\n   end\n   local input, output = assert(self.input.input), assert(self.output.output)\n   while not link.empty(input) do\n      ...\n   end\nend\nI think a method where apps could further initialize themselves (in new appname or links do not exist), seems necessary.\nSolutions I can think of:\n- We could discard this new configure method and call reconfig on start too, but that would make the reconfig and startstates less clear. In addition to that, the first time reconfig is called, links wouldn't exist. I doesn't seem like a good solution :(\n- I go back to my original request of adding a start event (I agree having a configure and reconfig callbacks would be confusing). I wanted to defer the call of the event until the links were created, but I didn't do it right. Should be something like:\nlua\nfunction apply_config_actions (actions, conf)\n   ...\n   local started_apps = {}\n   function ops.start (name)\n      ...\n      table.insert(started_apps, app)\n   end\n   -- Setup links.\n   ...\n   for _, app in ipairs(started_apps) do\n      if app.start then app:start() end\n   end\nend\n. @lukego Thanks for the detailed explanation, I like this new design more. In fact, before I needed to add a timer.deactivate method which I can remove now. I was not aware that apps should not depend on timers to implement their logic.\n. I agree with the proposal.\nWith regard to the start callback I proposed on #905 and which goal was to notify apps when configuration started, so they can do additional stuff (one use case was creating counters by appname), I'd go with @wingo proposed solution in https://github.com/snabbco/snabb/pull/905#issuecomment-215675364, which was to pass an additional second argument to new with extra information for the app ({ appname=..., shmpath=..}, etc. Example:\nlua\nfunction ops.start (name)\n   local class = conf.apps[name].class\n   local arg = conf.apps[name].arg\n   local app = class:new(arg, { appname = name})\n   ....\n   app.appname = name\n   app.output = {}\n   app.input = {}\nOtherwise, an app should have to wait until link or push to do something which requires the apps name (create counters by appname in this case).\n. @eugenia Thanks for the pointer, that covers my use case. As for renamingconfiguretolink, that sounds right to me. Would need to update the docs and the description ofconfigure.\n. Thanks @eugeneia. Fixed.\n. @eugenia I moved default initialization of theloopflag to the subprograms (replay, synth). CI should pass now. I added the--no-loopswitch tosynth` too.\n. A benchmark test failed:\nERROR snabbnfv-iperf-1500 -> 0.804771 of 5.03 (SD: 0.745252 )\nI believe the number is not good enough after the change. I run the benchmark the test individually, here's what I got:\nWith the patch:\n$ sudo bench/snabbnfv-iperf-1500\n3.91\nWithout the patch (v2016.08):\n$ sudo bench/snabbnfv-iperf-1500\n2.79\n@eugeneia Do you think the patch introduces a performance regression or there's a chance CI passes if run again?\n. I think this regression happened when the implementation of htonl and htons changed in commit ca86c9403a32a7c800c63143324613b1ee476ba5, from using C API to core/lib.lua (which relies on LuaJIT's bitwise function). C's htonl and htons return an unsigned 32-bit, so the original code was correct.\nThe cast should have changed in that commit too. Interestingly it was me who asked Kristian Larsson to use htonl and htons from core/lib.lua, so I take all the blame here :)\n. @kbara Definitely dealing with 12-bit numbers is harder than to deal with 16-bit or 32-bit numbers. Are you suggesting to check only the TCI part of a VLAN tag? That's one possible approach, but that's not what the original code did. Since I am OK with the original implementation I changed as little code as possible to fix the issue.\n. Using Snabb's core/lib htons and htonl is preferred than C.htonl or C.htons because it saves making a C call, saving a lot of CPU cycles. As a rule of thumb, Snabb prefers executing Lua code than making C calls, that's my understanding. In addition, other parts of Snabb's codebase use the core/lib.lua functions too. I think all code than needs to use htonl and htons should use the functions from core/lib.\nWhen we wrote down this app we were not aware of the core/lib functions. In my opinion, the problem is not to use the functions from lib, it's to not have updated our code accordingly.\n. @alexandergall I agree that check is safer if the implementation of how tag value was obtained changes.I made my comment assuming we know tag is a value obtained as a result of a LuaJIT bitwise operation (signed int). In that case casting a uint32 to a signed int can be saved if directly casting the original value as signed int.\n. @eugeneia @kbara I agree #1024 is the right fix for this case. Agree also the test case can become handy to prevent future regressions. I can rework the path and add only a test case.\n. Originally nh_fwd was its own app but then it was moved to apps/lwaftr. It's true that it could be used for other apps that need to forward traffic to a service (in this case the lwaftr) and to a VM, but until there's a real need to reuse it we decided to move it to apps/lwaftr. What @eugeneia spotted is the remaining README that we forgot to remove.\n. @emmericp Sorry for the late reply, I think I have some time now to take a look at this.\nWe have only tested the virtio-net driver together with SnabbNFV as network hypervisor. I would encourage you that, at least for testing purposes, to use SnabbNFV.\nThe line that my colleague @wingo pasted from https://github.com/snabbco/snabb/blob/master/src/program/lwaftr/virt/lwaftrctl sets up indeed a tap interface, but that interface is for ssh into the VM. The line that setups the VM with a virtio-net driver is: \n-chardev socket,id=char1,path=${VHU_SOCK1},server \\\n            -netdev type=vhost-user,id=net0,chardev=char1 \\\n            -device virtio-net-pci,netdev=net0,addr=0x8,mac=52:54:00:00:00:01 \\\nThe vhost-user driver allows QEMU to talk with an user-space program via a socket as IPC (since they're two different programs). On the other hand, Snabb features a VhostUser driver making the communication possible. I guess you got this part right, since it seems you have already tested SnabbNFV successfully.\nOnce you got into the VM, you should see an eth0 interface with address 0000:00:08.0. The vendor id should be virtio-net (0x1AF4).\n```\nip li sh eth0\n2: eth0:  mtu 1500 qdisc pfifo_fast \\\n   state UNKNOWN mode DEFAULT group default qlen 1000\n   link/ether 52:54:00:00:00:01 brd ff:ff:ff:ff:ff:ff\n```\n```\ncat /sys/devices/pci0000:00/0000:00:08.0/vendor\n0x1af4\n```\nIn the snippet you pasted above, you're using addresses 0x13 and 0x14, but the QEMU setup was:\n```\n-netdev type=tap,id=net1,ifname=tap10000i1,script=/var/lib/qemu-server/pve-bridge,downscript=/var/lib/qemu-server/pve-bridgedown,vhost=on\n-device virtio-net-pci,mac=2A:A1:99:DF:D4:37,netdev=net1,bus=pci.0,addr=0x13,id=net1,bootindex=301 \n-netdev type=tap,id=net2,ifname=tap10000i2,script=/var/lib/qemu-server/pve-bridge,downscript=/var/lib/qemu-server/pve-bridgedown,vhost=on\n-device virtio-net-pci,mac=FA:8E:98:13:3E:0F,netdev=net2,bus=pci.0,addr=0x14,id=net2,bootindex=302\n```\nI have never use virtio-net in combination with a tap network backend, only with vhost-user. This might be an issue.\nIf you want to try the vhost-user network backend, you would need to run SnabbNFV on the host so Snabb can work as an hypervisor of the card:\n$ sudo ./snabb snabbnfv traffic <pci> <ports.cfg> <socket-path>\nFor instance:\n$ sudo ./snabb snabbnfv traffic 0000:02:00.0 ports.cfg /tmp/vh1.sock\nWhere ports.cfg is a file like:\nreturn {\n   id = \"A\",\n}\nThe command line running QEMU should point to the same socket path:\n-chardev socket,id=char1,path=/tmp/vh1.sock,server \\\n   -netdev type=vhost-user,id=net0,chardev=char1 \\\n   -device virtio-net-pci,netdev=net0,addr=0x8,mac=52:54:00:00:00:01 \\\nSummarizing:\n- Snabb can be used as a networking hypervisor in QEMU. That's done via the vhost-user network backend which is part of QEMU and allows to communicate QEMU with an user space program (Snabb).\n- SnabbNFV is the program that allows to run Snabb as a hypervisor for QEMU/KVM.  On its most basic configuration snabbnfv traffic appends a VHostUser driver to a network card, making it possible to use the card virtualized from QEMU.\n- Network interfaces inside the VM handled by VhostUser appear as virtio-net devices (well probably, that's due to -device virtio-net-pci,...).\n- Running Snabb inside a VM using a VirtioNet driver allows Snabb to talk directly with a virtio-net device (0x1AF4), skipping the Linux kernel. \nIf I have more time I'd try to run your code example. In the meantime, I hope this comment shed some light.\nLastly, I think that it might be possible the issue was using a QEMU version different than 2.4. I would recommend you to try QEMU 2.4 (and SnabbNFV as hypervisor, of course).\n. @emmericp I got your example working (actually a simplified version of it that works with one NIC only). Here is a walk-through guide:\n1) Start SnabbNFV on a NIC:\nbash\n$ sudo ./snabb snabbnfv traffic 0000:02:00.0 ports.cfg /tmp/vh1.sock\nports.cfg\nlua\nreturn {\n    {\n        port_id = \"A\"\n    }\n}\nMy NIC is 0000:02:00.0 which is wired to 0000:82:00.0. Later I will send packets to 0000:82:00.0 with packetblaster and since both NICs are wired, packets will arrive to the VM\n2) Start QEMU:\nrun-qemu.sh\n``` lua\n!/usr/bin/env bash\nIMAGE=./qemu-virtio.img\nsudo qemu-system-x86_64 \\\n    -kernel ./bzImage -append \"earlyprintk root=/dev/vda rw console=tty0\" \\\n    -enable-kvm -drive file=${IMAGE},format=raw,if=virtio -m 1024M \\\n    -numa node,memdev=mem \\\n        -object memory-backend-file,id=mem,size=1024M,mem-path=/hugetlbfs,share=on \\\n    -chardev socket,id=char1,path=/tmp/vh1.sock,server \\\n        -netdev type=vhost-user,id=net1,chardev=char1 \\\n        -device virtio-net-pci,netdev=net1,addr=0x8,mac=52:54:00:00:00:01 \\\n    -serial stdio -curses\n```\nThere are tons of ways of launching a QEMU image. In this case I'm using just one single NIC with vhost-user backend, handled by SnabbNFV. The VM (qemu-virtio.img) already includes snabb source code in its /root folder (other approach is to have a mount point to the host). The VM uses stdio -curses as login system, that means it logs in directly (no telnet, no ssh). The command assumes HugePages are enabled.\nHere is a link to the bzImage and qemu-virtio.img I used.\n- bzImage: https://people.igalia.com/dpino/virtio/bzImage\n- qemu-virtio: https://people.igalia.com/dpino/virtio/qemu-virtio.img\nWhen QEMU is launched it keeps waiting until there is a connection in /tmp/vh1.sock. After running SnabbNFV, if things go well a connection should be established and QEMU continues booting until login into the VM.\n3) Run echo.lua\nOnce inside the VM, cd to snabb source code and create the echo.lua example (I think the script is already present in qemu-virtio.img). It's slightly different than the original version:\n``` lua\nlocal VirtioNet = require(\"apps.virtio_net.virtio_net\").VirtioNet\nfunction run (parameters)\n   if not (#parameters == 1) then\n      print(\"Usage: echo \")\n      main.exit(1)\n   end\n   local nic1 = parameters[1]\n   local c = config.new()\n   config.app(c, \"nic1\", VirtioNet, {pciaddr = nic1})\n   config.link(c, \"nic1.tx -> nic1.rx\")\n   engine.configure(c)\n   engine.main({duration = 180, report = {showlinks = true}})\nend\nrun(main.parameters)\n```\nStart running echo.lua:\nbash\n$ sudo ./snabb snsh echo.lua 0000:00:08.0\nI let the script run for 3 minutes to give enough time to setup and run packetblaster in the host.\n4) Send packets to the VM\nIn the host, use packetblaster to send packets to the VM:\nbash\n$ sudo ./snabb packetblaster replay v4v6.pcap 0000:82:00.0\nFile v4v6.pcap is here: https://people.igalia.com/dpino/v4v6.pcap. Other pcap file is of course possible.\nIf things are going OK, the window running SnabbNFV should print out it's receiving packets now.\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\nload: time: 1.00s  fps: 0         fpGbps: 0.000 fpb: 0   bpp: -    sleep: 100 us\n(These numbers should be different than 0 if packets are received by SnabbNFV).\nOnce the echo.lua script is done it prints out the total number of packets that echoed:\nlink report:\n         117,508,905 sent on nic1.tx -> nic1.rx (loss rate: 0%)\nAnd that's it. Please let us know if the same steps worked for you. I used QEMU2.4.\n. Thanks for the patch. Indeed, this was a regression introduced in fe286538e2f272a36aefe6ca1d7cc76af5d2d656. That said, I don't know whether is worth to merge this patch as the compile_binding_table command has been removed in the lwaftr development branch. Please, check out https://github.com/igalia/snabb for an up-to-date version of the lwaftr program.. @amanand The new lwAFTR configuration format is based in a YANG schema. The lwAFTR has a custom YANG schema called snabb-softwire-v1.yang (https://github.com/Igalia/snabb/blob/lwaftr/src/lib/yang/snabb-softwire-v1.yang). It also supports the latest version of the ietf-softwire schema (https://github.com/Igalia/snabb/blob/lwaftr/src/lib/yang/ietf-softwire.yang) (https://datatracker.ietf.org/doc/draft-sun-softwire-yang). Support of the ietf-sofwire schema is done through translation to snabb-softwire.yang (internally the lwAFTR works with this schema).\nThere's a tool to migrate the old configuration file plus binding table to the new format. The command is called \"migrate-binding-table\".\n```\n$ sudo ./snabb lwaftr migrate-configuration\nUsage: migrate-configuration LWAFTR.CONF\nOptions:\n  -h, --help                 Print usage information.\n  -f, --from=VERSION         Specify version from which to migrate.\nMigrate an old-style configuration and binding table to the new YANG\nconfiguration.  LWAFTR.CONF should be the name of an old lwAFTR\nconfiguration.  Available VERSION values are:\nlegacy\n    Configuration from pre-v3.0.0 lwAFTR.\n  3.0.1\n    lwAFTR versions where \"container\" nodes in schemas are missing\n    corresponding nodes in the data unless \"presence true\" is\n    specified.\n  3.0.1.1\n    lwAFTR development snapshot where \"br\" fields of softwires were\n    0-based instead of 1-based.\nThe default version is \"legacy\".\nThe resulting up-to-date configuration will be printed on standard\noutput, ready to be saved to a new file.\n```\nSince you're coming from v2, your VERSION is legacy (that's the default). The migrate configuration tool will translation your configuration to the most recent version. By default, output is printed to stdout, so use a pipe to redirect it to a file. Example:\nbash\n$ sudo ./snabb migrate-configuration lwaftr.conf > lwaftr-migrated.conf\nLet me know how it goes.. @eugeneia I remember I reported on some issues about snabb-bot when I first started dealing with the code, but I cannot remember if it was related with submodules :/. I want to comment on the failing SnabbVMX test reported by Snabb-bot.\nSome weeks ago, we were setting up a new Snabb-bot CI in our snabb2 server. At some point we had two Snabb-bot instances running on every PR (and I think that's still the case today). Both Snabb-bot instances checked the same PRs, however on snabb1 tests passed but on snabb2 tests failed because of an SnabbVMX test.\nAfter digging into this issue I reached to the conclusion that the cause for the failing test must be an environment problem. I compared the settings between snabb1 and snabb2 and a major difference was the kernel version used. snabb1 used 3.18.26 whereas snabb2 used 4.4.31.\nI switched snabb2 to the same kernel version as snabb1 and all the tests passed.\nIn addition to that, snabbvmx was not the only failing test when using 4.4.31, there was also lib.pmu_x86. If I run this test in my laptop (4.8.0-49-generic) I got the following error:\nbash\n$ sudo ./snabb snsh -t lib.pmu_x86\nselftest: pmu_x86\nnfixed  3       ngeneral        4\nSegmentation fault (core dumped)\nThis error goes undetected in current master because the Makefile doesn't run .dasl tests (I think the lwAFTR merge should fix that).\nWhat I suggest is to switch the Snabb-bot host to a 3.X kernel and check if tests pass as a first step. Eventually the failing snabbvmx test and lib.pmu_x86 should get fixed under 4.X.. @wingo: OK, sounds good to me. I overlooked the results of lib_pmu tests, although I still got the error I pasted above if I run the test in my laptop (4.X kernel, AVX2 support). Since CI passes that's all we should care about.\nI'd look deeper into the failing SnabbVMX test (program.snabbvmx.tests.selftest.sh). It could be the case that a Linux tool is missing in the VM. That would be an easy fix. But if that's not the cause, fixing the test could take longer. I'm OK with disabling it temporarily.\nTo disable it, I believe the easiest thing to do is to rename the test from \"selftest.sh\" to \"selftest.disabled.sh\", for example. This way the test won't be run on \"make test\".. wingo: OK, so then I will just simply revert the last commit.. A similar bug was reported here https://github.com/snabbco/snabb/issues/928. Can you provide a link to the pcap file you're using as well as the command line you're using to send the packets to p2p2 interface? It makes it easier to reproduce the bug.. An app's push method is called when receiving packets on that app. Your app graph is defined as:\nlua\nconfig.link(c, \"pushtest.output1 -> snabbtap.input\")\nSnabb's main loop consists of two steps: an inhale step that pulls packets into the links and an exhale step that pushes packets into the apps. According the graph you've defined above the following app methods will be called: pushtest:pull (inhale) and snabbapp:push (exhale). And that's why your PushApp:pull method gets called but PushApp:push not (it's Tap:push which is being called instead).. Yes, exactly. If you want to turn your pushtest app into an app that generates packets it has to behave similarly to other packet generator apps (a network driver, loadgen or pcap.PcapReader). All these apps overwrite the pullmethod and there create a packet out of somewhere (the network card, from scratch or fetching it from a .pcap file) and transmit the packet into an output link.\nIt would look something like this:\nlua\nfunction PacketGenerator:pull()\n   local output = self.output.output\n   while not link.full(output) do\n      -- Generate packet.\n      local pkt = packet.create()\n      link.transmit(output, pkt)\n   end\nend\nIn your graph, the output link has to have the same name as used in the pull method, otherwise the link could not be found.\nlua\nconfig.link(c, \"pushtest.output -> snabbtap.input\"). I run the test case with strace in a NUMA host and a non NUMA host.\nNUMA host:\nget_mempolicy([MPOL_DEFAULT], [000000000000000000], 64, NULL, 0) = 0\nnon-NUMA host:\nget_mempolicy(0x40c820c8, 0x40c820f8, 64, NULL, 0) = -1 EINVAL (Invalid argument)\nApparently the problem is in the first two parameters: mode and mask.. Some updates on the bug.\nI took a look a the v4.10 kernel source and apparently the problem is in maxnode value. When it gets initialized, it's default value is 64. According to get_mempolicy man page an EINVAL is returned if \"The value specified by maxnode is less than the number of node IDs supported by the system\". It's this check in the kernel source:\nhttps://github.com/torvalds/linux/blob/v4.10/mm/mempolicy.c#L1477\nIn my system (Ubuntu 17.04) if I modify ljsyscall and initialize mask.size to 1024, there's no error (lib/numa.lua selftest passes).\nIn the kernel, MAX_NUMNODES gets initialized here:\nhttps://github.com/torvalds/linux/blob/master/include/linux/numa.h\nI believe that in the kernel shipped by Ubuntu 17.04 the value of NODES_SHIFT is 10 thus MAX_NUMNODES is 1024.\n. @benagricola Thanks for the tip. I don't know either if there's a better way to retrieve MAX_NUMNODES value. I pushed a patch that calculates MAX_NUMNODES as you suggested.. The hugetlb flag requests to mmap a region of memory as a hugepage. For some reason requesting a mmap as a hugepage is not working in your system. It seems from the error report you attached that hugepages are enabled in your system. Snabb was at least able to allocate some hugepages and then failed:\n[memory: Provisioned a huge page: sysctl vm.nr_hugepages 503 -> 504]\n[memory: Provisioned a huge page: sysctl vm.nr_hugepages 504 -> 505]\n[memory: Provisioned a huge page: sysctl vm.nr_hugepages 505 -> 506]\ncore/main.lua:26: Failed to allocate a huge page for DMA\nfile not found: core/main.lua: No such file or directory\nNormally a hugepage size is 2048Kb. Check out how many hugepages are available in your system. This is what my system reports:\nbash\n$ cat /proc/meminfo | grep Huge\nAnonHugePages:         0 kB\nHugePages_Total:    4096\nHugePages_Free:     4096\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:       2048 kB\nUnless your system starts with 3 or less hugepages, there shouldn't be any issue. In any case, try increasing the number of hugepages in your system:\nsysctl -w vm.nr_hugepages=512\nAnother possible cause is that you're using a shmax limit that is lower than the default hugepage size. Snabb should automatically handle that if that's the case. See https://github.com/snabbco/snabb/issues/366. May trigger some ideas.\nAs for the kernel version, I run Snabb on a 3.18.X kernel and I never stumbled into this issue.. In summary what it seems it's happening is that when Snabb tries to allocate a huge page in your system, it cannot. After 3 attempts it fails.\nI agree shmax is not a problem since  \"memory: Enabling huge pages for shm...\" message is part of the log.\nTry the following minimal test case:\n```lua\nlocal syscall = require(\"syscall\")\nlocal memory = require(\"core.memory\")\nlocal resolve_physical = memory.resolve_physical\nlocal ensure_hugetlbfs = memory.ensure_hugetlbfs\nlocal tag = 0x500000000000ULL\nfunction allocate_huge_page (size,  persistent)\n   ensure_hugetlbfs()\n   local tmpfile = \"/var/run/snabb/hugetlbfs/alloc.\"..syscall.getpid()\n   local fd = syscall.open(tmpfile, \"creat, rdwr\", \"RWXU\")\n   assert(fd, \"create hugetlb\")\n   assert(syscall.ftruncate(fd, size), \"ftruncate\")\n   local tmpptr = syscall.mmap(nil, size, \"read, write\", \"shared, hugetlb\", fd, 0)\n   assert(tmpptr, \"mmap hugetlb\")\n   assert(syscall.mlock(tmpptr, size))\n   local phys = resolve_physical(tmpptr)\n   local virt = bit.bor(phys, tag)\n   local ptr = syscall.mmap(virt, size, \"read, write\", \"shared, hugetlb, fixed\", fd, 0)\n   local filename = (\"/var/run/snabb/hugetlbfs/%012x.dma\"):format(tonumber(phys))\n   if persistent then\n      assert(syscall.rename(tmpfile, filename))\n      shm.mkdir(shm.resolve(\"group/dma\"))\n      syscall.symlink(filename, shm.root..'/'..shm.resolve(\"group/dma/\"..lib.basename(filename)))\n   else\n      assert(syscall.unlink(tmpfile))\n   end\n   syscall.close(fd)\n   return ptr, filename\nend\nallocate_huge_page (2048 * 1024)\n```\nI can execute that program in my system without errors:\nbash\n$ sudo ./snabb snsh allocate.lua\n$ echo $?\n0\nIf you're not able to run the program above, that means there's a problem in your system environment. To figure out what's wrong I would run the program above with strace (sudo strace ./snabb snsh allocate.lua) and check the actual system call that is failing. Then try to isolate that system call in a more minimal C program and figure out why it's failing.\n.  So if I understand correctly as long as the file descriptor is from a hugetlbfs, there's no need to add the hugetlb / MAP_HUGETLB flag on a mmap (in fact, that created an error on CentOS 7). It's a bit strange, but if that makes Snabb can run on more systems I'm fine with it.. The virtio_net driver was mostly tested using QEMU v2.4  + SnabbNFV on the host side. Then on the guest side there's no requirements. As far as I recall, other users have reported the virtio_net driver was not working when using other hypervisors, or even with QEMU versions higher than 2.6.\nMy suggestion is, if possible, to use QEMU v2.4 + SnabbNFV on the host side. . PR https://github.com/snabbco/snabb/pull/1369 fixes vhost-user driver for QEMU >v2.8. Likely this was the reason of the error. FWIW, the virtio-net driver is still working as tested in https://github.com/Igalia/snabb/pull/1165.. Setting a fixed number of max numnodes didn't work. In my testing case, where maxnumnodes is 1024, if I set a number larger than that I got a segfault error. I rewrote the get_maxnumodes so it doesn't depend on a fixed file size.. Confirmed I will attend next edition of Fosdem.\nJAM Hotel looks really cool. Usually I stay at whatever hotel other igalians go (there's a bunch of us attending Fosdem every year so we tend to organize the trip together). I think we haven't chosen a hotel yet, so I suggested to stay at JAM Hotel (and kill two birds with one shot).\nAs for talks, I proposed an introductory Lighting Talk about Snabb (no notification yet whether was approved or not).. As far as I recall, #1200 was about fixing a bug in the numa library on systems that only have one NUMA node. This issue seems to be different. If I understand correctly, the issue is running the lib.numa selftest on systems that are not NUMA.. I cannot reproduce the bug as I haven't managed to disable NUMA in my system. Apparently it's possible to turn it off by passing 'numa=off' to the kernel command line arguments. I did that but when I run numactl -H it reports one NUMA node.\nIn any case, I realized there's a system call that reports whether a system has NUMA available. Apparently ljsyscall doesn't expose that syscall, which is part of libnuma. Could you try running the following program?\n```lua\nmodule(..., package.seeall)\nfunction selftest()\n   print(\"seltest\")\n   local ffi = require(\"ffi\")\n   local C = ffi.C\n   ffi.cdef[[\n   int numa_available(void);\n   ]]\n   local numa = ffi.load(\"numa\")\n   print(numa.numa_available())\nend\n```\nlua\nsudo ./snabb snsh -t numa\nseltest\n0\nIn my system it returns 0 (OK). In systems where NUMA is not available should return -1.\nWhat I would do is to make the selftest return SKIP_CODE if NUMA is not available.. Another approach. Instead of depending on the numa_available syscall, the test could simply run the numactl -H command and check its status code (I hope the call returns a number >0 when NUMA is not available).\n```lua\nmodule(..., package.seeall)\nlocal lib = require(\"core.lib\")\nfunction selftest()\n   print(\"seltest\")\n   if lib.readcmd(\"numactl -H\", 2) == \"No\" then\n      return 42\n   end\nend\n```. @eugenia works for me if that fixes the issue (sorry, I missed your PR changes).. @eugeneia Handled the loop-back interface case by showing an error (not supported interface as it's necessary a source ethernet address to build the request packet). . OK, I think you got a point. If eventually the new library replaces the current one it would be nice to have a simple version of the algorithm that is easy to grasp and that can be used to compared results to.\nActually I had an implementation in plain Lua to compare the results of the new implementation, but at the last minute I decided to compare against the current implementation.\nI pushed a new commit with the Lua implementation.. I modified the main loop to sum two 64-bit on each iteration, in other words, summing at 16-byte strides. Then I added a new waterfall level to handle 8 byte offsets. Also in the handling of the remaining bytes it's not necessary to loop. With those changes the generic implementation is better than the SSE implementation for all cases:\n$ sudo ./snabb snsh -t lib.newchecksum\nselftest: newchecksum\n14.4M; 44 bytes\nGen:    0.125743\nSSE2:   0.126113\nAVX2:   0.143601\nNew:    0.071434\n2M; 550 bytes\nGen:    0.192014\nSSE2:   0.096879\nAVX2:   0.049316\nNew:    0.067029\n1M; 1500 bytes\nGen:    0.276876\nSSE2:   0.099367\nAVX2:   0.057041\nNew:    0.082595. @tobyriddell Thanks for the pointer, it was an interesting reading. According to the article it seems that per-core frequency decreases if using AVX/AVX2 multiplication instructions. OTOH, computing the checksum only involves additions and shift instructions which it seems not to degrade per-core frequency. From the article:\n\nAnother interesting distinction is that ChaCha20-Poly1305 with AVX2 is slightly slower in OpenSSL but is the same in BoringSSL. Why might that be? The reason here is that the BoringSSL code does not use AVX2 multiplication instructions for Poly1305, and only uses simple xor, shift and add operations for ChaCha20, which allows it to run at the base frequency.. Added a new level of loop unrolling with 4 qwords.\n\nI learned there's some work already done by @lukego on a similar PR https://github.com/snabbco/snabb/pull/899 Luke already rewrote an AVX2 version of the algorithm in DynASM. Perhaps both issues could get combined somehow. IMHO is not worth to write a SSE version of the algorithm as the generic version with a loop unrolling of 4 qwords is already better than the SSE version in all cases. However, probably is worth to have an AVX version of the algorithm which makes use of YMM registers.\nBenchmarks for 4 qwords unrolling:\nbash\n$ sudo ./snabb snsh -t lib.newchecksum\nselftest: newchecksum\n14.4M; 44 bytes\nGen:    0.122729\nSSE2:   0.125478\nAVX2:   0.140538\nNew:    0.077302\n2M; 550 bytes\nGen:    0.20596\nSSE2:   0.098654\nAVX2:   0.049787\nNew:    0.047055\n1M; 1500 bytes\nGen:    0.273965\nSSE2:   0.100557\nAVX2:   0.058187\nNew:    0.068768. Updated results with nanoseconds per byte and nanoseconds per checksum.\n$ sudo ./snabb snsh -t lib.newchecksum\nselftest: newchecksum\n14.4M; 44 bytes\nGen:    elapse: 0.126386; ns_per_csum: 87.77; ns_per_byte: 1.99\nSSE2:   elapse: 0.123921; ns_per_csum: 86.06; ns_per_byte: 1.96\nAVX2:   elapse: 0.119802; ns_per_csum: 83.20; ns_per_byte: 1.89\nNew:    elapse: 0.075025; ns_per_csum: 52.10; ns_per_byte: 1.18\n2M; 550 bytes\nGen:    elapse: 0.211609; ns_per_csum: 1058.04; ns_per_byte: 1.92\nSSE2:   elapse: 0.102081; ns_per_csum: 510.40; ns_per_byte: 0.93\nAVX2:   elapse: 0.063685; ns_per_csum: 318.42; ns_per_byte: 0.58\nNew:    elapse: 0.054159; ns_per_csum: 270.79; ns_per_byte: 0.49\n1M; 1500 bytes\nGen:    elapse: 0.291010; ns_per_csum: 2910.10; ns_per_byte: 1.94\nSSE2:   elapse: 0.099104; ns_per_csum: 991.04; ns_per_byte: 0.66\nAVX2:   elapse: 0.066498; ns_per_csum: 664.98; ns_per_byte: 0.44\nNew:    elapse: 0.074388; ns_per_csum: 743.88; ns_per_byte: 0.50. I think the results I posted were from E5-2620v3, but in any case I run the benchmark again:\nE5-2620v3 (Haswell)\nbash\n$ sudo ./snabb snsh -t lib.newchecksum\nselftest: newchecksum\n14.4M; 44 bytes\nGen:    elapse: 0.126402; ns_per_csum: 87.78; ns_per_byte: 1.99\nSSE2:   elapse: 0.135262; ns_per_csum: 93.93; ns_per_byte: 2.13\nAVX2:   elapse: 0.130680; ns_per_csum: 90.75; ns_per_byte: 2.06\nNew:    elapse: 0.078119; ns_per_csum: 54.25; ns_per_byte: 1.23\n2M; 550 bytes\nGen:    elapse: 0.212166; ns_per_csum: 1060.83; ns_per_byte: 1.93\nSSE2:   elapse: 0.095822; ns_per_csum: 479.11; ns_per_byte: 0.87\nAVX2:   elapse: 0.054535; ns_per_csum: 272.68; ns_per_byte: 0.50\nNew:    elapse: 0.079838; ns_per_csum: 399.19; ns_per_byte: 0.73\n1M; 1500 bytes\nGen:    elapse: 0.293406; ns_per_csum: 2934.06; ns_per_byte: 1.96\nSSE2:   elapse: 0.105560; ns_per_csum: 1055.60; ns_per_byte: 0.70\nAVX2:   elapse: 0.059494; ns_per_csum: 594.94; ns_per_byte: 0.40\nNew:    elapse: 0.115417; ns_per_csum: 1154.17; ns_per_byte: 0.77\nLaptop (i7-6700HQ CPU @ 2.60GHz; Skylake)\nbash\n$ sudo ./snabb snsh -t lib.newchecksum\nselftest: newchecksum\n14.4M; 44 bytes\nGen:    elapse: 0.122758; ns_per_csum: 85.25; ns_per_byte: 1.94\nSSE2:   elapse: 0.121412; ns_per_csum: 84.31; ns_per_byte: 1.92\nAVX2:   elapse: 0.122230; ns_per_csum: 84.88; ns_per_byte: 1.93\nNew:    elapse: 0.074836; ns_per_csum: 51.97; ns_per_byte: 1.18\n2M; 550 bytes\nGen:    elapse: 0.215340; ns_per_csum: 1076.70; ns_per_byte: 1.96\nSSE2:   elapse: 0.089518; ns_per_csum: 447.59; ns_per_byte: 0.81\nAVX2:   elapse: 0.062259; ns_per_csum: 311.30; ns_per_byte: 0.57\nNew:    elapse: 0.053170; ns_per_csum: 265.85; ns_per_byte: 0.48\n1M; 1500 bytes\nGen:    elapse: 0.301346; ns_per_csum: 3013.46; ns_per_byte: 2.01\nSSE2:   elapse: 0.095864; ns_per_csum: 958.64; ns_per_byte: 0.64\nAVX2:   elapse: 0.065900; ns_per_csum: 659.00; ns_per_byte: 0.44\nNew:    elapse: 0.076165; ns_per_csum: 761.65; ns_per_byte: 0.51\nAs for the requests, I can tackle those changes, sure.. I added a few more commits that address some of the requested changes:\n\nRemoved AVX2 and SSE2 implementations.\nRandomized tests. I adapted the selftest in lib.checksum, which validates the computed checksum using the C algorithm (cksum_generic) is the same as the one computed by ipsum (DynASM implementation). The selftest in arch.checksum also validates the checksum computation is correct comparing the result to an algorithm written in Lua (I think we should remove the cksum_generic algorithm in the future as it's not used anymore, only for validation purposes).\nSnabbmark is pending. I plan to move the benchmark in arch.checksum there.. * Added checksum subprogram to snabbmark.. @eugeneia I think this patch fixes the issue: https://github.com/Igalia/snabb/commit/ea5228c6d9de864480381ea91c9a26fc01e6df1f I can make a PR to snabb upstream.  @eugeneia PR to next https://github.com/snabbco/snabb/pull/1282. @eugeneia OK, sorry it didn't fix the issue completely. But I'm sure that patch it's necessary. I pulled out max-next and I'm running it in our machines. lwAFTR's selftest don't pass yet. I give you a shout when I fix it.. I found out the issue. lwAFTR's loadtest was failing due to a non existing parameter \"pci-address\". To fix it i'ts necessary to apply this patch https://github.com/Igalia/snabb/pull/1005/commits/3e7d2b5e1864959186c393ea65f2e9f3a364a7d3. PR: https://github.com/snabbco/snabb/pull/1283. Tap support on the lwAFTR has recently landed. What this means is that the lwAFTR can run now on any Linux kernel network interface, but it won't be performant which makes it unusable in any practical sense. You can try it out by pulling https://github.com/Igalia/snabb/ or wait until Snabb's July release when likely these changes will get merged.. I run the test individually and it finishes with exit code 0. If I run the same test in raptorjit branch or master, I got the same output, and also it finished with exit code 0.\n\nI think CI failed in nfv test, which tends to be flaky, since there are not more tests after that one and it should.. I think I made a mistake when I benchmarked the branch earlier, because it's not possible that I'd run find-limit as it hasn't been merged in the raptorjit branch yet.\nSo I run the standard benchmark (packet-size: 550 byte; softwires: 1M) on this branch using loadtest. Max load is:\nApplying 9.200000 Gbps of load.\n  NIC 0:\n    TX 2003467 packets (2.003467 MPPS), 1101906850 bytes (9.199920 Gbps)\n    RX 2003467 packets (2.003467 MPPS), 1182045530 bytes (9.841030 Gbps)\n    Loss: 0 ingress drop + 0 packets lost (0.000000%)\n  NIC 1:\n    TX 2003467 packets (2.003467 MPPS), 1101906850 bytes (9.199920 Gbps)\n    RX 2003467 packets (2.003467 MPPS), 1021768170 bytes (8.558811 Gbps)\n    Loss: 0 ingress drop + 0 packets lost (0.000000%)\nThen I run the same benchmark but using 128-byte packets:\nApplying 2.800000 Gbps of load.\n  NIC 0:\n    TX 2108431 packets (2.108431 MPPS), 299397202 bytes (2.799996 Gbps)\n    RX 2108431 packets (2.108431 MPPS), 383734442 bytes (3.474694 Gbps)\n    Loss: 0 ingress drop + 0 packets lost (0.000000%)\n  NIC 1:\n    TX 1699026 packets (1.699026 MPPS), 309222732 bytes (2.799995 Gbps)\n    RX 1699026 packets (1.699026 MPPS), 241261692 bytes (2.256307 Gbps)\n    Loss: 0 ingress drop + 0 packets lost (0.000000%)\nAnd the same test on raptorjit branch (vmprofiling disabled for workers):\nApplying 2.800000 Gbps of load.\n  NIC 0:\n    TX 2108418 packets (2.108418 MPPS), 299395356 bytes (2.799979 Gbps)\n    RX 2108418 packets (2.108418 MPPS), 383732076 bytes (3.474673 Gbps)\n    Loss: 0 ingress drop + 0 packets lost (0.000000%)\n  NIC 1:\n    TX 1699016 packets (1.699016 MPPS), 309220912 bytes (2.799978 Gbps)\n    RX 1699016 packets (1.699016 MPPS), 241260272 bytes (2.256293 Gbps)\n    Loss: 0 ingress drop + 0 packets lost (0.000000%)\nPacket drops start at the same load in both this branch and raptorjit branch. At first sight, it seems having vmprofiling always on in workers doesn't have any impact in performance.. AFAIK, at the moment it's not possible to disable vmprofiling in Raptorjit, as it's always on. By that I mean there's not flag or environment variable to disable profiling. So in case of going for 3) it would be necessary as well to implement this mechanism.\nAs for 2) initially it sounds good to me, I have one question though. As the issue is with system calls fork, clone, etc won't ljsyscall functions will translate to these system calls too?. @eugeneia sgtm. The lwAFTR uses vMDQ by default. The reason is that according to the Intel 82599 datasheet, VLAN tag addition and removing by hardware is available only if vMDQ is enabled.\nIf vMDQ is not supported by i210 that is the likely cause for the fail (RTTDQSEL register not available in i210 but used when vMDQ is enabled).\nPerhaps you can work around the issue by disabling vMDQ in https://github.com/snabbco/snabb/blob/master/src/program/lwaftr/setup.lua#L217 and https://github.com/snabbco/snabb/blob/master/src/program/lwaftr/setup.lua#L227. If that fixes the issue, we can take a deeper look and come up with a better patch.\ncc @takikawa . Right, the solution is to either rename argument file to filename or either rename the instances of filename to file. I think the latter is preferred to match the naming convention of write_file_header.\nhttps://github.com/snabbco/snabb/blob/master/src/lib/pcap/pcap.lua#L55\nIf you send a patch with those changes it will get merged very likely.. To run all selftests, simply type:\nbash\n$ sudo make test\nTests that depend on hardware (NICs) will be skipped.. I'm arriving on Friday evening. I'm staying at Alma Hotel (near Grand Place), together with a few more igalians (although I think Andy is staying somewhere else). See you on Saturday at the SDN room!. Good investigation. Definitely this is a bug, enum_validator should validate on key, not value. If you need to continue with your work, simply disable the enum_validator. I'd try to look into this issue if I have some spare time.. Right, eventually the code could be rewritten in Lua + ljsyscall. In the meantime, I prefer to keep the differences respect the original xdpsock example small. Once there's support for batch fetching and using multiple sockets, I think the next step is to rewrite the code in Lua.\nI also think it should be possible to add the Linux source code dependencies as part of Snabb code base, instead of requiring to download the kernel sources.. Implemented BATCH_SIZE support.\nApparently the results are slightly worse, however I think the previous code was not working totally ok and this performance regression comes at the cost of fixing bugs in the previous code (for instance a packet.clone was needed in pull). Here are the results for the same benchmark:\n```\n$ sudo SNABB_PCI0=ens6f0 SNABB_PCI1=ens2f0 ./snabb snabbmark xdpsocket 10e6 550\nNo PMU available: single core cpu affinity required\nPackets: 10000000; Packet Size: 550; Timeout: 1000\nSending through ens6f0 (90:e2:ba:ac:b9:b4); Receiving through ens2f0 (90:e2:ba:ac:b9:48)\nProcessed 10.0 million packets in 18.37 seconds (rate: 0.5 Mpps, 2.23 Gbit/s, 0.65 % packet loss).\n```\nIf I try BATCH_SIZE=1, like before, results are much worse than BATCH_SIZE=16.\n```\n$ sudo SNABB_PCI0=ens6f0 SNABB_PCI1=ens2f0 ./snabb snabbmark xdpsocket 10e6 550\nNo PMU available: single core cpu affinity required\nPackets: 10000000; Packet Size: 550; Timeout: 1000\nSending through ens6f0 (90:e2:ba:ac:b9:b4); Receiving through ens2f0 (90:e2:ba:ac:b9:48)\nProcessed 10.0 million packets in 25.31 seconds (rate: 0.4 Mpps, 1.62 Gbit/s, 0.00 % packet loss).\n```\nIt seems BATCH_SIZE=16 is a hard limit. If try BATCH_SIZE=32, there are too many packet drops:\n```bash\n$ sudo SNABB_PCI0=ens6f0 SNABB_PCI1=ens2f0 ./snabb snabbmark xdpsocket 10e6 550\nNo PMU available: single core cpu affinity required\nPackets: 10000000; Packet Size: 550; Timeout: 1000\nSending through ens6f0 (90:e2:ba:ac:b9:b4); Receiving through ens2f0 (90:e2:ba:ac:b9:48)\nProcessed 10.0 million packets in 13.48 seconds (rate: 0.7 Mpps, 3.04 Gbit/s, 37.21 % packet loss).\n```. As far as I know, cannot. When trying to compile a statement like that LuaJIT returns error \"unexpected symbol near 'local'\".\nBefore applying the patch, I'd prefer to run the \"NFV loadgen benchmark\" in chur and check there are no regressions. What's the current the status of this?\n. To avoid this problem, constant numbers should be normalized using bit.tobit. Example:\n``` lua\n\nif bit.bswap(bit.bswap(0x80000000)) == 0x80000000 then print(\"true\") else print(\"false\") end\nfalse\nif bit.bswap(bit.bswap(0x80000000)) == bit.tobit(0x80000000) then print(\"true\") else print(\"false\") end\ntrue\n```\n\nAccording to LuaJIT docs \"this function is usually not needed since all bit operations already normalize all of their input arguments\" [1], but in the case of constants is necessary to call it explictly.\n[1] http://bitop.luajit.org/api.html\n. Since you're already doing this change, I think it would be also convenient to add an explicit inclusion of the BitOp module. It's not necessary, but the docs recommend it (and other modules do it too):\nThe suggested way to use the BitOp module is to add the following to the start of every Lua file that needs one of its functions [...]This makes the dependency explicit, limits the scope to the current file and provides faster access to the bit.* functions, too.\nhttp://bitop.luajit.org/api.html\n. Why not caching htons and ntohs here like in the other files?\nlua\nlocal ntohs, htons = lib.ntohs, lib.htons\n.  @alexandergall If you think is more convenient to have that function, I think is OK. My note was more like an information note :) I'd also prefer the BitOp module returns unsigned integers, LuaJIT docs say the reason to make it signed is cross-platform compatibility.\n\"Defining the result type as an unsigned number would not be cross-platform safe. All bit operations are thus defined to return results in the range of signed 32 bit numbers (converted to the Lua number type).\"\nhttp://bitop.luajit.org/semantics.html\n. I don't know. So far for accessing the data and length attributes of a packet I simply accessed those attributes directly, but I just saw there's a packet.data and packet.length functions in src/core/packet.lua. The point is that these functions are getters but there's no equivalent setter methods and in the code I need to update the length too. To me it seems more consistent to access these attributes always the same way. \nWhat's the preferred way of accessing these attributes? I can change it but I don't understand what are the benefits of using these functions.\n. I modified LoadGen so it reports on received packets as well as transmitted packets.  As traffic gets bounced to its originator and the NIC is locked by the program running the packetblaster, this was the only possible way I thought of of checking traffic gets actually bounced back.\nI missed updating LoadGen docs, if the change finally makes it I will update the docs.\n. It seems there's no support of VLAN tags. I would like to have it as it's something we use often.\nSince you're defining ethernet_header, one way of implementing support of VLAN tags is to change the definition of the ethernet header if VLAN tags are used. In that case, ether_header_ptr_type will point to the definition of a 802.1Q header. See https://github.com/Igalia/snabbswitch/blob/lwaftr_starfruit/src/apps/lwaftr/generator.lua#L103\nSame goes for the hardcoded offsets. In case of supporting VLAN tags, offsets should change (14 turns into 16, 34 (20 + 14) turns into 36, etc). As for me I prefer to use constants instead of hardcoded values since it makes easier to understand the meaning of a value (ipv4_header_size + ethernet_header_size instead of 34).\n. There's a typo, I think it should be ipv4_payload instead of ip4_payload (that makes later use of ipv4_payload global).\n. First definition of payload, should be defined as local.\n. Nit: udp is not used.\n. Should return 0 as exit code as the user explicitly requests to print out help.\n. In case of using invalid characters such as numbers or a dash, which cannot be used as property names directly, should use the alternative hash syntax using brackets. Example:\n``` lua\n\nopts = {} opts[\"4\"] = \"hi\"\n= opts.4\nstdin:1: '' expected near '.4'\n= opts[\"4\"]\nhi\n```\n\nThat said I don't know if lib.dogetopt would be able to parse '4' as a short name option.\n. Nit: I think there's an extra whitespace.\n. Nit: intel_app and C are not used.\n. long_opts defines more options (src, dst, sizes) than short options \"hD:\" (help, duration). It seems this was already present in master, but since you're moving the code I think is nice to fix it :)\n. Nit: intel_app, basic_apps and C are not used.\n. Nit: wrong alignment.\n. I wonder if this function could be moved to a helper file so it can be reused by 'synth' and 'reply'.\n. Wrong error message (copy-paste error I believe).\n. @mwiget My concern is that the function is now duplicated in synth.lua and replay.lua. Since packetblaster/packetblaster.lua has to exist anyway, it sounds good to me to put it there (instead of creating a new file with a single function) and require it from synth.lua and replay.lua, WDYT?\n. Good catch! In Lua there's wildcard '%x' which matches hexadecimal digits. See http://www.lua.org/pil/20.2.html.\n. Typo: s/packes/packets/\n. I know this code is moved from the lwAFTR but there still some bits that can be improved.\nC is cached. In addition, htonl, ntohl, htons and ntohs functions are defined at core/lib.lua, so actually it's not necessary to access them via the C namespace. The core/lib.lua module is always included so it's not needed to require it.\n. castis cached.\nThere's the functions rd16, rd32, w16, w32 in the lwAFTR codebase (apps/lwaftr/lwutil.lua) which do the same (casting of a pointer and derefence) but makes code easier to read. I understand that importing a lwAFTR module maybe understood as if this module was dependent on the lwAFTR. I think ideally these helper functions should be in core/lib.lua but that's something to do in another PR IMHO. In conclusion, I would leave as it is for now but I would love to have it changed in the future.\n. What about self.dot1q_tpid = htons(dot1q_tpid)? The code explains itself (host-to-network-byte-order) and can the comment can be skipped. In addition, I'd keep the same name for the variable (since it was the name that it was given before). Lastly, conf is not used (new methods may have an argument, it's not mandatory).\n. Sorry, my fault. core.lib is not included by default, it's necessary to import it. This will fixed:\nlua\nlocal lib = require(\"core.lib\")\nlocal htonl, htons, ntohs = lib.htonl, lib.htons, lib.ntohs\n. AFAIK there's no pattern. Some apps call its new argument arg or conf. What is true is that new methods always receive one argument. If the constructor needs more than one argument, a table is passed. Many apps use this pattern:\nlua\nfunction App:new(arg)\n   local conf = config.parse_app_arg(arg)\nend\nNow, checking what parse_app_arg does it converts a string with a pattern like \"{ property = value,...}\" into a Lua table. I think this is legacy code and it's not longer necessary (I'm not aware of any object that passes a string like a Lua table).\n. Right, only the arrays with the links are initialized (self.input and self.output) but the arrays are empty. Here is a good chance to do anything related with the app name which is the only thing that is available in start and it's not in new(). That's the only thing I can think for now. For instance, it would be the right place where to create counters by app name  (and remove them in stop).\n. Just checked jit module is included by default:\nlua\n$ luajit -e \"print(jit.version)\"\nLuaJIT 2.1.0-beta2\nbut probably it's a good idea to explicitly include it, for the same it's a good idea to do it for the bit module.\n. @eugeneia Yep, I made with_restart global was it was defined after the ingress drop monitor code. OK, I can move that code after the definition of with_restart.\n. What I noticed is that Loadgen replays packets continuously, that's why I needed to make this difference. For instance, consider the following script that prints out received packets on a NIC:\n``` lua\nmodule(..., package.seeall)\nlocal Intel82599 = require(\"apps.intel.intel_app\").Intel82599\nlocal basic_apps = require(\"apps.basic.basic_apps\")\nfunction run (args)\n   local pciaddr = assert(args[1], \"No NIC\")\nlocal c = config.new()\n   config.app(c, \"nic\", Intel82599, {\n      pciaddr = pciaddr,\n   }) \n   config.app(c, \"sink\", basic_apps.Sink)\n   config.link(c, \"nic.tx -> sink.in1\")\nengine.configure(c)\n   engine.main({duration=5, report = { showlinks = true }})\nend\n```\nIf I always use LoadGen:\n```\n$ sudo ./snabb packetblaster replay --no-loop v4v6-256.pcap 81:00.1\nfilename=v4v6-256.pcap\nlink report:\n    256 sent on pcap.output -> source.input (loss rate: 0%)\n    256 sent on source.1 -> nic1.input (loss rate: 0%)\n$ sudo ./snabb receiver 81:00.0                                                                                                                     \nlink report:\n    1,385,473 sent on nic.tx -> sink.in1 (loss rate: 0%)\n```\nIf I use Intel82599 when no-loop:\n```\n$ sudo ./snabb packetblaster replay --no-loop v4v6-256.pcap 81:00.1\nfilename=v4v6-256.pcap\nlink report:\n    256 sent on pcap.output -> source.input (loss rate: 0%)\n    256 sent on source.1 -> nic1.rx (loss rate: 0%)\n$ sudo ./snabb receiver 81:00.0                                                                                                                     \nlink report:\n    256 sent on nic.tx -> sink.in1 (loss rate: 0%)\n```\nOTOH, I realized there's a bug and the link should be \"nic.rx\" for the Intel82599 case.\n. No, there's no difference. I will remove the switch:\n```\n$ sudo ./snabb packetblaster synth -D 1 81:00.1                                                                                                     \nTransmissions (last 1 sec):\napps report:\nnic1\n81:00.1 TXDGPC (TX packets)     13,148,741      GOTCL (TX octets)       894,114,388\n81:00.1 RXDGPC (RX packets)     0       GORCL (RX octets)       0\n$ sudo ./snabb receiver 81:00.0\nlink report:\n    10,255,373 sent on nic.tx -> sink.in1 (loss rate: 0%)\n```\nWith --no-loop:\n```\n$ sudo ./snabb packetblaster synth --no-loop 81:00.1                                                                                                \nlink report:\n    1,278 sent on source.1 -> nic1.input (loss rate: 0%)\n$ sudo ./snabb receiver 81:00.0\nlink report:\n    10,014,215 sent on nic.tx -> sink.in1 (loss rate: 0%)\n``\n. Initially I thought that would be enough but not (as I explained above).\n. I'm not sure. The reason I think this should be per module and not per instance is thatnum_descriptorsis used to set RDLEN and TDLEN registers in the intel10g driver. In SF that should be OK, but what would happen in VF mode? VF inheritsset_receive_descriptorsandset_transmit_descriptorsfrom SF. If two VF instances are started with differentnum_descriptors` sizes the latter would override RDLEN and TDLEN registers in the card. Wouldn't that be a problem?\n. The modifier flag was introduced to avoid Snabb returning an error when the pcap file already exists. I initially set it to \"w+\" always but others preferred to introduce a flag. The way the flag was added doesn't match how other apps work (all of them expect one arg, which is either a string or a table), I didn't know at that time.\nIs there any way to keep this behaviour?\n. @eugenia That would work for me :+1: \n. Should be ffi.cast(\"uint32_t\", bswap(b))\n. Apparently random_bytes only exists if randomseed is called. It seems this is something the user should know beforehand before using random_bytes. Wouldn't be better that randombytes always exists and takes a default (possibly random_bytes_from_dev_urandom)?. I don't understand this code. res represents a 128-bit number, but seed is expected to be a uint32_t number. Why res is not an uint32_t number?. ipv4_eq always compares 4 bytes, so the third parameter in the function call can be removed.. Likewise.. mac_eq is never used.. There's a tab here. There's another tab in line 242. Those are the only tabs in the whole PR. Ideally they should be replaced with spaces.. These options should be removed too from the list of accepted options. Line 115:\nlua\nargs = lib.dogetopt(args, opt, \"hsD:i:o:p:m:a:c:M:\", long_opts). And also removing in the list of long options:\nlua\nstats = \"s\",. OK with me if it will always work.. Perhaps is worth moving this function to core/lib since is currently defined at many places (lwaftr/lwutil.lua, at several modules in program/alarms and ptree.lua).. Also can use the idiom opts = opts or {}, as you like more.. Why not main.exit(0)?. What's the goal of wrapping the self:tick() call instead of calling self:tick() directly?. Nit: could you move the double bracket to the line above? Since it's just two characters it looks like if there was a blank line between the arguments.. That's a good point. Probably is a good idea this patch gets into Snabb via the lwAFTR fork so it's adapted for the latest changes in the library. Although the patch is not strictly lwAFTR related, most of the changes in the YANG library come from Igalia's repository. Let's wait for @wingo opinion before merging it.. Maximum size of a packet is defined as a constant in core/packet.lua:max_payload.. This changed isn't justified. I benchmarked % and \"mod with logical and\" and they're the same speed. What's indeed much slower is math.fmod. I think this change should be reverted.. Is this comparison correct? My understanding is that the packet's next_header value is in network-byte-order, but it's compared to a constant in host byte order. . Sorry I didn't realize this wasn't a new change. Rather than cosmetic, the difference is that the size is being computed every time this function is called, when it can be retrieved from a constant. I leave it up to you if you'd like to change it.. The results I got from benchmarking % and & showed no difference. Here's a gist of the code I tried https://gist.github.com/dpino/925a79494d3da27e3681e8b3acf67b32. I don't know what LuaJIT compiles % to, though.. You can hoist dt outside of the function and initialize as a set:\nlua\ndt = lib.set({'int8', 'int16', 'int32', 'uint8', 'uint16', 'uint32'})\nThen return pt == 'string' or pt is in set:\nlua\nreturn pt == 'string'  or dt[pt]. Nit: I think the or statement should be indented, or written as:\nif (A) or\n   (B) then\n   ...\nend. I found this code a little bit hard to read. AFAIU, native_key stores the value of k as the loop is iterates. When native_key is not nil the function returns nil. Does it mean if the function iterates 2 times and native_key has a value, the function returns nil? Example\nlua\n   local t = {\n      int16 = {\n         type = 'scalar',\n         argument_type = {\n            primitive_type = 'int16'\n         }\n      }\n   }\n   local ret = table_native_key(t)\n   print(ret)\nReturns 'int16'.\nlua\n   local t = {\n      int8 = {\n         type = 'scalar',\n         argument_type = {\n            primitive_type = 'int8'\n         }\n      },\n      int16 = {\n         type = 'scalar',\n         argument_type = {\n            primitive_type = 'int16'\n         }\n      }\n   }\n   local ret = table_native_key(t)\n   print(ret)\nReturns 'nil'.. ",
    "ghost": "Please update your qemu to this one:\nhttps://github.com/SnabbCo/qemu/commits/nfv/icehouse\n. +10\nWe're the masters of Jumbo Frames now :)\n. If this is causing the bad iperf results we should definitely remove it out of the data path.\nOne question though - can we keep the code commented or \"debug\" only?\n. The \"recommended\" QEMU version to be used is this one. It is basically current upstream with a single patch that helps reconnecting vhost-user.\nhttps://github.com/virtualopensystems/qemu/tree/straightline\n. straightline loadgen is broken.You must use the master one and run it by hand in another console (tmux, screen etc.). I use it like this:\nsudo /home/luke/hacking/pristine/snabbswitch/src/snabb /home/luke/hacking/pristine/snabbswitch/src/digns/loadgen/loadgen /opt/bench/256.pcap 0000:82:00.1\nIf you want to use the integrated bench script src/scripts/bench_env/loadgen-snabb-nic-guest.sh\nYou'll need to change your ~/bench_env.conf:\nRUN_LOADGEN=\n. dpino, can you please post full configs and step by step what do you do to run the test? Lets figure it out together.\n. You'll need to change your ~/bench_env.conf:\nRUN_LOADGEN=\ni.e. unset the bench env to run loadgen from its own tree (which is still broken).\nThe lukes pristine loadgen you should run on another console and leave it there running while you're testing, no need to kill it every time.\n. Yes you can - loadgen is made to be untoched (remeber wht we did in DT - external loadgen)\nJust unset RUN_LOADGEN in your bench_env.conf.\nAs for the guest - also true, that is why we have the reconnect feature.\n. Yes - exactly.\nActually, a working loadgen on straightline is near completion, may be a quick review and will be integrated soon.\n. As Luke proposed:\nNikolay: 82 84 88\nDiego: 01 03\nLuke: 05\nI don't mind share some of the boards though.\nNote that yesterday I was running test on 82 too, so maybe conflicts were the cause of the behaviour.\nI can leave 82 for you if it suits you best - no problem for me.\n. OK so the straightline branch now has all Luke's updates + a small patch from me.\nThis effectively makes loadgen working in the tree. \n- Please update and remove  your RUN_LOADGEN= line from the bench_env.conf.\n- Use 82 - I dropped it from my PCI list\n- can you please verify also this issue https://github.com/SnabbCo/snabbswitch/issues/347\n  for me it means that changing the file src/test_fixtures/nfvconfig/test_functions/snabbnfv-bench.port from port_id = \"A\", to \"port_id = \"a\", gives me better numbers in the tests.\n. Can you use 82.\n. @lukego any ideas?\n. The current value of kernel.shmmax can be read from here /proc/sys/kernel/shmmax and then compared to the huge page size.\n. For me /var/run/snabb does not exsitby default so it has to be added explicitly, you can verify by deleting the folder and try run snabb.\n. Ok, so this was probably a false alarm related to write permissions.\nSorry for that - closing it.\n. For me it is OK. Have tested it here and was working.\n. @lukego, first we need to understand what it means for the kernel to send those  vhost_user: request set_vring_call 13. It probably tries to suppress interrupts although there is also other mechanism to do that (see VRING_F_NO_INTERRUPT in VirtioVirtq:signal_used()).\nThen - we should be able to detect that a re-initialization is happening and to shutdown our virtqs properly.\nI also very interesting that -smp 1 does not trigger it, might have to do something with using barriers etc.\nAll in all - it is about debugging and getting more info, maybe more on the QEMU/Linux kernel side.\n. Tracing down the issue, we found out that under high load the virtio-net driver in the guest is generating a lot of interrupt mask/unmask events to the virtio-net-pci device in QEMU. This results in QEMU resetting the virtio-net-pci device after ~1 minute of high load (tested with VM2VM example).\nThe generation of these events also slows down the throughput.\nThis behavior is not seen provided that the following .config options are set:\nCONFIG_X86_X2APIC=y\nCONFIG_KVM_GUEST=y\nCONFIG_HYPERVISOR_GUEST=y\nCONFIG_PARAVIRT=y\nIt seems that the critical part is to enable X2APIC support in the guest, which forwards the interrupts to the KVM instead of using the QEMU emulated interrupt controller.\n. I'm not sure that the docker creation should be part of the core SnabbSwitch tree.\nDoes anybody else share my opinion that this could be extracted as a separate project?\n. Maybe ./configure --disable-gnutls can do the trick?\n. It could be the problem is in this commit: https://github.com/virtualopensystems/dpdk/commit/7807fbbcd2ae9c1da8c9e0d20a3d5a4f783e9d6f. It is from the times we used a patched QEMU with 8192 sized vrings, so we wanted to propagate this to the Intel vritio PMD (same way as the Linux kernel driver does). Like this https://github.com/SnabbCo/qemu/commit/7a94322b279c5dd8fc5f2cb429814e0411ca0b0e.\nMaybe the patch is obsolete nowadays, and will give better performance with the default values.\nIt might be also a good idea to get a more recent DPDK (2.1 is stable, and 2.2 is on the way).\n. @lukego ... I guess great minds think alike - its just that you've been faster :)\nAnyway, i think that at least this one might be needed: https://github.com/virtualopensystems/dpdk/commit/dae0a7f57e5656ad6c8422a5ea6a368cf306ae24\nAnd for compiling on older versions of the kernel (14.04 LTS): https://github.com/virtualopensystems/dpdk/commit/d34d593d9de054e910e4081512a8a1ab48f654bf\nHope this helps.\n. @lukego we can rename the branch to upstream-contributions will this fit better with the project.\nWe would like to keep the names and avatars like they are now.\n. @lukego \nOK so we rename the branch to vosys-nfv, and our account names to nnikolaev-virtualopensystems. The logo we can't change for the time being.\nGiving credit to people/companies should be enough to be able to see their names/logos on the cotributors page.\n. I did some bisecting and I found which commit causes this low perfromance in our test scenario.\nThis is the commit after which the performance drops down:\n7063865 Revert \"Merge branch 'restore-snabbtop' into next\"\nThis is really strange as this commit only remove some code related to statistics in the app.lua.\nThis is the last known good commit:\n8ecc0c0 ipv6:pton: Fix error on invalid address\nAny hints how to profile with snsh will be useful.\n. @lukego I understand about separate PRs, reviews etc. Then I am not sure why not just merge the PRs themselves instead of moving the patches to vosys-nfv and merge from there?\nCan you clarify what \"rebasing\" means here: \"if you would rebase the branch then it will stop synchronizing until you rebase it back again.\" From what I can tell all development is done against master (btw nice git-workflow.md ), so rebasing to me happens only when new master appears.\nIs there a way that I get subscribed to receive mail notifications once new PR appears? It will be easier to monitor an pick whatever is important to us.\n. I did wrote those and I can confirm that they were based on what was already available inipv6 and icmp (why reinvent the wheel anyway).\nI can also confirm the tests are OK to me.\n. If we are not \"re-connecting\" we do not need those assignments at all. AFAIK these values should always be initialized with 0. Currently used.idx is 0 at start. Of course ring.avai.idx can be some other value (the driver will push some empty buffers in the avail ring on its Rx virtq). I am not sure this will work properly no matter if you are reconnecting or not.\nMaybe if we can detect reconnection it could be something like this:\n``` lua\nfunction VirtioNetDevice:set_vring_addr(idx, ring)\nself.virtq[idx].virtq = ring\n  if  then\n   self.virtq[idx].avail = tonumber(ring.used.idx)\n   self.virtq[idx].used = tonumber(ring.used.idx)\n  else\n   self.virtq[idx].avail = 0\n   self.virtq[idx].used = 0\n  end\n   debug(string.format(\"rxavail = %d rxused = %d\", self.virtq[idx].avail, self.virtq[idx].used))\n   ring.used.flags = C.VRING_F_NO_NOTIFY\nend\n```\nWhat is your use case? Which kind of virtio driver your are testing with?\n. In similar situations git submodule sync helped me.\n. Overall a very good and long awaited patch(es). Still it might need a spin or two before it gets the right shape, me thinks.\n. To run a VM with hugepages and shared memory (which is a requirement for vhost-user to work) using the upstream Nova, it is enough that the used flavor has hw:mem_page_size=large set.\nThis will work provided that the libvirt on the compute node is at least version 1.2.8 and qemu is at least 2.1.0.\nThis means that the rellated patches (https://github.com/SnabbCo/nova/commit/b45de6475699ae9952913843317a5f694cb52282 and https://github.com/SnabbCo/libvirt/commit/940dbda356c7c47b3f443410e2d8b04c049b999e) are no longer needed.\nThis patch https://github.com/SnabbCo/nova/commit/7b16b300c2a8570126870de57ab697e3c9a08bd6 is upstream, although from another committer. Still there is a tricky part on the neutron side how to set the vhost-user socket path, because this default https://github.com/SnabbCo/nova/commit/7b16b300c2a8570126870de57ab697e3c9a08bd6#diff-a5d7d7912f51e1c8fb10c94d5ecdc2caR70 is no longer in the code.\n. Regarding packetblaster is there a possibility that it is driver agnostic?\nWhat if I wanted to use it with a tap device?\n. @andywingo that is interesting, do you think this could have some nice effect on the virtio-net driver performance? Or it is completely different ball?\n. Thank you, @dpino !\nI didn't quite get the \"cross sending packets with each other\" part. Can you give a link to the \"proper\" configuration of the guest?\nRegarding the host - should we conclude that SnabbSwitch (being a framework) is not able to handle 2xNICs and 2xVhostUser interfaces in a single process? Because with the described configuration (two snabb-nfv processes) you map a single CPU core to read packets from the NIC and send then to the VM, and vice versa. I mean - 2 cores on a NUMA node are spent just to provide the connectivity to the VM.\nAnyway - it is great news, now that we know that the virtio-net is going upstream and with a decent performance. Great times for SnabbSwitch!\n. Thanks @dpino, however, this still confuses me. It turns out that if I want to pass the traffic bidirectional, I'll have to put up with low performance? I mean in the real world config.link(c, \"eth1.tx -> sink.rx\") does not make much sense.\n. The system in question is AArch64 compiled with CONFIG_ARM64_64K_PAGES.\nBut this is also valid for PowerPC which can have 16K, 64K and even 256K page size.\nThis patch does not support those platforms, it just uses a more portable. POSIX compliant and non-hardcoded way to determine the page size value.\n. I don't think that replacing 4096 with sysconf(_SC_PAGESIZE) is architecture specific.\nIt's just a more generic way to determine the value.\nIt is also a better programming style than having arbitrary and unexplained numbers all over the code.\nI understand #712, but this is not porting, although the patch makes this particular part of the code a little bit more portable. If this is not suitable for master then so be it.\n. Hello, @hannibalhuang, we at Virtual Opens Systems as a member of ETSI NFV, closely following IFA activities, as well as DPACC, and having contributed to OpenStack integration with SnabbSwitch, we can play with you the role of bridging through these technologies. We are very interested to discuss in more details a suitable joint working plan for these matters.\n. As stated here https://github.com/SnabbCo/snabbswitch/blob/master/COPYING#L178 ,the Apache 2.0 license clears this for us:\n```\nAPPENDIX: How to apply the Apache License to your work.\nTo apply the Apache License to your work, attach the following\n   boilerplate notice, with the fields enclosed by brackets \"[]\"\n   replaced with your own identifying information. (Don't include\n   the brackets!)  The text should be enclosed in the appropriate\n   comment syntax for the file format. We also recommend that a\n   file or class name and description of purpose be included on the\n   same \"printed page\" as the copyright notice for easier\n   identification within third-party archives.\nCopyright [yyyy] [name of copyright owner]\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\nThis is how one aplies the Apache lincense. I don't think we have much space left here for arguments.\n. Question: If I am submitting a whole module contribution, can I choose to put my Copyright line on the top of the module files? \nAlso the central COPYRIGHT file should be in src folder, as the lib folder imports source from other projects which are not a subject of this COPYRIGHT.\n. @wingo - OK let's just put the rules in place and see what happens :)\n. The issue with the cache hierarchy has been bothering me for a long time too. I completely share all the expressed thoughts and appreciate the pointed paper. And it seems that the situation is even worse than what I have imagined.\nI would say that this is not only a virtio-net problem. The moment when SnabbSwitch \"paralelises\" itself it will become its problem too. On the other hand, since the virtio-net driver is now in the master, how are those caching issues affecting the KVM VMs? \nThere is a nice tool called hwloc-ls part of the hwloc package in Ubuntu which prints the topology - NUMA, CPU cores, L1/L2/L3 caches and PCI allocation. hwloc-ps, hwloc-distances could be useful too. Other usefull tool could be likwid, which provides a number of microbenchmarks too.\nOne note from the paper: An optional Cluster-on-Die (COD) mode can be enabled in the BIOS.\nSo the \"internal NUMA\" could be \"show\" to the world, but instead of 8+4 (for 12 core CPU) it will present itself as 6+6. So again, far from perfect. And then: The asymmetrical chip layout which is mapped to a balanced NUMA topology causes performance variations. I could say that we have seen this many, many times.\n. As explained before once you have a HW switch between the l2fwd and the packet blaster then you need MAC swapping. I understand that this is not your use case, but that is what a forwarder should do - otherwise it is just a bridge, or softwire or something else but not L2FWD IMHO.\n. A small comment on the git history here. I personally would prefer to see 3 groups of patches (in this order):\n1. prepare the landscape for the new feature (i.e. if you need some changes in the core components)\n2. implement the new feature\n3. adapt the rest of the world to the new feature (add new tests, configurations etc.)\nI believe this could be much easier to review now, and to follow the git history in the future.\n. I agree with @lukego here. Keeping it simple is a good approach here, so one think at a time. But still I prefer to have a cleaner git history :)\n. When I access element number 1 from the list, I prefer that the next couple of elements are also fetched in the cacheline (which is 64 bytes on Intel's Xeon).  So 64 byte alignment of performance sensitive structures might be crucuial in some cases. \n. But maybe we can gain some performance here?\n. All these are nice thoughts but it'd be better to have some numbers.\nYou describe a scheme of allocating a batch of packets from a shared free list and using it as a local.\nBut then you have no guarantees of the order of returning the packets from the local free list to the shared one. Your shared list will get fragmented in a couple of iterations. How do you handle this?\nA couple of side notes. There is this thing called IOMMU which allows to pass VA addresses to the hardware and then it will do the translation to physical (which the NIC needs for it's DMA engine). The mechanism to leverage this approach in Linux is called VFIO. In the past Snabb tried to use it, but because of some issues (Luke knows better) the code was dropped. The approach that you describe with reading the pagemap is what substitutes this pure hardware offloading method.\nSo the rationale to use hugepages is to lower the syscalls that are needed to calculate the physical address. Of course, physical contiguity of the allocated packet buffers (for jumbo frames) is another reason.\nThis is a good start in any case, but as said - let's see some numbers and then we can judge.\n. I'm OK with this.\n. @lukego yep - you are right, the limit is about the Lua objects, and here we are accessing everything through directly, so no GC involved. Removing my comment, sory for the confusion.\nBTW nice to hear that 16GB VMs were tested and work ;)\n. I guess if \"All checks have passed\" it should be fine :)\n. Just out of curiosity, this is perl right? Is it somehow NiX specific way of automation?\nCould this be better implemented with libvirt and its python interface?\n. Is there any measurement of the performance impact of the tag/untag functionality?\n. I'll recreate the pull request from a different branch\n. ",
    "clopez": "I was able to reproduce it simply by running:\nsudo src/snabb src/designs/bench/basic1 10e6\nWith this patch there is a noticeable regression on the number of packages per second reported.\nSeems that the regression is caused for the change on src/core/buffer.lua\nI'm a bit puzzled with this performance regression, and still trying to make sense of it.\n. I'm closing this PR.\nThe proposed change makes the performance worse.\n. After running the iperf test for 10 seconds paralellized to 8 process I get this stats:\n```\n$ ifconfig eth0\n....\nRX packets:197546 errors:0 dropped:0 overruns:0 frame:0\nTX packets:1258441 errors:0 dropped:0 overruns:0 carrier:0\n$ netstat -s\n....\nTcpExt:\n    0 packet headers predicted\n    61163 acknowledgments not containing data payload received\n    136356 predicted acknowledgments\n    133 times recovered from packet loss by selective acknowledgements\n    170 fast retransmits\n    TCPLossProbes: 126\n    TCPSackShiftFallback: 8297\n```\n. I did some tests (with the same_vlan configuration) and I got very interesting results related to the MTU.\nI played with the tcp iperf test (no parallelized) and different MTU sizes. I found that iperf (no parallelized - 1 process) is capable of reaching ~10Gbit/sec when the MTU is [8000-8400] or > 9200.\nThis ( http://sprunge.us/QGiX ) is a report that I generated by setting on one VM a high MTU (10000) and on the other VM changing the MTU from 100 to 10000 in steps of 100 and running iperf (without paralellizing) for 10 seconds.\nThe bandwith seems to peak around an MTU of [8000-8400].  After that it lowers dramatically between an MTU of [8700-9200]. Passing 9200 it peaks again.\n. I'm suspecting that some buffer at lib/virtio/virtq.lua or lib/virtio/net_device.lua is not being filled efficiently.... @n-nikolaev any idea of why with an MTU of 8200 the iperf tcp performance (1 client) is near 10Gbps, but with an MTU of 9000 it degrades to ~6Gbps?\n. A proposed patch meanwhile this issue is not fixed:\n``` diff\n--- a/src/apps/intel/intel10g.lua\n+++ b/src/apps/intel/intel10g.lua\n@@ -144,7 +144,7 @@ function M_sf:set_rx_buffersize(rx_buffersize)\n end\nfunction M_sf:set_receive_descriptors ()\n-   self:set_rx_buffersize(16384)        -- start at max\n+   self:set_rx_buffersize(1024) -- Dont set this higher than 1KB without checking issue #322\nself.r.RDBAL(self.rxdesc_phy % 2^32)\nself.r.RDBAH(self.rxdesc_phy / 2^32)\n\n@@ -265,9 +265,6 @@ end\nfunction M_sf:add_receive_buffer (b)\n    assert(self:can_add_receive_buffer())\n-   if b.size < self.rx_buffersize then\n-      self:set_rx_buffersize(b.size)\n-   end\n    local desc = self.rxdesc[self.rdt].data\n    desc.address, desc.dd = b.physical, 0\n    self.rxbuffers[self.rdt] = b\n```\n. BTW, I'm using this to analyze the performance with different MTUs\nroot@vma:~# ifconfig eth0 10.69.23.1/24\nroot@vma:~# ifconfig eth0 mtu 12000\nroot@vma:~# iperf -s --bind 10.69.23.1 -p 5010\nroot@vmb:~# ifconfig eth0 10.69.23.2/24\nroot@vmb:~# ./checkmtu | tee mtu.output\nAnd checkmtu is:\n``` bash\n!/bin/bash\nipvma=\"10.69.23.1\"\nmtu=100\nwhile [[ ${mtu} -lt 12001 ]]; do\n    if ifconfig eth0 mtu ${mtu}; then\n        echo -n \"MTU ${mtu}  --> \"\n        iperf -c \"${ipvma}\" -p 5010 -t5 | grep -Po \"[0-9.]+ [a-zA-Z]+/sec\"\n    fi\n    mtu=$(( ${mtu} + 100 ))\ndone\n```\n. CC'ing @javierguerragiraldez (the author of commit 1bc97c2809d15f18b3ab0f4466a7253f1b2de7f2)\n. Thank you both for the very interesting comments. Now I understand this much better.\nI had the misunderstanding that the buffer size was directly related with the packet size, and I didn't realized that by forcing a higher buffer size on the Intel NIC I would cause it to overwrite the guest buffers (therefore explaining the bad performance due to memory corruption)\nI'm closing this issue, since there is no issue other than me not understanding correctly how this works. At least, now I have a better understanding of this.\nI'm wondering if we can make the Linux kernel of the VMs to use bigger buffers. I think I will take a look at the kernel code.\n. CC'ing @n-nikolaev (the author of that lines)\n. This causes bad results when running a kernel >= 3.14 inside the VMs because of the print flooding.\nBut is not causing the bad iperf results with MTU=9000 observed in PR #312 \nI will update the PR\n. Ok. Seems that I'm still not mastering the github workflow. I'm not sure how to rewrite a pull request branch...\nI have opened a new PR #324 to add a new command line argument to snabswitch to enable a debug mode, and changed the print to only happen when that debug mode is enabled\n. Maybe we can do something like:\nif _G.developer_debug then\n      if last_size ~= b.size then debug(\"size=\", b.size) last_size=b.size end\n   end\n?\n. ",
    "plajjan": "I agree with @eugeneia, I prefer determinism of nanosecond integer.\n. Thanks. Are the NICs wired in some certain topology? Couldn't find info on it at the wiki or elsewhere. How do you typically run tests on chur?\n. I added this reply to the wiki page for Snabb lab, hope you don't mind :)\nkll\nOn 2014-12-01 18:02, Luke Gorrie wrote:\n\nThe NICs are dual port cards and the ports of each card are\ncabled together. So PCI address 0000:xx:00.{0,1} are connected to each\nother.\n(The NICs also have a hardware loop back mode that ca be handy for \ntesting\n\"real\" traffic even with only one port and unknown cabling.)\nOn Monday, December 1, 2014, Kristian Larsson notifications@github.com\nwrote:\n\nThanks. Are the NICs wired in some certain topology? Couldn't find \ninfo on\nit at the wiki or elsewhere. How do you typically run tests on chur?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/SnabbCo/snabbswitch/pull/326#issuecomment-65080602.\n\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/SnabbCo/snabbswitch/pull/326#issuecomment-65096907.\n. I read it all at once... massive. Nice work! Answers a lot of stuff I had to find out the hard way. I'll read again but regardless I vote for merge as it's lightyears better than the current situation. Can always fix/add more later on.\n\nBTW, have you considered something like readthedocs.org or gitbook.io for keeping a fairly good-looking document up to date with latest git repo?\n. What ballpark are we talking about for the size of these tables? Thousands? Millions?\nI suppose that one usually doesn't track the state on incoming connections but rather on outgoing ones and therefore the incoming DDoS will hit a (hopefully) rather small table, correct?\nIf on the other hand one tracks incoming connections I think this could easily blow up. DDoS attacks are easily in the millions of new states per second...\n. For the future.. I don't think the git history need to reflect this very exact discussion, so I would have voted for squash in this case. I think a detailed commit log can explain many peculiarities and similar that would otherwise be seen in a non-squashed history.\nThis is a simple mistake, why should we let the history reflect a mistake? Had it not been git we would be sending around .patch files and certainly would ask over mail (or whatever medium we use to exchange said .patch files) to fix the mistake and resubmit, or possibly the person merging would simply fix it before merging.\n. Nope, IF-MIBs are replaced by the operational parts of a YANG model. Naturally, there is often a form of one-to-one mappings between them, mostly because we usually want to get the same type of data, like ifInOctets, regardless if it's SNMP or YANG.\nHere's a YANG model that does roughly what IF-MIB does and it includes a mapping table between how we used to do it in SNMP and what it is called with YANG.\nhttps://datatracker.ietf.org/doc/rfc7223/?include_text=1 \n. I suppose that there isn't really that much support or awareness in Snabb except for the naming of the values that follows the IF-MIB in your example, right!?\nValues are named differently in SNMP and YANG so supporting both means that at least one will need a translation table. My (completely unbiased :P) opinion is that we should try to be closer to YANG and do translation for SNMP.\nWith that said, I do understand your perspective. If you want to do monitoring today it is a lot easier using SNMP. While it certainly is possible with YANG (we do it!), there's a lot fewer tools (especially FOSS) available out there.\nI think of this similar to IPv4 and IPv6. If you want something today to communicate on the Internet you'd like IPv4 (that would be the equivalent of SNMP) while you know that IPv6 is the long term protocol (YANG). I'd rather design something for the long term and have mapping for the short term, than the other way around, again native YANG naming and translation for SNMP. On the other hand, I'm not religious about this, as long as there is a possibility to support both I'm happy :)\nOh, and great work @alexandergall :>\n. @lukego 1 through 5 are all rather interesting. For my use case I think I'm less interested in hierarchy between things as I will likely already know that (since I pushed the configuration from my provisioning system I know how it looks). It could be useful nonetheless.\nIf I were to give a priority order, I think it would follow the order you have written them in, perhaps with 4 and 5 swapped.\n. I quite like the idea of being able to compile and ship a contained binary that performs a specific duty. It obviously removes a lot of complexity for end users and seeing how Snabbswitch is more of a framework for developers, I think this is a great move. I haven't read through all of the code but did have a peek at the packetblaster app. What I do wonder is if we sacrifice flexibility through this? Can I still specify an app network the classic way?\nI'm still not sure how I will deploy Snabb in real production environments and how static/dynamic those environments will be, but let's say I have some NC/YANG integration for Snabb so that I can describe a Snabb design, complete with links, apps and configuration for those apps. Would I be able to deploy a Snabb-instance and configure a app network through YANG just the way I want it or would I need to build a specific binary for that and map some form of input to that? I think YANG is easier to map to the config language (albeit I agree that it's a tad Lua-centric) than to pure command line arguments and having a compile step in between app network config and deployment is obviously rather cumbersome.\n. Ok, cool. Then I'm all for!\nAgain, I haven't looked much at the code but from a high level functional perspective I like it a lot!\n. +1 on @eugeneia \nOne should never mess around with history on Snabb master. Published branches should be avoided if possible but if it's short-lived, like for the duration of a PR, I think it's perfectly okay to squash on to get it to a clean state.\n. No, squashing doesn't mean you compress everything into one commit - you can squash an arbitrary number of commits together and rewrite the commit message to reflect what is going on or do multiple squashes of different commits.\nI produce a new driver with a few commits:\n1. Initial commit with standard scaffolding\n2. Complete TX side of driver\n3. Complete RX side of drive\n4. Whopsi-dhopsi, fixed something for TX side\n5. Update documentation\n6. Initial work on straightline support\n7. Rewrite of RX for straightline\n8. Rewrite of TX for straightline\n9. Improved RX by switching to khash\nNow you can take commit 4 and squash it into commit 2. Since 4 seems like just a fix, you might not want to update the commit message as you still achieve the same end goal, ie a complete TX side driver but it is now fixed, perhaps as a consequence of some feedback on a PR. There is a git action for this called \"fixup\".\nDuring the time this driver was developed and before it was merged, Snabb has merged straightline, so is it really relevant to keep the pre-straightline version of the driver? Most likely not, so we can squash 7 into 3 and 8 into 2 but perhaps this time we rewrite the messages somewhat to reflect that we are inline with the straigtline concepts.\nAt the end, we are down to \n1. Initial commit with standard scaffolding\n2. Complete TX side of driver\n3. Complete RX side of driver\n4. Update documentation\n5. Improved RX by switching to khash\nWe can certainly discuss what kind of history we want to keep but I would like to highlight that there is a spectra on how squashing/fixups can be applied - it's not about squashing everything into one single commit.\nEDIT: funny how I picked pre vs post straightline for example. I just reread your comment and saw you might want to keep some stuff pre-straightline for history :)  I don't want to get into the specifics of what to keep or not - just outline that it's not about flattening an entire branch - it's about selectively squashing commits to achieve a clean history.\n. I tried to consistently use \"squash\" when talking about.. well, squashing commits. The command to do that is still \"git rebase ...\" but it differs very much from other rebase, like when you are just replaying changes over a new branch.\nShould we have this discussion on this PR? While somewhat on-topic, it is getting rather long and has a wider effect than on this PR. ML or open dummy issue to discuss? \n. I haven't read the code but the way I understand this PR it's all about the config format and interpreting that into an internal format, right?\nThe actual parsing of packets and matching towards filters remains the same?\npcap expressions (aren't they called BPF?) are probably well suited for network people but are they really superior for machine interaction? I imagine another system having an internal representation, that probably looks quite a lot like the current config format of packet_filter, and then translating that into a BPF expression only for snabb to again interpret it.\nIt's a bit like SQL. RDBMS were fitted with a query language that humans could write and there was a time when users, not developers or system administrators, actually interfaced with databases using SQL. But then we invented web2.0 and wrote pretty user interfaces that anyone, even novice users, could use. All of a sudden it's a machine generating SQL, the language invented for humans, and to make it easy for the programmers, ORMs were developed. So what is the job of the parser and planner in any RDBMS? To turn that pesky human-writeable format into something more suitable for a computer. Full circle.\nNewer databases, like Rethinkdb offers a different model, where there is never a format suitable for humans, instead you interface with it directly from your favourite language (see rethinkdb.com, although it's a bit off-topic ;).\nI'm all for a BPF style interface to ease human interaction but I'm not convinced it should be the only format.\n. Ok, cool. Sounds like we are on the same page.\nRegarding YANG, there's a draft (99% of YANG models are drafts at this point) at least; https://tools.ietf.org/html/draft-huang-netmod-acl-01\n. I think I'm confused by step 3 in your proposed process. Why would a tester merge a PR onto their own branch? If I review a PR and I think it looks good, you are saying I should merge it on my branch and then I should open a PR that to 100% contains code that someone else has written, ie my PR is identical to the original coders PR. Why? Seems easier to just comment on the original PR then :)\nYou can send your own changes as PR to the project, wait for someone to review and write \"+1\" on the PR then you push merge, or you let someone else have RW on the repo in addition to yourself, then you could assign your own PRs to be reviewed of them (this is how I typically work in small projects).\n. Fun router analogy :P\nI'm not sure the proposed workflow, let's call it linux-kernel-flow, would improve things for Snabb. It was invented for the kernel a long time ago. If it is so superior, why are other open source projects not adopting it?\nThe kernel is a big machine and requires an involved model for the relatively few people at the top to manage it. Snabb is the complete opposite - one of the design goals is even to keep it small, so I don't think you should try to imitate what the kernel is doing simply because it isn't a good fit.\nWith that said, I think the concept of multiple reviewers is a good thing and I fully support that, but it doesn't mean that all code must take the detour via someone elses branch. Many other projects (much bigger than this) merge patches directly onto master and I think we could continue that just with distributed reviewing responsibility.\n. Just to clarify, distributed reviewing is a good thing but from my perspective that doesn't mean you have to change the flow of git commit and merges.\nAgain, as a clarification, the alternative in my view would be to have a core team of trusted people, so in addition to @lukego we also have @eugeneia and perhaps someone else. This core team has RW on the repo so any core team developer is free to merge things on master. New PRs are assigned to one of the people in the core team for review and once they are happy with the quality of the proposed code they push the merge button. Luke puts his code in a PR and lets Max review it. Max lets Luke review his PRs. All code is thus reviewed by at least one core developer.\nThis is very similar to what Luke proposes, if not identical, as far as the distributed review go. The difference is the actual code flow, either PR -> master or PR -> personal-core-dev-branch -> master.\nYou are naturally free to do however you please but I would very much like to understand what you perceive as the gain by merging things on personal branches first? :)\n. There's a number of things that seems to get mixed up, at least from my perspective.\n\nSimple: no core team, no voting, no special processes for reaching consensus/quorum/etc. Code lands upstream when a short chain of people individually approve it. That chain is defined socially by who likes to pull what kind of changes from who.\nScalable: as the community grows we only need a small number of people to become subsystem maintainers by collecting good changes on branches. This process should hold up at least until we reach the size of the Linux kernel community :-)\n\nThis seems to be a lot about nomenclature. You just mentioned \"small number of subsystems maintainers\". This is what I would call a \"core team\", so to me the above is contradictory. You say \"no core team\" yet the second point specifically mentions a group of people that are more trusted than the rest, ie a \"core team\". We are naturally free to call it what we want but I think that you interpret \"core team\" as implying some form of magic voting and special processes where I don't think any such meaning can be derived merely from the name.\nFor the various topic branches I think we are looking at this from very different perspectives. If I develop an app it is only natural that I maintain my own branch of that and only occasionally merge this on master, like when I feel I've reached a certain point with new features, it's stable and so forth. In essence I have my own versioning for my particular app but for various reasons I choose to have it also included in the Snabb main master. I could just as well maintain this app completely outside of Snabb git. The choice of where it goes is probably more about what type of app it is, like ARP or GENEVE, which are fairly generic, should probably ship together with the Snabb framework, while my super-specific FooBar app that only works in the TeraStream network probably should be maintained outside.\nThis is no different from any feature in any project. If I am developing a PostgreSQL feature I would do so on my own branch in my own repo and once I'm happy with it I would submit it to master in one of their commit fests. If anyone elses wants to contribute to my Postgres feature while I'm working on it, it makes sense they send their code to me and merge it into my branch before merging on master.\nWhat you seem to suggest though is that person X will maintain branch X and since X works with ARP and GENEVE, I should send by bugfixes for ARP to branch X. Since person X is also developing a new VXLAN driver I will have to wait a few months for that to complete before person X merges branch X to master. If I had targeted my ARP bug fix directly to master, it would be merged faster and could be tested earlier by everyone and not just person X. It is rather common to have merge windows where new features are merged after which only bug fixes are allowed as so achieve a stable version that can be released every now and then.\nThe points you bring up about autonomy for developers is completely orthogonal to the rest of this discussion and I don't think it holds true. Anyone is always free to base their work on specific versions of an upstream repo. Splitting that repo up in many branches doesn't help this very much. You maintain the version of a core lib I want in your branch but you also have the new counter module which I'm not particularly interested in - in fact I have my own version and don't want yours at all. Having separate branches doesn't make this any easier - I think I'd argue to the contrary, it will likely just become a mess where I am merging features from many branches to get the latest components that I want.\nI hear what you are saying about OpenStack and I'm not sure if this is just a general observation (which I agree with) or if you are trying to argue against me. I don't think I'm advocating for anything that is remotely similar to OpenStacks workflow. I'm just talking about the way a piece of code travels before it hits master. As far as review of that code goes I think we are saying the same thing, just using different nomenclature. OpenStack is difficult to work with for completely different reasons but let's not delve into that as it would take us way off topic ;)\nWe've kept this discussion at a very generic level so when we think of good structure we probably have different particular examples in mind, which is also likely why we end up with different conclusions on what is the best way to structure the workflow. I am not particularly active on the project so my opinion isn't very relevant. You write code, and quite a lot of it, so you should stick to whatever you are happy with :)\n. RSS yay \\o/\n. The most convenient thing would obviously be if we could simply take parts of firehose and integrate into Snabb, yielding an overall performance boost but I suppose that the flexibility provided by Snabb will never perform the way a simple program like firehose does. For the time being I'm happy if I can reach linerate 10G per core (obviously my demand will go up as I go to higher NIC speeds and as long as the Snabb process-model stays).\nIf you have callback in the standard Intel10G app, how would that look? How would that work compared to the app network of today? Like a pre-process hook? So I can pass the packet from the hook onto the regular app network? Most of my packets would just be discarded in the hook.\nI'd be happy to optimise the slow path but I don't really know how. I would like to keep PCAP matching and short of removing that I'm not sure how to speed things up. Suggestions are very welcome!\nAs for testing on latest, I just ran a test, unfortunately with a decrease in performance.\nOld:\nsource sent: 31745\nrepeater sent: 110386950\nsink received: 68\nEffective rate: 11038695.0\nNew\nsource sent: 31745\nrepeater sent: 75812520\nsink received: 68\nEffective rate: 7581252.0\nThis is run on my laptop with the selftest, so no real hardware is involved.\nBTW, if you want to run the selftest on my app don't be discouraged by the \"test failed\". It runs two tests, one that tests the logic and one for performance. It is the logic test that fails because it expects the source to be blocked after 20 packets but for some reason it starts blocking at 21 packets. Not a big deal - the app works after all.\n. I was a tad naive thinking that pflua was a drop-in replacement of the older pcap lib. I realise that wasn't the case so I updated my code but performance is pretty much on par  :/\nTo test my slow-path I simply write a rule that doesn't match any of the packets in my test-pcap so that I stress the packet matching.\nOld (lib.pcap):\nEffective rate: 737514.7\nNew (pflua):\nEffective rate: 713727.8\n. When you say \"a new app\", do you mean like a standard app that we have today? So I could do intel10g.rx -> fireblock.input,  fireblock.output -> ddos.input ...? That would be sweet, above all because it fits the current standard model.\nA callback in the driver is a bit hackish and I think it should be avoided if possible. If the performance gain is big enough though, hacks like this are still worth the downside of added complexity and some ugliness ;)\nCould you elaborate further on what you were thinking about for snabbmark?\nSomething like the basic1, where we hook up source -> ddostop -> sink and measure? Or do you mean to write more explicit components to exercise pcap matching (slow path) / src block (fast path)?\nddostop includes performance testing in its selftest. Not sure if there are rules for what the selftest should contain, like only logic testing or performance as well. Is it generally desirable to put performance tests in snabbmark rather than in an apps selftest?\n. Speaking of 'optimize the hell out of the \"slow path\"', I just found a major and silly cause of my slow path being so slow. pairs() vs ipairs().\nI'm afraid I have to come up with a new name for the \"slow path\" now as performance went from 750kpps to 8-9Mpps.\nSee https://github.com/plajjan/snabbswitch/commit/0913bbe8a86c39e728e511218dfe6ecaca167aea\n@lukego if we ever have a recommendation on how to write fast things, this should probably be in there. It might be obvious for someone with a Lua background, but for a novice like myself it's less than obvious and worth pointing out. It's perfectly natural once I read the documentation but I just hadn't reflected over there being both a pairs() and ipairs().\n. @lukego another interesting change, credits to Daniel Barney - https://github.com/plajjan/snabbswitch/commit/d9d38ba82e4ef4bc5d25008f539e7bfbd10b6c81\n. I found another weak spot that I wasn't really expecting. I've been using PcapReader -> Repeater for my tests and the pcap file I used as input was a 2MB file with 32k packets in it. Most of the packets were the same (5 ICMP packets and then 31995 identical NTP packets) so for my test I extracted one NTP packet and just loop that instead. Performance selftest went from ~10Mpps to closer ~29Mpps. Apparently I spent too much time looping test packets.\nAgain on my laptop. Interestingly, my laptop seems to be faster than Chur. I didn't really expect that. It's an i7 and I bet it has like that frequency boost for a single core if nothing else is running, which probably kicks in and helps me out. I will do a test on chur with sending packets over real NICs to verify performance.\nGuess this is another tip for that perf guide we should write ;)\n. > My theory is that the simplest and neatest solution is to never apply flow control within the app network. That is: apps will always receive and process all of their available packets and never check whether the output has capacity beforehand. During congestion packets will be dropped at the bottleneck point.\n@lukego I don't agree with this, at least if \"never\" is really \"never\". I agree that this makes sense for vast majority of cases but you have to be able to implement QoS in Snabb, which is why this\n\nThe idea that makes most immediate sense to me would be for the basic behavior of apps to be (1) and for fancy algorithms like (3) to be implemented as their own apps.\n\nmakes sense.\nI imagine anyone deploying Snabb today would do so in an ideal environment, like 10G in and 10G out and so the need for QoS is typically small but with time this could change. Could Snabb be the forwarding plane of a CPE? Yes, of course (btw, people are hacking to get VPP on RPi - kind cool!). That certainly means a need for QoS.\n90%+ of use cases probably require no queueing whatsoever and can live with the simple link overflow you mention when CPU is scarce. Of the part that requires QoS, most will likely only need it on the last penultimate node, i.e. node closest to NIC.\nIn the VlanMux app I wrote I'm pretty sure there is a HOLB issue. Similar to this example:\ningress_nic -> app1 -> Tee ->  egress_nic1\n                        |\n                        +---->  egress_nic2\nIf you just look at what the Tee app does today, it will get the maximum number of packets it can send based on the output link that has the least capacity available. If egress_nic1 is congested and can take no more packets that number will be 0. app2 will thus not consume and build up pressure backwards through the graph. Your solution of blindly transmitting packets would solve this.\n. Just to be clear, my VlanMux doesn't use the Tee app. I just mentioned Tee since it's an example that's in master - VlanMux does however use the same method to get nwritable of all outputs as Tee does (it's copied from there), so they do suffer from the same problem.\nFor certain apps it might make sense to block when an output is full but for the VlanMux I think it shouldn't block just because one output is full. Like this topology, why stop forwarding Vlan2 stuff just because NIC1 is congested?\n+------+    +---------+    vlan1   +------+\n| NIC0 | -> | VlanMux | ---------> | NIC1 |\n+------+    +----+----+            +------+\n                 |         vlan2   +------+\n                 +---------------> | NIC2 |\n                                   +------+\nThere are multiple challenges here. The first one being that I need to look in the packet to determine whether it is going out on the vlan1 or vlan2 link. AFAIK if I do link.receive(input) then there is no way to put the packet back on the link. I was looking at the Tap driver and noticed it seems to peek the input link, read a packet and tries to send it out on the tap interface and only if it succeeds will it consume it from the ingress link. I could perhaps attempt something similar but I think I will quickly run into more problems. Can I peek/read/consume further back than one packet? Maybe, maybe not. Even if I could, so I could go through all packets of this breath I would still run into problems when I've consumed all vlan2 packets in the breath and for the next breath all packets are vlan1, thus blocking more vlan2 packets from ever being put on the link - i.e. the back pressure has now effectively moved back to NIC0 and there it's \"stupid\" drop.\nThis is very similar to the design of large routers, for example like this:\n.-- LC0 --------.\n|  +----+       |\n|  | NP |----+  |   .---.\n|  +----+    |  |   |   |\n|            +----->| F |\n|  +----+    |  |   |   |    .-- LC2 --------.\n|  | NP |----+  |   | A |    |        +----+ |\n|  +----+       |   |   |    |   +--->| NP | # port\n`---------------'   | B |    |   |    +----+ |\n                    |   |---->---+           |\n.-- LC1 --------.   | R |    |        +----+ |\n|  +----+       |   |   |    |        | NP | |\n|  | NP |----+  |   | I |    |        +----+ |\n|  +----+    |  |   |   |    `---------------'\n|            +----->| C |\n|  +----+    |  |   |   |\n|  | NP |----+  |   `---'\n|  +----+       |\n`---------------'\nWith traffic form multiple NPs on LC0 and LC1 to LC2 you are likely going to congest from fab to LC2. Most boxes solve this through virtual output queueing so the ingress NP will lookup egress NP+port. That packet is then enqueued in a VOQ for that egress port but it happens at the ingress NP. LC2 NP can signal back pressure over the fabric to the ingress NPs so they know what to do with the VOQ. There are various issues with this technique but it's usually the most reasonable solution. A completely output buffered router would be the ideal but that requires completely non-blocking fabric, i.e. the from-fab capacity for every linecard is equal to the sum of to-fab capacity and egress NP can process all of this. That's obviously not feasible in real life but if we could do this we would have our egress QoS policy implemented in the most convenient location from a policy enforcement perspective - we would be able to match every incoming packet to the policy and enqueue / drop packets accordingly. In reality we do the VOQ dance and when we get back pressure we signal this back to ingress NPs which in turn will start dropping packets. The problem here is that the back pressure / distributed drop is less refined so we might drop the wrong packets. Imagine we have high priority VoIP packets coming in on LC0 and from LC1 it's a DNS amplification DDoS attack. As long as all the packets make it to the egress NP it will look at DSCP values or similar and prioritise the VoIP packets leading to only DDoS packets being dropped. As soon as the from-fab queue is full, we signal back pressure and all of a sudden ingress NP are supposed to drop packets but they might not do the full classification that the egress NP usually does, so instead we just drop X% of the traffic, leading to both high-prio VoIP packets and DDoS packets being dropped - \"stupid drop\" again. Actual details naturally depends on the implementation...\nThe CRS-1 has 2.5:1 speedup in fabric meaning that a linecard with 40G frontplate capacity actually has 100G from the fabric, so even with a DDoS coming in from 2 other cards going out a single card, the egress card can receive those packets and process them. In the CRS-3 Cisco lowered the speedup to something like 1.5. Customers weren't really ready to pay for it...\nBlah bla, I'm rambling... that became a lot longer than intended. I think my point is that Snabb might have similarities with a large router, like if we have 40x10GE interfaces spread over 2 or 4 CPUs, each CPU is an ingress NP and the QPI bus is our fabric.. it will all become very complex and I think we should try to avoid that whole scenario, at least for now. Stick to the 90% cases and come up with a solution that fixes HOLB for those and still allows scheduling if that's what we want.\nAll apps that aren't explicitly some form of QoS app shouldn't have to think about this, so they should just put packets on their output links. If we are CPU constrained then we will drop packets, probably in an indeterministic way, but I think that's ok. It's easy to fix by adding more CPU. Assuming we are not CPU constrained the only potential congestion point is egress NIC and there we can have an explicit QoS app. Naturally it needs to be able to sense congestion on the NIC egress port and it might be that we want to do this differently than sensing backpressure via a link. If the link queue size is too big then it's impossible to do low latency queueing in software.\nHow does it work today? What's the queue depth on a link? What's the size of a breath? How are apps called to process packets, like does it follow the graph?\n. @lukego also for the QoS part, the static thing is difficult to implement in reality since it's difficult to know just what that static value should be. If you put it \"too low\" you will waste capacity and your users will be unhappy with the disconnect between ingress capacity and egress capacity. Too high a value and you will queue in the NIC instead which completely defeats the purpose. VLAN tagging changes the value. The ethernet clock is not stable, which also affects this, in particular if it's a link running congested over long periods of time.\n. @eugeneia you might be right that this should be done explicitly in the app. What I want is simplicity and deterministic behaviour which is my issue with the current incarnation of the Tee app. @lukego mentions he prefers non-blocking apps. Is the solution to simply drop packets if there isn't enough space on the output links?\nWhat reasons are there for a link being full? What's the queue depth of a link? What is the normal size of a breath?\n. @lukego maybe we have completely different philosophies here but I prefer to separate the problem statement (issue) from potential solutions (PRs), which allows us to write many of the latter without discarding the former.\n. When does an app:push() gets called? Exactly once during a breath? What's the order? Does that mean you can never build a program with appFoo1..11 (i.e. 1100 packets) sending packets to appBar without dropping packets? I guess push() is run-to-completion so at least it can process it's whole batch.\nWould it make sense to schedule appBar:push() once we see more than X packets on its link?\n. @lukego ah, I see, so push() will run repetitively until app network is empty. Makes sense.\nOverrunning a link buffer is probably quite exotic but if it's not then I suppose one would want to preempt and run the apps with the fullest input links before others. I think a naive approach is simple to implement but there are pitfalls (like you might want to start processing from egress side of app network) here so current scheme is prolly best kept until we have proven use cases causing problems.\n. @eugeneia should we invent a new mechanism to know wether a NIC is congested first? I guess link.full() together with relying on back pressure on the link to the NIC is the only method we have today, right?\nI think this is a case of leading by example and where the model of keeping all apps in one repo is a good one. Just rewrite everything to not use it full() and I'll bet we won't see any use of it :)\n. > Then if we would enumerate all of the tests we would have 10 * 5 * 10 * 16 * 2 * 2 * 2 * 2 * 3 = 384,000 test scenarios.\nThe last \"3\" representing three different CPUs probably means that you have three machines, thus these machines can run the tests in parallel and you end up with 128k tests, not 384k. With a minute per test it's 88 days. Do we run tests in parallel on the same box? If you could run 10 tests in parallel you have reduced running time to ~9 days, which is actually manageable. This could of course have negative impact since concurrent processes compete for same cache and so forth. I imagine the scientific CI to run in the background and whenever it identifies \"problematic combinations\", ie a certain combination of inputs that diverges from a \"baseline\" (some average of all runs!?), you could a) have a human look at it and/or b) add it to SnabbBot so that this combination is then run for all subsequent commits.\nMaybe you don't need to run this type of testing for every commit/PR but instead focus on every release. If that's the case, you actually have a few months to run a complete suit. Or you run it as fast as it can, ie if a run takes 9 days then you start the next run with whatever code has landed after those 9 days.\nIf you add more parameters or if you are unable to run tests in parallel then it certainly becomes more attractive to just do random sampling of the whole combination space.\nFor the more exhaustive option maybe some parameters can be excluded, ie if results are virtually identical over all versions of qemu then most of those versions can simply be removed from future testing.\n. Generating a YANG model from Lua is certainly one way but I think the more common approach is to write the YANG model by hand and then generate whatever code is required from it although given Lua (dynamically typed et al) I'm not sure there would be a lot of code to generate.\nYANG is typically more expressive (uh well, in one way at least, it is a data modelling language after all) than most programming languages. Like if you have variable that is going to store a percentage value then you would probably define that as:\nyang\n  leaf my-percentage-value {\n    type uint8 {\n      range \"0..100\";\n    }\n  }\nYou could make a percent datatype as well to shorten that further.\nSince Lua is dynamically typed I guess the equivalent would be something along the lines of:\nlua\nassert(type(my_percent) == \"number\", \"my-percent must be a number\")\nassert(my_percent > 0, \"my-percent must be between 0 and 100\")\nassert(my_percent > 100, \"my-percent must be between 0 and 100\")\nOnce you start having a bit more complex config, like dependencies between various leafs (I'll adopt YANG nomenclature) then doing it in code usually becomes much much more verbose (and error prone) than having a clean definition in a YANG model. I don't see a good way to generate such dependencies from code as you would essentially be reinventing a small data modelling language and that is what we  have YANG for.\nIt might seem like extra work at first to write a YANG model and then write your code but I think it actually reduces the amount of code you need to write as you can take a lot for granted. You can rely on values already being of a given type and within certain ranges. You don't have to deal with default values since YANG can do that for you and so forth.\nThe interface should preferably validate in both directions so if you try to set a value from your code for a leaf that isn't defined this should generate an error. Preferably this is done by static code analysis but could be done during runtime as well.\n. sysrepo is a project to alleviate some stuff when working with YANG from code. See http://sysrepo.org/blog/netconf_yang_blog.html  no Lua bindings yet ;)\nIt uses libyang (it was a byproduct for writing sysrepo), which someone could write Lua bindings for. Also look at netopeer, which is the NETCONF server that will likely be used by sysrepo in the future. It is currently using freenetconfd and it was chosen at a time when things looked different. Netopeer has now progressed to a point where it is likely a better candidate.\n. Indeed, instance data is expressed as XML over a NETCONF transport. RESTCONF, which is on the standards track (22nd of Jan deadline for feedback), allows XML or JSON.\nI don't think you need to deal with XML internally. You'd need a lib in between where you do config.container(\"foo\").leaf(\"bar\").set(\"asdf\") or something to set the value. If that container or leaf doesn't exist you get an exception. If the \"bar\" leaf is defined as type uint8 you get an exception. In the other direction the lib exposes the instance data as XML and JSON on top of which you then base your NETCONF / RESTCONF interface. This is essentially what libyang tries to do, so easiest way forward is probably to use it and write Lua bindings for it, perhaps with some more abstraction to get an elegant interface :)\nAs for NETCONF vs RESTCONF I think that NETCONF is better suited for a device while RESTCONF is probably something I'd put on the northbound interface of my OSS system. My experience with RESTCONF is still very limited (obviously not being a standard helps :P) but it seems limited which is why I'd prefer NETCONF for a device.\nI think you can use existing libs / daemons for the actual NETCONF server, like https://github.com/choppsv1/netconf, freenetconfd or netopeer. Please do have a peek at sysrepo too :)\n. @lukego,\n\nYANG/NETCONF interface for provisioning a cluster of devices as a single unit.\n\nNot sure what you mean. I don't think we try to cluster devices to have them behave as one or at least NC/YANG doesn't really play a part in that. We do have a component in our \"OSS\" that maps between low level device configuration and high level \"abstract service\" config. It can potentially hide devices but again, having that component and having a NETCONF interface isn't directly related.\n\nYANG/NETCONF interface for provisioning one device.\n\nCrucial since it's the only interface supported by the management system we are building.\n\nYANG/NETCONF interface exposing the internal workings of a device (e.g. Snabb app network).\n\nNice to have. Not crucial in a first release but I would probably expect to see something like this in a mature product. Debugging running stuff is nice++ :)\n\nYANG/NETCONF interface exposing \"generic\" information that is not application-specific e.g. hard disk utilization, management network (Linux kernel) details like interface names / addresses / routing tables, etc.\n\nYes, I expect to get these figures. Sysrepo takes the approach of having a system NETCONF server so that multiple daemons, Snabb being one, running on the system are still configured through one NETCONF interface. This way, you have one part that is responsible for system stuff so that Snabb doesn't need to.\n. I think you are wrong in that smaller ISPs like to operate using SNMP / manual config. That's not true. It's just that they do it because there are FOSS tools for SNMP and manual config because there is nothing standardised out there for configuring. Screen scraping is the next step up from CLI. Noone likes that, it's just a necessity when you have more than a handful devices. NC/YANG tools are scarce but there is great momentum in the industry and I'm sure we will see more stuff pop up.\nAs for SNMP (next comment), no, I don't really care for it. I expect to read those figures via NETCONF, or in the future, streamed to us. In all cases it follows a YANG model and a transport that is modeled by those YANG models. Admittedly, the support from vendors is quite bad today (JUNOS has 0 config:false nodes in their YANG model) so SNMP plays a role today but not tomorrow.\nAlso, the devops model. I know vendors like Juniper have implemented Puppet and stuff on the devices but AFAIK (I have only had a cursory look at it) it more or less wraps around the NC/YANG or XSD stuff. From my perspective this is more or less doing YANG but just with an alternative transport over some parts. No real benefit.\nAs I mentioned, there is great momentum on the whole NETCONF/YANG front. Over a hundred YANG model drafts in the IETF and so forth. I think the reason these devops thing popped up on the devies is because NC/YANG wasn't mature enough. No one would come up with this stuff if all network devices already had a NC/YANG interface. Chef/Puppet/Ansible/Salt would do better in implementing NC/YANG on their end so when a device has NC you can automatically configure it via these instead of getting specific device support for one of those config frameworks.\n. If you just export a chunk of JSON you don't get the validation part until it's too late. Components:\n+-------+               +------------------+                +-----+\n| Snabb |  -- JSON -->  | NC / YANG daemon | -- NETCONF --> | NMS |\n+-------+               +------------------+                +-----+\nSnabb puts a string in the uint8 field and exports this JSON chunk. The NC/YANG daemon takes the data, validates it against the YANG model and finds the problematic string. What should it do? Discard the data? Signal back? How does it signal? Are we inventing a new mini-NETCONF interface here? Or do we not care because we never write bugs so we'll just never have this problem?\n. Also, the devops model for network devices is for DC where the config is typically very very basic. I haven't seen anyone trying to use that model for tackling a more complex configuration. When 99% of the things you are managing are servers, you want the remaining parts to look the same.\n. I agree with @mwiget, which is why I keep repeating things like sysrepo because it tries to accomplish exactly this ;)\nThe important part is that the data that Snabb accepts as input or what it outputs conform to a YANG model. Mangling data from one format into another is what I really want to avoid. Like if everything was just a flat key value store and we would need to mangle that into a hierarchical YANG model, that'd be PITA.\nI thought the sysrepo approach of linking something that helps you with validation looked pretty nice but I understand you are not to keen to link to third party code. I think you should go talk to the sysrepo people about your concerns. Evidently they are of the view that people won't object to linking to their stuff to get convenience functions for dealing with data according to a YANG model. If this is not true I think they should hear about it. I am also interested in what is okay on this front? Lua libs is ok? C is not?\n. Related to @andywingo thought on getting large trees, NETCONF supports filter parameters so you can extract a subtree or part of it. I suppose that one would want to rely on a library to do this. At the same time, if the underlying infrastructure doesn't support it, you might end up sending large amounts of data (like from that FFI stored data) over to a NC agent that then filters it down to just a subset - clearly inefficient from one perspective, but perhaps easier to implement.\nAlso want to highlight a few common pitfalls and tricky parts NC/YANG as I have seen more than one vendor fail at this. From the top of my head and in no specific order;\nconfig symmetry\nIf you take a config coming in over a NC interface and convert this into another config format, let's say we want to configure a daemon that doesn't have NC / YANG support, like ntpd. We take the XML config, push it through a template config generator to produce a ntpd.conf file and then SIGHUP ntpd. Now if we do a get-config over the NC interface we either need to read ntpd.conf and apply the reverse to get back the XML representation, or we need to keep a cached copy. If we go with cached copy, no changes can be made to ntpd.conf as it wouldn't show up over NC. If we read back ntpd.conf we need to make sure the conversion is lossless. Best thing is config is kept in a lossless format so that get-config can be easily implemented.\nNETCONF isn't RESTful\nNETCONF is stateful. You have different operations that can specify how a \"patch\" is to be applied to the current configuration. Like 'create' that will create a node only if it doesn't exist, and will fail if it already exists. 'replace' will replace the node. 'merge' will.. well, merge and 'remove' will remove. Since you potentially don't get the entire config subtree over NETCONF you need to have a copy of the current config so you can do a merge operation (which btw is the default).\norder dependency\nFor example, you define an access-list and apply it to an interface. As this is done in one transaction it doesn't matter if the data you are sending contains the access-list definition first and the reference after that or the other way around. There are multiple NC implementations failing at this and requiring that the ACL is defined before it can be referenced.\nSimilarly, some implementations won't allow certain operations based on current state and not desired state as expressed in the candidate config. This means you have to first change A, commit, then change B (which relies on current state of A) and then commit.\nSince the internal components might actually want to apply config in a certain order, it could be up to the NC agent to reorder things so that it suits the internal implementation.\ntransactional\nDon't think I need to explain much about this. Doing actual transactions and being able to rollback requires an implementor to keep track over things in the time domain which can be tricky.\nvalidate\nBeing able to validate a config and not applying it typically requires internal support from a daemon. Basic syntax checks can be written outside of it but for true validation support I think you want to have support \"deep\" inside the app.\nModel translation\nThis one is really tricky and it's doubtful that Snabb should even attempt to translate instance data.\nJUNOS has recently started supporting (one way) translation of configuration data with the goal of supporting standard IETF / OpenConfig models. Since Juniper already have an internal model and a large existing user base they are not very keen on changing it. Representing the same internal data twice in different form is probably not a good idea so the only remaining option is to support translation. XR have started to support OpenConfig models (http://plajjan.github.io/Cisco-IOS-XR-6-0-and-YANG/) and do this through a kind of internal translator AFAIK.\nSo the question boils down to, is there a need to support the same data expressed differently through two (or more) models? If the answer is no, then no translation is required.\nThis type of functionality could of course be done in a NC agent so no headache for Snabb.\n. @andywingo that was what I was trying to allude to with the \"Model translation\" topic in my previous post :)\nIt would probably quickly drive code complexity so I think we should ignore that bit for now. Probably better to implement in a NC/YANG agent, using XSLT, than in Snabb anyway.\n. GitHub has admin/read/write access. Admin allows everything. Write is allowed to push to the repo while read obviously only allows reading. From an issue management perspective you need write to be able to add labels, set milestone and assignee. You can use protected branches to prevent people with write access from being able to push directly to a branch, so everyone has to go via PRs.\nSo if you want more people to help categorise issues you want to give them write but probably protect branches.\nThere's functionality so that you can write \"Fixes #1234\" in a commit and it will automatically close that issue when the commit mentioning it has landed on your base branch. \"master\" is the default base branch but you can change this in the repo settings as well. Since you use a next branch for everything maybe it makes sense to close the issues when it has landed on that one.\nAs for easy to fix vs hard you can just add labels for that. For example, a low-hanging-fruit label can help newcomers looking to contribute find simple things to fix.\nQuestions aren't really issues in the same way. A \"normal\" issue means there is some problem with the code but a question could be just to help a user get something working and doesn't necessarily indicate an issue with the code.\nFor ideas, which are more of high level concepts, I like to \"branch out\" the issue into more low level actionable items once it has been discussed and potential solutions have been reached. Once those low level issues are fixed, by committed code, then high level idea can be closed, or if you prefer you can close it as soon as the low level issues have been created.\n. And speaking of LPM, I find it very interesting that Juniper seem to have opted for a combination of bloom filters and equal match tables for implementing LPM. I don't think it's explicitly stated but it can kind of be derived from information, see http://forums.juniper.net/t5/Data-Center-Technologists/Juniper-QFX10002-Technical-Overview/ba-p/270358\nMy interpretation is that they have as many tables as there are bit lengths, so 32 for IPv4 and then use Bloom filters to quickly evaluate if it's probable that there is a match in a table for the destination address. \nfor pl in 32..0:\n    if bloom_check(dst_address, table[pl]):\n        nh = lookup(dst_address, table[pl])\n        if nh:\n            return nh\nSo, start with bloom check on /32 table, if \"maybe\", then do actual equal match in that table and return result. If bloom check ==  no, then check in next, less specific table and so forth. I suppose you can do parts of this in parallel with outstanding memory lookups et al.\nIt feels awfully wasteful. With different prefix lengths for the different tables I suppose the hashes used for the bloom check need to be recalculated for every prefix length. Not sure about difference in speed between bloom + lookup vs just the lookup.\nThey claim 1 search for LPM.\n\nGiven these limitations, we still wanted 1 read (search) per packet for LPM. Juniper\u2019s Bloom Filter Technology allows us to get 1 search for LPM in nearly all cases with a high number of entries, which enables the high logical scale.\n\nI don't understand how that would work, or they have different definition of \"search\". It's probably not \"memory accesses\", which is what I'm thinking about.\nStill, it would be interesting to see a test of this. Guess one could glue together PHM with some bloom stuff for a proof of concept. Like I said, it feels wasteful (and thus slow) but I'm not very familiar with the performance of \"normal\" LPM tables on a low level. Maybe there are advantages to this approach, or it's only beneficial when you have those HMC chips.\n. @andywingo Can you elaborate on the side table to map IP to prefix length? :) I don't understand what you mean. What's the key for that table? Why would there be 256 distinct prefixes in that table if we are dealing with let's say a DFZ table (~700k routes)?\nIt's worth remembering that the DFZ isn't uniform, e.g. there are way more /24 than there are /8 prefixes. I know the MSC on the CRS-1 used this to optimise the FIB. They did a 5 stage lookup in a TreeBitmap where each stage could contain different amount of bits to match to what is popular on the Internet. Paper: http://cseweb.ucsd.edu/~varghese/PAPERS/ccr2004.pdf \n. I just added a new more specific issue to cover LPM tables. I suppose ctable is fine for now, for equal matching.... (it's actually a tad slow).\n. My comments are still visible. There's a \"show outdated diff\"  next to the \"plajjan commented on an outdated diff\". I really think rebasing is the right thing to do here. Why would we want Snabb git history to reflect spelling mistakes. I don't think you should base your work on random branches on the Internet. First read what it's about and understand the intent. If it's long lived (like the next branches) then go ahead but like this, where it's obviously just to get things in order to quickly merge on master, then expect it to move. Publishing a branch for a PR is no contract...  :) \n. Assumption is the mother of all evil. I wrote my previous comment from a mobile device and didn't see whether Andy had rebased or not and I simply jumped on the bandwagon, assuming it was rebased, when Luke said he did :P \nI suppose my comments are moot given that he hasn't rebased and I feel I spurred the derailing off this issue from its original topic :/\n@wingo how would I use ctable with IPv6? I see src/lib/protocol/ipv6 using char[16] to represent an IPv6. I would instinctively go with uint64_t[2] - either way it's an array. Does this make a difference to how I would use ctable or will it happily eat that as well? Forgive my novice C/FFI knowledge.\n. @wingo ok!\nAs part of #718 I would like to see that we have the complete set of functions needed for lookups of common data types, including 16 byte / IPv6. This doesn't mean this PR necessarily needs to include a hash function for IPv6 but I don't think we should put it on the user to write one.\nI imagine myself being the target audience of Snabb, as a framework, to implement simple forwarding functions. I understand what a hash function does and I suppose I could convert the supplied hash_32 function to something else but I wouldn't really understand what I'm doing. I don't speak ASM. This is why I think Snabb should ship with ready-to-use classes/functions for this :)\n. Heh, biggest downside to this is probably that \"Snabb\" is in no way unique. \"SnabbSwitch\" is kind of unique combination whereas \"snabb\" is already used lots. It's still easy to find snabb.co (Google ranks it high) but I think it will be difficult to find other related things, like email threads, discussions, comments or similar.\n. > VLAN stripping / tagging, and stamping of outgoing MAC addresses. Should this be done by the workload program / app network I guess? An open question.\nIsn't it already? Or does the NIC stamp src MAC when VMDq is enabled? It cannot reasonably put dst MAC in there (how would it know?). I say it stays in app network.\nI'm torn over the subject in general. Sure, simplification sounds like a good thing but on the other hand I want 100G and I don't think it's reasonable to expect a single core to be able to take over the RSS job from the NIC, so I'd like RSS. At the same time I don't know enough about the drivers, are they really that complex?\n. @lukego I love it! I was actually thinking about something similar although I think you expressed my abstract spaghetti thoughts into something much more concrete. For my SnabbDDoS program I want to support:\n- number of NICs:\n  - dual NIC, i.e. use one port for ingress (\"dirty\") and another for egress (\"clean\") traffic\n  - single NIC, -on-a-stick. Since traffic is unidirectional we don't need full-duplex and we can halve required number of NICs with this approach\n- type of NIC\n  - 82599 for production\n  - tap for development, debugging or whatever\nSingle NIC (82599 or tap) requires VLAN tagging  (unless we want really weirdo config on the router with PBR or similar). Now, to complicate things I have an issue on the 82599EB-old-a-f card that vmdq doesn't seem to work properly so for most 82599 cards I want vmdq but for this particular version I want to use software for VLAN demux/muxing.\nThus the final config matrix is rather complicated. The current state of the code in SnabbDDoS program for initing the app network is a mess and it makes me sad panda just looking at it. I was thinking if I could abstract the 82599-vmdq-or-software-vlan into an app to simplify things and I think this is just what you have described here but in a much more generic and elegant way.\nComplexity of switched_nic could, as you point out, shoot through the roof if we are not careful. It's probably a good idea to start out with a minimal feature set and ignore more advanced features to begin with. I'm thinking just matching the vmdq feature set, like vlan (de)tag and src mac rewrite, so it's noop on  82599 but it will do things for a tap interface.\n. > I think this problem would be better thought about as a configuration problem, perhaps answered by supporting configuration macros that give a bundle of functionality but are transparent to the user, and thought about in terms of a declarative configuration file. It's not immediately obvious that making these composite apps flexible enough to support all use cases won't become another point of configuration and we will be back at square 1 with a configuration problem, but have composite apps as well. \nI was thinking about switched_nic as a form of config helper, not that it actually does all the work. For example, I instantiate a switched_nic and say I want VLAN X on NIC Y. If Y=02:00.0 is an 82599 it will be configured with VMDq for VLAN tagging. If Y happens to be a tap interface then switched_nic will instantiate a Tap driver and a VlanMux and connect those together. Same end result, something that takes input packets and strips VLAN tags!\n\nswitched_nic sounds like it's a lot of work but none of the following do\nAn app that dispatches to a set of links based on statically defined macs\nAn app that adds / removes Vlan tags\n\nNo, not really. It doesn't sound like a lot of work but that's probably because we have different ideas of what it is or should do. To me it's a convenience API so I don't have to think about what the NIC supports or not.\nJust adding the apps you list won't do anything. It's kind of the situation we have today. I already have an app that adds/removes VLANs (#863) but how do I add that to my app network and when do I need to? Should I never use VMDq so my app network is consistent? (StraightNIC!) or should I allow hardware offload for some stuff. That complexity is what I would like to abstract away.\n\nAn app that ECMPs traffic over a set of output links @plajjan requires MPLS labels to be attached cool have an mpls app @petebristow requires Vlans Someoneelse requires QinQ A.Nother wants a more labels for segment routing. All of these are configuration problems rather than app problems.\n\nWell, this is a problem for my program not my app. The DDoS app doesn't care about the NIC, it just wants packets. But the SnabbDDoS program does need to care, how else would this whole thing work?\nAlso, funny that you would mention MPLS since I'm working with a network where half the ho-ha is about us not using MPLS ;)\n\nThese are all simple configuration matters. Having pure configuration macros means they are seen as lightweight reusable chunks. I can now have a chunk that covers my physical nic + fastPathApp + tap interface for slowpath I want linux to handle. It would mean we need to revisit the lua as configuration decision but I think this needs to be done any way as part of the Yang debate. We also need to think about what configuring a multiprocess snabb network should look like.\n\nOkay. So it sounds like you have bigger things in mind here. I was thinking of switched_nic as something I'd write in a day. Rewriting Snabb's config to be something other than Lua sounds like a slightly bigger topic.\nThe way things are right now, Snabb offers nothing in this area. It is up to each developer to build a program that takes the configuration options they wish to support and in effect that will limit the deployment options you have as a user. I personally have no need for MPLS so I will not add that to SnabbDDoS. \nI definitely think there are improvements to be made here. I am not really interested in writing all this \"glue\" stuff for deployment. Snabb doesn't even have MPLS support today but let's say I upstream SnabbDDoS and someone adds MPLS tomorrow, wouldn't it be neat if, like you express, a user could just add some config options and use SnabbDDoS in an MPLS deployment? I think so!\nSnabb is really just a basic framework at this point. If we look at the only other thing out there that is vaguely similar - VPP - there is still a stark contrast in what they provide. Both Snabb and VPP have the ideas of nodes in a graph that each do one little thing well. Both use batching and various other clever techniques to achieve high performance. But VPP offers a lot more out of the box. You can spin one up, configure a couple of interfaces and some static routes, through a CLI, and have it forward packets for you. It's like Snabb but there is a default app network which provides you with lots of stuff.\nI wrote a DDoS app, a node in the graph/app network.\n- In VPP I would insert that node in a pre-existing graph so I could benefit from all the routing / tunneling / whatever that is already in VPP.\n- In Snabb I am expected to not only write the app/node but also to build the entire graph / app network around it.\nI guess Snabb will move in the direction of VPP. Once there are more standard apps available it makes sense to ship most of these in a default app network. Developers are always allowed to start over with a clean slate if they wish but they shouldn't need to.\n@petebristow I don't want my NMS to have to worry about whether NIC + VLAN is actually two apps or if one app will do the job. Internal app network is largely irrelevant to external parties/users. Who wants to think about having to put a \"reassembler\" app in there? It makes sense from a dev perspective to have it as a separate app but from a user perspective it sucks having to think about it.\n. @eugeneia \"is:issue is:open NOT wip in:title\" should do that. Here's an example (using 'NFV' instead of wip because there are no wip issues): https://github.com/snabbco/snabb/issues?utf8=%E2%9C%93&q=is%3Aissue%20is%3Aopen%20NOT%20NFV%20in%3Atitle%20\nNote that \"NOT\" must be all capital - syntax is described here: https://help.github.com/articles/searching-issues/\n. @lukego yeah, no relative date AFAICT but how about sorting by date? append \"sort:updated-desc\" for latest by update which is what makes sense most of the time.\nWhere should these queries be documented?\n. I'm a novice on the topic but isn't it relevant where it's a registered trademark? Doubt you could trademark (\"varum\u00e4rkesskydda\") \"Snabb\" in Sweden given it's a common Swedish word :P\nLet's say someone wishes to describe their product in Swedish and say it's fast, that would be the word \"Snabb\", or is there perhaps a distinction being made here through capitalisation? \"Snabb\" vs \"snabb\"? Doubt you can prevent Cisco from describing their CSR1000v as fast ;)\nApart from that it's a succinct text which I quite like.\n. Yeah, it is a rather boring topic and I agree on the intent and I think it's good adding this.\nI didn't want to argue to the contrary, it was more of a question. So +1 for merging :)\n. @mwiget What is WIP about this one? I'm looking at making additions/modifications to packetblaster and your code seems like a better foundation than what we currently have on master.\n. > Would it make sense to integrate this tighter with lwaftr, e.g. by importing lwutil.lua or leverage the protocol libraries for udp and datagram?\n@mwiget I think not. It's kind of nice with a second implementation from a testing perspective. One could argue this is just for performance testing and that such validation could be performed by another test app  but.. why? Better that that the test apps share code but we have to make sure we validate the program itself and if we share code with the lwAFTR how can we do that?\nI noticed you have 2 space indent. I think Snabb typically uses 3 space indent, no? (Do we have a style guide?)\n. @kbara heh, src/apps/lwaftr/vlan.lua isn't there so it wasn't upstreamed enough ;)\nWould you agree that it's better to put this as a separate app though? It's a fairly generic component and I think it's much cleaner if it's not inside the directory of a more specific application. I suppose lwaftr should start using this one as well instead of packing its own.\n. So I made something rather embarrassing and included some debug print statements that I had used before. Squashed that away :)\n. I can write a selftest for that or at least run some tests.\nIn general, do we want performance tests as part of selftest() ? Importing the various modules needed for a selftest feels like slight pollution.\n. @eugeneia sure, that makes sense. Since that is a completely different change from a git perspective I think we better discard this PR and open a new one.\n@nnikolaev-virtualopensystems I wrote a quick perftest as a selftest, given @eugeneia feedback I guess it should go into snabbmark instead but anyway, here goes:\nkll@lingloi320 ~/kod/snabbswitch/src $ make && sudo ./snsh -t apps.vlan.vlan\nLUA       obj/apps/vlan/vlan_lua.o\nLINK      snabb\nBINARY    2,1M snabb\nEffective rate - baseline: 65345535.0 Mpps\nEffective rate - Tagger  : 40018680.0 Mpps\nEffective rate - Untagger: 26598030.0 Mpps\nThis is on my laptop (Intel(R) Core(TM) i7-3687U CPU @ 2.10GHz but turboboost..). The perf drop is quite big but the relative difference seems smaller when used in a more realistic program. Like if pcapreader & repeater is used instead of Source I get:\nkll@lingloi320 ~/kod/snabbswitch/src $ make && sudo ./snsh -t apps.vlan.vlan\nLUA       obj/apps/vlan/vlan_lua.o\nLINK      snabb\nBINARY    2,1M snabb\nEffective rate - baseline: 37955730.0 Mpps\nEffective rate - Tagger  : 27742470.0 Mpps\nEffective rate - Untagger: 20868690.0 Mpps\n. Anyway, since we'll be using the one from lwaftr I'll close this PR now.\n. Awesome stuff, I suppose this would be very nice to look at the flow of programs and seeing hot spots.\n. Are you not able to reproduce this?\nWhich command?\n. Ah, okay, nice to hear it's fixed already. When is max-next hitting master next time? :)\nWhat's the process here, do we keep issues until it's on master or can we close it as soon as it's on one of the -next branches ?\n. @dpino I believe I have addressed all of your concerns now :)\n. @eugeneia mmkay, I was thinking that would be a separate PR but if you want to see it in the same I'll see if I can put something together :)\n. @eugeneia I actually found way fewer references than I thought so I've updated them. PTAL.\n. So maybe the best thing here is that we split this. First one PR to add apps.vlan.vlan that @eugeneia can  accept. Then a second one to change lwaftr to use apps.vlan.vlan. @kbara sounds good? Where should I open that second PR? Here? igalia/snabb? That first change need to land on whatever I'm submitting the second PR for...\n. @lperkov do you mean how to package Snabb into a docker container? Running one should be rather similar to running any container. \nBTW, is this for packet forwarding or mgmt side integration work? If the latter, why bother with Snabb lab? Just run it on your local laptop :) \n. Of course it's convenient to have some real programs to test with but that doesn't necessarily mean it should be included in the same git repo. To draw on your kernel analogies I bet it's important to test various kernel syscalls with actual user space apps yet the kernel git repo doesn't include user space applications.\nI know Snabb has adopted the Linux merge model and I have nothing to say about that in itself, but you cannot expect everything else to follow that model. I think it's awkward to ship my app hidden deep inside the source tree of snabb. Is this now the main repo of my app? Is this where users should open issues? With the changes you just asked me to do on lwAFTR to get the VLAN app accepted in here I have made changes to lwAFTR as if this was the master for lwAFTR where in reality I think igalia/snabb is the master. Are you going to accept or reject my changes to the app (SnabbDDoS) that I've written? I've received feedback that people don't understand where the source to my program is and it certainly isn't very obvious - you have to be familiar with the snabb eco system to know where to look. I can rewrite the top level README but that will likely give me a headache every time I pull in the latest master.\nI don't have an answer to where developers of programs using Snabb should place their code or how the overall structure should be. I just feel that the current solution is suboptimal.\n. @eugeneia are you happy with functional testing as is? i.e. a selftest() in each module that does varying things? Thoughts on using proper unit test framework with concepts of test units / cases?\n. I think the kernel analogy is halting. When I start the kernel.. I start the kernel. I then go and configure it in various ways, maybe load some modules or set some sysctls, but there is one \"entry point\". Similarly, I can start a Cisco IOS router and then go and configure various features of it. Snabb on the other hand doesn't work like that. We have multiple entry points, namely the \"programs\", thus it doesn't look like one cohesive system. It is a library/framework that people use to write their own programs. It's more similar to the Python Twisted library/framework on top of which you can base your program - like https://github.com/scrapy/scrapy (but that lives in separate git repos).\nNow, there is nothing stopping Snabb from becoming a cohesive system. VPP already is. You start the thing, you get a CLI and it does basic packet forwarding things out of the box, like it has an IPv4 RIB, IPV6, MPLS, does VRFs and so forth. You can write plugins for it, and if SnabbDDoS would be written for VPP I suppose it would be a plugin that would be inserted into the node graph between two other \"default\" nodes.\nThere are of course benefits and drawbacks to this approach. One drawback is that you kind of end up in a bit of the same situation as with kernel forwarding - one of the reasons it is slow is because there's so much features that have to be checked for every packet. With Snabb you can write something that is super-duper optimised for a specific use case and you don't have to waste a single CPU cycle on checking something you know you don't need.\nSo, I think to me, upstreaming all apps makes sense if Snabb acts as one system. Right now it acts like a library / framework and thus it doesn't make sense.\nIf I'm in charge of ddos then noone else should be allowed to make changes, which is in direct violation of what @eugeneia asked me to do for the VLAN PR. How are the Igalia folks supposed to handle that I've changed their code without them approving or it landing on their repo? (This is why I wanted to do a two-stage rocket out of that and not squeeze it into one PR). \nAnyway, this has consequences to end users. I think the Snabb eco system is built for developers and that is great... for developers. But if programs are the end product then they will be shipped as such, they will be documented as such and so forth. A network engineer wants to deploy an lwAFTR so he googles for \"open source lwAFTR\" (go ahead, try it!), finds igalia, ends up with the Snabb top level README and has no idea how to proceed (okay, granted this is also lacking in other areas but it is confusing). If he just wants an AFTR why isn't it packaged like that? Let's say I want to build a docker container out of SnabbDDoS so I setup a Dockerfile in the git repo root, oh wait.. ;)\nPS. I don't have a complete suggestion on how I want things to look, I know what parts I find weird today but what might come across as suggestions should be regarded as input to a discussion :)\n. > How about if the first section of the README would be links to the subdirectories of the in-tree applications? (ddos, packetblaster, nfv, lwaftr, etc.) This is much more important than the blah blah blah parts for people looking for the code to some application.\nFor a developer it is important to get some introduction to Snabb - the framework, while an end user probably cares only for the programs.\nWill a long list of programs, that target end users, scale? Maybe I'm getting ahead of myself. It's not a very long list right now, so perhaps it's fine ;)\n. @lukego have a look at the PR #863 again. @eugeneia asked me to update the current uses of the VLAN app so I've changed programs/lwaftr/setup.lua to use apps.vlan.vlan instead of apps.lwaftr.vlan \n. Content wise I think it's good and it addresses #864. I think a link from https://github.com/snabbco/snabb/blame/master/README.md#L91 would be nice as well.\n. Yay :)\nThis PR includes the changes from multiple other PRs, should it?\n. @wingo not really, for example #877 is commit 05f94d7 and 6608e9e which are included in this PR too so it would seem #877 and #878 are not needed if this PR is merged, no?\n. As for the documentation, I had a read and find it quite ok actually - don't get why you say it's so terrible.\nI would like to try and use it though - if I can do that then I think the docs are good enough :)\n. What happened to this one? Where did we end up? Something blocking merge or are we just waiting for the next release to come along?. #866 does this already and is merged\n. I like KISS (keep it simple stupid). 1GB doesn't sound much. If you have 64 VFs I'm sure you can spare 1GB memory, right?\n. 131072 buffers is ~1.3GB, right?\nWhat kind of limit is this? It's just a limit, right (as opposed to us actually allocating this)? Why do we have it? If you bump it to 10GB and my machine has 8GB, what happens then? PANIC? \n. I wasn't necessarily suggesting the limit be set to 1.3GB, just wanted to know what we are actually talking about :)\nCatching resource leaks sounds like a good thing but at the same time I'm afraid you will run out of memory before you hit this limit if it's set to 10GB. But we are entering the ops domain, what is typical amount of memory available to host? My gut feeling it doesn't get that much because all apps are running in VMs.\nI was about to suggest statically allocating all of it on startup. I remember an issue I had with a linecard that had faulty memory somewhere in its packet buffers. At normal workloads we didn't notice this but then two other paths in the network broke, forcing loads of traffic over this linecard - egress ports were congested, packet buffers filled up, hit the faulty memory region and crashed the linecard. Obviously with the two other paths down this happened at the worst possible time. I don't want something like that happening with my snabb processes, especially if they are supposed to do ddos mitigation...\nHow hard coded does this need to be? Can it be specified at startup? So like you give it a config file and if that config happens to consume more than the default (1GB?) then snabb simply won't start - you have to raise the limit. This makes it easy for most installations (since we have a reasonable default) and makes it explicit for those that need more.\nI'm thinking of tuning postgres. There's a rather reasonable default but if you intend to run heavy queries you need to increase certain values. It's clearly stated that more memory will be consumed.\n. I don't immediately see the problem with using JSON. @wingo can you elaborate? There is https://tools.ietf.org/html/draft-ietf-netmod-yang-json-09 which defines a mapping for JSON so shouldn't be a problem with the types but I haven't really looked at the details. With that said, I don't really mind XML either.\nAs for the output itself the counter64 is hex encoded, why? counter64 is a uint64 and the mapping for that is described here - https://tools.ietf.org/html/draft-ietf-netmod-yang-json-09#section-6.1 \nThe \"type\" is an identityref and current ones are described in rfc7224 - https://tools.ietf.org/html/rfc7224. I guess you can use existing values, like \"ethernetCsmacd\" where you currently have \"hardware\". For snabb specific stuff we need to create a new yang model that defines new identityrefs - like snabbLink.\n. @wingo I perceived this as a step in the right direction and not necessarily the final solution.\nI assumed one would have the schema handy but you have probably thought more about the internals of implementing this in Snabb. Right now I can't spare the brain cycles to think about this and come up with any sensible input so I'll just leave it in your capable hands :)\nIIRC XML (at least XML-RPC) also suffers from lack of large numbers, but maybe I'm thinking of > 64 bits.\n. Big thumbs up for RSS :)\nI'm not qualified to give any real feedback on this although I've skimmed through it - mostly out of curiosity. Did note 2 space indent which is different from the de facto 3 space indent.\n. Ok, understood :)\nFYI, YANG is very strict when it comes to making modifications between revisions so for example you can't rename a leaf if you are to follow the rules. We might want to discuss how to deal with versioning (simple way is to put version in the name, like my-model-v1.yang).\n. DXR is probably a good (best?) candidate for IPv4.\n. @petebristow you mentioned awhile ago you had done some work on writing lookup tables.. any updates? care to share? :). link.full() still seems to have a purpose for a QoS app. Should we just not use link.full() from normal applications but let it remain for special apps that need to detect congestion on egress NIC? Or am I missing something? (just back from vacation ;)\n. @lukego FYI there was a discussion on slack around this. I think my viewpoint aligned rather well with yours, like not implementing a complete database. Avoid version specific binary data blobs and so forth. I didn't feel this resonated with everyone though.\n. What are the data formats from/to snabb config ? JSON is mentioned but some of the stats / config examples above is another format.\nI recently wrote a quick hack integration between sysrepo and SnabbAFTR and noticed that the current binding table config is in a proprietary format which meant I had to write code to write my data in that format. What's the rationale for creating a new format when there are so many other formats readily available (and that already have libs for many many other languages). Can you clarify what the design is here going forward?\n. Ok so that's \"JUNOS style config\" - I don't think there's a standard for that. Why do you want to use this over say JSON - which does have libs for a tonne of languages and there's a standard for how to use JSON for YANG modelled data.\nThis is an orthogonal issue to which YANG model you are using. I am interested in hearing what tuneables don't align that forces you to write your own model. Have you discussed this with Ian?\n. There are two topics at hand;\n- which data format to use, like JUNOS-style vs JSON vs something-else\n- which YANG model to use and shortcomings of the standard one\nFor the first one, I am asking because I want to know why you want to use what is seemingly a not so common data format instead of using something that is very popular and wirespread. Yes, I have generated lwaftr configuration and that's when I first noticed you had implemented your own data format. Before that I had assumed it was JSON but that was an obviously an incorrect assumption on my part.\nFor the second one I am asking because I want Ian, who as I understand it, took part in writing that YANG model so I imagine feedback would be important - like should an updated RFC be released... I implemented my own YANG model that closely mimicks the binding table config file, so I never noticed discrepancies with the standard YANG model.\n. I was trying my very best not to conflate, which is why I said \"data format\" and not \"model\". I meant a choice like JSON vs XML. I understand your reasoning for non-standard syntax/model and I have no problem with that nor did I try to complain about it. I was asking about the data format. Like why not do the equivalent of \"require (\"json\"); json.decode(config)\". It irked me a bit since I got to be producer of config in a, to me, unknown format.\nExcellent on the YANG model topic. Exactly the response I was looking for. Thank you! :)\nI am not keeping up to date internally on all the details of the AFTR so this kind of information doesn't flow naturally to me, which is why I'm asking you about it. That means you might get seemingly weird question - apologies for that.\n. I would have hoped that it would do considerably better, assuming that your app does \"nothing\" else besides sending packets. With some more packet processing I guess we'll see much lower performance. What does the receive performance look like? On par?\n. Should the graph only show 8192 queue depth?\n. Heh, no problem. I'll await the test results :)\n. I just want some clarification. This is on a single core, right? And that core is not saturated? So we can't expect a performance increase simply by using more cores, correct? \n. Also very good job. Both on the driver implementation and on the analysis. It's always very interesting to read material from writers enthusiastic about a subject. You clearly are and it shows. A very interesting read. Interesting results and interesting ideas for the future. \n. I don't think you should go out of your way to support what seems to be a bad NIC, i.e. if it requires you to move packets to certain places and thus decreasing performance in Snabb it's a bad move.\nI want to be able to get wirespeed performance out of this by asking a vendor to produce a fast NIC and then just throw more cores out of it. If someone don't need wirespeed they can buy a bad/cheaper NIC (seemingly like this Mellanox) and can use less cores.\nMost importantly the decision on pps/bps should be with the end-user :)\nThe first 10G NICs I used didn't do much more than 5Gbps. I think it's too early in the life of 100G NICs to draw conclusions on general trends.\n. Interesting graph. Is it some fixed buffer size that leads to the plateaus?\n. I don't think 64b/66b has anything to do with this. That's just avoiding certain bit patterns on the wire and happens real close to the wire nor do I think it's related the AUI interface (which I assume you are referring to).\nDoesn't the NIC copy packets from RAM to some little circular transmit buffer just before it sends them out? Is that buffer carved up in 64 byte slices?\n. I think tests with fixed and variable sized packets are useful, if nothing else to spot patterns just like the ones I imagine will happen due to the optimization you mention Mellanox has implemented.\n. @lukego probably depends on use case but given pps of applications and the CPU consumed for that it would seem to me that at least we SPs doing VNFs would be more inclined to have a CPU per 100G. That's ~24 cores now/soon. 148Mpps / 24 = ~6Mpps per core so overall a CPU per 100G seems quite a good fit. With a dual socket machine we'd use two PCI cards and one 100G port per NIC. Thus increase in PCI bandwidth would be useful for us.\nIf you go talk to DC guys I am sure they will have a different story because they do more storage with large packets where pps doesn't matter but throughput will, so they are more inclined to max out PCI bus bandwidth than CPU.\n. @lukego going out and buying some switch to aggregate some 70% 100GE PCI NICs feels like a hack and I don't think the vision for Snabb's 100G support should be based on the premises of a hack. Isn't it trading (relatively) expensive switch port for cheap PCI lanes?\nThere are certainly compatible router ports out there. Did you mean competitive? Still more expensive by quite a fair margin.\nBlargh, why can't I read all the comments before writing... yes, PCI seems least scarce, so I'd rather be a bit wasteful on that side.. but like I said; DC guys probably want to use that I/O!\n. @lukego right, PCI issue is up to vendor to fix - which is why I previously said;\n\nI want to be able to get wirespeed performance out of this by asking a vendor to produce a fast NIC and then just throw more cores out of it. \n\nJust like you point out, someone at the vendors is probably working on this already and if they aren't, we, as customers of these NICs, should go the the vendor and ask for them to fix the issue. I don't want to see lots of dirty hacks or unnecessary work in Snabb to make up for bad NIC behaviour.\nThis is the first 100G NIC driver for Snabb. It would be very interesting to see how it compares to the second (which NIC is next in line?)...\n. I have an observation to make based on the \"core box does 256B so we don't need to care about more\". There is a difference here between that core box and Snabb. The core device have tens or hundreds of 100G and sees all types of traffic in the network of which the average packets size must be higher than 256B. Snabb is typically used to implement a specific application, so it will not see the networks average packet size - it will see the packet size for this specific application.\nIf Snabb is a single application and it gets a DDoS attack of 200Gbps at 64B packets it will have to process all of those. For my core box the influx of 200Gbps 64B packets will only marginally lower the average packet size since it is already shuffling around say 10Tbps of other traffic with a much higher average packet size.. I agree with all of this except for that last part. At least for YANG models exposed externally, the types defined in http://www.netconfcentral.org/modules/ietf-inet-types should be used - which is dotted quad for IPv4. You need to parse this into uint32.\nIf you have some internal data structure that is described by a YANG model then I suppose it might be fine leaving it as a uint32.\n. @wingo Ok, good. (wasn't very clear since there's a uint32 in YANG too ;)\n. Maybe it would look like P4 (p4.org) ?\nWhile fd.io / VPP even has \"vector processing\" in the name I think it's different. I would say it's more \"batching\" or similar to the streaming ctable where we do the same thing (lookup / transformation / whatever) on a bunch of packets, then move on to the next step where we do some other thing on that bunch of packets. It reduces cache trashing and so speeds up things considerably but is distinct from how a true vector processor works, i.e. actually process two or more packets at the exact same time.\nTo continue on the P4 thread... it's built for chips like Barefoot's Tofino chip. They have themselves drawn strong similarities to GPUs and other fast signal processing chips applying instructions in parallel, i.e. vector processing. P4 as a language provides constructs for parallel processing. I think it assumes a parallel platform but it doesn't help the user much since it also exposes parallel semantics so the whole burden of putting it together is on the user.\nI think the current (I guess \"scalar\") way of Snabb is very easy to learn. Ease of learning and short getting-going time are some of the strongest points for Snabb IMHO. Accomplished programmers might as well use VPP. If Snabb turns into some esoteric paralell-processing semantics it will significantly increase the threshold for many and thus the appeal. Therein lies a challenge of course: could some parallel processing be introduced without significantly making things more difficult? I don't know but I would suggest a thorough analysis and comparison with P4 to start with.\nI guess there might also be an advantage to a platform (snabb that is) that supports both mode of operations. One can start out scalar and a more experienced brogrammer can convert (parts?) of the program into parallel processing making it a rather smooth transition.... It appears to be some performance tests in the code that do some profiling. Are those tests run in CI? Can I see the output somewhere?. \"repeatedly by the\" .. engine? \n. Shouldn't this be:\nprint(word, sep)\n. seperated -> separated\n. millseconds -> milliseconds\n. I guess it's fairly evident that the input/output file is expected to be in pcap format given the name of the app, but perhaps it should be pointed out!?\n. Syntax error, I can't read this sentence ;)\n. s/that//\n. I don't think the hash_32 are class methods as they don't expect to be operating on a class.\nAt least in a different language (like python), new() would be a class method, returning an instance of the ctable class. But the hash functions don't return an instance of a class, just a value, so they are just functions that happen to be residing in this module.\n. I quite like the introduction, including both use and how, but maybe I'm more interested in the details of the how than the average user. I'm not against rearranging the text but I don't want to remove anything at least :)\n. Yes, I am aware. I explicitly did not want to require anything in lwaftr. I agree that these kinds of functions, if generally helpful, should be available in some core lib. I didn't do that here because a) it doesn't belong in same PR and b) I don't know if lwAFTR is the only program using this or if it would be adopted by more - maybe other devs prefer another pattern. Maybe I should have a local rd16 so it's easier to find afterwards if/when we introduce something into core!?\nWhat do you mean with \"cast is cached\"? I don't know what to do with this information, do you mean I should change something? :)\n. Yeah, htons is better, will change.\nconf is probably remnant since early versions. The \"config\" of the app is now ruled entirely by what is connected to its inputs/outputs. Do we have some preference that we always use conf so as to follow a standard pattern kind of?\n. Okay, see what you mean with cast. NVM :)\n. The way I interpret this I should be able to do, for example:\nself.dot1q_tpid = htons(dot1q_tpid)\nBut:\napps/vlan/vlan.lua:99: variable 'htons' is not declared\n. isn't fqdn in \"SSH\" column wrong for this and following rows? I guess name and SSH should match up.\n. Align columns so it's readable in text form as well?\n. I have a similar issue with my snabbddos-influxdb script and a rather similar approach - https://github.com/SnabbDDoS/snabbddos-influxdb/blob/master/snabbddos-influxdb.py#L33-38\nFor SnabbDDoS I imagine the proper solution is to have a startup script that wraps around the snabb instance itself, catches the PID of that and then launches monitoring stuff around it with the explicit PID.\nAs for the stale process directories in /var/run/snabb I think Snabb per default should clean up after itself. That's my gut feeling at least. If someone wants to look at the data after Snabb has finished it's better to have an explicit option for that than the other way around.\n. This will be interpreted as Gigabits per second and not bytes. Why not use bps ?\ntypedef bps {\n  type uint64;\n  unit \"bps\";\n  description \"bits per second\";\n}\n. Do you really need to say only L2TPv3 is allowed? That's kind of the point of the model itself - don't denormalise by putting info in multiple places :)\n. Missing a revision statement (it's mandatory in YANG).\n. Don't include unit in leaf name.\n. ",
    "AlexKordic": "Hi Luke and thank you. \n. ",
    "jeffbrl": "I borrowed some logic in the process_packet method from @plajjan's DDoS app with his permission.  I should have spotted the iovec line. I'll fix it.\n. Should commits to address issues noted in PR comment be squashed by the PR author?\n. I'd like to give this one a shot. I will make a proposal in this issue's comments before writing the code.\n. Use this logic after receiving -EINVAL from shmget.\nIf sysinfo's totalmem struct member <= 256 MB:\n\"Cannot allocate 128 MB of shared memory for snabbswitch. System memory is too low.\"\nelse:\n\"Cannot allocate 128 MB of shared memory for snabbswitch. Increase shared memory using 'sudo sysctl -w kernel.shmmax=1073741824'. Use the '-p' flag or edit sysctl.conf for persistence across reboots.\"\n. @mwiget Well done!\nCan Snabb NFV work with raw sockets to obviate the hardware dependency?\n. @eugeneia Thanks for the pointer to your labswitch hack. I'll have to examine it further.\n. ",
    "hcnimkar": "SSH key has been added to enable login to chur server\n. ",
    "byterians": "We are really sorry to send so many pull requests. I can see my ssh key in authorized_keys file. Although the issue is, when i try to login using the following command : ssh -p 2020 root@lab1.snabb.co \nI get the following error : Agent admitted failure to sign using the key.\nThanks!\n. ",
    "ghjung": "Okay. Are you going to fix it? then I will just close this case. \nThanks.\n. ",
    "hanshuebner": "I see that there are a number of CI failures, how can I run the CI tests\nmanually?\n2015-03-15 7:33 GMT+01:00 Luke Gorrie notifications@github.com:\n\nGreat!\nre: rebase/squash, I actually would prefer to preserve the history in this\ncase. There is a lot of interesting code buried in there from the more\ncomplex pre-straightline versions and maybe somebody will find it valuable\nto go digging through that one day when implementing something new for this\nNIC.\nThe PCI module is becoming more useful now that we have multiple NIC apps\nthat are interchangable at least for certain applications.\n@eugeneia https://github.com/eugeneia what do you think about the\nintegration to the pci module and the SnabbBot fail? and what if any\nchanges do you think we need to make in other layers of the code e.g.\ndocumentation?\n\nReply to this email directly or view it on GitHub\nhttps://github.com/SnabbCo/snabbswitch/pull/404#issuecomment-80877968.\n. Is there a way to make the CI run the tests again after I have pushed\nsomething to my PR?  It seems not to happen automatically.\n\n2015-03-15 10:11 GMT+01:00 Luke Gorrie notifications@github.com:\n\nThe CI is running make test (really: make test_ci) with an environment\nthat makes an Intel NIC available.\nYou can do for example:\nexport SNABB_TEST_INTEL10G_PCIDEVA=0000:XX:00.0\nexport SNABB_TEST_INTEL10G_PCIDEVB=0000:XX:00.1\nmake test\nwith XX replaced with the ID of an Intel NIC.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/SnabbCo/snabbswitch/pull/404#issuecomment-80928578.\n. I've run it locally and it passes except for the snabbnfv self test - It\nseems, though, that the self test is somewhat dependent on the private\nconfiguration, so I would like to know whether I have a local configuration\nproblem or the test really does not pass anymore for Intel NICs.  With my\nunchanged private configuration, it used the SolarFlare NICs when I ran\n\"make test\" which is nice, but also not what I wanted.\n. @eugeneia Thanks for taking care of the rebase!  I don't think that we need doc/device-interface.md - I wrote it mostly as a reminder as to how things work.\n\nWith respect to bench_env changes:  The logging changes have been obsoleted anyway, and I'm not married to any other changes as long as the SolarFlare driver is still supported.\n. The solarflare NIC is in grindelwald (0000:86:00.0/1).  It passes the nfv\nselftest and the tests that \"make test\" runs.  There also is\nprograms/solarflare which is a simple tx/rx test which I used for basic\nvalidation - That directory should probably not stay in the mainline once\nthe driver is integrated, I just find it useful as a quick confidence test.\n. 2015-03-16 22:04 GMT+01:00 Max Rottenkolber notifications@github.com:\n\nThat directory should probably not stay in the mainline once the driver is\nintegrated, I just find it useful as a quick confidence test.\nA standalone driver test would be important to keep, can we turn it into a\nselftest similar to make apps.intel.intel_app? The simplest way add it as\na unit test is to create an executable src/program/solarflare/selftest.sh\nshell script that invokes the test and exits with status 1 on failure,\nstatus 0 on success and status 43 if skipped (for instance if the\nhypothetical environment variablels SNABB_TEST_SOLARFLARE_PCIDEV[A|B] are\nnot set).\nYes, that should be the right way moving forward.\napps/solarflare/solarflare.lua:23: ef_vi library does not have the\ncorrect version identified, need 201502, got djr_b1c7f30a84a0\nWhat is this about? Is this something external or did I miss a submodule\nupdate along the rebase?\nYou are using  the wrong version of liciul.so, which is strange because I\ndid update the version that is installed on grindelwald (/lib/libciul.so).\nAre you using grindelwald?  Are is there some other version of libciul.so\non your LD_LIBRARY_PATH?\n\nThe SolarFlare driver needs a specific version of libciul.so because we're\nincluding a hand-edited copy of the SolarFlare header files in our source\nbase.  The reason for that is that the LuaJIT FFI cannot deal with the\noriginal header files directly.  This is painful, but we discussed our\noptions and concluded that this would be the way to implement it.  The\nversion probe that is in the code makes sure that we're not trying to use a\nshared library that is incompatible with our hand crafted header file.\n. ",
    "kbara": "Oops, this was a pull-request against the wrong reposity; it was supposed to be WIP notes elsewhere. Apologies.\n. That was a lucky accident; we'll be working on it in https://github.com/igalia/snabbswitch , and hadn't meant to do a PR here. I'm quite curious about lwAFTR now, having read the draft. :)\n. I'm mildly against this change, as I think it's actively misleading. Most implementations that take \"pflang\"[1]* input involve a transformation into BPF, which is then interpreted on a 6502-like virtual machine (or JIT compiled from Linux 3.0's implementation (though this is apparently off by default), and there are extensions to add registers, etc - but it's all still in this general 6502-like framework).\nPflua is fundamentally different. The main pipeline generates Lua code, which is then interpreted/JIT'd in the familiar way with LuaJIT. The main pipeline never involves BPF. (There is BPF-related code in pflua - a disassembler, and a bpf-lua pipeline that uses libpcap to compile to BPF after handling details like parsing - but this is an alternative pipeline, and even then, BPF is only an intermediate representation. The bpf-lua pipeline emits Lua, which in turn is also run through LuaJIT).\n- [1] pflang is a name invented by Andy. To the best of my team's knowledge, libpcap's input language was previously unnamed.\nCheck out https://www.kernel.org/doc/Documentation/networking/filter.txt and https://lwn.net/Articles/599755/ .\nFor added historical context about the 6502-like VM, see http://sharkfest.wireshark.org/sharkfest.11/presentations/McCanne-Sharkfest%2711_Keynote_Address.pdf\n. Sorry about this, the PR was meant for Igalia's repo.\n. This is still a work in progress, but the analysis on 225 (especially by mpeterv) shows pretty conclusively that this is a LuaJIT issue. I'd be tempted to stick with LuaJIT 2.0.x if I were you; when I benchmarked them last year, it was faster, and it's also more stable.\nDiego and I have both been able to intermittently reproduce the problem on our development machines by running sudo ./snabb snsh -t apps.packet_filter.pcap_filter (with version 8ac912d21c817f86618b6588ad13e23260deeea6 from the 'next' branch).\nIt can also be reproduced within pflua head without Snabb by running ./env pflua-compile \"(icmp6 and src net 3ffe:501:0:1001::2/128 and dst net 3ffe:507:0:1:200:86ff:fe05:8000/116) or (ip6 and udp and src net 3ffe:500::/28 and dst net 3ffe:0501:4819::/64 and src portrange 2397-2399 and dst port 53)\"; see mpeterv's analysis for how the details change between specific LuaJIT 2.1 commits.\n. Good points, and having 64-bit bitops is pretty nice; I'd be curious to know how Cloudflare's handling upgrades too.\n. @lukego The problem is fixed with LuaJIT fe565222a1cbf28fbae266da35c8a703fdcfa0dd , which disables the optimization you correctly bisected the problem to. I've checked the fix with both pflua-compile and sudo ./snabb snsh -t apps.packet_filter.pcap_filter, and everything appears to be fine, though I haven't run extensive tests beyond that.\n. Nice work. The use case I'd most like to see is this level of automation with setting up an environment with two QEMU instances and an arbitrary Snabb application between them, so getting started would be a matter of running one script and perhaps changing IP addresses and a few similar details in the QEMU instances.\nOne step beyond that, it would be lovely to be able to have one more level of abstraction, so that it would be equally easy to get started without Intel NICs. http://blog.ipspace.net/2014/12/l2vpn-over-ipv6-with-snabb-switch-on.html mentions Alexander Gall's initial setup without an Intel NIC, using raw IO on a standard Linux socket with an app that opens a raw socket instead of Snabbswitch's driver.  (at time 32:57-34:16). He says it gets a couple hundred megabits per second. It's a good option both for getting started, and for developing on a laptop without extra hardware.\nI've found getting started with Snabb applications that are being fed a pcap file is quite easy, while getting a working setup with QEMU on both ends and Snabb in the middle on chur is surprisingly time-consuming.\n. Everything that we have done by hand would be automated. It would become taboo to rely on instructions in README files and manually created binaries: our new religion would be automation and reproducibility.\nI would love to see this.\n. Linux supports 'sparse files', but most Linux filesystems don't support transparent compression or magically make files that could be sparse sparse, as far as I know. It's a mechanism to use with caution, since various tools can end up filling in the extra empty bytes if they aren't sparse file aware.\n. That sounds fantastic.\n. @eugeneia I agree with @lukego . Battle testing is planned. :-)\n. I think gaining experience with LuaJIT internals is vital at this point. I'm happy with stripping out as much as we can as an experiment and to learn, but I do hope that whichever fork of LuaJIT predominates in a few years is still multi-platform, etc.\n. This is a wonderful patch. As a convenience for anyone who wants to try it out before it's merged, I've created https://github.com/Igalia/snabbswitch/tree/lukegosubtree2_nutmeg_and_611profiling , which consists of your subtree2 branch, cherry-picked PRs 619, 620, and 621 (profiling enhancements that PR 617, your subtree2 branch, mentions), and this patch, since that takes a while to put together. No guarantees, but it seems to work for me:\n% sudo ./snabb snsh -jdump=+rs,x -jp=v -t apps.rate_limiter.rate_limiter\n48%  TRACE  13          ->loop     basic_apps.lua:27\n40%  TRACE  17          ->loop     basic_apps.lua:83\n 7%  TRACE  16          ->loop     rate_limiter.lua:73\n 4%  TRACE  19 (16/4)   ->16       rate_limiter.lua:82\n. Nice one!\n. Thank you. The lack of delayed-commit has made us have to make quite a bit of code uglier to avoid exactly that impact.\n. I like the sketch; I was pondering similar things a little a month or two ago. It's a little fiddly, but seems like a plausible option to me.\n. It's slightly awkward that it takes until the 3rd element of that to have a way to specify addresses like 01:00.1. I'm not sure that 1 and 01 are great abbreviations; they imply .0, making it more awkward to specify .1 in comparison, which I can imagine being annoying for people just starting to use Snabb (they'll get used to using 1, then have to scramble around how to figure out how to refer to a .1 address). And it's not obvious looking at a command line that you're referring to a PCI address at that point. '1' looks like it could equally be an overly-abbreviated non-canonical form of lots of different things.\nI think 01.0 is ok (I'm not enthusiastic about it, but it'll save some typing), I like 01:00.0 as the canonical representation, and 0000:01:00.0 should be allowed (but never required), I'd say. (One thing I've wondered about, but never dug into, is whether the 0000 is always 0000, and why; do you know?)\nI agree with you that exposing Lua patterns is the wrong approach.\nAll of the options you've mentioned could work for the lwAFTR.\n. Sounds fantastic.\n. Honestly, that feels like way too many options. Documenting it, visually parsing what other people are using, and the increased use of things that look almost IPv6-like strike me as all richer sources of confusion this way. I'm all for convenience, but I think this goes so far that it's less convenient or practical.\n. @eugeneia I was thinking the same earlier today.\n. It definitely still is an expensive operation; for the lwaftr, we had to ditch using lib.protocol entirely on our hot path, though we kept it for the less common operations (it does make the code nicer). This wasn't entirely due to packet copying overhead, but that was a double-digit percent of the massive speed difference it made. I'm currently writing lib.protocol style code for ARP, in the hopes that @alexandergall 's code to avoid multiple moves lands - and because ARP is a rare enough thing that it's not fatal if it doesn't. My ICMP mini-library, from before #637 , shuns the lib.protocol style entirely (so I need to figure out how to rewrite it to try to upstream it).\nThe broader question of ultra-fast copy routines is rather interesting.\n. For the value of \"it\" of \"I'm using the lib.protocol stack, and it's shifting memory around 3 times. Using it to make headers and manually copying them is faster (5 MPPS on one arbitrary benchmark); writing the bytes without involving lib.protocol is faster yet\" (8 MPPS on the same arbitrary benchmark) Unfortunately, I don't have the speed before those changes to hand, but I think it was around 1 or 2 MPPS on that same benchmark on the same hardware. While this is vague enough not to be particularly useful, having a factor of 1.5-2 difference by avoiding lib.protocol is major, unfortunately. ( https://github.com/Igalia/snabbswitch/pull/22 has a few raw performance dumps on repeated runs on a number of different packets/codepaths, but with enough different code changes going on that it's not very informative; it was quick-and-dirty heuristics that made sense at the time).\nWe were mainly testing on 550-byte packets, though we had similar results with much larger/smaller ones. We didn't explore different memory copying operations, due to time constraints and getting enough performance without doing that, but you're right that that's rich ground. The libav community still considered copies something to be avoided at all costs when I was collaborating with them in summer 2014, for what it's worth.\nI'd wonder about the performance impact of most of our packets not being a multiple of 64 bytes; there's a fairly long history of \"type X of copy can really go fast, type X' which is similar is quite a bit slower\". Things like overlapping data can also be an issue, like you've mentioned - if I want to shift a packet by 14 or 18 bytes as I add/drop a header, it's a bit messy.\nI'm really looking forward to seeing the work that gets done on faster packet copies. I like being able to streamline code and have cheap copies; my programming tastes run to the pure functional. I've just seen enough writeups over the last decade of minor variations killing copying performance from the 1980s onward, to not count on it until I see it.\n. I hope it doesn't exist too. Your thoughts on padding to multiples of X bytes might well be quite useful, though I'd wonder empirically about things like overlapping copies (ie, every time we want to shift something within a packet). Like you saw, average vs exact sizes can make a massive difference; if we can sidestep even most of that (the unaligned instructions you mentioned + packet buffers being a multiple of the relevant size sound promising), it'll be a good thing.\n. I'd definitely like to see this in practice. In an earlier version of the lwaftr, checking whether two output links were full was a non-trivial slow-down, and it didn't actually come with any real benefits.\n. Agreed; that one bit me, though a bit of reading source code and poking around made it clear.\n. I disagree with this patch (more details later today).\n. Short version: let's be a bit more careful when we're squashing compiler warnings.\nI propose setintfield(L, t, \"mcode\", (int32_t)(intptr_t) T->mcode);\n\nWe're dealing with a 32-bit pointer, even on 64-bit systems, in this case. (As mentioned on http://lua-users.org/lists/lua-l/2009-08/msg00202.html and confirmed by printing the pointer addresses). So, semantically, we're putting a 32-bit pointer into a 32-bit int:\nstatic void setintfield(lua_State *L, GCtab *t, const char *name, int32_t val).\nSo, it is legitimate to get rid of the warning.\nYou may rightfully object that int32_t cannot hold arbitrary pointers. This is true. However:\na) Casting it to long and then having it be downcast to int32_t has the same problem; see 'pedantic note 3'.\nb) We know it's a 32-bit pointer; this is apparently a key luajit abstraction.\nI really do not like casting to long here. As an alternative, I propose:\nsetintfield(L, t, \"mcode\", (int32_t)(intptr_t) T->mcode);\nWhy two casts? It most clearly reflects the semantics, while getting rid of the warning.\nThe int32_t cast shows what the value is actually truncated to, but is not sufficient by itself:\nlib_jit.c: In function \u2018lj_cf_jit_util_traceinfo\u2019:\nlib_jit.c:304:32: warning: cast from pointer to integer of different size [-Wpointer-to-int-cast]\n     setintfield(L, t, \"mcode\", (int32_t)T->mcode);\nWe know that it's a 32-bit pointer (potentially on a 64-bit system), but that information is not available to GCC.\nThe intptr_t cast is sufficient to get rid of the warning, but hides that the resulting value will then be truncated, and that this is only valid because it's actually a 32-bit pointer.\nPedantic note 1:\nThe type of T->mcode is not void*, it's MCode*. MCode is uint8_t or uint32_t.\n```\nif LJ_TARGET_X86ORX64\ntypedef uint8_t MCode;\nelse\ntypedef uint32_t MCode;\nendif\n```\nPedantic note 2:\n'long' is not big enough to hold pointers on all platforms. For instance, it's 32 bits on 64-bit Windows, which has 64-bit pointers.\nYou'd want intptr_t if this was your goal. It's part of C99, and used elsewhere in the codebase:\nlib_jit.c:    setintptrV(L->top++, (intptr_t)(void *)T->mcode);\nPedantic note 3:\nDowncasting is legal in C - and doesn't give warnings.\nvoid foo(int a) {/*code elided*/}\nint *somevar;\nfoo((long) somevar);\nThis is actively misleading code. Even if 'long' happens to be large enough to hold a pointer on the current platform, foo still only takes an int, and will silently downcast; if the pointer doesn't fit in an int, it will be silently truncated.\nA runnable example (run under 64 bit x86 Linux)\n```\ninclude \nlong long foo(int x) {\n    return (long long) x;\n}\nint main(int argc, char *argv)\n{\n    int a = 3;\n    int b = &a\n    printf(\"a: %d,  b: %p, foo-b: %llx, foo-long-b: %llx\\n\", a, b, foo(b), foo((long) b));\n    printf(\"%llx\\n\", (long long) 0x123456789ab);\n    return 0;\n}\n```\nand the results:\na: 3,  b: 0x7ffe29d02034, foo-b: 29d02034, foo-long-b: 29d02034\n123456789ab\n\n. Thank you.\n. @eugeneia I asked basically the same in https://github.com/SnabbCo/snabbswitch/pull/641 , and Luke pointed me to http://wiki.xenproject.org/wiki/Bus:Device.Function_%28BDF%29_Notation , which doesn't fully answer this but gives some interesting context.\n. Nice work. :-)\n. Seems reasonable.\n. Nice catch!\n. It's handy to be able to set the mac address for things like test suites, without messing around with external commands; that's something I'll miss.\n. @eugeneia I'm in favour of having a selftest.sh. I'm just not convinced by removing under a dozen lines of code that provide a useful feature, which is replaced by having to run another program to do the same configuration.\nWith the lwaftr, we have a bunch of end-to-end tests that take and emit packet captures, and we can compare them really easily because the output is expected to always be bit-identical. Part of this is that we set mac addresses. As we explore more complicated test setups (potentially including tap devices), setting mac addresses remains functionality I'd be glad to have in anything we're using or potentially using that has mac addresses.\nTest suites, including selftest() functions, routinely use other Snabb apps. Having to call out to a system-specific tool in the middle of those is doable, but...\n. That would be amazing.\n. Frankly, I'm extremely glad to have the out-of-line per-exposed-API documentation.\na) It can be skimmed a lot faster than the code\nb) It gives a clue as to what's public API vs implementation detail in the implementor's head\nI pretty much always end up reading the code after, but that serves a different set of purposes.\n. As far as I can tell, this isn't entirely irrelevant even on x86_64: https://en.wikipedia.org/wiki/X86-64#Page_table_structure\nI think this patch is worth merging, for what it might be worth.\n. If no one has stepped up by March, I'm interested. I'd really like to see this.\n. That looks like a pretty good example of some of the things I miss most working with Snabb. It'd be great to move more in that direction, especially as core lib.protocol APIs and similar stabilize.\n. I also like this set of ideas.\n. Yep, that's an extremely good idea. Nothing like finding a horrifically bad result you can't reproduce and aren't sure if it was an incidental artefact or something real, after all.\n. The R/graphing stuff isn't that hard; I had to do it as an undergrad, and it'd take anyone here an hour or two, tops. Deciding what the proper statistical tools to use are is a bit hairier. :-)\n. I'll volunteer.\n. Sounds good to me. :-)\n. I'm of the majority view on this one, with the caveat that I gave the specific patch a couple of minutes of thought rather than a deep review. I am in agreement with your stance that we should minimize various types of cruft in the main code. Avoiding autoconf and a bunch of system-dependent conditionals have advantages. Avoiding dark corners where language lawyering is a necessity is good to the extent it's feasible.\nIncreasing the correctness of code on the primary platform while not adding anything, though, is something I'm entirely in favor of. I view the pre-patch C as wrong stuff that happened to work on our platform.\n. I agree that we shouldn't collect half-made ports on master. Assuming anything about the size of void* with regards to integer sizes horrifies me, though, even in non-portable code. I'm ok with the project loving x86_64 above all other platforms, but the previous code strikes me as gratuitously un-portable, rather than something that made the x86_64 implementation actively better/simpler/shorter.\nMy general heuristic is that unportable code is more likely to have various subtle bugs in it that porting makes obvious but which are active bugs even on the main platform. I'd also like to see the work of porters made marginally easier rather than marginally more difficult.\nI consider the extra work for maintainers to go 'ok' to such patches an ok thing - I'm happy to have them go to kbara-next, anyhow.\n. I've just taken a long weekend... now that I'm back:\na) I am totally in favor of moving as much stuff from C to Lua as possible, and explicit stable ABIs.\nb) My bias is massively on the \"be super-portable\" side; I ran NetBSD for quite a few years. I see using intptr_t and similar even on one platform as a good thing. (Note that I do share your dismay at many of the methods used for portability, and agree that Snabb should look more like LuaJIT than a typical autoconf C program!)\nc) I agree that creating packages that don't work is not good. It'd be cool to see a 32-bit port (but if I were to scratch the itch of porting Snabb, I'd probably port it to aarch64 instead).\nd) It's definitely hard to write truly portable code, especially without test suites. All the more reason to improve Snabb's test suite over time. Referring to abstract specs isn't sufficient, either - it's surprising how often specifications and implementations disagree.\ne) I fully agree about well-defined behaviour. A bunch of my work on libav was about undefined behaviour, I've read all of Regehr's posts about it, and spent many hours discussing them with many people. (See point a!)\n. I like @xmre 's solution. \nhttps://gcc.gnu.org/onlinedocs/gcc-3.0.2/cpp_4.html mentions a couple of reasons the original approach wouldn't work (#if ends with #endif, and more seriously, it can't include sizeofs). It's possible to work around those limits, but horrifying: see http://www.pixelbeat.org/programming/gcc/static_assert.html for a rough idea.\n. Merged into kbara-next - https://github.com/kbara/snabbswitch/tree/kbara-next . Note that you'll want to land kbara-next after wingo-next, since the porting.md patch is there.\n. I think a notation dictionary is a good idea, as well as a glossary of acronyms.\n. @lukego I found the documentation on the Intel driver's 'vlan' argument to be extremely valuable yesterday. I'd say I routinely look at the documentation of Snabb code I'm using, if it has any. It gives an idea of what's intended API vs implementation detail, what's intentional or a bug, etc.\n. I read the markdown locally, as well as the source. \"If I can't grep for it, it doesn't exist\".\n. I don't mind reading various markup formats as source, as long as they're not as annoying as XML with a ton of attributes, fwiw.\n. I would nominate @eugeneia , on the grounds that he's shown the most enthusiasm/energy on the topic.\n. Oops, I'd managed to forget that it was his patch; excellent point.\nI'm ok with this change as-is, though I think both you and Luke have made reasonable points; a patch that addresses those is also welcome. Reducing the 'general considerations' to a pointer to the Lua Reference manual would perhaps be an improvement, but I'd be willing to accept the current text. The src.md/md.src changes are fine, and I mainly agree with the point about where implementation details should be documented.\nI suppose there's a case to be made for splitting it into the non-controversial and controversial bits, merging the former onto kbara-next, and hashing out the rest elsewhere or dropping them.\nBut the tl;dr version is that while I don't love every detail of this patch, I'm willing to merge it as-is, as well as to accept further patches from anyone changing the text.\n. If we want particularly cohesive high-level documentation, I think we need to use an approach like that. For API-level documentation, as you said, not every change should pass through such a mechanism.\n. @eugeneia are you willing to remove lines 102-131 as @wingo commented on, or would you prefer me to merge this patch set as-is? I'm willing to merge either.\n. Fair. As you said, the feedback hasn't been particularly actionable, and I think this commit is a net contribution. Merged, and thank you.\n. Small world; I did my bachelor thesis on visualizing Scheme programs in Moose.\n. It wasn't bad, though this was back in 2007 and I think it's changed a lot since then. You can ask me questions, but most of what I did was in Scheme, with just a couple of small shunts in Smalltalk to use the metrics I was calculating, so I wasn't ultra-deep into Moose itself (but my supervisor and his grad students were).\nUsed well, for the right problems (visualizing where complexity is in a system, for instance), Moose is stunning. For crash dump analysis, I honestly don't know. :-)\n. The only problem with asymmetry is with fragmentation - and even then, raw asymmetry by direction is ok, just having different fragments that need to be reassembled routed to different places isn't. Having to do arp/ndp multiple times is suboptimal but not really a big problem.\n. @lukego I totally agree about the N+1 process structure. Having a process handle things like NDP has been on my mind since at least October, and could shake out nicely from the direction this is going.\n. Hmm, yeah, that has potential.\n. I'd be tempted to say \"whichever you prefer of those options\", assuming they all work.\n. Fair. My preference is for the first of those, but that may well be a minority view.\n. All of that sounds reasonable to me, at least in light of the way the discussion on copyright notices in source files has gone in this project specifically. We should avoid ever making the coding styles document too heavy-weight.\n. If the ipsec support doesn't work yet, it's probably good to merge something without it. It can be anywhere from a frustrating waste of time to an active security problem to try to use code that doesn't work yet, as a third party who doesn't necessarily realize that at first.\nThat said, if ipsec is clearly documented as being WIP and is in a program-specific directory, I'm not sure that increasing the integration overhead on you is the right call.\n. LGTM. There should probably be a way for people submitting PRs to figure out who the maintainers are, perhaps a maintainers.md or similar.\n. I've been wondering that as well, though guessing the opposite - that our patches would land on the branches of another maintainer. Both options have quirks. :-)\n. I like that set of heuristics.\n. This looks fine to me. @eugeneia , do you consider it merge-ready?\n. Agreed. Merged, thank you.\n. That's an extremely coarse lock - I think a common use case is to only need one or two cards, and not be doing benchmarking.\n. Fair; it seems worth a try.\n. That does sound like a better place.\n. I'd like to nominate @eugeneia to be the reviewer/upstream on this one, if he's willing.\n. I think 'safe' variants should be used by default, with unsafe variants with 'unsafe' in the name being available when raw speed is necessary. Call it my Haskell bias. I'm in favor of speed at the expensive of safety only when it's really necessary, never as a default.\n. I don't see this as 'hand holding' so much as having non-stupid defaults that we can override when strictly necessary.\n. +1.\n. I'm ok with it as well.\n. I don't see why that would be a strawman. It would be a huge improvement in many cases.\n. In practice, Snabb needs to be run as root to do pretty much anything useful at the moment. A colleague of mine wrote a packet generator on Snabb; even that needed to be root, for incidental reasons.\nPhilosophically, I prefer software to use minimal permissions. Pragmatically, Snabb needs significant hacking to be usable as non-root, and bailing out fast if it's not would make it more usable.\nI'm happy to take patches to make permissions more granular on kbara-next.\n. My biases mirror those of @lukego on this, for what it's worth. Buffer bloat reached ridiculous proportions in some systems.\n. Why did you choose the order 'index, lo, hi, count' for the iterator, when 'count' (the number of observations in that bucket) is usually the relevant information (or so I'd think)?\n. I'd like count, lo, hi. When I implemented a similar iterator, it didn't have an optional prev parameter - with that parameter, our use cases that actually use the index disappear. (That iterator also had more state in the closure, which seems like a reasonable thing to do here.)\n. After those things are addressed, I'm happy with it, though it would be good to hear from @eugeneia about his thoughts on the app/shm/top.lua changes, if any.\n. It is too much machinery, it's just the least bad option in these contexts. ;-)\n. I agree; pinging @wingo .\n. If this is feasible, it would be excellent in many ways. The only downside is that it'll absolutely fully load a whole CPU core, and the budget per packet will be measured in the low tens of nanoseconds. I wonder how often we'll run into thermal throttling of other cores on the same physical CPU...\nNote that we'd then also need to deal with or strip vlans in software, and there has been previous discussion about how that much copying of memory is too costly.\nArchitecturally, I love it. Pragmatically, I'll have to see it to believe it.\n. Thanks. Shall I revert the similar commit in kbara-next itself now?\n. Thanks!\n. Awesome, thanks @lukego , @eugeneia , @dpino , @wingo and @aperezdc !\n. I'd rather not see src/lwaftr  - as the number of projects in the ecosystem that ship with Snabb increase, src/ would get incredibly cluttered. I like having things built on Snabb under directories like src/program/lwaftr . \n. I agree that the app/ and program/ distinction is questionable.\n. I definitely like the idea of having a section on Snabbiness, but I might call it \"The design principles of Snabb\" or \"The philosophy of Snabb\" or similar. I've been seeing more and more complaints these days about technical projects using religious/philosophical/cultural terms like zen, dao/tao, etc, in contexts that are at most tangentially related.\n. I fully agree it's a bit WIP, but I also figure that WIP is much better than totally undocumented - like you said, it's in a realm that's still dark arts to most of us, where the info available is mainly scattered webpages with less than half of this document's tips. Would you be willing to do the formatting cleanup you mentioned? I'd greatly appreciate it. :-)\n. The 512/1024/2048 numbers are ring buffer sizes. Tuning that to deal with latency variations without getting into the realm of buffer bloat is an ongoing dark art investigation, btw.\n. I've done the file rename suggested by @eugeneia . I haven't removed the PDF, as the diagram is useful and I don't think a PNG is a huge amount better (though it would be a small improvement). I also haven't dealt with the general WIP state of this PR. @eugeneia , concretely, what do you still consider blockers to merging this that need to be resolved?\n. Thanks, @mwiget . @eugeneia would you like to see the png in the src/doc/ directory, replacing the current PDF?\n. @eugeneia awesome, thanks; I've done that. Deal.\n. @lukego What's the status of this PR and its associated changes now?\n. It looks good, though the next-hop part needs filling in. I think that's currently the fuzziest part of the whole workflow to me.\n. Is it intentional that this contains both git-workflow.md and git-workflow.src.md?\n. Merged, belatedly!\n. That sounds like a good plan to me.\n. It seems like a fairly common sense Code of Conduct - this is a friendly project, and \"don't troll or throw insults around\" isn't a culture change, happily. If Luke's intention is to actually enforce it if it ever gets violated, and the existing community all buys in, I'm in favour of it.\n. Nice catch. While you're solidifying it, mind adding 3-5 retries, 1 second apart, if it doesn't get answers?\n. It does indeed usually use a 3 space indent.\n. The lwaftr has basically been upstreamed - see https://github.com/SnabbCo/snabbswitch/pull/784 for details.\nI'm happy to be the upstream for this PR, unless @eugeneia wants to be. @eugeneia , preferences/review/thoughts?\n. It's only in next, so we'll hopefully see it in the April release, which will be quite soon. :-)\nI'm quite happy to have the vlan tagger/untagger as separate apps, but I'd like @eugeneia and/or @lukego to chime in and ack/nack it as well.\n. LGTM, thank you.\n@lukego : what's the process for merging this, given that the PR is not based against kbara-next - a new PR, or manual merging to kbara-next and closing this?\n. I'm of two minds on that. I'd like @wingo or I to be the next hop on changes within the lwaftr-related directories, but I also like modifications that involve code spilling out from the lwaftr into Snabb as a whole to go through you/someone outside Igalia.\n. This change set LGTM as is, and I'm happy with it being merged as-is. My comment was with regards to @eugeneia 's musings on who should review such changes. Pragmatically, I think they should be LGTM'd by him and (either Wingo or I), which this now has been, regardless of whether the first or second set is officially the assignee.\nThanks for your work on this, @plajjan and @eugeneia .\n. I see Snabb as using a hybrid between the Linux model and the huge Google monorepo model at this point, to get the kinds of advantages http://danluu.com/monorepo/ mentions.\n. Thank you!\n. I'm happy to merge this as-is; really nice work. I'll wait until tomorrow to merge it in case @lukego or @eugeneia wants to chime in about the breaking API change or any other major details, if @petebristow makes a PR targeting kbara-next; otherwise, I can manually merge it, but I can't (and wouldn't) merge it directly to snabbco:master.\n. @eugeneia oops, indeed. Thanks.\n. @lukego is there anything it'd be helpful for me to do with kbara-next, like merge master into it? I'm still getting used to the upstream Snabb workflow... :-)\n. Excellent. I've run git merge upstream/master on kbara-next and pushed that; I agree that doing that after each release sounds like a good workflow.\n. Interesting. I've done that, it looks to me like it's updated, but the list of commits/etc above doesn't update.\n. Yeah; I couldn't find a way to manually do that either. All the more reason to do this right after releases, eh?\n. There is a trailing whitespace after +      tar xvzf ${test_env} -C ~/.test_env/; is this something people care about on this project?\n. +      export SNABB_PCI0=\"0000:01:00.0\"\n+      export SNABB_PCI_INTEL1=\"0000:01:00.1\"\n+      export SNABB_PCI_INTEL0=\"0000:01:00.0\"\nwill clearly only work on some machines. Is the intent for this to just be running on the snabblab servers? If not, how are people with other PCI IDs expected to use it - local modifications to this file?\n. Aside from those two nits, it looks good to me, with the caveat that I haven't dug deeply into nix yet. :-)\n. @wingo What's your preference for this and #877 and #878: a merge ASAP, or waiting on things like adding int64 support (as you mentioned in https://github.com/snabbco/snabb/issues/952 ).\n. I'm currently not merging these, as per out-of-band discussion with @wingo .\n. LGTM. Thank you for writing this; I think it's really important that Snabb handle these kinds of cases defensively. :-)\n. I am, and I've merged it to kbara-next. Thank you for your patience and your work on this.\n. Closing, as it's been merged into next: https://github.com/snabbco/snabb/pull/922\n. This could be useful if the cause of the drops is side traces, but seems counterproductive for any other cause (such as the machine swapping or being slightly too slow, or other potentially transient timing issues). Perhaps it should be off by default?\n. I see this PR as deprecating the wrapper methods for the length and data fields, plus making various other documentation improvements, rather than as primarily documenting the exact ctype involved. We're in perfect agreement over everything except whether to include the exact struct definition, as far as I can tell.\n. I'd like to retain snabb gc. This cleanup model isn't robust in the face of situations like losing power or the supervisor process getting hit with a kill -9, OOM killer, etc.\n. Snabb gc wasn't ideal for those reasons, true. After talking to my team, I'm ok with removing it. Thanks.\n. Nice work! I've merged it onto kbara-next.\n. @eugeneia Shall I merge your newest change onto kbara-next as well?\n. I'd like to see consensus about what to do about version handling before merging this.\n. LGTM, thanks for this. Merged into kbara-next, PR'd for next.\n. Merged into kbara-next, thanks.\n. This has merge conflicts with the packet documentation in README.md. Once those are fixed, I'm happy to merge this.\n. It has merge conflicts with kbara-next, due to the documentation patch that you also wrote (on packet structure) and which has been upstreamed. I'll resolve the conflict if you'd prefer not to.\n. Thanks for the ping on this; I'll review it now.\n. Nice work, I've PR'd it to next.\n. Both dependencies have been merged into next, but not via the same upstream branch. Any thoughts on the most reasonable way to proceed on this patch set short of waiting for an extra release, @lukego ?\n. Good idea; I've done so.\n. Thank you!\n. This now also includes Luke's \"core.timer: Inhibit debug messages\" patch.\n. Merged  intel1g: canonicalize env var name, pass it through dock.sh #942 \n. This now also includes https://github.com/snabbco/snabb/pull/953 ,  Various statistics counters for NFV apps\n. @eugeneia , are you happy to be the upstream on this?\n. Thank you :)\n. @mwiget shout if it lacks any features you need. :-)\n. I second everything Luke said, including about reducing the number of pointers being a task for another PR, so I've upstreamed this at #982 .\n. I've run git reset --hard HEAD^ and updated kbara-next on my fork; I'll update it on the main repo if/when the protected flag comes off.\n. I'm happy to peddle back. I have done a second push to the kbara fork re-instating the changes, and hadn't updated my kbara-next branch on the main repo. I've re-opened #982 as well. Glad to hear it was a false alarm - it looks like a nice set of changes. :-)\n. Sounds good, especially if there's an easy way for me to run a similar Hydra jobset on kbara-next.\n. Looks awesome overall. The main thing that worries me is the lack of any indication of expected error/noise - when I see two overlapping graphs that diverge a bit, I have no idea how likely it is to be significant or random.\n. Possibly better, but I honestly don't understand what is going on in those graphs without digging deeper. What is 'matrix'? How does 'matrix-next' differ from plain old next, since they're both being measured? What are score and count? What are their units, if applicable?\nIE, if I look under 'by benchmark', l2fwd goes up to nearly 12000, but iperf up to about 1000. I can make (uncertain) guesses as to what that means, but it seems unnecessarily opaque.\nThe success/failure data looks very readable.\n. I still really wish there were units on the axes - I have no idea on initially looking at the first graph if '15' is Gbps (on two cards? bidirectional?) or MPPS (your comment above about iperf vs l2fwd, plus the graph below it, makes me think Gbps, but it would be so, so much easier to just read that. Some of the graphs include words like iperf - they could just say 'iperf - Gbps', though a label on the axis would match my preferences from school and reading research papers).\nHow does it choose how many times to run a test? IE, on the overall summary, it looks like there are probably a lot more runs of master than nfv-test (certainly a lot more zeros, then the other results seem to look similar) - are there? If so, why? Also, doesn't that make the two smooth graphs impossible to compare without scaling? [Edit: actually, it looks like a density graph, so disregard the last question.]\nWhy do the nfv-test scores seem to be so much better than the master scores?\nI continue to really like how clear this format makes failure data. I think it needs a bit of tweaking on conveying the rest of the data, but it's improving!\n. Interesting, I've been seeing it in an almost entirely different set of ways. :-)\n(The following are musings on performance; I see correctness and being able to eliminate test failures as even more important, but that's another post entirely - and the current tooling is a big step in that direction already.)\nTo me, 'line rate' is a first class node in my mental model: \"on reasonable hardware X and card Y, does app network Z achieve line rate with packet size/distribution alpha\"? This leads me to think of comparing two branches or configurations in a handful of ways (I was hashing a bit of this out with someone with a deep stats background a couple weeks ago):\na) Raw averages: which one is better? The graphs so far are good at showing this.\nb) The autocorrelation of the tests results. IE, if we have 70% of tests failing, and we commit a fix that makes that 0% but halves the speed, 'a' (with failures normalized to zero) would show this as an improvement - which it is - but I'd still be extremely interested in knowing about the speed drop to see if there was any reasonable way to get the best of both worlds. Similarly, bimodal performance seems to come up quite often; I want to be able to understand and characterize that, reduce variance, increase the performance of the worst cases, etc. The graphs partly show this; we could also directly apply run lengths (googling for 'run length' and 'autocorrelated' seems to bring up a lot of paywalls.)\nQuestions I'd concretely like to answer include:\n- What is the overhead of adding an app to the app network in the usual case? Are the anecdotal observances that we've made that there comes a point where adding an extra app makes it drop sharply accurate? (This was a problem I saw when adding two apps to do pflang filtering - even when the apps didn't even have an empty filter and weren't invoking any potentially-slow code.)\n- Given an app network, what percent of time is consumed by each app? By functions/traces within an app? Basically, going in the direction of normal profilers. What's slow, and how good are attempts to fix it? (On a tangent, I'd love to be able to profile memory use analogously; I wish I'd looked harder at the Haskell tools for doing this.)\n- Given an app network, what packet size/distribution is the smallest I can achieve line rate with on specific hardware, without drops?\n- What are the worst cases that show up in tests? If a particular workload is ever dropping packets, I want to know about it - if it shows up one time in a thousand (the JIT was unhappy and there was an extra cache miss and ....), but we run 10,000 tests that could trigger it, we'll probably see it, and I ideally want to understand exactly what happened and why.\nA bit more pie-in-the-sky-ily, I could imagine using tooling like this to optimize things like the number of packets per breath to particular hardware, automatically, if that seemed like the kind of parameter that performance was actually sensitive to across different hardware. I see this kind of tooling as being able to give us really rich information about what matters, and whether it always matters in the same way. It's easy to overlook how much tuning constants can matter - and we can take a lot of the guesswork out of it, and empirically see how it works on different hardware.\n. Upstream-PR'd via https://github.com/snabbco/snabb/pull/980 . :-)\n. Re-opened as per Eugenia's comment on #972 about a false alarm.\n. Is there any code which currently needs this?\n. Fair; avoiding ctype pollution for no real loss of indirection seems like a very good tradeoff.\n. I've merged this into kbara-next.\n. Wow, it looks like tooling is getting really exciting. :-)\n. Given that the changes get merged onto next, it sounds like the easy solution to that problem is to simply make sure that kbara-next (and presuambly max-next) frequently merge next into themselves?\n. Oops, should've verified that it actually caused a ctype diversity problem. I'm happy to merge a reversion.\n. Sounds reasonable. I figure it has handy side effects, like Hydra performance evaluations being based on the combination of patches we'll actually merge; usually it won't matter, sometimes it will.\n. Interesting points. As currently implemented, the code this is being used for does not satisfy those invariants. (The use case is hardened reassembly, where a large, predetermined number of packets are getting reassembled in a ctable at the same time, as fragments come in.) The underlying ctable is large enough (around half a gigabyte with the default parameters I've set - but the same applies down to surprisingly small sizes) that hugetlb mmap fails and we fall back to using non-hugetlb mmap.\nSpecifically, the value of the ctable includes an embedded struct packet, which is used for in-place reassembly, then cloned to a normal packet when that is complete. (The logic of how clone_to_memory came to be is documented in https://github.com/Igalia/snabb/issues/393 - but basically, Igalia is using a more complicated struct packet, that needs to have book-keeping done, the right place to do the book-keeping is in packet.lua, and upstreaming the APi makes the fragmentation reassembly code closer to neutral with regards to the struct packet implementation being used).\nWhat would be your preferred approach to dealing with hundreds of thousands of packets at a time, since in the case of fragmentation their lifecycle can be quite long? Would you simply dispense with using a 'struct packet' at all during reassembly, and use an identical struct without the special semantics?\n. Agreed. In that case, let's forget this API ever existed. :-)\n. Excellent. I've gotten rid of it downstream, too. :-)\n. I'd  be perfectly happy with that; it sounds like a useful improvement.\n. The point, as I originally said, is to have a fully valid struct packet at the end - this originated (albeit in a more complex form) from the needs of the lwaftr's memory-bounded fragment reassembly code.\nI don't particularly want to modify new_packet. \nI want to be able to clone a packet to a memory address, and have packet.lua take care of all the details that depend on how struct packet is currently defined (in the lwaftr, we use a more complex struct). I don't particularly like adding an extra parameter to clone, because at that point the arguments are src, (optional dst) rather than dst, src as is found in most of the codebase, which is asking for stupid bugs.\n. Excellent work! A couple of minor notes:\na) Why is there a large blue spike in iperf filter, just under halfway along the x axis? It's quite a lot worse than the baseline data (although the blue density graph as a whole still looks better, barely).\nb) The reddish color of the first table with 100% successes is really distracting, especially since degree of redness is used for failures in the next similar table. Pretty much anything except a reddish color would be better!\n. That is incredibly shiny.\n. It's a rather shiny goal... :-)\n. Gladly.\n. My first knee-jerk response is \"I'll be glad to have a standardized sharing and error handling policy.\" :-)\n. On my dev laptop, this branch is deeply broken.\n% sudo make test | grep ERROR\nERROR     testlog/core.link\nERROR     testlog/core.memory\nERROR     testlog/lib.ipsec.esp\nERROR     testlog/lib.protocol.tcp\nERROR     testlog/lib.protocol.datagram\nERROR     testlog/lib.protocol.ipv4\nERROR     testlog/apps.packet_filter.pcap_filter\nERROR     testlog/apps.ipv6.nd_light\nERROR     testlog/apps.vpn.vpws\nERROR     testlog/apps.lwaftr.ndp\nERROR     testlog/apps.lwaftr.fragmentv4_test\nERROR     testlog/apps.test.match\nERROR     testlog/apps.test.synth\nERROR     testlog/apps.rate_limiter.rate_limiter\nERROR     testlog/apps.socket.unix\nERROR     testlog/apps.socket.raw\nERROR     testlog/apps.vlan.vlan\nERROR     testlog/apps.keyed_ipv6_tunnel.tunnel\nERROR     testlog/program.lwaftr.tests.end-to-end.selftest.sh\nERROR     testlog/program.packetblaster.selftest.sh\nselftest: link\ncore/memory.lua:151: mmap hugetlb\nstack traceback:\n        core/main.lua:142: in function <core/main.lua:140>\n        [C]: in function 'assert'\n        core/memory.lua:151: in function 'allocate_huge_page'\n        core/memory.lua:64: in function 'allocate_hugetlb_chunk'\n        core/memory.lua:47: in function 'allocate_next_chunk'\n        core/memory.lua:30: in function 'dma_alloc'\n        core/packet.lua:69: in function 'new_packet'\n        core/packet.lua:144: in function 'preallocate_step'\n        core/packet.lua:62: in function 'allocate'\n        core/link.lua:107: in function 'selftest'\n        program/snsh/snsh.lua:31: in function <program/snsh/snsh.lua:31>\n        core/lib.lua:424: in function 'dogetopt'\n        program/snsh/snsh.lua:65: in function 'run'\n        core/main.lua:60: in function <core/main.lua:36>\n        [C]: in function 'xpcall'\n        core/main.lua:199: in main chunk\n        [C]: at 0x00451b30\n        [C]: in function 'pcall'\n        core/startup.lua:3: in main chunk\n        [C]: in function 'require'\n        [string \"require \"core.startup\"\"]:1: in main chunk\nEXITCODE: 1\n. I do most of my dev work for Snabb on this laptop, and I've confirmed that current upstream (snabbco) master's tests pass on it.\nGist: https://gist.github.com/kbara/eaa8da0f9113296185746866afdf4268 - It seems to be working(?!), after several days of errors like the above. I wonder if there was somehow something stale in my tree. Sorry for the noise.\n. It seems to have gone away almost entirely. I can sometimes reproduce the symptom in the unix socket test when merged onto the lwaftr branch, though. (It failed, then I ran it with strace and it succeeded, and now I'm struggling to get it to fail again.) It failed several times, consistently, before the strace, and it had the same mmap-related backtrace. Edit: now it's back to failing, but ONLY when I don't use strace.\n. Nothing even slightly related in dmesg. I'm on 4.4.0-45-generic.\n. Several hundred test runs later, that's looking a lot like a fix, thank you. What's the basic idea behind retrying these allocations, by the way?\n. I think this is entirely the wrong approach. Vlan tags are a 12-bit positive integer, and should be represented as such. Messing around with negative 32 bit int comparisons is introducing an unnecessary epicycle into the code.\n. The root cause, as you said, is that we're using a broken htonl/htons; the C type signature specifies 'unsigned' very clearly.\n. Nope, more things could indeed break, but those are the ones that have test cases which show it. The whole Snabb codebase only has 35 ntohl and 50 htonl calls, though, which can be manually audited in a reasonable amount of time. (There's also no guarantee that we don't currently have things broken by the signed definition - I see you've seen https://github.com/snabbco/snabb/pull/1023 ).\nIt's a bit of a pain to change this to be unsigned now, but that pain will only get worse with time. I don't want the signed semantics to hound the project 10 years from now.\n. I believe it's optimized away, but would indeed have to verify that.\nThere are several legitimate ways to define bitops, and the signed version used in LuaJIT is the only reasonable choice given constraints described at http://bitop.luajit.org/ . On the other hand, I think ntohl/htonl should always be unsigned.\n. I see the signed nature of LuaJIT bitops as an implementation detail that shouldn't change ntohl/htonl's external behaviour.\n. This change has non-zero overhead - but none of the difference appears to be floating-point related, and the original version is already more expensive than one might expect.\n```\n% cat signed.lua \nffi = require(\"ffi\") -- not needed, but here to ease comparability.\nbit = require(\"bit\")\nfunction ntohl(x) return bit.bswap(x) end\nfor i = 1,100 do\n   print(ntohl(0xffffffffff))\nend\n```\n```\n% cat unsigned.lua\nffi = require(\"ffi\")\nbit = require(\"bit\")\nfunction ntohl(x) return tonumber(ffi.cast('uint32_t', bit.bswap(x))) end\nfor i=1,100 do\n   print(ntohl(0xffffffffff))\nend\n```\nI compared the machine code from each one.\nluajit -jv -jdump=mbi,dump.signed signed.lua\nluajit -jv -jdump=mbi,dump.unsigned unsigned.lua\nThey're very similar; the instructions other than mov/cmp/jnz are identical in both files - and mov/cmp/jnz are not floating point operations.\n% grep 0b dump.signed| grep -v mov | grep -v cmp | grep -v jnz        \n0bcafe81  cvttsd2si ebx, [rdx]\n0bcaff90  xorps xmm7, xmm7\n0bcaff93  cvtsi2sd xmm7, ebx\n0bcaffa1  sub eax, edx\n0bcaffa6  jb 0x0bca0014 ->1\n0bcaffe1  add edx, +0x38\n0bcafff4  jmp 0x0041f809\n0bcafe4a  addsd xmm7, xmm5\n0bcafe4e  ucomisd xmm6, xmm7\n0bcafe52  jb 0x0bca0014 ->1\n0bcafe61  jmp 0x0bcafe6d\nOddly, xorps is for single-precision floating point point numbers, which I didn't expect from reading http://bitop.luajit.org/semantics.html (which mandates doubles, since 24 bits of precision isn't enough - so, without digging into this, I'll assume that's not related to the bswap/bitops) - but addsd, cvtsi2sd, and ucomisd  work on doubles. Notably, both versions use cvttsd2si to convert a double to an int.\nBut there are significantly more mov/cmp/jnz instructions in the unsigned dump. The signed dump has 74 machine instructions, the unsigned dump has 97. (Nowhere near all of those are ntohl costs; trace2 is purely loop-related overhead, for instance.) The unsigned version contains extra IR:\n0016 >  p32 HREFK  0015  \"tonumber\" @8\n0017 >  fun HLOAD  0016\n0018 >  p32 HREFK  0015  \"ffi\" @35\n0019 >  tab HLOAD  0018\n0020    int FLOAD  0019  tab.hmask\n0021 >  int EQ     0020  +31 \n0022    p32 FLOAD  0019  tab.node\n0023 >  p32 HREFK  0022  \"cast\" @6\n0024 >  fun HLOAD  0023\n...\n0033 >  fun EQ     0024  ffi.cast\n0034 }  cdt CNEWI  +10   -1  \n0035 >  fun EQ     0017  tonumber\nNotably, nothing uses 0034, so the CNEWI is dead rather than alarming.\nI'd be inclined to say that the performance of this version is acceptable though suboptimal if the performance of the previous version was, unless hydra says differently.\n. I've isolated the calls to ntohl, by changing the loop in the test scripts to\nfor i = 1,100 do\n   collectgarbage()\n   a = ntohl(0xffffffff)\n   collectgarbage()\n   print(a)\nend\n(Why collectgarbage()? It's the first never-compiled function in the base library list on http://wiki.luajit.org/NYI ). \nThis results in 3 traces, and the second one is purely related to ntohl (including function lookups/etc), plus setting up the second collectgarbage.\nLooking more at the IR, the local dance helps, as always:\nlocal bswap = bit.bswap\nlocal cast = ffi.cast\nThe dumps (generated in the same way as the previous post, with byte code, IR, and machine code, three traces in each) shrink nicely with the locals dance:\n```\n% wc -l dump*\n  214 dump.lsigned -- with the locals dance\n  247 dump.signed\n240 dump.lunsigned -- with the locals dance\n  290 dump.unsigned\n```\nAdding local tonumber = tonumber helps further:\n% wc -l dump.l2unsigned\n222 dump.l2unsigned\n222, the number of dump lines with everything declared local,  is barely larger than the 214 of the localized signed version (and better than the 247 of the version currently in snabb!).\nIf I isolate the machine code of trace 2, both the signed and unsigned versions are now 50 instructions long - the extra IR is optimized away by this point. They visually look identical, aside from inconsequential details like exact addresses.\nI expect with the local change, this will be a mild performance boost over current upstream.\n. Correction: current upstream already localizes bswap, so I expect performance will be exactly the same.\n. A few details of the above analysis are subtly wrong due to the example code having the optimizer constant-fold away the bswap, as shown by the lack of BSWAP in the IR. The overall point about the generated assembly being identical with and without unsigned semantics continues to hold, though.\nThe code I'm using to avoid consonant folding and maximize isolation of the trace is this:\n```\nffi = require(\"ffi\")\nbit = require(\"bit\")\nio = require(\"io\")\nmath = require(\"math\")\nlocal cast = ffi.cast\nlocal bswap = bit.bswap\nlocal tonumber = tonumber\nlocal write = io.write\nlocal random = math.random\nfunction ntohl(x) return tonumber(cast('uint32_t', bswap(x))) end\nfor i=1,100 do\n   local base_num = math.random(0, 0xffffffff)\n   collectgarbage()\n   a = ntohl(base_num)\n   collectgarbage()\n   write(a)\nend\n```\n. To be even more precise, it's not absolutely identical, once constant folding is eliminated - there's the use of a 64-bit register vs only its lower 32 bits in one instruction, and some reordering that I think is trivial. But it's the same number of basically the same operations.\nSigned:\n0bcafdf6  movd r15d, xmm7\n0bcafdfb  bswap r15d\n0bcafdfe  mov rdi, 0xfffffffb41b1c310\n0bcafe08  cmp rdi, [rbp+0x5d8]\n0bcafe0f  jnz 0x0bca0010        ->0\n0bcafe15  cmp dword [rbx+0x10], +0x00\n0bcafe19  jnz 0x0bca0010        ->0\n0bcafe1f  xorps xmm7, xmm7 ; zero out xmm7\n0bcafe22  cvtsi2sd xmm7, r15d  ; Note the use of r15d here; it's just the lower 32 bits\n0bcafe27  movsd [rbp+0x5d0], xmm7\nUnsigned:\n0bcafd9d  movd r15d, xmm7\n0bcafda2  bswap r15d\n0bcafda5  xorps xmm7, xmm7 ; zero out xmm7\n0bcafda8  cvtsi2sd xmm7, r15 ; note the lack of a d - this is all 32 bits\n0bcafdad  mov rdi, 0xfffffffb41568e60\n0bcafdb7  cmp rdi, [rbp+0x5d8]\n0bcafdbe  jnz 0x0bca0010        ->0\n0bcafdc4  cmp dword [rbx+0x10], +0x00\n0bcafdc8  jnz 0x0bca0010        ->0\n0bcafdce  movsd [rbp+0x5d0], xmm7\nHow to see what it's doing in practice:\n1) Get a dump:\nluajit -jdump=mbi,dump.unsigned -Ohotloop=1 unsigned.lua\n2) Use GDB:\ngdb --args luajit -Ohotloop=1 unsigned.lua\nIn gdb, run the following (based on the assembly for the unsigned version, above), interpreting # to be a comment (although it isn't gdb syntax):\n(gdb) start # necessary for hbreak to work on x86_64\n(gdb) hbreak *0bcafd9d\nSet up gdb to show some useful info - r15, r15d, and xmm7 determined from the asm snippet for unsigned at the start of this comment:\n(gdb) display /i $pc\n1: x/i $pc\n=> 0x403e90 <main>:     push   %r12\n(gdb) display $r15\n2: $r15 = 0\n(gdb) display $r15d\n3: $r15d = 0\n(gdb) display $xmm7\n4: $xmm7 = {v4_float = {0, 0, 0, 0}, v2_double = {0, 0}, v16_int8 = {\n    0 <repeats 16 times>}, v8_int16 = {0, 0, 0, 0, 0, 0, 0, 0}, v4_int32 = {0, 0, 0, 0}, \n  v2_int64 = {0, 0}, uint128 = 0}\nContinue running it:\n(gdb) c\nBreakpoint 2, 0x000000000bcafd9d in ?? ()\n1: x/i $pc\n=> 0xbcafd9d:   movd   %xmm7,%r15d\n2: $r15 = 1073777184\n3: $r15d = 1073777184\n4: $xmm7 = {v4_float = {-6.61220694, 184, 0, 0}, v2_double = {6755402676148019, 0}, \n  v16_int8 = {51, -105, -45, -64, 0, 0, 56, 67, 0, 0, 0, 0, 0, 0, 0, 0}, v8_int16 = {\n    -26829, -16173, 0, 17208, 0, 0, 0, 0}, v4_int32 = {-1059875021, 1127743488, 0, 0}, \n  v2_int64 = {4843621402472060723, 0}, uint128 = 4843621402472060723}\nChanging values is easy (the following has no real impact, because $r15 is about to be set when the next instruction runs)\n(gdb) set $r15d = 0xffffffff\n(gdb) print $r15\n$1 = 4294967295\n(gdb) print $r15d\n$2 = -1\nGo forward one assembly instruction:\n(gdb) ni\n0x000000000bcafda2 in ?? ()\n1: x/i $pc\n=> 0xbcafda2:   bswap  %r15d\n2: $r15 = 3235092275\n3: $r15d = -1059875021\n4: $xmm7 = {v4_float = {-6.61220694, 184, 0, 0}, v2_double = {6755402676148019, 0}, \n  v16_int8 = {51, -105, -45, -64, 0, 0, 56, 67, 0, 0, 0, 0, 0, 0, 0, 0}, v8_int16 = {\n    -26829, -16173, 0, 17208, 0, 0, 0, 0}, v4_int32 = {-1059875021, 1127743488, 0, 0}, \n  v2_int64 = {4843621402472060723, 0}, uint128 = 4843621402472060723}\nRecognize $r15d as the first value in v4_int32.\n...\nTyping return in gdb on an empty line repeats the last command (ni). The cvtsi2sd, the one opcode that meaningfully diverges, is about to be called:\n(gdb) \n0x000000000bcafda8 in ?? ()\n1: x/i $pc\n=> 0xbcafda8:   cvtsi2sd %r15,%xmm7\n2: $r15 = 865588160\n3: $r15d = 865588160\n4: $xmm7 = {v4_float = {0, 0, 0, 0}, v2_double = {0, 0}, v16_int8 = {\n    0 <repeats 16 times>}, v8_int16 = {0, 0, 0, 0, 0, 0, 0, 0}, v4_int32 = {0, 0, 0, 0}, \n  v2_int64 = {0, 0}, uint128 = 0}\n(gdb) \n0x000000000bcafdad in ?? ()\n1: x/i $pc\n=> 0xbcafdad:   movabs $0xfffffffb40008168,%rdi\n2: $r15 = 865588160\n3: $r15d = 865588160\n4: $xmm7 = {v4_float = {-3.68934881e+19, 25.2245655, 0, 0}, v2_double = {865588160, 0}, \n  v16_int8 = {0, 0, 0, -32, -23, -53, -55, 65, 0, 0, 0, 0, 0, 0, 0, 0}, v8_int16 = {0, \n    -8192, -13335, 16841, 0, 0, 0, 0}, v4_int32 = {-536870912, 1103743977, 0, 0}, \n  v2_int64 = {4740544288130072576, 0}, uint128 = 4740544288130072576}\nAfter calling it, you can see 865588160 in v2_double.\n. Audit of ntohl/htonl signedness safety change in the Snabb source tree.\nSummary: most code using the signed version appears to believe it's unsigned (every single backing type with defined signedness in a struct in unsigned). Code where the signedness matters uses one of two other implementations(!), both unsigned. All in-tree uses seem to be fine; git blame on uses of ntohl in code without significant in-tree use suggest that @alexandergall, @eugenia, and @capr might want to make sure no out-of-tree code they have breaks.\n\nThe analysis.\n- I'm excluding documentation or 'local dance' lines, as well as .c or .h files and the defintions (rather than uses) of ntohl/htonl.\n- apps/lwaftr/binding_table.lua:      return map:lookup(ffi.C.ntohl(ipv4_as_uint), port)\n  - This was sidestepping the core/lib.lua version, perhaps because of the signedness issue.\n- apps/lwaftr/fragmentv6.lua:   return ntohl(rd32(pkt.data + frag_id_start))\n  - This is safe; it's the return value of a function (get_frag_id) which in turn is only compared to other results of that function.\n- apps/lwaftr/lwaftr.lua:   return ntohl(rd32(get_ipv4_src_address_ptr(ptr)))\n- apps/lwaftr/lwaftr.lua:   return ntohl(rd32(get_ipv4_dst_address_ptr(ptr)))\n  - These two lines are used to get src/dst IPv4 addresses and are used in two ways\n    - To generate ICMP (this goes through apps/lwaftr/lwaftr.lua, and then into a uint8_t[4] header field in lib/protocol/ipv4.lua, and is fine)\n    - to 'enqueue' encapsulation to IPv6 within the lwAFTR; these are put in softwire_key_t's uint32_t ipv4 field before they can cause trouble.\n- apps/test/lwaftr.lua:      local ipdst = C.ntohl(rd32(ipv4_hdr.dst_ip))\n  - Another C.ntohl....\n- core/lib.lua:ntohl = htonl\n- core/lib.lua:     [32] = { ntoh = ntohl, hton = htonl }\n  - These two just imply that htonl also needs to be audited\n- lib/protocol/esp.lua:      return(ntohl(h.spi))\n- lib/protocol/esp.lua:      return(ntohl(h.seq_no))\n  - Both of these clearly intend an unsigned ntohl; the backing header contains uint32_t spi; uint32_t seq_no;. The only in-tree user is lib/ipsec/esp.lua, which in turn only seems to be setting unsigned backing fields. Heads up to @eugeneia in case there are any out of tree uses he knows of.\n- lib/protocol/gre.lua:      return ntohl(self:header().key)\n  - This clearly intends to have a signed ntohl, as the is a uint32_t in the data structure. Not used elsewhere in the snabb tree. @alexandergall should probably get a heads-up about this one, as the person who seems most likely to need to check whether it breaks any out-of-tree code.\n- lib/protocol/keyed_ipv6_tunnel.lua:      return ntohl(h.session_id)\n  - Once again, the backing data type is a uint32_t. No substantial in-tree uses. Another heads up to @alexandergall.\n- lib/protocol/tcp.lua:   return ntohl(h.seq)\n- lib/protocol/tcp.lua:   return ntohl(h.ack)\n  - Backed by uint32_t    seq; uint32_t    ack;; all in-tree uses are in this file are fine with either signedness.\n- program/lisper/lisper.lua:   return ntohl(p.spi)\n- program/lisper/lisper.lua:   local sid = ntohl(p.session_id)\n- program/lisper/lisper.lua:      ntohl(p.session_id),\n  - The first two key into local tables and should be ok with either signedness, the last is in a print. Heads up to @capr just in case.\n\n\napps/keyed_ipv6_tunnel/tunnel.lua:      psession[0] = lib.htonl(conf.local_session)\npsession_id_ctype = ffi.typeof(\"uint32_t*\") - It's instantly cast to unsigned.\napps/lwaftr/conf_parser.lua:   return ffi.C.htonl(ffi.cast('uint32_t*', addr)[0])\nYet another use of ffi.C's signed version \napps/lwaftr/fragmentv6.lua:   wr32(base + 4,  htonl(frag_id))\nIt's writing a raw bit pattern, signedness doesn't matter.\napps/lwaftr/icmp.lua:local htons, htonl = lwutil.htons, lwutil.htonl\napps/lwaftr/lwaftr.lua:local htons, htonl = lwutil.htons, lwutil.htonl\napps/lwaftr/lwheader.lua:local htons, htonl = lwutil.htons, lwutil.htonl\nYes, these three use a third definition of htonl, explicitly unsigned...\napps/test/lwaftr.lua:      ipdst = C.htonl(ipdst + self.b4_ipv4_offset)\nYet another use of ffi.C's signed version \napps/vlan/vlan.lua:   return htonl(bit.bor(bit.lshift(dot1q_tpid, 16), vid))\nIn the end, the use boils down to cast(uint32_ptr_t, payload)[0] = tag - with an explicit cast to unsigned.\nlib/ipsec/aes_128_gcm.lua:   h.padding = htonl(0x1)\nHigh bit isn't set, signedness doesn't matter.\nlib/ipsec/aes_128_gcm.lua:   h.spi = htonl(spi)\nlib/ipsec/aes_128_gcm.lua:   h.seq_no[0] = htonl(seq_h)\nlib/ipsec/aes_128_gcm.lua:   h.seq_no[1] = htonl(seq_l)\nThe backing types are unsigned, so these three are implicitly cast.\nlib/protocol/esp.lua:      h.spi = htonl(spi)\nlib/protocol/esp.lua:      h.seq_no = htonl(seq_no)\nThe backing types are unsigned, so these two are implicitly cast.\nlib/protocol/gre.lua:      self:header().key = htonl(key)\nThe backing types is unsigned, so this is implicitly cast.\nlib/protocol/keyed_ipv6_tunnel.lua:      h.session_id = htonl(id)\nThe backing types is unsigned, so this is implicitly cast.\nlib/protocol/tcp.lua:      h.seq = htonl(seq)\nlib/protocol/tcp.lua:      h.ack = htonl(ack)\nThe backing types are unsigned, so these two are implicitly cast.\n. @eugeneia I believe this is ready to be merged. It has ~no performance impact (same number of assembly instructions, effectively identical beyond the use of a 64 bit vs 32 bit subset of a 64 bit register in a single one of them, slightly reordered), and does not appear to break any in-tree code. (In tree code that actually cares about signedness seems to use other implementations, mainly the ffi.C one).\n. There is a semantic difference between this patch and function htonl(b) return band(0xFFFFFFFF, bswap(b)) end, which has almost the same broken semantics as the version without the band.\n\nThat would still return a signed int, rather than an unsigned one, because luaJIT bitops are all signed by definition. IE, we would still have had #1023 and awkwardness with dealing with a signed ntohl. https://github.com/Igalia/snabb/issues/453 would still have been a bug, for the same underlying reason - the expression would have been 32 bit rather than sign-extended 64, but we'd still be comparing a signed 32 bit int to an unsigned 32 bit int and having the comparison fail.\nIn short:\n```\n\na = 0xf0000000\nprint(bit.band(0xffffffff, a) == a)\nfalse\n```\n\nAs we're using a LuaJIT with IEEE doubles as the backing numeric type, an unsigned 32 bit int will never be equal to a negative signed one.\nDemonstration that the backing type is double:\n```\n\nz = 18014398509481985\nprint(z)\n1.8014398509482e+16\nprint(z == 18014398509481984)\ntrue\n```\n\nSecondarily, it also might not do exactly what you expect if you give ctypes to an ntohl implemented purely with bitops:\nprint(ffi.sizeof(bit.band(0xffffffff, ffi.cast(\"uint32_t\", 1))))\n8\n. Looks like it is. Interesting.\nBy the way, what's 'id', and why is 'score' on y axis when it's usually on the x axis in the other graphs?\n. What's the plan with the rx-related changes? I'd like to be able to feed other things to next via kbara-next. :-)\n. Closing, as per discussion with @eugeneia \n. This is looking nice! Minor nit: your ' -- No headroom for the shift; re-set the headroom to the default.' comment is slightly stale with the latest change. It's now no headroom or a shift by an odd number of bytes.\n. Looks worse on DPDK + configuration test 2.1.0 noind, which looks bimodal. Probably noise.\n. Obligatory: http://www.xkcd.com/1725/\n. I think the current presentation is ok - we just need to bear in mind that if a couple of results look like outliers, they might well be noise, especially if they don't persist between runs.\nI see a lot of value in visualizing fine-grained data, seeing where bimodal results occur, etc. No matter how we display it, we have to think about it, test whether it's reproducible at times, etc. Even boiling all of the output down to one number and no graphs wouldn't shield us from misinterpreting significance.\n. It looks like a solid win on iperf results. On the three 'l2fwd by configuration' graphs, it looks like it may be slightly worse; I can't tell eyeballing it if that is significant.\nInterestingly, 2.1.0 noind is still bimodal.\n. I'd be willing to upstream it, if @lukego is happy with an Igalian upstreaming for an Igalian.\n. It changes  src/lib/virtio/virtq_driver.lua \n. Looks like @lukego has thumbed up it. I'm ok with that workflow, with the caveat that I think we really need to do our best to have test coverage of it in the next ~2 weeks, before the next release is cut.\n. I've reviewed it. LGTM, @wingo . As per out of band discussion, please merge this to wingo-next, since you'll need it there for the lwaftr merge and there's no point putting it in two maintainer-next branches. :)\n. LGTM. I'm assuming this should also go into wingo-next; if you'd rather it go into kbara-next, give me a shout.\n. The general person to do that is @lukego . \nThe usual process these days:\na) Someone submits a PR\nb) One of the three upstreams (hi fellow upstream!) assigns that to themselves\nc) The upstream merges the patch(es) into theirname-next, and Luke takes it from there\n. Yep, was just wanting to say \"in practice, that always seems to be Luke, by policy\" :)\n. Tons of nits, but this looks awesome overall. Thank you.\n. LGTM aside from not explicitly saying that hyperthreading causes packet loss. I think the \"best results and lower latency\" wording is too vague, at least by itself.\n. Awesome. :-)\n. I use markdown docs a bit (ie, the lwaftr ones on counters, troubleshooting, etc), but not as an API reference. I tend to view them locally in vim.\n. Frankly, I have no opinion on this in general, was just trying to throw in a data point. I'd be reluctant to lobby for Andy to write more documentation in the absence of stronger opinions.\n. I haven't spotted anything else that @eugeneia hasn't.\n. The three new commits look ok to me.\n. LGTM, thanks for all your work on this.\n. LGTM. I assume you'll wingo-next this one too.\n. Hi Christian,\nGreat content. Would you be willing to do the following?\n1) Fork the Snabb repo onto your private repo\n2) Check out the 'next' branch\n3) git checkout -b cgraf_troubleshooting_readme (or another branch name of your choice)\n4) apply your changes to your new branch, and push that to your fork\n5) open a PR (targeting snabb next, not snabb master), from your new branch\nIf that sounds like too much bother, tell me and I'll treat this as an issue and try to apply your diff by hand.\n. @wingo I'm happy to put this on kbara-next as soon as the tests pass.\n. I've opened https://github.com/snabbco/snabb/pull/1070 to upstream it\n. Gladly; if we don't do it for the trivial stuff, we'll start slipping on doing it where it matters. :)\n. Sorry, wrong root tree; this was meant to target Igalia/lwaftr.. Plausibly.. I don't have ~/.test_env or SNABB_TEST_FIXTURES - or any SNABB* environment variables at all (I set things like SNABB_PCI0 on individual command lines when I need them, rather than always having them in my environment). For reference, that's true both on my dev laptop, and on Igalia's snabb1 dev machine with real network cards.. PRs to snabb target 'next' by convention, rather than 'master', although it doesn't make an absolutely huge difference - we manually merge them into max-net/kbara-next/wingo-next rather than next anyhow. I'm happy to be the upstream on this one, unless @eugeneia explicitly says he wants to take it over. :-). That sounds amazing.. No worries. I think that's ok. I'm tagging @takikawa so that he knows as well.. Thank you.. I'd love to see an aarch64 port of Snabb, @jialiu02  . This PR looks like it's just getting started. Feel free to grab code/ideas from https://github.com/Igalia/snabb/tree/lwaftr_aarch64 , which is exploratory abandonware but got a bit farther along. I was doing the port on a Pine64 machine (which wouldn't be a useful production target, but is more than sufficient for getting the layers above the network cards working, in my opinion).\nI share @lukego 's concerns about memory models - as http://preshing.com/20120930/weak-vs-strong-memory-models/ says, arm has a weaker memory model than x86, which I expect will lead to some interesting fun in the driver ports. The amount of dynasm in Snabb is also a minor hurdle for porting.\nI reckon the same network cards would make sense - ie, there are linux kernel drivers for the Intel 82599 on aarch64 (and strictness of ordering is an issue - see https://www.spinics.net/lists/arm-kernel/msg568505.html for instance). Qemu's a painfully slow way to do aarch64 work. If this were to become a first-class port, I think the way to go would be a dedicated aarch64 machine with physical network cards in it, whether it's with other Snabb servers or in someone's cloud.. @lukego an unreasonable fondness of the platform, and an optimism about its future. No concrete short-term commercial reasons.\nI'd be in favour of merging a solid, complete port, but not WIP ones; the costs you mention are very real.. I like the concept. Would you like it merged to kbara-next?. Sounds good!. Thank you. @aperezdc has agreed to change the ljndpi license to Apache 2.0 from MIT as well.\nIf anyone in the Snabb community has any reservations about Snabb containing a wrapper for an LGPLv3 library (ndpi), please speak up quickly. :-). A 10-minute glance at the LuaJIT source tree suggests to me that it probably does, but I haven't absolutely verified it.\nhttp://luajit.org/ext_ffi_semantics.html mentions the alignment attributes LuaJIT accepts. The mechanism looks to be CTA_ALIGN (defined in lj_ctype.h, set in lj_cparse.c). Then, in lib_ffi.c, LJLIB_CF(ffi_new) calls lj_cdata_newx (which is in lj_cdata.c), which in turn calls lj_cdata_newv, which in turn says /* Allocate variable-sized or specially aligned C data object. */\nTL;DR: I haven't tested it, but the LuaJIT code certainly makes it look plausible.\n. I don't like that this has a field called 'count' that refers to the whole type, while the iterator refers to 'count' relative to a particular bucket. Would you be willing to rename the struct field?\n. Thanks. It gave me a headache yesterday. ;-)\n. Any totally compelling reason not to make 509 a named constant? I realize non-4096-byte-pages aren't a huge priority, but I like to avoid exposed magic numbers when feasible. (And things like 507 = 509 - 2 are basic constant folding.)\n. I'd say not crucial, though it doesn't hurt. Thanks.\n. I'm happy to be the upstream for the nix tributary.\n. I'd prefer to document that a packet is guaranteed to have a uint8_t data[pcket.max_payload] field and a uint16_t length field, keeping the actual structure opaque, given the work that's currently being done on potentially redefining it (ie, @wingo 's work on headroom for virtualization).\n. I'd prefer not to be documenting it that explicitly when the odds of it changing so soon are so high.\n. I've merged it, although I disagree with that reasoning. I think documenting the concrete type right now is a mistake, but I agree that the patch is enough of a win overall that it's worth merging despite that.\n. In future patches, I'd like comments or defined constants explaining numbers like that 46.\n. Why is this a multiple of kilobits?\n. I really don't approve of hardcoding PCI addresses, but I don't consider that a blocker for this patch, as it was also previously hardcoded.\n. Hm. What do you see as the main advantage of doing it that way?\n. Ok; the latter is a good enough reason for me. :-)\n. Excellent catch. I'm so used to dealing with the results of whichevertype_t = ffi.typeof(....) that I entirely overlooked that, but it does indeed make the tests pass locally. Uses of it will still need to be manually checked, though, since this is a breaking semantic change.\n. Pedantically, only 12 of those 16 bits are strictly the tag, but it doesn't really matter for this test case.\n. It might be worth mentioning tuning the size of the reassembly buffers?\n. It would be useful to have Hydra re-testing this overhead routinely - @tekniko. \n. (Notably, I wonder whether it really kills performance that badly on on-a-stick mode)\n. I've heard contradictory things on the benefits of 512 vs 1024 - the previous version strongly stumped for 1024. It might be worth explicitly preserving the information that some people have found 1024 to be a win compared to 512.\n. I'd s/efficiency/power efficiency/\n. Might be worth mentioning that having hyperthreading enabled can cause packet loss.\n. I seem to recall Turbo Boost backfiring slightly when you switch back from it, as well. Does your recollection match mine?\n. Is it intentional that the new document has nothing on ifInDiscards anymore?\n. Want to comment on what we've empirically observed with enabling/disabling busywait?\n. I think it's worth documenting that on older supported processors, having IOMMU on can (IIRC) lead to 30%-100% packet loss - that's pretty dramatic, and was hard-won non-obvious knowledge that it would be sad to have people have to troubleshoot from scratch again.\n. Thank you for writing --cpu!\n. Is it worth documenting cpulistaffinity+numactl, in addition to the grovel around in /sys approach?\n. The new doc doesn't mention lspci at all; it seems to me like it perhaps should.\n. I suspect it's also grasping at straws, but I'm a little reluctant to not document that it was done and what was tentatively found, even if just in passing - ie, a line like \"While we don't have robust evidence that changing the size of the ring buffer is a win, some users have reported better results with 1024 than 512. YMMV.\"\n. https://github.com/Igalia/snabb/blob/lwaftr/src/program/lwaftr/doc/README.configuration.md\nlwaftr.conf example includes:\nmax_fragments_per_reassembly_packet = 1,\nmax_ipv6_reassembly_packets = 10,\nmax_ipv4_reassembly_packets = 10,\nThe counters docs, in turn, show how to use lwaftr query -l to get the actual memory use for the ctable buffers.\nI'm ok with it not getting documented yet, but it is in our main lwaftr branch.\n. You're not misinterpreting. I just lazily think that being able to ^F for 'packet loss' should be able to find the most common red flags. :-)\n. I have a very fuzzy memory of switching back and forth between turbo boost and non-turbo boost mode having some overhead/latency, but perhaps I'm confounding it with hyperthreading switching costs.\n. Hmm. Probably just \"it was in the previous doc\" bias. README.running.md and README.troubleshooting.md mention it, and as you said it's basic, so I think that's good enough.\n. Okdokie.\n. I think it's worth explicitly saying \"having IOMMU on can cause severe (up to 100%) packet loss on some older supported processors.\" It's highly non-obvious, and the current documentation doesn't reflect just how severe the problems can be.\n. Nope, hence fishing to see if you did with that wording. :-)\n. Ok.\n. Having thought a bit more, I'd really like this to explicitly say that we have observed hyperthreading causing packet loss.\n. Still true. :-)\n(Not a blocker, no action required for this PR).\n. This seems like a spurious new line. (Not a blocker)\n. ",
    "liuran2011": "commit by liuran from china. \n. ",
    "noj": "Btw, turning on -Wextra would've caught it :-)\n. ",
    "sm101": "Thanks.\n. ",
    "mwiget": "A simple but effective temporary solution is the use of tasket on the current shell. Processes spawned from that shell inherit the cpu mask:\n```\n[mwiget@st:~]$ taskset -cp 14 $$\npid 21198's current affinity list: 0,4-8,12-15\npid 21198's new affinity list: 14\n[mwiget@st:~]$ bash\n[mwiget@st:~]$ taskset -cp $$\npid 21213's current affinity list: 14\n```\nI learned this the hard way when I realised that Docker containers ignore the kernels isolcpu settings:\n```\n[mwiget@st:~]$ taskset -cp $$\npid 21220's current affinity list: 0,4-8,12-15\n[mwiget@st:~]$ docker run -ti --name ubuntu ubuntu:14.04.4\nroot@63c70c480477:/# taskset -cp $$\npid 1's current affinity list: 0-15\nroot@63c70c480477:/# exit\nexit\n```\nSo my solution to the problem is setting the cpu list on the provisioning shell launched within the container, plus set the same taskset in /root/.bashrc, so debugging shells launched into a running container also adhere to the requested cpu affinity:\ntaskset -p $AFFINITY_MASK $$\necho \"taskset -p $AFFINITY_MASK \\$\\$\" >> /root/.bashrc\nBTW Docker has an option to set \"--cpuset-cpus=\" with \"docker run ...\", but doing so also prevents the Container to use any cpu outside that list. In my snabbvmx case, I wanted to have qemu and anything but snabb adhere to a specific cpu list, while assigning snabb another cpu. \n. This sounds indeed complicated. Have you looked at https://code.google.com/p/gerrit/ ? We use this for opencontrail and allows people to reject or accept, after CI gave the green light. Once the predefined gatekeepers all accepted, the changes merge automatically into master.\n. sounds good to me ;-).  Thanks for the detailed feedback. Looking forward to any suggestions to improve and correct my write-up.\nFrom: Luke Gorrie notifications@github.com<mailto:notifications@github.com>\nReply-To: SnabbCo/snabbswitch reply@reply.github.com<mailto:reply@reply.github.com>\nDate: Monday, July 6, 2015 at 9:50 AM\nTo: SnabbCo/snabbswitch snabbswitch@noreply.github.com<mailto:snabbswitch@noreply.github.com>\nCc: Marcel Wiget mwiget@juniper.net<mailto:mwiget@juniper.net>\nSubject: Re: [snabbswitch] Add snabbnfv getting-started.md page (#546)\nWe are consciously adopting the Linux kernel workflow instead of the Gerrit one.\nFortunately the short version \"Submit PR to master, respond to feedback, relax once it is merge onto any upstream branch\" is all that's needed for contributing improvements.\nI spelled out the long version mostly for the sake of getting me and Max onto the same page since this will be the first time a documentation addition is handled under this workflow. Maybe this was gratuitously confusing to do on the PR containing the change :-). This tends to happen since the project is young and a bunch of stuff needs to be worked out as we go along.\nThe backstory on the workflow can be seen in e.g. #482https://github.com/SnabbCo/snabbswitch/issues/482. This was definitely a situation where many people have different strong preferences and it's not possible to make everybody happy :-).\n\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/pull/546#issuecomment-118761839.\n. I took a closer look at the bogus idx value of 407011329. Its the same every time I ran into the issue. \n407011329 = 0x18428001 and usually I saw set_vring_num() being called with idx set to 0 or 1. Could this be an issue where an int16 being passed into an int64? As a quick-and-dirty test, I made the following code change and voila, this actually helped:\n```\ndiff --git a/src/lib/virtio/net_device.lua b/src/lib/virtio/net_device.lua\nindex 58b040b..a476c9d 100644\n--- a/src/lib/virtio/net_device.lua\n+++ b/src/lib/virtio/net_device.lua\n@@ -401,6 +401,10 @@ function VirtioNetDevice:set_vring_num(idx, num)\n       error(\"vring_num should be power of 2\")\n    end\n\nprint(string.format(\"TODO virtq idx=%d n=%d\", idx, n))\nif idx > 256 then\nidx = idx % 256\nend\n    self.virtq[idx].vring_num = n\n    -- update the curent virtq pairs\n    self.virtq_pairs = math.max(self.virtq_pairs, math.floor(idx/2)+1)\n```\n\nSnabb doesn't crash anymore, all while passing traffic (though low speed, just ping floods) ...\n. got a new crash after about an hour or so, this time in a different place.\nlib/virtio/net_device.lua:357: mapping to host address failedcdata<void *>: 0xffff8801677f7320\nstack traceback:\n    core/main.lua:118: in function <core/main.lua:116>\n    [C]: in function 'error'\n    lib/virtio/net_device.lua:357: in function 'map_from_guest'\n    lib/virtio/net_device.lua:129: in function 'packet_start'\n    lib/virtio/virtq.lua:67: in function 'get_buffers'\n    lib/virtio/net_device.lua:122: in function 'receive_packets_from_vm'\n    lib/virtio/net_device.lua:108: in function 'poll_vring_receive'\n    apps/vhost/vhost_user.lua:76: in function 'method'\n    core/app.lua:76: in function 'with_restart'\n    core/app.lua:281: in function 'breathe'\n    core/app.lua:234: in function 'main'\n    program/snabbnfv/traffic/traffic.lua:85: in function 'traffic'\n    program/snabbnfv/traffic/traffic.lua:61: in function 'run'\n    program/snabbnfv/snabbnfv.lua:15: in function 'run'\n    core/main.lua:56: in function <core/main.lua:32>\n    [C]: in function 'xpcall'\n    core/main.lua:125: in main chunk\n    [C]: at 0x0044d9e0\n    [C]: in function 'pcall'\n    core/startup.lua:1: in main chunk\n    [C]: in function 'require'\n    [string \"require \"core.startup\"\"]:1: in main chunk\n. thanks Luke !\nI ran the original code of snabb (without my quick-and-dirty modulo patch) under strace for over 30 minutes, no crash. I run the same snabb binary without strace and get a crash in less than a minute ;-). I'll let it run under strace, hoping to trigger the issue at some point. But strace clearly influences the result somehow...\n. Well, after an hour or so, I got a traceback, but for \"vhost_user: unrecognized request: 0\":\nrecvmsg(8, 0x7ffcd7f26760, MSG_DONTWAIT|MSG_WAITALL) = -1 EAGAIN (Resource temporarily unavailable)\nrecvmsg(9, 0x7ffcd7f26760, MSG_DONTWAIT|MSG_WAITALL) = -1 EAGAIN (Resource temporarily unavailable)\nrecvmsg(10, 0x7ffcd7f26760, MSG_DONTWAIT|MSG_WAITALL) = -1 EAGAIN (Resource temporarily unavailable)\nrecvmsg(11, 0x7ffcd7f26760, MSG_DONTWAIT|MSG_WAITALL) = -1 EAGAIN (Resource temporarily unavailable)\nrecvmsg(12, 0x7ffcd7f26760, MSG_DONTWAIT|MSG_WAITALL) = -1 EAGAIN (Resource temporarily unavailable)\nload: time: 1.00s  fps: 3,484     fpGbps: 0.021 fpb: 0   bpp: 749  sleep: 100 us\nrecvmsg(7, 0x7ffcd7f26850, MSG_DONTWAIT|MSG_WAITALL) = -1 EAGAIN (Resource temporarily unavailable)\nrecvmsg(8, {msg_name(0)=NULL, msg_iov(1)=[{\"\\r\\0\\0\\0\\1\\0\\0\\0\\10\\0\\0\\0\", 12}], msg_controllen=24, {cmsg_len=20, cmsg_level=SOL_SOCKET, cmsg_type=SCM_RIGHTS, {1021}}, msg_flags=0}, MSG_DONTWAIT|MSG_WAITALL) = 12\nrecvmsg(8, {msg_name(0)=NULL, msg_iov(1)=[{\"\\r\\0\\0\\0\\1\\0\\0\\0\\10\\0\\0\\0\", 12}], msg_controllen=24, {cmsg_len=20, cmsg_level=SOL_SOCKET, cmsg_type=SCM_RIGHTS, {1022}}, msg_flags=0}, MSG_DONTWAIT|MSG_WAITALL) = 12\nrecvmsg(8, {msg_name(0)=NULL, msg_iov(1)=[{\"\\r\\0\\0\\0\\1\\0\\0\\0\\10\\0\\0\\0\", 12}], msg_controllen=24, {cmsg_len=20, cmsg_level=SOL_SOCKET, cmsg_type=SCM_RIGHTS, {1023}}, msg_flags=0}, MSG_DONTWAIT|MSG_WAITALL) = 12\nrecvmsg(8, {msg_name(0)=NULL, msg_iov(1)=[{\"\\r\\0\\0\\0\\1\\0\\0\\0\\10\\0\\0\\0\", 12}], msg_controllen=0, msg_flags=MSG_CTRUNC}, MSG_DONTWAIT|MSG_WAITALL) = 12\nrecvmsg(9, 0x7ffcd7f26850, MSG_DONTWAIT|MSG_WAITALL) = -1 EAGAIN (Resource temporarily unavailable)\nrecvmsg(10, 0x7ffcd7f26850, MSG_DONTWAIT|MSG_WAITALL) = -1 EAGAIN (Resource temporarily unavailable)\nrecvmsg(11, 0x7ffcd7f26850, MSG_DONTWAIT|MSG_WAITALL) = -1 EAGAIN (Resource temporarily unavailable)\nrecvmsg(12, 0x7ffcd7f26850, MSG_DONTWAIT|MSG_WAITALL) = -1 EAGAIN (Resource temporarily unavailable)\nrecvmsg(7, 0x7ffcd7f26760, MSG_DONTWAIT|MSG_WAITALL) = -1 EAGAIN (Resource temporarily unavailable)\nrecvmsg(8, {msg_name(0)=NULL, msg_iov(1)=[{\"\\0\\0\\0\\0\\0\\0\\0\\0\", 12}], msg_controllen=0, msg_flags=0}, MSG_DONTWAIT|MSG_WAITALL) = 8\napps/vhost/vhost_user.lua:161: vhost_user: unrecognized request: 0\nstack traceback:\n        core/main.lua:118: in function <core/main.lua:116>\n        [C]: in function 'error'\n        apps/vhost/vhost_user.lua:161: in function 'method'\n        apps/vhost/vhost_user.lua:142: in function 'process_qemu_requests'\n        apps/vhost/vhost_user.lua:39: in function 'fn'\n        core/timer.lua:33: in function 'call_timers'\n        core/timer.lua:42: in function 'run_to_time'\n        core/timer.lua:23: in function 'run'\n        core/app.lua:235: in function 'main'\n        program/snabbnfv/traffic/traffic.lua:85: in function 'traffic'\n        program/snabbnfv/traffic/traffic.lua:61: in function 'run'\n        program/snabbnfv/snabbnfv.lua:15: in function 'run'\n        core/main.lua:56: in function <core/main.lua:32>\n        [C]: in function 'xpcall'\n        core/main.lua:125: in main chunk\n        [C]: at 0x0044d9e0\n        [C]: in function 'pcall'\n        core/startup.lua:1: in main chunk\n        [C]: in function 'require'\n        [string \"require \"core.startup\"\"]:1: in main chunk\nIsn't the message passed to snabb too short?\nmsg_iov(1)=[{\"\\0\\0\\0\\0\\0\\0\\0\\0\", 12}]\nI only see 8 bytes, whereas in other recvmsg calls i always get 12... \nIs that an issue within the guest VM?\n. Great suggestion! Thanks. I added some basic reporting to the main program lwaftrbench and see the difference in L2 cache hit/miss, but I can't get L3 cache counters. Somehow I'm using pmu.setup() wrong:\npmu.setup({\n      \"mem_load_uops_retired.l._hit\",\n      \"mem_load_uops_retired.l._miss\",\n      \"mem_load_uops_retired.l3_hit\",\n      \"mem_load_uops_retired.l3_miss\"})\nDoing a test run with single packet versus 50k packets, I clearly see the expected L2 cache misses at the beginning for the large packet capture. Would be great to get L3 counters. \n```\n $ sudo ./r2.sh\nmodel name  : Intel(R) Xeon(R) CPU D-1540 @ 2.00GHz\n=========================================================\nRunning snabb-port-200k.cfg\nPCAP files v6:b4-icmp-request-1502.pcap v4:aftr-icmp-reply-0098.pcap ...\ntaskset -c 0 ../../../snabb lwaftrbench -D 5 snabb-port-200k.cfg b4-icmp-request-1502.pcap aftr-icmp-reply-0098.pcap\nConfiguration file: snabb-port-200k.cfg\nrunning for 5 seconds\noffset=0 len=6 -> psmask=0xfc00\n200000 bindings parsed, using 15633 kBytes\nipv4_to_ipv6:  138 bpp 2.749 MPPS, 3.035 Gbps.\nipv6_to_ipv4: 1460 bpp 2.749 MPPS, 32.105 Gbps.\nipv4_to_ipv6:  138 bpp 2.778 MPPS, 3.067 Gbps.\nipv6_to_ipv4: 1460 bpp 2.778 MPPS, 32.451 Gbps.\nipv4_to_ipv6:  138 bpp 2.774 MPPS, 3.062 Gbps.\nipv6_to_ipv4: 1460 bpp 2.774 MPPS, 32.397 Gbps.\nipv4_to_ipv6:  138 bpp 2.780 MPPS, 3.069 Gbps.\nipv6_to_ipv4: 1460 bpp 2.780 MPPS, 32.467 Gbps.\nEVENT                                             TOTAL\ncycles                                   12,999,707,304\nref_cycles                                            0\ninstructions                             19,565,368,417\nmem_load_uops_retired.l1_hit              4,763,231,439\nmem_load_uops_retired.l1_miss               266,653,176\nmem_load_uops_retired.l2_hit                104,591,830\nmem_load_uops_retired.l2_miss               162,061,346\n\n=========================================================\nRunning snabb-port-200k.cfg\nPCAP files v6:b4-icmp-request-1502.pcap v4:lwaftr-ipv4-50k.pcap ...\ntaskset -c 0 ../../../snabb lwaftrbench -D 5 snabb-port-200k.cfg b4-icmp-request-1502.pcap lwaftr-ipv4-50k.pcap\nConfiguration file: snabb-port-200k.cfg\nrunning for 5 seconds\noffset=0 len=6 -> psmask=0xfc00\n200000 bindings parsed, using 15633 kBytes\nipv4_to_ipv6: 1496 bpp 0.009 MPPS, 0.110 Gbps.\nipv6_to_ipv4: 1460 bpp 0.009 MPPS, 0.108 Gbps.\nipv4_to_ipv6: 1496 bpp 0.008 MPPS, 0.090 Gbps.\nipv6_to_ipv4: 1460 bpp 0.008 MPPS, 0.088 Gbps.\nipv4_to_ipv6: 1496 bpp 0.005 MPPS, 0.055 Gbps.\nipv6_to_ipv4: 1460 bpp 0.005 MPPS, 0.053 Gbps.\nipv4_to_ipv6: 1496 bpp 0.003 MPPS, 0.040 Gbps.\nipv6_to_ipv4: 1460 bpp 0.003 MPPS, 0.039 Gbps.\nEVENT                                             TOTAL\ncycles                                   13,177,342,212\nref_cycles                                            0\ninstructions                             13,404,706,258\nmem_load_uops_retired.l1_hit              1,804,519,341\nmem_load_uops_retired.l1_miss               267,961,568\nmem_load_uops_retired.l2_hit                  2,129,026\nmem_load_uops_retired.l2_miss               265,832,542\n\n```\nNeed to investigate further how pmu finds matching counters. The CPU is D-1540 has L3 cache:\n$ lstopo|grep L3\n  Socket L#0 + L3 L#0 (12MB)\n. Great catch! Thanks. Indeed, when I specify just the L2 and L3 hit/miss counters, I get what I'm currently interested in. The main question I'm trying to answer for myself is the following: Given random traffic, at what point is the lookup impacting processes on other cores. I hope to get an indication from the L3 hit/miss ratio, though I'm aware from the limited reading I've done so far, I'm not sure if such a conclusion can be made ;-).\nAnyway, just run some tests again with the following 4 counters:\npmu.setup({\n      \"mem_load_uops_retired.l2.hit\",\n      \"mem_load_uops_retired.l2.miss\",\n      \"mem_load_uops_retired.l3_hit\",\n      \"mem_load_uops_retired.l3_miss\"})\nThis reports the following, first running 4k unique packets thru  hash table with 200k binding entries, then with 50k unique packets:\n```\nsnabb lwaftrbench -D 30 snabb-port-200k.cfg b4-icmp-request-0138.pcap lwaftr-ipv4-4k.pcap\n...\nipv4_to_ipv6: 1496 bpp 1.410 MPPS, 16.880 Gbps.\nipv6_to_ipv4:   98 bpp 1.410 MPPS, 1.106 Gbps.\nEVENT                                             TOTAL\ncycles                                   73,734,875,518\nref_cycles                                            0\ninstructions                             74,380,017,968\nmem_load_uops_retired.l2_hit                424,474,692\nmem_load_uops_retired.l2_miss             1,340,172,842\nmem_load_uops_retired.l3_hit                940,464,091\nmem_load_uops_retired.l3_miss                         0\nsnabb lwaftrbench -D 30 snabb-port-200k.cfg b4-icmp-request-0138.pcap lwaftr-ipv4-50k.pcap\n...\nipv4_to_ipv6: 1496 bpp 1.109 MPPS, 13.276 Gbps.\nipv6_to_ipv4:   98 bpp 1.109 MPPS, 0.870 Gbps.\nEVENT                                             TOTAL\ncycles                                   73,840,447,011\nref_cycles                                            0\ninstructions                             65,387,567,486\nmem_load_uops_retired.l2_hit                182,810,566\nmem_load_uops_retired.l2_miss             1,276,121,242\nmem_load_uops_retired.l3_hit                894,374,142\nmem_load_uops_retired.l3_miss                         0\n```\nThe l3_miss counter doesn't seem to get updated. But the L2 hit/miss ratio clearly shows the impact of the larger packet base.\n. I do get correct l3_miss counters on a E5-2690v3. Anyhow, I need to split the packet injection part away from the app under test, because reading and repeating 50k packets is eating up a major part of the L3 cache. Even with an empty binding table I do get lots of L3 cache miss when loading 50k packets:\nsnabb lwaftrbench -D 30 snabb-port-50k.cfg b4-icmp-request-0138.pcap lwaftr-ipv4-50k.pcap\nConfiguration file: snabb-port-50k.cfg\nrunning for 30 seconds\noffset=0 len=6 -> psmask=0xfc00\n50000 bindings parsed, using 6365 kBytes\n. . .\nipv4_to_ipv6: 1496 bpp 1.072 MPPS, 12.825 Gbps.\nipv6_to_ipv4:   98 bpp 1.072 MPPS, 0.840 Gbps.\nEVENT                                             TOTAL\ncycles                                   104,719,843,882\nref_cycles                                   64,216,854\ninstructions                             61,424,683,495\nmem_load_uops_retired.l2_hit                248,320,590\nmem_load_uops_retired.l2_miss             1,276,794,789\nmem_load_uops_retired.l3_hit                778,389,472\nmem_load_uops_retired.l3_miss               498,403,630\nWill try injecting the packets from another server over loopback cables. That will eliminate the PCAP file based packet injection.\n. Good news! Built a small app pcibench, which takes 2 PCAP files and 2 PCI addresses, then sends the content in a loop over both 10GE ports. Then I modified my lwaftrbench app to accept PCI addresses instead of PCAP files. Connecting both apps via 2 loopback cables helped me really test the lwaftr encap/decap function and its pretty good ;-).\nRunning the actual lwaftr app with a binding table of 200k entries:\n./runlwaftrbench.sh snabb-port-200k.cfg\nmodel name  : Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz\n../../../snabb lwaftrbench -D nil snabb-port-200k.cfg 0000:05:00.1 0000:0b:00.1\nConfiguration file: snabb-port-200k.cfg\noffset=0 len=6 -> psmask=0xfc00\n200000 bindings parsed, using 14971 kBytes\nand the pcibench with the actual B4 and AFTR packets, of which the latter contains 50k distinct packets:\n./runbenchpci-50k.sh\n=========================================================\nmodel name  : Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz\nPCAP files v6:b4-icmp-request-0138.pcap v4:lwaftr-ipv4-50k.pcap ...\ntaskset -c 0 ../../../snabb pcibench -D 30 b4-icmp-request-0138.pcap 0000:05:00.0 lwaftr-ipv4-50k.pcap 0000:0b:00.0\npci1_to_pci2:   98 bpp 0.012 MPPS, 0.009 Gbps.\npci2_to_pci1: 1496 bpp 0.012 MPPS, 0.142 Gbps.\npci1_to_pci2:   98 bpp 0.138 MPPS, 0.108 Gbps.\npci2_to_pci1: 1496 bpp 0.137 MPPS, 1.634 Gbps.\npci1_to_pci2:   98 bpp 2.216 MPPS, 1.737 Gbps.\npci2_to_pci1: 1496 bpp 0.822 MPPS, 9.842 Gbps.\npci1_to_pci2:   98 bpp 3.188 MPPS, 2.499 Gbps.\npci2_to_pci1: 1496 bpp 0.822 MPPS, 9.842 Gbps.\npci1_to_pci2:   98 bpp 3.251 MPPS, 2.549 Gbps.\npci2_to_pci1: 1496 bpp 0.822 MPPS, 9.842 Gbps.\npci1_to_pci2:   98 bpp 3.299 MPPS, 2.587 Gbps.\npci2_to_pci1: 1496 bpp 0.822 MPPS, 9.842 Gbps.\npci1_to_pci2:   98 bpp 3.296 MPPS, 2.584 Gbps.\npci2_to_pci1: 1496 bpp 0.822 MPPS, 9.842 Gbps.\npci1_to_pci2:   98 bpp 3.305 MPPS, 2.591 Gbps.\npci2_to_pci1: 1496 bpp 0.822 MPPS, 9.842 Gbps.\npci1_to_pci2:   98 bpp 3.309 MPPS, 2.594 Gbps.\npci2_to_pci1: 1496 bpp 0.822 MPPS, 9.842 Gbps.\npci1_to_pci2:   98 bpp 3.305 MPPS, 2.591 Gbps.\npci2_to_pci1: 1496 bpp 0.822 MPPS, 9.842 Gbps.\npci1_to_pci2:   98 bpp 3.306 MPPS, 2.592 Gbps.\npci2_to_pci1: 1496 bpp 0.822 MPPS, 9.842 Gbps.\npci1_to_pci2:   98 bpp 3.288 MPPS, 2.578 Gbps.\npci2_to_pci1: 1496 bpp 0.822 MPPS, 9.842 Gbps.\n^C\nSo there is still some ramp up for 2 seconds, but then it settles at line rate for the large packets and over 3 MPPS in the other. Need to look now at L3 cache usage and increase the binding table to really push it harder. \n. I got the answers I was looking for. My initial test setup was flawed by reading large PCAP files into memory. Thanks also for the resolution on how to use pmu.lua. Very cool tool! \n. Absolutely. That's how I got introduced to Snabb recently myself. The following getting started guide explains the steps to connect a VM via virtio to Intel 10GE NIC's managed by snabb:  https://github.com/SnabbCo/snabbswitch/blob/master/src/program/snabbnfv/doc/getting-started.md\n. I'd love to see a standardised interface between Snabb for configuration and operational data. Today every Snabb app deals with its own configuration file or command arguments and exports operational/statistical data as a tree of SHM values to the file system. \nI'm less convinced about having that interface be turned into something that communicates via NETCONF directly. Snabb first and foremost handles physical network interfaces and does so really well. Unless you build a \"single interface Snabb compute node\", you most likely end up with a compute node running many Snabb instances plus probably some virtual network functions on top of these Snabb instances. Shall now every Snabb instance be addressed via its own (ssh channel) based NETCONF interface? If yes, you'll need to assign an IP address plus SSH plus login credentials and bake all of this into the Snabb daemon. \nWhy not add some hierarchy and have \"something\" manage the Snabb instances that combined build a service, like L2TPv3 or lw4o6. There is more to this than just basic configuration. What about announcing network prefixes serviced by Snabb via BGP? Is this ask now to also embed BGP into Snabb? Probably not. \nNow one could argue, that a central OSS is the master of the universe and capable of micro-managing every single Snabb instances in its universe. Personally I don't believe in such a scenario and rather follow the old rule of \"divide and conquer\". To put the proposed hierarchical model to the test, I've been busy adding Snabb as network layer under a virtual router (Juniper vMX) and be managed from the vMX's control plane. Northbound, the vMX exposes its function via a single Netconf/YANG interface. Some in-line network functions are handled directly within Snabb, like lw4o6, the rest (mainly control plane traffic) is handled by the vMX (like ARP, IPv6 NDP, BGP, BFD etc):\n(expanding on @plajjan diagram):\n+-------+                    +-------+                +-----+\n| Snabb |  <-- cfg files --  |  vMX  | -- NETCONF --> | NMS |\n+-------+                 |  +-------+                +-----+\n+-------+                 |\n| Snabb |+ <-- cfg files -+\n+-------+|\n +-------+\nWould be great to have the transport  between Snabb and its Netconf/YANG entity (vMX in my case) standardised and expandable with statistical data beyond basic app links statistics. Snabb is highly efficient int what it does today. \nBut baking control plane connectivity and functions into Snabb seems like breaking some of its design principles (ASAIK) too. To me, that's like asking Intel's DPDK based interface driver to become a full fledged routing device. \n. @andywingo, fully agree and like your statement very much: \"Anyway, summary: no socket needed in the base case. All you need is the filesystem tree(s) and the schema.\"\nclean and simple. \n. I'm currently using VMDq to create two logical interfaces connecting to either side if  Igalia's snabb-lwaftr. Initially just for testing over a single loopback, but its actually very useful to place the IPv4 and/or IPv6 side into a VLAN or run the app \"on a stick\" using virtual MAC addresses. \nTaking this idea a step further, why not launch multiple lwaftr apps via VMDq over the same interface and optionally in the same VLAN (or untagged). If you read this far, you probably shout that this is exactly what snabbnfv does. And yes, it does, but wouldn't it be great to have separate snabb processes sharing a physical port via VMDq? That would give us/me an immediate boost by running multiple instances of lwaftr and hit them by flows spread across them. @wingo already solved the issue of sharing binding tables with many processes via shared memory mapped file. Suddenly just doubling the performance of Snabb NFV feels so yesterday ;-). \n. Tried to run selftest via SFP+ fibre loopback cable, but the card had a hard time bringing the link up, even when managed by the kernel/ethtool. No twinax cable at hand. But I got more info's about the card, in case someone else has one to give the test a shot: Dell X520-DP 10GB DA/SFP+[http://accessories.euro.dell.com/sna/productdetail.aspx?c=uk&l=en&s=dhs&cs=ukdhs1&sku=540-11141] \nDespite not having been able to run it in loopback mode, we successfully used both ports heavily during a full week of test (lwaftr), connected via fibre to a router. \n. No rush merging. I'm sure I'll be back in Greece  soon, carrying a twin ax loop back cable to retest.\nMarcel\nSent from Outlook Mobilehttps://aka.ms/qtex0l\nOn Tue, Feb 9, 2016 at 8:46 AM -0800, \"Max Rottenkolber\" notifications@github.com<mailto:notifications@github.com> wrote:\n@lukegohttps://github.com/lukego I would merge this since the card does work for lwaftr but I feel like we need to track that we need to document that the card is not supported by the CI, maybe a source comment would be enough. What do you think?\n\nReply to this email directly or view it on GitHubhttps://github.com/SnabbCo/snabbswitch/pull/731#issuecomment-181950956.\n. @wingo I like the idea as well. While one could chain a RateLimiter after the  Repeater app, it burns avoidable CPU cycles by first creating the packets, then dropping them mid flight. Had to combine RateLimiter also in my crude snabb based lwaftr flow generator in order to have enough CPU cycles to count incoming traffic.\n. Is there an API to retrieve the hardware mac of the intel nic via the intel10g app? I couldn't find one. Particularly in single-function app, its desirable to actually use the hardware mac instead of making up one. I know the workaround of loading the ixgbe driver and retrieve the mac via ethtool/ifconfig or read from the filesystem directly, but wonder why there isn't a Snabb function to do so directly.\n@lukego : on single-function NIC and disabling promiscuous mode, yes, I do see a use case. It has to do with VLAN filtering. If Snabb is connected to an Ethernet switch port, that port might offer additional VLANs, resulting in Snabb requiring to handle (throw away)  untagged, single or double tagged packets. VMDq helps here. \n. @alexandergall: cool and very useful, thanks!\n. any reason you run it on cpu 0? That cpu gets the most work from the kernel by handling all interrupts.\n. Thanks! I guess I can close this pull request now.\n. While I agree with @kbara about not cluttering src/, I struggle with having part of a solution under src/apps and src/program. For my snabbvmx work i currently use src/program/snabbvmx and src/apps/nh_fwd, though the latter also contains generator.lua v4v6.lua apps, which is less than ideal. Ideally I'd like to have everything under program/snabbvmx and only move things into src/apps when an app really becomes useful to other programs. \nWhat about a subdirectory for elements that are delegated to a team, like lwaftr by Igalia? Thats also the place I could put snabbvmx. In these directories, the structure is up to the team owning the directory, where I could again have program and apps under  src/contrib/snabbvmx/\n. On 16G VMs, yes, they seem to work but getting used to in tools like 'htop', because the virtual memory reported tends to be really large (slightly larger than whats assigned to the VM) for Snabb and highlighted in red, even on servers with ample amount of RAM.\n. Fully agree on WIP. Its basically a structured brain dump. Agree we don't need the PDF in there. Maybe there is a good link to an external web site about it? The graphical view in PDF (or PNG) is very useful.\nOn ring buffer size: indeed, tuning is needed, but mostly one should refrain from thinking larger numbers are better. I've seen oscillating effects when setting it too high (> 2048). \n. Providing the lstopo file in png format:\n\nProviding the same \n. I think I fixed all open issues so far, removing the [wip]. Happy to make more changes of course.\n. @plajjan I wanted to add some more test cases and I have a change on reporting packet loss in % instead of absolute numbers. So nothing major. I should take the WIP away again.\n. @plajjan, @kbara and @dpino: didn't even notice and my .vimrc did technically make the mistake ;-), but I fixed it now. \n. @kbara hitting Snabb with heavy traffic more often than not required jit.flush() to get to acceptable speed. Thats particularly true when Snabb is launched without any traffic, followed by heavy traffic, which is typical for traffic attracted to Snabb as a result of a routing protocol like BGP (routes announced once Snabb is operational). So I vote for having this enabled at all times. Not having it enabled requires an operator to manually restart Snabb.\n. Blasting maximum packet rate into a Snabb process is the best case for throughput. Gradually increasing the load, or worse, changing the packets and content, makes it more vulnerable for requiring jit.flush(). At least that where our findings, where we initially \"fixed\" the throughput on a running stream of packets by simply killing and restarting Snabb. So in that sense, the way Snabb NFV automated test is easier on snabb than what LWAFTR does. \n. @wingo: my experience with IMIX traffic at line rate however clearly shows the need for jit.flush(), even when all the performance settings are correct. We observed it when changing the binding table in-flight, e.g. from 1M to 2M entries, which btw impacted forwarding by less than 300ms. \nAnother use case where we always saw jit.flash() being triggered is with Snabb running idle for some (lets say 30 seconds), then hit with IMIX based line rate traffic, generating an equal amount of return traffic over the same interface. Without a jit.flush(), performance was typically less than 40% of line rate.\nI'm surprised no other application has run into this though ...\n. @kbara: super useful! Thanks and yes, I also have my bag of shell scripts for this task, which I'll happily retire ;-)\n. Hi @wingo. Great work! \nWhat is the exact content of the yang module specified via -m in snabb config add -m ietf-softwire? Do you parse an actual yang schema, extracted from the rfc?\nSpecifically on softwire, the draft doesn't have all the parameters to fully provision lwaftr. I ended up augmenting and deviate the schema (WIP):\nhttps://github.com/mwiget/vmxlwaftr/blob/master/yang/jnx-softwire.yang\nhttps://github.com/mwiget/vmxlwaftr/blob/master/yang/jnx-softwire-dev.yang\nThe resulting config looks then like this, covering two snabb instances (WIP), with the second one using a global binding table read in from a file:\nhttps://github.com/mwiget/vmxlwaftr/blob/master/tests/lwaftr1.txt#L158-L247\nIts pretty much a match now to what lwaftr requires in the config file, augmented by mtu's, vlan's and IPv4 and IPv6 addresses plus cache refresh timers used by snabbvmx.\n. @lukego on your anecdote. I've suffered also from 82599 cards suddenly not working properly and only a cold boot resolved it. In my case I typically got packets from the NIC shifted by 2 bytes, rendering them unusable. Strange that a full reset doesn't solve this. Probably experienced this 4 or 5 times so far, though its a while since it last happened. \n. @lukego: wow! Very impressive. \n. Makes a lot of sense. In fact, I created a rudimentary version of collecting support data here: \nhttps://github.com/Juniper/vmx-docker-lwaftr/blob/master/SUPPORT-INFO.md\nShell script: https://github.com/Juniper/vmx-docker-lwaftr/blob/master/tests/collect-support-infos.sh, where I collect all the relevant configuration files from within the container I run snabb in and save that into a tar file. \nWhile I didn't collect shm data, I do add the actual snabb binary plus config files and a shell script to run that binary standalone.  This allows the developer to rerun the very same application, thanks to how snabb binaries are built. \n. \nThat should work as described. I happen to run a very similar tiny snabb app, connecting a pci port to a unix socket / linux interface:\nKfrees/s        freeGbytes/s    breaths/s\n0.00            0.00            153000\nMin breath (us) Average         Maximum\n4.73            6.30            39.59\nLinks (rx/tx/txdrop in Mpps)    rx      tx      rxGb    txGb    txdrop\nint.tx -> nic.rx                0.00    0.00    0.00    0.00    0.00\nnic.tx -> int.rx                0.00    0.00    0.00    0.00    0.00\nWhile the MPPS don\u2019t show , I have connectivity.\nThe important elements of my lua program:\nif pciaddr then\n      local device_info = pci.device_info(pciaddr)\n      if vlan then\n         print(string.format(\"vlan set to %d\", vlan))\n      end\n      if device_info then\n         config.app(c, \"nic\", require(device_info.driver).driver,\n         {pciaddr = pciaddr, vmdq = false, macaddr = src_mac, mtu = 9500})\n         input, output = \"nic.rx\", \"nic.tx\"\n      else\n         fatal((\"Couldn't find device info for PCI or tap device %s\"):format(pciaddr))\n      end\n   end\nif int_interface then\n      config.app(c, \"int\", raw.RawSocket, int_interface)\n      config.link(c, output .. \" -> int.rx\")\n      config.link(c, \"int.tx -> \" .. input)\n   end\nSo two area\u2019s to watch for:\n1) are you disabling vmdq as I do to get any mac address pass?\n2) did you disable iommu_intel to pt or off? (on wouldn\u2019t work)\nMarcel\nFrom: Vincenzo Maffione notifications@github.com\nReply-To: snabbco/snabb reply@reply.github.com\nDate: Wednesday 12 April 2017 at 18:40\nTo: snabbco/snabb snabb@noreply.github.com\nCc: Subscribed subscribed@noreply.github.com\nSubject: [snabbco/snabb] Unable to connect VhostUser port to Intel82599 port (#1128)\nHi,\nI have two dual port Intel 82599, so 4 interfaces, and two cables connecting the two pairs.\nlspci | grep -i network\n01:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)\n01:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)\n03:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)\n03:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)\n09:00.0 Ethernet controller: Intel Corporation 82574L Gigabit Network Connection\nI've written this simple AppEngine (vm2nic.lua), that is meant to forward traffic between a VhostUser port and an 82599 port.\nmodule(..., package.seeall)\nlocal vhostuser = require(\"apps.vhost.vhost_user\")\nlocal ixgbe = require(\"apps.intel.intel_app\")\nfunction run (parameters)\nif not (#parameters == 2) then\n print(\"Usage: vm2nic <unix-socket> <nic-pci-addr>\")\n\n  main.exit(1)\n\nend\nlocal usock = parameters[1]\nlocal bdf = parameters[2]\nlocal c = config.new()\nconfig.app(c, \"vh\", vhostuser.VhostUser, {socket_path=usock,is_server=false})\nconfig.app(c, \"nic\", ixgbe.Intel82599, {pciaddr=bdf})\nconfig.link(c, \"vh.tx -> nic.rx\")\nconfig.link(c, \"nic.tx -> vh.rx\")\nengine.configure(c)\n-- engine.busywait = true\n-- engine.Hz = 20000\nengine.main({report = {showlinks=true, showapps=true}})\nend\nWhen I run sudo src/snabb vm2nic /var/run/vm10-10.socket 0000:01:00.1, snabb correctly connects to the VM (VhostUser port)\nGet features 0x18428001\nVIRTIO_F_ANY_LAYOUT VIRTIO_NET_F_MQ VIRTIO_NET_F_CTRL_VQ VIRTIO_NET_F_MRG_RXBUF VIRTIO_RING_F_INDIRECT_DESC VIRTIO_NET_F_CSUM\nvhost_user: Read cached features (0x18028001) from /tmp/vhost_featuresvar_runvm10-10.socket\nSet features 0x18028001\nVIRTIO_F_ANY_LAYOUT VIRTIO_NET_F_CTRL_VQ VIRTIO_NET_F_MRG_RXBUF VIRTIO_RING_F_INDIRECT_DESC VIRTIO_NET_F_CSUM\nrxavail = 0 rxused = 0\nrxavail = 0 rxused = 0\nbut when I transmit traffic on the virtio-net interface inside the VM, I don't see any traffic transmitted on the 82599 port identified by 0000:01:00.1 (e.g. I don't see traffic on the other end).\nThis is confirmed by snabb top, which shows all the packets are being dropped by snabb:\nKfrees/s        freeGbytes/s    breaths/s\n3198.68         0.19            25100\nMin breath (us) Average         Maximum\n1.24            13.82           74.10\nLinks (rx/tx/txdrop in Mpps)    rx      tx      rxGb    txGb    txdrop\nnic.tx -> vh.rx                 0.00    0.00    0.00    0.00    0.00\nvh.tx -> nic.rx                 0.00    0.00    0.00    0.00    3.20\nWhat is wrong here? Am I missing something? Should I use apps.intel_mp.intel_mp? If yes, how, and what is the difference?\nThanks\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/snabbco/snabb/issues/1128, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AC94sLcf0RJgOUT-NDJIV5M1W1wEw8oKks5rvP6bgaJpZM4M7rbw.\n. Indeed excellent work! Hope to take it for a spin some time for lw4o6.. It also fails on Ubuntu 16.04.3 LTS with Linux 4.10.0-xx kernel.. vlan=0 is valid and used to priority tag the traffic. If you google it, you'll find more info's about it:\nhttps://www.cisco.com/c/en/us/td/docs/switches/connectedgrid/cg-switch-sw-master/software/configuration/guide/vlan0/b_vlan_0.html#con_1055352\nSo a switch will still treat the traffic as untagged, but it processes the 802.1p bits.\n. I plan to join too. I guess it is too late to submit a talk, but happy to just tag along and mingle.. Turns out, the shell script based conversion from xpath to influxdb rest format works equally well and without any change when using another shema: snabb-softwire-v2:\nsnabb config get-state -s snabb-softwire-v2 -f xpath $HOSTNAME / \\\n      | tr '/' ' ' | tr '[|]' ' ' | awk '{print $4 \",\" $2 \",\" $1 \"=\" $3 \" value=\" $5}' \\\n      |grep -v 'value=$' \\\n      | curl -i -XPOST 'http://influxdb:8086/write?db=lwaftr' --data-binary @-\n. Adding an example output using snabb-softwire-v2 schema. The device key contains the interface name, making it very easy to select individual interfaces or stack them in Grafana.\n\n```\n~ # snabb config get-state -s snabb-softwire-v2 -f xpath lwaftr1 / | tr '/' ' ' | tr '[|]' ' ' | awk '{print $4 \",\" $2 \",\" $1 \"=\" $3 \" value=\" $5}' \\\n\n  |grep -v 'value=$'\n\ndrop-all-ipv4-iface-bytes,device=eth4,instance=softwire-state value=14284\ndrop-all-ipv4-iface-packets,device=eth4,instance=softwire-state value=98\ndrop-all-ipv6-iface-bytes,device=eth4,instance=softwire-state value=10552\ndrop-all-ipv6-iface-packets,device=eth4,instance=softwire-state value=88\ndrop-bad-checksum-icmpv4-bytes,device=eth4,instance=softwire-state value=0\ndrop-bad-checksum-icmpv4-packets,device=eth4,instance=softwire-state value=0\ndrop-in-by-policy-icmpv4-bytes,device=eth4,instance=softwire-state value=0\ndrop-in-by-policy-icmpv4-packets,device=eth4,instance=softwire-state value=0\ndrop-in-by-policy-icmpv6-bytes,device=eth4,instance=softwire-state value=0\ndrop-in-by-policy-icmpv6-packets,device=eth4,instance=softwire-state value=0\ndrop-in-by-rfc7596-icmpv4-bytes,device=eth4,instance=softwire-state value=0\ndrop-in-by-rfc7596-icmpv4-packets,device=eth4,instance=softwire-state value=0\ndrop-ipv4-frag-disabled,device=eth4,instance=softwire-state value=0\ndrop-ipv4-frag-invalid-reassembly,device=eth4,instance=softwire-state value=0\ndrop-ipv4-frag-random-evicted,device=eth4,instance=softwire-state value=0\ndrop-ipv6-frag-disabled,device=eth4,instance=softwire-state value=0\ndrop-ipv6-frag-invalid-reassembly,device=eth4,instance=softwire-state value=0\ndrop-ipv6-frag-random-evicted,device=eth4,instance=softwire-state value=0\ndrop-misplaced-not-ipv4-bytes,device=eth4,instance=softwire-state value=14284\ndrop-misplaced-not-ipv4-packets,device=eth4,instance=softwire-state value=98\ndrop-misplaced-not-ipv6-bytes,device=eth4,instance=softwire-state value=9782\ndrop-misplaced-not-ipv6-packets,device=eth4,instance=softwire-state value=81\ndrop-no-dest-softwire-ipv4-bytes,device=eth4,instance=softwire-state value=0\ndrop-no-dest-softwire-ipv4-packets,device=eth4,instance=softwire-state value=0\ndrop-no-source-softwire-ipv6-bytes,device=eth4,instance=softwire-state value=0\ndrop-no-source-softwire-ipv6-packets,device=eth4,instance=softwire-state value=0\ndrop-out-by-policy-icmpv4-packets,device=eth4,instance=softwire-state value=0\ndrop-out-by-policy-icmpv6-packets,device=eth4,instance=softwire-state value=0\ndrop-over-mtu-but-dont-fragment-ipv4-bytes,device=eth4,instance=softwire-state value=0\ndrop-over-mtu-but-dont-fragment-ipv4-packets,device=eth4,instance=softwire-state value=0\ndrop-over-rate-limit-icmpv6-bytes,device=eth4,instance=softwire-state value=0\ndrop-over-rate-limit-icmpv6-packets,device=eth4,instance=softwire-state value=0\ndrop-over-time-but-not-hop-limit-icmpv6-bytes,device=eth4,instance=softwire-state value=0\ndrop-over-time-but-not-hop-limit-icmpv6-packets,device=eth4,instance=softwire-state value=0\ndrop-too-big-type-but-not-code-icmpv6-bytes,device=eth4,instance=softwire-state value=0\ndrop-too-big-type-but-not-code-icmpv6-packets,device=eth4,instance=softwire-state value=0\ndrop-ttl-zero-ipv4-bytes,device=eth4,instance=softwire-state value=0\ndrop-ttl-zero-ipv4-packets,device=eth4,instance=softwire-state value=0\ndrop-unknown-protocol-icmpv6-bytes,device=eth4,instance=softwire-state value=0\ndrop-unknown-protocol-icmpv6-packets,device=eth4,instance=softwire-state value=0\ndrop-unknown-protocol-ipv6-bytes,device=eth4,instance=softwire-state value=770\ndrop-unknown-protocol-ipv6-packets,device=eth4,instance=softwire-state value=7\nhairpin-ipv4-bytes,device=eth4,instance=softwire-state value=0\nhairpin-ipv4-packets,device=eth4,instance=softwire-state value=0\nin-ipv4-bytes,device=eth4,instance=softwire-state value=8946\nin-ipv4-frag-needs-reassembly,device=eth4,instance=softwire-state value=0\nin-ipv4-frag-reassembled,device=eth4,instance=softwire-state value=0\nin-ipv4-frag-reassembly-unneeded,device=eth4,instance=softwire-state value=170\nin-ipv4-packets,device=eth4,instance=softwire-state value=63\nin-ipv6-bytes,device=eth4,instance=softwire-state value=770\nin-ipv6-frag-needs-reassembly,device=eth4,instance=softwire-state value=0\nin-ipv6-frag-reassembled,device=eth4,instance=softwire-state value=0\nin-ipv6-frag-reassembly-unneeded,device=eth4,instance=softwire-state value=108\nin-ipv6-packets,device=eth4,instance=softwire-state value=7\ningress-packet-drops,device=eth4,instance=softwire-state value=0\nmemuse-ipv4-frag-reassembly-buffer,device=eth4,instance=softwire-state value=463482888\nmemuse-ipv6-frag-reassembly-buffer,device=eth4,instance=softwire-state value=464549592\nout-icmpv4-bytes,device=eth4,instance=softwire-state value=0\nout-icmpv4-packets,device=eth4,instance=softwire-state value=0\nout-icmpv6-bytes,device=eth4,instance=softwire-state value=0\nout-icmpv6-packets,device=eth4,instance=softwire-state value=0\nout-ipv4-bytes,device=eth4,instance=softwire-state value=0\nout-ipv4-frag,device=eth4,instance=softwire-state value=0\nout-ipv4-frag-not,device=eth4,instance=softwire-state value=9\nout-ipv4-packets,device=eth4,instance=softwire-state value=0\nout-ipv6-bytes,device=eth4,instance=softwire-state value=11466\nout-ipv6-frag,device=eth4,instance=softwire-state value=0\nout-ipv6-frag-not,device=eth4,instance=softwire-state value=72\nout-ipv6-packets,device=eth4,instance=softwire-state value=63\ndrop-all-ipv4-iface-bytes,device=eth0,instance=softwire-state value=814507\ndrop-all-ipv4-iface-packets,device=eth0,instance=softwire-state value=849\ndrop-all-ipv6-iface-bytes,device=eth0,instance=softwire-state value=4083757\ndrop-all-ipv6-iface-packets,device=eth0,instance=softwire-state value=1158\ndrop-bad-checksum-icmpv4-bytes,device=eth0,instance=softwire-state value=0\ndrop-bad-checksum-icmpv4-packets,device=eth0,instance=softwire-state value=0\ndrop-in-by-policy-icmpv4-bytes,device=eth0,instance=softwire-state value=0\ndrop-in-by-policy-icmpv4-packets,device=eth0,instance=softwire-state value=0\ndrop-in-by-policy-icmpv6-bytes,device=eth0,instance=softwire-state value=0\ndrop-in-by-policy-icmpv6-packets,device=eth0,instance=softwire-state value=0\ndrop-in-by-rfc7596-icmpv4-bytes,device=eth0,instance=softwire-state value=0\ndrop-in-by-rfc7596-icmpv4-packets,device=eth0,instance=softwire-state value=0\ndrop-ipv4-frag-disabled,device=eth0,instance=softwire-state value=0\ndrop-ipv4-frag-invalid-reassembly,device=eth0,instance=softwire-state value=0\ndrop-ipv4-frag-random-evicted,device=eth0,instance=softwire-state value=0\ndrop-ipv6-frag-disabled,device=eth0,instance=softwire-state value=0\ndrop-ipv6-frag-invalid-reassembly,device=eth0,instance=softwire-state value=0\ndrop-ipv6-frag-random-evicted,device=eth0,instance=softwire-state value=0\ndrop-misplaced-not-ipv4-bytes,device=eth0,instance=softwire-state value=43424\ndrop-misplaced-not-ipv4-packets,device=eth0,instance=softwire-state value=264\ndrop-misplaced-not-ipv6-bytes,device=eth0,instance=softwire-state value=4083317\ndrop-misplaced-not-ipv6-packets,device=eth0,instance=softwire-state value=1154\ndrop-no-dest-softwire-ipv4-bytes,device=eth0,instance=softwire-state value=771083\ndrop-no-dest-softwire-ipv4-packets,device=eth0,instance=softwire-state value=585\ndrop-no-source-softwire-ipv6-bytes,device=eth0,instance=softwire-state value=0\ndrop-no-source-softwire-ipv6-packets,device=eth0,instance=softwire-state value=0\ndrop-out-by-policy-icmpv4-packets,device=eth0,instance=softwire-state value=585\ndrop-out-by-policy-icmpv6-packets,device=eth0,instance=softwire-state value=0\ndrop-over-mtu-but-dont-fragment-ipv4-bytes,device=eth0,instance=softwire-state value=0\ndrop-over-mtu-but-dont-fragment-ipv4-packets,device=eth0,instance=softwire-state value=0\ndrop-over-rate-limit-icmpv6-bytes,device=eth0,instance=softwire-state value=0\ndrop-over-rate-limit-icmpv6-packets,device=eth0,instance=softwire-state value=0\ndrop-over-time-but-not-hop-limit-icmpv6-bytes,device=eth0,instance=softwire-state value=0\ndrop-over-time-but-not-hop-limit-icmpv6-packets,device=eth0,instance=softwire-state value=0\ndrop-too-big-type-but-not-code-icmpv6-bytes,device=eth0,instance=softwire-state value=0\ndrop-too-big-type-but-not-code-icmpv6-packets,device=eth0,instance=softwire-state value=0\ndrop-ttl-zero-ipv4-bytes,device=eth0,instance=softwire-state value=0\ndrop-ttl-zero-ipv4-packets,device=eth0,instance=softwire-state value=0\ndrop-unknown-protocol-icmpv6-bytes,device=eth0,instance=softwire-state value=0\ndrop-unknown-protocol-icmpv6-packets,device=eth0,instance=softwire-state value=0\ndrop-unknown-protocol-ipv6-bytes,device=eth0,instance=softwire-state value=440\ndrop-unknown-protocol-ipv6-packets,device=eth0,instance=softwire-state value=4\nhairpin-ipv4-bytes,device=eth0,instance=softwire-state value=0\nhairpin-ipv4-packets,device=eth0,instance=softwire-state value=0\nin-ipv4-bytes,device=eth0,instance=softwire-state value=802323\nin-ipv4-frag-needs-reassembly,device=eth0,instance=softwire-state value=0\nin-ipv4-frag-reassembled,device=eth0,instance=softwire-state value=0\nin-ipv4-frag-reassembly-unneeded,device=eth0,instance=softwire-state value=1089\nin-ipv4-packets,device=eth0,instance=softwire-state value=805\nin-ipv6-bytes,device=eth0,instance=softwire-state value=440\nin-ipv6-frag-needs-reassembly,device=eth0,instance=softwire-state value=0\nin-ipv6-frag-reassembled,device=eth0,instance=softwire-state value=0\nin-ipv6-frag-reassembly-unneeded,device=eth0,instance=softwire-state value=1189\nin-ipv6-packets,device=eth0,instance=softwire-state value=4\ningress-packet-drops,device=eth0,instance=softwire-state value=0\nmemuse-ipv4-frag-reassembly-buffer,device=eth0,instance=softwire-state value=463482888\nmemuse-ipv6-frag-reassembly-buffer,device=eth0,instance=softwire-state value=464549592\nout-icmpv4-bytes,device=eth0,instance=softwire-state value=0\nout-icmpv4-packets,device=eth0,instance=softwire-state value=0\nout-icmpv6-bytes,device=eth0,instance=softwire-state value=0\nout-icmpv6-packets,device=eth0,instance=softwire-state value=0\nout-ipv4-bytes,device=eth0,instance=softwire-state value=0\nout-ipv4-frag,device=eth0,instance=softwire-state value=0\nout-ipv4-frag-not,device=eth0,instance=softwire-state value=9\nout-ipv4-packets,device=eth0,instance=softwire-state value=0\nout-ipv6-bytes,device=eth0,instance=softwire-state value=40040\nout-ipv6-frag,device=eth0,instance=softwire-state value=0\nout-ipv6-frag-not,device=eth0,instance=softwire-state value=229\nout-ipv6-packets,device=eth0,instance=softwire-state value=220\ndrop-all-ipv4-iface-bytes,device=eth2,instance=softwire-state value=2748\ndrop-all-ipv4-iface-packets,device=eth2,instance=softwire-state value=34\ndrop-all-ipv6-iface-bytes,device=eth2,instance=softwire-state value=1412\ndrop-all-ipv6-iface-packets,device=eth2,instance=softwire-state value=22\ndrop-bad-checksum-icmpv4-bytes,device=eth2,instance=softwire-state value=0\ndrop-bad-checksum-icmpv4-packets,device=eth2,instance=softwire-state value=0\ndrop-in-by-policy-icmpv4-bytes,device=eth2,instance=softwire-state value=0\ndrop-in-by-policy-icmpv4-packets,device=eth2,instance=softwire-state value=0\ndrop-in-by-policy-icmpv6-bytes,device=eth2,instance=softwire-state value=0\ndrop-in-by-policy-icmpv6-packets,device=eth2,instance=softwire-state value=0\ndrop-in-by-rfc7596-icmpv4-bytes,device=eth2,instance=softwire-state value=0\ndrop-in-by-rfc7596-icmpv4-packets,device=eth2,instance=softwire-state value=0\ndrop-ipv4-frag-disabled,device=eth2,instance=softwire-state value=0\ndrop-ipv4-frag-invalid-reassembly,device=eth2,instance=softwire-state value=0\ndrop-ipv4-frag-random-evicted,device=eth2,instance=softwire-state value=0\ndrop-ipv6-frag-disabled,device=eth2,instance=softwire-state value=0\ndrop-ipv6-frag-invalid-reassembly,device=eth2,instance=softwire-state value=0\ndrop-ipv6-frag-random-evicted,device=eth2,instance=softwire-state value=0\ndrop-misplaced-not-ipv4-bytes,device=eth2,instance=softwire-state value=2748\ndrop-misplaced-not-ipv4-packets,device=eth2,instance=softwire-state value=34\ndrop-misplaced-not-ipv6-bytes,device=eth2,instance=softwire-state value=752\ndrop-misplaced-not-ipv6-packets,device=eth2,instance=softwire-state value=16\ndrop-no-dest-softwire-ipv4-bytes,device=eth2,instance=softwire-state value=0\ndrop-no-dest-softwire-ipv4-packets,device=eth2,instance=softwire-state value=0\ndrop-no-source-softwire-ipv6-bytes,device=eth2,instance=softwire-state value=0\ndrop-no-source-softwire-ipv6-packets,device=eth2,instance=softwire-state value=0\ndrop-out-by-policy-icmpv4-packets,device=eth2,instance=softwire-state value=0\ndrop-out-by-policy-icmpv6-packets,device=eth2,instance=softwire-state value=0\ndrop-over-mtu-but-dont-fragment-ipv4-bytes,device=eth2,instance=softwire-state value=0\ndrop-over-mtu-but-dont-fragment-ipv4-packets,device=eth2,instance=softwire-state value=0\ndrop-over-rate-limit-icmpv6-bytes,device=eth2,instance=softwire-state value=0\ndrop-over-rate-limit-icmpv6-packets,device=eth2,instance=softwire-state value=0\ndrop-over-time-but-not-hop-limit-icmpv6-bytes,device=eth2,instance=softwire-state value=0\ndrop-over-time-but-not-hop-limit-icmpv6-packets,device=eth2,instance=softwire-state value=0\ndrop-too-big-type-but-not-code-icmpv6-bytes,device=eth2,instance=softwire-state value=0\ndrop-too-big-type-but-not-code-icmpv6-packets,device=eth2,instance=softwire-state value=0\ndrop-ttl-zero-ipv4-bytes,device=eth2,instance=softwire-state value=0\ndrop-ttl-zero-ipv4-packets,device=eth2,instance=softwire-state value=0\ndrop-unknown-protocol-icmpv6-bytes,device=eth2,instance=softwire-state value=0\ndrop-unknown-protocol-icmpv6-packets,device=eth2,instance=softwire-state value=0\ndrop-unknown-protocol-ipv6-bytes,device=eth2,instance=softwire-state value=660\ndrop-unknown-protocol-ipv6-packets,device=eth2,instance=softwire-state value=6\nhairpin-ipv4-bytes,device=eth2,instance=softwire-state value=0\nhairpin-ipv4-packets,device=eth2,instance=softwire-state value=0\nin-ipv4-bytes,device=eth2,instance=softwire-state value=0\nin-ipv4-frag-needs-reassembly,device=eth2,instance=softwire-state value=0\nin-ipv4-frag-reassembled,device=eth2,instance=softwire-state value=0\nin-ipv4-frag-reassembly-unneeded,device=eth2,instance=softwire-state value=42\nin-ipv4-packets,device=eth2,instance=softwire-state value=0\nin-ipv6-bytes,device=eth2,instance=softwire-state value=660\nin-ipv6-frag-needs-reassembly,device=eth2,instance=softwire-state value=0\nin-ipv6-frag-reassembled,device=eth2,instance=softwire-state value=0\nin-ipv6-frag-reassembly-unneeded,device=eth2,instance=softwire-state value=41\nin-ipv6-packets,device=eth2,instance=softwire-state value=6\ningress-packet-drops,device=eth2,instance=softwire-state value=0\nmemuse-ipv4-frag-reassembly-buffer,device=eth2,instance=softwire-state value=463482888\nmemuse-ipv6-frag-reassembly-buffer,device=eth2,instance=softwire-state value=464549592\nout-icmpv4-bytes,device=eth2,instance=softwire-state value=0\nout-icmpv4-packets,device=eth2,instance=softwire-state value=0\nout-icmpv6-bytes,device=eth2,instance=softwire-state value=0\nout-icmpv6-packets,device=eth2,instance=softwire-state value=0\nout-ipv4-bytes,device=eth2,instance=softwire-state value=0\nout-ipv4-frag,device=eth2,instance=softwire-state value=0\nout-ipv4-frag-not,device=eth2,instance=softwire-state value=8\nout-ipv4-packets,device=eth2,instance=softwire-state value=0\nout-ipv6-bytes,device=eth2,instance=softwire-state value=0\nout-ipv6-frag,device=eth2,instance=softwire-state value=0\nout-ipv6-frag-not,device=eth2,instance=softwire-state value=9\nout-ipv6-packets,device=eth2,instance=softwire-state value=0\ndrop-all-ipv4-iface-bytes,device=eth1,instance=softwire-state value=14214\ndrop-all-ipv4-iface-packets,device=eth1,instance=softwire-state value=97\ndrop-all-ipv6-iface-bytes,device=eth1,instance=softwire-state value=10442\ndrop-all-ipv6-iface-packets,device=eth1,instance=softwire-state value=87\ndrop-bad-checksum-icmpv4-bytes,device=eth1,instance=softwire-state value=0\ndrop-bad-checksum-icmpv4-packets,device=eth1,instance=softwire-state value=0\ndrop-in-by-policy-icmpv4-bytes,device=eth1,instance=softwire-state value=0\ndrop-in-by-policy-icmpv4-packets,device=eth1,instance=softwire-state value=0\ndrop-in-by-policy-icmpv6-bytes,device=eth1,instance=softwire-state value=0\ndrop-in-by-policy-icmpv6-packets,device=eth1,instance=softwire-state value=0\ndrop-in-by-rfc7596-icmpv4-bytes,device=eth1,instance=softwire-state value=0\ndrop-in-by-rfc7596-icmpv4-packets,device=eth1,instance=softwire-state value=0\ndrop-ipv4-frag-disabled,device=eth1,instance=softwire-state value=0\ndrop-ipv4-frag-invalid-reassembly,device=eth1,instance=softwire-state value=0\ndrop-ipv4-frag-random-evicted,device=eth1,instance=softwire-state value=0\ndrop-ipv6-frag-disabled,device=eth1,instance=softwire-state value=0\ndrop-ipv6-frag-invalid-reassembly,device=eth1,instance=softwire-state value=0\ndrop-ipv6-frag-random-evicted,device=eth1,instance=softwire-state value=0\ndrop-misplaced-not-ipv4-bytes,device=eth1,instance=softwire-state value=14214\ndrop-misplaced-not-ipv4-packets,device=eth1,instance=softwire-state value=97\ndrop-misplaced-not-ipv6-bytes,device=eth1,instance=softwire-state value=9782\ndrop-misplaced-not-ipv6-packets,device=eth1,instance=softwire-state value=81\ndrop-no-dest-softwire-ipv4-bytes,device=eth1,instance=softwire-state value=0\ndrop-no-dest-softwire-ipv4-packets,device=eth1,instance=softwire-state value=0\ndrop-no-source-softwire-ipv6-bytes,device=eth1,instance=softwire-state value=0\ndrop-no-source-softwire-ipv6-packets,device=eth1,instance=softwire-state value=0\ndrop-out-by-policy-icmpv4-packets,device=eth1,instance=softwire-state value=0\ndrop-out-by-policy-icmpv6-packets,device=eth1,instance=softwire-state value=0\ndrop-over-mtu-but-dont-fragment-ipv4-bytes,device=eth1,instance=softwire-state value=0\ndrop-over-mtu-but-dont-fragment-ipv4-packets,device=eth1,instance=softwire-state value=0\ndrop-over-rate-limit-icmpv6-bytes,device=eth1,instance=softwire-state value=0\ndrop-over-rate-limit-icmpv6-packets,device=eth1,instance=softwire-state value=0\ndrop-over-time-but-not-hop-limit-icmpv6-bytes,device=eth1,instance=softwire-state value=0\ndrop-over-time-but-not-hop-limit-icmpv6-packets,device=eth1,instance=softwire-state value=0\ndrop-too-big-type-but-not-code-icmpv6-bytes,device=eth1,instance=softwire-state value=0\ndrop-too-big-type-but-not-code-icmpv6-packets,device=eth1,instance=softwire-state value=0\ndrop-ttl-zero-ipv4-bytes,device=eth1,instance=softwire-state value=0\ndrop-ttl-zero-ipv4-packets,device=eth1,instance=softwire-state value=0\ndrop-unknown-protocol-icmpv6-bytes,device=eth1,instance=softwire-state value=0\ndrop-unknown-protocol-icmpv6-packets,device=eth1,instance=softwire-state value=0\ndrop-unknown-protocol-ipv6-bytes,device=eth1,instance=softwire-state value=660\ndrop-unknown-protocol-ipv6-packets,device=eth1,instance=softwire-state value=6\nhairpin-ipv4-bytes,device=eth1,instance=softwire-state value=0\nhairpin-ipv4-packets,device=eth1,instance=softwire-state value=0\nin-ipv4-bytes,device=eth1,instance=softwire-state value=8946\nin-ipv4-frag-needs-reassembly,device=eth1,instance=softwire-state value=0\nin-ipv4-frag-reassembled,device=eth1,instance=softwire-state value=0\nin-ipv4-frag-reassembly-unneeded,device=eth1,instance=softwire-state value=169\nin-ipv4-packets,device=eth1,instance=softwire-state value=63\nin-ipv6-bytes,device=eth1,instance=softwire-state value=660\nin-ipv6-frag-needs-reassembly,device=eth1,instance=softwire-state value=0\nin-ipv6-frag-reassembled,device=eth1,instance=softwire-state value=0\nin-ipv6-frag-reassembly-unneeded,device=eth1,instance=softwire-state value=106\nin-ipv6-packets,device=eth1,instance=softwire-state value=6\ningress-packet-drops,device=eth1,instance=softwire-state value=0\nmemuse-ipv4-frag-reassembly-buffer,device=eth1,instance=softwire-state value=463482888\nmemuse-ipv6-frag-reassembly-buffer,device=eth1,instance=softwire-state value=464549592\nout-icmpv4-bytes,device=eth1,instance=softwire-state value=0\nout-icmpv4-packets,device=eth1,instance=softwire-state value=0\nout-icmpv6-bytes,device=eth1,instance=softwire-state value=0\nout-icmpv6-packets,device=eth1,instance=softwire-state value=0\nout-ipv4-bytes,device=eth1,instance=softwire-state value=0\nout-ipv4-frag,device=eth1,instance=softwire-state value=0\nout-ipv4-frag-not,device=eth1,instance=softwire-state value=9\nout-ipv4-packets,device=eth1,instance=softwire-state value=0\nout-ipv6-bytes,device=eth1,instance=softwire-state value=11466\nout-ipv6-frag,device=eth1,instance=softwire-state value=0\nout-ipv6-frag-not,device=eth1,instance=softwire-state value=72\nout-ipv6-packets,device=eth1,instance=softwire-state value=63\ndrop-all-ipv4-iface-bytes,device=eth3,instance=softwire-state value=36214\ndrop-all-ipv4-iface-packets,device=eth3,instance=softwire-state value=219\ndrop-all-ipv6-iface-bytes,device=eth3,instance=softwire-state value=27572\ndrop-all-ipv6-iface-packets,device=eth3,instance=softwire-state value=208\ndrop-bad-checksum-icmpv4-bytes,device=eth3,instance=softwire-state value=0\ndrop-bad-checksum-icmpv4-packets,device=eth3,instance=softwire-state value=0\ndrop-in-by-policy-icmpv4-bytes,device=eth3,instance=softwire-state value=0\ndrop-in-by-policy-icmpv4-packets,device=eth3,instance=softwire-state value=0\ndrop-in-by-policy-icmpv6-bytes,device=eth3,instance=softwire-state value=0\ndrop-in-by-policy-icmpv6-packets,device=eth3,instance=softwire-state value=0\ndrop-in-by-rfc7596-icmpv4-bytes,device=eth3,instance=softwire-state value=0\ndrop-in-by-rfc7596-icmpv4-packets,device=eth3,instance=softwire-state value=0\ndrop-ipv4-frag-disabled,device=eth3,instance=softwire-state value=0\ndrop-ipv4-frag-invalid-reassembly,device=eth3,instance=softwire-state value=0\ndrop-ipv4-frag-random-evicted,device=eth3,instance=softwire-state value=0\ndrop-ipv6-frag-disabled,device=eth3,instance=softwire-state value=0\ndrop-ipv6-frag-invalid-reassembly,device=eth3,instance=softwire-state value=0\ndrop-ipv6-frag-random-evicted,device=eth3,instance=softwire-state value=0\ndrop-misplaced-not-ipv4-bytes,device=eth3,instance=softwire-state value=36214\ndrop-misplaced-not-ipv4-packets,device=eth3,instance=softwire-state value=219\ndrop-misplaced-not-ipv6-bytes,device=eth3,instance=softwire-state value=26822\ndrop-misplaced-not-ipv6-packets,device=eth3,instance=softwire-state value=201\ndrop-no-dest-softwire-ipv4-bytes,device=eth3,instance=softwire-state value=0\ndrop-no-dest-softwire-ipv4-packets,device=eth3,instance=softwire-state value=0\ndrop-no-source-softwire-ipv6-bytes,device=eth3,instance=softwire-state value=0\ndrop-no-source-softwire-ipv6-packets,device=eth3,instance=softwire-state value=0\ndrop-out-by-policy-icmpv4-packets,device=eth3,instance=softwire-state value=0\ndrop-out-by-policy-icmpv6-packets,device=eth3,instance=softwire-state value=0\ndrop-over-mtu-but-dont-fragment-ipv4-bytes,device=eth3,instance=softwire-state value=0\ndrop-over-mtu-but-dont-fragment-ipv4-packets,device=eth3,instance=softwire-state value=0\ndrop-over-rate-limit-icmpv6-bytes,device=eth3,instance=softwire-state value=0\ndrop-over-rate-limit-icmpv6-packets,device=eth3,instance=softwire-state value=0\ndrop-over-time-but-not-hop-limit-icmpv6-bytes,device=eth3,instance=softwire-state value=0\ndrop-over-time-but-not-hop-limit-icmpv6-packets,device=eth3,instance=softwire-state value=0\ndrop-too-big-type-but-not-code-icmpv6-bytes,device=eth3,instance=softwire-state value=0\ndrop-too-big-type-but-not-code-icmpv6-packets,device=eth3,instance=softwire-state value=0\ndrop-ttl-zero-ipv4-bytes,device=eth3,instance=softwire-state value=0\ndrop-ttl-zero-ipv4-packets,device=eth3,instance=softwire-state value=0\ndrop-unknown-protocol-icmpv6-bytes,device=eth3,instance=softwire-state value=0\ndrop-unknown-protocol-icmpv6-packets,device=eth3,instance=softwire-state value=0\ndrop-unknown-protocol-ipv6-bytes,device=eth3,instance=softwire-state value=750\ndrop-unknown-protocol-ipv6-packets,device=eth3,instance=softwire-state value=7\nhairpin-ipv4-bytes,device=eth3,instance=softwire-state value=0\nhairpin-ipv4-packets,device=eth3,instance=softwire-state value=0\nin-ipv4-bytes,device=eth3,instance=softwire-state value=25986\nin-ipv4-frag-needs-reassembly,device=eth3,instance=softwire-state value=0\nin-ipv4-frag-reassembled,device=eth3,instance=softwire-state value=0\nin-ipv4-frag-reassembly-unneeded,device=eth3,instance=softwire-state value=411\nin-ipv4-packets,device=eth3,instance=softwire-state value=183\nin-ipv6-bytes,device=eth3,instance=softwire-state value=750\nin-ipv6-frag-needs-reassembly,device=eth3,instance=softwire-state value=0\nin-ipv6-frag-reassembled,device=eth3,instance=softwire-state value=0\nin-ipv6-frag-reassembly-unneeded,device=eth3,instance=softwire-state value=228\nin-ipv6-packets,device=eth3,instance=softwire-state value=7\ningress-packet-drops,device=eth3,instance=softwire-state value=0\nmemuse-ipv4-frag-reassembly-buffer,device=eth3,instance=softwire-state value=463482888\nmemuse-ipv6-frag-reassembly-buffer,device=eth3,instance=softwire-state value=464549592\nout-icmpv4-bytes,device=eth3,instance=softwire-state value=0\nout-icmpv4-packets,device=eth3,instance=softwire-state value=0\nout-icmpv6-bytes,device=eth3,instance=softwire-state value=0\nout-icmpv6-packets,device=eth3,instance=softwire-state value=0\nout-ipv4-bytes,device=eth3,instance=softwire-state value=0\nout-ipv4-frag,device=eth3,instance=softwire-state value=0\nout-ipv4-frag-not,device=eth3,instance=softwire-state value=9\nout-ipv4-packets,device=eth3,instance=softwire-state value=0\nout-ipv6-bytes,device=eth3,instance=softwire-state value=33306\nout-ipv6-frag,device=eth3,instance=softwire-state value=0\nout-ipv6-frag-not,device=eth3,instance=softwire-state value=192\nout-ipv6-packets,device=eth3,instance=softwire-state value=183\n```\n. The experience can be even more transparent when using\n\nhttps://gist.github.com/mwiget/5696711d693eeb43506bcb92cbfc1777\n```\nmwiget-mbp:src mwiget$ snabb snsh -t core.timer\nselftest: timer\nok (973,855 callbacks in 0.2341 seconds)\nmwiget-mbp:src mwiget$ uname -a\nDarwin mwiget-mbp 16.7.0 Darwin Kernel Version 16.7.0: Thu Jan 11 22:59:40 PST 2018; root:xnu-3789.73.8~1/RELEASE_X86_64 x86_64\n```. Something came up unfortunately, can't make it this time. . Yes! Looking forward to another Snabb Lunch. . No worries. I wanted to get a separate fork for the public key, but git automatically used the fork I had already open ;-)\n. Interesting remark about not handling > 8GB. We've ran many tests with 16G and it works (packets flying). In case this is a limit and we just got lucky, that would be very important to note. Real VNF's tend to demand lots of memory. \n. Many thanks for your detailed feedback. I'll go thru all of them and make the required changes. On VLAN support I'm wondering if I could simply leverage VMDq instead of handling VLANs in software. One drawback is of course the lack of VLAN support on tap interface. On the other hand, my intention with the tap interface was simply a method to make the packets visible via tcpdump if needed.\n. Indeed. Good catch!\n. Yep. Same again for the ipv4 handling a few lines down. I wonder if it makes more sense to define payload as local outside the loop or if it doesn't matter.\n. Indeed.\n. Agree.\n. yes indeed. Fixed.\n. fixed.\n. Indeed. src/dst are only used in synth.lua. Removed them in replay.lua\n. fixed.\n. Yeah, my vim macro to align/autoindent lua has issues with functions across lines ;-)\nFixed it manually.\n. Hmm. VMDq is probably not a good idea, because it requires to register with a mac address unless a VLAN is given. Just thinking out loud here: maybe VMDq can still be used with VLAN tag, but must be disabled for untagged traffic. Setting MAC address is an issue for the single stick design, which can have two different destination MAC's for v4 and v6 traffic returned from LWAFTR.\n. Went with VMDq set to true when VLAN is given. Its an efficient way to add VLAN support without changing the source code: VLAN handling is done by the Intel NIC. That obviously doesn't work for tap interfaces and I don't have a warning about it just yet.\n. argh and thanks ;-)\n. thanks. That info helped a lot: Got it now working using this:\n```\n   local ipv4_only = false\n   function opt.v4 () ipv4_only = true end\n   opt[\"4\"] = opt.v4\nlocal ipv6_only = false\n   function opt.v6 () ipv6_only = true end\n   opt[\"6\"] = opt.v6\n```\n. @dpino: where would you put it? In a new file in src/program/packetblaster or simply place it in packetblaster.lua (without local) and call it from synth and reply? I kind of like its simplicity (1 print, 1 func call) right where its used right now.\n. @dpino: agree. I renamed the local reporting fn() into packetblaster.report() and moved also is_device_suitable() into packetblaster.lua. IMHO is_device_suitable() might even be worth moving into lib.hardware.pci, but I'm hesitant to make such a move. Now there is at least no redundant code anymore between synth, replay \n. @eugeneia: this is a result of breaking up the single packetblaster.lua command into individual files. Let me look into bringing this logic of building the apps and links into a function of packetblaster.lua and call it from replay and synth. Packetblaster lwaftr doesn't use this because its limited to a single PCI address.\n. @eugeneia: Hmm, that  was a result of moving synth and replay into their own directories, so they are in line with the added lwaftr. Would it make more sense to leave the original packetblaster.lua as is and simply enhance its logic to also allow commands taken from subdirectories? Kind of  having synth and replay be handled within packetblaster.lua (as before this pull) and only add lwaftr/lwaftr.lua?\nOr do you have something different in mind?\n. @eugeneia Ah, now I get it. Did as suggested and it does indeed look cleaner. Made a few more commits to address VLAN support without VMDq and creation of a pcap file plus selftest the pcap file. \n. ",
    "comotion": "Conditional back-versioning of memcpy  at c96f1e8 is incomplete and fixed by 30505c6.\n. Firstly I would like to figure out what is causing snabbswitch to fail despite compiling cleanly to musl. Making it compile cleanly was my initial stab at musl compatability,  However, I am also interested in finding out why my patches are causing snabbswitch to fail testing under glibc, as those patches were not meant to break anything. Do you have any idea why that might be? After I know more I can properly assess whether it is worth having a musl branch and if I can take responsibility for maintaining it.\n. Sorry I haven't been able to follow this up, am in my busy months, but did you figure out why STP was failing to load? With a working binary I would have less problems following up the other musl issues.\n. I see, I can relate to both the need to build statically and the need to depend on a particular version of the dependencies. What I am after is a way for the build system to allow me to use system-provided libraries not as a default, but as an option in the \"I know what I am doing\" situation, and certainly without disabling any dependency checks.\nContext here is that I am trying to package snabbswitch for a small embedded linux distro, where space is at a premium.\nPerhaps the build system can support some environment variable to tell it to use a particular library file instead?\n. I wouldn't mind attempting to port to i486. I'm currently targeting i486 and arm with my packaging work. It would be a start to include all ljsyscall stuff that is relevant for that arch :+1: \n. I am packaging for a minimal musl-based distro in its early stages. Since snabbswitch is \"the busybox for networking\" I thought it interesting to package snabbswitch for it.\nThe lack of explicitly missing architecture support is the source of your confusion.\nLet's back up a bit. \nMarking this bug as a confusing error message is a bit of a stretch, so the bug title is now misleading. Nowhere on the web pages or in the readme does it say \"x86_64 only\". In fact, front page refers to x86. Unless there is a lot of code to support i486 (and conceivably arm) then it feels like you're setting arbitrary arch limits for no particular reason. \n. ",
    "majek": "We are on debian squeeze:\n$ apt-cache policy coreutils\ncoreutils:\n  Installed: 8.5-1\n  Version table:\n *** 8.5-1 0\n        500 http://ftp.us.debian.org/debian/ squeeze/main amd64 Packages\n. ",
    "pavel-odintsov": "Hello, Luke!\nI want to implement scheme when netmap or PF_RING ZC enabled analyzer could be run on same interface with already working Linux with network services.\n\n. Now my tool working really anywhere. It could work inside KVM (PCI-E passthrough), on top of physical NIC, on virtual function inside KVM VM or on physical hardware. \nBut I want capture on wire speed without any changes from network infrastructure side. \nIntel decided do not add this function to the driver: https://sourceforge.net/p/e1000/bugs/480/ Very sad. You are my last resort.\nYes, I working with ixgbe, it's most popular, flexible, reliable and awesome card :) Sure, I will try with SnabbSwitch idea, it's on my roadmap too. But before this I need finish some tasks..\n. I'm really happy about your interest in this approach!\nI need check it, I have multiple machines with PF_RING and Netmap with 82599 NIC's. And will check it shortly. \n. Negative for PF_RING, detailed ticket there: https://github.com/ntop/PF_RING/issues/15\n. And awesome news from netmap, it working perfectly: http://www.stableit.ru/2015/06/how-to-run-netmap-on-ixgbe-virtual.html\nI could run two netmap instances on same physical NIC, one per each virtual NIC.\nAnd ticket to Luigi: https://github.com/luigirizzo/netmap/issues/63\n. Yes, I have custom really well tested on wire speed driver for ixgbe: https://github.com/pavel-odintsov/ixgbe-linux-netmap\nBut I have no knowledge what I should fix. \n. Luke, do you have support for mirrored VF's in Snabb Switch? Maybe I could port it from LUA to C?\n. Awesome! I will try to dig into ixgbe docs. I have sent letter to ixgbe community and send copy to you.\nHave you finished any performance tests? Could two applications on different VFs work in wire speed? Could we achieve wire speed for 10GE for mirror interface in both directions?\n. 2/3 of wire speed songs awesome too. Really nice approach if we could implement it. Thank you for your attention and your huge help! :) \n. Sure, thanks!\n. Hello!\nThey have wrote driver from the scratch with LUA :)\nOn Fri, Nov 6, 2015 at 2:43 AM, morphyno notifications@github.com wrote:\n\nI have a question in regard to Snabb Switch's code above to enable port\nmirror on 82599. How is Snabb interfacing with the driver via lua? Did you\nhave to compile the existing driver any differently?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/SnabbCo/snabbswitch/issues/518#issuecomment-154233787\n.\n\n\nSincerely yours, Pavel Odintsov\n. Awesome idea! Like it :) We could pass packets over multiple \"iterations\" and free they up when work finished. \n. Wow! We could build so much interesting things with it now :) Looking forward for dynamically generated packet parsers deeply optimized for specific architecture. \n. Hello, Luke!\n1. There are some additional fixes about -fPIC option. We could not build both binaries together because .so need -fPIC but app do not need it. \n2. Will be fine to support both. If user specified path to .so we use it. Instead we try to find it in global namespace.\nOption with mapping file is nic but my case is more flexible in case when I want multiple snabb instances for multiple NIC's.\n. Hello!\nWill be fine to get libfirehose.so instead libsnabb.so. It will be PoC for this idea. So if it success we could move forward. \n. I want to use SnabbSwitch driver from C. It's my main idea. So I need some way to build this .so file from upstream SnabbSwitch repository. Without any deep fixes.\n. So I looked on strace and found so much nanosleep calls:\nbash\nnanosleep({0, 100000}, NULL)            = 0\nnanosleep({0, 100000}, NULL)            = 0\nnanosleep({0, 100000}, NULL)            = 0\nnanosleep({0, 100000}, NULL)            = 0\nnanosleep({0, 100000}, NULL)            = 0\nnanosleep({0, 100000}, NULL)            = 0\nnanosleep({0, 100000}, NULL)            = 0\nnanosleep({0, 100000}, NULL)            = 0\nnanosleep({0, 100000}, NULL)            = 0\nnanosleep({0, 100000}, NULL)            = 0\nnanosleep({0, 100000}, NULL)            = 0\nnanosleep({0, 100000}, NULL)            = 0\nnanosleep({0, 100000}, NULL)            = 0\nnanosleep({0, 100000}, NULL)            = 0\n. Hello!\nThanks for answer! Will add this cards to blacklist! :)\nSo I could test it with netmap and share results.\n. Brilliant work! Awesome! :) Will check it soon and contribute feedback! \n. Brilliant idea! But what about full ixgbe emulation from hardware side? So we could run virtual PCI device and use it with upstream ixgbe driver or with snabbswitch. \n. Hello!\nThank you for response! Yes, direct use of assembler can make things complicated :(\nCan I ask about benefits of RaptorJIT over LuaJIT?\nThank you!. That's pretty interesting! Thank you for detailed answer! I will consider RaptorJIT as option. . ",
    "morphyno": "I have a question in regard to Snabb Switch's code above to enable port mirror on 82599. How is Snabb interfacing with the driver via lua? Did you have to compile the existing driver any differently? \n. Looking at this 2 yrs since this post started. Netdev has made alot of in roads on this, with tc_offloading also a common driver feature, I feel this could be implemented using tc and bcc and be a complementing to snabb. Thanks @lukego ,do you have any information formally outlining AWS bandwidth limitation? The closest I have thus far is http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-ec2-config.html. @virtuallynathan , thank you for all this info. Would you happen to know if the \"vif\" driver for regular EC2s standard XEN vif's? . @Posnet , I have not done anything yet, I'm working on a NFV POC on openstack right now, be interested getting it onto AWS after that. . ",
    "ghostli123": "Hello! Have you figure out the issue of how to enable and control traffic mirroring in 82599 ixgbe driver? Thanks!\n. ",
    "da4089": "But ... not STP between the bridge apps?\n. Yes.  STP (in one variant or another) is the traditional way of avoiding cycles between interconnected L2 segments. If the plan is to match each port with its own bridge, and to fully mesh them, STP might be suitable.\n. It might be possible to arrange a gratis NT100E3 from Napatech on the basis that a driver is made available in snabb for it.\nSilicom has their FM10k cards, plus the Fiberblaze Virtex7 cards.\nNallatech 385C looks like a dual 100Gbps board.\nEndace isn't at 100Gbps; Atto is only at 40Gbps; Bittware has a couple of boards, but they don't seem to be general purpose.\nIf there's solid interest, I know people at Napatech and Fiberblaze I can ask.\n. ",
    "blakedot": "This is an excellent read & a fascinating discussion.\nThere are two L2 architectures that I recommend studying if you want to take this further:\n- RFC7432 Ethernet VPN.  One core concept:  the \"mac default\" route, that sends broadcast/multicast/unknown-unicast traffic to the \"gateway(s)\" which then act as a sort of central MAC database so the network can scale to a very large number of nodes without flooding issues.  Also, packets are only looked up at ingress, & then have an MPLS label attached; this avoids egress lookup (egress router just pops & sends).\n- RFC6329 Shortest Path Bridging.  Instead of using spanning-tree, IS-IS is extended to support a MAC address family.  SPF can be calculated on a per-VLAN or per-MAC basis.  In the case of per-MAC, you can mutlipath load-balance without fearing loops.  IS-IS distributes the bridging table around the network, so unknown-unicast flooding is reduced.\n. also a great candidate for a multipath TCP proxy.... ",
    "jgroom33": "Exactly.  If this is applied to the scalable switch thread the concept would manifest as follows:\nA switch is a table that contains a set of strings (mac addresses)\nAn arbitrary set of data streams from an arbitrary list of ports can be attached to that table.  It should be possible to connect (broadcast between) 2 switch tables together if desired.\nThe ?only? thing that makes this memory table a switch is what occurs when a match is triggered (send frame to matching port) and what occurs when a match is not triggered (broadcast to all ports).\nIt should be the responsibility of something else to decide whether a loop does/should exist.  If it is desired to implement spanning tree within the (tables) switches of the heap, then that is one possible implementation.\nI think that implementing FBP will support the necessary flexibility.  turning on/off a flow would be synonymous to enabling/disable a control protocol.\nnote: flow is a difficult term to use in describing the above concept as it contains a definition already related to packet transport.  A flow is not meant in that context above.\n. Agreed. Mac swap is a routing function.\nA switch does similar actions:\nChange tpid on egress frames (0800,8100,88a8) - though, this is also an\ninsert\nVlan translate (stamp)\nI'm curious if all functions can be generically implemented\nCan a 'config' be a list of grep/pipe/modify commands?\nOn Tue, Aug 11, 2015, 4:21 AM Alexander Gall notifications@github.com\nwrote:\n\nI don't think it's relevant in your context, but I just want to make sure\nthere are no misconceptions: you seem to assume that a switch manipulates\nthe destination address of a packet during forwarding (that's what you mean\nby \"swap a MAC\" if I understand correctly). That's not how a switch works,\ni.e. the packet is forwarded unmodified.\nUnfortunately, I'm unfamiliar with the conecpt of FBP, so I can't comment\non its application to packet processing, at least not with a substantial\namount of reading :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/SnabbCo/snabbswitch/issues/583#issuecomment-129516254\n.\n. This presentation is promoting a similar concept:\nhttp://opennetsummit.org/blog/2015/11/webinar-replay-programming-the-network-dataplane-in-p4/\n\nMy personal preference would be to design the flows in a visual environment similar to the goals of http://noflojs.org/\nThe presentation above is promoting the use of a new language: P4\nI prefer, good, old-fashioned graph theory  :)\n. The run commands in the docker file could be grouped into a single command\nusing &&.   This will reduce the image size.\nOn Tue, Sep 1, 2015, 12:26 AM Max Rottenkolber notifications@github.com\nwrote:\n\nAfter tinkering on this for a while here is a progress update. My working\nbranch can be inspected here: master...eugeneia:simplify-test\nhttps://github.com/SnabbCo/snabbswitch/compare/master...eugeneia:simplify-test\nTo just see the docker related changes see: 03ba0a7\nhttps://github.com/SnabbCo/snabbswitch/commit/03ba0a710808fc4dbd8bd5eb84901e8f4e57e4d9\nFirst of all a summary of whats done (docker-wise):\n- There is a new top-level target make docker that will build a docker\n  image containing a full Snabb Switch test environment.\n- In src/ you can call scripts/dock.sh  to run command on the\n  current Snabb Switch tree within a fresh docker container based on\n  mentioned image.\nIssues I have encountered include:\n- The Dockerfile\n  https://github.com/SnabbCo/snabbswitch/blob/03ba0a710808fc4dbd8bd5eb84901e8f4e57e4d9/Dockerfile\n  is small but the script to build the test assets\n  https://github.com/SnabbCo/snabbswitch/blob/03ba0a710808fc4dbd8bd5eb84901e8f4e57e4d9/src/scripts/make-assets.sh\n  is \"big\". The reason is that the asset VM images are not docker images and\n  docker is not designed to build \"child images\" (it would be useful if there\n  was an easy way to turn docker images into disk images to be used by qemu).\n  To build the VM images we use mount -o loop ..., chroot and possibly\n  other (e.g. mkfs?) privileged commands. Since docker build can not be\n  run in privileged mode, we need to invoke the asset building in a docker\n  run followed by a docker commit. See docker/d ocker#1916\n  https://github.com/docker/docker/issues/1916\n- The original idea of using volumes to share test assets does not\n  work out, because volumes are bound to containers (instances of images) and\n  can not be shared on DockerHub which is for sharing images.\n- I see a 30% performance degradation in the packetblaster/dpdk\n  benchmark when run in docker. I have not yet been able to figure out why.\n- The resulting docker image is 5GB...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/SnabbCo/snabbswitch/issues/588#issuecomment-136358697\n.\n. I think this idea is a great use case to explore the benefits of flow based\nprogramming.\nAs the number of options for I/O process increase, the possibility of app\npermutations will increase exponentially.\n\nRegarding item 4:\nAny to any I/O connectivity should not be assumed. Strict I/O path mapping\nshould also be supported. For example, if an FPGA or asic is added to the\nI/O library, it would only allow connectivity to specific other I/O as\ndefined by the hardware connectivity.\nOn Wed, Dec 30, 2015, 2:58 AM Luke Gorrie notifications@github.com wrote:\n\nI would like to improve our I/O infrastructure to make applications more\nflexible in the future:\n1. Programs should be able to use different I/O resources\n   interchangeably: NIC, vhost-user, raw socket, tap device, etc. You should\n   easily be able to make substitutions, for example to run Snabb NFV using a\n   tap device instead of a vhost-user socket, or an interprocess link instead\n   of a hardware NIC. App networks should become more composable.\n2. Processes should be able to independently access multiqueue I/O\n   resources. Transmit and receive queues should be directly addressable from\n   different processes. For example multiple Snabb processes should be able to\n   independently transmit/receive on their own private hardware TX/RX queues\n   of a NIC, or on their own private software Virtio-net vrings of a\n   multiqueue vhost-user connection.\n3. Hardware traffic dispatching features (RSS, VMDq, etc) should be\n   configurable in terms of queues. For example, one app can instruct hardware\n   which packets to send to queues 0,1,2,3 and then other apps in other\n   processes can processes the packets from those queues. (See #522\n   https://github.com/SnabbCo/snabbswitch/issues/522.)\n4. Hardware features should always have optimized software equivalents\n   available. This is to make all I/O resources compatible with all\n   applications and only have minimally different performance characteristics.\n   For example, hardware features like VMDq switching, RSS hashing, and VLAN\n   insert/remove should be available in software. This is to avoid hard\n   dependencies between application X and hardware Y.\nThis would imply a bunch of development:\n1. Device driver apps should allow separation between initialization\n   of hardware and attachment to transmit and/or receive queues. Like\n   described in #522 https://github.com/SnabbCo/snabbswitch/issues/522\n   and implemented in the draft intel1g app interface\n   https://github.com/lukego/snabbswitch/blob/intel1g/src/apps/intel/README.md#intel1g-appsintelintel1gintel1g.\n   This would allow different processes to be responsible for processing\n   individual queues.\n2. The vhost-user app would need to adopt this interface too. One\n   process should be able to connect to the vhost-user socket and somehow\n   permit other processes to map individual vrings for processing. See related\n   snabb-devel braindump\n   https://groups.google.com/d/msg/snabb-devel/pDd_uBXszrY/AQX04Sc8CwAJ.\n3. Some fallback mechanism will be needed to multiplex/demultiplex\n   access to I/O resources when direct access is not possible e.g. if multiple\n   processes want to access a single-queue device or the dispatching\n   capabilities of a multiqueue device are not sufficient (e.g. protocol not\n   supported by hardware RSS). See #685\n   https://github.com/SnabbCo/snabbswitch/issues/685 for more on\n   mux/demux.\nMore details needed as separate issues, but feel free to criticise the\nidea already now! :-)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/SnabbCo/snabbswitch/issues/687.\n. Swagger is great for documenting and describing APIs. Could the server or\nclient code gen portion could be extended to generate the necessary lua\ncode for server side scaffolding?  Perhaps that is not even necessary.\n\nOn Fri, 15 Jan 2016, 03:40 Kristian Larsson notifications@github.com\nwrote:\n\nIndeed, instance data is expressed as XML over a NETCONF transport.\nRESTCONF, which is on the standards track (22nd of Jan deadline for\nfeedback), allows XML or JSON.\nI don't think you need to deal with XML internally. You'd need a lib in\nbetween where you do config.container(\"foo\").leaf(\"bar\").set(\"asdf\") or\nsomething to set the value. If that container or leaf doesn't exist you get\nan exception. If the \"bar\" leaf is defined as type uint8 you get an\nexception. In the other direction the lib exposes the instance data as XML\nand JSON on top of which you then base your NETCONF / RESTCONF interface.\nThis is essentially what libyang tries to do, so easiest way forward is\nprobably to use it and write Lua bindings for it, perhaps with some more\nabstraction to get an elegant interface :)\nAs for NETCONF vs RESTCONF I think that NETCONF is better suited for a\ndevice while RESTCONF is probably something I'd put on the northbound\ninterface of my OSS system. My experience with RESTCONF is still very\nlimited (obviously not being a standard helps :P) but it seems limited\nwhich is why I'd prefer NETCONF for a device.\nI think you can use existing libs / daemons for the actual NETCONF server,\nlike https://github.com/choppsv1/netconf, freenetconfd or netopeer.\nPlease do have a peek at sysrepo too :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/SnabbCo/snabbswitch/issues/696#issuecomment-171660305\n.\n. Netconf is a zombie. It will soon follow the path of ATM and frame relay.\n\nFollow the enterprise. Go restful\nOn 16 Jan 2016 5:34 a.m., \"Kristian Larsson\" notifications@github.com\nwrote:\n\n@andywingo https://github.com/andywingo that was what I was trying to\nallude to with the \"Model translation\" topic in my previous post :)\nIt would probably quickly drive code complexity so I think we should\nignore that bit for now. Probably better to implement in a NC/YANG agent,\nusing XSLT, than in Snabb anyway.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/SnabbCo/snabbswitch/issues/696#issuecomment-172008583\n.\n. \n",
    "fmadio": "dont forget all that complex branchy logic / smarts on deciding to copy the packet, part of the packet, or whatever can confuse the hw data prefetcher. For my own usage the effectiveness of the prefetcher has been the primary optimization/profiling goal, not the theoretical minimum instruction throughput.\n. probably want to look at L3 miss rate and/or DDR Rd counters as it steps.\nGen3 x16 will max out ~ 112Gbps after the encoding overhead in practice. \n. Yeah 64/66 encoding is not connected to this at all, there`s absolutely no flow control when your at that level, its 103.125Gbps or 0 Gbps with nothing in between.\nThere should be some wide FIFOs before transferring to the CMAC and down the wire, but even then it should be at least 64B wide (read 512bit ) interface, which means 512b x say 250Mhz -> 128Gbps. More importantly that will effect the PPS rate which even at 128B packets would clock in at 125Mpps (2 clocks @ 250Mhz). My money is on L3/LLC or UnCore or QPI cache/request counts.\n. Yup its probably QPI / L3 / DDR some where some how. Assuming the Tx payloads are unique memory locations the plateau is the PCIe requestor hitting a 64B line somewhere, the drop is additional latency  to fetch the next line, probably  Uncore -> QPI -> LLC/L3.  Note that the PCIe EP on the Uncore does not do any prefetching such as the CPUs DCU streamer, thus its a cold hard miss... back to fun days of CPUs with no cache!\nIf you really want to dig into it suggest getting a PCIe sniffer but those things are dam expensive :(\n. Cool, one thing totally forgot is 112Gbps is PCIe Posted Write bandwidth. As our capture devices is focused on Write to DDR I have not tested what the Max DDR Read bandwidth would be, its quite possible the system runs out of PCIe Tags at which point peek Read bandwidth would suffer.\nProbably the only way to prefech data into the L3 is via the CPU, but that assumes the problem is L3 / DDR miss not something else. Would be interested if you limit the Tx buffer address to be < total L3 size. e.g. is the problem L3 -> DDR miss or something else.\n. Also, for the Max PCIe/MLX4 green line. Looks like your off by 1 64B line some how ? \n. A few things.\n1) Pretty much all devices support \"PCIe Extended Tags\" which add a few more bits so you can have alot more transactions in flight at any one time. E.g. think about GPU`s reading crap from system memory ... nvidia, intel & co have a lot of smart ppl working on this.\n2) In practice you`ll run out of PCIe credits first. This is a flow control / throttling mechanism that allows the PCIe UnCore to throttle the data rate, so the UnCore never drops a request. For both Posted & Non-Posted requests, it gets split further into credits for headers and credits for data.\n3) Latency is closer to 500ns RTT last time I checked, putting it half that going one way. Keep in mind for non-posted reads from system DDR its a request to PCIe UnCore, then response so full RTT is more appropriate. Of course these are fully pipelined requests so 211ns sounds close.\nFor 100G packet capture we dont care about latency much, just maximum throughput thus havent dug around there much. We`ll add full nano accurate 100G line rate PCAP replay in a few months at which point latency and maximum non-posted read bandwidth becomes important.\n4) All of this is pretty easy test with an fpga. Problem is I dont have time mess around with this at the moment. \n. HDL these days is almost entirely packet based, all the flow control and processing inside those fancy asic`s are mostly packet based. So all the same algos are there, different names and format but a packet is still a packet regardless of it contains a TCP header or QPI header.\nSurprised the devices shows up as x16, means you`ve got PLX chip there somewhere acting as bridge. It should be 2 separate and distinct pcie devices.\nYou can`t just make a PCIe sniffer, a bridge would be easier. You realize a Oscilloscope that capable of sampling PCIe3 signals will cost $100-$500K ?  those things are dam expensive. PCIe sniffer will \"only\" cost a meager $100K+ USD.\nOn the FPGA side monitoring the credits is pretty trivial. Forget if the Intel PCM kit has anything about PCIe Credits or monitoring Uncore PCIe FIFO sizes. Its probably there somewhere, so if you find something would be very cool to share.\n. wow im blinded by the shinyness, very cool. \nR2PCIe.* looks interesting, would have to read the manual to work out what each actually mean.\n. cool thanks for sharing.\nmight also find the following discussion relevant. Uncore freq scaling really only helps for extreme microbursts and likely unrelated to anything here. Would suggest tho is monitoring your uncore frequency in realtime, its pretty trivial via MSR`s\nhttps://software.intel.com/en-us/forums/software-tuning-performance-optimization-platform-monitoring/topic/600913\n. to be clear its something like this\nstruct\n{\n     40b descriptor stuff / size / blah\n     8b   payload io addresss\n     16b first payload\n}\nstruct\n{\n     60B of payload data (including duplicated 16bit in descriptor)\n}\n(e.g. for 60B payload)\nWhat happens if every single Tx packet you send, you make the payload IO address (in the descriptor) split across a 64B boundary?  and compare it to nicely aligned single cache line address.\nThis is a nice pic from anandtech.\n\nhttp://images.anandtech.com/doci/8423/HaswellEP_DieConfig.png?_ga=1.10720884.1957720941.1472434760\n. for the graphs. when you say its a 508B packet, do you mean 508B of payload + 4B FCS? or do you mean 508B includes the FCS.\nIts also possible the controller by default assumes the FCS is in the payload size, then the descriptor has a flag to tell it generate it. In the 99.9% of case the extra 4B fetched over DMA is 0x00000000/garbage, but when you disable hardware FCS the driver or kernel or someone calculates it\n. Congrats on finding the reason for non pow2 packet size stepping. Makes total sense.\nWould expect 100G NICs to go Gen4x8. Tho, that assumes the CPU will provide the same number of lanes as the Gen3 chips.  Given that most NICs cant do line rate anything on the first few generations, its not that surprising. \n. Seriously doubt the PCIe problem is on the CPU side. Its not something the architecture guys at intel would overlook and fully expect you can get > 100Gbps posted read bandwidth to a PCIe device. Having said that, we had to optimize our fpga for maximum sustained throughput at the PCIe level. The naive implementation of a DMA controller will not reach maximum 100Gbps+ throughput.\nBased on the numbers suspect ConnectX DMA controller is a some what naive implementation and not optimized for 100Gbps throughput on Gen3x16. \nAs you mentioned the wiggle room is very tight, a sloppy implementation wont cut it. The problem is likely the CPU throttling the NIC by PCIe posted packets per second (posted header credits). So ... NIC guys could do something more smart (unlikely as it requires a completely different architecture to be verified) or just wait for Gen4x16. Expect the latter.\nThink about the problem differently. Whats the maximum number of PCIe packets the CPU Gen3x16 side can process per second :) \n.... hint: its not 148Mpps\n. Youre also welcome to try our fpga 100G NICs for snabb <g>\n. Thats interesting youre only seeing a 20% bump for small packets with x2 ConnectX boards.  Suspect its a bottleneck in the Uncore somewhere, based on maximum TLP throughput (PPS not Gbps) rather than L3. e.g. Intel designers made the assumption of a certain Request to Payload ratio < X. That X ratio likely based on whatever the GPUs are doing at the TLP level to hit peek read bandwidth to GDDR on Gen3x16.\nThe Internal Ring, DDR controller or L3 are unlikely to be a bottleneck. The bandwidth requirements of these with all cores running heavy workloads must be far greater than 200Gbps IMHO.\nGen3 has also added \"TPH\" Transaction Processing Hints to the PCIe packet so the PCIe EP can tell the Uncore how the data is to be used. So far in practice (on Ivy bridge atleast) it seemed like the CPU completely ignores the hints. Not tested it on Haswell yet.\nAs for a fmadio 100G NIC for snabb. Its an economy of scale thing. We will have all the HDL IP components to make a full line rate 100G NIC. Its actually part of our roadmap to support a fully open Rx/Tx path on our capture/generate system. This allows customers/partners to build their own layer 4-7 protocol decoders/generators ontop of our box. Taking that and providing it as a standalone NIC + fully open spec is a logical extension. Expected 0 market traction for that thus never pushed it. \nTo answer some of your questions:\n1) Price\nIts expensive ~ $10K per board includes everything. But its still cheap when compared to a Napatech or other FPGA vendors.\n2) Availability\nYes easy to buy in low volumes ( < 10 boards). Board vendor keeps stock as do we, its not BTO 8-12 week lead times like most FPGA vendors.\n3) Open spec\nYes fully open software (HDL is proprietary) published on github etc etc. open and community driven spec, exactly like Snabb. Would be an interesting approach actually, can`t think a similar project.\n.. can also operate in 8x10G or 2x40G mode with firmware changes. If you can get COTS to work and they will get there eventually thats a far better approach. But if full line rate 100G Rx/Tx is a requirement I think your only real option is an FPGA solution in the near term. I know how hard it is to get line rate with todays hardware spec... had to re-write our HDL a few times to get there.\n(*HDL -> Hardware Description Language e.g. Verilog / VHDL)\n. @lukego Thanks, hope the blog its helpful to people.\nMy experiences with NetFPGA has been poor, to incredibly bad. Its focused on education/research community, certainly not for a commercial product in telecoms.\nHave put some serious thought into opening up all our HDL on multiple occasions, as we need to compete in non-traditional ways to disrupt the packet capture market.  Much like Snabb is disrupting the existing vendors by approaching problems in a different way.\nSome thoughts.\n1) The NIC is only part of capture systems. Its an important component but our business is focused on complete solutions. Rack it, Turn it on, Capture. No messing around with boards or software or other crap. Because of this, the impact of making our NIC widely available is probably low.\n2) Because we are a commercial vendor using/developing the code our profitability is independent from revenue directly from the NIC. It means the code has legs, and can be bootstrapped into a sustainable long term project.\nProblems:\n3) What kind of advantage does this give our capture competitors? This is my primary concern, and need to consider more deeply.\n4) Who would actually purchase commercial licenses of this? Any idea on who and what kind of volumes?\n... and of course you guys know  the benefits of open development, dont need to say anything there. Its the primary advantage of doing it.\nAs mentioned earlier, if you can get an ASIC to meet your requirements thats by far the best option. But if 100G line rate anything is a requirement, will be hard perhaps impossible in the next 18-24 months. \nIts a pretty exciting topic IMHO. This would be the first time (?) a commercial grade NIC is released as living open HDL code, driven by the community. If people get onboard and it gains momentum who knows where it might lead.\n. @lukego  Struggling to see how this could work. e.g. would you personally contribute code to a project, where that code is directly licensed to makes someone else money? This is quite a different model to making commercial products/services that use opensource code. Its literally charging for the code, seems something is wrong. \nThe other thing to consider is the pool of people who could contribute. Its an unbelievably tiny number of people. The whole idea of software guys learning and writing HDL is completely rejected by 99% of the ASIC/FPGA community as laughable. Which even further reduces who could contribute to such a project. Also keep in mind this is cutting edge high speed network designs here, poking around with HDL in university means almost nothing.\nback on topic, you`ll need to re-consider your ABI approach in the future IMHO. In the 148Mpps era the usual descriptor/payload design no longer works. You need to optimize and minimize the number of PCIe transactions to hit line rate numbers. In this model descriptor and payload are all inlined and just a chunk of memory the DMA controller fetches in large batches e.g. probably the intention of the ConnectX 16B payload in the descriptor design.\n. Its a good point the value of QA has to such a project, plus the ability to integrate your requests/changes upstream into a production level quality product is quite powerful. That and transparent source / drivers could make for a compelling solution. Requiring a paid license is secondary to transparency and ability to modify.\nSo many times Ive wanted to  modify and improve a vendors product but its either fully closed,  documentation is firewalled, support non existent etc etc. Most certainly would have invested the time and energy to add/improve the product had the source/docs been available, wouldn`t really care if it gets merged upstream(would certainly be good tho). And having the contribution owned by the vendor is far less important than the vendors product actually solving my problem. Its a compelling thought, been in that position too many times to count. In that exact position now with SSD firmware actually. Your thoughts?\nYour underestimating the difficulty of HDL i think. Will try make an shoddy analogy here. My guess is your only seeing the difficulty of understanding say Verilog. To be more concrete, the difficult of understanding assembler instructions/code is quite different to the skillset required to build a complex program written entirely in assembler. In HDL the difficulty is building complex behaviours and abstractions that can run on real hardware. As all the mental models and algos from software are completely useless and in fact a negative influence on how to think about problems.\nIt took me ~ 2 months of 100H / week to get a very basic foot hold with HDL. Another 1 year to reach a comfortable ability, and now ~ 5 years on can move between C or Verilog, in the same way as switching between C with Lua. Its hard, but someone of your skillset could bridge the gap in a few months I think, even quicker if there`s a cool project to hack on :)\nEdit: to be clear, foot hold with HDL means writing non-trivial programs running on real hardware.\n. Guess once you take the bitterly painful Red pill the world looks different. Dont like the descriptor format.. then change it. Its truely liberating to not be constrained by whatever crap ASIC vendors deliver. Some what similar to working with closed source vs opensource - freedom.\nIn that context I`m offloading as much as possible to the fpga, so the CPU can be used for high level things, protocol decoding / analysis / etc etc. Would imagine Snabb with an open fpga could do some pretty amazing things, problem is the costs go up significantly. And seems one of your primary goals is reducing cost per port, something an fpga does no do.\nSurprised ConnectX4 runs at 156Mhz, its awfully low, almost too low. Guess you guys need to decide if the cost of sustained 100G line rate is worth it, even Napatech dont do this :)\n. @lukego I`m still game for this, except HDL can not be opensource. Could do source access with an NDA if you want to hack on it tho. Driver and other stuff can be full GPL.\nYour starting to realize some of the cool things that can be done if you throw out the existing ring/descriptor paradigm, the architecture starts looking pretty different :). @lukego yes saw lowRISC, its a non-profit and grant (pseudo VC) funded. We are obviously for-profit and bootstrapped, almost the complete opposite.\nThe PCAP header is horribly inefficient and slow. We do use a 16B header except its much better packed and optimized for HDL.\nTry work us, maybe being fully open will be less important to you once we get a rapid feedback loop running. Could do some pretty cool things with our NIC + Snabb I think.. Re: Headline blocking, would expect its reasonable to just drop packets on any egress ports who`s FIFO is full ? . @mlilja01 cool, when did you achieve sustained line rates? Last time I checked your 100G boards were burst capture only and limited to 40Gbps of host bandwidth?. @mlilja01 ok, guess I can no longer mouth off about it :) tho you need to tell the marketing folks.. ",
    "gonzopancho": "\nit seems that if the working set can be kept within L3 cache then the packet data need never be written out to DRAM. I also read there that Intel are planning to improve DDIO so that you don't have to worry about NUMA affinity.\n\nXeon E5-2xxx processors (Sandy Bridge/v1, Ivy Bridge/v2, and Haswell/v3) have PCIe interfaces on each processor (normally expressed these days as a 'socket', in order to express a collection of cores and associated circuitry on a discrete substrate of silicon and ceramic.)  Whether these are all exposed on the motherboard depends on the vendor. In any case, if the NIC is on socket N, and the processing happens on any socket other than N, then the DMA (even DDIO) is going to have to happen across some intra-socket link.\nQPI gets replaced in the \"Skylake\" CPUs with UltraPath Interconnect (UPI).  UPI will operate at 9.6GT/s or 10.4GT/s data-rates and will be considerably more efficient than today\u2019s QPI as it will support multiple requests per message.\nSo it will be faster, but won't eliminate the problems of QPI.\nI think NUMA affinity will be with us for a long time to come.\n. netmap uses this, which is similar.\nC\n/*\n * this is a slightly optimized copy routine which rounds\n * to multiple of 64 bytes and is often faster than dealing\n * with other odd sizes. We assume there is enough room\n * in the source and destination buffers.\n *\n * XXX only for multiples of 64 bytes, non overlapped.\n */\nstatic inline void\npkt_copy(void *_src, void *_dst, int l)\n{\n        uint64_t *src = _src;\n        uint64_t *dst = _dst;\n        if (unlikely(l >= 1024)) {\n                memcpy(dst, src, l);\n                return;\n        }\n        for (; likely(l > 0); l-=64) {\n                *dst++ = *src++;\n                *dst++ = *src++;\n                *dst++ = *src++;\n                *dst++ = *src++;\n                *dst++ = *src++;\n                *dst++ = *src++;\n                *dst++ = *src++;\n                *dst++ = *src++;\n        }\n}\n. They probably measured it (actually, I know they did) and found it faster than the bcopy()/memcpy() in the freebsd and linux kernels.\nUsing clang on freebsd 11, \njim@x230:~ % clang -v\nFreeBSD clang version 3.7.0 (tags/RELEASE_370/final 246257) 20150906\nTarget: x86_64-unknown-freebsd11.0\nThread model: posix\nand cc -S, the inner loop that is generated is thus:\n.LBB0_3:                                # %for.body\n                                        # =>This Inner Loop Header: Depth=1\n    movq    (%rax), %rcx\n    movq    %rcx, (%rsi)\n    movq    8(%rax), %rcx\n    movq    %rcx, 8(%rsi)\n    movq    16(%rax), %rcx\n    movq    %rcx, 16(%rsi)\n    movq    24(%rax), %rcx\n    movq    %rcx, 24(%rsi)\n    movq    32(%rax), %rcx\n    movq    %rcx, 32(%rsi)\n    movq    40(%rax), %rcx\n    movq    %rcx, 40(%rsi)\n    movq    48(%rax), %rcx\n    movq    %rcx, 48(%rsi)\n    movq    56(%rax), %rcx\n    movq    %rcx, 56(%rsi)\n    addl    $-64, %edx\n    addq    $64, %rax\n    addq    $64, %rsi\n    cmpl    $64, %edx\n    jg  .LBB0_3\ncompiling with -O3 (clang has -O4 == -O3) and using gdb to disassemble:\n```\n(gdb) disassemble pkt_copy \nDump of assembler code for function pkt_copy:\n0x0000000000000000 :    push   %rbp\n0x0000000000000001 :    mov    %rsp,%rbp\n0x0000000000000004 :    mov    %rdi,%rax\n0x0000000000000007 :    cmp    $0x3ff,%edx\n0x000000000000000d :   jg     0x70 \n0x000000000000000f :   test   %edx,%edx\n0x0000000000000011 :   jle    0x6e \n0x0000000000000013 :   add    $0x40,%edx\n0x0000000000000016 :   nopw   %cs:0x0(%rax,%rax,1)\n0x0000000000000020 :   mov    (%rax),%rcx\n0x0000000000000023 :   mov    %rcx,(%rsi)\n0x0000000000000026 :   mov    0x8(%rax),%rcx\n0x000000000000002a :   mov    %rcx,0x8(%rsi)\n0x000000000000002e :   mov    0x10(%rax),%rcx\n0x0000000000000032 :   mov    %rcx,0x10(%rsi)\n0x0000000000000036 :   mov    0x18(%rax),%rcx\n0x000000000000003a :   mov    %rcx,0x18(%rsi)\n0x000000000000003e :   mov    0x20(%rax),%rcx\n0x0000000000000042 :   mov    %rcx,0x20(%rsi)\n0x0000000000000046 :   mov    0x28(%rax),%rcx\n0x000000000000004a :   mov    %rcx,0x28(%rsi)\n0x000000000000004e :   mov    0x30(%rax),%rcx\n0x0000000000000052 :   mov    %rcx,0x30(%rsi)\n0x0000000000000056 :   mov    0x38(%rax),%rcx\n0x000000000000005a :   mov    %rcx,0x38(%rsi)\n0x000000000000005e :   add    $0xffffffffffffffc0,%edx\n0x0000000000000061 :   add    $0x40,%rax\n0x0000000000000065 :  add    $0x40,%rsi\n0x0000000000000069 :  cmp    $0x40,%edx\n0x000000000000006c :  jg     0x20 \n0x000000000000006e :  pop    %rbp\n0x000000000000006f :  retq \n0x0000000000000070 :  movslq %edx,%rdx\n0x0000000000000073 :  mov    %rsi,%rdi\n0x0000000000000076 :  mov    %rax,%rsi\n0x0000000000000079 :  pop    %rbp\n0x000000000000007a :  jmpq   0x7f\nEnd of assembler dump.\n```\nGiven that\nRAX, RBX, RCX, RDX, RBP, RSI, RDI, RSP, and R8-R15 are the labels of the 16 64-bit registers, I think we have an answer.\nAnd it's not dissimilar to the fairly modern code in glibc: http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/arch/x86/lib/memcpy_64.S?id=HEAD\nthe body of the 'forward' loop is\n```\n.Lcopy_forward_loop:\n    subq $0x20, %rdx\n/*\n * Move in blocks of 4x8 bytes:\n */\nmovq 0*8(%rsi), %r8\nmovq 1*8(%rsi), %r9\nmovq 2*8(%rsi), %r10\nmovq 3*8(%rsi), %r11\nleaq 4*8(%rsi), %rsi\n\nmovq %r8,   0*8(%rdi)\nmovq %r9,   1*8(%rdi)\nmovq %r10,  2*8(%rdi)\nmovq %r11,  3*8(%rdi)\nleaq 4*8(%rdi), %rdi\njae  .Lcopy_forward_loop\naddl $0x20, %edx\njmp  .Lhandle_tail\n\n```\nIf you assume AVX, then glibc has already done the work for you\nhttps://sourceware.org/git/?p=glibc.git;a=blob;f=sysdeps/x86_64/multiarch/memcpy-avx-unaligned.S;h=9f033f54568c3e5b6d9de9b3ba75f5be41070b92;hb=HEAD\ndpdk has some good work: http://dpdk.org/browse/dpdk/tree/lib/librte_eal/common/include/arch/x86/rte_memcpy.h\nand a test harness:\nhttp://dpdk.org/browse/dpdk/tree/app/test/test_memcpy_perf.c\nThis is ancient history, but when I was at Convex (1986-ish?), it was trivial to write a short 'C' program that would saturate the memory bus.  A couple 'for' loops would do it.  Of course, Convex had a compiler that would vectorize the loops...\nNetmap, of course, has a lot of architecture to avoid copies.  Avoided copies are the fastest copies.\n. Cool.  Your \"merge all changes while preserving performance\" is literally the theme of the paper that George Neville-Neil and I did: \"Measure twice, code once\".  Of course, preserving performance starts with observing performance.\nhttps://github.com/gvnn3/netperf\nhttps://github.com/gvnn3/conductor\nI think it's likely that the finding will be that the copy is cheap, but its effect on the cache (pollution of same) may be quite expensive.\n. ",
    "fozog": "Hello!\nIn the case of packet copies, the instructions are not that important. The internal architecture of the processor has Execution Units. Among those, you have load/store \"ports\" that limit the number of parallel requests to memory:\nhttp://www.anandtech.com/show/6355/intels-haswell-architecture/8\nhttp://www.anandtech.com/show/9582/intel-skylake-mobile-desktop-launch-architecture-analysis/5\nFor Skylake, as far as I know, you can only 2 memory operations in parallel, not 3, and this is valid for the whole core, not per thread. The net result is when you copy all packets for 148Mpps of a 100Gbps at line rate, hyperthreading is useless, worse, you loose \"turbo\" benefits.\nBut execution core are not the only limits. Cache is a precious resource and copying reduces by a factor of two the active dataset. Then you have to ensure proper positioning because there are also other issues related to cacheline placement in an Intel processor. A hash is calculated and a cacheline is stored on the relevant CBo (Cachebox: http://images.anandtech.com/doci/8423/HaswellEP_DieConfig.png?_ga=1.10720884.1957720941.1472434760). So the CBo itself is a limitation. In addition, you reach the CBo by moving the cacheline in store and forward mode along the dual ring(s) of the processor.  In large core count (>16) there are two dual rings interconnected. So storing a cacheline may further incur other costs through crossing dual ring bridge. I made measurements 4 years ago (may be very different now) and packet copy latency and throughput can be impacted as much as 30% depending on the core you use and the CBo set that happen to be mostly touched...\nLastly, cacheline activity may incur eviction, which also touches on the limits of the Integrated Memory Controllers... There is a queue depth of 10 requests to memory per channel.... So the theoretical limit would be 16 cores doing 2 memory requests in parallel on a 4 channel system with perfectly balanced memory locations. But based on the DIMMM timings to answer, the activity of the prefetcher, you cannot have that many parallel memory transactions. I never made a scalability measurements for memory bandwidth and latency of this type, so I don't have data to share.\nAnother angle would be to use non temporal hints to leverage \"Write Buffers\" instead of cachelines, but that is also a scarce resource of the processor.\nBottom line, for packet copy impact evaluation, instruction count is the tip of the iceberg problem (you may not even know if REP MOVSB does not translate into more efficient micro-instructions than your two AVX512 instructions).\nFor high performance networking, memory subsystem is key. The early generation of Xeon E5-4600 had a very bad \"Home Agent\" that routed memory request along a uni-directional ring among the 4 NUMA nodes. So the cost to access remote memory was incredibly high.\nSo to assess the impact of packet copy, you must figure all the details of the life of a cacheline outside the instructions themselves. And you can't stay at high level and picture a 20MB L3 cache anymore: you have independent CBos interconnected by a dual ring...\n. PCIe has a limitation in number of DMA transactions that can be completed. As a result, for NICs that transfer one packet at a time, Mpps = Mdma/s. This is why most of the NICs cannot achieve 40Gbps 64 byte line rate.\nThat said, a NIC based on Chelsio chips or Xilinx FPGAs have a different DMA transfer strategy and coalesce multiple packets per DMA transactions: Netcope can reach 148Mpps on a 100Gbps link, Chelsio can achieve 60Mpps on a 40Gbps link.\nHandling multiple packets per second, imply that packet placement in buffers is driven by the hardware, not the software. So to leverage that, the packet handling software framework must handle the situation.\nDPDK applications tells the hardware where to put each individual packet (ring is allocated and each ring entry points to an area where the HW should put the packet). Dealing with HW managed packet placement need the application to give large buffers (2MB, 32MB...) to HW: the application initialize the ring with empty slots (as opposed to pointing to a packet location). Upon packet reception, the HW place packets in the large buffers next to each other (may be with some head/tail room), then it update the descriptors to tell where it actually placed the packets.\nProblem is that this is at the opposite of DPDK logic. So to be able to write PMDs for those NICs, you need to have intermediary buffers and copy packets back and forth between hardware buffers and rte_mbufs.\nBut by doing this copy, the memory becomes overloaded, and there is not left much for application logic and \"databases\" (MAC, FIB,...).\nSo DPDK may not be the best suited software packet handling framework for high end networking..... @lukego Thanks. \nA DMA transaction needs a completion.  So we have the current timing:\nt0: NIC DMA engine send 64 byte packet = 64 + 24 (PCI decorations) bytes to transfer\nt0+12ns: CPU DMA engine (in Intel that would be the uBOX in the uncore domain) send to L3 cache (best case -> 20ns)\nt0+32ns: CPU DMA engine send completion\nt0+34ns: NIC DMA engine can send another packet.\nThis means roughly 30M DMA transactions per second...\nIf I am not mistaken, early completions are sent when the DMA is received at the uBOX. but because of delays, you can't have too many early completions, hence the credit mechanism. In addition this theoretical reasoning assumes that all packets fit in L3, which may be possible for L2 switch that just operates on MAC, but typically not possible for complex firewalls, openflow switches with decent number of rules...\nSo concretely, this means that the maximum observable is 36M DMA transactions per second. \nHaving mega fast DMA engines at both CPU and NIC will not change the result.\nIf we double DMA capacity for PCIe gen4 and use x16 ports, it may be possible to support 100Gbps line rate but I think it is weak and depends on perfect distribution of packets in queues and limits the functionality that can be applied. In addition, the talks I have with people designing for 400Gbps conclude that moving one packet at a time is a huge waste while moving packets in batch allow more time budget for packet handling (it is an intuitive truth don't you think?).\nOne scarce resource of data centers in square meters, or more precisely volume. On a 1U server, you can squeeze the CPU power to handle 200Gbps of data (I made the test with a simple web server built on commercial TCP/IP stack leveraging DPDK) but only two NICs.... So using 10G adapters instead of 40G or 100G is not really efficient...\nDid I won the cold beverage ?\n. You can use Chelsio NICs that implement such coalescing scheme since 2012 (not sure about the date, I mean this is not new) with their T4 chip. The kernel driver wraps SKBs around packets placed in the buffers managed by T5. It is unclear whether the DPDK driver supports T4 and T5 chips.\nThe key challenge is to coalesce packets on the send side because it is the software that has to do that.\nAccording to statistics I gathered from Internet Exchanges a few years ago, 64 byte packets account for 30% of packets observed on a 10Gbps link (TCP SYN, SYN-ACK, ACK, RST; some UDP...). Being line rate at 256 bytes is the typical metric of telecom operator RFPs so I could agree with you that this is the practical line rate target.\nThat said, when you have DDoS (TCP SYN or DNS amplified) then 64 byte packets are representing the vast majority of packets.\nSo the 64 byte packet rate behavior becomes the metric for attack resilience. For a DPI or a security box, you want 64 byte line rate. For other nodes in the network, 256 line rate is good enough.. @lukego 5.9 cycles per RAM! That was a stricking number when I mentally compared to DRAM timing characteristics I am aware of. The site says 5.9ns. Well that seemed more realistic, but if you read on top it says \"RAM Latency = 42 cycles + 51 ns\" which is what I would have thought. I am affraid the 5.9ns calculation is close to the following reasoning:  \"with nine women we can get a baby in one month\".\nBut you are making very good points on the future of PCI and and L3. I think the fundamental benefit of coalesced packets is the cache protection. By having 2K aligned packet buffers, I suspect that packets are not that well distributed and tend to hit particular areas of the L3 cache (no concrete measures made) and the number of cache ways may not suffice to absorb the pressure.\nBy contrast, consecutive packets should dispatch cachelines to all L3 slots.. ",
    "petebristow": "Is is particularly meaningful to know which app is dropping packets due to overload? Presumably the back pressure is being caused by an exit point from the app network be it IPC link / NIC / Tap interface and you could just track it from there. The more overloaded the system the closer the 'dropping' app will be to the source of the packets.\nIf a ratelimiter app is dropping packets because your trying to send 11meg down a 10meg cap then that's meaningful but tracking of that is something internal to the ratelimiter app. \n@plajjan I don't think push() is run until the app network is empty. If an app stalled because it's output link was full it wouldn't get run again if it's output link subsequently emptied whilst running push() on the rest of the app network unless one of it's input links saw more traffic.\nThe contract between breathe apps in the network is a little bit hazy.\n@lukego My reading of breathe suggests you missed a step\n1a. Call push() on every app, regardless of traffic on the input link, Repeater wouldn't work without this.\nThe current 'contract' is defined as\n\n\u2014 Method myapp:pull\nOptional. Pull packets into the network.\nFor example: Pull packets from a network adapter into the app network by transmitting them to output ports.\n\u2014 Method myapp:push\nOptional. Push packets through the system.\nFor example: Move packets from input ports to output ports or to a network adapter.\n\nWhat if we amended this contract.\n\u2014 Method myapp:pull\nOptional. Pull packets into the network, this is the only way an unbounded number of packets can enter a snabb network.\nFor example: Pull packets from a network adapter into the app network by transmitting them to output ports.\n\u2014Method myapp:push\nOptional. Push packets through the system. \n1. The number of new packets that a push() can introduce into a network must be bounded by some function over the number of packets received on an input link. Generating a packet in response to an ICMP echo request is fine, while true transmit(o, packet.clone(self.packet)) is not. \n2. A push() should not transmit() packets to a full output link, unless they are leaving the app network, NICs / IPC links can drop packets buffers are full.\n3. After a sufficient number of push() calls all input links to the app should be empty.\n. I've removed the ability to set the MAC address, if it needs to be done it should be done with the standard linux commands, including it leads to a slippery slope of supporting all ip2 commands which would be crazy. \nThere is a selftest which replays pcaps. \nTEST_SKIPPED is exported by the Makefile so it can be used in tests.\nAdditional comments have been added to make the code clearer.\nIs using a network namespace reasonable to isolate tests? Where/how is the test suite run?\n. @lukego \n- packet.free issue is fixed\n- I'll open another CR to harmonize the macaddress parts and we can debate that there. \n- As a strawman how about \n  docker run -w /snabb -u root --privileged=true --rm=true -it -v $PWD:/snabb lts /snabb/apps/tap/selftest.sh\n  - Ubuntu LTS can be swapped out for other targets as required\n  - A docker image of the target OS is a prerequisite but this aids testing against the target in future.\n  - Builds need to be done inside the target container\n- Can you expand a little on this. I'm not seeing the use case where packets will arrive with the 'wrong' mac address so promiscuous mode is required.\n@kbara If other tests are using Tap I think they should have supporting selftest.sh that setup the environment before hand. For some of the other apps I'm working on it would be 'nice' to have support to set ipv4/v6 addresses and setting the MTU which would warrant inclusion if setting the mac makes it in. Then Tap turns into a probably buggy ifconfig reimplementation. \n. Suppressing stack traces for configuration/setup errors and the like would be great. \n. Sounds like a good plan have you done much work on making it a reality ?\nAre all the existing benchmarks based around the scripts in the bench/ directory ?\nI think adopting a standardized benchmark run 'data format' would be good, json?. The existing benchmarks just give a pps value as output. \nHave you done any work on this ?\nI've got some preliminary work on a 'benchmark' app that works in a similar style to 690, allowing embedded micro benchmarks for each app. I've used it to start looking into questions such as \n- What is the base pps cost of an app? (that does nothing but transmit(output, recieve(input)))\n- What does an app cost compared to composable function calls?\n- Do large numbers of idle links have a material pps impact on the system as a whole?\nAll my uses have high pps with small packets and none use VMs so it would be great if whatever goes forward isn't completely nfv centric.\n. @eugeneia \nGreat review thank you.\nHarness doesn't play well if you need more supporting apps to correctly test the candidate app. \nIs there or should there be a wider framework?\nTestSink was a response to the following\n- selftest() in socket/raw.lua looks painful for a replay type test\n- selftest() in socket/raw.lua ignores the race condition of interacting with linux, fuzzy helps here\n- selftest() in tap/tap.lua doesn't work as it miscalls C.memcmp\n- selftest() in tap/tap.lua tries to reimplement engine.main() badly\n- not calling engine.configure(config.new()) between tests can leave debris from previous test runs, counters for example\n- A pcapfile1 == pcapfile2 doesn't given any indication which packets are broken or how.\n- The required boilerplate leads to shortcuts, along the lines of  foo == encode(decode(foo)), which was used in selftest in tunnel.lua. Which has caught me out in the past where both encode and decode have the same error. harness doesn't support directly messing with config\n- There are at least 2 approaches to test failure, os.exit(), error(). It might be nice to have something a bit more structured. \n``` lua\n-- I've removed the simple benchmark as it doesn't really test anything.\nfunction selftest ()\n   print(\"Keyed IPv6 tunnel selftest\")\n   local tunnel_config = {\n      local_address = \"00::2:1\",\n      remote_address = \"00::2:1\",\n      local_cookie = \"12345678\",\n      remote_cookie = \"12345678\",\n      default_gateway_MAC = \"a1:b2:c3:d4:e5:f6\"\n   } -- should be symmetric for local \"loop-back\" test\nlocal testsink = require(\"apps.testsink.testsink\")\n   assert(testsink.harness(SimpleKeyedTunnel, tunnel_config,\n      { \"dencapsulated\" = \"apps/keyed_ipv6_tunnel/selftest.cap.input\" }, \n      { \"encapsulated\"  = \"apps/keyed_ipv6_tunnel/selftest.cap.output\" }\n   ))\nassert(testsink.harness(SimpleKeyedTunnel, tunnel_config\n      { \"encapsulated\"  = \"apps/keyed_ipv6_tunnel/selftest.cap.output\" },\n      { \"dencapsulated\" = \"apps/keyed_ipv6_tunnel/selftest.cap.input\" }\n   ))\nend\n```\n``` lua\n-- tap.lua\nfunction selftest()\n-- tapsrc and tapdst are bridged together in linux. Packets are sent out of tapsrc and they are expected\n-- to arrive back on tapdst. Linux may create other control-plane packets so to avoid races if a packet \n-- doesn't match the one we just sent keep looking until it does match. \n-- The linux bridge does mac address learning so some care must be taken with the preparation of \n-- selftest.cap A mac address should appear only as the source address or destination address\n-- This test should only be run from inside apps/tap/selftest.sh\n   if not os.getenv(\"SNABB_TAPTEST\") then os.exit(engine.test_skipped_code) end\nlocal testsink = require(\"apps.testsink.testsink\")\n   assert(testsink.harness(Tap, {},\n      { \"input\" = \"apps/tap/selftest.cap\" }, \n      { \"output\"  = \"apps/tap/selftest.cap\" },\n      { fuzzy = true }\n   )\nend\n```\n. # Benchmarking\nSolid benchmark coverage means we can make changes with confidence there aren't negative impacts. This benchmark module is inspired by the golang testing module. It supports placing benchmark specifications along side the code being benchmarked. If benchmarking / testing is made sufficiently painless it will be more widely adopted.\nJson was chosen as it gives flexibility that csv doesn't. \n- SNABB_DPDK is important to NFV but irrelevant to benchmarkTee\n- Stats on multiple links may be of interest to some benchmarks\nsnabbmark\n./snabb snabbmark lib modulename [funcRegex] loads modulename and benchmarks all functions matching funcRegex which defaults to bechmark.*\nIt is expected that benchmark specific configuration be passed through environment variables. Environment variables should also contain any other information pertinent to the benchmark such as DPDK version. Whilst this information is not used by snabbmark directly it needs to be included in the resultant json result.\nSNABB_PCI0=0000:01:00.0\nSNABB_TELNET1=5001\nSNABB_DPDK=1.7\nBenchmark results are output as json objects, 1 object per result.  \njson\n{\n  \"duration\": {\n    \"actual\": 5.000008995994,\n    \"requested\": 5\n  },\n  \"hostname\": \"dock\",\n  \"date\": \"2016-01-01T03:05:51Z\",\n  \"hz\": false,\n  \"module\": \"apps.basic.basic_apps\",\n  \"links\": {\n    \"input\": {\n      \"bytes\": 18651403800,\n      \"packets\": 310856730\n    }\n  },\n  \"test\": \"benchmarkSourceToSink\",\n  \"busywait\": false,\n  \"env\": { \n  }\n}\nEmbedded benchmarks\nAny function of the form\nfunction benchmarkXXX(c) return c end\nwill be timed by snabbmark. c is a core.config object that describes the benchmark with the following additions.\n- c.duration is passed to engine.main\n- c.Hz is passed to engine.Hz\n- c.busywait is passed to engine.busywait\n- c:src(\"appName\", \"pcapfile\") will create a PcapReader / basic_apps.Repeater app pair such that appName.output is a continuous loop of the packets from pcapfile\n- config.app(c, \"sink\", basic_apps.Sink) has already been created. Packet and byte counts are reported for the input links of \"sink\". \nlua\nfunction benchmarkTee (c)\n   config.app(c, \"source\", Source)\n   config.app(c, \"t1\", Tee)\n   config.app(c, \"t2\", Tee)\n   config.link(c, \"source.output -> t1.input\")\n   config.link(c, \"t1.output -> t2.input\")\n   config.link(c, \"t2.output -> sink.input\")\n   return c\nend\n(lib.benchmark)\nlib.benchmark.runbenchmarks\n. What about something like the following. I couldn't resist passing something into the benchmark function.\n``` lua\nlocal ffi = require('ffi')\nlocal C = ffi.C\nlocal lib = require('core.lib')\nlocal S = require(\"syscall\")\nlocal B = {}\nfunction B:new()\n    local cpuinfo = lib.readfile(\"/proc/cpuinfo\", \"a\")\n    return setmetatable({\n            -- use data from issue 478\n        build = lib.readcmd('git rev-parse HEAD', 'l'),\n        hostname = S.gethostname(),\n        sse2 = (cpuinfo:match('sse2') == \"sse2\"),\n        avx2 = (cpuinfo:match('avx2') == \"avx2\"),\n                date = os.date(\"!%Y-%m-%dT%TZ\"),\n        cpumodel = cpuinfo:match('model name%s:%s([^\\n]*)')\n    }, { __index = B })\nend\nfunction B:start()\n    self.start_time = C.get_monotonic_time()\nend\nfunction B:finish()\n    local finish = C.get_monotonic_time()\n    if self.duration then return end\n    self.duration = finish - self.start_time\n    self.start_time = nil\nend\nfunction benchmark(f)\n    local b = B:new()     \n    b:start()\n    f(b)\n    b:finish()\n    return b\nend\nfunction jbenchmark(benchmarks)\n    local enc = require(\"lib.json\").encode\n    for _, f in pairs(benchmarks) do\n        print(enc(benchmark(f)))\n    end\nend\nfunction benchmarkChecksums()\n    local initial = 0\n    local n = 1000000\n    local array = ffi.new(\"char[?]\", n)\n    local res = ffi.new(\"char[?]\", 5001)\n    for i = 0, n-1 do array[i] = i end\n    require(\"lib.checksum_h\")\nassert(lib.csum(array, 20) == 42395, \"lib.csum does not calculate the correct csum\")\nassert(C.cksum_generic(array, 20, initial) == 42395, \"cksum_generic does not calculate the correct csum\")\nassert(C.cksum_sse2(array, 20, initial) == 42395, \"cksum_sse2 does not calculate the correct csum\")\nassert(C.cksum_avx2(array, 20, initial) == 42395, \"cksum_avx2 does not calculate the correct csum\")\n\njbenchmark{function(b)\n    b.name = \"csum\"\n    b.runs = 100000 * 5000\n    local csum = lib.csum\n    for i=1,100000 do\n        for j=1,5000 do\n            res[j] = csum(array + 20 * j, 20)\n        end\n    end\nend}\njbenchmark{function(b)\n    b.name = \"generic\"\n    b.runs = 100000 * 5000\n    local csum = C.cksum_generic\n    for i=1,100000 do\n        for j=1,5000 do\n            res[j] = csum(array + 20 * j, 20, initial)\n        end\n    end\nend}\njbenchmark{function(b)\n    b.name = \"sse2\"\n    b.runs = 100000 * 5000\n    local csum = C.cksum_sse2\n    for i=1,100000 do\n        for j=1,5000 do\n            res[j] = csum(array + 20 * j, 20, initial) \n        end\n    end\nend}\njbenchmark{function(b)\n    b.name = \"avx2\"\n    b.runs = 100000 * 5000\n    local csum = C.cksum_avx2\n    for i=1,100000 do\n        for j=1,5000 do\n            res[j] = csum(array + 20 * j, 20, initial)\n        end\n    end\nend}\n\nend\nfunction benchmarkPairs() \n    local tab = {}\n    for i=1,1000000 do\n        table.insert(tab, i)\n    end\n    jbenchmark{function(b)\n        b.name = \"pairs\"\n        b.runs = 2000\n        for i=1,b.runs do\n            local i = 0 \n            for l,w in pairs(tab) do\n                i = w\n            end\n        end\n    end}\n    jbenchmark{function(b)\n        b.name = \"ipairs\"\n        b.runs = 2000\n        for i=1,b.runs do\n            local i = 0 \n            for l,w in ipairs(tab) do\n                i = w\n            end\n        end\n    end}\nend\nfunction selfbenchmark()\n    benchmarkPairs()\n    --benchmarkChecksums()\nlocal apps = require(\"apps.basic.basic_apps\")\nlocal counter = require(\"core.counter\")\nfor _,v in pairs({10,20,30,40,50,60,100}) do\n    jbenchmark{\n        function(b)\n            b.name = \"Sink -> Source\" .. tostring(v)\n            b.links = v\n            engine.configure(config.new())\n            local c = config.new()\n            config.app(c, \"source\", apps.Source)\n            config.app(c, \"sink\", apps.Sink)\n            local l = \"source.output -> sink.input\"\n            config.link(c, l)\n            for i=1,v do\n                config.link(c, \"sink.output -> source.input\" .. tostring(i))\n            end\n            engine.configure(c)\n            b:start()\n            engine.main({ duration = 1, no_report=true })\n            b:finish()\n\n            b.Hz = engine.Hz\n            b.busywait = engine.busywait\n            b.runs = tonumber(counter.read(engine.link_table[l].stats.rxpackets))\n        end\n    }\nend\n\nend\nselfbenchmark()\n``\n. It's not clear to me what benefitselfbenchmarks = {}brings over and above callingfunction selfbenchmark()and expecting it to calljbenchmark()` or a wrapper as appropriate.\nTiming the functions in selfbenchmarks means\n- You can't do expensive setup\n- You can't easily run a series of benchmarks. I hit a couple of subtleties with closures which broke things, why make it easier to get wrong?\n- It's more difficult to run a series of benchmarks independent of the selfbenchmarks={} whole, without naming hacks and regexes. \n- Taking no options and returning nothing means you will only ever know the duration and potentially the function name. In terms of returning the table from lib.pmu. What makes the particularly more interesting than any other data the benchmark might generate?\n- Passing in b means you can affect the benchmark run, at the moment start / finish it. If you have startup / cleanup costs. Perhaps turn on pmu or something else. \n- Reporting values because b is pass by ref is a bit tacky, but I had imagined that b:recordLinkStats() or b:recordPmuStats() might be added and lukego didn't like return values much.\n- Is returning a value materially better than pass by ref for stopping the compiler a) optimizing b) optimizing too much?\n. @lukego Right back at you.\n``` lua\n-- Run a series of checksum benchmarks with different alignment, size,\n-- and architecture-specific code.\nfunction selfbenchmark ()\n   local bench = require(\"lib.benchmark\")\n-- work out what versions make sense to test on this cpu.\n   -- Other similar benchmark selection operations should happen here not \n   -- hidden in b:start\n   local instructions\n   local cpuinfo = lib.readfile(\"/proc/cpuinfo\", \"*a\")\n   for _, code in ipairs({\"sse\", \"avx\", \"avx512\"})\n      if cpuinfo:match(code) then \n         table.insert(instructions, code)\n      end\n   end\nfor ,align in ipairs({0, 1, 2, 3, 4, 5, 6, 7, 8}) do\n      for , size in ipairs({32, 64, 100, 128, 200, 256, 500, 512, 1000, 1024, 9000}) do\n         for _, code in ipairs(instructions) do\n            local name = 'checksum-align:'..align..'-size:'..size..\"code:\"..code\n            local data = allocate_packets(size, align, 1e3)\n            bench.run(name, function(b)\n               for i = 1, 1e3 do\n                  for j = 1, #data do\n                     functionscode\n                  end\n               end\n   -- I prefer the pass in a function to run approach. It makes it very clear\n   -- the boundaries of what is being benchmarked. It stops you starting the \n   -- benchmark in one function and stopping it in another entirely. \n-- If we want to support such approaches, for example to benchmark (or profile)\n   -- every thousand'th breath then I'd want to make that intention explicit.\n   -- Particularly as you as the core/app.lua maintainer might want to profile x\n   -- whilst at the same time I as the apps/widget Ops guy want to profile my app\n   -- on packets > 1500 bytes.\n-- start / finish might be more appropriate, finish is more final, start/stop \n   -- suggests to me that a you can start / stop / start / stop the same \n   -- benchmark run. Is that a good thing? Isn't that 2 benchmarks, bodged \n   -- together.\n-- Return from the bottom of the benchmark, there are some things you might not\n   -- know until the end of the run. Namely how much work your doing. \n   -- For example it's not possible to send exactly\n   -- 1e6 packets through an app network. engine.main({ duration = about1second }) \n   -- and then see how many packets have been processed. Adding packet counts \n   -- to basic.Source and basic.Repeater gets around that but I'm not sure it's\n   -- the answer.\n-- If we decide to support 'profiling' this becomes more important, you have \n   -- no idea at the begining of the breath how much stuff your going to process.\n   -- b.stop({ vars }) could be another approach.\n-- I don't like the idea of burying parameters just in the benchmark name.\n   -- Having them 'machine readable' from the start means they can be ignored\n   -- in pretty 'tabled' output that's for immediate human consumption. When\n   -- we log the benchmark results and start to find trends or CI walk for local\n   -- minima then having keyvalue is nicer.\n               return { name = name, align = align, code = code, \n                        size = size, checksum = 1e6, bytes = 1e6*size }\n            end)\n         end\n      end\n   end\nend\n-- Very simple selfbenchmark()\nfunction selfbenchmark\n   local bench = require(\"lib.benchmark\")\n   bench.run(\"simpletest\", function()\n        -- do something\n        return { packets = 1e4 }\n   end)\nend\n```\nI'm generally against adding stuff to the global namespace because once it's there\nit's very hard to remove. If we start by requiring a require(\"benchmark\") then we can always\npromote it at a later date.\nchecksum-align:0-size:32-code:generic = 312312312ns (312ns/checksum, 10ns/byte)\nchecksum-align:0-size:32-code:sse = ...\nHow do we communicate what the 'interesting' keys are for formatting purposes. \nFor your benchmark of csum maybe ns per (packet, byte) are the interesting things.\nIf you have pmu then you might want per cycle or cache miss per packet and byte.\nFor my benchmark of pairs/ipairs none of those are really meaningful. It's an \nissue I side stepped when I wrote the JSON outputs as it was assumed that something\nelse would take the json and do the 'RightThing' with it.\nPerhaps, a (pre)process step happens on the data before display. The default \nprocess would look for keys of packets / bytes and divide those out by duration\n(and if pmu is around cycles/misses ect ect). But we provide a means to add others\nfunction foo(tab)\n   return {\n      nsPerChecksum = tab.duration / tab.checksum,\n      -- bytes is automatically tried\n      -- nsPerByte  = tab.duration / tab.bytes\n   }\nend\nfunction selfbenchmark\n   local bench = require(\"lib.benchmark\")\n   bench.run(\"simpletest\", function()\n      b.process_with(foo)\n   end)\nend\nThis would also allow filtering of which columns to display in the human readable\noutput, we just display name,duration,bytes,packets,nsPerPacket,nsPerByte + anything \nin the table returned from process ?\nI'm not convinced by bench.set_json_output('/tmp/%s.json') to me the output \npresentation has nothing to do with the benchmark specification and this should\nbe hidden by benchmark.run. If benchmark is being run interactively then a human\nreadable table should be output as it's meant for immediate interpretation. If \nit's being run by a CI or by make sendBenchmarksToSnabbDevelopersPrivacyOff \nthen it would output json or to a rest endpoint directly. \n. @andywingo \nMatch was @eugeneia suggestion, I'm happy with whatever. \nI can certainly dump exceptions to a pcap if that's seen as useful, personally I found hexdumps more helpful.\nI started by comparing to nil, however core/config.lua explicitly uses \"nil\" which means it doesn't work.\nIt seemed weird but I assumed it was an intentional, known thing. \n-- Example: config.app(c, \"nic\", Intel82599, {pciaddr = \"0000:00:01.00\"})\nfunction app (config, name, class, arg)\n   arg = arg or \"nil\"\n   assert(type(name) == \"string\", \"name must be a string\")\n   assert(type(class) == \"table\", \"class must be a table\")\n   config.apps[name] = { class = class, arg = arg}\nend\n. @eugeneia I'm happy to move it to apps/match but what's changed since the PR690 where you suggested TestSink should be moved to basic_apps as Match? \nAlso I pulled in the changes from max-next so that I got the fix for config.parse_app_config so the my tests passed. That's added lots of files to the PR that probably don't make sense and probably not what I wanted to do? Can you give me a pointer on what I should of done?\n. Is synth envisaged to be a general purpose packet crafter? \nSo far I've been using scapy to generate pcaps on the fly and /dev/stdout and /dev/stdin as hacks to run it as a single command. That is then combined with PcapReader and Repeater\n'wrpcap(\"/dev/stdout\", [ Ether()/Dot1Q(vlan=x)/IP(src=\"127.0.0.1\")/UDP(sport=10)/Raw(open(\"/dev/urandom\",\"rb\").read(10)) for x in range(5,70) ])' | scapy | snabb snsh vlan.lua\nlib.protocol doesn't really have the brevity of scapy.\nWould you see synth eventually replacing this use case?\nIf it doesn't what's the line of reasoning to not include this in Source?\nIs the use case for synth particularly wide spread as it currently stands?\nDoes having Match join Synth under test make sense?\nMatch doesn't help where tests haven't run for long enough but would help where more than 1 breath is taken. \n. \"L. Gorrie: Treatise on the nature of simplicity in software development\" sounds quite an interesting read where can I find a copy!\n. I'd like to see a 'cheat sheet' of guidance be put together, and ideally linked to or included in CONTRIBUTING.md. Things like\n- PR against master ?\n- 3 space indent\n- No tabs allowed\n- Expect someone to pick up CRs after X days, chase after that if it's important to you ?\n- We use snake case ?\n- @eugeneia shared some git aliases for easy PR merging.\nThis is coming from the perspective of working on multiple projects each with there own set of preferences and a cheat sheet would help context switches. CONTRIBUTING.md is also linked from the PR creation page so is a nice reminder to self check.\n. What you describe sounds like relatively heavy weight composite apps and is using the language of an app.\nI think this problem would be better thought about as a configuration problem, perhaps answered by supporting configuration macros that give a bundle of functionality but are transparent to the user, and thought about in terms of a declarative configuration file. It's not immediately obvious that making these composite apps flexible enough to support all use cases won't become another point of configuration and we will be back at square 1 with a configuration problem, but have composite apps as well. \nswitched_nic sounds like it's a lot of work but none of the following do\n- An app that dispatches to a set of links based on statically defined macs\n- An app that adds / removes Vlan tags\n- An app that ECMPs traffic over a set of output links\n  @plajjan requires MPLS labels to be attached cool have an mpls app\n  @petebristow requires Vlans\n  Someoneelse requires QinQ\n  A.Nother wants a more labels for segment routing. All of these are configuration problems rather than app problems. These are all simple configuration matters.\n  Having pure configuration macros means they are seen as lightweight reusable chunks. I can now have a chunk that covers my physical nic + fastPathApp + tap interface for slowpath I want linux to handle. \n  It would mean we need to revisit the lua as configuration decision but I think this needs to be done any way as part of the Yang debate. We also need to think about what configuring a multiprocess snabb network should look like. \nI'm all in favor of the StraightNIC idea with the extension of RSS support. It's my feeling that most of the acute performance concerns that lead to fancy hardware and complex drivers could be alleviated with having more cores. Some apps designs might not parallelize to many cores nicely, but it's my slightly biased opinion that they were going to hit problems anyway. If the role is 'popular' enough to need more than 1 core it probably also needs more than 1 10gig link, more than 1 server and more than one site. So you may as well figure out how to scale horizontally from the start. Not all NICs support all RSS hash modes, but then there aren't many guarantees on what upstream ECMP hashing will look like either. Huge flows that would overwhelm a given path are a pain but they always have been and always will be. 100gig was great until we needed n*100gig :( \nIf you get 1million pps per core and have 6 mpps of load you run 6 processes each with an rx and tx queue. This stops working when you run out of queues, but the 82599 has 16 queues, the solarflare appears to have 64 \nI've added RSS support to the Intel1g driver and I'm just finishing the multi process support for it. It's run as entirely separate snabb instances each with it's own distinct config, which is nice to demo but sounds pretty horrific to operationalize. I'll try and get a [wip] PR in, in the next few days.\n. @plajjan \nShould SnabbDos have to deal with site specific configuration like that? Wouldn't having a stand app + a site specific config file be much better? I feel like that config should be able to define the app network  in a declarative non lua way. Your NMS/CMS/OSS then worries about the class of host and what the config should look like.\n. @lukego \n897 is a stab at my StraightNIC + RSS vision. Next is to add multi queue to the tap driver. From there I'd like to strip down the existing 82599 driver and add RSS support. This would be in conflict with the virtio/vmdq approach but would buy some head room for pure packet forwarding, so I can see it being  a separate driver, perhaps becoming the driver if / when virtio multiqueue arrives, who knows.\nEither way on the face of it is a refactored 82599 driver likely to get accepted?\n. The problem with the X710 is that I have none of them in my network but have a huge number of 82599 that I would like to power with snabb but need RSS in place first.\nI haven't looked into the VMDq facilities if it can follow a similar scheme to RSS then great. \n. No %x does not match upper case letters, or at least it didn't in my testing. \nLinux uses lower case in it's paths, if we accept upper case we should probably flatten it to lower case as part of the process?\n. Blocking the whole snabb process with usleep was by design. There was an issue floating around a while ago about back pressure was it good / was it bad. I don't remember it reaching a conclusion so it's on my list of things to play with. I wanted to avoid putting back pressure in place with this app or buffering packets in memory, which could go rather wrong if it's being fed by Repeater. \nI'd like to leave it as is if possible, perhaps expanding the comment to make it explicit that it will stall the whole snabb process. \n. @eugeneia Is there anything else you want to see before this can be merged?\n. @eugeneia Is there anything else you want to see before this can be merged?\n. I would certainly prefer input / output.\nYes I think all references to rx/tx should become input / output. \n. @eugeneia Thanks, I couldn't see it for looking.\n. @kbara Please don't merge this for now. \n. Just to echo wingo, being to able to raise the limit at run time from inside an app would be very useful.\n. From a driver users perspective does it look like the kind of interface you would want, particularly if it was replicated into the 82599 driver?\n. This driver is now in reasonably good shape. An assignee and review would be great.\n. This is the first driver that can support a herd of snabb processes. Do you envisage a need for a herd of VMDq processes as well? intel_rss probably doesn't make sense if this driver is going to get VMDq support. Obviously intel1g is also a poor choice as it supports 10g. intel_herd intel_mp ?\nI've started to manage the herd with something a little like \nhttps://gist.github.com/petebristow/1231338296c2dfe8c9a28d4939411aee\nREADME.md better comments and some tests that exercise the 82599 portion are in the works. \n@lukego Is the Mellanox driver going to take a similar approach to multiple processes?\n. A quick and dirty (so take with a pinch of salt) grep of the source code shows ~10 uses of packet.data() and ~250 uses of p.data. The methods haven't had much impact. \nI'm also in favor of documenting the structure, is there consensus enough to put through a PR to update the readme?\nShould the API functions remain? They seem slightly redundant if the structure is documented.\n. @kbara Are you happy with this?\n. It would be great to have a validate_config or similar. Mixing initialization with config checking is ok in new() for initial start up. On reconfiguration however there isn't any way to communicate that the config was rejected, instead continue running in a the known steady state. YANG when it arrives may enforce that an app must have appropriately named input and output links but it can't enforce that the new pciaddr for Intel82599 is actually a pci device or that the pflang expression passed to packet_filter is valid.\n. @eugeneia If snabb bot agrees can you merge this update. The code above breaks under load, packets with length 0 start flying around. I'm not really sure why, but the new code lets luajit hoist the self.pkt rather than doing it manually.\n. Yes that was the failure mode. The socket write is being asked to write a packet of length 0, which causes an EINVAL. I haven't been able to work out why 82eecef fixes things, but I haven't been able to replicate the issue. Which isn't very satisfying.\nThe original patch would reliably break after a few hundred thousand packets, and sometimes break under selftest. 82eecef has run several hundred million packets and not had any issues. \n. Thanks, a sleep deprived blonde moment.\n. @raj2569 the intel_mp app uses link names of input and output rather than rx and tx. Also if you want to receive packets you need to specify an rxq to listen on. The docs mark the field as optional, but that means you don't have to listen on a nic not that you don't have to specify an rxq and still be able to listen. You have to specify an rxq if you want to receive and a txq if you want to send.\nhttps://gist.github.com/anonymous/4fe4c57cc39feec4093b57e2538d985a\nIs a complete gist showing the intel_mp driver in receive only mode. . @plajjan They are run as part of the CI, it doesn't pin the test to a single core though so lib.pmu complains. @eugeneia @lukego Is that a problem with my tests or a CI problem ?\nThe results below are run on a 2.30GHz Sandybridge. \nThe routes are randomly generated, this makes DXR slow, it's much faster with the DFZ.\nLPM4_trie is very slow, it's more a RIB thing.\nLPM4_248 with 15bit values\nEVENT                                             TOTAL     /lookup\ncycles                                    1,802,696,930      18.027\nref_cycles                                            0       0.000\ninstructions                              2,002,673,892      20.027\nmem_load_uops_retired.l2_hit                      2,806       0.000\nmem_load_uops_retired.l2_miss                       525       0.000\nmem_load_uops_retired.llc_hit                       416       0.000\nmem_load_uops_retired.llc_miss                      103       0.000\nlookup                                      100,000,000       1.000\ndata dependency\nEVENT                                             TOTAL     /lookup\ncycles                                    6,041,667,188      60.417\nref_cycles                                            0       0.000\ninstructions                              3,609,026,013      36.090\nmem_load_uops_retired.l2_hit                    654,274       0.007\nmem_load_uops_retired.l2_miss                99,145,450       0.991\nmem_load_uops_retired.llc_hit                99,110,090       0.991\nmem_load_uops_retired.llc_miss                   35,288       0.000\nlookup                                      100,000,000       1.000\nno dependency\nEVENT                                             TOTAL     /lookup\ncycles                                    3,877,605,508      38.776\nref_cycles                                            0       0.000\ninstructions                              3,105,981,080      31.060\nmem_load_uops_retired.l2_hit                    683,391       0.007\nmem_load_uops_retired.l2_miss                98,879,708       0.989\nmem_load_uops_retired.llc_hit                75,359,843       0.754\nmem_load_uops_retired.llc_miss               23,519,812       0.235\nlookup                                      100,000,000       1.000\nLPM4_248 with 31bit values\nEVENT                                             TOTAL     /lookup\ncycles                                    1,802,816,314      18.028\nref_cycles                                            0       0.000\ninstructions                              2,002,669,707      20.027\nmem_load_uops_retired.l2_hit                      2,742       0.000\nmem_load_uops_retired.l2_miss                       660       0.000\nmem_load_uops_retired.llc_hit                       397       0.000\nmem_load_uops_retired.llc_miss                      241       0.000\nlookup                                      100,000,000       1.000\ndata dependency\nEVENT                                             TOTAL     /lookup\ncycles                                    6,038,493,398      60.385\nref_cycles                                            0       0.000\ninstructions                              3,509,189,183      35.092\nmem_load_uops_retired.l2_hit                    329,337       0.003\nmem_load_uops_retired.l2_miss                99,480,639       0.995\nmem_load_uops_retired.llc_hit                99,441,646       0.994\nmem_load_uops_retired.llc_miss                   38,855       0.000\nlookup                                      100,000,000       1.000\nno dependency\nEVENT                                             TOTAL     /lookup\ncycles                                    5,052,005,406      50.520\nref_cycles                                            0       0.000\ninstructions                              3,107,544,344      31.075\nmem_load_uops_retired.l2_hit                    344,854       0.003\nmem_load_uops_retired.l2_miss                99,245,765       0.992\nmem_load_uops_retired.llc_hit                37,834,608       0.378\nmem_load_uops_retired.llc_miss               61,411,092       0.614\nlookup                                      100,000,000       1.000\nLPM4_dxr\nEVENT                                             TOTAL     /lookup\ncycles                                    1,802,773,939      18.028\nref_cycles                                            0       0.000\ninstructions                              2,002,707,041      20.027\nmem_load_uops_retired.l2_hit                      2,589       0.000\nmem_load_uops_retired.l2_miss                       575       0.000\nmem_load_uops_retired.llc_hit                       448       0.000\nmem_load_uops_retired.llc_miss                      120       0.000\nlookup                                      100,000,000       1.000\ndata dependency\nEVENT                                             TOTAL     /lookup\ncycles                                   17,693,204,598     176.932\nref_cycles                                            0       0.000\ninstructions                             11,262,319,609     112.623\nmem_load_uops_retired.l2_hit                 31,486,717       0.315\nmem_load_uops_retired.l2_miss               357,800,594       3.578\nmem_load_uops_retired.llc_hit               357,798,489       3.578\nmem_load_uops_retired.llc_miss                    1,820       0.000\nlookup                                      100,000,000       1.000\nno dependency\nEVENT                                             TOTAL     /lookup\ncycles                                   14,990,055,096     149.901\nref_cycles                                            0       0.000\ninstructions                             10,783,918,937     107.839\nmem_load_uops_retired.l2_hit                 29,846,837       0.298\nmem_load_uops_retired.l2_miss               340,099,663       3.401\nmem_load_uops_retired.llc_hit               340,098,552       3.401\nmem_load_uops_retired.llc_miss                      841       0.000\nlookup                                      100,000,000       1.000\nLPM4_trie\nEVENT                                             TOTAL     /lookup\ncycles                                       18,104,359      18.104\nref_cycles                                            0       0.000\ninstructions                                 20,063,983      20.064\nmem_load_uops_retired.l2_hit                        167       0.000\nmem_load_uops_retired.l2_miss                       222       0.000\nmem_load_uops_retired.llc_hit                       221       0.000\nmem_load_uops_retired.llc_miss                        1       0.000\nlookup                                        1,000,000       1.000\ndata dependency\nEVENT                                             TOTAL     /lookup\ncycles                                    1,186,457,814    1186.458\nref_cycles                                            0       0.000\ninstructions                              1,502,740,505    1502.741\nmem_load_uops_retired.l2_hit                  4,868,971       4.869\nmem_load_uops_retired.l2_miss                 9,343,821       9.344\nmem_load_uops_retired.llc_hit                 9,333,537       9.334\nmem_load_uops_retired.llc_miss                   10,257       0.010\nlookup                                        1,000,000       1.000\nno dependency\nEVENT                                             TOTAL     /lookup\ncycles                                    1,187,639,448    1187.639\nref_cycles                                            0       0.000\ninstructions                              1,497,713,970    1497.714\nmem_load_uops_retired.l2_hit                  4,863,939       4.864\nmem_load_uops_retired.l2_miss                 9,348,050       9.348\nmem_load_uops_retired.llc_hit                 9,293,004       9.293\nmem_load_uops_retired.llc_miss                   55,023       0.055\nlookup                                        1,000,000       1.000. The reset logic is in the loop because the card can wedge trying to come up. Particularly if the port is already seeing significant traffic during the reset. If you have it hooked up to a switch span port for instance. I wasn't able to come up with a test that would reliably fail if this code wasn't in place but if I ran selftest 1000 times I would get a few failures that went away with repeated resetting of the card. It's possible this is more an indication on the cheap cards / SFP+ DACs I'm using. 2 seconds appeared to be the sweet spot if it didn't come up after that period of time then it wasn't going to.. I don't follow, https://github.com/snabbco/snabb/blob/master/src/apps/intel/intel10g.lua#L126-L154 shows the Intel82599 doing a very similar thing of waiting on a link up and bouncing the card if it's not seen within a period of time. The control flow is a little more convoluted but the gist is the same. Are you objecting to the \"something went wrong bounce the card until it works\" methodology or that 2 seconds is too long to wait for the majority of cases where the card comes up ok first time?. Looking at my Amazon order history that's the card I developed the driver\nagainst, so yes it should work.\nPete\nOn Mon, 26 Jun 2017 at 10:08, adw555 notifications@github.com wrote:\n\nHi,\nI'd like to test snabb with kernal-passthrough and was wondering if I was\nto purchase an HP branded Intel i210 NIC, would this be supported or must\nit be an actual Intel card? (\nhttps://www.amazon.co.uk/d/34m/Intel-I210-T1-Network-adapter-Express-profile-Ethernet/B00ESFF2JC\n)\nThanks\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/snabbco/snabb/issues/1160, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/APckdGmZ5mbtRnex9YJmlJHHBWAU6R5Wks5sH3UngaJpZM4OFDNe\n.\n. Hi @lukego @dpino @leolovenet,\n\nhttps://groups.google.com/forum/#!topic/linux.kernel/Hwyme8jQyDM sheds some light on this. Basically hugetlb on a file on hugetlbfs was considered invalid at the time they minted CentOS 7. It has since become accepted but isn't required.\nDoes any one have objections to just removing the hugetlb / MAP_HUGETLB ?\nPete. Hi @benagricola,\nWould you mind adding a test for this ?\nPete. Looks good to me.. Having thought about this some more over in https://github.com/snabbco/snabb/issues/1238 and looked at my internal version. The reason the search function doesn't react well to a missing entry is because parts of the code base assume there will always be a default route. It's not documented, nor are there assertions in the code. Given that it may be a bad idea to merge this.. Yes as it stands. I'm thinking it would be better if\na) 0 is defined as the 'Not Found' entry\nb) If you don't install an explicit 0.0.0.0/0 then build adds one implicitly\nc) This is all documented in the README rather than a surprise feature. \nWhat do you think ?. I didn't have an exhaustive list of IDs nor did I really want to add IDs that I couldn't test. \nAre you going to have this hardware hanging around to verify for breaking changes that might happen?. Apologies for my tardiness. You've hit an undocumented feature. (It's documented in my internal version of LPM but I'm not sure that counts :S) Every LPM instance requires a 0.0.0.0/0 entry. If you don't have a a default route then you need to install an explicit \"I don't know\" value.. Perhaps moving the lock out of map_pci_memory would make sense ? That way you can grab the lock unbind and then release and avoid this. I only used the device entry for the lock because at the time /var/snabb/* didn't have a particularly obvious place to put it.. There are some tests, fixes and cleanup pending.. Yes that was a mistake. Do I just push the removal change to the original\nCR? Or is that something that you'll handle?\nOn 27 April 2016 at 13:58, Max Rottenkolber notifications@github.com\nwrote:\n\nIn src/core/memory.lua\nhttps://github.com/snabbco/snabb/pull/896#discussion_r61251908:\n\nlocal want = have + 1\n-      lib.writefile(\"/proc/sys/vm/nr_hugepages\", tostring(want))\n     syscall.sysctl(\"vm.nr_hugepages\", tostring(want))\n\n@petebristow https://github.com/petebristow ping. Pretty sure it was\nmistakenly leftover though.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/snabbco/snabb/pull/896/files/cbe97a4b68968c62825b2b74fd9486c1e27fbfc8..519311bcc79dd3bc41b016e9f9be4f383f397f01#r61251908\n. Doesn't that result in an extra copy for every packet?\nI guess worrying about extra copies is probably mute given that it's a Tap interface, but it seems a slight dubious habit to get into.\n. Is the \" in the wrong place?. \n",
    "xrme": "For what it may be worth, this error can happen when following the directions in the top-level README.md file.\nThe $ prompt in the \"How do I get started\" section implies that running  src/snabb --help as a non-root user should be OK, but this will fail if src/snabb --help is the first time the program is ever run.\n. At the risk of looking like Barbie (\"learning new tools is hard!\"), I checked in the totally non-working code that I've been writing.\nSee xrme/snabbswitch@636ab74372014c4152b3e1bdc8c63c8ad1729b8b.\n. I expect that it will be important for multi-process Snabb Switch to manage (via taskset or whatever) which cores the cooperating Snabb Switch processes run on.  They will almost certainly need to be in the same NUMA domain.\nAs I understand it, the cache line is the unit of storage that gets moved around between cores.  As you probably know, that's the source of the false sharing problem.  (That's where two different cores are writing to different parts of the same cache line, obliging the system to keep transferring the dirty line back and forth, even though process 1 may only be interested in byte 0 of the line, and process 2 only interested in byte 63.)\n. I agree that autoconf deserves to be given a very wide berth.  However, it seems to me that avoiding C in this case is not a very great simplification.  Nevertheless, if the C code smells funny and you just want it out of the house, that's an understandable reason.\nThe only technical argument I might make against replacing the C code is that it is easy to call C functions from assembly.  On the other hand, I don't know that assembly language code has any business calling these particular functions.\nI prefer this design (mmap and hugetlbfs) to the current System V shared memory approach. It's a real win to be able to easily derive the appropriate name for a huge page directly from a pointer value. This makes it straightforward for multiple processes to allocate packet memory dynamically and share it with other processes.\nA handler for SIGSEGV can get info->si_addr, see if it's properly tagged, and if so, construct the appropriate file name, open and mmap it, and proceed.  I like it.\n. The apparent performance regression is bothering me.\nI don't think I understand the comment by @nnikolaev-virtualopensystems regarding the alignment of the list vector.  Is there some reason it should be more than 8-byte aligned?\n. > I wonder about \"struct freelist\" -- I can see this name as easily colliding with a name from some other user's module, struct tags being global and all. Unless you need this struct to have a tag...\nDynASM has a feature where you can say\n```\n|.type link, struct link\n-- Emit code to receive packet pointer into register p\n-- from the ring buffer pointer in register link.\n-- i is a temp register\nlocal function emit_receive(p, link, i)\n   local mask = (C.LINK_RING_SIZE - 1)\n   | mov Rd(i), link:Rq(link)->read\n   -- assumes packets[] is first in struct link\n   -- could use displacement of offsetof(link,packets)\n   | mov Rq(p), [Rq(link)+Rq(i)*8]\n   | add Rd(i), 1\n   | and Rd(i), mask\n   | mov dword link:Rq(link)->read, Rd(i)\nend\n```\nSee http://corsix.github.io/dynasm-doc/reference.html#_type\nI'd like to be able to use notation like mov Rd(i), link:Rq(link)->read, and I think I need struct packet to exist to do that.\n. I'm not sure if it's kosher to keep commenting here, but this little snippet of the benchmark also causes cache line ping-ponging.  If you add a crude C.usleep(100) in there, --mode ff throughput goes up 40%.\n-- Spin until enough packets have been processed                             \n   while counters[0] < c.packets do\n      core.lib.compiler_barrier()\n   end\nI think we could make the main process wait on a semaphore rather than spinning like this.\n. I pushed 65abcd3 to my mp-ring branch, but my commit isn't showing up over on #813.\n. lugano-4:src rme$ sudo ./snabb snabbmark mp-ring --mode ff --processes 2\nBenchmark configuration:\n       burst: 100\n  writebytes: 0\n   processes: 2\n   readbytes: 0\n     packets: 100000000\n        mode: ff\n   pmuevents: false\n   7.51 Mpps ring throughput per process\nStill really bad.  But it may not be a valid result anyway, since using --processes of more than 2 doesn't seem to work with mode ff.\n. I put back the changes to struct link and made everything local to mp.lua.\nI added another type of link based on MCRingBuffer (paper cited above).\nI think I'll try assembly versions of some rx/tx functions, just so I can be sure of what's going on instruction-wise.  Although fewer instructions are always better (the fastest work is the work that you don't do, after all), we know that the performance challenge here is probably a question of memory/cache efficiency.\n. I believe that the correct way to exit here would be to re-signal the SIGSEGV.  Something like\nsignal(SIGSEGV, SIG_DFL);\nreturn kill(getpid(), SIGSEGV);\nSee http://www.cons.org/cracauer/sigint.html for a detailed explanation.\n. ",
    "hb9cwp": "More selftest() with i350 on chur:\n```\n[hb9cwp@chur:~/snabbswitch/hb9cwp/snabbswitch/src/program/top]$ ps ax|grep snabb\n 1610 pts/0    S+     0:00 sudo SNABB_SELFTEST_INTEL1G_0=0000:0c:00.1 ./snabb snsh -t apps.intel.intel1g\n 1611 pts/0    R+     1:00 ./snabb snsh -t apps.intel.intel1g\n[hb9cwp@chur:~/snabbswitch/hb9cwp/snabbswitch/src/program/top]$ sudo snabb top 1611\nKfrees/s        freeGbytes/s    breaths/s    \n 2979.00         0.18            159700         \nLinks (rx/tx/txdrop in Mpps)    rx      tx      rxGb    txGb    txdrop \n nic.tx -> sink.rx               1.49    1.49    0.09    0.09    0.00 \n source.tx -> nic.rx             1.49    1.49    0.09    0.09    0.00   \n```\n```\n$ uname -a\nLinux chur 4.2.5 #1-NixOS SMP Thu Jan 1 00:00:01 UTC 1970 x86_64 GNU/Linux\n$ less /proc/cpuinfo\n...\nvendor_id       : GenuineIntel\ncpu family      : 6\nmodel           : 45\nmodel name      : Intel(R) Xeon(R) CPU E5-2650 0 @ 2.00GHz\nstepping        : 7\nmicrocode       : 0x70d\ncpu MHz         : 2799.687\ncache size      : 20480 KB\n...\nbogomips        : 3999.98\n```\nselftest() with i210 on APU2b4 with Ubuntu 14.04.3 LTS:\n```\nr@a2b:~/snabbswitch/hb9cwp/snabbswitch/src/program/top$ sudo snabb top 1450\n Kfrees/s        freeGbytes/s    breaths/s    \n 1451.50         0.09            3000           \nLinks (rx/tx/txdrop in Mpps)    rx      tx      rxGb    txGb    txdrop \n nic.tx -> sink.rx               0.68    0.68    0.04    0.04    0.02 \n source.tx -> nic.rx             0.75    0.75    0.05    0.05    0.00 \n```\n```\n$ uname -a\nLinux a2b 3.13.0-71-generic #114-Ubuntu SMP Tue Dec 1 02:34:22 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\n$ less /proc/cpuinfo\n...\nvendor_id       : AuthenticAMD\ncpu family      : 22\nmodel           : 48\nmodel name      : AMD GX-412TC SOC                             \nstepping        : 1\nmicrocode       : 0x7030105\ncpu MHz         : 1000.000\ncache size      : 2048 KB\n...\nbogomips        : 1996.09\nTLB size        : 1024 4K pages\n...\n```\n. Thanks Luke for merging and asking!\nDigging into the intel1g app turned out to be a good learning exercise into both the internal workings of Snabb Switch, and Lua at the same time (but supposedly much less steep than DPDK would be :-) \nIt took me a while to convince myself that your intel1g code was a draft that required more work. Initially, I could not understand from the code how the (shadow) descriptor ring buffers work. In particular, (can_)add_receive_buffer() were misleading until I figured out that they were \"dead code\", probably just pseudo-code to illustrate what the NIC does in hardware *). And that the receiver was still lacking the handling of packet buffers. \nThe last struggle was to figure out why RDH index was never advancing although the receiver received frames from the MAC loopback. The solution was to initialize RDT to (ndesc-1) instead of 0 in order to trigger the NIC to re-load descriptors into its cache...\nNow, I want to add some input parameter checks, for ex. the i350 supports 8 rings each whereas the i210 only 4 each. Then I intend to add support for initializing the PHY in order to send and receive frames between interfaces/hosts over cables, instead of MAC loopback.\nYet, I am not sure how to do that in line with your intention with this new simple 1GE driver. First, I will study the intel10g_app, and I appreciate any guidance, hints and corrections to get that working.\nMy objective is to have a working intel1g app that can serve as basis for further experiments with L2TPv3 over IPsec not only on 10GE machines, but with 1GE NICs (i210, i350) as well.\n*) The pseudo-code is buggy as it bumps rdt whereas it should bump rdh. Also, I could not understand why the \"auxillary\" pointer rxnext is used instead of rdh. Maybe I should try to fix this in intel10g as well and send PRs.\n. Great, so I push ahead with some input validation, elimination of possible race condition while reading RDH or TDH, and setting up PHY.\nIs is worth to spend a few cycles on trying to apply some of my lessons learned to the intel10g app (see my footnote which I edited into my previous comment), or are they better spent on writing a new 10GE driver based on the intel1g skeletton, once this becomes more feature-complete and settles down?\n. Showing the 'model' is user-friendlier than hex codes of 'vendor' and 'device ID' (which 'lspci -nn' shows as well): \n[hb9cwp@chur:~/snabbswitch/hb9cwp/snabbswitch/src]$ ./snabb snsh -t lib.hardware.pci             \nselftest: pci\npciaddress  model              interface  status  driver               usable\n01:00.0     Intel 82599 SFP    -          -       apps.intel.intel10g  yes\n01:00.1     Intel 82599 SFP    -          -       apps.intel.intel10g  yes\n03:00.0     Intel 82599 SFP    -          -       apps.intel.intel10g  yes\n03:00.1     Intel 82599 SFP    -          -       apps.intel.intel10g  yes\n05:00.0     Intel 82599 SFP    -          -       apps.intel.intel10g  yes\n05:00.1     Intel 82599 SFP    enp5s0f1   up      apps.intel.intel10g  no\n07:00.0     Intel 82599 SFP    -          -       apps.intel.intel10g  yes\n07:00.1     Intel 82599 SFP    enp7s0f1   up      apps.intel.intel10g  no\n09:00.0     Intel 82599 SFP    enp9s0f0   up      apps.intel.intel10g  no\n09:00.1     Intel 82599 SFP    enp9s0f1   up      apps.intel.intel10g  no\n0c:00.0     Intel 350          enp12s0f0  up      apps.intel.intel1g   no\n0c:00.1     Intel 350          -          -       apps.intel.intel1g   yes\n82:00.0     Intel 82599 SFP    enp130s0f0 up      apps.intel.intel10g  no\n82:00.1     Intel 82599 SFP    enp130s0f1 up      apps.intel.intel10g  no\n86:00.0     Intel 82599 SFP    enp134s0f0 down    apps.intel.intel10g  yes\n86:00.1     Intel 82599 SFP    enp134s0f1 down    apps.intel.intel10g  yes\n88:00.0     Intel 82599 SFP    enp136s0f0 up      apps.intel.intel10g  no\n88:00.1     Intel 82599 SFP    enp136s0f1 up      apps.intel.intel10g  no\n. I've now pulled in and pushed your README.*. Is that all that was missing?\nAlso, I observe that CI check fails repeatedly. But it looks as if it is related to SnabbNFV which I have not touched?\n. Backed out my renaming of the intel10g driver name which breaks CI...\n. @lukego Still feeling as a newcomer and still lacking enough insight to give you a good answer right now. \nHowever, at this very moment, I am working on building separate transmitter and receiver programs to test drive MAC/PHY parts of the intel1g app, e.g. do simplex one-way transmissions, adding sequence numbers to the packet payloads of an extended basic.Source, and evtl. some packet loss and bit errors. Also, I might try to split in half intel1g into tx and rx paths somewhere along your outline. Perhaps I will have an opinion about your idea after that experience.\n. @lukego Paraphrasing your design idea using your new triangle notation from #684 for sending minimal packets with 60 Byte dummy payload over a cable between two hosts that have (at least) one NIC each, and eventually maps two processes onto two cores on each host: \n\nSo the key question is if there is a reasonable implementation which splits a NIC triangle into two separate tx and rx sub-triangles along the jagged lines in red.\n. @eugeneia Just backed out that debug code in apps.basic.basic_apps. \nSorry for the delay, because last week I had focus on getting l2vpn from Alex to work over ipsec.esp with  over intel1g... \nDuring that work, I have spotted another performance degradation in the combination of l2vpn with intel1g (it uses almost 100% CPU of one core, even if the bridge is idling). I search for the culprit now. Thus consider intel1gas \"working\", but still work-in-progress :-)\nUpdate: Fortunately, there is no such performance degradation, but a) just a mis-interpretation from my part of the output from top and mpstat on SMP machines, and b) the load generator which uses tcpbench on OpenBSD was accidentally running a single processor kernel on the 4 core AMD GX-412TC on one of my APU2 from PC Engine...\n. Ack, clever & neat idea. Also motivates your other idea in #683 about splitting NIC driver apps into separate \"half-duplex\" tx and rx paths.\n. @lukego I may have time to work on that by the end of this month onwards.\n. @eugeneia Done, thanks.\n. ",
    "domenkozar": "Here are my 2 cents on the topic.\nAs @lukego stated in the first comment, testing all possible scenarios (different versions of software we integrate to) means an explosion of possibilities.\nRegression tests\nThe alternative way, hopefully easier to pull off, is to fixate time and use regression tests (with a CI) against the whole collection of software.\nThinking about all inputs for the test suite for a snapshot of all software. Then we update snabb and collection of software used for integration run all regression tests.\nIf we find a regression in performance, we know that snabb changed from commit X to Y OR collection of software changed from Z to W. If we don't bump both at the same time, it's easier to pinpoint what commit is faulty for the regression. Once we have one range of commits, we just git bisect them using the test that introduces a regression.\nIf we wanted we could still have different sets of software against which we could run the tests. For example, two versions of OpenStack. Or more generally, the exact versions that will/are be used in production.\nExample\nTests for the Nix itself are a good example. Looking at http://hydra.nixos.org/jobset/nix/master you'll see that there are just two moving targets under \"Input changes\" column. nix and nixpkgs (software collection) repositories.\nIf the tests executed provided a formatted output of performance results, some other script/entity could collect those and figure out if there was a regression.\nHydra has this functionality partially implemented. For example, we graph the closure size (whole runtime dependency tree) for EC2 instances to see if something caused the size to grow more than we'd like to. \nIn this case, if someone bumped Qemu in Nixpkgs, once we pinned a newer commit, the regression would be detected by our tests. We should strive to do these updates in smallest possible intervals so it's easier to pinpoint the problems (ideally for each commit, but that's an overkill).\nPros\n\nprovides an easy way to chase down what commit exactly introduces the regression\ndoesn't interfere with software development workflow\nmost of the infrastructure is already available (we'd need to automate \n\nCons\n\nwe need a good collection of regression tests, one for each functional case of software that should be performance\nit doesn't cover regressions before this system is in place\n. If /bin/sh also works on guix, we could use that. But unfortunately that will break if we use bash features and folks have a different shell set.\n. Separating docs from source has (a possible) big downside, separating source changes from documentation changes. It's harder to pinpoint the two. It's better to not commit build artifacts in the first place, since that makes the repository quite big.\n\nI'd recommend using a simple Markdown builder like http://www.mkdocs.org/ and https://readthedocs.org/ (supports multiple versions and multiple formats) for hosting the documentation, they're doing an awesome job keeping that site running. We could hook readthedocs into git so it would build the docs, but if that won't be possible due to dependencies we can use Nix+Hydra to publish them. Very SaaSy, but I use this workflow for all my projects and so far it worked well.\n. I'll update the lab now.\n. I've updated the lab and supporting server. Note that you'll have to run nix-env -u to update your imperatively installed packages.\n. @eugeneia done. It's taking a long time for snabbbot to report back, but locally it looks good.\n. @eugeneia Silly mistake - pushed the wrong repo. This is ready for review. cc @lukego \n. Note to myself: this is required for https://github.com/snabblab/snabblab-nixos/issues/13\n. @eugeneia I'll take a look at this tomorrow\n. During the inspection fo benchmarking failures I've also hit following error:\ncore/packet.lua:84: packet payload overflow\nstack traceback:\n    core/main.lua:128: in function <core/main.lua:126>\n    [C]: in function 'assert'\n    core/packet.lua:84: in function 'append'\n    lib/virtio/net_device.lua:148: in function 'buffer_add'\n    lib/virtio/virtq_device.lua:77: in function 'get_buffers'\n    lib/virtio/net_device.lua:128: in function 'receive_packets_from_vm'\n    lib/virtio/net_device.lua:114: in function 'poll_vring_receive'\n    apps/vhost/vhost_user.lua:78: in function 'method'\n    core/app.lua:83: in function 'with_restart'\n    core/app.lua:301: in function 'thunk'\n    core/histogram.lua:135: in function 'breathe'\n    ...\n    program/snabbnfv/traffic/traffic.lua:63: in function 'run'\n    program/snabbnfv/snabbnfv.lua:17: in function 'run'\n    core/main.lua:47: in function <core/main.lua:34>\n    [C]: in function 'xpcall'\n    core/main.lua:157: in main chunk\n    [C]: at 0x00451ac0\n    [C]: in function 'pcall'\n    core/startup.lua:3: in main chunk\n    [C]: in function 'require'\n    [string \"require \"core.startup\"\"]:1: in main chunk\nSee https://hydra.snabb.co/build/128546 as an example failure.\nNot sure if this is relevant, it's the only issue the matches the description. Let me know if I should open a separate issue.\n. cc @lukego @eugeneia\n. Make sense. Would then Makefile boil out with an error message saying it should be created?\n. In any case, make test (as currently described in docs) will always fail as at least some tests require root.\n. @eugeneia sure, could you help me with that paragraph?\n. Thanks! This can be closed now?\n. @lukego Very close. I wouldn't collect the results from machines, but rather hydra. It's going to be easier since you need to use hydra to know what derivations were built, then you might aswell download the results (or go to the servers, but then you need to use http and ssh).\nOther than that it sounds doable. We'd limit one job per slave to avoid artifacts.\n. All Nix builds are executed in chroot, so I'll have to see if it bind mounts something like /run or /tmp, otherwise I could add that so lock can be shared\n. @eugeneia @lukego I can add this cronjob to https://github.com/snabblab/snabblab-nixos/ repository and configure it to run on eiger. \n@eugeneia \nWhat credentials are you using for github?\nCan you paste me the cronjob line currently on davos?\nThanks!\n. One nitpick. I'd also rather place the expression into src/doc/default.nix, then it can be just invoked in that directory with nix-build.\n. @lukego sure! I'd give #835 a go.\n. Done: https://github.com/SnabbCo/snabbswitch/pull/836\n. This can be closed as it's included in https://github.com/snabbco/snabb/pull/876\n. Thanks! How long do we keep this PR open?\n. Looks good. I've yet to test it.\nNot sure if src/scripts/ is the best place to keep nixos modules, maybe src/nixos/?\n. @lukego the informal convention is:\n- to put default.nix in directories that are built using that expression (just like Makefiles)\n- to put release.nix at toplevel, which imports all default.nix and builds all parts of the repository\n- to put nixos modules upstream somewhere, there is really no convention about it. I'd keep them in separate folder to avoid clobber\n@lukego so TL;DR #831 was a Nix expression, while this PR is a NixOS module.\n. @eugeneia Will do, thanks for this expression!\n. This is now part of https://github.com/snabblab/snabblab-nixos/blob/master/modules/snabb_bot.nix, thanks @eugeneia \n. TL;DR: not without refactoring driver code to support non-QEMU \"backend\".\nLooking at https://github.com/NixOS/nixpkgs/blob/master/nixos/lib/test-driver/Machine.pm, all of the commands are executed via qemu monitor to the guest machine. Some are very qemu specific such as start, which would start a VM. We could refactor that to introduce \"local\" backend and have a subset of commands available there.\nFor all the tests not doing performance testing, we could just port them to existing functional testing framework.\n. @nnikolaev-virtualopensystems Yes. See https://github.com/NixOS/nixpkgs/issues/5241\n. @lukego I'll write a draft and post it here.\n. @plajjan thanks for the review. I've addressed the two comments. @lukego do you agree we should mention snabblab in main README.md?\n. @eugeneia any idea why it's complaining Could not find data file .images/Architecture.png?\n. Yup, it builds now: https://snabbco.github.io/sha1/e5f41835a8b9ef0b8503fc091dd2d9c8824a151e.html#snabblab\n. @lukego @eugeneia what's missing here? I'd like to get this into next release so we have something to start with :)\n. On second thought, if ~/.test_env exists, it should(?) override the $SNABB_TEST_FIXTURES. Is that a bit better? Currently one would need to export SNABB_TEST_FIXTURES=~/.test_env. \n. There is one trick to get around that, basically plain COW: https://github.com/domenkozar/snabb-openstack-testing/blob/master/tests.nix#L74\n. It won't let me force push since it's a protected branch, so can't rebase on top of kbara-next. Sorry for the mess :-) cc @lukego @eugeneia \n. You're right. I'll make sure the `SNABB_PCI* exports can be superseeded via bash env. And that whitespace should be gone.\n. Pushed the two fixes, but the diff is bigger now:\n$ git log --oneline origin/kbara-next..origin/nix\n6cce8f4 release.nix: whitespace\n8982625 release.nix: respect bash env, provide fallback values\ne1ff0a3 Swapped in lstopo.png, replacing lstopo.pdf\naa1aac9 Renamed performance tuning file, as per review request\nafc0bfb added info on mlock=on\n294fc3b braindump on performance parameters\ndbe9a9a Initial stub on performance tuning\na357165 Add Nix expressions to build Snabb project\n. I've removed WIP, the two issues I mentioned can be addressed later.\n. @lukego nice. Wouldn't it be better to remove some outliers before plotting results?\n. Relatively easy. Depending on what distros to support. We have most deb and rpm distros already prepared and installed inside the VM: https://github.com/NixOS/nixpkgs/blob/master/pkgs/build-support/vm/default.nix#L753\nNix itself is tested that way so it installs: http://hydra.nixos.org/eval/1251528\n. You should be able to set attribute QEMU_OPTS on your mkDerivation.\nYou'll also need to run qemu as sudo, which is a bit tricky. Replace:\nwith import <nixpkgs> {}; to\nwith import <nixpkgs> {\n  config.packageOverrides = pkgs: {\n    vmTools.qemuProg = \"/var/setuid-wrappers/sudo ${pkgs.qemu}/bin/qemu-kvm\";\n  };\n};\nThat's not enough, the whole build is done in chroot, so you have to escape it (I just got an idea how to do this better using https://github.com/snabblab/snabblab-nixos/issues/33).\nlib.overrideDerivation (runInLinuxImage (snabb // { diskImage = diskImages.centos65x86_64; })) (attrs:\n  { __noChroot = true; requiredSystemFeatures = [ \"performance\" ] }\n);\n. We should make this easier eventually once https://github.com/snabblab/snabblab-nixos/issues/33 is implemented.\n. runInLinuxVM doesn't respect diskImage, so you're using plain NixOS in that case.\n. Looking at https://github.com/NixOS/nix/blob/master/release.nix#L284, there's a bit more setup involved.\n. Scratch that, that's only needed if you want to build .deb or .rpm packages. You were very close, see https://hydra.snabb.co/eval/478\nEDIT: closed the issue by accident\n. @lukego this is ready for fixing in snabb :)\nFails on CentOS 6.5:\ncc1: error: unrecognized command line option \"-mavx2\"\nNote: I'll soon also test on Ubuntu 16.04.\n. For anyone interested, we're now doing building Snabb on a few different platforms. See https://hydra.snabb.co/eval/2832\n. I just pushed my code for that part yesterday, but tests don't pass yet: https://hydra.snabb.co/eval/475#tabs-still-succeed\n. @eugeneia I've implemented that approach, ready to be merged.\n. While I agree this is annoying, it would be better if we started using qcow2 format as it's more convenient (less space intensive, only takes as much space as actually needed). I think that currently saves ~100MB for copying.\n@eugeneia what do you think?\n. Sure, setting raw for now is a good step forward to reduce noise in logs :+1: \n. There's one unresolved question: where to run OpenStack tests. They require different kernel parameters what other tests will trip over. Maybe I could spend some time finding out is there is a common denominator that works for both, but probably not before end of the month.\n. For now those are ran on Grindelwald.\n. I'm open for suggestions, but in Nix we can't depend on .git since nix hash\nis content addressed and .git folder is not deterministic for a given git\ncommit rev.\nOn Jun 13, 2016 12:12 PM, \"Max Rottenkolber\" notifications@github.com\nwrote:\n\nIs there any way we could avoid the .version file? The way I see it it\nwill be wrong/misleading in most commits and this information is already\nencoded in git tags.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/snabbco/snabb/pull/939#issuecomment-225552490, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/AAHtg5P-qNPl_LR_-dEz7FtZo51m_jj_ks5qLTr0gaJpZM4Iy5jB\n.\n. I agree we should make release.nix more like a flat list of helper functions and disregard snabblab information.\n\n.version could denote \"upstream version\" on which Snabb fork is based. I realize it's extra work for releases, we could name development branch 2016.07.dev or something similar.\nIt's possible to avoid this in Nix, but for every git checkout I need to explicitly specify the version. Not the end of the world, but also quite tedious.\nThat being said, I'll accept if we remove .version with \"too much of a burden\" reasoning.\n. @kbara we decided not to hardcode due to the forking workflow, which would be very confusing for downstream. Instead, we'll just provide the version manually based from where Snabb will be imported.\nReady for review :)\n. Aha - this fails due to problems with davos/CI.\n. @kbara snabb-nfv-test-vanilla passes, so this is ready to be merged.\n. Strange, it worked locally, I'll test again.\n. @eugeneia I can't reproduce this using Nix toolchain on lugano-3\n$ git clone -b refactor/matrix https://github.com/snabblab/snabblab-nixos.git\n$ cd snabblab-nixos\n$ nix-build jobsets/snabb.nix --arg snabbSrc \"builtins.fetchTarball https://github.com/domenkozar/snabbswitch/tarball/qemu-dmesg\" -A tests\nI'll close the PR and reopen, hopefully that will retrigger SnabbBot build.\n. I've tested also with qemu 2.4.1, @eugeneia if you find some time to test this locally I'd appreciate it :)\n. @eugeneia could you try different qemu version? It does clearly hint to be a qemu bug. I could try 1.4.0 and see if I can reproduce.\n. Thanks!\n. @eugeneia now it passes!\n. I've managed to prepare most of what's needed, I'm waiting for our existing matrix to build successfully, then I'll create another one for this issue.\nI expect to have the matrix ready tomorrow morning, then it's just a matter of generating reports.\n. Here we go: https://hydra.snabb.co/eval/2033\n- some of the benchmark runs fail (I fail to remember why, I'll look into qemu logs)\n- only using master branch\n- no reports yet\n. I've included the report: https://hydra.snabb.co/build/81154/download/2/report.html\n. @lukego with git bisect that would indeed be the easiest. I'll look into https://github.com/snabblab/snabblab-nixos/issues/35 over the weekend, which should be what we need here.\n. @lukego not sure apparently :) Use inputs from https://hydra.snabb.co/build/81169/#tabs-buildinputs and use nix-build jobsets/snabb-matrix-packet-ABC.nix ... --arg snabbBsrc /home/luke/snabb/\n. @lukego I've renamed the CI jobs to next-regression-benchmarks and next-regression-benchmarks-small: https://hydra.snabb.co/project/snabb-new-tests\nOnce /optimize branches exist we can also change those :) \nBy \"profiler information\" do you mean we should write those tests and include them? I'm +1 on that, let's add more regressions tests.\n. Looks like SnabbBot errored out due to https://github.com/snabblab/snabblab-nixos/issues/52\n. I'll prioritize this to get qemu bisected. I've counted occurrences of errors in benchmarks and this one is the most frequent.\non lugano servers:\nFailed builds in total: 305\nFailed builds due to 'mapping to host address failedcdata': 166\nFailed builds due to being terminated after 2min: 72\nFailed builds due to 'packet payload overflow': 31\nFailed builds due to qemu telnet timeout: 0\nFailed builds due to 'no server running on /tmp/tmux-0/default': 36\non murren servers using softnic:\nFailed builds in total: 2932\nFailed builds due to 'mapping to host address failedcdata': 1777\nFailed builds due to being terminated after 2min: 408\nFailed builds due to 'packet payload overflow': 309\nFailed builds due to qemu telnet timeout: 3\nFailed builds due to 'no server running on /tmp/tmux-0/default': 427\nPS: I've yet to check if any of these issues were due to lugano-4 having intel_iommu=pt set, but since all also happen on murren, that's unlikely.\n. @lukego wrote a quick script: https://github.com/snabblab/snabblab-nixos/blob/master/scripts/hydra_parse_logs.py\n. Qemu/dpdk custom source support landed in https://github.com/snabblab/snabblab-nixos/pull/62\nThis issue appears to be fixed on next.\n$ ./scripts/hydra_parse_logs.py --url \"https://hydra.snabb.co/eval/2913?full=1\"\n...\nFailed builds in total: 2341\nFailed builds due to 'mapping to host address failedcdata': 0\nFailed builds due to being terminated after 2min: 0\nFailed builds due to 'packet payload overflow': 0\nFailed builds due to qemu telnet timeout: 0\nFailed builds due to 'no server running on /tmp/tmux-0/default': 2341\nFailed builds due to 'lib/virtio/net_device.lua:254: assertion failed': 0\nFailed builds with unknown cause: 0: []\n. These are now all fixed.\n. cc @kbara \n. Yeah. We could deduplicate the list, but still you'd have to bump snabb source to get an update.\n. @lukego the only reason I can think of is monitoring https://github.com/snabblab/snabblab-nixos/issues/89\nI'd test the hypothesis by commiting a dummy change to the branch causing problems. I'd turn off monitoring before that. Sounds good?\n. Yeah let's go with that. I'll deploy meanwhile a monitoring script that uses lock just be sure it doesn't clobber benchmarks.\n. @lukego it's quite easy, see https://hydra.mayflower.de/project/hydra-pr-jobs\nIt creates .jobset jobset, which fetches a git repository containing jobsets definitions, for example https://github.com/mayflower/hydra-prs\nThen if new configuration appears, a new jobset is created or modified.\n. Yes :) The main issue is getting Hydra built with declarative jobsets support, since Nix and Hydra master are currently in need of fixing :) But Mayflower have a branch from which they run their hydra, worth taking a look.\n</offtopic>\n. Just for the record, Hydra+Github PRs integration prototype is done, but depends on https://github.com/NixOS/nixpkgs/pull/19396\n. These are harmless, they are placeholder values for nixpkgs arhitecture to know what packages to build. I'll remove them in master :)\n. @lukego Addressed in https://github.com/snabblab/snabblab-nixos/commit/9b05d047672c09316160a6c6bd4d8aab4cc191d9\n. I'll get this upstream via igalia/lwaftr.\n. ERROR     testlog/program.snabbvmx.tests.selftest.sh\n. @lukego it is possible to specify qemu and dpdk from git: https://github.com/snabblab/snabblab-nixos/blob/master/jobsets/snabb-matrix.nix#L41\nI've just pushed a fix for qemu testing in https://github.com/snabblab/snabblab-nixos/commit/8621ed424fe5cbdb1c833f76efb763b53231188b. I'd rather add here:\n```\npostPatch = ''\n  patchShebangs .\n'';\nThis will make sure all shebangs are converted from any form into full path to bash executable. Then we can keep /usr/bin/env bash shebangs that should work on all systems including NixOS.\n``\n. Would be nice to know the branch name to open Github Pull Request to start with.master?\n. (nitpick) it would be better to set these also just likeenvironment.SSL_CERT_FILE`\n. While I prefer to use systemd timers - as they're integrated into systemd/NixOS, I'm fine using a cron here.\n. Maybe there should be a section at beginning explaining the difference between the two so the reader chooses which part to read?\n. Yes - sorry. Arch has a good quick overview: https://wiki.archlinux.org/index.php/Systemd/Timers\nNixOS reference documentation: http://nixos.org/nixos/manual/options.html#opt-systemd.timers\nExample usage: https://github.com/NixOS/nixpkgs/blob/master/nixos/modules/services/backup/tarsnap.nix#L318-L322\n. Good catch!\n. Done.\n. ",
    "tsuraan": "Is this still progressing? It looks like things were close to done, and then the ticket stalled or something. It looks like Snabb is seeing a lot of development; is there just a ton of stuff going on around the I/O 2.0 that is holding up these other tickets?. ",
    "hannibalhuang": "Hi @lukego , thx for the feedback and 100% agree with you that with regard to Snabb related work,we should concentrate on more tech details, that why I start an issue here rather than doing it in standardization bodies : P\n@nnikolaev-virtualopensystems thx for the reaching out ! We could definitely start to discuss ideas on design by proposing specs for Nomad : ) \n. ",
    "bradjonesca": "Naive suggestion here, Algolia's free \"Hacker\" package? https://www.algolia.com/pricing\n. ",
    "capr": "My 2c on initial points:\npoint 1. re versioning the API: something like love2d? that's a good example of versioned documentation IMHO.\npoints 3. 4. 5. re decoupling source file paths from namespaces: I would actually do the reverse: put an __index on the snabb table that loads submodules automatically so that accessing the unknown key snabb.link triggers snabb.link = require'snabb.link'. -- and yes, I know what you're thinking 1) that's magic -- yes, but you don't have to update a global table everytime you add a submodule; 2) that's slow -- but in my experience LuaJIT always hoists out these things from loops so I never really understood the need for local bor = bit.bor with LuaJIT. Am I wrong about this? Did anyone ever manage to speed up a loop by caching loop-constant things in locals?\npoint 6. I used this pattern a lot, works great.\n. @wingo @lukego  thanks, I'll look into this. I wish I'd have a mental model of what side traces are and when they happen.\n. what a can of worms I opened :) I understand the need for the metatype early-binding hack now, although TBH I would still cache on an as-needed basis, close-to-the-loop and documenting why I'm doing it in each case.\n...and now for the comment that I don't expect anyone to take seriously (i.e. what I'd really like to happen):\n- break snabb into separate libs, each lib at github.com/snabb/<lib>\n  - if not, at least break programs away from it\n- remove all hierarchy: put all modues flat in snabb/<module>.lua\n- let snabb work as a lib with user-provided luajit 2.1, and make it clonable in user's LUA_PATH\n- document each module in snabb/<module>.md\n  - better yet, move it all to github wiki\nI know it sounds crazy, but I speak in good faith :) I have a 500-modules-in-100-repos project that works like this and I wouldn't go back to a single-repo setup or hierarchies.\n. @lukego not without any standard-behavior-modifying-magic ;) As you say the foo/init.lua convention doesn't work with embedded modules but I guess you could hack the Makefile to generate a temporary foo.lua file for each foo/init.lua whose contents is return require'foo.init' and embed that too. But then all the open files in your editor will be called init.lua, ouch.\nI know I sound like a broken record, but what's the deal with all the directories anyway? Taxonomic hierarchies (i.e. directories that are not really functional namespaces like /bin and /lib are in Linux, but cognitive namespaces used solely for classification) only add confusion (eg. lib vs core/lib) and make you cd a lot (or navigate a lot) for no benefit. Modules are APIs and APIs are not hierarchical in nature, the API dependency is a graph. The only legitimate use of directories is for submodules which don't expose an API but serve for breaking down a large implementation, or enable an optional part of the main API, and even then I wouldn't make a directory for less than 20 submodules (foo_bar.lua is just fine). That's a long way of saying \"don't fight the runtime, work with it\".\nAlso I think the README.md convention of github is a terrible idea and promotes deep hierarchies (or cramming unrelated modules into a single doc, take your pick). What's wrong with foo.md for each foo.lua? Also note that github's flavor of markdown is a small case of vendor lock-in. If you switch to pandoc there might be small incompatibilities that need fixing (no biggie but can be annoying). Alas, pandoc has many cool extensions that won't render on github.\n@wingo why did you tried hard to avoid giving your opinion? :)\n. The current directory structure doesn't reflect this layering that you're talking about. lib and apps are both in the root namespace. You have to actually look at the code to see this layering, same as with a flat namespace. From the app dev pov the layering is arbitrary. A dev's question is \"show me the spices\", not \"show me the spice rack\". What I think is happening is that those directories are loaded with policy as you explained above, but this policy is not directly visible by users, they only see the directories and wonder what's up with that. So as long as you have to explain the meaning of the layering out-of-band in some documentation, you can have any kind of directory structure or none at all, it's all the same.\n. The user wants require\"snabb.spice\" but the layering makes them do require\"snabb.first_row.second_place.spice\". \nAnyway I think I'm not doing a very good job at explaining this so I'll stop here, but I wanted to at least try. Naming and organizing is complicated business :)\n. I'm trying to identify actionable stuff for me:\n- extreme normal form -- from my pov. it already is but let's talk about it\n- squashing history -- should I do that? I think history is not a very efficient way of understanding the code (it gets more important after the first release)\n- removing ipsec support - currently the code is disabled with a simple if false until I make it work (. does it help if I remove it?\n. @eugeneia Ack\n. Ok i get it now. Coming up.\n. got it. not annoyed at all, this is useful.\nre whitespace -- is there a policy on this? my editor removes them automatically when I save but I see that some people's editors don't do that.\n. (in fact my editor normalizes the files in many ways: convert tabs-to-spaces or viceversa, single empty line at eof, no whitespace, crlf->lf or viceversa)\n. Makes sense.\n. Ok, let us know when it's ready so we can switch the remote.\n. I removed the tabs and added a comment on why that change is needed: basically if you modify the arg table directly, you won't be able to reuse it and when the app gets reconfigured it crashes or something like that IIRC.\n. @eugeneia probably not. Thing is I don't remember making those changes myself, maybe they come from a merge. Hm...\n. @eugeneia  I tried to revert that merge but it's given me a lot of conflicts. I wonder, since it's a merge of master, why do I need to revert it since the PR targets master anyway which should have those already. (scratching head)\n. ",
    "mraleph": "I would not say the idea (replying pre-header into the side-trace) is completely simple - it comes with a lot of tiny things to figure out (one of the most important things is how to avoid increasing the size of generated code to much). \n. ",
    "teknico": "I found this discussion while looking for a way to improve code readability.\nI share @wingo's concerns about decoupling the API from the code's actual structure: that would impair code maintainability. A cleaner, if more arduous, way would be to actually restructure the package layout.\nI also share @eugeneia's point that there's value in keeping libraries independent from the core, and able to be used and evolve independently from it.\nMomentarily disregarding the public API and package layout issue, I'd like to first concentrate on namespacing and module interdependence, for which I created a new discussion: #1132.. > Ideally there would be a way to pass a --debug or --backup option to keep the files if that is desired.\n\nThis would require additional command line option parsing before forking (NYI).\n\nAn option to keep the data directory around upon termination would be valuable.\n. > I have made _G.developer_debug configurable via environment (SNABB_DEBUG)\n\nand main no longer deletes the run-time directory when _G.developer_debug is true.\n\nGreat, thank you. Are env vars documented anywhere?\n. Yes, quite reasonable, thank you. :-) The implementation plan is in this issue.. The pain points are two:\n1) encapsulation leaks and risks of unwanted side-effects among modules, due to reliance on an opaque and crowded global namespace;\n2) low clarity of where names in the global namespace come from, which impacts readability.\nSee the resources linked above for details.\nI'd like to confine usage of the global namespace to library names, and avoid adding to it. To help detect unwanted usage, a tool like luacheck will be useful.\nI propose two phases:\n1) remove access to names in the global namespace like config, engine, link etc., as done in Igalia PR 777 and Igalia PR 778;\n2) remove addition of names to the global namespace by defining a table for each module where to put the public names it defines (again, see the linked resources for details). The require statements at the top of each module will then get the names other modules export from their respective tables rather than from the global namespace.. The lookup optimization using FFI objects seems quite promising, and the sprayer.lua rewrite, with the lexical convention for modules, does increase encapsulation and localization.\nHowever, I'm wary of automatic module loading and of increased reliance on the global namespace.\nI disagree with your current self (and agree with a former one :-) ) that the require lines are boilerplate. They are an explicit indication of what external code the modules is referencing: without them, I fear readability will decrease.\nAs you indicated previously, at the moment seven names are added to the global  namespace from a single spot in the code, the core.main module (and that's not currently documented, AFAICS). With this proposal, IIUC, all intermodule references would be implicitly injected in the global namespace.\nIf that's going to be the outcome of this discussion, I fear I'll regret (re)starting it. Is there any way to get lookup optimizations and code encapsulation without harming readability?. > Is it reasonable to classify this discussion as subjective?\nI guess it is, and enough has been said.\nAs mentioned, I like the capitalization idea for modules. It reminds me of the Go convention for private/public names, even though different.. LGTM. 0.1 seconds seems a good middle ground between the old driver behavior and 2 seconds.. Not work-in-progress anymore, so feel free to look at these changes in detail.\nWe already have another branch, based on this one, which switches the whole codebase to use the intel_mp driver, and will create a PR for it once we deal with any changes needed here.. Superseded by #1197.. I don't want to lose all stderr output and it's too complex to selectively filter the output at the test level, so I'll reinstate the message, but only when using a different seed from the last one.. Right, and indeed it does not help. I looked again at the code and cannot find from where all the calls to lib.randomseed come, so I give this part up.. Ah, I solved a conflict in the wrong direction, sorry. I'll revert.. Should this say intel10g instead of intel_mp.lua?. ",
    "someonelse": "My user name is @someonelse, I don't think I was meant to be included in this coding haha. \n. ",
    "mention-bot": "By analyzing the blame information on this pull request, we identified @dpino, @alexandergall and @eugeneia to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @eugeneia, @lukego and @aequabit to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @nnikolaev-virtualopensystems, @lukego and @justincormack to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @nnikolaev-virtualopensystems, @justincormack and @eugeneia to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @eugeneia and @hb9cwp to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @justincormack and @noj to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @alexandergall, @lukego and @hb9cwp to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @nnikolaev-virtualopensystems, @javierguerragiraldez and @alexandergall to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @eugeneia to be a potential reviewer\n. By analyzing the blame information on this pull request, we identified @wingo to be a potential reviewer\n. By analyzing the blame information on this pull request, we identified @alexandergall and @javierguerragiraldez to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @eugeneia, @javierguerragiraldez and @nnikolaev-virtualopensystems to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @altexy, @eugeneia and @javierguerragiraldez to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @lukego, @dpino and @alexandergall to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @lukego and @nnikolaev-virtualopensystems to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @eugeneia and @aequabit to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @eugeneia, @lukego and @javierguerragiraldez to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @eugeneia, @alexandergall and @hb9cwp to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @lukego, @nnikolaev-virtualopensystems and @llelf to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @alexandergall, @lukego and @hb9cwp to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @eugeneia, @nnikolaev-virtualopensystems and @javierguerragiraldez to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @nnikolaev-virtualopensystems to be a potential reviewer\n. By analyzing the blame information on this pull request, we identified @eugeneia, @javierguerragiraldez and @lukego to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @wingo, @alexandergall and @nnikolaev-virtualopensystems to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @eugeneia to be a potential reviewer\n. By analyzing the blame information on this pull request, we identified @lukego, @eugeneia and @altexy to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @lukego, @eugeneia and @llelf to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @lukego, @eugeneia and @javierguerragiraldez to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @lukego, @altexy and @javierguerragiraldez to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @lukego, @javierguerragiraldez and @llelf to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @lukego and @javierguerragiraldez to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @eugeneia and @wingo to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @lukego and @aequabit to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @lukego and @eugeneia to be potential reviewers\n. By analyzing the blame information on this pull request, we identified @eugeneia and @hb9cwp to be potential reviewers\n. ",
    "aperezdc": "Some background on how I got to write this patch: while adding support for running the lwAFTR using TAP interfaces (see Igalia/snabbswitch#236), one of my tests was to use a TAP interface for the B4 side and RawSocket for the Internet side, and it would crash with:\napps/tap/tap.lua:46: Failed read on aftr: Bad file descriptor\nstack traceback:\n        core/main.lua:122: in function <core/main.lua:120>\n        [C]: in function 'error'\n        apps/tap/tap.lua:46: in function 'method'\n        core/app.lua:76: in function 'with_restart'\n        core/app.lua:287: in function 'breathe'\n        core/app.lua:241: in function 'main'\n        program/lwaftr/run_nohw/run_nohw.lua:112: in function 'run'\n        program/lwaftr/lwaftr.lua:17: in function 'run'\n        core/main.lua:41: in function <core/main.lua:32>\n        [C]: in function 'xpcall'\n        core/main.lua:151: in main chunk\n        [C]: at 0x004511b0\n        [C]: in function 'pcall'\n        core/startup.lua:1: in main chunk\n        [C]: in function 'require'\n        [string \"require \"core.startup\"\"]:1: in main chunk\nUsing strace I found out that at the Tap application would be issuing a few hundreds of calls to read(), then close() would be called on the FD, and the next call to read() would cause the above error.\nThen I added calls to print(debug.traceback()) inside syscall.close() to better understand from where the calls were coming. During the initialization, in RawSocket:new(), which ends up calling if_nametoindex(), file descriptor 5 gets closed, and then the same number gets assigned to the Tap socket FD, and then after a while a __gc metamethod kicks in and closes the same FD number 5 again, which causes the next read() to fail:\nclose(5)\nstack traceback:\n        syscall/syscalls.lua:80: in function 'close'\n        syscall/linux/util.lua:50: in function 'if_nametoindex'\n        apps/socket/raw.lua:20: in function 'new'\n        core/app.lua:165: in function <core/app.lua:162>\n        core/app.lua:201: in function 'apply_config_actions'\n        core/app.lua:110: in function 'configure'\n        program/lwaftr/run_nohw/run_nohw.lua:90: in function 'run'\n        program/lwaftr/lwaftr.lua:17: in function 'run'\n        core/main.lua:41: in function <core/main.lua:32>\n        [C]: in function 'xpcall'\n        core/main.lua:151: in main chunk\n        [C]: at 0x004511b0\n        [C]: in function 'pcall'\n        core/startup.lua:1: in main chunk\n        [C]: in function 'require'\n        [string \"require \"core.startup\"\"]:1: in main chunk\nTAP socket FD: 5\n...\nclose(5)\nstack traceback:\n        syscall/syscalls.lua:80: in function 'close'\n        syscall/methods.lua:145: in function '__gc'\n        apps/socket/raw.lua:43: in function 'can_receive'\n        apps/socket/raw.lua:37: in function 'method'\n        core/app.lua:76: in function 'with_restart'\n        core/app.lua:287: in function 'breathe'\n        core/app.lua:241: in function 'main'\n        program/lwaftr/run_nohw/run_nohw.lua:112: in function 'run'\n        program/lwaftr/lwaftr.lua:17: in function 'run'\n        core/main.lua:41: in function <core/main.lua:32>\n        [C]: in function 'xpcall'\n        core/main.lua:151: in main chunk\n        [C]: at 0x004511b0\n        [C]: in function 'pcall'\n        core/startup.lua:1: in main chunk\n        [C]: in function 'require'\n        [string \"require \"core.startup\"\"]:1: in main chunk\napps/tap/tap.lua:47: Failed read on aftr: Bad file descriptor\n...\nThat's what led me to look at syscall.util.if_nametoindex() to see how it works and understand why it would cause the file descriptor it used to be GCd later on. There's where I noticed that the function was incorrectly passed an additional argument, and decided to move the call to the beginning of RawSocket:new() to simplify error handling, in an attempt to avoid the file descriptor for the socket being GCd later on.\n(Related to this, I have also posted a patch for ljsyscall to immediately close sockets on errors when calling the lower-level implementation of if_nametoindex(), see justincormack/ljsyscall#188.)\n. @eugeneia: This patch is enough by itself to fix the issue.\n(The other patch for ljsyscall is not technically needed: it avoids having the file descriptor open waiting for the GC to collect it in case of an error, by explicitly closing it. I made the patch to ljsyscall as a small enhancement while investigating this issue.)\n. @lukego: I really like the idea of including documentation in the source files :+1: \nIf the idea goes forward, it would be amazing to adopt it also for generating the output of --help for programs. That way the \u201ccommand invocation help text\u201d (which, from my POV, is also a kind of \u201conline\u201d documentation) would also be next to the source code that it is describing.\n. If it is not a big lot of work (and hopefully it shouldn't!), I also think it would indeed nicer to have the full history.\nRelated to @lukego's comment on git subtree: AFAIU it should be possible to keep the whole history of a subtree by using git read-tree when importing for the first time, then using git pull -s subtree to pick new commits (more details). I was thinking of doing that when https://github.com/aperezdc/ljndpi was imported into the Snabb repository, but then I didn't because the other two modules under lib/ have their history squashed, so I did the squashing for consistency.\n. I have just pushed version 0.1.0 of ljndpi. Despite the version bump, the functionality is the same as the previous (v0.0.3), with the license now being Apache 2.0. It should be reasonably easy to pull the changes into your local branch and update the PR.\nEdit: Given that ljndpi has not needed changes for a good while and it has been working flawlessly, it seemed like a good idea to bump the minor version number to transmit the idea that it's more stable than one would usually expect from a 0.0.x version :wink:. It sets the base table itself as metatable for created instances. It is a common Lua idiom in many projects, though in Snabb I have seen the return setmetatable(obj, { __index == BaseObject }) used more often (if not always).\nIt can be argued that setting the base object as metatable (Base = {}; obj.__index = Base) as done in the patch saves the creation of one table per instance created. In the particular case of Snabb applications, there will never be huge amounts of them being created, so the gain is marginal.\n. ",
    "harishiitd": "HI Luke,\n   Thanks for the inputs. It helped me to understand the snabb 10g driver better.\nMy goal is to fix selftest bug and run it successfully with X540-AT2 nic .\nWhen I tried to debug , i could see that there is wait loop checking  linkup state, and we are starting traffic only aftter reading the linkup bit  of \"LINKS\" register (bit no 30) in the init functions of both PF and SF.\ncode sample:\n  local mask = bits{Link_up=30}\n   if band(self.r.LINKS(), mask) == mask then\n         return self\nSo my assumption is that link status is not  an issue. Please correct me if i am wrong.\nHarish\n. Hi Luke,\n     It looks like the test cases involving VFs in selftest are failing (mq_sw and mq_sq).\nTestcase for sending traffic between two SFs(sq_sq) is passing.\nIs there any difference in VF initialization between 82599 and X540. Or anything need to be taken care for X540 VFs?\nRegards\nHarish\n. Hi Luke,\n    Looks like there is a difference in VF initialization for X540.\nIn section \"4.6.11.3.3 DCB-Off, VT-On\" of datasheet \n  The value to be filled in RXPBSIZE[0] register is \"0x60000\" (0x180 << 10)\nFor 82599 this value is \"0x80000\" (0x200 << 10).\nAfter making this change , all test cases in driver selftest are passing.\nhere is my workspace changes:\ndiff --git a/src/apps/intel/intel10g.lua b/src/apps/intel/intel10g.lua\nindex 05852ed..d52504d 100644\n--- a/src/apps/intel/intel10g.lua\n+++ b/src/apps/intel/intel10g.lua\n@@ -725,7 +725,7 @@ function M_pf:set_vmdq_mode ()\n    self.r.MTQC(bits{VT_Ena=1, Num_TC_OR_Q=2})    -- 128 Tx Queues, 64 VMs (4.6.11.3.3)\n    self.r.PFVTCTL(bits{VT_Ena=0, Rpl_En=30, DisDefPool=29})     -- enable virtualization, replication enabled\n    self.r.PFDTXGSWC:set(bits{LBE=0})             -- enable Tx to Rx loopback\n-   self.r.RXPBSIZE0             -- no DCB: all queues to PB0 (0x200<<10)\n-   self.r.RXPBSIZE0             -- no DCB: all queues to PB0 (0x180<<10)\n  self.r.TXPBSIZE0             -- (0xA0<<10)\n  self.r.TXPBTHRESH0\n  self.r.FCRTH0\nThe final change could be depending on the device model fill the appropriate value in RXPBSIZE[0] register.\nRegards\nHarish\n. Hi Luke,\nThanks for the reply, it works perfectly :).\nI have a doubt ..\n    Does the packet switching between VMs happens in software or through NIC's L2 switch?\nRegards\nHarish\n. ",
    "DemiMarie": "@eugeneia I think the fact that the precondition for shiftright being unclear makes it especially important to check.\nI think that security is the most important reason to ensure safety on public APIs.  Buffer overflows are exploitable, and Snabb Switch usually runs with superuser privileges and direct access to the hardware.  Furthermore, Snabb Switch runs on machines that could do a lot of damage should they become compromised, if nothing else due to the volume of traffic that passes through them.  A buffer overflow in Snabb Switch is just as exploitable \u2013 and with the same consequences to the vulnerable machines \u2013 as a buffer overflow in the Linux kernel's network code.\n. One feature that would be very helpful is to add assertions that ensure that the Snabb API is used correctly.  An assertion failure is much more helpful than a random segfault (or worse) due to memory corruption.\n. ",
    "fsaintjacques": "I just tried to run the tests and it failed because /var/run/snabb didn't exists. Since I don't have root on the host (I am aware this is not ideal to test such package), I had to manually edit core/shm.lua for something else. Might I suggest you add a runtime flag to snabb command-line, e.g. --run-path defaulting to the current value. This would allow to run tests in a localized fashion.\nFIY, my goal was to tests some changes in non-root required path.\n. ",
    "darius": "To help catch problems like this one early, I tend to stick a line like\nset -euo pipefail # 'bash strict mode'\nat the top of my shell scripts -- then they fail if any of their commands fails unexpectedly.\n. ",
    "yu-kasuya": "@nnikolaev-virtualopensystems \nchar0 and net0 are already used when you boot ubuntu1, so you have to another ids in ubuntu2.\n. ",
    "RossBencina": "HDR Histogram is not the same thing, but possibly worthy of a see also: https://github.com/HdrHistogram/HdrHistogram\n. ",
    "heijiu": "I want custom a web Application Firewall\n. ",
    "lperkov": "It would be nice if there were instructions how to run docker images... We have some features we need to develop on top of snabb swtich and were planing on using docker which we could run on these machines.\nWhat do you think if we did some recording using asciinema and posted it for everybody to see? Something like this:\nhttps://asciinema.org/a/05cdmz78fhcl5jeo4xyiqqr33\n. ",
    "jsnell": "My usual worry with auto-vectorization for critical hotspots is that at some point it'll fail to trigger for whatever reason, and you'll end up silently running the naive version of the code. It's ok in a closed environment where the compiler versions are guaranteed to be fixed and only get upgraded at very specific points in time, but more problematic when you as the author have little control over how this gets compiled.\nThough the simplicity of that new C code is pretty sweet.\n. ",
    "AkihiroSuda": "What is current status?\n. ",
    "leinilein": "a selftest makes perfect sense! I will do so but can not promise it before mid of next week\n. I have implemented a selftest with source and sink app; before pushing it i would like to run the test but did not find any kind of documentation how your selftests really work. Is there any source or most recent example that i missed?\n. Yes, that was the piece of information i was looking for; unfortunately this is not documented - or at least i did not find this documentation. I will commit probably by the end of this week; thanks for the helping hints @eugeneia.\n. ",
    "adw555": "For me, working from the latest release backwards, using RawSocket I don't get correct execution until the snabb-2016.02 release. For all releases after February, the packet throughput is minimal for my application.\n. Just some figures to illustrate. My application is listening on eth0 using a RawSocket and piping the packets to the input of my application. Working correctly I will get just over a 1000 packets in a minute on this test server. You can seel all is well in the February release, but not from March onwards:\nsnabb-2016.02\nMain Report:\nlink report:\n               1,038 sent on interface.tx -> amqp_app.input (loss rate: 0%)\nsnabb-2016.03\nMain Report:\nlink report:\n                   0 sent on interface.tx -> amqp_app.input (loss rate: 0%)\nsnabb-2016.04\nMain Report:\nlink report:\n                   0 sent on interface.tx -> amqp_app.input (loss rate: 0%)\nsnabb-2016.04.1\nMain Report:\nlink report:\n                  15 sent on interface.tx -> amqp_app.input (loss rate: 0%)\nsnabb-2016.05\nMain Report:\nlink report:\n                  15 sent on interface.tx -> amqp_app.input (loss rate: 0%)\n. So I changed the standard example_spray code to use a RawSocket instead of a PcapReader as it then does the same as my real application. You see below I've commented out the pcapReader and used a RawSocket instead. You can see the difference between 2016-02 and 2016-03 release results.\nmodule(..., package.seeall)\nlocal pcap = require(\"apps.pcap.pcap\")\nlocal sprayer = require(\"program.example_spray.sprayer\")\nlocal raw = require(\"apps.socket.raw\")\nfunction run (parameters)\n   if not (#parameters == 2) then\n      print(\"Usage: example_spray  \")\n      main.exit(1)\n   end\n   local input = parameters[1]\n   local output = parameters[2]\nlocal c = config.new()\n   --config.app(c, \"capture\", pcap.PcapReader, input)\n   config.app(c, \"capture\", raw.RawSocket, input)\n   config.app(c, \"spray_app\", sprayer.Sprayer)\n   config.app(c, \"output_file\", pcap.PcapWriter, output)\nconfig.link(c, \"capture.tx -> spray_app.input\")\n   config.link(c, \"spray_app.output -> output_file.input\")\nengine.configure(c)\n   engine.main({duration=60, report = {showlinks=true}})\nend\ntest:~/Workspace/snabb-2016.02$ sudo src/snabb example_spray eth0 /tmp/test1.pcap\nlink report:\n               1,485 sent on capture.tx -> spray_app.input (loss rate: 0%)\n                 742 sent on spray_app.output -> output_file.input (loss rate: 0%)\ntest:~/Workspace/snabb-2016.03$ sudo src/snabb example_spray eth0 /tmp/test5.pcap\nlink report:\n                   0 sent on capture.tx -> spray_app.input (loss rate: 0%)\n                   0 sent on spray_app.output -> output_file.input (loss rate: 0%)\n. Many Thanks Peter.. ",
    "tobyriddell": "I want to de-lurk briefly... I am doing some work on automated benchmarking of systems to track performance changes/regressions and I came across a paper a couple of weeks ago. Whilst I won't claim to understand it all yet it might be of some use.\nA snippet from the abstract: 'we provide a statistically rigorous methodology for repetition and summarising results that makes efficient use of experimentation  time.  ...  We capture experimentation cost with  a  novel  mathematical  model,  which  we  use  to  identify  the number of repetitions at each level of an experiment necessary and sufficient to obtain a given level of precision.'\n\"Rigorous Benchmarking in Reasonable Time\"\nhttps://kar.kent.ac.uk/33611/7/paper.pdf\nRight, now back to lurking until I have the time to make a proper contribution :-)\n. Just a thought, but is 'intel' the correct name? How about 'intel_nic' so that it doesn't name-clash with the as-yet-unwritten code that takes a packet and passes it through the CPU-attached FPGA on the upcoming Cascade Lake series of chips (having previously programmed said FPGA to do deep packet inspection)?. This sounds like a great idea! But as this is explored further please keep in mind the impact of AVX2 (and also AVX512, should it be used in future) on CPU frequency scaling as it is a potential source of jitter.\nThere's some discussion here: https://blog.cloudflare.com/on-the-dangers-of-intels-frequency-scaling/. Some info. on blocking system calls here:\nhttps://stackoverflow.com/questions/2853653/deferring-signal-handling-in-linux\nOn Wed, May 2, 2018, 08:14 Luke Gorrie, notifications@github.com wrote:\n\nThe PC-losering https://www.dreamsongs.com/WIB.html problem just won't\ndie eh :).\nI have a vague feeling that Linux signals can be setup to not interrupt\nsystem calls i.e. to defer the signal until after the system call\ncompletes. This could be a nice solution since we are not that interested\nin profiling system calls? I can't find how to do that right now so it is\npossible that I dreamed it.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/snabbco/snabb/issues/1337#issuecomment-385957441, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AE7ethP_ObXezPOkodRn9Mb5ZmLMaCeJks5tuaMbgaJpZM4TusHx\n.\n. \n",
    "zeronewb": "@lukego \n\nDPDK code quietly uses an Intel-proprietary interface to a software component that is not available as open source. Intel are not shipping FM10K NICs. I am not sure how OEMs like Supermicro and Silicom are handling the software side when they sell FM10K-based NICs (tell me if you know). \n\nIt's kinda open. From abandoned match-interface project:\nAPI\nhttps://github.com/andriymoroz/IES\nUserspace switch control utility\nhttps://github.com/ronenil/match\nOr talk with Silicom, they provide a bit more custom utility.\n. @lukego We've solved this in funny way: silicom provide details about used ies version and rdifd sources. So, we patched IES from match-interface. But, rewrite IES in pure lua  for snabb will be huge task.\n. ",
    "williammccall": "@lukego \nre: storing packets contiguous in memory/aggregated frames across the bus\nI am aware of some silicon forwarding implementations that rely on this \"aggregated frames\" concept across the backplane switch fabric. As I recall, the packet processors were sending/receiving them to/from fab deaggregated because of an intermediate step in fabric tx/rx (which I reckon had more to do with locality breeding easier interconnect wrt deaggregated frames and packet processors). The way packets were brought across fabric to prevent HOL blocking in this case was to arbitrate frames in VoQ fashion. This assumes, I think, that one can A) model the output queues and B) determine or, perhaps, even simply predict the output device before bringing it in.\nIf you have tighter control of the PCI interaction and expose a set of \"VoQs\" as (example) a separate credit recipient from PCI perspective, would this let you stop HOL blocking and potentially just drop in HW? Would the complexities of modeling some sorts of \"predictive\" queueing outweigh the benefits? And more importantly, is this idea really just too device-specific, situation-specific and overall just a pain to handle? Hoping this will breed some other ideas too, perhaps without the need for such complicated predictions in our COTS NIC HW.. ",
    "mlilja01": "@fmadio I totally agree that 100G sustained come with costs but I just want to point out that Napatech actually do 100G sustained for all packet sizes ;)\n@lukego This is quite an interesting discussion that touch several good topics. One of the things are \u201ccontiguous serial memory\u201d and in that regards I just want to stress out that this approach from a PCIe perspective has the advantage that you avoid lots of overhead enabling you to get sustained 100G with packet sizes < 128B. It\u2019s going to be a while before PCIe Gen4 is to be found in any Xeon systems, so right now if you really want to have 100G sustained my take is that the \u201ccontiguous serial memory\u201d is the only approach that works and then you must RSS scale to multiple cores to hide the overhead of the potentially involved memcpy().. @fmadio I think we have had that since last summer :). ",
    "rohitjoshi": "planning to create WAF using lua based OWASP  rules.  If TCP Stack is available, I can build lightweight HTTP stack and implement rules.\n. ",
    "emmericp": "We use Open vSwitch at the moment for a simple test setup.\nBut we also tried it with Snabb NFV vhost user recently and got the same problem: it works fine with the kernel virtio driver in the VM but the Snabb virtio app doesn't see any packets in the VM.\n. Which version of qemu are you using?\n. Hi,\nthanks for the detailed response. Snabb NFV would probably be a work-around. However, using SnabNFV is not what we are looking for at the moment.\nWe will probably skip the virtio setup for now, it's not yet important for us.\nHowever, it would be nice if this could be fixed. As I've mentioned above: I can provide access to a test VM with two VirtIO interfaces connected to a packet generator that work fine with the kernel driver and the DPDK driver but not with Snabb.\nI understand that Open vSwitch will be a huge bottleneck for the small packet IO performance, but we are looking for something that plays well with different hypervisors and configurations for a NFV project for a 5G network backend.\nPaul\n. Thanks for the detailed explanation!\nI've installed qemu-2.4.1 and it's working with SnabbNFV :)\nUnfortunately, it still doesn't work with a \"normal\" virtio backend. But SnabbNFV is close enough for our project for now.\n. Okay, so I ran a few very basic tests with all 4 combinations of busy/non-busy.\nThroughput\nSnabbNFV/Snabb in VM\n(B = busy, N = default/non-busy)\nB/B: 4.09 Mpps\nB/N: 3.23 Mpps\nN/B: 4.04 Mpps\nN/N: 2.23 Mpps\nThis is already quite surprising, I didn't expect such a big effect here. Enabling busywait inside the VM seems to be a good idea.\nLatency\nMeasured at the three (arbitrarly chosen) packet rates: 0.2 Mpps, 1.4 Mpps, 2.0 Mpps\nSnabbNFV/Snabb in VM\n(B = busy, N = default/non-busy)\nB/B: 22/25/45 \u00b5s\nB/N: 56/85/200 \u00b5s\nN/B: 16/25/44 \u00b5s\nN/N: 95/90/95 \u00b5s\n(Take these measurement results with a grain of salt: they are only based on a few thousand samples and these values are averages and I had active SSH sessions on the device under test. So the difference between 16\u00b5s und 22\u00b5s is not significant)\n--> The high latency is not SnabbNFV's fault, but an effect of the VirtIO driver in the VM.\nRunning the same script without VMs with the Intel82599 driver results in a consistently low latency of 10-20\u00b5s.\n. I'll be giving a talk at Saturday 13:00 with one of my students: https://fosdem.org/2019/schedule/event/writing_network_drivers_in_high_level_languages/\nSee https://github.com/ixy-languages/ixy-languages, might be of interest to some of you?\nEdit: our talk will be mostly about Rust due to time constraints. Recordings are available:\nLuke's talk: https://fosdem.org/2019/schedule/event/how_connectx_device_driver_works/\nOur talk: https://fosdem.org/2019/schedule/event/writing_network_drivers_in_high_level_languages/. ",
    "xray7224": "@eugeneia I decided it was probably fine as is. One of the main reasons for deciding this would be having to change SNABB_SHM_ROOT which is user configurable to not be the directory with the PID files in. Currently my code uses that as a base and creates \"by-name\" inside of it however, changing it to be by-pid leaves me with two choices:\n1. Change what the shm.root (SNABB_SHM_ROOT) actually points to (a directory with both by-pid and by-name).\n2. Use it as is but step back one directory from the root which seems like encroaching outside of the space set aside by the user.\nIf i were to do this I think I'd definitely go for option one and perhaps this change should be made WDYT?\n. Yeah it's landed upstream, thanks for the reminder. I'll close it.. Oops, posted to the wrong fork, this is what one gets for doing PRs when tired.. I shall be there too with the other Igalian folks :) . ",
    "daurnimator": "IMO this is a bad choice: I'd prefer to see IPv6 and IPv4 be usable with the same style of code.\n. ",
    "chrgraf": "Agreed Andy. Many thanks\nChristian\n\nChristian Graf\nSenior Consulting Engineer\nTelephone +49 5651/336174\nMobile +49 175/2929 167\ncgraf@juniper.net\nwww.juniper.net\nJuniper Networks GmbH\nSitz der Gesellschaft / Registered office: Oskar-Schlemmer-Str. 15, 80807 M\u00fcnchen\nRegistergericht / Commercial registry: M\u00fcnchen, HRB 205571\nGesch\u00e4ftsf\u00fchrer / Directors: Johannes Albers, Stephen Byrne, Terrance Floyd Spidell\n\nThis email contains material that is confidential. The content of this email is for the sole use of the intended recipient(s).\nAny review or distribution by persons other than the intended recipient(s) without the express permission of Juniper Networks is strictly prohibited.\nIf you are not the intended recipient, please contact the sender and delete/destroy all copies of this email and any related attachments.\nJuniper does not guarantee the accuracy or completeness of third party materials or information\n\nFrom: Andy Wingo notifications@github.com<mailto:notifications@github.com>\nReply-To: snabbco/snabb reply@reply.github.com<mailto:reply@reply.github.com>\nDate: Mittwoch, 9. November 2016 13:08\nTo: snabbco/snabb snabb@noreply.github.com<mailto:snabb@noreply.github.com>\nCc: Christian Graf cgraf@juniper.net<mailto:cgraf@juniper.net>, Mention mention@noreply.github.com<mailto:mention@noreply.github.com>\nSubject: Re: [snabbco/snabb] updated README.troubleshooting.md (#1065)\nClosing PR as it continues to get commits from Igalia/lwaftr and generating notifications -- @chrgrafhttps://github.com/chrgraf I think what @kbarahttps://github.com/kbara suggests sounds like the right thing.\n\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/snabbco/snabb/pull/1065#issuecomment-259400235, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AU_G6pwosU3Fe1lqGux4SmbNIQyZceDTks5q8bfAgaJpZM4KtVRQ.\n. ",
    "takikawa": "Yes, thanks that's fine with me too. (and having this merged right after the next release will be helpful for the upcoming Snabbwall PR, which depends on a few changes in this PR). Thanks for the comments! :)\nWith regard to dependencies, there are two reasons why we went with a runtime dependency. The first is that the license (LGPL) of nDPI is incompatible with statically including in Snabb as far as I know, but should be ok for dynamic linking.\nThe second is that Snabbwall's scanner backend is intended to be interchangeable (the backend just needs to implement the generic Scanner class), and so it may not make sense to include nDPI if the user wanted only a different backend. (in practice the nDPI backend is the only one available currently)\nRe: squashing vs. whole history, I'll revise this PR to include the whole history instead of just the squashed commit. I didn't have a strong preference either way, so I'm happy to make the change.\nFor the upstream branch, maybe the coordination would be easier with another Igalian, but I'm happy to fit to whatever the usual workflow is. (@wingo or @kbara do you have any thoughts on that?). Hi @lukego. Thanks, that plan sounds good to me! For the long-lived branch, my current plan is to host the branch on Igalia/snabb. I'll put together the pull request for src/doc/branches.md soon, and it looks like the next-hop won't be a problem either.\nSo I'll go ahead and close this sketch PR and proceed in separate PRs. Cheers.. Note: I updated snabbwall by merging in next (usually snabbwall has tracked only released versions) to avoid the merge conflict in this PR (so, e.g., CI can run).. @aperezdc thanks, the branch should be updated with your license change now.. Superseded by #1180.. Hello folks. We updated this branch with more recent development work on intel_mp. At this point, the intel_mp driver code in this PR is compatible enough with intel10g that we can switch over all the snabb programs to use it and CI will pass. We put the code required to switch over to intel_mp on a separate branch to keep this PR smaller and less invasive.\nThe PR also fixes a few performance issues and we are working on doing more extensive performance testing.\nAny feedback would be very helpful, especially on user-facing aspects like the configuration of the VMDq pools or the general approach to integrating these features. Apologies for the large code dump, but we figured it would be more useful to have the entire driver code instead of the initial PR that we had.\nNote: switched the base branch to master to have the CI run.. Yes, this PR has been superseded by #1197 (#1199 builds on #1197 instead of this PR) which has a few more commits, so I'll go ahead and close this one. Thanks for the reminder.. @wingo very nice, and hooray for better performance! I'll go ahead and close #1149. Sounds good on verifying that it still works with a collector. It would probably be nice to ship a test script to run the probe and collector (I packaged nfdump for NixOS a while back for this purpose) with some sample data.. Whoops, I must not have notifications turned on correctly since I didn't see the last few comments until now. I just pushed some additional commits to fix some bugs, add tests, and resolve merge conflicts.\nThanks for the comments @lukego! In terms of what users should expect, we made an effort to support the documented interface of intel10g for intel_mp for the most part. One difference is the input and output link names (which you've already noticed). That might be a hurdle in switching over though, so it might make sense to use intel10g's names.\nBTW, commit 7d13977 was there to avoid a specific bug that happened on app reconfiguration. I think we may have fixed the underlying cause though, so we may be able to revert it now. I'll try it out and see.\nI think it would be helpful to make sure the API for setting VMDq pools and Rx/Tx queues is the right one. We made the rxq and txq config parameters default to 0 (for selecting RSS queue numbers) and VMDq pools are selected automatically like intel10g. That way, the default behavior resembles intel10g with no arguments needed.\nIn case someone needs just RSS with no VMDq, providing explicit rxq and txq lets you have serve multiple queues (setting a default doesn't let you explicitly disable Rx/Tx on a specific app instance though, maybe we should allow this via a \"disable\" config value). For both VMDq + RSS, you can set rxq and txq from 0 to 1 for 2-way RSS but you probably need to explicitly choose pool numbers to ensure all the RSS queues are assigned to the same pool (the automatic selection will put them all on different pools). That last one does seem a little clunky perhaps.\nIf that API seems ok, then I think we're good. But we can also change it now if there's anything that looks off.\nAlso I agree with @wingo on needing to figure out the reliability. We've of course been testing the driver and writing more tests, but real-world testing is bound to find more issues. :). @lukego Looking through some of the log files, it looks like something with vhost_user might be failing on some of the iperf tests. The snabb log has vhost_user: Connection went down: vhost_A.sock and qemu log has entries like\nqemu-system-x86_64: unable to start vhost net: 1: falling back on userspace virtio\nkvm_mem_ioeventfd_add: error adding ioeventfd: File exists\nThe l2fwd failure cases are puzzling, since the logs don't seem very different except that in the snabb logs it just stops transmitting packets after a bit. Anyhow, that provides some hints that I'll look into.\nI haven't been able to reproduce any failures by running the selftest.sh bench (for iperf) and dpdk_bench.sh (for l2fwd) on our Igalia snabb machines though. If you have any tips on reproducing it, I'd be happy to try things.. I wonder if the qemu error is related to this issue? https://github.com/snabbco/snabb/issues/1103 The error message appears to be the same as the one reported there.. I just pushed a commit that will hopefully address some of the iperf failures. Not totally sure it will fix it, but I guess we will see when the Hydra results are up. And thanks for the test setup simplification @lukego, it made it easier to sort through the Hydra listings. :). I tried merging next into this branch to see if maybe some of the performance differences in iperf was due to other changes between the branches. It looks like that may have been the case. Here's a graph using the script @lukego posted above:\n\nI found this somewhat surprising, since there seemed to still be a bigger performance difference when I tried this comparison on the Igalia machine. There's also that gap near score 13 that could be an actual issue though.. @lukego Sorry for the unclear upstreaming path, my fault for not updating it. Anyhow, we decided it would make sense to go with wingo-next so I'll switch over the PR target (and I added that commit to branches.md). Thanks!. I looked into this issue a bit last week and I suspect that the displacement issue in this test isn't due to a problem with the hash function (which I've tested and looks to match the reference implementation too), but due to some weird memory corruption bug either in ctable or in LuaJIT.\nMy reason for suspecting this is first of all that the faulty displacement is always 43 and not a random amount based on the seed. Actually, setting the hash seed to be a constant does not change the assertion failures. This also suggests it's not the algorithm's fault.\nThe other is that if you look at this line in the load method, the header.max_displacement field is not set to 43 before the call but gets set to 43 after the call on failing runs. The hash_seed field also seems to get written over too. It's non-deterministic though, so I'm not sure what exactly it is about that function call that causes it.\n(Note: to inspect what's going on, I've been running a shell snippet like status=0; while [ $status -eq 0 ]; do sudo ./snabb snsh -t lib.ctable; status=$?; done and printing the values of fields). Update: I think the real bug here is that the selftest uses the FFI in such a way that the buffer underlying the header struct ends up getting GCed before it's read.\nHere's a simplified example that shows the issue:\n```\n-- Example showing how Luajit GC works\nlocal ffi = require(\"ffi\")\nffi.cdef[[\n  struct s_t {\n    uint8_t x;\n    uint8_t y;\n  };\n]]\n-- Uncomment to make the two print calls at the end the same\n--local prevent_gc\nlocal function read()\n   local buf = ffi.new(\"uint8_t[2]\")\n   --prevent_gc = buf\n   buf[0] = 255; buf[1] = 254;\n   return buf\nend\nlocal function read_s()\n   return ffi.cast(\"struct s_t*\", read())\nend\nlocal s = read_s()\nprint(s.x)\nprint(s.y)\ncollectgarbage()\nprint(s.x)\nprint(s.y)\n```\nThis test script will print \"255\\n254\" and then some junk data because buf gets deallocated. Similarly, the read_ptr function at https://github.com/snabbco/snabb/blob/master/src/lib/ctable.lua#L690 lets the buffer in read get deallocated. I think this is consistent with the FFI semantics but it's kind of hard to tell.. Bumping this old PR because we ran into this issue recently. On some machines, the snabb loadtest transient program will crash because some memory gets overwritten by get_mempolicy.\nThe patch in the PR didn't quit work for me though, I think in particular line 491 with\nmask = t.bitmask:__new(mask, get_maxnumnodes())\nshould be something like:\nmask = t.bitmask(mask, get_maxnumnodes())\nBefore I found out about this PR, I ended up writing my own version of the fix which was very similar: https://github.com/takikawa/snabb/commit/90e9b679d3d6f18b03a59f52989be88292fada3f. I put a fixed (I think) version of this PR on the lwaftr repo downstream (https://github.com/Igalia/snabb/pull/1198) and propose to merge it there, and then upstream it in a batch with other lwaftr changes.. Thanks, I'll look into this. I've been trying to reproduce it more consistently to narrow it down but haven't had much luck so far. Will keep trying.. Some notes on performance follow. With the changes, here is an output from perf:\n```\n$ sudo perf stat -e cycles,instructions,cache-misses -B bench/basic1-100e6 \nNo PMU available: single core cpu affinity required\n24.3\nPerformance counter stats for 'bench/basic1-100e6':\n11,857,204,189      cycles                   \n30,613,368,426      instructions              #    2.58  insns per cycle        \n       219,476      cache-misses\n\n   4.155743635 seconds time elapsed\n\n```\nWithout changes:\n```\n$ sudo perf stat -e cycles,instructions,cache-misses -B bench/basic1-100e6 \nNo PMU available: single core cpu affinity required\n23.4\nPerformance counter stats for 'bench/basic1-100e6':\n12,294,127,748      cycles                   \n31,284,447,271      instructions              #    2.54  insns per cycle        \n       645,667      cache-misses\n\n   4.320305929 seconds time elapsed\n\n```\nYou can see the cache-misses metric goes up quite a bit and performance seems generally lower without this commit.. I think this is made redundant by commit acb499aa7da3b845590f8d0b890ede6eb26e1049 which is already in v2018.04 so I'll close it.. #1326 is a simpler solution for this, so I'll close this PR.. @eugeneia Yes, that's correct. That was one of the sources of intermittent test failures. It looks like there may be some intermittent failures with the non-RSS tests too (like this one where the 2 queue version is failing) that might not be solved by this, so I'll keep looking into that.. Addendum to previous comment: the 2 queue test failure was probably due to the random hash key too. It turns out the 2 queue vmdq tests also ends up exercising the RSS functionality (see https://github.com/snabbco/snabb/blob/master/src/apps/intel_mp/test_10g_2q_blast_vmdq.sh#L10 where both queues 0 and 1 are used for RX, meaning that RSS is set up to distribute between two queues). So I think issue #1313 can be closed now.. @lukego Thanks, that sounds like a good idea! I made a PR for that at #1340. It seems to get rid of the selftest hang nicely. Not sure if it's made much difference in vmprofile results, but some basic sanity checks comparing the results with/without ITIMER_PROF seemed ok anyway.. I started reviewing this a few days ago (sorry for the delay), but I don't actually have access to a suitable machine with useable i350 NICs to test any changes. Is there a machine somewhere I could use for that?. I looked into the issue with test_1g_2q_blast_vmdq_auto.sh and I am suspecting that part of the issue is on the tx side in the test script. In particular, testsend.snabb uses the Repeater app to repeatedly send the contents of the test pcap. The test pcap has half packets from one MAC and half with another to exercise VMDq.\nChanging testsend.snabb by adding in a Sample app after the Repeater fixes the imbalance, with both NIC apps receiving roughly the same number of packets. Maybe testsend.snabb is currently dropping packets such that mostly only the first half of the pcap gets transmitted to the receive interfaces.\nI'm having trouble connecting to davos right now, but I'll keep looking into this later.. Just a small update on this: I'm working on a set of patches that might fix this issue. Basically it would let the lwAFTR run without enabling VMDq in certain cases, which should make it possible to run on i210 NICs. Will update when the PR is more ready.. Yeah that sounds reasonable to me, as long as there aren't conflicting needs for setting the seed.. This is pretty minor, but apparently \"queuing\" and \"queueing\" are both valid spellings so I'd prefer not to change it to avoid breaking any existing uses.. ",
    "mmanoj": "Dear All,\nAny update/ advice to resolve this issue. Your advice is highly appreciated.. Hi Alex,\nThanks for the advice, I'm using one physical and one virtual interface. Let me try two physical interfaces and the behavior. My app network is simple bridge eth01 get the input and send to eth02. In between I will do some traffic selection/DPI/QoS.\nI will keep you updated the progress. Thanks again for the guidelines.. Hi Alex,\nFollowing is my network interfaces:\nRJ45:\nenp0s31f6 Link encap:Ethernet  HWaddr 50:7b:9d:68:74:7e\n          UP BROADCAST MULTICAST  MTU:1500  Metric:1\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000 \n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\n          Interrupt:16 Memory:e1300000-e1320000 \nWireless:\nwlp1s0    Link encap:Ethernet  HWaddr e0:94:67:47:63:f5\n          inet addr:192.168.1.102  Bcast:192.168.1.255  Mask:255.255.255.0\n          inet6 addr: fe80::e01:6c63:6d93:f3b2/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:62335 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:27556 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000 \n          RX bytes:77358308 (77.3 MB)  TX bytes:4274770 (4.2 MB)\nBoth having same MTU of 1500, however still I'm getting the same error. Please advice if any way forward.\nlocal c = config.new()\n  -- config.app(c, \"capture\", pcap.PcapReader, input)\n   config.app(c, \"spray_app\", sprayer.Sprayer)\n  -- config.app(c, \"output_file\", pcap.PcapWriter, output)\n   config.app(c, \"nic1\", raw.RawSocket, \"wlp1s0\",input)\n   config.app(c, \"nic2\", raw.RawSocket, \"enp0s31f6\",output)\n-- config.link(c, \"capture.output -> spray_app.input\")\n  -- config.link(c, \"spray_app.output -> output_file.input\")\nconfig.link(c, \"nic1.tx -> spray_app.input\")\n   config.link(c, \"spray_app.output -> nic2.rx\")\nengine.configure(c)\n   engine.main({duration=10, report = {showlinks=true}})\nend\n\nThank you,\nManoj M. @alexandergall \nI'm able to change only on 100Mbps intel card only, wireless not allow o change. So I use one physical and one virtual interface (with tso off) but still same error. Any clue ?. @alexandergall \nIf any alternative to achieve this task also welcome.\n. @alexandergall \nThanks for the advice, now no error message ;) but no traffic receiving to NIC2 while capturing traffic in second NIC.. ",
    "nzinfo": "Sorry for reply so late, these days were the Spring Festival in China. : )\nthe gist is the output, I can't figure out why the bug happen.\nhttps://gist.github.com/nzinfo/22aff96a21343168a6ddbfbbecf68fc6. I found my mistake, I use local lookup adapter ( lo ), packet passed the device twice, one for sending, and one for receiving.  But another question raised, how to identity whether a packet is send / receive ? Great Thanks.. I found there has some flag of sll_pkttype , PACKET_HOST | PACKET_BROADCAST | PACKET_OTHERHOST | PACKET_OUTGOING,  but I have no idea of how to set it. Would you please give me some help ? @lukego. ",
    "vmaffione": "Hi @eugeneia ,\n  Thanks a lot for your deep explanation, I am comparing different virtual switches solutions.\nI'm sure you are using lockless queues to implement links, so probably you could implement cross-engine links in the same way using inter-process shared memory.\nIndeed, OS synchronization mechanisms are expensive, so most of the high performance packet I/O solution explicitely avoid them; I just wanted a confirmation that this was the case also for Snabb.\nThanks.. Thanks a lot for the quick reply!\nI think this makes the job (first question):\n````\nmodule(..., package.seeall)\nlocal vhostuser = require(\"apps.vhost.vhost_user\")\nfunction run (parameters)\n   if not (#parameters == 2) then\n     print(\"Usage: vm2vm  \")\n      main.exit(1)\n   end\n   local usock1 = parameters[1]\n   local usock2 = parameters[2]\nlocal c = config.new()\n   config.app(c, \"vh1\", vhostuser.VhostUser, {socket_path=usock1,is_server=false})\n   config.app(c, \"vh2\", vhostuser.VhostUser, {socket_path=usock2,is_server=false})\nconfig.link(c, \"vh1.tx -> vh2.rx\")\n   config.link(c, \"vh2.tx -> vh1.rx\")\nengine.configure(c)\n   engine.main({report = {showlinks=true, showapps=true}})\nend\n````\nI was not able to make it work because QEMU segfaults when using vhost-user, after the vhost-user session has been established with a snabb vhostuser App (I see Snabb logging the vhost-user connection setup). This is the QEMU command that I use (I have setup the hugepages):\n````\nqemu-system-x86_64 /path/to/image.qcow2 -m 512M -object memory-backend-file,id=mem,size=512M,mem-path=/dev/hugepages,share=on -numa node,memdev=mem -chardev socket,id=chr0,server,path=/var/run/vhu1.sock -netdev type=vhost-user,id=net0,chardev=chr0 -device virtio-net-pci,netdev=net0\nAnd I run Snabb with\nsrc/snabb vm2vm /var/run/vhu1.sock /var/run/vhu2.sock\n````\nwhere \"vm2vm\" is my script above.\nDo you think I'm getting something wrong?\nDo you happen to have some tutorials on how to setup QEMU + vhost-user + Snabb?\nThanks!. Ah ok, I will check, thanks!\nBtw, I was using the upstream QEMU, because vhost-user is now a standard feature that is also used by OpenVSwitch.\n. Ok! In any case I'll try to follow the instructions. Thanks!\n2017-02-22 11:47 GMT+01:00 Luke Gorrie notifications@github.com:\n\nUpstream should be fine, except that if you restart Snabb you may also\nneed to restart the VM.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/snabbco/snabb/issues/1103#issuecomment-281634200, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AEsSwTlwN6XaF-PaUPsw_7yWT4PTmHVGks5rfBIngaJpZM4MGSET\n.\n\n\n-- \nVincenzo Maffione\n. Hi,\n  I've managed at least to run the two VMs with vhost-user + snabb. I see some initialization going on snabb output (vm2vm is the simple snabb script above)\nsudo src/snabb vm2vm /var/run/vm10-10.socket /var/run/vm11-11.socket\nGet features 0x18428001\n VIRTIO_F_ANY_LAYOUT VIRTIO_NET_F_MQ VIRTIO_NET_F_CTRL_VQ VIRTIO_NET_F_MRG_RXBUF VIRTIO_RING_F_INDIRECT_DESC VIRTIO_NET_F_CSUM\nGet features 0x18428001\n VIRTIO_F_ANY_LAYOUT VIRTIO_NET_F_MQ VIRTIO_NET_F_CTRL_VQ VIRTIO_NET_F_MRG_RXBUF VIRTIO_RING_F_INDIRECT_DESC VIRTIO_NET_F_CSUM\nvhost_user: Skipped old feature cache in /tmp/vhost_features___var__run__vm10-10.socket\nvhost_user: Caching features (0x18008001) in /tmp/vhost_features___var__run__vm10-10.socket\nSet features 0x18008001\n VIRTIO_F_ANY_LAYOUT VIRTIO_NET_F_MRG_RXBUF VIRTIO_RING_F_INDIRECT_DESC VIRTIO_NET_F_CSUM\nrxavail = 0 rxused = 0\nrxavail = 0 rxused = 0\nHowever, I don't see any traffic. That is because something is failing, and indeed qemu reports:\nqemu-system-x86_64: unable to start vhost net: 1: falling back on userspace virtio\nwhich means that is not really using vhost.\nTracking qemu code, I see the failure happens in the vhost_user_set_vring_enable function: QEMU checks for the VHOST_USER_F_PROTOCOL_FEATURES (bit 30), that apparently is not exported by Snabb VhostUser App. Any idea why?\nI'll also try with Snabb QEMU fork.. Hi,\n  Indeed, your QEMU branch works also for me. I'm using upstream qemu, that is the latest code on the qemu master branch on the git repository (git://git.qemu-project.org/qemu.git). The latest QEMU release is 2.8.0 (the one I have installed on my system as original QEMU package).. Ok sure. It should fail at feature negotiation time, so your VMs should not able to receive/send any traffic through the virtio-net interfaces.. I'm not using a QEMU release (tarball), I just use the current master branch on the official QEMU git repository (git://git.qemu-project.org/qemu.git). It may be that the difference is due to developments after 2.8.0, that are already in the master branch (note that the QEMU master branch is stable code).. QEMU just tells me\nqemu-system-x86_64: unable to start vhost net: 1: falling back on userspace virtio\nas explained above, and the cause is the feature bit.\nThis is the full QEMU command line I use:\nqemu-system-x86_64 /home/vmaffione/git/vm/netmap.qcow2 -enable-kvm -smp 2 -m 512M -vga std -nographic -snapshot -device e1000,netdev=mgmt,mac=00:AA:BB:CC:0a:99 -netdev user,id=mgmt,hostfwd=tcp::20010-:22 -numa node,memdev=mem0 -object memory-backend-file,id=mem0,size=512M,mem-path=/dev/hugepages,share=on -device virtio-net-pci,netdev=data10,mac=00:AA:BB:CC:0a:0a,ioeventfd=on,mrg_rxbuf=on -chardev socket,id=char10,path=/var/run/vm10-10.socket,server -netdev type=vhost-user,id=data10,chardev=char10. I understand your doubts, but I'm sure :)\nI pointed you at the exact \"if branch\" that causes the failure, and it is the first check in the function vhost_user_set_vring_enable, line 387 of  hw/virtio/vhost-user.c. So it is vhost user code.\nI just added a printf() statement there to be sure that check is failing.\nThis function (and the associated check) is not present on your QEMU fork, that's why.\n. The missing bit VHOST_USER_F_PROTOCOL_FEATURES is in QEMU since 2015, whereas I see that your QEMU fork is aligned with 2014. This explains why your QEMU fork works and the current one doesn't.. I can do that if you point me at the release tarball.. With 2.7.1 I don't even see the virtio-net interface for yet another problems (unrelated)\n[    1.214409] virtio_net virtio0: virtio: device uses modern interface but does not have VIRTIO_F_VERSION_1\nYou could try to run snabb and qemu manually to check for the error I'm getting.. ok!. Agree, there is a failure in the gpa --> hva translation. In this specific case the gpa is passed on the transmit vring: the trace says so, and my guest application is really accessing the tx ving (see pkt-gen cmdline).. On guest side I don't even see GPAs, standard virtio functions do the GVA-->GPA translation, see https://github.com/luigirizzo/netmap/blob/master/LINUX/virtio_netmap.h#L420-L427\n. I don't know NixOS, but netmap is just an out-of-tree kernel module with some example applications, that you build in the usual way\n$ ./configure\n$ make\n$ make install\nIn any case, the very same setup works when user-space virtio or vhost-net (in kernel accelerator) are used as network backend. So I deduce it must be some translation case that is not managed by snabb. Maybe printing the snabb translation map will reveal the issue.. Yes. I just modified the netmap \"bridge\" example to be able to do that:\n````\nbridge -L -i netmap:ethX\n````\nwill do the trick (once I publish the update).. Code is here https://github.com/luigirizzo/netmap.git , master branch.\nYou can build and install it with\n$ ./configure --drivers=virtio_net.c\n$ make\n$ sudo make install\nBut you need to make sure the configure system is able to find kernel sources, because the virtio_net driver (a kernel module) needs to be patched. You can use the --kernel-sources ./configure argument if necessary.\nAlso, you need to make sure the modified virtio_net module is loaded rather than the original one.. ok, thanks! I dont understand whether do you want to automate this or not (and why), but maybe it would be just enough to catch the bug :). Sure, it sounds good. Regression tests are a powerful tool.\nBut I would like to point out that netmap is accessing the vring using the same API used by the Linux regular virtio-net driver, without renegotiating anything. So at least in principle there should not be any difference, that's why the failure is weird. In any case I will double check our virtio-net netmap code to make sure it is not circumventing the negotiated features.. Linux 4.10.6.. Just checking... does Snabb support the virtio-net header? If yes, does it support the 10 bytes one or the 12 bytes?. Some more information: the offending GPA (0x2036a8b0 in the trace above) corresponds to a global variable which is part of the (patched) virtio_net kernel module: https://github.com/luigirizzo/netmap/blob/master/LINUX/virtio_netmap.h#L357\nAnd yes, the address is pushed on the RX vring.\nHowever, I don't see what should be wrong with a static address vs a kmalloc()ed address... Confirmed, if I use only kmalloc()ed buffers, Snabb does not crash anymore and everything works as expected.\nTo make it more clear, I was using two shared buffers to hold the virtio-net-header. Those buffers were allocated as global variables (e.g. in the .data section). This actually works for both virtio+vhost-net and virtio+vhost-user+ovs-dpdk. But it doesn't work for Snabb, so there must be a bug somewhere in Snabb.\nIf I alloc the shared buffers with kmalloc(), the problem disappears.. Ah I had iommu_intel=on.. By putting that off it works!\nThanks a lot!. ",
    "amanand": "Thanks Diego. Can you please let me know the format of the new binding file and cfg/conf files. I tried to take a look at the README.md but the lwaftr link (https://github.com/Igalia/snabb/blob/lwaftr/src/program/lwaftr) takes me to the old format.. ",
    "FongHou": "Hey @lukego, I learned a lot from your write-up like this one!\nHave you looked into http://terralang.org ? It uses LuaJIT as compile-time macro language, as well as hosting language at runtime (LuaJIT + llvm).\nCheers!\n. ",
    "jialiu02": "I'd like to maintain a branch for aarch64 before it's merged into master.\nNow you are pulling LuaJIT v2.1,  I will send a request to create aarch64 branch after that.\nI know your concern about this porting. But we need to start the work, and let people join us.\nThe differences between x86 and arm are not hard to solve. CI and testing hardware are also not an issue if more and more people are interested in snabb switch in ARM.\nSorry for late reply because I'm busy with other things.. @lukego I agree to host aarch64 branch here. And it's better to create a new branch after you finish merging luajit v2.1 because we need some new fixes in it for aarch64.\nI understand your concern and we will be careful and not break the applications running on x86.\nThanks @kbara. It's glad to see ARM player here. I'll take a look at your repos, and you can also pull your changes to the new aarch64 after it's created. Yes, memory ordering is difficult in writing drivers,  the other thing is luajit as it supports amd64 not for a long time and may still have bugs. \nAnd don't worry about the devices, I have several different kinds of arm servers which can be tested on. :). I recreated aarch64 branch from master, then applied this patch only, Is is good for you to merge now?. ",
    "suraj0208": "Thanks alot . The FFI solved my problem. It would have been great to see shared library example.. ",
    "raj2569": "Thank you, this solved my issue.. The problem was that all links to ARP app has to be connected, Updated example program is:\n```\nmodule(..., package.seeall)\n-- local test  = require(\"program.example_test.testintel\")\nlocal intel = require(\"apps.intel_mp.intel_mp\")\nlocal ipv4_apps  = require(\"apps.lwaftr.ipv4_apps\")\nlocal basic  = require(\"apps.basic.basic_apps\")\nfunction run (parameters)\n   if not (#parameters == 1) then\n      print(\"Usage: example_spray \")\n      main.exit(1)\n   end\n   local pci_id = parameters[1]\nlocal c = config.new()\n   config.app(c, \"arp\", ipv4_apps.ARP,{src_ipv4 = \"192.168.5.194\", src_eth = \"08:35:71:00:97:15\",\n                                       dst_ipv4 = \"192.168.5.94\"})\n   config.app(c, \"if\", intel.Intel, {pciaddr=pci_id, rxq = 0, txq = 0})\n   config.app(c, \"test_app\", basic.Sink)\nconfig.link(c, \"if.output -> arp.south\")\n   config.link(c, \"arp.south -> if.input\")\n   config.link(c, \"arp.north -> test_app.input\")\n   config.link(c, \"test_app.output -> arp.north\")\nengine.configure(c)\n   -- engine.main({duration=10, report = {showlinks=true}})\n   engine.main({report = {showlinks=true}})\nend\n. Ah, ok thanks, this gives me some thing to start looking! . I added an assert and a check for length before calling new_from_mem to figure out what was happening.\n   while not link.empty(input) do\n      local p = link.receive(input)\n      local data, length = p.data, p.length\n      if (length < 14) then\n         print (\"Dwarf packet length is \" .. length)\n      end\n      assert(length >= 14)\n```\nThe output I am getting is:\nDwarf packet length is 0. Update: \nI have tried using apps.intel.intel_app but I am still getting this error. So this may not be a but in intel_mp.  \nI have added a condition to ignore packets with zero bytes. So far there seems to be no other issues. \nAnother observation is that, say my app network looks like the following dig:\nNIC - > [a] -> [b] -> [c] ->NIC\nMost of the time the zero packets are being received in a, but I very rarely  am getting this error in  b and c also. As I have mentioned, I am ignoring zero length packets in a, but still getting errors in b or c.\nThis error happens randomly, or at least so far I am not able to isolate any pattern, which is one of the reason why making a small test case is difficult.\n. Some more updates:\nI printed the packet count from the NIC till my app to check if any packets are getting lost on the way.\nPrinted the following stats from the shm directory:\npci/0000\\:04\\:00.0/q0_rxpackets.counter\nlinks/south_if.output\\ -\\>\\ sort.input/rxpackets.counter\nlinks/south_if.output\\ -\\>\\ sort.input/txpackets.counter\napps/sort/rxpackets.counter\napps/sort/zeropackets.counter\nIn sort app, rxpackets.counter is updated immediately after link.receive(), if length > 0, otherwise zeropackets.counter is incremented.\nValues of these counters after running for some time is:\npci_rxpkt               10284438\nlink_rxpkt              10284461\nlink_txpkt              10284461\nsort_rxpkt              10284051\nzero_pkt                     410\ndifference        410\ndifference is the difference between  link_txpkt and sort_rxpkt. \nAs it can be seen, the difference matches exactly with zero_pkt. So all packets received by links are being transmitted to app, but some of them turns out to be of zero length. . oops, left that out  :) . Updated the branch with select() replaced with read() in socket/unix.lua. While running I am getting an error:\nsyscall/helpers.lua:66: attempt to call method 'getfd' (a nil value)\nstack traceback:\n        core/main.lua:138: in function 'getfd'\n        syscall/helpers.lua:66: in function 'getfd'\n        syscall/syscalls.lua:90: in function 'read'\n        apps/socket/unix.lua:101: in function 'can_receive'\n        apps/socket/unix.lua:145: in function 'method'\n        core/app.lua:127: in function 'with_restart'\n        core/app.lua:429: in function 'thunk'\n        core/histogram.lua:98: in function 'breathe'\n        core/app.lua:376: in function 'main'\n        program/snabbmark/snabbmark.lua:89: in function 'socket_test'\n        program/snabbmark/snabbmark.lua:20: in function 'run'\n        core/main.lua:56: in function <core/main.lua:43>\n        [C]: in function 'xpcall'\n        core/main.lua:179: in main chunk\n        [C]: at 0x0044f270\n        [C]: in function 'pcall'\n        core/startup.lua:3: in main chunk\n        [C]: in function 'require'\n        [string \"require \"core.startup\"\"]:1: in main chunk\nAlso noticed that the socket/unix.lua file has a structure different from that of other apps. Only top level function is UnixSocket:new(arg) with every other function defined inside it, I don't know if that will make a difference though.. ```\n$ sudo ./snabb snsh -jdump=+rT,dump1.txt -p snabbmark socket_test 100e6 8\nProcessed 100.1 million packets in 5.45 seconds (rate: 18.4 Mpps).\n$ sudo ./snabb snsh -jdump=+rT,dump1.txt -p snabbmark socket_test 100e6 16\nProcessed 100.1 million packets in 7.28 seconds (rate: 13.7 Mpps).\n```\nMuch better results!. Closing as the issue has been fixed. . After running about 36 hours, the graphs trends are the same. Throughput decreases steadily. Not sure what could be wrong here. . That sounds good. I will run this tonight and update tomorrow. Eventually, I think, some sort of time series db like rrd would be a nice addition to tackle the change in parameters across time. \nTo generate the graphs above, I had written a shell script to log the counters I am interested in to a text file along with time stamp and then used plotly to generate the graph. . IMHO all these should be outside snabb, separate daemon can query the /var/run/snabb/$pid and do stuff like populate in RRD(and other time series db like influxdb) , provide SNMP etc...\nbtw, what does latency histogram and VMProfile data\nI have interest in exposing the snabb data via SNMP, Hopefully I will get time to take a stab at it.. I have captured 15 hours worth of shm in tar.xz files using the script. . Put the vmprofile data in a spread sheet. I have selected only the interesting data here, and removed all which do not show much variation or big enough value. \n\nSome observations:\n\nEngine usage increases slightly over the period, while others decrease\nengine.gc shows a big increase, while engine.gc.94 decreases\nInterpreted code shows big increases over the period, along with c and gc. head and foreign reduces.\nEngine interpreted code, c and gc also shows increase.\n\nPlease let me know if any data I choose not to put here is important, I will update the sheet with it.\n. @lukego Thanks for the fine docs, I will follow the docs and see if I can identify it. \nOne thing that I find strange is that the percentage of the interpreted code increases with time. What could be the reason for it?. Thanks for the clarification. \nBut, how do I configure my app graph if pushtest app is like packetgen? i.e. It has only a single output link, on to which the generated packets needs to be pushed?. After checking the source of Source it became clear that I should be using pushtest:pull to generate packets. I misunderstood that the push is used to send out packets from the links, while pull is used to pull in the packets from links. . @dpino Thank you for your insight, just one more question to complete this discussion. What are the general use cases for push? I am still not very clear about the difference between pulling the packets into the links and pushing the packets into the app. Slightly different error here:\n```lua\nmodule(..., package.seeall)\nlocal lib      = require(\"core.lib\")\nlocal logger   = lib.logger_new({ rate = 32, module = 'lpm'});\nlocal lpm4     = require(\"lib.lpm.lpm4_dxr\").LPM4_dxr\nRoute = {}\nfunction Route:new (conf)\n   local o = {lpm_hash  = lpm4:new()}\nlogger:log(\"Adding entries to lpm\")\n   o.lpm_hash:add_string(\"16.0.0.0/24\", 1)\n   o.lpm_hash:add_string(\"16.0.0.1/24\", 2)\n   o.lpm_hash:add_string(\"48.0.0.1/24\", 3)\nlogger:log(\"Building lpm\")\n   o.lpm_hash:build()\nlogger:log(\"Added entries to lpm\")\n   return setmetatable(o, {__index = self})\nend\nfunction Route:push()\nend\n``\nIts stuck after printing \"Added entries to lpm\". I have worked on this some more and have some additional observations. To prevent thefailed to allocate mcode memory` I have increased the luajit tuneables in commit a268212.\nI am getting around 6mpps when running this and can see from perf top that most of the time CPU is running on compiled trace. When running under snsh using the command sudo ./snabb snsh -jdump=+rsxaA,dump01.txt -jtprof -p jit_loop the performance drops to 2mpps. \ndump01.txt is attached.\ndump01.txt.gz\nThe perf top shows the following when running under snsh:\n12.65%  perf-25499.map      [.] 0x000000002a4d3a60                                                                                             \n  10.31%  perf-25499.map      [.] 0x000000002a4d5e06                                                                                             \n   7.18%  perf-25499.map      [.] 0x000000002a4d5e0a                                                                                             \n   3.85%  perf-25499.map      [.] 0x000000002a4d3a64                                                                                             \n   1.88%  perf-25499.map      [.] 0x000000002a4e237c                                                                                             \n   1.82%  snabb               [.] lj_tab_len                                                                                                     \n   1.82%  perf-25499.map      [.] 0x000000002a4d3a6a                                                                                             \n   1.76%  perf-25499.map      [.] 0x000000002a4c9c7f                                                                                             \n   1.67%  perf-25499.map      [.] 0x000000002a4cc582                                                                                             \n   1.66%  perf-25499.map      [.] 0x000000002a4d5e10                                                                                             \n   1.41%  perf-25499.map      [.] 0x000000002a4d39da                                                                                             \n   1.21%  perf-25499.map      [.] 0x000000002a4d1e36                                                                                             \n   1.11%  perf-25499.map      [.] 0x000000002a4e2d1d                                                                                             \n   1.03%  perf-25499.map      [.] 0x000000002a4d5300                                                                                             \n   0.75%  [vdso]              [.] __vdso_clock_gettime                                                                                           \n   0.69%  perf-25499.map      [.] 0x000000002a4d5e31                                                                                             \n   0.66%  perf-25499.map      [.] 0x000000002a4d5d80                                                                                             \n   0.57%  perf-25499.map      [.] 0x000000002a4d3a8b                                                                                             \n   0.55%  perf-25499.map      [.] 0x000000002a4d20d6                                                                                             \n   0.51%  [kernel]            [k] native_write_msr_safe                                                                                          \n   0.50%  perf-25499.map      [.] 0x000000002a4d224d                                                                                             \n   0.48%  perf-25499.map      [.] 0x000000002a4c65b5                                                                                             \n   0.47%  perf-25499.map      [.] 0x000000002a4d433c                                                                                             \n   0.47%  snabb               [.] release_unused_segments                                                                                        \n   0.46%  snabb               [.] lj_tab_getinth                                                                                                 \n   0.46%  perf-25499.map      [.] 0x000000002a4d1e6e                                                                                             \n   0.45%  perf-25499.map      [.] 0x000000002a4d1e73                                                                                             \n   0.40%  snabb               [.] lj_alloc_malloc                                                                                                \n   0.40%  perf-25499.map      [.] 0x000000002a4f09a9                                                                                             \n   0.39%  snabb               [.] lj_alloc_free                                                                                                  \n   0.39%  [kernel]            [k] __switch_to                                                                                                    \n   0.38%  [kernel]            [k] apic_timer_interrupt                                                                                           \n   0.37%  perf-25499.map      [.] 0x000000002a4f112c                                                                                             \n   0.35%  perf-25499.map      [.] 0x000000002a4d2029                                                                                             \n   0.34%  [kernel]            [k] __schedule                                                                                                     \n   0.34%  perf-25499.map      [.] 0x000000002a4f0dd2                                                                                             \n   0.34%  perf-25499.map      [.] 0x000000002a4e3913                                                                                             \n   0.34%  perf-25499.map      [.] 0x000000002a4d20b6                                                                                             \n   0.34%  perf-25499.map      [.] 0x000000002a4ea2a7                                                                                             \n   0.33%  perf-25499.map      [.] 0x000000002a4f0f29                                                                                             \n   0.32%  perf-25499.map      [.] 0x000000002a4f0ba6                                                                                             \n   0.30%  snabb               [.] get_monotonic_time                                                                                             \n   0.28%  perf-25499.map      [.] 0x000000002a4d1e6a                                                                                             \n   0.28%  [kernel]            [k] enqueue_task_fair                                                                                              \n   0.27%  perf-25499.map      [.] 0x000000002a4bf504                                                                                             \n   0.27%  libc-2.19.so        [.] __clock_gettime                                                                                                \n   0.24%  [kernel]            [k] menu_select. After adding one more app at commit b7b14cc the performance dropped drastically to around 600kpps. Most of the time snabb spent on interpreted code. I am wondering what could have caused all the traces to be aborted and run in interpreted mode.\nDump file is attached,\ndump02.txt.gz\nThe perf top shows:\n7.77%  libc-2.19.so        [.] vfprintf                                                                                                       \n   5.57%  snabb               [.] lj_str_new                                                                                                     \n   5.46%  snabb               [.] match                                                                                                          \n   3.28%  snabb               [.] lj_strscan_scan                                                                                                \n   2.65%  snabb               [.] release_unused_segments                                                                                        \n   2.61%  snabb               [.] lj_alloc_free                                                                                                  \n   2.40%  perf-25587.map      [.] 0x000000002a4d55db                                                                                             \n   2.21%  libc-2.19.so        [.] _IO_default_xsputn                                                                                             \n   2.06%  perf-25587.map      [.] 0x000000002a4d4ea5                                                                                             \n   2.04%  perf-25587.map      [.] 0x000000002a4d4ea9                                                                                             \n   1.36%  snabb               [.] match_class                                                                                                    \n   1.34%  snabb               [.] max_expand                                                                                                     \n   1.30%  snabb               [.] gc_sweep                                                                                                       \n   1.23%  perf-25587.map      [.] 0x000000002a4d55df                                                                                             \n   1.15%  snabb               [.] str_find_aux                                                                                                   \n   1.15%  libc-2.19.so        [.] _IO_vsprintf                                                                                                   \n   0.97%  libc-2.19.so        [.] strchrnul                                                                                                      \n   0.92%  snabb               [.] lj_alloc_malloc                                                                                                \n   0.73%  perf-25587.map      [.] 0x000000002a4c4f29                                                                                             \n   0.70%  snabb               [.] lj_cdata_free                                                                                                  \n   0.69%  perf-25587.map      [.] 0x000000002a4c5858                                                                                             \n   0.66%  perf-25587.map      [.] 0x000000002a4d5555                                                                                             \n   0.64%  perf-25587.map      [.] 0x000000002a4e5d65                                                                                             \n   0.59%  snabb               [.] lj_tab_len                                                                                                     \n   0.54%  snabb               [.] lj_lib_checkstr                                                                                                \n   0.53%  snabb               [.] lj_cdata_newv                                                                                                  \n   0.50%  snabb               [.] lua_pushlstring                                                                                                \n   0.47%  snabb               [.] singlematch                                                                                                    \n   0.45%  libc-2.19.so        [.] 0x00000000000460a0                                                                                             \n   0.42%  snabb               [.] lj_cont_stitch                                                                                                 \n   0.40%  perf-25587.map      [.] 0x000000002a4e67fc                                                                                             \n   0.40%  perf-25587.map      [.] 0x000000002a4d4e1f                                                                                             \n   0.38%  snabb               [.] classend.isra.9                                                                                                \n   0.37%  perf-25587.map      [.] 0x000000002a4d55e5                                                                                             \n   0.37%  perf-25587.map      [.] 0x000000002a4cd9e8                                                                                             \n   0.36%  snabb               [.] lj_strscan_num                                                                                                 \n   0.33%  snabb               [.] lj_mem_newgco                                                                                                  \n   0.32%  snabb               [.] push_onecapture                                                                                                \n   0.29%  [kernel]            [k] native_write_msr_safe                                                                                          \n   0.29%  [vdso]              [.] __vdso_clock_gettime                                                                                           \n   0.29%  snabb               [.] lj_BC_FUNCC                                                                                                    \n   0.29%  perf-25587.map      [.] 0x000000002a4ed2b4                                                                                             \n   0.29%  snabb               [.] lj_BC_JLOOP                                                                                                    \n   0.28%  perf-25587.map      [.] 0x000000002a4d444e                                                                                             \n   0.28%  [kernel]            [k] __switch_to                                                                                                    \n   0.28%  perf-25587.map      [.] 0x000000002a4d4eaf                                                                                             \n   0.27%  perf-25587.map      [.] 0x000000002a4ecf87. Just wondering which aborted trace could be behind the high use of interpreted code? How can that be identified? It may be very well be possible for many trace aborts to be normal and not have a bearing on performance, but some could be. feeling kind of lost here :(. I am seeing quite a lot of trace aborts related to loop, not just in\nintel_mp, but in other modules also. The code in PR is just a small\nsample we can use for diagnosis.\nOn Thu, Nov 2, 2017 at 5:51 PM, Luke Gorrie notifications@github.com\nwrote:\n\nThank you for using a Pull Request to raise this issue. It is very helpful\nto have the exact code you are working with connected with the bug report.\nThat way the source code line numbers in the trace can be looked up in the\nright code version. \ud83d\udc4d\nThe JIT log does not explicitly say what is being blacklisted, but it does\ninclude a lot of aborts for the same region of code, and that seems likely\nto be the problem (or in any event something worth understanding.)\n130930:---- TRACE 112 start 95/18 intel_mp.lua:595\n 134405:---- TRACE 112 abort intel_mp.lua:596 -- loop unroll limit reached\n 134408:---- TRACE 112 start 95/18 intel_mp.lua:595\n 137883:---- TRACE 112 abort intel_mp.lua:596 -- loop unroll limit reached\n 137886:---- TRACE 112 start 95/18 intel_mp.lua:595\n 141361:---- TRACE 112 abort intel_mp.lua:596 -- loop unroll limit reached\n 141364:---- TRACE 112 start 95/18 intel_mp.lua:595\n 144839:---- TRACE 112 abort intel_mp.lua:596 -- loop unroll limit reached\nMy working hypothesis is that this code is being blacklisted and this is\ncausing some important code path to run interpreted. Could be that\nraptorjit/raptorjit#102\nhttps://github.com/raptorjit/raptorjit/issues/102 is the solution i.e.\nmake the JIT give up on the unrolling optimization after a while instead of\ngiving up on the whole trace.\nWe have seen other instances of blacklistings for code starting in\nintel_mp lately right? Does anybody already have some insight into these?\n(I have \"swapped out\" the traces that I was looking at before vacation but\nmaybe somebody can refresh my memory? cc @alexandergall\nhttps://github.com/alexandergall @wingo https://github.com/wingo\n@eugeneia https://github.com/eugeneia)\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/snabbco/snabb/pull/1239#issuecomment-341404408, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAB3whF9LCWyu7R_SCdnwejFx6_xrBPVks5sybOxgaJpZM4PsWIb\n.\n. Merged lukego/jit-tracebarrier and ran the tests again.\nDumpfile is attached.  dump-with-1242.txt.gz\n. \n",
    "benbarbour": "Wait, sorry. That's luajit that's not building - might not be a snabb issue.. Aaaand this project only runs on x86_64. I did not RTFM. :(. ",
    "ichung": "I think that the extra output from STP isn't very useful without telling STP about all the functions and tables.  While I think that would be valuable, it would be a larger undertaking than the scope of this PR, so let's keep this one as-is.  (Though it is very easy to switch out debug.traceback for STP.stacktrace in core/main.lua).\nEdit: Actually I take that back.  Looking at the output from STP, even without annotations, actually just solved the bug I was facing, and the bare traceback wasn't enough.  I will add a commit to use STP.  I think it will still be valuable to register functions + tables with STP, since it improves the readability of the output.. The disadvantage is that the output is quite verbose; compare the different versions above.  Maybe these should be controlled by a \"verbose\" option, like -v, -vv, etc?. Sounds good.  I'm fine with you all merging this if you are :). @alexandergall These remaining commits assert pretty much every use of new_from_mem in snabb.  Most of the changes in 22fa8c8 in particular are kinda \"unnecessary\" since an error would be caused immediately by calling a function on the returned object, but I included that commit for completeness.  I believe the state after 20a1f19 contains all of the \"necessary\" assertions and checks.. ",
    "MatthiasKauer": "Thank you for the explanations.\nWhen you say \"Want to access protocol fields in a structured way? See lib.protocol library.\" what do you mean by \"protocol fields\". I want to access fields in the payload in a structured way and would like to create my own protocol for that, I suppose. Is the protocol lib still a good starting point?\nFinally, you say that the network app doesn't know when it's finished: I had no issues with the app finishing; the only source is the PCAP reader and that is just done at some point, no?\n. ",
    "benagricola": "Having the same issue on a CentOS 7 server with the elrepo 4.12.10 kernel, using the numa.bind_to_cpu() function. \nAs @dpino mentioned, setting mask.size to 1024 by default avoids this, but isn't portable in any fashion.\nI assume that the correct fix is to read the value of MAX_NUMNODES at runtime and initialise mask.size based on that. There doesn't seem to be an easy, direct way to do this (but my knowledge here is massively lacking).\nnumactl mimics reading MAX_NUMNODES by reading /proc/self/status and parsing the Mems_allowed: field :) - see here\nEach 9 characters in the field represents 32 bits of mask, so the value of MAX_NUMNODES is ((strlen($mems_allowed)+1)/9)*32\nI'd hope there's a better way of getting the value of MAX_NUMNODES but maybe there isn't.... @petebristow Added a test for nonexistent entries returning nil, and also a nonexact prefix test.\nDoes this suffice?. @petebristow so to confirm, the correct way to use the LPM library is to always add a default route entry. If there's no actual default route, storing a 'NOT FOUND' value of some sort is required?. Yep that sounds good. This is only a problem if you don't know that LPM requires a default / not found so implicitly adding one makes sense. I suppose you could still hit this issue if you deleted the implicitly added not found but as long as it's documented i think that makes sense.. Wow, missed your comment on this completely @petebristow.\nFWIW pretty much all of the Snabb stuff we use is using one of these cards (X520 SR2) - I've seen no issues with the intel_mp driver on this for the last 6 or so months and it appears the ID is changed because it's an SR card which is supplied with SFP's pre-installed.. Hi @paolss,\nCoincidentally I'm working on almost exactly that at the moment, based on how the VPP Router plugin was designed (from fd.io - https://wiki.fd.io/view/VPP_Sandbox/router#Design)\nIt's very far from complete and it's more of a proof of concept than anything else, but shows that the integration between netlink and snabb is feasible.\nedit: Current code is in https://github.com/benagricola/snabb/tree/snabbrouter/src , the relevant apps are the route app, and netlink app under socket. It's started and configured using the dtcbridge program.. @wingo re: should we ever config test a compiled file - no, that's a good point \ud83d\ude03- I'll rework this as config compile.. Yep, pending a rewrite I'm hoping to get around to after more $work related things :D. @wingo This is now rebased against wingo-next and I've changed the base branch to that as well - should be gtg. Ahh yep - my original patch was against the lwaftr branch which has mem.open_input_string at the start of the line which I just deleted for this one. Commit updated.. ",
    "Posnet": "I am also interested in this. @morphyno have you done any work so far in adding support to the intel_mp driver that I might be able to look at? . @virtuallynathan is there an official datasheet, or is the source code for the driver the only reference available? Also is there anywhere that specifies the amazon specific difference of the  Intel 82599 VF Interface provided by enhanced networking supported instances?. ",
    "leolovenet": "I figured it out,  when i readed the code https://github.com/torvalds/linux/blob/master/tools/testing/selftests/vm/hugepage-mmap.c, Found no MAP_HUGETLB flag required, so I remove the hugetlb flage too, and It work. \n :)\nmemory.lua\uff0cShould be like the following:\n149: local tmpptr = syscall.mmap(nil, size, \"read, write\", \"shared\", fd, 0)\n154: local ptr = syscall.mmap(virt, size, \"read, write\", \"shared, fixed\", fd, 0)\nBTW, I think this is a bug.\n. @dpino Sorry, this log misled you (I didn't notice it) because I did a lot of tests, so the number of vm.nr_hugepages started from 503.\nafter, I cleaned up everything and rebooted my server, re-tested again. Here is the \nnew log.\nI know that the value of vm.nr_hugepages is growing, that is because this line \nlib.writefile(\"/proc/sys/vm/nr_hugepages\", tostring(want))\n was successfully executed. The reason why it was repeated execution, is because here, not return OK.\nI have also tested this under CentOS7, Also like this. \nI know the shmax limit\uff0cbut I'm sure it has nothing to do with it.\nHere is the output on my system\n```\n\ncat /proc/sys/kernel/shmmax\n68719476736\n```\n\naccording to https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt \"Using Huge Pages\" section,\n\nAny files created on /mnt/huge uses huge pages. \n\nand\n\nAlso, it is important to note that no such mount command is required if\napplications are going to use only shmat/shmget system calls or mmap with\nMAP_HUGETLB. \n\nWe have used  ensure_hugetlbfs function ensure that mounted the hugetlbfs filesystem (on  /var/run/snabb/hugetlbfs),  so the file created in function allocate_huge_page by \nlocal tmpfile = \"/var/run/snabb/hugetlbfs/alloc.\"..syscall.getpid()\nlocal fd = syscall.open(tmpfile, \"creat, rdwr\", \"RWXU\")\nshould used huge pages.\nSo I deleted the hugetlb flag in the memory.lua file, and It work.\n149: local tmpptr = syscall.mmap(nil, size, \"read, write\", \"shared\", fd, 0)\n154: local ptr = syscall.mmap(virt, size, \"read, write\", \"shared, fixed\", fd, 0)\nWhen my program was running, I found that HugePages_Free was reduced.\nThis may be the operating system, or what other reasons, I do not know.\nHope this information can help to other people.\n. ",
    "kullanici0606": "Any updates in this issue? I also tried the example_replay program from the \"Snabb Getting Started Guide\" on CentOS 7 i.e \nsudo src/snabb example_replay src/program/example_replay/input.pcap veth0\nand also the minimal test case and they both failed with the same error.\nWhen I remove hugetlb, as suggested, from mmap, the minimal test case works.\n. If I remove the hugetlb parameter in the following lines:\nhttps://github.com/snabbco/snabb/blob/732eae91ff1d9e18570804b42e2ff90534361f9b/src/core/memory.lua#L149-L154\nand run make -j again, can I assume that Snabb will still use huge pages?. If it is enough to remove parameters, then I made the changes and created a PR : https://github.com/snabbco/snabb/pull/1373\nCould you please review it or give feedback what else needs to be done?. You're welcome. Thank you for this great project.. ",
    "EaseTheWorld": "I met same error when running example_replay.\nRemoving the hugetlb parameter does work.\nI checked HugePages_Free increased(app start) and decreased to 0(app stop)\nTested on RHEL 6.9 and 7.4. ",
    "krawthekrow": "Oops, that's correct. Fixed.. ",
    "corsix": "I will be around from the afternoon of the 2nd through until the afternoon of the 6th, staying at JAM.. FWIW, the latest upstream DynASM should have BMI2 in it (c.f. https://github.com/LuaJIT/LuaJIT/blob/26f1023819efb843e10014232cd88bb1d52ea4f5/dynasm/dasm_x86.lua#L1690, present there since February 2018). Not having Haswell everywhere is a valid reason not to use it though (I would however be interested in knowing what performance improvement - if any - it could give here, if that experiment is something you're willing to try).. Very nice; I like seeing concrete performance numbers.. Aha, the w got lost as the PR became the upstream commit. That is unfortunate.. shrd from a pair of uint64_t registers comes to mind. Not a cheap\ninstruction (e.g. four uops on Broadwell), but should do what you need.\nOn Fri, 8 Feb 2019 at 08:40, Max Rottenkolber notifications@github.com\nwrote:\n\n@corsix https://github.com/corsix On another note, I am currently\nupdating this code to work with keys up to 128 bits (IPv6) and am wondering\nif you know any neat tricks for extracting 6 bits from a uint8_t[16] at\nan arbitrary offset in x86 asm? (I suppose what is currently an uint64_t\nargument will have to become a pointer.)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/snabbco/snabb/pull/1397#issuecomment-461729686, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAGhlWhgvHiBE7hW2JajDkcsGFQxDvJks5vLTf7gaJpZM4ZTsDc\n.\n. mov ecx, offset   -- Prefer 32-bit operations where possible\nmov rax, [key]\nmov rbx, [key+8]\ntest cl, 64       -- Prefer test to cmp (archaic habbit)\ncmovnz rax, rbx   -- This is your mov within the branch\n                  -- Do not need to sub rcx, 64\n                  -- as shrd takes cl mod 64, and subtracting 64 mod 64 is a no-op\nshrd rax, rbx, cl\nand eax, 0x3F     -- Prefer 32-bit operations where possible\n(ecx is not mutated, so bonus points if offset is already in ecx. Additional bonus points if the loads from [key+?] are cached in registers. ). This mov/and/cmp/je could be test index, index / jns?. Unless you're explicitly preserving flags (which I assume you're not), xor R, R is typically better than mov R, 0 (e.g. because it is a byte or two shorter to encode).. If you can assume BMI2 (i.e. Haswell or later IIRC), this mov/mov/shr can probably be expressed as a single shrx.. This cmp is entirely redundant - the immediately preceding and will already have set ZF=1 if the result was zero.. If you can assume BMI2, then this construction of rax and subsequent and can be replaced by something like:\nlea rcx, [v+1]\nbzhi rax, vec, rcx. This sub can be fused into the subsequent memory operand (i.e. word [leaves+index*2-2]).. This would probably be better as movzx eax, .... The assumption in the comment is true, but the CPU is not clever enough to know this, and so if you do a mov into ax, then when eax is next used, it'll insert a micro-up to combine ax and the previous high 16-bits of eax (whereas movzx here is the same price as mov, and doesn't have the extra micro-op penalty down the line).. Alternatively, the entire sequence mov/mov/shl/and/cmp/je might be better as bt vec, v/jnc.. There might be value in transposing this add and this sub - the add cannot execute until both the mov and the popcnt are done, and the sub cannot execute until the add is done - whereas if transposed, the sub could execute in parallel with the popcnt.. You could consider rotating the loop to get rid of this unconditional jump on every iteration, i.e. turning:\nhead:\nbody part 1\njump to tail if condition\nbody part 2\njmp head\ntail:\ninto:\njmp middle\nhead:\nbody part 2\nmiddle:\nbody part 1\njump to head if not condition\nor:\nbody part 1\njump to tail if condition\nhead:\nbody part 2\nbody part 1\njump to head if not condition\ntail:. Or perhaps mov eax, index / btr eax, 31 / jnc, with the bonus of then eliminating the subsequent mov/and (this is marginal, as btr cannot fuse with jnc, whereas test and cmp can both fuse with a conditional jump).. I'd probably write this mov/and as mov v, key / and v, direct_mask, as that can allow for a more compact encoding (mov r32, imm32 always encodes the immediate as 32 bits, whereas and r32, imm32 can encode the immediate as 8 bits if it is small enough).. The other rationale here that I forgot last night: on contemporary architectures, mov reg, reg can be zero latency and not require any execution unit, whereas mov reg, imm does have latency and does require an execution unit (op reg, reg and op reg, imm are same cost for most/all ops other than mov).. The mov and the add here could be fused, as in:\n| add eax, dword [node+16] -- nodes[index].base0\n   | movzx eax, word [leaves+eax*2-2] -- leaves[index]\n(No improvement in unfused-domain uops, but does save one fused-domain uop per function invocation, if that is even measurable against noise). \n",
    "Reperator": "Looks like the test fails because of a copy-paste-error which gets fixed in #1280.. If something really blows up I'd say :stop() may not be necessary; at least as a user I wouldn't expect my apps to be shut down properly if something crashes hard.\nOTOH having something like a packet capture or a logging app write stuff to disk in case of a crash seems useful for debugging. I'm not sure how I'd go about implementing that though.. Counter-argument: Apps that expose a :stop() method need to be stopped to release resources, finish writing files, etc. As a user I'd expect my apps to be stopped automatically once Snabb terminates.\nOff the top of my head I can't think of a case where it'd hurt to stop apps (even if a script does not care about stopping) while I came across the problem when writing a custom pcap-App. It took me a while to find out why my app wasn't stopped upon termination. That may be on me for not knowing Snabb's internals but I'd say stopping apps would be more user-friendly.\nWe could make the stopping optional by implementing a -nostop or -dostop command line flag.\n(I'm obvioulsy in favor of opt-out.). ",
    "sebastianczech": "Thank you for the response. \nI have also tried to start lwAFTR after changing in VMware network interfaces to:\n```\nlspci | grep -i ethernet\n02:01.0 Ethernet controller: Intel Corporation 82545EM Gigabit Ethernet Controller (Copper) (rev 01)\n02:02.0 Ethernet controller: Intel Corporation 82545EM Gigabit Ethernet Controller (Copper) (rev 01)\n```\nbut I doesn't help. In this case it is e1000 driver:\n```\nlshw -class network | grep -i driver\n   configuration: autonegotiation=on broadcast=yes driver=e1000 driverversion=7.3.21-k8-NAPI duplex=full ip=10.0.0.85 latency=0 link=yes mingnt=255 multicast=yes port=twisted pair speed=1Gbit/s\n   configuration: autonegotiation=on broadcast=yes driver=e1000 driverversion=7.3.21-k8-NAPI duplex=full latency=0 link=no mingnt=255 multicast=yes port=twisted pair speed=1Gbit/s\n\n```. ",
    "xnhp0320": "Oh, sorry!\nThe other patches are used to make snabb run on our own servers, which seems have a \nold version of libc that does not support HUGETLB flags. \nYou can just use the two patches. \nSorry for the late response. . ",
    "aouinizied": "@dpino @lukego \nNote that ntop team starts to implement (experimental) hyperscan based algorithm. If you have hyperscan lib on your machine, nDPI compilation process will take this headers and compile with hyperscan enabled. Such new headers and functions are not handled with current ljndpi bindings. \nI will take a look at it when I have free slots. At least, we have a trace here.\nBest regards,\nZied. UP. as it was merged on September release. Do you need any further checks?\nZied. ",
    "ok1cyc": "Thanks Diego for the advice. I disabled vMDQ as you suggested but hit the other issue:\n\nroot@linux:/opt/snabb/src#  ./snabb lwaftr run -v --name lwaftr --conf ../conf/lwaftr.conf --on-a-stick 0000:01:00.0 --cpu 3\n../conf/lwaftr.conf: loading compiled configuration from ../conf/lwaftr.o\n../conf/lwaftr.conf: compiled configuration is out of date; recompiling.\n../conf/lwaftr.conf: loading source configuration\n../conf/lwaftr.conf: wrote compiled configuration ../conf/lwaftr.o\nBound main process to NUMA node: 0 (CPU 0)\nBinding data-plane PID 7524 to CPU 3.\nhugetlb mmap failed (Cannot allocate memory), falling back.\nhugetlb mmap failed (Cannot allocate memory), falling back.\napps/intel_mp/intel_mp.lua:964: attempt to index field 'PFVFSPOOF' (a nil value)\nline not found\nStack Traceback\n(1) Lua metamethod '__index' at file 'core/main.lua:168'\n        Local variables:\n         reason = string: \"apps/intel_mp/intel_mp.lua:964: attempt to index field 'PFVFSPOOF' (a nil value)\"\n         (temporary) = C function: print\n(2) Lua method 'set_transmit_MAC' at file 'apps/intel_mp/intel_mp.lua:964'\n        Local variables:\n         self = table: 0x406089c0  {shm_root:/intel-mp/01:00.0/, pciaddress:0000:01:00.0, rate_limit:0, vmdq:false (more...)}\n         poolnum = number: 1\n         (temporary) = nil\n         (temporary) = number: 0\n         (temporary) = number: 0.125\n         (temporary) = number: 279.374\n         (temporary) = RAH[01][0000540c]:80080291  {longname:Receive Address High, offset:21516, name:RAH[01], ptr:cdata: 0x7f92c7ea440c (more...)}\n         (*temporary) = string: \"attempt to index field 'PFVFSPOOF' (a nil value)\"\n(3) Lua method 'set_MAC' at file 'apps/intel_mp/intel_mp.lua:926'\n        Local variables:\n         self = table: 0x406089c0  {shm_root:/intel-mp/01:00.0/, pciaddress:0000:01:00.0, rate_limit:0, vmdq:false (more...)}\n(4) Lua method 'new' at file 'apps/intel_mp/intel_mp.lua:410'\n        Local variables:\n         self = table: 0x40725fc8  {rss_tab:function: 0x40727510, transmit:function: 0x407276f8, rss_tab_build:function: 0x40727530 (more...)}\n         conf = table: 0x4072cae0  {rate_limit:0, vmdq:false, pciaddr:0000:01:00.0, poolnum:1, ring_buffer_size:2048 (more...)}\n         self = table: 0x406089c0  {shm_root:/intel-mp/01:00.0/, pciaddress:0000:01:00.0, rate_limit:0, vmdq:false (more...)}\n         vendor = string: \"0x8086\"\n         device = string: \"0x1533\"\n         byid = table: 0x40726dd8  {driver:table: 0x40726680, registers:i210, max_q:4}\n(5) Lua function 'ops' at file 'core/app.lua:348' (best guess)\n        Local variables:\n         name = string: \"b4sideNic\"\n         class = table: 0x40725fc8  {rss_tab:function: 0x40727510, transmit:function: 0x407276f8, rss_tab_build:function: 0x40727530 (more...)}\n         arg = table: 0x4072cae0  {rate_limit:0, vmdq:false, pciaddr:0000:01:00.0, poolnum:1, ring_buffer_size:2048 (more...)}\n(6) Lua field 'apply_config_actions' at file 'core/app.lua:375'\n        Local variables:\n         actions = table: 0x411c47c8  {1:table: 0x4041ca20, 2:table: 0x40424120, 3:table: 0x40d56a78, 4:table: 0x40d66b80 (more...)}\n         ops = table: 0x411c4998  {unlink_output:function: 0x411c49e0, stop_app:function: 0x411c5190, free_link:function: 0x411c4a78 (more...)}\n         remove_link_from_array = Lua function 'remove' (defined at line 295 of chunk core/app.lua)\n         (for generator) = C function: builtin#6\n         (for state) = table: 0x411c47c8  {1:table: 0x4041ca20, 2:table: 0x40424120, 3:table: 0x40d56a78, 4:table: 0x40d66b80 (more...)}\n         (for control) = number: 9\n         _ = number: 9\n         action = table: 0x40f4d970  {1:start_app, 2:table: 0x40f35858}\n         name = string: \"start_app\"\n         args = table: 0x40f35858  {1:b4sideNic, 2:table: 0x40725fc8, 3:table: 0x4072cae0}\n(7) Lua method 'commit_pending_actions' at file 'lib/ptree/worker.lua:77'\n        Local variables:\n         self = table: 0x404140e0  {no_report:false, channel:table: 0x404141e0, duration:inf, period:0.001 (more...)}\n         to_apply = table: 0x411c47c8  {1:table: 0x4041ca20, 2:table: 0x40424120, 3:table: 0x40d56a78, 4:table: 0x40d66b80 (more...)}\n         should_flush = boolean: true\n(8) Lua method 'handle_actions_from_manager' at file 'lib/ptree/worker.lua:89'\n        Local variables:\n         self = table: 0x404140e0  {no_report:false, channel:table: 0x404141e0, duration:inf, period:0.001 (more...)}\n         channel = table: 0x404141e0  {ring_buffer:cdata: 0x7f931fcd6000}\n         (for index) = number: 3\n         (for limit) = number: 4\n         (for step) = number: 1\n         i = number: 3\n         len = number: 4\n         action = table: 0x411c4748  {1:commit, 2:table: 0x411c4788}\n(9) Lua field 'main' at file 'lib/ptree/worker.lua:104'\n        Local variables:\n         self = table: 0x404140e0  {no_report:false, channel:table: 0x404141e0, duration:inf, period:0.001 (more...)}\n         stop = number: inf\n         next_time = number: 1570.39\n(10) main chunk of [string \"require('lib.scheduling').apply({[\"cpu\"]=3,[\"...\"] at line 2\n(11) Lua function 'main' at file 'core/main.lua:58' (best guess)\n        Local variables:\n         expr = string: \"require('lib.scheduling').apply({[\\\"cpu\\\"]=3,[\\\"busywait\\\"]=true,[\\\"ingress_drop_monitor\\\"]=\\\"flush\\\"})\\\nrequire('lib.ptree.worker').main()\"\n         f = Lua function '?' (defined at line 0 of chunk \"require('lib.scheduling').apply({[\"cpu\"]=3,[\"...\"])\n(12) global C function 'xpcall'\n(13) main chunk of file 'core/main.lua' at line 242\n(14)  C function 'require'\n(15) global C function 'pcall'\n(16) main chunk of file 'core/startup.lua' at line 3\n(17) global C function 'require'\n(18) main chunk of [string \"require \"core.startup\"\"] at line 1\n        nil\nError while running fiber: core/shm.lua:37: shm open error (7524/apps/fragmenterv4/out-ipv4-frag.counter):No such file or directory\n. \n",
    "mingodad": "Hello !\nThanks for reply !\nThere isn't any way to test without root ?\nIsn't it a bit scary ?\nCheers !. Hello again !\nNow snabb-ljs passes all selftests that snabb pass on my machine.\nThanks !. ",
    "mestorsi": "Thanks for the reply, I will try your recommendation. I think struct { int size; uint8_t values[0]; } should help.. ",
    "insekt": "Feb 8.\nI spent some time thinking of the NIC development. To eliminate the risks associated with ECP5 and PCIe soft IP core I suggest this plan.\n1. Buy ECP5 PCIe devkit - http://www.latticesemi.com/Products/DevelopmentBoardsAndKits/ECP5PCIExpressDevKit\nI'll buy it.\n2. Develop FPGA code according to your spec - https://github.com/lukego/easynic\nSuggested devkit doesn't have 10G link, so we emulate it in FPGA - for example, we can make a simple loop in FPGA between Tx and Rx right after MAC IP core.\nMy friend has agreed to help with it.\n3. Develop the driver according to your spec.\n@lukego agreed to help with it.\n4. Make complex performance tests\nI will setup server with devkit and provide remote access to it so we can do tests. . Feb 12.\nSmall update on EasyNIC.\n1. ECP5 devkit with PCIe x4 is on the way to me\nhttp://www.latticesemi.com/Products/DevelopmentBoardsAndKits/ECP5PCIExpressDevKit\n2. I got developer docs for Microsemi 10G PHY VSC8486, schematic and pcb ref designs, etc. It seems pretty good. Need some time to deep dive into details.. Reading through the user guide for ECP5 IP core, the max performance of the core is 8 Gbit/s, either in x2 or x4 case. In x2 transceivers can work at 5 Gbit/s as result we get 1 GB/s or 8 Gbit/s. In x4 transceivers can work only at 2.5 Gbit/s as the result we get the same 1 GB/s or 8 Gbit/s. \nx2\n5 GT/s x 8/10 bit/\u0422 = 4 x 10^9 bit/s = 4 Gbit/s = 0.500 GB/s\n0.500 GB/s x 2 lanes = 1 GB/s = 8 Gbit/s\nx4\n2.5 GT/s x 8/10 bit/\u0422 = 2 x 10^9 bit/s = 2 Gbit/s = 0.250 GB/s\n0.250 GB/s x 4 lanes = 1 GB/s = 8 Gbit/s\n@lukego Does it suitable for 10G NIC? \nBut for all that if we agree to use x2, we may try to use PCIe port bifurcation, split x4 to 2 by x2 and put 2 x 10G PHY on one board.\nTwo datapaths in one FPGA:\nECP5-5G\n           ----------------------------------------------\n10G PHY - | 10G MAC - Custom IP core - PCIe IP core - x2 | - \\\n          |                                              |    > PCIe x4\n10G PHY - | 10G MAC - Custom IP core - PCIe IP core - x2 | - /\n           ----------------------------------------------\nIt is the only idea for now.. > I wonder if a better PCIe core is available that can do 16G? I'm asking the Twitterverse.\nI didn't find anything. Also if we will find anything I doubt that it would perform better than IP core from chip manufacturer.\n\nPCIe bifurcation should be something that we can live with if we have to but I think (?) that significantly restricts the set of compatible motherboards/slots.\n\nYep. Agree with that.\n\nP.S. Does using the Lattice PCIe core also lock us into their proprietary toolchain instead of Yosys/nextpnr?\n\nI don't know at all. Me or somebody whom I know have never tried to use such toolchain. It would be great to get into this conversation people from these projects.\n\nHow are the licensing costs for people deploying EasyNIC?\n\nPCIe IP core is licensed per project. If we buy it for this project I think everybody can use it. But I'm not 100% sure.\nThis varies from $1000 to $3000, looked at Digikey, Mouser, etc. We may also try to request a free license, I think for open project Lattice may give it for free to promote ECP5 FPGAs.\n\n(also: sounds plausible to ship an EasyNIC with 8Gbps PCIe bandwidth if this can later be upgrade to 16Gbps with a firmware update (new PCIe endpoint core). This is a bit like the way Mellanox bring hardware to the market quickly, it's hair-raising to see all the stuff that is fixed in their early firmware updates!)\n\nTo make it possible it is necessary to write PCIe IP core from scratch or modify some opensource (Lattice IP core is provided as \"binary\"), it's very hard work. And I think it's not achievable in principle. If it was possible then Lattice would already implement it.. Having in mind restrictions and risks with ECP5 and 10G PHY, I would like to suggest an alternative way for EasyNIC.\nI looked for the available boards based on Xilinx and Intel which is suitable to the initial request - make NIC with 1 or better 2 10G ports and I found one on Aliexpress:\nhttps://www.aliexpress.com/item/xilinx-board-xilinx-fpga-board-xilixn-fpga-development-board-pcie-board-Kintex-7-XC7K420T-XC7K325T-xilinx/32907109444.html\nIt based on Kintex 7 K420T FPGA.\nIt has 4 ports of 10G.\nIt has built-in 10G transceivers.\nIt has hard IP core PCIe 2.0 x8 (32 Gbit/s)\nIt provides full access to FPGA.\nIt costs around $170.\nThe manufacturer has pretty good feedback from people from all around the world.\nMaybe we will consider this board as a starting point? Or at least we can do it parallel. While we considering ECP5, we can start writing code for this board. The main FPGA code - 10G MAC, the custom IP core (to glue 10G MAC and PCIe) - should remain the same and the driver too, I think.. Hi @mithro .\nAbout LiteEth, no I didn't. I definitely will look at it.\nDo you think it's possible to get 16 Gbps on ECP5-5G with LitePCIe?. I almost get ECP5 devkit with PCIe x4. It has LFE5UM-85 chip with 3 Gbps transceivers. I can swap the chip to LFE5UM5G-85 with 5 Gbps transceivers (they pin-to-pin compatible). Need to check devkit schematic to be 100% sure. So it's possible to prototype.. @lukego I share you point.\nI looked at YosysHQ/nextpnr, they hope to get support of Xilinx 7 series with help of https://github.com/SymbiFlow/prjxray .\nAs I said before the major problem with ECP5 based EasyNIC is XGMII 10G PHY. It is outdated technology and there is only one chip from one vendor on the market right now. And it is 10 years old. I'm waiting for reply from Microsemi (BTW, now part of Mircochip) about EoL/EoS. I don't think there will be any other vendor who will be release a new chip with XGMII interface.\nPlease don't get me wrong but I'm trying to be realistic and think what will be if some day VSC8486 disappears from stocks.. My current thoughts on the topic.\n\nI found a way how to deal with the 10G PHY problem. We can use 10G PHY with XAUI interface but additional ECP5 will be necessary.\n10G PHY (Marvell 88X3310) - XAUI - ECP5 - XGMII - ECP5-5G - PCIe x4\n\nBut we still have problems:\n- IP soft core PCIe x4 for ECP5-5G that is capable to handle 10Gpbs\n- IP soft core XAUI<->XGMII is required (a quick search didn't  show any open source version)\nLattice has one https://www.latticesemi.com/en/Products/DesignSoftwareAndIP/IntellectualProperty/IPCore/IPCores01/XAUI10GbEthernetAttachment\nCosts about $1000.\nAdditional ECP5 will also add some cost.\nFor now, IP soft core PCIe x4 for ECP5-5G seems to be impractical to be developed. At least from my point of view. If someone can invest his time to this work I can help him with HW. I don't know anybody who can help with developing of the IP core.\n\nSuggested Xilinx 7 PCIe card looks nice. Now looking through the ref. projects from the board's developer.\nhttp://www.mediafire.com/file/x44lzh3woq5bdln/XC7K420T_core_sfp_pcie.rar/file\n\nYeap, there is not open source or free IDE for Xilinx Kintex 7. ICE WebPACK doesn't support it. But I think for the current stage of the EasyNIC project it's not so critical.\nThe main FPGA code will be open source. I'm sure it is more valuable.\nFor the snabb user this board will be like Intel 82599, just the NIC with 2x10G interfaces but the driver is open and low-level firmware is open. Maybe also it will force adding support of Xilinx 7 Series in YosysHQ/nextpnr.\nWorking with Xilinx also opens up the doors to 25G and 100G cards.\n\nrethink the whole project (e.g. switch from FPGA to SoC, etc.)\n\n@lukego How do you imagine that? What kind of SoC? NPU or what?\n. Read the Marvell 88X3310 datasheet carefully.\nWith XAUI on the host side (towards FPGA) - it supports only 10GBASE-T (copper, RJ-45) on the line side.. > Guessing that really any 10G ECP5-to-ECP5 I/O interface would be okay provided it doesn't need SERDES?\nYes, but it's better not to reinvent the wheel =) \n\nECP5s cost around $5 for low-end up to $50 for high-end right?\n\nI think $5 is the cost of ECP5 without transceivers at all. ECP5 with 3G transceivers $15-20, I guess.. It seems that only Marvell 88X2222 supports XAUI for host and XFI for line (SFP+). But this PHY provides min. 2 ports 10G.. I have bad news =(\nI won't get ECP5 PCIe devkit - http://www.latticesemi.com/Products/DevelopmentBoardsAndKits/ECP5PCIExpressDevKit\nHere is the story. I ordered devkit from a local reseller, who in turn bought the devkit from one of the global stock. When my local reseller got the parcel he saw that the original package was seriously damaged and there were signs of water on the package and corrosion on the board (it seems that that the package with the board spent some time in a very wet place).  What is the saddest thing is that it was the last devkit in the stock. I have decided to not accept such parcel and requested for refund.\nI spent quite a lot of time looking for other ways to buy it but not yet achieved success.\nSo now, it is a turning point, the only way to move on with ECP5 is to design and manufacture custom board.. @lukego \nECP5 dev board are still available but this board is the only one with PCIe x4. All other boards comes with x1.. @lukego yes.\nWe still have an option to design and develop ECP5 based NIC but there are a lot of risks which we unable to research.\nSpeaking of Microsemi 10G PHY VSC8486, I didn't get any answer for Microsemi but several official resellers answered that there is no plan for EOS/EOL.. @lukego I do not have doubt that ECP5 will work as specified in Lattice docs:\n2.5G - x1, x4 modes\n5G - x2 mode\nIf the vendor who develops FPGA and PCIe IP core for this FPGA doesn't claim support of 5G x4 mode then for me, it seems suspicious. The thing that is obvious to be implemented, is missed.\nI planned to verify exactly this point on the devboard with PCIe x4 by swapping ECP5 to ECP5-5G (and some modification in power system). What I'm thinking now is to make a simple board - just ECP5-5G, PCIe x4, Flash memory, JTAG.\nI also found this board https://www.sanitaseg.com/project/singrab-board/\nthat also seems to be a good candidate for experiments. I sent an email with request twice (in Jan and Feb) but didn't get the answer.. ",
    "mithro": "Have you considered using LiteEth instead of EasyNic? It would good to see a comparison of the two cores.\nLitePCIe is also in the process of being ported to the ECP5.. @insekt The limitation is not going to be LitePCIe, it'll be the transceivers and other hard blocks.. Quick question - Do you have a working PCIe 1.0 + GigE (or maybe quad-GigE) version? Would it make sense to start somewhere there and then move towards the 10G / 25G standards once the basics work?. ",
    "szbnwer": "hi there! :)\ni just had to check out what is this yang, and i realized that ur not listed here:\nhttps://en.wikipedia.org/wiki/YANG#Implementations\nso if u would like to get some free ad for snabb, and u feel suitable for ur implementation to do so, then u could extend this list. :D\nbtw sry for the off topic!\nbests! :). "
}